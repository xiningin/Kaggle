{"cell_type":{"2086be4f":"code","c4a8536c":"code","9f8a7229":"code","2d680390":"code","9e561072":"code","624f147f":"code","526cccb9":"code","ab6758e4":"markdown","c694d399":"markdown","d18a53d6":"markdown","80e78d74":"markdown","38620b25":"markdown","062ca7d5":"markdown"},"source":{"2086be4f":"# For testing, multiprocessing and chaining dictionaries\nimport numpy as np\nimport multiprocessing\nfrom collections import ChainMap\nimport matplotlib.pyplot as plt","c4a8536c":"# Default RLenc\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  # list of run lengths\n    r = 0  # the current run length\n    pos = 1  # count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs += [pos, r]\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs += [pos, r]\n        pos += r\n        r = 0\n\n    return runs\n\n# RLE encoding, as suggested by Tadeusz Hupa\u0142o\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths","9f8a7229":"class Consumer(multiprocessing.Process):\n    \"\"\"Consumer for performing a specific task.\"\"\"\n\n    def __init__(self, task_queue, result_queue):\n        \"\"\"Initialize consumer, it has a task and result queues.\"\"\"\n        multiprocessing.Process.__init__(self)\n        self.task_queue = task_queue\n        self.result_queue = result_queue\n\n    def run(self):\n        \"\"\"Actual run of the consumer.\"\"\"\n        while True:\n            next_task = self.task_queue.get()\n            if next_task is None:\n                # Poison pill means shutdown\n                self.task_queue.task_done()\n                break\n            # Fetch answer from task\n            answer = next_task()\n            self.task_queue.task_done()\n            # Put into result queue\n            self.result_queue.put(answer)\n        return\n\n\nclass RleTask_Suggested(object):\n    \"\"\"Wrap the RLE Encoder into a Task.\"\"\"\n\n    def __init__(self, idx, img):\n        \"\"\"Save image to self.\"\"\"\n        self.idx = idx\n        self.img = img\n\n    def __call__(self):\n        \"\"\"When object is called, encode.\"\"\"\n        return {self.idx: rle_encoding(self.img)}\n\nclass RleTask(object):\n    \"\"\"Wrap the RLE Encoder into a Task.\"\"\"\n\n    def __init__(self, idx, img):\n        \"\"\"Save image to self.\"\"\"\n        self.idx = idx\n        self.img = img\n\n    def __call__(self):\n        \"\"\"When object is called, encode.\"\"\"\n        return {self.idx: RLenc(self.img)}\n\nclass MultiOriginal(object):\n    \"\"\"Perform RLE in paralell.\"\"\"\n\n    def __init__(self, num_consumers=2):\n        \"\"\"Initialize class.\"\"\"\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n        self._add_count = 0\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n\n    def add(self, idx, img):\n        \"\"\"Add a task to perform.\"\"\"\n        self._add_count += 1\n        self._tasks.put(RleTask(idx, img))\n\n    def get_results(self):\n        \"\"\"Close all tasks.\"\"\"\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        for _ in range(self._add_count):\n            singles.append(self._results.get())\n        return dict(ChainMap({}, *singles))\n\nclass MultiSuggested(object):\n    \"\"\"Perform RLE in paralell.\"\"\"\n\n    def __init__(self, num_consumers=2):\n        \"\"\"Initialize class.\"\"\"\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n        self._add_count = 0\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n\n    def add(self, idx, img):\n        \"\"\"Add a task to perform.\"\"\"\n        self._add_count += 1\n        self._tasks.put(RleTask_Suggested(idx, img))\n\n    def get_results(self):\n        \"\"\"Close all tasks.\"\"\"\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        for _ in range(self._add_count):\n            singles.append(self._results.get())\n        return dict(ChainMap({}, *singles))","2d680390":"example_batch = np.random.uniform(0, 1, size=(100, 101, 101)) > 0.5\n\n# Wrap the FastRle class into a method so we measure the time\ndef original(array):\n    results = {}\n    for i, arr in enumerate(array):\n        results['%d' % i] = RLenc(arr)\n    return results\n\ndef multi_original(array):\n    rle = MultiOriginal(4)\n    for i, arr in enumerate(array):\n        rle.add('%d' % i, arr)\n    return rle.get_results()\n\ndef suggested(array):\n    results = {}\n    for i, arr in enumerate(array):\n        results['%d' % i] = rle_encoding(arr)\n    return results\n\ndef multi_suggested(array):\n    rle = MultiSuggested(4)\n    for i, arr in enumerate(array):\n        rle.add('%d' % i, arr)\n    return rle.get_results()\n    \n# Measure the time\n%timeit -n1 original(example_batch)\n%timeit -n1 multi_original(example_batch)\n%timeit -n1 suggested(example_batch)\n%timeit -n1 multi_suggested(example_batch)\n","9e561072":"# Create a loop, collect time info for different methods\nsample_sizes = [100, 250, 500, 1000, 2000, 5000, 10000]\norg, morg, sug, msug = ([], [], [], [])\nfor n_samples in sample_sizes:\n    example_batch = np.random.uniform(0, 1, size=(n_samples, 101, 101)) > 0.5\n    result_org = %timeit -n1 -o original(example_batch)\n    result_multi_org = %timeit -n1 -o multi_original(example_batch)\n    result_sug = %timeit -n1 -o suggested(example_batch)\n    result_multi_sug = %timeit -n1 -o multi_suggested(example_batch)\n    \n    org.append(result_org.average)\n    morg.append(result_multi_org.average)\n    sug.append(result_sug.average)\n    msug.append(result_multi_sug.average)\n","624f147f":"# Plot the results\nplt.figure(dpi=150)\nax = plt.axes()\nax.plot(sample_sizes, org, label='original');\nax.plot(sample_sizes, morg, label='multi-original');\nax.plot(sample_sizes, sug, label='suggested');\nax.plot(sample_sizes, msug, label='multi-suggested');\nplt.legend()","526cccb9":"example_batch = np.random.uniform(0, 1, size=(10, 101, 101)) > 0.5\na = original(example_batch)\nb = multi_original(example_batch)\nc = suggested(example_batch)\nd = multi_suggested(example_batch)\n# Make sure they are the same\nfor key in a:\n    if a[key] != b[key]:\n        print(\"Multi processed original differs from original!\")\n    if a[key] != c[key]:\n        print(\"Suggested differs from original!\")\n    if a[key] != d[key]:\n        print(\"Multi processed suggested differs from original!\")","ab6758e4":"# Sanity check\nJust to make sure, that all the new solutions are returning the same thing!","c694d399":"# Introduction\nI don't really like waiting, so I wanted to speed up the submission process a little bit. So here is a class you can use to utilize multiprocessing in Python for creating the submission file.","d18a53d6":"# Comparisons\nJust to given an idea of the speed increase.","80e78d74":"The different solutions are, unsurprisingly, heavily dependent on sample size.","38620b25":"# Conclusion\nUsing the method provided by Tadeusz Hupa\u0142o is blazingly fast!  No performance gains are seen when using multiple processes to perform the task.\n\nMight be possible to make it faster, but I'm quite happy with this new method! \n\n\nHappy Kaggling!","062ca7d5":"# Changelog\n* Fixed argument order\n* Implemented another RLE encoding method, suggested by  Tadeusz Hupa\u0142o\n* Implemented an counter in the tasks, to properly empty queues."}}