{"cell_type":{"20e366b6":"code","c906b9e5":"code","a2f40768":"code","f0de19bf":"code","d8ad8b93":"code","47dc1d55":"code","560152c3":"code","f1fec695":"code","252baaf3":"code","b14daa83":"code","51f306f7":"code","6e0be64f":"code","8fe0b378":"code","74e0e024":"code","4d936714":"code","1d0cdd37":"code","da94a0f8":"code","ab055e0f":"code","54ee4d41":"code","379dce6a":"code","ba6e939f":"code","043cc29c":"code","5399fec2":"code","051bfc24":"code","8a970ec6":"code","f679d17b":"code","ba1bab6a":"code","d5ebde94":"code","b989f38c":"code","fb6be479":"markdown","30d1b3de":"markdown","8e190c7e":"markdown","43e0c3cb":"markdown","bc158402":"markdown","5bfe0ac2":"markdown","9d64fb9c":"markdown","de19aa02":"markdown","a203b843":"markdown","90e19dbd":"markdown","f09fd061":"markdown","07c038dc":"markdown","232533cc":"markdown","43011d0d":"markdown"},"source":{"20e366b6":"!pip install tensorflow_addons\n!pip install scikit-learn==0.22","c906b9e5":"import pandas as pd\nimport pandas_profiling\nimport numpy as np\nimport gc\nfrom sklearn.model_selection import train_test_split","a2f40768":"df = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv', low_memory=False)\ndf = df[~df[df.columns[df.isnull().any()]].isnull().any(axis=1)]\ndf = df.reset_index(drop=True)\ndf.shape","f0de19bf":"df = df.sort_values(by=['City', 'IntersectionId'])","d8ad8b93":"targets = [\n'TotalTimeStopped_p20', \n#'TotalTimeStopped_p40',\n'TotalTimeStopped_p50',\n#'TotalTimeStopped_p60', \n'TotalTimeStopped_p80',\n#'TimeFromFirstStop_p20', \n#'TimeFromFirstStop_p40',\n#'TimeFromFirstStop_p50', \n#'TimeFromFirstStop_p60',\n#'TimeFromFirstStop_p80',\n'DistanceToFirstStop_p20',\n#'DistanceToFirstStop_p40', \n'DistanceToFirstStop_p50',\n#'DistanceToFirstStop_p60', \n'DistanceToFirstStop_p80'\n]\n\nto_drop = [\n    'DistanceToFirstStop_p60', \n    'DistanceToFirstStop_p40',\n    'TotalTimeStopped_p40',\n    'TotalTimeStopped_p60', \n    'TimeFromFirstStop_p20', \n    'TimeFromFirstStop_p40',\n    'TimeFromFirstStop_p50', \n    'TimeFromFirstStop_p60',\n    'TimeFromFirstStop_p80',\n]","47dc1d55":"Y = df[targets]\nX = df.drop(columns=targets + to_drop)","560152c3":"X, X_valid, Y, Y_valid = train_test_split(X, Y, test_size=0.1, shuffle=False, random_state=42)\nX = X.reset_index(drop=True)\nX_valid = X_valid.reset_index(drop=True)\nY = Y.reset_index(drop=True)\nY_valid = Y_valid.reset_index(drop=True)","f1fec695":"del df","252baaf3":"monthly_av = {'Atlanta1': 43, 'Atlanta5': 69, 'Atlanta6': 76, 'Atlanta7': 79, 'Atlanta8': 78, 'Atlanta9': 73,\n              'Atlanta10': 62, 'Atlanta11': 53, 'Atlanta12': 45, 'Boston1': 30, 'Boston5': 59, 'Boston6': 68,\n              'Boston7': 74, 'Boston8': 73, 'Boston9': 66, 'Boston10': 55,'Boston11': 45, 'Boston12': 35,\n              'Chicago1': 27, 'Chicago5': 60, 'Chicago6': 70, 'Chicago7': 76, 'Chicago8': 76, 'Chicago9': 68,\n              'Chicago10': 56,  'Chicago11': 45, 'Chicago12': 32, 'Philadelphia1': 35, 'Philadelphia5': 66,\n              'Philadelphia6': 76, 'Philadelphia7': 81, 'Philadelphia8': 79, 'Philadelphia9': 72, 'Philadelphia10': 60,\n              'Philadelphia11': 49, 'Philadelphia12': 40}\nmonthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, 'Atlanta8': 3.67, 'Atlanta9': 4.09,\n              'Atlanta10': 3.11, 'Atlanta11': 4.10, 'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n              'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,'Boston11': 3.98, 'Boston12': 3.73,\n              'Chicago1': 1.75, 'Chicago5': 3.38, 'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n              'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, 'Philadelphia1': 3.52, 'Philadelphia5': 3.88,\n              'Philadelphia6': 3.29, 'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 , 'Philadelphia10': 2.75,\n              'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\nmonthly_snowfall = {'Atlanta1': 0.6, 'Atlanta5': 0, 'Atlanta6': 0, 'Atlanta7': 0, 'Atlanta8': 0, 'Atlanta9': 0,\n              'Atlanta10': 0, 'Atlanta11': 0, 'Atlanta12': 0.2, 'Boston1': 12.9, 'Boston5': 0, 'Boston6': 0,\n              'Boston7': 0, 'Boston8': 0, 'Boston9': 0, 'Boston10': 0,'Boston11': 1.3, 'Boston12': 9.0,\n              'Chicago1': 11.5, 'Chicago5': 0, 'Chicago6': 0, 'Chicago7': 0, 'Chicago8': 0, 'Chicago9': 0,\n              'Chicago10': 0,  'Chicago11': 1.3, 'Chicago12': 8.7, 'Philadelphia1': 6.5, 'Philadelphia5': 0,\n              'Philadelphia6': 0, 'Philadelphia7': 0, 'Philadelphia8': 0, 'Philadelphia9':0 , 'Philadelphia10': 0,\n              'Philadelphia11': 0.3, 'Philadelphia12': 3.4}\n\nmonthly_daylight = {'Atlanta1': 10, 'Atlanta5': 14, 'Atlanta6': 14, 'Atlanta7': 14, 'Atlanta8': 13, 'Atlanta9': 12,\n              'Atlanta10': 11, 'Atlanta11': 10, 'Atlanta12': 10, 'Boston1': 9, 'Boston5': 15, 'Boston6': 15,\n              'Boston7': 15, 'Boston8': 14, 'Boston9': 12, 'Boston10': 11,'Boston11': 10, 'Boston12': 9,\n              'Chicago1': 10, 'Chicago5': 15, 'Chicago6': 15, 'Chicago7': 15, 'Chicago8': 14, 'Chicago9': 12,\n              'Chicago10': 11,  'Chicago11': 10, 'Chicago12': 9, 'Philadelphia1': 10, 'Philadelphia5': 14,\n              'Philadelphia6': 15, 'Philadelphia7': 15, 'Philadelphia8': 14, 'Philadelphia9':12 , 'Philadelphia10': 11,\n              'Philadelphia11': 10, 'Philadelphia12': 9}\n\nmonthly_sunshine = {'Atlanta1': 5.3, 'Atlanta5': 9.3, 'Atlanta6': 9.5, 'Atlanta7': 8.8, 'Atlanta8': 8.3, 'Atlanta9': 7.6,\n              'Atlanta10': 7.7, 'Atlanta11': 6.2, 'Atlanta12': 5.3, 'Boston1': 5.3, 'Boston5': 8.6, 'Boston6': 9.6,\n              'Boston7': 9.7, 'Boston8': 8.9, 'Boston9': 7.9, 'Boston10': 6.7,'Boston11': 4.8, 'Boston12': 4.6,\n              'Chicago1': 4.4, 'Chicago5': 9.1, 'Chicago6': 10.4, 'Chicago7': 10.3, 'Chicago8': 9.1, 'Chicago9': 7.6,\n              'Chicago10': 6.2,  'Chicago11': 3.6, 'Chicago12': 3.4, 'Philadelphia1': 5.0, 'Philadelphia5': 7.9,\n              'Philadelphia6': 9.0, 'Philadelphia7': 8.9, 'Philadelphia8': 8.4, 'Philadelphia9':7.9 , 'Philadelphia10': 6.6,\n              'Philadelphia11': 5.2, 'Philadelphia12': 4.4}\n\ncenter_latitude = {\"Atlanta\":33.753746,\n                             \"Boston\":42.361145,\n                             \"Chicago\":41.881832,\n                             \"Philadelphia\":39.952583\n                  }\ncenter_longitude = {\"Atlanta\":-84.386330,\n                             \"Boston\": -71.057083,\n                             \"Chicago\": -87.623177,\n                             \"Philadelphia\":-75.165222\n                   }\n\ndirections = {\n    'N': 0,\n    'NE': np.pi\/4,\n    'E': np.pi\/2,\n    'SE': 3*np.pi\/4,\n    'S': np.pi,\n    'SW': -3*np.pi\/4,\n    'W': -np.pi\/2,\n    'NW': -np.pi\/4\n}","b14daa83":"def pre_process(X, Y=None):\n    X['IntersectionId'] = X['IntersectionId'].astype('str') + X['City']\n    X['city_month'] = X[\"City\"] + X[\"Month\"].astype(str)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly temperature\n    X[\"average_temp\"] = X['city_month'].map(monthly_av)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n    X[\"average_rainfall\"] = X['city_month'].map(monthly_rainfall)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly snowfall\n    X[\"average_snowfall\"] = X['city_month'].map(monthly_snowfall)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly daylight\n    X[\"average_daylight\"] = X['city_month'].map(monthly_daylight)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly sunshine\n    X[\"average_sunshine\"] = X['city_month'].map(monthly_sunshine)\n    \n    \n    X[\"Center_Latitude\"] = X['City'].map(center_latitude)\n    X[\"Center_Longitude\"] = X['City'].map(center_longitude)\n    X[\"CenterDistance\"] = np.sqrt((X['Latitude'] - X[\"Center_Latitude\"]) ** 2 + (X['Center_Longitude'] - X[\"Longitude\"]) ** 2)\n    \n    X['SameStreet'] = X['EntryStreetName'] ==  X['ExitStreetName']\n    X['SameHeading'] = X['EntryHeading'] ==  X['ExitHeading']\n    X['Vector'] = X['EntryHeading'] + X['ExitHeading']\n    X['Hour_x'] = np.cos(X['Hour'] * np.pi\/12.)\n    X['Hour_y'] = np.sin(X['Hour'] * np.pi\/12.)\n    X['Month_x'] = np.cos(X['Month'] * np.pi\/6.)\n    X['Month_y'] = np.sin(X['Month'] * np.pi\/6.)\n    X['is_day'] = 0\n    X.iloc[X[(X['Hour'] > 5) & (X['Hour'] < 20)].index, X.columns.get_loc('is_day')] = 1 \n    \n    for street_dir in ['Entry', 'Exit']:\n        data = np.char.lower(X[street_dir + 'Heading'].values.astype('str'))\n        # N => Y +1\n        # S => Y -1\n        # E => X +1\n        # W => X -1\n        X['NS_' + street_dir] = np.where(np.char.rfind(data, 'N') > -1, 1, 0)\n        X['NS_' + street_dir] = np.where(np.char.rfind(data, 'S') > -1, -1, X['NS_' + street_dir].values)\n        X['EW_' + street_dir] = np.where(np.char.rfind(data, 'E') > -1, 1, 0)\n        X['EW_' + street_dir] = np.where(np.char.rfind(data, 'W') > -1, -1, X['EW_' + street_dir].values)\n        X[street_dir + '_Angle'] = X[street_dir + 'Heading'].map(directions)\n\n    X['Angle'] = X['Exit_Angle'] - X['Entry_Angle'] \n    X['x_Angle'] = np.cos(X['Angle'].values)\n    X['y_Angle'] = np.sin(X['Angle'].values)\n\n    X['NS'] = X['NS_Exit'] - X['NS_Entry'] \n    X['EW'] = X['EW_Exit'] - X['EW_Entry']\n    \n    for street_dir in ['Entry', 'Exit']:\n        data = np.char.lower(X[street_dir + 'StreetName'].values.astype('str'))\n        for type_cat in ['road', 'way', 'street', 'avenue', 'boulevard', 'lane', 'drive', 'terrace', 'place', 'court', 'plaza', 'square']:\n            X['Is' + street_dir + type_cat] = np.char.rfind(data, type_cat) > -1\n            \n    #X = X.drop(columns=['IntersectionId', 'Center_Latitude', 'Center_Longitude', 'city_month', 'Latitude', 'Longitude', 'CenterDistance' ])\n    #X = X.drop(columns=['EntryStreetName', 'ExitStreetName' ])\n\n    road_type = []\n    for street_dir in ['Entry', 'Exit']:\n        for type_cat in ['road', 'way', 'street', 'avenue', 'boulevard', 'lane', 'drive', 'terrace', 'place', 'court', 'plaza', 'square']:\n            road_type.append('Is' + street_dir + type_cat)\n    \n    return X[[\n        'CenterDistance',\n        'EntryHeading',\n        'ExitHeading',\n        'NS_Entry',\n        'EW_Entry',\n        'NS_Exit',\n        'EW_Exit',\n        'Entry_Angle',\n        'Exit_Angle',\n        'NS',\n        'EW',\n        'Angle',\n        'x_Angle',\n        'y_Angle',\n        'is_day',\n        'SameStreet',\n        'SameHeading',\n        'Vector',\n        'Hour_x',\n        'Hour_y',\n        'Month_x',\n        'Month_y',\n        'City',\n        'average_temp',\n        'average_rainfall',\n        'average_snowfall',\n        'average_daylight',\n        'average_sunshine',\n        *road_type\n    ]]","51f306f7":"class custom_column_selector:\n    def __init__(self, *, type_select, min_nunique=1, max_nunique=None, unicity_ratio=0.7, reverse=False):\n        self.type_select = type_select\n        self.min_nunique = min_nunique\n        self.max_nunique = max_nunique\n        self.unicity_ratio = unicity_ratio\n        self.reverse = reverse\n\n    def __call__(self, df):\n        if not hasattr(df, 'iloc'):\n            raise ValueError(\"make_column_selector can only be applied to \"\n                             \"pandas dataframes\")\n        df_row = df.iloc[:1]\n        df_row = df_row.select_dtypes(include=self.type_select)\n        cols = df_row.columns.tolist()\n        min_cols = df_row.columns[df[cols].nunique() > self.min_nunique].tolist()\n        max_cols = cols\n        if self.max_nunique is not None:\n            max_cols = df_row.columns[df[cols].nunique() < self.max_nunique].tolist()\n\n        return list(set(min_cols).intersection(set(max_cols)))      ","6e0be64f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder, FunctionTransformer, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline","8fe0b378":"ct = ColumnTransformer(\n    [\n        (\n            'one-hot', \n            OneHotEncoder(sparse=False, handle_unknown='ignore'),\n            custom_column_selector(type_select=['object', 'int64', 'float64'], max_nunique=100)\n        ),\n        (\n            'label-encode', \n            OrdinalEncoder(),\n            custom_column_selector(type_select=['object', 'int64', 'float64'], min_nunique=100, max_nunique=1000)\n        ),\n        (\n            'identity', \n            'passthrough',\n            custom_column_selector(type_select=['int64', 'float64'], min_nunique=1000)\n        )\n    ], \n    remainder='drop', \n    sparse_threshold=0, \n    n_jobs=None, \n    #transformer_weights=None, \n    verbose=True)","74e0e024":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow_addons.optimizers import RectifiedAdam, Lookahead\nfrom sklearn.base import BaseEstimator\n\nfrom tensorflow_addons.activations import gelu","4d936714":"def create_model(grid_params, in_dim, out_dim, patience=20, loss='rmse', activation='sigmoid'):\n    \n    mul_input = grid_params['mul_input']\n    n_layer = grid_params['n_layer']\n    \n    first_layer_size = int(in_dim*mul_input)\n    hidden_layers = []\n    for i_layer in range(n_layer, 0, -1):\n        layer_size = int(((first_layer_size - out_dim) \/ n_layer) * i_layer + out_dim)\n        hidden_layers.append(layer_size)\n\n    print(\"Input dim:\" + str(in_dim))\n    print(\"Hidden Layers:\" + str(hidden_layers))\n    print(\"Output dim:\" + str(out_dim))\n\n    model = Sequential()\n    \n    model.add(Dense(in_dim,input_shape=[in_dim],activation=gelu))\n    #model.add(BatchNormalization())\n    model.add(Dropout(.5))\n    \n    for layer in hidden_layers:\n        model.add(Dense(layer,activation=gelu))\n        #model.add(BatchNormalization())\n        model.add(Dropout(.5))\n    \n    model.add(Dense(out_dim, activation=activation))\n    \n    radam = RectifiedAdam()\n    ranger = Lookahead(radam, sync_period=6, slow_step_size=0.5)\n    optimizer = ranger#Adam(learning_rate=0.001)\n    \n    es = EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience, restore_best_weights=True)\n    es.set_model(model)\n\n    model.compile(optimizer=optimizer, loss=[loss], metrics=[])\n    \n    return model, [ es ]\n","1d0cdd37":"class KerasModel(BaseEstimator):\n\n    def __init__(\n        self, \n        n_layer=1, \n        mul_input=1.75, \n        patience=5,\n        batch_size=32,\n        loss='msle',\n        activation='sigmoid'\n        ):\n        self._estimator_type = 'reg' \n        self.n_layer = n_layer\n        self.mul_input = mul_input\n        self.patience = patience\n        self.loss = loss\n        self.activation = activation\n        self.batch_size = batch_size\n        #self.__name__ = self._wrapped_obj.__class__.__name__ + \"PredictWrapper\"\n\n    def __repr__(self):\n        if not hasattr(self, 'model'):\n            return \"Empty\"\n        return self.model.__repr__()\n\n    def __str__(self):\n        if not hasattr(self, 'model'):\n            return \"Empty\"\n        return self.model.__str__()\n        \n    def fit(self, X, Y):\n        model, cbs = create_model(\n            self.get_params(),\n            X.shape[1],\n            Y.shape[1],\n            patience=self.patience,\n            loss=self.loss,\n            activation=self.activation\n        )\n        X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n        self.model = model\n        self.model.fit(X_train,y_train, batch_size=self.batch_size,epochs=10000, validation_data=[X_valid,y_valid], verbose=2, callbacks=cbs)\n        return self\n\n    def predict(self, *args, **kwargs):\n        return self.model.predict(*args, **kwargs)\n","da94a0f8":"model = KerasModel(n_layer=3, mul_input=8, batch_size=1024, patience=10, activation=None, loss='mse')","ab055e0f":"pipeline = Pipeline(steps=[\n    ('feature-engineering', FunctionTransformer(pre_process)), # First, we build more features\n    ('data-prep', ct), # Then, we apply columns transformations\n    ('robust-scaler', RobustScaler()), # Then, we standard-scale the whole dataset\n    #('pca', PCA(.9999)), # Should I use PCA to reduce dimension ?\n    ('model', model)\n], verbose= True)","54ee4d41":"%%time\n# Fitting the whole pipeline\npipeline.fit(X, Y.values)","379dce6a":"#raise Exception('Stop HERE')","ba6e939f":"# Forcing memory cleaning (needed for XGBppst or LGBM)\ndel X, Y\ngc.collect()","043cc29c":"%%time\nY_pred = pipeline.predict(X_valid)","5399fec2":"from sklearn.metrics import mean_squared_error\nmean_squared_error(Y_valid, Y_pred, squared=False)","051bfc24":"del X_valid, Y_valid, Y_pred\ngc.collect()","8a970ec6":"X_test = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv', low_memory=False)","f679d17b":"Y_test = pipeline.predict(X_test)\nres_df = pd.DataFrame(data=Y_test, columns=targets)\nres_df['RowId'] = X_test['RowId']\ndel X_test\ngc.collect()","ba1bab6a":"res_map = {\n    'TotalTimeStopped_p20':'0',\n    'TotalTimeStopped_p50':'1',\n    'TotalTimeStopped_p80':'2',\n    'DistanceToFirstStop_p20':'3',\n    'DistanceToFirstStop_p50':'4',\n    'DistanceToFirstStop_p80':'5'\n}\nfinal_df = pd.DataFrame()\nfinal_df['RowId'] = res_df['RowId']\nfor key, value in res_map.items():\n    final_df[value] = res_df[key]\nfinal_df = pd.melt(final_df, id_vars=['RowId'], value_vars=['0','1','2','3','4','5'], var_name='target', value_name='result')","d5ebde94":"final_df['RowId'] = final_df['RowId'].astype('str')\nfinal_df['target'] = final_df['target'].astype('str')\nfinal_df['RowId'] = final_df['RowId'] + '_' + final_df['target']\nfinal_df = final_df.rename(columns={\n    'RowId': 'TargetId',\n    'result': 'Target'\n})\nfinal_df = final_df.drop(columns=['target'])\nfinal_df.to_csv('final_res.csv', index=False, sep=',', encoding='utf-8')","b989f38c":"del res_df\ngc.collect()","fb6be479":"# Feature engineering","30d1b3de":"# Evaluating model performance","8e190c7e":"#\u00a0Preparing ColumnTransformer","43e0c3cb":"# Defining whole pipeline with model","bc158402":"Columns with 2-100 modalities should be one hot encoded, whatever the type\nColumns with 101-1000 modalities should be label-encoded, whatever the type\nString columns with more than 1000 modalities should be dropped\nNumber columns with more than 1000 modalities should be kept intact","5bfe0ac2":"# Defining valid set for testing","9d64fb9c":"# Making prediction on test","de19aa02":"# Keras model definition","a203b843":"Note : Most of the following come from public kernels","90e19dbd":"# Post processing","f09fd061":"# Importing data + defining targets","07c038dc":"# Installing packages","232533cc":"Sklearn function to auto-select columns based on conditions","43011d0d":"# Custom column selector"}}