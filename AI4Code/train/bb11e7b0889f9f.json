{"cell_type":{"e2e2d10e":"code","a4a54602":"code","f02e7056":"code","3b3aa631":"code","781d4324":"code","81334574":"code","ff15d802":"code","d44b23cb":"code","929cf479":"code","ad4e1410":"code","1e0a0692":"code","7ae66494":"code","d1a2e3df":"code","f4935416":"code","66294500":"code","399b550a":"code","9af84064":"code","b5de535b":"code","d5de931b":"code","99b52236":"code","84f5294c":"code","ad024d16":"code","8e5d6fd0":"code","86c6c2a4":"code","71628e32":"code","8525bc2b":"markdown","306164e1":"markdown","d49c2abf":"markdown","a84303db":"markdown","f3b98dc2":"markdown","90245185":"markdown","6e1735b2":"markdown","167d4767":"markdown","5aba63b1":"markdown","e7c37ac4":"markdown","750409ee":"markdown","5d87b595":"markdown","a3804ac8":"markdown","80e4fd52":"markdown","e6d16ea9":"markdown"},"source":{"e2e2d10e":"import re\nimport string\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport collections\nimport seaborn as sns\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","a4a54602":"plt.rcParams['figure.figsize']=(16,8)\nsw = set(STOPWORDS) \nsns.set()","f02e7056":"df = pd.read_csv(\"..\/input\/trumps-legacy\/Trumps Legcy.csv\")\ndf.head()","3b3aa631":"emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)","781d4324":"def clean_tweets(tweet):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(tweet)\n \n    tweet = re.sub(r':', '', tweet)\n    tweet = re.sub(r'\u201a\u00c4\u00b6', '', tweet)\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n \n    tweet = emoji_pattern.sub(r'', tweet)\n \n    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n    filtered_tweet = []\n \n    for w in word_tokens:\n        if w not in stop_words and w not in string.punctuation:\n            filtered_tweet.append(w)\n    return ' '.join(filtered_tweet).lower()\n\ndf['text'] = df['text'].apply(lambda x: clean_tweets(x))","81334574":"df.head()","ff15d802":"emo_words_new = pd.read_csv('..\/input\/emolex\/DepecheMood.tsv', delimiter='\\t')\nemo_words_new['EMOTIONS'] = list(emo_words_new.eq(emo_words_new.max(1), axis=0).dot(emo_words_new.columns))\nemo_words_new.head()","d44b23cb":"!pip install xlrd","929cf479":"!pip install openpyxl","ad4e1410":"emo_words = pd.read_excel('..\/input\/emolex\/NRC EmoLex.xlsx')\nemo_words = emo_words[emo_words['association'] == 1]\nemo_words = emo_words.drop(['association'], axis=1)","1e0a0692":"pos_words = pd.read_excel('\/kaggle\/input\/sentiment-lexicons\/pos-words.xlsx')\nneg_words = pd.read_excel('\/kaggle\/input\/sentiment-lexicons\/neg-words.xlsx')","7ae66494":"filters = ['positive', 'negative']\nemolex_sents = emo_words[emo_words.emotion.isin(filters)]\nemolex_sents.head()","d1a2e3df":"n_pos = ['positive'] * len(pos_words)\nn_neg = ['negative'] * len(neg_words)\n\ntemp_pos_df = pd.DataFrame()\ntemp_pos_df['word'] = list(pos_words.words)\ntemp_pos_df['emotion'] = n_pos\n\ntemp_neg_df = pd.DataFrame()\ntemp_neg_df['word'] = list(neg_words.words)\ntemp_neg_df['emotion'] = n_neg\n\ntemp_final = pd.concat([temp_pos_df, temp_neg_df])\nemolex_sents = pd.concat([emolex_sents, temp_final])\n\nemolex_sents.head()","f4935416":"def pos_neg_words(text):\n    pos = []\n    neg = []\n    for word in text.split():\n        if word in list(emolex_sents.word):\n            emo = emolex_sents[emolex_sents.word == word].iloc[0,1]\n            if emo == 'positive':\n                pos.append(word)\n            elif emo == 'negative':\n                neg.append(word)\n    return pos, neg","66294500":"pos = []\nneg = []\ntexts = []\nfor text in df.text:\n    p, n = pos_neg_words(text)\n    pos.append(p)\n    neg.append(n)\n\ndf['positive_words'] = pos\ndf['negative_words'] = neg","399b550a":"df.head()","9af84064":"# Happy Emoticons\nemoticons_happy = set([\n    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n    '<3'\n    ])\n \n# Sad Emoticons\nemoticons_sad = set([\n    ':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('\n    ])","b5de535b":"def emotion_of_text(text):\n    emo = []\n    for word in text.split():\n        if word in list(emo_words_new.WORDS):\n            e = emo_words_new[emo_words_new.WORDS == word].iloc[0,-1]\n            emo.append(e.lower())\n        if word in emoticons_happy:\n            print(word)\n            emo.append('happy')\n        if word in emoticons_sad:\n            print(word)\n            emo.append('sad')\n    return list(set(emo))","d5de931b":"# emo = []\n\n# for text in df.text:\n#     emo.append(emotion_of_text(text))\n\n# df['emotions_in_tweet'] = emo","99b52236":"df = pd.read_csv('..\/input\/trumpp\/trump.csv')\ndf.head()","84f5294c":"the_list = list(df.text)\np_words = list(df.positive_words)\nflat_list = [item for sublist in p_words for item in literal_eval(sublist)]\np_words_count = collections.Counter(flat_list)\ndf2 = pd.DataFrame(p_words_count.most_common(20), columns=['word', 'frequency'])\ndf2.plot(kind='barh', x='word', figsize=(16,8))","ad024d16":"n_words = list(df.negative_words)\nflat_list = [item for sublist in n_words for item in literal_eval(sublist)]\nn_words_count = collections.Counter(flat_list)\ndf3 = pd.DataFrame(n_words_count.most_common(20), columns=['word', 'frequency'])\ndf3.plot(kind='barh', x='word', figsize=(16,8))","8e5d6fd0":"e_words_count = {}\nfor emo_list in df.emotions_in_tweet:\n    for emo in literal_eval(emo_list):\n        if emo in e_words_count:\n            e_words_count[emo] += 1\n        else:\n            e_words_count[emo] = 1\n\nplt.pie([float(v) for v in e_words_count.values()], labels=[k for k in e_words_count],\n           autopct=None, startangle=140, explode=(0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02))\nplt.axis('equal')\nplt.show()","86c6c2a4":"neg_p = 0\npos_p = 0 \nangry_words = []\nfor row in df.iterrows():\n    if 'inspired' in literal_eval(row[1][-1]):\n        neg_p += len(set(literal_eval(row[1][-2])))\n        angry_words.append(literal_eval(row[1][-2]))\n        pos_p += len(set(literal_eval(row[1][-3])))\n        angry_words.append(literal_eval(row[1][-3]))\nangry_words = ' '.join([item for sublist in angry_words for item in sublist])\nwordcloud = WordCloud(width = 600, height = 600, \n                background_color ='white', \n                stopwords = sw, \n                min_font_size = 10).generate(angry_words) ","71628e32":"# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","8525bc2b":"## Clean Tweets","306164e1":"#### Cleaning","d49c2abf":"## Utilizing External Lexicon Data","a84303db":"## Importing Libraries","f3b98dc2":"#### Emoji Pattern","90245185":"## Load Data","6e1735b2":"## 20 Most Common Positive Words","167d4767":"## Extracting Emotions From Tweets","5aba63b1":"#### Findings:\n* 62% Positive Words\n* 38% Negative Words","e7c37ac4":"#### Uncomment the code in cell below to extract emotions. To save time, I already did it on a personal machine and will load the .csv instead.","750409ee":"## Emotions Distribution","5d87b595":"## Extracting Positive, Negative Words From Tweets","a3804ac8":"## 20 Most Common Negative Words","80e4fd52":"## Words Behind Most Dominant Emotion (Inspired)","e6d16ea9":"## Set Parameterts"}}