{"cell_type":{"c72b556c":"code","c3aa149f":"code","35532d76":"code","e80faa9e":"code","dff880b5":"code","014d450b":"code","4f327f3a":"code","acfd0b73":"code","c7874f2f":"code","c48df7f7":"code","a9b59414":"code","b850e037":"code","6d3cf362":"code","c0315cd6":"code","eaccfe67":"code","3b092f90":"code","f95ec5ce":"code","6fabbc0a":"code","2aa97ad9":"code","d8d06639":"code","37f88588":"code","e6a58961":"code","ad94b490":"code","f09d5bed":"code","978ec898":"code","d764e6f8":"code","01d66855":"code","f6199a87":"markdown","0f410d0b":"markdown","bdffcfa3":"markdown","751779af":"markdown","47f41a76":"markdown","1975f7c9":"markdown","7abc84fd":"markdown","82af3f0d":"markdown","aed21db5":"markdown","2d0d70ab":"markdown","c20662e0":"markdown","e0778b4c":"markdown","2fb891dd":"markdown","393afb15":"markdown","36cacaa6":"markdown","014fdfb9":"markdown"},"source":{"c72b556c":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()\nsns.set(rc={'figure.figsize':(11.7,8.27)})","c3aa149f":"train = pd.read_csv('..\/input\/train.csv')","35532d76":"# Those are key identities model's bias of which we need to minimize\nkey_ident = ['male',\n             'female',\n             'homosexual_gay_or_lesbian',\n             'christian',\n             'jewish',\n             'muslim',\n             'black',\n             'white',\n             'psychiatric_or_mental_illness']","e80faa9e":"# Subset of data labeled based on mentioned identity of the text\nidentity_df = train.iloc[:, train.columns != 'parent_id'].dropna()\nprint('labeled identity samples:', identity_df.shape[0])","dff880b5":"identity_df = train[key_ident + ['comment_text', 'target']].dropna()\nfor identity in key_ident:\n    identity_df[identity] = identity_df[identity]\\\n                            .apply(lambda x: 1 if x >= 0.5 else 0)\nidentity_df['sum'] = identity_df[key_ident].sum(axis=1)\nidentity_df['sum'].value_counts().plot(kind='bar').set_title('number of key identities per comment')","014d450b":"# Somehow this comment managed to mention almost all of them\nidentity_df[identity_df['sum'] == 8]['comment_text'].values[0].replace('\\n', ' ')","4f327f3a":"groups_sampels = identity_df[key_ident].sum()\ny = groups_sampels.index.values\nx = groups_sampels.values\nsns.barplot(x=x, y=y).set_title('# of key identity samples')","acfd0b73":"identity_df['target'] = identity_df['target'].apply(lambda x: 1 if x >= 0.5 else 0)\ninsults = []\nfor ident in key_ident:\n    total = identity_df[(identity_df[ident] == 1) & (identity_df['target'] == 1)].shape[0]\n    insults.append(total)\ndf = pd.DataFrame(data={'toxic':np.array(insults), 'total': x, 'group':y})\nsns.barplot(x='toxic', y='group', data=df).set_title('# of hatefull comments per group')","c7874f2f":"sns.barplot(x='total', y='group', data=df, color = \"red\")\nsns.barplot(x='toxic', y='group', data=df, color = \"#0000A3\").set_title('toxic to all comments')","c48df7f7":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn import metrics","a9b59414":"# Define bias metrics, then evaluate our new model for bias using the validation set predictions\n# https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples \n        and the background positive examples.\n    \"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples \n        and the background negative examples.\n    \"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups \n      and one model.\n    \"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, \n                                                    subgroup, \n                                                    label_col, \n                                                    model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, \n                                            subgroup, \n                                            label_col, \n                                            model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, \n                                            subgroup, \n                                            label_col, \n                                            model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n#-----------------------------------------------------------------------------------------------------\n# Calculate the final score\n\ndef calculate_overall_auc(df, oof_name):\n    true_labels = df['target']\n    predicted_labels = df[oof_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n","b850e037":"# adding preprocessing from this kernel: https:\/\/www.kaggle.com\/taindow\/simple-cudnngru-python-keras\npunct_mapping = {\"_\":\" \", \"`\":\" \"}\npunct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])    \n    for p in punct:\n        text = text.replace(p, f' {p} ')     \n    return text\nidentity_df['comment_text'] = identity_df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\nidentity_df['comment_text'] = identity_df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","6d3cf362":"# Our Model and predict function\n# Taken from https:\/\/www.kaggle.com\/artgor\/toxicity-eda-model-interpretation-and-more\nlogreg = LogisticRegression()\noof_name = 'predicted_target'\n\ndef fit_log_reg(X_train, y_train, valid_df):\n    logreg.fit(X_train, y_train)\n    valid_df[oof_name] = logreg.predict_proba(valid_vectorized)[:, 1]\n    return valid_df","c0315cd6":"train_df, valid_df = train_test_split(identity_df, test_size=0.2)\ny_valid = valid_df['target']\nfor col in key_ident + ['target']:\n    valid_df[col] = np.where(valid_df[col] >= 0.5, True, False)","eaccfe67":"%%time\ntokenizer = TweetTokenizer()\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), \n                             tokenizer=tokenizer.tokenize, \n                             max_features=30000)\nvectorizer.fit(identity_df['comment_text'].values)\nvalid_vectorized = vectorizer.transform(valid_df['comment_text'].values)","3b092f90":"# Select only examples with toxic comments mentioning identity\nwomen_negative = train_df[~((train_df['female'] == True) & \n                            (train_df['target'] == False))]\ny_train = women_negative['target']\ntrain_vectorized = vectorizer.transform(women_negative['comment_text'].values)","f95ec5ce":"track_column = ['female']\nvalid_df = fit_log_reg(train_vectorized, y_train, valid_df)\npretty_cols = ['subgroup', 'subgroup_auc', 'bpsn_auc', 'bnsp_auc', 'subgroup_size']\n\ndef get_scores(valid_df, track_column, oof_name):\n    bias_metrics_df = compute_bias_metrics_for_model(valid_df, \n                                                     track_column, \n                                                     oof_name, 'target')\n    final_metric = get_final_metric(bias_metrics_df, \n                                    calculate_overall_auc(valid_df, oof_name))\n    \n    return bias_metrics_df[pretty_cols], final_metric\n\nmetrics_df, final_metric = get_scores(valid_df, track_column, oof_name)\nroc_auc = metrics.roc_auc_score(valid_df[track_column], valid_df['predicted_target'])\nprint('bias score:', final_metric)\nprint('classic roc_auc_score:', roc_auc)\nmetrics_df","6fabbc0a":"# Now select only examples without toxic coments mentioning the identity\nwomen_pos = train_df[((train_df['female'] == True) & (train_df['target'] == False))]\n# We will inject this data in 10 steps\nn = 10\nsplit = women_pos.shape[0]\/\/n\ndfs = [women_pos.iloc[i*split:(i+1)*split].copy() for i in range(n)]","2aa97ad9":"# Injecting positive comments, mentioning identity and retraining model each time\nmetrics_dfs = [metrics_df]\nfinal_metrics = [(final_metric, roc_auc)]\nfor i in range(n):\n    women_negative = pd.concat([women_negative, dfs[i]])\n    train_vectorized = vectorizer.transform(women_negative['comment_text'].values)\n    y_train = women_negative['target']\n    valid_df = fit_log_reg(train_vectorized, y_train, valid_df)\n    metrics_df, final_metric = get_scores(valid_df, track_column, oof_name)\n    roc_auc = metrics.roc_auc_score(valid_df[track_column], valid_df['predicted_target'])\n    metrics_dfs.append(metrics_df)\n    final_metrics.append((final_metric, roc_auc))\n    print(final_metric, roc_auc)\n    print(metrics_df)\n","d8d06639":"graf = pd.concat(metrics_dfs)\ngraf['data injection steps'] = np.arange(len(metrics_dfs))\ngraf.set_index(['data injection steps'], inplace = True)\ngraf[['subgroup_auc', 'bpsn_auc', 'bnsp_auc']].plot()","37f88588":"plt.plot(*zip(*final_metrics))\nplt.xlabel('Competition metrics')\nplt.ylabel('Standard roc_auc')","e6a58961":"# Select only examples with toxic comments related to key identities\nrelated = train_df[~(train_df['sum'] == 0)]\ny_train = related['target']\ntrain_vectorized = vectorizer.transform(related['comment_text'].values)","ad94b490":"valid_df = fit_log_reg(train_vectorized, y_train, valid_df)\nmetrics_df, final_metric = get_scores(valid_df, track_column, oof_name)\nroc_auc = metrics.roc_auc_score(valid_df[track_column], valid_df['predicted_target'])\nprint('bias score:', final_metric)\nprint('classic roc_auc_score:', roc_auc)\nmetrics_df","f09d5bed":"# Now select only examples of unrelated to key identity comments\nunrelated = train_df[(train_df['sum'] == 0)]\n# We will inject this data in 10 steps\nn = 10\nsplit = unrelated.shape[0]\/\/n\ndfs = [unrelated.iloc[i*split:(i+1)*split].copy() for i in range(n)]","978ec898":"# Injecting unrelated comments\nmetrics_dfs = [metrics_df]\nfinal_metrics = [(final_metric, roc_auc)]\nfor i in range(n):\n    related = pd.concat([related, dfs[i]])\n    train_vectorized = vectorizer.transform(related['comment_text'].values)\n    y_train = related['target']\n    valid_df = fit_log_reg(train_vectorized, y_train, valid_df)\n    metrics_df, final_metric = get_scores(valid_df, track_column, oof_name)\n    roc_auc = metrics.roc_auc_score(valid_df[track_column], valid_df['predicted_target'])\n    metrics_dfs.append(metrics_df)\n    final_metrics.append((final_metric, roc_auc))\n    print(final_metric, roc_auc)\n    print(metrics_df)","d764e6f8":"graf = pd.concat(metrics_dfs)\ngraf['data injection steps'] = np.arange(len(metrics_dfs))\ngraf.set_index(['data injection steps'], inplace = True)\ngraf[['subgroup_auc', 'bpsn_auc', 'bnsp_auc']].plot()","01d66855":"plt.plot(*zip(*final_metrics))\nplt.xlabel('Competition metrics')\nplt.ylabel('Standard roc_auc')","f6199a87":"Again, here is my interpretation. I can be wrong of course. Here is the opposite picture **bnsp_auc**  grows and **bpsn_auc** slowly decreases. Probably more unrelated samples give model a better feeling on what is considered to be toxic, so it becomes less \"hesitant\" to classify toxic comments with identity as positive. Strong correlation of identity meaningfull words with in general toxic words may lead to more false positives and lower **bpsn_auc** score. But that decrease is offset by grows in **bnsp_auc** so the final metrics is slowly growing.\n\nIn these case classic roc_auc score has positive correlation with competition metrics. Additional data doesn't have complicated cases where similar words might be used negatively and positively. So, it doesn't introduce additional complexity to the problem, and does not affect negatively model's ability to separate classes.\n\nSome conclusions:\n1. Unrelated data has a positive overall effect on bias and accuracy and should be deffinately used if there is no alternative.\n2. Though unrelated data has a positive effect it comes with a heavy price, in order to increase competition metrics on about 2% requred 2.5 increase in amount of data. 324096 vs just 130896 samples. \n\nOur goal is to reduce bias for many identity groups, not just one. In this case data unrelated to any of the group adds a lot to computational cost and not so much to overall perfomance. If that data is replaced with identity related data it may significantly reduce bias for one group, and simultenuasly have a positive effect on perfomance for other groups, though not as strong. At the same time we preserve the same level of computational cost.","0f410d0b":"#### Metrics breakdown to help interpret results\n\n**subgroup_auc**: Here, we restrict the data set to only the examples that mention the specific identity subgroup.* A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.*\n\n**bpsn_auc**: Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not,* likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.*\n\n**bnsp_auc**: Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not,* likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.*","bdffcfa3":"So, data in this compretition is very unbalanced... But not in a way you might thing it is.","751779af":"As expected it damaged roc_auc score, but bias isn't strongly affected. It went down slightly. May be some data missclassified, or what is more likely unrelated toxic comments help the model to figure out when the comment is toxic in general. ","47f41a76":"Key Identities distribution ","1975f7c9":"Wast majority of labeled comments belong to other groups or don't have any identitites.\nSome comments are labeled with multiple identities.","7abc84fd":"So, there is lot to unpack here. Let's go step by step. First the change in **bpsn_auc** is predictably going up. We add more and more sentiment positive comments mentioning identity, model is forced to accomodate and stop discriminate \"mindlessly\". But side effect of that is falling **bnsp_auc**. The model now tries to find what separates normal comments from toxic with the same identity, that leeds to higher rate of false negative classification. **bnsp_auc** is falling because model is more \"hessitant\" to classify positive examples as such. That dinamics is seen in **subgroup_auc** score. It first rises because model begins to be less bias, but then slows down as model struggles to find a good balance between not creating false negatives and reducing biase simultaneously, as amount of data with positive sentiment grows.\n\nWhat even more interesting is, that area under roc curve and competition metrics might not be positively correlated. It maight be connected to increase in data complexity, where similar words have different meaning making the task of distinguishing between classes way harder. Since it becomes harder to find a good threashold to separate the data roc_aur falls. But our bias metrics go up, for the reasons described above. At the end of the day or goal is to create an unbiased classifier, not accurate one.\n\nCouple of conclusions:\n1. Use of roc_auc metrics in this competition is questionable decision. Relationships between it and biase are not that simple.\n2. We can dramaticly reduce biase just by adding more data, without any ml hacks. The question is just how much? it's defenitely not 50\/50 ratio. My best guess would be adding as long as competition metrics grows.\n\n![](http:\/\/imgur.com\/eozrQwR.jpg)\n\nThe image is taken from [starting kernal](http:\/\/www.kaggle.com\/dborkan\/benchmark-kernel). Judging by the resaults it seems there is plenty of room to add more data with positive sentiment. And I have to admit that it's a very counterintuitive statement, that we don't have enough postive sentiment data. I might be wrong, so take that with a grain of salt. ","82af3f0d":"Please keep in mind, that all of that is just my speculations. I might be reading data incorrectly, or there is bug in code. The main idea of the kernel is to get a better intuition on the competiton metrics. If you've managet to read till the end, here is a meme to cheer you up.\n\n![](https:\/\/imgur.com\/ROf6Iss.jpg)","aed21db5":"### 2. Effect of unrelated to identity comments\n-----------------------------\nBig advantage of data given in that competition is amount of positive identity comments. Unlike the first jigsaw competition we are dealing with comments on different political events. Thankfully those comments are full of both positive sentiment and negative sentiment samples. But what about nonrelated to any key identity comments? What role do they play in bias estimate?","2d0d70ab":"## Actual experiments\n\nWe will use Logistic Regression as our model.\nWe will use only subset of data with known identities. And also will track only one subgroup in oreder to make it simple.","c20662e0":"# Analysis of Key identity groups ","e0778b4c":"### 1. Effect of positive data mentioning identity","2fb891dd":"## Metrics Definition","393afb15":"According to the data description:\n\n> Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. **Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold.**","36cacaa6":"**Here we semulated the bias problem noticed by the jigsaw teem: the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.(namely low bpsn_auc score)\nTheir explanation of the problem:**\n\n**Why are these errors happening?**\n\n> Identity terms for more frequently targeted groups (e.g. words like \u201cblack\u201d, \u201cmuslim\u201d, \u201cfeminist\u201d, \u201cwoman\u201d, \u201cgay\u201d etc) often have higher scores because comments about those groups are over-represented in abusive and toxic comments. Unfortunately, that means that the data we used to train Perspective exhibits that same trend: the names of targeted groups appear far more often in abusive comments. For example, in many forums unfortunately it\u2019s common to use the word \u201cgay\u201d as an insult, or for someone to attack a commenter for being gay, but it is much rarer for the word gay to appear in a positive, affirming statements (e.g. \u201cI am a proud gay man\u201d). When the training data used to train machine learning models contain these comments, ML models adopt the biases that exist in these underlying distributions, picking up negative connotations as they go. When there\u2019s insufficient diversity in the data, the models can over-generalize and make these kinds of errors.\n\n[Full article](http:\/\/medium.com\/the-false-positive\/unintended-bias-and-names-of-frequently-targeted-groups-8e0b81f80a23)","014fdfb9":"# Metrics experementations\n-----------------------------------"}}