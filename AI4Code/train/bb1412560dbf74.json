{"cell_type":{"9977dd9c":"code","aded80a6":"code","8aeb7908":"code","bfcb747b":"code","1a8ed51a":"code","d04c0901":"code","fee505a1":"code","195690aa":"code","c0f21c8f":"code","23fccd2d":"code","5ba49759":"code","6c120895":"code","484b8546":"code","59ad4d81":"code","cdff9aa1":"code","a52c8da1":"code","7c84693a":"code","93843b3a":"code","7cee7cbc":"code","25fb34bd":"code","36ec5d07":"code","86105d0e":"code","fec1b2a9":"code","11d0a7f2":"code","c2dcedf8":"code","f1216f0c":"code","7f3acb43":"code","c4494907":"code","8713ecad":"code","9bd2eab7":"code","48626edc":"code","46a1057d":"code","db1c802a":"code","49dde106":"code","c0f9c6eb":"code","1e9ed299":"code","033007a0":"code","1e6edeb9":"code","ae80f7db":"code","5e677e9f":"code","5f73c5c9":"code","dcaedf38":"code","365cf6ed":"code","2f36406e":"code","09268081":"code","e96456b6":"code","e568ea3c":"code","63faebef":"code","cb6e8f40":"markdown","fb3e4603":"markdown","2da0d151":"markdown","4000eb53":"markdown","6ab60486":"markdown","986747ce":"markdown","39e1f46b":"markdown","c79ec687":"markdown","0fc10a7d":"markdown","f01dc939":"markdown","b096440b":"markdown","d25e6b2c":"markdown","e3236d93":"markdown","20ea7656":"markdown","621557e1":"markdown","aaa44e40":"markdown","180b0adf":"markdown","1f5598ac":"markdown","3d6799b4":"markdown","a2c301b8":"markdown","85404ff3":"markdown","f0e06d17":"markdown","ce724b47":"markdown","81b05e30":"markdown","08b90113":"markdown","2eeb59b0":"markdown","6bcb6f2b":"markdown","be54d341":"markdown","7f0677bd":"markdown","2827edc1":"markdown","32f47580":"markdown","500999cd":"markdown","e6d6c781":"markdown","a8597eb5":"markdown","c4a4f470":"markdown","153139a9":"markdown"},"source":{"9977dd9c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, auc, matthews_corrcoef, f1_score \nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport pickle\nimport tensorflow as tf\nfrom sklearn.model_selection import RandomizedSearchCV\n\n%matplotlib inline","aded80a6":"train=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')","8aeb7908":"train.head()","bfcb747b":"sns.countplot(x=\"target\", data=train);","1a8ed51a":"train.info()","d04c0901":"train.dtypes.to_dict()","fee505a1":"[i for i in train.columns if train[i].isna().any()]","195690aa":"X = train.iloc[:,1:-1]\ny = train.target","c0f21c8f":"def ReduceMem(data):\n    feature_cols = data.columns.tolist()\n    \n    print(\"Memory usage before: \", data.memory_usage(deep=True).sum()\/(1024**3),\" GB\")\n    for col in feature_cols:\n        if data[col].dtype=='float64':\n            data[col] = data[col].astype('float32')\n        else:\n            data[col] = data[col].astype('uint8')\n    \n    print(\"Memory usage after: \", data.memory_usage(deep=True).sum()\/(1024**3), \"GB\")\n    \n    return data","23fccd2d":"X = ReduceMem(X)","5ba49759":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)","6c120895":"X_train.shape","484b8546":"X_test.shape","59ad4d81":"model = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)","cdff9aa1":"filename = 'feature_model.sav'\npickle.dump(model, open(filename, 'wb'))","a52c8da1":"#model = pickle.load(open('feature_model.sav', 'rb'))","7c84693a":"important_features = pd.Series(model.feature_importances_, index=X_train.columns)\nimportant_features_sorted = important_features.sort_values(ascending=False)\nimportant_features_sorted","93843b3a":"plt.figure(figsize=(10,8))\nfeatures = pd.Series(important_features_sorted[:10], index=important_features_sorted.index[:10])\nfeatures.plot(kind='barh')\nplt.show()","7cee7cbc":"model1 = SelectFromModel(model, prefit=True)","25fb34bd":"#Training\nX_train_imp = model1.transform(X_train)\nX_train_imp.shape","36ec5d07":"#Testing\nX_test_imp = model1.transform(X_test)\nX_test_imp.shape","86105d0e":"X_train.columns[model1.get_support()]","fec1b2a9":"X_test.columns[model1.get_support()]","11d0a7f2":"result = []","c2dcedf8":"def print_roc_auc_score(model, test_data, label):\n    y_pred = model.predict_proba(test_data)[::,1]\n    auc = metrics.roc_auc_score(label, y_pred)    \n    fpr_logit, tpr_logit, _ = metrics.roc_curve(label, y_pred)    \n    plt.plot(fpr_logit,tpr_logit,label=\"AUC Curve, auc={:.3f})\".format(auc))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=4)\n    plt.show()    \n    print(\"AUC Score :\", auc)  \n    \n    return auc","f1216f0c":"#train the model\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X_train_imp, y_train);\nlr = print_roc_auc_score(logistic_model, X_test_imp, y_test)\nresult.append([\"Logistic regression\",lr])","7f3acb43":"#train the model\nlogistic_model_all = LogisticRegression(max_iter=500)\nlogistic_model_all.fit(X_train, y_train);\nlr_all = print_roc_auc_score(logistic_model_all, X_test, y_test)\nresult.append([\"Logistic regression all features\",lr_all])","c4494907":"xgb = XGBClassifier(random_state=10, use_label_encoder=False)\nxgb.fit(X_train_imp, y_train, eval_metric='aucpr');\nxgboost = print_roc_auc_score(xgb, X_test_imp, y_test)\nresult.append([\"XGBoost\",xgboost])","8713ecad":"xgb_all = XGBClassifier(random_state=10, use_label_encoder=False)\nxgb_all.fit(X_train, y_train, eval_metric='aucpr');\nxgboost_all = print_roc_auc_score(xgb_all, X_test, y_test)\nresult.append([\"XGBoost all features\",xgboost_all])","9bd2eab7":"adaBoost = AdaBoostClassifier()\nadaBoost.fit(X_train_imp, y_train);\nadaBoostAuc = print_roc_auc_score(adaBoost, X_test_imp, y_test)\nresult.append([\"AdaBoost\",adaBoostAuc])","48626edc":"adaBoost_all = AdaBoostClassifier()\nadaBoost_all.fit(X_train, y_train);\nadaBoost_all_auc = print_roc_auc_score(adaBoost_all, X_test, y_test)\nresult.append([\"AdaBoost all features\",adaBoost_all_auc])","46a1057d":"catBoost = CatBoostClassifier()\ncatBoost.fit(X_train_imp, y_train);\ncatBoostAuc = print_roc_auc_score(catBoost, X_test_imp, y_test)\nresult.append([\"catBoost\",catBoostAuc])","db1c802a":"catBoost_all = CatBoostClassifier()\ncatBoost_all.fit(X_train, y_train);\ncatBoostAucAll = print_roc_auc_score(catBoost_all, X_test, y_test)\nresult.append([\"catBoost all features\",catBoostAucAll])","49dde106":"lgbm = LGBMClassifier()\nlgbm.fit(X_train_imp, y_train);\nlgbm_auc = print_roc_auc_score(lgbm, X_test_imp, y_test)\nresult.append([\"lightgbm\",lgbm_auc])","c0f9c6eb":"lgbm_all = LGBMClassifier()\nlgbm_all.fit(X_train, y_train);\nlgbm_all_auc = print_roc_auc_score(lgbm_all, X_test, y_test)\nresult.append([\"lightgbm all features\",lgbm_all_auc])","1e9ed299":"df_results = pd.DataFrame(result)\ndf_results = df_results.rename(columns={0:\"ModelName\",1:\"Auc Score\"})\ndf_results","033007a0":"df_results.iloc[df_results[\"Auc Score\"].argmax()]","1e6edeb9":"lgb = LGBMClassifier(random_state = 42)\nrs_params = dict(learning_rate = [0.05,0.03,0.01,0.1,0.3],\n                 reg_lambda = [0, 20, 40],\n                 n_estimators = [1000,2000,2500,3000],\n                 max_depth = [1, 3, 4, 7, 10, 15],\n                 subsample = [0.7, 0.8, 0.9, 1],\n                 colsample_bytree = [0.7, 0.8, 0.9, 1],\n                 reg_alpha = [0, 20, 40, 50, 60])\n\nrs_lgb = RandomizedSearchCV(estimator = lgb,\n                            param_distributions = rs_params,\n                            scoring = 'roc_auc',\n                            cv = 2,\n                            random_state = 42)\n\nrs_lgb.fit(X_train, y_train)\nprint(rs_lgb.best_params_)","ae80f7db":"catBoost_final = CatBoostClassifier()\ncatBoost_final.fit(X, y)","5e677e9f":"lgb = LGBMClassifier(**rs_lgb.best_params_)\nlgb.fit(X,y)","5f73c5c9":"X_test = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\", index_col='id')\npredict = lgb.predict_proba(X_test)[::,1]\npredictions = pd.DataFrame({\"id\":X_test.index, \"target\":predict})","dcaedf38":"predictions.to_csv('submission.csv', index=False)","365cf6ed":"#pip install git+git:\/\/github.com\/keras-team\/autokeras@master#egg=autokeras","2f36406e":"#import autokeras as ak","09268081":"# Initialize the structured data classifier.\n#clf = ak.StructuredDataClassifier(overwrite=True, max_trials=5)  # It tries 3 different models.\n\n#clf.fit(X_train_imp, y_train, epochs=10)\n\n#predicted_y = clf.predict(X_test_imp)\n# Evaluate the best model with testing data.\n#print(clf.evaluate(X_test_imp, y_test))","e96456b6":"#model = clf.export_model()\n#model.summary()","e568ea3c":"#model.save('model_keras.tf')","63faebef":"#y_pred = model.predict(X_test_imp)\n\n#auc = metrics.roc_auc_score(y_test, y_pred)    \n#fpr_logit, tpr_logit, _ = metrics.roc_curve(y_test, y_pred)    \n#plt.plot(fpr_logit,tpr_logit,label=\"AUC Curve, auc={:.3f})\".format(auc))\n#plt.plot([0, 1], [0, 1], 'k--')\n#plt.xlabel('False positive rate')\n#plt.ylabel('True positive rate')\n#plt.title('ROC curve')\n#plt.legend(loc=4)\n#plt.show()    \n#print(\"AUC Score :\", auc)","cb6e8f40":"### Summary ","fb3e4603":"#### Selected features","2da0d151":"Here are the list of features selected by SelectFromModel","4000eb53":"#### All features","6ab60486":"## Classification","986747ce":"#### All features","39e1f46b":"## LightGBM","c79ec687":"## Parameter tuning","0fc10a7d":"### DataSplit \n\nThe provided test set doesn't contain a target variable, So we'll create a test set from training data to evaluate its performance.","f01dc939":"### Logistic Regression","b096440b":"#### Selected features","d25e6b2c":"#### Reading Data","e3236d93":"#### Selected features","20ea7656":"### Insights from data\n\n- The complete data contains 1000000 rows and 287 columns out of which we have 285 feature columns.\n- The target column is either 0 or 1 and both contain approximately same amount of data\n- There are no null values\n- There are no categorical variables","621557e1":"#### selected features","aaa44e40":"### XGBoost","180b0adf":"#### Importing Libraries","1f5598ac":"#### All features","3d6799b4":"#### Reduce Memory usage\n\nThere are a million rows with 285 feature columns which is taking around 2.1 gb of RAM. So, it is better to reduce some memory by changing its datatype to store fewer floating points precision.\n\nHere's a quick comparison of integer data type. which shows the values we can store in it.\n- int8 can store integers from -128 to 127.\n- int16 can store integers from -32768 to 32767.\n- int64 can store integers from -9223372036854775808 to 9223372036854775807.","a2c301b8":"The data doesn't contain any categorical variable so we can start ahead. Now lets check for any null value","85404ff3":"### Introduction\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\nIn this notebook, we'll try performance on different algorithms and choose the best one among them.","f0e06d17":"## CatBoost","ce724b47":"## Submission","81b05e30":"#### Selected features","08b90113":"#### All features","2eeb59b0":"#### Selected features","6bcb6f2b":"#### All features","be54d341":"selectFromModel object from sklearn to automatically select the features.","7f0677bd":"ploting 10 most important features","2827edc1":"### Feature Selection\n\nAs there are a lot of features, lets try by selecting only the features which contribute more to the final output","32f47580":"As we can see that models trained on selected features using ExtraTreesClassifier have performed really well even after removing 276 of its features, which really shows the power of choosing the right features. Training only on 9 features take small amount of time for training but for the sake of submission we are going to use the model with highest score i.e. catBoost model with all of the features of training data.","500999cd":"#### All features","e6d6c781":"## AdaBoost","a8597eb5":"We are going to use different models and choose the best performing model among those.","c4a4f470":"## AutoKeras\n\nAutomated machine learning (AutoML) automates the selection, composition and parameterization of machine learning models.\n\n\nwork in progress...","153139a9":"work in progress..."}}