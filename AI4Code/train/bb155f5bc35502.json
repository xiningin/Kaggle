{"cell_type":{"d77ab237":"code","aa900fff":"code","ba1605a9":"code","302b8c26":"code","18ef51fa":"code","3a582285":"code","1d85aac5":"code","21e4e454":"code","5faf6222":"code","b59106e9":"code","50aaf31a":"code","7bfd03b3":"code","4882d4d7":"code","9d8042a7":"code","693bd30b":"code","c5b25082":"code","321740a0":"code","cec68cfa":"code","0a69caea":"code","14805fa0":"code","d1e6b3d1":"code","bac5e179":"code","080a5b5e":"code","6c3a6264":"code","331aeda8":"code","08bcdc41":"code","e09040de":"code","cb027ba7":"code","d0374e5e":"code","8be8645f":"code","6bce6a3a":"markdown","b20cf2da":"markdown","6565f09e":"markdown","36673043":"markdown","1572d6b6":"markdown","f3163871":"markdown","0c7392fc":"markdown","21d6214e":"markdown","4e40c62c":"markdown","62f8f5a9":"markdown","516dd3bd":"markdown","b4464eb9":"markdown","3d1a366c":"markdown","83b17d0a":"markdown","f0f701d6":"markdown","a59c5c21":"markdown"},"source":{"d77ab237":"%%capture\n!pip install torchsummary\n!pip install imblearn","aa900fff":"import os\nimport random\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\n\nimport cv2\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom PIL import *\nfrom tqdm import tqdm\nfrom torchsummary import summary\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import  DataLoader, Dataset, ConcatDataset\nfrom sklearn.model_selection import train_test_split","ba1605a9":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","302b8c26":"train_folder= '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\nval_folder = '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'\ntest_folder = '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'","18ef51fa":"data_transforms = {\n    'train': {\n        'dataset1': transforms.Compose([transforms.Resize(255),\n            transforms.CenterCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.RandomGrayscale(),\n            transforms.RandomAffine(translate=(0.05,0.05), degrees=0),\n            transforms.ToTensor()\n           ]),\n\n        'dataset2' : transforms.Compose([transforms.Resize(255),\n            transforms.CenterCrop(224),\n            transforms.RandomHorizontalFlip(p=1),\n            transforms.RandomGrayscale(),\n            transforms.RandomAffine(translate=(0.1,0.05), degrees=10),\n            transforms.ToTensor()\n\n           ]),\n        'dataset3' : transforms.Compose([transforms.Resize(255),\n            transforms.CenterCrop(224),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(15),\n            transforms.RandomGrayscale(p=1),\n            transforms.RandomAffine(translate=(0.08,0.1), degrees=15),\n            transforms.ToTensor()\n           ]),\n    },\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]\n        )\n    ]),\n}","3a582285":"dataset1 = datasets.ImageFolder(train_folder, \n                      transform=data_transforms['train']['dataset1'])\n\ndataset2 = datasets.ImageFolder(train_folder, \n                      transform=data_transforms['train']['dataset2'])\n\ndataset3 = datasets.ImageFolder(train_folder, \n                      transform=data_transforms['train']['dataset3'])\n\nnorm1, _ = train_test_split(dataset2, test_size= 3875\/(1341+3875), shuffle=False)\nnorm2, _ = train_test_split(dataset3, test_size= 4023\/(1341+3875), shuffle=False)","1d85aac5":"dataset = ConcatDataset([dataset1, norm1, norm2])","21e4e454":"train_ds, val_ds = train_test_split(dataset, test_size=0.3, random_state=2000)","5faf6222":"Datasets = {\n    'train': train_ds,\n    'test' : datasets.ImageFolder(test_folder, data_transforms['test']),\n    'val'  : val_ds\n}","b59106e9":"Dataloaders = {\n    'train': DataLoader(Datasets['train'], batch_size = 32, num_workers = 2),\n    'test': DataLoader(Datasets['test'], batch_size = 32, shuffle = True, num_workers = 2),\n    'val': DataLoader(Datasets['val'], batch_size = 32, shuffle = True, num_workers = 2),\n}","50aaf31a":"files = []\ncategories = []\nfilenames = os.listdir(os.path.join(train_folder,'NORMAL'))\nfor name in filenames:\n    files.append(os.path.join(train_folder, 'NORMAL', name))\n    categories.append('NORMAL')\n\nfilenames = os.listdir(os.path.join(train_folder,'PNEUMONIA'))\nfor name in filenames:\n    files.append(os.path.join(train_folder, 'PNEUMONIA', name))\n    categories.append('PNEUMONIA')","7bfd03b3":"random_file_index = random.sample(range(len(files)), 9)\nrandom_fig = plt.figure(figsize = (12, 12))\nrows, cols = 3, 3\nfor i in range(9):\n    random_fig.add_subplot(rows, cols, i+1)\n    plt.imshow(cv2.imread(files[random_file_index[i]]))\n    plt.title(categories[random_file_index[i]])\n    plt.axis('off')\nplt.show()","4882d4d7":"Tr_PNEUMONIA = len([label for _, label in Datasets['train'] if label == 1])\nTr_NORMAL = len(Datasets['train']) - Tr_PNEUMONIA\nV_PNEUMONIA = len([label for _, label in Datasets['val'] if label == 1])\nV_NORMAL = len(Datasets['val']) - V_PNEUMONIA\nTe_PNEUMONIA = len([label for _, label in Datasets['test'] if label == 1])\nTe_NORMAL = len(Datasets['test']) - Te_PNEUMONIA\nPn = [Tr_PNEUMONIA, V_PNEUMONIA, Te_PNEUMONIA]\nNo = [Tr_NORMAL, V_NORMAL, Te_NORMAL]\nPn, No","9d8042a7":"fig = plt.subplots(figsize =(4, 4))\n\nbr1 = np.arange(len(Pn))\nbr2 = [x + 0.25 for x in br1]\n\nplt.bar(br1, Pn, color='r', width = 0.25, label = 'Pneumonia')\nplt.bar(br2, No, color='b', width = 0.25, label = 'Normal')\n\nplt.ylabel('Count')\nplt.xticks([r + 0.25 for r in range(len(Pn))],\n        ['Train', 'Validation', 'Test'])\nplt.legend()\nplt.show()","693bd30b":"model = torchvision.models.resnet18(pretrained=True)\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(512, 2)\n)\n\nmodel = model.to(device)\nsummary(model, input_size = (3, 224, 224))","c5b25082":"epochs = 15\nalpha = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=alpha)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)\nloss_fn = nn.CrossEntropyLoss()","321740a0":"def trainer_loop(model, trainloader, loss_fn, optimizer, scheduler = None, t_gpu = False):\n    model.train()\n    tr_loss, tr_acc = 0.0, 0.0\n    for i, data in enumerate(tqdm(trainloader)):\n        img, label = data\n        if t_gpu:\n                img, label = img.cuda(), label.cuda()\n        optimizer.zero_grad()\n        output = model(img)\n        _, pred = torch.max(output.data, 1)\n        loss = loss_fn(output, label)\n        loss.backward()\n        optimizer.step()\n        \n        tr_loss += loss.item()\n        tr_acc += torch.sum(pred == label.data)\n        torch.cuda.empty_cache()\n\n    scheduler.step() if scheduler != None else None\n    return tr_loss\/len(trainloader.dataset), 100*tr_acc\/len(trainloader.dataset)","cec68cfa":"def val_loop(model, val_loader, loss_fn, t_gpu=False):\n    model.train(False)\n    model.eval()\n    val_loss, val_acc = 0.0, 0.0\n    with torch.no_grad():\n        for i, data in enumerate(tqdm(val_loader)):\n            img, label = data\n            if t_gpu:\n                    img, label = img.cuda(), label.cuda()\n            output = model(img)\n            _, pred = torch.max(output.data, 1)\n            loss = loss_fn(output, label)\n\n            val_loss += loss.item()\n            val_acc += torch.sum(pred == label.data)\n\n    return val_loss\/len(val_loader.dataset), 100*val_acc\/len(val_loader.dataset)","0a69caea":"def train_model(epochs, model, trainloader, valloader, loss_fn, optimizer, scheduler = None, t_gpu = False):\n    stat_dict = {\n        'learning_rate':[],\n        'train_loss':    [],\n        'train_acc':     [],\n        'val_loss':      [],\n        'val_acc':       []    \n    }\n    print('*'*5+'Training Started'+'*'*5)\n    for ep in range(epochs):\n        print(f'Training epoch: {ep+1}')\n        t_loss, t_acc = trainer_loop(\n            model, trainloader, loss_fn, optimizer, scheduler, t_gpu\n        )\n        v_loss, v_acc = val_loop(\n            model, valloader, loss_fn, t_gpu\n        )\n        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n        print(f'Training   : Loss: {t_loss}    Accuracy: {t_acc}%')\n        print(f'Validation : Loss: {v_loss}    Accuracy: {v_acc}%')\n        stat_dict['learning_rate'].append(optimizer.param_groups[0][\"lr\"])\n        stat_dict['train_loss'].append(t_loss)\n        stat_dict['val_loss'].append(v_loss)\n        stat_dict['train_acc'].append(t_acc)\n        stat_dict['val_acc'].append(v_acc)\n    print('Finished Training')\n    return stat_dict","14805fa0":"hist = train_model(epochs, model, Dataloaders['train'], Dataloaders['val'], loss_fn, optimizer,  scheduler, device == 'cuda')","d1e6b3d1":"fig, ax = plt.subplots(figsize=(8,5))\n\nLT = ax.plot(np.linspace(1, epochs, epochs), hist['train_loss'], 'b-', label='Train Loss')\nLV = ax.plot(np.linspace(1, epochs, epochs), hist['val_loss'], 'r-', label='Val Loss')\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\n\nax.set_xlim([1, (len(hist['val_loss']))])\nif len(hist['val_loss']) >= 30:\n    ax.set_xticks(range(1, (len(hist['val_loss'])+1), 5))\nelif len(hist['val_loss']) >= 20:\n    ax.set_xticks(range(1, (len(hist['val_loss'])+1), 2))\nelif len(hist['val_loss']) < 20:\n    ax.set_xticks(range(1, (len(hist['val_loss'])+1)))\n\nlns = LT+LV\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc='upper right')\nax.grid('on')","bac5e179":"fig, ax = plt.subplots(figsize=(8,5))\n\nAT = ax.plot(np.linspace(1, epochs, epochs), hist['train_acc'], 'g-', label='Train Acc')\nAV = ax.plot(np.linspace(1, epochs, epochs), hist['val_acc'], 'y-', label='Val Acc')\n\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\n\nax.set_xlim([1, (len(hist['val_acc']))])\nif len(hist['val_acc']) >= 30:\n    ax.set_xticks(range(1, (len(hist['val_acc'])+1), 5))\nelif len(hist['val_acc']) >= 20:\n    ax.set_xticks(range(1, (len(hist['val_acc'])+1), 2))\nelif len(hist['val_acc']) < 20:\n    ax.set_xticks(range(1, (len(hist['val_acc'])+1)))\n\nlns = AT+AV\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc='upper right')\nax.grid('on')","080a5b5e":"fig, ax = plt.subplots(figsize=(8,5))\n\nLR = ax.plot(np.linspace(1, epochs, epochs), hist['learning_rate'], 'g-', label='Learning Rate')\n\nax.set_xlabel('Epochs')\nax.set_ylabel('Learning Rate')\n\nlabs = [l.get_label() for l in LR]\nax.legend(LR, labs, loc='upper right')\nax.grid('on')","6c3a6264":"@torch.no_grad()\ndef test_loop(model, testdata, loss_fn, t_gpu):\n    print('*'*5+'Testing Started'+'*'*5)\n    model.train(False)\n    model.eval()\n    \n    full_pred, full_lab = [], []\n    \n    TestLoss, TestAcc = 0.0, 0.0\n    for data, target in testdata:\n        if t_gpu:\n            data, target = data.cuda(), target.cuda()\n\n        output = model(data)\n        loss = loss_fn(output, target)\n\n        _, pred = torch.max(output.data, 1)\n        TestLoss += loss.item() * data.size(0)\n        TestAcc += torch.sum(pred == target.data)\n        torch.cuda.empty_cache()\n        full_pred += pred.tolist()\n        full_lab += target.data.tolist()\n\n    TestLoss = TestLoss \/ len(testdata.dataset)\n    TestAcc = TestAcc \/ len(testdata.dataset)\n    print(f'Loss: {TestLoss} Accuracy: {TestAcc}%')\n    return full_pred, full_lab","331aeda8":"testset = datasets.ImageFolder(test_folder, \n                           transform=transforms.Compose([transforms.Resize(255),\n                                                 transforms.CenterCrop(224),                                                              \n                                                 transforms.ToTensor(),\n                                                ]))\ntest_dl = DataLoader(testset, batch_size=32)","08bcdc41":"pred, lab = test_loop(model, test_dl, loss_fn, True)","e09040de":"from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm  = confusion_matrix(lab, pred)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.xlabel('Predicted Label',fontsize=18)\nplt.ylabel('True Label',fontsize=18)\nplt.show()","cb027ba7":"tn, fp, fn, tp = cm.ravel()\n\naccuracy = (np.array(pred) == np.array(lab)).sum() \/ len(pred)\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\nf1 = 2*((precision*recall)\/(precision+recall))\n\nprint(\"Accuracy of the model is {:.2f}\".format(accuracy))\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))\nprint(\"F1 Score of the model is {:.2f}\".format(f1))","d0374e5e":"torch.save(model.state_dict(), 'pneumonia_xray.pth')","8be8645f":"model.eval()\ndummy_input,_ = next(iter(Dataloaders['test']))\ntorch.onnx.export(model,\n         dummy_input.cuda(),\n         \"Pneumonia_Res.onnx\",\n         export_params=True,\n         opset_version=10,\n         do_constant_folding=True,\n         input_names = ['modelInput'],\n         output_names = ['modelOutput'],\n         dynamic_axes={'modelInput' : {0 : 'batch_size'},\n                                'modelOutput' : {0 : 'batch_size'}}) \nprint(\" \") \nprint('Model has been converted to ONNX')","6bce6a3a":"# Data Preprocessing","b20cf2da":"### Accuracy","6565f09e":"## Creating Dataloaders","36673043":"# Training","1572d6b6":"# Defining the model","f3163871":"# Preparing DataSets","0c7392fc":"## Creating Datasets","21d6214e":"### Augmentations","4e40c62c":"# Saving the Model","62f8f5a9":"## Statistics","516dd3bd":"# Prediction on the Test Set","b4464eb9":"## Statistics\n### Loss","3d1a366c":"### Learning Rate","83b17d0a":"## converting to onnx","f0f701d6":"# Setting Up Device","a59c5c21":"# Plotting confusion matrix"}}