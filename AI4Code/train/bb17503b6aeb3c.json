{"cell_type":{"6107c39b":"code","2b73a67c":"code","27cbd693":"code","f185fb89":"code","0fe4545c":"code","ff345b31":"code","408ca76a":"code","6264aab3":"code","2ddc2857":"code","c279b84e":"code","9eb968d3":"code","8ba1f91d":"code","886637b5":"code","ab19c283":"markdown","d951bdb3":"markdown","a0f6ae87":"markdown","2b3a14d2":"markdown","05408ac4":"markdown","6e0825c9":"markdown","457fc7a9":"markdown","bc3197e6":"markdown"},"source":{"6107c39b":"# importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O \n\nprint(\"list of files under the input directory:\\n\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# reading the train data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\nprint(\"\\ntrain data info looks like:\\n\")\ntrain_data.info()\ntrain_data.head(10)","2b73a67c":"# reading the test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\nprint(\"test data info looks like:\\n\")\ntest_data.info()\ntest_data.head(5)","27cbd693":"# defining the target\nX = train_data.drop(['target'],axis=1)\nprint('the shape of X is {}'.format(X.shape))\ny = train_data['target']\nprint('the shape of y is {}'.format(y.shape))\nX.head()","f185fb89":"from sklearn.preprocessing import LabelEncoder\n\nmy_encoder = LabelEncoder()\n\nX_encoded = X.copy()\nfor c in X.columns:\n    if (X[c].dtype == 'object'):\n        X_encoded[c] = my_encoder.fit_transform(X[c])\n\nX_encoded.head()","0fe4545c":"# dividing the train data to 75% train set and 25% evaluation set\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X_encoded,y,random_state=1,test_size=0.25)\nprint('the shape of X_train is {}'.format(X_train.shape))\nprint('the shape of X_test is {}'.format(X_test.shape))\nprint('the shape of y_train is {}'.format(y_train.shape))\nprint('the shape of y_test is {}'.format(y_test.shape))","ff345b31":"from sklearn.linear_model import  LogisticRegression\n\nmodel_LogR = LogisticRegression(max_iter=500, C=0.10)\nmodel_LogR.fit(X_train,y_train)\ny_pre_LogR = model_LogR.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_LogR))","408ca76a":"from sklearn.neighbors import  KNeighborsClassifier\n\nmodel_KNN = KNeighborsClassifier(n_neighbors=1)\nmodel_KNN.fit(X_train,y_train)\ny_pre_KNN = model_KNN.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_KNN))","6264aab3":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_RF = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=1)\nmodel_RF.fit(X_train,y_train)\ny_pre_RF = model_RF.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_RF))","2ddc2857":"from sklearn.preprocessing import OneHotEncoder\n\nmy_encoder_OH = OneHotEncoder()\nmy_encoder_OH.fit(X)\n\nX_encoded_OH = my_encoder_OH.transform(X)\n\nprint('the shape of X_encoded_OH is {}'.format(X_encoded_OH.shape))\n\n# dividing the train data to 75% train set and 25% evaluation set\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X_encoded_OH,y,random_state=1,test_size=0.25)\nprint('the shape of X_train is {}'.format(X_train.shape))\nprint('the shape of X_test is {}'.format(X_test.shape))\nprint('the shape of y_train is {}'.format(y_train.shape))\nprint('the shape of y_test is {}'.format(y_test.shape))","c279b84e":"from sklearn.linear_model import  LogisticRegression\n\nmodel_LogR = LogisticRegression(max_iter=5000, C=0.01)\nmodel_LogR.fit(X_train,y_train)\ny_pre_LogR = model_LogR.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_LogR))","9eb968d3":"from sklearn.neighbors import  KNeighborsClassifier\n\nmodel_KNN = KNeighborsClassifier(n_neighbors=1)\nmodel_KNN.fit(X_train,y_train)\ny_pre_KNN = model_KNN.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_KNN))","8ba1f91d":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_RF = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=1)\nmodel_RF.fit(X_train,y_train)\ny_pre_RF = model_RF.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy : ',accuracy_score(y_test,y_pre_RF))","886637b5":"# creating a dataframe of all samples\nX_test_actual=test_data.copy()\nprint('the shape of X_test_actual is {}'.format(X_test_actual.shape))\nX_all = pd.concat([X, X_test_actual])\nprint('the shape of X_all is {}'.format(X_all.shape))\n\n# encoding the dataframes\nfrom sklearn.preprocessing import OneHotEncoder\n\nmy_encoder_OH_all = OneHotEncoder()\nmy_encoder_OH_all.fit(X_all)\nX_test_actual_OH = my_encoder_OH_all.transform(X_test_actual) \nprint('the shape of X_test_actual_OH is {}'.format(X_test_actual_OH.shape))\nX_OH = my_encoder_OH_all.transform(X)\nprint('the shape of X_OH is {}'.format(X_OH.shape))\nprint('the shape of y is {}'.format(y.shape))\n\n# fitting logistic regression\nfrom sklearn.linear_model import  LogisticRegression\n\nmodel_LogR = LogisticRegression(max_iter=5000, C=0.1)\nmodel_LogR.fit(X_OH,y)\ny_pre_LogR = model_LogR.predict(X_test_actual_OH)\nprint('the shape of y_pre_LogR is {}'.format(y_pre_LogR.shape))\n\noutput = pd.DataFrame({'id': X_test_actual.id, 'target': y_pre_LogR})\noutput.to_csv('my_submission_v1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ab19c283":"For KNN, the following accuracies are achieved using different N values:\n* N=5, 0.63161\n* N=10, 0.67569\n* N=20, 0.68756\n* N=40, 0.69322\n* N=80, 0.69421\n* N=200, 0.69425\n\nAccuracy seems to plateau when increasing N. Tried weights='distance' with N=200 and it didn't change anything.\n\nNext, I try `RandomForestClassifier`.","d951bdb3":"Now I try to encode the data using `LabelEncoder` and then fit logistic regression model to predict the targets.","a0f6ae87":"The logistic regression achieves better accuracy with the onehot encoding. Here are the values for different regularization:\n* C=10, 0.75421\n* C=1, 0.75918\n* C=0.1, 0.76377\n* C=0.01, 0.75722\n* C=0.001, 0.73924\n\nNext is KNN.\n ","2b3a14d2":"The KNN runs take too long, perhaps due to the fact that calculating the distances in a 316000 dimensional space takes a lot of cpu power. For n=5, accuracy us 0.67650 which is better than the previous encoding.\n\nNext I try random forests.","05408ac4":"The above model yields classification accuracy of 0.69004 for the defaul regularization (C=1). Here's how regularization affects the accuracy:\n* C=1000, 0.68990\n* C=100, 0.69004\n* C=10, 0.69009\n* C=1, 0.69004\n* C=0.1, 0.68954\n* C=0.01, 0.68954\n* C=0.001, 0.69\n\nNext I try `KNeighborsClassifier`.\n","6e0825c9":"With random forest, the following accuracies are achieved:\n* n=100, depth=5, 0.69425\n* n=200, depth=5, 0.69425\n* n=200, depth=7, 0.69425\n* n=500, depth=10, 0.69425\n* n=1000, depth=10, 0.69425\n\nNot sure why there's no improvement in the accuracy of the RF.\n\nAt this point, the best result is from logistic regression with C=0.1. I will repeat that below and output the submission file. However, I realized that I need to fit my encoding machine to all the data rather than just X. I do that below and make the prediction. However, the experiments done upto this point with the three classifiers were on X only and so the hyperparameters will probably need to be tuned again. However, for now I am focusing to make my first prediction and will try to improve things later on. I especially need to attend to cross-validation. ","457fc7a9":"In this kernel I try to encode categorical features from [a past competition](https:\/\/www.kaggle.com\/c\/cat-in-the-dat), in order to build models that predict a binary (0\/1) target. I am using [this notebook](https:\/\/www.kaggle.com\/shahules\/an-overview-of-encoding-techniques) to learn about various encoding approaches but my main goal is to learn somethings about preprocessing achieve a good prediction using simple classifiers, rather than comparing all the encoding techniques. \n\n(In retrospect, I realized I have some repeated lines in this notebook and understood that my coding and writing could be more neat but I'm fine with this style for a start).\n\nFirst things first ...","bc3197e6":"With random forest, the following accuracies are achieved:\n* n=100, depth=5, 0.69826\n* n=200, depth=5, 0.69873\n* n=200, depth=7, 0.70930\n* n=500, depth=10, 0.72189\n\nIt's doing better than previous ones and it seems that the accuracy can be pushed by increasing n and depth, though it takes longer to calculate. \n\nNext, I tried `SVC` with rbf and linear kernels but had to stop them due to long run time. Decided to stick to the above classifiers (logistic regression, KNN, and random forests) for now and explore the effect of another encpding method. That is `OneHotEncoder`."}}