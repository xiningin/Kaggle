{"cell_type":{"fac7b0fe":"code","9ffed947":"code","e6a62227":"code","9624151f":"code","3235c16a":"code","ea155bf3":"code","aa260a9c":"code","f7ffe681":"code","7a76373a":"code","411bfe21":"code","852f4ff4":"code","fb2d6a30":"code","bc5429cc":"code","25398353":"code","5add1b7f":"code","eb70082c":"code","a85a080e":"code","f68dbf32":"code","d8f779a8":"code","9795cf23":"code","2ce48835":"code","91c1f967":"code","4a7df8dd":"code","832da276":"code","ae4ea3cf":"code","9199f052":"code","b6eb57b7":"code","d7a3dc5e":"code","18c72144":"code","5ab09c4e":"code","c2698161":"code","67ec33b4":"code","8202d8a4":"code","2acea0ea":"code","c5a1c5a8":"code","d7d53f49":"code","45e08e2d":"code","827cb758":"code","c988bab5":"code","f22b4d91":"code","e817c6aa":"code","2a0764b4":"code","cd186004":"code","97127905":"code","bae97e50":"code","88516e4c":"code","fde2e3d6":"code","e256d0db":"code","1d77f5f6":"code","8bf35f4c":"code","ff3a57ca":"code","5c664946":"code","a0143e9b":"code","9d6eb762":"code","84a27b4d":"code","5b1034a0":"code","80aba7f7":"code","f9bee1ec":"code","d1835f8a":"code","1225c1a8":"code","cd6cc4d1":"code","aebf1356":"code","e9552d7c":"code","425733f1":"code","9c01250e":"code","2380e730":"markdown","f762d7b8":"markdown","099b50af":"markdown","ce5b2ffd":"markdown","ada88df9":"markdown","a5c16613":"markdown","87c91ed4":"markdown","5e4808dd":"markdown","d9b2223a":"markdown","27968cf4":"markdown","3d385082":"markdown","5c3600f1":"markdown","35239988":"markdown","4e6522f8":"markdown","ac1f2467":"markdown","3236e158":"markdown","634870fb":"markdown","5dac6cdd":"markdown","75b11325":"markdown","b3e3e121":"markdown","7a78748e":"markdown","3a99e0da":"markdown","5f53dd82":"markdown","11eccf24":"markdown","5184bd24":"markdown","e11b6a64":"markdown","c39edf43":"markdown","5af13511":"markdown","e5aca216":"markdown","10f0f472":"markdown","9c6e3b04":"markdown","c9e638c0":"markdown","c89e5db3":"markdown","ec2999a1":"markdown","14173e69":"markdown","79224df1":"markdown","2a2ff729":"markdown","73dd959d":"markdown","f5d26ef3":"markdown","6b99b723":"markdown","9821e7d2":"markdown","eb6d13de":"markdown","8ecf23ff":"markdown","c669b629":"markdown","eef3fb7e":"markdown","d6a1bd9f":"markdown","3a6bf5d3":"markdown","51956303":"markdown","fb5dace2":"markdown","6b63e964":"markdown","628cfb76":"markdown","5748bed7":"markdown","82763259":"markdown","09ca3e72":"markdown","8a35b048":"markdown","6eb704ae":"markdown","36ec381a":"markdown","f4416f31":"markdown","2641e3c2":"markdown","7c6b3430":"markdown","d01ada42":"markdown","ef53687f":"markdown"},"source":{"fac7b0fe":"# import some necessary libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, csv file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt # matlib-style plotting\nimport seaborn as sns\n\ncolor = sns.color_palette() \nsns.set_style('darkgrid')\nimport warnings\n\ndef ignore_warn(*args, **kwargs): \n    pass\n# *args: \ubcf5\uc218\uc758 \uc778\uc790\ub97c \ud29c\ud50c \ud615\uc2dd\uc758 \ud568\uc218 \ud638\ucd9c\n# **kwargs: \ub515\uc154\ub108\ub9ac \ud615\ud0dc\ub85c (key: value) \ud568\uc218 \ud638\ucd9c\n\nwarnings.warn = ignore_warn # ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew # for some statistics\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n# limitting floats output to 3 decimal points\n\nfrom subprocess import check_output\nprint(check_output(['ls', '..\/input']).decode('utf8'))","9ffed947":"# Now let's import and put the train adn test datasets in pandas dataframe\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","e6a62227":"# display the first five rows of the train dataset\ntrain.head(5)","9624151f":"# display the first five rows of the test dataset\ntest.head(5)","3235c16a":"# check the numbers of samples and features\nprint('The train data size before dropping Id feature is: {}'.format(train.shape))\nprint('The test data size before dropping Id feature id: {}'.format(test.shape))\n\n# save the 'ID' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now the drop the \"Id\" Column since it's unnecessary for the prediction process\ntrain.drop('Id', axis=1, inplace = True)\ntest.drop('Id', axis=1, inplace = True)\n\n# check again the data size after dropping the 'Id' variable\nprint('\\nThe train data size after dropping Id feature is : {}'.format(train.shape))\nprint('the test data size after dropping Id feature is : {}'.format(test.shape))","ea155bf3":"fig, ax = plt.subplots()\nax.scatter(x = train[\"GrLivArea\"], y=train['SalePrice'])\n# GrLivArea : Above grade (ground) living area square feet\nplt.xlabel('GrLivArea', fontsize=13)\nplt.ylabel('SalePrice', fontsize=13)\nplt.show()","aa260a9c":"# Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & \n                         (train['SalePrice']<300000)].index)\n\n# check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.xlabel('SalePrice', fontsize=13)\nplt.ylabel('GrLivArea', fontsize=13)\nplt.show()","f7ffe681":"sns.distplot(train['SalePrice'], fit=norm);\n\n# Gett the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n# \ubaa8\uc9d1\ub2e8\uc758 \ud3c9\uade0 = mu(\ubba4), \ubaa8\uc9d1\ub2e8\uc758 \ud45c\uc900\ud3b8\ucc28 = sigma\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.format(mu, sigma) ], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","7a76373a":"# We use the numpy function log1p which applies log(1+x) to all elements of the column\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n# log1p = \uc790\uc5f0\ub85c\uadf8\n\n# check the new distribution\nsns.distplot(train['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Now plot the distribution\nplt.legend(['Noraml dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.\n           format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","411bfe21":"ntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis = 1, inplace = True)\nprint('all_data size is : {}'.format(all_data.shape))","852f4ff4":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending = False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : all_data_na})\nmissing_data.head(20)","fb2d6a30":"f, ax = plt.subplots(figsize = (15,12))\n# plt.subplots() is a function that returns a tuple containing a figure and axes object(s). \n# Thus when using fig, ax = plt.subplots() you unpack this tuple into the variables fig and ax. \n\nplt.xticks(rotation = '90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize = 15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","bc5429cc":"# Correlation map to see how features are correlated with SalePrice\n\ncorrmat = train.corr()\nplt.subplots(figsize =(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","25398353":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')","5add1b7f":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","eb70082c":"all_data['Alley'] = all_data['Alley'].fillna('None')","a85a080e":"all_data['Fence'] = all_data['Fence'].fillna('None')","f68dbf32":"all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna(\"None\")","d8f779a8":"# Group by neighborhood and fill in missing value by the median \n# LotFrontage of all the neighborhood.\n\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n    lambda x : x.fillna(x.median()))","9795cf23":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","2ce48835":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","91c1f967":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","4a7df8dd":"for col in ('BsmtQual', \"BsmtCond\", 'BsmtExposure', 'BsmtFinType1',\n            'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","832da276":"all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)","ae4ea3cf":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","9199f052":"all_data = all_data.drop(['Utilities'], axis=1)","b6eb57b7":"all_data['Functional'] = all_data['Functional'].fillna('Typ')","d7a3dc5e":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","18c72144":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n# mode: \ucd5c\ube48\uac12","5ab09c4e":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","c2698161":"all_data[\"SaleType\"] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","67ec33b4":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna('None')","8202d8a4":"# check remaining missing values if any\nall_data_na = (all_data.isnull().sum()) \/ len(all_data) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio': all_data_na})\nmissing_data.head()","2acea0ea":"# MSSubClass = The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n# changing overallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n# Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","c5a1c5a8":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# process columns, apply LabelEncoder to categorical features \nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[c].values))\n    all_data[c] = lbl.transform(list(all_data[c].values))\n    \n# shape\nprint('Shape all_data : {}'.format(all_data.shape))","d7d53f49":"# Adding total sqfootage feature\n\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","45e08e2d":"numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n# get the features except object types\n\n# check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending = False)\nprint('\\n Skew in numberical features: \\n')\nskewness = pd.DataFrame({'Skew' : skewed_feats})\nprint(skewness.head(10))","827cb758":"skewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features to Box Cos transform'.format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    # all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","c988bab5":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","f22b4d91":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","e817c6aa":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","2a0764b4":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","cd186004":"lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state=1))","97127905":"ENet = make_pipeline(RobustScaler(), \n                     ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","bae97e50":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","88516e4c":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                  max_depth=4, max_features='sqrt',\n                                  min_samples_leaf=15, min_samples_split=10,\n                                  loss='huber', random_state=5)","fde2e3d6":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, verbosity=0,\n                             random_state =7, nthread = -1)","e256d0db":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","1d77f5f6":"score = rmsle_cv(lasso)\nprint('\\n Lasso score: {:.6f} ({:.4f})\\n'.format(score.mean(), score.std()))","8bf35f4c":"score = rmsle_cv(ENet)\nprint('ElasticNet score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","ff3a57ca":"score = rmsle_cv(KRR)\nprint('\\n Kernel Ridge score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","5c664946":"score = rmsle_cv(GBoost)\nprint('\\n Gradient Boosting score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","a0143e9b":"score = rmsle_cv(model_xgb)\nprint('\\n XGboost score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","9d6eb762":"score = rmsle_cv(model_lgb)\nprint('\\n LGBM score: {:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))","84a27b4d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","5b1034a0":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","80aba7f7":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","f9bee1ec":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","d1835f8a":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1225c1a8":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n\nprint(rmsle(y_train, stacked_train_pred))","cd6cc4d1":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","aebf1356":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","e9552d7c":"# RMSE on the entire Train data when averaging\n\nprint('RMSLE score on train data: ')\nprint(rmsle(y_train, stacked_train_pred * 0.70 +\n           xgb_train_pred * 0.15 + lgb_train_pred*0.15))","425733f1":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","9c01250e":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv', index=False)","2380e730":"* **XGBoost** :","f762d7b8":"### Ensembling StackedRegressor, XGBoost and LightGBM\n\nWe add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","099b50af":"* **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood, we can **fill in missing values by the median LotFrontage of the neighborhood.**","ce5b2ffd":"### File descriptions\n\n* train.csv - the training set\n* test.csv - the test set\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly to match the column names used here\n* sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot squeare footage, and number of bedrooms\n\n### Data fields\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass - The building class\n* MSZoning - The general zoning classification\n* LotFrontage - Linear feet of street connected to property\n* LotArea - Lot size in square feet\n* Street - Type of road access\n* Alley - Type of alley access\n* LotShape - General shape of property\n* LandContour - Flatness of the property\n* Utilities - Type of utilities available\n* LotConfig - Lot configuration\n* Landslope - Slope of property\n* Neighborhood - Physical locations within Ames city limits\n* condition1 - Proximity to main road or railroad\n* condition2 - Proximity to main road or railorad (if a second is present)\n* BidgType - Type of dwelling\n* HouseStyle - Style of dwelling\n* OverallQual - Overall material and finish quality\n* OverallCond - Overall condition rating\n* YearBuilt - Original construction date\n* YearRemodAdd - Remodel date\n* RoofStyle - Type of roof\n* RoofMati - Roof material\n* Exterior1st - Exterior covering on house\n* Exterior2nd - Exterior covering on house (if more than one material)\n* MaxVnrType - Masonry veneer type\n* MasVnrArea - Masonry veneer area in square feet\n* ExterQual - Exterior material quality\n* ExterCond - Present condition of the matrial on the exterior\n* Foundation - Type of foundation\n* BsmtQual - Height of the basement\n* BsmtCond - General condition of the basement\n* BsmtExposure - Walkout or garden level basement walls\n* BsmtFinType1 - Quality of basement finished area\n* BsmtFinSF2 - Type 2 finished square feet\n* BsmtUnfSF - Unfinished square feet of basement area\n* TotalBsmtSF - Total square feet of basement area\n* Heating - Type of heating\n* HeatingQC - Heating quality and condition\n* CentralAir - Central air conditioning\n* Electrical - Electrical system\n* 1stFlrSF - First Floor square feet\n* 2ndFlrSF - Second Floor square feet\n* LowQualFinSF - Low quality finished square feet (all floors)\n* GrLivArea - Above grade (ground) living area square feet\n* BsmtFullBath - Basement full bathrooms\n* BsmtHalfBath - Basement half bathrooms\n* Bedroom - Number of bedrooms\n* Kitchen - Number of kitchens\n* KitchenQual - Kitchen quality\n* TotRmsAbvGrd - Total rooms above grade (does not include bathrooms)\n* Functional - Home functionality rating\n* Fireplaces - Number of fireplaces\n* FireplaceQu - Fireplace quality\n* GarageType - Garage location\n* GarageYrBit - Year garage was built\n* GarageFinish - Interior finish of the garage\n* GarageCars - Size of garage in car capacity\n* GarageArea - Size of garage in square feet\n* GarageQual - Garage quality\n* Garagecond - Garage condition\n* PavedDrive - Paved driveway\n* WoodDeckSF - Wood deck area in square feet\n* OpenPorchSF - Open porch area in square feet\n* EnclosedPorch - Enclosed porch area in square feet\n* 3SsnPorch - Three season porch area in square feet\n* ScreenPorch - Screen porch area in square feet\n* PoolArea - Pool area in square feet\n* PoolQc - Pool quality\n* Fence - Fence quality\n* MiscFeature - Miscellaneous feature not covered in the categorires (composed of or containing a variety of things; mixed; varied)\n* MiscVal - $Value of miscellaneous feature\n* MoSold - Month Sold\n* YrSold - Year Sold\n* SaleType - Type of sale\n* SaleCondition - Condition of sale","ada88df9":"* **Kernel Ridge Regression** :","a5c16613":"* **SaleType** : Fill in again with the most frequent which is 'WD'.","87c91ed4":"* **Elastic Net Regression** : again made robust to outliers","5e4808dd":"* **GarageYrBit, GarageArea and GarageCars** : Replacing missing data with 0 (since No garage = no cars in such garages.)","d9b2223a":"**Label Encoding some categorical variables that may contain information in their ordering set**","27968cf4":"**Ensemble prediction :**","3d385082":"**Getting the new train and test sets.**","5c3600f1":"Is there any remaining missing value?","35239988":"* **Utilities** : For this categorical feature all records are 'AllPub', except for one 'NoSeWa'and 2 NA. Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling'. We can then safely remove it.","4e6522f8":"* **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for theses houses. We can fill 0 for the area and None for the type.","ac1f2467":"**Adding one more important feature**\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and seconf floor areas of each house","3236e158":"### Data processing\n\n#### Outliers\n\nDocumentation for the Ames Housing Data indicates that there outliers present in the training data\n\nLet's explore these outliers","634870fb":"* **MSZoning (The general zoning classfication)** : 'RL' is by the most common value. So we can fill in missing values with 'RL'.","5dac6cdd":"**Stacking averaged Models Class**","75b11325":"The target variable is right skewed. As (linear) models love normally distributed data, we need to transform this variable and make it more noramlly distributed.","b3e3e121":"**Submission**","7a78748e":"* **Eletrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","3a99e0da":"### Final Training and Prediction\n\n#### StackedRegressor:","5f53dd82":"* **FireplaceQu** = data description says NA means 'No fireplace'","11eccf24":"### Target Variable\n\n**SalePrice** is the variable we need to predict.\nSo let's do some analysis on this variable first.","5184bd24":"##### Data Correlation","e11b6a64":"## Stacked Regressions to predict House prices\n##### Stacking Regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity contraints to determine the coefficients in the combination.\n###### \uc2a4\ud0dc\ud0b9(Stacking) \uc124\uba85: https:\/\/hwi-doc.tistory.com\/entry\/%EC%8A%A4%ED%83%9C%ED%82%B9Stacking-%EC%99%84%EB%B2%BD-%EC%A0%95%EB%A6%AC\n\nThis kernel is copied by Serigne July 2017\n###### url: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling\n\n\n---------------------------------\nThis competition is very important to me as it helped me to begin my journey to Kaggle few months ago. I've read some great notebooks here. To name a few:\n\n1. Comprehensive data exploration with Python by Pedro Marelino : Great and very motivational data analysis\n\n2. A study on Regression applied to the Ames dataset by Julien Cohensolal : Through features engeneering and deep dive into linear regression analysis but really easy to follow for beginners.\n\n3. Regularized Linear Models by Alexandru Papiu : Great Starter Kernel on modelling and Cross-validation.\n\nI can't recommend enough every beginner to go carefully through these kernels (and of course through many others great kernels) and get their first insights in data science and kaggle competitions.\n\nAfter that (and some basic practices) you should be more confident to go through this great script by Human Analog who did an impressive work on features engeneering.\n\nAs the dataset is particularly handy, I decided few days ago to get back in this competition and apply things I learnt so far, especially stacking models. For that purpose, we build two stacking classes (the simplest approach and a less simple one).\n\nAs these classes are written for general purpose, you can easilly adapt them and\/or extend them for your regression problems. The overall approach is hopefully concise and easy to follow.\n\nThe features engeneering is rather parsimonious (at least compared to some others great scripts). \n###### (Parsimonious means the simplest model\/theory with the least assumptions and variables but with greatest explanatory power.)\n\nIt is pretty much: \n* Imputing missing values by proceeding sequentially through the data\n* Transforming some numerical variables that seem really categorical\n* Label Encoding some categorical variables that may contain information in their ordering set\n* Box Cox Transformation of skewed features (instead of logtransformation): This gave me a slightly better result both on leaderboard and cross-validaton.\n* Getting dummy variables for categorical features.\n\nThen we choose many base models (mostly sklearn based models + sklearn API of DMLC's XGBoost and Microsoft's LightGBM), cross-validate them on the data before stacking\/ensembling them. The key here is to make the (linear) models robust to outliers.\n###### (\uc774\uc0c1\uac12\uc5d0 \uc601\ud5a5\uc744 \uc801\uac8c \ubc1b\ub294 \ud1b5\uacc4\ub7c9 : robust data)\nThis improved the result both on LB and cross-validation.\n\nHope thaa at the end of this notebook, stacking will be clear for these, like myself, who found the concept not so easy to grsap\n","c39edf43":"**Skewed features**","5af13511":"#### Less simple Stacking : Adding a Meta-model\n\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\n1. Split the total training set into two disjoint sets (here train and .holdout )\n\n2. Train several base models on the first part (train)\n\n3. Test these base models on the second part (holdout)\n\n4. Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.","e5aca216":"* **Averaged base models score**\n\nWe just average four models here, **ENet, GBoost, KRR and lasso**. Of course we could easily add more models in the mix.","10f0f472":"* **Base models scores** :\n\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","9c6e3b04":"##### Imputing missing values\n\nwe impute them by procedding sequentially through features with missing values\n\n\n* **PoolQC**: data description says NA means 'No Pool'. That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.","c9e638c0":"* **Alley** = data description says NA means 'no alley access'","c89e5db3":"* **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string.","ec2999a1":"##### Features engineering\n\nlet's first concatenate the train and test data in the same dataframe","14173e69":"**LightGBM** :\n","79224df1":"### Base models\n\n* **LASSO Regression** :\nThis model may be very sensitive to outliers. So we need to be made it more robust on them. For that we use the sklearn's **Robustscaler()** method on pipeline\n\n- **robust data**: resistant to errors in the results, produced by deviations from assumptions\n\n- **explain url** : [link](https:\/\/m.blog.naver.com\/PostView.naver?isHttpsRedirect=true&blogId=ssdyka&logNo=221232820330)","2a2ff729":"**Getting dummy categorical features**","73dd959d":"* **LightGBM** :","f5d26ef3":"### Stacking models\n\n#### Simplest Stacking approach : Average base models\n\nwe begin with this simple approach of averaging base models. We build a new **class** to extend scikit-learn with our model and also to laverage encapsulation and code reuse\n\n* **Averaged base models class**","6b99b723":"#### Stacking Averaged models Score\n\nTo make the two approaches comparable (by using the same number of models), we just average **Enet KRR and Gboost**, then we add **lasso as meta-model.**","9821e7d2":"The skew seems now corrected and the data appears more nomally distributed.","eb6d13de":"* **MiscFeature** : data description says NA means 'No misc Feature'.","8ecf23ff":"##### seaborn heatmap features\n\nSyntax: seaborn.heatmap(data, *, vmin=None, vmax=None, cmap=None, center=None, annot_kws=None, linewidths=0, linecolor=\u2019white\u2019, cbar=True, **kwargs)\n\nImportant Parameters:\n\ndata: 2D dataset that can be coerced into an ndarray.\n\nvmin, vmax: Values to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.\n\ncmap: The mapping from data values to color space.\n\ncenter: The value at which to center the colormap when plotting divergent data.\n\nannot: If True, write the data value in each cell.\n\nfmt: String formatting code to use when adding annotations.\n\nlinewidths: Width of the lines that will divide each cell.\n\nlinecolor: Color of the lines that will divide each cell.\n\ncbar: Whether to draw a colorbar.","c669b629":"* **Gradient Boosting Regression** :\nwith **huber** loss that makes it robust to outliers","eef3fb7e":"* **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","d6a1bd9f":"###### kernel density estimation information : https:\/\/darkpgmr.tistory.com\/147","3a6bf5d3":"* **Fence**: data description says NA menas 'no fence'","51956303":"**Define a cross validation strategy**\n\nWe use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one lline of code, in order to shuffle the dataset prior to cross-validation","fb5dace2":"##### QQ-Plot, Quantile explanation:\n##### https:\/\/blog.naver.com\/PostView.nhn?blogId=sw4r&logNo=221026102874&parentCategoryNo=&categoryNo=117&viewDate=&isShowPopularPosts=true&from=search)\n","6b63e964":"### Modelling\n\n**Import Librairies**","628cfb76":"**Box Cox Transformation of (highly) skewed features**\n\nWe use the scipy function boxcop1p which computes the Box-Cox transformation of 1+x.\n\nNote that seeting \u03bb=0 is equivalent to log1p used above for the target variable.\n\nSee [this page](https:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) for more details on Box Cox Transformation as well as the scipy function's page ","5748bed7":"* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 :** For all these categorical basement-related features, NaN means that there is no basement.","82763259":"* **Functional** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","09ca3e72":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge outliers. Therefore, we can safely delete them.","8a35b048":"**XGBoost** :","6eb704ae":"It remains no missing value.\n\n**More features engeneering**\n\n**Transforming some numerical variables that are really categorical**","36ec381a":"#### Note:\n\nOutliers removal is note always safe. We decided to delete these two as they are very huge and really bad (extremely large areas for very low prices).\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why, instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.","f4416f31":"* **MSSubClass** : Na most likely means No building class. We can replace missing values with None.","2641e3c2":"* **KitchenQual** : Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","7c6b3430":"* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : For all these categorical basement-related features, NaN means that there is no basement.","d01ada42":"#### Log-transformation of the target variable","ef53687f":"##### Missing Data"}}