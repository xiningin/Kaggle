{"cell_type":{"7fd8d0fc":"code","64bba88b":"code","cb455f61":"code","dc3ce72e":"code","3f63ed4a":"code","ee9732dd":"code","67b4c965":"code","e496c635":"code","1ced3447":"code","df03dc87":"code","208691eb":"code","918ba903":"markdown","996a664c":"markdown","e1e8d92e":"markdown","ccc3e1af":"markdown","89f7dab3":"markdown","9e3a0247":"markdown","3751d830":"markdown","13b33bf9":"markdown"},"source":{"7fd8d0fc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nimport cv2\nimport csv\nimport shutil\nfrom glob import glob\nfrom PIL import Image\nfrom IPython.display import FileLink\n\nimport numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom glob import glob\nimport random\nimport cv2\nimport matplotlib.pylab as plt\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom keras.optimizers import Adam,RMSprop,SGD\n","64bba88b":"print(os.listdir(\"..\/input\/dogs-vs-cats-redux-kernels-edition\/\"))","cb455f61":"!unzip ..\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip\n","dc3ce72e":"#!unzip ..\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip","3f63ed4a":"#make directories\nos.mkdir('train\/cats\/')\nos.mkdir('train\/dogs\/')","ee9732dd":"#filling the directori for cat images\n\ncats = glob('train\/cat*.jpg')\nshuf = np.random.permutation(cats)\n\nfor i in range(len(cats)): \n    shutil.move(shuf[i], 'train\/cats\/')\n\n\n\n#filling the directori for dog images    \ndogs = glob('train\/dog*.jpg')\nshuf = np.random.permutation(dogs)\n\nfor i in range(len(dogs)): \n    shutil.move(shuf[i], 'train\/dogs\/')\n\n","67b4c965":"image_path = 'train\/'\nimages_dict = {}\n\n\nfor image in os.listdir(image_path):\n    folder_path = os.path.join(image_path, image)\n    images = os.listdir(folder_path)\n    \n    images_dict[image] = [folder_path, image]\n    img_idx = random.randint(0,len(image)-1)\n    image_img_path = os.path.join(image_path, image, images[img_idx])\n    #printing image\n    img = cv2.imread(image_img_path)\n    #print(image_img_path) # to get the path of one image with the .jpg number; uncommen this line\n    plt.imshow(img);","e496c635":"height=150\nwidth=150\nbatch_size=32     \nseed=1337\n\ntrain_dir = Path('train\/')\ntest_dir = Path('train\/')\n\n# Training generator first step rescale and gives more images in different angels and zoom range and even flipping the image\ntrain_datagen = ImageDataGenerator(rotation_range = 30       \n                                   ,rescale=1. \/ 255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True, validation_split = 0.2) #splitting data in traininf\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, #load data \n                                                    target_size=(height,width), #what size image we want\n                                                    batch_size=batch_size,  #how many images to read at the time \n                                                    seed=seed,\n                                                    class_mode='categorical', #we are classifing images into different categories\n                                                    subset = \"training\")      # we use the subset created for training data\n\n\n# Test generator we do the same as in train_generator without the rotation on images. \ntest_datagen = ImageDataGenerator(rescale=1.\/255, validation_split = 0.2) #splitting data in validation\ntest_generator = test_datagen.flow_from_directory(test_dir, \n                                                  target_size=(height,width), \n                                                  batch_size=batch_size,\n                                                  seed=seed,\n                                                  class_mode='categorical',\n                                                  subset = \"validation\")","1ced3447":"model = Sequential()\nmodel.add(Conv2D(12, kernel_size=(3,3),\n                 activation='relu',\n                 input_shape=(150, 150, 3)))","df03dc87":"model.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.5))                    #dropout means how offent to drop nodes, to make new connections\nmodel.add(Dense(256,activation='relu'))    #relu means rectified linear unit and is y=max(0, x) and 'Dense' means how dense you want the model in the given activation\nmodel.add(Dense(2, activation='softmax')) #softmax turns it into properbelities","208691eb":"model.compile(optimizer = 'adam',loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])   #you can change (optimizer = 'adam') to (Adam(lr=0.0001)) here is lr=learning rate\n# compile tells tenserflow how to update the dense connections, when we are training on the data\nfitting_model = model.fit_generator(train_generator,\n                    steps_per_epoch = 1097\/\/batch_size, #just a calulation (train size\/batch size) also how many pictures we want to load each time\n                    validation_data = test_generator, \n                    validation_steps = 272\/\/batch_size, #just a calulation (validation size\/batch size)\n                    epochs = 2,                       #epochs means how many cycle we want the model to go though our dataset\n                    verbose  = 1)                     #verbose just means whar you want to see while the model is training, 0=nothing, 1=a bar of proces, 2=the number of runs it wil take","918ba903":"# Image viewing","996a664c":"# Add the remaining layers","e1e8d92e":"# Preprocessing","ccc3e1af":"![image.png](attachment:image.png)","89f7dab3":"## Disclaimer! This kernel is only for educational purposes and made for fun therefor the content of the kernel should not be taken to seriously!","9e3a0247":"# Add first layer","3751d830":"# Loading data\nThise files is .zip, so you have to unzip them to get the pictures, and then you have to move to a directori","13b33bf9":"# Compile Your Model & Fit The Model"}}