{"cell_type":{"9fccdfca":"code","888024a8":"code","3be2d00a":"code","6662cc7b":"code","609a064f":"code","b8268e79":"code","bfa19712":"code","e51a0e73":"code","a1ff77db":"code","aa91c70a":"code","31796e91":"code","fd403871":"code","a56b8018":"code","6ef5f328":"code","bf3abcf7":"code","46c67952":"code","c76212ff":"code","d6e122c1":"code","6dfe968f":"code","36fdb6bd":"code","33049cd4":"code","65427853":"code","54daf5e8":"code","3b748813":"code","f38c24f1":"code","9c853d0e":"code","ebbb4b22":"code","27cc8178":"code","37fd7005":"code","7a6a375f":"code","ee38b42d":"code","38af0d99":"code","3aef2f77":"code","9c84a981":"code","551a8021":"code","38768d3e":"code","46996b7a":"code","807f9881":"code","7d7bb96c":"code","b4d49e9d":"code","cea6968f":"code","69472a2a":"code","d1973b00":"code","c7b9fe5b":"code","8bdb5995":"code","87babc49":"code","8141ec73":"code","00aaa25a":"code","32743d33":"code","718584b9":"code","cd48b4f6":"code","64d651d1":"code","95c0e5ba":"code","df1b67bf":"code","4b06cc80":"code","846f911b":"code","7f67bdae":"code","f8f7b176":"code","2756c12e":"code","8804b658":"markdown","cf1e32c1":"markdown","a92a6b4d":"markdown","fddbc23a":"markdown","c2fe54c6":"markdown","629c0636":"markdown","bf1dc943":"markdown","75f5a880":"markdown","bcfb3592":"markdown","ba2bd9e0":"markdown","775a5109":"markdown","bf8a8d3b":"markdown","477a3a36":"markdown","a1f54626":"markdown","b2d8ead0":"markdown","e6a55682":"markdown","789090bc":"markdown","2185740f":"markdown","f4359feb":"markdown","e43abb52":"markdown","536f85cd":"markdown","1a8afc07":"markdown","1fc8bc5b":"markdown","6e577761":"markdown","f050b40d":"markdown","b46cbb86":"markdown","8cdc62da":"markdown","74afb61d":"markdown","eed0ffbd":"markdown","9ba55de3":"markdown","ed9fe6ea":"markdown","17bde576":"markdown","aafad465":"markdown","51e43ee9":"markdown","4d447dd3":"markdown","8f663f07":"markdown","a1f4dece":"markdown","d83590b6":"markdown","65880580":"markdown","49bce240":"markdown","a52902c5":"markdown","5a9c2269":"markdown","abf990c4":"markdown","4afb78d8":"markdown","6a9c8b03":"markdown","4c1de351":"markdown","f8ebfc6e":"markdown","df857cd6":"markdown","387f1f70":"markdown","a4a97599":"markdown","36c5280e":"markdown","2a8760ba":"markdown","f31262a1":"markdown","f192f447":"markdown","22168af0":"markdown"},"source":{"9fccdfca":"# Analyzing\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom scipy import stats\nimport datetime\nimport math\n\n# Visualizing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport folium\nfrom folium import plugins\n#import geojson\nfrom shapely.geometry import shape, Point, multipolygon\n#from shap import TreeExplainer, summary_plot\n%matplotlib inline\nimport matplotlib.ticker as ticker\n\n# Others\nimport warnings \nimport gc\nwarnings.filterwarnings('ignore')\n\n# Modeling\nimport xgboost as xgb","888024a8":"## Save to Var\ntrain = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\nall = pd.concat([train,test],axis=0)\n## Checking\nprint(\"1.train Shape: \",train.shape)\nprint(\"2.test Shape: \",test.shape)\nprint(\"3.all Shape: \",all.shape,\"\\n\")","3be2d00a":"def eda_features(data) : \n  checked = pd.DataFrame.from_records([(col, data[col].dtype, data[col].count(),data[data[col]==0][col].count(),data[data[col]!=0][col].min(),data[data[col]!=0][col].max(),data[col].isnull().sum(),data[col].nunique(), data[col].unique()[:5]) for col in data.columns],\n                          columns=['Column_Name', 'Data_Type','Counts','0#','Min(0x)','Max(0x)','Nulls','Nunique','Val_Unique']).sort_values(by=['Nunique'])\n  return checked\n\n## add skewness and kurtosis","6662cc7b":"def cutting(data,col,n) : \n  data[col] = pd.cut(data[col],n)\n  data = data\n  return data","609a064f":"def counting_plot(data, count_info, n):\n  rows = math.ceil(len(count_info)\/3)\n  fig, axes = plt.subplots(rows,3, figsize=(20, rows*4))\n  for col in count_info:\n    if data[col].nunique() > n:\n      data = cutting(data, col, n)\n    r = count_info.index(col)\/\/3\n    c = count_info.index(col)%3\n    plot = sns.countplot(data=data,x=col,ax=axes[r,c])\n","b8268e79":"def relation_box(data, count_info, n):\n  rows = math.ceil(len(count_info)\/3)\n  fig, axes = plt.subplots(rows,3, figsize=(20, rows*4))\n  data = data[data['price'].isnull()==False]\n  for col in count_info:\n    r = count_info.index(col)\/\/3\n    c = count_info.index(col)%3\n    if data[col].nunique() > n: \n      data = cutting(data, col, n)\n    sns.boxplot(x=col, y='price', data=data ,ax=axes[r,c])\n  plt.show()\n    ","bfa19712":"def relation_bar(data, count_info, n):\n  rows = math.ceil(len(count_info)\/3)\n  fig, axes = plt.subplots(rows,3, figsize=(20, rows*4))\n  data = data[data['price'].isnull()==False]\n  for col in count_info:\n    r = count_info.index(col)\/\/3\n    c = count_info.index(col)%3\n    if data[col].nunique() > n: \n      data = cutting(data, col, n)\n    sns.barplot(x=col, y='price',  data=data ,ax=axes[r,c])\n  plt.show()","e51a0e73":"##combination\ndef combination_chart(data, count_info, n):\n  rows = len(count_info)\n  fig, axes = plt.subplots(rows,4, figsize=(24, rows*3.7))\n  data = data[data['price'].isnull()==False]\n  for col in count_info:\n    r = count_info.index(col)\n    if data[col].dtype in ['int64','float64'] :\n      sns.regplot(x=col, y='price', data=data, ax=axes[r,3])\n    if data[col].nunique() > n: \n      data = cutting(data, col, n)\n    sns.countplot(data=data,x=col,ax=axes[r,0])\n    if col != 'price':\n      sns.barplot(x=col, y='price', data=data ,ax=axes[r,1])\n      sns.boxenplot(x=col, y='price', data=data ,ax=axes[r,2])\n  plt.show()\n  \n  \n  \n ","a1ff77db":"## step0. Pricehist\ndef pricehist(data, x, type):\n    data = data[data['price'].isnull()==False] #If the dataset is concated with train and test datas, we sholud get rid of test data which don't have target feature('price')\n    if data[x].nunique() > 30:\n        apple = data.groupby(pd.cut(data[x],30))['price'].mean().sort_index().plot(kind=type,color=(0.2,0.4,0.6,0.6))\n    else :\n        apple = data.groupby([x])['price'].mean().sort_index().plot(kind=type,color=(0.2,0.4,0.6,0.6))\n    return apple\n  ","aa91c70a":"## step1. Logarize = Independent var or Dependent var it-self.\ndef logarize(data,*variables):\n  for x in variables :\n    data['log_{}'.format(x)] = np.log(data[x])\n    data.drop(x,axis=1,inplace=True)\n  return data","31796e91":"### step1. Delete\ndef delete(data, *variables):\n  for var in variables :\n    del data[var]\n  return data","fd403871":"## step2. Categorize\ndef categorize(data,*variables):\n  for x in variables:\n    data[x] = data[x].astype('category')\n    data = pd.concat([data,pd.get_dummies(data[x],prefix=x,drop_first=True)], axis=1)\n    data.drop([x], axis=1, inplace = True)\n  return data","a56b8018":"## step2. Binarize = 0=>0, other=>1\ndef binarize(data,*variables):\n  for x in variables :\n    data['{}_binarized'.format(x)] = [(0 if x== 0 else 1) for x in data[x]]\n    data.drop([x], axis=1, inplace = True)\n  return data","6ef5f328":"## step2. Standardize = \ndef standardize(data, *variables):\n  for x in variables:\n    data['{}_standardized'.format(x)] = minmax_scale(data[x])\n    data.drop([x],axis=1,inplace=True)\n  return data","bf3abcf7":"## step3. Cutagorize : alternative to Categorize, cut and categorize to infinitive data \ndef cutagorize(data,n,*variables) :\n  tn_list = list(map(lambda x:str(x+1), range(n)))\n  for var in variables : \n    data['{}_cut'.format(var)] = pd.qcut(data[var],n,labels = tn_list)\n    data = categorize(data,'{}_cut'.format(var))\n    data.drop([var],axis=1,inplace=True)\n  return data","46c67952":"# Date\n## Divide date values \ndef divide_date(x):\n  return x[:8]\n\n## divide year values\ndef divide_year(x):\n  return x[:4]\n\n## divide month values\ndef divide_month(x):\n  return x[4:6]\n\n## divide day values\ndef divide_day(x):\n  return x[6:8]\n\n## divide year and month values\ndef divide_yearmonth(x):\n  return x[:6]\n\n## transfer the data str to datetime\ndef todatetime(x):\n  return datetime.datetime.strptime(divide_date(x),'%Y%m%d')","c76212ff":"## toDatetime\ndef date_maker(data,dt):\n  data[dt] = data[dt].apply(lambda x : todatetime(x))\n  return data","d6e122c1":"def year_maker(data,dt):\n  data['year'] = data[dt].apply(lambda x : divide_year(x))\n  return data","6dfe968f":"def month_maker(data,dt):\n  data['month'] = data[dt].apply(lambda x : divide_month(x))\n  return data","36fdb6bd":"def yearmonth_maker(data,dt):\n  data['yearmonth'] = data[dt].apply(lambda x : divide_yearmonth(x))\n  return data","33049cd4":"# Yr_built vs Yr_renovated\n## returning the most recent year\ndef bigger(data,a,b):\n  results = []\n  if data[a] >= data[b]:\n    results.append(data[a])\n  else :\n    results.append(data[b])\n  return results","65427853":"def bigger_select(data,a,b):\n  results = []\n  for i in range(len(data[a])):\n    if data[a][i] >= data[b][i]:\n      results.append(data[a][i])\n    else :\n      results.append(data[b][i])\n  return results\n  ","54daf5e8":"#bigger(train,\"yr_built\",\"yr_renovated\")\ntrain.columns","3b748813":"train = pd.read_csv('..\/input\/train.csv')\ntrain.head(10)","f38c24f1":"train.head()","9c853d0e":"eda_features(train)","ebbb4b22":"train = pd.read_csv('..\/input\/train.csv')\ntrain = yearmonth_maker(train,'date')\ndate_maker(train,'date')\ncol_list = ['date','yearmonth','bedrooms','bathrooms','waterfront','floors','view','condition','grade','sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15','yr_built','yr_renovated','lat','long','zipcode']\ncombination_chart(train,col_list,30)","27cc8178":"corrMatt = train[train.columns]\ncorrMatt = corrMatt.corr()\nmask = np.zeros_like(corrMatt)\nmask[np.triu_indices_from(mask)] = True","37fd7005":"corrMatt['price'].sort_values(axis=0, ascending=False)","7a6a375f":"fig, axes = plt.subplots(figsize=(14,12))\nsns.heatmap(corrMatt, mask=mask, square=True, annot=True, cmap='Greens')\naxes.set_title('Correlation Matters', fontsize = 20)\nplt.show()","ee38b42d":"fig, axes = plt.subplots(1,2, figsize=(10, 4))\n\nsns.regplot(data=train, x='long', y='lat',\n           fit_reg=False,\n           scatter_kws={'s': 10},\n           ax=axes[0])\naxes[0].grid(False)\naxes[0].set_title('Location of train data', fontsize=15)\n\nsns.regplot(data=test, x='long', y='lat',\n           fit_reg=False,\n           scatter_kws={'s': 10},\n           ax=axes[1])\naxes[1].grid(False)\naxes[1].set_title('Location of test data', fontsize=15)\n\nplt.show()","38af0d99":"map_count = folium.Map(location=[train['lat'].mean(), train['long'].mean()],\n                      min_zoom=8,\n                      #max_zoom=11,\n                      width=660,  # map size scaling\n                      height=440)\n\nlat_long_data = train[['lat', 'long']].values.tolist()\nh_cluster = folium.plugins.FastMarkerCluster(lat_long_data).add_to(map_count)\n\nmap_count","3aef2f77":"## Save to Var\ntrain = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\nall = pd.concat([train,test],axis=0, ignore_index=True)\n## Checking\nprint(\"1.train Shape: \",train.shape)\nprint(\"2.test Shape: \",test.shape)\nprint(\"3.all Shape: \",all.shape)","9c84a981":"# sqft_above \/ bedroomss\nall['sqft_bedroom'] = all['sqft_above']\/all['bedrooms']","551a8021":"# sqft_above \/ bathrooms = fail\n#all['sqft_bathroom'] = all['sqft_above']\/all['bathrooms']","38768d3e":"# waterfront area higher\nwaterfront_zipcode = all[(all['waterfront']!=0)]['zipcode'].unique()\nbool_waterfront=[]\nfor zc in all['zipcode']:\n  if zc in waterfront_zipcode:\n    code = 1\n  else : \n    code = 0\n  bool_waterfront.append(code)\nall['bool_wf_zc'] = bool_waterfront","46996b7a":"# sqft_living \/ sqft_lot - fail\n#all['sqft_living_lot'] = all['sqft_living']\/all['sqft_lot']","807f9881":"#yr_recent - fail\n#all['yr_recent'] = bigger_select(all,'yr_built','yr_renovated')","7d7bb96c":"all = delete(all, 'id')##sqft_living15>\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","b4d49e9d":"all = yearmonth_maker(all,'date')\nall = delete(all, 'date')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","cea6968f":"all = logarize(all,'price','sqft_living','sqft_lot','sqft_above','sqft_lot15','sqft_living15','sqft_bedroom')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","69472a2a":"all = categorize(all, 'zipcode','yearmonth')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","d1973b00":"#all = cutagorize(all,24,'date','yearmonth')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)\n","c7b9fe5b":"all = standardize(all,'lat','long','yr_built')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","8bdb5995":"all = binarize(all,\"yr_renovated\",'sqft_basement')\nprint(\"all shape: \",all.shape)\nprint(\"all shape: \",all.columns)","87babc49":"# bedroom per bathroom -> \n#all['bedroom_bathroom'] = all['bedrooms']\/all['bathrooms']\n# fail","8141ec73":"#all = delete(all, 'lat', 'long')","00aaa25a":"## columns \ud655\uc778\nprint(all.columns, all.shape)","32743d33":"## all\uc744 train\uacfc test\ub85c \ub2e4\uc2dc \ub098\ub204\uae30\ntrain = all[all['log_price'].isnull()==False]\ntest = all[all['log_price'].isnull()==True]\n\n## \ub2e4\uc2dc dataset\uc744 x_train, y_train, x_test\ub85c \ub098\ub204\uae30\nx_train = train.drop(['log_price'],axis=1)\ny_train = train['log_price']\nx_test = test.drop(['log_price'],axis=1)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","718584b9":"# XGB Parameter\nxgb_params = {\n    'eta': 0.02,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'objective': 'reg:linear',     # \ud68c\uadc0\n    'eval_metirc': 'rmse',         # kaglle\uc5d0\uc11c \uc694\uad6c\ud558\ub294 \uac80\uc99d\ubaa8\ub378\n    'silent': True                 # \ud559\uc2b5 \ub3d9\uc548 \uba54\uc138\uc9c0 \ucd9c\ub825 \uc5ec\ubd80\n}","cd48b4f6":"# DMatrix\ndtrain = xgb.DMatrix(x_train,y_train)\ndtest = xgb.DMatrix(x_test)","64d651d1":"# Feval(RMSE_exp)\ndef rmse_exp(predictions, dmat):\n    labels = dmat.get_label()\n    diffs = np.exp(predictions) - np.exp(labels)\n    squared_diffs = np.square(diffs)\n    avg = np.mean(squared_diffs)\n    return ('rmse_exp', np.sqrt(avg))","95c0e5ba":"# cv_output ## -> \ub0b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac1d\uad00\uc801\uc73c\ub85c \ud30c\uc545\ud558\uae30 \uc704\ud568\uc774\ub2e4. -> test-rmse\uac00 \ud648\ud398\uc774\uc9c0\uc5d0 \uc788\ub294 \uac12\uacfc \ube44\uc2b7\ud55c \uac83.\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         # \ud559\uc2b5 \ud69f\uc218\n                   early_stopping_rounds=100,    # overfitting \ubc29\uc9c0\n                   nfold=5,                      # \ub192\uc744 \uc218\ub85d \uc2e4\uc81c \uac80\uc99d\uac12\uc5d0 \uac00\uae4c\uc6cc\uc9c0\uace0 \ub0ae\uc744 \uc218\ub85d \ube60\ub984\n                   verbose_eval=100,             # \uba87 \ubc88\uc9f8\ub9c8\ub2e4 \uba54\uc138\uc9c0\ub97c \ucd9c\ub825\ud560 \uac83\uc778\uc9c0\n                   feval=rmse_exp,               # price \uc18d\uc131\uc744 log scaling \ud588\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc2dc exponential\n                   maximize=False,\n                   show_stdv=False,              # \ud559\uc2b5 \ub3d9\uc548 std(\ud45c\uc900\ud3b8\ucc28) \ucd9c\ub825\ud560\uc9c0 \ub9d0\uc9c0\n                   )\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')","df1b67bf":"model = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ny_pred = model.predict(dtest)\ny_pred = np.exp(y_pred)\ny_pred","4b06cc80":"# Read the Sample submission file\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","846f911b":"sample_submission.head()","7f67bdae":"# Change the data\nsubmission = pd.DataFrame(data={'id':sample_submission['id'],'price':y_pred})","f8f7b176":"submission.head()","2756c12e":"# Save the data\nfile_name = 'submission.csv'\nsubmission.to_csv(file_name,index=False)","8804b658":"### Experiments2","cf1e32c1":"## 2.2 Changing Features","a92a6b4d":"### Featuring","fddbc23a":"1. Read Sample Submission : \uc81c\ucd9c \ud615\ud0dc \ud655\uc778\n2. Save Submission : \uc81c\ucd9c\ubb3c \uc81c\uc791 ","c2fe54c6":"## 0.4 Functions","629c0636":"## 4.1 Read Sample Submission","bf1dc943":"### Reset","75f5a880":"### [Map] Lat, Long, Zipcode","bcfb3592":"## 2.1 Base","ba2bd9e0":"### 3.1.1 Parameters","775a5109":"# 2 Feature Engineering","bf8a8d3b":"https:\/\/www.kaggle.com\/c\/2019-2nd-ml-month-with-kakr","477a3a36":"### PCA\n","a1f54626":"## 3.0 Base","b2d8ead0":"### Cutagorize","e6a55682":"### 3.1.2 Train Boosting","789090bc":"### Binarize\nYr_renovated","2185740f":"### [Plot] Combination Chart","f4359feb":"Date\n- categorizing : there are parts of lower house prices\n- CUTTING -> CATEGORIZING","e43abb52":"### Overview","536f85cd":" Feature Descriptions\n","1a8afc07":"# 1 EDA","1fc8bc5b":"## 0.0 Descriptions","6e577761":"### 3.1.3 Test Boosting","f050b40d":"## 0.1 References\n","b46cbb86":"### Logarzie\nprice, sqft_living, sqft_above, sqft_basement, sqft_lot, sqft_lot15, sqft_living15","8cdc62da":"### [Table] Traits","74afb61d":"\uc804\ubc18\uc801\uc778 \ub370\uc774\ud130 \uad6c\uc870\n- [Data] : head\ub85c \uba87 \uc0d8\ud50c \ub370\uc774\ud130\ub4e4 \uc0b4\ud3b4\ubcf4\uae30\n- [Table] Traits : feature\ub4e4\uc758 \uc8fc\uc694 \uc18d\uc131\uc744 \ud55c\ub208\uc5d0 \ud45c\ub85c \uc815\ub9ac\n- [Plot] Combination Chart : feature\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8 \uaf34, price\uc640\uc758 \uad00\uacc4 \ubc0f \uc0b0\ud3ec\ub3c4 \ucc28\ud2b8\ub97c \ud55c\ub208\uc5d0 \uc815\ub9ac\n- [Heatmap] Correlations : price\uc640 \uc0c1\uad00\uad00\uacc4\uac00 \uac00\uc7a5 \ub192\uc740 features\ub97c \ud0d0\uc0c9, features \ub4e4\ub07c\ub9ac \uc0c1\uad00\uc131 \ub192\uc740 \uac83\uc744 \ud55c\ub208\uc5d0 \uc0b4\ud53c\uae30\n- [Map] Lat, Long, Zipcode : \uc9c0\ub3c4\ub97c \ud65c\uc6a9\ud558\uc5ec \ub370\uc774\ud130 \uc0b4\ud54c","eed0ffbd":"### Local","9ba55de3":"### No Change\nFloor, Waterfront, View, Condition, Bathrooms, grade","ed9fe6ea":"### Substitute\ndate","17bde576":"- Id : discard\n- Price : logarize\n- Date : todatetime()\n- Yr_built : \n- Yr_renovated :\n- Waterfront : same\n- View : same\n- Condition : same\n- Grade : same\n- Bedrooms : same\n- Bathrooms : same\n- Sqft_living : logarize\n- Sqft_lot : logarize\n- Sqft_living15 : logarize - IDK it would be better to rid of it.\n- Sqft_lot15 : logarize\n- Sqft_above : logarize\n- Sqft_basement : logarize\n- Lat : discard\n- Long : discard\n- Zipcode : categorize","aafad465":"XGBoost \ub85c \ub2e8\uc77c \ubaa8\ub378\ub9c1 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n1. Data Arrange : \ub370\uc774\ud130 \uc815\ub9ac\n2. Parameter : parameter \uc124\uc815\n3. Train Boosting : Cross Validation\n4. Test Boosting : Training & Predicting","51e43ee9":"## 1.1 Overall","4d447dd3":"# 4 Submission","8f663f07":"### 3.1.0 Data Arrange\n","a1f4dece":"### Strategy","d83590b6":"1. Base : \ub370\uc774\ud130 \ub9ac\uc14b\n2. Changing Features :\n- Experiment : \uc2e4\ud5d8\ud55c \uac00\uc124\ub4e4 (\uc131\uacfc \ub192\uc600\ub358 \uac00\uc124 : \uce68\uc2e4 \ub2f9 living space \ud06c\uae30, waterfront\uac00 \uc788\ub294 \uc9c0\uc5ed\uc758 zipcode \/ \uc2e4\ud328\ud55c \uac00\uc124 : \ud654\uc7a5\uc2e4 \ub2f9 living space \ud06c\uae30, living\/lot, yr_recent)\n- Delete : \uc0ad\uc81c\ud55c feature (id)\n- Substitute : \uc218\uc815\ud55c features (date)\n- Logarize : \ub85c\uadf8 \ucde8\ud55c features ('price','sqft_living','sqft_lot','sqft_above','sqft_lot15','sqft_living15','sqft_bedroom')\n- Categorize : \uce74\ud14c\uace0\ub77c\uc774\uc988(\ub354\ubbf8) \ud55c features ('zipcode','yearmonth')\n- Standardize : \uc815\uaddc\ud654 \ud55c features ('lat','long','yr_built')\n- Binarize : \ubc14\uc774\ub108\ub77c\uc774\uc988 \ud55c features (\"yr_renovated\",'sqft_basement')","65880580":"0. \ub370\uc774\ud130\uc14b feature\ub4e4\uc5d0 \ub300\ud55c \uc18c\uac1c\n1. \ucc38\uc870\ud55c \uc790\ub8cc & \ucee4\ub110\n2. \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30\n3. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 & \ubcd1\ud569\n4. \uc0ac\uc6a9\uc790 \uc815\uc758 \ud568\uc218 \ub9cc\ub4e4\uae30","49bce240":"- ID : Numbers to identify houses\n- date : Date to buy the house\n- bedrooms: Numbers of bedrooms\n- bathrooms: Numbers of bathrooms\n- sqft_living: \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n- sqft_lot : \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n- floors : \uc9d1\uc758 \uce35 \uc218\n- waterfront : \uc9d1\uc758 \uc804\ubc29\uc5d0 \uac15\uc774 \ud750\ub974\ub294\uc9c0 \uc720\ubba4(a.k.a \ub9ac\ubc84\ubdf0)\n- view : Has been viewed\n- condition : \uc9d1\uc758 \uc804\ubc18\uc801\uc778 \uc0c1\ud0dc\n- grade : King Country gradinf \uc2dc\uc2a4\ud15c \uae30\uc900\uc73c\ub85c \ub9e4\uae34 \uc9d1\uc758 \ub4f1\uae09\n- sqft_basement : \uc9c0\ud558\uc2e4\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n- yr_built : \uc9c0\uc5b4\uc9c4 \ub144\ub3c4\n- zipcode : \uc6b0\ud3b8\ubc88\ud638\n- lat : \uc704\ub3c4\n- long : \uacbd\ub3c4\n- sqft_living15 : 2015\ub144 \uae30\uc900 \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74 \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c)\n- sqft_lot15: 2015\ub144 \uae30\uc900 \uc8fc\ucc28 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74 \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\ub2e4.)\n- lot\/living \uc774\ub7f0\uac78 \ubcc0\uc218\ub85c \ub9cc\ub4e4 \uc218 \uc788\ub2e4.\n- price : price of house at the 'date' to buy ","a52902c5":" Local User Functions","5a9c2269":"## 1.2 Each Features","abf990c4":"## 3.1 XGboost","4afb78d8":"# 0 Basic","6a9c8b03":"## 0.2 Modules","4c1de351":"### Experiment1","f8ebfc6e":"### Categorize\nzipcode, yearmonth","df857cd6":"# 3 Modeling","387f1f70":"## 0.3 Datas","a4a97599":"### [Data]","36c5280e":"### Delete\nid, long, yr_built, sqft_lot15, sqft_living15","2a8760ba":"\n\n**Original Dataset Source**\n\n* [House Sales in King County, USA](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction)\n\n---\n\n**Refered Kernels**\n\n* [House Price Prediction EDA (updated 2019.03.12)](https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12)\n\n* [2019 ML month 2nd baseline \ucee4\ub110](https:\/\/www.kaggle.com\/kcs93023\/2019-ml-month-2nd-baseline)\n\n* [A note on using a single model: XGBoost](https:\/\/www.kaggle.com\/ivoryrabbit\/a-note-on-using-a-single-model-xgboost)\n\n* [Map visualizing](https:\/\/www.kaggle.com\/fulrose\/eda-with-map-visualization)\n\n* [Default EDA](https:\/\/www.kaggle.com\/yeonmin\/default-eda-stacking-introduction)\n\n* [skewed feature and target log tralation](https:\/\/www.kaggle.com\/harangdev\/skewed-feature-target-log)\n\n* [EDA and LAsso, RF, SVM, XGB](https:\/\/www.kaggle.com\/psystat\/eda-and-lasso-rf-svm-xgb-grid-search)\n\n* [suwon study kernel](https:\/\/www.kaggle.com\/wodlfrh\/suwon-study-kernel)\n\n* [geo data eda and feature engineering**](https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering)\n---\n\n**Refered Links**\n\n* [King County Home Sales: Analysis and the limitations of a multiple regression model](https:\/\/rstudio-pubs-static.s3.amazonaws.com\/155304_cc51f448116744069664b35e7762999f.html)\n\n* [kernel hotkeys](https:\/\/www.kaggle.com\/fulrose\/smart)","f31262a1":"## 4.2 Save Submission","f192f447":"### Standardize\nlat, long, yr_built","22168af0":"### [Heatmap] Correlations"}}