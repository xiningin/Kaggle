{"cell_type":{"bfca22a5":"code","fb7bcb65":"code","c8673658":"code","c220ec97":"code","6a26c61a":"code","913a7d97":"code","1d326e23":"code","cddf966d":"code","81e3221f":"code","78aae3d8":"code","e8e0adfa":"code","18385584":"code","8333f855":"code","86b4e683":"code","3e17f0f3":"code","f430b52f":"code","c6e80798":"code","ff71b3c3":"code","12c100d2":"code","b4be2484":"code","7ec72b4d":"code","3918a64a":"code","f84902ff":"code","28d72437":"code","1b1544db":"code","a1ee509e":"code","27820620":"code","b542c7d1":"code","055a9a12":"code","d5286e76":"code","423d0e29":"code","06cc0769":"code","a1d12699":"code","ceaf9b72":"code","c9405f56":"code","a8f289b0":"code","661dd600":"code","38fc9ea9":"code","631d55a3":"code","8a91c915":"code","ef0b2360":"code","4022be7d":"markdown","3d7a9652":"markdown","8035211c":"markdown","59cc3dac":"markdown","cce84b35":"markdown","6c7e7ba0":"markdown","8491a287":"markdown","1ad0d714":"markdown","64979f70":"markdown","8fa5dc21":"markdown","243d3fea":"markdown","3357ad96":"markdown","d8c7d191":"markdown","fbd9b3fd":"markdown"},"source":{"bfca22a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb7bcb65":"# import the training and test dataset\ntrain_data = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","c8673658":"# See the dataset\ntrain_data.head()","c220ec97":"test_data.head()","6a26c61a":"train_data.describe()","913a7d97":"# cheack the column number and missing values\ntrain_data.columns \n\n","1d326e23":"test_data.columns","cddf966d":"# missing value\ntrain_data.isnull().sum()","81e3221f":"test_data.isnull().sum()","78aae3d8":"# there are 14 continuous data in dataset so there will be 7 rows and 2 column for plot the kde plot\nrows = 7\ncolumns = 2\nfig,ax = plt.subplots(rows,columns,figsize=(30,10))\ni = 0\nfor r in range(rows):\n    for c in range(columns):\n        sns.kdeplot(train_data['cont'+str(i)],shade=True,ax=ax[r,c])\n        i+=1\n","e8e0adfa":"# choose the column to perform log trfansformation.\n# float_col = ['cont3','cont4','cont7']","18385584":"# apply the log transformation\n# for column in float_col[:-1]:\n#     train_data[column] = train_data[column].apply(lambda x:np.log10(x))","8333f855":"# # plot after log transformer\n# rows = 7\n# columns = 2\n# fig,ax = plt.subplots(rows,columns,figsize=(30,10))\n# i = 0\n# for r in range(rows):\n#     for c in range(columns):\n#         sns.kdeplot(train_data['cont'+str(i)],shade=True,ax=ax[r,c])\n#         i+=1\n","86b4e683":"# Perform encoding on categorical data\n# import the required module to work with machine learning problem.\nfrom sklearn.preprocessing import LabelEncoder\ncategory_column = [ col for col in train_data.columns if train_data[col].dtypes == \"object\"]\nlabelencoder = LabelEncoder()\nfor column in category_column:\n    train_data[column] = labelencoder.fit_transform(train_data[column])\n    test_data[column] = labelencoder.transform(test_data[column])","3e17f0f3":"train_data.head()","f430b52f":"# plot the corelation of dataset\nfig = plt.figure(figsize=(20,20))\nsns.heatmap(train_data.corr().round(2),annot = True)","c6e80798":"# training_column = ['cat5','cat6','cont0','cont2','cont7','cont8','cont10','cont11','cont12']","ff71b3c3":"#X = train_data[training_column]\nX = train_data.drop(['id','target'],axis = 1)\ny = train_data['target']\nX_valid = test_data.drop(['id'],axis = 1)","12c100d2":"X.head()","b4be2484":"# Feature Scaling\n# from sklearn.preprocessing import StandardScaler\n# featurescale = StandardScaler()\n# train_data = featurescale.fit_transforim(train_data[:-1])\n# test_data = featurescale.transform(test_data)","7ec72b4d":"# Now split  the dataset in to feature and target \nfrom sklearn.model_selection import train_test_split","3918a64a":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=0)","f84902ff":"# perform Hyperparameter tuning for Randomizesearch\n# from sklearn.model_selection import RandomizedSearchCV\n","28d72437":"# param = {\n#    'n_estimators' : [10,100,500,1000],\n#    'learning_rate' : [0.05,0.10,0.15,0.20,0.30,0.02],\n#     'max_depth' : [3,4,5,6,7,8,10,12],\n#     'min_child_weight' : [1,3,5,7,10],\n#     'gamma' : [0.0,0.1,0.2,0.3,0.4,0.5],\n#     'colsample_bytree' : [0.3,0.4,0.5,0.7,0.9]\n# }","1b1544db":"# xgbregressor = XGBRegressor()","a1ee509e":"# import sklearn\n# sorted(sklearn.metrics.SCORERS.keys())\n# all scores for regression and classifivcation","27820620":"# randomsearch = RandomizedSearchCV(xgbregressor,\n#                                 param_distributions=param,\n#                                 n_iter=5,\n#                                 scoring = 'neg_mean_absolute_error',\n#                                 n_jobs=-1,\n#                                 cv=5,\n#                                 verbose=3)\n\n","b542c7d1":"# randomsearch.fit(X_train,y_train)","055a9a12":"# randomsearch.best_params_","d5286e76":"# randomsearch.best_score_","423d0e29":"# With the help of hyperparameter tunig we got thye best parameter for xgboostregressor","06cc0769":"# use different parameter for xgbregressor\nxgbregressorwithp = XGBRegressor(n_estimators=500,\n                           min_child_weight = 7,\n                           max_depth = 4,\n                           learning_rate = 0.15,\n                           gamma = 0.5,\n                           tree_method='gpu_hist',\n                           colsample_bytree = 0.7)\n\n# 'colsample_bytree': 0.5,\n#  'gamma': 0.2,\n#  'learning_rate': 0.2,\n#  'max_depth': 4,\n#  'min_child_weight': 1,\n#  'n_estimators': 1000","a1d12699":"xgbregressorwithp.fit(X_train,y_train,early_stopping_rounds=5,eval_set=[(X_test,y_test)],verbose=False)","ceaf9b72":"xgbpredict = xgbregressorwithp.predict(X_test)","c9405f56":"from sklearn.metrics import mean_squared_error,mean_absolute_error\nprint(mean_squared_error(y_test,xgbpredict,squared=False))\nprint(mean_absolute_error(y_test,xgbpredict))","a8f289b0":"predict_test = xgbregressorwithp.predict(X_valid)","661dd600":"# save the predected value of testing dataset\nsample_sub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","38fc9ea9":"#sample_sub.head()","631d55a3":"submission = pd.DataFrame({'id':sample_sub['id'],'target':predict_test.astype(float)})","8a91c915":"#submission","ef0b2360":"submission.to_csv('.\/my_submission.csv', index = False)\nprint(\"Your submission was successfully saved!\")","4022be7d":"## Log Transformation","3d7a9652":"# Steps for creating model for Regression \n<ul>\n    <li>Load the Dataset<\/li>\n    <li>Perform EDA<\/li>\n    <li>Perform Feature selection<\/li>\n     <li>Feature Extraction<\/li>\n    <li>Model Creatiion<\/li>\n    <li>Evaluate model<\/li>\n    <li>Create a Submission file<\/li>\n<\/ul>","8035211c":"# 1. Load the dataset\nWe are going to load the dataset for creating model. There are two datasets, one is training and another is testing dataset.","59cc3dac":"# 3. Feature Selection","cce84b35":"# 2. Perform EDA ","6c7e7ba0":"# 5. Model Evoluation","8491a287":"now we are going to with **feature selection** part and **feature scaling** part.","1ad0d714":"### plot the dataset to analyze it.\n#### 1. Univariant","64979f70":"above cells tell us that there are no missing values in train_data and test_data. ","8fa5dc21":"To remove skewness from data in dataset apply the log transformation on continuous column, in which skewness are available.","243d3fea":"# Hyperparameter tuning for RandomForestRegression","3357ad96":"# 6. Create a Submission","d8c7d191":"As we have seen that there is no missing value in training and test data,","fbd9b3fd":"# 4. Model Creation"}}