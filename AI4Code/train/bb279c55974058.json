{"cell_type":{"4173d074":"code","d4bc99ad":"code","7a1a3907":"code","105701a5":"code","5b39d59e":"code","dbeb6ac9":"code","aab9aaf2":"code","0eebc1f7":"code","2c2c10c2":"code","4747bbcc":"code","da679f0a":"code","6873e934":"code","ce470e2b":"code","1653db90":"code","5dde623c":"code","0c2be776":"code","a74c60d3":"code","ebc55ba0":"code","c3d65671":"code","0ec37291":"code","945c2529":"code","4209cab7":"code","4be7d35e":"code","7ad75b47":"code","d99971b3":"code","87f1ae27":"code","a63c7a8b":"code","f3077a91":"code","5d7e0b50":"code","defe9a6b":"code","da00d66a":"code","13cd0c05":"code","c9ef2fd0":"code","f284433e":"code","89661fd3":"code","94e556ad":"code","3d1234ea":"markdown","18911c50":"markdown","523ee4be":"markdown","10f4e2da":"markdown"},"source":{"4173d074":"import os\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\nimport tensorflow as tf\nimport IPython.display as display\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\nmpl.rcParams['axes.grid'] = False","d4bc99ad":"def tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)>3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)","7a1a3907":"content_path0='https:\/\/storage.googleapis.com\/kagglesdsdata\/competitions\/21755\/1475600\/photo_jpg\/009ddaed1f.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211031%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211031T130537Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=9d47286638e7971fad55e71b1620ab409f3aae78e513d23aac5a7853efffa083d78681d728bdf05d8c128ba030d5408cf3cfe1cfa330d2c091cfbdb2843e5aa57507acbd5e251c237d44ba95ec395317f727b58acd8c765fc23e3b051c6f5d2fe4887b9edae7970f4290e27d3281b60447b3ffa7625414b8718b9bb60fb8ec05fc3d03c667240c4d2abaf3f4225a420f9a1bd7558a7d45367b2938ef15485adc0a8118022d9cfd952c866ded67591fae445821cf423ff0ac04a3c39f74a8d8e06f33302ce25441e12a6c93133cfc6e79bb8d74f8984a7b2ab85eab741c43b0ec54f4e81a0aeffa1844d21d9f4e8fbf0277af6a35cc058070bc3d1400af16f772'\nstyle_path0='https:\/\/storage.googleapis.com\/kagglesdsdata\/competitions\/21755\/1475600\/monet_jpg\/0e3b3292da.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211101%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211101T140027Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=877fe59481f2744bd0a3d856808ca06e12a4b2f16a8dc3d1ef0b2a8a58ad10b2c8fdcf66240a23833076c93ec3cb2f919efa16517be1f48259357c7c8267b7cfa679859dfcb8f237bebc1f59eace3d0c4464a45560efe0e0626217832016715f80c2293dccd56e32e2abfde90f27c17ba785df611220b3a948c8ccc972c0b0a97ec28d25af176da5f763135864060445a8eed4e8921249d222f0a46f1acee6b632c1f3a92189c0ea2f85dd2424c59d9fb9b33e4ad5f7adc7740e82d295a47bde6a3f3b4a9a19b63f8e8c44e5ecc1813732da53e2b38338f7f35df1e8d4b263577dd6615633bd92b4dcf0b56b278af6e3e2717d1be9034f5b26177aeb935a8fd9'","105701a5":"content_path = tf.keras.utils.get_file(\"Content.jpg\", content_path0)\nstyle_path = tf.keras.utils.get_file(\"Style.jpg\", style_path0)","5b39d59e":"def load_img(path_to_img):\n  max_dim = 512\n  img = tf.io.read_file(path_to_img)\n  img = tf.image.decode_image(img, channels=3)\n  img = tf.image.convert_image_dtype(img, tf.float32)\n\n  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n  long_dim = max(shape)\n  scale = max_dim \/ long_dim\n\n  new_shape = tf.cast(shape * scale, tf.int32)\n\n  img = tf.image.resize(img, new_shape)\n  img = img[tf.newaxis, :]\n  return img","dbeb6ac9":"def imshow(image, title=None):\n  if len(image.shape) > 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)","aab9aaf2":"content_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')","0eebc1f7":"x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\nx = tf.image.resize(x, (224, 224))\nvgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\nprediction_probabilities = vgg(x)\nprediction_probabilities.shape","2c2c10c2":"predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n[(class_name, prob) for (number, class_name, prob) in predicted_top_5]","4747bbcc":"vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n\nprint()\nfor layer in vgg.layers:\n  print(layer.name)","da679f0a":"content_layers = ['block5_conv2'] \n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","6873e934":"def vgg_layers(layer_names):\n  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n  # Load our model. Load pretrained VGG, trained on imagenet data\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n  \n  outputs = [vgg.get_layer(name).output for name in layer_names]\n\n  model = tf.keras.Model([vgg.input], outputs)\n  return model","ce470e2b":"style_extractor = vgg_layers(style_layers)\nstyle_outputs = style_extractor(style_image*255)\n\n#Look at the statistics of each layer's output\nfor name, output in zip(style_layers, style_outputs):\n  print(name)\n  print(\"  shape: \", output.numpy().shape)\n  print(\"  min: \", output.numpy().min())\n  print(\"  max: \", output.numpy().max())\n  print(\"  mean: \", output.numpy().mean())\n  print()","1653db90":"def gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result\/(num_locations)","5dde623c":"class StyleContentModel(tf.keras.models.Model):\n  def __init__(self, style_layers, content_layers):\n    super(StyleContentModel, self).__init__()\n    self.vgg = vgg_layers(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.vgg.trainable = False\n\n  def call(self, inputs):\n    \"Expects float input in [0,1]\"\n    inputs = inputs*255.0\n    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n    outputs = self.vgg(preprocessed_input)\n    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n                                      outputs[self.num_style_layers:])\n\n    style_outputs = [gram_matrix(style_output)\n                     for style_output in style_outputs]\n\n    content_dict = {content_name: value\n                    for content_name, value\n                    in zip(self.content_layers, content_outputs)}\n\n    style_dict = {style_name: value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n    return {'content': content_dict, 'style': style_dict}","0c2be776":"extractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))\n\nprint('Styles:')\nfor name, output in sorted(results['style'].items()):\n  print(\"  \", name)\n  print(\"    shape: \", output.numpy().shape)\n  print(\"    min: \", output.numpy().min())\n  print(\"    max: \", output.numpy().max())\n  print(\"    mean: \", output.numpy().mean())\n  print()\n\nprint(\"Contents:\")\nfor name, output in sorted(results['content'].items()):\n  print(\"  \", name)\n  print(\"    shape: \", output.numpy().shape)\n  print(\"    min: \", output.numpy().min())\n  print(\"    max: \", output.numpy().max())\n  print(\"    mean: \", output.numpy().mean())\n","a74c60d3":"style_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']","ebc55ba0":"image = tf.Variable(content_image)","c3d65671":"def clip_0_1(image):\n  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","0ec37291":"opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","945c2529":"style_weight=1e-2\ncontent_weight=1e4","4209cab7":"def style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight \/ num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight \/ num_content_layers\n    loss = style_loss + content_loss\n    return loss","4be7d35e":"@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))","7ad75b47":"train_step(image)\ntrain_step(image)\ntrain_step(image)\ntensor_to_image(image)","d99971b3":"import time\nstart = time.time()\n\nepochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n  for m in range(steps_per_epoch):\n    step += 1\n    train_step(image)\n    print(\".\", end='', flush=True)\n  display.clear_output(wait=True)\n  display.display(tensor_to_image(image))\n  print(\"Train step: {}\".format(step))\n  \nend = time.time()\nprint(\"Total time: {:.1f}\".format(end-start))","87f1ae27":"def high_pass_x_y(image):\n  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n\n  return x_var, y_var","a63c7a8b":"x_deltas, y_deltas = high_pass_x_y(content_image)\n\nplt.figure(figsize=(14, 10))\nplt.subplot(2, 2, 1)\nimshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n\nplt.subplot(2, 2, 2)\nimshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n\nx_deltas, y_deltas = high_pass_x_y(image)\n\nplt.subplot(2, 2, 3)\nimshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n\nplt.subplot(2, 2, 4)\nimshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")","f3077a91":"plt.figure(figsize=(14, 10))\n\nsobel = tf.image.sobel_edges(content_image)\nplt.subplot(1, 2, 1)\nimshow(clip_0_1(sobel[..., 0]\/4+0.5), \"Horizontal Sobel-edges\")\nplt.subplot(1, 2, 2)\nimshow(clip_0_1(sobel[..., 1]\/4+0.5), \"Vertical Sobel-edges\")","5d7e0b50":"def total_variation_loss(image):\n  x_deltas, y_deltas = high_pass_x_y(image)\n  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))","defe9a6b":"total_variation_loss(image).numpy()","da00d66a":"tf.image.total_variation(image).numpy()","13cd0c05":"total_variation_weight=30","c9ef2fd0":"@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n    loss += total_variation_weight*tf.image.total_variation(image)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))","f284433e":"image = tf.Variable(content_image)","89661fd3":"import time\nstart = time.time()\n\nepochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n  for m in range(steps_per_epoch):\n    step += 1\n    train_step(image)\n    print(\".\", end='', flush=True)\n  display.clear_output(wait=True)\n  display.display(tensor_to_image(image))\n  print(\"Train step: {}\".format(step))\n\nend = time.time()\nprint(\"Total time: {:.1f}\".format(end-start))","94e556ad":"file_name = 'stylized-image.png'\ntensor_to_image(image).save(file_name)\n\ntry:\n  from google.colab import files\nexcept ImportError:\n   pass\nelse:\n  files.download(file_name)","3d1234ea":"Style Transfer TensorFlow Hub Sample<br\/>\nhttps:\/\/www.kaggle.com\/stpeteishii\/style-transfer-tensorflow-hub-sample<br\/>\nStyle Transfer Tensorflow Sample<br\/>\nhttps:\/\/www.kaggle.com\/stpeteishii\/style-transfer-tensorflow-sample<br\/>\nStyle Transfer Keras Sample<br\/>\nhttps:\/\/www.kaggle.com\/stpeteishii\/style-transfer-keras-sample<br\/>\nStyle Transfer Pytorch Sample<br\/>\nhttps:\/\/www.kaggle.com\/stpeteishii\/style-transfer-pytorch-sample<br\/>","18911c50":"# Style Transfer Tensorflow Sample\nThis notebook reffered to Tensorflow Tutorials 'Neural style transfer'<br\/>\nhttps:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/style_transfer.ipynb","523ee4be":"### Import and configure modules","10f4e2da":"Choose intermediate layers from the network to represent the style and content of the image:\n"}}