{"cell_type":{"cdcb78fd":"code","2642c856":"code","74924173":"code","7e1d6f50":"code","0cf4d2e0":"code","3774155e":"code","9b3a0de9":"code","dfe184c2":"code","b6dfec19":"code","ae577ddc":"code","cd8283dc":"code","dc05c230":"code","8f99f6e3":"code","0cf935c9":"code","699bf843":"code","4a62e357":"markdown","4ad2f63e":"markdown","244e38d1":"markdown","4370fb9d":"markdown","c3c5c82e":"markdown","1fec606e":"markdown","a986f71d":"markdown"},"source":{"cdcb78fd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport requests # process http requests\nfrom time import sleep # timer functions\nimport json # process json objects\nfrom tqdm import tqdm # progress bar\nfrom hashlib import md5 # md5 hash for caching\nimport networkx as nx # generate d3 compatible graph\nimport IPython.display\nfrom IPython.core.display import display, HTML, Javascript\nfrom string import Template\nimport networkx as nx\nfrom difflib import SequenceMatcher\nfrom os import path","2642c856":"# ************************************** FUNCTION DEFINITIONS ***********************************************\n\"\"\"\nUser defined Topics that forces search engine to look in their neighborhood also\n\"\"\"\nselect_topics = set(['ACTIVITY', 'ADE', 'AGENT', 'ANIMAL', 'ANIMALS', 'ANTAGONIST', 'ANTIVIRAL', 'ASYMPTOMATIC',\n                     'BAT', 'BINDING', 'BUFFER', 'CELL', 'CELLS', 'CIRCULATION', 'CLARITHROMYCIN', 'CO-INFECTIONS',\n                     'CO-MORBIDITIES', 'DISEASE', 'DRUG', 'DRUGS', 'ENVIRONMENT', 'ENZYME', 'ENZYMES', 'EXPERIMENTAL',\n                     'FARMERS', 'GENOME', 'HIGH-RISK', 'HISTONE', 'HOST', 'HYDROPHILIC', 'HYDROPHOBIC', 'INFECTION',\n                     'INTERACTIONS','IMMUNE', 'LIGAND', 'LIVESTOCK', 'MINOCYCLINE', 'MODEL', 'NAGOYA', 'NAPROXEN',\n                     'NEONATES', 'NUCLEOTIDE', 'PATIENT', 'PATHOGENESIS', 'PEPTIDE', 'PEPTIDES', 'PHENOTYPE', 'PLATES',\n                     'POLYPROTEIN', 'PPE', 'PRE-EXISTING', 'PREGNANCY', 'PROTEIN', 'PROTOCOL', 'PROPHYLAXIS',\n                     'PULMONARY', 'RBD', 'RANGE', 'REAGENT', 'REAGENTS', 'RECEPTER', 'REPLICATION', 'RESIDUES', 'RESPONSE'\n                     'SEQUENCING', 'SHEDDING', 'SMOKING', 'STRAIN', 'STRUCTURES', 'THERAPEUTIC', 'TRACKING',\n                     'TRANSCRIBE', 'TRANSCRIPTASE', 'TRANSMISSION', 'TREATMENT', 'VACCINE', 'VIRAL', 'VIRUS',\n                     'WILDLIFE', 'UNIVERSAL'])\n\n\"\"\"\nExtract concepts and topics and their relationship from the search engine including user-defined topics\n\"\"\"\ndef get_graph(project_name=\"cord19-dataset\", source=\"coronavirus\", target=\"transmission\", auth=\"test-key\"):\n\n    params = {\n        'auth': auth,\n        'u_name': source,\n        \"v_name\": target,\n        \"return_dataframe\": True,\n        \"return_type\": \"dataframe\",        \n        \"additional_topics\": \",\".join(map(lambda word: word.lower(), select_topics)),\n        \"project_name\": project_name\n    }\n    r = requests.get(\"https:\/\/apis.nlpcore.com\/apis\/get_graph\/\", params=params)\n    if r.status_code != 200:\n        raise RuntimeError(\"Failed to get_graph, please try again., %s\" % r.content)\n    dataframe = pd.DataFrame(json.loads(r.content))\n    return dataframe\n\n\"\"\"\nFilter rows in a dataframe to specific topics\n\"\"\"\ndef subset_dataframe(dataframe, given_topics):\n\n    select_dataframe_rows = []\n    for _,row in dataframe.iterrows():\n        source_topics = set(row['source_topics'])\n        target_topics = set(row['target_topics'])\n        if (given_topics & source_topics or given_topics & target_topics):\n            select_dataframe_rows.append(row)\n    return pd.DataFrame(select_dataframe_rows)\n\n\"\"\"\nGet Article metadata\/attributes for a given document id, store results in caches for repeated calls\n\"\"\"\ndef document_metadata(project_name, document_id, auth=\"test-key\"):\n    \n    cache_key_str = \"%s-%s\" % (project_name, document_id)\n    cache_key = md5(cache_key_str.encode()).hexdigest()\n    cache_path = \"\/tmp\/metadata2-%s.json\" % cache_key\n\n    try:\n        return json.load(open(cache_path))\n    except FileNotFoundError:\n        pass\n\n    r = requests.get(\"https:\/\/apis.nlpcore.com\/apis\/get_document_metadata\/\", params={'project_name': project_name,\n                                                                            'auth': auth,\n                                                                            'd': document_id})\n    if r.status_code == 200:\n        reference_data = r.json()\n        json.dump(reference_data, open(cache_path, \"w\"))\n\n    return r.json()\n\n\"\"\"\nDataframe returned from the above calls has a list of concepts and their references. For each reference we can request \ntext segments. The parameter \"r\" is a comma seperated list of integers which are senetence numbers.\nCache references for repeated calls.\n\"\"\"\ndef get_references(project_name, document_id, r, auth=\"test-key\"):\n    \n    cache_key_str = \"%s-%s-%s\" % (project_name, r, document_id)\n    cache_key = md5(cache_key_str.encode()).hexdigest()\n    cache_path = \"\/tmp\/%s.json\" % cache_key\n    \n    try:\n        return json.load(open(cache_path))\n    except FileNotFoundError:\n        pass\n    \n    r = requests.get(\"https:\/\/apis.nlpcore.com\/apis\/get_references\/\", params={'project_name': project_name,\n                                                                             'auth': auth, 'r': r,\n                                                                             'd': document_id})\n    if r.status_code == 200:\n        reference_data = r.json()\n        json.dump(reference_data, open(cache_path, \"w\"))\n    \n    return r.json()\n\n\"\"\"\nAugment the dataframe with article and sentence references for each of the co-occuring concepts in each row\n\"\"\"\ndef refine_dataframe(project_name, dataframe, auth=\"test-key\"):\n    select_dataframe_rows = []\n    for _,row in tqdm(list(dataframe.iterrows())):\n        source_topics = set(row['source_topics'])\n        target_topics = set(row['target_topics'])\n        if (select_topics & source_topics and select_topics & target_topics) and row['source_idf'] < 3 and row['target_idf'] < 3:            \n            reference_texts = []\n            for document_id,references in row['references'].items():\n                title = document_metadata(project_name=project_name, document_id=document_id)['title'] or \"<No Title>\"\n                sections = {}\n                for reference in references[:]:\n                    r = \"%d,%d\" % (reference['u_curr_ref'], reference['v_curr_ref'])\n                    text = get_references(project_name=project_name, document_id=document_id, r=r)\n                    for section in text.values():\n                        try:\n                            section_title = section['section_title']\n                        except Exception as e:\n                            raise e\n                        try:\n                            bucket = sections[section_title]\n                        except KeyError:\n                            bucket = []\n                            sections[section_title] = bucket                        \n                        bucket.append(section['sentence'])\n                reference_texts.append({'title': title, 'sections': sections})                    \n            select_dataframe_rows.append({'source': row['u_name'], 'target': row['v_name'], 'source_types': \", \".join(source_topics),\n                                         'target_types': \", \".join(target_topics), 'count': row['count'],\n                                         'references': reference_texts})\n    return pd.DataFrame(select_dataframe_rows)\n\n\"\"\"\nAugment the dataframe with select sentences that match keywords from task\n\"\"\"\ndef search_task_words(dataframe, given_topics):\n    \n    select_dataframe_rows = []\n    given_topics = [word.lower() for word in given_topics] \n    for _,row in dataframe.iterrows():\n        matched_sentences = []\n        matched_words = []\n        for reference_obj in row['references']:\n            for section_title, sentences in reference_obj['sections'].items():\n                for sentence in sentences:\n                    matched = [word for word in given_topics if word in sentence.lower()]\n                    if matched:\n                        matched_sentences.append(sentence)\n        row['sentences'] = matched_sentences\n        select_dataframe_rows.append(row.to_dict())\n    return pd.DataFrame(select_dataframe_rows)        \n\ndef convert_df(dataframe):\n    def similar(a, b):\n        return SequenceMatcher(None, a, b).ratio()\n\n    g = nx.DiGraph()\n    groups = {}\n\n    def get_group(group_name):\n        try:\n            group_id = groups[group_name]\n        except KeyError:\n            group_id = len(groups) + 1\n            groups[group_name] = group_id\n        return group_id\n\n    def add_node(concept_name, group_name):\n        concept_name = concept_name.lower()\n        \n        if concept_name in g.nodes:\n            g.nodes[concept_name]['size'] += 1\n        else:\n            sim_scores = [(_node, similar(_node, concept_name)) for _node in g.nodes]\n            if len(sim_scores) > 0:\n                _node, score = max(sim_scores, key=lambda item: item[1])\n                if score > 0.7:\n                    return add_node(_node, g.nodes[_node]['group'])\n            g.add_node(concept_name, size=1, group=get_group(group_name))\n        return concept_name\n\n    for _, row in dataframe.iterrows():\n        source_id = add_node(row['source'], row['source_types'])\n        target_id = add_node(row['target'], row['target_types'])\n        edge = g.get_edge_data(source_id, target_id)\n        if edge:\n            edge['value'] += 1\n        else:\n            g.add_edge(source_id, target_id, value=1)\n\n    dataframe_rows = []\n    for node_id in g.nodes:\n        node = g.nodes[node_id]\n        name = \"project.%d.%s\" % (node['group'], node_id)\n        dataframe_rows.append({'id': name, 'value': node['size'], 'value1': node['size']})\n    dataframe_rows = sorted(dataframe_rows, key=lambda item: item['value'], reverse=True)[:100]\n    return pd.DataFrame(dataframe_rows)\n\ndef return_bubble_data(csv_file_path, html_element_id):\n    html = \"\"\"<!DOCTYPE html><svg id=\"%s\" width=\"760\" height=\"760\" font-family=\"sans-serif\" font-size=\"10\" text-anchor=\"middle\"><\/svg>\"\"\" % html_element_id\n    js = \"\"\"require.config({paths: {d3: \"https:\/\/d3js.org\/d3.v4.min\"}});require([\"d3\"], function(d3) {var svg=d3.select(\"#%s\"),width=+svg.attr(\"width\"),height=+svg.attr(\"height\"),format=d3.format(\",d\"),color=d3.scaleOrdinal(d3.schemeCategory20c);console.log(color);var pack=d3.pack().size([width,height]).padding(1.5);d3.csv(\"%s\",function(t){if(t.value=+t.value,t.value)return t},function(t,e){if(t)throw t;var n=d3.hierarchy({children:e}).sum(function(t){return t.value}).each(function(t){if(e=t.data.id){var e,n=e.lastIndexOf(\".\");t.id=e,t.package=e.slice(0,n),t.class=e.slice(n+1)}}),a=(d3.select(\"body\").append(\"div\").style(\"position\",\"absolute\").style(\"z-index\",\"10\").style(\"visibility\",\"hidden\").text(\"a\"),svg.selectAll(\".node\").data(pack(n).leaves()).enter().append(\"g\").attr(\"class\",\"node\").attr(\"transform\",function(t){return\"translate(\"+t.x+\",\"+t.y+\")\"}));a.append(\"circle\").attr(\"id\",function(t){return t.id}).attr(\"r\",function(t){return t.r}).style(\"fill\",function(t){return color(t.package)}),a.append(\"clipPath\").attr(\"id\",function(t){return\"clip-\"+t.id}).append(\"use\").attr(\"xlink:href\",function(t){return\"#\"+t.id}),a.append(\"svg:title\").text(function(t){return t.value}),a.append(\"text\").attr(\"clip-path\",function(t){return\"url(#clip-\"+t.id+\")\"}).selectAll(\"tspan\").data(function(t){return t.class.split(\/(?=[A-Z][^A-Z])\/g)}).enter().append(\"tspan\").attr(\"x\",0).attr(\"y\",function(t,e,n){return 13+10*(e-n.length\/2-.5)}).text(function(t){return t})});});\"\"\" % (html_element_id, csv_file_path)\n    return html, js\n\n# ************************************** END OF FUNCTION DEFINITIONS ***********************************************","74924173":"\"\"\"\nFilter the results to only focus on most relvant topics for this challenge\n\"\"\"\ntask_topics = set(['ANIMAL', 'ANIMALS', 'ASYMPTOMATIC', 'MODEL', 'TRANSMISSION', 'INCUBATION', \n                   'SHEDDING' 'HYDROPHILIC', 'HYDROPHOBIC', 'VIRUS', 'DISEASE', 'PHENOTYPE',\n                   'PPE'])\nproject_name=\"cord19-dataset\"\nsource=\"coronavirus\"\ntarget=\"transmission\"\nauth=\"test-key\"","7e1d6f50":"\"\"\"\nGet the initial dataframe and filter it down to topics of interest and add article references\n\"\"\"\nif path.isfile(\"\/kaggle\/input\/nlpcore-cord19-output\/task_1.csv\"):\n    task_df = pd.read_csv(\"\/kaggle\/input\/nlpcore-cord19-output\/task_1.csv\", index_col=0)\nelse:\n    df = get_graph(project_name, source, target, auth)\n    task_df = refine_dataframe(project_name, subset_dataframe(df, task_topics), auth)\n    task_df = search_task_words(task_df, task_topics)\n    task_df.to_csv(\"\/kaggle\/working\/task_1.csv\")\n\n# Print results\n\ngraph_data = convert_df(task_df)","0cf4d2e0":"task_df","3774155e":"\"\"\"\nFilter the results to only focus on most relvant topics for this challenge\n\"\"\"\ntask_topics = set(['SMOKING', 'PRE-EXISTING', 'PULMONARY', 'DISEASE', 'CO-INFECTIONS', 'CO-MORBIDITIES', 'NEONATES', 'PREGNANCY', \n                   'HIGH-RISK', 'PATIENT'])\nproject_name=\"cord19-dataset\"\nsource=\"coronavirus\"\ntarget=\"disease\"\nauth=\"test-key\"","9b3a0de9":"\"\"\"\nGet the initial dataframe and filter it down to topics of interest and add article references\n\"\"\"\nif path.isfile(\"\/kaggle\/input\/nlpcore-cord19-output\/task_2.csv\"):\n    task_df = pd.read_csv(\"\/kaggle\/input\/nlpcore-cord19-output\/task_2.csv\", index_col=0)\nelse:\n    df = get_graph(project_name, source, target, auth)\n    task_df = refine_dataframe(project_name, subset_dataframe(df, task_topics), auth)\n    task_df = search_task_words(task_df, task_topics)\n    task_df.to_csv(\"\/kaggle\/working\/task_2.csv\")\n\ntask_df.to_csv(\"\/kaggle\/working\/task_2.csv\")\ngraph_data = convert_df(task_df)","dfe184c2":"graph_data.to_csv(\"task_2_graph.csv\")\nhtml, js = return_bubble_data(\"task_2_graph.csv\", \"graph_2_csv\")\n\nh = display(HTML(html))\nj = IPython.display.Javascript(js)\nIPython.display.display_javascript(j)","b6dfec19":"task_df","ae577ddc":"\"\"\"\nFilter the results to only focus on most relvant topics for this challenge\n\"\"\"\ntask_topics = set(['GENOME', 'TRACKING', 'STRAIN', 'CIRCULATION', 'NAGOYA', 'LIVESTOCK', 'RECEPTER', 'BINDING', \n                   'FARMERS' 'WILDLIFE', 'HOST', 'RANGE', 'EXPERIMENTAL', 'INFECTION', 'ANIMAL', 'PROTOCOL'\n                   'HOST'])\nproject_name=\"cord19-dataset\"\nsource=\"coronavirus\"\ntarget=\"strain\"\nauth=\"test-key\"","cd8283dc":"\"\"\"\nGet the initial dataframe and filter it down to topics of interest and add article references\n\"\"\"\nif path.isfile(\"\/kaggle\/input\/nlpcore-cord19-output\/task_3.csv\"):\n    task_df = pd.read_csv(\"\/kaggle\/input\/nlpcore-cord19-output\/task_1.csv\", index_col=0)\nelse:\n    df = get_graph(project_name, source, target, auth)\n    task_df = refine_dataframe(project_name, subset_dataframe(df, task_topics), auth)\n    task_df = search_task_words(task_df, task_topics)\n    task_df.to_csv(\"\/kaggle\/working\/task_3.csv\")\n\ntask_df.to_csv(\"\/kaggle\/working\/task_3.csv\")","dc05c230":"task_df","8f99f6e3":"\"\"\"\nFilter the results to only focus on most relvant topics for this challenge\n\"\"\"\ntask_topics = set(['NAPROXEN', 'CLARITHROMYCIN', 'MINOCYCLINE', 'ADE', 'THERAPEUTIC', 'ANTIVIRAL', 'AGENT', 'UNIVERSAL'\n                  'VACCINE', 'PROPHYLAXIS', 'IMMUNE', 'RESPONSE'])\nproject_name=\"cord19-dataset\"\nsource=\"coronavirus\"\ntarget=\"vaccine\"\nauth=\"test-key\"","0cf935c9":"\"\"\"\nGet the initial dataframe and filter it down to topics of interest and add article references\n\"\"\"\nif path.isfile(\"\/kaggle\/input\/nlpcore-cord19-output\/task_4.csv\"):\n    task_df = pd.read_csv(\"\/kaggle\/input\/nlpcore-cord19-output\/task_4.csv\", index_col=0)\nelse:\n    df = get_graph(project_name, source, target, auth)\n    task_df = refine_dataframe(project_name, subset_dataframe(df, task_topics), auth)\n    task_df = search_task_words(task_df, task_topics)\n    task_df.to_csv(\"\/kaggle\/working\/task_4.csv\")\n\ntask_df.to_csv(\"\/kaggle\/working\/task_4.csv\")","699bf843":"task_df","4a62e357":"# Task 1: What is known about transmission, incubation, and environmental stability?\nFrom the description of the task, we identified following key phrases as our primary search topics:\n\n* Transmission\n* Incubation\n* asymptomatic shedding\n* hydrophilic surface\n* hydrophobic surface\n* virus shedding\n* disease model\n* animal model\n* phenotype change\n* PPE effectiveness\n\nFollowing code block computes the dataframe that returns the most applicable result set for the same.","4ad2f63e":"## END OF FILE","244e38d1":"# Task 3: What do we know about virus genetics, origin, and evolution?\nFrom the description of the task, we identified following key phrases as our primary search topics:\n\n* Genome tracking\n* strain circulation\n* Nagoya Protocol\n* livestock\n* recepter binding\n* farmers\n* wildlife\n* host range\n* experimental infection\n* animal host\n\nFollowing code block computes the dataframe that returns the most applicable result set for the same.","4370fb9d":"# Project Description\n\nThis collaborative project is put together by students of TCSS 592 at the [School of Engineering and Technology, University of Washington Tacoma](https:\/\/www.tacoma.uw.edu\/set\/school-engineering-technology-home) and [NLPCORE](https:\/\/nlpcore.com) a Seattle, WA startup using NLPCORE's search engine to extract meaningful phrases (concepts) grouped together in named categories (topics) along with their specific linkages \/ relationships (joint references) in the literature. These topics could be dictionary terms such as Proteins, Cell Lines or user specified such as (Host Cells, Viruses) or dynamically extracted by the search engine based upon search terms.\n\n \n\nThe objective of the project is to provide most relevant and specific references (not just articles but specific sentences with-in each article) along with relevant biological materials as a response for the questions posed in this challenge. Our goal is to enable life sciences researchers to quickly gather, triage and identify most applicable subset of candidate proteins and\/or reagents for their experiments related to Covid-19 research.\n\n \n\nBesides extracting references, we also validated search results against LitCovid (https:\/\/www.ncbi.nlm.nih.gov\/research\/coronavirus\/) an expertly curated set articles on Covid-19 and found both good matches as well as data set anomalies. We have provided this reproducible validation test scripts along with the dataset in an accompanying Jupyter notebook available at https:\/\/www.kaggle.com\/varunmittalnlpcore\/litcovid-validation. For optimal performance, we highly recommend users to download both of these notebooks and run locally on their personal workstations. Publically hosted environments such as Kaggle and Google Collab have resource limitations that may severely impact performance and limit functionality.\n\n \n\nFinally we provide visualizations that show distribution and correlation of results - both through sample code in this notebook as well as click-through URLs to our Search Portal, to help researchers sift through results and narrow them down to a more relevant subset for their experiments or further investigations.\n\n \n\n## Background\n\nVarun Mittal - cofounder at NLPCORE, is a University of Washington alumni with Masters CS degree in AI and ML techniques and has remained as faculty support for Prof. Dr. Ka Yee Yeung at UW, who is conducting its TCSS592 class this spring. This CORD19 challenge together with expertly curated Covid-19 datasets have provided a unique opportunity for both UW TCSS592 class students and NLPORE team to work together under guidance of Dr. Ka Yee.\n\n \n\nNLPCORE is a knowledge discovery platform powered by its unique AI and ML techniques (US Patents:  [#10102274](https:\/\/patents.google.com\/patent\/US10102274B2) & [#10372739](https:\/\/patents.google.com\/patent\/US20190005049A1)) that delivers contextual and actionable results for users across various verticals \u2013 life sciences, case law, patents, insurance and more.\n\n \n\n![Identify Entities and Relationships using Part of Speech tags](https:\/\/i.imgur.com\/dXT19EW.png)\n\n \n\nIts search technology collects statistics such as word frequencies, offsets as well as part of speech tags (e.g. noun, pronoun, or verb) in its index. Words that appear most frequently and closest to the search keyword(s), provide seed articles for its neural-net algorithms that also factor in heuristics, dictionaries and in-place user-feedback. For any given search keyword(s), its search engine scans across all matching articles deploying a (Hadoop like) cluster of processing nodes to identify and retrieve the most appropriate concepts, their grouping into meaningful topics, their relationships to each other and their specific annotated references from the entire text corpus.\n\n \n\nIn this project submission, we used both the dataset provided as well as the open-access subset from NIH (pubmed central) to focus on all coronavirus related research and extract related content from both existing and newly available research. Furthermore for validation, we used expertly curated LitCovid dataset that we have enclosed as additional datasets along with this submission for reproducibility.\n\n \n\n## Extracting, Analyzing and Presenting Results\n\nIn order to respond to challenge questions, we submitted a number of search keywords to NLPCORE along with suggested topics to extract based upon students' research and suggestions. We collected these results i.e. concepts, topics and their joint references into DataFrames. We then experimented with a number of concept\/link attributes such as frequency of terms or co-occurrences, distance of these terms from searched keywords, their part of speech tags (mostly pronouns, nouns or verbs), topics they belong to, etc. as way to filter the DataFrames to the most meaningful subset. We then present the output along with individual text references (Article Ref, Title, Section Title, Surrounding Sentences) in recommended table format that can be readily exported as a CSV file and consumed by the researchers for their further analysis and experimentation.\n\n \n\n## Validating Results\n\nAs part of extracting results down to specific phrases (topics and concepts), we also compared research articles that contained these phrases against expertly curated dataset. To do so, we validated search results against LitCovid (https:\/\/www.ncbi.nlm.nih.gov\/research\/coronavirus\/) an expertly curated set. We found both good matches as well as data set anomalies (when using our search engine and CORD-19 dataset versus using our search engine and LitCovid dataset). We have provided this reproducible validation test scripts along with the dataset in an accompanying Jupyter notebook available at https:\/\/www.kaggle.com\/varunmittalnlpcore\/litcovid-validation. For optimal performance, we highly recommend users to download both of these notebooks and run locally on their personal workstations. Publically hosted environments such as Kaggle and Google Collab have resource limitations that may severely impact performance and limit functionality. The notebook contains both the datasets and scripts that were used to perform the comparison for reproducibility. We found that in cases where we were able to uniquely identify articles with proper identity (such as Pubmed or PMC ID) - both in the curated set and that used by our search engine, we had a high degree of match but when the dataset lacked a matching identification for the articles, our matches decreased significantly.\n\n \n\n## Visualization of Results\n\nWe also provided sample scripts in this notebook as well as a blog post at our website (see references below) to help researchers visualize the results for easy categorization and filtering. To this end, we provide a grouping of results by various topics and ability to drill-down to results with-in a topic. We also provide one-click access URLs that allow users to access these results at our Search Portal as well where they can further explore results with various filters and visual representations.\n\n \n\n## Reusability of our Approach\n\nThe methodology and approach that we have taken to respond to this challenge as well as to validate our search results is very easily extensible by virtue of user-defined topics in NLPCORE search engine. Besides search terms, we force the search engine to look for cluster of most relevant words (concepts) for one or more user-defined topics and therefore go above and beyond simple text or regular expression matches to identify articles and references with-in them (hit highlighting) containing these concepts. One has to simply change the list of user-defined topics and search terms to extract references and articles for any specific use case and text corpus.\n\n \n\n## Use of cache & pre-computed results\n\nOur notebooks have used cached results and reused pre-computed results both for reproducibility and more importantly as a work-around for resource limitations be it CPU usage, memory usage and network bandwidth (for web API calls) enforced by publically hosted Jupyter Notebook platforms including Kaggle and Google Collab. We therefore recommend that users download these notebooks, make changes at will and execute them locally.\n\n \n\n## References\n\n1. Kaggle CORD19 Challenge Submission (this notebook): https:\/\/www.kaggle.com\/varunmittalnlpcore\/cord19-round1-response-by-uw-and-nlpcore\n\n2. Validation of search results (accompanying notebook): https:\/\/www.kaggle.com\/varunmittalnlpcore\/litcovid-validation\n\n3. Visualization of results (sample code and blogpost): https:\/\/www.nlpcore.com\/blog_interna.html\n\n4. NLPCORE search platform (requires one-time free registration): https:\/\/search.nlpcore.com\/search-results?asp=&d=1&p=cord19-dataset&q=covid-19&rViewType=graph\n\n5. LitCovid Dataset: https:\/\/www.ncbi.nlm.nih.gov\/research\/coronavirus\/\n\n \n\n## Future Plans\n\nWe continue to improve both the quality and presentation of our results at our search portal where users can interact with results in various formats such as document, list or graph views, filter them at will for any combination of attributes from topics, concepts and\/or articles and jump to specific article with color-coded highlights (where color represents a topic category). We continue to further our engagements with life sciences researchers, help them apply results from our technology (that remains available to research community at no cost) for their experiments and identify areas of further improvements in our toolset.\n\n \n\n## Acknowledgements\n\nWe at NLPCORE acknowledge Prof. Dr. Ka Yee Yeung and her class of TCSS592 along with our intern and MIT freshman Yos Wagenmans who contributed immensely to research for and prepare this submission.\n\n","c3c5c82e":"# Tasks Attempted\nFor round 1, we have attempted to respond to following CORD-19 challenges.\n\n* Task 1: What is known about transmission, incubation, and environmental stability?\n* Task 2: What do we know about COVID-19 risk factors?\n* Task 3: What do we know about virus genetics, origin, and evolution?\n* Task 4: What do we know about vaccines and therapeutics?\n\nFor each task, we took the key phrases (mostly unique words) from the description of the task itself and forced our search engine to search the neighborhood of these words together with mention of coronavirus itself in the literature and extracted the most frequent concepts, biomaterials (proteins and cells) and their combined references in articles. These references should in most cases approximate the response to the challenge posed. We may have noisy results in our first round submission but will attempt to improve upon the same in our subsequent submission.","1fec606e":"# Task 2: What do we know about COVID-19 risk factors?\nFrom the description of the task, we identified following key phrases as our primary search topics:\n\n* Smoking\n* pre-existing pulmonary disease\n* co-infections\n* co-morbidities\n* neonates\n* pregnancy\n* high-risk patient group\n\nFollowing code block computes the dataframe that returns the most applicable result set for the same.","a986f71d":"# Task 4: What do we know about vaccines and therapeutics?\nFrom the description of the task, we identified following key phrases as our primary search topics:\n\n* naproxen\n* clarithromycin\n* minocycline\n* Antibody Dependent Enhancement (ADE)\n* therapeutic\n* antiviral agent\n* universal vaccine\n* prophylaxis (preventative)\n* vaccine immune response\n\nFollowing code block computes the dataframe that returns the most applicable result set for the same."}}