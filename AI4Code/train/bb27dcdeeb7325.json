{"cell_type":{"026e0d0d":"code","6ab6dbcf":"code","77bf4823":"code","5536c495":"code","8171e730":"code","e5d16de0":"code","24f94129":"code","0c2cd1b9":"code","24bf74e9":"code","53a4edb3":"code","fb29f2b5":"code","f518a5f4":"code","1bed9c3a":"code","ca326839":"code","afadccd8":"code","d114d938":"code","26e6870e":"code","13477535":"code","c89fd171":"code","dc5049bf":"code","1dbaeecd":"code","43db8d1c":"code","5bef3fb8":"code","ec220da1":"code","d60858e3":"code","c3045ee2":"code","0d5017ba":"code","5afff426":"code","3d9e1d2a":"code","0d5b64e7":"markdown","b2b7beb7":"markdown","609a239a":"markdown","3e475db1":"markdown","16b9d8ee":"markdown","f4be847c":"markdown","78086d6e":"markdown","f4f14e76":"markdown","a691223b":"markdown","b463f3ff":"markdown","8a495d26":"markdown","b78479bf":"markdown","95f57b5f":"markdown","0b30c848":"markdown","e4f7498a":"markdown","8e587c5e":"markdown","4bc27575":"markdown","2441b4b3":"markdown","d67826b2":"markdown","4edb8b93":"markdown","15e5e134":"markdown","53f210f7":"markdown","caf8337d":"markdown","1f8e4f23":"markdown","10d06a21":"markdown"},"source":{"026e0d0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ab6dbcf":"df = pd.read_csv(\"..\/input\/usa-cers-dataset\/USA_cars_datasets.csv\")\ndf.head()","77bf4823":"df.tail()","5536c495":"df = df.drop(['country', 'condition'], axis = 1)\ndf.head()","8171e730":"df.shape\n","e5d16de0":"duplicate_rows_df = df[df.duplicated()]\nduplicate_rows_df.shape","24f94129":"df.count()","0c2cd1b9":"df.describe()","24bf74e9":"df[df['price']==0].count()","53a4edb3":"import seaborn as sns\nsns.set(color_codes=True)\nsns.boxplot(x=df['price'])","fb29f2b5":"df[df['mileage']==0].count()","f518a5f4":"sns.boxplot(x=df['mileage'])","1bed9c3a":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","ca326839":"df = df[~((df < (Q1-1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf.shape","afadccd8":"df[df['price']==0].count()","d114d938":"df[df['mileage']==0].count()","26e6870e":"import matplotlib.pyplot as plt #visualisation\n%matplotlib inline ","13477535":"# Plotting a Histogram\ndf['brand'].value_counts().nlargest(40).plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of cars by brand\")\nplt.ylabel(\"Number of cars\")\nplt.xlabel(\"Brand\");","c89fd171":"df['state'].value_counts().nlargest(40).plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of cars by state\")\nplt.ylabel(\"Number of cars\")\nplt.xlabel(\"State\");","dc5049bf":"df['color'].value_counts().nlargest(40).plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of cars by color\")\nplt.ylabel(\"Number of cars\")\nplt.xlabel(\"Color\");","1dbaeecd":"df['year'].value_counts().nlargest(40).plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of cars by Year\")\nplt.ylabel(\"Number of cars\")\nplt.xlabel(\"Year\");","43db8d1c":"# Finding the relations between the variables\nplt.figure(figsize=(20,10))\nc= df.corr()\nsns.heatmap(c,cmap=\"BrBG\",annot=True)\nc","5bef3fb8":"# Plotting a scatter plot\nfig, ax = plt.subplots(figsize=(10,6))\nax.scatter(df['year'], df['price'])\nax.set_xlabel('Year')\nax.set_ylabel('Price')\nplt.show()","ec220da1":"fig, ax = plt.subplots(figsize=(10,6))\nax.scatter(df['mileage'], df['price'])\nax.set_xlabel('Mileage')\nax.set_ylabel('Price')\nplt.show()","d60858e3":"x= np.array(df['mileage'])\ny= np.array(df['price'])\nplt.plot(x, y, 'o')\nm, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x + b)\n","c3045ee2":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\n\nX = df['mileage'].values.reshape(-1,1)\ny = df['price'].values.reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train)","0d5017ba":"y_pred = regressor.predict(X_test)\ndf_test = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ndf_test","5afff426":"df1 = df_test.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","3d9e1d2a":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('10% of Mean Price:', df['price'].mean() * 0.1)","0d5b64e7":"From these scatter plots we can conclude that there is a linear regression between the price and the mileage.","b2b7beb7":"As we can see, there are no duplicate rows in this dataset","609a239a":"As we can notice that all the columns have 2499 rows, we can therefore conclude that there are no missing values","3e475db1":"In the above heat map we know that the price feature depends mainly on the year and the mleage.","16b9d8ee":"# 4- Finding missing Values and Outliers :","f4be847c":"# **1- Loading the data into a data frame:**\n","78086d6e":"**A- Applying the model :**","f4f14e76":"In this step we will try to use a linear regression between the \"mileage\" and the \"price\" in the aime to estimate the price.","a691223b":"*I hope you found this usefull, I will appreciate it if you could help me with your advises and I will be more than happy if you could upvote this. Thanks :) *","b463f3ff":"You can see that the value of root mean squared error is 8690, which is much greater than 10% of the mean value which is 1916. This means that our algorithm was not very accurate","8a495d26":"As we can see, we have removed 336 rows, which represents 13% of our original dataset.","b78479bf":"**B- Evaluation of the Model**","95f57b5f":"As we can notice from the boxplots, we don't only have very low points but we also have a lot of high points.\n\nIn order to detect and remove outliers we are going to use a technique called \"IQR score technique\".","0b30c848":"In the table above, we were able to find rows with the value \"0\" as a price or as mileage which cannot be correct, so it is clear that we have some incomplete rows.","e4f7498a":"**A- Missing Values :**","8e587c5e":"Since all of the cars are from the United States, I think the \"Country\" column doesn't mean much.\nAnd I also believe that the \"Condition\" column has nothing to do with the price of the car, and I can't find what it could add to our model. So in this step, we are going to delete these two columns.","4bc27575":"From the table and the bar graphe above, we can see that the difference is much or less significent. to be more precise in evaluating this model we are going to calculate the MAE, the MSE and the RMS","2441b4b3":"# 2- Dropping irrelevant columns :","d67826b2":"Some Datasets have some duplicate data which might be disturbing, In this step we will try to find the duplicate rows and remove them.","4edb8b93":"# 5- Visualizations :","15e5e134":"**B- Outliers :**","53f210f7":"# 3- Finding duplicate rows :","caf8337d":"# 6- Linear Regression : ","1f8e4f23":"We haven't deleted all rows where the price is equal to 0, but let's just say that 3 rows is better then 43.","10d06a21":"Let's compare between the real and the predicted prices from the test data after applying the linear regression."}}