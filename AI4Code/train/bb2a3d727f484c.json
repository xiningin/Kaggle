{"cell_type":{"434e23cc":"code","4458f574":"code","42cc51a4":"code","31d22e94":"code","4106424d":"code","fec99503":"code","93344729":"code","7e4e3947":"code","6829e977":"code","27f2a39e":"code","116746cf":"code","aa37711c":"code","3d6ba636":"code","b3cba285":"code","8c9f458a":"code","f92170f2":"code","0d122004":"markdown","99b08353":"markdown","7cb201ab":"markdown","aa2df661":"markdown","56d0da1c":"markdown","7aa756ec":"markdown","92c6d724":"markdown","08b01645":"markdown","33a2a568":"markdown","14530196":"markdown","6d329834":"markdown"},"source":{"434e23cc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom kaggle.competitions import twosigmanews","4458f574":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()","42cc51a4":"lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())} #the function that gets a label for our assetcodes\n\ndef prep_data(market_data):\n    # add asset code representation as int (as in previous kernels)\n    market_data['assetCodeT'] = market_data['assetCode'].map(lbl) #assigns an integer label to each asset code using the function above\n    market_col = ['assetCode','assetCodeT', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', \n                        'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', \n                        'returnsOpenPrevMktres10']\n    \n    X = market_data[market_col].fillna(0).values #fillna values with zero\n    if \"returnsOpenNextMktres10\" in list(market_data.columns):#if training data we need these to train\n        up = (market_data.returnsOpenNextMktres10 >= 0).values\n        r = market_data.returnsOpenNextMktres10.values\n        universe = market_data.universe\n        day = market_data.time.dt.date\n        assert X.shape[0] == up.shape[0] == r.shape[0] == universe.shape[0] == day.shape[0]\n    else:#data for prediction only we won't have or need these\n        up = []\n        r = []\n        universe = []\n        day = []\n    return X, up, r, universe, day","31d22e94":"X, up, r, universe, day = prep_data(market_train) #prepare the data","4106424d":"# r, u and d are used to calculate the scoring metric on test\nX_train, X_test, up_train, up_test, _, r_test, _, u_test, _, d_test = \\\ntrain_test_split(X, up, r, universe, day, test_size=0.25, random_state=99)","fec99503":"xgb_market = XGBClassifier(n_jobs=4, n_estimators=200, max_depth=8, eta=0.1)\n\nXGB_X_train = np.delete(X_train,0,1) #get rid of assetcodes in both train and test data\nXGB_X_test = np.delete(X_test,0,1)\n\nt = time.time()\nprint('Fitting Up')\nxgb_market.fit(XGB_X_train,up_train)\nprint(f'XGB Done, time = {time.time() - t}s')","93344729":"cat_up = CatBoostClassifier(thread_count=4, n_estimators=200, max_depth=8, eta=0.1, loss_function='Logloss' , verbose=10)\n\ncat_X_train = np.delete(X_train,1,1) #get rid of encoded labels since this is duplicate information, CatBoost will handle the assetCodes directly\ncat_X_test = np.delete(X_test,1,1)\n\nt = time.time()\nprint('Fitting Up')\ncat_features=[0] #this is just the column(s) in our data set that has categorical data and then we pass it to the model fitting function below\ncat_up.fit(cat_X_train, up_train, cat_features) \nprint(f'cat Done, time = {time.time() - t}')","7e4e3947":"confidence_test = xgb_market.predict_proba(XGB_X_test)[:,1]*2 -1\n\nprint(accuracy_score(confidence_test>0,up_test))\nplt.hist(confidence_test, bins='auto')\nplt.title(\"XGB predicted confidence\")\nplt.show()","6829e977":"catconfidence_test = cat_up.predict_proba(cat_X_test)[:,1]*2 -1\n\nprint(accuracy_score(catconfidence_test>0,up_test))\nplt.hist(catconfidence_test, bins='auto')\nplt.title(\"CatBoost predicted confidence\")\nplt.show()","27f2a39e":"# calculation of actual metric that is used to calculate final score\nr_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_test * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean \/ std\nprint(f'XGBoost score: {score_test}')","116746cf":"r_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_icat = catconfidence_test * r_test * u_test\ndata2 = {'day' : d_test, 'x_t_icat' : x_t_icat}\ndf2 = pd.DataFrame(data2)\nx_tcat = df2.groupby('day').sum().values.flatten()\nmean = np.mean(x_tcat)\nstd = np.std(x_tcat)\nscore_testcat = mean \/ std\nprint(f'CatBoost score: {score_testcat}')","aa37711c":"days = env.get_prediction_days()","3d6ba636":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n    # discard assets that are not scored\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_market_obs = prep_data(market_obs_df)[0]\n    X_market_obs  = np.delete(X_market_obs ,1,1) #drop the label encoded column again\n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = cat_up.predict_proba(X_market_obs)[:,1]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","b3cba285":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"CatBoost predicted confidence\")\nplt.show()","8c9f458a":"market_col = ['assetCodeT', 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', \n                        'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', \n                        'returnsOpenPrevMktres10']\nplt.figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.bar(range(len(xgb_market.feature_importances_)), xgb_market.feature_importances_)\nplt.title(\"XGB Feature Importance\")\nplt.xticks(range(len(xgb_market.feature_importances_)), market_col, rotation='vertical');","f92170f2":"plt.figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.bar(range(len(cat_up.get_feature_importance(prettified=False))), cat_up.get_feature_importance(prettified=False))\nplt.title(\"Cat Feature Importance\")\nplt.xticks(range(len(cat_up.get_feature_importance(prettified=False))), market_col, rotation='vertical');","0d122004":"# Data Preparation\n\nWe're going to define a function to make our analysis easier and that we can call when we want to process the data one day at a time for our submission. We'll keep all the columns except time and we'll add an extra column for our label encoded assets. We'll then delete the assetCode column when we use the XGBoost model, or delete the label encoded column when we use the CatBoost model since it can handle the strings directly.","99b08353":"# Prediction","7cb201ab":"# Evaluation of Test\n\nHere we'll compare plots of the predicted confidence for both models, and then score each using the criteria of the competition.","aa2df661":"This is really shockingly good. I'm not sure what exactly happened here, it may just be an outlier from our dataset, or maybe we are overfitting somehow. The leaderboard submission scores a bit better than XGBoost I think, but definitely not as well as the above result would imply. Finally let's use our CatBoost model to make predictions we can submit and then we'll look at feature importance.","56d0da1c":"Again we get a pretty good normal distribution which is a good consistency test.","7aa756ec":"# Feature importances\n\nLast we can compare the importance of each feature in the two models and see how they compare.","92c6d724":"It's interesting to note the different shape of the distributions, CatBoost predicts a much more normal distribution, while XGBoost seems to be a little skewed. I think we should expect a roughly normal distribution with mean just barely larger than zero. The CatBoost plot agrees with my intuition more but I do not have any good explanation as to why they differ. Finally, scoring both:","08b01645":"This is really striking because it looks like assetCodes are way more important in the CatBoost model. (Note: assetCodeT in the CatBoost plot is the real assetCode, not the encoded label, I just didn't change the bin label) It's very likely this is due to the nature of however CatBoost is encoding the labels,  you can read about what it's doing here: https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/algorithm-main-stages_cat-to-numberic-docpage\/  To me at least it's a  bit unclear how much of a good thing this is. Should whether an asset price goes up or down depend on who\/what it is? It strikes me sort of like saying \"well Netflix stock always goes up.\" \n\nWe also see Volume is the 3rd most important predictor in XGBoost and the worst in CatBoost which is a bit strange. You'd think if it were good for one it should be good for both although it's unclear to me why volume would be a good predictor of a stock increasing or decreasing. I could see it being a good predictor of the confidence e.g. if a stock is being sold off, a high volume might imply that it is really being sold heavily and will end the day down signficantly.\n\nClose price is reasonably close in both models, 4th in XGB and 3rd in Cat.\n\nWe also see both models agree that returnsOpenPrevRaw10 and returnsOpenPrevMktres10 are fairly important. But why the return from ten days ago and not the return from yesterday? Does it have something to do with the fact that we are predicting ten days in the future? That would seem odd.\n\nOverall I wouldn't try to pull too much out of these plots. I think the disagreement between the two may indicate just how noisy the data is and how difficult it is to really find patterns in.\n\nI hope this was helpful and if you have any questions or think you can answer any of mine please comment! Cheers!","33a2a568":"# Comparing XGBoost and Catboost\n\nThis is a simple comparison of how both models do on only the market data. I wanted to compare CatBoost with XGBoost since a lot of people have been using XGBoost and CatBoost is supposedly faster, and also is enabled to handle categorical data. As you'll see we do our own label encoding for XGBoost, but CatBoost handles this very conveniently for us.","14530196":"# Fit\n\nLet's fit both models and then see how they do. Before fitting each I'll get rid of the duplicate column, either the assetCode or the label encoding.","6d329834":"So we can see the CatBoost model has a much faster fitting with similar parameters set which should be a relief to anyone fitting the XGBoost model over and over again like I'd been doing. It took 26 minutes for the XGBoost model to fit for me, and only 5.5 for the CatBoost model. Let's see how the models prediction's compare."}}