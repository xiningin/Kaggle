{"cell_type":{"427a44d6":"code","853633b5":"code","81092142":"code","b2f9c175":"code","ebcf57dd":"code","050b1c84":"code","181e01c6":"markdown","032f6150":"markdown","17d80b96":"markdown"},"source":{"427a44d6":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef mse(y_pred, y):\n    error = np.sum(np.subtract(y_pred, y)**2)\n    return 1 \/ (2 * len(y_pred)) * error","853633b5":"y_pred = np.array([[1, 0, 1, 0, 0],\n                   [1, 0, 1, 0, 0],\n                   [1, 0, 1, 0, 0],\n                   [1, 0, 1, 0, 0]])\ny = np.array([[1, 1, 0, 0, 1], \n              [1, 0, 0, 0, 1],\n              [1, 0, 1, 0, 1],\n              [1, 0, 1, 0, 0]])\n\nerr = []\nfor pred, target in zip(y_pred, y):\n    err.append(mse(pred, target))","81092142":"plt.title(\"MSE\")\nplt.plot(err, 'r--', label=\"error\")\nplt.xlabel(\"i\")\nplt.ylabel(\"error\")\nplt.grid()\nplt.legend()","b2f9c175":"plt.plot(y[0], \"--ro\", label=\"real y\")\nplt.plot(y_pred[0], \"--ro\", label=\"predict y\", color=\"blue\")\nplt.grid()\nplt.legend()","ebcf57dd":"plt.plot(y[1], \"--ro\", label=\"real y\")\nplt.plot(y_pred[1], \"--ro\", label=\"predict y\", color=\"blue\")\nplt.grid()\nplt.legend()","050b1c84":"def cost_function(y, y_pred):\n    return (1 \/ len(y)) * sum([k**2 for k in (y - y_pred)])\n\ndef update_w(W, wb, b, bd, learning_rate):\n    return W - learning_rate * wb, b - learning_rate * bd\n\ndef gradient_descent(X, y, learning_rate=0.05, \n                           max_epochs=10,\n                           W=0.01, b=0.01):\n    \n    cost_, epoch_ = list(), list()\n    for i in range(max_epochs):\n        y_pred = W * X + b\n        cost = cost_function(y, y_pred)\n        wd = -(2 \/ len(X)) * sum(X *(y - y_pred))\n        bd = -(2 \/ len(X)) * sum(y - y_pred)\n        \n        W, b = update_w(W, wd, b, bd, learning_rate)\n        \n        cost_.append(cost)\n        epoch_.append(i)\n        \n    return cost_, epoch_\n\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 7, 9, 11, 13])\n\ncost, epoch = gradient_descent(X, y)\nplt.plot(epoch, cost, \"--ro\", label=\"error\")\nplt.grid()\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cost\")\nplt.title(\" Gradient descent\")","181e01c6":"## My example","032f6150":"## Gradient Descent\n\nIt is an optimization algorithm to find the minimum of a function. We start with a random point on the function and move in the negative direction of the gradient of the function to reach the local\/global minima.","17d80b96":"# Gradient Descent from scratch\n## Cost Function\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n\n<img src=\"https:\/\/miro.medium.com\/max\/286\/0*v5NxDJHE8Oy8Rf-2.png\">\n\n- i - index of sample,\n- \u0177 - predicted value,\n- y - expected value,\n- m - number of samples in dataset.\n"}}