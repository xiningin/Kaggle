{"cell_type":{"eec05690":"code","7381d70b":"code","e21025e3":"code","d5e47e6b":"code","14cb8f12":"code","da03cced":"code","d9b705e7":"code","637559e8":"code","944d0aef":"code","2a6d39fd":"code","a5764ec2":"code","d9b0595e":"code","0f3867f1":"code","e8676035":"code","71c3e514":"code","c543b89f":"code","fe033cca":"code","955f15bf":"code","55c662d8":"code","0d4d6b34":"code","bba0d92c":"code","cb31cc6f":"code","09a28519":"code","bc4349ea":"code","75219749":"code","17045d97":"code","33af34b3":"code","208804b6":"code","4e7926dc":"code","a3b37e1b":"code","7fc129a6":"code","7d807b47":"code","ec803392":"code","ca411ebb":"code","ccfa7601":"code","08c7576f":"code","d1cd0de3":"code","a63e0911":"code","21c661f9":"code","e360e1cd":"code","fc6d8f9a":"code","62e5f79f":"code","7960531c":"code","5359ce29":"code","7f08f2d4":"code","521ec09a":"code","f66aca2f":"code","f2607e2e":"code","a671decb":"code","420831df":"code","b6865cf5":"code","39ad5025":"code","8e3cb8c3":"code","cbd1a4b9":"code","6964307f":"code","ccaf66b4":"code","03bc5945":"code","c585382a":"code","75fe9e29":"code","880319ce":"code","e1d48fc3":"code","44d7b31a":"code","8441a15f":"code","f5dfe470":"code","e830157b":"code","091cc80c":"code","9a01b3e4":"code","5a19a16b":"code","c7459094":"code","3d200d20":"code","df7f0bc5":"code","f156e931":"code","933f078f":"code","a8816496":"code","78e6132e":"code","f4907a59":"code","1cb1b4b7":"code","2c6dbb82":"code","072bb5b0":"code","fdef97c9":"code","a7ee5c3e":"code","39a39fdd":"code","68a25f7b":"code","14760cec":"code","29e1caba":"code","29988bab":"code","642e0f96":"code","06511a02":"code","4ab97605":"code","5e22449f":"code","df0e1030":"code","6a660475":"code","fa878b52":"code","33ce3841":"code","c72c2cab":"code","1eec1d0e":"code","75e9261f":"code","963404fb":"code","a2c3b20f":"code","ec2f4151":"code","05797883":"code","3e326fbe":"code","b8cb6373":"code","7b1c9e81":"code","9e05490c":"markdown"},"source":{"eec05690":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\n\n\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport numpy as np","7381d70b":"# Loading the data","e21025e3":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","d5e47e6b":"train.head(5) ##checking if the data loaded properly","14cb8f12":"test.head(5) ##checking if the data loaded properly","da03cced":"## lets see the list of numeric fields and their summary. SOme of these numeric fields like Survived and pclass are \n## actually categories and have to be handled accordingly.\n\ntrain.describe()","d9b705e7":"## lets see the list of categorical fields and their summary\n\ntrain.describe(include=['O'])","637559e8":"## identifying the proportion of missing data in each column of the train dataset.\n\npercent_missing_train = train.isnull().sum() * 100 \/ len(train)\npercent_missing_train","944d0aef":"## identifying the proportion of missing data in each column of the test dataset.\n\npercent_missing_test = test.isnull().sum() * 100 \/ len(test)\npercent_missing_test","2a6d39fd":"## Analyzing if varaibles have a impact on the survival rate","a5764ec2":"train[['Survived','Pclass']].groupby(['Pclass']).agg({'Survived':[np.size,np.mean]})","d9b0595e":"train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':[np.size,np.mean]})","0f3867f1":"train[['Survived','Sex']].groupby(['Sex']).mean().sort_values(by='Survived')","e8676035":"train[['Survived','SibSp']].groupby(['SibSp']).agg({'Survived':[np.size,np.mean]})","71c3e514":"train[['Survived','Parch']].groupby(['Parch']).agg({'Survived':[np.size,np.mean]})","c543b89f":"train['Age'].hist(by=train['Survived'])","fe033cca":"# Extracting some additional information from the existing fields and checking if they have an impact on the survival rate. \n\n\n# To do that I am merging the datasets together so that i dont have to keep replicating the changes between the datasets.","955f15bf":"train['dataflag']='Train'\ntest['dataflag']='Test'\n\nconsolidated=train.append(test,sort=False)","55c662d8":"# Extracting the first name, last anme and the title from the Name field.\n\n# The last name and the title will be used to create additional fields.\n\nconsolidated['LastName']=consolidated['Name'].str.split(',', n = 1, expand = True)[0]\nconsolidated['Remain']=consolidated['Name'].str.split(',', n = 1, expand = True)[1]\n\nconsolidated['title']=consolidated['Remain'].str.split('.', n = 1, expand = True)[0]\nconsolidated['FirstName']=consolidated['Remain'].str.split('.', n = 1, expand = True)[1]\nconsolidated.drop('Remain', axis=1,inplace=True)","0d4d6b34":"## checking the distribution of titles. We see that some of them have very few passengers associated with them \n## so we are mergin them to a new title 'important'. Also, some of the mlle are same as other titles so merging them together.\n\nconsolidated.title.value_counts()","bba0d92c":"for row in ['Dr', 'Rev','Col', 'Major', 'Lady', 'Sir', 'Dona', 'the Countess', 'Capt', 'Don','Jonkheer']:\n    consolidated['title'] = consolidated['title'].str.replace(row, 'Important')\n    \nconsolidated['title'] = consolidated['title'].str.replace('Mlle', 'Miss')\nconsolidated['title'] = consolidated['title'].str.replace('Ms', 'Miss')\nconsolidated['title'] = consolidated['title'].str.replace('Mme', 'Mrs')\n\nconsolidated.title.value_counts()","cb31cc6f":"consolidated.pivot_table(\"PassengerId\",\"title\",\"Survived\",aggfunc=np.size).plot(kind='bar')","09a28519":"## There are multiple passengers with the same ticket, so i am creating a new field which has the count of passengers \n## for each ticket. These could be members of the same family or at least people who know each other.\n\nbyTicketPassengers= pd.DataFrame(consolidated.groupby('Ticket').count()['PassengerId']) \n\n## this field counts the number of unique last name per ticket. THis is to identify if the ticket was all members of a family.\n\nbyTicketuniquelastName=pd.DataFrame(consolidated.groupby('Ticket').agg({'LastName':'nunique'}))\n\n## renaming the fields\n\nbyTicketPassengers=byTicketPassengers.rename(columns={'PassengerId': 'NoOfPassengersinTicket'})\nbyTicketuniquelastName=byTicketuniquelastName.rename(columns={'LastName': 'NoOfFamiliesinTicket'})\n\n\n## merging it back to the consolidated dataset\nconsolidated=pd.merge(consolidated,byTicketPassengers,how='inner',on='Ticket')\nconsolidated=pd.merge(consolidated,byTicketuniquelastName,how='inner',on='Ticket')","bc4349ea":"consolidated[['Survived','NoOfFamiliesinTicket']][consolidated['dataflag']=='Train'].groupby(['NoOfFamiliesinTicket']).agg({'Survived':[np.size,np.mean]})","75219749":"## Now, when I saw the fare field, I noticed that the fare is not the individual passenger cost but the consolidated cost of\n## all passengers in the field. SO i am dividing the fare with the number of passengers in ticket.\n\nconsolidated['ticket_rate']=consolidated['Fare']\/consolidated['NoOfPassengersinTicket']","17045d97":"consolidated['ticket_rate'].hist(by=consolidated['Survived'],sharey=True)","33af34b3":"## merge the SibSp & Parch together\n\n\nconsolidated['FamilySize'] = consolidated['SibSp'] + consolidated['Parch'] + 1","208804b6":"consolidated['FamilySize'].hist(by=consolidated['Survived'],sharey=True,sharex=True)","4e7926dc":"## creating a correlation matrix to find highly correlated numeric variables.\n\nsn.heatmap(consolidated[['Age','SibSp','Parch','Fare','NoOfFamiliesinTicket','ticket_rate','FamilySize']].corr(), annot=True)","a3b37e1b":"## deleting some of the highly correlated fields\n\nconsolidated.drop(['SibSp','Parch','Fare'],axis=1, inplace=True)","7fc129a6":"## deleting fields which definitly wont help with the predictions.\n\nconsolidated.drop(['Name','Ticket','LastName','FirstName'],axis=1, inplace=True)","7d807b47":"## the family size field is highly skewed towards 0, so it might make sense to create a flag instead.\n\nconsolidated['WithFamily']=consolidated['FamilySize'].apply(lambda x: 1 if x>1 else 0)\nconsolidated.drop(['FamilySize'],axis=1, inplace=True)","ec803392":"consolidated[['Survived','WithFamily']].groupby(['WithFamily']).agg({'Survived':[np.size,np.mean]})","ca411ebb":"## start filling the missing values in the data\n\n## replacing the missing embarked with the value with highest frequency\nconsolidated.loc[consolidated.Embarked.isnull(),'Embarked']=consolidated.Embarked.mode()\n\n## replacing the missing cabins with the value u. \nconsolidated.loc[consolidated.Cabin.isnull(),'Cabin']='U'\nconsolidated['Cabin'] = consolidated['Cabin'].astype(str).str[0]\n\n## seems like the distribution is highly skewed towards u, so replacing it with a flag instead\nconsolidated['Cabin']=consolidated['Cabin'].apply(lambda x: 0 if x=='U' else 1)","ccfa7601":"consolidated.pivot_table(\"PassengerId\",\"Cabin\",\"Survived\",aggfunc=np.size).plot(kind='bar')","08c7576f":"## the missing ticket rate is for pclass =3 so replacing it with the average of the rate os the same pclass.\n\nconsolidated.loc[consolidated.ticket_rate.isnull(),'ticket_rate']=consolidated.ticket_rate[(consolidated.ticket_rate.notnull())&(consolidated.Pclass==3)].mean()\n#consolidated.loc[consolidated.Fare.isnull(),'Fare']=consolidated.Fare[(consolidated.Fare.notnull())&(consolidated.Pclass==3)].mean()","d1cd0de3":"## To predict the missing ages I am trying to create a simple model to estimate the age. ","a63e0911":"## before creating model we have to convert all categorical variables to numeric using the one hot encoding method.\n\ndata_for_Age = pd.concat([consolidated,pd.get_dummies(consolidated['Pclass'], prefix='Pclass')],axis=1)\ndata_for_Age = pd.concat([data_for_Age,pd.get_dummies(data_for_Age['Sex'], prefix='Sex')],axis=1)\ndata_for_Age = pd.concat([data_for_Age,pd.get_dummies(data_for_Age['Embarked'], prefix='Embarked')],axis=1)\ndata_for_Age = pd.concat([data_for_Age,pd.get_dummies(data_for_Age['title'], prefix='title')],axis=1)\ndata_for_Age = pd.concat([data_for_Age,pd.get_dummies(data_for_Age['Cabin'], prefix='Cabin')],axis=1)\n\ndata_for_Age.drop(['Pclass','Sex','Embarked','title','Cabin','dataflag','Survived','PassengerId'],axis=1, inplace=True)","21c661f9":"data_for_Age.head(5).transpose()","e360e1cd":"## creating a dataset with the rows with missing age removed.\n\ndata_for_Age_not_null = data_for_Age[ (data_for_Age.Age.notnull()) ]","fc6d8f9a":"data_for_Age_not_null.columns","62e5f79f":"X=data_for_Age_not_null[['NoOfFamiliesinTicket', 'ticket_rate', 'WithFamily', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C',\n       'Embarked_Q', 'Embarked_S', 'title_ Important', 'title_ Master',\n       'title_ Miss', 'title_ Mr', 'title_ Mrs']]\n\nX_all=data_for_Age[['NoOfFamiliesinTicket', 'ticket_rate', 'WithFamily', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C',\n       'Embarked_Q', 'Embarked_S', 'title_ Important', 'title_ Master',\n       'title_ Miss', 'title_ Mr', 'title_ Mrs']]\n\ny=data_for_Age_not_null['Age']","7960531c":"from sklearn.model_selection import train_test_split","5359ce29":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)","7f08f2d4":"## we will first create a simple linear regression model and see the accuracy\n\n\nfrom sklearn.linear_model import LinearRegression","521ec09a":"lm=LinearRegression()","f66aca2f":"lm.fit(X_train,y_train)","f2607e2e":"predictions=lm.predict(X_test)","a671decb":"errors = abs(predictions - y_test)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n\nmape = 100 * (errors \/ y_test)\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","420831df":"predictions_all=lm.predict(X_all)\nplt.scatter(data_for_Age.Age,predictions_all)","b6865cf5":"## running a decision tree to see if we can improve on accuracy\n\n\nfrom sklearn.tree import DecisionTreeRegressor","39ad5025":"regressor = DecisionTreeRegressor(max_depth=8,min_samples_split=20,min_samples_leaf=10,  random_state=0)\nregressor.fit(X,y)","8e3cb8c3":"from sklearn.tree import export_graphviz","cbd1a4b9":"export_graphviz(regressor, out_file ='tree.dot') ","6964307f":"predictions = regressor.predict(X_test)\n\nerrors = abs(predictions - y_test)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n\nmape = 100 * (errors \/ y_test)\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","ccaf66b4":"predictions = regressor.predict(X_all)","03bc5945":"plt.scatter(data_for_Age.Age,predictions)","c585382a":"## as decision tree has given a better accuracy we will update the missing ages with the value from this model.\n\nconsolidated=consolidated.assign(Pred=predictions)\n\nconsolidated.Age.fillna(consolidated.Pred, inplace=True)","75fe9e29":"consolidated.columns","880319ce":"## before creating the actual models we have to convert all categorical variables to numeric using the one hot encoding method.\n\nconsolidated = pd.concat([consolidated,pd.get_dummies(consolidated['Pclass'], prefix='Pclass')],axis=1)\nconsolidated = pd.concat([consolidated,pd.get_dummies(consolidated['Sex'], prefix='Sex')],axis=1)\nconsolidated = pd.concat([consolidated,pd.get_dummies(consolidated['Embarked'], prefix='Embarked')],axis=1)\nconsolidated = pd.concat([consolidated,pd.get_dummies(consolidated['Cabin'], prefix='Cabin')],axis=1)\n\nconsolidated.drop(['Pred','Pclass','Sex','Embarked','title','Cabin'],axis=1, inplace=True)","e1d48fc3":"train = consolidated[consolidated['dataflag']=='Train'].copy()\n\ntest=consolidated[consolidated['dataflag']=='Test'].copy()","44d7b31a":"test.drop(['dataflag','Survived'],axis=1, inplace=True)\ntrain.drop(['dataflag'],axis=1, inplace=True)","8441a15f":"from sklearn.model_selection import train_test_split","f5dfe470":"X = train.drop(['Survived','PassengerId'],axis=1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=11)","e830157b":"## I start with a decision tree model. \n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc","091cc80c":"## Tuned Model\n\ndtree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n max_leaf_nodes=None, min_samples_split=.01,min_samples_leaf=.2,\n random_state=123, splitter='best')","9a01b3e4":"dtree.fit(X_train,y_train)\npredictions = dtree.predict(X_test)\npredictions_train = dtree.predict(X_train)","5a19a16b":"print(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))","c7459094":"print(confusion_matrix(y_train,predictions_train))\nprint(accuracy_score(y_train,predictions_train))","3d200d20":"test_run=test.copy()\n\nif 'Survived' in test.columns:\n    test_run = test.drop(['Survived'],axis=1)\n    \ntest_run = test_run.drop(['PassengerId'],axis=1)\ndt_pred = dtree.predict(test_run)\n\ntest=test.assign(Survived=dt_pred)\n\nfinal=test[['PassengerId','Survived']].copy()\n\nfinal.to_csv('dt_20200512.csv')","df7f0bc5":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score","f156e931":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)","933f078f":"auc(false_positive_rate, true_positive_rate)","a8816496":"## then i run a random forest model for better accuracy\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=60,max_depth=6,min_samples_leaf=9,max_features=6,min_samples_split=.3\n                             ,random_state=123)\nrfc.fit(X_train, y_train)","78e6132e":"rfc_pred = rfc.predict(X_test)\nrfc_pred_train = rfc.predict(X_train)","f4907a59":"print(confusion_matrix(y_test,rfc_pred))\n\naccuracy_score(y_test,rfc_pred)","1cb1b4b7":"print(confusion_matrix(y_train,rfc_pred_train))\n\naccuracy_score(y_train,rfc_pred_train)","2c6dbb82":"print(classification_report(y_test,rfc_pred))","072bb5b0":"test_run=test.copy()\n\nif 'Survived' in test.columns:\n    test_run = test.drop(['Survived'],axis=1)\n    \ntest_run = test_run.drop(['PassengerId'],axis=1)\nrfc_pred = rfc.predict(test_run)\n\ntest=test.assign(Survived=rfc_pred)\n\nfinal=test[['PassengerId','Survived']].copy()\n\nfinal.to_csv('rfC_20200515.csv')","fdef97c9":"## then i run a xgboost model for better accuracy\n\n\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nfrom sklearn import  metrics   #Additional scklearn functions\nfrom sklearn import model_selection, metrics   #Additional scklearn functions\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","a7ee5c3e":"from sklearn.model_selection import GridSearchCV","39a39fdd":"X = train.drop(['Survived','PassengerId'],axis=1)\ny = train['Survived']\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=123)\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.20,random_state=0)","68a25f7b":"print(y_train.value_counts()\/y_train.count())\nprint(y_test.value_counts()\/y_test.count())","14760cec":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n","29e1caba":"## i run a series of codes to get the optimized parameters for each learning rate i select.\n\nlearning_rate_=.01\nparams = {\n    # Parameters that we are going to tune.\n     \"learning_rate\" :learning_rate_,\n     \"max_depth\":5,\n     \"min_child_weight\":1,\n     \"subsample\":.8,\n     \"colsample_bytree\":.8,\n     \"objective\": 'binary:logistic',\n     \"seed\":123\n}\nparams['eval_metric'] = \"error\"\nnum_boost_round = 500\n","29988bab":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n    ,verbose_eval=1000\n)","642e0f96":"\nparam_test1 = {\n 'max_depth':range(1,10,1),\n 'min_child_weight':range(1,6,1)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =learning_rate_, n_estimators=model.best_iteration, max_depth=2,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=123,), \n param_grid = param_test1, scoring='accuracy',n_jobs=4, cv=5,verbose=True)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_\nmax_depth_=gsearch1.best_params_['max_depth']\nmin_child_weight_=gsearch1.best_params_['min_child_weight']","06511a02":"param_test1 = {\n'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =learning_rate_, n_estimators=model.best_iteration, max_depth=max_depth_,\n min_child_weight=min_child_weight_, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=123), \n param_grid = param_test1, scoring='accuracy',n_jobs=4, cv=5,verbose=True)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_\ngamma_=gsearch1.best_params_['gamma']","4ab97605":"param_test1 = {\n 'subsample':[i\/10.0 for i in range(4,10)],\n 'colsample_bytree':[i\/10.0 for i in range(4,10)]\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =learning_rate_, n_estimators=model.best_iteration, max_depth=max_depth_,\n min_child_weight=min_child_weight_, gamma=gamma_, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=123), \n param_grid = param_test1, scoring='accuracy',n_jobs=4, cv=5,verbose=True)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_\nsubsample_=gsearch1.best_params_['subsample']\ncolsample_bytree_=gsearch1.best_params_['colsample_bytree']","5e22449f":"param_test1 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =learning_rate_, n_estimators=model.best_iteration, max_depth=max_depth_,\n min_child_weight=min_child_weight_, gamma=gamma_, subsample=subsample_, colsample_bytree=colsample_bytree_,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=123), \n param_grid = param_test1, scoring='accuracy',n_jobs=4, cv=5,verbose=True)\ngsearch1.fit(X_train,y_train)\ngsearch1.best_params_, gsearch1.best_score_\nreg_alpha_=gsearch1.best_params_['reg_alpha']","df0e1030":"print(learning_rate_)\nprint(model.best_iteration)\nprint(max_depth_)\nprint(min_child_weight_)\nprint(gamma_)\nprint(reg_alpha_)\nprint(subsample_)\nprint(colsample_bytree_)","6a660475":"xg_reg = XGBClassifier( learning_rate =learning_rate_,\n                       n_estimators=model.best_iteration, \n                       max_depth=max_depth_,\n                       min_child_weight=min_child_weight_,\n                       gamma=gamma_, \n                       reg_alpha=reg_alpha_,\n                       subsample=subsample_, \n                       colsample_bytree=colsample_bytree_,\n                       objective= 'binary:logistic', \n                       nthread=4, scale_pos_weight=1, seed=123\n                      )\n","fa878b52":"xg_reg.fit(X_train,y_train)","33ce3841":"xg_pred = xg_reg.predict(X_test)\nxg_pred_train = xg_reg.predict(X_train)","c72c2cab":"print(confusion_matrix(y_test,xg_pred>.5))","1eec1d0e":"accuracy_score(y_test,xg_pred>.50)","75e9261f":"accuracy_score(y_train,xg_pred_train>.50)","963404fb":"print(classification_report(y_test,xg_pred))","a2c3b20f":"from xgboost import plot_importance","ec2f4151":"plot_importance(xg_reg)","05797883":"test_run=test.copy()\n\nif 'Survived' in test.columns:\n    test_run = test_run.drop(['Survived'],axis=1)\n\ntest_run = test_run.drop(['PassengerId'],axis=1)\nxgb_pred = xg_reg.predict(test_run)","3e326fbe":"test=test.assign(Survived=xgb_pred>.5)\ntest.Survived=test['Survived'].apply(lambda x: 1 if x else 0)","b8cb6373":"final=test[['PassengerId','Survived']].copy()","7b1c9e81":"final.to_csv('xgb_20200515.csv', index=False)","9e05490c":"# Titanic Disaster: Predicting which passenger survived the shipwreck"}}