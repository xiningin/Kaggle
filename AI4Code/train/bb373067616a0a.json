{"cell_type":{"f168fcc8":"code","43abc780":"code","3ffaa5f9":"code","7a392522":"code","2ad2d339":"code","0335b3a4":"markdown","9b872811":"markdown"},"source":{"f168fcc8":"from random import randint\n\nimport torch\nimport torch.cuda as cuda\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pack_sequence, pad_packed_sequence\n\ndevice = \"cuda\" if cuda.is_available() else \"cpu\"\nprint(f\"Using device {device}\")","43abc780":"SEQ_LENGTH = 10\nNO_SEQS = 1000\n\nlengths = torch.randint(1, SEQ_LENGTH + 1, (NO_SEQS, ))\n\nx = [ torch.randint(0, 10, (length, 1)).float().to(device) for length in lengths ]\npadx = pad_sequence(x, batch_first=True)\npacx = pack_padded_sequence(padx, lengths, batch_first=True, enforce_sorted=False)\ny = padx.sum(2).sum(1)\n\nx_mean = torch.mean(pacx.data, dim=0)\nx_std = torch.std(pacx.data, dim=0)\npadx = (padx - x_mean) \/ x_std\npacx = pack_padded_sequence(padx, lengths, batch_first=True, enforce_sorted=False)\n\ny_mean = y.mean()\ny_std = y.std()\ny = (y - y_mean) \/ y_std","3ffaa5f9":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = 4\n        self.gru = nn.GRU(input_size=1, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n        self.lin = nn.Linear(self.hidden_size, 1)\n\n    def forward(self, x):\n        _, o = self.gru(x)\n        return self.lin( F.relu(o.view(-1, self.hidden_size), inplace=True) )","7a392522":"net = Net().to(device)\n\nloss_fn = nn.MSELoss()\nopt = optim.Adam(net.parameters(), lr=1e-3)\n\nB = 16\n\nfor i in range(100):\n\n    losses = []\n\n    idx = torch.randperm(padx.shape[0])\n\n    for b in range(0, padx.shape[0], B):\n\n        bx = padx[idx[b:b+B]]\n        by = y[idx[b:b+B]]\n        bp = net(pack_padded_sequence(bx, lengths[idx[b:b+B]], batch_first=True, enforce_sorted=False))\n        bp = bp.view(-1)\n\n        loss = loss_fn(bp, by)\n        net.zero_grad()\n        loss.backward()\n        opt.step()\n\n        losses.append(loss.item())\n\n    if (i+1) % 20 == 0:\n        print(\"\"\"\n        Epoch    {}\n        Loss     {}\"\"\".format(i+1, np.mean(losses)))","2ad2d339":"test_lengths = torch.randint(1, SEQ_LENGTH + 1, (10, ))\n\nx_test = [ torch.randint(0, 10, (length, 1)).float().to(device) for length in test_lengths ]\npadtx = pad_sequence(x_test, batch_first=True)\npactx = pack_padded_sequence(padtx, test_lengths, batch_first=True, enforce_sorted=False)\ny_test = padtx.sum(2).sum(1)\n\npadtx = (padtx - x_mean) \/ x_std\npactx = pack_padded_sequence(padtx, test_lengths, batch_first=True, enforce_sorted=False)\n\nprediction = net(pactx).view(-1).detach()\nprint(prediction * y_std + y_mean)\nprint(y_test)","0335b3a4":"## Sequence classification using PyTorch and Gated Recurrent Units (GRU)\nIn this notebook, we aim to classify a sequence of digits by its total sum. The dataset consists of randomly generated sequences of random length.","9b872811":"## Generation of data\nData sequences of random length (up to `SEQ_LENGTH`) are generated containing digits in the range of 0-9."}}