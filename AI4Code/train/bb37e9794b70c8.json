{"cell_type":{"a293caa3":"code","b5d32e4b":"code","484fbef2":"code","5585e0aa":"code","c0c55323":"code","5fc8bb6c":"code","85c96550":"code","ed309b9a":"code","6f6b6b3c":"code","9bd50b49":"code","a14c26e4":"code","a0fceea3":"code","ae2b6bf2":"code","20bab075":"code","cd97a65e":"code","1296c3a5":"code","bb45ebeb":"code","2e829dc7":"code","f7b3c9d1":"code","29ebbde4":"code","6139c353":"code","6029e47e":"code","dff323f7":"code","5087d15c":"code","b9ce8700":"code","e93a64df":"code","d59c8436":"code","1d8298ce":"code","b35f4750":"code","55980fad":"code","291a3c5a":"code","4576fe7c":"code","bc47c23e":"markdown","9be131e9":"markdown","4c8f0880":"markdown","f013f3f9":"markdown","c215a7af":"markdown","c6cf4a33":"markdown","ba2cb3e0":"markdown","da48de92":"markdown","fb4510eb":"markdown","5da328a1":"markdown","ad2afda7":"markdown","73773107":"markdown"},"source":{"a293caa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime as dt\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\n# Any results you write to the current directory are saved as output.","b5d32e4b":"%%time\ndf_train = pd.read_csv('..\/input\/train.csv', low_memory=True, nrows=500000)","484fbef2":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5585e0aa":"df_train = reduce_mem_usage(df_train)","c0c55323":"df_train.info()","5fc8bb6c":"df_train.isna().sum()","85c96550":"plt.figure(figsize=(5,5))\nax = sns.countplot(df_train.HasDetections)\nplt.xlabel('HasDetections');","ed309b9a":"df_train['HasDetections'].value_counts()","6f6b6b3c":"df_train.shape # So. Many. Features.","9bd50b49":"target = df_train['HasDetections']\ndf_train.drop(['HasDetections'],axis=1,inplace=True)","a14c26e4":"df_train = df_train.select_dtypes(['int8','int16','float16','float32'])\ndf_train.shape","a0fceea3":"df_train.isna().sum() # None of these have nans, let's go with these 15 features for now","ae2b6bf2":"# What % of these features are nans, only retain the ones with < 50% nans\ncols_to_keep = []\nfor col in df_train.columns:\n    if ((df_train[col].isna().sum()\/len(df_train)))<0.5:\n        print(col, (df_train[col].isna().sum()\/len(df_train)))\n        cols_to_keep.append(col)\n        print('+++++++')","20bab075":"df_train = df_train[cols_to_keep]\ndf_train.shape","cd97a65e":"# Now just for a first effort\ncols_to_keep_2 = []\nfor col in df_train.columns:\n    if len(df_train[col].value_counts())<20:\n        print(col, len(df_train[col].value_counts()))\n        cols_to_keep_2.append(col)\n        print()","1296c3a5":"df_train = df_train[cols_to_keep_2]\ndf_train.shape","bb45ebeb":"cols_for_test = df_train.columns.tolist()","2e829dc7":"df_train = pd.get_dummies(df_train,columns=df_train.columns.tolist())\ndf_train.head()","f7b3c9d1":"df_train.shape","29ebbde4":"%%time\ndf_test = pd.read_csv('..\/input\/test.csv',low_memory=True, usecols=cols_for_test)","6139c353":"df_test.head()","6029e47e":"df_test = reduce_mem_usage(df_test)","dff323f7":"df_test = df_test.select_dtypes(['int8','int16','float16','float32'])\ncols_to_keep = []\nfor col in df_test.columns:\n    if (((df_test[col].isna().sum()\/len(df_test)))<0.5) and (len(df_test[col].value_counts())<20):\n        cols_to_keep.append(col)\n        \ndf_test = df_test[cols_to_keep]","5087d15c":"df_test = pd.get_dummies(df_test,columns=df_test.columns.tolist())\ndf_test.shape","b9ce8700":"# Let's drop the columns (other than the target that are not in common between the train and test sets)\nset1 = set(df_train.columns.tolist())\nset2 = set(df_test.columns.tolist())\n\nset1 - set2","e93a64df":"set2 - set1","d59c8436":"cols_in_common = list(set1 & set2)\ndf_train = df_train[cols_in_common]\ndf_test = df_test[cols_in_common]\n\nprint(df_train.shape, df_test.shape)","1d8298ce":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","b35f4750":"features = list(df_train.columns)\n\nlgb_params = {'num_leaves': 100,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 42,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=10, shuffle=True, random_state=42)\noof_lgb = np.zeros(len(df_train))\npredictions_lgb = np.zeros(len(df_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=150)\n    oof_lgb[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions_lgb += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","55980fad":"print(\"CV score: {}\".format(roc_auc_score(target, oof_lgb)))","291a3c5a":"df_sub = pd.read_csv('..\/input\/sample_submission.csv',low_memory=True)\ndf_sub['HasDetections'] = predictions_lgb\ndf_sub.shape","4576fe7c":"filename = 'subm_{:.6f}_{}.csv'.format(roc_auc_score(target, oof_lgb), \n                     dt.now().strftime('%Y-%m-%d-%H-%M'))\nprint('save to {}'.format(filename))\n\ndf_sub.to_csv(filename, index=False)","bc47c23e":"### Let's explore these","9be131e9":"### A lot of NaNs which we shall have to tackle in due time","4c8f0880":"### Basic LightGBM (no parameter tuning yet, I just used what was working well for me on the ELO competition)","f013f3f9":"### I think the file can be read in more efficiently, but for now let's go with this and try to investigate some of the features. Our target variable is HasDetections. ","c215a7af":"## We can work with this, let's do the whole get_dummies stuff and get a basic LightGBM going","c6cf4a33":"### From the Data description::\n> Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine.","ba2cb3e0":"### So, while a number of these features have many unique values, from the names itself, we see that these are also all categorical variables. In keeping with the general lazy theme, I will only retain the variables with < 20 unique values","da48de92":"### Obviously this is a very very cursory effort, but I just feel more comfortable when I have the baseline going. Here's to a fun competition! ","fb4510eb":"### That is an extremely balanced dataset. Doubt we'll need SMOTE for this one. ","5da328a1":"### Explore the distribution of the target variable HasDetections","ad2afda7":"### Let's follow the same steps for the test set. Just read in numeric columns.","73773107":"### First lazy effort we will just retain the columns with numeric datatype and no nans"}}