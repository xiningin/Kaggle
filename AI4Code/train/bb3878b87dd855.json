{"cell_type":{"54619ced":"code","5da803b1":"code","433f0f82":"code","5fa53979":"code","dd025275":"code","89e0c230":"code","5e9d12b2":"code","99b2748e":"code","f4e56ae7":"code","eddb2e98":"code","276de7c8":"code","6d9d0577":"code","71a97aaf":"code","e128d9f2":"code","5886773b":"code","577865e9":"code","199f7c86":"code","58107cd0":"code","a490c68e":"code","06e62906":"code","95eeaaf6":"code","706a244d":"code","9766373e":"code","ac491cd6":"code","d7c63540":"code","c2d40cf4":"code","d7eacbf6":"code","111fca4b":"code","001f0e46":"markdown","3053ae47":"markdown","772133b4":"markdown","86d52cb7":"markdown","20b6d579":"markdown","caf273b2":"markdown","1348f734":"markdown","b43aeace":"markdown","f9523ba4":"markdown","3f63cf4b":"markdown","a78a427e":"markdown","6198b902":"markdown","eafe9bde":"markdown","d3a805c2":"markdown"},"source":{"54619ced":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf","5da803b1":"train = pd.read_csv(\"..\/input\/iti-mlp-iml3-rnn\/train.csv\")\ntrain.head()","433f0f82":"# Check for any null values\ntrain.isna().sum()","5fa53979":"train.info()","dd025275":"#Quantas not\u00edcias de cada classe?\nsns.countplot(x='class', data=train)","89e0c230":"# Qual o tipo de not\u00edcia mais comum por assunto?\nplt.figure(figsize=(10,5))\nsns.countplot(x='subject', data=train, hue='class')","5e9d12b2":"# processando as datas\ntrain['date'] = pd.to_datetime(train['date'], errors='coerce')\ntrain['Year'] = train['date'].dt.year\ntrain['Month'] = train['date'].dt.month\n\ntrain.head()","99b2748e":"# checando to impacto do ano em not\u00edcias fakes\nsns.countplot(x='Year', data=train, hue='class')","f4e56ae7":"# fazendo o mesmo para o m\u00eas\nsns.countplot(x='Month', data=train, hue='class')","eddb2e98":"train['text'] = train['title'] + train['text']\ntrain.drop(labels=['title'], axis=1,inplace=True)\ntrain.head()","276de7c8":"train.drop(labels=['subject','date', 'Year','Month'], axis=1, inplace=True)\ntrain.head()","6d9d0577":"# dividindo o dataset em conjuntos de treino e valida\u00e7\u00e3o\nfrom sklearn.model_selection import train_test_split\n\ntrain_sentences, val_sentences, train_labels, val_labels=train_test_split(train['text'].to_numpy(),\n                                                                            train['class'].to_numpy(),\n                                                                            test_size=0.2,\n                                                                            random_state=42)\n","71a97aaf":"# s\u00e3o 32000 not\u00edcias para treinamento e 8000 para valida\u00e7\u00e3o\nlen(train_sentences),len(val_sentences),len(train_labels),len(val_labels)","e128d9f2":"# dando uma olhada nas 10 primeiras not\u00edcias\ntrain_sentences[:10], train_labels[:10]","5886773b":"# encontrando o n\u00famero de tokens (palavras) que possu\u00edmos na base de treino\nround(sum([len(i.split()) for i in train_sentences])\/len(train_sentences))","577865e9":"# Tokenizando os textos\nmax_vocab_length = 10000\nmax_length = 416\n\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                   output_mode='int',\n                                   output_sequence_length=max_length)\n\ntext_vectorizer.adapt(train_sentences)","199f7c86":"#Criando um exemplo aleat\u00f3rio e visualizando o o texto e sua respectiva tokeniza\u00e7\u00e3o\nsample_sentence = \"Please Do Not Forget To Upvoted\"\ntext_vectorizer([sample_sentence])","58107cd0":"# Escolhendo aleatoriamente uma not\u00edcia do dataset de treino e visualizando a sua tokeniza\u00e7\u00e3o\nimport random\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text;\\n{random_sentence}\\\n\\n\\n Vectorized Version:\")\ntext_vectorizer([random_sentence])","a490c68e":"from tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length,\n                            output_dim=128,\n                            embeddings_initializer='uniform',\n                            input_length=max_length)\nembedding","06e62906":"# Vamos pegar aleatoriamente uma not\u00edcia e pass\u00e1-la pelo nosso embedding para visualizar a sa\u00edda\nrandom_sentenc = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedd version: \")\nembedding(text_vectorizer([random_sentence]))","95eeaaf6":"# Vamos usar a arquitetura de RNN chamada LSTM\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,),dtype='string')\n\n# Passando os inputs (as not\u00edcias) pelo text_vectorizer(convertendo texto em uma sequ\u00eancia de tokens)\nx = text_vectorizer(inputs) \n\n# Convertendo a sequencia de tokens em sequencias de vetors de embeddings\nx = embedding(x)\n\n# Criando a primeira camamda do modelo\nx = layers.LSTM(64)(x)\n\n\n# Camada de sa\u00edda\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\n# Ligando a camada de entrada com a camada de sa\u00edda\nmodel = tf.keras.Model(inputs, outputs, name='model_LSTM')","706a244d":"model.summary()","9766373e":"model.compile(loss='binary_crossentropy',\n             optimizer=tf.keras.optimizers.Adam(),\n             metrics=['accuracy'])\nmodel_history = model.fit(train_sentences,\n                         train_labels,\n                          epochs=5,\n                         validation_data=(val_sentences, val_labels))","ac491cd6":"plt.figure(figsize=(10,5))\nplt.plot(model_history.history['loss'], label='train_loss')\nplt.plot(model_history.history['val_loss'], label='validation_loss')\nplt.title('Loss per epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss]')\nplt.legend()","d7c63540":"#carregando e processando dados de teste\ntest = pd.read_csv('..\/input\/iti-mlp-iml3-rnn\/test.csv', index_col=0)\ntest['text'] = test['title'] + test['text']\ntest=test[['text']]\ntest_sentences = test['text'].to_numpy()\ntest_sentences","c2d40cf4":"#Utilizando o modelo que treinamos para fazer predi\u00e7\u00f5es para os dados de teste\nmodel_preds = model.predict(test_sentences)","d7eacbf6":"submission = pd.DataFrame(model_preds)\nsample_sub = pd.read_csv('..\/input\/iti-mlp-iml3-rnn\/sample_submission.csv', index_col=0)\nsubmission.index = sample_sub.index\nsubmission.columns=['Predicted']\nsubmission","111fca4b":"submission.to_csv('submission.csv')","001f0e46":"# Vamos concatenar o t\u00edtulo com o texto para utilizar as duas informa\u00e7\u00f5es de maneira simplificada","3053ae47":"# Treinamento do modelo","772133b4":"# Carregando os datasets","86d52cb7":"# An\u00e1lise Explorat\u00f3ria","20b6d579":"# Feature enginering (transformando as colunas em informa\u00e7\u00f5es que possam ser inputadas nos modelos)","caf273b2":"# Criando o modelo de RNN","1348f734":"# Avaliando as curvas de performance do modelo (treino vs valida\u00e7\u00e3o)","b43aeace":"# Importando as bibliotecas","f9523ba4":"# Criando uma camada de embedding","3f63cf4b":"Para fazer o nosso embedding, vamos usar a camada de embeddings do TensorFlow\n\nOs par\u00e2metros com os quais mais nos preocupamos para criar o embedding s\u00e3o:\n\n* **input_dim** = o tamanho do nosso vocabul\u00e1rio.\n\n* **output_dim** = o tamanho de sa\u00edda do vetor de embedding, por exemplo, um valor de 100 significaria que cada token (palavra) \u00e9 representado por um vetor tamanho 100.\n\n* **comprimento_de_entrada** = comprimento das sequ\u00eancias que est\u00e3o sendo passadas para a camada de embedding.","a78a427e":"# Dando uma olhada nos dados","6198b902":"# Gerando o arquivo de submiss\u00e3o","eafe9bde":"# Convertendo texto em valores num\u00e9ricos","d3a805c2":"Como vimos nas aula,ao lidar com um problema de texto, uma das primeiras coisas que voc\u00ea ter\u00e1 que fazer antes de construir um modelo \u00e9 converter seu texto em valores num\u00e9ricos.\n\nExistem algumas maneiras de fazer isso, por exemplo:\n\n**Tokeniza\u00e7\u00e3o** - mapeamento direto do token (um token pode ser uma palavra ou um caractere) para um n\u00famero.\n\n**Embedding** - cria\u00e7\u00e3o de um vetor para cada token (o tamanho do vetor pode ser definido e esse embedding pode ser aprendido)."}}