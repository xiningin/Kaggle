{"cell_type":{"3eb21b00":"code","27ea7249":"code","491085dd":"code","ab5fd5d7":"code","772f5670":"code","2ba59189":"code","6549a293":"code","c8f646f9":"code","cf622fb3":"code","8a67fe60":"code","56779529":"code","a398f29e":"code","ec788e12":"code","5298e56c":"code","e53078ec":"code","c32eb244":"code","4da73df6":"code","a4ca5c23":"code","03f03cfd":"code","7925da22":"code","7ff25354":"code","14cbf495":"code","32fbf42b":"code","18bcbc0a":"code","12c0390b":"code","6019fbea":"code","69c8db6c":"code","7dd70ace":"code","6889eae4":"code","7c8a1cf8":"markdown","8be37c48":"markdown","cd361759":"markdown","f41b5dca":"markdown","d1d1a9b1":"markdown"},"source":{"3eb21b00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27ea7249":"\nimport shutil\nimport random\nroot_dir = '..\/input\/surface-crack-detection\/' # data root path\nclasses_dir = ['Negative', 'Positive'] #total labels\nto_dir = '.\/'\nval_ratio = 0.20\ntest_ratio = 0.01\n\nfor cls in classes_dir:\n    os.makedirs(to_dir +'train\/' + cls)\n    os.makedirs(to_dir +'val\/' + cls)\n    os.makedirs(to_dir +'test\/' + cls)\n    # Creating partitions of the data after shuffeling\n    src = root_dir + cls # Folder to copy images from\n    allFileNames = os.listdir(src)\n    allFileNames = allFileNames[:len(allFileNames)\/\/2]\n    np.random.shuffle(allFileNames)\n    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n                                                              [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n                                                               int(len(allFileNames)* (1 - test_ratio))])\n\n    train_FileNames = [src+'\/'+ name for name in train_FileNames.tolist()]\n    val_FileNames = [src+'\/' + name for name in val_FileNames.tolist()]\n    test_FileNames = [src+'\/' + name for name in test_FileNames.tolist()]\n    print('Total images: ', len(allFileNames))\n    print('Training: ', len(train_FileNames))\n    print('Validation: ', len(val_FileNames))\n    print('Testing: ', len(test_FileNames))\n\n    # Copy-pasting images\n    for name in train_FileNames:\n        shutil.copy(name, to_dir +'train\/' + cls)\n\n    for name in val_FileNames:\n        shutil.copy(name, to_dir +'val\/' + cls)\n\n    for name in test_FileNames:\n        shutil.copy(name, to_dir +'test\/' + cls)\n","491085dd":"import tensorflow as tf","ab5fd5d7":"from keras.preprocessing.image import ImageDataGenerator","772f5670":"train_data = ImageDataGenerator(rescale = 1.\/255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True )\ntrain_data = train_data.flow_from_directory( 'train',target_size=(64, 64),batch_size=32,class_mode='binary')\n        ","2ba59189":"test_data = ImageDataGenerator(rescale = 1.\/255 )\ntest_data = test_data.flow_from_directory( 'val',target_size=(64, 64),batch_size=32,class_mode='binary')","6549a293":"cnn = tf.keras.models.Sequential()\n","c8f646f9":"cnn.add(tf.keras.layers.Conv2D(32,3,activation = \"relu\",input_shape = (64,64,3)))\ncnn.add(tf.keras.layers.MaxPooling2D(2,strides = 2))\nprint(\"hello\")","cf622fb3":"cnn.add(tf.keras.layers.Conv2D(32,3,activation = \"relu\"))\ncnn.add(tf.keras.layers.MaxPooling2D(2,strides = 2))\nprint(\"hello\")","8a67fe60":"cnn.add(tf.keras.layers.Flatten())","56779529":"cnn.add(tf.keras.layers.Dense(units = 128,activation = 'relu'))\ncnn.add(tf.keras.layers.Dense(units = 128,activation = 'relu'))","a398f29e":"cnn.add(tf.keras.layers.Dense(units = 1,activation = 'sigmoid'))","ec788e12":"cnn.compile(optimizer = \"adam\",loss = 'binary_crossentropy',metrics = ['accuracy'])","5298e56c":"cnn.summary()","e53078ec":"cnn.fit(x = train_data, validation_data = test_data, epochs = 10)","c32eb244":"from keras.preprocessing import image\n#Image location test\/cracked \n#finding image location\nallfile = os.listdir(to_dir+'test\/'+'Positive')\none_image = allfile[0 :5]\nimage_locations =[]\nfor i in range(5):\n    image_locations.append( to_dir+'test\/'+'Positive\/'+one_image[i])\nallfile = os.listdir(to_dir+'test\/'+'Negative')\none_image = allfile[0 :5]\n#image_locations =[]\nfor i in range(5):\n    image_locations.append( to_dir+'test\/'+'Negative\/'+one_image[i])\n","4da73df6":"test_img = image.load_img(image_locations[7],target_size =(64,64))\ntest_img = image.img_to_array(test_img)\ntest_img = np.expand_dims(test_img ,axis = 0)","a4ca5c23":"res = cnn.predict(test_img\/255)\nprint(res)","03f03cfd":"from IPython.display import display, Image\ndef predict_and_display(test_img,index):\n    res = cnn.predict(test_img\/255)\n    #print(res)\n    display(Image(filename= image_locations[index]))#image_locations[i]\n    if res[0][0] >= 0.5 :\n        print(\"cracked\")\n    else :\n        print(\"not cracked\")","7925da22":"for i in range(10):\n    print()\n    print()\n    print(\"<---------------\"+str(i)+' th image'+'------------------>')\n    other_img = image.load_img(image_locations[i],target_size =(64,64))\n    other_img = image.img_to_array(other_img)\n    other_img = np.expand_dims(other_img ,axis = 0)\n    predict_and_display(other_img,i)\n   ","7ff25354":"#test_data = ImageDataGenerator(rescale = 1.\/255 )\n#test_data = ver_data.flow_from_directory( 'test',target_size=(64, 64),batch_size=32,class_mode='binary')","14cbf495":"import math\nfilecount = len(test_data.filenames)\nbatch_count = 32\ntotal_batches = math.ceil(filecount\/(1.0*batch_count))\ny_exp =[]\nfor i in range(total_batches):\n    #print(test_data[i][1])\n    y_exp.extend(np.array(test_data[i][1]))","32fbf42b":"y_pred = cnn.predict(test_data)","18bcbc0a":"for i in range(len(y_pred)):\n    y_pred[i][0] = int(y_pred[i][0]>0.5)","12c0390b":"print(y_pred)","6019fbea":"from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay","69c8db6c":"cm = confusion_matrix(y_exp,y_pred)","7dd70ace":"print(cm)","6889eae4":"import matplotlib.pyplot as plt\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm,display_labels = ['Not_Cracked','Cracked'])\ndisp.plot()\nplt.show()","7c8a1cf8":"i","8be37c48":"display image and label","cd361759":"importing shutile and doing a test train and validation split","f41b5dca":"compile the model","d1d1a9b1":"model development"}}