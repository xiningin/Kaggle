{"cell_type":{"95faf72e":"code","dd18a314":"code","fa4d61ef":"code","7659d600":"code","81623337":"code","c736a979":"code","0c629a66":"code","18a4057f":"code","d3b3cee8":"code","3d37fc04":"code","836cfc56":"code","8032b8c9":"code","b82ddd5e":"code","3bc5faa4":"code","8d8c6b9f":"code","8ee9d80f":"code","000df11e":"code","b3e2d620":"code","573b92ab":"code","758cd06a":"code","bd72fd06":"code","edf9d5dc":"code","e32a00ef":"code","00af6b32":"code","43164d6b":"code","e112a6c6":"code","0396e4ce":"code","16d55dab":"code","ce983c55":"code","da7a4c9a":"code","e07fa621":"code","a5825169":"code","432ddff7":"code","4d04838f":"code","7ede4acc":"code","c1cc3edf":"code","39885f22":"code","0497c62f":"code","d36ecd83":"code","a09024b2":"code","357df501":"code","edf1e259":"code","18a58f17":"code","2cc14866":"code","7c7ef6cb":"code","833ce7cf":"code","9a2ecf59":"code","544f8795":"code","8c7e341d":"code","0201251b":"code","e15ce06c":"code","b2975461":"code","70bcc187":"code","e86d12d3":"code","5f241322":"code","362260c7":"code","c3409ae6":"code","3696f505":"markdown","1a664639":"markdown","632e6c67":"markdown","daf9b53b":"markdown","c4e272cc":"markdown","de03a621":"markdown","8e2cbb8f":"markdown","d5592e20":"markdown","e1a59703":"markdown","15d6e5fb":"markdown","30ec53cc":"markdown","ee27fa69":"markdown","9bb865da":"markdown","b649bfdb":"markdown","8bc8647b":"markdown","93efcbcc":"markdown","f5392792":"markdown","f57251cb":"markdown","758c67a0":"markdown","b1fe1af6":"markdown","a969726a":"markdown","58e8eec4":"markdown","bf69f416":"markdown","fed39e16":"markdown","503d28a2":"markdown","82ad3325":"markdown","cb853725":"markdown","d5ed0090":"markdown"},"source":{"95faf72e":"!pip install joblib","dd18a314":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.compose import ColumnTransformer\nfrom joblib import dump, load\nfrom sklearn.metrics import plot_confusion_matrix\nimport warnings\nwarnings.filterwarnings(action='ignore')\n%matplotlib inline","fa4d61ef":"dataset = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","7659d600":"dataset.head()","81623337":"dataset.info()","c736a979":"dataset.describe()","0c629a66":"dataset.target.value_counts()","18a4057f":"sns.countplot(x=\"target\", data=dataset, palette=\"bwr\")\nplt.show()","d3b3cee8":"countNoDisease = len(dataset[dataset.target == 0])\ncountHaveDisease = len(dataset[dataset.target == 1])\nprint(\"Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(dataset.target))*100)))\nprint(\"Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(dataset.target))*100)))","3d37fc04":"sns.countplot(x='sex', data=dataset, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","836cfc56":"countFemale = len(dataset[dataset.sex == 0])\ncountMale = len(dataset[dataset.sex == 1])\nprint(\"Female Patients: {:.2f}%\".format((countFemale \/ (len(dataset.sex))*100)))\nprint(\"Male Patients: {:.2f}%\".format((countMale \/ (len(dataset.sex))*100)))","8032b8c9":"pd.crosstab(dataset.age,dataset.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","b82ddd5e":"dataset['age'].describe()\ndataset['age'] = pd.cut(dataset['age'], bins=[0,40,45,50,55,60,65,70,300], labels=[1,2,3,4,5,6,7,8])","3bc5faa4":"dataset.groupby(['age']).target.value_counts()","8d8c6b9f":"dataset.groupby(['age']).target.apply(lambda g: g.value_counts()\/len(g))","8ee9d80f":"dataset.groupby(['age']).target.apply(lambda g: g.value_counts()\/dataset.target.value_counts())","000df11e":"# sex (1 = male; 0 = female)\n\ndataset[dataset['age'] == 2][dataset['target'] == 1][dataset['sex'] == 0].target.count()\n","b3e2d620":"col = dataset.columns       # .columns gives columns names in data\nprint(col)","573b92ab":"target = 'target'\nfeatures = col[:-1]","758cd06a":"total = dataset[features].isnull().sum().sort_values(ascending = False)\npercent = (dataset[features].isnull().sum()\/dataset[features].isnull().count()*100).sort_values(ascending = False)\nmissing  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing","bd72fd06":"data_map = dataset[[target]]\ndata_map[features] = dataset[features]\nplt.figure(figsize=(15,15))\nsns.heatmap(data_map.corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()","edf9d5dc":"for column in features:\n    plt.figure(figsize = (20, 3))\n    dataset.plot(kind='scatter', x=column, y=target)","e32a00ef":"duplicated_data = dataset.duplicated()\ndataset[duplicated_data]","00af6b32":"dataset.drop_duplicates(keep = False, inplace = True)","43164d6b":"duplicated_data = dataset.duplicated()\ndataset[duplicated_data]","e112a6c6":"y = dataset[target]\nX = dataset.drop([target], axis=1)","0396e4ce":"numerical_columns = list(X._get_numeric_data().columns)\ncategorical_columns = list(set(X.columns) - set(numerical_columns))","16d55dab":"numerical_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])","ce983c55":"categorical_pipeline = Pipeline([\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])","da7a4c9a":"transformer = ColumnTransformer([\n    (\"numerical\", numerical_pipeline, numerical_columns),\n    (\"categorical\", categorical_pipeline, categorical_columns)\n])","e07fa621":"X, X_validation, y, y_validation = train_test_split(X, y, test_size = 0.3, random_state = 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","a5825169":"y_train.value_counts()","432ddff7":"y_test.value_counts()","4d04838f":"y_validation.value_counts()","7ede4acc":"def train_model(k_value,metric='euclidean'):\n    knn = KNeighborsClassifier(n_neighbors=k_value,metric=metric)\n    X_train_transformer = transformer.fit_transform(X_train)\n    X_test_transformer = transformer.transform(X_test)\n    knn.fit(X_train_transformer,y_train.values.ravel())\n    y_pred = knn.predict(X_test_transformer)\n    return np.mean(y_pred != y_test.values.ravel())","c1cc3edf":"def train_model_without_scaler(k_value,metric='euclidean'):\n    knn = KNeighborsClassifier(n_neighbors=k_value,metric=metric)\n    knn.fit(X_train,y_train.values.ravel())\n    y_pred = knn.predict(X_test)\n    return np.mean(y_pred != y_test.values.ravel())","39885f22":"def plot_error_rate(error_rate):\n    plt.figure(figsize=(10,6))\n    plt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. K Value')\n    plt.xlabel('K')\n    plt.ylabel('Error Rate')","0497c62f":"error_rate = [ train_model(k_value) for k_value  in range(1,50) ]\nplot_error_rate(error_rate)","d36ecd83":"error_rate = [ train_model(k_value,metric='cosine') for k_value  in range(1,50) ]\nplot_error_rate(error_rate)","a09024b2":"error_rate = [ train_model(k_value,metric='correlation') for k_value  in range(1,50) ]\nplot_error_rate(error_rate)\n","357df501":"def train_ensemble_models(X, y):\n    clf1 = KNeighborsClassifier(n_neighbors=12, metric='euclidean')\n    clf2 = GaussianNB()\n    clf3 = DecisionTreeClassifier()\n    clf4 = RandomForestClassifier()\n\n    for clf, label in zip([clf1, clf2, clf3, clf4], ['KNeighborsClassifier', 'GaussianNB', 'DecisionTreeClassifier','RandomForestClassifier']):\n        execute_pipeline(clf, X, y, label)","edf1e259":"def execute_pipeline(clf, X, y, title):\n    pipe = Pipeline([\n        ('transformer', transformer),\n        ('reduce_dim', 'passthrough'),\n        ('classify', clf)\n    ])\n\n    N_FEATURES_OPTIONS = [2, 4, 8, 12]\n\n    param_grid = [\n        {\n            'reduce_dim': [PCA()],\n            'reduce_dim__n_components': N_FEATURES_OPTIONS\n        },\n        {\n            'reduce_dim': [SelectKBest()],\n            'reduce_dim__k': N_FEATURES_OPTIONS\n        },\n    ]\n    reducer_labels = ['PCA', 'KBest']\n\n    grid = GridSearchCV(pipe,  param_grid=param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1, return_train_score=True)\n    grid.fit(X, y)\n\n    mean_train_scores = np.array(grid.cv_results_['mean_train_score'])\n    mean_scores = np.array(grid.cv_results_['mean_test_score'])\n    mean_scores = mean_scores.reshape(2, len(N_FEATURES_OPTIONS))\n    bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + .5)\n\n    plt.figure()\n    COLORS = 'bgrcmyk'\n    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n        plt.bar(bar_offsets + i, mean_train_scores[i], label='{} train'.format(label),alpha=.7)\n        plt.bar(bar_offsets + i, reducer_scores, label='{} test'.format(label), color=COLORS[i])\n\n    plt.title(title)\n    plt.xlabel('Number of features')\n    plt.xticks(bar_offsets + len(reducer_labels) \/ 2, N_FEATURES_OPTIONS)\n    plt.ylabel('Classification accuracy')\n    plt.ylim((0, 1))\n    plt.legend(bbox_to_anchor=(0,1), loc=\"upper right\", bbox_transform=plt.gcf().transFigure)\n    plt.show()\n\n","18a58f17":"grid_result = train_ensemble_models(X_train, y_train)","2cc14866":"def train_best_model(X_train, y_train, X_validation, y_validation):\n    reduction = SelectKBest(k=8)\n    model = GaussianNB()\n\n    X_train_transformer = transformer.fit_transform(X_train)\n    X_validation_transformer = transformer.transform(X_validation)\n\n    X_train_reduction_transformer = reduction.fit_transform(X_train_transformer, y_train)\n    X_validation_reduction_transformer = reduction.transform(X_validation_transformer)\n\n    model.fit(X_train_reduction_transformer, y_train)\n    y_predict = model.predict(X_validation_reduction_transformer)\n\n    # Acur\u00e1cia, precis\u00e3o e recall\n\n    print(classification_report(y_predict, y_validation))\n    plot_confusion_matrix(model, X_validation_reduction_transformer, y_validation)\n\n    return reduction, X_train_reduction_transformer, model","7c7ef6cb":"reduction, X_train_reduction_transformer, model = train_best_model(X_train, y_train, X_validation, y_validation)","833ce7cf":"cols = reduction.get_support(indices=True)\nnew_features = []\nfor bool, feature in zip(cols, X_train.columns):\n    new_features.append(feature)\ndataframe = pd.DataFrame(X_train_reduction_transformer, columns=new_features)\ndataframe","9a2ecf59":"dataframe['target'] = y_train","544f8795":"dataframe.describe()","8c7e341d":"dataframe.tail()\n","0201251b":"plt.figure(figsize=(15,15))\nsns.heatmap(dataframe.corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()","e15ce06c":"new_data = X\nnew_data[target] = y\n\ndata_male = new_data[new_data['sex'] == 1]\ndata_female = new_data[new_data['sex'] == 0]\n\n\nX_train_male, X_test_male, y_train_male, y_test_male = train_test_split(data_male.drop([target], axis=1), data_male[target], test_size = 0.2, random_state = 42)\nX_train_female, X_test_female, y_train_female, y_test_female = train_test_split(data_female.drop([target], axis=1), data_female[target], test_size = 0.2, random_state = 42)","b2975461":"_ ,_ ,_ = train_best_model(X_train_male, y_train_male, X_test_male, y_test_male)\n","70bcc187":"_ ,_ ,_ = train_best_model(X_train_female, y_train_female, X_test_female, y_test_female)","e86d12d3":"persistence = {}\npersistence['transformer'] = transformer\npersistence['reduction'] = reduction\npersistence['model']  = model\ndump(persistence, 'persist.joblib')","5f241322":"persistence = load('persist.joblib')\n\ntransformer = persistence['transformer']\nreduction = persistence['reduction']\nmodel = persistence['model']\n\ndataset_test_transformer = transformer.transform(X_validation)\ndataset_test_reduction_transformer = reduction.transform(dataset_test_transformer)\n\npredictions = model.predict(dataset_test_reduction_transformer)","362260c7":"output = X_validation.copy()\noutput['target'] = predictions","c3409ae6":"output.to_csv('.\/answer.csv', header=col, index=False)\nprint(\"Your submission was successfully saved!\")","3696f505":"### Chooise the best K in KNN","1a664639":"## Data Exploration","632e6c67":"## Data Analysis","daf9b53b":"#### Quais as vari\u00e1veis que mais influenciam no resultado da predi\u00e7\u00e3o?","c4e272cc":"#### Caso eu fa\u00e7a um modelo s\u00f3 pra sexo masculino e outro para feminino isso melhora o resultado para cada g\u00eanero na predi\u00e7\u00e3o?","de03a621":"#### Missing data","8e2cbb8f":"#### Model Male","d5592e20":"### Imports","e1a59703":"R.:\n12 Pessoas","15d6e5fb":"#### Correlation","30ec53cc":"## Save models and results","ee27fa69":"### Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to\nthis date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n\nOne file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\n### Content\n* age: The person's age in years\n* sex: The person's sex (1 = male, 0 = female)\n* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n* chol: The person's cholesterol measurement in mg\/dl\n* fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: The person's maximum heart rate achieved\n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* ca: The number of major vessels (0-3)\n* thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* target: Heart disease (0 = no, 1 = yes)\n\n### Acknowledgements\nThe original dataset is provided by UCI (https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease).\n\n### Inspiration\nThe objective is to explore the dataset to achieve a better understanding of the heart disease in the exams results.","9bb865da":"R.:\nN\u00e3o foi observado melhoras significativas no modelo de homens, entretanto no de mulher teve uma melhora de 10%.\n","b649bfdb":"#### Qual a faixa de idade que as pessoas costumam a ter mais problemas no cora\u00e7\u00e3o?","8bc8647b":"#### Heatmap features","93efcbcc":"### Validated Model","f5392792":"## Clean Dataset","f57251cb":"# Heart Disease","758c67a0":"####  Euclidean","b1fe1af6":"#### Model Female","a969726a":"## Conclusion","58e8eec4":"#### Quantas pessoas do sexo feminino entre 40 e 45 anos tem problema no cora\u00e7\u00e3o?","bf69f416":"## Models","fed39e16":"## Data Preprocessing","503d28a2":"#### Explicar a escolha do modelo.\nR.: Foi escolhido o Naive Bayes pelo melhores resultados avaliados na etapa anterior.\n\u00c9 poss\u00edvel observar que as arvores de decis\u00e3o tiveram um comportamento de overfit.\nEntre o KNN e naive Bayes tiveram poucas diferen\u00e7as comparando os resultados, entretanto o naive bayes possuiu um resultado um pouco melhor do que o KNN.","82ad3325":"R.:\n\nSegundo a pesquisa se formos levar em conta apenas os dados da faixa et\u00e1ria,\na faixa entre 40 aos 45 anos possuem 77% de pessoas que tiveram problemas do\ncora\u00e7\u00e3o.\n\nEntretanto quando observamos os dados de um modo geral a mesma faixa que possui algum problema no cora\u00e7\u00e3o\npossui 21% das pessoas da pesquisa enquanto a faixa entre 50 e 55 anos possuem\n22% das pessoas que se possuiram problemas do cora\u00e7\u00e3o relatos na pesquisa.\n","cb853725":"#### Cosine","d5ed0090":"## Read dataset"}}