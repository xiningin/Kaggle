{"cell_type":{"22083f88":"code","00694675":"code","b8182830":"code","e229c9e4":"code","17f0270e":"code","af16ac3b":"code","c71ce158":"code","1fcca855":"code","4d11fb71":"code","8ff6fbe3":"code","1f89fae3":"code","2a7e6f45":"code","79fb1320":"code","1f4d994b":"markdown","ad4c8933":"markdown","f9dc53ed":"markdown","1e1c2f2b":"markdown","fac09a50":"markdown","ccb4eb0f":"markdown","e1e17b66":"markdown","65f92dc8":"markdown","19a0df8e":"markdown","b2632506":"markdown","42fab773":"markdown","9ffdf8c6":"markdown"},"source":{"22083f88":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndataset = pd.read_csv(\"..\/input\/50-startups-data\/50_Startups.csv\")\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,4].values","00694675":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nst = ColumnTransformer([(\"State\", OneHotEncoder(), [3])], remainder = 'passthrough')\nX = st.fit_transform(X)\nX","b8182830":"X = X[:,1:]\nX","e229c9e4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","17f0270e":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","af16ac3b":"y_pred = regressor.predict(X_test)","c71ce158":"print('- y_pred : ')\nprint(y_pred)\nprint('- y_test : ')\nprint(y_test)","1fcca855":"import numpy as np\nX = np.append(arr = np.ones((50, 1)).astype(float), values = X, axis = 1)\nX_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)","4d11fb71":"import statsmodels.api as sm\nmodel = sm.OLS(endog = y, exog = X_opt)\nregressor_OLS = model.fit()\nregressor_OLS.summary()","8ff6fbe3":"X_opt = X[:, [0, 1, 3, 4, 5]]\nX_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","1f89fae3":"X_opt = np.array(X[:, [0,3, 4, 5]], dtype=float)\n#X_opt = X[:, [0, 3, 4, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","2a7e6f45":"X_opt = np.array(X[:, [0, 3, 5]], dtype=float)\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","79fb1320":"X_opt = np.array(X[:, [0,3]], dtype=float)\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","1f4d994b":"Now we will repeat the process after removing the independent variables with highest p value.","ad4c8933":"Let us compare are see how well our model did. As you can see below, our model did pretty well.","f9dc53ed":"Finally we are left with only 1 independent variable which is the R&D spent.\n\n### Conclusion\n\nThe entire backward elimination process can be automated. I\u2019ve done it step by step so we can understand it better.\nWe can build our model again but this time taking only 1 independent variable which is the R&D spent and do the prediction and our results will be better than the first time.","1e1c2f2b":"After running the above code snippet, we can see that 3 dummy variables have been added as we had 3 different States.\nNow, we have to remove one of the dummy variables. You can read about the dummy variable trap and why we need to remove one of the dummy variables.","fac09a50":"### Split dataset into training set and test set\nNext, we have to split the dataset into training and testing. We will use the training dataset for training the model and then check the performance of the model on the test dataset.\nFor this we will use the *train_test_split* method from library *model_selection*","ccb4eb0f":"### Fit our model to training set\nThis is a very simple step. We will be using the **LinearRegression** class from the library *sklearn.linear_model*.","e1e17b66":"Let\u2019s examine the output:\nx1 and x2 are the 2 dummy variables we added for state.\nx3 is R&D spent.\nx4 is Admin spent.\nx5 is marketing spent.\nWe have to look for the highest P value greater than 0.5 which in this case is 0.99 (99%) for x2.\nSo we have to remove x2 (2nd dummy variable for state) which has index 2.","65f92dc8":"I\u2019m going to be looking at Multiple Linear Regression. Unlike Simple Linear Regression where there is one independent variable and one dependent variable, in Multiple Linear Regression there are several independent variables that could have an effect on determining the dependent variable.\n\n### Objective\nWe want our model to predict the profit based on the independent variables described above. So Profit is the dependent variable and the other 4 are independent variables.","19a0df8e":"### Predict the test set\nUsing the regressor we trained in the previous step, we will now use it to predict the results of the test set and compare the predicted values with the actual values.","b2632506":"Now we will start the backward elimination process. Since we will be creating a new optimal matrix of features, we will call it X_opt. This will contain only the independent features that are significant in predicting profit.\nNext we create a new regressor of the OLS class (Ordinary Least Square) from statsmodels library.\nIt takes 2 arguments\n- endog : which is the dependent variable.\n- exog : which is the matrix containing all independent variables.\n\nNow we need to fit the OLS algorithm, then we will look at the summary to see which independent variable has p value higher than SL (0.05).","42fab773":"### Backward Elimination\nIn the model that we just built, we used all the independent variables but its possible that some independent variables are more significant than others and have a greater impact on the profit and some are not significant meaning if we remove them from the model, we may get better predictions.\nThe first step is for us to add a column of 1\u2019s to our X dataset as the first column. ","9ffdf8c6":"### Convert text variable to numbers\nWe can see that in our dataset we have a categorical variable \u201cState\u201d, which we need to encode. Here the \u201cState\u201d variable is at index 3.\nWe use *LabelEncoder* class to convert text to numbers."}}