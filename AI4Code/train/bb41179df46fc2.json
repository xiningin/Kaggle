{"cell_type":{"5cb10f28":"code","658b8a33":"code","6e35f5a5":"code","0f9d058a":"code","398a6d42":"code","bc5790f7":"code","137f5ec1":"code","3cb8d84e":"code","f9c79540":"code","5deeefa9":"code","71be61a9":"code","a92911cc":"code","de392258":"code","98af1552":"code","30779796":"code","76904079":"code","9962a4c1":"code","1807a796":"code","9e2c0faa":"code","a7515f3f":"code","fbabda27":"code","52e313a7":"code","f039bb42":"code","93c27caf":"code","b638b176":"code","9553bd93":"code","5c01868e":"code","8e9211a7":"code","72dad062":"code","8bb97027":"code","6e3212e0":"code","c1d92f1f":"code","569fe236":"code","725ece79":"code","09ebc163":"code","b5c99f70":"code","bfa56e41":"code","29358521":"code","6a40d95a":"markdown","1383a52b":"markdown","a311a560":"markdown","a7fb7ade":"markdown","a130b5c8":"markdown","4d111bc9":"markdown","29da0102":"markdown","83f6e727":"markdown","d164d0ef":"markdown","76e08332":"markdown","801d39ec":"markdown","33d122d6":"markdown","3578a7c8":"markdown","add52dc0":"markdown","09846e2f":"markdown","fa76c5b6":"markdown","cf8ae998":"markdown","c581ad20":"markdown","0f348eda":"markdown","fcbee16c":"markdown"},"source":{"5cb10f28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.manifold import TSNE \nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","658b8a33":"# Read and store train and test data\ntrain_df = pd.read_csv(\"..\/input\/X_train.csv\", squeeze = True)\ntest_df = pd.read_csv(\"..\/input\/X_test.csv\")","6e35f5a5":"# shape of train data\ntrain_df.shape","0f9d058a":"# Each window has 128 readings\nprint(\"Dimension of train data\", train_df.shape[0]\/\/128)\n# To see first 5 train data-points\ntrain_df.head()","398a6d42":"# Read y_train(target)\ny_train = pd.read_csv(\"..\/input\/y_train.csv\", skipinitialspace = True, squeeze = True)\ny_train.head()","bc5790f7":"# Encode class label\ny_train[\"surface_label\"] = y_train[\"surface\"].map({'fine_concrete':0, 'concrete':1, 'soft_tiles':2, 'tiled':3, 'soft_pvc':4,\n       'hard_tiles_large_space':5, 'carpet':6, 'hard_tiles':7, 'wood':8}) ","137f5ec1":"# To merge train_data with corresponding class label\ntrain_df = pd.merge(train_df, y_train[[\"surface_label\", \"series_id\"]], on = \"series_id\") \ntrain_df.head()","3cb8d84e":"# Unique feature names \n# Belongs to 9 categories\ny_train.surface.unique()","f9c79540":"# To see first 5 data-points\nprint(\"Test data dimension\",test_df.shape[0]\/\/128)\ntest_df.head() ","5deeefa9":"# Check for number of datapoints per class\ny_train[\"surface\"].value_counts()","71be61a9":"# Plot per class data-points\nplt.figure(figsize = (12, 8))\nsns.countplot(y_train[\"surface\"])\nplt.title(\"Number of datapoints per class\")\nplt.ylabel(\"Number of datapoints\")\nplt.xlabel(\"Class name\")\nplt.show()","a92911cc":"# Check for duplicates row in train and test\nprint(\"Number of duplicates row in train data {}\".format(sum(train_df.duplicated())))\nprint(\"Number of duplicates row in test data {}\".format(sum(test_df.duplicated())))","de392258":"# Check for null\/nan values in both\nprint(\"Total number of null\/nan values in train data \\n{}\".format(train_df.isnull().sum()))","98af1552":"print(\"Total number of null\/nan values in test data \\n{}\".format(test_df.isnull().sum()))","30779796":"# Boxplot of angular_velocity_X,Y,Z\nplt.figure(figsize = (12, 10))\nsns.boxplot(x = y_train[\"surface\"], y = train_df[\"angular_velocity_Z\"], data = train_df)\nplt.show()","76904079":"# Boxplot for orientation_X\nplt.figure(figsize = (10, 8))\nsns.boxplot(x = y_train[\"surface\"], y = train_df[\"orientation_X\"], data = train_df)\nplt.show()","9962a4c1":"# Boxplot for linear_acceleration_X\nplt.figure(figsize = (10, 8))\nsns.boxplot(x = y_train[\"surface\"], y = train_df[\"linear_acceleration_X\"], data = train_df)\nplt.show()","1807a796":"# Distribution plot for linear_acceleration_X\nlabel = y_train[\"surface\"].unique()\nplt.figure(figsize = (10, 8))\ncolor = [\"r\", \"g\", \"b\", \"c\", \"k\", \"y\", \"lime\", \"orange\", \"m\"]\nfor i in range(len(y_train.surface.unique())):\n    df = train_df[train_df[\"surface_label\"] == i]\n    sns.distplot(df[\"linear_acceleration_X\"], color = color[i], hist = False, label = label[i])\n    plt.tight_layout()\nplt.show()","9e2c0faa":"train_data = train_df.drop([\"surface_label\"], axis = 1)\ntrain_data.columns[3:]","a7515f3f":"# Signal magnitude area\nimport math\ndef sma(x, y, z):\n    sum = 0\n    for i in range(len(x)):\n        sum += (abs(x[i]) + abs(y[i]) + abs(z[i]))\n    return sum\/len(x)","fbabda27":"train_data['sma'] = sma(train_data['angular_velocity_X'], train_data['angular_velocity_Y'], train_data['angular_velocity_Z'])\ntest_df['sma'] = sma(test_df['angular_velocity_X'], test_df['angular_velocity_Y'], test_df['angular_velocity_Z'])","52e313a7":"# https:\/\/www.kaggle.com\/jesucristo\/1-robots-eda-rf-predictions-0-72\ndef feat_eng(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z'])** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z'])**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n                             data['orientation_Z'])**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_q25'] = data.groupby(['series_id'])[col].quantile(0.25)\n        #df[col + '_q50'] = data.groupby(['series_id'])[col].quantile(0.5)\n        df[col + '_q75'] = data.groupby(['series_id'])[col].quantile(0.75)\n        #df[col + '_mad'] = data.groupby(['series_id'])[col].mad()\n        #df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        #df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n        #df[col + '_angle'] = data.groupby(['series_id'])[col].apply(lambda x: np.angle(x, deg = True))\n        #df[col + 'perm_entropy'] = data.groupby(['series_id'])[col].apply(lambda x: ent.permutation_entropy(x, order = 3, normalize = True))\n    return df","f039bb42":"X_train = feat_eng(train_data)\ntest_data = feat_eng(test_df)\nprint (X_train.shape)\nX_train.head()","93c27caf":"#tsne_data = train_df.drop(\"surface_label\", axis = 1)\ntsne_data = X_train\n#sampled = tsne_data[0:3810]\nx_tsne = MinMaxScaler().fit_transform(tsne_data) \ny_tsne = y_train[\"surface\"]\n# Convert nan to num\nx_tsne = np.nan_to_num(x_tsne)","b638b176":"# performs t-sne with different perplexity values and their repective plots..\n\ndef perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n        \n    for index,perplexity in enumerate(perplexities):\n        # perform t-sne\n        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n        print('Done..')\n        \n        # prepare the data for seaborn         \n        print('Creating plot for this t-sne visualization..')\n        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n        \n        # draw the plot in appropriate place in the grid\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n                   palette=\"Set1\")\n        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n        print('saving this plot as image in present working directory...')\n        plt.savefig(img_name)\n        plt.show()\n        print('Done')\n","9553bd93":"# Call method to plot tsne\nperform_tsne(X_data = x_tsne,y_data = y_tsne, perplexities = [2, 5, 10, 20, 50])","5c01868e":"plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8e9211a7":"def perform_model(test_data, model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, \\\n                 print_cm=True, cm_cmap=plt.cm.Greens):\n    \n    \n    # to store results at various phases\n    results = dict()\n    \n    # time at which model starts training \n    train_start_time = datetime.now()\n    print('training the model..')\n    model.fit(X_train, y_train)\n    print('Done \\n \\n')\n    train_end_time = datetime.now()\n    results['training_time'] =  train_end_time - train_start_time\n    print('training_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['training_time']))\n    \n    \n    # predict test data\n    print('Predicting test data')\n    test_start_time = datetime.now()\n    y_pred = model.predict(X_test)\n    prediction = model.predict(test_data)\n    #y_pred = np.argmax(y_pred, axis=1)\n    test_end_time = datetime.now()\n    print('Done \\n \\n')\n    results['testing_time'] = test_end_time - test_start_time\n    print('testing time(HH:MM:SS:ms) - {}\\n\\n'.format(results['testing_time']))\n    results['predicted'] = y_pred\n   \n    # calculate overall accuracty of the model\n    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n    # store accuracy in results\n    results['accuracy'] = accuracy\n    print('---------------------')\n    print('|      Accuracy      |')\n    print('---------------------')\n    print('\\n    {}\\n\\n'.format(accuracy))\n    \n    \n    # confusion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    results['confusion_matrix'] = cm\n    if print_cm: \n        print('--------------------')\n        print('| Confusion Matrix |')\n        print('--------------------')\n        print('\\n {}'.format(cm))\n        \n    # plot confusin matrix\n    plt.figure(figsize=(8,8))\n    plt.grid(b=False)\n    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix', cmap = cm_cmap)\n    plt.show()\n    \n    # get classification report\n    print('-------------------------')\n    print('| Classifiction Report |')\n    print('-------------------------')\n    classification_report = metrics.classification_report(y_test, y_pred)\n    # store report in results\n    results['classification_report'] = classification_report\n    print(classification_report)\n    \n    # add the trained  model to the results\n    results['model'] = model\n    \n    return prediction, results\n    \n    ","72dad062":"def print_grid_search_attributes(model):\n    # Estimator that gave highest score among all the estimators formed in GridSearch\n    print('--------------------------')\n    print('|      Best Estimator     |')\n    print('--------------------------')\n    print('\\n\\t{}\\n'.format(model.best_estimator_))\n\n\n    # parameters that gave best results while performing grid search\n    print('--------------------------')\n    print('|     Best parameters     |')\n    print('--------------------------')\n    print('\\tParameters of best estimator : \\n\\n\\t{}\\n'.format(model.best_params_))\n\n\n    #  number of cross validation splits\n    print('---------------------------------')\n    print('|   No of CrossValidation sets   |')\n    print('--------------------------------')\n    print('\\n\\tTotal numbre of cross validation sets: {}\\n'.format(model.n_splits_))\n\n\n    # Average cross validated score of the best estimator, from the Grid Search \n    print('--------------------------')\n    print('|        Best Score       |')\n    print('--------------------------')\n    print('\\n\\tAverage Cross Validate scores of best estimator : \\n\\n\\t{}\\n'.format(model.best_score_))","8bb97027":"X_train = np.nan_to_num(X_train)\n#X_test = np.nan_to_num(X_test)\ntest_data = np.nan_to_num(test_data)","6e3212e0":"X_train.shape, y_train[\"surface_label\"].shape","c1d92f1f":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train[\"surface_label\"], test_size = 0.3, random_state = 4, stratify = y_train[\"surface_label\"], shuffle = True)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","569fe236":"# start Grid search\nlabels = ['fine_concrete', 'concrete', 'soft_tiles', 'tiled', 'soft_pvc', 'hard_tiles_large_space', 'carpet', 'hard_tiles', 'wood']\nparameters = {'C':[0.01, 0.1, 1, 10, 20, 30], 'penalty':['l2','l1']}\nlog_reg = linear_model.LogisticRegression()\nlog_reg_grid = GridSearchCV(log_reg, param_grid = parameters, cv = 10, verbose = 1, n_jobs = -1)\npredict_lr, log_reg_grid_results =  perform_model(test_data, log_reg_grid, X_train, y_train, X_test, y_test, class_labels = labels)","725ece79":"# Confusion matrix\nplt.figure(figsize = (8,8))\nplt.grid(b = False)\nplot_confusion_matrix(log_reg_grid_results['confusion_matrix'], classes = labels, cmap = plt.cm.Greens, )\nplt.show()","09ebc163":"# observe the attributes of the model \nprint_grid_search_attributes(log_reg_grid_results['model'])","b5c99f70":"# Model\nfrom sklearn.ensemble import GradientBoostingClassifier\nparam_grid = {'max_depth': np.arange(5,8,1), \\\n             'n_estimators':np.arange(130,170,10)}\ngbdt = GradientBoostingClassifier()\ngbdt_grid = GridSearchCV(gbdt, param_grid=param_grid, n_jobs=-1)\npredict_gbdt, gbdt_grid_results = perform_model(test_data, gbdt_grid, X_train, y_train, X_test, y_test, class_labels=labels)\nprint_grid_search_attributes(gbdt_grid_results['model'])","bfa56e41":"# Model\nparams = {'n_estimators': np.arange(10,201,20), 'max_depth':np.arange(3,15,2)}\nrfc = RandomForestClassifier()\nrfc_grid = GridSearchCV(rfc, param_grid=params, cv = 10, n_jobs=-1)\nlabels = ['fine_concrete', 'concrete', 'soft_tiles', 'tiled', 'soft_pvc', 'hard_tiles_large_space', 'carpet', 'hard_tiles', 'wood']\npredict_rf, rfc_grid_results = perform_model(test_data, rfc_grid, X_train, y_train, X_test, y_test, class_labels = labels)\nprint_grid_search_attributes(rfc_grid_results['model']) ","29358521":"# Submission \nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission[\"surface\"] = predict_rf\nsubmission[\"surface\"] = submission[\"surface\"].map({0:'fine_concrete', 1:'concrete', 2:'soft_tiles', 3:'tiled', 4:'soft_pvc',\n       5:'hard_tiles_large_space', 6:'carpet', 7:'hard_tiles', 8:'wood'}) \nsubmission.to_csv(\"sample_submission.csv\", index = False)","6a40d95a":"> We do not have any null\/nan and duplicated data in the dataset. That's great!","1383a52b":"# **Evaluation\/Performence  Metric** <br>\nThe problems comes under the multiclass classification and the performence metric is **Multiclass Accuracy**, which is simply the average number of observations with the correct label. ","a311a560":"# **Generic method to run any model**","a7fb7ade":"# Stay Tuned...........","a130b5c8":"# **RandomForest Classifier with Hyperparameter Tuning**","4d111bc9":"We have data imbalanced problem. We will look into this concern and handle it. ","29da0102":"# ** Logistic Regression with Hyperparameter Tuning**","83f6e727":"- Data are not fully clusterd together but they are nicely clustered togeather means it can be seperated in higher dimension space and also we can get more clean plot(All the same class points can be in a group) if we change perplexity and iteration. ","d164d0ef":"# **Method to print gridserach attribute**","76e08332":"# **Descriptions**<br>\nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\nWe\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.","801d39ec":"# **Feature Engineering**","33d122d6":"- Accuracy is low and confusion matrix is self-explanatory. We will use another model. ","3578a7c8":"# **Data Exploration**","add52dc0":"# **Function to plot confusion matrix**","09846e2f":"We will introduce some features that may useful in prediction and will explore some of them later. \n* **mean():** Mean value\n* **std():** Standard deviation\n* **mad():** Median absolute deviation\n* **max():** Largest value in array\n* **min():** Smallest value in array\n* **sma():** Signal magnitude area\n* **iqr():** Interquartile range\n* **entropy():** Signal entropy\n* **arCoeff():** Autorregresion coefficients with Burg order equal to 4\n* **correlation():** correlation coefficient between two signals\n* **maxInds():** index of the frequency component with largest magnitude\n* **meanFreq():** Weighted average of the frequency components to obtain a mean frequency\n* **skewness():** skewness of the frequency domain signal\n* **kurtosis():** kurtosis of the frequency domain signal\n* **angle():** Angle between to vectors.","fa76c5b6":"# **GBDT With Hyperparameter Tuning**","cf8ae998":"# **DataSet Information** <br>\n   - **Train_data & Test_data** - Used to train model. It contains 10 sensor channels and 128 measurements per time series plus three ID columns. We can think of as a sensors signals are processed by applying some filters and then sampled in fixed-windows that contains 128 readings each. <br> \n      - **row_id** - Current row number<br>\n      - **series_id** - ID number for the measurement series. Foreign key to y_train\/sample_submission.<br>\n      - **measurement_number** - measurement number within the series<br>\n      - **orientation_W,X,Y,Z** - The 10 sensor channels that measures the current angles of how robot is oriented as quaternion<br>\n      - **angular_velocity_X,Y,Z** - The 10 sensor channels that measures anguler velocity(rotational angle per unit time) and speed of motion same as gyroscope sensor<br>\n      - **Linear_accleration_X,Y,Z** - The 10 sensor channels that measure how speed is changing at different times.<br>\n   - **Y_train **- The surface of training set<br>\n      - **series_id** - ID number for the measurement series.<br>\n      - **group_id** - Number of all measurement in recording sessions.<br>\n      - **surface ** - Class label\/target.<br>\n   - **sample_submission** - We need to submit prediction that contains series_id and target<br>\n   \n   ","c581ad20":"- Looks like distribution of feature linear_acceleration_X is peaked(i.e. kurtosis is high) and almost centered at 0. It seems like gaussion but it is not. \n- Data doesn't looks like linearly sepearable so we will create some features that might be useful in predicting class label.\n- Without domain knowledge eda has no meaning.","0f348eda":"# **Objective**<br>\nWe have to predict which one of the 9 floor types robot is standing. ","fcbee16c":"# **Y_labels(Encoded)**<br>\nAs the problems is multiclass problem so we will encode all class labels into 1 to 9.<br>\n - fine_concrete                    1\n - concrete                            2\n - soft_tiles                            3\n - tiled                                   4\n - soft_pvc                            5\n - carpet                               6\n - hard_tiles_large_space    7\n - hard_tiles                         8\n - wood                                9"}}