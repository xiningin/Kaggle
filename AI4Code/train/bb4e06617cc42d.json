{"cell_type":{"517d362b":"code","cb42ebbe":"code","d06310b6":"code","53918472":"code","a08b7989":"code","afae773a":"code","84770cb4":"code","3f27f032":"code","d12b91e1":"code","5f05836a":"code","dfd913fd":"code","a63d7115":"code","59420571":"code","d7e61879":"code","7a0ef83b":"code","2048abf3":"code","d704efd1":"code","8a46da0e":"code","8c4ac63a":"code","dfb7ccb5":"code","4464e147":"code","5179aaf9":"code","1585a844":"code","ebe9d1cc":"code","c4486277":"code","9e440b8d":"code","32a5cc15":"code","4f3468f6":"code","af0d0fd1":"code","f0e27938":"code","dd6ee572":"code","da4cec13":"code","d5e789cd":"code","7651a37b":"code","2e9dd9fe":"code","8bbe5f5f":"code","d0dbebaa":"code","db8e9e5c":"code","83354fb1":"code","0bc72a68":"code","5b4fe392":"code","c3cbf754":"code","2f0c6cb5":"code","2f5c02c8":"code","84a7c919":"code","d3b72b1e":"code","15e7cea0":"code","b042d3d4":"code","548f45b8":"code","3b6ad5b9":"code","32e9842f":"code","25de0557":"code","2806bd41":"code","a1f60379":"code","945ef638":"code","1b489187":"code","a739bda2":"code","3f9b022c":"code","d3e915d2":"code","884eac73":"code","96be10f2":"code","66594b5b":"markdown","b8dd253e":"markdown","b5facada":"markdown","4509d396":"markdown","8c2b02b6":"markdown","0cae8b5f":"markdown","e18f63a7":"markdown","e6437325":"markdown","0cc86cc4":"markdown","c4139ec7":"markdown","2a4c13ce":"markdown","4a0e16b2":"markdown","0b85b385":"markdown","97c92311":"markdown","0b9079a6":"markdown","b5d1074d":"markdown","2b541693":"markdown"},"source":{"517d362b":"# from google.colab import drive\n# drive.mount('\/content\/drive')","cb42ebbe":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport pickle\n\ntitanic=pd.read_csv('..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv')\n","d06310b6":"import matplotlib.pyplot as plt\n%matplotlib inline","53918472":"titanic.keys()","a08b7989":"titanic","afae773a":"titanic.isnull()","84770cb4":"sns.heatmap(titanic.isnull(),cbar=False,cmap='viridis')\n#This indicates that if cabin values are absent in the dataset we should remove \n#them and we see in age column there are\n# moderate missing values so  we will try to replace them with appropriate values ","3f27f032":"titanic.drop(columns='Cabin',inplace=True)","d12b91e1":"sns.set_style('whitegrid')","5f05836a":"sns.countplot(x='Survived',data=titanic)#We see the number of people who surived vs who didnt","dfd913fd":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=titanic)\n#more female survivedand more male died","a63d7115":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=titanic,palette='rainbow')\n# We see that the the better the class(lesser number) the more the chance of survival comparitively","59420571":"sns.distplot(titanic['Age'].dropna(),kde=False,bins=40)\n# we infer the age of people were in the range 20-30 and infants and \n#kids were more in on the ship","d7e61879":"sns.countplot(x='SibSp',data=titanic)#people with  siblings and spouse","7a0ef83b":"sns.boxplot(x='Pclass',y='Age',data=titanic)\n# here we infer that people in class1 are more older as they would be more wealthy","2048abf3":"def impute_age(cols):# we assume that the better class the mean from \n#boxplot will give the predicted age like better class(lower number)\n#so better class corresponds to higher age we substitute theses valuesin our datset\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 36\n\n        elif Pclass == 2:\n            return 28\n\n        else:\n            return 23\n\n    else:\n        return Age","d704efd1":"sns.jointplot(x='Fare',y='Age',data=titanic,kind='scatter')\n# here we see more peole go for a cheaper fare and \n#more passengers have ages near 20-30 as found earlier ","8a46da0e":"sns.countplot(x='Sex',data=titanic)\n# we have more males on the ship but we see ore female survived","8c4ac63a":"sns.heatmap(titanic.corr(),cmap='coolwarm')\n#p class is oppositley related to fare as awe see more the fare less the class number and also better calss means better chance of survival\n#here diagonal is 1 as diagonal has same values  and Sibsp and Parch are closely related\n#Parch - Number of Parents\/Children Aboard\n#Sibsp - Number of Siblings\/Spouses Aboard\n#pclass is related to age thats older age corresponds to  lower class\n#similarly \n#and this makes sense as they will have a family so they are closely related\n#also we see the fare increases so the chance of survival ","dfb7ccb5":"sns.heatmap(titanic.isnull(),cbar=False,cmap='viridis')#check the missing values in age","4464e147":"titanic['Age'] = titanic[['Age','Pclass']].apply(impute_age,axis=1)#called our custom fn for putting mssing values","5179aaf9":"sns.heatmap(titanic.isnull(),cbar=False,cmap='viridis')","1585a844":"titanic.keys()","ebe9d1cc":"titanic.isna().sum()","c4486277":"titanic.dropna(inplace=True)","9e440b8d":"titanic.info()","32a5cc15":"titanic.head()","4f3468f6":"pd.get_dummies(titanic['Embarked'],drop_first=True).head()#get dummies to conver the 3 categories of Embarked now we drop th first coz if the 2 are false then it automatically means that 3rd is true","af0d0fd1":"sex=pd.get_dummies(titanic['Sex'],drop_first=True)\nembarked=pd.get_dummies(titanic['Embarked'],drop_first=True)","f0e27938":"titanic.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","dd6ee572":"titanic.head()","da4cec13":"titanic=pd.concat([titanic,sex,embarked],axis=1)","d5e789cd":"titanic","7651a37b":"from sklearn.model_selection import train_test_split\n#titanic.drop(labels=['Date','Location','WindDir9am','WindGustDir','WindDir3pm'],axis=1,inplace=True)\n# #weather.drop(labels=['WindDir3pm'],axis=1,inplace=True)\n# titanic.RainTomorrow.replace(to_replace=['No', 'Yes'], value=[0, 1],inplace=True)\n# titanic.RainToday.replace(to_replace=['No', 'Yes'], value=[0, 1],inplace=True)\n\n#weather=weather.fillna(0)\n\nX=titanic.drop('Survived',axis=1)\ny=titanic['Survived']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=20)\n","2e9dd9fe":"X_train\n","8bbe5f5f":"from sklearn.linear_model import LogisticRegression\n#logmodel = LogisticRegression()\nlogmodel = LogisticRegression(solver='liblinear')\nlogmodel.fit(X_train,y_train)\npredictions = logmodel.predict(X_test)\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions))","d0dbebaa":"print(classification_report(y_test,predictions))","db8e9e5c":"from sklearn.neighbors import KNeighborsClassifier\n","83354fb1":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \n","0bc72a68":"error_rate","5b4fe392":"error_rate.index(min(error_rate))","c3cbf754":"knn = KNeighborsClassifier(n_neighbors=34)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=34')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","2f0c6cb5":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\npredictions = dtree.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","2f5c02c8":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\n","84a7c919":"rfc_pred = rfc.predict(X_test)\n\nprint(confusion_matrix(y_test,rfc_pred))\n","d3b72b1e":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,rfc_pred)","15e7cea0":"print(classification_report(y_test,rfc_pred))\nmodel=rfc","b042d3d4":"# rfc.best_score_","548f45b8":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\ngbm_param_grid = {\n    'n_estimators': range(8, 20),\n    'max_depth': range(6, 10),\n    'learning_rate': [.4, .45, .5, .55, .6],\n    'colsample_bytree': [.6, .7, .8, .9, 1]\n}\ngbm = XGBClassifier(n_estimators=100)\nxgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator = gbm, scoring = \"accuracy\", \n                                    verbose = 1, n_iter = 50, cv = 4)\nxgb_random.fit(X, y)\nrfc2=xgb_random.predict(X_test)\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", xgb_random.best_params_)\nprint(\"Best accuracy found: \", xgb_random.best_score_)\nprint(\"accuracy=\",accuracy_score(y_test,rfc2))\nprint(classification_report(y_test,rfc2))","3b6ad5b9":"print(confusion_matrix(y_test,rfc2))","32e9842f":"print(classification_report(y_test,rfc2))\n","25de0557":"from sklearn.svm import SVC","2806bd41":"svc_model=SVC()\nsvc_model.fit(X_train,y_train)","a1f60379":"from sklearn.model_selection import GridSearchCV","945ef638":"param_grid = {'C': [1, 10], 'gamma': [10,1,0.1,0.01]} ","1b489187":"grid=GridSearchCV(SVC(),param_grid,verbose=2,refit=2)","a739bda2":"grid.fit(X_train,y_train)","3f9b022c":"grid_predictions=grid.predict(X_test)","d3e915d2":"print(confusion_matrix(y_test,grid_predictions))","884eac73":"print(classification_report(y_test,grid_predictions))","96be10f2":"filename = 'finalized_model.sav'\n\npickle.dump(model, open(filename, 'wb'))\n\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)","66594b5b":"# Training","b8dd253e":"## **Logistic regression**","b5facada":"# Predictions","4509d396":"### Fitting","8c2b02b6":"## **K-nearest Neighbours**","0cae8b5f":"## **Descision Trees**","e18f63a7":"### With XGB","e6437325":"<a href=\"https:\/\/colab.research.google.com\/github\/a-ma-n\/Codechef\/blob\/main\/titanicCodechef_task_1.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","0cc86cc4":"## **Random Forests**","c4139ec7":"## **Logistic Regression**\n[[109  20]\n [ 28  57]]\n precision    recall  f1-score   support\n\n           0       0.80      0.84      0.82       129\n           1       0.74      0.67      0.70        85\n\n    accuracy                           0.78       214\n   macro avg       0.77      0.76      0.76       214\nweighted avg       0.77      0.78      0.77       214\n\n## **KNN:**\n[[125   4]\n [ 62  23]]\n\n\n              precision    recall  f1-score   support\n\n           0       0.67      0.97      0.79       129\n           1       0.85      0.27      0.41        85\n\n    accuracy                           0.69       214\n   macro avg       0.76      0.62      0.60       214\nweighted avg       0.74      0.69      0.64       214\n## **Descision Tree**\n[[90 39]\n [28 57]]\n              precision    recall  f1-score   support\n\n           0       0.76      0.70      0.73       129\n           1       0.59      0.67      0.63        85\n\n    accuracy                           0.69       214\n   macro avg       0.68      0.68      0.68       214\nweighted avg       0.70      0.69      0.69       214\n\n## **Random Forests:**\ntrees=100\n\n[[114  15]\n [ 27  58]]\n\n precision    recall  f1-score   support\n\n           0       0.81      0.88      0.84       129\n           1       0.79      0.68      0.73        85\n\n    accuracy                           0.80       214\n   macro avg       0.80      0.78      0.79       214\nweighted avg       0.80      0.80      0.80       214\n\n## **Random Forests with boosting:**\ntrees=8\n[[163   3]\n [ 14  87]]\n\n  precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95       166\n           1       0.97      0.86      0.91       101\n\n    accuracy                           0.94       267\n   macro avg       0.94      0.92      0.93       267\nweighted avg       0.94      0.94      0.94       267\n\n\n\n\n\n## **SVM:**\n\n[[129   0]\n [ 85   0]]\n\nprecision    recall  f1-score   support\n\n           0       0.60      1.00      0.75       129\n           1       0.00      0.00      0.00        85\n\n    accuracy                           0.60       214\n   macro avg       0.30      0.50      0.38       214\nweighted avg       0.36      0.60      0.45       214\n","2a4c13ce":"# Results:","4a0e16b2":"*choosing k value*","0b85b385":"### Data Analysis","97c92311":"### We conclude that the random forest classifier with XGBoost is the best model with a accuracy of 93.632958801%","0b9079a6":"## Conversion to categorical variables","b5d1074d":"# Saving the model","2b541693":"## **SVM**\n"}}