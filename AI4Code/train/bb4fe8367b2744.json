{"cell_type":{"b2d43d0b":"code","9d4108d4":"code","8a04365a":"code","d4eeb1fa":"code","844c7e84":"code","9d259525":"code","7152054a":"code","ecbff268":"code","fdda42da":"code","f1d6b7ba":"code","0c470498":"code","b0eac645":"code","eac2303c":"markdown","fac54bdf":"markdown","dee66ecb":"markdown","f6fd5a72":"markdown","796e0bed":"markdown","c64f0fb7":"markdown","0e7af13e":"markdown","af6c8ede":"markdown","bf7a629c":"markdown","54b574ab":"markdown"},"source":{"b2d43d0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d4108d4":"import re\nimport os\nimport nltk\nimport time\nimport scipy\nimport string\nimport operator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as pt\nimport plotly.graph_objects as go\nfrom collections import defaultdict\nfrom wordcloud import WordCloud, STOPWORDS\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, RandomizedSearchCV, train_test_split","8a04365a":"\"\"\"Loading the train and test data\"\"\"\nquora_train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\nquora_test = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","d4eeb1fa":"punctuation = \"=\u2208\u2022\u2192\u2248\u2260(\u2217\u2639\uff0f\u00bc\u061f\u30fb\u0311\u2103\u00d7\u00a6\\\\\u2282\u23a0\u2026\u222b\u0334\u0300\uff09\u2601|\u00b4{\u2202\u2032\u02dc\u00a5\u2154\u25be\u2220\u00ae\u0bc1\u00a2\u2153\u2714\u2261\u2191\u2502\u00a8\u0301\u0336\u030a\/\u2205\u00ac\u2013\u2609\u2228\uff08\u0338\u032a\uff04\u060c\uff01\uff0c\u221e\u02c2\u2265&)\u270f\u2229\u0309-\u2193\u201d?\u2245\u0302\u00f7\u2033\u27e8\u0303+\u0335\u2713\u2661\u0333;\u0308\\\n[\u0bbe\u00be*\u0342:\uff1e\u22c5\u00b0]_\u00a3\u2668\u270c\u261d#\u0964\u264f\u20ac\uff1d\u00b1\u239b\u2105\u2234\u27e9\u0bbf\u2014\u030e\u2264\u0323\u0305\u0652\u239d\u2018\u00ab@\u030c\u22ef\u25b3\u0331\u222a\u2020\u3001\u0332%\u0315\u25e6\u00bf\uff0d\u2218$\u0313\u0321\u2663\u239e^\u2206<\u2286\u0317.\u2764\uff1b\u2019\u223c\u201e\u2753\u00b7\u2122,\u0316\u00bb\u00bd\u207b\u2640\u00b8\u201b\u00a9\u00a1\u0310\u0337\u2030\u2211\u221a\u21d2>\uff1a\u02da\u2705\u033e\u0bcd`\u22a5\u2207\u0361!\u0304\u27a1~\u033f\\x92\u00b6\\\n\u263a\u00a7\u2212\u266d\u3002\u2592\u201c\uff1f\u221d\u2b07\uff3e\u00af\u25cc}\\'\u2200\u2227\\x02\u09cd\u0d41\u0346\u0324\u0327\u0e35\u0944\\xad\u030b\u0903\u0318\u300a\u035c\\u200c\u09bf\u032b\\u2060\u0357\u0e38\u0c42\u2981\u0351\\u202a\\x7f\u17bc\u0e49\u0b3e\u0e39\u0352\u0330\u033c\u033a\u20a6\\x06\u034b\u064c\u17c7\u0c02\u035a\u0c46\u17d2\u231a\u0c4d\u20b1\u33d1\u09be\u0341\u093e\u0329\u034c\u093c\u20a9\\uf0d8\u3016\u0345\u22a8\u05bf\u3017\u0a3e\u030d\ufe0f\u300c\u0963\\x8d\u0e48\u0940\u33d2\u0350\u09cb\u0bcb\u0326\u3007\u300d\u0325\\\n\\u200b\u0a82\\x13\u221b\u034a\\x9d\u26a7\u300b\u0348\u031d\u031c\u09c7\u0343\u2216\u0c40\u0c3e\u032d\u0314\\x8f\u0356\u09c3\\u200e\u064e\u0312\u0344\u035b\u0a70\\ufeff\u0abe\u20b9\\u200f\u0949\u0bc8\u17b8\ufe21\ufffc\u0a3c\u0355\u032f\u034e\u17b6\\ue019\\x1b\u0349\u0901\\x10\\x01\u032e\\u2061\u094d\u0ccb\\u202c\\\n\u2011\u0d3f\u0947\u035d\u0acd\u17b7\u0358\u0948\\x17\u0a40\\uf02d\\x1a\u0306\u2221\u09c0\u0cbf\u0a41\u29fd\u17c3\u033d\\x03\u0319\u031b\u0902\u0c3f\u180c\u0a3f\u2271\u29fc\u0e31\u0ccd\u0360\u0650\u0942\u031a\u0340\u208a\\ue01b\u0cbe\u094c\u17be\u17c6\u0347\u09bc\u031e\u0354\u20d7\u094b\u0943\u032c\u0c4a\u0cc1\u0d4d\u033b\ufe20\u0c41\u031f\u0a02\u0322\u0d3e\u0962\u093f\u0bcc\u0339\u0941\u0651\u2010\"\n\ncontractions = {\"'aight\": 'alright', \"ain't\": 'am not', \"amn't\": 'am not', \"aren't\": 'are not', \n                \"can't\": 'can not', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \n                \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \n                \"dasn't\": 'dare not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \n                'dunno': \"don't know\", \"d'ye\": 'do you', \"e'er\": 'ever', \"everybody's\": 'everybody is', \n                \"everyone's\": 'everyone is', 'finna': 'fixing to', \"g'day\": 'good day', 'gimme': 'give me', \n                \"giv'n\": 'given', 'gonna': 'going to', \"gon't\": 'go not', 'gotta': 'got to', \n                \"hadn't\": 'had not', \"had've\": 'had have', \"hasn't\": 'has not', \"haven't\": 'have not', \n                \"he'd\": 'he had', \"he'll\": 'he will', \"he's\": 'he is', \"he've\": 'he have', \"how'd\": 'how did',\n                'howdy': 'how do you do', \"how'll\": 'how will', \"how're\": 'how are', \"how's\": 'how is', \n                \"I'd\": 'I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'm\": 'I am', \n                \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', 'innit': 'is it not', \"I've\": 'I have', \n                \"isn't\": 'is not', \"it'd \": 'it would', \"it'll\": 'it will', \"it's \": 'it is', \n                'iunno': \"I don't know\", \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \n                \"may've\": 'may have', 'methinks': 'me thinks', \"mightn't\": 'might not', \n                \"might've\": 'might have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \n                \"must've\": 'must have', \"needn't\": 'need not', 'nal': 'and all', \"ne'er\": 'never', \n                \"o'clock\": 'of the clock', \"o'er\": 'over',\"ol'\": 'old', \"oughtn't\": 'ought not', \"'s\": 'is',\n                \"shalln't\": 'shall not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \n                \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"somebody's\": 'somebody has', \n                \"someone's\": 'someone has', \"something's\": 'something has', \"so're\": 'so are', \"that'll\": 'that will', \n                \"that're\": 'that are', \"that's\": 'that is', \"that'd\": 'that would', \"there'd\": 'there would', \n                \"there'll\": 'there will', \"there're\": 'there are', \"there's\": 'there is', \"these're\": 'these are', \n                \"they've\": 'they have', \"this's\": 'this is', \"those're\": 'those are', \"those've\": 'those have', \"'tis\": 'it is', \n                \"to've\": 'to have', \"'twas\": 'it was', 'wanna': 'want to', \"wasn't\": 'was not', \"we'd\": 'we would', \n                \"we'd've\": 'we would have', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \n                \"what'd\": 'what did', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what does', \"what've\": 'what have',\n                \"when's\": 'when is', \"where'd\": 'where did', \"where'll\": 'where will', \"where're\": 'where are',\n                \"where's\": 'where is',\"where've\": 'where have', \"which'd\": 'which would', \"which'll\": 'which will', \n                \"which're\": 'which are',\"which's\": 'which is', \"which've\": 'which have', \"who'd\": 'who would',\n                \"who'd've\": 'who would have', \"who'll\": 'who will', \"who're\": 'who are', \"who'ves\": 'who is', \"who'\": 'who have',\n                \"why'd\": 'why did', \"why're\": 'why are', \"why's\": 'why does', \"willn't\": 'will not', \"won't\": 'will not',\n                'wonnot': 'will not', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have',\n                \"y'all\": 'you all', \"y'all'd've\": 'you all would have', \"y'all'd'n've\": 'you all would not have',\n                \"y'all're\": 'you all are', \"cause\":\"because\",\"have't\":\"have not\",\"cann't\":\"can not\",\"ain't\":\"am not\",\n                \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have', 'cannot': 'can not', \n                'wont': 'will not', \"You'\": 'Am not', \"Ain'\": 'Am not', \"Amn'\": 'Am not', \"Aren'\": 'Are not',\n                \"Can'\": 'Because', \"Could'\": 'Could have', \"Couldn'\": 'Could not have', \"Daren'\": 'Dare not', \n                \"Daresn'\": 'Dare not', \"Dasn'\": 'Dare not', \"Didn'\": 'Did not', \"Doesn'\": 'Does not', \"Don'\": \"Don't know\", \n                \"D'\": 'Do you', \"E'\": 'Ever', \"Everybody'\": 'Everybody is', \"Everyone'\": 'Fixing to', \"G'\": 'Give me', \n                \"Giv'\": 'Going to', \"Gon'\": 'Got to', \"Hadn'\": 'Had not', \"Had'\": 'Had have', \"Hasn'\": 'Has not', \n                \"Haven'\": 'Have not', \"He'\": 'He have', \"How'\": 'How is', \"I'\": 'I have', \"Isn'\": 'Is not', \"It'\": \"I don't know\", \n                \"Let'\": 'Let us', \"Ma'\": 'Madam', \"Mayn'\": 'May not', \"May'\": 'Me thinks', \"Mightn'\": 'Might not', \n                \"Might'\": 'Might have', \"Mustn'\": 'Must not have', \"Must'\": 'Must have', \"Needn'\": 'And all', \"Ne'\": 'Never',\n                \"O'\": 'Old', \"Oughtn'\": 'Is', \"Shalln'\": 'Shall not', \"Shan'\": 'Shall not', \"She'\": 'She is', \n                \"Should'\": 'Should have', \"Shouldn'\": 'Should not have', \"Somebody'\": 'Somebody has', \"Someone'\": 'Someone has', \n                \"Something'\": 'Something has', \"So'\": 'So are', \"That'\": 'That would', \"There'\": 'There is',\n                \"They'\": 'They have', \"This'\": 'This is', \"Those'\": 'It is', \"To'\": 'Want to', \"Wasn'\": 'Was not',\n                \"Weren'\": 'Were not', \"What'\": 'What have', \"When'\": 'When is', \"Where'\": 'Where have', \"Which'\": 'Which have', \n                \"Who'\": 'Who have', \"Why'\": 'Why does', \"Willn'\": 'Will not', \"Won'\": 'Will not', \"Would'\": 'Would have',\n                \"Wouldn'\": 'Would not have', \"Y'\": 'You all are',\"What's\":\"What is\",\"What're\":\"What are\",\"what's\":\"what is\",\n                \"what're\":\"what are\", \"Who're\":\"Who are\", \"your're\":\"you are\",\"you're\":\"you are\", \"You're\":\"You are\",\n                \"We're\":\"We are\", \"These'\": 'These have', \"we're\":\"we are\",\"Why're\":\"Why are\",\"How're\":\"How are \",\n                \"how're \":\"how are \",\"they're \":\"they are \", \"befo're\":\"before\",\"'re \":\" are \",'don\"t ':\"do not\", \n                \"Won't \":\"Will not \",\"could't\":\"could not\", \"would't\":\"would not\", \"We'\": 'We have',\"Hasn't\":\"Has not\",\n                \"n't\":\"not\", 'who\"s':\"who is\"}\n\ncorrect_words = dict({\"\u221a\":\" sqrt \",\"\u03c0\":\" pi \",\"\u03b1\":\" alpha \",\"\u03b8\":\" theta \",\"\u221e\":\" infinity \",\"\u221d\":\" proportional to \",\"sinx\":\" sin x \",\n                \"cosx\":\" cos x \", \"tanx\":\" tan x \",\"cotx\":\" cot x \", \"secx\":\" sec x \", \"cosecx\":\" cosec x \", \"\u00a3\":\" pound \", \"\u03b2\":\" beta \", \n                \"\u03c3\": \" theta \", \"\u2206\":\" delta \",\"\u03bc\":\" mu \",'\u222b': \" integration \", \"\u03c1\":\" rho \", \"\u03bb\":\" lambda \",\"\u2229\":\" intersection \",\n                \"\u0394\":\" delta \", \"\u03c6\":\" phi \", \"\u2103\":\" centigrade \",\"\u2260\":\" does not equal to \",\"\u03a9\":\" omega \",\"\u2211\":\" summation \",\"\u222a\":\" union \",\n                \"\u03c8\":\" psi \", \"\u0393\":\" gamma \",\"\u21d2\":\" implies \",\"\u2208\":\" is an element of \", \"\u2261\":\" is congruent to \",\"x\u207f\":\" x power n\",\n                \"\u2248\":\" is approximately equal to \", \"~\":\" is distributed as \",\"\u2245\":\" is isomorphic to \",\"\u2a7d\":\" is less than or equal to \",\n                \"\u2265\":\" is greater than or equal to \",\"\u21d0\":\" is implied by \",\"\u21d4\":\" is equivalent to \", \"\u2209\":\" is not an element of \",\n                \"\u2205\" : \" empty set \", \"\u221b\":\" cbrt \",\"\u00f7\":\" division \",\"\u33d2\":\" log \",\"\u2207\":\" del \",\"\u2286\":\" is a subset of \",\"\u00b1\":\" plus\u2013minus \",\n                \"\u2282\":\" is a proper subset of \",\"\u20ac\":\" euro \",\"\u33d1\":\" ln \",\"\u20b9\":\" rupee \",\"\u2200\":\" there exists \",\"\u2206\":\" delta \",\"\u2211\":\" summation\",\n                \"=\":\" equal to \",\"\u20b9\":\" rupee \",\"\u2264\":\" less than or equal to \", \"\u00b1\":\" plus or minus \", \"\u00a3\":\" pound \",\"\u221d\":\" propertional to \",\n                \"\u00bc\":\" one by four \",\"&\":\" and \",\"\u2122\":\" trade mark \",\"\u00bd\":\" one by two \",\"\uff04\":\" dollar \",\"quorans\":\"quora\",\n                \"cryptocurrencies\":\"cryptocurrency\",\"haveheard\":\"have heard\",\"amafraid\":\"am afraid\",\"amplanning\":\"am planning\",\n                \"demonetisation\":\"demonetization\",\"pok\u00e9mon\":\"pokemon\",\"havegot\":\"have got\",\"amscared\":\"am scared\",\"qoura\":\"quora\",\n                \"haveread\":\"have heard\",\"fianc\u00e9\":\"fiance\", \"amworried\":\"am worried\",\"amfeeling\":\"am feeling\",\"havetried\":\"have tried\", \n                \"amwriting\":\"am writing\",\"havealways\":\"have always\",\"amconfused\":\"am confused\", \"havejust\":\"have just\",\"amgay\":\"am gay\",\n                \"amstudying\":\"am studying\",\"amtalking\":\"am talking\",\"amdepressed\":\"am depressed\",\"havenoticed\":\"have noticed\",\n                \"amdating\":\"am dating\", \"x\u00b2\":\"x square\",\"quoras\":\"quora\",\"amcurious\":\"am curious\",\"havelost\":\"have lost\",\n                \"amunable\":\"am unable\", \"haverecently\":\"have recently\", \"amasking\":\"am asking\", \"amsick\":\"am sick\", \"clickbait\":\"click bait\", \n                \"haveever\": \"have ever\", \"amapplying\":\"am applying\", \"haveknown\":\"have known\",\"ampregnant\":\"am pregnant\",\n                \"haveonly\":\"have only\",\"amalone\":\"am alone\",\"havestarted\":\"have started\", \"\u00b2\":\"square\", \"amlearning\":\"am learning\",\n                \"amconstantly\":\"am constantly\", \"amugly\":\"am ugly\", \"amstruggling\":\"am struggling\", \"amready\":\"am ready\", \"s\u00e3o\":\"sao\", \n                \"amturning\":\"am turning\", \"genderfluid\":\"gender fluid\", \"wouldrather\":\"would rather\", \"chapterwise\":\"chapter wise\", \n                \"undergraduation\":\"under graduation\", \"blockchains\":\"blockchain\", \"amwondering\":\"am wondering\",\"havecompleted\":\"have completed\", \n                \"amextremely\":\"am extremely\", \"amattracted\":\"am attracted\", \"amlosing\":\"am losing\", \"fianc\u00e9e\":\"fiance\",\n                \"amangry\":\"am angry\", \"amaddicted\":\"am addicted\", \"havegotten\":\"have gotten\", \"makaut\":\"make out\", \"havegotten\":\"have gotten\", \n                \"amyoung\":\"am young\", \"amfalling\":\"am falling\", \"clich\u00e9s\":\"cliches\", \"beyonc\u00e9\":\"beyonce\",\n                \"erdo\u011fan\":\"erdogan\", \"atat\u00fcrk\":\"ataturk\", \"amfinding\":\"am finding\", \"ampreparing\":\"am preparing\", \"whyis\":\"why is\", \n                \"haveused\":\"have used\", \"ammarried\":\"am married\",  \"2k17\":\"2017\", \"cos2x\":\"cos 2x\", \"flipcart\":\"flipkart\", \n                \"brexit\":\"britain exit\", \"havefallen\":\"have fallen\",\"demonitisation\":\"demonetization\", \"microservices\":\"micro services\",\n                \"amallergic\":\"am allergic\", \"amskinny\":\"am skinny\", \"amaware\":\"am aware\",\"amdoing\":\"am doing\",\"amtired\":\"am tired\",\n                \"p0rnographic\":\"pornographic\",\"1st\":\"first\",\"2nd\":\"second\",\"3rd\":\"third\",\"ww2\":\"www\",\"ps4\":\"play station four\"})\n\ndefined_stopwords = ['i','me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n                    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n                    'himself', 'she', \"she's\", 'her', 'hers','herself', 'it', \"it's\", 'its', 'itself', \n                    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'or', 'because', 'as',\n                    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n                    'who', 'whom', 'this', 'why','that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n                    'was', 'were', 'be', 'been', 'the', 'and', 'but', 'if', 'through', 'during', 'before', \n                    'after', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n                    'an', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n                    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n                    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n                    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's',\n                    't', 'u', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', \n                    'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n                    \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n                    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n                    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \n                    \"won't\", 'wouldn', \"wouldn't\",\"would\", \"could\", 'the']\n\n\"\"\"mapping all questions text into string and making it lowercase\"\"\"\ndef preprocessing(text,stopwords,contractions,correct_words,punctuation):\n\n    start_time = time.time()\n    #removing punctuation\n    translate_table = dict((ord(char), None) for char in punctuation) \n\n    #convert all the words to lower case first and then remove the stopwords\n    text = text.map(str)\n    for line in range(len(text.values)):\n        text.values[line] = text.values[line].lower()\n        \n    #decontraction\n    for idx,val in enumerate(text.values):\n        val = ' '.join(word.replace(word,contractions[word]) if word in contractions else word for \n                       word in val.split())\n        #generic one\n        val = re.sub(r\"\\'s\", \" \", val); val = re.sub(r\"\\''s\", \" \", val); val = re.sub(r\"\\\"s\", \" \", val);\n        val = re.sub(r\"n\\''t\", \" not \", val); val = re.sub(r\"n\\\"t\", \" not \", val); \n        val = re.sub(r\"\\'re \", \" are \", val); val = re.sub(r\"\\'d \", \" would\", val); \n        val = re.sub(r\"\\''d \", \" would\", val); val = re.sub(r\"\\\"d \", \" would\", val);\n        val = re.sub(r\"\\'ll \", \" will\", val); val = re.sub(r\"\\''ll \", \" will\", val); \n        val = re.sub(r\"\\\"ll \", \" will\", val);val = re.sub(r\"\\'ve \", \" have\", val); \n        val = re.sub(r\"\\''ve \", \" have\", val); val = re.sub(r\"\\\"ve \", \" have\", val);\n        val = re.sub(r\"\\'m \", \" am\", val); val = re.sub(r\"\\''m \",\" am\", val); \n        val = re.sub(r\"\\\"m \",\" am\", val); val = re.sub(\"''\",\"\",val); val = re.sub(\"``\",\"\",val);\n        val = re.sub('\"','',val); val = re.sub(\"\u0307\",'',val); val = re.sub(\"\\s{2}\",\" \",val)\n        \n        #replacing correct word with incorrect one\n        val = ' '.join(word.replace(word,correct_words[word]) if word in correct_words else word \n                       for word in val.split())\n        \n        #removing stopwords\n        val = ' '.join(e.lower() for e in val.split() if e.lower() not in stopwords)\n        \n        #Removing special characters\n        val = val.translate(translate_table)\n        \n        #mapping text after above steps\n        text.values[idx] = val.strip() \n    \n    #lemmatization\n    lemmatizer = WordNetLemmatizer()\n    for idx,val in enumerate(text.values):\n        sent = ''\n        for word in word_tokenize(val):\n            sent += lemmatizer.lemmatize(word) + ' '\n        text.values[idx] = sent.strip()\n    \n    hours, rem = divmod(time.time()-start_time, 3600)\n    print(\"Time to process the whole function : {:02}.{:0>2}.{:02} minutes.\".format(int(hours),\n                                                                            int(divmod(rem, 60)[0]), \n                                                                            int(divmod(rem, 60)[1])))\n\n    return text ","844c7e84":"quora_train['question_text_preprocessed'] = preprocessing(quora_train['question_text'],defined_stopwords,\n                                                          contractions,correct_words,punctuation)\nquora_test['question_text_preprocessed'] = preprocessing(quora_test['question_text'],defined_stopwords,\n                                                         contractions,correct_words,punctuation)","9d259525":"\"\"\"Here I am omitting diversity_score, total_punctuations and total_digit as these features \n    are not kind of indentical for both class.\"\"\"\n\nstart_time = time.time()\n\n\"\"\"number of words in the question text\"\"\"\nquora_train[\"total_words\"] = quora_train[\"question_text\"].apply(lambda sent: len(str(sent).split()))\nquora_test[\"total_words\"] = quora_test[\"question_text\"].apply(lambda sent: len(str(sent).split()))\n\n\"\"\"number of characters in the question text\"\"\"\nquora_train[\"total_chars\"] = quora_train[\"question_text\"].apply(lambda sent: len(str(sent)))\nquora_test[\"total_chars\"] = quora_test[\"question_text\"].apply(lambda sent: len(str(sent)))\n\n\"\"\"total unique words in the question text\"\"\"\nquora_train[\"total_unique_words\"] = quora_train[\"question_text\"].apply(lambda sent: len(set(str(sent)\n                                                                                            .split())))\nquora_test[\"total_unique_words\"] = quora_test[\"question_text\"].apply(lambda sent: len(set(str(sent)\n                                                                                          .split())))\n\n\"\"\"word score in the question text\"\"\"\nquora_train['word_score'] = quora_train[\"total_unique_words\"]\/quora_train[\"total_words\"]\nquora_test['word_score'] = quora_test[\"total_unique_words\"]\/quora_test[\"total_words\"]\n\n\"\"\"total number of stopwords in the question text\"\"\"\n#gettings stopwords from nltk library\nStopwords = stopwords.words('english')\nquora_train[\"total_stopwords\"] = quora_train[\"question_text\"].apply(lambda sent: len([s for s in str(sent)\n                                                                                      .lower().split() if s \n                                                                                      in Stopwords]))\nquora_test[\"total_stopwords\"] = quora_test[\"question_text\"].apply(lambda sent: len([s for s in str(sent)\n                                                                                    .lower().split() if s \n                                                                                    in Stopwords]))\n\n\"\"\"total number of UPPERcase words in the question text\"\"\"\nquora_train[\"total_upper\"] = quora_train[\"question_text\"].apply(lambda sent: len([u for u in str(sent)\n                                                                                  .split() if u.isupper()]))\nquora_test[\"total_upper\"] = quora_test[\"question_text\"].apply(lambda sent: len([u for u in str(sent)\n                                                                                .split() if u.isupper()]))\n\n\"\"\"total number of lowercase words in the question text\"\"\"\nquora_train[\"total_lower\"] = quora_train[\"question_text\"].apply(lambda sent: len([l for l in str(sent)\n                                                                                  .split() if l.islower()]))\nquora_test[\"total_lower\"] = quora_test[\"question_text\"].apply(lambda sent: len([l for l in str(sent)\n                                                                                .split() if l.islower()]))\n\n\"\"\"total number of word title in the question text\"\"\"\nquora_train[\"total_word_title\"] = quora_train[\"question_text\"].apply(lambda sent: len([u for u in \n                                                                                       str(sent).split() \n                                                                                       if u.istitle()]))\nquora_test[\"total_word_title\"] = quora_test[\"question_text\"].apply(lambda sent: len([u for u in \n                                                                                     str(sent).split() \n                                                                                     if u.istitle()]))\n\n\n\"\"\"median word length of the question text\"\"\"\nquora_train[\"median_word_len\"] = quora_train[\"question_text\"].apply(lambda sent: np.median([len(w) \n                                                                                            for w in \n                                                                                            str(sent)\n                                                                                            .split()]))\nquora_test[\"median_word_len\"] = quora_test[\"question_text\"].apply(lambda sent: np.median([len(w) \n                                                                                          for w in \n                                                                                          str(sent)\n                                                                                          .split()]))\n\n\"\"\"Truncating Outliers\"\"\"\n#Total number of words\nquora_train['total_words'].loc[quora_train[\"total_words\"] > 60] = 60\nquora_test['total_words'].loc[quora_test[\"total_words\"] > 60] = 60\n#Total number of characters\nquora_train['total_chars'].loc[quora_train[\"total_chars\"] > 250] = 250\nquora_test['total_chars'].loc[quora_test[\"total_chars\"] > 250] = 250\n#Total number of unique words\nquora_train['total_unique_words'].loc[quora_train[\"total_unique_words\"] > 60] = 60\nquora_test['total_unique_words'].loc[quora_test[\"total_unique_words\"] > 60] = 60\n#Word Score\nquora_train['word_score'].loc[quora_train[\"word_score\"] < 0.6] = 0.6\nquora_test['word_score'].loc[quora_test[\"word_score\"] < 0.6] = 0.6\n#Total number of stopwords\nquora_train['total_stopwords'].loc[quora_train[\"total_stopwords\"] > 30] = 30\nquora_test['total_stopwords'].loc[quora_test[\"total_stopwords\"] > 30] = 30\n#Total number of uppercase word\nquora_train['total_upper'].loc[quora_train[\"total_upper\"] > 6] = 6\nquora_test['total_upper'].loc[quora_test[\"total_upper\"] > 6] = 6\n#Total number of lowercase word\nquora_train['total_lower'].loc[quora_train[\"total_lower\"] > 45] = 45\nquora_test['total_lower'].loc[quora_test[\"total_lower\"] > 45] = 45\n#Total number of word title\nquora_train['total_word_title'].loc[quora_train[\"total_word_title\"] > 15] = 15\nquora_test['total_word_title'].loc[quora_test[\"total_word_title\"] > 15] = 15\n#median_word_length\nquora_train['median_word_len'].loc[quora_train[\"median_word_len\"] > 10] = 10\nquora_test['median_word_len'].loc[quora_test[\"median_word_len\"] > 10] = 10\n\n#time calculation\nhours, rem = divmod(time.time()-start_time, 3600)\nprint(\"Time to process the whole function : {:02}.{:0>2}.{:02} minutes.\".format(int(hours),\n                                                                                int(divmod(rem, 60)[0]), \n                                                                                int(divmod(rem, 60)[1])))","7152054a":"\"\"\"splitting into train and test data\"\"\"\ny = quora_train['target']\nX = quora_train.drop(columns = ['target'])\n\nX_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20, stratify=y)\n#Printing the shape of train,cv and test dataset\nprint(\"The shape of train,cv & test dataset before conversion into vector\")\nprint(X_train.shape, y_train.shape)\nprint(X_cv.shape, y_cv.shape)\nprint(quora_test.shape)","ecbff268":"from sklearn.feature_extraction.text import TfidfVectorizer\nstart = time.time()\n\n#tokenization function\ndef tokenize(sentence): \n    tokens = re.sub('[^a-zA-Z0-9]',\" \",sentence).split()\n    return tokens\n\n#tfidf vectorizer\ntfidfvec = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_df=0.9, strip_accents='unicode', \n                           tokenizer=tokenize,use_idf=True, smooth_idf=True, sublinear_tf=True)\n\nXtrain_text = tfidfvec.fit_transform(X_train['question_text_preprocessed'].values.astype(str))\nXcv_text = tfidfvec.transform(X_cv['question_text_preprocessed'].values.astype(str))\nXtest_text = tfidfvec.transform(quora_test['question_text_preprocessed'].values.astype(str))\n\nprint(\"Shape of matrix after one hot encoding:\")\nprint(Xtrain_text.shape, y_train.shape)\nprint(Xcv_text.shape, y_cv.shape)\nprint(Xtest_text.shape)\n\nhours, rem = divmod(time.time()-start, 3600)\nprint(\"Time to process the whole function : {:02}.{:0>2}.{:02} minutes.\".format(int(hours),\n                                                                                int(divmod(rem, 60)[0]), \n                                                                                int(divmod(rem, 60)[1])))","fdda42da":"from sklearn.preprocessing import Normalizer\nnormalizer = Normalizer()\n#total_words\nnormalizer.fit(X_train['total_words'].values.reshape(-1,1))\nXtrain_total_words = normalizer.transform(X_train['total_words'].values.reshape(-1,1))\nXcv_total_words = normalizer.transform(X_cv['total_words'].values.reshape(-1,1))\nXtest_total_words = normalizer.transform(quora_test['total_words'].values.reshape(-1,1))\n\n#total_chars\nnormalizer.fit(X_train['total_chars'].values.reshape(-1,1))\nXtrain_total_chars = normalizer.transform(X_train['total_chars'].values.reshape(-1,1))\nXcv_total_chars = normalizer.transform(X_cv['total_chars'].values.reshape(-1,1))\nXtest_total_chars = normalizer.transform(quora_test['total_chars'].values.reshape(-1,1))\n\n#total_unique_words\nnormalizer.fit(X_train['total_unique_words'].values.reshape(-1,1))\nXtrain_total_unique_words = normalizer.transform(X_train['total_unique_words'].values.reshape(-1,1))\nXcv_total_unique_words = normalizer.transform(X_cv['total_unique_words'].values.reshape(-1,1))\nXtest_total_unique_words = normalizer.transform(quora_test['total_unique_words'].values.reshape(-1,1))\n\n#word_score\nnormalizer.fit(X_train['word_score'].values.reshape(-1,1))\nXtrain_word_score = normalizer.transform(X_train['word_score'].values.reshape(-1,1))\nXcv_word_score = normalizer.transform(X_cv['word_score'].values.reshape(-1,1))\nXtest_word_score = normalizer.transform(quora_test['word_score'].values.reshape(-1,1))\n\n#total_stopwords\nnormalizer.fit(X_train['total_stopwords'].values.reshape(-1,1))\nXtrain_total_stopwords = normalizer.transform(X_train['total_stopwords'].values.reshape(-1,1))\nXcv_total_stopwords = normalizer.transform(X_cv['total_stopwords'].values.reshape(-1,1))\nXtest_total_stopwords = normalizer.transform(quora_test['total_stopwords'].values.reshape(-1,1))\n\n#total_upper\nnormalizer.fit(X_train['total_upper'].values.reshape(-1,1))\nXtrain_total_upper = normalizer.transform(X_train['total_upper'].values.reshape(-1,1))\nXcv_total_upper = normalizer.transform(X_cv['total_upper'].values.reshape(-1,1))\nXtest_total_upper = normalizer.transform(quora_test['total_upper'].values.reshape(-1,1))\n\n#total_lower\nnormalizer.fit(X_train['total_lower'].values.reshape(-1,1))\nXtrain_total_lower = normalizer.transform(X_train['total_lower'].values.reshape(-1,1))\nXcv_total_lower = normalizer.transform(X_cv['total_lower'].values.reshape(-1,1))\nXtest_total_lower = normalizer.transform(quora_test['total_lower'].values.reshape(-1,1))\n\n#total_word_title\nnormalizer.fit(X_train['total_word_title'].values.reshape(-1,1))\nXtrain_total_word_title = normalizer.transform(X_train['total_word_title'].values.reshape(-1,1))\nXcv_total_word_title = normalizer.transform(X_cv['total_word_title'].values.reshape(-1,1))\nXtest_total_word_title = normalizer.transform(quora_test['total_word_title'].values.reshape(-1,1))\n\n#median_word_len\nnormalizer.fit(X_train['median_word_len'].values.reshape(-1,1))\nXtrain_median_word_len = normalizer.transform(X_train['median_word_len'].values.reshape(-1,1))\nXcv_median_word_len = normalizer.transform(X_cv['median_word_len'].values.reshape(-1,1))\nXtest_median_word_len = normalizer.transform(quora_test['median_word_len'].values.reshape(-1,1))","f1d6b7ba":"from scipy.sparse import hstack\n#stacking all features\nXtrain_quora = hstack((Xtrain_total_words, Xtrain_total_chars, Xtrain_total_unique_words, Xtrain_word_score,\n                    Xtrain_total_stopwords, Xtrain_total_upper, Xtrain_total_lower, Xtrain_total_word_title,\n                    Xtrain_median_word_len, Xtrain_text)).tocsr()\n\nXcv_quora = hstack((Xcv_total_words, Xcv_total_chars, Xcv_total_unique_words, Xcv_word_score,\n                    Xcv_total_stopwords, Xcv_total_upper, Xcv_total_lower, Xcv_total_word_title,\n                    Xcv_median_word_len, Xcv_text)).tocsr()\n\nXtest_quora =hstack((Xtest_total_words, Xtest_total_chars, Xtest_total_unique_words, Xtest_word_score,\n                    Xtest_total_stopwords, Xtest_total_upper, Xtest_total_lower, Xtest_total_word_title,\n                    Xtest_median_word_len, Xtest_text)).tocsr()","0c470498":"#https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nstart = time.time()\nimport lightgbm as lgbm\n#LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, \n#objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n#colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=- 1, silent=True, importance_type='split', **kwargs)\nlgb = lgbm.LGBMClassifier(boosting_type='gbdt', objective=\"binary\", metric=\"auc\", boost_from_average=False,max_depth=-1,\n                          learning_rate=0.3, max_bin=100, num_leaves=31, bagging_fraction = 0.8, feature_fraction = 0.8,\n                          scale_pos_weight = 1, num_threads = 32, verbosity = 0, n_jobs=- 1)\n\nvalues = [round(0.2 * x,2) for x in range(1, 6)]\nparam_grid = {'min_gain_to_split': values, 'lambda_l1':values, 'lambda_l2':values,\n              \"n_estimators\":[100,200,500,1000]}\nclf = RandomizedSearchCV(lgb, param_grid, scoring='f1',return_train_score=True, verbose=5, n_jobs=-1)\nclf.fit(Xtrain_quora, y_train)\n\nprint(\"Best cross-validation score: {:.2f}\".format(clf.best_score_))\nprint(\"Best parameters: \", clf.best_params_)\n\n\nlgb = lgbm.LGBMClassifier(boosting_type='gbdt', objective=\"binary\", metric=\"auc\", boost_from_average=False, \n                          max_depth = -1,learning_rate=0.3, max_bin=100, num_leaves=31, bagging_fraction = 0.8, \n                          feature_fraction = 0.8,min_gain_to_split = clf.best_params_['min_gain_to_split'],\n                          lambda_l1 = clf.best_params_['lambda_l1'],lambda_l2 = clf.best_params_['lambda_l2'],\n                          n_estimators = clf.best_params_['n_estimators'], scale_pos_weight = 1, num_threads = 32,\n                          verbosity = 0)\nlgb.fit(Xtrain_quora,y_train)\ny_pred=lgb.predict(Xcv_quora)\n\nprint(\"----LightGBM----\")\nprint(\"Overall f1 score:\",round((metrics.f1_score(y_cv,y_pred)),2))\nprint(\"Overall Precision:\",round((metrics.precision_score(y_cv,y_pred)),2))\nprint(\"Overall Recall:\",round((metrics.recall_score(y_cv,y_pred)),2))\nprint(\"Classification Report:\\n\",metrics.classification_report(y_cv,y_pred))\n\nfig, ax = plot_confusion_matrix(conf_mat=metrics.confusion_matrix(y_cv,y_pred), figsize=(5, 5))\npt.show()\n\n#predicting output\nytestPred = lgb.predict(Xtest_quora)\nytestPred = (ytestPred>0.25).astype(int)\nquora_test = pd.DataFrame({'qid':quora_test['qid'].values})\nquora_test['prediction'] = ytestPred\nprint(\"Quora Test Output:\\n\",quora_test['prediction'].value_counts())\n\nhours, rem = divmod(time.time()-start, 3600)\nprint(\"Time to process the whole function : {:02}.{:0>2}.{:02} minutes.\".format(int(hours),\n                                                                                int(divmod(rem, 60)[0]), \n                                                                                int(divmod(rem, 60)[1])))","b0eac645":"#Submitting values\nquora_test.to_csv('submission.csv', index=False)","eac2303c":"<h2>Vectorizing the text and numberical data<\/h2>","fac54bdf":"<h2>stacking features<\/h2>","dee66ecb":"<h2>Importing Libraries<\/h2>","f6fd5a72":"<h1>Quora Insincere Question Classification<\/h1>","796e0bed":"<h2>Train Test Split<\/h2>","c64f0fb7":"<h2>Feature Engineering<\/h2>","0e7af13e":"<h2>Loading Dataset<\/h2>","af6c8ede":"<h2>ML Models<\/h2>","bf7a629c":"<h2>Text Preprocessing<\/h2>","54b574ab":"<h3>LightGBM<\/h3>"}}