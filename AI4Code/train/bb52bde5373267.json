{"cell_type":{"f617afe1":"code","28a12823":"code","a4ea0ee2":"code","5e4addbb":"code","198028a2":"code","5bfcc1a8":"code","3623c4a5":"code","145e8ede":"code","6f26c6ef":"code","0a2a999a":"code","a3754337":"code","5b210f17":"code","f317bf2e":"code","7621180e":"code","53d888ef":"code","c1f01259":"code","cbc47a16":"code","eb18f5c4":"code","d896d84a":"code","bb559df0":"code","6746c01c":"code","8d4fb4c6":"code","5ecd2d2e":"code","31aa3e03":"code","9fbf52de":"code","aab6b69c":"code","ce025fb7":"code","27968a6f":"code","92ece8e8":"code","11948e66":"code","41ae548f":"code","37444efd":"code","036efd7a":"code","661678d1":"code","2d2ac140":"code","ffe638b1":"code","e2ab2ba6":"code","50b39561":"code","a1a7d071":"code","11dc37e8":"code","1f837b26":"code","10886d5b":"code","8af5f0e4":"code","f3de7bc4":"code","69e393ee":"code","b92f549d":"code","b8dcc181":"code","110439cd":"code","9e76569d":"code","e0166a47":"code","860efc23":"markdown","c773e288":"markdown","fc82b4d7":"markdown","1b19291a":"markdown","5a39d618":"markdown","19916f78":"markdown","79b2d3fa":"markdown","7dcaf444":"markdown","35002695":"markdown","68543c72":"markdown","64fddbac":"markdown","5d7aeb3b":"markdown","034e23bd":"markdown","d6bd3284":"markdown","944acc05":"markdown","bbd3c8ce":"markdown","bcee366f":"markdown","fc33c9bc":"markdown","57439bbd":"markdown","5a41a98a":"markdown","04ba76f0":"markdown","1b471229":"markdown","2ab7670b":"markdown","22012297":"markdown","0e5928f4":"markdown","08e0f0d3":"markdown","eaeedf79":"markdown","751727c4":"markdown","122d9753":"markdown","409baac3":"markdown","ff93eaf6":"markdown","63a73d02":"markdown","aad690a9":"markdown","fc79b0b0":"markdown","8354a66f":"markdown","2554b8b7":"markdown"},"source":{"f617afe1":"# Basic libraries\nimport sys\nimport os\nimport gc\nimport time              \nimport pickle                \nimport numpy as np\nimport pandas as pd\nfrom time import time\n\n# Data visualization & printing libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nfrom PIL import Image\nfrom IPython.display import HTML, display\nimport tabulate\n\n# Deep learning libraries\nimport tensorflow as tf\nfrom keras import Model\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization\nfrom keras.layers import Flatten, Activation, Input, AveragePooling2D, Lambda, add\nfrom keras.layers.merge import concatenate\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\nfrom keras.utils import plot_model\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Utility functions\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.utils import shuffle\n\n# Setting random seeds\nnp.random.seed(7)\ntf.random.set_seed(7)\n\n# Magic functions\n%matplotlib inline","28a12823":"# Checking that tesorflow is using the GPU\n\nfrom tensorflow.python.client import device_lib \nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')),'\\n')\nprint(device_lib.list_local_devices(),'\\n')\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))","a4ea0ee2":"# Getting image paths and creating pandas dataframe for each dataset (train\/val\/test)\n\ntrain_p = [('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA'))]\ntrain_n = [('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL'))]\nval_p = [('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA'))]\nval_n = [('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL'))]\ntest_p = [('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/PNEUMONIA'))]\ntest_n = [('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL'))]\n\ntrain_data = pd.DataFrame(train_p+train_n, columns=['image_path', 'label'],index=None)\nval_data = pd.DataFrame(val_p+val_n, columns=['image_path', 'label'],index=None)\ntest_data = pd.DataFrame(test_p+test_n, columns=['image_path', 'label'],index=None)","5e4addbb":"print(\"The head of the training dataframe:\")\ntrain_data.head()","198028a2":"# Count plot of the class representations.\n\nplt.figure(figsize=(15,4))\nplt.subplot(1,3,1)\nsb.countplot(data= train_data, x='label')\nplt.title('Number of cases in training data', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(train_data.label.unique())), ['Normal', 'Pneumonia'])\n\nplt.subplot(1,3,2)\nsb.countplot(data= val_data, x='label')\nplt.title('Number of cases in validation data', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(val_data.label.unique())), ['Normal', 'Pneumonia'])\n\nplt.subplot(1,3,3)\nsb.countplot(data= test_data, x='label')\nplt.title('Number of cases in test data', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(test_data.label.unique())), ['Normal', 'Pneumonia']);","5bfcc1a8":"# Visualizing a sample of 4 training images.\n\nrandom_imgs = np.random.randint(1,train_data.shape[0],4)\nplt.figure(figsize=(15,4))\nplt.suptitle('Sample of 4 training images with labels', y=0.95,fontsize=15)\nfor i, img in enumerate(train_data.iloc[random_imgs,0]):\n    image_temp = Image.open(img)                                           # Conversion to Black & White\n    plt.subplot(1,4,i+1)\n    plt.imshow(image_temp, cmap='gray')\n    if train_data.iloc[random_imgs[i],1] == 0:\n        plt.xlabel('Normal')\n    else:\n        plt.xlabel('Pneumonia')","3623c4a5":"# Creating a dataframe for image properties. \n\nl=[]\npath_list=train_p+train_n+val_p+val_n+test_p+test_n\nfor i in path_list:\n    with Image.open(i[0]) as image_temp:\n        l.append(list(np.asarray(image_temp).shape))\nimgs_size_df = pd.DataFrame(l, columns=['height', 'width','channels'],index=None)\nimgs_size_df.channels.fillna(1, inplace=True)","145e8ede":"# Example of B&W image stored in RGB format\n\nimg = Image.open(path_list[imgs_size_df.query('channels==3').index[0]][0])\nplt.imshow(img)\nplt.title('Example of a Grayscale image stored as RGB image')\nprint('Image size:', np.asarray(img).shape)","6f26c6ef":"plt.figure(figsize=(8,5))\nsb.scatterplot(data=imgs_size_df, x='height', y='width', hue='channels', palette=\"deep\")\nplt.title('Distribution of the image sizes in the training dataset');","0a2a999a":"imgs_size_df.fillna(value=int(1), inplace=True)\nimgs_size_df.query('channels==3').head()\nprint('There are', imgs_size_df.query('channels==3').shape[0], 'B&W images stored in RGB format.')","a3754337":"# Resizing all the images: New size 256x256\n\ndirect_lists=[train_p,train_n,val_p,val_n,test_p,test_n]\nsave_paths=['\/kaggle\/working\/modified\/train\/PNEUMONIA\/',\n            '\/kaggle\/working\/modified\/train\/NORMAL\/',\n            '\/kaggle\/working\/modified\/val\/PNEUMONIA\/',\n            '\/kaggle\/working\/modified\/val\/NORMAL\/',\n            '\/kaggle\/working\/modified\/test\/PNEUMONIA\/',\n            '\/kaggle\/working\/modified\/test\/NORMAL\/']\n\nfor path in save_paths:\n    os.makedirs(path)","5b210f17":"save_prefix=['train_pneumonial','train_normal','val_pneumonial','val_normal','test_pneumonial','test_normal']\nfor direct,save_path,prefix in zip(direct_lists,save_paths, save_prefix):\n    i=1\n    for path in direct:\n        with Image.open(path[0]) as img_temp:\n            resized_gray = img_temp.resize((256,256)).convert('L')\n            resized_gray.save(save_path+prefix+'_'+str(i)+'.jpg')\n        i+=1","f317bf2e":"# Modified image paths\n\ntrain_p = [('\/kaggle\/working\/modified\/train\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/train\/PNEUMONIA\/'))]\ntrain_n = [('\/kaggle\/working\/modified\/train\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/train\/NORMAL\/'))]\nval_p = [('\/kaggle\/working\/modified\/val\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/val\/PNEUMONIA\/'))]\nval_n = [('\/kaggle\/working\/modified\/val\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/val\/NORMAL\/'))]\ntest_p = [('\/kaggle\/working\/modified\/test\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/test\/PNEUMONIA\/'))]\ntest_n = [('\/kaggle\/working\/modified\/test\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/test\/NORMAL\/'))]\n\ntrain_data = pd.DataFrame(train_p+train_n, columns=['image', 'label'],index=None)\nval_data = pd.DataFrame(val_p+val_n, columns=['image', 'label'],index=None)\ntest_data = pd.DataFrame(test_p+test_n, columns=['image', 'label'],index=None)","7621180e":"l=[]\npaths = train_p+train_n+val_p+val_n+test_p+test_n\nfor i in paths:\n    with Image.open(i[0]) as image_temp:\n        l.append(list(np.asarray(image_temp).shape))\nimgs_size_df = pd.DataFrame(l, columns=['height', 'width'],index=None)","53d888ef":"print('All the images are now of size',imgs_size_df.height.value_counts().index[0],\n      'x',imgs_size_df.width.value_counts().index[0],'x 1')","c1f01259":"# Creating an image generator\n\nImageGen = ImageDataGenerator(\n    rotation_range=10,\n    brightness_range=[0.9,1.1],\n    zoom_range = 0.15,\n    width_shift_range=0.1, \n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False,\n    fill_mode=\"nearest\"\n)","cbc47a16":"# Function that generates and save images by batch and up to a certain threshold.\n\ndef GenerateImages(data, save_dir, b_size, max_size):\n    img_array_list=[]\n    for path in data:\n        with Image.open(path[0]) as image_temp:\n            img_array = img_to_array(image_temp)\n            img_array = img_array.reshape((1,)+img_array.shape)\n            img_array_list.append(img_array)\n    img_array_list = np.concatenate(img_array_list, axis=0)\n    i=0\n    for batch in ImageGen.flow(img_array_list, batch_size=b_size,\n                            save_to_dir='\/kaggle\/working\/modified\/'+save_dir,\n                            save_prefix='Aug', save_format='JPEG'):\n        i+=1\n        if i>(max_size-len(data))\/\/b_size:\n            break","eb18f5c4":"os.makedirs('\/kaggle\/working\/modified\/train\/AUGMENTED\/PNEUMONIA')\nos.makedirs('\/kaggle\/working\/modified\/train\/AUGMENTED\/NORMAL')\nos.makedirs('\/kaggle\/working\/modified\/val\/AUGMENTED\/PNEUMONIA')\nos.makedirs('\/kaggle\/working\/modified\/val\/AUGMENTED\/NORMAL')","d896d84a":"# Generating train and validation data\n\nGenerateImages(train_p, 'train\/AUGMENTED\/PNEUMONIA', 16, 8000)\nGenerateImages(train_n, 'train\/AUGMENTED\/NORMAL', 16, 8000)\nGenerateImages(val_p, 'val\/AUGMENTED\/PNEUMONIA', 8, 400)\nGenerateImages(val_n, 'val\/AUGMENTED\/NORMAL', 8, 400)","bb559df0":"# Getting the augmented data paths and adding them to the old ones\n\ntrain_p_aug = [('\/kaggle\/working\/modified\/train\/AUGMENTED\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/train\/AUGMENTED\/PNEUMONIA'))]\ntrain_p = train_p + train_p_aug\ntrain_n_aug = [('\/kaggle\/working\/modified\/train\/AUGMENTED\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/train\/AUGMENTED\/NORMAL'))]\ntrain_n = train_n + train_n_aug\n\nval_p_aug = [('\/kaggle\/working\/modified\/val\/AUGMENTED\/PNEUMONIA\/' + filename,1) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/val\/AUGMENTED\/PNEUMONIA'))]\nval_p = val_p + val_p_aug\nval_n_aug = [('\/kaggle\/working\/modified\/val\/AUGMENTED\/NORMAL\/' + filename,0) for count, filename in enumerate(os.listdir('\/kaggle\/working\/modified\/val\/AUGMENTED\/NORMAL'))]\nval_n = val_n + val_n_aug","6746c01c":"train_data = pd.DataFrame(train_p+train_n, columns=['image', 'label'],index=None)\nval_data = pd.DataFrame(val_p+val_n, columns=['image', 'label'],index=None)\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsb.countplot(data= train_data, x='label')\nplt.title('Number of cases in the training set', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(train_data.label.unique())), ['Normal', 'Pneumonia'])\n\nplt.subplot(1,2,2)\nsb.countplot(data= val_data, x='label')\nplt.title('Number of cases in the validation set', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(train_data.label.unique())), ['Normal', 'Pneumonia']);","8d4fb4c6":"Train_data=[]\nTrain_label=[]\nfor path in train_n+train_p:\n    with Image.open(path[0]) as image_temp:\n        img_array = img_to_array(image_temp)\n        img_array = (img_array-img_array.mean())\/img_array.std()\n        img_array = img_array.reshape((1,)+img_array.shape)\n        Train_data.append(img_array)\n    Train_label.append(path[1])\nTrain_data = np.concatenate(Train_data, axis=0)\nTrain_label = np.array(Train_label)\n\nVal_data=[]\nVal_label=[]\nfor path in val_n+val_p:\n    with Image.open(path[0]) as image_temp:\n        img_array = img_to_array(image_temp)\n        img_array = (img_array-img_array.mean())\/img_array.std()\n        img_array = img_array.reshape((1,)+img_array.shape)\n        Val_data.append(img_array)\n    Val_label.append(path[1])\nVal_data = np.concatenate(Val_data, axis=0)\nVal_label = np.array(Val_label)\n\nTest_data=[]\nTest_label=[]\nfor path in test_n+test_p:\n    with Image.open(path[0]) as image_temp:\n        img_array = img_to_array(image_temp)\n        img_array = (img_array-img_array.mean())\/img_array.std()\n        img_array = img_array.reshape((1,)+img_array.shape)\n        Test_data.append(img_array)\n    Test_label.append(path[1])\nTest_data = np.concatenate(Test_data, axis=0)\nTest_label = np.array(Test_label)","5ecd2d2e":"Train_data, Train_label = shuffle(Train_data, Train_label, random_state=7)\nVal_data, Val_label = shuffle(Val_data, Val_label, random_state=7)\nTest_data, Test_label = shuffle(Test_data, Test_label, random_state=7)","31aa3e03":"VGG_model = Sequential(name='VGG_model')\nVGG_model.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal',\n                       input_shape=(256,256,1) ))\nVGG_model.add(MaxPooling2D((2,2), padding='same'))\n\nVGG_model.add(Conv2D(32,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(Conv2D(32,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(BatchNormalization())\nVGG_model.add(MaxPooling2D((2,2), strides=2 ,padding='same'))\n\nVGG_model.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(BatchNormalization())\nVGG_model.add(MaxPooling2D((2,2), strides=2 ,padding='same'))\n\nVGG_model.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' ))\nVGG_model.add(BatchNormalization())\nVGG_model.add(MaxPooling2D((2,2), strides=2 ,padding='same'))\n\nVGG_model.add(Flatten())\n\nVGG_model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\nVGG_model.add(Dropout(0.15))\nVGG_model.add(Dense(1, activation=\"sigmoid\"))\n\nVGG_model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nVGG_model.summary()","9fbf52de":"plot_model(VGG_model, show_shapes=True, to_file='VGG_model.png')","aab6b69c":"callback = ReduceLROnPlateau(monitor='val_loss', patience = 3, cooldown=0, verbose=1, factor=0.6, min_lr=0.000001)\nstart_vgg = time()\nVGG_results = VGG_model.fit(\n    x=Train_data,\n    y=Train_label,\n    batch_size=16,\n    validation_data=(Val_data,Val_label),\n    class_weight={0:12, 1:0.5},\n    epochs=20, callbacks=[callback])\nend_vgg = time()\nvgg_train_dur = end_vgg - start_vgg","ce025fb7":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(VGG_results.history['accuracy'])\nplt.plot(VGG_results.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Val'])\nplt.title('Accuracy evolution')\n\nplt.subplot(122)\nplt.plot(VGG_results.history['loss'])\nplt.plot(VGG_results.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'])\nplt.title('Loss evolution');","27968a6f":"print('Classification report:')\nVGG_pred = VGG_model.predict(Test_data)\nprint(classification_report(VGG_pred.round(),Test_label))","92ece8e8":"VGG_train_loss = VGG_results.history['loss']\nVGG_val_loss = VGG_results.history['val_loss']\nVGG_train_acc = VGG_results.history['accuracy']\nVGG_val_acc = VGG_results.history['val_accuracy']\n\nVGG_rep = classification_report(VGG_pred.round(),Test_label, output_dict=True)","11948e66":"del VGG_model\ndel VGG_results\ngc.collect()","41ae548f":"# Creating the inception module\n\ndef inception_module(layer_input, fb1, fb2_in, fb2_out, fb3_in, fb3_out, fb4_out):\n\n    conv1 = Conv2D(fb1, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_input)\n\n    conv3 = Conv2D(fb2_in, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_input)\n    conv3 = Conv2D(fb2_out, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(conv3)\n\n    conv5 = Conv2D(fb3_in, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_input)\n    conv5 = Conv2D(fb3_out, (5,5), padding='same', activation='relu', kernel_initializer='he_normal')(conv5)\n\n    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_input)\n    pool = Conv2D(fb4_out, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(pool)\n\n    layer_output = concatenate([conv1, conv3, conv5, pool], axis=-1)\n    return layer_output","37444efd":"visible = Input(shape=(256,256,1))\n\nlayer = Conv2D(32, (7,7), strides=2, padding='same', activation='relu', kernel_initializer='he_normal' )(visible)\nlayer = MaxPooling2D((3,3), strides=(2,2))(layer)\nlayer = Lambda(tf.nn.local_response_normalization)(layer)\nlayer = Conv2D(32, (3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' )(layer)\nlayer = MaxPooling2D((3,3), strides=(2,2))(layer)\nlayer = inception_module(layer, 32, 64, 64, 16, 32, 32)\nlayer = inception_module(layer, 32, 64, 64, 16, 32, 32)\nlayer = MaxPooling2D((3,3), strides=(2,2), padding='same')(layer)\nlayer = Conv2D(32, (3,3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal' )(layer)\nlayer = AveragePooling2D((3,3), padding='valid')(layer)\nlayer = Flatten()(layer)\nlayer = Dense(4, activation='relu', kernel_initializer='he_normal')(layer)\nlayer = Dense(1, activation='sigmoid')(layer)\n\nInception_model = Model(inputs=visible, outputs=layer, name='Inception_model')\n\nInception_model.summary()\n","036efd7a":"plot_model(Inception_model, show_shapes=True, to_file='inception_module.png')","661678d1":"Inception_model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])\ncallback = ReduceLROnPlateau(monitor='val_loss', patience = 3, cooldown=0, verbose=1, factor=0.6, min_lr=0.000001)\nstart_inception = time()\nresults_inception = Inception_model.fit(\n    x=Train_data,\n    y=Train_label,\n    batch_size=16,\n    validation_data=(Val_data,Val_label),\n    class_weight={0:12, 1:0.5},\n    epochs=15, callbacks=[callback])\nend_inception = time()\ninception_train_dur = end_inception - start_inception","2d2ac140":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(results_inception.history['accuracy'])\nplt.plot(results_inception.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Val'])\nplt.title('Accuracy evolution')\n\nplt.subplot(122)\nplt.plot(results_inception.history['loss'])\nplt.plot(results_inception.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'])\nplt.title('Loss evolution');","ffe638b1":"print('Classification report:')\nInception_pred = Inception_model.predict(Test_data)\nprint(classification_report(Inception_pred.round(),Test_label))","e2ab2ba6":"Inception_train_loss = results_inception.history['loss']\nInception_val_loss = results_inception.history['val_loss']\nInception_train_acc = results_inception.history['accuracy']\nInception_val_acc = results_inception.history['val_accuracy']\n\nInception_rep = classification_report(Inception_pred.round(),Test_label, output_dict=True)","50b39561":"del Inception_model\ndel results_inception\ngc.collect()","a1a7d071":"# function that will use the identity if possible, otherwise a projection of the number of filters\n# in the input does not match the n_filters argument.\n\ndef residual_module(layer_in, n_filters):\n    \n    merge_input = layer_in\n    if layer_in.shape[-1] != n_filters[2]:\n        merge_input = Conv2D(n_filters[2], (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n        \n    conv1 = Conv2D(n_filters[0], (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    conv2 = Conv2D(n_filters[1], (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(conv1)\n    conv3 = Conv2D(n_filters[2], (1,1), padding='same', activation='linear', kernel_initializer='he_normal')(conv2)\n    \n    layer_out = add([conv3, merge_input])\n    layer_out = Activation('relu')(layer_out)\n    \n    return layer_out","11dc37e8":"visible = Input(shape=(256,256,1))\n\nlayer = Conv2D(32, (7,7), strides=(2,2) ,padding='same', activation='relu', kernel_initializer='he_normal')(visible)\nlayer = MaxPooling2D((2,2), strides=2 ,padding='same')(layer)\nlayer = residual_module(layer, [8,8,16])\nlayer = residual_module(layer, [8,8,16])\nlayer = BatchNormalization()(layer)\nlayer = residual_module(layer, [8,8,16])\nlayer = residual_module(layer, [8,8,16])\nlayer = BatchNormalization()(layer)\nlayer = residual_module(layer, [16,16,32])\nlayer = residual_module(layer, [16,16,32])\nlayer = BatchNormalization()(layer)\nlayer = residual_module(layer, [32,32,64])\nlayer = residual_module(layer, [32,32,64])\nlayer = AveragePooling2D((3,3), padding='valid')(layer)\nlayer = Flatten()(layer)\nlayer = Dense(16, activation='relu')(layer)\nlayer = Dropout(0.1)(layer)\nlayer = Dense(1, activation='sigmoid')(layer)\n\nResNet_model = Model(inputs=visible, outputs=layer, name='ResNet_model')\n\nResNet_model.summary()","1f837b26":"plot_model(ResNet_model, show_shapes=True, to_file='ResNet_module.png')","10886d5b":"ResNet_model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])\ncallback = ReduceLROnPlateau(monitor='val_loss', patience = 2, cooldown=0, verbose=1, factor=0.6, min_lr=0.000001)\nstart_ResNet = time()\nresults_ResNet = ResNet_model.fit(\n    x=Train_data,\n    y=Train_label,\n    batch_size=16,\n    validation_data=(Val_data,Val_label),\n    class_weight={0:20, 1:0.5},\n    epochs=15, callbacks=[callback])\nend_ResNet = time()\nResNet_train_dur = end_ResNet - start_ResNet","8af5f0e4":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(results_ResNet.history['accuracy'])\nplt.plot(results_ResNet.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Val'])\nplt.title('Accuracy evolution')\n\nplt.subplot(122)\nplt.plot(results_ResNet.history['loss'])\nplt.plot(results_ResNet.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'])\nplt.title('Loss evolution');","f3de7bc4":"print('Classification report:')\nResNet_pred = ResNet_model.predict(Test_data)\nprint(classification_report(ResNet_pred.round(),Test_label))","69e393ee":"ResNet_train_loss = results_ResNet.history['loss']\nResNet_val_loss = results_ResNet.history['val_loss']\nResNet_train_acc = results_ResNet.history['accuracy']\nResNet_val_acc = results_ResNet.history['val_accuracy']\n\nResNet_rep = classification_report(ResNet_pred.round(),Test_label, output_dict=True)","b92f549d":"del ResNet_model\ndel results_ResNet\ngc.collect()","b8dcc181":"plt.figure(figsize=(13,5))\n\nplt.subplot(121)\nplt.plot(VGG_val_acc)\nplt.plot(Inception_val_acc)\nplt.plot(ResNet_val_acc)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['VGG','Inception','ResNet'])\nplt.title('Validation accuracy evolution')\nplt.grid(True)\n\n\nplt.subplot(122)\nplt.plot(VGG_val_loss)\nplt.plot(Inception_val_loss)\nplt.plot(ResNet_val_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['VGG','Inception','ResNet'])\nplt.title('Validation loss evolution')\nplt.grid(True);","110439cd":"# Function to plot the confusion matrix of the trained model.\n\ndef plot_cm(mat,y_ture):\n    df_cm = pd.DataFrame(mat, columns=np.unique(y_ture), index = np.unique(y_ture))\n    df_cm.index.name = 'True Label'\n    df_cm.columns.name = 'Predicted Label'\n    sb.heatmap(df_cm, cmap=\"Blues\", cbar=False, annot=True,annot_kws={\"size\": 10})\n    plt.yticks(fontsize=10)\n    plt.xticks(fontsize=10)","9e76569d":"plt.figure(figsize=(11,4))\nplt.subplot(131)\nplot_cm(confusion_matrix(VGG_pred.round(),Test_label, normalize='true'), Test_label)\nplt.title('VGG model confusion matrix')\nplt.subplot(132)\nplot_cm(confusion_matrix(Inception_pred.round(),Test_label, normalize='true'), Test_label)\nplt.title('Inception model confusion matrix')\nplt.subplot(133)\nplot_cm(confusion_matrix(ResNet_pred.round(),Test_label, normalize='true'), Test_label)\nplt.title('ResNet model confusion matrix')\nplt.tight_layout()","e0166a47":"vgg_acc = round(VGG_rep['accuracy'],2)\ninception_acc = round(Inception_rep['accuracy'],2)\nresnet_acc = round(ResNet_rep['accuracy'],2)\n\nvgg_normal_f1 = round(VGG_rep['0.0']['f1-score'],2)\ninception_normal_f1 = round(Inception_rep['0.0']['f1-score'],2)\nresnet_normal_f1 = round(ResNet_rep['0.0']['f1-score'],2)\n\nvgg_pneu_f1 = round(VGG_rep['1.0']['f1-score'],2)\ninception_pneu_f1 = round(Inception_rep['1.0']['f1-score'],2)\nresnet_pneu_f1 = round(ResNet_rep['1.0']['f1-score'],2)\n\n\nprint('Comparaison of model results:')\n\nresult_table=[['Model','Training epochs' ,'Training duration (min)', 'Test accuracy', '(Normal) F1-score', '(Pneumonia) F1-score'],\n             ['VGG model', 20, round(vgg_train_dur\/60), vgg_acc, vgg_normal_f1, vgg_pneu_f1],\n             ['Inception model', 15,round(inception_train_dur\/60), inception_acc, inception_normal_f1, inception_normal_f1],\n             ['ResNet model ', 15,round(ResNet_train_dur\/60), resnet_acc, resnet_normal_f1, resnet_pneu_f1]]\n\ndisplay(HTML(tabulate.tabulate(result_table, colalign=(\"center\",)*6, tablefmt='html')))","860efc23":"# Importing libraries","c773e288":"The images are stored in three main directories (train\/val\/test), and each directory contains 2 sub-directories, one for each class (normal\/pneumonia).\n\nFirst, we're going to create three datasets; train_data, val_data, and text_data. Each dataframe has two columns; image_path and label.","fc82b4d7":"# ResNet\n\nThe ResNet was introduced in 2015 by Kaiming He et al. in the [paper](https:\/\/arxiv.org\/abs\/1512.03385) entitled (Deep Residual Learning for Image Recognition). ResNet was the winner of the ILSVRC 2015 classification task. The main power of the ResNet is that it can go deeper than other architectures. It's reported in the previously mentioned paper that it goes 8x deeper than VGG nets while still maintaining lower complexity. This is basically due to the residual blocks which permits the ResNet to learn easilly learn the identity function, which permits the network to \"hop over\" or skip layers.\n\nMore details about the architecture can be found [here](https:\/\/towardsdatascience.com\/introduction-to-resnets-c0a830a288a4).\n\nThe implementation of this architecture is also simplified in the following model (only 8 residual blocks), in order to maintain a reasonable complexity that can be handled using a local machine.","1b19291a":"Balance issues:\n\n    1- Very inbalanced representation of classes in the training data.\n    2- Insufficient number of validation images.\n    3- Slight imbalance between the two classes classes.","5a39d618":"## Converting to arrays\n\nThe final step of pre-processing is converting the images into numpy arrays, then normalize and shuffle them so that they're ready to use in keras sequential models.","19916f78":" Since, it's not clear enough that the majority of the images have only 1 channel, let's calculate how many images in the whole set of data have in fact 3 channels.","79b2d3fa":"I've chosen to save the generated images then use them so that I can make sure that the three CNN models will use the exactly same data.","7dcaf444":"**Normalized confusion matrix of each model**","35002695":"Data augmentation is a well-known and a powerful way to address the lack of data problem. In our case, we can use data augmentation to restore the class balance in the training dataset, and to enlarge the validation dataset.\n\nThe process of iamge data augmentation is basically creating copies of the images we already have and applying some sorts of modifications to them so that they would look as if they're new images that have the same labels as the old ones.\nExample of the modifications that we could apply:\n\n    - Zooming\n    - Rotating\n    - Fliping\n    - Modifying brightness\n    .\n    .\nMore details about data augmentation can be found [here](https:\/\/nanonets.com\/blog\/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2\/).","68543c72":"**Summary table of the three trained models results**","64fddbac":"## Resizing\nTo begin the pre-processing step, we're going to resize all the images to a smaller size that would be acceptable to train on a local machine.\n\nAfter some attempts, I found that 256x256x1 is a good size to use. Here's a [Quora answer](https:\/\/www.quora.com\/To-what-resolution-should-I-resize-my-images-to-use-as-training-dataset-for-deep-learning) that I found helpful for me to chose the image size.\n\nThe new resized imaged will be stored in new directories, so that we can have the older version of our data.","5d7aeb3b":"# Pneumonia detection using different CNN architectures\n\n## What is pneumonia exactly?\n\n![Pneumonia-image.jpg](attachment:Pneumonia-image.jpg)\n\n\n**Pneumonia** is an inflammatory condition of the lung primarily affecting the small air sacs known as alveoli. Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. The severity of the condition is variable. Pneumonia is usually caused by infection with viruses or bacteria, and less commonly by other microorganisms. Identifying the responsible pathogen can be difficult. Diagnosis is often based on symptoms and physical examination. **Chest X-rays**, blood tests, and culture of the sputum may help confirm the diagnosis. The disease may be classified by where it was acquired, such as community- or hospital-acquired or healthcare-associated pneumonia.\nEach year, pneumonia affects about 450 million people globally (7% of the population) and results in about 4 million deaths. With the introduction of antibiotics and vaccines in the 20th century, survival has greatly improved. For more details refer to the [pneumonia Wikipedia page](https:\/\/en.wikipedia.org\/wiki\/Pneumonia)\n\n## What are we going to do?\nIn this notebook, we're going to use some of the popular CNN architectures to detect pneumonia on chest X-ray image [dataset](https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia) provided by [Paul Mooney](https:\/\/www.kaggle.com\/paultimothymooney) on kaggle.\nWe have chosen 3 architectures for this problem:\n - VGG\n - Inception\n - ResNet\n \nIt's important to note that a modified smaller version of each architecture is used.\n## Dataset brief description\n\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal).\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n\n### Acknowledgements\n\nData: https:\/\/data.mendeley.com\/datasets\/rscbjbr9sj\/2\n\nLicense: CC BY 4.0\n\nCitation: http:\/\/www.cell.com\/cell\/fulltext\/S0092-8674(18)30154-5","034e23bd":"Now that we know the images we have comes in different shapes, let's inspect their format.","d6bd3284":"### Results","944acc05":"**Freeing up memory to be able to train next model**","bbd3c8ce":"# Results\n\nIn this final section we're going to compare the results of the 3 trained models. \n\n**Evolution of the validation Loss\/Accuracy of the three trained models:**","bcee366f":"### Results","fc33c9bc":"Now, let's check the dataset balance. To do that we're going to plot the count of images for each class in each dataframe.","57439bbd":"Second, let's check the size and format of the images.","5a41a98a":"## Checking that tensorflow is using GPU\n\nTo run keras models on GPU locally. you need to check few things:\n\n1. your system has GPU (Nvidia. As AMD doesn't work yet)\n2. You have installed the GPU version of tensorflow\n3. You have installed CUDA [installation instructions](https:\/\/www.tensorflow.org\/install\/install_linux)\n4. Verify that tensorflow is running with GPU [check if GPU is working](https:\/\/stackoverflow.com\/questions\/38009682\/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell)","04ba76f0":"In our case we're chosing to:\n\n - Rotate the images with 10\u00b0\n - Modifty the brightness by 10%\n - Zoom in\/out by 15%\n - Apply height\/width shift within a range of 10%\n \nWe're chosing to not apply any sort of flip and that is because in reality the X-ray images would not come in flipped form (mirrored).","1b471229":"There are some images stored in a RGB format where in fact they are black and white, this could be done if the three channels are identical, however maintaining this representation wouldn't be efficient since we're using more memory than necessary. Also, the reason we need the convert RGB images to B&W in this task is because the input data to a CNN need to have a consistent shape, in our case 1 channel instead of 3.\n\nBelow we can see the representation of the trainig set images in a scatter plot in function of their width, hight and number of channel.","2ab7670b":"# Inception architecture","22012297":"The Inception-v1, which was named also GoogLeNet, was the winners of the (ImageNet Large Scale Visual Recognition Competition) of 2014. \n\nIn their [paper](https:\/\/arxiv.org\/abs\/1409.4842), Christian Szegedy et al. introduced the first version of the Inception architecture, and mentioned that the chosen name \"Inception\" was motivated by the famous internet meme of the Inception movie captionned \"We need to go deeper\".\n\nBriefly, the Inception architecture mainly differ from the previous architecture such as the VGG, which was the runner-up in the ILSVRC 2014 competition, ZFNet, and AlexNet in the geometrical relation between layers. The inception architecture introducesd the inception module which uses multiple convolutional layers parallelly and then concatenate their outputs.\nOne main difference worth mentionning is the use of a global average pooling. More details about the Inception architecture can be found [here](https:\/\/medium.com\/coinmonks\/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7).\n\nThe following model is a simpler and non-deep version of the inception network, using only 2 inception modules.","0e5928f4":"**Freeing up memory to be able to train next model**","08e0f0d3":"Checking that all the images are resized correctly.","eaeedf79":"In our VGG model we're going to use a He Normal initialization for the convolutional layes. We added dropout to control the overfitting of the model.\nThe total number of layers are 9, where the last two layer are fully connected and the output layer contains a single sigmoid unit. \n\nThe full architecture can be visualized in the following summary table or the image below it.","751727c4":"# Brief Exploratory Data Analysis","122d9753":"**Note:** Inception architecture uses local response normalisation instead of batch normalization after the first convolutional layer. More details about the difference between the two types of normalizations can be found [here](https:\/\/towardsdatascience.com\/difference-between-local-response-normalization-and-batch-normalization-272308c034ac).\n\nThis can be achieved using the Keras lambda layer ad the tensorflow local_responce_normalization function.","409baac3":"After some tuning of the hyper parameters, we've found that the follwoing ones gave the best results.","ff93eaf6":"## Data Augmentation","63a73d02":"# Data Pre-processing","aad690a9":"# Conclusion\n\nIn this pneumonia detection task, we've used three CNN architectures and compared their results. With very similar hyper-parameters, the inception architecture gave the best results on the test set. It's possible to increase the performance of these models by further optimizing the hyper-parameters and using more sophisticated callbacks, such as EarlyStopping.  ","fc79b0b0":"Now, let's check that the images are generated correctly and that we have balanced the classes in the training dataset.","8354a66f":"# CNN Models\n\nWe're going to build all the CNN models in this section using the Keras API.\n\n# VGG architecture\n\nThe VGG convolutional neural network is one of the most famous CNNs. It was introduced by Simonyan and Zisserman in their 2014 paper, [Very Deep Convolutional Networks for Large Scale Image Recognition](https:\/\/arxiv.org\/abs\/1409.1556). It has a simple architecture mainly characterized by:\n\n - The use of 3x3 filters only.\n - The increasing of number of filters as the network goes deeper (64-128-256-512).\n - The use of the ReLU activation function.\n - The use of MaxPooling to reduce the volume size.\n - The convolutional layers use a 'same' padding and strides of 1.\n \nAlthough, the most famous VGG networks are the VGG-16 and the VGG-19 which contatins respectively 16 and 19 layers. We're going to implement a smaller version considering the available computational power.","2554b8b7":"### Results"}}