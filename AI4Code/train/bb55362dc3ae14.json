{"cell_type":{"8253dc51":"code","46ce13a8":"code","71961376":"code","db4ccac0":"code","519c11ff":"code","94c20681":"code","5612450f":"code","525537f4":"code","11db326e":"code","f373e72d":"code","5c4d2f53":"code","0ecdd5ba":"code","0c1c525b":"code","6351296f":"code","3e6be32a":"code","3f0573df":"code","7cb48f6b":"code","6729c873":"code","91591e9b":"code","dc0d4201":"code","bd0542c4":"code","bd9c094f":"code","e4f8d4ff":"code","036472e9":"code","95165938":"markdown","48400b72":"markdown","061ec74b":"markdown","338472d0":"markdown","2bfb012c":"markdown","5327acd4":"markdown","2474564e":"markdown","988e4964":"markdown","fb1d0cac":"markdown","4de2b100":"markdown","c8044bfa":"markdown","34b078d3":"markdown","02c02136":"markdown","85c8e78d":"markdown","9d5cc6bb":"markdown","24968955":"markdown"},"source":{"8253dc51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.cm as cm\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","46ce13a8":"trainData = pd.read_csv('..\/input\/train.csv')","71961376":"trainData.head()","db4ccac0":"trainData.info() # We have 42000 samples with 785 columns including label column","519c11ff":"# Visualization of our digits\nsns.countplot(trainData.label)\nplt.title('Digit Count', color = 'blue', fontsize = 15)","94c20681":"trainData = trainData[(trainData[\"label\"] == 3) | (trainData[\"label\"] == 8)] # Filtering the train data","5612450f":"trainData.head() # Now we have only 3 and 8 labeled numbers","525537f4":"trainData.label = [1 if each == 3 else 0 for each in trainData.label] # We will call 1 for number three and 0 for number eight. We need binary results for logistic regression.","11db326e":"y = trainData.label.values # Only labels for model training\nx_data = trainData.drop([\"label\"], axis = 1)","f373e72d":"x_data.head() # Train data wihtout label.","5c4d2f53":"x = (x_data \/ 255.0)","0ecdd5ba":"print('digits({0[0]},{0[1]})'.format(x.shape))","0c1c525b":"def showDigit(index):\n    sampleDigit = x.iloc[:,0:].values[index]\n    digit = sampleDigit.reshape(28, 28)\n    plt.axis('off')\n    plt.imshow(digit, cmap = cm.binary) ","6351296f":"showDigit(26) # 25 - 26 is random member of our array. Just for check the how the numbers are look","3e6be32a":"showDigit(25)","3f0573df":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","7cb48f6b":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train : \", x_train.shape)\nprint(\"x_test : \", x_test.shape)\nprint(\"y_train : \", y_train.shape)\nprint(\"y_test : \", y_test.shape)","6729c873":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b","91591e9b":"def sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))\n    return y_head","dc0d4201":"def forward_backward_propogation(w, b, x_train, y_train):\n    \n    #forward propogation z = (w.T)x + b \n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]  # x_train.shape[1] is for scaling\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1] # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","bd0542c4":"def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    costList = []\n    costListForPlot = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        # make forward and backward propogation and find costs and gradients\n        cost,gradients = forward_backward_propogation(w, b, x_train, y_train)\n        costList.append(cost)\n        #lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            costListForPlot.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n    parameters = {\"weight\" : w, \"bias\" : b}\n    plt.plot(index, costListForPlot)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, costList","bd9c094f":"# After forward and backward propagation. We will predict results from out model.\n\ndef predict(w,b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n    \n    return Y_prediction","e4f8d4ff":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    \n    # initialize\n    dimension = x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations) \n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy : {} %.\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","036472e9":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.1, num_iterations = 1000) ","95165938":"*Lets look some sample of digits.*","48400b72":"* We will use two digits for binary classification. Lets take two label from train data. (3 and 8)","061ec74b":"<a id=\"9\"><\/a> \n**Defining The Logistic Regression Method**","338472d0":"<a id=\"10\"><\/a> \n## Conclusion\n\n* The above transactions are called simple neural network (logistic regression). These are made without sklearn library for learn logic behind logistic regression. <br\/>\n\nThanks in advance for suggestions.","2bfb012c":"<a id=\"5\"><\/a> \n**Sigmoid function for logistic regression**","5327acd4":"<a id=\"8\"><\/a> \n**Prediction Method for Logistic Regression Model**","2474564e":"We have 8414 images. 28x28 pixels consisting of three and eight","988e4964":"<a id=\"2\"><\/a> \n**Normalization** <br\/>\n\n&emsp; * Each pixel value is in the range 0..256. We will convert this range to 0..1*","fb1d0cac":"* Lets test our model. For 1000 iterations with 0.1 learning rate.","4de2b100":"### **Content**\n* [Data Exploration](#1)\n* [Normalization](#2)\n* [Train-Test Split](#3)\n* [Initialization of Weights and Bias](#4)\n* [Sigmoid Function For Logistic Regression](#5)\n* [Forward - Backward Propagation](#6)\n* [Updating Parameters Using Forward and Backward Propagation](#7)\n* [Prediction Method For Logistic Regression Model](#8)\n* [Defining The Logistic Regression Method](#9)\n* [Conclusion](#10)","c8044bfa":"<a id=\"4\"><\/a> \n**Initialization of weights and bias.**","34b078d3":"## **Digit Recognizer with Logistic Regression**","02c02136":"<a id=\"6\"><\/a> \n**Forward - Backward Propagation**","85c8e78d":"<a id=\"1\"><\/a> \n**Data Exploration**","9d5cc6bb":"<a id=\"7\"><\/a> \n**Updating Parameters Using Forward and Backward Propagation**","24968955":"<a id=\"3\"><\/a> \n***Train-Test Split***"}}