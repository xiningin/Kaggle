{"cell_type":{"c392557d":"code","c5e77869":"code","c76de92a":"code","a4a31fb0":"code","fd111a27":"code","3f7140f8":"code","c26c1d48":"code","8124e610":"code","878cb665":"code","5bf36812":"code","01c4e4d6":"code","f567d6b0":"code","08177672":"code","ecfdbb80":"code","dcca9a0f":"code","a4be5edf":"code","1092829c":"code","9f7c3ee5":"code","cbfa1640":"code","a8ebdf4c":"code","41d41397":"code","b52c314d":"code","fa277280":"code","5af48b2e":"code","d328e049":"code","b91b8446":"code","2c9112a0":"code","83e2ff40":"code","495b4e15":"code","0a7b1fe2":"code","bb46fa0c":"code","f86a9baa":"code","0f45b777":"code","179ecc57":"markdown","486704f1":"markdown","613ce809":"markdown","650ad700":"markdown","b6a77301":"markdown","ffd9e551":"markdown","5c11c7ff":"markdown","6608c3a7":"markdown","cc7d93cb":"markdown"},"source":{"c392557d":"#import all the requirements \nimport numpy as np\nimport os\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K","c5e77869":"#checking the GPU\n!nvidia-smi","c76de92a":"INIT_LR = 1e-3\nBATCH_SIZE = 16\nEPOCHS = 100\nIMAGE_SIZE = 256\ndefault_image_size = tuple((IMAGE_SIZE, IMAGE_SIZE))\nimage_size = 0\ndata_dir = \"..\/input\/tomatoleaf\/tomato\/val\"\nCHANNELS=3\nAUTOTUNE = tf.data.AUTOTUNE","a4a31fb0":"#spliting the Dataset\ndef get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n    assert (train_split + test_split + val_split) == 1\n    \n    ds_size = ds.cardinality().numpy()\n    \n    if shuffle:\n        ds = ds.shuffle(shuffle_size, seed=12)\n    \n    train_size = int(train_split * ds_size)\n    val_size = int(val_split * ds_size)\n    \n    train_ds = ds.take(train_size)    \n    val_ds = ds.skip(train_size).take(val_size)\n    test_ds = ds.skip(train_size).skip(val_size)\n    \n    return train_ds, val_ds, test_ds","fd111a27":"#dataset spliting into training and testing \ndataset = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  seed=123,\n  image_size=default_image_size,\n  batch_size=BATCH_SIZE\n)\n\n\ntrain_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)","3f7140f8":"#checking the avaiable classes\nclass_names = dataset.class_names\nn_classes = len(class_names)\nprint(n_classes, class_names)","c26c1d48":"#Displaying the sample images from the dataset\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","8124e610":"#checking the batch Size\nfor image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","878cb665":"train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)","5bf36812":"#Creating a Layer for Resizing and Normalization\nresize_and_rescale = tf.keras.Sequential([\n  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n])","01c4e4d6":"#Data Augmentation\ndata_augmentation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomRotation(0.2),\n])","f567d6b0":"input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nbatch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n    chanDim = 1","08177672":"#CNN with softmax activation layer in last output layer\nmodel = models.Sequential([\n    resize_and_rescale,\n    data_augmentation,\n    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(n_classes, activation='softmax'),\n])\n\nmodel.build(input_shape=batch_input_shape)","ecfdbb80":"model.summary()","dcca9a0f":"#Compiling the Model\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    metrics=['accuracy']\n)","a4be5edf":"#Trainig the network\nhistory = model.fit(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    validation_data=val_ds,\n    verbose=1,\n    epochs=EPOCHS,\n)","1092829c":"#testing the CNN model\nprint(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(test_ds)\nprint(f\"Test Accuracy: {round(scores[1],4)*100}%\")","9f7c3ee5":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","cbfa1640":"import os\nmodel.save(\"CNN.h5\")","a8ebdf4c":"def predict(model, img):\n    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n    img_array = tf.expand_dims(img_array, 0) # Create a batch\n\n    predictions = model.predict(img_array)\n\n    predicted_class = class_names[np.argmax(predictions[0])]\n    confidence = round(100 * (np.max(predictions[0])), 2)\n    return predicted_class, confidence\n","41d41397":"\nimport numpy as np\nfor images_batch, labels_batch in test_ds.take(1):\n    \n    first_image = images_batch[0].numpy().astype('uint8')\n    first_label = labels_batch[0].numpy()\n    \n    print(\"first image to predict\")\n    plt.imshow(first_image)\n    print(\"actual label:\",class_names[first_label])\n    \n    batch_prediction = model.predict(images_batch)\n    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])\n\n","b52c314d":"plt.figure(figsize=(15, 15))\nfor images, labels in test_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        predicted_class, confidence = predict(model, images[i].numpy())\n        \n        actual_class = class_names[labels[i]] \n        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n        plt.axis(\"off\")","fa277280":"#Create a Quantization Aware Model\n!pip install --upgrade tensorflow-model-optimization\nimport tensorflow_model_optimization as tfmot","5af48b2e":"#Quantize only the Dense, MaxPool2D, Conv2D Layers\ndef apply_quantization(layer):\n    if (\n        isinstance(layer, layers.Dense)\n        or isinstance(layer, layers.MaxPool2D)\n        or isinstance(layer, layers.Conv2D)\n    ):\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n    return layer","d328e049":"annotated_model = tf.keras.models.clone_model(\n    model,\n    clone_function=apply_quantization,\n)\n\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\nquant_aware_model.summary()","b91b8446":"#Finr Tuning the model\n\nquant_aware_model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    metrics=['accuracy']\n)\n\n","2c9112a0":"q_history = quant_aware_model.fit(train_ds,\n    batch_size=BATCH_SIZE,\n    validation_data=val_ds,\n    verbose=1,\n    epochs=25,\n)","83e2ff40":"#Testing the accuracy after Fine tunning \nprint(\"[INFO] Calculating Quant Aware model accuracy\")\nscores = quant_aware_model.evaluate(test_ds)\nprint(f\"Test Accuracy: {round(scores[1],4)*100}%\")","495b4e15":"#saving the fine tunning modeol\nimport os\nmodel.save(\"FineTuning.h5\")","0a7b1fe2":"converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\nquantized_tflite_model = converter.convert()","bb46fa0c":"#Testing the Tfmodel\n\n\n# I tried to convert FINETUNE WEIGHTS into TFLITE  model its take about 3 hour to runned but still running in kaggle.If you want to convert the model.Please move to Google colab Platform \n#No chances in the code to convert tflite model\n\ndef evaluate_tflite_model (dataset, interpreter):\n    input_index = interpreter.get_input_details()[0][\"index\"]\n    output_index = interpreter.get_output_details()[0][\"index\"]\n\n    prediction_digits = []\n    test_labels = []\n    for image, label in dataset.unbatch().take(dataset.unbatch().cardinality()):\n\n        test_image = np.expand_dims(image, axis=0).astype(np.float32)\n        interpreter.set_tensor(input_index, test_image)\n        interpreter.invoke()\n        \n        output = interpreter.tensor(output_index)\n        digit = np.argmax(output()[0])\n        prediction_digits.append(digit)\n        test_labels.append(label)\n\n    prediction_digits = np.array(prediction_digits)\n    accuracy = (prediction_digits == test_labels).mean()\n    return accuracy\n\ninterpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\ninterpreter.allocate_tensors()\n\ntest_accuracy = evaluate_tflite_model(dataset, interpreter)\n\nprint('Quant TFLite test_accuracy:', test_accuracy)","f86a9baa":"import os\nmodel.save(\"tflite_model.pb\")","0f45b777":"#testing the tflite model \ninput_index = interpreter.get_input_details()[0][\"index\"]\noutput_index = interpreter.get_output_details()[0][\"index\"]\n\nplt.figure(figsize=(15, 15))\nfor images, labels in test_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))        \n\n        actual_class = class_names[labels[i]]\n\n        test_image = np.expand_dims(images[i], axis=0).astype(np.float32)\n        interpreter.set_tensor(input_index, test_image)\n        interpreter.invoke()\n        output = interpreter.tensor(output_index)\n        digit = np.argmax(output()[0])\n\n        predicted_class = class_names[digit]\n        confidence = np.max(output()[0])*100\n\n        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n        plt.axis(\"off\")\n\n","179ecc57":"**Spliting the Dataset**\n\n\nIn this Function is use for Perform the Spliting the images for training the Nerrtwork\n\n\n\nSeed Size = 123\n\nThis will helpfull for this Dataset Spliting easily for 10 classes ","486704f1":"**HYPERPARAMETERS**","613ce809":"**Checking the GPU enable for Training the Model**","650ad700":"**Bulid the model**","b6a77301":"**Plotting the accuracy Graph**","ffd9e551":"**Hello Fokers!**\n\nIn this Notebook Trained and Converted into the CNN Model to Tensorflow Lite Model which can easily implemented in Cloud Platforms for Classification and Detection on raw images \n\nA CNN model as been Derived for using the own Architecture and it convert the model using the tensorflow Model Lib","5c11c7ff":"**Convert Quanitzation Aware Model to TF Lite Model**","6608c3a7":"**DATA AUGMENTATION**","cc7d93cb":"**Model Architecture**"}}