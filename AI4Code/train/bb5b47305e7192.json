{"cell_type":{"18a6c6c5":"code","6059229e":"code","04766bdb":"code","6278bf8e":"code","8f01404d":"code","a9062c46":"code","02703a8a":"code","43ee3afa":"code","0bae34cc":"code","1ccb82fc":"code","a96cc2d7":"code","24b429fa":"code","a378aacd":"code","3fdac6f5":"code","1638d8a4":"code","ca6994fb":"code","cca0f3f4":"code","9b78bb7e":"code","9fee11aa":"code","1d11e4ab":"code","c341443f":"code","37f46bd1":"code","1f066a87":"code","9c299db8":"code","f43a1edf":"code","5fce9d2a":"code","41cc8347":"code","43bb4e64":"code","fe51d69e":"code","8efea906":"code","46dd9cae":"code","4e4a90f0":"code","83a27c81":"code","9c1fa057":"code","7649e467":"code","714ed4ed":"code","b935d6ff":"code","a83dfa82":"code","9556e2c3":"code","f975ef85":"code","085c2d89":"code","a079cc4f":"code","4347093b":"code","8d8549c4":"code","a0644dc1":"code","e415ae44":"code","cff7f638":"code","a1a72e2d":"code","6d1c9c11":"code","daf14b9b":"code","ee9ab339":"code","579b7d31":"code","3c3ebe81":"code","0c1bdd33":"code","8a6ec3d5":"code","e0e1de77":"code","386a5188":"code","45cfdd59":"code","cfbf58d5":"code","70afba8b":"markdown","b3680e8e":"markdown","020c2526":"markdown","d4fb21c4":"markdown","fd34d491":"markdown","2fe7a63d":"markdown","d5ae3d6a":"markdown","85978311":"markdown","c3aa32b7":"markdown","c664bcb4":"markdown","787161ee":"markdown","dd1b03f3":"markdown","493ea8a4":"markdown","3032b1cb":"markdown","70475abf":"markdown","f64570b0":"markdown","dc4650cc":"markdown","40a83588":"markdown","4cdace84":"markdown","f265fc46":"markdown","1cb781e4":"markdown","b725c7aa":"markdown","34a1b042":"markdown","6a895ca7":"markdown","35a2a59a":"markdown","480430e0":"markdown","4d195086":"markdown","43e377c1":"markdown","3298591a":"markdown","c318cad6":"markdown","bc9ae529":"markdown","c5dc04af":"markdown","e1765d16":"markdown","f0e14d3a":"markdown","371e9047":"markdown","ef77a5fa":"markdown","1392cf4e":"markdown","3d7d1e63":"markdown"},"source":{"18a6c6c5":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6059229e":"# List files available\nprint(os.listdir(\"..\/input\/\"))","04766bdb":"# Training data\napp_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","6278bf8e":"# Testing data features\napp_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","8f01404d":"app_train['TARGET'].value_counts()","a9062c46":"app_train['TARGET'].astype(int).plot.hist();","02703a8a":"total = app_train.isnull().sum().sort_values(ascending = False)\npercent = (app_train.isnull().sum()\/app_train.isnull().count()*100).sort_values(ascending = False)\nmiss_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmiss_train.head(10)","43ee3afa":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","0bae34cc":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","1ccb82fc":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","a96cc2d7":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","24b429fa":"(app_train['DAYS_BIRTH'] \/ -365).describe()","a378aacd":"app_train['DAYS_EMPLOYED'].describe()","3fdac6f5":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","1638d8a4":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","ca6994fb":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# Check values distribution after repalcementin in train dataset\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","cca0f3f4":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","9b78bb7e":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","9fee11aa":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","1d11e4ab":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","c341443f":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","37f46bd1":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","1f066a87":"bureau = pd.read_csv('..\/input\/bureau.csv')\nbureau.head()","9c299db8":"bureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')\nbureau_balance.head()","f43a1edf":"def agg_numeric(df, group_var, df_name):\n\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","5fce9d2a":"def count_categorical(df, group_var, df_name):\n \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","41cc8347":"# Calculate value count statistics for each `SK_ID_CURR` in bureau\nbureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","43bb4e64":"# Counts of bureau\nbureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","fe51d69e":"# Calculate value count statistics for each `SK_ID_CURR` in butrau_balance\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","8efea906":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","46dd9cae":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","4e4a90f0":"bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","83a27c81":"# Merge with the value counts of bureau\ntrain = app_train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","9c1fa057":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))","7649e467":"# Merge with the value counts of bureau\ntest = app_test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","714ed4ed":"print('Shape of test Data: ', test.shape)","b935d6ff":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\napp_train, app_test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels\n","a83dfa82":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel bureau_balance, bureau_agg,  bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","9556e2c3":"corrs = train.corr()","f975ef85":"corrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","085c2d89":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","a079cc4f":"# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","4347093b":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","8d8549c4":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","a0644dc1":"app_train = train_corrs_removed\napp_test = test_corrs_removed","e415ae44":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","cff7f638":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","a1a72e2d":"app_train = app_train_domain\napp_test = app_test_domain","6d1c9c11":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","daf14b9b":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","ee9ab339":"# Make predictions\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","579b7d31":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","3c3ebe81":"# Save the submission to a csv file\nsubmit.to_csv('logistic_regression.csv', index = False)","0c1bdd33":"from sklearn.ensemble import RandomForestClassifier\n\napp_train = app_train.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train)\ndomain_features_test = imputer.transform(app_test)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","8a6ec3d5":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest.csv', index = False)","e0e1de77":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","386a5188":"app_train['TARGET'] = train_labels","45cfdd59":"submission, fi, metrics = model(app_train, app_test)\nprint('Light GBM metrics')\nprint(metrics)","cfbf58d5":"submission.to_csv('LightBMG.csv', index = False)","70afba8b":"Let's now look at the number of unique entries in each of the `object` (categorical) columns.","b3680e8e":"### Correlations","020c2526":"The test set is considerably smaller and lacks a `TARGET` column. ","d4fb21c4":"### Collinear Variables","fd34d491":"### Read in bureau and Bureau Balance","2fe7a63d":"##### Find correlation for new training data","d5ae3d6a":"## Imports","85978311":"##### Create new features with percentages.","c3aa32b7":"##### Calculate aggregation statistics (mean, count, max, min) for numeric variables.","c664bcb4":"This submission scores 0.7628. ","787161ee":"##  Random Forest model\n","dd1b03f3":"### Label Encoding and One-Hot Encoding\n\nFor any categorical variable (`dtype == object`) with 2 unique categories, use label encoding, and for any categorical variable with more than 2 unique categories, use one-hot encoding. \n\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.","493ea8a4":"The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict).","3032b1cb":"`DAYS_BIRTH` (the age in days of the client at the time of the loan in negative days) is the most positive correlation, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). ","70475abf":"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. ","f64570b0":"## Read in Data \n","dc4650cc":"### Aligning Training and Testing Data\n","40a83588":"##### Compute counts and normalized counts of each category in a categorical variable.","4cdace84":"### Function for Numeric Aggregations","f265fc46":"### Calculate Information for test Data","1cb781e4":"### Align the testing and training dataframes","b725c7aa":"### Light Gradient Boosting Machine","34a1b042":"### Aggregated Stats of Bureau Balance by Client","6a895ca7":"### Function to Handle Categorical Variables","35a2a59a":"### Find outlier for column\"DAYS_BIRTH\" and \"DAYS_EMPLOYED\"\n\n\n","480430e0":"### Aggregated Stats of Bureau Balance by loan","4d195086":"##### Remove variables with correlation coefficient above 0.8.","43e377c1":"### Logistic Regression Implementation\u00b6","3298591a":"From this plot, we can see it is an imbalanced class problem.","c318cad6":"## Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","bc9ae529":"### Insert Computed Features into app_train Data","c5dc04af":"Fill in the anomalous values with not a number (`np.nan`) in training and test dataset, then create a new boolean column indicating whether or not the value was anomalous.\n\n","e1765d16":"### Effect of Age on Repayment","f0e14d3a":"##### This model scores 0.6781.","371e9047":"Check the anomalous clients to see they have higher or low rates of default than the rest of the clients.","ef77a5fa":"##### This model scores 0.6837.","1392cf4e":"### Find correlations\n\n\n","3d7d1e63":"## Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column. "}}