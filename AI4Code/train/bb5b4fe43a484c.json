{"cell_type":{"8564dfda":"code","01bad0f5":"code","d93d1a56":"code","80adf1ea":"code","64c3f7ce":"code","15b20129":"code","ebe431f0":"code","62ae6ee0":"code","73f4fe27":"code","e7de3869":"code","02ed2804":"code","0171c086":"code","75e4f7a7":"code","3a9566d4":"code","b3578144":"code","38a23616":"code","46141931":"code","d5a65304":"code","f208cd77":"code","e4c335d9":"code","e23175e0":"code","c0246149":"code","fff3f872":"code","03e60841":"code","8920c9ee":"code","3de7af5a":"code","1197a9ef":"code","400145e6":"code","3185fd44":"code","077ec30b":"code","bb414066":"code","15861a81":"code","2dc31e0c":"code","8b271f07":"code","74065332":"code","1c6d7bdb":"code","3f75b194":"code","a9798f94":"code","ff87b0f9":"code","61c3c951":"code","06015da3":"code","6e49ed0e":"code","76a2b563":"code","7c3a3889":"code","238f038e":"code","d3da61e7":"code","642741b0":"code","5c9a1848":"code","38a97086":"code","3ea9bd7b":"code","fb8b3274":"code","ea181224":"code","de7cd216":"code","c4e4ef13":"code","5d69d3b6":"code","afecb9d0":"code","9752f2f7":"code","86e45b78":"code","13a6dac5":"code","bb1e11b0":"code","2b0f0106":"code","766f19d2":"code","33a4352d":"code","ceb0ecca":"code","2a5935a3":"code","a588c0bb":"code","0d1c21ea":"code","5cf6d694":"markdown","9ed21a9c":"markdown","f109bf91":"markdown","fd51c1c7":"markdown","856f0269":"markdown"},"source":{"8564dfda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01bad0f5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\n\npd.set_option('display.max_columns',None)\nsns.set_style(style='darkgrid')","d93d1a56":"train = pd.read_excel('..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')\ntrain.head()","80adf1ea":"train.describe()","64c3f7ce":"train.info()","15b20129":"train.isnull().sum()","ebe431f0":"train['Additional_Info'].unique()","62ae6ee0":"train['Duration'].value_counts()","73f4fe27":"train.dropna(inplace=True)","e7de3869":"train.shape","02ed2804":"train.columns","0171c086":"train['Day_of_Journey'] = pd.to_datetime(train.Date_of_Journey,format='%d\/%m\/%Y').dt.day\ntrain['Month_of_Journey'] = pd.to_datetime(train.Date_of_Journey,format='%d\/%m\/%Y').dt.month","75e4f7a7":"train.head()","3a9566d4":"train.drop(['Date_of_Journey'],axis =1, inplace=True)","b3578144":"train['Dep_hour'] = pd.to_datetime(train['Dep_Time']).dt.hour\ntrain['Dep_min'] = pd.to_datetime(train['Dep_Time']).dt.minute\ntrain.drop(['Dep_Time'],axis =1, inplace=True)","38a23616":"train.head()","46141931":"train[\"Arrival_hour\"] = pd.to_datetime(train.Arrival_Time).dt.hour\ntrain[\"Arrival_min\"] = pd.to_datetime(train.Arrival_Time).dt.minute\ntrain.drop([\"Arrival_Time\"], axis = 1, inplace = True)","d5a65304":"train.head()","f208cd77":"duration = list(train[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:   \n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   \n        else:\n            duration[i] = \"0h \" + duration[i]           \n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    \n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))","e4c335d9":"\ntrain['Duration_hours'] = duration_hours\ntrain['Duration_minutes'] = duration_mins","e23175e0":"train.head()","c0246149":"train.drop(['Duration'],axis=1,inplace=True)","fff3f872":"train['Airline'].value_counts()","03e60841":"sns.catplot(x='Airline',y='Price',data=train.sort_values('Price',ascending=False),kind='boxen',height=6,aspect=3)\nplt.show()","8920c9ee":"fig_dims = (25, 15)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x='Airline',y='Price', data = train)","3de7af5a":"Airline = train[['Airline']]\nAirline = pd.get_dummies(Airline,drop_first=True)\nAirline.head()","1197a9ef":"train['Source'].value_counts()","400145e6":"plt.hist(train['Source'])","3185fd44":"train['Destination'].value_counts()","077ec30b":"plt.hist(train['Destination'],bins=15)\nplt.show()","bb414066":"sns.catplot(x='Source',y='Price',data = train.sort_values('Price',ascending=False),kind ='boxen',height=6,aspect=3)","15861a81":"sns.catplot(x='Destination',y='Price',data = train.sort_values('Price',ascending=False),kind ='boxen',height=6,aspect=3)","2dc31e0c":"Source = train[['Source']]\nSource = pd.get_dummies(Source,drop_first=True)\nSource.head()","8b271f07":"Destination = train[['Destination']]\nDestination= pd.get_dummies(Destination,drop_first=True)\nDestination.head()","74065332":"train['Route'].head()","1c6d7bdb":"train.drop('Route',axis=1, inplace=True)\ntrain.head()","3f75b194":"train.drop('Additional_Info',axis=1, inplace=True)","a9798f94":"train['Total_Stops'].unique()","ff87b0f9":"train['Total_Stops'].replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\ntrain.head()","61c3c951":"\ndata_train = pd.concat([train,Airline,Source,Destination],axis=1)\ndata_train.head()","06015da3":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","6e49ed0e":"data_train.shape","76a2b563":"plt.figure(figsize = (18,18))\nsns.heatmap(train.corr(), annot = True, cmap = \"RdYlGn\")\nplt.show()","7c3a3889":"fig_dims = (10,10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.regplot(x='Duration_hours',y='Price',data=data_train)","238f038e":"fig_dims = (10,10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.boxplot(x='Total_Stops',y='Price',data=data_train,palette='rainbow')","d3da61e7":"test_data = pd.read_excel('..\/input\/flight-fare-prediction-mh\/Test_set.xlsx')\ntest_data.head()","642741b0":"print(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","5c9a1848":"data_test.head()","38a97086":"data_train.columns","3ea9bd7b":"X = data_train.drop(['Price'],axis=1)\nX.head()","fb8b3274":"y = data_train['Price']\ny.head()","ea181224":"reg = ExtraTreesRegressor()\nreg.fit(X,y)","de7cd216":"print(reg.feature_importances_)","c4e4ef13":"plt.figure(figsize=(15,10))\nimp_features = pd.Series(reg.feature_importances_, index=X.columns)\nimp_features.nlargest(20).plot(kind='barh')\nplt.show()","5d69d3b6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","afecb9d0":"reg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","9752f2f7":"y_pred = reg_rf.predict(X_test)","86e45b78":"reg_rf.score(X_train, y_train)","13a6dac5":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","bb1e11b0":"metrics.r2_score(y_test, y_pred)","2b0f0106":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","766f19d2":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","33a4352d":"rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","ceb0ecca":"rf_random.fit(X_train,y_train)","2a5935a3":"rf_random.best_params_","a588c0bb":"prediction = rf_random.predict(X_test)","0d1c21ea":"metrics.r2_score(y_test,prediction)","5cf6d694":"All strings except price i.e. the Dependent Variable.","9ed21a9c":"## Hypertuning","f109bf91":"## One Hot Encoding","fd51c1c7":"## What now?\n**Deploy it!**","856f0269":"# EDA"}}