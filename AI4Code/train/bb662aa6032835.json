{"cell_type":{"c07db46c":"code","8e28aa04":"code","fef0f091":"code","4d830425":"code","97d9a84b":"code","7a826bab":"code","1c596898":"code","aa2edd7f":"code","11c066c7":"code","b41bcdf1":"code","97325209":"code","ad5c05e0":"code","2ab66dfd":"code","37dcd0c1":"code","619edb1e":"code","66382085":"code","b9e0ca59":"code","6227da0c":"markdown","df3d64e1":"markdown","aec5d001":"markdown","baf630e6":"markdown","08b39577":"markdown","7159cdaf":"markdown","15272ed7":"markdown","134f7f50":"markdown","2a054892":"markdown","1330dd21":"markdown","ae1fe3c5":"markdown","b7d53a78":"markdown","2078d5d5":"markdown","dc274ea3":"markdown","d52bcbb8":"markdown","20b5d032":"markdown"},"source":{"c07db46c":"import advertools as adv # package used to import data from Google's API\nimport pandas as pd # pandas is pandas! \npd.options.display.max_columns = None\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\n\ncx = 'YOUR_GOOGLE_CSE_SEARCH_ENGINE_ID' # get it here https:\/\/cse.google.com\/cse\/\nkey = 'YOUR_GOOGLE_DEVELOPER_KEY' # get it here https:\/\/console.cloud.google.com\/apis\/library\/customsearch.googleapis.com\n","8e28aa04":"# coffee_df = adv.serp_goog(cx=cx, key=key, q='coffee',\n#                           gl=adv.SERP_GOOG_VALID_VALS['gl'])\n\n# cafe_df = adv.serp_goog(cx=cx, key=key, q='cafe',\n#                         gl=adv.SERP_GOOG_VALID_VALS['gl'])","fef0f091":"coffee_df = pd.read_csv('..\/input\/coffee_serps.csv')\ncafe_df = pd.read_csv('..\/input\/cafe_serps.csv')\n\ncountry_codes = sorted(adv.SERP_GOOG_VALID_VALS['gl'])\nprint('number of available locations:', len(country_codes))\nprint()\nprint('country codes:')\nprint(*[country_codes[x:x+15] for x in range(0, len(country_codes), 15)],\n      sep='\\n')","4d830425":"print('Coffee domains:', coffee_df['displayLink'].nunique())\nprint('Cafe domains:', cafe_df['displayLink'].nunique())\ncommon = set(coffee_df['displayLink']).intersection(cafe_df['displayLink'])\nprint('# Common domains:', len(common))\ncommon\n\n","97d9a84b":"num_domains = 15  # the number of domains to show in the chart\nopacity = 0.02  # how opaque you want the circles to be\ndf = coffee_df  # which DataFrame you are using","7a826bab":"top_domains = df['displayLink'].value_counts()[:num_domains].index.tolist()\ntop_domains","1c596898":"top_df = df[df['displayLink'].isin(top_domains)]\ntop_df.head(3)\n","aa2edd7f":"top_df_counts_means = (top_df\n                       .groupby('displayLink', as_index=False)\n                       .agg({'rank': ['count', 'mean']})\n                       .set_axis(['displayLink', 'rank_count', 'rank_mean'],\n                                 axis=1, inplace=False))\ntop_df_counts_means\n","11c066c7":"top_df = (pd.merge(top_df, top_df_counts_means)\n          .sort_values(['rank_count', 'rank_mean'],\n                       ascending=[False, True]))\ntop_df.iloc[:3, list(range(8))+ [-2, -1]]\n","b41bcdf1":"num_queries = df['queryTime'].nunique()\n\nsummary = (df\n           .groupby(['displayLink'], as_index=False)\n           .agg({'rank': ['count', 'mean']})\n           .sort_values(('rank', 'count'), ascending=False)\n           .assign(coverage=lambda df: (df[('rank', 'count')].div(num_queries))))\nsummary.columns = ['displayLink', 'count', 'avg_rank', 'coverage']\nsummary['displayLink'] = summary['displayLink'].str.replace('www.', '')\nsummary['avg_rank'] = summary['avg_rank'].round(1)\nsummary['coverage'] = (summary['coverage'].mul(100)\n                       .round(1).astype(str).add('%'))\nsummary.head(10)\n","97325209":"print('number of queries:', num_queries)\nfig = go.Figure()\nfig.add_scatter(x=top_df['displayLink'].str.replace('www.', ''),\n                y=top_df['rank'], mode='markers',\n                marker={'size': 35, 'opacity': opacity},\n                showlegend=False)\nfig.layout.height = 600\nfig.layout.yaxis.autorange = 'reversed'\nfig.layout.yaxis.zeroline = False\niplot(fig)","ad5c05e0":"rank_counts = (top_df\n               .groupby(['displayLink', 'rank'])\n               .agg({'rank': ['count']})\n               .reset_index()\n               .set_axis(['displayLink', 'rank', 'count'],\n                         axis=1, inplace=False))\nrank_counts[:15]\n","2ab66dfd":"fig.add_scatter(x=rank_counts['displayLink'].str.replace('www.', ''),\n                y=rank_counts['rank'], mode='text',\n                marker={'color': '#000000'},\n                text=rank_counts['count'], showlegend=False)\niplot(fig)","37dcd0c1":"for domain in rank_counts['displayLink'].unique():\n    rank_counts_subset = rank_counts[rank_counts['displayLink']==domain]\n    fig.add_scatter(x=[domain.replace('www.', '')],\n                    y=[11], mode='text',\n                    marker={'size': 50},\n                    text=str(rank_counts_subset['count'].sum()))\n    fig.add_scatter(x=[domain.replace('www.', '')],\n                    y=[12], mode='text',\n                    text=format(rank_counts_subset['count'].sum() \/ num_queries, '.1%'))\n    fig.add_scatter(x=[domain.replace('www.', '')],\n                    y=[13], mode='text',\n                    marker={'size': 50},\n                    text=str(round(rank_counts_subset['rank']\n                                   .mul(rank_counts_subset['count'])\n                                   .sum() \/ rank_counts_subset['count']\n                                   .sum(),2)))\nfig.layout.title = ('Google Search Results Rankings<br>keyword(s): ' + \n                    ', '.join(list(df['searchTerms'].unique())) + \n                    ' | queries: ' + str(df['queryTime'].nunique()))\nfig.layout.hovermode = False\nfig.layout.yaxis.autorange = 'reversed'\nfig.layout.yaxis.zeroline = False\nfig.layout.yaxis.tickvals = list(range(1, 14))\nfig.layout.yaxis.ticktext = list(range(1, 11)) + ['Total<br>appearances','Coverage', 'Avg. Pos.'] \nfig.layout.height = 700\nfig.layout.width = 1200\nfig.layout.yaxis.title = 'SERP Rank (number of appearances)'\nfig.layout.showlegend = False\nfig.layout.paper_bgcolor = '#eeeeee'\nfig.layout.plot_bgcolor = '#eeeeee'\niplot(fig)","619edb1e":"def plot_serps(df, opacity=0.1, num_domains=10, width=1200, height=700):\n    \"\"\"\n    df: a DataFrame resulting from running advertools.serp_goog\n    opacity: the opacity of the markers [0, 1]\n    num_domains: how many domains to plot\n    \"\"\"\n    top_domains = df['displayLink'].value_counts()[:num_domains].index.tolist()\n    top_df = df[df['displayLink'].isin(top_domains)]\n    top_df_counts_means = (top_df\n                       .groupby('displayLink', as_index=False)\n                       .agg({'rank': ['count', 'mean']})\n                       .set_axis(['displayLink', 'rank_count', 'rank_mean'],\n                                 axis=1, inplace=False))\n    top_df = (pd.merge(top_df, top_df_counts_means)\n          .sort_values(['rank_count', 'rank_mean'],\n                       ascending=[False, True]))\n    rank_counts = (top_df\n               .groupby(['displayLink', 'rank'])\n               .agg({'rank': ['count']})\n               .reset_index()\n               .set_axis(['displayLink', 'rank', 'count'],\n                         axis=1, inplace=False))\n    num_queries = df['queryTime'].nunique()\n    fig = go.Figure()\n    fig.add_scatter(x=top_df['displayLink'].str.replace('www.', ''),\n                    y=top_df['rank'], mode='markers',\n                    marker={'size': 35, 'opacity': opacity},\n                    showlegend=False)\n    fig.layout.height = 600\n    fig.layout.yaxis.autorange = 'reversed'\n    fig.layout.yaxis.zeroline = False\n    fig.add_scatter(x=rank_counts['displayLink'].str.replace('www.', ''),\n                y=rank_counts['rank'], mode='text',\n                marker={'color': '#000000'},\n                text=rank_counts['count'], showlegend=False)\n    for domain in rank_counts['displayLink'].unique():\n        rank_counts_subset = rank_counts[rank_counts['displayLink']==domain]\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[11], mode='text',\n                        marker={'size': 50},\n                        text=str(rank_counts_subset['count'].sum()))\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[12], mode='text',\n                        text=format(rank_counts_subset['count'].sum() \/ num_queries, '.1%'))\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[13], mode='text',\n                        marker={'size': 50},\n                        text=str(round(rank_counts_subset['rank']\n                                       .mul(rank_counts_subset['count'])\n                                       .sum() \/ rank_counts_subset['count']\n                                       .sum(),2)))\n    fig.layout.title = ('Google Search Results Rankings<br>keyword(s): ' + \n                        ', '.join(list(df['searchTerms'].unique())) + \n                        ' | queries: ' + str(df['queryTime'].nunique()))\n    fig.layout.hovermode = False\n    fig.layout.yaxis.autorange = 'reversed'\n    fig.layout.yaxis.zeroline = False\n    fig.layout.yaxis.tickvals = list(range(1, 14))\n    fig.layout.yaxis.ticktext = list(range(1, 11)) + ['Total<br>appearances','Coverage', 'Avg. Pos.'] \n    fig.layout.height = height\n    fig.layout.width = width\n    fig.layout.yaxis.title = 'SERP Rank (number of appearances)'\n    fig.layout.showlegend = False\n    fig.layout.paper_bgcolor = '#eeeeee'\n    fig.layout.plot_bgcolor = '#eeeeee'\n    iplot(fig)","66382085":"plot_serps(coffee_df, opacity=0.07, num_domains=15)","b9e0ca59":"plot_serps(cafe_df, opacity=0.07, num_domains=8)","6227da0c":"In the chart above does having ncausa.org before wikipedia.org make sense?  \nIt's clear that Wikipedia's average rank is higher, but what about the number of appearnces and coverage?  \nWe add those data points for each domain with the following loops, so we can have a final number summarizing each of those metrics, and place them right below the domain on the chart. ","df3d64e1":"You will also have to [get credentials for the project](https:\/\/console.developers.google.com\/apis\/api\/customsearch.googleapis.com\/credentials) and \nthen [enable billing](https:\/\/console.cloud.google.com\/billing\/projects).\n\n","aec5d001":"Now it's easy to get a summary for `cafe_df` as the function is ready. You can change `num_domains` if you want to have more or fewer domains on the chart. The `opacity` parameter becomes more important the more queries you run. The more domains, the lower you want to keep it, so you can better see the denser positions. ","baf630e6":"We can now try to reproduce the same vizualization using `coffee_df`:","08b39577":"The opacity of the circle makes it very easy to see the top spots. Yet, it is difficult to to know really how different they are, since there are many levels of opacity. For this we can add the actual numbers to the chart, and this way you can visually spot the top locations, and get the actual number of appearance per position. \nWe first create the DataFrame `rank_counts`, which shows how many times each domain ranked per position. ","7159cdaf":"# \"Coffee\" and \"Cafe\" Search Engine Rankings All Over the World\n### A quick analysis of the SERP rankings of \"coffee\" and \"cafe\" in all available countries on Google. \n\nThis is an update on the recipe for visualizing a large number of SERPs as a quick SEO summary. \n\nThere are two minor adjustments: \n\n- **Sorting:** results are now sorted by the number of appearances on SERPs, then by average position. This is not ideal, and will be discussed below.\n- **New summary numbers:** three numbers are added to make it easier to evaluate results:  \n\n\n1. Total appearances: the number of times the domain appeared in SERPs (top 10)\n2. Coverage: total appearances divided by the total queries (shown as a percentage)\n3. Average position \n\nHere is how it looks like:\n![](https:\/\/drive.google.com\/uc?id=1LfD7M99G3IWoI7Ydqa0tfUTE9Vg476qY)\n\n## Some issues\nI'd like to start by mentioning some of the possible issues in the dataset: \n\n* Keywords: \"coffee\" doesn't say much about the intention of the user, and it is very generic. So is \"cafe\", although the intention is likely easier to figure out than \"coffee\".\n* Language: coffee is an English word, and cafe is used in many other languages. They requests were run in all available countries, many of which don't speak English, or the languages that have \"cafe\".  \n* Country weights: probably the most important issue. The visualization and counting assumes all countries are equal in value. They are not, whether in terms of population, GDP, coffe consumption etc. Ideally, you would have your own weights for countries, or whatever filtering parameters you are using and you can apply them for a better evaluation of the SERP ranks.\n\nWith the above issues in mind, let's take a look at how to produce the summary visualization shown above. \n\n## Basic Setup","15272ed7":"`top_df` is basically the same DataFrame filtered by having only results that are in `top_domains`.  \nThis is a summary of the number of appearances `rank_count` and the average position `rank_mean` for the domains that are in our `top_df`:","134f7f50":"Getting the top domains is done by simply getting the ones that appeared the most in the dataset. As mentioned above this is not ideal, but once you see the top ten or fifteen side by side, you will get a good idea of who is performing the best. \n\nFeel free to create your own criteria for how you want to define the top domains. ","2a054892":"## Quick Overview of the Domains","1330dd21":"If you want to have a summary in a table format, you can do it as follows: (those numbers will eventually be added to the chart)","ae1fe3c5":"The following code was used to generate the data. It is commented out just to show how the data was gathered, later we read them with `pandas.read_csv`. \n\nThe `gl` parameter of the `adv.serp_goog` function stands for \"geo-location\". \nThe available `gl`'s are available as a dictionary as well, so you don't have to worry about which countries are available.","b7d53a78":"Now we can easily add another layer to the chart, with the numbers mentioned: ","2078d5d5":"Now we merge the `top_df` with `top_df_counts_means`. Note that since the only common column name is `displayLink` I didn't specify the columns on which to merge. ","dc274ea3":"If you want to quickly visually see how often each domain ranks per position on SERPs you can do so with the following code. \n\nWhat the code does is plot a circle for every time a domain appears on a certain rank\/position. The more often the domain appears there, the more opaque the circle is, which helps to visually immediately spot how those ranks are distributed.\n\nTo make it more intuitive to view, I add the line `fig.layout.yaxis.autorange = 'reversed'`. It simply reverses the ranking so they appear in a similar way to actual SERPs, with rank one at the top, followed by two, all the way down to ten.","d52bcbb8":"Now we have a fuller picture about the domains.  \nYou can immediately see which domains are performing the best. You can also see how many times they ranked for each position. Then you can see aggregates (Total appearances, Coverage, and Avg. Pos.)  \n\nYou will also notice that I added the number of queries dynamically to the chart's title, as well as the list of queries that were used  in the SERP DataFrame (in this case we only have one keyword).  \n\nNow let's make it a function by putting all the code in sequence, and providing a few options to customize the chart:","20b5d032":"As you can see, the number of domains ranking for \"cafe\" is almost 2.5 times that of \"coffee\". I think it makes sense, because the fomer is more of a local keyword, and the geo-location makes a difference. So Google is probably giving more local domains for each country. On the other hand \"coffee\" is a generic term about the plant\/drink so the location doesn't play an important role. \n\nIt's also interesting to see that only fifteen domain are common. If you remove the local versions of some of those domains, you will notice that they are even less than that. \n\nWe now define some parameters to use in our visualization (later to be used in a function to create the vizualization)."}}