{"cell_type":{"e0dbc9cd":"code","ae0276fa":"code","54b22a45":"code","203d448f":"code","551f8e49":"code","7521a061":"code","ca045d20":"code","b8e7f22f":"code","1c1ba71a":"code","d7c1201c":"code","9b3aa2dc":"code","0be9c832":"code","f7487dfc":"code","b2e0e280":"markdown","3cf0e814":"markdown","a2ab36a0":"markdown","c89cf0fe":"markdown","0e75e055":"markdown","8ad50eb0":"markdown","bc8a6867":"markdown","dfa821ad":"markdown","4fc44e31":"markdown","5d94066a":"markdown","9cc16f24":"markdown","2be42ea9":"markdown","9deee1bf":"markdown","ef5b38b6":"markdown","5a4a28cd":"markdown","ccfc5182":"markdown","7440e92d":"markdown","63d42357":"markdown"},"source":{"e0dbc9cd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom textblob import Word\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae0276fa":"test = pd.read_csv('\/kaggle\/input\/job_description_dataset.csv')\n## Delete empty rows (In case I missed parsing a row)\ntest = test.dropna()\nprint(\"\\n ** raw data **\\n\")\nprint(test.head())\nprint(\"\\n ** data shape **\\n\")\nprint(test.shape)","54b22a45":"fig=plt.figure(figsize=(10, 5), dpi= 80, facecolor='w', edgecolor='k')\ntest.job_title.hist()","203d448f":"## Lower case\ntest['description'] = test['description'].apply(lambda x: \" \".join(x.lower()for x in x.split()))\n## remove tabulation and punctuation\ntest['description'] = test['description'].str.replace('[^\\w\\s]',' ')\n## digits\ntest['description'] = test['description'].str.replace('\\d+', '')\n\n#remove stop words\nstop = stopwords.words('english')\ntest['description'] = test['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n## lemmatization\ntest['description'] = test['description'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\nprint(\"Preprocessed data: \\n\")\nprint(test.head())","551f8e49":"## jda stands for job description aggregated\njda = test.groupby(['job_title']).sum().reset_index()\nprint(\"Aggregated job descriptions: \\n\")\nprint(jda)","7521a061":"## Visualize data\njobs_list = jda.job_title.unique().tolist()\nfor job in jobs_list:\n\n    # Start with one review:\n    text = jda[jda.job_title == job].iloc[0].description\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n    print(\"\\n***\",job,\"***\\n\")\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","ca045d20":"## Delete more stop words\nother_stop_words = ['junior', 'senior','experience','etc','job','work','company','technique',\n                    'candidate','skill','skills','language','menu','inc','new','plus','years',\n                   'technology','organization','ceo','cto','account','manager','data','scientist','mobile',\n                    'developer','product','revenue','strong']\n\ntest['description'] = test['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in other_stop_words))","b8e7f22f":"## Converting text to features \nvectorizer = TfidfVectorizer()\n#Tokenize and build vocabulary\nX = vectorizer.fit_transform(test.description)\ny = test.job_title\n\n# split data into 80% training and 20% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=109) \nprint(\"train data shape: \",X_train.shape)\nprint(\"test data shape: \",X_test.shape)\n\n# Fit model\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n## Predict\ny_predicted = clf.predict(X_test)","1c1ba71a":"y_train.hist()\ny_test.hist()","d7c1201c":"#evaluate the predictions\nprint(\"Accuracy score is: \",accuracy_score(y_test, y_predicted))\nprint(\"Classes: (to help read Confusion Matrix)\\n\", clf.classes_)\nprint(\"Confusion Matrix: \")\n\nprint(confusion_matrix(y_test, y_predicted))\nprint(\"Classification Report: \")\nprint(classification_report(y_test, y_predicted))","9b3aa2dc":"print(clf.coef_)\nprint(clf.coef_.shape)","0be9c832":"from textblob import TextBlob\ntechnical_skills = ['python', 'c','r', 'c++','java','hadoop','scala','flask','pandas','spark','scikit-learn',\n                    'numpy','php','sql','mysql','css','mongdb','nltk','fastai' , 'keras', 'pytorch','tensorflow',\n                   'linux','Ruby','JavaScript','django','react','reactjs','ai','ui','tableau']\nfeature_array = vectorizer.get_feature_names()\n# number of overall model features\nfeatures_numbers = len(feature_array)\n## max sorted features number\nn_max = int(features_numbers * 0.1)\n\n##initialize output dataframe\noutput = pd.DataFrame()\nfor i in range(0,len(clf.classes_)):\n    print(\"\\n****\" ,clf.classes_[i],\"****\\n\")\n    class_prob_indices_sorted = clf.feature_log_prob_[i, :].argsort()[::-1]\n    raw_skills = np.take(feature_array, class_prob_indices_sorted[:n_max])\n    print(\"list of unprocessed skills :\")\n    print(raw_skills)\n    \n    ## Extract technical skills\n    top_technical_skills= list(set(technical_skills).intersection(raw_skills))[:6]\n    #print(\"Top technical skills\",top_technical_skills)\n    \n    ## Extract adjectives\n    \n    # Delete technical skills from raw skills list\n    ## At this steps, raw skills list doesnt contain the technical skills\n    #raw_skills = [x for x in raw_skills if x not in top_technical_skills]\n    #raw_skills = list(set(raw_skills) - set(top_technical_skills))\n\n    # transform list to string\n    txt = \" \".join(raw_skills)\n    blob = TextBlob(txt)\n    #top 6 adjective\n    top_adjectives = [w for (w, pos) in TextBlob(txt).pos_tags if pos.startswith(\"JJ\")][:6]\n    #print(\"Top 6 adjectives: \",top_adjectives)\n    \n    output = output.append({'job_title':clf.classes_[i],\n                        'technical_skills':top_technical_skills,\n                        'soft_skills':top_adjectives },\n                       ignore_index=True)","f7487dfc":"print(output.T)","b2e0e280":"# Introduction\n\n\nThis project consists of finding a correlation between job descriptions and skills.\n\nWe will focus on the following jobs: Data Scientist- Mobile Developer- Account Manager- CTO- CEO","3cf0e814":"There are approximatively 30 rows for each job.\n\n**Our data is balanced** so let's move on to preprocessing it.","a2ab36a0":"I chose for this exercise to train the naive bayes algorithm.","c89cf0fe":"# Model evaluation and interpretation\n**Our accuracy score is 80% which is acceptable.**\n\n*NOTE:* Model accuracy dropped down after deleting the job titles from their respective descriptions. Which is expectable. ( If most job descriptions for CEO contain the word CEO, then the token CEO will be the most important feature for the class CEO)\n\nThis way our model will give more weight to other remaining\/meaningful tokens ","0e75e055":"I noticed the presence of meaningless words such as: Technology, Organization, Company.\nAs well as the presence of the job title itself.\n\nWe can safely delete these words from our data.","8ad50eb0":"# Read data\nThe following data was created manually.\n\nI searched for the job titles (CEO, Data scientist etc) needed for this exercise on different websites such as LinkedIn and Indeed. And I parsed the whole job description as it in a csv file.\n\nLet's start by reading this data.","bc8a6867":"## MODEL EVALUATION ","dfa821ad":"# Feature extraction\nLet's now extract the most meaningful features of each class.\n\nTo do so, we can access the attribute *feature_log_prob_* from our model which returns the log probability of features given a class.\n\nWe will next sort the log probabilies descendingly.\n\nAnd finally map the most important tokens to the classes\n","4fc44e31":"# Output\nAt this step, we have for each class\/job a list of the most representative words\/tokens found in job descriptions.\n\nLet's shrink this list of words to only:\n* 6 technical skills\n* 6 adjectives\n\nTo do so, we use the library *TextBlob* to identify adjectives.\n\nAlso, given a (non-exhaustive) list of programming languages, we can extract the top technical skills.\n","5d94066a":"This data contains 149 job descriptions and is structured into two columns: \n\n* job_title : for the job title.\n* description : raw text describing the job requirements.\n\nLet's now check if our data is balanced and therefore eligible to modeling.","9cc16f24":"Let's do a quick sanity check for the distribution of our train and test data.","2be42ea9":"# Preprocess text data\nSince the data we're now working with is at its rawest form, we need to preprocess it before extracting information from it.\n\nin this step, we will:\n* Convert all text to lower cases\n* Delete all tabulation,spaces, and new lines\n* Delete all numericals\n* Delete nltk's defined stop words \n* Lemmatize text","9deee1bf":"Correlation between jobs and skills:","ef5b38b6":"The confusion matrix shows that the features for the account manager, data scientist and mobile developer are differenciable. Therefore, we expect to extract meaningful features out of these classes.\n\nMeanwhile, 3 out of 8 CEO classes were classified as CTO. So there is a little confusion between CTO and CEO.\nAnd 2 out of 4 CTO classes were classified as Data Scientist and Mobile developer. I think this is due to the fact that in training data there was less CTO data than the rest.\n\n","5a4a28cd":"We reached acceptable accuracy with the basic model Naive Bayes.\n\nThis solution can be improved by:\n* adding a larger dataset and thus a larger training data for naive bayes algorithm\n* Extracting more accurate adjectives: library TextBlob that we used for this exercice has some inaccuracies when extracting adjectives. For example, it faulty considered the terms \"app\", \"web\" \"test\" as adjectives.\n* Experimenting with other models for better model accuracy score\n* Using bi-grams tokens and not only uni-grams ones. \n* Using technologies such as pyspark to make the data manipulation pipeline more scalable\n* Adding an exhaustive list of technologies","ccfc5182":"# Visualize data\nIn this step, **we will aggregate our data by job titles** in order to visualy detect the most frequent words for each job.","7440e92d":"# Modeling\nWe are now going to translate this skill-extraction problem into a classification one first.\nAnd then extract the most important features from each class.\n\nThe most important features, in this case, represent the words that most likely will belong to a class ( in our case job title) \n","63d42357":"# Conclusion"}}