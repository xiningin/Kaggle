{"cell_type":{"39497504":"code","bdc3e5d0":"code","de26624d":"code","a866aed2":"code","dfedb259":"code","c37196d1":"code","2dade157":"code","6a6c1fbf":"code","1a278125":"code","11518b75":"code","0bdb3f70":"code","64825f7b":"code","79cee684":"code","d934ea64":"code","1edb9442":"code","a7432cb9":"code","872da81b":"code","d1954fe3":"code","270d316a":"code","a9f29566":"code","2da56772":"markdown","94d5cf6a":"markdown","d45b5bac":"markdown","3790e56e":"markdown","d7c37369":"markdown","49b204f6":"markdown","8801ee19":"markdown","bccc8b1c":"markdown","79bbf379":"markdown","28837915":"markdown"},"source":{"39497504":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdc3e5d0":"df1 = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")\ndf2 = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","de26624d":"df1.info()","a866aed2":"df1.head()","dfedb259":"df1.isnull().sum()","c37196d1":"df2.info()","2dade157":"df2.head()","6a6c1fbf":"df2.isnull().sum()","1a278125":"df1.corr()","11518b75":"sns.heatmap(df1.corr(), annot = True, fmt = \".2f\")\nplt.show()","0bdb3f70":"pelvic_incidence = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\nsacral_slope = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","64825f7b":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n\n# Predict space\npredict_space = np.linspace(min(pelvic_incidence), max(pelvic_incidence)).reshape(-1,1)\n# Fit\nreg.fit(pelvic_incidence,sacral_slope)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(pelvic_incidence, sacral_slope))\n# Plot regression line and scatter\nplt.figure(figsize=(10,10))\nplt.plot(predict_space, predicted, color='red', linewidth=2)\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","79cee684":"x = (df1.iloc[:,[0,2]]).values # [pelvic_incidence,lumbar_lordosis_angle]\ny = df1.sacral_slope.values.reshape(-1,1)","d934ea64":"\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1,b2:\",multiple_linear_regression.coef_)\n\nmultiple_linear_regression.predict(np.array([[63.0278175 , 39.60911701],[40.47523153, 39.60911701]]))\n# first values of [[pelvic_incidence,lumbar_lordosis_angle],[sacral_slope,lumbar_lordosis_angle]]","1edb9442":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nlr.fit(x,y)\n# predict\ny_head = lr.predict(x)\n\nplt.plot(x,y_head,color=\"red\",label = \"linear\")\n\n# polynomial regression = y = b0 + b1*x + b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_regression = PolynomialFeatures(degree = 3)\n\nx_polynomial = polynomial_regression.fit_transform(x)\n# fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y) \n\n# visualize\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\n\nplt.plot(x,y_head2,color = \"green\", label = \"poly\")\nplt.legend()\nplt.show()","a7432cb9":"# normalization\nx = (x - np.min(x))\/(np.max(x)-np.min(x))\ny = (y - np.min(y))\/(np.max(y)-np.min(y))","872da81b":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.tree import DecisionTreeRegressor # random state = 0\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\n\ntree_reg.predict([[5.5]])\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n\ny_head = tree_reg.predict(x_)\n#%% visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","d1954fe3":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators = 40,random_state = 42)\n\nrf.fit(x,y)\n\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)\n\n#visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,y_head,color = \"red\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","270d316a":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nrf.fit(x,y)\n\ny_head = rf.predict(x)\n\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_score\", r2_score(y,y_head))","a9f29566":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression()\n\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head , color = \"red\")\n\n\n#%% \n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score: \", r2_score(y, y_head))","2da56772":"<a id=\"6\"><\/a> <br>\n## R Square with Random Forest Regression\n","94d5cf6a":"<a id=\"5\"><\/a> <br>\n## Decision Tree Regression","d45b5bac":"<a id=\"1\"><\/a> <br>\n## Loading Data and Explanation of Features\n* There are no missing values.\n<br>\n<font color=\"green\">\n* 'column_3C_weka.csv'\n    * has 7 features and 310 samples.\n    * 6 of them are in float data type and the other is object.,\n<br>\n<font color=\"blue\">\n* 'column_2C_weka.csv'\n    * has 7 features and 310 samples.\n    * 6 of them are in float data type and the other is object.","3790e56e":"<a id=\"4\"><\/a> <br>\n## Polinomial Linear Regression\n* y = b0 + b1*x + b2*x^2 + ... + b2*x^n\n\n\n* n = degree","d7c37369":"# Introduction\n* Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'features').  \n* The most common form of regression analysis is linear regression.\n* Linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables.\n\n    * [Loading Data and Explanation of Features](#1)\n    * [Simple Linear Regression](#2)\n    * [Multiple Linear Regression](#3)\n    * [Polinomial Linear Regression](#4)\n    * [Decision Tree Regression](#5)\n    * [Random Forest Regression](#6)\n    * [R Square with Random Forest Regression](#7)\n    * [R Square with Linear Regression](#8)","49b204f6":"### Correlation Between Features","8801ee19":"<a id=\"7\"><\/a> <br>\n## R Square with Linear Regression","bccc8b1c":"<a id=\"5\"><\/a> <br>\n## Random Forest Regression\n* n_ estimator = number of tree, \n* random_state = Count random values according to randon_state.","79bbf379":"<a id=\"3\"><\/a> <br>\n## Multiple Linear Regression\n* Purpose : Minimum MSE","28837915":"<a id=\"2\"><\/a> <br>\n## Simple Linear Regression\n\n* residual = y - y_head (In MSE, we take square of residual. Residual values can be negative and positive, in this situation their summation can be zero and we can lost error values. Reason of that, we take square of residual)\n* y = real values of y\n* y_head = predicted values of y\n* b0 = constant - bias (The point where it intersects the y axis)\n* b1 = coefficient - slope\n* y = b0 + b1*x\n\n### Mean Squared Error\n* n = sample size\n\n* MSE = 1\/n(sum(residual^2)\/n"}}