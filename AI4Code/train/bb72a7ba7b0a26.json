{"cell_type":{"925e190b":"code","50da8c5d":"code","0954d584":"code","b01383fb":"code","dafa1b60":"code","f5d1737c":"code","14158493":"code","87d1f7e5":"code","8e4708cf":"code","6876fb90":"code","212568d6":"code","338c8fad":"code","6d3f65a5":"code","13774bca":"code","dd7f5d3d":"code","19fa90eb":"code","aee125a7":"code","a66074b7":"code","4e4403e5":"code","bae79b61":"markdown","943a16c2":"markdown","23bf8153":"markdown","26090bbe":"markdown","7f972e68":"markdown","8766fbca":"markdown","980abe57":"markdown","df394550":"markdown","ed1b8c2a":"markdown","c4ef3647":"markdown","766ddd29":"markdown"},"source":{"925e190b":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\nimport math\nwarnings.filterwarnings('ignore')","50da8c5d":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)","0954d584":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","b01383fb":"########################### Model params\n# These parameters we will keep untouched\n# for each lgbm model\n# the unique param that we will look at\n# is n_estimators\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':20000,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","dafa1b60":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('..\/input\/ieee-data-minification\/train_transaction.pkl')\n\n# We will prepare simulation here\n# Last month will be our test test \ntrain_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n\ntest_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\ntrain_df = train_df[train_df['DT_M']<(train_df['DT_M'].max())].reset_index(drop=True)\n    \nprint('Shape control:', train_df.shape, test_df.shape)","f5d1737c":"########################### Encode Str columns\n# For all such columns (probably not)\n# we already did frequency encoding (numeric feature)\n# so we will use astype('category') here\nfor col in list(train_df):\n    if train_df[col].dtype=='O':\n        print(col)\n        train_df[col] = train_df[col].fillna('unseen_before_label')\n        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')","14158493":"########################### Model Features \n# Remove Some Features\nrm_cols = [\n    'TransactionID','TransactionDT', # These columns are pure noise right now\n    TARGET,                          # Not target in features))\n    'DT_M'                           # Column that we used to simulate test set\n]\n\n# Remove V columns (for faster training)\nrm_cols += ['V'+str(i) for i in range(1,340)]\n\n# Final features\nfeatures_columns = [col for col in list(train_df) if col not in rm_cols]\n\n## Baseline LB score is 0.9360","87d1f7e5":"## Let's creat dataframe to compare results\n## We will join prepdictions\nRESULTS = test_df[['TransactionID',TARGET]]\n\n# We will always use same number of splits\n# for training model\n# Number of splits depends on data structure\n# and in our case it is better to use \n# something in range 5-10\n# 5 - is a common number of splits\n# 10+ is too much (we will not have enough diversity in data)\n# Here we will use 3 for faster training\n# but you can change it by yourself\nN_SPLITS = 3","8e4708cf":"# Main Data\n# We will take whole train data set\n# and will NOT use any early stopping \nX,y = train_df[features_columns], train_df[TARGET]\n\n# Test Data (what we need to predict)\nP = test_df[features_columns]\n\n# We don't know where to stop\n# so we will try to guess \n# number of boosting rounds\nfor n_rounds in [500,1000,2500,5000]:\n    print('#'*20)\n    print('No Validation training...', n_rounds, 'boosting rounds')\n    corrected_lgb_params = lgb_params.copy()\n    corrected_lgb_params['n_estimators'] = n_rounds\n    corrected_lgb_params['early_stopping_rounds'] = None\n\n    train_data = lgb.Dataset(X, label=y)\n    \n    estimator = lgb.train(\n                corrected_lgb_params,\n                train_data\n            )\n\n    RESULTS['no_validation_'+str(n_rounds)] = estimator.predict(P)\n    print('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['no_validation_'+str(n_rounds)]))\n    print('#'*20)\n\n# Be careful. We are printing auc results\n# for our simulated test set\n# but in real Data set we do not have True labels (obviously)\n# and can't be sure that we stopped in right round\n# lb probing can give you some idea how good our training is\n# but this leads to nowhere -> overfits or completely bad results\n# bad practice for real life problems!","6876fb90":"print('#'*20)\nprint('KFold training...')\n\n# You can find oof name for this strategy\n# oof - Out Of Fold\n# as we will use one fold as validation\n# and stop training when validation metric\n# stops improve\nfrom sklearn.model_selection import KFold\nfolds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\n\n# Test Data\nP = test_df[features_columns]\nRESULTS['kfold'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=v_y)  \n\n    estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n\n    RESULTS['kfold'] = estimator.predict(P)\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['kfold']))\nprint('#'*20)\n    \n## We have two \"problems\" here\n## 1st: Training score goes upto 1 and it's not normal situation\n## It's nomally means that model did perfect or\n## almost perfect match between \"data fingerprint\" and target\n## we definitely should stop before to generalize better\n## 2nd: Our LB probing gave 0.936 and it is too far away from validation score\n## some difference is normal, but such gap is too big","212568d6":"print('#'*20)\nprint('StratifiedKFold training...')\n\n# Same as normal kfold but we can be sure\n# that our target is perfectly distribuited\n# over folds\nfrom sklearn.model_selection import StratifiedKFold\nfolds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\n\n# Test Data and expport DF\nP = test_df[features_columns]\nRESULTS['stratifiedkfold'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=y)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=v_y)  \n\n    estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n\n    # we are not sure what fold is best for us\n    # so we will average prediction results \n    # over folds\n    RESULTS['stratifiedkfold'] += estimator.predict(P)\/N_SPLITS\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['stratifiedkfold']))\nprint('#'*20)\n\n## We have same \"problems\" here as in normal kfold\n## 1st: Training score goes upto 1 and it's not normal situation\n## we definitely should stop before \n## 2nd: Our LB probing gave 0.936 and it is too far away from validation score\n## some difference is normal, but such gap is too big","338c8fad":"print('#'*20)\nprint('LBO training...') \n\n## We need Divide Train Set by Time blocks\n## Convert TransactionDT to Months\n## And use last month as Validation\ntrain_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n\nmain_train_set = train_df[train_df['DT_M']<(train_df['DT_M'].max())].reset_index(drop=True)\nvalidation_set = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n\n## We will use oof kfold to find \"best round\"\nfolds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# Main Data\nX,y = main_train_set[features_columns], main_train_set[TARGET]\n\n# Validation Data\nv_X, v_y = validation_set[features_columns], validation_set[TARGET]\n\nestimators_bestround = []\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(v_X, label=v_y)  \n\n    estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n    estimators_bestround.append(estimator.current_iteration())\n\n## Now we have \"mean Best round\" and we can train model on full set\ncorrected_lgb_params = lgb_params.copy()\ncorrected_lgb_params['n_estimators'] = int(np.mean(estimators_bestround))\ncorrected_lgb_params['early_stopping_rounds'] = None\nprint('#'*10)\nprint('Mean Best round:', corrected_lgb_params['n_estimators'])\n\n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\n\n# Test Data\nP = test_df[features_columns]\nRESULTS['lbo'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n    train_data = lgb.Dataset(tr_x, label=tr_y)\n\n    estimator = lgb.train(\n            corrected_lgb_params,\n            train_data\n        )\n    \n    RESULTS['lbo'] += estimator.predict(P)\/N_SPLITS\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['lbo']))\nprint('#'*20)   ","6d3f65a5":"print('#'*20)\nprint('GroupKFold timeblocks split training...') \n\nfrom sklearn.model_selection import GroupKFold\nfolds = GroupKFold(n_splits=N_SPLITS)\n\n## We need Divide Train Set by Time blocks\n## Convert TransactionDT to Months\ntrain_df['groups'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain_df['groups'] = (train_df['groups'].dt.year-2017)*12 + train_df['groups'].dt.month \n\n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\nsplit_groups = train_df['groups']\n\n# Test Data and expport DF\nP = test_df[features_columns]\nRESULTS['groupkfold_timeblocks'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=v_y)  \n\n    estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n\n    RESULTS['groupkfold_timeblocks'] += estimator.predict(P)\/N_SPLITS\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['groupkfold_timeblocks']))\nprint('#'*20)","13774bca":"print('#'*20)\nprint('GroupKFold uID split training...') \n\nfrom sklearn.model_selection import GroupKFold\nfolds = GroupKFold(n_splits=N_SPLITS)\n\n## We need Divide Train Set by virtual client ID\n## If we do everuthing well\n## (I'm not sure that this columns are good ones\n## 'card1','card2','card3','card5','addr1','addr2')\n## our model will not have \"personal client information\"\n## shared by folds\n\ntrain_df['groups'] = ''\nfor col in ['card1','card2','card3','card5','addr1','addr2',]:\n    train_df['groups'] = '_' + train_df[col].astype(str)\n    \n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\nsplit_groups = train_df['groups']\n\n# Test Data and expport DF\nP = test_df[features_columns]\nRESULTS['groupkfold_uid'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=v_y)  \n\n    estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n\n    RESULTS['groupkfold_uid'] += estimator.predict(P)\/N_SPLITS\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['groupkfold_uid']))\nprint('#'*20)","dd7f5d3d":"print('#'*30)\nprint('Intermediate results...')\nfinal_df = []\nfor current_strategy in list(RESULTS.iloc[:,2:]):\n    auc_score = metrics.roc_auc_score(RESULTS[TARGET], RESULTS[current_strategy])\n    final_df.append([current_strategy, auc_score])\n    \nfinal_df = pd.DataFrame(final_df, columns=['Stategy', 'Result'])\nfinal_df.sort_values(by=['Result'], ascending=False, inplace=True)\nprint(final_df)","19fa90eb":"print('#'*20)\nprint('LBO full set training...') \n\n## We need Divide Train Set by Time blocks\n## Convert TransactionDT to Months\n## And use last month as Validation \n## to find best round\ntrain_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n\nmain_train_set = train_df[train_df['DT_M']<(train_df['DT_M'].max())].reset_index(drop=True)\nvalidation_set = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n\n# Main Data\nX,y = main_train_set[features_columns], main_train_set[TARGET]\n\n# Validation Data\nv_X, v_y = validation_set[features_columns], validation_set[TARGET]\n\nestimators_bestround = []\n\nfor current_model in range(3):\n    print('Model:',current_model+1)\n    SEED += 1\n    seed_everything(SEED)\n    corrected_lgb_params = lgb_params.copy()\n    corrected_lgb_params['seed'] = SEED\n\n    train_data = lgb.Dataset(X, label=y)\n    valid_data = lgb.Dataset(v_X, label=v_y)  \n\n    estimator = lgb.train(\n            corrected_lgb_params,\n            train_data,\n            valid_sets = [train_data, valid_data],\n            verbose_eval = 1000,\n        )\n    estimators_bestround.append(estimator.current_iteration())\n\n## Now we have \"mean Best round\" and we can train model on full set\ncorrected_lgb_params = lgb_params.copy()\ncorrected_lgb_params['n_estimators'] = int(np.mean(estimators_bestround))\ncorrected_lgb_params['early_stopping_rounds'] = None\nprint('#'*10)\nprint('Mean Best round:', corrected_lgb_params['n_estimators'])\n\n# Main Data\nX,y = train_df[features_columns], train_df[TARGET]\n\n# Test Data\nP = test_df[features_columns]\nRESULTS['lbo_full'] = 0\nNUMBER_OF_MODELS = 3\n\nfor current_model in range(NUMBER_OF_MODELS):\n    print('Model:',current_model+1)\n    SEED += 1\n    seed_everything(SEED)    \n    train_data = lgb.Dataset(X, label=y)\n\n    estimator = lgb.train(\n            corrected_lgb_params,\n            train_data\n        )\n    \n    RESULTS['lbo_full'] += estimator.predict(P)\/NUMBER_OF_MODELS\n\nprint('AUC score', metrics.roc_auc_score(RESULTS[TARGET], RESULTS['lbo_full']))\nprint('#'*20)   ","aee125a7":"print('#'*30)\nprint('Intermediate results...')\nfinal_df = []\nfor current_strategy in list(RESULTS.iloc[:,2:]):\n    auc_score = metrics.roc_auc_score(RESULTS[TARGET], RESULTS[current_strategy])\n    final_df.append([current_strategy, auc_score])\n    \nfinal_df = pd.DataFrame(final_df, columns=['Stategy', 'Result'])\nfinal_df.sort_values(by=['Result'], ascending=False, inplace=True)\nprint(final_df)","a66074b7":"test_df['DT_W'] = test_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nRESULTS['DT_W'] = (test_df['DT_W'].dt.year-2017)*52 + test_df['DT_W'].dt.weekofyear \n\nfor curent_time_block in range(RESULTS['DT_W'].min(), RESULTS['DT_W'].max()+1):\n    print('#'*20)\n    print('Time Block:', curent_time_block)\n    final_df = []\n    temp_df = RESULTS[RESULTS['DT_W']==curent_time_block]\n    for current_strategy in list(temp_df.iloc[:,2:]):\n        auc_score = metrics.roc_auc_score(temp_df[TARGET], temp_df[current_strategy])\n        final_df.append([current_strategy, auc_score])\n    \n    final_df = pd.DataFrame(final_df, columns=['Stategy', 'Result'])\n    final_df.sort_values(by=['Result'], ascending=False, inplace=True)\n    print(final_df)\n    print('#'*30)\n    \n# Naive analize.\n# But we can see temporal auc degradation\n# Probably for test set with larger monthly gap\n# from training set we need to use less boosting rounds (or more).","4e4403e5":"print('#'*30)\nprint('Small bonus')\n\n## Blend models with different input features\n## with identity and without\ntest_df = pd.read_pickle('..\/input\/ieee-data-minification\/test_transaction.pkl')\n\nkernel_with_identity = pd.read_csv('..\/input\/ieee-gb-2-make-amount-useful-again\/submission.csv')\nkernel_no_identity = pd.read_csv('..\/input\/ieee-experimental\/submission.csv')\n\ntest_df = test_df[['TransactionID']]\ntest_df['isFraud'] = kernel_with_identity['isFraud'] + kernel_no_identity['isFraud']\ntest_df[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","bae79b61":"----\n## We need to go deeper\nAs we see kfold doesn't work well. \nBut why are we still using kfold options even with lbo?\nLet's combine no_validation options and lbo. We will not use kfold splits but several lgbm models with diffrent seeds.","943a16c2":"----\n## Results Analise","23bf8153":"----\n## GroupKFold\n> The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.\n\nWhy we may use it?\nLet's imagine that we want to separate train data by time blocks groups or client IDs or something else.\nWith GroupKFold we can be sure that our validation fold will contain groupIDs that are not in main train set.\nSometimes it helps to deal with \"dataleakage\" and overfit.","26090bbe":"----\nYou may think that training without validation is a best option.\nBut it's not true. Let's simulate small gap in values between train and test sets.\nhttps:\/\/www.kaggle.com\/kyakovlev\/ieee-cv-options-with-gap\n\n                Stategy    Result\n                    lbo  0.913454\n     no_validation_1000  0.913336\n        stratifiedkfold  0.911419\n      no_validation_500  0.911041\n     no_validation_2500  0.910774\n     no_validation_5000  0.908515\n                  kfold  0.906118\n                  \n                  \nAny \"manual\" strategy is normaly (99.9%) worse than good CV option. ","7f972e68":"----\n## StratifiedKFold\nThere are situations when normal kfold split doesn't perform well because of train set imbalance.\nWe can use StratifiedKFold to garant that each split will have same number of positives and negatives samples.","8766fbca":"## CV concept\n\n### Basics\n\n> Cross-validation is a technique for evaluating ML models \n> by training several ML models on subsets of the available \n> input data and evaluating them on the complementary \n> subset of the data. \n\n> In k-fold cross-validation, you split the input data \n> into k subsets of data (also known as folds).\n\n\n### Main strategy\n1. Divide Train set in subsets (Training set itself + Validation set)\n2. Define Validation Metric (in our case it is ROC-AUC)\n3. Stop training when Validation metric stops improving\n4. Make predictions for Test set\n\nSeems simple but he devil's always in the details.","980abe57":"----\n## Intermediate results","df394550":"----\n## No Validation","ed1b8c2a":"----\n## Final results","c4ef3647":"----\n## LBO (last block out)\nFor Time series data (what we have here) we can use (sometimes) last Time block as validation subset and track mean early stopping round.\n\nLet's code it.","766ddd29":"----\n## Kfold"}}