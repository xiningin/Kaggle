{"cell_type":{"8599a7b6":"code","fd1a2b09":"code","c5f93718":"code","08dbc36d":"code","333f5962":"code","963ea561":"code","999a9b3b":"code","2c24ee2f":"code","3b6b2524":"code","f862d565":"code","d91abe92":"code","84bb7c28":"code","a720d5ca":"code","d9cf13f1":"code","0c79eec4":"code","107a4962":"code","ecebdf36":"code","7aaa8d1a":"code","b195cb17":"code","4435a277":"code","007a6a41":"code","ae1e9f71":"code","4caeebc6":"code","584e450b":"code","6a947280":"code","dc9ad00c":"code","28536022":"code","aaf486a9":"code","4e74e388":"code","903eb060":"code","668ab996":"code","0c42248a":"code","50fc0ad8":"code","055b463b":"code","a6bf3d51":"code","65538ade":"code","b5a90130":"code","348ca72b":"code","6f26177d":"code","e45cec29":"code","3a6a0fd8":"code","80dfbdd7":"code","a79c4e08":"code","0a578ebe":"code","d363087b":"code","e063ab0c":"code","ed33fb75":"code","7e14dc76":"code","4b28ab9f":"code","b2c5a630":"code","142d031b":"code","8453a69d":"code","4943f2f3":"code","f883be74":"code","6f378fd6":"code","87240b87":"code","fe3685ac":"code","73470a55":"code","44b7ee58":"code","5313ef83":"code","079a93e7":"code","fa9cdd3c":"code","535cbb47":"code","b2c48db0":"code","d6c14374":"code","62fdea5d":"code","fc8872fb":"code","7898b29d":"code","b74afa29":"markdown","3be6baba":"markdown","629e2584":"markdown","042261c1":"markdown","2fce7e01":"markdown","737707e3":"markdown","fa88297f":"markdown","0bcc614d":"markdown","efa93f62":"markdown","7e27bf3e":"markdown","593b244e":"markdown","86f10ca6":"markdown","0391e73a":"markdown","386a3d92":"markdown","3e033e0f":"markdown","210ad4ea":"markdown","cae940f3":"markdown","e9ea27f1":"markdown","681c6aab":"markdown","650fcb28":"markdown","888137cc":"markdown","4df60521":"markdown","1f1f6a16":"markdown","390e7d68":"markdown","4bee168a":"markdown","f32ed5a4":"markdown","4a06eefe":"markdown","3096c2c2":"markdown","120248e9":"markdown","bfad04fd":"markdown","492e4640":"markdown","c38d5aa8":"markdown","764cfd68":"markdown","b74e28e2":"markdown","103cde8e":"markdown","87578d53":"markdown"},"source":{"8599a7b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport scikitplot as skplt","fd1a2b09":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5f93718":"data=pd.read_csv('\/kaggle\/input\/airplane-accidents-severity-dataset\/train.csv')","08dbc36d":"data.head()","333f5962":"data.dtypes","963ea561":"data.info()","999a9b3b":"data.shape","2c24ee2f":"na_values=data.isna().sum()\nprint(na_values)","3b6b2524":"data.drop(['Accident_ID'],axis=1,inplace=True)\ndata.describe().T","f862d565":"# Distribution of continous data\n\n# Safety_Score, Control_Metric,Turbulence_In_gforce\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Safety_Score')\nsns.distplot(data['Safety_Score'],color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Control_Metric')\nsns.distplot(data['Control_Metric'],color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Turbulence_In_gforces')\nsns.distplot(data['Turbulence_In_gforces'],color='green')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('Safety_Score')\nsns.boxplot(data['Safety_Score'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Control_Metric')\nsns.boxplot(data['Control_Metric'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Turbulence_In_gforces')\nsns.boxplot(data['Turbulence_In_gforces'],orient='horizondal',color='green')\n","d91abe92":"# Distribution of continous data\n# Cabin_Temprature, Max_Elevation, Adverse_Weather_Metric\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Cabin_Temperature')\nsns.distplot(data['Cabin_Temperature'],color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Max_Elevation')\nsns.distplot(data['Max_Elevation'],color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Adverse_Weather_Metric')\nsns.distplot(data['Adverse_Weather_Metric'],color='green')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('Cabin_Temperature')\nsns.boxplot(data['Cabin_Temperature'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Max_Elevation')\nsns.boxplot(data['Max_Elevation'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Adverse_Weather_Metric')\nsns.boxplot(data['Adverse_Weather_Metric'],orient='horizondal',color='green')\n","84bb7c28":"##### Days_Since_Inspection, Total_Safety_Compliant\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,2,1)\nplt.title('Days_Since_Inspection')\nsns.countplot(data['Days_Since_Inspection'],color='red')\n\n#Subplot 2\nplt.subplot(1,2,2)\nplt.title('Total_Safety_Complaints')\nsns.countplot(data['Total_Safety_Complaints'],color='blue')\n","a720d5ca":"# Accident_Type_Code and Violations\n\nplt.figure(figsize=(30,6))\n\n\n#Subplot 1\nplt.subplot(1,2,1)\nplt.title('Accident_Type_Code')\nsns.countplot(data['Accident_Type_Code'],color='red')\n\n#Subplot 2\nplt.subplot(1,2,2)\nplt.title('Violations')\nsns.countplot(data['Violations'],color='blue')","d9cf13f1":"plt.figure(figsize=(30,6))\n\nplt.title('Severity')\nsns.countplot(data['Severity'],color='red')","0c79eec4":"sns.pairplot(data,palette=\"Set2\", diag_kind=\"kde\", height=2.5)","107a4962":"correlation=data.corr()\ncorrelation.style.background_gradient(cmap='coolwarm')","ecebdf36":"data.corr()>0.5","7aaa8d1a":"data.corr()<-0.5","b195cb17":"df=data.drop(['Severity'],axis=1)\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=20):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(df, 3))","4435a277":"data.info()","007a6a41":"dataNumericals = pd.DataFrame(data, columns =data.columns[data.dtypes == 'float64']) \ndataNumericals.head()","ae1e9f71":"dataNumericals=dataNumericals.apply(zscore)","4caeebc6":"dataNumericals.head()","584e450b":"floats = dataNumericals.columns[dataNumericals.dtypes == 'float64']\nfor columns in floats:\n    indexNames_larger = dataNumericals[dataNumericals[columns]>3].index\n    indexNames_lesser = dataNumericals[dataNumericals[columns]<-3].index\n    # Delete these row indexes from dataFrame\n    dataNumericals.drop(indexNames_larger , inplace=True)\n    dataNumericals.drop(indexNames_lesser , inplace=True)\n    data.drop(indexNames_larger , inplace=True)\n    data.drop(indexNames_lesser , inplace=True)","6a947280":"dataNumericals.info()","dc9ad00c":"data.info()","28536022":"data.drop(data.columns[data.dtypes == 'float64'],axis=1,inplace=True)","aaf486a9":"data.head()","4e74e388":"for column in dataNumericals.columns:\n    data[column]=dataNumericals[column]","903eb060":"data.head()","668ab996":"data.info()","0c42248a":"data['Severity'].unique()","50fc0ad8":"encoder=LabelEncoder()\ndata['Severity']=encoder.fit_transform(data['Severity'])","055b463b":"data.head()","a6bf3d51":"data['Total_Safety_Complaints'] = np.power(2, data['Total_Safety_Complaints'])\ndata['Days_Since_Inspection'] = np.power(2, data['Days_Since_Inspection'])\ndata['Safety_Score'] = np.power(2, data['Safety_Score'])","65538ade":"X=data.drop(['Severity'],axis=1)","b5a90130":"Y=data['Severity']","348ca72b":"Xtrain_val,X_test,ytrain_val,Y_test=train_test_split(X,Y,test_size=0.2,random_state=22)","6f26177d":"kf = KFold(n_splits=10,random_state=2,shuffle=True)\nkf.get_n_splits(Xtrain_val)\nprint(kf)\n\n\nfor train_index, val_index in kf.split(Xtrain_val):\n    print(\"TRAIN:\", train_index, \"VALIDATION:\", val_index)\n    X_train, X_val = Xtrain_val.iloc[train_index], Xtrain_val.iloc[val_index]\n    y_train, y_val = ytrain_val.iloc[train_index], ytrain_val.iloc[val_index]","e45cec29":"#Pipeline\npipe_GBR = Pipeline([('GBR', GradientBoostingClassifier())]) \n\n#Parameter-grid\nparam_grid = {'GBR__n_estimators': [50,100,150],'GBR__learning_rate':[0.1,0.2,0.5]} \n \n#Using RandomSearchCV\nRandom_GBR = RandomizedSearchCV( pipe_GBR , param_distributions=param_grid, cv= 5, n_iter=3) \n\n#Fitting the data in the model\nRandom_GBR.fit(X_train, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( Random_GBR.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", Random_GBR.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( Random_GBR.score( X_train, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( Random_GBR.score( X_val, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( Random_GBR.score( X_test, Y_test)))","3a6a0fd8":"y_pred=Random_GBR.predict(X_test)","80dfbdd7":"accuracy_score=metrics.accuracy_score(Y_test,y_pred)\npercision_score=metrics.precision_score(Y_test,y_pred,average='macro')\nrecall_score=metrics.recall_score(Y_test,y_pred,average='macro')\nf1_score=metrics.f1_score(Y_test,y_pred,average='macro')\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percision of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The f1 score of this model is {0:.2f}%\".format(f1_score*100))","a79c4e08":"Random_GBR.cv_results_","0a578ebe":"classification_report=metrics.classification_report(Y_test,y_pred)","d363087b":"print(classification_report)","e063ab0c":"#Pipeline\npipe_XGB = Pipeline([('XGB', XGBClassifier())]) \n\n#Parameter-grid\nparam_grid = {'XGB__learning_rate':[0.1,0.2,0.3],'XGB__max_depth' :[10,50,100], 'XGB__gamma':[0.1,0.3,0.5]} \n \n#Using RandomSearchCV\nRandom_XGB = RandomizedSearchCV( pipe_XGB , param_distributions=param_grid, cv= 5, n_iter=3) \n#Fitting the data in the model\nRandom_XGB.fit(X_train, y_train)\n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( Random_XGB.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", Random_XGB.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( Random_XGB.score( X_train, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( Random_XGB.score( X_val, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( Random_XGB.score( X_test, Y_test)))","ed33fb75":"y_pred=Random_XGB.predict(X_test)","7e14dc76":"accuracy_score=metrics.accuracy_score(Y_test,y_pred)\npercision_score=metrics.precision_score(Y_test,y_pred,average='macro')\nrecall_score=metrics.recall_score(Y_test,y_pred,average='macro')\nf1_score=metrics.f1_score(Y_test,y_pred,average='macro')\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percision of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The f1 score of this model is {0:.2f}%\".format(f1_score*100))","4b28ab9f":"Random_XGB.cv_results_","b2c5a630":"classification_report=metrics.classification_report(Y_test,y_pred)","142d031b":"print(classification_report)","8453a69d":"skplt.metrics.plot_confusion_matrix(Y_test,y_pred,figsize=(12,12))","4943f2f3":"testData=pd.read_csv(\"\/kaggle\/input\/airplane-accidents-severity-dataset\/test.csv\")","f883be74":"testData.drop(['Accident_ID'],axis=1,inplace=True)\ntestData.head()","6f378fd6":"testData.info()","87240b87":"testDataNumericals = pd.DataFrame(testData, columns =testData.columns[testData.dtypes == 'float64']) \ntestDataNumericals.head()","fe3685ac":"testDataNumericals=testDataNumericals.apply(zscore)","73470a55":"testData.drop(testData.columns[testData.dtypes == 'float64'],axis=1,inplace=True)\ntestData.head()","44b7ee58":"for column in testDataNumericals.columns:\n    testData[column]=testDataNumericals[column]","5313ef83":"testData.head()","079a93e7":"testData['Total_Safety_Complaints'] = np.power(2, testData['Total_Safety_Complaints'])\ntestData['Days_Since_Inspection'] = np.power(2, testData['Days_Since_Inspection'])\ntestData['Safety_Score'] = np.power(2, testData['Safety_Score'])","fa9cdd3c":"testPredictions=Random_XGB.predict(testData)","535cbb47":"testData['Severity']=encoder.inverse_transform(testPredictions)","b2c48db0":"testData.head()","d6c14374":"finalData=pd.read_csv(\"\/kaggle\/input\/airplane-accidents-severity-dataset\/test.csv\")","62fdea5d":"finalData['Severity']=testData['Severity']","fc8872fb":"finalData.head()","7898b29d":"finalData.to_csv('test.csv')","b74afa29":"##### Safety_Score, Control_Metric,Turbulence_In_gforces,Cabin_Temprature,Max_Elevation,Adverse_Weather_Metric are continous in nature\n##### Severity, Days_Since_Inspection, Total_Safety_Compliants, Accident_Type_Code and Violations are Discreet categorical data","3be6baba":"#### Uni-Variate Analysis","629e2584":"##### Cabin_Temprature is skewed to the right but normally distributed with outliers\n##### Max_Elevation is t normally distributed with outliers\n##### Adverse_Weather_Metric is highly skewed to the right with huge number of outliers ","042261c1":"##### Safety score is normally distributed  and has few outliers\n##### Control_Metric is skewed to the left but normally distributed with outliers\n##### Turbulence_In_gForce is skewed to the left but normally distributed with outliers ","2fce7e01":"##### \u25cf Acciedent_ID : This is the unique ID of the accident as recorded by the authorities\n##### \u25cf Adverse_Weather_Metric : measured weather metric in the basis of the adverse events occured\n##### \u25cf Violations: number of violations that the aircraft received during inspections\n##### \u25cf Max_Elevation : maximim altitude the airplane has reached during the event\n##### \u25cf Accident_Type_Code : Code of the accident classified by the authorities\n##### \u25cf Cabin_Temperature: the last recorded temperature before the incident, measured in degrees Fahrenheit\n##### \u25cf Turbulence_In_gforces :the recorded\/estimated turbulence experienced during the accident\n##### \u25cf Control_Metric : an estimation of how much control the pilot had during the incident given the factors at play\n##### \u25cf Total_Safety_Complaints:number of complaints from mechanics prior to the accident\n##### \u25cf Days_Since_Inspection:how long the plane went without inspection before the incident\n##### \u25cf Safety_Score: a measure of how safe the plane was deemed to be","737707e3":"#### Checking the data-types of the data","fa88297f":"#### Splitting X-independent attributes and Y-dependent attributes and keeping the test set seperate\n#### Creating multiple cross-validation to reduce overfitting","0bcc614d":"##### Mean of the following attributes are not in sync with the median which implies the presence of outliers\n###### Total_Safety_Complaints, Violations, Adverse_Weather_Metric","efa93f62":"#### Correlation Matrix","7e27bf3e":"### Reading and understanding the data-set","593b244e":"#####  Accident_Type_Code 5 has fewer number of records\n##### 2 violations has the highest number of records","86f10ca6":"#### Handle Outliers","0391e73a":"##### There are 10000 records with 11 independent variables and 1 target variable","386a3d92":"##### These pairs of independent attibutes have good correlation","3e033e0f":"### Exploratory Data Analytics","210ad4ea":"#### Applying z-score to scale the data and standardize the data\n","cae940f3":"#### There is a slight class imbalance with 'Significant_Damage_And_Fatalities' group having fewer number of records","e9ea27f1":"### Problem Statement: \nHow severe can an airplane accident be?\nFlying has been the go-to mode of travel for years now; it is timesaving, affordable, and extremely convenient. According to the FAA, 2,781,971 passengers fly every day in the US, as in June 2019. Passengers reckon that flying is very safe, considering strict inspections are conducted and security measures are taken to avoid and\/or mitigate any mis happenings. However, there remain a few chances of unfortunate incidents.\nImagine you have got a project from leading airline. You are required to build Machine Learning models to anticipate and classify the severity of an airplane accident based on past incidents. With this, all airlines, even the entire aviation industry, can predict the severity of airplane accidents caused due to various factors and, correspondingly, have a plan of action to minimize the risk associated with them.\n\n### Data:\nThe dataset comprises 2 files (Data is shared in the same folder) : \n\u25cf\tTrain.csv: [10000 x 12 excluding the headers] contains Training data\n\u25cf\tTest.csv: [2500 x 11 excluding the headers] contains Test data\n#### Columns\tDescription\nAccident_ID\tunique id assigned to each row\nAccident_Type_Code\tthe type of accident (factor, not numeric)\nCabin_Temperature\tthe last recorded temperature before the incident, measured in degrees Fahrenheit\nTurbulence_In_gforces\tthe recorded\/estimated turbulence experienced during the accident\nControl_Metric\tan estimation of how much control the pilot had during the incident given the factors at play\nTotal_Safety_Complaints\tnumber of complaints from mechanics prior to the accident\nDays_Since_Inspection\thow long the plane went without inspection before the incident\nSafety_Score\ta measure of how safe the plane was deemed to be\nViolations\tnumber of violations that the aircraft received during inspections\nSeverity\ta description (4 level factor) on the severity of the crash [Target]\n\n\u2003\n \n### Solution Approach:\n#### Libraries Used:\n\u2022\tPandas for data manipulation\n\u2022\tNumPy for performing mathematical operations on the data\n\u2022\tMatplotlib, Seaborn and Scikitplot for visualization\n\u2022\tSklearn for pre-processing and model building & evaluation\n#### Steps and Inferences:\n1.\tData was read and loaded into a DataFrame and first few records were visualized\n2.\tThe dataTypes were understood and we could infer the following\na.\tSafety_Score, Control_Metric, Turbulence_In_gforces, Cabin_Temprature, Max_Elevation and Adverse_Weather_Metric are continuous in nature\nb.\tSeverity, Days_Since_Inspection, Total_Safety_Compliants, Accident_Type_Code and Violations are Discreet categorical data\n3.\tThere were no null values present in the dataset\n4.\tFive point summary was performed on the continuous data and following were observed\na.\tMean of the following attributes are not in sync with the median which implies the presence of outliers\ni.\tTotal_Safety_Complaints\nii.\tViolations\niii.\tAdverse_Weather_Metric\n5.\tExploratory Data Analysis was performed starting with univariate analysis and following were observed\na.\tSafety score is normally distributed  and has few outliers\nb.\tControl_Metric is skewed to the left but normally distributed with outliers\nc.\tTurbulence_In_gForce is skewed to the left but normally distributed with outliers\nd.\tCabin_Temprature is skewed to the right but normally distributed with outliers\ne.\tMax_Elevation is t normally distributed with outliers\nf.\tAdverse_Weather_Metric is highly skewed to the right with huge number of outliers\ng.\tAverage Days_Since_Inspection is 13\nh.\tnumber of records with Total_Safety_Compliants as 0-10 is high\ni.\tAccident_Type_Code 5 has fewer number of records\nj.\t2 violations has the highest number of records\nk.\tThere is a slight class imbalance with 'Significant_Damage_And_Fatalities' group having fewer number of records\n6.\tMultivariate analysis were performed along with correlation matrix and below were the features with top correlations\nAccident_Type_Code  \tAdverse_Weather_Metric    \t0.739361\nSafety_Score        \tDays_Since_Inspection     \t0.685386\nControl_Metric      \tTurbulence_In_gforces     \t0.643285\n7.\tNumerical data was standardized using z-score\n8.\tRemoved all records with z-score greater and lesser than 3 and -3 respectively as the values are outliers\n9.\t493 records were removed as they were considered outliers\n10.\tLabel Encoding was performed on the Target Column\n11.\tBasic Feature Engineering was performed in the interest of time and lack of domain expertise to create new features\n12.\tIndependent features and Target columns were split as X and Y\n13.\tK-Fold cross-validation was performed to create multiple folds of  train and validation data\n14.\t2 Models were taken into consideration based on the data\na.\tGradient Boosting\nb.\tXG Boost\n15.\tXGBoost was considered as final model and RandomSearchCV was used for hyper-parameterization\n16.\tValidation set was evaluated and all metrics (F1, recall, precision and accuracy) was above 95%\n17.\tTest Set was also loaded and predicted with the model already built\n\n#### Things that can improve model performance (Not done in this analysis due to time constraints):\n1.\tThere was slight imbalance in the dataset which we can over-sample to get better model performance for all classes\n2.\tBy gaining domain expertise, we can create new features which the model can interpret better.\n3.\tTest Data outliers can be imputed to get better results\n4.\tStatistical inference testing can be done to improve feature selection\n5.\tFeature importance can be obtained and model can be retrained\n","681c6aab":"### Importing Necessary Libraries","650fcb28":"### Model Building- Gradient Boosting classifier is used along with RandomSearch cross validation","888137cc":"#### Test Evaluation Metrics","4df60521":"### Predicting the Test Data","1f1f6a16":"#### Multi-variate Analysis","390e7d68":"#### Predictions using Xtreme Gradient Boosting","4bee168a":"#### Pre-processing the test data","f32ed5a4":"#### To describe the data- Five point summary- Remove Accident ID as it does not help with analysis of data","4a06eefe":"#### Check for any null values in the data","3096c2c2":"#### Checking the information of the data","120248e9":"### Feature Engineering","bfad04fd":"### Model Building-XG Boosting classifier is used along with RandomSearch cross validation- Final Model","492e4640":"#### Removing all records with z-score greater and lesser than 3 and -3 respectivley as the values are outliers","c38d5aa8":"#### Label Encoding the Target Column","764cfd68":"#### Checking the head of the data-set","b74e28e2":"#### 493 records were removed as they were considered outliers","103cde8e":"##### Average Days_Since_Inspection is 13\n##### number of records with Total_Safety_Compliants as 0-10 is high","87578d53":"#### Merging the scaled columns back to the original dataframe"}}