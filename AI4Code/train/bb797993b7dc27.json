{"cell_type":{"91323542":"code","59eef646":"code","c12a0b07":"code","25fe7219":"code","2a4c0dc6":"code","f505b867":"code","9700e0a2":"code","be5047fc":"code","3683a294":"code","5dd8e463":"code","81548fb3":"code","be0b272d":"code","2f3a3f44":"code","82318326":"code","633a1286":"code","825bb25c":"code","143b93ff":"code","c5a4adfb":"code","d0491f13":"code","4708b80f":"code","a92e9933":"code","a3804f67":"code","ebe4adaa":"code","13ebd135":"code","860689cd":"code","0637658d":"code","593a9ce5":"code","0ac7156b":"code","693e8fec":"code","19ae22a9":"code","b90868ab":"code","8056601f":"code","7094b99f":"code","2e4ce529":"code","1aa1296f":"code","47b2197b":"code","00898fe1":"code","609c283e":"code","f3c0e5a4":"code","ada257d3":"code","38a8bb91":"code","16aaec57":"code","8f64c775":"code","f5f929bb":"code","aa18ebdd":"code","bd14dc29":"code","7090c4b0":"code","2ad7748c":"code","75c6202d":"code","1bc98030":"code","de236c43":"code","c3d39280":"code","26e6c49b":"code","7c3ee2d8":"code","41fa5d85":"code","a164dede":"code","eb1b04cc":"code","f786397e":"code","23b4bb3d":"code","b17cdf3a":"code","90ad228c":"code","b644b33d":"code","6e67217c":"code","478b1aa4":"code","613cd63a":"code","f7c54cc4":"code","f5ce0d94":"code","a67a851f":"code","8b9761e0":"code","04b5571f":"code","86b85638":"code","a5e5acad":"code","9bd266e8":"code","890c93c1":"code","3ff098cc":"code","7b1ee238":"code","1c1d81d9":"code","721b9b05":"code","7258d6f9":"code","fa0384bb":"code","2df17270":"code","5e8eca17":"code","3d1988a9":"code","d3deddf0":"code","0991143f":"code","40c294aa":"code","503d0267":"code","edc515c2":"code","c0e5ef7c":"code","b91b9ad4":"code","209f7da0":"code","07df3887":"code","863f66b9":"code","d14ba4c7":"code","45278681":"code","a1228531":"code","73766e01":"code","0d1d3914":"code","c06f3653":"code","f0d83cf1":"code","602f8d8b":"code","471ed141":"code","38ee6fb2":"code","415bec4d":"code","b2ba3f56":"code","534e5f36":"code","412806f6":"code","67afce2c":"code","8386c897":"code","f59eb90d":"code","86251006":"code","943a71e1":"code","d7d9a1c7":"code","5f53b64d":"code","8b9c8545":"code","a5bc41c1":"code","3488cedb":"code","e4ba9e4a":"code","4e0a801d":"code","d1061cb1":"code","0e8d4801":"code","b6a28dad":"code","1629be7e":"code","9c3b443b":"code","78415f9e":"code","41356b2f":"code","1627f1f5":"code","4ba0433d":"code","8dcd3a5e":"code","b46aa781":"code","aec85ece":"code","8bab133d":"code","79ec7b54":"code","2b4db74d":"code","59e16c8b":"code","df236b81":"code","f8aedb50":"code","0537c5d3":"markdown","0a03f0e7":"markdown","647658ca":"markdown","59c03fde":"markdown","61ebc8b1":"markdown","40fc9fab":"markdown","7f5334b8":"markdown","81985945":"markdown","41624dbb":"markdown","fc795f28":"markdown","b7e4e411":"markdown","eba11485":"markdown","fc597365":"markdown","e2ad1039":"markdown","1f2623eb":"markdown","d9a953a7":"markdown","a88fe50b":"markdown","c9e9bdfe":"markdown","069e5f06":"markdown","f811a27d":"markdown","0d01ace3":"markdown","5606417c":"markdown","b423c395":"markdown","99f330f8":"markdown","bd6cedb3":"markdown","5060c6f7":"markdown","e27b8e67":"markdown","7eb50643":"markdown","ab66689a":"markdown","04d729e3":"markdown","250cd7fc":"markdown","08bebd1d":"markdown","0a7aaea1":"markdown","8edf607f":"markdown","dd612574":"markdown","b10fe8ba":"markdown","e63ff77c":"markdown","c1a56ca4":"markdown","0657b2bb":"markdown","e7e598b3":"markdown","f270fdd6":"markdown","21a994f4":"markdown","a95d9d79":"markdown","6c754964":"markdown","4243518e":"markdown","34381046":"markdown","804e5c89":"markdown","379f134a":"markdown","40e5d934":"markdown","5fc6831d":"markdown","34f58d46":"markdown","c57aea7c":"markdown","0c729177":"markdown","1fda186d":"markdown","406f9311":"markdown","8883218a":"markdown","3de56920":"markdown","aa5d7871":"markdown","f52166c8":"markdown","5a7b602c":"markdown","d51d3c37":"markdown","4b3407ad":"markdown","4cb7af50":"markdown","82322f13":"markdown"},"source":{"91323542":"!pip install dltk_ai","59eef646":"import os #file handling\nimport dltk_ai\nfrom dltk_ai.dataset_types import Dataset     #importing datasets\nfrom dltk_ai import visualization as vs #importing visualizations\nfrom dltk_ai import preprocessor       #importing preprocessor\nimport json\nfrom sklearn import datasets #Machine Learning \nimport numpy as np #Numerical\nimport seaborn as sns #plot\nimport matplotlib.pyplot as plt #plot\nfrom sklearn.model_selection import train_test_split #ML","c12a0b07":"import numpy as np\nimport pandas as pd\ndf =  pd.read_csv('..\/input\/life-expectancy-who\/Life Expectancy Data.csv')\ndata, test_data = train_test_split(df, test_size=0.2,random_state = 42) \ntest_data.dropna(inplace = True)\ntest_data.to_csv('Life Expectancy Data test.csv',index = False)","25fe7219":"data.head()","2a4c0dc6":"data.describe()","f505b867":"data.info()","9700e0a2":"data.isnull().sum()","be5047fc":"print(data.columns)\nprint(data.shape)","3683a294":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (20,15))\nsns.heatmap(data.corr(),annot = True)","5dd8e463":"# removing null value of 'Adult Morality' and 'Life expectancy ' columns\ndata['Adult Mortality']=data['Adult Mortality'].fillna(value=data['Adult Mortality'].mean())\ndata['Life expectancy ']=data['Life expectancy '].fillna(value=data['Life expectancy '].mean())","81548fb3":"sns.scatterplot(data=data,x='Life expectancy ',y='Schooling')","be0b272d":"# Imputing missing values of 'Schooling' column \ndef impute_schooling(c):\n    s=c[0]\n    l=c[1]\n    if pd.isnull(s):\n        if l<= 40:\n            return 8.0\n        elif 40<l<=44:\n            return 7.5\n        elif 44<l<50:\n            return 8.1\n        elif 50<l<=60:\n            return 8.2\n        elif 60<l<=70:\n            return 10.5\n        elif 70<l<=80:\n            return 13.4\n        elif l>80:\n            return 16.5\n    else:\n        return s\n    \ndata['Schooling']=data[['Schooling','Life expectancy ']].apply(impute_schooling,axis=1)","2f3a3f44":"sns.scatterplot(data=data,x='Alcohol',y='Schooling')","82318326":"# Imputing missing values of 'Alcohol' column \ndef impute_Alcohol(cols):\n    al=cols[0]\n    sc=cols[1]\n    if pd.isnull(al):\n        if sc<=2.5:\n            return 4.0\n        elif 2.5<sc<=5.0:\n            return 1.5\n        elif 5.0<sc<=7.5:\n            return 2.5\n        elif 7.5<sc<=10.0:\n            return 3.0\n        elif 10.0<sc<=15:\n            return 4.0\n        elif sc>15:\n            return 10.0\n    else:\n        return al\n    \ndata['Alcohol']=data[['Alcohol','Schooling']].apply(impute_Alcohol,axis=1)","633a1286":"sns.scatterplot(data=data,x='Income composition of resources',y='Life expectancy ')","825bb25c":"# Imputing missing values of ''Income composition of resources'' column \ndef impute_Income(c):\n    i=c[0]\n    l=c[1]\n    if pd.isnull(i):\n        if l<=40:\n            return 0.4\n        elif 40<l<=50:\n            return 0.42\n        elif 50<l<=60:\n            return 0.402\n        elif 60<l<=70:\n            return 0.54\n        elif 70<l<=80:\n            return 0.71\n        elif l>80:\n            return 0.88\n    else:\n        return i\n        \ndata['Income composition of resources']=data[['Income composition of resources','Life expectancy ']].apply(impute_Income,axis=1)\n","143b93ff":"sns.scatterplot(data=data,x=' BMI ',y='Life expectancy ')","c5a4adfb":"def outlier_replace(col):\n    for i in countries:\n        for j in groups.get_group(i)[col]:\n            threshold = 3\n            mean = np.mean(groups.get_group(i)[col])\n            std = np.std(groups.get_group(i)[col])\n            if std != 0:                     \n                z_score = (j - mean) \/ std\n                if np.abs(z_score) > threshold:\n                    j = data[col][data['Country'] == i].mean()","d0491f13":"data.dropna(subset=['Life expectancy '],inplace = True) \ncountries = data['Country'].unique()\n\n# we are creating groups of countries \n\ngroups = data.groupby('Country')\n\n# from the avialble data we know that values depends on country , \n# so we are going to handle the missing values and outliers of some columns  with respect to the country\n\n\n# creating a new list contains gdp null values greater than 10.. this simply means that we cannot update the null with \n# the respective country mean .. \ngdpnull_c = []\nfor i in countries:\n    if groups.get_group(i)['GDP'].isna().sum() >10:\n        gdpnull_c.append(i)\n        \n        \n        \n# for countries with less gdp null then fill it with mean of gdp  values with respect to each country\nfor i in countries:\n    if i not in gdpnull_c:\n        for j in groups.get_group(i)['GDP']:\n            data['GDP'][data['Country'] == i]= groups.get_group(i)['GDP'].fillna(groups.get_group(i)['GDP'].mean()) \n            \n# for those countries null values more than 10 fill it with  mean of 'GDP'  of entire dataframe\nfor i in gdpnull_c:\n    data['GDP'][data['Country'] == i]=groups.get_group(i)['GDP'].fillna(data['GDP'].mean())\n    \n# replacing outlier with mean of the rest values in the respective country:\n\noutlier_replace('GDP')\n\n# there are some countries for which we dont have the 15 years data.. so eventhough we did above steps, we may not replace\n# null values of such coutries... \n\n# so , we are droping rest na values ( 5 rows)\ndata.dropna(subset=['GDP'],inplace = True) ","4708b80f":"# hepatities     outlier  and null analysis\n\n# same process in the case of gdp data handling ( refer )\ncountries = data['Country'].unique()\ngroups = data.groupby('Country')\ngnull_c = []\nfor i in countries:\n    if groups.get_group(i)['Hepatitis B'].isna().sum() >10:\n        gnull_c.append(i)","a92e9933":"# treating outlier 'Hepatitis B'values among countries which contain less number of nulls\noutlier_replace('Hepatitis B') \n\n# we replace all null values by mean 'Hepatities B' of the corresponding countries ( countries not in gnull_c)    \nfor i in countries:\n    if i not in gnull_c:\n        for j in groups.get_group(i)['Hepatitis B']:\n            data['Hepatitis B'][data['Country'] == i]= groups.get_group(i)['Hepatitis B'].fillna(groups.get_group(i)['Hepatitis B'].mean()) \n# for those countries in gnull_c we replace it with mean of 'Hepatitis B'  in the entire dataframe\nfor i in gnull_c:   \n    data['Hepatitis B'][data['Country'] == i]=groups.get_group(i)['Hepatitis B'].fillna(data['Hepatitis B'].mean())\n\n# same processing ( refer gdp data handling process)   \ndata.dropna(subset=['Hepatitis B'],inplace = True) ","a3804f67":"\n\ngnull_c = []\nfor i in countries:\n    if groups.get_group(i)['Total expenditure'].isna().sum() >10:\n        gnull_c.append(i)\n\n        \noutlier_replace('Total expenditure') \n\n\nfor i in countries:\n    if i not in gnull_c:\n        for j in groups.get_group(i)['Total expenditure']:\n            data['Total expenditure'][data['Country'] == i]= groups.get_group(i)['Total expenditure'].fillna(groups.get_group(i)['Total expenditure'].mean()) \n\nfor i in gnull_c:   \n    data['Total expenditure'][data['Country'] == i]=groups.get_group(i)['Total expenditure'].fillna(data['Total expenditure'].mean())\n\ndata.dropna(subset=['Total expenditure'],inplace = True) ","ebe4adaa":"sns.scatterplot(x=' BMI ',y=' thinness  1-19 years',data=data)","13ebd135":"# Another imputation technique\n\ndata = data.drop(' thinness 5-9 years',axis = 1)\ndef impute_BMI(c):\n    b=c[0]\n    l=c[1]\n    if pd.isnull(b):\n        if l<=50:\n            return 25.0\n        elif 50<l<=60:\n            return 25.0\n        elif 60<l<=70:\n            return 32.0\n        elif 70<l<=80:\n            return 46.8\n        elif 80<l<=100:\n            return 60.0\n    else:\n        return b\n    \ndata[' BMI ']=data[[' BMI ','Life expectancy ']].apply(impute_BMI,axis=1)","860689cd":"sns.scatterplot(x='Population',y='infant deaths',data=data)","0637658d":"def impute_population(c):\n    p=c[0]\n    i=c[1]\n    if pd.isnull(p):\n        if i<=100:\n            return 0.19*((10)**9)\n        elif 100<i<=250:\n            return 0.18*((10)**9)\n        elif 250<i<=350:\n            return 0.02*((10)**9)\n        elif 350<i<=900:\n            return 0.1*((10)**9)\n        elif 900<i<=1100:\n            return 0.18*((10)**9)\n        elif 1100<i<=1250:\n            return 0.05*((10)**9)\n        elif 1250<i<=1500:\n            return 0.19*((10)**9)\n        elif 1500<i<=1750:\n            return 0.05*((10)**9)\n        elif i>1750:\n            return 0.1*((10)**9)\n    else:\n        return p\ndata['Population']=data[['Population','infant deaths']].apply(impute_population,axis=1)","593a9ce5":"sns.scatterplot(data=data,x=' thinness  1-19 years',y=' BMI ')","0ac7156b":"def impute_Thin_1(c):\n    t=c[0]\n    b=c[1]\n    if pd.isnull(t):\n        if b<=10:\n            return 5.0\n        elif 10<b<=20:\n            return 10.0\n        elif 20<b<=30:\n            return 8.0\n        elif 30<b<=40:\n            return 6.0\n        elif 40<b<=50:\n            return 3.0\n        elif 50<b<=70:\n            return 4.0\n        elif b>70:\n            return 1.0\n    else:\n        return t\n    \ndata[' thinness  1-19 years']=data[[' thinness  1-19 years',' BMI ']].apply(impute_Thin_1,axis=1)","693e8fec":"\ncountries = data['Country'].unique()\ngroups = data.groupby('Country')\ngnull_c = []\nfor i in countries:\n    if groups.get_group(i)['Polio'].isna().sum() >10:\n        gnull_c.append(i)\n\noutlier_replace('Polio') \n\nfor i in countries:\n    if i not in gnull_c:\n        for j in groups.get_group(i)['Polio']:\n            data['Polio'][data['Country'] == i]= groups.get_group(i)['Polio'].fillna(groups.get_group(i)['Polio'].mean()) \nfor i in gnull_c:   \n    data['Polio'][data['Country'] == i]=groups.get_group(i)['Polio'].fillna(data['Polio'].mean())\ndata.dropna(subset=['Polio'],inplace = True) ","19ae22a9":"\ncountries = data['Country'].unique()\ngroups = data.groupby('Country')\ngnull_c = []\nfor i in countries:\n    if groups.get_group(i)['Diphtheria '].isna().sum() >10:\n        gnull_c.append(i)\n\noutlier_replace('Diphtheria ') \n\nfor i in countries:\n    if i not in gnull_c:\n        for j in groups.get_group(i)['Diphtheria ']:\n            data['Diphtheria '][data['Country'] == i]= groups.get_group(i)['Diphtheria '].fillna(groups.get_group(i)['Diphtheria '].mean())\n            \n            \nfor i in gnull_c:   \n    data['Diphtheria '][data['Country'] == i]=groups.get_group(i)['Diphtheria '].fillna(data['Diphtheria '].mean())\ndata.dropna(subset=['Diphtheria '],inplace = True) ","b90868ab":"life = data['Life expectancy ']","8056601f":"dataplt = data\ndataplt = dataplt.drop(['Status'],axis=1)\ndataplt = dataplt.drop(['Country'],axis=1)","7094b99f":"#sns.pairplot(dataplt,palette='flare')","2e4ce529":"fig, axs = plt.subplots(ncols=3, nrows=7, figsize=(30, 30))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataplt.items():\n    sns.boxplot(y=k, data=dataplt, ax=axs[index],color = '#7c4780')\n    index += 1","1aa1296f":"for cols in dataplt:\n    sns.displot(dataplt[cols],color= '#7c4780')\n    plt.title('Distribution Plot of '+cols)\n    plt.show()","47b2197b":"sns.set_style(\"whitegrid\")\n\ncmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\ng = sns.relplot(\n    data=dataplt,\n    color= '#7c4780',\n    x=\"Schooling\", y=\"Income composition of resources\",\n    hue=\"Year\", size=\"Life expectancy \",\n    palette=cmap, sizes=(10, 200),\n)\n\ng.ax.xaxis.grid(True, \"minor\", linewidth=.25)\ng.ax.yaxis.grid(True, \"minor\", linewidth=.25)\ng.despine(left=True, bottom=True) ","00898fe1":"sns.set_style(\"darkgrid\")\n\ng = sns.jointplot(y=\"Schooling\", x=\"Life expectancy \", data=dataplt,\n                  kind=\"reg\", truncate=False,\n                  color=\"#7c4780\", height=7)","609c283e":"\ng = sns.jointplot(y=\"Income composition of resources\", x=\"Life expectancy \", data=dataplt,\n                  kind=\"reg\", truncate=False,\n                  color=\"#7c4780\", height=7)","f3c0e5a4":"## Applying scalar transformation \ndata = data.drop('Country',axis = 1)","ada257d3":"I = data\nfrom sklearn.preprocessing import MinMaxScaler\nI = I.drop(['Status','Life expectancy '],axis = 1)","38a8bb91":"scaler=MinMaxScaler()\nscaler.fit(I)\nscaled_data=scaler.transform(I)\nscaled_data = pd.DataFrame(scaled_data)\nlife = np.array(data['Life expectancy '])[:,np.newaxis]\nscaled_data['target'] = life\nscaled_data.columns = ['Year','Adult Mortality',\n       'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B',\n       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',\n       'Diphtheria ', ' HIV\/AIDS', 'GDP', 'Population',\n       ' thinness  1-19 years', 'Income composition of resources','Schooling','Life expectancy ']\ndata= scaled_data.copy()\ndata.to_csv('processed.csv',index=False)","16aaec57":"# initialize dltk client with API key\nclient = dltk_ai.DltkAiClient('7dbbe2f4-3fb4-4a2a-95d3-1454bb6bc09e')","8f64c775":"train_data_store_response = client.store('processed.csv', Dataset.TRAIN_DATA)\nprint(train_data_store_response)\ntrain_data = train_data_store_response['fileUrl']","f5f929bb":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"LinearRegression\"\n\nremoved_features =  ['Measles ', 'percentage expenditure',\n                     'infant deaths','Diphtheria ', 'Total expenditure',\n                     'Population'\n                     'Hepatitis B']\nnp.random.seed(24)\n# features to be used for training\nfeature = ['Adult Mortality',\n       'Alcohol', ' BMI ',\n       'under-five deaths ', 'Polio',\n       ' HIV\/AIDS', ' thinness  1-19 years', 'Schooling','Income composition of resources']  \n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 90\n\n# Save model \nsave_model = 'true'\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              feature,\n                              \"Life Expectancy Prediction Model\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","aa18ebdd":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","bd14dc29":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response ","7090c4b0":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","2ad7748c":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= feature)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']\n\n# further processing of predictions_data set  \nscaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","75c6202d":"test_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","1bc98030":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","de236c43":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library,features=feature)\npredict_response","c3d39280":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","26e6c49b":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","7c3ee2d8":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","41fa5d85":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","a164dede":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","eb1b04cc":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score'])","f786397e":"sns.regplot(x=actual,y=df['_score'])","23b4bb3d":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"RandomForest\"\n\nremoved_features =  ['percentage expenditure',\n                     \n                     'Population','Hepatitis B'\n                    ]\nnp.random.seed(42)\n# features to be used for training\nfeatures = ['Adult Mortality',\n       'Alcohol', ' BMI ',\n       'under-five deaths ', 'Polio',\n       ' HIV\/AIDS', ' thinness  1-19 years', 'infant deaths','Schooling','Total expenditure','Measles ','Diphtheria ','Income composition of resources']  \n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 80\n\n# Save model \nsave_model = 'true'\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              features,\n                              \"Life Expectancy Prediction Model\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","b17cdf3a":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","90ad228c":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response","b644b33d":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","6e67217c":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= features)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']","478b1aa4":"# further processing of predictions_data set  ","613cd63a":"scaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","f7c54cc4":"life_exp_predictions.head()","f5ce0d94":"# Upload test dataset\ntest_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","a67a851f":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","8b9761e0":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library)\npredict_response","04b5571f":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","86b85638":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","a5e5acad":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","9bd266e8":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","890c93c1":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","3ff098cc":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score']) ","7b1ee238":"sns.regplot(x=actual,y=df['_score'])","1c1d81d9":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"RandomForest\"\n\nremoved_features =  ['Measles ', 'percentage expenditure',\n                     'infant deaths','Diphtheria ', 'Total expenditure',\n                     'Population'\n                     'Hepatitis B']\nnp.random.seed(42)\n# features to be used for training\nfeatures = ['Income composition of resources', \n            'Schooling',\n            ' thinness  1-19 years',\n            ' HIV\/AIDS',\n            'Adult Mortality']  \n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 95\n\n# Save model \nsave_model = True\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              features,\n                              \"Life Expectancy Prediction Model\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","721b9b05":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","7258d6f9":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response","fa0384bb":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","2df17270":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= features)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']\n\n# further processing of predictions_data set  \nscaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","5e8eca17":"# Upload test dataset\ntest_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","3d1988a9":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","d3deddf0":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library)\npredict_response","0991143f":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","40c294aa":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","503d0267":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","edc515c2":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","c0e5ef7c":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","b91b9ad4":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score']) ","209f7da0":"actual_predicted.columns","07df3887":"sns.regplot(x=actual,y=df['_score'])","863f66b9":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"RandomForest\"\n\nremoved_features =  ['Measles ', \n                     'percentage expenditure',\n                     'infant deaths',\n                     'Diphtheria ', \n                     'Total expenditure',\n                     'Population',\n                     ' HIV\/AIDS', \n                     'Schooling',\n                     'Hepatitis B']\nnp.random.seed(42)\n# features to be used for training\nfeatures = ['Adult Mortality',\n       'Alcohol', ' BMI ',\n       'under-five deaths ', 'Polio',\n        ' thinness  1-19 years','Income composition of resources']  \n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 80\n\n# Save model \nsave_model = True\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              features,\n                              \"Life Expectancy Prediction Model1\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","d14ba4c7":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","45278681":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response ","a1228531":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","73766e01":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= features)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']\n\n# further processing of predictions_data set  \nscaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","0d1d3914":"test_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","c06f3653":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","f0d83cf1":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library,features=features)\npredict_response","602f8d8b":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","471ed141":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","38ee6fb2":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","415bec4d":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","b2ba3f56":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","534e5f36":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score'])","412806f6":"sns.regplot(x=actual,y=df['_score'])","67afce2c":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"RandomForest\"\n\nremoved_features =  [ \n                   'Total expenditure','Schooling','under-five deaths ', ' BMI ', 'Polio', 'Measles ' \n                   , 'Population', 'percentage expenditure'\n                     ]\nnp.random.seed(24)\n# features to be used for training\n\n\nfeatures = ['Adult Mortality','Hepatitis B',' thinness  1-19 years',\n            ' HIV\/AIDS','Diphtheria ','Income composition of resources','infant deaths'\n       , 'Alcohol'] \n\n\n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 90\n\n# Save model \nsave_model = 'true'\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              features,\n                              \"Life Expectancy Prediction Model\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","8386c897":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","f59eb90d":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response ","86251006":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","943a71e1":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= features)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']\n\n# further processing of predictions_data set  \nscaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","d7d9a1c7":"test_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","5f53b64d":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","8b9c8545":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library,features=features)\npredict_response","a5bc41c1":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","3488cedb":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","e4ba9e4a":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","4e0a801d":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","d1061cb1":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","0e8d4801":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score'])","b6a28dad":"sns.regplot(x=actual,y=df['_score'])","1629be7e":"# Create ML Model\n# Its a regression problem, where we need to predict  \"Life Expectancy\" which is a continous value\ntask = \"regression\"\n\n# Library to use (scikit, weka, h2o)\nlibrary = 'weka'\nalgorithm = \"RandomForest\"\n\nremoved_features =  [ \n                   'Total expenditure','Schooling','under-five deaths ', ' BMI ', 'Measles ' \n                   ,  'percentage expenditure','Population'\n                     ]\nnp.random.seed(24)\n# features to be used for training\n\n\nfeatures = ['Adult Mortality','Hepatitis B',' thinness  1-19 years','Year','under-five deaths ','Polio',\n            ' HIV\/AIDS','Diphtheria ','Income composition of resources','infant deaths', 'Alcohol'] \n\n\n# Label to predict\nlabel = 'Life expectancy '\n# Train-test split percentage\ntrain_percentage = 90\n\n# Save model \nsave_model = 'true'\ntrain_response = client.train(task,\n                              algorithm,\n                              train_data,\n                              label,\n                              features,\n                              \"Life Expectancy Prediction Model\",\n                              library,\n                              train_percentage,\n                              save_model)\nprint(train_response)","9c3b443b":"train_job_status_response = client.job_status(train_response['data']['jobId'])\nprint(train_job_status_response)\nprint(json.dumps(train_job_status_response, indent=2))","78415f9e":"# Model Evaluation Metrics\ntrain_job_output_response = client.job_output(train_response['data']['jobId'])\ntrain_job_output_response ","41356b2f":"# Error rate if predictions are given based in mean of the target variable\nprint(data[label].mean())\nprint(data[label].std())\n\n# here label is the target variable ","1627f1f5":"# load the predictions data set and preprocess it as per training\nlife_exp_predictions = preprocessor.read_csv('Life Expectancy Data test.csv',usecols= features)\nj = preprocessor.read_csv('Life Expectancy Data test.csv')\nactual = j['Life expectancy ']\n\n# further processing of predictions_data set  \nscaler.fit(life_exp_predictions)\nscaled_data_t=scaler.transform(life_exp_predictions)\nlife_exp_predictions= pd.DataFrame(scaled_data_t, columns = life_exp_predictions.columns)\nlife_exp_predictions.to_csv('life_exp_predictions.csv',index=False)","4ba0433d":"test_file_store_response = client.store('life_exp_predictions.csv', Dataset.TEST_DATA)\nprint(test_file_store_response)\ntest_data = test_file_store_response['fileUrl']","8dcd3a5e":"# load the model built\nmodel = train_job_output_response['output']['modelUrl']\nmodel","b46aa781":"# Predict using created ML Model\npredict_response = client.predict(task, test_data, model, library,features=features)\npredict_response","aec85ece":"predict_job_status_response = client.job_status(predict_response['data']['jobId'])\npredict_job_status_response","8bab133d":"predict_job_output_response = client.job_output(predict_response['data']['jobId'])\npredict_job_output_response","79ec7b54":"pred_file = predict_job_output_response['output']['predFileUrl']\nresponse = client.download(pred_file)","2b4db74d":"from io import StringIO\nimport pandas as pd\npred_data = StringIO(response.text)\ndf = pd.read_csv(pred_data, sep=\",\")\ndf","59e16c8b":"# creating a dataframe for comparing model predictions and actual value\nactual_predicted = pd.DataFrame(df['Life expectancy '])\nactual_predicted['actual'] = actual\nactual_predicted.columns = ['model_prediction', 'actual']\nactual_predicted","df236b81":"# for regression problems we use R^2 metric \n# using sklearn packagge for calculating r^2 value\nfrom sklearn.metrics import r2_score \nr2_score(actual,df['_score'])","f8aedb50":"sns.regplot(x=actual,y=df['_score'])","0537c5d3":"# <font color=darkblue> Importing Packages","0a03f0e7":"## More Exploration ","647658ca":"## <font color = green> Handling Outliers and missing values","59c03fde":"## <font color = green> Correlation Analysis","61ebc8b1":"# <font color=red> Model One","40fc9fab":"Loading the dataset and splitting it into 2 parts, train and test","7f5334b8":"A pairplot, helps in understanding all the feature correlation in one single frame, which is useful for feature analysis.","81985945":"Dropping catergorical variable and target variable","41624dbb":"## Creating Model","fc795f28":"# <font color=darkblue> Data","b7e4e411":"The other columns, wer replaced by group mean.\n\nIf the number of null was less than 10 then the mean of the column was taken, or else, the mean of that value belonging to that country were taken.","eba11485":"## Testing","fc597365":"## Testing","e2ad1039":"# <font color=red> Model Six","1f2623eb":"# Summary","d9a953a7":"Heatmap lets us see how different values are correlated, in a visual manner.","a88fe50b":"## Further outlier removal and missing values","c9e9bdfe":"We can see that there are many outliers, but since we have already done outlier detection, we can say these are true values, and dropping them can mess with data.","069e5f06":"Income Composition and schooling","f811a27d":"## Creating Model","0d01ace3":"Using MinMax scaler to scale down the data to a normalized form\n\n$x_i-min(x)\/(max(x)-min(x)) $ ","5606417c":"Boxplots are helpful in understanding outliers and central tendency","b423c395":"## Training","99f330f8":"## Training","bd6cedb3":"Dropping the categorical data","5060c6f7":"## Creating Model","e27b8e67":"Lets see what our data looks like","7eb50643":"## Training","ab66689a":"Creating a dummy dataframe, so that the original dataframe doesnt get destroyed","04d729e3":"<font color=blue>Checking training status<\/font>","250cd7fc":"<font color=blue>Checking training status<\/font>\n\nAs training a model might take lot of time depending on size of dataset, we can check current status of model training using below functions","08bebd1d":"## Creating Model","0a7aaea1":"## Testing","8edf607f":"Income Composition and Life expectancy\n\nThe linear relationship can be because, the with increasing in income, health facility become better and hence better life expectancy.","dd612574":"First, the dataset was cleaned and scaled. For scaling min-max scaler was used.\n\nAll the outliers were removed, using three different techniques, imputation, grouped mean and dropping.\nSpecific attributes were found to be highly correlated to the target variable, which was 'Life Expectancy'. \n\nModels were built by tweaking the parameters, to receive the highest accuracy.\nThe highest accuracy reached was 97.02% on test data.\n","b10fe8ba":"We can see a nice relation between both","e63ff77c":"Creating a function for hadling outliers in certain columns.The function will calculate the z score of all the values in the passed column, if the zscore is greater than the threshold we replace it with the group(country) mean.","c1a56ca4":"# <font color=red> Model Four","0657b2bb":"Since we have a lot of nulls, we cannot simply drop them","e7e598b3":"For imputation, first, the correlation was seen,. for example, column X is highly correlated to column Y.  Now, if column X has null values and column Y is filled fully, then column X can be filled using column Y \n\nIf we see the scatter plot of X and Y we can see where majority of points fall and what is the relation","f270fdd6":"## Testing","21a994f4":"## Training","a95d9d79":"# Pre Model Data Processing","6c754964":"# <font color=darkblue> Sneak Peak at Dataset","4243518e":"<font color=blue> Uploading training data","34381046":"## Training","804e5c89":"Schooling and Life Expectancy:\n\nThis also follows a beautiful relationship, it can be thought that, since schooling increases awareness and hence awareness about health also increases, and hence the results","379f134a":"# <font color = 'darkblue'> Data preprocessing and Feature anlysis","40e5d934":"## Creating Model","5fc6831d":"Next step after uploading the dataset is to train a model using Train Dataset.","34f58d46":"# <font color=red> Model Five","c57aea7c":"<font color=blue> API key setup\n\nWe need to provide APIkey to connect to DLTK","0c729177":"\n\n# <font color=red> Model Three","1fda186d":"To fill the null values, imputation, grouped mean and dropping were used","406f9311":"## Training","8883218a":"## Creating Model","3de56920":"# <font color=red> Model Two ","aa5d7871":"Plotting highly correlarted data","f52166c8":"A distribution plot is helpful for analysing the distribution that a certain attribute holds.\n\nHere we are looping through all the columns and making their distplot.","5a7b602c":"## Testing","d51d3c37":"We can see that the data is not following normal distribution and, we also have nulls","4b3407ad":"Dropping the nulls which are very few in number","4cb7af50":"Dropping the categorical data.\n\nAnd since dltk_ai can take upto 20 parameters, it is better to drop this column","82322f13":"## Testing"}}