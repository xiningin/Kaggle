{"cell_type":{"3bcceb5c":"code","b55fa373":"code","12679c7b":"code","9e89a6cb":"code","3c1ff2eb":"code","655a0d11":"code","b0804e96":"code","6fa8b808":"code","4e0f8f9e":"code","005301c7":"code","34fe4131":"code","ac176226":"code","bf15dcaa":"code","d3a188d4":"code","059fa436":"code","2b8669ad":"code","2a5fe3f4":"code","cd32bc65":"markdown","b9d1a1da":"markdown","1eeb22e1":"markdown","7bc8c052":"markdown","b3f486f0":"markdown","0abfb479":"markdown","9899b6ad":"markdown","7b06c121":"markdown","14af0fe0":"markdown"},"source":{"3bcceb5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b55fa373":"import itertools\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics \nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns","12679c7b":"df = pd.read_csv(\"..\/input\/data.csv\")\ndf.drop(columns=[\"Unnamed: 32\"], inplace=True)\nclass_names = df.diagnosis.unique()\ndf.head()","9e89a6cb":"df.describe()","3c1ff2eb":"df.info()","655a0d11":"col = [\"diagnosis\"]\nd = df\n\nnum_cols = d.columns[d.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n\nscaler = StandardScaler()\nd[num_cols] = scaler.fit_transform(d[num_cols])\n# print(scaler.mean_)\n# # scaler.transform(d)\ncolumns = d.columns","b0804e96":"sns.pairplot(d)","6fa8b808":"# g = sns.PairGrid(\n#     d, \n#     diag_sharey=True, \n#     height=2.5, \n#     aspect=1, \n#     despine=True, \n#     dropna=False)\n# g = g.map(plt.scatter)\n# g.map_diag(plt.hist)\n# g.map_offdiag(plt.scatter);","4e0f8f9e":"sns.set(style=\"white\")\nsns.clustermap(d.corr(), \n               pivot_kws=None, \n#                method='average', \n#                metric='euclidean', \n               z_score=None, \n               standard_scale=None,\n               figsize=None,\n               cbar_kws=None, \n               row_cluster=True, \n               col_cluster=True, \n               row_linkage=None, \n               col_linkage=None,\n               row_colors=None, \n               col_colors=None, \n               mask=None,\n               center=0,\n               cmap=\"vlag\",\n               linewidths=.75, \n#                figsize=(13, 13)\n              )","005301c7":"# Compute the correlation matrix\ncorr = d.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(len(columns), len(columns)))\n\n# Generate a custom diverging colormap\n# cmap = sns.diverging_palette(220, 10, as_cmap=True)\ncmap = sns.diverging_palette(h_neg=220, h_pos=10, s=75, l=50, sep=10, n=len(columns), center='light', as_cmap=True)\n\nsns.set(style=\"white\")\nsns.heatmap(corr,\n         vmin=None,\n         vmax=None,\n         cmap=cmap,\n         center=None,\n         robust=True,\n         annot=True, \n#          fmt='.2g',\n         annot_kws=None, \n#          linewidths=0.5, \n#          linecolor='white',\n         cbar=True,\n         cbar_kws={\"shrink\": .5},\n         cbar_ax=None, \n         square=True, \n         xticklabels='auto',\n         yticklabels='auto', \n         mask=mask, \n         ax=None)\n\n\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)","34fe4131":"target_val = set(df[\"diagnosis\"])\nm = {i:v for v,i in enumerate(target_val)}\ndf[\"diagnosis\"] = df[\"diagnosis\"].map(m)","ac176226":"# df.dropna()\ny = df[\"diagnosis\"]\nX = df.drop(columns=[\"id\", \"diagnosis\"])\n\nX = X.values\ny = y.values","bf15dcaa":"def print_performance(model, X_test, y_test, class_names):\n    preds = model.predict(X_test)\n\n    # accuracy_score = metrics.accuracy_score(y_test, preds)\n    # auc = metrics.auc(y_test, preds)\n    # average_precision_score = metrics.average_precision_score(y_test, preds)\n    # balanced_accuracy_score = metrics.balanced_accuracy_score(y_test, preds)\n    # brier_score_loss = metrics.brier_score_loss(y_test, preds)\n    classification_report = metrics.classification_report(y_test, preds)\n    # cohen_kappa_score = metrics.cohen_kappa_score(y_test, preds)\n    confusion_matrix = metrics.confusion_matrix(y_test, preds)\n    f1_score_ = metrics.f1_score(y_test, preds, average=\"weighted\")\n    # fbeta_score = metrics.fbeta_score(y_test, preds, average=\"weighted\")\n    # hamming_loss = metrics.hamming_loss(y_test, preds)\n    # hinge_loss = metrics.hinge_loss(y_test, preds)\n    # jaccard_similarity_score = metrics.jaccard_similarity_score(y_test, preds)\n    # log_loss = metrics.log_loss(y_test, preds)\n    # matthews_corrcoef = metrics.matthews_corrcoef(y_test, preds)\n    # precision_recall_curve = metrics.precision_recall_curve(y_test, preds)\n    # precision_recall_fscore_support = metrics.precision_recall_fscore_support(y_test, preds)\n    # precision_score = metrics.precision_score(y_test, preds, average=\"weighted\")\n    # recall_score = metrics.recall_score(y_test, preds, average=\"weighted\")\n    # roc_auc_score = metrics.roc_auc_score(y_test, preds, average=\"weighted\")\n    # roc_curve = metrics.roc_curve(y_test, preds)\n    # zero_one_loss = metrics.zero_one_loss(y_test, preds)\n    \n    print(\"-\"*55)\n    print(\"Performance\")\n    print(\"-\"*55)\n    # print(\"{} : {:.4f} \".format(\"Accuracy Score                  \", accuracy_score))\n    # print(\"{} : {:.4f} \".format(\"AUC                             \", auc))\n    # print(\"{} : {:.4f} \".format(\"Average Precision Score         \", average_precision_score))\n    # print(\"{} : {:.4f} \".format(\"Balanced Accuracy Score         \", balanced_accuracy_score))\n    # print(\"{} : {:.4f} \".format(\"Brier Score Loss                \", brier_score_loss))\n#     print(\"{} : {:.4f} \".format(\"Classification Report           \", classification_report))\n    # print(\"{} : {:.4f} \".format(\"Cohen Kappa Score               \", cohen_kappa_score))\n#     print(\"{} : {:.4f} \".format(\"Confusion Matrix                \", confusion_matrix))\n    print(\"{} : {:.4f} \".format(\"F1 Score                        \", f1_score_))\n    # print(\"{} : {:.4f} \".format(\"Fbeta Score                     \", fbeta_score))\n    # print(\"{} : {:.4f} \".format(\"Hamming Loss                    \", hamming_loss))\n    # print(\"{} : {:.4f} \".format(\"Hinge Loss                      \", hinge_loss))\n    # print(\"{} : {:.4f} \".format(\"Jaccard Similarity Score        \", jaccard_similarity_score))\n    # print(\"{} : {:.4f} \".format(\"Log Loss                        \", log_loss))\n    # print(\"{} : {:.4f} \".format(\"Matthews Corrcoef               \", matthews_corrcoef))\n    # print(\"{} : {:.4f} \".format(\"Precision Recall Curve          \", precision_recall_curve))\n    # print(\"{} : {:.4f} \".format(\"Precision Recall Fscore Support \", precision_recall_fscore_support))\n    # print(\"{} : {:.4f} \".format(\"Precision Score                 \", precision_score))\n    # print(\"{} : {:.4f} \".format(\"Recall Score                    \", recall_score))\n    # print(\"{} : {:.4f} \".format(\"Roc Auc Score                   \", roc_auc_score))\n    # print(\"{} : {:.4f} \".format(\"Roc Curve                       \", roc_curve))\n    # print(\"{} : {:.4f} \".format(\"Zero One Loss                   \", zero_one_loss))\n    print(classification_report)\n    \n    print(\"-\"*55)\n    print(\"\\n\\n\")\n    \n\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(confusion_matrix, classes=class_names,\n                          title='Confusion matrix, without normalization')\n\n    # Plot normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(confusion_matrix, classes=class_names, normalize=True,\n                          title='Normalized confusion matrix')\n\n    plt.show()\n    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n    \ndef print_performance_grid(clf):\n    # print(\"*\"*100)\n    # print(\"{}{}{}\".format(\"*\"*40,\"Performance\", \"*\"*40))\n    print(\"{}\".format(\"Performance\"))\n    print(\"*\"*90)\n    print(\"Score            : {}\".format(clf.score(X, y)))\n    print(\"Best Estimator   : {}\".format(clf.best_estimator_))\n    print(\"Best Score       : {}\".format(clf.best_score_))\n    print(\"Best Params      : {}\".format(clf.best_params_))\n    print(\"Best Index       : {}\".format(clf.best_index_))\n    # print(\"Scorer           : {}\".format(clf.scorer_))\n    print(\"Refit Time       : {}\".format(clf.refit_time_))\n    # print(\"CV Results       : {}\".format(clf.cv_results_))\n\n    params = clf.get_params()\n    best_estimator = clf.best_estimator_\n    cv_results = clf.cv_results_\n    \n    return params, best_estimator, cv_results","d3a188d4":"# parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'), \n#               'degree': np.arrange(10),\n#               'C':np.arrange(10)}\n\nparameters = {'kernel':('linear', 'rbf'), \n              'degree': [1, 10],\n              'C': [1, 10]}\n\nsvc = svm.SVC(C=1.0,\n    kernel='rbf',\n    degree=3, \n    gamma='auto',\n    coef0=0.0,\n    shrinking=True, \n    probability=False,\n    tol=0.001,\n    cache_size=200,\n    class_weight=None, \n    verbose=False,\n    max_iter=-1, \n    decision_function_shape='ovr',\n    random_state=None)\n\n\nsvc = svm.SVC(gamma='auto')\n\nclf = GridSearchCV(estimator=svc, \n                   param_grid=parameters,\n                   scoring=None, \n                   fit_params=None, \n                   n_jobs=None,\n                   iid='warn',\n                   refit=True,\n                   cv=5,\n                   verbose=0,\n                   pre_dispatch='2*n_jobs',\n                   error_score='raise-deprecating',\n                   return_train_score='warn')\n\nclf.fit(X, y)\n \nparams, best_estimator, cv_results = print_performance_grid(clf)","059fa436":"parameters = {'kernel':('linear', 'rbf'), \n              'degree': np.arange(1, 10),\n              'C': np.arange(1, 10)}\n\nsvc = svm.SVC(C=1.0,\n    kernel='rbf',\n    degree=3, \n    gamma='auto',\n    coef0=0.0,\n    shrinking=True, \n    probability=False,\n    tol=0.001,\n    cache_size=200,\n    class_weight=None, \n    verbose=False,\n    max_iter=-1, \n    decision_function_shape='ovr',\n    random_state=None)\n\n\nsvc = svm.SVC(gamma='auto')\n\nclf = GridSearchCV(estimator=svc, \n                   param_grid=parameters,\n                   scoring=None, \n                   fit_params=None, \n                   n_jobs=-1,\n                   iid='warn',\n                   refit=True,\n                   cv=5,\n                   verbose=1,\n                   pre_dispatch='2*n_jobs',\n                   error_score='raise-deprecating',\n                   return_train_score='warn')\n\n\nclf.fit(X, y)\n\nparams, best_estimator, cv_results = print_performance_grid(clf)","2b8669ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# svc = svm.SVC(C=1.0,\n#     kernel='rbf',\n#     degree=3, \n#     gamma='auto',\n#     coef0=0.0,\n#     shrinking=True, \n#     probability=False,\n#     tol=0.001,\n#     cache_size=200,\n#     class_weight=None, \n#     verbose=False,\n#     max_iter=-1, \n#     decision_function_shape='ovr',\n#     random_state=None)\n\n# best estimator found using grid search cv\nsvc = svm.SVC(C=4, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=1, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\n\nclf = svc\n\nprint(\"Cross Val Score            : {}\".format(cross_val_score(clf, X, y, cv=5)))\n\nclf.fit(X_train, y_train)\nprint(\"Score (training data only) : {}\".format(clf.score(X_train, y_train)))\n\ny_pred = clf.predict(X_test)\nprint(\"F-1 Score                  : {}\".format(f1_score(y_test, y_pred, average='weighted')))\n      ","2a5fe3f4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint(\"SVM\")\nmodel = clf\nmodel.fit(X_train, y_train)\nprint_performance(model, X_test, y_test, class_names)","cd32bc65":"Cluster map of feature correleation  ","b9d1a1da":"Scale data for visualization","1eeb22e1":"Pair plot of all features","7bc8c052":"Plot of difference between actual value and predicted value without scaling","b3f486f0":"Grid search for best estimator and parameters in a range - (1, 10) for linear and radial kernel","0abfb479":"**Training**","9899b6ad":"Heatmap of scaled feature correleation  ","7b06c121":"Grid search for best estimator and parameters for linear and radial kernel","14af0fe0":"SVM has shown much better result"}}