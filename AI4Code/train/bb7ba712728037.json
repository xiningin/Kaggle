{"cell_type":{"32e122d3":"code","1be6e443":"code","f5e94d8c":"code","51d2f53d":"code","2753ee98":"code","6427ce9b":"code","a8ae2912":"code","0ee2ab80":"code","1eb87ae1":"code","3866fb9f":"code","113aef13":"code","a6b15a9d":"code","afa99940":"code","0f70bc06":"code","b95806aa":"code","a23cafb5":"code","74aeec16":"code","45fac4a5":"code","94f45701":"code","586707da":"code","c00b89b7":"markdown","565f81f9":"markdown","5948bf5d":"markdown","5aca6f54":"markdown","95aa6455":"markdown","7dc2330f":"markdown","aad43408":"markdown","05fd8b92":"markdown","1b052a51":"markdown","a83c635b":"markdown","51052c9a":"markdown","6bd57d24":"markdown"},"source":{"32e122d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import (\n    LinearRegression,\n    Ridge,\n    Lasso\n)\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef min_max_normalization(x):\n    x_min = x.min()\n    x_max = x.max()\n    x_norm = (x - x_min) \/ ( x_max - x_min)\n    return x_norm\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1be6e443":"train = pd.read_csv('..\/input\/pchs-data-science-covid-19-competition\/Train.csv')\ntest = pd.read_csv('..\/input\/pchs-data-science-covid-19-competition\/Test.csv')","f5e94d8c":"train.corr()","51d2f53d":"train.isnull().sum()","2753ee98":"cat_list1 = {*train.State, *test.State}\ncat_list2 = {*train.County, *test.County}\n# \u65b9\u6cd51\ntrain.State = train.State.astype(pd.CategoricalDtype(cat_list1))\n# \u65b9\u6cd52\ntest.State = pd.Categorical(test.State, cat_list1)\n# \u65b9\u6cd51\ntrain.County = train.County.astype(pd.CategoricalDtype(cat_list2))\n# \u65b9\u6cd52\ntest.County = pd.Categorical(test.County, cat_list2)\ntraining = pd.get_dummies(train, columns=[\"State\"], drop_first=True)\ntester = pd.get_dummies(test, columns=[\"State\"], drop_first=True)\ntraining","6427ce9b":"##training.columns.str.contains('County')\n##training.iloc[:, training.columns.str.contains('County')]","a8ae2912":"training[\"Population 2018\"] = min_max_normalization(training[\"Population 2018\"])*100\ntraining[\"Median Household Income 2018 ($)\"] = min_max_normalization(training[\"Median Household Income 2018 ($)\"])*100","0ee2ab80":"from sklearn.linear_model import LogisticRegression\ncols = [\"Population 2018\", \"Unemployment Rate 2018 (%)\", \"Poverty 2018 (%)\", \"Confirmed Cases Per 100,000 people\", \"Deaths Per 100,000 people\", \"Mortality Rate (%)\", \"White Alone (%)\", \"Black Alone (%)\", \"Native American Alone (%)\", \"Asian Alone (%)\", \"Hispanic (%)\", \"Less than a High School Diploma (%)\", \"Only a High School Diploma (%)\", \"Some College\/Associate's Degree (%)\", \"Bachelor's Degree or Higher (%)\"]\nX = training.drop(['Unnamed: 0', 'FIPS', 'County', 'Confirmed Cases', 'Confirmed Deaths', 'Confirmed Cases Per 100,000 people', 'Deaths Per 100,000 people', 'Mortality Rate (%)'], axis=1)\ny1 = training['Confirmed Cases']\ny2 = training['Confirmed Deaths']\n# Build a logreg and compute the feature importances\nmodel1 = LogisticRegression(max_iter=1000)\nmodel2 = LogisticRegression(max_iter=1000)\n# create the RFE model and select 8 attributes\nmodel1.fit(X,y1)\nmodel2.fit(X,y2)","1eb87ae1":"from sklearn.metrics import mean_squared_error\ny1_train_predict = model1.predict(X)\nmean_squared_error(y1, y1_train_predict)","3866fb9f":"y1_train_predict","113aef13":"from sklearn.metrics import mean_squared_error\ny2_train_predict = model2.predict(X)\nmean_squared_error(y2, y2_train_predict)","a6b15a9d":"y2_train_predict","afa99940":"model3 = LogisticRegression(max_iter=1000)\nX['confirm cases'] = min_max_normalization(y1_train_predict)*100\nmodel3.fit(X,y2)\ny3_train_predict = model3.predict(X)\nmean_squared_error(y2, y3_train_predict)","0f70bc06":"test.isnull().sum()","b95806aa":"tester[\"Population 2018\"] = min_max_normalization(tester[\"Population 2018\"])*100\ntester[\"Median Household Income 2018 ($)\"] = min_max_normalization(tester[\"Median Household Income 2018 ($)\"])*100\ntester","a23cafb5":"X_test = tester.drop(['Unnamed: 0', 'FIPS', 'County'], axis=1)\nprint(X_test.dtypes)\ntest1_predicted = model1.predict(X_test)\ntest2_predicted = model2.predict(X_test)\nX_test['confirm cases'] = min_max_normalization(test1_predicted)*100\ntest3_predicted = model3.predict(X_test)","74aeec16":"test1_predicted","45fac4a5":"test2_predicted","94f45701":"test3_predicted","586707da":"sub = pd.read_csv('..\/input\/pchs-data-science-covid-19-competition\/Sample.csv')\nsub['Confirmed'] = list(map(int, test1_predicted))\nsub['Deaths'] = list(map(int, test3_predicted))\nsub.to_csv('submission.csv', index=False)\nsub","c00b89b7":"# \u30c7\u30fc\u30bf\u306e\u4ee3\u5165","565f81f9":"# \u30e2\u30c7\u30eb\u306e\u751f\u6210\uff08\u6b7b\u4ea1\u6570\u30fb\u75c7\u4f8b\u6570\u306e\u60c5\u5831\u3092\u542b\u3080\u7279\u5fb4\u91cf\u306e\u9664\u53bb\uff09\u6b7b\u4ea1\u6570\u30fb\u75c7\u4f8b\u6570\u306e\uff12\u30e2\u30c7\u30eb\u3092\u751f\u6210","5948bf5d":"\u6a19\u6e96\u5316(\u4eba\u53e3\u30fb\u5e73\u5747\u4e16\u5e2f\u53ce\u5165)0~100\u306e\u9593","5aca6f54":"# \u75c7\u4f8b\u6570\u4e88\u6e2c\u7d50\u679c\uff08MSE\uff1a\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3068\u5b9f\u969b\u306e\u5024\uff09","95aa6455":"# \u75c7\u4f8b\u4e88\u6e2c\u3092\u5229\u7528\u3057\u305f\u6b7b\u4ea1\u6570\u4e88\u6e2c","7dc2330f":"> # \u78ba\u8a8d\u7528\uff08\u30c0\u30df\u30fc\u5909\u6570\uff09","aad43408":"# \u5dde\u3068\u753a\u306e\u30c0\u30df\u30fc\u5909\u6570\u3092\u4f5c\u6210\u3057\uff0c\u5143\u306e\u6c34\u6e96\u3092\u6d88\u53bb","05fd8b92":"# \u6b7b\u4ea1\u6570\u4e88\u6e2c\u7d50\u679c\uff08MSE\uff1a\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3068\u5b9f\u969b\u306e\u5024\uff09","1b052a51":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u78ba\u8a8d","a83c635b":"\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u540c\u69d8\u306e\u524d\u51e6\u7406","51052c9a":"# \u6b20\u640d\u5024\u306e\u78ba\u8a8d","6bd57d24":"# \u5404\u30c7\u30fc\u30bf\u306e\u76f8\u95a2"}}