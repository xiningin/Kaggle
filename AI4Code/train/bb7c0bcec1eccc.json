{"cell_type":{"42036c53":"code","12ea1747":"code","0e3285f3":"code","74f8a679":"code","91438a9c":"code","a80df4e0":"code","28d78c30":"code","364dfa73":"code","0fa33fca":"code","f0b04a33":"code","c97d9210":"code","773e478e":"code","86557c0b":"code","63a6b7fa":"code","73e40c9e":"code","3ade7322":"code","78b4f2fa":"code","20685cc9":"code","c194fd08":"code","8409e081":"code","84d37496":"code","ddda9252":"code","47fb6577":"code","dac96d26":"code","d406816d":"code","d2dfb878":"code","d462bbae":"code","380bd3be":"code","4011addd":"code","72a61117":"code","b275c9f1":"code","77bbf9d9":"code","4b6f0128":"code","c817e3fb":"code","09976525":"code","45a9cffa":"code","163fdf7d":"code","ebbc94d6":"code","26674975":"code","80025be7":"code","dcb1ff71":"code","b684699f":"code","9641a867":"code","f55856f8":"code","1327e567":"code","cc39bdba":"code","f9e053b0":"code","319a6990":"code","9acf7968":"code","d40e834e":"markdown","cae04aab":"markdown","e32c7c43":"markdown","a185a599":"markdown","75638d86":"markdown","d9d6bc41":"markdown","05e5c264":"markdown","1e99d9b6":"markdown","0e00765c":"markdown","7e846b75":"markdown","815c3748":"markdown","c1b7497c":"markdown","a5e2aac2":"markdown","acb43393":"markdown","d3688357":"markdown"},"source":{"42036c53":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12ea1747":"Train = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\n","0e3285f3":"# here we are printing first five lines of our train dataset\nTrain.head()","74f8a679":"# here we are Getting the Independent Features\nX=Train.drop('label',axis=1)","91438a9c":"# printing head of our independent features\nX.head()","a80df4e0":"# here we are printing shape of our dataset\nTrain.shape\n","28d78c30":"# here we are checking if there is null value or not\nTrain.isnull().sum()","364dfa73":"# here we are droping NaN values from our dataset\nTrain=Train.dropna()","0fa33fca":"# here we are checking again if there is any NaN value or not\nTrain.isnull().sum()","f0b04a33":"Train.head(10)","c97d9210":"# here we are copying our dataset .\nTrain=Train.copy()","773e478e":"# here we are reseting our index\nTrain.reset_index(inplace=True)","86557c0b":"# here we are printing our first 10 line of dataset for checking indexing\nTrain.head(10)","63a6b7fa":"x=Train['title']\n# here we are making independent features\ny=Train['label']\ny.shape","73e40c9e":"# here we are importing nltk,stopwords and porterstemmer we are using stemming on the text \n# we have and stopwords will help in removing the stopwords in the text\n\n#re is regular expressions used for identifying only words in the text and ignoring anything else\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","3ade7322":"ps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(Train)):\n    review = re.sub('[^a-zA-Z]', ' ', Train['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","78b4f2fa":"corpus[30]","20685cc9":"# here we are setting vocabulary size\nvoc_size=5000","c194fd08":"# here we are performing one hot representation\nfrom tensorflow.keras.preprocessing.text import one_hot\none_hot_rep=[one_hot(words,voc_size)for words in corpus] ","8409e081":"# here we are printing length of first line\nlen(one_hot_rep[0])","84d37496":"# here we are printing length of 70 line\nlen(one_hot_rep[70])","ddda9252":"# here we are importing library for doind padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# here we are specifying a sentence length so that every sentence in the corpus will be of same length\n\nsentence_length=25\n\n# here we are using padding for creating equal length sentences\n\n\nembedded_docs=pad_sequences(one_hot_rep,padding='pre',maxlen=sentence_length)\nprint(embedded_docs)","47fb6577":"# here we are imporitng important libraries for building model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n ","dac96d26":"#Creating model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sentence_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","d406816d":"z =np.array(embedded_docs)\ny =np.array(y)","d2dfb878":"# here we are printing shape \nz.shape,y.shape","d462bbae":"# here we are splitting the data for training and testing the model\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(z, y, test_size=0.10, random_state=42)\n","380bd3be":"model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=64)","4011addd":"# here we are loading our test dataset for prediction\n\nTest=pd.read_csv('..\/input\/fake-news\/test.csv') \nTest_id=Test[\"id\"]","72a61117":"Test_id","b275c9f1":"# here we are printing first 5 line of our dataset\nTest.head()","77bbf9d9":"# here we are removing these columns as they are not so important\nTest=Test.drop(['text','id','author'],axis=1)","4b6f0128":"# printing first 5 line of our dataset\nTest.head()","c817e3fb":"# here we are checking if null values in the test dataset or not\n\nTest.isnull().sum()","09976525":"Test.fillna('fake fake fake',inplace=True)\n# here we are filling NaN value with \"fake,fake,fake\".we cannot drop the NaN value because\n# as the solution file that we have to submitted in kaggle expects \n# it to have 5200 rows so we can't drop rows in the test dataset","45a9cffa":"# here we are printing shape\nTest.shape","163fdf7d":"# here we are creating corpus for the test dataset exactly the same as we created for the \n# training dataset\nps = PorterStemmer()\ncorpus_test = []\nfor i in range(0, len(Test)):\n    review = re.sub('[^a-zA-Z]', ' ',Test['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_test.append(review)","ebbc94d6":"corpus[30]","26674975":"# here we are creating one hot representation for the test corpus\n\none_hot_rep_Test=[one_hot(words,voc_size)for words in corpus_test] ","80025be7":"# here we are doing padding for the test dataset\nsentence_length=25\n\nembedded_docs_test=pad_sequences(one_hot_rep_Test,padding='pre',maxlen=sentence_length)\nprint(embedded_docs_test)","dcb1ff71":"x_test=np.array(embedded_docs_test)","b684699f":"#making predictions for the test dataset\n\ncheck=model.predict_classes(x_test)","9641a867":"check","f55856f8":"check.shape","1327e567":"Test.shape","cc39bdba":"val=[]\nfor i in check:\n    val.append(i[0])","f9e053b0":"submission = pd.DataFrame({'id':Test_id, 'label':val})\nsubmission.shape","319a6990":"submission.head()","9acf7968":"#saving the submission file\n\nsubmit_sample.to_csv('submission.csv',index=False)","d40e834e":"# Reading dataset","cae04aab":"# Dataset","e32c7c43":"# Spiliting and Training","a185a599":"# Introduction","75638d86":"1. train.csv: A full training dataset with the following attributes:\n2. id: unique id for a news article\n3. title: the title of a news article\n4. author: author of the news article\n5. text: the text of the article; could be incomplete\n6. label: a label that marks the article as potentially unreliable.\nWhere 1: unreliable and 0: reliable.","d9d6bc41":"# Contents\n","05e5c264":"Thanks for reading. I hope you like my analysis and found it to be helpful. If you have any questions or suggestions, feel free to write them down in the comment section.","1e99d9b6":"# Data Pre-Processing","0e00765c":"# Conclusion","7e846b75":"In the following analysis, we will talk about how one can create an NLP to detect whether the news is real or fake.\nNowadays, fake news has become a common trend. Even trusted media houses are known to spread fake news and are losing their credibility. So, how can we trust any news to be real or fake?\n","815c3748":"# Submission File","c1b7497c":"as above we are observing that after removing NaN values index number 6 and number 8 is missing because they are NaN value so they are get removed so for managing index we have to do index reseting","a5e2aac2":"1. Introduction\n2. Dataset\n3. Importing important libraries\n4. Reading dataset\n5. Data Pre-Processing\n6. Building model\n7. Spiliting and Training\n8. Submission file\n9. Conclusion","acb43393":"# Importing important libraries","d3688357":"# Building Models"}}