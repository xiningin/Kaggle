{"cell_type":{"b2706745":"code","c63a5eed":"code","d555f2c3":"code","64d3c9a4":"code","3b2f7728":"code","d31e8428":"code","2271dd3e":"code","3e5ef072":"code","4a3b5d4a":"code","cef726fe":"code","3f5e99bf":"code","83451060":"code","a76da88c":"code","fcef6961":"code","72034ca7":"code","0b0a14c0":"code","1f380d18":"code","86f08b21":"code","0b3f170a":"code","39cc666b":"code","f21bc89f":"code","72565d58":"code","78eff191":"code","2b40ff2a":"code","083a5b15":"code","bdd0c43b":"markdown","88ddc49c":"markdown","7d3f5919":"markdown","8ee0c8b7":"markdown","9c740071":"markdown","dd45db2f":"markdown","f4cd53f3":"markdown","7102b327":"markdown","2fe245ce":"markdown","ddc1013a":"markdown","e13e6394":"markdown","a5ef27ce":"markdown","6c43c84a":"markdown","d77a2219":"markdown","14b723f6":"markdown","f2383c11":"markdown","912facbf":"markdown","834e18d0":"markdown","80588469":"markdown","d4c082a6":"markdown","21ebe14c":"markdown","6b7b5355":"markdown","4e0cb003":"markdown","dad3686b":"markdown","9d6da009":"markdown","4e434b6e":"markdown","3467d948":"markdown","c753ccc2":"markdown","bb431775":"markdown","14855b13":"markdown"},"source":{"b2706745":"%matplotlib inline    \n# To make data visualisations display in Jupyter Notebooks \n\nimport numpy as np    # linear algebra \nimport pandas as pd    # Data processing, Input & Output load    \nimport matplotlib.pyplot as plt    # Visualization & plotting\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom math import sqrt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression    # Linear Regression Algorithm\nfrom sklearn.linear_model import Lasso, LassoCV    # Lasso Regression Algorithm\nfrom sklearn.linear_model import Ridge    # Ridge Regression Algorithm\nfrom sklearn.linear_model import ElasticNet    # ElasticNet Algorithm\nfrom sklearn.svm import SVR, LinearSVR    # Support Vector Regressor\nfrom sklearn.tree import DecisionTreeRegressor    # Decision Tree Regressor\nfrom sklearn.ensemble import RandomForestRegressor    # Random Forest Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor    # Gradient Boosting Regressor\nfrom sklearn.ensemble import AdaBoostRegressor    # Ada Boost Regressor\n\nimport joblib  #Joblib is a set of tools to provide lightweight pipelining in Python (Avoid computing twice the same thing)\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RepeatedKFold\n                                    # GridSearchCV - Implements a \u201cfit\u201d and a \u201cscore\u201d method\n                                    # train_test_split - Split arrays or matrices into random train and test subsets\n                                    # cross_val_score - Evaluate a score by cross-validation     \n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import f1_score, precision_score, accuracy_score, roc_auc_score, recall_score, roc_curve\nfrom sklearn.metrics import make_scorer, confusion_matrix, classification_report   # Differnt metrics to evaluate the model\nimport pandas_profiling as pp    # simple and fast exploratory data analysis of a Pandas Dataframe\n\nimport warnings    # To avoid warning messages in the code run\nwarnings.filterwarnings('ignore')\nsns.set_style('darkgrid')","c63a5eed":"data = pd.read_csv(\"..\/input\/body-fat-prediction-dataset\/bodyfat.csv\")\n\n# Copying the original data into a new python variable object data_new\ndata_new = data.copy()\n\nprint(\"Data Shape - \", data_new.shape)\n\ndata_new.head()","d555f2c3":"data_new.describe(include = 'all').transpose()","64d3c9a4":"data_new.info()","3b2f7728":"data_new.isnull().sum()","d31e8428":"data_new.describe().loc[['min', 'max']]","2271dd3e":"data_new = data_new.drop('Density', axis = 1)\ndata_new = data_new.loc[data_new['BodyFat'] != 0.0]\ndata_new = data_new.loc[data_new['Height'] != 29.50]\n\nprint(\"Data Shape - \", data_new.shape)\ndata_new.head()","3e5ef072":"pp.ProfileReport(data_new)","4a3b5d4a":"num_cols = data_new.select_dtypes(include = [np.number]).columns.tolist()","cef726fe":"num_cols = data_new.drop(['BodyFat'], axis = 1).select_dtypes(include = [np.number]).columns.tolist()\nprint('Numeric Columns \\n', num_cols)","3f5e99bf":"data_new.boxplot(column = ['Age', 'Weight', 'Height', 'Neck', 'Chest', 'Abdomen'])","83451060":"data_new.boxplot(column = ['Hip', 'Thigh', 'Knee', 'Ankle', 'Biceps', 'Forearm', 'Wrist'])","a76da88c":"data_new.isnull().sum(axis = 0)","fcef6961":"X = data_new.drop(['BodyFat'], axis = 1)\ny = data_new['BodyFat']","72034ca7":"X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 42) \n\nprint('Train Shape: ', X_train.shape)\nprint('Test Shape: ', X_test.shape)","0b0a14c0":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1f380d18":"def fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[name] = model.score(X_test,y_test)\n\n    model_scores = pd.DataFrame(model_scores, index=['Accuracy'])\n    model_scores = model_scores.transpose().sort_values('Accuracy')\n\n    return model_scores","86f08b21":"models = {'Ridge' : Ridge(), 'Lasso' : Lasso(), 'ElasticNet' : ElasticNet(), 'SVR' : SVR(),\n          'DecisionTreeRegressor' : DecisionTreeRegressor(), 'RandomForestRegressor': RandomForestRegressor(),\n          'GradientBoostingRegressor': GradientBoostingRegressor(), 'AdaBoostRegressor': AdaBoostRegressor()}","0b3f170a":"baseline_model_scores_data_new_final = fit_and_score(models, X_train, X_test, y_train, y_test)\n\n# Sorting baseline model basis their accuracy scores\nbaseline_model_scores_data_new_final.sort_values('Accuracy')","39cc666b":"plt.figure(figsize=(20,10))\nsns.barplot(data=baseline_model_scores_data_new_final.T)\nplt.title('Baseline Model Accuracy Score')\nplt.xticks(rotation=90);","f21bc89f":"def gridsearch_cv_scores(models, params, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_gs_scores = {}\n    model_gs_best_param = {}\n    \n    for name, model in models.items():\n        gs_model = GridSearchCV(model,\n                                param_grid=params[name],\n                                scoring='neg_mean_squared_error',\n                                n_jobs=-1,\n                                cv=5,\n                                verbose=2)\n        \n        gs_model.fit(X_train,y_train)\n\n        model_gs_scores[name] = gs_model.score(X_test,y_test)\n        model_gs_best_param[name] = gs_model.best_params_\n\n    model_gs_scores = pd.DataFrame(model_gs_scores, index=['neg_mean_squared_error'])\n    model_gs_scores = model_gs_scores.transpose().sort_values('neg_mean_squared_error')\n        \n    return model_gs_scores, model_gs_best_param","72565d58":"models = {'AdaBoostRegressor':AdaBoostRegressor()}\nparams = {'AdaBoostRegressor': {'n_estimators': [30,40,50,60,70],\n                                'learning_rate': [0.3,0.4,0.5,0.6,0.7],\n                                'loss': ['linear', 'square', 'exponential']}}","78eff191":"model_gs_scores_1, model_gs_best_param_1 = gridsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)\nprint(model_gs_scores_1)\nprint(model_gs_best_param_1)","2b40ff2a":"model = AdaBoostRegressor(learning_rate = 0.7, loss = 'square', n_estimators = 40, random_state = 42)\nmodel.fit(X_train, y_train)\ny_preds = model.predict(X_test)","083a5b15":"r2 = r2_score(y_test,y_preds)\nmae = mean_absolute_error(y_test, y_preds)\nmse = mean_squared_error(y_test, y_preds)\nrmse = np.sqrt(mse)\n\n\nprint(f'R2 Score: {r2}')\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Square Error: {mse}')\nprint(f'Root Mean Square Error: {rmse}')","bdd0c43b":"## 1. Importing Necessary Libraries","88ddc49c":"### b) Performing Train, Test & Split","7d3f5919":"* The entire dataset contains <b>252<\/b> rows and <b>15<\/b> columns.","8ee0c8b7":"## 5. EDA(Exploratory Data Analysis)","9c740071":"* Now let's see if all the values of the variables are physically possible and if there are any clear outliers.","dd45db2f":"## 7.1) Creating Model Dataset","f4cd53f3":"## 7.2) Splitting the model data into train and test data","7102b327":"## 7. Feature Engineering","2fe245ce":"## 9) Model Evaluation","ddc1013a":"### a) Applying AdaBoost Regression","e13e6394":"### a) Defining GridSearchCV Criteria To Display Best Model Score And Best Model Parameters","a5ef27ce":"### b) AdaBoost Regression metrics of test data","6c43c84a":"### c) Performing standard scaling on train and test data","d77a2219":"1. First, let's get the summary of the numerical data","14b723f6":"* The minimal value of body fat is <b>0.0<\/b>, which is definitely not possible and we would delete this record.\n\n* The minimal height is <b>29.5 inches<\/b>, which is definitely not possible considering that the minimal age in the dataset is <b>22 years<\/b> and we would delete this record as well.","f2383c11":"## 6. Data Profiling Report","912facbf":"* Let's check if there are any null variables in the <b>data_new<\/b> dataset.","834e18d0":"## 1. Data Categorization","80588469":"## 8) Hyperparameter Tuning Of AdaBoost Regressor","d4c082a6":"### b) Defining AdaBoostRegressor Model And Parameters For Hyperparameter Tuning","21ebe14c":"## 6.1) Bivariate Analysis","6b7b5355":"* We would categorize the existing variables of our existing dataframe into <b>numerical<\/b> and <b>categorical<\/b> variables.","4e0cb003":"## 2. Importing Dataset","dad3686b":"* With the scoring of the baseline model, we will use the following model to tune the hyperparameter - <b>AdaBoost Regressor : 0.684220<\/b>","9d6da009":"### a) Separating the target variable - BodyFat from the data_new_final dataframe","4e434b6e":"## 3. Let's Understand Our Data","3467d948":"* As in the task, it's mentioned that the use of the <b>Density<\/b> variable is not allowed, as <b>BodyFat<\/b> is calculated directly with <b>Density<\/b>, we will drop the <b>Density<\/b> variable from our Dataset.\n* Also, we shall drop the outlier records from our dataset.","c753ccc2":"### a) Null value check in the final dataset before model run","bb431775":"* Let's drop the columns which we won't be using.","14855b13":"### d) Defining different baseline models and scores"}}