{"cell_type":{"4ec8324c":"code","7a2ef61f":"code","33b92562":"code","9d26caba":"code","eb7e4f09":"code","e93f62d8":"code","5657928a":"code","f36c602b":"code","de15a136":"code","426b13a8":"code","891acb28":"code","b1043a56":"code","55193ef1":"code","f24469aa":"code","e0448e61":"code","d6eb408d":"code","9c042f85":"code","e6b26437":"markdown","8bc1d022":"markdown","7fa0a602":"markdown","822f458c":"markdown","2009d583":"markdown","c68c981d":"markdown","3fe2f5bd":"markdown","152c67db":"markdown","a780642f":"markdown","49bbc215":"markdown","6a858031":"markdown","37899f92":"markdown","2119b2b5":"markdown","0957cff5":"markdown","05af2df2":"markdown","20383178":"markdown","9e7f1dac":"markdown","0b157634":"markdown"},"source":{"4ec8324c":"!wget --quiet https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d7\/Green_Sea_Turtle_grazing_seagrass.jpg\n!wget --quiet https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/0a\/The_Great_Wave_off_Kanagawa.jpg","7a2ef61f":"import matplotlib.pyplot as plt\nimport tensorflow.keras as kr\nimport tensorflow as tf\nimport numpy as np\nfrom IPython import display\nfrom PIL import Image\n\n\ntf.enable_eager_execution()\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))","33b92562":"CONTENT = 'Green_Sea_Turtle_grazing_seagrass.jpg'\nSTYLE = 'The_Great_Wave_off_Kanagawa.jpg'\n\nIMAGE_HEIGHT = 300\nIMAGE_WIDTH = 400","9d26caba":"content = Image.open(CONTENT)\nstyle = Image.open(STYLE)\n\nplt.figure(figsize=(10, 10))\n\nplt.subplot(1, 2, 1)\nplt.imshow(content)\nplt.title('Content Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(style)\nplt.title('Style Image')\n\nplt.tight_layout()\nplt.show()","eb7e4f09":"def img_parser(filename):\n    img_string = tf.io.read_file(filename)\n    img = tf.image.decode_jpeg(img_string, channels=3)\n    img = tf.cast(img, dtype=tf.float32)\n\n    # Resize the image\n    img = tf.image.resize_images(img, size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n    img = tf.expand_dims(img, axis=0)   # Add batch dimension\n    return img","e93f62d8":"def load_image(filename):\n    img = img_parser(filename)\n    img = kr.applications.vgg19.preprocess_input(img)\n    return img","5657928a":"def deprocess_img(processed_img):\n    x = processed_img.copy()\n    if len(x.shape) == 4:\n        x = np.squeeze(x, 0)\n    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n    if len(x.shape) != 3:\n        raise ValueError(\"Invalid input to deprocessing image\")\n  \n    # perform the inverse of the preprocessing step\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    x = x[:, :, ::-1]  # Convert back to RGB from BGR\n\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","f36c602b":"_vgg = kr.applications.vgg19.VGG19(include_top=False, \n                                   weights=None, \n                                   input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n_vgg.summary()","de15a136":"# Content layer where will pull our feature maps\ncontent_layers = ['block5_conv2'] \n\n# Style layers\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']","426b13a8":"def get_model(styles, contents):\n    \"\"\" Creates our model with access to intermediate layers. \n    Loads the VGG19 model and access the intermediate layers. \n    These layers will then be used to create a new model that will take input image\n    and return the outputs from these intermediate layers from the VGG model.\n    \n    Parameters:\n    -----------\n    styles: list\n        A list containing all style layers from VGG19 model\n    contents: list\n        A list containing all content layers from VGG19 model\n\n    Returns:\n    --------\n    VGG: kr.Model\n        A keras model that takes image inputs and outputs the style and \n        content intermediate layers. \n    \"\"\"\n    \n    vgg = kr.applications.vgg19.VGG19(include_top=False, \n                                      weights='imagenet', \n                                      input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n    vgg.trainable = False\n    \n    # Get output layers corresponding to style and content layers \n    style_outputs = [vgg.get_layer(layer_name).output for layer_name in styles]\n    content_outputs = [vgg.get_layer(layer_name).output for layer_name in contents]\n    model_outputs = style_outputs + content_outputs\n   \n    return kr.Model(vgg.input, model_outputs)","891acb28":"def get_content_loss(content, generated):\n    return tf.reduce_mean(tf.square(content - generated))","b1043a56":"def get_layer_style_loss(style, generated):\n    def gram_matrix(tensor):\n        channels = int(tensor.shape[-1])\n        a = tf.reshape(tensor, [-1, channels])\n        gram = tf.matmul(a, a, transpose_a=True)\n        return gram \/ tf.cast(tf.shape(a)[0], tf.float32)\n\n    gram_style = gram_matrix(style)\n    gram_generated = gram_matrix(generated)\n    return tf.reduce_mean(tf.square(gram_style - gram_generated))\n\n\ndef get_style_loss(style, generated):\n    loss = 0\n    coeffs = [0.2, 0.2, 0.2, 0.2, 0.2]\n    for s, g, coeff in zip(style, generated, coeffs):\n        loss += coeff * get_layer_style_loss(s, g)\n    \n    return loss","55193ef1":"def compute_loss(model, image, style_features, content_features, alpha=0.1, beta=0.002):\n    \"\"\"This function will compute the loss total loss.\n  \n    Parameters:\n    -----------\n    model: kr.Model \n        The model that will give us access to the intermediate layers\n    image: Tensor\n        Initial image. This is what we are updating with the optimization process. \n    style_features: Tensor\n        Precomputed style features from our Style image.\n    content_features: Tensor\n        Precomputed content features from our Content image.\n      \n    Returns:\n    loss: Tensor\n        Returns the total loss\n    \"\"\"\n\n    # Feed our init image through our model.\n    model_outputs = model(image)\n\n    content_generated = [content_layer[0] for content_layer in model_outputs[len(style_layers):]][0]\n    style_generated = [style_layer for style_layer in model_outputs[:len(style_layers)]]\n    \n    content_loss = alpha * get_content_loss(content_features, content_generated)\n    style_loss = beta * get_style_loss(style_features, style_generated)\n\n    # Get total loss\n    loss = style_loss + content_loss\n    return loss","f24469aa":"def compute_grads(cfg):\n    with tf.GradientTape() as tape: \n        loss = compute_loss(**cfg)\n    # Compute gradients with respect to input image\n    return tape.gradient(loss, cfg['image']), loss","e0448e61":"def transfer_style(content_img, style_img, epochs=1000): \n    def generate_noisy_image(content_image, noise_ratio):\n        \"\"\"Generates a noisy image by adding random noise to the content image\"\"\"\n    \n        noise_image = tf.random.uniform([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3], minval=-20, maxval=20)\n        input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n        return input_image\n\n    # We don't want to train any layers of our model \n    model = get_model(style_layers, content_layers) \n    for layer in model.layers:\n        layer.trainable = False\n        \n    S = load_image(style_img)\n    C = load_image(content_img)\n\n    style_outputs = model(S)\n    content_outputs = model(C)\n\n    # Get the style and content feature representations (from our specified intermediate layers) \n    _content = [content_layer[0] for content_layer in content_outputs[len(style_layers):]][0]\n    _style = [style_layer[0] for style_layer in style_outputs[:len(style_layers)]]\n  \n    # Set initial image\n    G = generate_noisy_image(C, 0.6)\n    G = tf.contrib.eager.Variable(G, dtype=tf.float32)\n\n    best_loss, best_img = float('inf'), None\n  \n    # Create a nice config \n    cfg = {\n        'model': model,\n        'image': G,\n        'style_features': _style,\n        'content_features': _content\n    }\n    \n    # Create our optimizer\n    opt = tf.train.AdamOptimizer(learning_rate=2, beta1=0.99, epsilon=1e-1)\n    \n    # For displaying\n    display_interval = epochs\/(2*5)\n  \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        grads, cost = compute_grads(cfg)\n        opt.apply_gradients([(grads, G)])\n        clipped = tf.clip_by_value(G, min_vals, max_vals)\n        G.assign(clipped)\n        \n        if cost < best_loss:\n            best_loss = cost\n            best_img = deprocess_img(G.numpy())\n\n        if i % display_interval== 0:\n            plot_img = G.numpy()\n            plot_img = deprocess_img(plot_img)\n            imgs.append(plot_img)\n            display.clear_output(wait=True)\n            display.display_png(Image.fromarray(plot_img))\n            print('Epoch: {}, LOSS: {:.4e}'.format(i, cost))\n        \n            \n    display.clear_output(wait=True)\n    plt.figure(figsize=(14,4))\n    for i,img in enumerate(imgs):\n        plt.subplot(2, 5, i+1)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n      \n    return best_img, best_loss ","d6eb408d":"best, best_loss = transfer_style(CONTENT, STYLE, epochs=200)","9c042f85":"plt.figure(figsize=(10, 10))\ncontent = Image.open(CONTENT) \nstyle = Image.open(STYLE)\n\nplt.subplot(1, 2, 1)\nplt.imshow(content)\nplt.title('Content Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(style)\nplt.title('Style Image')\n\nplt.figure(figsize=(10, 10))\n\nplt.imshow(best)\nplt.title('Output Image')\nplt.show()","e6b26437":"In order to view the outputs of our optimization, we are required to perform the inverse preprocessing step. Also,  we must clip to maintain our values from within the 0-255 range.   ","8bc1d022":"## Losses\n\nWe will build the NST algorithm in three steps:\n\n- Build the content loss function $L_{content}(C,G)$\n- Build the style loss function $L_{style}(S,G)$\n- Put it together to get $L(G) = \\alpha L_{content}(C,G) + \\beta L_{style}(S,G)$. \n\n### Content Loss","7fa0a602":"# Neural Style Transfer ","822f458c":"## Visualize Outputs ","2009d583":"## Apply Style Transfer to Images","c68c981d":"The content of an image is represented by the values of the intermediate feature maps.\n\nIt turns out, the style of an image can be described by the means and correlations across the different feature maps. We will calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. \n\nAfter generating the Style matrix (Gram matrix), we will minimize the distance between the Gram matrix of the image S and image G. For now, we can derive the loss using only a single hidden layer $a^{[l]}$, and the corresponding style loss for this layer is defined as: \n\n$$L_{style}^{[l]}(S,G) = \\sum_{i,j}(Gram^{(S)}_{ij} - Gram^{(G)}_{ij})^2 $$\n\nwhere $G^{(S)}$ and $G^{(G)}$ are respectively the Gram matrices of the style image and the generated image, computed using the hidden layer activations for a particular hidden layer in the network.\n\nIt turns out that you get more visually pleasing results if you use the style loss function from multiple different layers. So, the overall style cost function, we can define as:\n\n$$L_{style}(S,G) = \\sum_{l} \\lambda^{[l]} L^{[l]}_{style}(S,G)$$\n\nwhere the values for $\\lambda^{[l]}$ are given in `STYLE_LAYERS`. \n","3fe2f5bd":"Neural Style Transfer (NST) is one of the most fun techniques in deep learning. It merges two images, namely, a \"content\" image (C) and a \"style\" image (S), to create a \"generated\" image (G). The generated image G combines the \"content\" of the image C with the \"style\" of image S. \n\nFor example, let\u2019s take an image of this turtle and Katsushika Hokusai's *The Great Wave off Kanagawa*:\n\n<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1560278258\/example.jpg\">\n\nStyle transfer is an interesting technique that showcases the capabilities and internal representations of neural networks.  ","152c67db":"## Conclusions\n\nThis is of course not the deployment ready code, just a basic implementation. This was inspired by the [TensorFlow Neural Style Tutorial](https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/nst_blogpost\/4_Neural_Style_Transfer_with_Eager_Execution.ipynb). Last output is the interpolated version so it doesn't seem to be a high quality image but you can try to change image size in the model from 300x400 to let's say 900x900 to get better results :). Lastly, please upvote my kernel if you find it useful.","a780642f":"## Define Total Loss\n\nThis is where we will combine style and content losses together. \n\n$$L(G) = \\alpha L_{content}(C,G) + \\beta L_{style}(S,G)$$ ","49bbc215":"## Visualize the input","6a858031":"## Build the Model \n\nLet's create a model that will take an input image and output the content and style intermediate layers","37899f92":"### Gradient Tape and Eager Execution\n\nA lot of things are changing with TensorFlow 2.0 like default execution mode changing from Graph which is where we build our graph and then execute it to eager which is where the operations are executed immediately like a normal Python variable. The problem we are dealing with here is not the build a model and use it to make predictions but rather use an existing model to extract feature maps and then use them to generate a new image. What we will be doing is that using the error we calculated just now to apply it on our G image which is a noisy image or could be the content image itself. Remember we use optimizer to minimize the loss and that would train our network?. The idea behind that is that optimizer calls two function behind the minizie function:\n\n- Compute gradients\n- Apply gradients\n\n**Compute Gradients**: This is where tensorflow helps us the most by calculating the derivaties between the output and input according to our loss function.\n\n**Apply Gradients**: After the gradients were calculated, optimize calls this function to apply gradients to trainable variables in our graph. That's how the backpropagation works :) \n\nNow, back to our problem. We need to be able to retrieve to gradients and distribute them to G image to change its content. So, everything sounds fine in Graph mode but what about the eager execution mode? In eager mode, because there's no graph, the optimize can't know which values to follow and apply gradients. So we use Gradient tape to record our gradient process so then when we call the gradients function it will be okay to compute it. Now, I still wanted to make this in Graph mode by using compute gradients and apply gradients seperately but it just didn't work as I expected. So I switched to eager mode to make it work. You are welcome to give it a try. ","2119b2b5":"## Setup\n\n### Download Images","0957cff5":"## Style Loss","05af2df2":"Our content loss function is actually quite simple. We will try to measure how much is the G(Generated image) and C(Content image) are similar to each other. Now, we will set the image C as the input to the pretrained VGG network, and run forward propagation. $a^{(C)}$ will be the hidden layer activations in the layer we had chosen. This will be a 3D Tensor with the shape of [H, W, C]. Then we'll repeat this process with the image G as well. Therefore $a^{(G)}$ will be the corresponding hidden layer activation. We then can define as the content loss function as:\n\n$$L_{content}(C,G) = \\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2 $$","20383178":"After the computation of gradients, we need to apply them to our G image. We will use optimizer's `apply_gradients` function to do that. Also note that, in eager mode we can't use the `tf.Variable` and we need to wrap our G image to a variable to calculate the gradients. That's why we will use the `tf.contrib.eager.Variable` to create one when in eager mode.","9e7f1dac":"## Preprocess\n\nWe will be using VGG19 model using Transfer learning as it was done in the paper. One thing to note that we need to process our images into image preprocessing step. VGG networks( and other ImageNet models) are trained on image with each channel normalized by `mean = [103.939, 116.779, 123.68]`and with channels BGR. Although it's not a complicated function to write, it's already written in keras so we don't need to do it ourselves.","0b157634":"### Define content and style representations\n\nWe will use the intermediate layers of the model to get the content and style representations of the image. \n\n**Why intermediate layers?**\n\nAt a high level, in order for a network to perform image classification, it must understand the image. This involves taking the raw image as input pixels and building an internal representation through transformations that turn the raw image pixels into a complex understanding of the features present within the image. Starting from the network's input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level features. These intermediate layers are necessary to define the representation of content and style from the images. The model serves as a complex feature extractor; hence by accessing intermediate layers, we\u2019re able to describe the content and style of input images. \n\nWe would like the \"generated\" image G to have similar content as the input image C. Suppose we have chosen some layer's activations to represent the content of an image. We'll get the most visually pleasing results if we choose a layer in the middle of the network--neither too shallow nor too deep. (Feel free to experiment with using different layers, to see how the results vary.)"}}