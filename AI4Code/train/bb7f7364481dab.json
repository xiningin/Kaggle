{"cell_type":{"5ee33942":"code","e9b5e126":"code","f92d5c8a":"code","ed37a340":"code","749fc2db":"code","80c7c715":"code","65a70518":"code","eb4ac983":"code","d7e6cf42":"code","a511b0f8":"code","2e610c4b":"code","56161422":"code","19b19651":"code","e401ae61":"code","dee55aeb":"code","d44c438b":"code","f9b664f0":"code","662b51ab":"code","0f35b037":"code","980b9de8":"code","8db0cbd7":"code","def8708a":"code","4423d1f9":"code","eb6a9197":"code","2c8c6269":"code","24023382":"code","083dc44c":"code","0e2eeba7":"markdown","111f281b":"markdown","0fbf6300":"markdown","ad30b071":"markdown","aac3dd31":"markdown","c0ed3482":"markdown","bb1dddb9":"markdown","48dfb017":"markdown","3976784b":"markdown","86b77db3":"markdown","0fbe7e0e":"markdown","de5e478e":"markdown","a52877d9":"markdown","70d4fcef":"markdown","8a9319fb":"markdown"},"source":{"5ee33942":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier","e9b5e126":"#Loading the dataset..\ndf = pd.read_csv('..\/input\/mushroom.csv')","f92d5c8a":"df.head()","ed37a340":"df.columns","749fc2db":"# converting the data from categorical to ordinal ..\nlabelencoder=LabelEncoder()\nfor column in df.columns:\n    df[column] = labelencoder.fit_transform(df[column])","80c7c715":"#checking the information of the dataset......\ndf.info()","65a70518":"#dropping the column \"veil-type\" is 0 \ndf=df.drop([\"veil-type\"],axis=1)\n","eb4ac983":"df.head()","d7e6cf42":"df.describe()","a511b0f8":"#Question a:\nplt.figure()\npd.Series(df['edible']).value_counts().sort_index().plot(kind = 'bar')\nplt.ylabel(\"Count\")\nplt.xlabel(\"edible\")\nplt.title('Number of poisonous\/edible mushrooms (0=edible, 1=poisonous)');\n","2e610c4b":"plt.figure(figsize=(14,12))\nsns.heatmap(df.corr(),linewidths=.1, annot=True)\nplt.yticks(rotation=0);","56161422":"df.corr()","19b19651":"df[['edible', 'gill-color']].groupby(['gill-color'], as_index=False).mean().sort_values(by='edible', ascending=False)","e401ae61":"#Looking closely at the feature 'gill-color'\nnew_var=df[['edible', 'gill-color']]\nnew_var=new_var[new_var['gill-color']<=3.5]\nsns.factorplot('edible', col='gill-color', data=new_var, kind='count', size=2.5, aspect=.8, col_wrap=4);","dee55aeb":"new_var=df[['edible', 'gill-color']]\nnew_var=new_var[new_var['gill-color']>3.5]\n\nsns.factorplot('edible', col='gill-color', data=new_var, kind='count', size=2.5, aspect=.8, col_wrap=4);","d44c438b":"X=df.drop(['edible'], axis=1)\nY=df['edible']","f9b664f0":"X_train, X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.1)","662b51ab":"# Building and fitting my_forest\nforest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\nmy_forest = forest.fit(X_train, Y_train)\n\n# Print the score of the fitted random forest\nprint(my_forest.score(X, Y)*100)\nacc_randomforest=(my_forest.score(X, Y)*100)","0f35b037":"def plot_confusion_matrix(df, title='Confusion matrix', cmap=plt.cm.gray_r):\n    plt.matshow(df, cmap=cmap)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n\nplot_confusion_matrix(df)","980b9de8":"clf = DecisionTreeClassifier()\nclf = clf.fit(X_train, Y_train)","8db0cbd7":"features_list = X.columns.values\nfeature_importance = clf.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\nplt.figure(figsize=(5,7))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\nplt.xlabel('Importance')\nplt.title('Feature importances')\nplt.draw()\nplt.show()\n","def8708a":"X=df.drop(['edible'], axis=1)\nY=df['edible']\ny_pred=clf.predict(X_test)","4423d1f9":"X_train, X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.1)","eb6a9197":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred7 = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\nacc_decision_tree","2c8c6269":"cfm=confusion_matrix(Y_test, y_pred)\n\nsns.heatmap(cfm, annot = True,  linewidths=.5, cbar =None)\nplt.title('Decision Tree Classifier confusion matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","24023382":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=3, n_neighbors=10, p=2, \n                           weights='uniform')\nknn.fit(X_train, Y_train)\nknn_predictions = knn.predict(X_test)\nacc_knn = round(knn.score(X_test, Y_test) * 100, 2)\nacc_knn","083dc44c":"objects = ('Decision Tree', 'Random Forest','KNN Model ')\nx_pos = np.arange(len(objects))\naccuracies1 = [acc_decision_tree, acc_randomforest, acc_knn]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='b')\nplt.xticks(x_pos, objects, rotation='vertical')\nplt.ylabel('Accuracy')\nplt.title('Classifier Outcome')\nplt.show()","0e2eeba7":">>** So if my friend who is a data analalyst as well as a Machine Learning enthu , says that using neural network with 3 layers containg 3 layers with 10 nodes each will give me a better accuracy then as we can see above that using both decision tree classifier as well as random forest models we get the accuracy as 100 % and using the KNN model the accuracy is 99.75 %**","111f281b":"# Splitting the data into test and train (Using Random forest classifier)","0fbf6300":">> So we have a good model that has 100% accuracy in predicting if a person will survive or not.","ad30b071":"This is a heatmap which shows the correlation between the variables ","aac3dd31":"**Based on the above data visualization, we can use machine learning on the dataset because the dataset becomes balanced now **","c0ed3482":"So we can observe fromthe above graph that the classification of different type of mushrooms (Edible vs non edible ), not let us check the chances of survival from this graph.","bb1dddb9":"# Confusion matrix","48dfb017":"# Using KNN ","3976784b":"I would prefer using the Random Forest classifier as well as the Decesion Tree Models to get the prediction because trees are easy to understand .","86b77db3":"> Let us use the Random Forest Classifier on this data to check the accuracy ","0fbe7e0e":"> By all methods examined before the feature that is most important is \"gill-color\".","de5e478e":">> **IMPORTING THE REQUIRED MODULES **","a52877d9":"# So we can see that Using Both the models the accuracy is 100% ","70d4fcef":"# Let us use a decision tree classifier for the same dataset to compare between the two machine learning models ","8a9319fb":"**Usually the least correlating variable is the most important one for classification. In this case, \"gill-color\" has -0.53 so let's look at it closely:**"}}