{"cell_type":{"4175a82d":"code","8abca0c6":"code","6ef773b9":"code","e1ffb713":"code","fd4a25f0":"code","af4274be":"code","d28b1690":"code","1a5997ae":"code","8710c1fe":"code","efa67823":"code","588898b6":"code","52fbca88":"code","9702862d":"code","eecceebd":"code","5cae92e4":"code","f8f9a778":"code","d8df9dce":"code","c0c2d48a":"code","0796d8ec":"code","967f5cc9":"code","949c2e71":"code","fcc6628f":"code","3cc8e5fe":"code","a11b70f9":"code","cd064946":"code","cb7f526b":"code","c018dd1e":"code","67d997e2":"code","3df133bf":"code","4ba2b01f":"code","1d975b11":"code","b472d787":"code","f4bfb53b":"code","760a1994":"code","d68f84f7":"code","c4ddf31b":"code","64b59174":"code","355ae785":"code","d7eb5c72":"code","a885eaf1":"code","d11c0d0c":"code","d4358d63":"code","16c30998":"code","83e7da65":"code","61e8bb1f":"code","a3b2ff30":"code","213d8aa3":"code","ac1c6d76":"code","cbbc3492":"code","20864a37":"code","aff0dd99":"code","2ae72439":"code","88ed7190":"code","257a56f8":"code","2bdac89d":"code","24875879":"markdown","2b980dc9":"markdown","104ddd33":"markdown","377f0186":"markdown","f0633f93":"markdown","30e5ecef":"markdown","59a5c10a":"markdown","29808d5c":"markdown","cad616b9":"markdown","444dab35":"markdown","57d27421":"markdown","c77d2a3c":"markdown","e5540c64":"markdown","c38cb037":"markdown","d67ec032":"markdown","26e3e5fd":"markdown","7aa74fc8":"markdown","4bae7992":"markdown","b5b43797":"markdown","15f9c509":"markdown","058bdc38":"markdown","91ee0851":"markdown","eb10f7ca":"markdown","29fb2370":"markdown","fea073b5":"markdown","11c3eb12":"markdown","4cf6aac7":"markdown","126cb88a":"markdown","36a8f6bb":"markdown","fecc9260":"markdown","8d6a75f3":"markdown","691e8b1d":"markdown","c487d7bf":"markdown","0d4a9f1b":"markdown","e61d291a":"markdown","352c40e3":"markdown","44cfbe99":"markdown","2a4898c9":"markdown","2d64120f":"markdown","969ab836":"markdown","9d860271":"markdown","cf807338":"markdown","fdd6e007":"markdown","116c980f":"markdown","ae4f57ae":"markdown","90054e49":"markdown"},"source":{"4175a82d":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport random\nrandom.seed(42)","8abca0c6":"credits = pd.read_csv('..\/input\/tmdb_5000_credits.csv', index_col='movie_id')\nmovies = pd.read_csv('..\/input\/tmdb_5000_movies.csv', index_col='id')\n\ndata = pd.merge(movies, credits)\nprint(data.shape)\n\ndata = data.loc[data['revenue'] != 0]\ndata['revenue'].dropna(inplace=True)\nprint(data.shape)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['revenue'], axis=1), data['revenue']) \n\nExploratory = X_train.copy() # I'm using the copy of the data (not the view!) just in case, not to mess with the original dataset.","6ef773b9":"nan_percent = Exploratory.isna().mean()*100\nnan_count = Exploratory.isna().sum()\npd.concat([nan_count.rename('missing_count'), nan_percent.round().rename('missing_percent')], axis=1)","e1ffb713":"columns_to_drop = ['original_title', 'overview', 'tagline', 'title']\n# original_title\/title - not informative\n# overview\/tagline - similar features may be found in 'keywords'\n\nExploratory = Exploratory.drop(columns_to_drop, axis=1)","fd4a25f0":"dtypes_description = pd.Series(['ratio', 'nominal', 'nominal', 'nominal', 'nominal', 'ratio', 'nominal', 'nominal', \\\n                     'interval', 'ratio', 'nominal', 'nominal', 'ratio', 'ratio', 'nominal', 'nominal'], \\\n                     index=Exploratory.dtypes.index)\n\npd.concat([Exploratory.dtypes.rename('dtype'), Exploratory.iloc[420].rename('example'), dtypes_description.rename('description')], axis=1)","af4274be":"Exploratory[['genres', 'spoken_languages', 'crew']].head()","d28b1690":"Exploratory[['homepage', 'original_language', 'status']].head()","1a5997ae":"Exploratory['cast'].head().to_frame()","8710c1fe":"Exploratory['release_date'].head().to_frame()","efa67823":"from sklearn.base import BaseEstimator, TransformerMixin","588898b6":"class FeatureSelector(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X[self.feature_names]","52fbca88":"prod_companies = FeatureSelector('production_companies').fit_transform(Exploratory)\nprod_companies.to_frame().head()","9702862d":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n\ndef extract_items(list_, key, all_=True):\n    sub = lambda x: re.sub(r'[^A-Za-z0-9]', '_', x)\n    if all_:\n        target = []\n        for dict_ in eval(list_):\n            target.append(sub(dict_[key].strip()))\n        return ' '.join(target)\n    elif not eval(list_):\n        return 'no_data'\n    else:\n        return sub(eval(list_)[0][key].strip())\n\nclass DictionaryVectorizer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, key, all_=True):\n        self.key = key\n        self.all = all_\n    \n    def fit(self, X, y=None):\n        genres = X.apply(lambda x: extract_items(x, self.key, self.all))\n        self.vectorizer = CountVectorizer().fit(genres)        \n        self.columns = self.vectorizer.get_feature_names()\n        return self\n        \n    def transform(self, X):\n        genres = X.apply(lambda x: extract_items(x, self.key))\n        data = self.vectorizer.transform(genres)\n        return pd.DataFrame(data.toarray(), columns=self.vectorizer.get_feature_names(), index=X.index)","eecceebd":"prod_companies_vectorized = DictionaryVectorizer('name').fit_transform(prod_companies)\nprod_companies_vectorized.head()","5cae92e4":"class TopFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, percent):\n        if percent > 100:\n            self.percent = 100\n        else:\n            self.percent = percent\n    \n    def fit(self, X, y=None):\n        counts = X.sum().sort_values(ascending=False)\n        index_ = int(counts.shape[0]*self.percent\/100)\n        self.columns = counts[:index_].index\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]","f8f9a778":"top_companies = TopFeatures(1).fit_transform(prod_companies_vectorized)\ntop_companies.head()","d8df9dce":"class SumTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, series_name):\n        self.series_name = series_name\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.sum(axis=1).to_frame(self.series_name)","c0c2d48a":"companies_count = SumTransformer('companies_count').fit_transform(prod_companies_vectorized)\ncompanies_count.head()","0796d8ec":"class Binarizer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, condition, name):\n        self.condition = condition\n        self.name = name\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X.apply(lambda x : int(self.condition(x))).to_frame(self.name)","967f5cc9":"missing_homepage = Binarizer(lambda x: isinstance(x, float), 'missing_homepage').fit_transform(Exploratory['homepage'])\nmissing_homepage.head(15)","949c2e71":"from datetime import datetime\n\ndef get_year(date):\n    return datetime.strptime(date, '%Y-%m-%d').year\n\ndef get_month(date):\n    return datetime.strptime(date, '%Y-%m-%d').strftime('%b')\n\ndef get_weekday(date):\n    return datetime.strptime(date, '%Y-%m-%d').strftime('%a')\n\nclass DateTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        year = X.apply(get_year).rename('year')\n        month = pd.get_dummies(X.apply(get_month))\n        day = pd.get_dummies(X.apply(get_weekday))\n        return pd.concat([year, month, day], axis=1)        ","fcc6628f":"date = DateTransformer().fit_transform(Exploratory['release_date'])\ndate.head()","3cc8e5fe":"def get_list_len(list_):\n    return len(eval(list_))\n\nclass ItemCounter(BaseEstimator, TransformerMixin):\n        \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.apply(lambda x: int(get_list_len(x)))","a11b70f9":"language_count = ItemCounter().fit_transform(Exploratory['spoken_languages'])\nlanguage_count.head().to_frame('language_count')","cd064946":"year = DateTransformer().fit_transform(Exploratory['release_date'])['year']\ntop_cast_count = make_pipeline(FeatureSelector('cast'), DictionaryVectorizer('name'), \n                               TopFeatures(0.25), SumTransformer('top_cast_count')).fit_transform(Exploratory)","cb7f526b":"notional_to_numeric = pd.concat([year, top_cast_count], axis=1)\nnotional_to_numeric.head(15)","c018dd1e":"numeric = pd.concat([Exploratory.select_dtypes(['int64', 'float64']), notional_to_numeric], axis=1)\n\nnumeric.hist(figsize=(15,15), bins=25)","67d997e2":"numeric.corr().style.background_gradient(cmap='coolwarm')","3df133bf":"numeric.plot(kind='scatter', x='popularity', y='vote_count')\npossible_outliers = Exploratory[Exploratory['popularity'] > 400]\n\nnumeric[['popularity', 'vote_count']] = np.log(Exploratory[['popularity', 'vote_count']] + 1)\nnumeric.plot(kind='scatter', x='popularity', y='vote_count')","4ba2b01f":"possible_outliers","1d975b11":"numeric.corr().style.background_gradient(cmap='coolwarm')","b472d787":"class MeanTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, name):\n        self.name = name\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X.mean(axis=1).to_frame(self.name)","f4bfb53b":"feature_mean = make_pipeline(FeatureSelector(['vote_count', 'popularity']), MeanTransformer('popularity_vote')).fit_transform(Exploratory)\nfeature_mean.head()","760a1994":"numeric['vote_popularity'] = feature_mean\nnumeric.drop(columns=['popularity', 'vote_count'], inplace=True)","d68f84f7":"sns.pairplot(numeric)","c4ddf31b":"from scipy.stats import pearsonr\n\ntransformations = [lambda x: x, np.sqrt, lambda x: np.log(x+1)]\ntran_description = [' no transformation', ' sqrt', ' log']\nnumeric_columns = numeric.columns\n\nfig, axes = plt.subplots(len(numeric_columns), len(transformations), figsize=(20,15))\nfig.tight_layout()\n\nfor col_idx, col in enumerate(numeric_columns):\n    for tran_idx, tran in enumerate(transformations):\n        axes[col_idx, tran_idx].scatter(x=numeric[col], y=tran(y_train))\n        axes[col_idx, tran_idx].set_xticklabels([])\n        axes[col_idx, tran_idx].set_xticks([]) \n        R2 = pearsonr(numeric[col], tran(y_train))[0]**2     \n        axes[col_idx, tran_idx].title.set_text(f'{col}, {tran_description[tran_idx]} \\n R2 coefficient: {R2:.2f}')\n               \nplt.show()","64b59174":"from sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.pipeline import FeatureUnion, _fit_transform_one, _transform_one, _name_estimators\nfrom scipy import sparse\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass PandasFeatureUnion(FeatureUnion):\n    def fit_transform(self, X, y=None, **fit_params):\n        self._validate_transformers()\n        result = Parallel(n_jobs=self.n_jobs)(\n            delayed(_fit_transform_one)(\n                transformer=trans,\n                X=X,\n                y=y,\n                weight=weight,\n                **fit_params)\n            for name, trans, weight in self._iter())\n\n        if not result:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        Xs, transformers = zip(*result)\n        self._update_transformer_list(transformers)\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n\n    def merge_dataframes_by_column(self, Xs):\n        return pd.concat(Xs, axis=\"columns\", copy=False)\n\n    def transform(self, X):\n        Xs = Parallel(n_jobs=self.n_jobs)(\n            delayed(_transform_one)(\n                transformer=trans,\n                X=X,\n                y=None,\n                weight=weight)\n            for name, trans, weight in self._iter())\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n    \ndef make_union(*transformers, **kwargs):\n    n_jobs = kwargs.pop('n_jobs', None)\n    verbose = kwargs.pop('verbose', False)\n    if kwargs:\n        # We do not currently support `transformer_weights` as we may want to\n        # change its type spec in make_union\n        raise TypeError('Unknown keyword arguments: \"{}\"'\n                        .format(list(kwargs.keys())[0]))\n    return PandasFeatureUnion(\n        _name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)","355ae785":"union = make_union(\n    make_pipeline(\n        FeatureSelector('genres'),\n        DictionaryVectorizer('name')\n    ),\n    make_pipeline(\n        FeatureSelector('homepage'),\n        Binarizer(lambda x: isinstance(x, float), 'missing_homepage')\n    ),\n    make_pipeline(\n        FeatureSelector('keywords'),\n        DictionaryVectorizer('name'),\n        TopFeatures(0.5)\n    ),\n    make_pipeline(\n        FeatureSelector('original_language'),\n        Binarizer(lambda x: x == 'en', 'en')\n    ),\n    make_pipeline(\n        FeatureSelector('production_companies'),\n        DictionaryVectorizer('name'),\n        TopFeatures(1)\n    ),\n    make_pipeline(\n        FeatureSelector('production_countries'),\n        DictionaryVectorizer('name'),\n        TopFeatures(25)\n    ),\n    make_pipeline(\n        FeatureSelector('release_date'),\n        DateTransformer()\n    ),\n    make_pipeline(\n        FeatureSelector('spoken_languages'),\n        ItemCounter(),\n        Binarizer(lambda x: x > 1, 'multilingual')\n    ),\n    make_pipeline(\n        FeatureSelector('original_language'),\n        Binarizer(lambda x: x == 'Released', 'Released')\n    ),    \n    make_pipeline(\n        FeatureSelector('cast'),\n        DictionaryVectorizer('name'),\n        TopFeatures(0.25),\n        SumTransformer('top_cast_count')\n    ),\n    make_pipeline(\n        FeatureSelector('crew'),\n        DictionaryVectorizer('name', False),\n        TopFeatures(1)\n    ),\n    make_pipeline(\n        FeatureSelector(['budget', 'runtime', 'vote_average'])\n    ),\n    make_pipeline(\n        FeatureSelector(['popularity', 'vote_count']),\n        MeanTransformer('popularity_vote')\n    )\n)","d7eb5c72":"union.fit(X_train)\n\nX_train_T = union.transform(X_train)\nX_test_T = union.transform(X_test)\n\nprint(X_train_T.shape)\nprint(X_test_T.shape)","a885eaf1":"X_train_T.head()","d11c0d0c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","d4358d63":"lin_params = dict(alpha=np.logspace(1,7,7), normalize=(False, True))\nfor_params = dict(n_estimators=np.linspace(10,40,4).astype(int), min_samples_split=(2,3), min_samples_leaf=(1,2,3))\ngbr_params = dict(n_estimators=np.linspace(100,300,3).astype(int), min_samples_split=(2,3))","16c30998":"ridge_grid = GridSearchCV(Ridge(random_state=42), lin_params, cv=10)\nforest_grid = GridSearchCV(RandomForestRegressor(random_state=42), for_params, cv=10)\ngbr_grid = GridSearchCV(GradientBoostingRegressor(random_state=42), gbr_params, cv=10)","83e7da65":"ridge_grid.fit(X_train_T, y_train)","61e8bb1f":"forest_grid.fit(X_train_T, y_train)","a3b2ff30":"gbr_grid.fit(X_train_T, y_train)","213d8aa3":"print(f'Ridge:\\n\\t *best params: {ridge_grid.best_params_}\\n\\t *best score: {ridge_grid.best_score_}')\nprint(f'Forest:\\n\\t *best params: {forest_grid.best_params_}\\n\\t *best score: {forest_grid.best_score_}')\nprint(f'Gradient Boost:\\n\\t *best params: {gbr_grid.best_params_}\\n\\t *best score: {gbr_grid.best_score_}')","ac1c6d76":"best_ridge = Ridge(alpha=100, normalize=False)\nbest_forest = RandomForestRegressor(min_samples_leaf=3, min_samples_split=2, n_estimators=40)\nbest_gbr = GradientBoostingRegressor(min_samples_split=2, n_estimators=300)","cbbc3492":"from sklearn.metrics import r2_score","20864a37":"best_ridge.fit(X_train_T, y_train)\npredicted = best_ridge.predict(X_test_T)\n\nprint(f'Ridge test score: {r2_score(y_test, predicted)}')\n\nbest_forest.fit(X_train_T, y_train)\npredicted = best_forest.predict(X_test_T)\n\nprint(f'Random Forest test score: {r2_score(y_test, predicted)}')\n\nbest_gbr.fit(X_train_T, y_train)\npredicted = best_gbr.predict(X_test_T)\n\nprint(f'Gradient Boosted Regressor test score: {r2_score(y_test, predicted)}')","aff0dd99":"ridge_coefs_df = pd.DataFrame(dict(score=best_ridge.coef_, column=X_test_T.columns))\nridge_coefs_df.sort_values(['score'], ascending=False).head(10)","2ae72439":"print(f'Train target variable mean: ${round(y_train.mean()):,}.')","88ed7190":"ridge_coefs_df.loc[136:]","257a56f8":"pd.DataFrame(dict(score=best_forest.feature_importances_, column=X_test_T.columns)).sort_values(['score'], ascending=False).head(10)","2bdac89d":"pd.DataFrame(dict(score=best_gbr.feature_importances_, column=X_test_T.columns)).sort_values(['score'], ascending=False).head(10)","24875879":"Now we just need to apply transformations on each column we intend to work on.","2b980dc9":"Now we're left with uncorrelated columns. That's good news as we don't actually count any feature 'twice'.\n\nMaybe we also should transform our target value, so our data fits it better?","104ddd33":"What's left now is to fit our pipeline on our training data and transform both train and test data.","377f0186":"Seems like everything looks fine, although the data is skewed.","f0633f93":"## 2.2. Model Selection\n\nNow that we have the data set in the form we wanted, let's fit some models and see wich performs best. `Sklearn` provides very useful utilities for this purpose, namely Grid Search CV. It performs a search of best parameters provided by us, using cross-validation.","30e5ecef":"Luckily only two features have any missing values.\n\nLet's think if we really need all the features we have so far. I believe the *title* won't tell us much about future revenue and we already have a similar feature to *overview* and *tagline*, which is *keywords*, so let's get rid of those.","59a5c10a":"I'm dropping observations where target data (*revenue*) is missing, so I'm only left with data I can use for predictions.","29808d5c":"# Conclusion\n\n`Pipelines` can be thought of as a useful way to transform and model your data. If used correctly, can save a lot of unnecessary lines of code and unexpected issues, as data leakage. I believe that proficiency in those can make workflow more smooth and the code readable and easy to maintain.","cad616b9":"Now that we have the best models with the best parameters, let's find out how they perform on test data.","444dab35":"# Table of Contents\n\n* [Introduction](#Introduction)\n* [1. EDA & Data Transformation](#1.-EDA-&-Data-Transformation)\n  * [1.1. Checking missing values](#1.1.-Checking-missing-values)\n  * [1.2. Measurement Scales](#1.2.-Measurement-Scales)\n  * [1.3. Nominal Data](#1.3.-Nominal-Data)\n    * [1.3.1. Dummifying](#1.3.1.-Dummifying)\n    * [1.3.2. Binarizaiton](#1.3.2.-Binarizaiton)\n    * [1.3.3. Counts](#1.3.3.-Counts)\n    * [1.3.4. Date Extraction](#1.3.4.-Date-Extraction)\n  * [1.4. Nominal Data Transformation](#1.4.-Nominal-Data-Transformation)\n    * [1.4.1. Feature Selector](#1.4.1.-Feature-Selector)\n    * [1.4.2. Dictionary Vectorizer](#1.4.2.-Dictionary-Vectorizer)\n    * [1.4.3. Top Features](#1.4.3.-Top-Features)\n    * [1.4.4. Sum Transformer](#1.4.4.-Sum-Transformer)\n    * [1.4.5. Binarizer](#1.4.5.-Binarizer)\n    * [1.4.6. Date Transformer](#1.4.6.-Date-Transformer)\n    * [1.4.7. Item Counter](#1.4.7.-Item-Counter)\n  * [1.5. Numerical data](#1.5.-Numerical-data)\n* [2. Building a Pipeline](#2.-Building-a-Pipeline)\n  * [2.1. Feature Union](#2.1.-Feature-Union)\n  * [2.2. Model Selection](#2.2.-Model-Selection)\n* [Conclusion](#Conclusion)\n* [References](#References)","57d27421":"### 1.3.3. Counts\n\nAs an example, we'll count how many popular actors (having most appearances) are cast in a movie. Perhaps the more of them playing in one movie, the higher is the revenue...","c77d2a3c":"# Introduction\n\nIn this kernel, I'll focus on feature engineering using `sklearn pipelines`. \n\nWhat are `pipelines`?. In short `pipelines` are ways to organize your transformers in a manageable, linear way. I'd like to think about each `pipeline` as a list of step-by-step instructions to transform your data. More information, you can find in [sklearn.Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) documentation.\n\nI noticed once you will get used to them, you can quickly and easily deal with data imputation and transformation. Moreover, it prevents data leakage and you only write transformer once - you can easily fit (and if needed transform as well) it on the training dataset and use it on the test set.\n\nHere are some resources I find useful to explain what `pipelines` are and how to use them:\n* [Kevin Goetsch - Deploying Machine Learning using sklearn pipelines](https:\/\/www.youtube.com\/watch?v=URdnFlZnlaE)\n* [Julie Michelman - Pandas, Pipelines, and Custom Transformers](https:\/\/www.youtube.com\/watch?v=BFaadIqWlAg)\n\nAs this is my first data science project I'm aiming to establish a clear and consistent workflow for future projects, so I'd say this notebook is rather directed towards beginners, looking for an inspiration\/reference.\n\n<br\/><br\/>\n\n**The goal of the project is to predict revenue of a movie using TMDB 5000 Movie Dataset.**\n\n<br\/><br\/>\n\nLet's start with imports and train\/test split!","e5540c64":"What we can conclude is that *popularity_vote* and *budget* are the strongest predictors, where the importance of other features is almost insignificant.","c38cb037":"### 1.3.2. Binarizaiton\n\nHere we'll simple binarize the data - the column will get label `True` or `False` (or `1` or `0`) on certain, established condition.\n\n**Columns to binarize**:\n\n*homepage, original_language, status, spoken_languages*","d67ec032":"## 1.2. Measurement Scales\n\nThis overview of a random sample shows us what kind of data we are dealing with. Six columns are a list of dictionaries (genres, keywords, production_companies, production_countries, spoken_languages, cast, and crew). We also have five numerical columns (budget, popularity, runtime, vote_average, and vote_count) and other string columns, which are labeled as an object - original_language, release_date, and status.\n\nYou probably noticed I labeled each column with its type. Each type refers to the Measurement Scale. In short, these scales refer to the quality of the data, where:\n* **ratio** - it's a numerical scale with absolute zero, for example, age;\n* **interval** - it's also a numerical scale, but without absolute zero, as it is the case for Fahrenheit scale. For temperature measurement Kelvin would be a ratio scale;\n* **ordinal** - which is not present in our data set, refers to measurements you can put in order, but you cannot tell the quantitive difference between adjacent measurements;\n* **nominal** - in this scale each item is treated as having the same quality, for example, city names;\n\nMore detailed overview of measurement scales you can find in [Multivariate Data Analysis](https:\/\/www.pearson.com\/us\/higher-education\/program\/Hair-Multivariate-Data-Analysis-7th-Edition\/PGM263675.html) book.","26e3e5fd":"### 1.4.3. Top Features\n\nThis transformer expects dummified data set and extract most popular features.","7aa74fc8":"## 2.1. Feature Union\n\nTo combine the data, we need a class to do this for us. Unfortunately `sklearn` doesn't provide a class that works out of the box with `Pandas`, as we would expect. Instead `sklearn` Feature Union takes `Pandas` Data Frame as input and gave `numpy` array as output and we would like to have `Pandas` Data Frame as output as well. In order to do this, we need to modify `sklearn` source code, so it works as intended. Luckily someone has already done that for us. \n\nTo learn more you can read this [blog post](https:\/\/zablo.net\/blog\/post\/pandas-dataframe-in-scikit-learn-feature-union\/) by Marcin Zab\u0142ocki, along with the [source code](https:\/\/github.com\/marrrcin\/pandas-feature-union\/blob\/master\/pandas_feature_union.py).","4bae7992":"After our transformation *vote_count* and *popularity* are even more correlated. It's time to combine them into one feature. I believe taking their average is good enough.","b5b43797":"As I find *tagline* uninformative, I got rid of it completely, therefore we're left with only *homepage* column as the only feature having missing values. As the *homepage* feature has about 60% of missing data, we can, later on, binarize this column on the criteria whether or not a movie has a homepage (**True** if a movie has a homepage **False** if a homepage is missing).","15f9c509":"# 2. Building a Pipeline\n\nNow that we've dealt with transformers, it's time to combine them into a `pipeline`. What will we do now is we apply our transformers to certain columns and we will combine those transformed data into one `Data Frame`.","058bdc38":"We also could notice some outliers. Let's take a look at observations with popularity higher than 400.","91ee0851":"We see we have two features (*popularity* and *vote_count*) that are strongly correlated. Let's take a closer look.","eb10f7ca":"## 1.5. Numerical data\n\nNow that we've dealt with nominal data it is time to take care of numerical data. Due to use of our transformers, we have new numerical columns: *year* and *top_cast_count*.","29fb2370":"# 1.4. Nominal Data Transformation\n\nLet's now get to writing actual transformers to, well, transform the data. First, we need to import certain classes our custom transformers need to inherit from.","fea073b5":"We had to take care of heteroscedasticity. Luckily log transformation took care of it. Now we have more or less the same variance of residuals across all values.","11c3eb12":"## 1.3. Nominal Data\n\nLet's now try to come up with a plan to deal with our nominal data. As the data comes with multiple different forms, we have a wide field of options on how to deal with it.","4cf6aac7":"Top coefficients refer to our dummy variables. How can we interpret this? Basically, it shows how a variable differs from a global mean. As the mean of the target variable is counted in hundreds of millions of dollars, no wonder that those values are so high! In addition to those coefficients, Ridge Regression has also regularization terms, that weaken those coefficients.\n\nWhat about the numerical values?","126cb88a":"We can also interpret each model by looking at its coefficients.","36a8f6bb":"### 1.3.1. Dummifying\n\nFor most of the data coming in the form of a list of dictionaries, we'll simply extract fields that interest us, and dummify them. In some cases, to avoid sparsification, we'll choose some fraction of the most occurring values.\n\n**Columns to dummify**:\n\n*genres, keywords production_companies, production_countries, crew*","fecc9260":"It seems that we're dealing with huge blockbusters here. I guess there's nothing to worry about in this case.","8d6a75f3":"Now that our models are fit on test data, let's take a look which one performs the best, along with its best parameters chosen by Grid Search.","691e8b1d":"Looks like it's not worth the hassle, as we get similar, or lower R2 scores for transformed target data.","c487d7bf":"### 1.4.4. Sum Transformer\n\nSum Transformer simply computes a sum across given features. We'll use it on our sparse data (after dummification).","0d4a9f1b":"First, we import models, and then we set parameters for each model in the form of a dictionary.\n\n**Attention!** You can also put models in `pipelines`, but I find this approach a bit messy, so I decided to implement them separately.","e61d291a":"### 1.4.1. Feature Selector\n\nThis transformer is really straightforward - it simply takes the name of the column we want to extract and if we use it, it will 'spit out' the data column of our Data Frame.","352c40e3":"And here is our result:","44cfbe99":"### 1.3.4. Date Extraction\n\nIn this data set, we have *release_date* in a string form. Probably it will be better if we extract from it: year, month and day, and dummify the latter two.","2a4898c9":"Let's take a look if we have any abnormal values in our numerical columns.","2d64120f":"### 1.4.2. Dictionary Vectorizer\n\nThis one is a bit more complex. It's role is to:\n* 1<sup>st<\/sup> - extract values from dictionaries,\n* 2<sup>nd<\/sup> - join them in one string,\n* 3<sup>rd<\/sup> - dummify it using `sklearn` Count Vectorizer.","969ab836":"Both Random Forest and Gradient Boost are subsets of ensemble regressors. In this case, all scores (feature importances) should add up to 1. We can interpret those as the influence of the feature in predicting target value.","9d860271":"### 1.4.7. Item Counter\n\nItem Counter counts how many items are in a list.","cf807338":"As we can see, these are much lower. Especially *budget* variable, which we can interpret, that for every dollar invested in a move, we expect about $1.6 revenue.","fdd6e007":"### 1.4.5. Binarizer\n\nBiniarizer takes as an input function that decides whether or not label value as `True` or `False`.","116c980f":"### 1.4.6. Date Transformer\n\nAs mentioned earlier, this transformer takes a date in string format and extract values of interest.","ae4f57ae":"# 1. EDA & Data Transformation\n\nExploratory Data Analysis (EDA) and Data Transformation are often thought as a cycle (EDA &rarr; Transform &rarr; EDA etc.), I'll start with **checking missing values**, then I'll get to transforming nominal data, and later on I'll do EDA on numeric data. As the nominal data is quite entangled (efter extraction reasembles text corpus) in this dataset, I'll omit the EDA of this part. \n\n## 1.1. Checking missing values","90054e49":"# References\n* [PyData Youtube Channel](https:\/\/www.youtube.com\/user\/PyDataTV)\n* [Marcin Zab\u0142ocki blog](https:\/\/zablo.net\/)\n* [Multivariate Data Analysis - Joseph F. Hair Jr. William C. Black Barry J. Babin Rolph E. Anderson](https:\/\/www.pearson.com\/us\/higher-education\/program\/Hair-Multivariate-Data-Analysis-7th-Edition\/PGM263675.html)\n* [Feature Engineering for Machine Learning - Alice Zheng, Amanda Casari](http:\/\/shop.oreilly.com\/product\/0636920049081.do)"}}