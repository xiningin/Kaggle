{"cell_type":{"210b80cd":"code","45cc0b73":"code","3803e0f7":"code","e52b384b":"code","7970d72c":"code","91436172":"code","5b4c8966":"code","c88246c6":"code","81d45a61":"code","5eee10d3":"code","c24b37c6":"code","a65b8ebb":"code","683471ae":"code","6134a82c":"code","00d40ac4":"code","deb829d5":"code","4c9c3432":"code","4bd50a91":"code","ac4396c3":"code","7bc6e250":"code","f054a41f":"code","2a9306ec":"code","a1496792":"code","6a3a6edb":"code","f322c1b1":"code","1d2b5bb7":"code","10c86320":"code","084e950f":"code","37c98ffa":"code","acfc75dd":"code","a4ffd231":"code","dbdec2fd":"code","c3df66d2":"code","b8a25393":"code","fe1649f8":"code","2391dd20":"code","42c0a264":"code","d0a2108f":"code","2fc81959":"code","65fb7879":"code","7a50c4d2":"code","1bbd8f13":"code","40c8b2c0":"code","b1d5a2a7":"code","efcc8408":"code","b030575d":"code","74a709da":"code","093197e2":"code","11c4e98a":"code","5cfaf0aa":"code","dc89977e":"code","6e73aefa":"code","13a1daed":"markdown","e20beaf1":"markdown","79166c02":"markdown","37d0b24c":"markdown","2c73c854":"markdown","e19e84d2":"markdown","4ec6da09":"markdown","7c36ce46":"markdown","4896bf6f":"markdown","b12c7688":"markdown","dd2778e5":"markdown","a5a5d93d":"markdown","f7b6f8b7":"markdown","9c31974f":"markdown","56112868":"markdown","32ea35f4":"markdown","45b203b9":"markdown","ec7c1d34":"markdown","25282af8":"markdown","5785f473":"markdown","3ca356ae":"markdown","e01d88bc":"markdown","a2597769":"markdown","352b91a1":"markdown","329f2ebc":"markdown","cdb7145b":"markdown","4ba9248f":"markdown","ff3986b7":"markdown","aae1e362":"markdown","f72d3448":"markdown","125b28e8":"markdown","2c217a59":"markdown","9d024a2b":"markdown","e3d37883":"markdown","e6106caf":"markdown","33caf392":"markdown","62796c7a":"markdown","ed7b2a72":"markdown","30e0b46f":"markdown","c69cb280":"markdown","d006e8c4":"markdown","166ab4f2":"markdown","d4257edd":"markdown","78d9aa51":"markdown","abc684c7":"markdown","865be398":"markdown","3e442660":"markdown"},"source":{"210b80cd":"import sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","45cc0b73":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\n\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import EnglishStemmer\nimport pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","3803e0f7":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain.head(3)","e52b384b":"df_concat = pd.concat([train, test], axis = 0) \n\nfor df in [train, test, df_concat]:\n\n\n    df.keyword.fillna('no_keyword', inplace = True)\n\n    df.location.fillna('no_location', inplace = True)\n\n\n\n# top 30 locations in the dataset\n\ntop_30 = df_concat.groupby(['location']).count().text.sort_values(ascending = False)[:30]\n\n# plot the top 30\n\nplt.figure(figsize = (6,10))\nsns.barplot(x = top_30, y = top_30.index);\nplt.xlabel('number of tweets');","7970d72c":"test.head(3)","91436172":"print('There are {} rows and {} colums in train'.format(train.shape[0], train.shape[1]))\n\nprint('There are {} rows and {} colums in test'.format(test.shape[0], test.shape[1]))","5b4c8966":"x = train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples');","c88246c6":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","81d45a61":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","5eee10d3":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet');","c24b37c6":"from nltk.corpus import stopwords\nimport nltk\nimport re\nimport string, collections\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nstemmer = EnglishStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = text.lower() #make text lowercase and fill na\n    text = re.sub('\\[.*?\\]', '', text) \n    text = re.sub('\\\\n', '',str(text))\n    text = re.sub(\"\\[\\[User.*\",'',str(text))\n    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(text))\n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) #remove hyperlinks\n    text = re.sub(r'\\:(.*?)\\:', '', text) #remove emoticones\n    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', str(text)) #remove email\n    text = re.sub(r'(?<=@)\\w+', '', text) #remove @\n    text = re.sub(r'[0-9]+', '', text) #remove numbers\n    text = re.sub(\"[^A-Za-z0-9 ]\", '', text) #remove non alphanumeric like ['@', '#', '.', '(', ')']\n    text = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', '', text) #remove punctuations from sentences\n    text = re.sub('<.*?>+', '', str(text))\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text))\n    text = re.sub('\\w*\\d\\w*', '', str(text))\n    text = tokenizer.tokenize(text)\n    text = [word for word in text if not word in stop_words]\n    #text = [lemmatizer.lemmatize(word) for word in text]\n    text = [stemmer.stem(word) for word in text]\n    final_text = ' '.join( [w for w in text if len(w)>1] ) #remove word with one letter\n    return final_text\n\n\ntrain['clean_text'] = train['text'].apply(lambda x : clean_text(x))","a65b8ebb":"def clean(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    \n    #correct some acronyms while we are at it\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)      \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet) \n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)  \n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)  \n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    \n    #and some typos\/abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    #hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n\n    \n    return tweet\n\ntrain['clean_text'] = train['clean_text'].apply(lambda x : clean(x))","683471ae":"train[['text', 'clean_text']].head()","6134a82c":"# How many unique words have this text\ndef counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count\n\ntext_values = train[\"clean_text\"]\n\ncounter = counter_word(text_values)\n\nvocab_size = len(counter)\nembedding_dim = 32\n\n# Max number of words in each complaint.\nmax_length = 1500 #20\ntrunc_type='post'\npadding_type='post'\n\n# oov_took its set for words out our word index\noov_tok = \"<XXX>\"\ntraining_size = 6090\nseq_len = 12\n\n\n\n# this is base in 80% of the data, an only text and targert at this moment\n\ntraining_sentences = train.text[0:training_size]\ntraining_labels = train.target[0:training_size]\n\ntesting_sentences = train.text[training_size:]\ntesting_labels = train.target[training_size:]\n\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\n\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nword_index = tokenizer.word_index","00d40ac4":"import time\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(14, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","deb829d5":"start_time = time.time()\n\nnum_epochs = 10\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose = 2)\n\nfinal_time = (time.time()- start_time)\/60\nprint(f'The time in minutes: {final_time}')","4c9c3432":"import matplotlib.pyplot as plt\n\n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","4bd50a91":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","ac4396c3":"testing_sequences2 = tokenizer.texts_to_sequences(test.text)\ntesting_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)","7bc6e250":"predictions = model.predict(testing_padded2)","f054a41f":"submission =  pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","2a9306ec":"submission['target'] = (predictions > 0.5).astype(int)","a1496792":"submission.head()","6a3a6edb":"submission.to_csv(\"submission.csv\", index=False, header=True)","f322c1b1":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1d2b5bb7":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import f1_score","10c86320":"stop_words = set(stopwords.words('english'))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nstemmer = EnglishStemmer()\nlemmatizer = WordNetLemmatizer()\n\ntrain['clean_text'] = train['text'].apply(lambda x : clean_text(x))\n\n\ntrain['clean_text'] = train['clean_text'].apply(lambda x : clean(x))","084e950f":"train[['text', 'clean_text']].head()","37c98ffa":"uniqueWordFrequents = {}\nfor tweet in train['clean_text']:\n    for word in tweet.split():\n        if(word in uniqueWordFrequents.keys()):\n            uniqueWordFrequents[word] += 1\n        else:\n            uniqueWordFrequents[word] = 1\n            \n#Convert dictionary to dataFrame\nuniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\nuniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\nuniqueWordFrequents.head(10)","acfc75dd":"counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\nbagOfWords = counVec.fit_transform(train['clean_text']).toarray()","a4ffd231":"X = bagOfWords\ny = train['target']\nprint(\"X shape = \",X.shape)\nprint(\"y shape = \",y.shape)\n\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.20, random_state=55, shuffle =True)\nprint('data splitting successfully')","dbdec2fd":"decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\n\ndecisionTreeModel.fit(X_train,y_train)\n\nprint(\"decision Tree Classifier model run successfully\")","c3df66d2":"gradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train)\n\nprint(\"gradient Boosting Classifier model run successfully\")","b8a25393":"KNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train)\n\nprint(\"KNeighbors Classifier model run successfully\")","fe1649f8":"LR = LogisticRegression(penalty='l2', \n                                        solver='saga', \n                                        random_state = 55)  \n\nLR.fit(X_train,y_train)\n\nprint(\"LogisticRegression Classifier model run successfully\")","2391dd20":"import eli5\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\neli5.show_weights(LogisticRegression, vec=counVec, top=15,\n                  target_names=['Not Disaster','Disaster'])","42c0a264":"print(train['clean_text'][2])\nprint('\\n')\nprint(y_train[2])","d0a2108f":"import eli5\neli5.show_prediction(LR, train['clean_text'][2], vec=counVec,\n                     target_names=['Not Disaster','Disaster'],top=15)\n","2fc81959":"SGD = SGDClassifier(loss = 'hinge', \n                              penalty = 'l1',\n                              learning_rate = 'optimal',\n                              random_state = 55, \n                              max_iter=100)\n\nSGD.fit(X_train,y_train)\n\nprint(\"SGDClassifier Classifier model run successfully\")","65fb7879":"SVClassifier = SVC(kernel= 'linear',\n                   degree=3,\n                   max_iter=10000,\n                   C=2, \n                   random_state = 55)\n\nSVClassifier.fit(X_train,y_train)\n\nprint(\"SVClassifier model run successfully\")","7a50c4d2":"bernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train)\n\nprint(\"bernoulliNB model run successfully\")","1bbd8f13":"gaussianNBModel = GaussianNB()\ngaussianNBModel.fit(X_train,y_train)\n\nprint(\"gaussianNB model run successfully\")","40c8b2c0":"multinomialNBModel = MultinomialNB(alpha=0.1)\nmultinomialNBModel.fit(X_train,y_train)\n\nprint(\"multinomialNB model run successfully\")","b1d5a2a7":"modelsNames = [('LogisticRegression',LR),\n               ('SGDClassifier',SGD),\n               ('SVClassifier',SVClassifier),\n               ('bernoulliNBModel',bernoulliNBModel),\n               ('multinomialNBModel',multinomialNBModel)]\n\nVC = VotingClassifier(voting = 'hard',estimators= modelsNames).fit(X_train,y_train)\nprint(\"votingClassifier model run successfully\")","efcc8408":"from sklearn import metrics\n\ndef roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","b030575d":"#evaluation Details\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LR, \n          SGD, SVClassifier, bernoulliNBModel, gaussianNBModel, multinomialNBModel, VC]\n\nfor model in models :\n    print(type(model).__name__,' Train Score is : ',model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is  : ',model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    \n    print(type(model).__name__,' AUC is   : ' ,roc_auc(y_pred, y_test))\n    \n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n    print('--------------------------------------------------------------------------')","74a709da":"from nltk.corpus import stopwords\nimport nltk\nimport re\nimport string, collections\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nstemmer = EnglishStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = text.lower() #make text lowercase and fill na\n    text = re.sub('\\[.*?\\]', '', text) \n    text = re.sub('\\\\n', '',str(text))\n    text = re.sub(\"\\[\\[User.*\",'',str(text))\n    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(text))\n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) #remove hyperlinks\n    text = re.sub(r'\\:(.*?)\\:', '', text) #remove emoticones\n    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', str(text)) #remove email\n    text = re.sub(r'(?<=@)\\w+', '', text) #remove @\n    text = re.sub(r'[0-9]+', '', text) #remove numbers\n    text = re.sub(\"[^A-Za-z0-9 ]\", '', text) #remove non alphanumeric like ['@', '#', '.', '(', ')']\n    text = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', '', text) #remove punctuations from sentences\n    text = re.sub('<.*?>+', '', str(text))\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text))\n    text = re.sub('\\w*\\d\\w*', '', str(text))\n    text = tokenizer.tokenize(text)\n    text = [word for word in text if not word in stop_words]\n    #text = [lemmatizer.lemmatize(word) for word in text]\n    text = [stemmer.stem(word) for word in text]\n    final_text = ' '.join( [w for w in text if len(w)>1] ) #remove word with one letter\n    return final_text\n\ntest['clean_text'] = test['text'].apply(lambda x : clean_text(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x : clean(x))","093197e2":"#Create our dictionary \nuniqueWordFrequents = {}\nfor tweet in test['clean_text']:\n    for word in tweet.split():\n        if(word in uniqueWordFrequents.keys()):\n            uniqueWordFrequents[word] += 1\n        else:\n            uniqueWordFrequents[word] = 1\n            \n#Convert dictionary to dataFrame\nuniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\nuniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)","11c4e98a":"counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\nbagOfWords = counVec.fit_transform(test['clean_text']).toarray()","5cfaf0aa":"#counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n#bagOfWords = counVec.fit_transform(train['clean_text']).toarray()\n\n#X = bagOfWords\n#y = train['target']\n#print(\"X shape = \",X.shape)\n#print(\"y shape = \",y.shape)\n\n#X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.20, random_state=55, shuffle =True)\n#print('data splitting successfully')","dc89977e":"from sklearn.linear_model import LogisticRegression\nLogisticRegression = LogisticRegression(penalty='l2', solver='saga', random_state = 55).fit(X_train[:3263], y_train[:3263])\npred_test = LogisticRegression.predict(bagOfWords)","6e73aefa":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = pred_test\nsample_submission.to_csv(\"submission3.csv\", index=False)","13a1daed":"Let's split our data before modelling :","e20beaf1":"Here, we can see the specific word the model use to make his probability. Like shelter or evacu, who tend to be quite the word we associate to disaster.\n\nThis package is absolutely wonderful to explain to non-iniate our models. So let's keep it in mind.\n\nWe can go on with our next models !","79166c02":"Now ve can focus on the modelling aspect, first we will try **Deep Learning methods** and after we will try **Machine Learning methods**.","37d0b24c":"On the basis of our models performances, I think Bernoulli, Multinomial, Logistic Regression and VotingClassifier are my best pick.","2c73c854":"*Number of words in a tweet*","e19e84d2":"*Alonso Evan*\n\n*02 June 2020*\n\n\n----------\n\n# Real or Not? NLP with Disaster Tweets\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)","4ec6da09":"<a id=\"5\"><\/a> <br>\n# 5.  Model performances","7c36ce46":"<a id=\"6\"><\/a> <br>\n# 6.  First submission to Kaggle","4896bf6f":"#### Logistic Regression model","b12c7688":"<a id=\"7\"><\/a> <br>\n# 7.  Models performances","dd2778e5":"I obtained 0.75 on Kaggle's submission score, not great. Maybe the Deep learning methods aren't that much specialised for database that small. \n\n<a id=\"4\"><\/a> <br>\n# 4. Machine Learning methods\n\n\nLet's try the machine learning most classic packages :\n\n\n- Decision Tree Model\n- Gradient Boosting Model\n- K - Nearest Neighbors Model\n- Logistic Regression Model\n- Stochastic Gradient Descent Model\n- Support Vector Machine Model\n- Bernoulli Naive Bayes Model\n- Gaussian Naive Bayes Model\n- Multinomial Naive Bayes Model\n- Voting Classifier Model","a5a5d93d":"Let's preprocessing the data with another methods but in a same way than for Deep Learning methods, to train.","f7b6f8b7":"### 3.1. Basic NLP Techniques\n<img src='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcT6TkPpD8nWsbTVa9ExwfCQUnFmzkNE8zjZJ3uXSaBVd09ErhvZ' alt='text preprocessing' style='float:left' width=50% >\n<div style='clear:both'><\/div>","9c31974f":"<a id=\"8\"><\/a> <br>\n# 8.  Second Submission to Kaggle","56112868":"#### Bernoulli model","32ea35f4":"#### Gaussian model","45b203b9":"#### VotingClassifier model","ec7c1d34":"Oh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets) but not that much","25282af8":"#### GradientBoostingClassifier model","5785f473":"<a id=\"1\"><\/a> <br>\n## 1. Import packages","3ca356ae":"#### KNeighborsClassifier model","e01d88bc":"We can see the most impactful word (green word) who contributed most to Disaster comments and Red words who contributed to opposite class that is Not Disaster comments class.\n\nThe green word are quite obvious for detect disaster, or not, we can see earthquake, typhoon, storm and so on. \n\nLet's see one prediction in particular :","a2597769":"### 4.2. Building models","352b91a1":"\nFirst,we will do very basic analysis,that is character level,word level and sentence level analysis.\n\n*Number of characters in tweets*","329f2ebc":"To begin with, let's take the most commonly used hyperparameters for our models ;","cdb7145b":"The list of common places include strings like 'earth' or 'Everywhere'! It's because this field is the users input and were not automatically generated and is very dirty. we'll get rid of it right away.","4ba9248f":"#### Decision Tree model","ff3986b7":"### 4.1. Basic NLP Techniques","aae1e362":"<ul style=\"list-style-type:square;\">\n  <li><span class=\"label label-default\">id<\/span> a unique identifier for each tweet<\/li>\n  <li><span class=\"label label-default\">text <\/span> the text of the tweet<\/li>\n  <li><span class=\"label label-default\">location<\/span>  the location the tweet was sent from (may be blank)<\/li>\n    <li><span class=\"label label-default\">keyword<\/span>  a particular keyword from the tweet (may be blank)<\/li>\n    <li><span class=\"label label-default\">target<\/span>  in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)<\/li>\n<\/ul>\n","f72d3448":"### 3.2. Building model","125b28e8":"#### SGDClassifier model","2c217a59":"*Location* :\n\nCan this field help? lets see what are the most common locations and how many there are. This field contains users inputs and were not automatically generated. take a look at it. it's very dirty and even same places don't have the same name. but first it's good to take a look at the common places people tweet from.","9d024a2b":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Table of Contents:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">Import packages<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\"> Data Exploration<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">Deep Learning methods<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">Machine Learning methods<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">Model performances<span class=\"badge badge-primary badge-pill\">5<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">First submission to Kaggle<span class=\"badge badge-primary badge-pill\">6<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">Models performances<span class=\"badge badge-primary badge-pill\">5<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">Second submission to Kaggle<span class=\"badge badge-primary badge-pill\">6<\/span><\/a> \n<\/div>","e3d37883":"<a id=\"3\"><\/a> <br>\n## 3. Deep Learning methods","e6106caf":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nIn this competition, we\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. We have access to a dataset of 10,000 tweets that were hand classified.","33caf392":"*Average word length in a tweet*","62796c7a":"Before we move on with the others models, I would like to point out LIME, let me explain.\n\nRecent times have seen a renewed focus on model interpretability. Machine Learning Experts are able to understand the importance of a model interpretability in it\u2019s subsequent adaption by business. The problem with model explainability is that it\u2019s very hard to define a model\u2019s decision boundary in human understandable manner, especially with boosting model. LIME and eli5 are python libraries which tries to solve for model interpretability by producing locally faithful explanations. Below is an example of one such explanation for my NLP problem.\n\nLIME use a representation that is understood by the humans irrespective of the actual features used by the model. This is coined as interpretable representation. An interpretable representation would vary with the type of data that we are working with for example :\n\n- For text : It represents presence\/absence of words.\n- For image : It represents presence\/absence of super pixels ( contiguous patch of similar pixels ).\n- For tabular data : It is a weighted combination of columns\n\nSo let's see this.","ed7b2a72":"That doesn't look good. Our model clearly overfit !","30e0b46f":"I tried with Word Embeddings but it was way too long to process the Deep Learning Models so I did it without that.","c69cb280":"This time I will not use TfidfVectorizer but CountVectorizer instead :","d006e8c4":"#### SVCClassifier model","166ab4f2":"#### Multinomial model","d4257edd":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","78d9aa51":"<a id=\"2\"><\/a> <br>\n\n## 2. Data Exploration","abc684c7":"We obtained the same result as before, let's see the most frequent word :","865be398":"Before modeling, we will aplly this preprocessing I found on this notebook :\n    \nhttps:\/\/www.kaggle.com\/tuckerarrants\/nlp-with-twitter-eda-glove-keras-rnns?fbclid=IwAR06tf2sh831TCqItHZzPl7EfGBFSBsdSxd_qk-3Kg55bOkmKbRPimoBG3M\n\nIt's very specific at this database so don't use it for another problem.","3e442660":"Let's visualize our test before and after our cleaning :"}}