{"cell_type":{"17481644":"code","0bee95e5":"code","dce8445d":"code","a1ac396b":"code","5968a6b7":"code","0fc792e1":"code","6379fc7d":"code","23654117":"code","2db85a54":"code","e69e898e":"code","2781d332":"code","75fd8f39":"code","3c1a522b":"code","ffbdf002":"code","427820ea":"code","91d36215":"code","f6de0263":"code","bb280d28":"code","830d2a23":"code","798c8ce1":"code","13e38a35":"code","2b9d1039":"code","8fe0f67e":"code","a3ef75ad":"code","ea088f9e":"code","30eacb5b":"code","ca034cdf":"code","c281e543":"code","f357f0ec":"code","8429db06":"code","98a552c4":"code","3f9a60da":"code","5bbc4ee2":"code","540da661":"markdown","edc81868":"markdown","cdbf0b90":"markdown","efe6b66c":"markdown","53ecd437":"markdown","b81d6dca":"markdown","562f1b6c":"markdown"},"source":{"17481644":"import random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nimport tensorflow as tf\n\n# Huggingface transformers\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0bee95e5":"from sklearn.preprocessing import LabelEncoder","dce8445d":"DATA_PATH = '\/kaggle\/input\/sf-dl-movie-genre-classification\/'\nPATH      = '\/kaggle\/working\/'","a1ac396b":"train = pd.read_csv(DATA_PATH+'train.csv',)","5968a6b7":"train.head()","0fc792e1":"train.info()","6379fc7d":"train.genre.value_counts().plot(kind='bar',figsize=(12,4),fontsize=10)\nplt.xticks(rotation=60)\nplt.xlabel(\"Genres\",fontsize=10)\nplt.ylabel(\"Counts\",fontsize=10)","23654117":"train.genre.nunique()","2db85a54":"test = pd.read_csv(DATA_PATH+'test.csv',)\ntest.head()","e69e898e":"# \u0417\u0430\u044d\u043d\u043a\u043e\u0434\u0438\u043c \u0436\u0430\u043d\u0440\u044b\n\nle = LabelEncoder()\n\ntrain['genre'] = le.fit_transform(train['genre'])\n\ntrain['genre']","2781d332":"import string\nprint(string.punctuation)\n\nspec_chars = string.punctuation + '\u00ab\u00bb\u2014\u2026\u2019\u2018\u201d\u201c\u00a9'\nprint(spec_chars)\n\n# \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u044f \u0441\u043f\u0435\u0446 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432\ndef remove_chars_from_text(text, chars):\n    return \"\".join([ch for ch in text if ch not in chars])","75fd8f39":"# # \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0440\u0435\u0446\u0435\u043d\u0437\u0438\u0439\n# def text_tokenizer(text):\n#     Text_ = text.strip()\n\n#     Text_ = Text_.lower()\n\n#     # \u0423\u0434\u0430\u043b\u0438\u043c \u0432\u0441\u0435 \u0441\u043f\u0435\u0446 \u0441\u0438\u043c\u0432\u043e\u043b\u044b\n#     Text_ = remove_chars_from_text(Text_, spec_chars)\n\n#     # \u0423\u0434\u0430\u043b\u0438\u043c \u0432\u0441\u0435 \u0446\u0438\u0444\u0440\u044b\n#     Text_ = remove_chars_from_text(Text_, string.digits)\n\n#     Text_ = Text_.replace('\\n',' ').replace('\\t',' ')\n\n#     # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442\n#     tokens = word_tokenize(Text_)\n\n#     # \u0421\u043f\u0438\u0441\u043e\u043a \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u043a \u043a\u043b\u0430\u0441\u0441\u0443 Text\n#     token_text = nltk.Text(tokens)\n\n#     # \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\n#     filtered_token_text = [w for w in token_text if not w in english_stopwords]\n    \n#     return filtered_token_text","3c1a522b":"# \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0446\u0435\u043d\u0437\u0438\u0438\n\n#train['text'] = train['text'].apply(lambda x: text_tokenizer(x))","ffbdf002":"# \u0417\u0430\u0434\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel_name = 'bert-base-uncased'\n\n# \u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u043b\u0438\u043d\u043d\u0430\u044f \u0442\u043e\u043a\u0435\u043d\u043e\u0432\nmax_length = 100\n\n# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0438 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c output_hidden_states - False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# \u0413\u0440\u0443\u0437\u0438\u043c \u0431\u0435\u0440\u0442\u043e\u0432\u0441\u043a\u0438\u0439 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# \u0413\u0440\u0443\u0437\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ntransformer_model = TFBertModel.from_pretrained(model_name, config = config)","427820ea":"# \u0413\u0440\u0443\u0437\u0438\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0441\u043b\u043e\u0439\nbert = transformer_model.layers[0]\n\n# \u0412\u044b\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# \u0413\u0440\u0443\u0437\u0438\u043c \u0431\u0435\u0440\u0442\u043e\u0432\u0441\u043a\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u0430\u043a \u0441\u043b\u043e\u0439 \u043a\u0435\u0440\u0430\u0441\u0430\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n# \u0412\u044b\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ngenre = Dense(units=len(train['genre'].value_counts()), activation = 'softmax',\n              kernel_initializer=TruncatedNormal(stddev=config.initializer_range), \n              name='genre')(pooled_output)\n\noutputs = {'genre': genre}\n\nmodel_2 = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n\nmodel_2.summary()","91d36215":"# \u0417\u0430\u0434\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0430\u0439\u0437\u0435\u0440\noptimizer = Adam(\n    learning_rate=5e-05,\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# \u0417\u0430\u0434\u0430\u0435\u043c \u043b\u043e\u0441\u0441-\u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\nloss = {'genre': CategoricalCrossentropy(from_logits = True)}\nmetric = {'genre': CategoricalAccuracy('accuracy')}\n\n# \u041a\u043e\u043c\u043f\u0438\u043b\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel_2.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)\n\n# \u0413\u043e\u0442\u043e\u0432\u0438\u043c \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ny_genre = to_categorical(train['genre'])\n\n# \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\nx = tokenizer(\n    text=train['text'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = False)","f6de0263":"# \u041f\u043e\u0448\u043b\u0430 \u0436\u0430\u0440\u0430\nhistory = model_2.fit(\n    x={'input_ids': x['input_ids']},\n    y={'genre': y_genre},\n    validation_split=0.2,\n    batch_size=64,\n    epochs=10)","bb280d28":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","830d2a23":"model_2.save('keras_2st_softmax.h5')","798c8ce1":"# load_model = tf.keras.models.load_model('\/kaggle\/input\/keras-model-1st\/keras_1st.h5')","13e38a35":"x_test = tokenizer(\n    text=test['text'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = False)","2b9d1039":"predictions_2 = model_2.predict(x={'input_ids': x_test['input_ids']})\npredictions_2","8fe0f67e":"with open('pred_2.csv', 'w') as f:\n    for key in predictions_2.keys():\n        f.write(\"%s,%s\\n\"%(key,predictions_2[key]))","a3ef75ad":"sub_list = []\nfor i in range(len(predictions_2['genre'])):\n    genr = np.argmax(predictions_2['genre'][i])\n    sub_list.append(genr)","ea088f9e":"pd.DataFrame(sub_list)","30eacb5b":"submission = pd.DataFrame(sub_list,\n                          columns=['genre'])","ca034cdf":"submission = pd.read_csv('\/kaggle\/input\/submission-2\/submission_2st.csv')","c281e543":"submission","f357f0ec":"submission['genre'] = le.inverse_transform(submission['genre'])","8429db06":"submission = pd.DataFrame({'id':range(1, len(submission)+1),\n                           'genre':submission['genre'].values},\n                          columns=['id', 'genre'])\n","98a552c4":"submission.to_csv('submission_new.csv', index=False)","3f9a60da":"submission","5bbc4ee2":"submission.to_csv('submission_2st.csv', index=False)\nsubmission.head()","540da661":"\u0413\u0434\u0435-\u0442\u043e \u043d\u0430 5-\u0439 \u044d\u043f\u043e\u0445\u0435 \u0441\u0435\u0442\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u043b\u0430\u0441\u044c. \u0412\u0438\u0434\u0438\u043c\u043e \u043d\u0443\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0434\u0440\u043e\u043f\u0430\u0443\u0442.","edc81868":"# MODEL \n","cdbf0b90":"\u0412 \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u043d\u0435 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u043b\u0430\u0441\u044c, \u0442\u0430\u043a \u043a\u0430\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0441\u044f \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c BERT\u0430","efe6b66c":"## Text tokenization","53ecd437":"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043a\u0430 \u043d\u0435 \u043e\u0441\u043e\u0431\u043e \u0440\u0430\u0434\u0443\u0435\u0442 - \u043b\u0438\u0448\u044c 0,65. \n\u041d\u0443\u0436\u043d\u043e \u0435\u0449\u0435 \u043f\u043e\u043a\u0440\u0443\u0442\u0438\u0442\u044c fine-tuning \u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c XLNet","b81d6dca":"#### Test","562f1b6c":"# Data\n#### TRAIN"}}