{"cell_type":{"0f5a1154":"code","502cd9b8":"code","db06b425":"code","b334a515":"code","7f36f405":"code","4d53fd2d":"code","bdd4303d":"code","cb53d459":"code","13720826":"code","548eef10":"code","5d5266f3":"code","f6a91131":"code","81d731b1":"code","c2dd963f":"code","9d361cbe":"code","d4f3acf8":"code","aa95b838":"markdown","a0a7ba56":"markdown","6037ab9e":"markdown","247b1711":"markdown","f55d631c":"markdown","04224b45":"markdown","4fb74e12":"markdown","4dfab25b":"markdown","c888ff5f":"markdown"},"source":{"0f5a1154":"seed = 42\n\nimport random\nimport numpy as np\nfrom tensorflow import set_random_seed\n\nrandom.seed(seed)\nnp.random.seed(seed)\nset_random_seed(seed)\n\n\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam","502cd9b8":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.tsv',  sep=\"\\t\")\ntest = pd.read_csv('..\/input\/test.tsv',  sep=\"\\t\")","db06b425":"train.head()","b334a515":"train['Phrase'].str.len().mean()","7f36f405":"train['Phrase'].str.len().max()","4d53fd2d":"train['Sentiment'].value_counts()","bdd4303d":"def format_data(train, test, max_features, maxlen):\n    \"\"\"\n    Convert data to proper format.\n    1) Shuffle\n    2) Lowercase\n    3) Sentiments to Categorical\n    4) Tokenize and Fit\n    5) Convert to sequence (format accepted by the network)\n    6) Pad\n    7) Voila!\n    \"\"\"\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.utils import to_categorical\n    \n    train = train.sample(frac=1).reset_index(drop=True)\n    train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n    test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n\n    X = train['Phrase']\n    test_X = test['Phrase']\n    Y = to_categorical(train['Sentiment'].values)\n\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(X))\n\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    test_X = tokenizer.texts_to_sequences(test_X)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    return X, Y, test_X","cb53d459":"maxlen = 170\nmax_features = 20000\nX, Y, test_X = format_data(train, test, max_features, maxlen)","13720826":"X","548eef10":"Y","5d5266f3":"test_X","f6a91131":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.10, random_state=seed)","81d731b1":"model = Sequential()\nmodel.add(Embedding(max_features,100,mask_zero=True))\nmodel.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4, return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel.summary()","c2dd963f":"epochs = 12\nbatch_size = 64","9d361cbe":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)","d4f3acf8":"sub = pd.read_csv('..\/input\/sampleSubmission.csv')\n\nsub['Sentiment'] = model.predict_classes(test_X, batch_size=batch_size, verbose=1)\nsub.to_csv('sub_cnn.csv', index=False)","aa95b838":"Now we will read our data","a0a7ba56":"Let's take a look at how the data looks:","6037ab9e":"Let's take a look at how sentiments are distributed.","247b1711":"Finally, we will make our predictions on the test set.","f55d631c":"As you can see each row is zero-padded on the left.","04224b45":"With the formatted data at hand, we move to split our training set to training and validation. The validation set will help as determine whether our model generalizes well or not.","4fb74e12":"The values correspond to sentiments as follows:\n\n```\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive\n```\n\nWe can see that most phrases are neutral, followed by the 'somewhats' (somewhat positive - somewhat negative). Then, far behind are positive and negative phrases. This (seemingly) makes classification harder, since most phrases are congregated towards the middle\/neutral.\n\nTo train our model, we need to format our data.\n\nSince we are dealing with text, we will first convert everything to lowercase. Then, we will tokenize our text. Currently, we build the tokenizer only on the training data. We could add to the ingredients the testing data, but results may go up or down. Testing is needed to determine whether or not adding the testing data will help. After the tokenization, we also need to pad the rows with zeros.\n\nApart from that, we need to convert the numerical output to categorical. Specifically, we need to one-hot encode the labels.\n\nFinally, we need to shuffle our data as well.","4dfab25b":"Having build our network, we will start training.\n\nSince this problem is multi-class classification, we will optimize the categorical crossentropy loss function. The optimizer we will use is ADAM.","c888ff5f":"The mean and max lengths of phrases are the following:"}}