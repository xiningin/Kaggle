{"cell_type":{"d54f121f":"code","75126a06":"code","5ae6b3ad":"code","64ebf378":"code","2e28e5fc":"code","4cc21e3a":"code","51521c22":"code","2ca3f8df":"code","4f9ba991":"code","c575ede4":"code","e289b343":"code","b618f3ca":"code","84caff31":"code","e9a576c7":"code","2fca2367":"code","0bbe5e37":"code","21605635":"markdown","dbf8ebf7":"markdown","15645d69":"markdown","1ee815d7":"markdown","dae1ea94":"markdown","563b8ee7":"markdown","743b5c0f":"markdown","61165e15":"markdown","f8817877":"markdown"},"source":{"d54f121f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75126a06":"# KNN Classification\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n%matplotlib inline \nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('\/kaggle\/input\/wisc-bc-data\/wisc_bc_data.csv')\ndf.columns","5ae6b3ad":"df.head()","64ebf378":"# The first column is id column which is patient id and nothing to do with the model attriibutes. So drop it.\n\ndf = df.drop(labels = \"id\", axis = 1)","2e28e5fc":"X = df.drop(labels=\"diagnosis\", axis = 1)\ny = df[\"diagnosis\"]","4cc21e3a":"# convert the features into z scores as we do not know what units \/ scales were used and store them in new dataframe\n# It is always adviced to scale numeric attributes in models that calculate distances.\n\nXScaled  = X.apply(zscore)  # convert all attributes to Z scale \n\nXScaled.describe()","51521c22":"num_folds = 10\nkfold = KFold(n_splits=num_folds, random_state=7)\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, XScaled, y, cv=kfold)\nprint(results.mean())","2ca3f8df":"# Split X and y into training and test set in 75:30 ratio\n\n\nX_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size=0.30, random_state=1)","4f9ba991":"KNN = KNeighborsClassifier(n_neighbors= 5 , weights = 'distance' )\n# Call Nearest Neighbour algorithm\n\nKNN.fit(X_train, y_train)","c575ede4":"# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\npredicted_labels = KNN.predict(X_test)\nKNN.score(X_test, y_test)","e289b343":"# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\nprint(\"Confusion Matrix\")\ncm=metrics.confusion_matrix(y_test, predicted_labels, labels=[\"M\", \"B\"])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"M\",\"B\"]],\n                  columns = [i for i in [\"Predict M\",\"Predict B\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True,fmt='.5g',cmap=\"YlGn\")\n","b618f3ca":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix,plot_confusion_matrix\n\nprint(classification_report(y_test,predicted_labels))","84caff31":"from sklearn.model_selection import GridSearchCV\n#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, cv=10)\n#Fit the model\nbest_model = clf.fit(XScaled,y)\n#Print The value of best Hyperparameters\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])","e9a576c7":"predicted_labels1 = best_model.predict(X_test)\nbest_model.score(X_test, y_test)","2fca2367":"# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\nprint(\"Confusion Matrix\")\ncm=metrics.confusion_matrix(y_test, predicted_labels1, labels=[\"M\", \"B\"])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"M\",\"B\"]],\n                  columns = [i for i in [\"Predict M\",\"Predict B\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True,fmt='.5g',cmap=\"YlGn\")","0bbe5e37":"print(classification_report(y_test,predicted_labels1))","21605635":"## Introduction\nkNN is distance based algorithm in which there is no learning step, instead dataset is stored in memory used is used to classify on the fly. kNN is one of the simplest methods of classfication.\n\nIn kNN, 'k' is a parameter which refers to the number of nearest neighbours. Often people find it difficult to specify optimal 'k' value. In article we will see basic implementation of kNN and lateron  we will find out how to find optimal 'k' value and how it improves the overall accuracy.\n","dbf8ebf7":"Below code gives usestimated mean accuracy. Now we can proceed further.","15645d69":"## Other variant of kNN\n1. Radius Neighbour Classifier - Within a fixed radious it coumputes number of neighbours. Radius Neighbour classifier can be helpful when data is not uniform.\n2. KD Tree Nearest Neighbour - This method uses tree based approach and is effective when sample size is huge. \n3. KNN Regression- In this method target variable is continous. Hence nearest neighbours are calculated based on average.","1ee815d7":"![Source: dslytics.files.wordpress.com](https:\/\/dslytics.files.wordpress.com\/2017\/11\/knn.png?w=400)","dae1ea94":"#  k-NN Classifier with HyperparameterTuning","563b8ee7":"> \u201cthe k-nearest neighbors algorithm (kNN) is a non-parametric machine learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.\u201d-Wikipedia","743b5c0f":"## Pros and Cons\nPros - \n* kNN algorithm is very simple to implement and it makes predictions on the fly by calculating the similarity between an input sample and each training variables.\n* It is robust and works with multiple classess.\n* There are many distance measures to choose from.\n\nCons - \n* Can have high comuputation cost if more dimensions are present.\n* May suffer from Curse of Dimensionality due to increase in dimensions.\n* It is not effective when distribution overlaps with each other. \n* User may find it challenging to find an optimal 'k' value.","61165e15":"## How kNN finds nearest neighbour?\nWhen a new query point (let's say p) is added then the classification procedure for query point 'p' works in two steps as:\n1. kNN find the K neighbours in the dataset which are closest to 'p' query point. For regression it is mean value. Here no. of neighbours are specified as K.\n2. Use these K neighbours to determine the class of 'p' using voting mechanism.\n\n### Distance measures used\n* Euclidean Distance  \n* Manhattan Distance\n* Chebyshev Distance\n* Minkowski Distance\n* Mahalanobis Distance\n* Hamming Distance\n","f8817877":"We can see that accuracy has improved from 96% to 98%."}}