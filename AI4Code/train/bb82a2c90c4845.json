{"cell_type":{"16854a69":"code","fb66bc46":"code","2c9b0b34":"code","7aca3561":"code","c1a1c2e2":"code","621ca4f6":"code","fddc95e9":"code","4309c03e":"code","686fda38":"code","57f7dcf4":"code","6f32fb30":"code","19533617":"code","783e12f0":"code","c3df0c39":"code","cfa89432":"code","8fd795a3":"code","45fb74a5":"code","63364401":"code","6cac9ab1":"code","1dd11d26":"code","564fef77":"code","b665bda0":"code","2a1f22d4":"code","a643e8f2":"code","36520420":"code","29fc4ce9":"code","bb5e3ef1":"code","69f6c8a4":"code","9a1b7fab":"code","55febb99":"code","5dbe499a":"code","553f761f":"code","76c2e1db":"code","45a055dd":"code","efc8adec":"code","23759e8a":"code","4988d182":"code","51f3bbe2":"code","e38dd8aa":"code","738fc40e":"code","029238e7":"code","7eed6f90":"code","948bc668":"code","72aa86f6":"code","74e1b8a1":"code","b555b955":"code","90198e8e":"code","ed18f41c":"code","48841e26":"code","37e9383b":"code","287e4371":"code","2a2defbc":"code","91db7134":"code","3e5a4d75":"code","c8149adb":"code","e29f6566":"code","b802bd96":"code","844f800e":"code","abbc9df8":"code","056a02f1":"code","0047dc0f":"code","86d16c26":"code","a819b2d1":"code","e1dc248f":"code","5c7d2f8f":"code","1767baca":"code","0f92b5e0":"code","b0d8ff6d":"code","3b57b6b0":"code","949148a4":"code","ce881add":"code","20b9b07d":"code","7e23afe4":"code","642ea88b":"code","41bb903e":"code","597465ba":"markdown","c97822e5":"markdown","47b10d0a":"markdown","9c2d33df":"markdown","6ecb8882":"markdown","353a6a1e":"markdown","0cb9ddc7":"markdown","fa4e2306":"markdown","82668047":"markdown","37c181af":"markdown","f6bbf468":"markdown","92690197":"markdown","e884b4d7":"markdown","87c4e0b9":"markdown","1c4f0c92":"markdown","edbc5eaa":"markdown","df570023":"markdown","a03a4121":"markdown","907d928f":"markdown","01c7bce9":"markdown","c48a2741":"markdown","e8fbc3df":"markdown","c7d6bf0a":"markdown","be5ff5a7":"markdown","bae02191":"markdown","70a6fc4a":"markdown","7ec917df":"markdown","ab1b00a8":"markdown","5e1b9a7e":"markdown","d4834e36":"markdown","bce8aaef":"markdown","9d7c79ca":"markdown","456cc176":"markdown","d27b6248":"markdown","99531bc3":"markdown","272ade48":"markdown","e090a12a":"markdown","1f4f6fe3":"markdown","2a331768":"markdown","9b2a6964":"markdown","5b3b70f2":"markdown","33ff7835":"markdown","f3450a87":"markdown","6cbf0ae4":"markdown","e6b6a5ff":"markdown","a1198f4f":"markdown","2d6458de":"markdown","48fdf361":"markdown","5f9d7d74":"markdown","7ad8cafa":"markdown","de5362ed":"markdown","31ef2ec4":"markdown","60099cc3":"markdown","411afbb4":"markdown","9463a6a9":"markdown","6f2bb6b5":"markdown","27235ca0":"markdown","f2bb7538":"markdown","732abea0":"markdown","ede5b2c6":"markdown","f84eaa1f":"markdown","56cbd6eb":"markdown","1659f534":"markdown","c5ff2917":"markdown","95d50108":"markdown","af4e2ea2":"markdown","b28ae000":"markdown","13d854f0":"markdown","681f3aa2":"markdown","b7223c68":"markdown","14119adc":"markdown","7aae6e8d":"markdown","188c5e9e":"markdown","5bc83dd2":"markdown","474bd7ea":"markdown","ee886d20":"markdown","8b06d534":"markdown","5ebfc306":"markdown"},"source":{"16854a69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb66bc46":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nsns.set_style('whitegrid')","2c9b0b34":"#let's load the dataset\ndf = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndf","7aca3561":"df_expl = df.copy()","c1a1c2e2":"df_expl.info()","621ca4f6":"df_expl.describe()","fddc95e9":"category_df = pd.DataFrame([\n    [0, \"T-shirt\/top\"],\n    [1, \"Trouser\"],\n    [2, \"Pullover\"],\n    [3, \"Dress\"],\n    [4, \"Coat\"],\n    [5, \"Sandal\"],\n    [6, \"Shirt\"],\n    [7, \"Sneaker\"],\n    [8, \"Bag\"],\n    [9, \"Ankle boot\"],\n], columns=[\"id\", \"category\"])\ncategory_df","4309c03e":"df_expl = df_expl.merge(right=category_df,how=\"inner\",left_on=\"label\",right_on=\"id\").drop(axis=1,columns=\"id\")\ndf_expl","686fda38":"from random import seed\nfrom random import randint\n\nfig=plt.figure(figsize=(16, 16))\ncolumns = 5\nrows = 10\n\nseed(1)\np=0\nfor x in range(1,rows+1):\n    for i in range(1,columns+1):\n        value = randint(0, 6000)\n        fig.add_subplot(rows, columns, (x-1)*5+i)\n        plt.imshow(df[df['label']==(x-1)].to_numpy()[value,1:].reshape((28,28)), cmap=\"Greys\")\n\nplt.show()","57f7dcf4":"def display_image_in_actual_size(im):\n    dpi = 80\n    im_data = im\n    (height, width), depth = im_data.shape, 1\n\n    # What size does the figure need to be in inches to fit the image?\n    figsize = width \/ float(dpi), height \/ float(dpi)\n\n    # Create a figure of the right size with one axis that takes up the full figure\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    # Hide spines, ticks, etc.\n    ax.axis('off')\n\n    # Display the image.\n    ax.imshow(im_data, cmap='Greys')\n\n    plt.show()","6f32fb30":"display_image_in_actual_size(df.to_numpy()[1,1:].reshape((28,28)))","19533617":"df_count = df_expl.groupby(['label','category'])['pixel1'].count()\ndf_count","783e12f0":"values = df_count.values\nlabels = ['T-shirt\/top','Trousers','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle_boot']\nfig, ax = plt.subplots(figsize=(6,5))\nax.pie(values, labels=labels,textprops={'fontsize': 12})\n#fig.savefig('piechart', transparent=True)","c3df0c39":"# Averaging pixel intensity for each row\ndf_expl['avg_intensity'] = df_expl.replace(0, np.NaN).drop('label', axis=1).mean(axis=1)\n# Now counting number of \"active\" pixels (intensity value strictly greater than 0)\ndf_expl['tot_pixel'] = df_expl.replace(0, np.NaN).drop('label', axis=1).count(axis=1)\ndf_expl","cfa89432":"df_lgr = df_expl.groupby(['label','category']).agg({'avg_intensity':'mean','tot_pixel':'mean'}).reset_index()\ndf_lgr","8fd795a3":"fig, ax = plt.subplots(figsize=(8,4)) \nax2 = ax.twinx() #we want to plot 2 variables on 2 axes, so we use twinx to generate a second y axis\n\nwidth = 0.4\n\nrects1 = df_lgr.avg_intensity.plot(kind='bar', color='black', ax=ax, width=width, position=1)\nrects2 = df_lgr.tot_pixel.plot(kind='bar', color='grey', ax=ax2, width=width, position=0)\n\nax.set_ylabel('Intensity of color', fontsize=12)\nax2.set_ylabel('Number of colored pixels', fontsize=12)\nax.set_title('Intensity and number of colored pixels by cloth category', fontsize=14)\nax.tick_params(axis=\"y\", labelsize=12)\nax2.tick_params(axis=\"y\", labelsize=12)\n\nlabels = df_lgr['category']\nx = np.arange(len(labels))  \n\nax.legend(fontsize=12, loc='lower center', bbox_to_anchor=(0.3, -0.4))\nax2.legend(fontsize=12, loc='lower center', bbox_to_anchor=(0.6, -0.4))\n\nax.set_xticks(x)\nax.set_xticklabels(labels, fontsize=12, rotation=45)\n\nplt.xlim(-1,10) #this because saving the fig was cutting some parts\n\nplt.show()\n\nplt.subplots_adjust(right=4)\n\n#fig.savefig(\"hist\", transparent=True, bbox_inches='tight')","45fb74a5":"descend = df_expl.replace(0, np.NaN).drop(['label','category','tot_pixel','avg_intensity'], axis=1).count().sort_values(ascending = False)\nmost_common = descend.head(20)\nmost_common","63364401":"less_common = descend.tail(20)\nless_common","6cac9ab1":"#here I save the most common pixels\nnumb = [494,493,466,465,521,488,438,522,467,489,516,437,460,410,487,492,439,517,354,495]","1dd11d26":"zer_arr = np.zeros((784)) #this is a series with 0s only, with length 784 because we have 28x28 pixels\ni = 0\n\nfor y in numb:\n    for i in range(784):\n        if i == y:\n            zer_arr[i+1] = 1 #here I replace zeros with 1 for the most common pixels. Note that the first pixel is 1, not 0, so we need i+1","564fef77":"matr = zer_arr.reshape((28,28)) #now the series becomes an array ","b665bda0":"fig, ax = plt.subplots(figsize=(6,5))\nsns.heatmap(matr, ax=ax, cmap=\"gnuplot\", cbar=False)\nax.set_title(\"Most common pixels\", fontsize=14)\nax.set_xlabel(\"Pixels\", fontsize=12)\nax.set_ylabel(\"Pixels\", fontsize=12)\n\n#fig.savefig(\"Mostcomm\", transparent = True)","2a1f22d4":"tot = df_expl.replace(0, np.NaN).drop(['label','category','tot_pixel','avg_intensity'], axis=1).count()\nval = tot.values\nval = val.reshape((28,28))","a643e8f2":"fig, ax = plt.subplots(figsize=(6,5))\nsns.heatmap(val, ax=ax, cbar_kws={'label': 'Frequency of pixels'}, cmap=\"inferno\")\nax.set_title(\"Pixel distribution\", fontsize=14)\nax.set_xlabel(\"Pixels\", fontsize=12)\nax.set_ylabel(\"Pixels\", fontsize=12)\n#fig.savefig(\"pixeldistr\", transparent=True)","36520420":"fig=plt.figure(figsize=(16, 28))\ncolumns = 2\nrows = 5\n\ni=1\nfor cat in df_expl['category'].unique():\n    tot_cat = df_expl[df_expl['category']==cat].replace(0, np.NaN).drop(['label','category','tot_pixel','avg_intensity'], axis=1).count()\n    val_cat = tot_cat.values.reshape((28,28))\n    ax = fig.add_subplot(rows, columns, i)\n    sns.heatmap(val_cat, ax=ax, cbar_kws={'label': 'Frequency of pixels'}, cmap=\"inferno\")\n    ax.set_title(\"Pixel distribution of \"+cat, fontsize=14)\n    ax.set_xlabel(\"Pixels\", fontsize=12)\n    ax.set_ylabel(\"Pixels\", fontsize=12)\n    \n    #fig.savefig(\"Distr\" + str(i))\n    i += 1\nplt.show()","29fc4ce9":" #we divide by 255 because we want values between 0 and 1. The highest intensity is 255, the lowest (empty pixel) is 0.\nX = df.to_numpy()[:,1:]\/255\nY = df.to_numpy()[:,0]","bb5e3ef1":"df_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ndf_test","69f6c8a4":"X_test = df_test.to_numpy()[:,1:]\/255 \nY_test = df_test.to_numpy()[:,0]","9a1b7fab":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42, max_iter=500, n_jobs=4, verbose=1).fit(X, Y)","55febb99":"print(clf.intercept_)","5dbe499a":"print(clf.coef_)","553f761f":"len(clf.coef_[0]) #this returns 784","76c2e1db":"print(clf.score(X_test,Y_test))","45a055dd":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #I want to see the whole matrix although is huge\nones = np.ones(10000) #I need an array of ones to count later\ntest_predictions = clf.predict(X_test) #array with all the predictions\nconf = pd.DataFrame({'actual':Y_test, 'predicted':test_predictions, 'tot':ones}) #I merge the predictions with actual values\nconf_matrix = conf.groupby(['actual','predicted'])['tot'].count().reset_index() #I count how many times each case happens\nconf_matrix","efc8adec":"conf_matrix['perc'] = conf_matrix['tot']\/1000\nconf_matrix","23759e8a":"conf_matrix['category_act'] = np.where(conf_matrix['actual']== 0, 'T-shirt\/top', np.where(conf_matrix['actual']== 1,'Trousers',np.where(conf_matrix['actual']== 2,'Pullover',np.where(conf_matrix['actual']== 3,'Dress',np.where(conf_matrix['actual']== 4,'Coat',np.where(conf_matrix['actual']== 5,'Sandal',np.where(conf_matrix['actual']== 6,'Shirt',np.where(conf_matrix['actual']== 7,'Sneaker',np.where(conf_matrix['actual']== 8,'Bag','Ankle_boot')))))))))\nconf_matrix['category_pred'] = np.where(conf_matrix['predicted']== 0, 'T-shirt\/top', np.where(conf_matrix['predicted']== 1,'Trousers',np.where(conf_matrix['predicted']== 2,'Pullover',np.where(conf_matrix['predicted']== 3,'Dress',np.where(conf_matrix['predicted']== 4,'Coat',np.where(conf_matrix['predicted']== 5,'Sandal',np.where(conf_matrix['predicted']== 6,'Shirt',np.where(conf_matrix['predicted']== 7,'Sneaker',np.where(conf_matrix['predicted']== 8,'Bag','Ankle_boot')))))))))\nconf_matrix","4988d182":"fig=plt.figure(figsize=(16, 28))\ncolumns = 2\nrows = 5\n\ni=1\nfor j, g in conf_matrix.groupby('category_act'):\n    fig.add_subplot(rows, columns, i)\n    labels = g['category_pred']\n    plt.pie(x=g['perc'],labels=labels)\n    plt.title(\"Confusion pie of \"+j, fontsize=14)\n    plt.legend()\n    i += 1","51f3bbe2":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(figsize=(16,12))\nclean_conf_mat = pd.DataFrame(confusion_matrix(Y_test, test_predictions), index=category_df[\"category\"], columns=category_df[\"category\"])\nsns.heatmap(clean_conf_mat, annot=True, ax = ax)","e38dd8aa":"max_lr = conf_matrix.groupby(['actual'])['perc'].max().reset_index().rename(columns={\"perc\": \"log_regr\"})\nmax_lr","738fc40e":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\nneigh.fit(X, Y)","029238e7":"predictions = neigh.predict(X_test)","7eed6f90":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_test, predictions)","948bc668":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #I want to see the whole matrix although is huge\nones = np.ones(10000) #I need an array of ones to count later\ntest_predictions = predictions #array with all the predictions\nconf = pd.DataFrame({'actual':Y_test, 'predicted':test_predictions, 'tot':ones}) #I merge the predictions with actual values\nconf_matrix = conf.groupby(['actual','predicted'])['tot'].count().reset_index() #I count how many times each case happens\nconf_matrix['perc'] = conf_matrix['tot']\/1000\nconf_matrix['category_act'] = np.where(conf_matrix['actual']== 0, 'T-shirt\/top', np.where(conf_matrix['actual']== 1,'Trousers',np.where(conf_matrix['actual']== 2,'Pullover',np.where(conf_matrix['actual']== 3,'Dress',np.where(conf_matrix['actual']== 4,'Coat',np.where(conf_matrix['actual']== 5,'Sandal',np.where(conf_matrix['actual']== 6,'Shirt',np.where(conf_matrix['actual']== 7,'Sneaker',np.where(conf_matrix['actual']== 8,'Bag','Ankle_boot')))))))))\nconf_matrix['category_pred'] = np.where(conf_matrix['predicted']== 0, 'T-shirt\/top', np.where(conf_matrix['predicted']== 1,'Trousers',np.where(conf_matrix['predicted']== 2,'Pullover',np.where(conf_matrix['predicted']== 3,'Dress',np.where(conf_matrix['predicted']== 4,'Coat',np.where(conf_matrix['predicted']== 5,'Sandal',np.where(conf_matrix['predicted']== 6,'Shirt',np.where(conf_matrix['predicted']== 7,'Sneaker',np.where(conf_matrix['predicted']== 8,'Bag','Ankle_boot')))))))))\nfig=plt.figure(figsize=(16, 28))\ncolumns = 2\nrows = 5\n\ni=1\nfor j, g in conf_matrix.groupby('category_act'):\n    fig.add_subplot(rows, columns, i)\n    labels = g['category_pred']\n    plt.pie(x=g['perc'],labels=labels)\n    plt.title(\"Confusion pie of \"+j, fontsize=14)\n    plt.legend()\n    i += 1","72aa86f6":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(figsize=(16,12))\nclean_conf_mat = pd.DataFrame(confusion_matrix(Y_test, test_predictions), index=category_df[\"category\"], columns=category_df[\"category\"])\nsns.heatmap(clean_conf_mat, annot=True, ax = ax)","74e1b8a1":"from sklearn.ensemble import RandomForestClassifier\nrdf_cf = RandomForestClassifier(n_jobs=4)\nrdf_cf.fit(X,Y)","b555b955":"predictions = rdf_cf.predict(X_test)","90198e8e":"rdf_cf.score(X_test,Y_test)","ed18f41c":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #I want to see the whole matrix although is huge\nones = np.ones(10000) #I need an array of ones to count later\ntest_predictions = predictions #array with all the predictions\nconf = pd.DataFrame({'actual':Y_test, 'predicted':test_predictions, 'tot':ones}) #I merge the predictions with actual values\nconf_matrix = conf.groupby(['actual','predicted'])['tot'].count().reset_index() #I count how many times each case happens\nconf_matrix['perc'] = conf_matrix['tot']\/1000\nconf_matrix['category_act'] = np.where(conf_matrix['actual']== 0, 'T-shirt\/top', np.where(conf_matrix['actual']== 1,'Trousers',np.where(conf_matrix['actual']== 2,'Pullover',np.where(conf_matrix['actual']== 3,'Dress',np.where(conf_matrix['actual']== 4,'Coat',np.where(conf_matrix['actual']== 5,'Sandal',np.where(conf_matrix['actual']== 6,'Shirt',np.where(conf_matrix['actual']== 7,'Sneaker',np.where(conf_matrix['actual']== 8,'Bag','Ankle_boot')))))))))\nconf_matrix['category_pred'] = np.where(conf_matrix['predicted']== 0, 'T-shirt\/top', np.where(conf_matrix['predicted']== 1,'Trousers',np.where(conf_matrix['predicted']== 2,'Pullover',np.where(conf_matrix['predicted']== 3,'Dress',np.where(conf_matrix['predicted']== 4,'Coat',np.where(conf_matrix['predicted']== 5,'Sandal',np.where(conf_matrix['predicted']== 6,'Shirt',np.where(conf_matrix['predicted']== 7,'Sneaker',np.where(conf_matrix['predicted']== 8,'Bag','Ankle_boot')))))))))\nfig=plt.figure(figsize=(16, 28))\ncolumns = 2\nrows = 5\n\ni=1\nfor j, g in conf_matrix.groupby('category_act'):\n    fig.add_subplot(rows, columns, i)\n    labels = g['category_pred']\n    plt.pie(x=g['perc'],labels=labels)\n    plt.title(\"Confusion pie of \"+j, fontsize=14)\n    plt.legend()\n    i += 1","48841e26":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(figsize=(16,12))\nclean_conf_mat = pd.DataFrame(confusion_matrix(Y_test, test_predictions), index=category_df[\"category\"], columns=category_df[\"category\"])\nsns.heatmap(clean_conf_mat, annot=True, ax = ax)","37e9383b":"from imageio import imread\nimport glob\nimport math\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image_path in glob.glob(\"..\/input\/real-world-fashion-mnist\/*.png\"):\n    image = imread(image_path)\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image)\n    i += 1\n\nplt.show()","287e4371":"def preprocess_shape(image):\n    if image.shape[0] == image.shape[1]:\n        return image\n\n    lines_to_add = math.floor(abs(image.shape[0] - image.shape[1])\/2)\n    reshaped_image = None\n    if image.shape[0] > image.shape[1]:\n        reshaped_image = np.zeros((image.shape[0],image.shape[0],4), dtype=np.uint8)\n        reshaped_image[:image.shape[0],lines_to_add:image.shape[1]+lines_to_add] = image\n    else:\n        reshaped_image = np.zeros((image.shape[1],image.shape[1],4), dtype=np.uint8)\n        reshaped_image[lines_to_add:image.shape[0]+lines_to_add,:image.shape[1]] = image\n    \n    return reshaped_image\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image_path in glob.glob(\"..\/input\/real-world-fashion-mnist\/*.png\"):\n    image = preprocess_shape(imread(image_path))\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image)\n    i += 1\n\nplt.show()","2a2defbc":"def preprocess_channels(image):\n    return image[:,:,:3].mean(axis=-1).astype(np.uint8)*image[:,:,3]\n    \nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image_path in glob.glob(\"..\/input\/real-world-fashion-mnist\/*.png\"):\n    image = preprocess_channels(preprocess_shape(imread(image_path)))\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image, cmap=\"Greys\")\n    i += 1\n\nplt.show()","91db7134":"humanized_classes = [\n    \"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n]","3e5a4d75":"import torch.nn as nn\nimport torch\n\ndef preprocess_down_scale_with_max(image):\n    target_width = target_height = 28\n    kernel_size = image.shape[0] \/\/ 28\n    return nn.MaxPool2d(kernel_size=kernel_size)(torch.tensor(image, dtype=torch.float32).unsqueeze(0)).squeeze(0).numpy()\/255\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image_path in glob.glob(\"..\/input\/real-world-fashion-mnist\/*.png\"):\n    image = preprocess_down_scale_with_max(preprocess_channels(preprocess_shape(imread(image_path))))\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image, cmap=\"Greys\")\n    plt.title(humanized_classes[rdf_cf.predict(image.reshape(1,28*28))[0]])\n    i += 1\n\nplt.show()","c8149adb":"def preprocess_down_scale_with_avg(image):\n    target_width = target_height = 28\n    kernel_size = image.shape[0] \/\/ 28\n    return nn.AvgPool2d(kernel_size=kernel_size)(torch.tensor(image, dtype=torch.float32).unsqueeze(0)).squeeze(0).numpy()\/255\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image_path in glob.glob(\"..\/input\/real-world-fashion-mnist\/*.png\"):\n    image = preprocess_down_scale_with_avg(preprocess_channels(preprocess_shape(imread(image_path))))\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image, cmap=\"Greys\")\n    plt.title(humanized_classes[rdf_cf.predict(image.reshape(1,28*28))[0]])\n    i += 1\n\nplt.show()","e29f6566":"from sklearn.cluster import KMeans\nkmn_cl = KMeans(n_clusters=10)\nkmn_cl.fit(X)","b802bd96":"clusters = kmn_cl.predict(X)","844f800e":"zero = X[clusters == 0][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","abbc9df8":"zero = X[clusters == 1][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","056a02f1":"zero = X[clusters == 2][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","0047dc0f":"zero = X[clusters == 3][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","86d16c26":"zero = X[clusters == 4][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","a819b2d1":"zero = X[clusters == 5][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","e1dc248f":"zero = X[clusters == 6][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","5c7d2f8f":"zero = X[clusters == 7][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","1767baca":"zero = X[clusters == 8][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","0f92b5e0":"zero = X[clusters == 9][:30]\n\nfig=plt.figure(figsize=(8, 8))\ncolumns = 5\nrows = 6\ni = 1\n\nfor image in zero:    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(image.reshape((28,28)), cmap=\"Greys\")\n    i += 1\n\nplt.show()","b0d8ff6d":"np.unique(clusters, return_counts=True)","3b57b6b0":"X_plus_cluster = np.zeros((X.shape[0],X.shape[1]+10))\nX_plus_cluster.shape","949148a4":"import torch\none_hotted = torch.nn.functional.one_hot(torch.tensor(clusters, dtype=torch.int64), num_classes=10).numpy()","ce881add":"X_plus_cluster[:,:X.shape[1]] = X\nX_plus_cluster[:,X.shape[1]:] = one_hotted","20b9b07d":"rdf_cf = RandomForestClassifier(n_jobs=4)\nrdf_cf.fit(X_plus_cluster,Y)","7e23afe4":"X_plus_cluster = np.zeros((X_test.shape[0],X_test.shape[1]+10))\nX_plus_cluster[:,:X_test.shape[1]] = X_test","642ea88b":"one_hotted_test = torch.nn.functional.one_hot(torch.tensor(kmn_cl.predict(X_test), dtype=torch.int64), num_classes=10).numpy()\nX_plus_cluster[:,X_test.shape[1]:] = one_hotted_test","41bb903e":"rdf_cf.score(X_plus_cluster,Y_test)","597465ba":"- Fashion MNIST is a dataset of Zalando cloth images. There are 60000 datatpoints in the training set, and 10000 in the test set.\n- Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels, which are our predictors. \n- Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. \n- The first column consists of the class labels, and represents the article of clothing. There are 10 classes: <br>\n0 T-shirt\/top <br>\n1 Trouser <br>\n2 Pullover <br>\n3 Dress <br>\n4 Coat <br>\n5 Sandal <br>\n6 Shirt <br>\n7 Sneaker <br>\n8 Bag <br>\n9 Ankle boot <br>","c97822e5":"## Clusturisation","47b10d0a":"Why are pictures so blurried? The obvious reason is that 28x28 pixels is not a good resolution, these pictures are very very small. Let's plot them in their original size","9c2d33df":"Nice job ! Images are squarred and centered.","6ecb8882":"## Pixel activation map\n\nWhich are the most common pixels (i.e. the ones that most images have), and which ones are the less frequent?","353a6a1e":"Let's see what our best model can do on images taken on the web. For simplification, we've only taken images with transparent background. But we know that this is one of the limitations of our model : if background isn't unified (for example the image of somebody wearing a shirt outside) our model would not be able to identify the shirt.","0cb9ddc7":"## Labels distribution\n\nIs the data homogeneous? How many datapoints for each category do we have?","fa4e2306":"The idea behind this is actually complex. It's that in a matrix representing an image, pixels that are closer with regards to the manhattan distance should be processed together.\nSo we will average blocks of pixels or we will take the max out of these blocks. Both methods are valid.\nThe size of the blocks (variable kernel_size in the following code) depends here of the image processed. Each image has a different resolution in our small hand made dataset. But we know that at the end we want a 28x28 resolution. We hence just have to divide the image resolution (which we have previously squarred) by 28, in order to find the size of the square blocks.","82668047":"## Do our clusters improve classification ?","37c181af":"These torch functions work with channeled images, so we'll have to artificially transform our image matrices into 3 dimensional tensors of size (1, squarred_image_size, squarred_image_size) using torch unsqueeze function.\nPlus we have to convert all our numpy matrices to torch tensors with float32 types. That's how torch works.","f6bbf468":"Let's see how this model worked and how it performs. First of all, we want to look at the intercept of the regression and the coefficients.","92690197":"### Average pooling","e884b4d7":"The following cell is taking only the maximum values, to have an idea about the accuracy of the model for each cloth category.","87c4e0b9":"This is the complete picture: the more yellow, the more frequent they appear, the more black, the least. Of course, pictures are in the middle, while corners are empty. Couple of more observations:\n* If you can focus, you can see a shirt in the picture\n* You can also see a shoe in the middle\n* In the middle of the yellow area, the most frequent pixels, there is an orange line, meaning less frequent pixels than yellow. Probably, this is because of trousers, which have a big hole between legs.","1c4f0c92":"With only one channel and with our method we still have more than enough information to distinguish categories.","edbc5eaa":"We have 10 intercepts, not 1. Why? Because we have 10 possible labels (outcome). About the values of these intercepts there is not much to say.","df570023":"K-Means finds classes that make sens, because we can see strong similarities inside them. But there are not usable for our classification goal, because they don't correspond to our classes.\nWe can see that it classified shoes into three different classes depending on their height, two classes for bags depending on their width and height, one class for bright clothes, and 4 classes for other cloths depending on their color intensity and their width.","a03a4121":"Now we want to see how good the model performs on our test dataset.","907d928f":"Now, we can group by the category and see average values for these two variables","01c7bce9":"We want to add one column with categories names instead of labels, this will be helpful later on.","c48a2741":"## Labels pixels intensity\nAre there any significant differences between cloth categories, in terms of average intensity of colour and average total number of pixels per image?\n\nWhen a pixel has value of 0, it means that it is \"empty\". Since we want to get the average of pixels' intensity and the total number of pixels for each image in the dataset, we replace these zeros with NaN, which are not counted in the calculations for definition. We also need to temporarly drop label column because it is numerical and shouldn't be included in the calculations.","e8fbc3df":"Now we have to do something about the 4 channels. Remember, we only want one.\nThere are different ways to proceed, we've selected the one that makes the most sense.\nFor each image we've taken the mean over the 3 color channels (RGB) and masked the pixels with the transparence channel.\nThe masking part was necessary for some images, because althought the transparency channel was very well defining the borders of the images, other channels didn't and were active outside the cloth. We could not see it because the transparency channel did well its job. However when we averaged the 4 channels, we saw it.\nSo here it's nicely done, everything works fine with this way to go.","c7d6bf0a":"# Conclusion","be5ff5a7":"## Best model","bae02191":"# Modelization","70a6fc4a":"## Random Forest","7ec917df":"## Decolorization","ab1b00a8":"In class, we always splitted our dataset into training and test set. In our case, the test dataset was already provided separated from the training one. This is actually good, because we cannot afford to \"lose\" training data. Indeed, we have many predictors, and to avoid overfitting, we need to ensure many observations as well. 784 predictors and 60000 observations sounds fine. Let's now import our test dataframe.","5e1b9a7e":"## Knn","d4834e36":"This is very interesting. \n* By plotting the frequencies of pixel by category, we get the \"average\" image of that category. \n* Some categories are more defined, other not really. This depends on how similar images of the same category are. Sandals are apparently the most heterogeneous.","bce8aaef":"Observations:\n\n* trousers, sandal, and sneaker have the less tot_pixel, i.e. the image is more \"empty\". Indeed, if we think about trousers, there is usually a big space between the legs, and if we think about shoes, they are two with also spaces in between. So, these three types have less concentrated images\n* trousers, coat, and boots have higher average intensity. This could be due to the usual colours of these clothes, which are higher often (dark colors are more popular than light ones).\n\nHowever, there are no strong pattern, conclutions are hard to be inferred","9d7c79ca":"0.85 is a very high value, the model gives good results. What are the most common mistakes? To do that, we need a confusion matrix.","456cc176":"Once more, we train our algorithm using our available data. ","d27b6248":"### Image visualization","99531bc3":"Let's start by creating our X and Y. All pixels are predictors, the label of cloth category is our target.","272ade48":"## Logistic regression\nWe want to apply one of the first methods we saw in class, logistic regression. ***Our dataset is huge, training takes very long, roughly 20 min***.","e090a12a":"Sklearn Logistic regression doesn't seem to be parallizable. But there is a way to transform it into something parallelizable so as to faster the training. It's by using neural nets in a certain way. We won't do it in there, as it is far away from the scope of our class.","1f4f6fe3":"And now the most interesting part : how do we achieve to reduce the signal of the images without losing to much information. There are several ways to go. We've selected the most simpliest, but still very interesting : it's called pooling and is very useful in deep convolutional learning to avoid overfitting in models with millions of parameters.","2a331768":"Again, we need to create our X and Y, same as before.","9b2a6964":"Let's see what these images look like for real.","5b3b70f2":"Observations:\n\n* Ankle boots are mainly confused with sneakers, which is ok because they are both shoes, but still different. So the gravity of this error is medium\n* Coat is confused with many other categories: pullover, shirt, dress. Here the difference * is huge, the gravity of this error is high\n* Pullover is confused with coat and shirt. Again the gravity is high\n* Sandals are confused with sneakers, since they are both shoes, the gravity is medium\n* Shirts are super confused, with t-shirt is fine (low gravity), with pullover and coats it's not that good (high gravity)\n* Trousers are the least confused. That sounds logical.","33ff7835":"We have 6000 datapoints per label, this means the dataset is homogenously distributed. This is great, having more data for one label would have created some troubles. Now we want to visualize this through a pie chart.","f3450a87":"## Testing our best model on almost real images","6cbf0ae4":"Now the picture is way more defined, but very small indeed.","e6b6a5ff":"We begin with some squarring through padding around the image. This kind of padding is convenient because it centers the images, exactly as in our dataset. Maybe we could have used some deep learning layers made for padding, but we didn't think of it for this step.","a1198f4f":"Let's add the cloth category names again, with numbers it is a bit tricky to draw conclusions.","2d6458de":"We have a 2D array with 784 elements (the pixels), each one has 10 values (the labels). If we look at the values, they are incredibly low. Why? Because one single pixel cannot predict the label. It's the combination that is unique for a cloth category.","48fdf361":"## Downscaling","5f9d7d74":"Now we want to plot percentages with a pie chart for each cloth category. Instead of a confusion matrix we now have a confusion pie.","7ad8cafa":"Once again, we train the algorithm using our train data, and then predict the nature of the clothes in our test set. ","de5362ed":"# Fashion MNIST","31ef2ec4":"To have a better understanding of data, we want to plot some images. In the following cell, we used the function randint to generate random numbers.\nWe want to have 5 random picture for each category,so we have first a for cycle to loop through categories and then a for cycle to generate 5 images each time.","60099cc3":"Not really !","411afbb4":"In our case, these functions are not very insightful.","9463a6a9":"We won't do any hyperparameters optimization in this notebook, as it is already a dense notebook and hyperopt takes a long time.","6f2bb6b5":"Let's start with some basic functions to explore the data: info() and describe()","27235ca0":"But we cannot directly fit these images to our model because :\n* We have four color channel (RGB plus transparency) making the image signal a 3 dimensionnal tensor like (4, rows, columns) and our model only works for images with one color channel, so with matrix tensors\n* Images are not squared, so we have to add padding around them\n* Images don't have a homogeneous resolution, our model only works with images of size 28x28, so we will have to downscale them !","f2bb7538":"# Exploratory Analysis","732abea0":"Here we can see that some clusters are bigger than others.","ede5b2c6":"Looks more like our initial dataset ! Prediction score is far from perfect though. We've got some big errors, a few well classified object and a few small classification errors.","f84eaa1f":"There are two main reasons why we're doing K-Means here :\n* Can it serves as a classification model, assuming the classes it finds are the same as ours ?\n* How does K-Means behaves generally speaking ?","56cbd6eb":"As opposed to what was done in class, we will not be using classic binary classification. Indeed, we are looking to sort clothes that belong to 10 different classes. Therefore sklearn will automatically resort to methods that generalize the binary classification to multiple classes: namely the one versus one approach (which happens to be more efficient than the one versus all approach).  ","1659f534":"We've selected random forest as our best model, because it gives same prediction score as the others, but take way less training time than Logistic Regression and way less inference time than KNN Classifier.","c5ff2917":"Now I want to see the percentages, which is more interesting. Note that the test dataset has 10000 datapoints, 1000 per category, so I can simply divide each value by 1000 to have % in each category.","95d50108":"Now we want to see the frequency of each pixel, so to have an idea about the distribution. To do this, we count how many times each pixel appear in pictures and then we plot these frequencies through a heatmap. It's same as before but we plot every pixel, not only the most frequent ones.","af4e2ea2":"Since we will add some columns, better to make a copy of the dataset when exploring the data.","b28ae000":"With max-pooling the images don't look like the ones of our training dataset. One interesting thing is that it underlines the borders of our objects.","13d854f0":"Let's plot them using a barplot","681f3aa2":"Wow real images with colors and high resolution !","b7223c68":"This seems pretty obvious: \n* the most common pixels are the ones in the \"middle\" of an image (remember we have 784 pixels, so pixels 400  \/ 500 are almost in the middle)\n* the less common are the ones in the corners (pixel 1, 2, 28, 758...)\n\nLet's plot the most common ones. How to? We need to:\n1. Save these pixels somewhere\n2. Create an empty series\n3. Put the most common pixels in the empty series\n4. Reshape in 28x28 to have an array\n5. Plot them with a heatmap (seaborn)\n","14119adc":"Here we'll use some functions of the torch convolutional toolkit. To have a visual idea of what those functions are doing, please look at : https:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/README.md\n","7aae6e8d":"We have seen that some characteristics of images are recognizable in the distribution of pixels. What happens if we do the same for each cloth category?","188c5e9e":"To this we count how many times each pixel appears and we take the first 20 and last 20.","5bc83dd2":"### Maxpooling","474bd7ea":"We can then use the KNN method to predict the types of clothes our data belongs to. ","ee886d20":"**All this process has taught us one new limitation of our algorithm : we loose to much information with this small resolution. Of course, here, even with low resolution we could have had a near perfect prediction score with a deep convolutional residual network, but classic algorithms would need more signal and thus a higher resolution. The only problem is the curse of the dimensionnality. And we are not sure that classic algorithm would be appropriate for higher resolution images**","8b06d534":"## Padding","5ebfc306":"Let us try predicting the nature of the clothes in our dataset using one more method: Random Forest! "}}