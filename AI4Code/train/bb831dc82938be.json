{"cell_type":{"b4b35ab2":"code","814aab21":"code","f93940fb":"markdown"},"source":{"b4b35ab2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","814aab21":"import pandas as pd\nimport numpy as np\nimport sqlite3\n%load_ext tensorboard\n\nimport os\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorboard.plugins import projector\nimport datetime as dt\n\n# Connect to the file (.db)\n#Use this line to run this notebook on Kaggle\ndb = sqlite3.connect('\/kaggle\/input\/clubhouse-dataset\/Clubhouse_Dataset_v1.db')\n\n#Use this line to run this notebook on Google Colab\n#db = sqlite3.connect('\/home\/Clubhouse_Dataset_v1.db')\n\n# Read the file as Data frame\ndf = pd.read_sql_query(\"SELECT * FROM user\", db)\n\n#Generate descriptive statistics on dataset\n# Set data as float\npd.set_option('float_format', '{:f}'.format)\nprint('Describe-----------------------------------------')\nprint(df.describe())\n#Summary of dataframe\nprint('Info-----------------------------------------')\nprint(df.info())\n#Returns the first 5 rows of object\nprint('Head-----------------------------------------')\nprint(df.head())\n#Returns the last 5 rows of object\nprint('Tail--------------------------------------------------')\nprint(df.tail())\nprint('--------------------------------------------------')\n\n#Reduce data to top 20000 for visualisation\ndf = df.head(20000)\n\n#Convert string to date time\ndf['time_created'] = df['time_created'].apply(lambda x: \n                                    dt.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%f%z'))\n\n#Convert time to integer\ndf['time_created'] = df['time_created'].apply(lambda x: x.value)\ndf['time_created'] = df['time_created'].apply(lambda x: x\/1000000000000000)\n\n#Standardisation\n#Normalise between zero and one \nfeature_name = 'num_followers'\nmax_value = df[feature_name].max()\nmin_value = df[feature_name].min()\ndf[feature_name] = df[feature_name].apply(lambda x: (x - min_value) \/ (max_value - min_value))\n\n#Normalise between zero and one \nfeature_name = 'time_created'\nmax_value = df[feature_name].max()\nmin_value = df[feature_name].min()\ndf[feature_name] = df[feature_name].apply(lambda x: (x - min_value) \/ (max_value - min_value))\n\n#Normalise between zero and one \nfeature_name = 'num_following'\nmax_value = df[feature_name].max()\nmin_value = df[feature_name].min()\ndf[feature_name] = df[feature_name].apply(lambda x: (x - min_value) \/ (max_value - min_value))\n\n#Transform logic\ndf['invited_by_user_profile'] = df['invited_by_user_profile'].fillna(1, inplace=True)\n#Replace nulls with zero.\ndf.fillna(0, inplace=True)\n# Change field to int to allow for inclusion in principle component analysis\ndf['invited_by_user_profile'] = df['invited_by_user_profile'].astype(int)\n#print(df.head())\n#Move the invided by user profile to the first column so tensorboard will priortise analysis based on this field and connect users who share a common inviter.\ndf = df[['invited_by_user_profile','user_id', 'name', 'photo_url', 'username', 'twitter', 'instagram', 'num_followers', 'num_following', 'time_created']]\n\n\n#Prepare data for tensorboard\nclubhouse_features = df.copy()\n#Remove non numeric fields\nclubhouse_features.pop('photo_url')\nclubhouse_features.pop('username')\nclubhouse_features.pop('twitter')\nclubhouse_features.pop('instagram')\n#Create Dataframe to hold names as labels \nclubhouse_target = clubhouse_features.pop('name')\n\nprint('Features-----------------------------------------')\nprint(clubhouse_features)\nprint('Labels-----------------------------------------')\nprint(clubhouse_target)\nprint('-----------------------------------------')\n\n#Save data to log file for visualisation in Tensorboard\nLOG_DIR='\/logs\/clubhouse\/'\nif not os.path.exists(LOG_DIR):\n    os.makedirs(LOG_DIR)\n\nembeddings = tf.Variable(clubhouse_features, name='embeddings')\nCHECKPOINT_FILE = LOG_DIR + '\/model.ckpt'\nckpt = tf.train.Checkpoint(embeddings=embeddings)\nckpt.save(CHECKPOINT_FILE)\n\nreader = tf.train.load_checkpoint(LOG_DIR)\nmap = reader.get_variable_to_shape_map()\nkey_to_use = \"\"\nfor key in map:\n    if \"embeddings\" in key:\n        key_to_use = key\n\nconfig = projector.ProjectorConfig()\nembedding = config.embeddings.add()\nembedding.tensor_name = key_to_use\nembedding.metadata_path = '\/logs\/clubhouse\/metadata.tsv'\n\nwriter = tf.summary.create_file_writer(LOG_DIR)\nprojector.visualize_embeddings(LOG_DIR, config)\n\n# Save Labels separately on a line-by-line manner.\nwith open(os.path.join(LOG_DIR, 'metadata.tsv'), \"w\") as f:\n  for name in clubhouse_target:\n    f.write(\"{}\\n\".format(name))\n    \nfrom tensorboard import notebook\nnotebook.display(port=6006, height=100) \n%tensorboard --logdir \/logs\/clubhouse\/","f93940fb":"<h1>Principle Component Analysis of Clubhouse Data<\/h1> \n\n[![PCA Screenshot.png](attachment:f24ece5e-8caf-44c0-bf15-04bc7d3aed61.png)](http:\/\/)\n\nVideo of PCA showing interactive nature of visualising data in Tensorboard.\nhttps:\/\/youtu.be\/i6IW4npQC98\n\nPrincipal component analysis (PCA) makes it easier to explore and visualize large datasets. \n\nPCA is a method that trades accuracy for simplicity to allow the analysis of large datasets.\nPCA is similar to compressing a music file into mp3, the compression process trades precision for reducing the memory required.\n\nFor the purpose of this task the PCA feature of Tensorboard is used to visualise the Clubhouse dataset in a visually appealing way. \n\nThis notebook prepares the Clubhouse V1 dataset for visualisation in Tensorboard.\n\nKaggle does not currently the support Tensorboard https:\/\/www.kaggle.com\/product-feedback\/89671\n\nThe steps to reproduce this visualisation in Tensorboard are:\n<ol>\n<li>Open https:\/\/colab.research.google.com\/<\/li>\n<li>Download the Clubhousev1.db dataset<\/li>\n<li>Upload the Clubhousev1.db dataset to colab notebook into the Colab \/home folder\n<ol>\n<li>Click on the Files icon on left side of the screen<\/li>\n<li>Navigate to \/home folder<\/li>\n<li>Click on the 3 dots connected to the home folder <\/li>\n<li>Choose Upload. The upload will take a few minutes to complete.\n<\/ol>\n<\/li>    \n<li>Comment out this line of the code with '#'<\/li>\ndb = sqlite3.connect('\/kaggle\/input\/clubhouse-dataset\/Clubhouse_Dataset_v1.db')\n<li>Uncomment this line of code by removing the '#'<\/li>\n#db = sqlite3.connect('\/home\/Clubhouse_Dataset_v1.db')\n<li>Run the notebook code to produce the Tensorboard visualisation of Clubhouse data<\/li>\n<\/ol> "}}