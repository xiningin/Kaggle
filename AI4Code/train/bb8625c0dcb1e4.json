{"cell_type":{"1de80887":"code","150fbc2c":"code","58737d7b":"code","2d910730":"code","d022615c":"code","c07531c6":"code","9d663649":"code","760ecbc8":"code","ff447952":"code","90dbbbc8":"code","9d2f3bce":"code","9a153f80":"code","0d76480b":"code","91e20a05":"code","ce8eb74c":"code","5888decd":"code","cc16b610":"code","1b9e3341":"code","dde799d4":"code","b76ba079":"code","61a1c3cc":"code","37fa53a2":"code","5119f801":"code","0bdbe93e":"code","85bfc7ee":"markdown","9a004010":"markdown","c582e023":"markdown","0a6ab97a":"markdown","73ebfe90":"markdown","b154466d":"markdown","5364f8cb":"markdown","976cf5e8":"markdown","f33ab8ba":"markdown","6b6bc092":"markdown","d0f7e45b":"markdown","1ef2d3fb":"markdown","26fcbb37":"markdown","ba3e0ca4":"markdown","f3e2b544":"markdown","c28668e1":"markdown","a0118215":"markdown","bd8d2bc3":"markdown","ff08ce95":"markdown"},"source":{"1de80887":"pip install autocorrect","150fbc2c":"# Importing Libraries\nimport unidecode\nimport pandas as pd\nimport re\nimport time\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom autocorrect import Speller\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string\nimport timeit\nstoplist = stopwords.words('english') \nstoplist = set(stoplist)\nspell = Speller(lang='en')\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()","58737d7b":"# Read Dataset\nDf = pd.read_csv(r'..\/input\/fakereal-news\/New Task.csv', encoding = 'latin-1')\nprint('Number of Data points : ', Df.shape[0])\nprint('Number of features :', Df.shape[1])\nprint('features :', Df.columns.values)\n# Show Dataset\nDf.drop(Df.columns[Df.columns.str.contains('Unnamed: 0',case = False)],axis = 1, inplace = True)\nDf.head()\n","2d910730":"# This command tells information about the non-null values of attributes of Dataset.\nDf.info()","d022615c":"Df['News_Headline'][0]","c07531c6":"# Shows statistics for every numerical column in our dataset.\nDf.describe()","9d663649":"#Type of attribute \"Title\"\ntype(Df['News_Headline'])\n","760ecbc8":"def remove_newlines_tabs(text):\n    \"\"\"\n    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n        \n    Example:\n    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n    Output : This is her first day at this place. Please, Be nice to her. \n    \n    \"\"\"\n    \n    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    return Formatted_text\n# len of data :- 1618647 lac words","ff447952":"def strip_html_tags(text):\n    \"\"\" \n    This function will remove all the occurrences of html tags from the text.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of html tags.\n        \n    Example:\n    Input : This is a nice place to live. <IMG>\n    Output : This is a nice place to live.  \n    \"\"\"\n    # Initiating BeautifulSoup object soup.\n    soup = BeautifulSoup(text, \"html.parser\")\n    # Get all the text other than html tags.\n    stripped_text = soup.get_text(separator=\" \")\n    return stripped_text\n# len of string:- 1616053 lac words","90dbbbc8":"def remove_links(text):\n    \"\"\"\n    This function will remove all the occurrences of links.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of all types of links.\n        \n    Example:\n    Input : To know more about cats and food & website: catster.com  visit: https:\/\/catster.com\/\/how-to-feed-cats\n    Output : To know more about cats and food & website: visit:     \n    \n    \"\"\"\n    \n    # Removing all the occurrences of links that starts with https\n    remove_https = re.sub(r'http\\S+', '', text)\n    # Remove all the occurrences of text that ends with .com\n    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    return remove_com\n# len of words:- 1616053","9d2f3bce":"def remove_whitespace(text):\n    \"\"\" This function will remove \n        extra whitespaces from the text\n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after extra whitespaces removed .\n        \n    Example:\n    Input : How   are   you   doing   ?\n    Output : How are you doing ?     \n        \n    \"\"\"\n    pattern = re.compile(r'\\s+') \n    Without_whitespace = re.sub(pattern, ' ', text)\n    # There are some instances where there is no space after '?' & ')', \n    # So I am replacing these with one space so that It will not consider two words as one token.\n    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n    return text    \n# len of words:- 1596248 lac words","9a153f80":"# Code for accented characters removal\ndef accented_characters_removal(text):\n    # this is a docstring\n    \"\"\"\n    The function will remove accented characters from the \n    text contained within the Dataset.\n       \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" with removed accented characters.\n        \n    Example:\n    Input : M\u00e1laga, \u00e0\u00e9\u00ea\u00f6hello\n    Output : Malaga, aeeohello    \n        \n    \"\"\"\n    # Remove accented characters from text using unidecode.\n    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n    text = unidecode.unidecode(text)\n    return text\n# Len of data:- 1593952 lac of words","0d76480b":"# Code for text lowercasing\ndef lower_casing_text(text):\n    \n    \"\"\"\n    The function will convert text into lower case.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n         value: text in lowercase\n         \n    Example:\n    Input : The World is Full of Surprises!\n    Output : the world is full of surprises!\n    \n    \"\"\"\n    # Convert text to lower case\n    # lower() - It converts all upperase letter of given string to lowercase.\n    text = text.lower()\n    return text\n","91e20a05":"# Code for removing repeated characters and punctuations\n\ndef reducing_incorrect_character_repeatation(text):\n    \"\"\"\n    This Function will reduce repeatition to two characters \n    for alphabets and to one character for punctuations.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Finally formatted text with alphabets repeating to \n        two characters & punctuations limited to one repeatition \n        \n    Example:\n    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n    Output : Reallyy, Greeaatt !?.;:)\n    \n    \"\"\"\n    # Pattern matching for all case alphabets\n    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n    \n    # Limiting all the  repeatation to two characters.\n    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n    \n    # Pattern matching for all the punctuations that can occur\n    Pattern_Punct = re.compile(r'([.,\/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n    \n    # Limiting punctuations in previously formatted string to only one.\n    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n    \n    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n    return Final_Formatted\n","ce8eb74c":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}\n# The code for expanding contraction words\ndef expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n    \"\"\"expand shortened words to the actual form.\n       e.g. don't to do not\n    \n       arguments:\n            input_text: \"text\" of type \"String\".\n         \n       return:\n            value: Text with expanded form of shorthened words.\n        \n       Example: \n       Input : ain't, aren't, can't, cause, can't've\n       Output :  is not, are not, cannot, because, cannot have \n    \n     \"\"\"\n    # Tokenizing text into tokens.\n    list_Of_tokens = text.split(' ')\n\n    # Checking for whether the given token matches with the Key & replacing word with key's value.\n    \n    # Check whether Word is in lidt_Of_tokens or not.\n    for Word in list_Of_tokens: \n        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n         if Word in CONTRACTION_MAP: \n                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n                \n    # Converting list of tokens to String.\n    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n    return String_Of_tokens     \n   ","5888decd":"# The code for removing special characters\ndef removing_special_characters(text):\n    \"\"\"Removing all the special characters except the one that is passed within \n       the regex to match, as they have imp meaning in the text provided.\n   \n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text with removed special characters that don't require.\n        \n    Example: \n    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n    \n   \"\"\"\n    # The formatted text after removing not necessary punctuations.\n    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n    return Formatted_Text\n","cc16b610":"# The code for removing stopwords\nstoplist = stopwords.words('english') \nstoplist = set(stoplist)\ndef removing_stopwords(text):\n    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n       & they can be remove safely without comprimising meaning of the sentence.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text after omitted all stopwords.\n        \n    Example: \n    Input : This is Kajal from delhi who came here to study.\n    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n    \n   \"\"\"\n    # repr() function actually gives the precise information about the string\n    text = repr(text)\n    # Text without stopwords\n    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n    # Convert list of tokens_without_stopwords to String type.\n    words_string = ' '.join(No_StopWords)    \n    return words_string\n","1b9e3341":"# The code for spelling corrections\ndef spelling_correction(text):\n    ''' \n    This function will correct spellings.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text after corrected spellings.\n        \n    Example: \n    Input : This is Oberois from Dlhi who came heree to studdy.\n    Output : This is Oberoi from Delhi who came here to study.\n      \n    \n    '''\n    # Check for spellings in English language\n    spell = Speller(lang='en')\n    Corrected_text = spell(text)\n    return Corrected_text\n","dde799d4":"# The code for lemmatization\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\ndef lemmatization(text):\n    \"\"\"This function converts word to their root words \n       without explicitely cut down as done in stemming.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text having root words only, no tense form, no plural forms\n        \n    Example: \n    Input : text reduced \n    Output :  text reduce\n    \n   \"\"\"\n    # Converting words to their root forms\n    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n    return lemma\n","b76ba079":"# Writing main function to merge all the preprocessing steps.\ndef text_preprocessing(text, accented_chars=True, contractions=True, lemma = True,\n                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n                       lowercase=True, punctuations=True, mis_spell=True,\n                       remove_html=True, links=True,  special_chars=True,\n                       stop_words=False):\n    \"\"\"\n    This function will preprocess input text and return\n    the clean text.\n    \"\"\"\n        \n    if newlines_tabs == True: #remove newlines & tabs.\n        Data = remove_newlines_tabs(text)\n\n    if remove_html == True: #remove html tags\n        Data = strip_html_tags(Data)\n\n    if links == True: #remove links\n        Data = remove_links(Data)\n\n    if extra_whitespace == True: #remove extra whitespaces\n        Data = remove_whitespace(Data)\n\n    if accented_chars == True: #remove accented characters\n        Data = accented_characters_removal(Data)\n\n    if lowercase == True: #convert all characters to lowercase\n        Data = lower_casing_text(Data)\n\n    if repeatition == True: #Reduce repeatitions   \n        Data = reducing_incorrect_character_repeatation(Data)\n\n    if contractions == True: #expand contractions\n        Data = expand_contractions(Data)\n\n    if punctuations == True: #remove punctuations\n        Data = removing_special_characters(Data)\n\n    stoplist = stopwords.words('english') \n    stoplist = set(stoplist)\n    \n    if stop_words == True: #Remove stopwords\n        Data = removing_stopwords(Data)\n\n    spell = Speller(lang='en')\n    \n    if mis_spell == True: #Check for mis-spelled words & correct them.\n        Data = spelling_correction(Data)\n\n    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n     \n    if lemma == True: #Converts words to lemma form.\n        Data = lemmatization(Data)\n\n           \n    return Data","61a1c3cc":"# Pre-processing for Content\nList_Content = Df['News_Headline'].to_list()\nFinal_Article = []\nComplete_Content = []\nfor article in List_Content:\n    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n    Final_Article.append(Processed_Content)\nComplete_Content.extend(Final_Article)\nDf['Processed_Title'] = Complete_Content\n","37fa53a2":"Df['Processed_Title']","5119f801":"Df.head()","0bdbe93e":"Cleaned_Data = Df.to_csv('Cleaned_Data_with_Stopwords.csv', index = False)","85bfc7ee":"### Step2: Case Conversion","9a004010":"### Step9: Putting all in single function","c582e023":"### Remove Links ","0a6ab97a":"### Checking spellings for all the stopwords ","73ebfe90":"### Check type of Dataframe attribute that has to processed","b154466d":"### Remove WhiteSpaces","5364f8cb":"## Step7: Lemmatization","976cf5e8":"### Step3: Reduce repeated characters and punctuations\u00b6","f33ab8ba":"## Step5: Remove special characters","6b6bc092":"### Punctuations that I am considering Important as per my Dataset.\n**,.?!** --> These are some frequent punctuations that occurs a lot and needed to preserved as to understand the context of text.\n\n__:__ --> This one is also frequent as per the  Dataset. It is important to keep bcz it is giving sense whenever there is a occurrence of time like: **9:05 p.m.**\n\n**%** --> This one is also frequently used in many articles and telling more precisely about the data, facts & figures.\n\n**$** --> This one is used in many articles where prices are considered. So, omitting this symbol will not give much sense about those prices that left as just some numbers only.","d0f7e45b":"## Step8: Correct mis-spelled words in text","1ef2d3fb":"# Text Preprocessing","26fcbb37":"### Explanation for using some symbols in above regex expression\n**\\1** \u2014> is equivalent to re.search(...). group(1). It Refers to first capturing group. \\1 matches the exact same text that was matched by the first capturing group.\n\n**{1,}** --> It means we are matching for repeatation that occurs more than one times. \n\n**DOTALL** -> It matches newline character as well unlike dot operator which matches everything in the given text except newline character. \n\n**sub()** --> This function is used to replace occurrences of a particular sub-string with another sub-string. This function takes as input the following: The sub-string to replace. The sub-string to replace with.\n\n**r'\\1\\1'** --> It limits all the repeatation to two characters.\n\n**r'\\1'** --> Limits all the repeatation to only one character.\n\n**{2,}** --> It means to match for repeatation that occurs more than two times","ba3e0ca4":"### Import Libraries","f3e2b544":"## Step4: Expand contraction words","c28668e1":"### Remove newlines & tabs ","a0118215":"### Strip Html Tags","bd8d2bc3":"## Step6: Remove stopwords","ff08ce95":"### Step1: Remove Accented Characters\n"}}