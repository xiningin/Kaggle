{"cell_type":{"189c1d1a":"code","7ee2db39":"code","dbaabb39":"code","febdf9b4":"code","20cf068c":"code","51107c08":"code","d1aabf31":"code","f203088c":"code","a6929b3f":"code","964d5cba":"code","27f8c5dc":"code","e4a9dea4":"code","703ecd16":"code","4044a10a":"code","8d3491e3":"code","e6b60751":"code","9b094c90":"code","360e22da":"code","4a8821f3":"code","e114f164":"code","60179fb8":"code","0d9cda29":"code","079acc84":"code","51c03f91":"code","43e9b04d":"code","f86524d8":"code","685c61df":"code","1f036ec8":"code","7d6d4d25":"code","2b6a0be0":"markdown","67972911":"markdown","a4439686":"markdown","cd7d54df":"markdown","a5463b96":"markdown","5e4d7d6e":"markdown","c353fa2b":"markdown","34fba3f5":"markdown","e8561746":"markdown","f183e53d":"markdown","3a5448d0":"markdown","cf0fffc7":"markdown","9a919a72":"markdown","d259d391":"markdown","31a79f4a":"markdown","11be520d":"markdown","515cff42":"markdown","e01852a9":"markdown","d8c28889":"markdown","705cb645":"markdown"},"source":{"189c1d1a":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","7ee2db39":"train_data = pd.read_csv(\"..\/input\/Train.csv\")\ntest_data = pd.read_csv(\"..\/input\/Test.csv\")","dbaabb39":"train_data.head()","febdf9b4":"test_data.head()","20cf068c":"# Checking for null values\ntest_data.isnull().sum()","51107c08":"# Pot Distribution\n\nfig, ax = plt.subplots(figsize = (3, 4), nrows = 2, ncols = 3)\nax_1 = sns.boxplot(y=\"SalesInMillions\", data=train_data, ax = ax[0][0]);\nax_1.set_title(\"SalesInMillions\", fontsize = 18)\n\nax_2 = sns.boxplot(y=\"CRITICS_POINTS\", data=train_data, ax = ax[0][1]);\nax_2.set_title(\"CRITICS_POINTS\", fontsize = 18)\n\nax_3 = sns.boxplot(y=\"USER_POINTS\", data=train_data, ax = ax[0][2]);\nax_3.set_title(\"USER_POINTS\", fontsize = 18)\n\nax_4 = sns.boxplot(y=\"CRITICS_POINTS\", data=test_data, ax = ax[1][0]);\nax_4.set_title(\"CRITICS_POINTS\", fontsize = 18)\n\nax_5 = sns.boxplot(y=\"USER_POINTS\", data=test_data, ax = ax[1][1]);\nax_5.set_title(\"USER_POINTS\", fontsize = 18)\n\nfig.subplots_adjust(right = 6, top = 3)\nfig.show()","d1aabf31":"train_data = train_data[train_data['SalesInMillions'] < 40]","f203088c":"# Bar Plots\nfig, ax = plt.subplots(figsize=(10, 8),ncols = 4)\n\nax1 = sns.barplot(x = 'CONSOLE', y = 'SalesInMillions', data = train_data, ax = ax[0]);\nax1.set_title('CONSOLE',  fontsize = 18)\n\nax1 = sns.barplot(x = 'CATEGORY', y = 'SalesInMillions', data = train_data, ax = ax[1]);\nax1.set_title('CATEGORY',  fontsize = 18)\n\nax1 = sns.barplot(x = 'PUBLISHER', y = 'SalesInMillions', data = train_data, ax = ax[2]);\nax1.set_title('PUBLISHER',  fontsize = 18)\n\nax1 = sns.barplot(x = 'RATING', y = 'SalesInMillions', data = train_data, ax = ax[3]);\nax1.set_title('RATING',  fontsize = 18)\n\nfig.subplots_adjust(left = 1, right = 3)\nfig.show()","a6929b3f":"print('Train Data missing values :\\n',train_data.isnull().sum())\nprint('\\nTest Data missing values :\\n',test_data.isnull().sum())","964d5cba":"train_data.head()","27f8c5dc":"train_data['YEAR'] = 2020 - train_data['YEAR']\ntest_data['YEAR'] = 2020 - test_data['YEAR']\ntrain_data.head()","e4a9dea4":"# FIXING PUBLISHERS\n\n# Sort publishers by counts\nvals, cnts = np.unique(train_data['PUBLISHER'], return_counts = True)\nmapping = {k : v for k,v in zip(vals,cnts)}\nmapping = {k : v for (k,v) in sorted(mapping.items(), key = lambda x: x[-1], reverse = True)}\n\n# Binning publishers by count\nnew_map = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[]}\nfor k, v in mapping.items():\n    if v >= 200 :\n        new_map[6].append(k)\n    elif v >= 150 :\n        new_map[5].append(k)\n    elif v >= 100 :\n        new_map[4].append(k)\n    elif v >= 50 :\n        new_map[3].append(k)\n    elif v >= 10 :\n        new_map[2].append(k)\n    else :\n        new_map[1].append(k)\n#new_map    \n\n# Assign new categories to publishers (Numeral Label Encoding is used deliberately)\npub_mapping = {}\nfor a, b in mapping.items() :\n    for l, m in new_map.items() :\n        if a in m :\n            pub_mapping[a] = l\n\n#MAKE CHANGES\ntrain_data['PUBLISHER'] = train_data['PUBLISHER'].map(pub_mapping)\ntest_data['PUBLISHER'] = test_data['PUBLISHER'].map(pub_mapping)\ntrain_data.head()","703ecd16":"# FIXING CATEGORIES\n\n# Sort categories by counts\nvals, cnts = np.unique(train_data['CATEGORY'], return_counts = True)\nmapping = {k : v for k,v in zip(vals,cnts)}\nmapping = {k : v for (k,v) in sorted(mapping.items(), key = lambda x: x[-1], reverse = True)}\n\n# New Mapping\ncat_mapping = {'action': 'action',\n                 'sports': 'sports',\n                 'shooter': 'shooter',\n                 'role-playing': 'role-playing',\n                 'racing': 'sports',\n                 'misc': 'misc',\n                 'platform': 'misc',\n                 'fighting': 'misc',\n                 'simulation': 'misc',\n                 'strategy': 'misc',\n                 'adventure': 'misc',\n                 'puzzle': 'misc'}\n\n# MAP BY NEW ORDER\ntrain_data['CATEGORY'] = train_data['CATEGORY'].map(cat_mapping)\ntest_data['CATEGORY'] = test_data['CATEGORY'].map(pub_mapping)\n\n# Get dummies\ncategories_train = pd.get_dummies(train_data['CATEGORY'], drop_first = True)\ncategories_test = pd.get_dummies(test_data['CATEGORY'], drop_first = True)\ntrain_data.head()","4044a10a":"# FIXING CONSOLE\n\n# Sort consoles by count\nvals, cnts = np.unique(train_data['CONSOLE'], return_counts = True)\nmapping = {k : v for k,v in zip(vals,cnts)}\nmapping = {k : v for (k,v) in sorted(mapping.items(), key = lambda x: x[-1], reverse = True)}\n\n# New Mapping\nconsole_mapping = {'ps2': 'ps',\n     'x360': 'xbox',\n     'ps3': 'ps',\n     'pc': 'pc',\n     'x': 'xbox',\n     'wii': 'other',\n     'ds': 'other',\n     'psp': 'ps',\n     'gc': 'other',\n     'gba': 'other',\n     'ps4': 'ps',\n     'xone': 'xbox',\n     '3ds': 'other',\n     'ps': 'ps',\n     'psv': 'ps',\n     'wiiu': 'other',\n     'dc': 'other'}\n\n# Map by new order\ntrain_data['CONSOLE'] = train_data['CONSOLE'].map(console_mapping)\ntest_data['CONSOLE'] = test_data['CONSOLE'].map(console_mapping)\n\n# Get dummies\nconsole_train = pd.get_dummies(train_data['CONSOLE'], drop_first = True)\nconsole_test = pd.get_dummies(test_data['CONSOLE'], drop_first = True)\n#console\n\ntrain_data.head(5)","8d3491e3":"# FIXING RATING\n\n# Sort ratings by count\nvals, cnts = np.unique(train_data['RATING'], return_counts = True)\nmapping = {k : v for k,v in zip(vals,cnts)}\nmapping = {k : v for (k,v) in sorted(mapping.items(), key = lambda x: x[-1], reverse = True)}\n\n# New Mapping\nrate_mapping = {'T': 'T', 'E': 'E', 'M': 'M', 'E10+': 'E10+', 'RP': 'M', 'K-A': 'M'}\n\n# Map by new order\ntrain_data['RATING'] = train_data['RATING'].map(rate_mapping)\ntest_data['RATING'] = test_data['RATING'].map(rate_mapping)\n\n# Get Dummies\nrating_train = pd.get_dummies(train_data['RATING'], drop_first = True)\nrating_test = pd.get_dummies(test_data['RATING'], drop_first = True)\ntrain_data.head(5)","e6b60751":"# Concatenate dummies\ntrain_data = pd.concat([train_data, categories_train, console_train, rating_train], axis = 1)\ntest_data = pd.concat([test_data, categories_test, console_test, rating_test], axis = 1)\n\n# Drop redundant columns\ntrain_data.drop(['ID', 'CATEGORY', 'RATING', 'CONSOLE'], axis = 1, inplace = True)\ntest_data.drop(['ID', 'CATEGORY', 'RATING', 'CONSOLE'], axis = 1, inplace = True)\ntrain_data.head()","9b094c90":"# Make (Add missing cols in test) cols equal\n\ntrain_cols = list(train_data.columns)\ntest_cols = list(test_data.columns)\n#print(cols)\nfor col in train_cols :\n    if col not in test_cols :\n        test_data[col] = 0\n\n\ntest_data = test_data[train_data.columns]  # Rearrange cols acc to train data\n#test_data.head()\ntest_data.drop(['SalesInMillions'], axis = 1, inplace = True)\n\n# fill null values in test_dataset which may be created by the respective 'mode'\nfor column in test_data.columns:\n    test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n#print(test_data.shape)","360e22da":"print(test_data.shape)\nprint(train_data.shape)\ntest_data.head()","4a8821f3":"temp_data = train_data\ny = temp_data['SalesInMillions']\nX = temp_data.drop(['SalesInMillions'], axis = 1)","e114f164":"# Shape Check\nprint('X: ',X.shape)\nprint('y: ', y.shape)\nprint(train_data.shape)\nprint(test_data.shape)","60179fb8":"# Creating train\/val\/test splits\nfrom sklearn.model_selection import train_test_split\n\n# Train test split\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=1)\n\n# validation split from test set\nX_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5, random_state=1) ","0d9cda29":"# Shape Check\nprint('X_train: ',X_train.shape, 'y_train: ', y_train.shape)\nprint('X_test: ',X_test.shape, 'y_test: ', y_test.shape)\nprint('X_val: ',X_val.shape, 'y_val: ', y_val.shape)","079acc84":"# Normalizing values\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_val = sc.transform(X_val)\n\ntest_data = sc.transform(test_data)","51c03f91":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nRegressors = [\n    LinearRegression(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    SVR(),\n    KNeighborsRegressor(),\n    XGBRegressor(),]\n\nlog_cols = [\"Regressor\", \"RMSE\"]\nlog \t = pd.DataFrame(columns=log_cols)\n\nacc_dict = {}\n\n\nfor clf in Regressors:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    train_predictions = clf.predict(X_test)\n    #acc = accuracy_score(y_test, train_predictions)\n    # rmse = np.sqrt(mean_squared_error(y_test, train_predictions))\n    rmse = cross_val_score(clf, X_train, y_train, scoring='neg_mean_squared_error', cv=3)\n    rmse = np.sqrt(-rmse)\n    rmse = rmse.mean()\n    if name in acc_dict:\n        acc_dict[name] += rmse\n    else:\n        acc_dict[name] = rmse\n\nfor clf in acc_dict:\n\t#acc_dict[clf] = acc_dict[clf] \/ 10.0\n\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n\tlog = log.append(log_entry)\n\nplt.figure(figsize=(15,8))\nplt.xlabel('RMSE')\nplt.title('Regressor RMSE')\n\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='RMSE', y='Regressor', data=log, color=\"b\")","43e9b04d":"# Get predictions from base models\nfrom sklearn.ensemble import RandomForestRegressor\n\nclassifiers = [\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    SVR(),\n    KNeighborsRegressor(),\n    XGBRegressor(),]\n\n\nX_test_preds = pd.DataFrame({})\nX_val_preds = pd.DataFrame({})\n\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    val_predictions = clf.predict(X_val)\n    X_val_preds[name] = val_predictions\n    \n    test_predictions = clf.predict(X_test)\n    X_test_preds[name] = test_predictions","f86524d8":"# Concatenating predictions with respective datasets\nX_test_preds = pd.concat([pd.DataFrame(X_test), X_test_preds], axis = 1)\nX_val_preds = pd.concat([pd.DataFrame(X_val), X_val_preds], axis = 1)\n\n# Normalizing values\nsc = StandardScaler()\nX_val_preds = sc.fit_transform(X_val_preds)\nX_test_preds = sc.transform(X_test_preds)\n\n# Fitting summerization(aggregator) and single(to compare with)\nsummer = RandomForestRegressor()\nsingle = RandomForestRegressor()\n\nsummer.fit(X_val_preds, y_val)\nsingle.fit(X_train, y_train)\n\n\nsummer_predictions = summer.predict(X_test_preds)\nsingle_predictions = single.predict(X_test)\n\nrmse_single = np.sqrt(mean_squared_error(y_test, single_predictions))\nrmse_summer = np.sqrt(mean_squared_error(y_test, summer_predictions))\n\nprint('RMSE Single: ',rmse_single, 'RMSE Combined: ',rmse_summer)","685c61df":"from sklearn.ensemble import RandomForestRegressor\n\nregressors = [\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    SVR(),\n    KNeighborsRegressor(),\n    XGBRegressor(),]\n\nX_train = np.concatenate((X_train, X_val))\ny_train = np.concatenate((y_train, y_val))\n\nfinal_X_test_predictions = pd.DataFrame({}) # for training aggregator\nfinal_test_predictions = pd.DataFrame({}) # for prediction by aggregator\n\nfor clf in regressors:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    \n    predictions = clf.predict(X_test)\n    final_X_test_predictions[name] = predictions\n    \n    test_predictions = clf.predict(test_data)\n    final_test_predictions[name] = test_predictions","1f036ec8":"# Concatenating predictions with respective datasets\nfinal_train = pd.concat([pd.DataFrame(X_test), final_X_test_predictions], axis = 1)\nfinal_test = pd.concat([pd.DataFrame(test_data), final_test_predictions], axis = 1)\n\n# Normalizing values\nsc = StandardScaler()\nfinal_train = sc.fit_transform(final_train)\nfinal_test = sc.transform(final_test)\n\n# Get aggregator predictions\nsummer = RandomForestRegressor()\nsummer.fit(final_train, y_test)\nsummer_predictions = summer.predict(final_test)\n","7d6d4d25":"# Save predictions to csv file\nsub = pd.DataFrame({'SalesInMillions': summer_predictions})\nsub.to_csv('submission.csv', index = False)","2b6a0be0":"**Conclusion** : Sales seems to contain an outlier with > 80 million sales. Remove the outliers","67972911":"# **Reading Data**","a4439686":"Converting year to number of years since it may represent age information","cd7d54df":"# **Testing Ensemble of Predictions**\nThe idea is to use base models to make predictions on test set and use these predictions as new features. Then use an aggregator model to predict values from new features\n- **Base Models** : Train on 'train' set and get predictions on 'val' and 'test' set\n- **Aggregator** : Train using 'val' predictions and get final predictions on 'test' set","a5463b96":"# **Data Cleaning**","5e4d7d6e":"# **Checking Feature Distribution**","c353fa2b":"### **To Do** :\n\n- Categorical : (console\/ year\/ category\/ publisher\/ rating(rank)\n    - no ofcategories ?\n    - fix missing with mode\n    - bin \/ one hot \n\n- Numerical : (year\/ critiv_points\/ user_points\/ sales)\n    - check distribution\n    - fix outliers\n    - fix missing with mean\/ median (match_dist)\n\n- Normalize\n- Base-Line","34fba3f5":"# **Further Work**\n- Pick out best features (**feature selection**)\n- Try using varoius **high capacity models** for base predictions\n- Use **different aggregator** model\n- **Tune hyperparameters** to obtain better base predictions","e8561746":"# **Dealing with categorical variables**\nThe PUBLISHER column contains large number of values(unique). Hence OneHotEncoding would lead to a sparse dataset. Instead here I tried to group various values together based on some intuition and domain knowledge (Numeral encoding is used to preserve relative importance). This made the dataset easy to work with","f183e53d":"**Conclusion** : We saw that using predictions as extra features helped us to further improve our results! Hence, we use this approach to make final predictions","3a5448d0":"# **Importing Libraries**","cf0fffc7":"# **Baseline Model Comparision**\nFit various base models and compare their performance","9a919a72":"- Train Data Shape : (3506, 9)\n- Test Data Shape  : (1503, 8)","d259d391":"### **To Do** :\n\n- Categorical : (CONSOLE\/ YEAR\/ CATEGORY\/ PUBLISHER\/ RATING\n    - no ofcategories ?\n    - bin \/ one hot \n\n- Numerical : (YEAR\/ CRITIC_POINTS\/ USER_POINTS\/ SALES)\n    - check distribution\n\n- Normalize\n- Base-Line","31a79f4a":"**Conclusion** : No missing values in train and test data","11be520d":"Here empty columns were added to test dataset since it did not contain all values present in train set. This step was needed inorder to make the shape of train and test set same to fit various model.","515cff42":"**Conclusion** : \n- RandomForestRegressor seems to perform the best (XGBoost and other high end models are still not used)\n- Linear Regression also seems to do a fair job indicating that the problem at hand may not be that complex","e01852a9":"**Conclusion** : Categorical data seems fairly balanced","d8c28889":"# **Final Predictions for test dataset**\nTo make final predictions using aggregator on the test dataset, testset dataframe should contain predictions by base models.\nHence get the following predictions by training base models on new train data (train + val) and predicting on new test (test):\n- **final_X_test_predictions** --> for training aggregator for test dataset\n- **final_test_predictions** --> to get final predictions of aggregator\n","705cb645":"Similar grouping method of variable values was followed which were then converted to One Hot Values to designate no specific heirarchy among values"}}