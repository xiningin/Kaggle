{"cell_type":{"a608efeb":"code","a86f719a":"code","ec2917c2":"code","17db1f77":"code","de23647e":"code","2abed7a6":"code","1e6f6c23":"code","6e732dd2":"code","dc698a7e":"code","948f2026":"code","05c4288c":"code","a81af848":"code","bc4cf5f8":"code","841bbe2e":"code","05574c29":"code","f1fb0c0e":"code","4fc18da1":"code","a6a66c43":"code","e82381bb":"code","ed39803c":"code","547cbb2c":"code","872aa3a1":"code","3e10166e":"code","80214b67":"code","f3dbebe0":"code","1ff31807":"code","f06d64b8":"code","588f24c9":"code","0269674b":"code","f7f18416":"code","f40890bc":"code","9c793766":"code","6b9ed925":"code","a8b5f7a9":"code","19d29ffd":"markdown","fdec622d":"markdown","773e6b54":"markdown","0d4b475a":"markdown","c0b3e512":"markdown","1788b733":"markdown","7a79d485":"markdown","9fdf3f7a":"markdown","50f857d8":"markdown","562a4d45":"markdown","47aa60e6":"markdown","c3f29cc4":"markdown","ac36e03a":"markdown","b13fc126":"markdown","a0c207ba":"markdown","34281b6d":"markdown","a11d1193":"markdown","4700d316":"markdown","5a91639b":"markdown","2a9f62ad":"markdown"},"source":{"a608efeb":"# importing the standard libraries for a Machine Learning Problem.\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt","a86f719a":"# Loading the test and training data and concatenating them in order to apply same changes to them and to miss any outliers.\n#train = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\New folder\\projects\\house price\\New folder\\train.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n#test = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\New folder\\projects\\house price\\New folder\\test.csv\")","ec2917c2":"df = pd.concat((train, test), axis = 0)\ndf\n","17db1f77":"# NAN values\ndf.info()\n","de23647e":"df.isna().sum(axis = 0)","2abed7a6":"df.drop(['Id'], axis=1, inplace = True)\ndf.drop(['Alley'], axis=1, inplace = True)\ndf.drop(['PoolQC'], axis=1, inplace = True)\ndf.drop(['MiscFeature'], axis=1, inplace = True)\ndf.drop(['Fence'], axis=1, inplace = True)","1e6f6c23":"# Initially df shape was: (2919 rows \u00d7 81 columns)\ndf.shape","6e732dd2":"sns.heatmap(df.isnull(), yticklabels = False, cbar = False)","dc698a7e":"# This is a list of columns which have categorical data and their null value cannot be replaced with the mean.\n\ncat_lis = ['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n        'SaleCondition','ExterCond',\n         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n         'CentralAir',\n         'Electrical','KitchenQual','Functional',\n         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive', 'YrSold']\n\nlis = df.columns.tolist()\n\nfor col in lis:\n    if col in cat_lis:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    elif col != 'SalePrice':\n        df[col] = df[col].fillna(df[col].mean())","948f2026":"# We create a heatmap to make sure we did not missed any column.\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False)","05c4288c":"def one_hot_encod(col):\n    df_final = df\n    i = 0\n    for field in col:\n        df1 = pd.get_dummies(df[field], drop_first = True)\n        df.drop([field], axis = 1, inplace = True)\n        \n        if i == 0:\n            df_final = df1.copy()\n        else:\n            df_final = pd.concat([df_final, df1], axis = 1)\n        i += 1\n    df_final = pd.concat([df_final, df], axis = 1)\n    return df_final\n\ndf_final = one_hot_encod(cat_lis)\ndf_final","a81af848":"df_final = df_final.loc[:, ~df_final.columns.duplicated()]\ndf_final","bc4cf5f8":"df_final.info()","841bbe2e":"df_final_train = df_final.iloc[:1460]\ndf_final_test = df_final[1460:]","05574c29":"# Basic Libraries that could be used in model fitting.\nfrom sklearn.model_selection import train_test_split as tts\nimport math\nfrom sklearn.metrics import mean_squared_error","f1fb0c0e":"train = df_final_train.copy()\ntest = df_final_test.copy()\ntrain","4fc18da1":"def assign_data(df):\n    X = df.copy()\n    #X = X.drop([\"Id\"], axis = 1)\n    X = X.drop([\"SalePrice\"], axis = 1)\n    return X\n\nX = assign_data(train)\nX_test = assign_data(test)\nY = train[\"SalePrice\"]","a6a66c43":"#VISUALIZING THE HISTOGRAM OF Y.\nY.hist()","e82381bb":"Y = np.log(Y)\nY.hist()","ed39803c":"# Splitting the train dataset into train and cross-validation dataset.\nX_train, X_cv, Y_train, Y_cv = tts(X, Y, test_size = 0.2, random_state = 0)","547cbb2c":"from sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Ridge\nlr1 = Ridge(normalize = True, random_state = 0)\n\nparam_grid1 = {'alpha': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100, 300, 1000]}\n\nmodel1 = GridSearchCV(estimator = lr1, param_grid = param_grid1, \n                          cv = 3, n_jobs = -1, verbose = 2)\nmodel1.fit(X_train, Y_train)","872aa3a1":"model1.best_params_","3e10166e":"# PREDICTING THE CROSS-VALIDATION DATASET\npred_train1 = model1.predict(X_train)\npred_cv1 = model1.predict(X_cv)","80214b67":"# VISUALIZING OUR PREDICTION AGAINST THE REAL VALUES OF CV DATASET\nplt.plot(pred_train1, Y_train, '.')\nplt.xlabel(\"pred_train\")\nplt.ylabel(\"Y_train\")\nplt.show()\n\nplt.plot(pred_cv1, Y_cv, '.')\nplt.xlabel(\"Pred_cv\")\nplt.ylabel(\"Y_cv\")\nplt.show()","f3dbebe0":"print(\"The RMSE error in predicting the training examples is:\",mean_squared_error(pred_train1, Y_train))\nprint(\"The RMSE error in predicting the cross-validation examples is:\",mean_squared_error(pred_cv1, Y_cv))","1ff31807":"from sklearn.ensemble import RandomForestRegressor as rfr\nmodel2 = rfr(random_state = 0)\nmodel2.fit(X_train, Y_train)\npred_train2 = model2.predict(X_train)\npred_cv2 = model2.predict(X_cv)\n\n# VISUALIZING OUR PREDICTION AGAINST THE REAL VALUES OF CV DATASET\nplt.plot(pred_train2, Y_train, '.')\nplt.xlabel(\"pred_train\")\nplt.ylabel(\"Y_train\")\nplt.show()\n\nplt.plot(pred_cv2, Y_cv, '.')\nplt.xlabel(\"Pred_cv\")\nplt.ylabel(\"Y_cv\")\nplt.show()","f06d64b8":"## Finding the RMSE error for our training and cv set for random forest regressor\n\nprint(\"The RMSE error in predicting the training examples is:\",mean_squared_error(pred_train2, Y_train))\nprint(\"The RMSE error in predicting the cross-validation examples is:\",mean_squared_error(pred_cv2, Y_cv))","588f24c9":"from sklearn.ensemble import GradientBoostingRegressor as gbr\nreg = gbr(random_state = 0)\n\n# PERFORMING THE HYPER-PARAMETER TUNING ON GBR MODEL\nparam_grid3 = {\n    'learning_rate': [0.03, 0.1, 0.3, 1.0, 10],\n    'max_depth': [1, 2, 3],\n    'n_estimators': [100, 200, 300]\n}\nmodel3 = GridSearchCV(estimator = reg, param_grid = param_grid3, \n                          cv = 3, n_jobs = -1, verbose = 2)\nmodel3.fit(X_train, Y_train)","0269674b":"model3.best_params_","f7f18416":"pred_train3 = model3.predict(X_train)\npred_cv3 = model3.predict(X_cv)\n\n# VISUALIZING OUR PREDICTION AGAINST THE REAL VALUES OF CV DATASET\nplt.plot(pred_train3, Y_train, '.')\nplt.xlabel(\"pred_train\")\nplt.ylabel(\"Y_train\")\nplt.show()\n\nplt.plot(pred_cv3, Y_cv, '.')\nplt.xlabel(\"Pred_cv\")\nplt.ylabel(\"Y_cv\")\nplt.show()","f40890bc":"## Finding the RMSE error for our training and cv set for GradientBoostingRegressor Model\n\nprint(\"The RMSE error in predicting the training examples is:\",mean_squared_error(pred_train3, Y_train))\nprint(\"The RMSE error in predicting the cross-validation examples is:\",mean_squared_error(pred_cv3, Y_cv))","9c793766":"#Ensemble Techniques\n#Average\n\ndef predict_avg(X):\n    pred_1 = model1.predict(X)\n    pred_2 = model2.predict(X)\n    pred_3 = model3.predict(X)\n    pred_cv = (pred_1+pred_2+pred_3)\/3\n    return pred_cv\n\npred_train = predict_avg(X_train)\npred_cv = predict_avg(X_cv)","6b9ed925":"# VISUALIZING OUR PREDICTION AGAINST THE REAL VALUES OF CV DATASET\nplt.plot(pred_train, Y_train, '.')\nplt.xlabel(\"pred_train\")\nplt.ylabel(\"Y_train\")\nplt.show()\n\nplt.plot(pred_cv, Y_cv, '.')\nplt.xlabel(\"Pred_cv\")\nplt.ylabel(\"Y_cv\")\nplt.show()","a8b5f7a9":"## Finding the RMSE error for our training and cv set for Ensembled Model\n\nprint(\"The RMSE error in predicting the training examples is:\",mean_squared_error(pred_train, Y_train))\nprint(\"The RMSE error in predicting the cross-validation examples is:\",mean_squared_error(pred_cv, Y_cv))","19d29ffd":"### Once we dealt with Null values, now we have to make sense out of the categorical data too. In this problem all the categorical data given to us the nominal categorical data, so we use one hot encoding or make dummy variables with binary digits as its value.","fdec622d":"## Ensembling the models","773e6b54":"## MODEL-1( RIDGE)","0d4b475a":"## In this model we want to add some Ensemble Techniques, which basically is a method to combine the results of multiple models together. We will use the Average method in this case of regression.\n\n### We will use Ridge Regressor, randomForest Regressor and BosstingGradientRegressor as our base models to ensemble.","c0b3e512":"### We have to make a list of all the columns which contain null values and replace the null values with the mode if data is categorical otherwise with the mean of that column.","1788b733":"### By Visualizing the Histogram we can infer that the Y is not normally distributed. So what we can do is take natural logarithmic of Y and see if it becomes somewhat gaussian distributed. It is a common technique in these kind of cases.","7a79d485":"# MODEL TRAINING AND FITTING","9fdf3f7a":"### We want to see how well our model predicted and whether or not it overfitted the trainng data. So we find the mean-squared error of our training as well as cv dataset.","50f857d8":"### There is no column left with the textual pattern values or values in form that cannot be used directly in our model.\n### So we again split the training and test datasets.","562a4d45":"### Combining the training and test datasets in order to perform all the preprocessing and data cleaning singlehandedly.","47aa60e6":"### Now we do basic data cleaning by dealing with missing values or NAN values, converting textual data into usable form and to handle categorical Data.","c3f29cc4":"### We need to make sure that no two columns are identical. So if there are any duplicate columns we drop them.","ac36e03a":"# House_Price_Regression_Problem\n## This is a regression problem which is like a mandatory problem for ML Beginners on kaggle. I had tried to explain as much as possible and work hard to make the code easy to read and understand.\n# IF YOU LIKE IT PLEASE UPVOTE!! ","b13fc126":"## MODEL-3( BoostingGradientRegressor)","a0c207ba":"### Here it is observed that few columns have much greater number of missing values than non null ones. They cannot help fit our model better on first go. So we basically drop them and if felt the need we would visit them again.\n### list = (Alley, PoolQC, Fence, MiscFeature) \n### These columns are gonna be dropped from our dataframe.","34281b6d":"### We can work with that as it kind of looks like a gaussian curve. Although we have not checked for normal distribution.","a11d1193":"### So Now there are only 76 columns left in df DataFrame.\n### Now we visualise heatmap for our null values.","4700d316":"## MODEL-2( RandomForestRegressor)","5a91639b":"# DATA PREPROCESSING","2a9f62ad":"## When we predicted our ensembled model on our test dataset and submitted it on kaggle it showed RMSE error of 0.13452.\n# If you like the ideology behind the code please UPVOTE it.\n## Thank You!!"}}