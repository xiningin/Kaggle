{"cell_type":{"1c18fb8b":"code","50568122":"code","12d1dd65":"code","601acfbe":"code","dde2048f":"code","f5b1900e":"code","176e2661":"code","6ce20d51":"code","38a74082":"code","6100b0d4":"code","05ac7f32":"code","34e7945d":"code","42746291":"code","436b2ca0":"code","abf59bbf":"code","6f00acdf":"code","71bd2ee8":"markdown","5431efc0":"markdown","7af36a60":"markdown","bd66d8c2":"markdown","e2a2e5b6":"markdown","978c445a":"markdown","80373add":"markdown","df158c7e":"markdown","d208cfe9":"markdown","c0514996":"markdown"},"source":{"1c18fb8b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\nfrom scipy.cluster.hierarchy import fcluster, linkage, dendrogram\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN","50568122":"path = '..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv'\ndata = pd.read_csv(path)\ndata.head()","12d1dd65":"data.describe()","601acfbe":"data.dtypes","dde2048f":"plt.figure(figsize = (15 , 6))\ncount = 0 \nfor feature in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    count += 1\n    plt.subplot(1,3,count)\n    plt.subplots_adjust(hspace=0.5 , wspace=0.5)\n    sns.distplot(data[feature], bins=20)\n    plt.title('Distribui\u00e7\u00e3o {}'.format(feature))\nplt.show()","f5b1900e":"plt.figure(figsize=(15,7))\ncount = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    for y in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n        count += 1\n        plt.subplot(3,3, count)\n        plt.subplots_adjust(hspace=0.5, wspace=0.5)\n        sns.regplot(x=x, y=y, data=data)\n        plt.ylabel(y.split()[0]+' '+y.split()[1] if len(y.split()) > 1 else y )\nplt.show()","176e2661":"# features \nX = data[['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)']]\n\n# One hot encoding \nencoding = OneHotEncoder(sparse=False)\nX['Gender'] = encoding.fit_transform(X[['Gender']])\n\n\n# StandardScaler \nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# K-Means \nmodel = KMeans(n_clusters=4, init='k-means++', max_iter=100, random_state=42)\nmodel.fit(X)\n\n\n# labels \nlabels = model.labels_\n\n\n# centroids \ncentroids = model.cluster_centers_\n\n\n# n\u00fameros de clusters\nprint('Numbers of cluster: ', model.n_clusters)\n\n\n# metrics \nprint('Silhoutte:', silhouette_score(X, labels))\nprint('Davies-Bouldin:', davies_bouldin_score(X, labels))","6ce20d51":"# set of features \nX1 = data[['Age' , 'Spending Score (1-100)']]\ninercia = []\nfor k in range(1 , 11):\n    kmeans = KMeans(n_clusters=k,\n                     init='k-means++',\n                     n_init=10,\n                     max_iter=100, \n                     random_state=42,\n                     algorithm='elkan')\n    kmeans.fit(X1)\n    inercia.append(kmeans.inertia_)","38a74082":"# Curve Inertia X Number of clusters\n\nplt.figure(figsize = (15 ,6))\nplt.plot(np.arange(1 , 11), inercia, 'o')\nplt.plot(np.arange(1 , 11) , inercia, '-', color='green',alpha = 0.5, lw=2)\nplt.title('Inercia Vs Clusters')\nplt.xlabel('Numbers of cluster')\nplt.ylabel('Inercia')\nplt.show()","6100b0d4":"# Defined cluster centroids\nkmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=42)\nkmeans.fit(X)\nlabels = kmeans.labels_\nprint('Silhoutte:', silhouette_score(X, labels))\nprint('Davies-Bouldin:', davies_bouldin_score(X, labels))","05ac7f32":"# Silhouete points \n\nfig, ax = plt.subplots(figsize=(10,5))\nplt.title('Silhouette coefficient', fontsize=15)\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick', ax=ax)\nvisualizer.fit(X)\nplt.tight_layout()","34e7945d":"# Elbow method\n\nfig, ax = plt.subplots(figsize=(10,5))\nplt.title('KElbow', fontsize=15)\nvisualizer = KElbowVisualizer(KMeans(), k=(1,11))\nvisualizer.fit(X)\nplt.tight_layout()","42746291":"# with scikit-learn  \nhc = AgglomerativeClustering(n_clusters=4, affinity='euclidean')\nhc.fit(X)\nlabels_hc = hc.labels_\n\n\n# metrics\nprint('Silhoutte:', silhouette_score(X, labels_hc))\nprint('Davies-Bouldin:', davies_bouldin_score(X, labels_hc))","436b2ca0":"# with scipy \ndistance_matrix = linkage(X, metric='euclidean')\n\n# dendogram \nfig, ax = plt.subplots(figsize=(14,7))\ndendograma = dendrogram(distance_matrix, ax=ax)\nplt.show()","abf59bbf":"hierarquico = fcluster(distance_matrix, 4, criterion='maxclust')\nprint('Silhouette: {} '.format(silhouette_score(X,hierarquico)))","6f00acdf":"# DBSCAN \ndbscan = DBSCAN(eps=1, min_samples=5, metric='euclidean', algorithm='auto')\ndbscan.fit(X)\nlabels_db = dbscan.labels_\n\n\nprint('Silhoutte:', silhouette_score(X, labels_db))\nprint('Davies-Bouldin:', davies_bouldin_score(X, labels_db))","71bd2ee8":"<br>\n\n\n##### <b> Elbow method <\/b> \n\nthe \u201celbow\u201d method to help data scientists select the optimal number of clusters by fitting the model with a range of values for K. If the line chart resembles an arm, then the \u201celbow\u201d (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. In the visualizer \u201celbow\u201d will be annotated with a dashed line.\n\n<br>","5431efc0":"<br>\nIn the Elbow method, we always choose to choose a low K value with a very reduced inertia, as the best K value will be K = number of instances, so finding a low K value with a low inertia is the goal central.\n\n<br>\n<hr>","7af36a60":"### Hierarchical Cluster\n<br>\n\nAnother method of clustering unsupervised learning.\nIn general, merges and divisions are determined in a greedy way. The results of the hierarchical grouping are usually presented in a dendrogram.\n\n\n<br>\n\n\n<p align=center>\n<img src=\"https:\/\/miro.medium.com\/max\/1254\/1*KpYv1mhEaJbbafiC5udHPA.png\" width=\"50%\"><\/p>\n\n\n<br>","bd66d8c2":"<br>\n<hr>\n<br>\n<br>\n\n\n### <b>K-Means<\/b> \n\n\n<br>\n\nThe Kmeans algorithm is an iterative algorithm that attempts to partition the data set into K predefined distinct and non-overlapping subgroups (clusters), where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible, in addition to keeping the clusters as different as possible. It assigns data points to a cluster so that the sum of the squared distance between the data points and the cluster centroid (arithmetic mean of all data points that belong to that cluster) is minimal. The smaller the variation we have in the clusters, the more homogeneous (similar) the data points are in the same cluster.\n\n\n<br>\n\n\nThe objetive function is: \n<br>\n\n<p align=center>\n<img src=\"https:\/\/miro.medium.com\/max\/548\/1*myXqNCTZH80uvO2QyU6F5Q.png\" width=\"50%\"><\/p>\n\n\n\n\n\n\n<hr>\n<br>","e2a2e5b6":"### <b>Clustering Guide<\/b>\n<br>\n\n\n\n> <center><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ea\/K-means_convergence.gif\"><\/center>\n\n\n\n##### <b> What is Clustering? <\/b>\nClustering is the task of dividing the population or data points into several groups, so that the data points in the same groups are more similar to other data points in the same group than those of other groups. In simple words, the goal is to segregate groups with similar traits and assign them to clusters.\n\n\n\nCluster analysis can be done based on the resources in which we try to find subgroups of samples based on resources or based on samples in which we try to find subgroups of resources based on samples. We will address resource-based clustering here. Clustering is used in market segmentation; where we try to fine customers who are similar to each other, whether in terms of behaviors or attributes, segmentation \/ compression of images; where we try to group similar regions, group documents based on topics, etc.\n\n<br>\n\n\n\n<b> Context <\/b>\nThis data set is created only for the purpose of learning the concepts of customer segmentation, also known as market basket analysis. I will demonstrate this using the unsupervised ML technique (KMeans clustering algorithm) in the simplest way.\n\n\n<br>\n\n<b> Problem statement <\/b>\n  The mall owner wants to understand customers as someone who can easily converge [target customers] so that it is possible to make sense of the marketing team and plan the strategy accordingly.\n\n\n\n<b> Inspiration <\/b>\nAt the end of this case study, you can answer the questions below.\n* 1- How to achieve customer segmentation using the machine learning algorithm (KMeans Clustering) in Python in the simplest way.\n* 2- Who are your target customers with whom you can start the marketing strategy [easy to talk to]\n* 3- How the marketing strategy works in the real world\n\n<br>\n<hr>\n<br>\n\n\n\n<p align=center>\n<img src=\"https:\/\/miro.medium.com\/max\/1280\/1*5UHmgCaTD8EegsPuKcxC1Q.png\" width=\"50%\"><\/p>\n\n\n\n\n\n<br>\n<hr>","978c445a":"<br>\n<br>\n\n\n### Inertia X Number of clusters\n\nThe trade-off graph of inertia in relation to the number of clusters is a technique of identifying the best possible K value, which makes the cluster compact and with a smaller point dispersion.\n\n<br>","80373add":"<br>\n<hr>","df158c7e":"<br>\n<hr>\n<br>\n\n\n### DBSCAN \n<br>\n\n\nDBSCAN - Spatial clustering of applications based on density. Finds high density main samples and expands clusters from them. Good for data containing clusters of similar density.\n\n##### <b> How DBSCAN works <\/b>\n\nConsider a set of points in some space to be grouped. Let \u03b5 be a parameter that specifies the radius of a neighborhood in relation to some point. For DBSCAN agglomeration purposes, points are classified as central points, (density -) attainable points and extreme values, as follows:\n\n\n<br>\n\n<p align=center>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/af\/DBSCAN-Illustration.svg\/400px-DBSCAN-Illustration.svg.png\" width=\"50%\"><\/p>\n\n<br>\n\n\nA point p is a central point if at least points minPts are at a distance \u03b5 from it (including p).\nA point q is directly accessible from p if point q is at a distance \u03b5 from the central point p. The points are said to be reachable only directly from the main points.\nA point q is accessible from p, if there is a path p 1, ..., p n with p 1 = p and p n = Q, where each p i 1 is directly accessible from p i. Note that this implies that the starting point and all points on the path must be main points, with the possible exception of q.\nAll points inaccessible from any other point are outliers or noise.\nNow, if p is a central point, it forms a cluster along with all the points (main or non-central) that are accessible from it. Each cluster contains at least one main point; non-essential points may be part of a cluster, but they form their \"edge\" as they cannot be used to reach more points.\n\n<br>\n<br>\n\n##### <b> Steps of the algorithm <\/b>\n\nThe DBSCAN algorithm can be abstracted in the following steps:\n\n  1. Find the points in the neighborhood \u03b5 (eps) of each point and identify the main points with more than neighboring minPts.\n  2. Find the connected components of the main points on the neighboring graph, ignoring all non-essential points.\n  3.Assign each non-central point to a nearby cluster if the cluster is a neighbor \u03b5 (eps), otherwise, assign it to noise.\nA naive implementation of this requires storing neighborhoods in step 1, thus requiring substantial memory.\n\n\n\n\n\n\n\n<br>","d208cfe9":"### Data visualization \n\n\n<br>\n<hr>","c0514996":"<br>\n\n\n\n#### <b> I hope this notebook helps in studies! Thank you <\/b> "}}