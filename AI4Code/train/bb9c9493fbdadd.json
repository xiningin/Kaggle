{"cell_type":{"323a4f40":"code","548af7e1":"code","5c1c68ca":"code","aed77b1a":"code","e0b614d9":"code","2ad483dc":"code","8176075c":"code","bf16277e":"code","8641320c":"code","d7e87599":"code","fafd6c8b":"code","be4e7f79":"code","477b5e43":"code","a38f2700":"code","7458386c":"code","95be6ed9":"code","b194be3a":"code","bf3f700b":"markdown","fddf8708":"markdown","820fc13c":"markdown","031d3ed5":"markdown","10ac856c":"markdown","5fa383e9":"markdown","97a17ba5":"markdown","4b2192d8":"markdown","6a71c45b":"markdown","d9ef905a":"markdown","ac13f122":"markdown","61407be8":"markdown","65dec033":"markdown","a4bea7f7":"markdown","e2e80887":"markdown","9c242952":"markdown","e6fb4396":"markdown"},"source":{"323a4f40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","548af7e1":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np","5c1c68ca":"df=pd.read_csv(\"..\/input\/KNN_Data\")\ndf.head()","aed77b1a":"sns.pairplot(df,hue=\"TARGET CLASS\")\n#here we will vizualize all the mutual relations between column and the hue value will be the target column","e0b614d9":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler() # here we create an object if Standart Scaler","2ad483dc":"scaler.fit(df.drop(\"TARGET CLASS\",axis=1))\n#We will only fit the scaler for the features not for the target column, so we just drop it","8176075c":"scaled_features=scaler.transform(df.drop(\"TARGET CLASS\",axis=1))\n# here we standartize our features in our dataset apart from the target column\n# the .transform() method to transform the features to a scaled version\nscaled_features","bf16277e":"df_features=pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_features.head() #the features are now a pandas dataframe","8641320c":"#We will split our data before training the algorithm:\nX=df_features\ny=df[\"TARGET CLASS\"]\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=101)","d7e87599":"#Here we call the algorithm we will use and make the algorithm fit with our training dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1) \n# we select 1 as our k value, we will start with one and change it after evaluation ig it fails to predict the target variable\nknn.fit(X_train,y_train)  # the algorithm fits with our data","fafd6c8b":"predictions=knn.predict(X_test)\npredictions","be4e7f79":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,predictions))","477b5e43":"print(classification_report(y_test,predictions))","a38f2700":"error_rate=list() #here we create a list in order to keep record of every k value we will use in the for loop\nfor i in range(1,100):\n    knn=KNeighborsClassifier(n_neighbors=i) \n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test)) \n# here we add the mean of predictions of each k value which does not match with the target variable ","7458386c":"plt.figure(figsize=(20,15))\nplt.plot(range(1,100),error_rate,color=\"blue\",linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate and K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel=\"Error Rate\"","95be6ed9":"#Here we select k=39 which provides one of lowest error rate as our key value and retrain the algorithm\nknn=KNeighborsClassifier(n_neighbors=39)\nknn.fit(X_train,y_train)\npredictions=knn.predict(X_test)\npredictions","b194be3a":"print(confusion_matrix(y_test,predictions))\nprint(\"\\n\")\nprint(classification_report(y_test,predictions))","bf3f700b":"*Now we will plot the prediction error rates of different k values","fddf8708":"Importing Necassary Libraries and the Data:","820fc13c":"4. The Evalution of the Performance of the Model:","031d3ed5":"3. Splitting the Data and Training the Algorithm:","10ac856c":"We have to convert the scaled features of the data into pandas dataframe ","5fa383e9":"* Now all of the features has been standartized and is ready to be put into machine learning algorithm","97a17ba5":"First of all we have to standartize all of the columns in our dataset because the scale of the observation does matter when we use KNearest Neighbors Algorithm.\nAny column with large scale will make a larger effect than others","4b2192d8":"Those are the predictions of our algorithm, but we have to compare it with the target data","6a71c45b":"\nIt seems there is good separation in terms of the two classes of the target class in each feature columns K Nearest Neighbors Algorithm can give us good results to predict the target variable","d9ef905a":"* Now our new k value, the algorithm has % 84 accuracy and the confusion matrix results are also far better than before","ac13f122":"Now we will reevaluate our model with our ideal k value","61407be8":"*The precision and accuracy precentages are around %72, it is not good with k=1\n\n*We need to change k value\n\n*Trying different k value for each time is very time consuming, so we will use a for loop in order to get different results ","65dec033":"2. Standartization of the Variables:","a4bea7f7":"1. Explanatory Data Analysis:","e2e80887":"According to the figure above, k values between 27 and 55 give lower error rate, we will select one of the lowest error rate","9c242952":"*Here we get a lot of unidentified columns with a target class which is classified as 0 and 1\n\n*Therefore we need to explore data","e6fb4396":"\nThe value in confusion matrix above means:\n#TP=109 : true positive\n#FN=43 : false negative\n#FP=41 :false positive \n#TN=107 : true negative\n#The errors are not too high and absorable"}}