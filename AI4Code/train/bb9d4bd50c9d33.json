{"cell_type":{"70e26641":"code","f5eb21a1":"code","3a2026d6":"code","fd0a83d6":"code","50444e5c":"code","c37d5fda":"code","37545645":"code","11951b6f":"code","770fb31d":"code","423e4d42":"code","9252272a":"code","9919687b":"code","a872063e":"code","f54ecda2":"code","450216fa":"code","b6c0bebe":"code","e76260ae":"code","92f403f8":"code","877d820f":"code","9f344941":"code","09510f64":"code","89e920e7":"code","b73515a9":"code","6f8cc11c":"code","c5920dda":"code","d55785c5":"code","ea463344":"code","eb1b34f9":"code","c368a6ac":"code","c2e413c4":"code","891af546":"code","5b802ef2":"code","53b119f4":"code","c516c4ce":"code","a1bba3b9":"code","db66276a":"code","01f1ef80":"code","f28319fb":"code","dd1ed56b":"code","1c214d1a":"code","53934f28":"code","1e4f3822":"code","c78f3091":"code","9d96f91b":"code","bc362696":"code","3149f105":"code","b0ac6061":"code","6d0a304d":"code","006777db":"code","cb31583c":"code","e259c4ca":"code","6edd068a":"code","7b4515c7":"code","30599d3a":"code","8760b8eb":"code","b2475052":"code","089fd0eb":"code","0e2cdb34":"code","db834508":"code","b76522df":"code","2fd2edee":"code","d17af504":"code","c5f712c4":"code","538b02ae":"code","108e63d6":"code","4ed980e0":"code","390da8bc":"code","f1fc2326":"code","8ddec49f":"markdown","a3b26fd7":"markdown","2c96c0e1":"markdown","158ffa75":"markdown","e5f0b4bc":"markdown","066fde5f":"markdown","a42a242a":"markdown","a01f37d3":"markdown","52a873f0":"markdown","1d4753f4":"markdown","abcb6111":"markdown"},"source":{"70e26641":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nos.listdir('..\/input\/histopathologic-cancer-detection')\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f5eb21a1":"path2csv='..\/input\/histopathologic-cancer-detection\/train_labels.csv'\nlabels_df=pd.read_csv(path2csv)","3a2026d6":"labels_df.head(5)","fd0a83d6":"print(labels_df['label'].value_counts())","50444e5c":"%matplotlib inline\nlabels_df['label'].hist();","c37d5fda":"import matplotlib.pylab as plt\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport os\n%matplotlib inline","37545645":"# get ids for malignant images\nmalignantIds = labels_df.loc[labels_df['label']==1]['id'].values","11951b6f":"path2train=\"..\/input\/histopathologic-cancer-detection\/train\"","770fb31d":"color=False","423e4d42":"plt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nnrows,ncols=3,3","9252272a":"for i,id_ in enumerate(malignantIds[:nrows*ncols]):\n    full_filenames = os.path.join(path2train , id_ +'.tif')\n    # load image\n    img = Image.open(full_filenames)\n    # draw a 32*32 rectangle\n    draw = ImageDraw.Draw(img)\n    draw.rectangle(((32, 32), (64, 64)),outline=\"green\")\n    plt.subplot(nrows, ncols, i+1)\n    if color is True:\n        plt.imshow(np.array(img))\n    else:\n        plt.imshow(np.array(img)[:,:,0],cmap=\"gray\")\n    plt.axis('off')","9919687b":"print(\"image shape:\", np.array(img).shape)\nprint(\"pixel values range from %s to %s\" %(np.min(img),\nnp.max(img)))","a872063e":"from PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchvision.transforms as transforms\nimport os","f54ecda2":"# fix torch random seed\ntorch.manual_seed(0)","450216fa":"class histoCancerDataset(Dataset):\n    def __init__(self, data_dir, transform,data_type=\"train\"):\n        # path to images\n        path2data = os.path.join(data_dir,data_type)\n        # get a list of images\n        filenames = os.listdir(path2data)\n        # get the full path to images\n        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n        # labels are in a csv file named train_labels.csv\n        csv_filename=data_type+\"_labels.csv\"\n        path2csvLabels=os.path.join(data_dir,csv_filename)\n        labels_df=pd.read_csv(path2csvLabels)\n        # set data frame index to id\n        labels_df.set_index(\"id\", inplace=True)\n        # obtain labels from data frame\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n        self.transform = transform\n        \n        \n    def __len__(self):\n        # return size of dataset\n        return len(self.full_filenames)\n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx]) # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]","b6c0bebe":"import torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor()])","e76260ae":"data_dir = \"..\/input\/histopathologic-cancer-detection\"\nhisto_dataset = histoCancerDataset(data_dir, data_transformer,\"train\")\nprint(len(histo_dataset))","92f403f8":"# load an image\nimg,label=histo_dataset[256]\nprint(img.shape,torch.min(img),torch.max(img))","877d820f":"from torch.utils.data import random_split\nlen_histo=len(histo_dataset)\nlen_train=int(0.8*len_histo)\nlen_val=len_histo-len_train\ntrain_ds,val_ds=random_split(histo_dataset,[len_train,len_val])\nprint(\"train dataset length:\", len(train_ds))\nprint(\"validation dataset length:\", len(val_ds))\n","9f344941":"for x,y in train_ds:\n    print(x.shape,y)\n    break\nfor x,y in val_ds:\n    print(x.shape,y)\n    break\n","09510f64":"#Import the required packages:\nfrom torchvision import utils\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nnp.random.seed(0)","89e920e7":"#helper function\ndef show(img,y,color=True):\n    # convert tensor to numpy array\n    npimg = img.numpy()\n    # Convert to H*W*C shape\n    npimg_tr=np.transpose(npimg, (1,2,0))\n    if color==False:\n        npimg_tr=npimg_tr[:,:,0]\n        plt.imshow(npimg_tr,interpolation='nearest',cmap=\"gray\")\n    else:\n        # display images\n        plt.imshow(npimg_tr,interpolation='nearest')\n    plt.title(\"label: \"+str(y))","b73515a9":"#making grid \ngrid_size=4\nrnd_inds=np.random.randint(0,len(train_ds),grid_size)\nprint(\"image indices:\",rnd_inds)\nx_grid_train=[train_ds[i][0] for i in rnd_inds]\ny_grid_train=[train_ds[i][1] for i in rnd_inds]\nx_grid_train=utils.make_grid(x_grid_train, nrow=4, padding=2)\nprint(x_grid_train.shape)\nplt.rcParams['figure.figsize'] = (10.0, 5)\nshow(x_grid_train,y_grid_train)\n","6f8cc11c":"train_transformer = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(45),\n    transforms.RandomResizedCrop(96,scale=(0.8,1.0),ratio=(1.0,1.0)),\n    transforms.ToTensor()])\n#we do not distort validation dataset except for convetring it to tensor\nval_transformer = transforms.Compose([transforms.ToTensor()])","c5920dda":"#overwrite the transform functions\ntrain_ds.transform=train_transformer\nval_ds.transform=val_transformer","d55785c5":"from torch.utils.data import DataLoader\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=64, shuffle=False)","ea463344":"# extract a batch from training data\nfor x, y in train_dl:\n    print(x.shape)\n    print(y.shape)\n    break\nfor x, y in val_dl:\n    print(x.shape)\n    print(y.shape)\n    break","eb1b34f9":"# get labels for validation dataset\ny_val=[y for _,y in val_ds]","c368a6ac":"def accuracy(labels, out):\n    return np.sum(out==labels)\/float(len(labels))","c2e413c4":"# accuracy all zero predictions\nacc_all_zeros=accuracy(y_val,np.zeros_like(y_val))\nprint(\"accuracy all zero prediction: %.2f\" %acc_all_zeros)\n\n","891af546":"# accuracy all ones predictions\nacc_all_ones=accuracy(y_val,np.ones_like(y_val))\nprint(\"accuracy all one prediction: %.2f\" %acc_all_ones)","5b802ef2":"# accuracy random predictions\nacc_random=accuracy(y_val,np.random.randint(2,size=len(y_val)))\nprint(\"accuracy random prediction: %.2f\" %acc_random)","53b119f4":"#now the full cnn model\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\n","c516c4ce":"#helper function to give output size\ndef findConv2dOutShape(H_in,W_in,conv,pool=2):\n    # get conv arguments\n    kernel_size=conv.kernel_size\n    stride=conv.stride\n    padding=conv.padding\n    dilation=conv.dilation\n    H_out=np.floor((H_in+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)\/stride[0]+1)\n    W_out=np.floor((W_in+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)\/stride[1]+1)\n    if pool:\n        H_out\/=pool\n        W_out\/=pool\n    return int(H_out),int(W_out)","a1bba3b9":"class Net(nn.Module):\n    def __init__(self, params):\n        super(Net, self).__init__()\n        C_in,H_in,W_in=params[\"input_shape\"]\n        init_f=params[\"initial_filters\"]\n        num_fc1=params[\"num_fc1\"]\n        num_classes=params[\"num_classes\"]\n        self.dropout_rate=params[\"dropout_rate\"]\n        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3)\n        h,w=findConv2dOutShape(H_in,W_in,self.conv1)\n        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv2)\n        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv3)\n        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n        h,w=findConv2dOutShape(h,w,self.conv4)\n        # compute the flatten size\n        self.num_flatten=h*w*8*init_f\n        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n        self.fc2 = nn.Linear(num_fc1, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv4(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, self.num_flatten)\n        x = F.relu(self.fc1(x))\n        x=F.dropout(x, self.dropout_rate, training= self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n    ","db66276a":"# dict to define model parameters\nparams_model={\n    \"input_shape\": (3,96,96),\n    \"initial_filters\": 8,\n    \"num_fc1\": 100,\n    \"dropout_rate\": 0.25,\n    \"num_classes\": 2,\n    }","01f1ef80":"# create model\ncnn_model = Net(params_model)","f28319fb":"if torch.cuda.is_available():\n    device=torch.device(\"cuda\")\n    cnn_mdel = cnn_model.to(device) ","dd1ed56b":"print(cnn_model)","1c214d1a":"pip install torchsummary","53934f28":"from torchsummary import summary\nsummary(cnn_model, input_size=(3, 96, 96),device=device.type)","1e4f3822":"loss_func = nn.NLLLoss(reduction=\"sum\")","c78f3091":"from torch import optim\nopt = optim.Adam(cnn_model.parameters(), lr=3e-4)","9d96f91b":"# get learning rate\ndef get_lr(opt):\n    for param_group in opt.param_groups:\n        return param_group['lr']\ncurrent_lr=get_lr(opt)\nprint('current lr={}'.format(current_lr))","bc362696":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n# define learning rate scheduler\nlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5,\npatience=20,verbose=1)\nfor i in range(100):\n    lr_scheduler.step(1)","3149f105":"def metrics_batch(output, target):\n    # get output class\n    pred = output.argmax(dim=1, keepdim=True)\n    # compare output class with target class\n    corrects=pred.eq(target.view_as(pred)).sum().item()\n    return corrects","b0ac6061":"#loss value per batch\ndef loss_batch(loss_func, output, target, opt=None):\n    loss = loss_func(output, target)\n    with torch.no_grad():\n        metric_b = metrics_batch(output,target)\n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    return loss.item(), metric_b","6d0a304d":"#loss per epoch\ndef loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\n    running_loss=0.0\n    running_metric=0.0\n    len_data=len(dataset_dl.dataset)\n    \n    for xb, yb in dataset_dl:\n        # move batch to device\n        xb=xb.to(device)\n        yb=yb.to(device)\n        # get model output\n        output=model(xb)\n        # get loss per batch\n        loss_b,metric_b=loss_batch(loss_func, output, yb, opt)\n        \n        # update running loss\n        running_loss+=loss_b\n        # update running metric\n        if metric_b is not None:\n            running_metric+=metric_b\n        # break the loop in case of sanity check\n        if sanity_check is True:\n            break\n            \n    # average loss value\n    loss=running_loss\/float(len_data)\n    # average metric value\n    metric=running_metric\/float(len_data)\n    return loss, metric","006777db":"def train_val(model, params):\n    # extract model parameters\n    num_epochs=params[\"num_epochs\"]\n    loss_func=params[\"loss_func\"]\n    opt=params[\"optimizer\"]\n    train_dl=params[\"train_dl\"]\n    val_dl=params[\"val_dl\"]\n    sanity_check=params[\"sanity_check\"]\n    lr_scheduler=params[\"lr_scheduler\"]\n    path2weights=params[\"path2weights\"]\n    \n    \n    # history of loss values in each epoch\n    loss_history={\n    \"train\": [],\n    \"val\": [],\n        }\n    # history of metric values in each epoch\n    metric_history={\n    \"train\": [],\n    \"val\": [],\n        }\n    # a deep copy of weights for the best performing model\n    best_model_wts = copy.deepcopy(model.state_dict())\n    \n    # initialize best loss to a large value\n    best_loss=float('inf')\n    \n    #we will define a loop that will calculate the training loss over an epoch:\n    # main loop\n    for epoch in range(num_epochs):\n        # get current learning rate\n        current_lr=get_lr(opt)\n        print('Epoch {}\/{}, current lr={}'.format(epoch, num_epochs- 1, current_lr))\n        # train model on training dataset\n        model.train()\n        train_loss,train_metric=loss_epoch(model,loss_func,train_dl,sanity_check,opt)\n        # collect loss and metric for training dataset\n        loss_history[\"train\"].append(train_loss)\n        metric_history[\"train\"].append(train_metric)\n        \n        # evaluate model on validation dataset\n        model.eval()\n        with torch.no_grad():\n            val_loss,val_metric=loss_epoch(model,loss_func,val_dl,sanity_check)\n    \n        # collect loss and metric for validation dataset\n        loss_history[\"val\"].append(val_loss)\n        metric_history[\"val\"].append(val_metric)\n    \n    \n        # store best model\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            # store weights into a local file\n            torch.save(model.state_dict(), path2weights)\n            print(\"Copied best model weights!\")\n        # learning rate schedule\n        lr_scheduler.step(val_loss)\n        if current_lr != get_lr(opt):\n            print(\"Loading best model weights!\")\n            model.load_state_dict(best_model_wts)\n        \n            \n        print(\"train loss: %.6f, dev loss: %.6f, accuracy: %.2f\"%(train_loss,val_loss,100*val_metric))\n        print(\"-\"*10)\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, loss_history, metric_history\n        ","cb31583c":"import copy\nloss_func = nn.NLLLoss(reduction=\"sum\")\nopt = optim.Adam(cnn_model.parameters(), lr=3e-4)\nlr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5,\npatience=20,verbose=1)","e259c4ca":"params_train = {\n    \"num_epochs\": 100,\n    \"optimizer\": opt,\n    \"loss_func\": loss_func,\n    \"train_dl\": train_dl,\n    \"val_dl\": val_dl,\n    \"sanity_check\": True,\n    \"lr_scheduler\": lr_scheduler,\n    \"path2weights\": \"weights.pt\",\n    \n} ","6edd068a":"# train and validate the model\ncnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)","7b4515c7":"# Train-Validation Progress\nnum_epochs=params_train[\"num_epochs\"]\n# plot loss progress\nplt.title(\"Train-Val Loss\")\nplt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\nplt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Training Epochs\")\nplt.legend()\nplt.show()\n# plot accuracy progress\nplt.title(\"Train-Val Accuracy\")\nplt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\nplt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Training Epochs\")\nplt.legend()\nplt.grid()\nplt.show()","30599d3a":"#First, we'll create an object of the Net class and load the stored weights into the model\n# model parameters\nparams_model={\n    \"input_shape\": (3,96,96),\n    \"initial_filters\": 8,\n    \"num_fc1\": 100,\n    \"dropout_rate\": 0.25,\n    \"num_classes\": 2,\n}","8760b8eb":"# initialize model\ncnn_model = Net(params_model)","b2475052":"# load state_dict into model\n# load state_dict into model\npath2weights=\"weights.pt\"\ncnn_model.load_state_dict(torch.load(path2weights))","089fd0eb":"cnn_model.eval()","0e2cdb34":"# move model to cuda\/gpu device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    cnn_model=cnn_model.to(device)","db834508":"import time\ndef deploy_model(model,dataset,device,num_classes=2,sanity_check=False):\n    len_data=len(dataset)\n    # initialize output tensor on CPU: due to GPU memory limits\n    y_out=torch.zeros(len_data,num_classes)\n    # initialize ground truth on CPU: due to GPU memory limits\n    y_gt=np.zeros((len_data),dtype=\"uint8\")\n    # move model to device\n    model=model.to(device)\n    elapsed_times=[]\n    with torch.no_grad():\n        for i in range(len_data):\n            x,y=dataset[i]\n            y_gt[i]=y\n            start=time.time()\n            y_out[i]=model(x.unsqueeze(0).to(device))\n            elapsed=time.time()-start\n            elapsed_times.append(elapsed)\n            if sanity_check is True:\n                break\n    inference_time=np.mean(elapsed_times)*1000\n    print(\"average inference time per image on %s: %.2f ms \"%(device,inference_time))\n    return y_out.numpy(),y_gt\n            ","b76522df":"# deploy model\ny_out,y_gt=deploy_model(cnn_model,val_ds,device=device,sanity_check=False)\nprint(y_out.shape,y_gt.shape)","2fd2edee":"from sklearn.metrics import accuracy_score\n# get predictions\ny_pred = np.argmax(y_out,axis=1)\nprint(y_pred.shape,y_gt.shape)\n# compute accuracy\nacc=accuracy_score(y_pred,y_gt)\nprint(\"accuracy: %.2f\" %acc)","d17af504":"path2csv=\".\/..\/input\/histopathologic-cancer-detection\/sample_submission.csv\"\nlabels_df=pd.read_csv(path2csv)\nlabels_df.head()","c5f712c4":"class histoCancerDataset_test(Dataset):\n    def __init__(self, data_dir, transform,data_type=\"train\"):\n        # path to images\n        path2data = os.path.join(data_dir,data_type)\n        # get a list of images\n        filenames = os.listdir(path2data)\n        # get the full path to images\n        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n        # labels are in a csv file named train_labels.csv\n        csv_filename=\"sample_submission.csv\"\n        path2csvLabels=os.path.join(data_dir,csv_filename)\n        labels_df=pd.read_csv(path2csvLabels)\n        # set data frame index to id\n        labels_df.set_index(\"id\", inplace=True)\n        # obtain labels from data frame\n        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n        self.transform = transform\n        \n        \n    def __len__(self):\n        # return size of dataset\n        return len(self.full_filenames)\n    def __getitem__(self, idx):\n        # open image, apply transforms and return with label\n        image = Image.open(self.full_filenames[idx]) # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]","538b02ae":"histo_test = histoCancerDataset_test(data_dir,val_transformer,data_type=\"test\")\nprint(len(histo_test))\n","108e63d6":"y_test_out,_=deploy_model(cnn_model,histo_test, device,sanity_check=False)\n\n\n","4ed980e0":"y_test_pred=np.argmax(y_test_out,axis=1)\nprint(y_test_pred.shape)","390da8bc":"grid_size=4\nrnd_inds=np.random.randint(0,len(histo_test),grid_size)\nprint(\"image indices:\",rnd_inds)\nx_grid_test=[histo_test[i][0] for i in range(grid_size)]\ny_grid_test=[y_test_pred[i] for i in range(grid_size)]\nx_grid_test=utils.make_grid(x_grid_test, nrow=4, padding=2)\nprint(x_grid_test.shape)\nplt.rcParams['figure.figsize'] = (10.0, 5)\nshow(x_grid_test,y_grid_test)\n","f1fc2326":"print(y_test_out.shape)\ncancer_preds = np.exp(y_test_out[:, 1])\nprint(cancer_preds.shape)","8ddec49f":"Next, we will load an image using the custom dataset:","a3b26fd7":"We will define a class for the custom dataset, define the transformation function, and then load an image from the dataset using the Dataset class. ","2c96c0e1":" creating dataloaders","158ffa75":"Building the CLASSIFICATION MODEL","e5f0b4bc":"First, let's create dumb baselines for the validation dataset.","066fde5f":"# **Deploying the model**","a42a242a":"We need to provide a validation dataset to track the model's performance during training. We use 20% of histo_dataset as the validation dataset and use the rest as the training dataset.","a01f37d3":"# **Training and evaluation of the model**","52a873f0":"Let's display a few samples from train_ds.","1d4753f4":"# **Applying on ths test set**","abcb6111":"Augmentation of data to trainset and other transformation "}}