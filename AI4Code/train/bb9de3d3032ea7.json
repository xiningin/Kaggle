{"cell_type":{"6b4c51f7":"code","19087582":"code","e76f1123":"code","dce584e6":"code","fd024cd4":"code","6cb8ff66":"code","8ea4f42a":"code","b9be08ac":"code","94aa5eb6":"code","0fb4c6e1":"code","8f15cc6b":"code","165d93a5":"code","1df98a87":"code","114eeaed":"code","63a43793":"code","fb5b4033":"code","0ad6be39":"code","b643f4c3":"code","71c6babd":"code","c3b8457b":"code","84caf122":"code","53ebd485":"code","19535169":"code","ba9b5356":"code","dd97bfd9":"code","5fcd5067":"code","fdac8b10":"code","75919f81":"code","40d828da":"code","28ae6612":"code","2bd3bb2f":"code","70d6eaeb":"code","c9b77fe1":"code","23fd736e":"code","533a4bcb":"code","20c553f9":"code","0c24206e":"code","03ae0836":"code","fc3949b0":"code","1ed94f55":"code","0aca69ef":"code","f773967f":"code","d6c82fb5":"code","5d96cf33":"code","d4263c49":"code","97e9581e":"code","3b187885":"code","987130a3":"code","8f1d49fe":"code","b068d1f5":"code","198da330":"code","dcc8a45f":"code","cb8dc05b":"code","7fca591f":"markdown","cd1bd139":"markdown","d5ed8525":"markdown","d8580b28":"markdown","4fb1894f":"markdown","aa073ef4":"markdown","50a6b32d":"markdown","611177b2":"markdown","69442186":"markdown","8e1d4201":"markdown","6872c5e3":"markdown","b166f070":"markdown","51d42058":"markdown","7ab34f5f":"markdown","7c378c6a":"markdown","db90b9a9":"markdown"},"source":{"6b4c51f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport datetime\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","19087582":"covid_19 = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv')\ncovid_19.head()","e76f1123":"# We will first try to prepare the data for exploration and then will look to do some predictions\n\ncovid_19 = covid_19.drop(['SNo'],axis=1)\n\n# Defining a function to check the NULL Values\n\ndef check_null_values(dataframe_to_check):\n    null_value  = dataframe_to_check.isnull().sum()\n    null_value_precent = round(100*(dataframe_to_check.isnull().sum()\/len(dataframe_to_check.index)),2)\n    df_to_return  = pd.DataFrame({'Missing Value Count': null_value,'Missing Value Percentage':null_value_precent})\n    print(df_to_return)","dce584e6":"print(covid_19.shape)\n\n# Checking the Missing Value Distribution\ncheck_null_values(covid_19)","fd024cd4":"# Now we will look at the Distribution of the State and Provinces to find the Most Affected Places. We can similarly use this technique of Mode to Impute the missing value so that we donot loose that 40% of Data.\n\ncovid_19['Province\/State'].value_counts()","6cb8ff66":"covid_19['Province\/State'] = covid_19['Province\/State'].fillna('Location Not Disclosed',axis=0)\n\ncheck_null_values(covid_19)","8ea4f42a":"# Now just for some Verification we will check if we have the Same Count of Missing Value or Not\n\nprint(covid_19[covid_19['Province\/State'] == 'Location Not Disclosed'].shape)\n\n# Hence We have 2462 Intact Places, Let's plot Them Now!","b9be08ac":"fig = px.scatter(covid_19, y=\"Deaths\",x = \"Recovered\", color=\"Country\/Region\",\n                 size='Confirmed', hover_data=['Province\/State','Confirmed','Deaths','Recovered'],log_x=True,log_y=True)\nfig.show()","94aa5eb6":"confirmed_df = covid_19.groupby(['Country\/Region','Province\/State']).agg({'Confirmed':sum}).reset_index()\n\nconfirmed_df.shape","0fb4c6e1":"print('Uniqe Countries in our Summed up Data Set:',confirmed_df['Country\/Region'].nunique())\n\nprint('Unique Countries in our Original Data Set:',covid_19['Country\/Region'].nunique())","8f15cc6b":"# Similarly\n\ndeaths_df = covid_19.groupby(['Country\/Region','Province\/State']).agg({'Deaths':sum}).reset_index()\nrecovered_df = covid_19.groupby(['Country\/Region','Province\/State']).agg({'Recovered':sum}).reset_index()\n\nprint('Uniqe Countries in our Summed up Data Set:',deaths_df['Country\/Region'].nunique())\n\nprint('Unique Countries in our Original Data Set:',covid_19['Country\/Region'].nunique())","165d93a5":"# Next we will merge all these to form the correct dataframe\n\ncovid_19_1 = confirmed_df.merge(deaths_df,how='inner',on=['Country\/Region','Province\/State'])\ncovid_19_df = covid_19_1.merge(recovered_df,how='inner',on=['Country\/Region','Province\/State'])\n\ncheck_null_values(covid_19_df)","1df98a87":"# Let's have a look at Data\n\ncovid_19_df.head(15)","114eeaed":"# Now let's plot it again\ndf_df = covid_19_df.groupby(['Country\/Region']).agg({'Deaths':sum,'Confirmed':sum,'Recovered':sum}).reset_index()\n\nfig = px.scatter(df_df, y=\"Deaths\",x = \"Recovered\", color=\"Country\/Region\",\n                 size='Confirmed', hover_data=['Confirmed','Deaths','Recovered'])\nfig.update_yaxes(nticks=20)\nfig.update_xaxes(nticks=50)\n\nfig.show()","63a43793":"covid_19_time_analysis = covid_19.loc[covid_19['Country\/Region'].isin(['Mainland China','Italy','Iran','South Korea','Spain'])]\n\ncovid_19_time_analysis.shape","fb5b4033":"print(covid_19_time_analysis.info())\n\n# We will drop Province as we donot need that column, since we will be having the Country in common for those Provinces\n\ncovid_19_time_analysis = covid_19_time_analysis.drop(['Province\/State'],axis=1)\ncovid_19_time_analysis.rename({'Country\/Region':'Country'},inplace=True,axis=1)\ncovid_19_time_analysis.head()","0ad6be39":"covid_19_time_analysis['Month of Observation'] = pd.to_datetime(covid_19_time_analysis['ObservationDate']).dt.strftime('%B')\n#covid_19_time_analysis['Month of Observation'] = covid_19_time_analysis['Month of Observation'].dt.strftime('%B')\ncovid_19_time_analysis['Year of Observation'] = pd.to_datetime(covid_19_time_analysis['ObservationDate']).dt.year","b643f4c3":"print(covid_19_time_analysis.describe())\n\n# We will also look for the Unique values in the created columns\n\nprint('Unique Values in Month of Observation',covid_19_time_analysis['Month of Observation'].nunique())\nprint('Unique Values in Month of Observation',covid_19_time_analysis['Year of Observation'].nunique())\n\n# This means that we have data from January to March and for the Year of 2020 only.","71c6babd":"covid_19_time_analysis['Hour of Observation'] = pd.to_datetime(covid_19_time_analysis['Last Update']).dt.hour","c3b8457b":"covid_19_time_analysis.head(10)","84caf122":"fig = px.bar(covid_19_time_analysis.groupby('Country').get_group('Mainland China'), x='Hour of Observation', y='Confirmed', color='Month of Observation')\nfig.update_xaxes(nticks=24)\nfig.show()","53ebd485":"fig = px.bar(covid_19_time_analysis, x=\"Hour of Observation\", y=\"Confirmed\", facet_col=\"Country\",color='Month of Observation',log_y=True)\nfig.update_xaxes(nticks=6)\n#fig.update_yaxes(nticks=20)\nfig.show()","19535169":"fig = px.bar(covid_19_time_analysis, x=\"Hour of Observation\", y=\"Deaths\", facet_col=\"Country\",color='Month of Observation',log_y=True,hover_data=[\n    'Country','Confirmed','Hour of Observation','Month of Observation'\n])\nfig.update_xaxes(nticks=6)\n#fig.update_yaxes(nticks=50)\nfig.show()","ba9b5356":"fig = px.bar(covid_19_time_analysis, x=\"Hour of Observation\", y=\"Recovered\", facet_col=\"Country\",color='Month of Observation',log_y=True,hover_data=[\n    'Country','Confirmed','Hour of Observation','Month of Observation','Deaths','Recovered'],title='Looking at the Countries based on Recovered Rate and Hourly Cases for different Months!'\n)\nfig.update_xaxes(nticks=6)\n#fig.update_yaxes(nticks=50)\nfig.show()","dd97bfd9":"fig = px.scatter(covid_19_time_analysis, x='ObservationDate', y='Deaths', color='Country',title='Looking at the Countries based on Observation Rate and Death Cases!')\nfig.update_yaxes(nticks=20)\nfig.show()","5fcd5067":"fig = px.scatter(covid_19_time_analysis, x='ObservationDate', y='Recovered', color='Country',title='Looking at the Countries based on Observation Rate and Recovered Cases!')\nfig.update_yaxes(nticks=20)\nfig.show()","fdac8b10":"fig = px.scatter(covid_19_time_analysis, x='ObservationDate', y='Confirmed', color='Country',title='Looking at the Countries based on Observation Rate and Confirmed Cases!')\nfig.update_yaxes(nticks=20)\nfig.show()","75919f81":"covid_19_time_analysis.head(10)","40d828da":"city_wise = covid_19_time_analysis.groupby('Country').sum()\ncity_wise['Death Rate'] = city_wise['Deaths'] \/ city_wise['Confirmed'] * 100\ncity_wise['Recovery Rate'] = city_wise['Recovered'] \/ city_wise['Confirmed'] * 100\ncity_wise['Active'] = city_wise['Confirmed'] - city_wise['Deaths'] - city_wise['Recovered']\ncity_wise = city_wise.sort_values('Deaths', ascending=False).reset_index()\ncity_wise","28ae6612":"px.scatter(city_wise,y = 'Recovery Rate',color='Country',x='Active',size='Confirmed',title='Looking at the Countries based on Recovery Rate and Active Cases!')\n","2bd3bb2f":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=city_wise['Recovery Rate'], y=city_wise['Country'],\n                    mode='lines+markers',\n                    name='Recovery rate'))\nfig.add_trace(go.Scatter(x=city_wise['Death Rate'], y=city_wise['Country'],\n                    mode='lines+markers',\n                    name='Death Rate'))\nfig.show()","70d6eaeb":"# Now we will try to Look for the \nfrom plotly.subplots import make_subplots\n\nconfirm_death_recovery_cases = covid_19_time_analysis.groupby('ObservationDate')['Confirmed','Deaths','Recovered'].sum().reset_index()\n\nplot = make_subplots(rows=1, cols=3, subplot_titles=(\"Comfirmed\", \"Deaths\", \"Recovered\"))\n\nsubPlot1 = go.Scatter(\n                x=confirm_death_recovery_cases['ObservationDate'],\n                y=confirm_death_recovery_cases['Confirmed'],\n                name=\"Confirmed\",\n                line=dict(color='royalblue', width=4, dash='dot'),\n                opacity=0.8)\n\nsubPlot2 = go.Scatter(\n                x=confirm_death_recovery_cases['ObservationDate'],\n                y=confirm_death_recovery_cases['Deaths'],\n                name=\"Deaths\",\n                line=dict(color='red', width=4, dash='dot'),\n                opacity=0.8)\n\nsubPlot3 = go.Scatter(\n                x=confirm_death_recovery_cases['ObservationDate'],\n                y=confirm_death_recovery_cases['Recovered'],\n                name=\"Recovered\",\n                line=dict(color='firebrick', width=4, dash='dash'),\n                opacity=0.8)\n\nplot.append_trace(subPlot1, 1, 1)\nplot.append_trace(subPlot2, 1, 2)\nplot.append_trace(subPlot3, 1, 3)\nplot.update_layout(template=\"ggplot2\", title_text = '<b><i>Global Spread of the Covid 19 Over Time<\/i><\/b>',xaxis_title='Observation Dates',\n                   yaxis_title='Count')\n\nplot.show()","c9b77fe1":"fig = px.pie(city_wise, values='Recovery Rate', names='Country')\nfig.show()","23fd736e":"fig = make_subplots(rows=1, cols=2,specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=city_wise['Country'], values=city_wise['Death Rate'], name=\"Death Rate\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=city_wise['Country'], values=city_wise['Recovery Rate'], name=\"Recovery Rate\"),\n              1, 2)\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Death and Recovery rate for Different Countries!\",\n    annotations=[dict(text='Death Rate', x=0.18, y=0.5, font_size=12, showarrow=False),\n                 dict(text='Recovery Rate', x=0.82, y=0.5, font_size=12, showarrow=False)])\nfig.show()","533a4bcb":"recovered_df = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_recovered.csv')\nconfirmed_df = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv')\ndeaths_df = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_deaths.csv')","20c553f9":"confirmed_df.head()","0c24206e":"cols = confirmed_df.keys()\nprint(cols)\n# gettingg all the Dates\n\nconfirmed = confirmed_df.loc[:, cols[4]:cols[-1]]\ndeaths = deaths_df.loc[:, cols[4]:cols[-1]]\nrecoveries = recovered_df.loc[:, cols[4]:cols[-1]]","03ae0836":"over_time_date = confirmed.keys()\n\n# Number of cases Over time\nworld_cases = []\n# Number of deaths Over time\ntotal_deaths = [] \n# The Rate at which Death id occuring\nmortality_rate = []\n# Number of people Recovered Over Time\ntotal_recovered = [] \n\nfor i in over_time_date:\n    confirmed_sum = confirmed[i].sum()\n    death_sum = deaths[i].sum()\n    recovered_sum = recoveries[i].sum()\n    world_cases.append(confirmed_sum)\n    total_deaths.append(death_sum)\n    mortality_rate.append(death_sum\/confirmed_sum)\n    total_recovered.append(recovered_sum)\n\n","fc3949b0":"# Converting them into Array for Modelling\n\nfrom_starting = np.array([i for i in range(len(over_time_date))]).reshape(-1, 1)\nworld_cases = np.array(world_cases).reshape(-1, 1)","1ed94f55":"moving_day_span = 7\nsingle_week_prediction = np.array([i for i in range(len(over_time_date)+moving_day_span)]).reshape(-1, 1)\nmodified_date = single_week_prediction[:-7]","0aca69ef":"start_date_data = '1\/22\/2020'\nstart_date = datetime.datetime.strptime(start_date_data, '%m\/%d\/%Y')\nin_future_dates = []\nfor i in range(len(single_week_prediction)):\n    in_future_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m\/%d\/%Y'))\n\nX_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(from_starting, world_cases, test_size=0.15, shuffle=False)\n\nprint(X_train_confirmed.shape)\nprint(X_test_confirmed.shape)\nprint(y_train_confirmed.shape)\nprint(y_test_confirmed.shape)","f773967f":"kernel = ['poly', 'sigmoid', 'rbf']\nc = [0.001, 0.01, 0.1, 1,10]\ngamma =[0.001, 0.01, 0.1, 1,10]\nepsilon = [0.01, 0.1, 1,10]\nshrinking = [True, False]\nsvm_grid = {'kernel': kernel, 'C': c, 'gamma' : gamma, 'epsilon': epsilon, 'shrinking' : shrinking}\n\nsvm = SVR()\nbest_model = RandomizedSearchCV(svm, svm_grid, scoring='neg_mean_squared_error', cv=5, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\nbest_model.fit(X_train_confirmed, y_train_confirmed)","d6c82fb5":"print(best_model.best_params_)\n\n# Now comes, what we have been waiting for, make those predictions\n\nbest_estimator_from_model = best_model.best_estimator_\nmake_predictions = best_estimator_from_model.predict(single_week_prediction)","5d96cf33":"plt.figure(figsize=(20, 8))\nplt.plot(modified_date, world_cases)\nplt.plot(single_week_prediction, make_predictions, linestyle='dashed', color='red')\nplt.title('# of Coronavirus Cases Over Time', size=15)\nplt.xlabel('Days Since 1\/22\/2020', size=10)\nplt.ylabel('Count of Cases', size=10)\nplt.legend(['Confirmed Cases', 'Model Predictions'])\nplt.xticks(size = 20)\nplt.show()","d4263c49":"# Now you people can see the precition till 22nd March 2020. Similarly, we would be doing some prediction for Deaths and recoveries\n\nover_time_deaths = deaths.keys()\nfrom_starting_deaths = np.array([i for i in range(len(over_time_deaths))]).reshape(-1, 1)\ndeath_cases = np.array(total_deaths).reshape(-1, 1)\n\nmoving_day_span = 7\nsingle_week_prediction = np.array([i for i in range(len(over_time_deaths)+moving_day_span)]).reshape(-1, 1)\nmodified_date = single_week_prediction[:-7]\n\nstart_date_data = '1\/22\/2020'\nstart_date = datetime.datetime.strptime(start_date_data, '%m\/%d\/%Y')\nin_future_dates = []\nfor i in range(len(single_week_prediction)):\n    in_future_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m\/%d\/%Y'))\n\nX_train_confirmed_deaths, X_test_confirmed_deaths, y_train_confirmed_deaths, y_test_confirmed_deaths = train_test_split(from_starting_deaths, death_cases, test_size=0.15, shuffle=False)\n\nprint(X_train_confirmed_deaths.shape)\nprint(X_test_confirmed_deaths.shape)\nprint(y_train_confirmed_deaths.shape)\nprint(y_test_confirmed_deaths.shape)","97e9581e":"kernel = ['poly', 'sigmoid', 'rbf']\nc = [0.001, 0.01, 0.1, 1,10]\ngamma =[0.001, 0.01, 0.1, 1,10]\nepsilon = [0.01, 0.1, 1,10]\nshrinking = [True, False]\nsvm_grid = {'kernel': kernel, 'C': c, 'gamma' : gamma, 'epsilon': epsilon, 'shrinking' : shrinking}\n\nsvm = SVR()\nbest_model = RandomizedSearchCV(svm, svm_grid, scoring='neg_mean_squared_error', cv=5, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\nbest_model.fit(X_train_confirmed_deaths, y_train_confirmed_deaths)","3b187885":"print(best_model.best_params_)\n\n# Now comes, what we have been waiting for, make those predictions\n\nbest_estimator_from_model = best_model.best_estimator_\nmake_predictions = best_estimator_from_model.predict(single_week_prediction)","987130a3":"plt.figure(figsize=(20, 8))\nplt.plot(modified_date, death_cases)\nplt.plot(single_week_prediction, make_predictions, linestyle='dashed', color='red')\nplt.title('# of Coronavirus Death Cases Over Time', size=15)\nplt.xlabel('Days Since 1\/22\/2020', size=10)\nplt.ylabel('Count of Death Cases', size=10)\nplt.legend(['Death Cases', 'Model Predictions'])\nplt.xticks(rotation = 90)\nplt.show()","8f1d49fe":"# Now you people can see the precition till 22nd March 2020. Similarly, we would be doing some prediction for Deaths and recoveries\n\nover_time_recoveries = recoveries.keys()\nfrom_starting_recoveries = np.array([i for i in range(len(over_time_recoveries))]).reshape(-1, 1)\nrecovered_cases = np.array(total_recovered).reshape(-1, 1)\n\nmoving_day_span = 7\nsingle_week_prediction = np.array([i for i in range(len(over_time_recoveries)+moving_day_span)]).reshape(-1, 1)\nmodified_date = single_week_prediction[:-7]\n\nstart_date_data = '1\/22\/2020'\nstart_date = datetime.datetime.strptime(start_date_data, '%m\/%d\/%Y')\nin_future_dates = []\nfor i in range(len(single_week_prediction)):\n    in_future_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m\/%d\/%Y'))\n\nX_train_confirmed_recoveries, X_test_confirmed_recoveries, y_train_confirmed_recoveries, y_test_confirmed_recoveries = train_test_split(from_starting_recoveries, recovered_cases, test_size=0.15, shuffle=False)\n\nprint(X_train_confirmed_recoveries.shape)\nprint(X_test_confirmed_recoveries.shape)\nprint(y_train_confirmed_recoveries.shape)\nprint(y_test_confirmed_recoveries.shape)","b068d1f5":"kernel = ['poly', 'sigmoid', 'rbf']\nc = [0.001, 0.01, 0.1, 1,10]\ngamma =[0.001, 0.01, 0.1, 1,10]\nepsilon = [0.01, 0.1, 1,10]\nshrinking = [True, False]\nsvm_grid = {'kernel': kernel, 'C': c, 'gamma' : gamma, 'epsilon': epsilon, 'shrinking' : shrinking}\n\nsvm = SVR()\nbest_model = RandomizedSearchCV(svm, svm_grid, scoring='neg_mean_squared_error', cv=5, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\nbest_model.fit(X_train_confirmed_recoveries, y_train_confirmed_recoveries)","198da330":"print(best_model.best_params_)\n\n# Now comes, what we have been waiting for, make those predictions\n\nbest_estimator_from_model = best_model.best_estimator_\nmake_predictions = best_estimator_from_model.predict(single_week_prediction)","dcc8a45f":"plt.figure(figsize=(20, 8))\nplt.plot(modified_date, recovered_cases)\nplt.plot(single_week_prediction, make_predictions, linestyle='dashed', color='red')\nplt.title('# of Coronavirus Recovered Cases Over Time', size=15)\nplt.xlabel('Days Since 1\/22\/2020', size=10)\nplt.ylabel('Count of Recovery Cases', size=10)\nplt.legend(['Recovered Cases', 'Model Predictions'])\nplt.xticks(rotation = 90)\nplt.show()","cb8dc05b":"covid_india = pd.read_csv('\/kaggle\/input\/covid19-in-india\/covid_19_india.csv')\ncovid_india.head()","7fca591f":"My Main Motive is to Explore more and find the Loopholes which can be actually covered as Corona is on Stage 3 in India and this is where it becomes Clumsy!\n\nStay Safe, Stay Indoor, Keep Kaggling and hit a **Upvote** if you like this Kernel. It will be an encouragement for us!!","cd1bd139":"We can see rright from the Chart that for Countries like **China** and **South Korea** got a lot of patients in the Month of **January Only** and it got worsen for the Countries like Iran and Italy in the month of **March**.  \nThey had 2 whole months to take action and prevent it, but they failed it seems. We still haven't looked at the Ditribution of Deaths, and next we are going to do the same!","d5ed8525":"We can see that **Hubei** is the Location where the maximum number of Cases are getting recorded, but in a Scientific way, same place is getting the number of cases updated, hence we cannot look on it in a Unique Fashion. No problem, what we can do is create a dummy DataFrame separately for **Confirmed,Deaths and Recovered** situations and sum them individually. Let me show you, what i am talking about!","d8580b28":"\nOk, let me explain to you what I am onto Next.  \n\nI want to build a predictive Model which will be predicting on a **7 day Gap** of Confirmed Cases for these, irrespective of the Countries, bcoz now we are looking on the Data with a Global Eye.  \n\nI intend to use a SVM model with all the possible kernels and then get the best Kernel performing and then make the predictions too!. I was initially thinking of making a predictive model, but was in dillema as to start from which one and then going by multiple notebooks, found that if we have a very Sparse Data, it is suitable for SVM to give a try as it will obviously cover your **Linear and Polynomial** model. Have any suggestions, let me know in the comment.\n\nTill then let's do this!","4fb1894f":"Till Now we see that the Only column which is having Null Values is the **State** column which means around 40% of Cases have not been disclosed with the Location which sounds fishy, but we know that **Gansu** has the most number of Cases, hence we will impute all such Cases with **Gansu** Location.   \n\nThis is our call of imputation. We could have also imputed with **Location Not Available**, which sounds logically correct as we can keep track of Such Cases. \nOk so let's impute them.  ","aa073ef4":"We can see the top 5 Countries which have a Gloabal Crisis in terms of **Corona Virus**. These are:\n    1. China\n    2. Italy\n    3. Iran\n    4. South Korea\n    5. Spain\n    \nThis Kernel is too late to tell you about these Countries as everyone is already aware of these countries being in crisis. But what is important that how quicky which country got it's count of patients up. \n\nThat's what we are going to do next. We have already cleaned our DataFrame and for ease of Understanding, we will be only taking these Countries and will do a Analysis, using the **Last Update** column to see week by week or maybe Month by Month analysis for these Countries.\n\nLet's do this!","50a6b32d":"Now the plan goes like this:\n\n    1. We will be looking at the Observation Date as that will give us the Month for that Patient.\n    2. Next we will be building a new feature which will be looking at the time duration and based on a 24 hour format, we  can look at different situations.\n    3. One of the sitauation could be the Deaths happening within how much time difference and on which date. Things like this.\n    4. Then we can even have a Ditribution of Countries, Monthly wise.\n    \nLet's do this:","611177b2":"What do we infer from Here?\n\n**Italy is the Country which is having a Very High Death Rate and Having Low Recovery rate** which is very concerning for the Country!","69442186":"Interpreting this Graph makes me more comfortable as in China in the Hour of **14, 360 people recovered  from 452 cases with only 8 deaths recorded** and that too in the month of March.  \nThis inference in it's own stands that It took them 3 months to fight this whole Pandemic and somehow they are coming out of it.\n\nOne scientific look i want to make if the **Percentage of Deaths\/Confirmeness of Cases and Cases of Recovering**. \n\nGiving you the Glimpse of What we will be doing!!\nLet's look at the percentage distribution.","8e1d4201":"The Cases kept on Piling up and the Deaths keep happening!","6872c5e3":"We tried a Predictive Modelling of Identifying the Pattern and We have done this from the Dataset of **Confirmed Cases and Death Cases**.","b166f070":"We will just Draw two more **Dynamic Pie Charts** which will be our Final Inference and that same comparison we can use when we will be exploring the India's CoronaVirus Dataset!!","51d42058":"These three Scatter Plots do give us the Glipmse of What pattern which we are looking for!!.\n\n\nAdding my Partners!! Let's rock and Roll!","7ab34f5f":"## Pattern of Recovered Cases!","7c378c6a":"## Pattern of Death Occuring Cases!","db90b9a9":"## Pattern of Confirmed Cases!"}}