{"cell_type":{"345e9ccc":"code","e43cd804":"code","99f9dd1e":"code","7aa3cd51":"code","b3812e81":"code","e44c852e":"code","40a51d80":"code","8c08bc2a":"code","75067771":"code","25d483ee":"code","4ffbea3e":"code","dab31bd3":"code","d1fbb91b":"code","12db3dfc":"code","32ee5e3b":"code","23ebbd0d":"code","1cb0e9f6":"code","ae8a7353":"code","93aa09c1":"code","df1284d4":"code","150d4a4c":"code","23b9091e":"code","45c686e7":"code","0c72095b":"code","38c490be":"code","d7ba24de":"code","86c1b51f":"code","1bf34b9d":"code","f4aaee32":"code","f4d6a5e6":"code","421c2446":"code","7c563dd4":"code","cda0750a":"code","1e731b35":"code","4ccd7973":"code","63bf2fb5":"code","4d0a647c":"code","fb86f3d3":"code","b86aad46":"code","d9e36fc5":"code","fecfe4e2":"code","0e5e0bbd":"code","3d9cb2c2":"code","e124585b":"code","da9405c1":"code","3dad7b06":"code","6559b8af":"code","43661d77":"code","e4bc1e0e":"code","40d54fee":"code","7d257aea":"code","21b73acf":"code","44952eac":"code","c7dd9b84":"code","18431a23":"code","1391fcaf":"code","e685e1ea":"code","82f1990d":"code","2cf3598c":"code","e31d0394":"code","36fb8975":"code","0d474140":"code","e58e1be4":"code","d6f0d097":"code","9e4fd677":"code","461cb199":"code","ff7f70f2":"code","c645779c":"code","8f81d60a":"code","a3906bff":"code","00899af2":"code","58bcd05b":"code","f242f91a":"code","14a0aca3":"code","c56c4706":"code","e4ba1e8e":"code","141c93b5":"code","dc83d382":"code","c0abb591":"code","f5e529e1":"code","64a44093":"code","be8e5f1b":"code","a8d1f5c5":"code","e864bd89":"code","f3f5a3e8":"code","0b278029":"code","30dec954":"code","fd063130":"markdown","ff511313":"markdown","5e715a31":"markdown","bc16f84b":"markdown","e355f530":"markdown","4ca8907a":"markdown","45cc4a61":"markdown","87c3aa92":"markdown","ef70b9e1":"markdown","4a5a3aeb":"markdown","d94c2a1f":"markdown","7076c33f":"markdown","6602b5a8":"markdown","aeb26a59":"markdown","123daffa":"markdown","75cfe368":"markdown","63acfce6":"markdown","ad1a469c":"markdown","085e59d4":"markdown","b874c229":"markdown","0ef2d227":"markdown","12e7a16d":"markdown","7ba96f7d":"markdown","825c296d":"markdown","c4ea5b74":"markdown","e98ff699":"markdown","7f046e55":"markdown","451826fd":"markdown","8e72caa7":"markdown","d6ce4a7e":"markdown","720ff4c0":"markdown","d029950d":"markdown","66ce8055":"markdown"},"source":{"345e9ccc":"import sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","e43cd804":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","99f9dd1e":"train = pd.read_csv('..\/input\/seleksidukungaib\/train.csv')\ntest = pd.read_csv('..\/input\/seleksidukungaib\/test.csv')","7aa3cd51":"# We use copy, in order to preserve the nature of the native code\ndata1_train = train.copy(deep=True)\ndata1_test = test.copy(deep=True)","b3812e81":"# Find Out the shape of data\nprint(f\"Training Shape : {data1_train.shape}, Testing Shape : {data1_test.shape}\")","e44c852e":"# Describe the data\nprint(data1_train.describe())","40a51d80":"# Data Information\nprint(data1_train.info())\nprint(data1_test.info())","8c08bc2a":"# Read few sample size \nprint(data1_train.sample(10))","75067771":"# Checking UserId : There are multiple userId that have multiple transaction, might need to check on that\nprint(f\"Maximum Transaction : {data1_train.userId.value_counts().max()}\")\nprint(f\"Minimum Transaction : {data1_train.userId.value_counts().min()}\")","25d483ee":"# Visualizing the number of transaction\ndata1_train.groupby('userId').size().value_counts().plot(kind='bar')","4ffbea3e":"holder_group_by_userId = data1_train.groupby('userId').sum(numeric_only=True)\nholder_group_by_userId.head(5)","dab31bd3":"# Checking either the distribution of isChurned different after aggregation : There are difference, might be the key of solving duplicate userId mystery\nprint(f\"IsChurned Before Aggregation | True : {data1_train.loc[data1_train.isChurned > 0].shape[0]} , False : {data1_train.loc[data1_train.isChurned == 0].shape[0]}\")\nprint(f\"IsChurned After Aggregation | True : {holder_group_by_userId.loc[holder_group_by_userId.isChurned > 0].shape[0]}, False : {holder_group_by_userId.loc[holder_group_by_userId.isChurned == 0].shape[0]}\")","d1fbb91b":"# see missing value after aggregation\nprint(\"Checking null values : \")\nprint(holder_group_by_userId.isnull().sum())\nprint(\"Checking na values : \")\nprint(holder_group_by_userId.isna().sum())","12db3dfc":"# Checking date and date_collected\ndata2_train = data1_train[['date', 'date_collected']].copy()\n# Creating a new feature to check the correlation\ndata2_train[\"isSameDate\"] = data1_train['date'] == data1_train['date_collected']\ndata2_train[\"isSameDate\"].value_counts()\n\n# Conclusion : 2 features with same value, might remove either 1 to reduce redundancy\n# Probably because of joining process of 2 database (same feature, different name in different database)","32ee5e3b":"# Checking the categorical values of datetime collected\nprint(data2_train.date.value_counts())","23ebbd0d":"data_train_check = train.copy()\n\ndata_train_check.columns","1cb0e9f6":"print(f\"Duplicated data : {data_train_check.drop(['idx'], axis=1).duplicated().sum()}\")","ae8a7353":"print(f\"Duplicated user Id : {data_train_check[['userId']].duplicated().sum()}\")","93aa09c1":"# Checking the date\ngroup_by_res = data_train_check.drop(['date_collected'], axis=1).groupby(['date'])","df1284d4":"print(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-09-01').drop(['idx','random_number'], axis = 1).duplicated().sum()}\")\nprint(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-10-01').drop(['idx','random_number'], axis = 1).duplicated().sum()}\")\nprint(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-11-01').drop(['idx','random_number'], axis = 1).duplicated().sum()}\")","150d4a4c":"group_by_res.get_group('2017-09-01').columns","23b9091e":"print(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-09-01')[['userId']].duplicated().sum()}\")\nprint(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-10-01')[['userId']].duplicated().sum()}\")\nprint(f\"Duplicated user collected in 2017-09-01 : {group_by_res.get_group('2017-11-01')[['userId']].duplicated().sum()}\")","45c686e7":"data = pd.concat([train.copy(),test.copy()],ignore_index=True)\n\nfirst_month_data = group_by_res.get_group('2017-09-01')[['userId', 'isChurned']].drop_duplicates().copy()\nsecond_month_data = group_by_res.get_group('2017-10-01')[['userId', 'isChurned']].drop_duplicates().copy()\nthird_month_data = group_by_res.get_group('2017-11-01')[['userId', 'isChurned']].drop_duplicates().copy()\nforth_month_data = test[['userId']].drop_duplicates().copy()\n\nfirst_month_data = first_month_data.loc[first_month_data.isChurned == 0].copy()\nsecond_month_data = second_month_data.loc[second_month_data.isChurned == 0].copy()\nthird_month_data = third_month_data.loc[third_month_data.isChurned == 0].copy()\n\nuser_id_data = data['userId'].copy()\nuser_id_set = set(user_id_data)","0c72095b":"# First, let seperate data\ndata_group_train = []\nfor date in train.date.unique():\n    data_group_train.append(group_by_res.get_group(date).copy())","38c490be":"for data in data_group_train:\n    print(data.isnull().sum())","d7ba24de":"for data in data_group_train:\n    print(data.shape)","86c1b51f":"for data in data_group_train:\n    new_data = data.groupby('userId').sum()\n    print(new_data.shape)","1bf34b9d":"research_group_train = data_group_train[0].copy()\n\ndict_agg = {\n    \"num_recharge_trx\" : \"sum\",\n    \"num_topup_trx\" : \"sum\",\n    \"num_transfer_trx\" : \"sum\",\n    \"num_transaction\" : \"sum\",\n    \"average_recharge_trx\" : \"mean\",\n    \"average_topup_trx\" : \"mean\",\n    \"average_transfer_trx\" : \"mean\",\n    \"max_recharge_trx\" : \"max\",\n    \"max_topup_trx\" : \"max\",\n    \"max_transfer_trx\" : \"max\",\n    \"min_recharge_trx\" : \"min\",\n    \"min_topup_trx\" : \"min\",\n    \"min_transfer_trx\" : \"min\",\n    \"isChurned\" : lambda x: x.value_counts().index[0],  # highest frequency\n    \"pinEnabled\" : \"first\",\n    \"userLevel\" : \"first\",\n    \"super\" : \"first\",\n    \"premium\" : \"first\",\n    \"blocked\" : \"first\",\n    \"isUpgradedUser\" : \"first\",\n    \"isVerifiedEmail\" : \"first\",\n    \"isVerifiedPhone\" : \"first\",\n    \"total_transaction\" : \"sum\",\n    \"isActive\" : \"first\"\n}\n\nresearch_group_train['user_counter'] = research_group_train.groupby('userId')['userId'].transform('count')\n# research_group_train.drop(['idx','date','random_number'], axis = 1).groupby('userId').agg(dict_agg)['userId'].value_counts()","f4aaee32":"categorical_cols = ['userLevel']","f4d6a5e6":"# We need to check the correlation of each userLevel toward to the isChurned features\n# In this situation, we will use ANOVA to check the correlation\nfrom scipy import stats\n\ndata_cat_train = train[categorical_cols + ['isChurned']].copy()\n\nF, p = stats.f_oneway(data_cat_train[data_cat_train.userLevel==1].isChurned,\n                      data_cat_train[data_cat_train.userLevel==2].isChurned,\n                      data_cat_train[data_cat_train.userLevel==3].isChurned)\n\nprint(f\"F Score  : {F}\")\n\n# Conclusion : Considering a pretty big number for the F score, the reason might be because of imbalance value in each group, or a high correlation between userLevel and isChurned","421c2446":"boolean_cols = ['blocked', 'premium', 'isUpgradedUser', 'isVerifiedPhone', 'isVerifiedEmail', 'isActive', 'super', 'pinEnabled']","7c563dd4":"# Creating new dataset to target only boolean features\n\ndata_bool_train = data1_train[boolean_cols].copy()\ndata_bool_test = data1_test[boolean_cols].copy()\n\nprint(data_bool_train.info())","cda0750a":"# Some columns in boolean features need preprocessing\npreprocess_cols = ['premium', 'super', 'pinEnabled']\n\nfor col in preprocess_cols:\n    print(f\"Value counts of {col}\")\n    print(data_bool_train[col].value_counts())","1e731b35":"replacement = {\n    False : 0,\n    True : 1\n}\n\nfor col in preprocess_cols:\n    data_bool_train[col].replace(replacement, inplace=True)","4ccd7973":"print(data_bool_train.head())","63bf2fb5":"# Seeing is believing. Visualization is better than calculation\n\nfig = plt.figure(figsize=(18,10), dpi=1600)\n#2 rows 4 cols\n\nax1 = plt.subplot2grid((2,4),(0,0))\nplt.pie(data_bool_train.blocked.value_counts(),colors=(\"g\",\"b\"))\nplt.title('Blocked')\n\nax1 = plt.subplot2grid((2,4), (0, 1))\nplt.pie(data_bool_train.isUpgradedUser.value_counts(),colors=(\"g\",\"r\"))\nplt.title('IsUpgradedUser')\n\nax1 = plt.subplot2grid((2,4), (0, 2))\nplt.pie(data_bool_train.isVerifiedPhone.value_counts(),colors=(\"g\",\"b\"))\nplt.title('isVerifiedPhone')\n\n\nax1 = plt.subplot2grid((2,4), (0, 3))\nplt.pie(data_bool_train.premium.value_counts(),colors=(\"g\",\"r\"))\nplt.title('premium')\n\nax1 = plt.subplot2grid((2,4),(1,0))\nplt.pie(data_bool_train.isVerifiedEmail.value_counts(),colors=(\"g\",\"b\"))\nplt.title('isVerifiedEmail')\n\nax1 = plt.subplot2grid((2,4), (1, 1))\nplt.pie(data_bool_train.isActive.value_counts(),colors=(\"g\",\"r\"))\nplt.title('isActive')\n\nax1 = plt.subplot2grid((2,4), (1, 2))\nplt.pie(data_bool_train.super.value_counts(),colors=(\"g\",\"b\"))\nplt.title('super')\n\n\nax1 = plt.subplot2grid((2,4), (1, 3))\nplt.pie(data_bool_train.pinEnabled.value_counts(),colors=(\"g\",\"r\"))\nplt.title('pinEnabled')","4d0a647c":"# Considering that ['blocked', 'isUpgradedUser', 'isActive' , 'super'] is not balanced data, we need to check its correlation to the isChurned \n\nfor col in boolean_cols:\n    print(f\"Correlation of {col} to isChurned : {data_bool_train[col].corr(train.isChurned)}\")","fb86f3d3":"# Missing value\nprint(f\"Records with any missing value : {data_bool_train[data_bool_train.isnull().any(axis=1)].shape[0]}\")\n\nprint(\"Index List : \")\nindexes = data_bool_train[data_bool_train.isnull().any(axis=1)].index\nfor index in indexes:\n    print(f\"User with index : {index}\")","b86aad46":"# We can check the data for those user with those index\ntrain.loc[train.idx.isin(indexes)]\n\n# Conclusion : Can drop dataset because many Nan value detected","d9e36fc5":"numerical_cols = ['num_recharge_trx', 'average_recharge_trx', 'max_recharge_trx', 'min_recharge_trx', 'num_topup_trx', 'average_topup_trx', 'max_topup_trx', 'min_topup_trx', 'num_transfer_trx', 'average_transfer_trx', 'max_transfer_trx', 'min_transfer_trx', 'num_transaction', 'total_transaction']\ntopup_cols = [col for col in numerical_cols if \"topup\" in col]\nrecharge_cols = [col for col in numerical_cols if \"recharge\" in col]\ntransfer_cols = [col for col in numerical_cols if \"transfer\" in col]\ntransaction_cols = [col for col in numerical_cols if col not in topup_cols + recharge_cols + transfer_cols]\n\nprint(f\"Topup Cols : {topup_cols}\")\nprint(f\"Recharge Cols : {recharge_cols}\")\nprint(f\"Transfer Cols : {transfer_cols}\")\nprint(f\"Transaction Cols : {transaction_cols}\")\n","fecfe4e2":"# First, let analyse the topup columns\ndata_topup_train = train[topup_cols].copy()\n\n# Check missing value in the dataset\nprint(data_topup_train.isna().sum())","0e5e0bbd":"# Second, let analyse the recharge columns\ndata_recharge_train = train[recharge_cols].copy()\n\n# Check missing value in the dataset\nprint(data_recharge_train.isna().sum())","3d9cb2c2":"# Lastly, let analyse the transfer columns\ndata_transfer_train = train[transfer_cols].copy()\n\n# Check missing value in the dataset\nprint(data_transfer_train.isna().sum())","e124585b":"# Checking the transaction\ndata_transaction_train = train[transaction_cols].copy()\n\n# Check missing value in the dataset\nprint(data_transaction_train.isna().sum())","da9405c1":"# Need to check the correlation\nused_cols = [col for col in numerical_cols if col not in ['num_transfer_trx', 'average_transfer_trx', 'max_transfer_trx', 'min_transfer_trx']]\ncorr_numerical = train[used_cols + ['isChurned']].copy()\nsns.heatmap(corr_numerical.corr(), annot = True, fmt='.1g')\n\n# Conclusion : Most of the data seem to have affecting the isChurned attribut quite significant, but there still some data that contains many missing value, thus correlation cannot be calculated","3dad7b06":"# Using prior knowledge, we try to aggregate based on userId, (notes : sum process is only for faster research, to truly aggregate, need seperate aggregation)\n\naggr_numeric_train = train[numerical_cols + ['isChurned' , 'userId']].copy().groupby('userId').sum(numeric_only=True)\naggr_numeric_train.describe()","6559b8af":"# First, non-aggregate data\nsns.pairplot(corr_numerical.drop(['isChurned'], axis=1))","43661d77":"sns.heatmap(train.corr(), annot = True, fmt='.1g')","e4bc1e0e":"# Define few important function\ndef show_missing_data(df):\n    print(f\"Shape : {df.shape}\")\n    print(df.isnull().sum())","40d54fee":"# Remove the 10 problematic rows\ndata_cleaned_train = train.copy()\ndata_cleaned_train = data_cleaned_train.loc[~data_cleaned_train.idx.isin(indexes)]\nshow_missing_data(data_cleaned_train)","7d257aea":"data_drop_duplicate = data_cleaned_train.drop(['idx','random_number'], axis = 1)\ndata_drop_duplicate = data_drop_duplicate.drop_duplicates()","21b73acf":"# Impute missing value using provided data context \n\ndata_imputed_train = data_drop_duplicate.copy()\n\n# Fill average_topup_trx\ndata_imputed_train['average_topup_trx'] = data_imputed_train.apply(\n    lambda row: (row['max_topup_trx'] + row['min_topup_trx']) \/ 2.0 if pd.isnull(row['average_topup_trx']) else row['average_topup_trx'],\n    axis = 1\n)\n\n# Fill average_transfer_trx\ndata_imputed_train['average_transfer_trx'] = data_imputed_train.apply(\n    lambda row: (row['max_transfer_trx'] + row['min_transfer_trx']) \/ 2.0 if pd.isnull(row['average_transfer_trx']) else row['average_transfer_trx'],\n    axis = 1\n)\n\n# Fill max_recharge_trx\ndata_imputed_train['max_recharge_trx'] = data_imputed_train.apply(\n    lambda row: (row['average_recharge_trx'] * 2 + row['min_recharge_trx']) if pd.isnull(row['max_recharge_trx']) else row['max_recharge_trx'],\n    axis = 1\n)\n\n\ndata_imputed_train['total_transaction'] = data_imputed_train.apply(\n    lambda row: (row['average_recharge_trx'] + row['average_transfer_trx'] + row['average_topup_trx']) if pd.isnull(row['total_transaction']) else row['total_transaction'],\n    axis = 1\n)\n\n# Check missing value\ndata_imputed_train.info()","44952eac":"# Since all datatype were already numerical, can check correlation\ncleaned_train = data_imputed_train.copy()\nsns.heatmap(cleaned_train.corr(), annot = True, fmt='.1g')","c7dd9b84":"# First creating `have` feature\ndata_engineering_train = cleaned_train.copy()\n\ndata_engineering_train['have_transfer_trx'] = data_engineering_train.apply(\n    lambda row: True if row['num_transfer_trx'] > 0 else False,\n    axis = 1\n)\n\ndata_engineering_train['have_recharge_trx'] = data_engineering_train.apply(\n    lambda row: True if row['num_recharge_trx'] > 0 else False,\n    axis = 1\n)\n\ndata_engineering_train['have_transaction'] = data_engineering_train.apply(\n    lambda row: True if row['num_recharge_trx'] > 0 else False,\n    axis = 1\n)\n\ndata_engineering_train['have_topup_trx'] = data_engineering_train.apply(\n    lambda row: True if row['num_recharge_trx'] > 0 else False,\n    axis = 1\n)","18431a23":"# Lets try feature engineering \n\nchange_to_dollar_cols = ['average_recharge_trx', 'average_topup_trx', 'total_transaction']\n\nfor col in change_to_dollar_cols : \n    print(col, \"starting\")\n    data_engineering_train[col + '_dollar'] = data_engineering_train.apply(\n        lambda row: float(\"{:.2f}\".format(row[col] \/ 15000)) ,\n        axis = 1\n    )\n    print(col, \"finished\")","1391fcaf":"data_engineering_train[[col + '_dollar' for col in change_to_dollar_cols]].head()","e685e1ea":"sns.heatmap(data_engineering_train.corr(), annot = True, fmt='.1g')","82f1990d":"data_engineering_train.num_recharge_trx.head()","2cf3598c":"# Binning\ndata_engineering_train['average_recharge_trx_bin'] = pd.qcut(data_engineering_train.average_recharge_trx,q=10, duplicates='drop')","e31d0394":"factor = 3\nfor col in data_engineering_train.columns:\n    if data_engineering_train[col].dtypes == 'float' or data_engineering_train[col].dtypes == 'int':\n        print(col, \"checking ...\")\n        upper_lim = data_engineering_train[col].mean () + data_engineering_train[col].std () * factor\n        lower_lim = data_engineering_train[col].mean () - data_engineering_train[col].std () * factor\n        print(\"upper limit : \", upper_lim)\n        print(\"lower limit : \", lower_lim)\n        \n        print(\"Non Outlier : \",len(data_engineering_train[(data_engineering_train[col] < upper_lim) & (data_engineering_train[col] > lower_lim)]))\n        print(\"Outlier : \",len(data_engineering_train[(data_engineering_train[col] >= upper_lim) | (data_engineering_train[col] <= lower_lim)]))\n        \n\n# Conclusion : Using checking similar to boxplot, we can see many outliers in the data\n# One choice is to cap them\n# Other choice is to bin them","36fb8975":"# Checking with percentile\nfor col in data_engineering_train.columns:\n    if data_engineering_train[col].dtypes == 'float' or data_engineering_train[col].dtypes == 'int':\n        print(col, \"checking ...\")\n        upper_lim = data_engineering_train[col].quantile(.95)\n        lower_lim = data_engineering_train[col].quantile(.5)\n        print(\"upper limit : \", upper_lim)\n        print(\"lower limit : \", lower_lim)\n        \n        print(\"Non Outlier : \",len(data_engineering_train[(data_engineering_train[col] < upper_lim) & (data_engineering_train[col] > lower_lim)]))\n        print(\"Outlier : \",len(data_engineering_train[(data_engineering_train[col] >= upper_lim) | (data_engineering_train[col] <= lower_lim)]))\n        print(\"Outlier Upper Lim only : \",len(data_engineering_train[(data_engineering_train[col] >= upper_lim)]))\n        \n# When checking using quantile, things go so bad, many of the data were an outlier\n# But most of them probably due to the many 0 in the numerical data","0d474140":"# Quantile Binning\nquantile_list = [0, .25, .5, .75, 1.]\nquantiles = data_engineering_train['average_recharge_trx_dollar'].quantile(quantile_list)\nquantiles","e58e1be4":"# Initial state cols\ncategorical_cols = ['userLevel']\nnumerical_cols = ['num_recharge_trx', 'average_recharge_trx', 'max_recharge_trx', 'min_recharge_trx', 'num_topup_trx', 'average_topup_trx', 'max_topup_trx', 'min_topup_trx', 'num_transfer_trx', 'average_transfer_trx', 'max_transfer_trx', 'min_transfer_trx', 'num_transaction', 'total_transaction']\ntopup_cols = [col for col in numerical_cols if \"topup\" in col]\nrecharge_cols = [col for col in numerical_cols if \"recharge\" in col]\ntransfer_cols = [col for col in numerical_cols if \"transfer\" in col]\ntransaction_cols = [col for col in numerical_cols if col not in topup_cols + recharge_cols + transfer_cols]\nboolean_cols = ['blocked', 'premium', 'isUpgradedUser', 'isVerifiedPhone', 'isVerifiedEmail', 'isActive', 'super', 'pinEnabled']","d6f0d097":"# Define unused cols\nunused_cols = [\"idx\", \"userId\", \"random_number\",\"isVerifiedPhone\",\"blocked\", \"date\",  \"isUpgradedUser\", \"isActive\" , \"average_transfer_trx\", \"num_transfer_trx\", \"max_transfer_trx\" , \"min_transfer_trx\",\"date_collected\",\"super\", \"have_transfer_trx\",\"min_recharge_trx\",\"min_topup_trx\",\"max_recharge_trx\",\"max_topup_trx\",\"total_transaction\",\"average_recharge_trx\",\"average_topup_trx\",\"num_recharge_trx\",\"num_topup_trx\",\"num_transaction\",\"max_recharge_trx\",\"max_topup_trx\",\"min_recharge_trx\",\"min_topup_trx\",\"range_topup_trx\",\"range_recharge_trx\"]","9e4fd677":"# Preprocess train a little bit\ntrain_backup = train.copy()\n\n# from the cleaned checkpoint\ntrain = data_imputed_train.copy()","461cb199":"boolean_cols = boolean_cols + ['have_transfer_trx', 'have_recharge_trx' , 'have_transaction' , 'have_topup_trx']\ncategorical_cols = categorical_cols + [\"num_recharge_trx_bin\",\"num_topup_trx_bin\", \"num_transaction_bin\"]\nnumerical_cols = numerical_cols + ['average_recharge_trx_log', 'average_topup_trx_log', 'total_transaction_log',\"max_recharge_trx_log\",\"max_topup_trx_log\",\"min_recharge_trx_log\",\"min_topup_trx_log\",\"range_topup_log\",\"range_recharge_log\"] \n# add any new feature into the categories","ff7f70f2":"# Create the complete preprocessing function to be tested on train and test dataset\ndef change_datatype(df):\n    new_df = df.copy()\n    bool_datatype = [col for col in new_df.columns if col in boolean_cols]\n    categorical_datatype = [col for col in new_df.columns if col in categorical_cols]\n    numerical_datatype = [col for col in new_df.columns if col in numerical_cols]\n    datetime_datatype = [col for col in new_df.columns if col in ['date', 'date_collected']]\n    \n    # Boolean Convertion\n    for col in bool_datatype:\n        new_df[col] = new_df[col].astype('bool')\n\n    # Categorical convertion\n    for col in categorical_datatype:\n        new_df[col] = new_df[col].astype('category')\n\n    # Datetime convertion\n    for col in datetime_datatype:\n        new_df[col] = pd.to_datetime(new_df[col], format=\"%Y-%d-%m\")\n        \n    return new_df\n    \ndef convert_features(df):\n    new_df = df.copy()\n    \n    # First, need to label encode userLevel\n    bool_datatype = [col for col in new_df.columns if col in boolean_cols]\n    categorical_datatype = [col for col in new_df.columns if col in categorical_cols]\n    col =  bool_datatype + categorical_datatype\n    le = LabelEncoder()\n    for i in col: \n        new_df[i] = le.fit_transform(list(new_df[i].values))\n\n    #Second, need to normalize the numerical value\n    from sklearn.preprocessing import StandardScaler\n    transformer = StandardScaler()\n    numerical_datatype = [col for col in new_df.columns if col in numerical_cols]\n    for i in numerical_datatype:\n        new_df[i] = transformer.fit_transform(new_df[i].values.reshape(-1, 1))\n        \n    return new_df\n\ndef remove_unrelevant_features(df):\n    new_df = df.copy()\n    relevant_feature = [col for col in new_df.columns if col not in unused_cols]\n    return new_df[relevant_feature]\n\ndef preprocess(df):\n    new_df = df.copy()\n    new_df = change_datatype(new_df)\n    new_df = outlier_capping(new_df)\n    new_df = convert_features(new_df)\n    new_df = remove_unrelevant_features(new_df)\n    return new_df","c645779c":"# Feature engineering defined\ndef feature_engineered(df):\n    df_engineered = df.copy()\n    \n    # Creating boolean from any transaction\n    bool_change_cols = ['_transaction']\n    \n    for col in bool_change_cols:     \n        df_engineered['have' + col] = df_engineered.apply(\n            lambda row: True if row['num' + col] > 0 else False,\n            axis = 1\n        )\n        \n    # bin the feature \n    change_to_bin_cols = [\"num_recharge_trx\",\"num_topup_trx\",\"num_transaction\"]\n    for col in change_to_bin_cols:\n        df_engineered[col+'_bin'] = pd.qcut(df_engineered[col], q=10, duplicates='drop')\n        \n    # create new row\n    new_numeric_cols = ['_recharge_trx', '_topup_trx']\n    for col in new_numeric_cols:\n        df_engineered['range' + col] = df_engineered.apply(\n            lambda row: row['max' + col] - row['min' + col],\n            axis = 1\n        )\n    \n    \n    # log transformation\n    change_to_log_cols = ['average_recharge_trx', 'average_topup_trx', 'total_transaction','range_recharge_trx','range_topup_trx','min_recharge_trx','min_topup_trx','max_recharge_trx','max_topup_trx']\n\n    for col in change_to_log_cols : \n        df_engineered[col + '_log'] = (df_engineered[col]+1).transform(np.log)\n\n    return df_engineered","8f81d60a":"def outlier_capping(df):\n    new_df = df.copy()\n    \n    train_data = new_df.loc[~pd.isnull(new_df.isChurned)].copy()\n    \n    for col in train_data:\n        if train_data[col].dtypes == 'float' or train_data[col].dtypes == 'int':\n            upper_lim = train_data[col].quantile(.95)\n            lower_lim = train_data[col].quantile(.5)\n            \n            # cap the upper limit \n            train_data.loc[(train_data[col] >= upper_lim)][col] = upper_lim\n            \n    new_df.loc[~pd.isnull(new_df.isChurned)] = train_data \n    \n    return new_df","a3906bff":"# Saving the test index\ntest_idx = test['idx'].copy()","00899af2":"data = pd.concat([train.copy(),test.copy()],ignore_index=True)\n\nprint(\"start feature engineering ...\")\ndata_cleaned = feature_engineered(data.copy())\nprint(\"finish feature engineering ...\")\n\nprint(\"start preprocessing ...\")\ndata_cleaned = preprocess(data_cleaned.copy())\nprint(\"finish preprocessing ...\")\n\n\nprint(\"data splitting ...\")\ntrain_cleaned = data_cleaned[~data_cleaned.isChurned.isnull()]\ntest_cleaned = data_cleaned[data_cleaned.isChurned.isnull()]\nprint('finish ...')","58bcd05b":"# Verify process\nsns.heatmap(train_cleaned.corr(), annot = True, fmt='.1g')","f242f91a":"train_cleaned.info()","14a0aca3":"train_cleaned[['average_recharge_trx_log','average_topup_trx_log','total_transaction_log','max_recharge_trx_log','max_topup_trx_log','min_recharge_trx_log','min_topup_trx_log']].describe()","c56c4706":"test_cleaned[['average_recharge_trx_log','average_topup_trx_log','total_transaction_log','max_recharge_trx_log','max_topup_trx_log','min_recharge_trx_log','min_topup_trx_log']].describe()","e4ba1e8e":"print(f\"Training Dataset shape : {train_cleaned.shape}\")\nprint(f\"Testing Dataset shape : {test_cleaned.shape}\")","141c93b5":"train_cleaned.info()","dc83d382":"train_cleaned['isChurned'] = train_cleaned['isChurned'].astype('int').astype('category')\ntest_cleaned = test_cleaned.drop(['isChurned'],axis = 1)","c0abb591":"# Outputing cleaned dataset\ntrain_cleaned.to_csv(\"train_cleaned.csv\", index= False)\ntest_cleaned.to_csv(\"test_cleaned.csv\", index=False)","f5e529e1":"from sklearn.model_selection import train_test_split\n\nX = train_cleaned.drop(['isChurned'], axis = 1)\ny = train_cleaned['isChurned']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25) ","64a44093":"y_test.value_counts()","be8e5f1b":"# Logistic Regression Modelling\nmodel = linear_model.LogisticRegression()\nmodel.fit(X_train,y_train)\npred = model.predict(test_cleaned)","a8d1f5c5":"from sklearn.model_selection import cross_val_score\n\ndef validation_models(model, X, y):\n    mean_score = cross_val_score(model, X, y, scoring=\"f1\", cv = 20).mean()\n    return mean_score","e864bd89":"print(f\"Validation for model : {validation_models(model, X_test, y_test)}\")","f3f5a3e8":"# Validation using stratisfied K-fold\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=8,shuffle=True)\nscore = 0\nfor train_index, test_index in skf.split(X.copy(), y.copy()):\n    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n    X_train_k_fold, X_test_k_fold = X.iloc[train_index], X.iloc[test_index]\n    y_train_k_fold, y_test_k_fold = y[train_index], y[test_index]\n    \n    model_k_fold = svm.LinearSVC()\n    model_k_fold.fit(X_train_k_fold, y_train_k_fold)\n    curr_score = validation_models(model_k_fold, X_test_k_fold, y_test_k_fold)\n    score += curr_score\n    print(f\"Validation mean k fold score: {curr_score}\")\nprint(f\"Mean validation : {score \/ 8}\")","0b278029":"submission = pd.DataFrame({'idx':test_idx,'isChurned':pred.astype(int)})\nsubmission.to_csv('submission.csv',index=False)","30dec954":"submission.isChurned.value_counts()","fd063130":"## Split Train Test","ff511313":"## Output the cleaned dataset (when submitting the notebook, please delete this part)","5e715a31":"### Check Unique Value","bc16f84b":"Something is not right, only 1 part of the data is missing on every type of transaction","e355f530":"\"min_recharge_trx\",\"min_topup_trx\",\"max_recharge_trx\",\"max_topup_trx\",\"num_recharge_trx\",\"num_topup_trx\",\"total_transaction\",\"average_recharge_trx\",\"average_topup_trx\" \"isVerifiedPhone\", \"isVerifiedEmail\", \"isActive\",","4ca8907a":"### Correlation HeatMap","45cc4a61":"## Import Library","87c3aa92":"Researching feature engineering here","ef70b9e1":"### Check seperate month","4a5a3aeb":"## Define Problem\nSummary : As the bright new Data Scientist in the company, you are tasked to help them build this model from scratch. You are given a dataset from the data analyst team containing user activity data. The data is activity done in one month starting from the date collected attribute. Each data is given the label isChurned which indicates wether or not that user churns in the next month .\n\nProblem : Defining wether user will churn or not in the next month based on current data\n\nTask : *Binary Classification*","d94c2a1f":"Outlier Detection ","7076c33f":"**Conclusion on this step** : (need update)\n1. Some data are missing on training set, but no data are missing on testing set.\n2. There are 27 features need to be examined, with isChurned as the target for this problem\n3. After aggregating the userId, we see an unexpected result, in which many userId is duplicated in the same month. Current hypothesis is whenever the person make a transaction, it is saved in the database.\n4. Date and date_collected is essentially same data, so either 1 can be removed. Maybe because it is joined from different database, but have different names.\n5. userLevel might be a good feature, because it have a really high F value when being comparet to the isChurned features\n6. Other numerical value also seems to have a pretty good correlation with isChurned, but may be able to extract more via feature engineering (binning and regularization).","6602b5a8":"## Checking user per month data approach","aeb26a59":"Adaptive Binning","123daffa":"### Check Numeric","75cfe368":"## Check Data (Exploratory Data Analysis)","63acfce6":"## Modeling ","ad1a469c":"## Make Submission","085e59d4":"We can see that :\n- blocked\n- isUpgradedUser\n- isActive\n- super\nis not really affecting the isChurned\n\nps : We also find out that there were 10 records that were missing data from all boolean category, need to remove it later from training set","b874c229":"Using pairplot to infer the data more","0ef2d227":"**An unique identifier data (not used in machine learning process) :**\n1. idx\n2. userId\n\n**Not relevant columns (not used in machine learning process):**\n1. random_number\n\n**Constant value columns (not used in machine learning process):**\n1. num_transfer_trx\n2. average_transfer_trx\n3. max_transfer_trx\n4. min_transfer_trx\n\n**Same feature, different name (not used in machine learning process):**\n1. date or date_collected (either one)\n\n**Some feature that nearly have no variation (maybe not used in machine learning process):**\n1. blocked\n2. isUpgradedUser\n3. isActive\n4. super","12e7a16d":"There are a few problem with this numerical aggregate value : \n1. num_transfer_trx, average_transfer_trx, max_transfer_trx, min_transfer_trx have 0 constant value, thus not affecting the columns at all (must be removed from training)\n2. As we can infer from the pairplot, we get more varied information when aggregating the result, and to accomodate the test data that have all unique userId, for training need to be aggregated. (in the pairplot, we can see the distribution of the scatter plot is not much changed, but it reduced the missing value for the dataset)\n3. Normalization and standarization is needed, this happens because the span between data is too high (because of Rupiah currency), might need to normalize that","7ba96f7d":"## Cleaning Data","825c296d":"Initial processing for train data","c4ea5b74":"Check Handled Missing Data","e98ff699":"### Check Boolean","7f046e55":"Conclusion on this check process : \n1. Data from each month have its own duplicates, it might be better to process each month seperately\n2. Aggregation style for each data should be handled seperately\n3. Since average, max, min is available, might be able to impute it \n4. Should be done seperately from the test data, because test only contains data from 1 month (not from multiple month like in training dataset)","451826fd":"## Validation Using F1 Score","8e72caa7":"### Check Categorical","d6ce4a7e":"## Read Data","720ff4c0":"**A few data types that need to be changed :** \n1. date_collected, date : should be datetime, not object\n2. blocked , premium ,isUpgradedUser , isVerifiedPhone , isVerifiedEmail , isActive : should be bool\n3. userLevel : should be categorical (ordinal data types) -> use Label Encoder\n","d029950d":"### Check DateTime data","66ce8055":"## Removing unused cols \n- idx\n- userId\n- random_number\n- date\n- isVerifiedPhone\n- isVerifiedEmail\n- isUpgradedUser\n- isActive\n- average_transfer_trx\n- num_transfer_trx\n- min_transfer_trx\n- max_transfer_trx\n- date_collected\n- super"}}