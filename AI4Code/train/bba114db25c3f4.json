{"cell_type":{"51d4c943":"code","e76e01e9":"code","fa4ca4ac":"code","5ad15889":"code","288f404e":"code","16adea48":"code","e4cefda5":"code","07cf09eb":"code","a02da4cb":"code","8529cf40":"code","f9b5fac1":"code","2bfd2b01":"code","9edc4744":"code","b49931ab":"code","afaf7d86":"code","b5e3b65d":"code","5182f224":"code","b8f1c16a":"code","d0571898":"code","db520de1":"markdown","9cec61a7":"markdown","e78dc33d":"markdown","5a8fcef3":"markdown","62775a83":"markdown","f482a870":"markdown"},"source":{"51d4c943":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import interp\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, auc\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\".\"))\n\n\n# Any results you write to the current directory are saved as output.","e76e01e9":"train_trans = pd.read_csv('..\/input\/train_transaction.csv')\ntest_trans = pd.read_csv('..\/input\/test_transaction.csv')\ntrain_iden = pd.read_csv('..\/input\/train_identity.csv') \ntest_iden = pd.read_csv('..\/input\/test_identity.csv')\n\ntrain_trans.shape, test_trans.shape, train_iden.shape, test_iden.shape","fa4ca4ac":"df_train = train_trans.merge(train_iden, on=\"TransactionID\", how=\"left\")\ndf_test = test_trans.merge(test_iden, on=\"TransactionID\", how=\"left\")\ndel train_iden, train_trans, test_iden, test_trans","5ad15889":"# df_train = pd.read_csv('train_reduced.csv')\n# df_test = pd.read_csv('test_reduced.csv')","288f404e":"df_train.head()","16adea48":"# filling NAs\ndf_train.fillna(-999, inplace=True)\ndf_test.fillna(-999, inplace=True)","e4cefda5":"drop_cols = [\"TransactionID\", \"TransactionDT\"]\nlabel_col = \"isFraud\"","07cf09eb":"y_train = df_train.loc[:,label_col]","a02da4cb":"df_train.drop([label_col]+drop_cols, axis=1, inplace=True)\ndf_test.drop([\"TransactionDT\"], axis=1, inplace=True)","8529cf40":"features = list(df_train)","f9b5fac1":"gc.collect()","2bfd2b01":"cat_cols = [\"card{}\".format(i) for i in range(1,7)] +\\\n            ['ProductCD', 'P_emaildomain', 'R_emaildomain',  'DeviceType', 'DeviceInfo'] +\\\n            [\"M{}\".format(i) for i in range(1,10)] +\\\n            [\"id_{}\".format(i) for i in range(12, 39)] +\\\n            [\"addr{}\".format(i) for i in range(1,3)]","9edc4744":"cat_idx = [df_train.columns.get_loc(c) for c in cat_cols]","b49931ab":"nfold = 5\n# skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n# tscv = TimeSeriesSplit(n_splits=5)\npredictions_df = pd.DataFrame()\n\nseeds = [1,12,123,21,423]\n\nmean_fpr = np.linspace(0,1,100)\nroc_aucs = []\ntprs = []\ncms= []\naucs = []\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\n# for train_idx, valid_idx in skf.split(df_train_con, df_train[label_col].values):\nfor i in range(nfold):\n    np.random.seed(seeds[i])  # to reporoduce\n    size_95_per = int(0.95*df_train.shape[0])\n    train_idx = np.random.choice(range(size_95_per),\n                                 int(0.5*size_95_per), replace=False)  # 70% sampling random\n    valid_idx = range(size_95_per, df_train.shape[0]) # last 5% of data\n    print(\"\\nfold {}\".format(i))\n    print(\"train pool\")\n    trn_data = Pool(df_train.iloc[train_idx].values,\n                     y_train.iloc[train_idx].values,\n                     cat_features=cat_idx)\n    gc.collect()\n    print(\"valid pool\")\n    val_data = Pool(df_train.iloc[valid_idx].values,\n                    y_train.iloc[valid_idx].values,\n                    cat_features=cat_idx) \n    gc.collect()\n    clf = CatBoostClassifier(iterations=1500,\n                           random_state=10,\n                           learning_rate=0.08,\n                           task_type = \"GPU\",\n                           eval_metric= 'AUC', \n                           scale_pos_weight = sum(y_train.iloc[train_idx]==0)\/sum(y_train.iloc[train_idx]==1)\/5.,\n#                            one_hot_max_size = 4,\n#                            has_time=True,\n#                            min_data_in_leaf=5,\n                           early_stopping_rounds = 50,\n                          )\n    print(\"model\")\n    clf.fit(trn_data,\n          use_best_model=True,\n          eval_set=val_data,\n          verbose=False,\n          plot=True)\n    gc.collect()\n    print(\"predict valid\")\n    oof = clf.predict_proba(df_train.iloc[valid_idx].values)[:,1] \n    print(\"predict test\")\n    this_fold_preds = clf.predict_proba(df_test.drop(\"TransactionID\", axis=1))[:,1]\n    predictions += this_fold_preds\/np.float(nfold)\n    predictions_df[str(i)] = this_fold_preds\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(y_train.iloc[valid_idx].values, oof))\n   \n    # Roc curve by fold\n    fpr, tpr, t = roc_curve(y_train.iloc[valid_idx].values, oof)\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    # don't need them now\n    del trn_data, val_data\n","afaf7d86":"del df_train\ngc.collect()","b5e3b65d":"h=plt.hist(predictions)  # mean of folds ","5182f224":"h = plt.hist(predictions_df.median(1))","b8f1c16a":"test_submission = pd.DataFrame()\ntest_submission['TransactionID'] = df_test.iloc[0:506691,0]\ntest_submission['isFraud'] = predictions_df.median(1)\ntest_submission.head()","d0571898":"test_submission.to_csv('submission_catboost_baseline.csv', index=False)","db520de1":"**Note** - The score is low from kernel submission because only 50% of training data was used for each fold. I did an median ensemble of 4 such models with random 50% of data. The score will improve if you use full dataset with shuffled k-fold (>0.935). I had to do that because my kernel was dying when I was trying to create *Dataset Pool* for Catboost. The creation of Pool eats up a lot of RAM. Another option for me was to reduce memory of dataframe. But I didn't try that. \n\n- Catboost with last 5% as validation set\n- Only null treatment required in pre-processing\n- Categorical variables handled by passing as index\n\nOnly 50% of data has been used in each fold. The score is better if use stratified fold so that it covers all the data. \n\nReduce data memory size by using this kernel - https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee\/output\nFound this kernel to be useful for experiments and took some code from here - https:\/\/www.kaggle.com\/vincentlugat\/ieee-lgb-bayesian-opt    [](http:\/\/)","9cec61a7":"Defining categorical features. Defining Card, Email, Device, Product, M, id12-id38, addr as categorical features","e78dc33d":"## End","5a8fcef3":"* We create random subsamples from the training data. The reason is that running Pool function exceeds the kernel memory limit and kernel dies. So we create 5 models with 66% bootstrap samples and average predictions from these 5 models for test set. ","62775a83":"Test submission","f482a870":"Submitting median of predictions. You can also submit meanm"}}