{"cell_type":{"7a07638f":"code","941f8516":"code","eb5cbcc5":"code","17fd7aae":"code","b8847549":"code","e69d6b36":"code","ee31166d":"code","d3578ec6":"code","0f9f0e13":"code","782351d3":"code","00d342f1":"code","0d3ea294":"code","878c7c84":"code","852f4f0b":"code","8585ba45":"markdown","3face735":"markdown","7c2f17f8":"markdown","b7259272":"markdown","ae7c42d7":"markdown"},"source":{"7a07638f":"import json\nimport keras.layers as layers\nimport numpy as np\nimport pandas as pd\nimport spacy\nfrom gensim.corpora import Dictionary\nfrom keras.models import Model\nfrom keras.preprocessing import sequence\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom spacy.util import minibatch","941f8516":"nlp = spacy.load('en')\ndata = pd.read_json('..\/input\/News_Category_Dataset.json', lines=True)","eb5cbcc5":"categories = data.groupby('category').size().sort_values(ascending=False)\ncategories","17fd7aae":"TOP_N_CATEGORIES = 7\ndata = data[data.category.apply(lambda x: x in categories.index[:TOP_N_CATEGORIES]) &\\\n            (data.headline.apply(len) > 0)]\ndata_train, data_test = train_test_split(data, test_size=.1, random_state=31)","b8847549":"# check if `textcat` is already in the pipe, add if not\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add labels to the model    \nfor label in categories.index[:TOP_N_CATEGORIES]:\n    textcat.add_label(label)\n\n# preprocess training data\ndata_train_spacy = list(\n    zip(data_train.headline,\n        data_train.category.apply(\n            lambda cat: {'cats': {c: float(c == cat)\n                                  for c in categories.index[:TOP_N_CATEGORIES]}}))\n)\n\n# train the model\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):\n    optimizer = nlp.begin_training()\n    for i in range(5):\n        print('Epoch %d' % i)\n        losses = {}\n        batches = minibatch(data_train_spacy, size=128)\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            docs = [nlp.tokenizer(h) for h in data_test.headline]\n            test_pred = np.array(\n                [sorted(doc.cats.items(), key=lambda x: -x[1])[0][0]\n                 for doc in textcat.pipe(docs)])\n            print('Test Acc: %.4f' %\n                  (pd.Series(test_pred == data_test.category.values).sum() \/ data_test.shape[0]))","e69d6b36":"spacy_y_pred = [sorted(doc.cats.items(), key=lambda x: -x[1])[0][0]\n                for doc in nlp.pipe(data_test.headline)]\nprint(classification_report(data_test.category, spacy_y_pred))","ee31166d":"MAX_SEQUENCE_LEN = 20\nUNK = 'UNK'\nPAD = 'PAD'\n\ndef text_to_id_list(text, dictionary):\n    return [dictionary.token2id.get(tok, dictionary.token2id.get(UNK))\n            for tok in text_to_tokens(text)]\n\ndef texts_to_input(texts, dictionary):\n    return sequence.pad_sequences(\n        list(map(lambda x: text_to_id_list(x, dictionary), texts)), maxlen=MAX_SEQUENCE_LEN,\n        padding='post', truncating='post', value=dictionary.token2id.get(PAD))\n\ndef text_to_tokens(text):\n    return [tok.text.lower() for tok in nlp.tokenizer(text)\n            if not (tok.is_punct or tok.is_quote)]\n\ndef build_dictionary(texts):\n    d = Dictionary(text_to_tokens(t)for t in texts)\n    d.filter_extremes(no_below=3, no_above=1)\n    d.add_documents([[UNK, PAD]])\n    d.compactify()\n    return d","d3578ec6":"dictionary = build_dictionary(data.headline)","0f9f0e13":"x_train = texts_to_input(data_train.headline, dictionary)\nx_test = texts_to_input(data_test.headline, dictionary)","782351d3":"lb = LabelBinarizer()\nlb.fit(categories.index[:TOP_N_CATEGORIES])\ny_train = lb.transform(data_train.category)\ny_test = lb.transform(data_test.category)","00d342f1":"EMBEDDING_DIM = 50\n\ninp = layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='float32')\nemb = layers.Embedding(len(dictionary), EMBEDDING_DIM, input_length=MAX_SEQUENCE_LEN)(inp)\nfilters = []\nfor kernel_size in [2, 3, 4]:\n    conv = layers.Conv1D(32, kernel_size, padding='same', activation='relu', strides=1)(emb)\n    pooled = layers.MaxPooling1D(pool_size=MAX_SEQUENCE_LEN-kernel_size+1)(conv)\n    filters.append(pooled)\n\nstacked = layers.Concatenate()(filters)\nflatten = layers.Flatten()(stacked)\ndrop = layers.Dropout(0.2)(flatten)\nout = layers.Dense(7, activation='softmax')(drop)\n\nmodel = Model(inputs=inp, outputs=out)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","0d3ea294":"model.summary()","878c7c84":"model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))","852f4f0b":"y_test_pred = [lb.classes_[i] for i in np.argmax(model.predict(x_test), axis=1)]\nprint(classification_report(data_test.category, y_test_pred))","8585ba45":"I'll limit my effort to the top seven of these categories. The reason for this is to avoid sparser and perhaps less clearly defined categories (e.g. why is ARTS & CULTURE separate from ARTS?).","3face735":"## Conclusion\n\nThe SpaCy model is pretty similar in performance to the simple CNN.\n\n### Todo\n* Experiment further with the CNN parameters\n* Add pretrained embeddings and see how that affects performance\n* Experiment with other methods in [that](http:\/\/www.aclweb.org\/anthology\/D14-1181) paper\n* Look into more advanced CNN architectures","7c2f17f8":"## Simple CNN using Keras\n\nNext, I'll train my own model. I pad (or clip) the sentences to have length 20 and limit the vocabulary to words that appear in at least three documents. Then, I start with an embedding layer, stack convolutional layers with different filter sizes, and finally end with a dense softmax output.","b7259272":"## Intro\n\nIn this notebook, I approach the task of headline classification on a subset of the categories. I compare SpaCy's `TextCategorizer` feature inroduced in v2.0 with a custom CNN architecture inspired by [this](http:\/\/www.aclweb.org\/anthology\/D14-1181) paper and implemented in Keras. The point of this exercise is really just to see if I can beat SpaCy's model.","ae7c42d7":"## SpaCy Baseline\n\nSpaCy has a builtin `TextCategorizer` module that does multilabel classification. The docs for that are [here](https:\/\/spacy.io\/api\/textcategorizer), and while the specifics of the model are not documented, the docs do give a high level description:\n\n> The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length `nr_class`, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present.\n\nI'll train this model and then later compare results with a custom CNN architecture. Since the model does multilabel classification, it will be interesting to see if just a multiclass setup with softmax instead of logistic activation on the output layer can boost performance.\n\nAdditionally, the only SpaCy model currently available in the Kaggle kernel environment is `en_web_core_sm` which does not have pretrained word embeddings. Therefore, I assume that the embedding layer in this model is randomly initialized and trainable. This will be important when training my custom model later.\n\nThe code below was adapted from the official SpaCy example [here](https:\/\/github.com\/explosion\/spacy\/blob\/master\/examples\/training\/train_textcat.py)."}}