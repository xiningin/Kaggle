{"cell_type":{"2432d533":"code","66610ff3":"code","b94e414b":"code","5901c80e":"code","7213ee25":"code","660fbf68":"code","1c93284f":"code","dd1f40db":"code","13e4c77f":"code","42fe4a0c":"code","6d1bbb60":"code","1caf964c":"code","38dbf6c3":"code","af276463":"code","d0706b08":"code","1926cf2c":"code","3ddb06fe":"code","e01883bc":"code","545a2eeb":"code","ae4a08be":"code","cfc56266":"code","3b8cd4ea":"code","ef06cb23":"code","e8c421dd":"code","ba237cac":"code","a3b20bfd":"code","5868ccef":"code","2a72c1a6":"code","bd2d51c7":"code","860d97fc":"code","2690d2d7":"code","3a48fda0":"code","46dc24e2":"code","6fd5e9be":"code","56afe8e1":"code","10521645":"code","96b050e3":"code","732114ef":"code","1449faac":"code","9578c5ae":"code","f0e49d0d":"code","58d6a923":"code","953d5c1b":"code","39aa9361":"code","dad7ea98":"code","933104dd":"code","e345343e":"code","faaefa9d":"code","d0ac91d5":"code","165e4ea2":"code","92d734d3":"code","d5ff4431":"code","05911630":"code","d3b56144":"code","13d28675":"code","8c77175c":"code","fa5130fa":"code","8f3d7955":"code","47ea1b2f":"code","2c311305":"code","f493b3d9":"code","bb9014f3":"code","535f7081":"markdown","707f8c21":"markdown","3244f319":"markdown","7b5c5e20":"markdown","db668c5e":"markdown","a23de120":"markdown","19ccff45":"markdown","9898a4d8":"markdown","f7a8f0d7":"markdown","bdbefb19":"markdown","cc995aed":"markdown","e6670cc3":"markdown","d2864edb":"markdown","03a13230":"markdown","d9cb0685":"markdown","d4f7511c":"markdown","a8ddd74c":"markdown","744477ba":"markdown","0fadd015":"markdown","59397a0b":"markdown","dc3a7c8f":"markdown","3ac8f955":"markdown","c39fb473":"markdown","b9ac3597":"markdown","30102497":"markdown","fa5f5304":"markdown","d56508a0":"markdown","fd240eb1":"markdown","d30a7e10":"markdown","45b56f19":"markdown","63d07bba":"markdown","c385bb43":"markdown","6ca4c97b":"markdown","bb1755d8":"markdown","0c619cda":"markdown","da9212e2":"markdown","aa230f10":"markdown","55e2ad43":"markdown","dc19a753":"markdown","6a4a2fd7":"markdown","89806f54":"markdown","6822e94c":"markdown","abcef5ed":"markdown","d4bbd5c1":"markdown","45e30304":"markdown","1d5f85e8":"markdown","6da1f285":"markdown","8d1e05a0":"markdown","1de42c60":"markdown","fc90dfbb":"markdown","0736df83":"markdown","7472909d":"markdown","b1e2ed47":"markdown","e1930f2f":"markdown","6c3b32ec":"markdown","75e61e95":"markdown","e7bef3d6":"markdown","466169dd":"markdown","caa97352":"markdown","0ba77734":"markdown","c90fdceb":"markdown","32670d25":"markdown","70a25553":"markdown","619801c6":"markdown","c2dc5f9c":"markdown","6cb1aee8":"markdown","5a7b0251":"markdown","34ed13f1":"markdown","5a367c19":"markdown","b6a4d422":"markdown","5b758e2a":"markdown","95804643":"markdown","5bc616f4":"markdown","c92ab237":"markdown","3a259c04":"markdown","707728c8":"markdown","317abe7f":"markdown","bb954a99":"markdown","6c8132af":"markdown","c82004bb":"markdown","b0d72157":"markdown","fa34c486":"markdown","467d88c5":"markdown","2fa322f5":"markdown","2041069f":"markdown","a30ff788":"markdown","18fc8d3f":"markdown","45bf733b":"markdown","1f747449":"markdown","6a9a54b5":"markdown","eb5cb700":"markdown","4faf04da":"markdown","e5aee609":"markdown","b43752d0":"markdown","0d7ab6ed":"markdown","e7521a6c":"markdown","2ad4a6c1":"markdown","7575eb2b":"markdown","c6d0695e":"markdown","ee5f399c":"markdown","3d44d336":"markdown","43fba843":"markdown","e01db5b6":"markdown","e4eb256a":"markdown","ef96ba91":"markdown","ded303b4":"markdown","153e86ba":"markdown","c637ff8c":"markdown","dcc17e94":"markdown","8bf3e2db":"markdown"},"source":{"2432d533":"import numpy as np\nfrom numba import cuda","66610ff3":"n = 1024*1024 # 1M\n\nthreads_per_block = 1024\nblocks = int(n \/ threads_per_block)\n\nstride = 16\n\n# Input Vectors of length stride * n\na = np.ones(stride * n).astype(np.float32)\nb = a.copy().astype(np.float32)\n\n# Output Vector\nout = np.zeros(n).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_b = cuda.to_device(b)\nd_out = cuda.to_device(out)","b94e414b":"@cuda.jit\ndef add_experiment(a, b, out, stride, coalesced):\n    i = cuda.grid(1)\n    # The above line is equivalent to\n    # i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    if coalesced == True:\n        out[i] = a[i] + b[i]\n    else:\n        out[i] = a[stride*i] + b[stride*i]","5901c80e":"%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, True); cuda.synchronize","7213ee25":"result = d_out.copy_to_host()\ntruth = a[:n] + b[:n]","660fbf68":"np.array_equal(result, truth)","1c93284f":"%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, False); cuda.synchronize","dd1f40db":"result = d_out.copy_to_host()\ntruth = a[::stride] + b[::stride]","13e4c77f":"np.array_equal(result, truth)","42fe4a0c":"from IPython.display import IFrame\nIFrame('https:\/\/view.officeapps.live.com\/op\/view.aspx?src=https:\/\/developer.download.nvidia.com\/training\/courses\/C-AC-02-V1\/coalescing-v3.pptx', 800, 450)","6d1bbb60":"import numpy as np\nfrom numba import cuda","1caf964c":"n = 16384 # matrix side size\nthreads_per_block = 256\nblocks = int(n \/ threads_per_block)\n\n# Input Matrix\na = np.ones(n*n).reshape(n, n).astype(np.float32)\n# Here we set an arbitrary row to an arbitrary value to facilitate a check for correctness below.\na[3] = 9\n\n# Output vector\nsums = np.zeros(n).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_sums = cuda.to_device(sums)","38dbf6c3":"@cuda.jit\ndef row_sums(a, sums, n):\n    idx = cuda.grid(1)\n    sum = 0.0\n    \n    for i in range(n):\n        # Each thread will sum a row of `a`\n        sum += a[idx][i]\n        \n    sums[idx] = sum","af276463":"%timeit row_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()","d0706b08":"result = d_sums.copy_to_host()\ntruth = a.sum(axis=1)","1926cf2c":"np.array_equal(truth, result)","3ddb06fe":"import numpy as np\nfrom numba import cuda","e01883bc":"n = 16384 # matrix side size\nthreads_per_block = 256\nblocks = int(n \/ threads_per_block)\n\na = np.ones(n*n).reshape(n, n).astype(np.float32)\n# Here we set an arbitrary column to an arbitrary value to facilitate a check for correctness below.\na[:, 3] = 9\nsums = np.zeros(n).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_sums = cuda.to_device(sums)","545a2eeb":"@cuda.jit\ndef col_sums(a, sums, ds):\n    # TODO: Write this kernel to store the sum of each column in matrix `a` to the `sums` vector.\n    idx = cuda.grid(1)\n    sum = 0.0\n    \n    for i in range(ds):\n        # Each thread will sum a row of `a`\n        sum += a[i][idx]\n        \n    sums[idx] = sum","ae4a08be":"%timeit col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()","cfc56266":"result = d_sums.copy_to_host()\ntruth = a.sum(axis=0)","3b8cd4ea":"np.array_equal(truth, result)","ef06cb23":"import numpy as np\nfrom numba import cuda","e8c421dd":"A = np.zeros((4,4)) # A 4x4 Matrix of 0's\nd_A = cuda.to_device(A)\n\n# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n# by using a Python tuple to signify grid and block dimensions.\nblocks = (2, 2)\nthreads_per_block = (2, 2)","ba237cac":"@cuda.jit\ndef get_2D_indices(A):\n    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n    x, y = cuda.grid(2)\n    # The above is equivalent to the following 2 lines of code:\n    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    \n    # Write the x index followed by a decimal and the y index.\n    A[x][y] = x + y \/ 10","a3b20bfd":"get_2D_indices[blocks, threads_per_block](d_A)","5868ccef":"result = d_A.copy_to_host()\nresult","2a72c1a6":"import numpy as np\nfrom numba import cuda","bd2d51c7":"n = 2048*2048 # 4M\n\n# 2D blocks\nthreads_per_block = (32, 32)\n# 2D grid\nblocks = (64, 64)\n\n# 2048x2048 input matrices\na = np.arange(n).reshape(2048,2048).astype(np.float32)\nb = a.copy().astype(np.float32)\n\n# 2048x2048 0-initialized output matrix\nout = np.zeros_like(a).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_b = cuda.to_device(b)\nd_out = cuda.to_device(out)","860d97fc":"@cuda.jit\ndef matrix_add(a, b, out, coalesced):\n    # TODO: set x and y to index correctly such that each thread\n    # accesses one element in the data.\n    x, y = cuda.grid(2)\n    \n    if coalesced == True:\n        # TODO: write the sum of one element in `a` and `b` to `out`\n        out[y][x] = a[y][x] + b[y][x]\n    else:\n        out[x][y] = a[x][y] + b[x][y]   # TODO: write the sum of one element in `a` and `b` to `out`\n                                        # using an uncoalesced memory access pattern.","2690d2d7":"%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize","3a48fda0":"result = d_out.copy_to_host()\ntruth = a+b","46dc24e2":"np.array_equal(result, truth)","6fd5e9be":"%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize","56afe8e1":"result = d_out.copy_to_host()\ntruth = a+b","10521645":"np.array_equal(result, truth)","96b050e3":"import numpy as np\nfrom numba import types, cuda","732114ef":"@cuda.jit\ndef swap_with_shared(vector, swapped):\n    # Allocate a 4 element vector containing int32 values in shared memory.\n    temp = cuda.shared.array(4, dtype=types.int32)\n    \n    idx = cuda.grid(1)\n    \n    # Move an element from global memory into shared memory\n    temp[idx] = vector[idx]\n    \n    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n    cuda.syncthreads()\n    #...the following operation is reading an element written to shared memory by another thread.\n    \n    # Move an element from shared memory back into global memory\n    swapped[idx] = temp[3 - cuda.threadIdx.x] # swap elements","1449faac":"vector = np.arange(4).astype(np.int32)\nswapped = np.zeros_like(vector)\n\n# Move host memory to device (global) memory\nd_vector = cuda.to_device(vector)\nd_swapped = cuda.to_device(swapped)","9578c5ae":"vector","f0e49d0d":"swap_with_shared[1, 4](d_vector, d_swapped)","58d6a923":"# Move device (global) memory back to the host\nresult = d_swapped.copy_to_host()\nresult","953d5c1b":"from IPython.display import IFrame\nIFrame('https:\/\/view.officeapps.live.com\/op\/view.aspx?src=https:\/\/developer.download.nvidia.com\/training\/courses\/C-AC-02-V1\/shared_coalescing.pptx', 800, 450)","39aa9361":"from numba import cuda\nimport numpy as np","dad7ea98":"n = 4096*4096 # 16M\n\n# 2D blocks\nthreads_per_block = (32, 32)\n#2D grid\nblocks = (128, 128)\n\n# 4096x4096 input and output matrices\na = np.arange(n).reshape((4096,4096)).astype(np.float32)\ntransposed = np.zeros_like(a).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_transposed = cuda.to_device(transposed)","933104dd":"@cuda.jit\ndef transpose(a, transposed):\n    x, y = cuda.grid(2)\n\n    transposed[x][y] = a[y][x]","e345343e":"%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()","faaefa9d":"result = d_transposed.copy_to_host()\nexpected = a.T","d0ac91d5":"np.array_equal(result, expected)","165e4ea2":"import numpy as np\nfrom numba import cuda, types as numba_types","92d734d3":"n = 4096*4096 # 16M\n\n# 2D blocks\nthreads_per_block = (32, 32)\n#2D grid\nblocks = (128, 128)\n\n# 4096x4096 input and output matrices\na = np.arange(n).reshape((4096,4096)).astype(np.float32)\ntransposed = np.zeros_like(a).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_transposed = cuda.to_device(transposed)","d5ff4431":"@cuda.jit\ndef tile_transpose(a, transposed):\n    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n    # and that `a` is a multiple of these dimensions.\n    \n    # 1) Create 32x32 shared memory array.\n    \n    tile = cuda.shared.array((32, 32), numba_types.int32)\n\n\n    # Compute offsets into global input array. Recall for coalesced access we want to map threadIdx.x increments to\n    # the fastest changing index in the data, i.e. the column in our array.\n    # Note: `a_col` and `a_row` are already correct.\n    a_col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    a_row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    \n    # 2) Make coalesced read from global memory (using grid indices)\n    # into shared memory array (using thread indices).\n    \n    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[a_row, a_col]\n\n    # 3) Wait for all threads in the block to finish updating shared memory.\n    \n    cuda.syncthreads()\n\n    \n    # 4) Calculate transposed location for the shared memory array tile\n    # to be written back to global memory. Note that blockIdx.y*blockDim.y \n    # and blockIdx.x* blockDim.x are swapped (because we want to write to the\n    # transpose locations), but we want to keep access coalesced, so match up the\n    # threadIdx.x to the fastest changing index, i.e. the column.\/\n    # Note: `t_col` and `t_row` are already correct.\n    t_col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n    t_row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n\n    # 5) Write from shared memory (using thread indices)\n    # back to global memory (using grid indices)\n    # transposing each element within the shared memory array.\n    \n    transposed[t_row, t_col] = tile[cuda.threadIdx.x, cuda.threadIdx.y]","05911630":"%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()","d3b56144":"result = d_transposed.copy_to_host()\nexpected = a.T","13d28675":"np.array_equal(result, expected)","8c77175c":"from IPython.display import IFrame\nIFrame('https:\/\/view.officeapps.live.com\/op\/view.aspx?src=https:\/\/developer.download.nvidia.com\/training\/courses\/C-AC-02-V1\/bank_conflicts.pptx', 800, 450)","fa5130fa":"import numpy as np\nfrom numba import cuda, types as numba_types","8f3d7955":"n = 4096*4096 # 16M\nthreads_per_block = (32, 32)\nblocks = (128, 128)\n\na = np.arange(n).reshape((4096,4096)).astype(np.float32)\ntransposed = np.zeros_like(a).astype(np.float32)\n\nd_a = cuda.to_device(a)\nd_transposed = cuda.to_device(transposed)","47ea1b2f":"@cuda.jit\ndef tile_transpose_conflict_free(a, transposed):\n    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n    # and that `a` is a multiple of these dimensions.\n    \n    # 1) Create 32x32 shared memory array.\n    tile = cuda.shared.array((32, 33), numba_types.int32)\n\n    # Compute offsets into global input array.\n    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n    \n    # 2) Make coalesced read from global memory into shared memory array.\n    # Note the use of local thread indices for the shared memory write,\n    # and global offsets for global memory read.\n    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[y, x]\n\n    # 3) Wait for all threads in the block to finish updating shared memory.\n    cuda.syncthreads()\n    \n    # 4) Calculate transposed location for the shared memory array tile\n    # to be written back to global memory.\n    t_x = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n    t_y = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n\n    # 5) Write back to global memory,\n    # transposing each element within the shared memory array.\n    transposed[t_y, t_x] = tile[cuda.threadIdx.x, cuda.threadIdx.y]","2c311305":"%timeit tile_transpose_conflict_free[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()","f493b3d9":"result = d_transposed.copy_to_host()\nexpected = a.T","bb9014f3":"np.array_equal(result, expected)","535f7081":"![image.png](attachment:70810d3a-d5c9-4682-989d-3843cf3eb977.png)","707f8c21":"Now that you have completed this session you are able to:\n\n* Write CUDA kernels that benefit from coalesced memory access patterns.\n* Work with multi-dimensional grids and thread blocks.\n* Use shared memory to coordinate threads within a block.\n* Use shared memory to facilitate coalesced memory access patterns.\n* Resolve shared memory bank conflicts.","3244f319":"### 2D Matrix Add","7b5c5e20":"In this cell we create an input matrix, as well as a vector for storing the solution, and transfer each of them to the device. We also define the grid and block dimensions to be used when we launch the kernel below. We set an arbitrary row of data to some arbitrary value to facilitate checking for correctness below.","db668c5e":"Here we create a 4096x4096 input matrix `a` as well as a 4096x4096 output matrix `transposed`, and copy them to the device.\n\nWe also define a 2-dimensional grid with 2-dimensional blocks to be used below. Note that we have created a grid with a total number of threads equal to the number of elments in the input matrix.","a23de120":"**Check Correctness**","19ccff45":"Your job is to complete the TODOs in `matrix_add` to correctly sum `a` and `b` into `out`. As a challenge to your understanding of coalesced access patterns, `matrix_add` will accept a `coalesced` boolean indicating whether the access patterns should be coalesced or not. Both modes (coalesced and uncoalesced) should produce correct results, however, you should observe significant speedups below when running with `coalesced` set to `True`.\n\n","9898a4d8":"Here we pass `True` as the `coalesced` value, and observe the performance of the kernel over several runs:","f7a8f0d7":"**This is a 3 Notebook Series on:**\n# Fundamental of Accelerated Computing with CUDA Python\nIn this 3 Notebook Series you will learn about:\n1. [Introduction to CUDA Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/1-introduction-to-cuda-python-with-numba)\n2. [Custom CUDA Kernels in Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/2-custom-cuda-kernels-in-python-with-numba)\n3. [Multidimensional Grids and Shared Memory for CUDA Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/3-multidimensional-grids-shared-memory-for-cuda)","bdbefb19":"If you have completed the refactor, observed it's run time to be less than 840 \u00b5s, and confirmed that it runs correctly, execute the following cells to run the assessment against your kernel definition.","cc995aed":"`col_sums` will use each thread to iterate over a column of data, summing it, and then store its column sum in `sums`. ","e6670cc3":"**Naive Matrix Transpose Kernel**","d2864edb":"**Check Performance**","03a13230":"### Launch Kernel Using Uncoalesced Access","d9cb0685":"The performance of the uncoalesced data access pattern was far worse. Now you will learn why, and how to think about data access patterns in your kernels to obtain high performing kernels.","d4f7511c":"Check the performance of your refactored transpose kernel. You should see a speedup compared to the baseline transpose performance above.","a8ddd74c":"**Check Results**","744477ba":"In this exercise you will implement what was just demonstrated in the presentation by writing a matrix transpose kernel which, using shared memory, makes coalesced reads and writes to the output matrix in global memory.","0fadd015":"## Excercise: Used Shared Memory for Coalesced Reads and Writes With Matrix Transpose","59397a0b":"While this is a significant speedup for only a few lines of code, but you might think that the performance improvement is not as stark as you expected based on earlier performance improvements to use coalesced access patterns. There are 2 main reasons for this:\n\n1. The naive transpose kernel was making coalesced reads, so, your refactored version only optimized half of the global memory access throughout the execution of the kernel.\n2. Your code as written suffers from something called shared memory bank conflicts, a topic to which we will now turn our attention.","dc3a7c8f":"In this cell we define `n` and create a grid with threads equal to `n`. We also create an output vector with length `n`. For the inputs we create vectors of size `stride * n` for reasons that will be made clear below:","3ac8f955":"### Coalesced Reads, Uncoalesced Writes","c39fb473":"Execute the following cell to load the slides, then click on \"Start Slide Show\" to make them full screen.","b9ac3597":"For this exercise you will be asked to write a column sums kernel that uses fully coalesced memory access patterns. To begin you will observe the performance of a row sums kernel that makes uncoalesced memory accesses.","30102497":"In this cell we define 2048x2048 elmement input matrices `a` and `b`, as well as a 2048x2048 0-initialized output matrix. We copy these matrices to the device.\n\nWe also define the 2-dimensional block and grid dimensions to be used below. Note that we are creating a grid with the same number of total threads as there are input and output elements, such that each thread in the grid will calculate the sum for a single element in the output matrix.","fa5f5304":"## Objectives","d56508a0":"The `tile_transpose_conflict_free` kernel is a working matrix transpose kernel which utilizes shared memory so that both reads from and writes to global memory are coalesced. Your job is to refactor the kernel so that it does not suffer from memory bank conflicts.\n\n**Note:** Because this final exercise counts towards certification in the course, a solution will not be provided.","fd240eb1":"**Data Creation**","d30a7e10":"# 3-Multidimensional Grids & Shared Memory for CUDA","45b56f19":"### Check Performance","63d07bba":"Assuming you have written `col_sums` to use coalesced access patterns, you should see a significant (almost 2x) speed up compared to the uncoalesced `row_sums` you ran above:","c385bb43":"So far we have been differentiating between host and device memory, as if device memory were a single kind of memory. But in fact, CUDA has an even more fine-grained [memory hierarchy](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#memory-hierarchy). The device memory we have been utilizing thus far is called **global memory** which is available to any thread or block on the device, can persist for the lifetime of the application, and is a relatively large memory space.\n\nWe will now discuss how to utilize a region of on-chip device memory called **shared memory**. Shared memory is a programmer defined cache of limited size that [depends on the GPU](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#compute-capabilities) being used and is **shared** between all threads in a block. It is a scarce resource, cannot be accessed by threads outside of the block where it was allocated, and does not persist after a kernel finishes executing. Shared memory however has a much higher bandwidth than global memory and can be used to great effect in many kernels, especially to optimize performance.\n\nHere are a few common use cases for shared memory:\n\n * Caching memory read from global memory that will need to be read multiple times within a block.\n * Buffering output from threads so it can be coalesced before writing it back to global memory.\n * Staging data for scatter\/gather operations within a block.","6ca4c97b":"Your job will be to refactor the `transpose` kernel to use shared memory and make both reads to and writes from global memory in a coalesced fashion.","bb1755d8":"** Kernel Definition**","0c619cda":"### Imports","da9212e2":"**Row Sums Performance**","aa230f10":"## Assessment: Resolve Memory Bank Conflicts","55e2ad43":"**Check Correctness**","dc19a753":"### Check Correctness","6a4a2fd7":"### Data Creation","89806f54":"**Imports**","6822e94c":"> _**Footnote**: for additional details about global memory segment size across a variety of devices, and with regards to caching, see [The CUDA Best Practices Guide](https:\/\/docs.nvidia.com\/cuda\/cuda-c-best-practices-guide\/index.html#coalesced-access-to-global-memory)._","abcef5ed":"This kernel will take an input matrix of 0s and write to each of its elements, its (x,y) coordinates within the grid in the format of `X.Y`:","d4bbd5c1":"**Check Performance**","45e30304":"**Write a Transpose Kernel that Uses Shared Memory**","1d5f85e8":"`Please go through the slides first before Moving ahead this will explain you the concept of Global Memory Coalescing`","6da1f285":"### Refactor for Coalesced Reads and Writes","8d1e05a0":"`row_sums` will use each thread to iterate over a row of data, summing it, and then store its row sum in `sums`.","1de42c60":"## Shared Memory","fc90dfbb":"## Summary","0736df83":"## 2 and 3 Dimensional Blocks and Grids","7472909d":"## Presentation: Memory Bank Conflicts","b1e2ed47":"**Imports**","e1930f2f":"Both grids and blocks can be configured to contain a 2 or 3 dimensional collection of blocks or threads, respectively. This is done mostly as a matter of convenience for programmers who often work with 2 or 3 dimensional datasets. Here is a very trivial example to highlight the syntax. You may need to read *both* the kernel definition and its launch before the concept makes sense.","6c3b32ec":"In this cell we create an input matrix, as well as a vector for storing the solution, and transfer each of them to the device. We also define the grid and block dimensions to be used when we launch the kernel below. We set an arbitrary column of data to some arbitrary value to facilitate checking for correctness below.","75e61e95":"**Imports**","e7bef3d6":"## Exercise: Coalesced 2-Dimensional Matrix Add","466169dd":"We will use `numba.types` to define the types of values in shared memory.","caa97352":"Execute the following cell to load the slides, then click on \"Start Slide Show\" to make them full screen.","0ba77734":"### Run the Assessment","c90fdceb":"## Exercise: Column and Row Sums","32670d25":"### Data Creation","70a25553":"As reference, and for performance comparison, here is a naive matrix transpose kernel that makes coalesced reads from input, but uncoalesced writes to output.","619801c6":"### Shared Memory Syntax","c2dc5f9c":"### Data Creation","6cb1aee8":"**Data Creation**","5a7b0251":"## The Problem: Uncoalesced Memory Access Hurts Performance","34ed13f1":"**Imports**","5a367c19":"### Make the Kernel Bank Conflict Free","b6a4d422":"Here we make sure the kernel ran as expected:","5b758e2a":"Complete the TODOs inside the `tile_transpose` kernel definition.\n\nIf you get stuck, feel free to check out [the solution](..\/edit\/solutions\/tile_transpose_solution.py).","95804643":"Now that you can write correct CUDA kernels, and understand the importance of launching grids that give the GPU sufficient opportunity to hide latency, you are going to learn techniques to effectively utilize GPU memory subsystems. These techniques are widely applicable to a variety of CUDA applications, and some of the most important when it comes time to make your CUDA code go fast.\n\nYou are going to begin by learning about memory coalescing. To challenge your ability to reason about memory coalescing, and to expose important details relevent to many CUDA applications, you will then learn about 2-dimensional grids and thread blocks. Next you will learn about a very fast, user-controlled, on-demand memory space called shared memory, and will use shared memory to facilitate memory coalescing where it would not have otherwise been possible. Finally, you will learn about shared memory bank conflicts, which can spoil the performance possibilities of using shared memory, and a technique to address them.","5bc616f4":"**Check Performance**","c92ab237":"Numba provides [functions](https:\/\/numba.pydata.org\/numba-doc\/dev\/cuda\/memory.html#shared-memory-and-thread-synchronization) for allocating shared memory as well as for synchronizing between threads in a block, which is often necessary after parallel threads read from or write to shared memory.\n\nWhen declaring shared memory, you provide the shape of the shared array, as well as its type, using a [Numba type](https:\/\/numba.pydata.org\/numba-doc\/dev\/reference\/types.html#numba-types). **The shape of the array must be a constant value**, and therefore, you cannot use arguments passed into the function, or, provided variables like `numba.cuda.blockDim.x`, or the calculated values of `cuda.griddim`. Here is a convoluted example to demonstrate the syntax with comments pointing out the movement from host memory to global device memory, to shared memory, back to global device memory, and finally back to host memory:","3a259c04":"Execute the following cell to load the slides, then click on \"Start Slide Show\" to make them full screen.","707728c8":"**Data Creation**","317abe7f":"### Launch Kernel Using Coalesced Access","bb954a99":"** Kernel Definition**","6c8132af":"## Presentation: Shared Memory for Memory Coalescing","c82004bb":"If you Gain some knowledge then **Please Upvote**\n\nThanks to NVIDIA Deeplearning Institute for such a Good Content.","b0d72157":"Confirm your kernel is working as expected.","fa34c486":"** Run Kernel**","467d88c5":"Here we make sure the kernel ran as expected:","2fa322f5":"**Check for Correctness**","2041069f":"Assuming you have correctly resolved the bank conflicts, this kernel should run significantly faster than both the naive transpose kernel, and, the shared memory (with bank conflicts) transpose kernel. In order to pass the assessment, your kernel will need to run on average in less than 840 \u00b5s.\n\nThe first value printed by running the following cell will give you the average run time of your kernel.","a30ff788":"### Imports","18fc8d3f":"The following kernel takes an input vector, where each thread will first write one element of the vector to shared memory, and then, after syncing such that all elements have been written to shared memory, will write one element out of shared memory into the swapped output vector.\n\nWorth noting is that each thread will be writing a swapped value from shared memory that was written into shared memory by another thread.","45bf733b":"Before you learn the details about what **coalesced memory access** is, run the following cells to observe the performance implications for a seemingly trivial change to the data access pattern within a kernel.","1f747449":"In this cell we pass `False`, to observe the perfomance of the uncoalesced data access pattern for `add_experiment`:","6a9a54b5":"### Results","eb5cb700":"Run both cells below to launch `matrix_add` with both the coalesced and uncoalesced access patterns you wrote into it, and observe the performance difference. Additional cells have been provided to confirm the correctness of your kernel.","4faf04da":"By the time you complete this section, you will be able to:\n* Write CUDA kernels that benefit from coalesced memory access patterns.\n* Work with multi-dimensional grids and thread blocks.\n* Use shared memory to coordinate threads within a block.\n* Use shared memory to facilitate coalesced memory access patterns.\n* Resolve shared memory bank conflicts.","e5aee609":"**Swap Elements Using Shared Memory**","b43752d0":"**Data Creation**","0d7ab6ed":"### Kernel Definition","e7521a6c":"### Imports","2ad4a6c1":"### Check Performance","7575eb2b":"**Uncoalesced**","c6d0695e":"**Coalesced**","ee5f399c":"In order to pass the assessment, your kernel also needs to work correctly. Run the following 2 cells to confirm this is true.","3d44d336":"In `add_experiment`, every thread in the grid will add an item in `a`, and an item in `b` and write the result to `out`. The kernel has been written such that we can pass a `coalesced` value of either `True` or `False` to affect how it indexes into the `a` and `b` vectors. You will see the performance comparison of the two modes below.","43fba843":"As a final exercise, and to get credit towards a certificate in the course for this final section of the workshop, you will refactor the transpose kernel utilizing shared memory to be shared memory bank conflict free.","e01db5b6":"### Row Sums","e4eb256a":"This kernel correctly transposes `a`, writing the transposition to `transposed`. It makes reads from `a` in a coalesced fashion, however, its writes to `transposed` are uncoalesced.","ef96ba91":"### Column Sums","ded303b4":"**Check Correctness**","153e86ba":"**Data Creation**","c637ff8c":"## Presentation: Global Memory Coalescing","dcc17e94":"**Imports**","8bf3e2db":"### Why Such a Small Improvement?"}}