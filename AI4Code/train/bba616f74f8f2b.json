{"cell_type":{"6dbab431":"code","e82382ff":"code","820325a0":"code","42f0d5d3":"code","18a4b8ce":"code","e125eaf2":"code","373ed4e0":"code","d5082d20":"code","9890ec15":"code","1156e3a2":"code","516c2177":"code","684d67c7":"markdown"},"source":{"6dbab431":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  MinMaxScaler, LabelEncoder","e82382ff":"# \u7b80\u5355\u5904\u7406\u7279\u5f81\uff0c\u5305\u62ec\u586b\u5145\u7f3a\u5931\u503c\uff0c\u6570\u503c\u5904\u7406\uff0c\u7c7b\u522b\u7f16\u7801\ndef data_process(data_df, dense_features, sparse_features):\n    data_df[dense_features] = data_df[dense_features].fillna(0.0)\n    for f in dense_features:\n        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)\n        \n    data_df[sparse_features] = data_df[sparse_features].fillna(\"-1\")\n    for f in sparse_features:\n        lbe = LabelEncoder()\n        data_df[f] = lbe.fit_transform(data_df[f])\n    \n    return data_df[dense_features + sparse_features]","820325a0":"def build_input_layers(feature_columns):\n    # \u6784\u5efaInput\u5c42\u5b57\u5178\uff0c\u5e76\u4ee5dense\u548csparse\u4e24\u7c7b\u5b57\u5178\u7684\u5f62\u5f0f\u8fd4\u56de\n    dense_input_dict, sparse_input_dict = {}, {}\n\n    for fc in feature_columns:\n        if isinstance(fc, SparseFeat):\n            sparse_input_dict[fc.name] = Input(shape=(1, ), name=fc.name)\n        elif isinstance(fc, DenseFeat):\n            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)\n        \n    return dense_input_dict, sparse_input_dict","42f0d5d3":"def build_embedding_layers(feature_columns, input_layers_dict, is_linear):\n    # \u5b9a\u4e49\u4e00\u4e2aembedding\u5c42\u5bf9\u5e94\u7684\u5b57\u5178\n    embedding_layers_dict = dict()\n    \n    # \u5c06\u7279\u5f81\u4e2d\u7684sparse\u7279\u5f81\u7b5b\u9009\u51fa\u6765\n    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n    \n    # \u5982\u679c\u662f\u7528\u4e8e\u7ebf\u6027\u90e8\u5206\u7684embedding\u5c42\uff0c\u5176\u7ef4\u5ea6\u4e3a1\uff0c\u5426\u5219\u7ef4\u5ea6\u5c31\u662f\u81ea\u5df1\u5b9a\u4e49\u7684embedding\u7ef4\u5ea6\n    if is_linear:\n        for fc in sparse_feature_columns:\n            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, 1, name='1d_emb_' + fc.name)\n    else:\n        for fc in sparse_feature_columns:\n            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='kd_emb_' + fc.name)\n    \n    return embedding_layers_dict","18a4b8ce":"def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):\n    # \u5c06\u6240\u6709\u7684dense\u7279\u5f81\u7684Input\u5c42\uff0c\u7136\u540e\u7ecf\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5f97\u5230dense\u7279\u5f81\u7684logits\n    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))\n    dense_logits_output = Dense(1)(concat_dense_inputs)\n    \n    # \u83b7\u53d6linear\u90e8\u5206sparse\u7279\u5f81\u7684embedding\u5c42\uff0c\u8fd9\u91cc\u4f7f\u7528embedding\u7684\u539f\u56e0\u662f\uff1a\n    # \u5bf9\u4e8elinear\u90e8\u5206\u76f4\u63a5\u5c06\u7279\u5f81\u8fdb\u884conehot\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u5f53\u7ef4\u5ea6\u7279\u522b\u5927\u7684\u65f6\u5019\uff0c\u8ba1\u7b97\u6bd4\u8f83\u6162\n    # \u4f7f\u7528embedding\u5c42\u7684\u597d\u5904\u5c31\u662f\u53ef\u4ee5\u901a\u8fc7\u67e5\u8868\u7684\u65b9\u5f0f\u83b7\u53d6\u5230\u54ea\u4e9b\u975e\u96f6\u7684\u5143\u7d20\u5bf9\u5e94\u7684\u6743\u91cd\uff0c\u7136\u540e\u5728\u5c06\u8fd9\u4e9b\u6743\u91cd\u76f8\u52a0\uff0c\u6548\u7387\u6bd4\u8f83\u9ad8\n    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)\n    \n    # \u5c06\u4e00\u7ef4\u7684embedding\u62fc\u63a5\uff0c\u6ce8\u610f\u8fd9\u91cc\u9700\u8981\u4f7f\u7528\u4e00\u4e2aFlatten\u5c42\uff0c\u4f7f\u7ef4\u5ea6\u5bf9\u5e94\n    sparse_1d_embed = []\n    for fc in sparse_feature_columns:\n        feat_input = sparse_input_dict[fc.name]\n        embed = Flatten()(linear_embedding_layers[fc.name](feat_input))\n        sparse_1d_embed.append(embed)\n\n    # embedding\u4e2d\u67e5\u8be2\u5f97\u5230\u7684\u6743\u91cd\u5c31\u662f\u5bf9\u5e94onehot\u5411\u91cf\u4e2d\u4e00\u4e2a\u4f4d\u7f6e\u7684\u6743\u91cd\uff0c\u6240\u4ee5\u540e\u9762\u4e0d\u7528\u518d\u63a5\u4e00\u4e2a\u5168\u8fde\u63a5\u4e86\uff0c\u672c\u8eab\u4e00\u7ef4\u7684embedding\u5c31\u76f8\u5f53\u4e8e\u5168\u8fde\u63a5\n    # \u53ea\u4e0d\u8fc7\u662f\u8fd9\u91cc\u7684\u8f93\u5165\u7279\u5f81\u53ea\u67090\u548c1\uff0c\u6240\u4ee5\u76f4\u63a5\u5411\u975e\u96f6\u5143\u7d20\u5bf9\u5e94\u7684\u6743\u91cd\u76f8\u52a0\u5c31\u7b49\u540c\u4e8e\u8fdb\u884c\u4e86\u5168\u8fde\u63a5\u64cd\u4f5c(\u975e\u96f6\u5143\u7d20\u90e8\u5206\u4e58\u7684\u662f1)\n    sparse_logits_output = Add()(sparse_1d_embed)\n\n    # \u6700\u7ec8\u5c06dense\u7279\u5f81\u548csparse\u7279\u5f81\u5bf9\u5e94\u7684logits\u76f8\u52a0\uff0c\u5f97\u5230\u6700\u7ec8linear\u7684logits\n    linear_part = Add()([dense_logits_output, sparse_logits_output])\n    return linear_part","e125eaf2":"class BiInteractionPooling(Layer):\n    def __init__(self):\n        super(BiInteractionPooling, self).__init__()\n\n    def call(self, inputs):\n        # \u4f18\u5316\u540e\u7684\u516c\u5f0f\u4e3a\uff1a 0.5 * \uff08\u548c\u7684\u5e73\u65b9-\u5e73\u65b9\u7684\u548c\uff09  =>> B x k\n        concated_embeds_value = inputs # B x n x k\n\n        square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keepdims=False)) # B x k\n        sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keepdims=False) # B x k\n        cross_term = 0.5 * (square_of_sum - sum_of_square) # B x k\n\n        return cross_term\n\n    def compute_output_shape(self, input_shape):\n        return (None, input_shape[2])","373ed4e0":"def get_bi_interaction_pooling_output(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):\n    # \u53ea\u8003\u8651sparse\u7684\u4e8c\u9636\u4ea4\u53c9\uff0c\u5c06\u6240\u6709\u7684embedding\u62fc\u63a5\u5230\u4e00\u8d77\n    # \u8fd9\u91cc\u5728\u5b9e\u9645\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u53ea\u4f1a\u5c06\u90a3\u4e9b\u975e\u96f6\u5143\u7d20\u5bf9\u5e94\u7684embedding\u62fc\u63a5\u5230\u4e00\u8d77\n    # \u5e76\u4e14\u5c06\u975e\u96f6\u5143\u7d20\u5bf9\u5e94\u7684embedding\u62fc\u63a5\u5230\u4e00\u8d77\u672c\u8d28\u4e0a\u76f8\u5f53\u4e8e\u5df2\u7ecf\u4e58\u4e86x, \u56e0\u4e3ax\u4e2d\u7684\u503c\u662f1(\u516c\u5f0f\u4e2d\u7684x)\n    sparse_kd_embed = []\n    for fc in sparse_feature_columns:\n        feat_input = sparse_input_dict[fc.name]\n        _embed = dnn_embedding_layers[fc.name](feat_input) # B x 1 x k\n        sparse_kd_embed.append(_embed)\n\n    # \u5c06\u6240\u6709sparse\u7684embedding\u62fc\u63a5\u8d77\u6765\uff0c\u5f97\u5230 (n, k)\u7684\u77e9\u9635\uff0c\u5176\u4e2dn\u4e3a\u7279\u5f81\u6570\uff0ck\u4e3aembedding\u5927\u5c0f\n    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x n x k\n    \n    pooling_out = BiInteractionPooling()(concat_sparse_kd_embed)\n\n    return pooling_out","d5082d20":"def get_dnn_logits(pooling_out):\n    # dnn\u5c42\uff0c\u8fd9\u91cc\u7684Dropout\u53c2\u6570\uff0cDense\u4e2d\u7684\u53c2\u6570\u90fd\u53ef\u4ee5\u81ea\u5df1\u8bbe\u5b9a, \u8bba\u6587\u4e2d\u8fd8\u8bf4\u4f7f\u7528\u4e86BN, \u4f46\u662f\u4e2a\u4eba\u89c9\u5f97BN\u548cdropout\u540c\u65f6\u4f7f\u7528\n    # \u53ef\u80fd\u4f1a\u51fa\u73b0\u4e00\u4e9b\u95ee\u9898\uff0c\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e9b\uff0c\u8fd9\u91cc\u5c31\u5148\u4e0d\u52a0\u4e0a\u4e86\n    dnn_out = Dropout(0.5)(Dense(1024, activation='relu')(pooling_out))  \n    dnn_out = Dropout(0.3)(Dense(512, activation='relu')(dnn_out))\n    dnn_out = Dropout(0.1)(Dense(256, activation='relu')(dnn_out))\n\n    dnn_logits = Dense(1)(dnn_out)\n\n    return dnn_logits","9890ec15":"from collections import namedtuple\n\n# \u4f7f\u7528\u5177\u540d\u5143\u7ec4\u5b9a\u4e49\u7279\u5f81\u6807\u8bb0\nSparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])\nDenseFeat = namedtuple('DenseFeat', ['name', 'dimension'])\nVarLenSparseFeat = namedtuple('VarLenSparseFeat', ['name', 'vocabulary_size', 'embedding_dim', 'maxlen'])","1156e3a2":"def NFM(linear_feature_columns, dnn_feature_columns):\n    # \u6784\u5efa\u8f93\u5165\u5c42\uff0c\u5373\u6240\u6709\u7279\u5f81\u5bf9\u5e94\u7684Input()\u5c42\uff0c\u8fd9\u91cc\u4f7f\u7528\u5b57\u5178\u7684\u5f62\u5f0f\u8fd4\u56de\uff0c\u65b9\u4fbf\u540e\u7eed\u6784\u5efa\u6a21\u578b\n    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)\n\n    # \u5c06linear\u90e8\u5206\u7684\u7279\u5f81\u4e2dsparse\u7279\u5f81\u7b5b\u9009\u51fa\u6765\uff0c\u540e\u9762\u7528\u6765\u505a1\u7ef4\u7684embedding\n    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))\n\n    # \u6784\u5efa\u6a21\u578b\u7684\u8f93\u5165\u5c42\uff0c\u6a21\u578b\u7684\u8f93\u5165\u5c42\u4e0d\u80fd\u662f\u5b57\u5178\u7684\u5f62\u5f0f\uff0c\u5e94\u8be5\u5c06\u5b57\u5178\u7684\u5f62\u5f0f\u8f6c\u6362\u6210\u5217\u8868\u7684\u5f62\u5f0f\n    # \u6ce8\u610f\uff1a\u8fd9\u91cc\u5b9e\u9645\u7684\u8f93\u5165\u4e0eInput()\u5c42\u7684\u5bf9\u5e94\uff0c\u662f\u901a\u8fc7\u6a21\u578b\u8f93\u5165\u65f6\u5019\u7684\u5b57\u5178\u6570\u636e\u7684key\u4e0e\u5bf9\u5e94name\u7684Input\u5c42\n    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())\n\n    # linear_logits\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662fdense\u7279\u5f81\u7684logits\u548csparse\u7279\u5f81\u7684logits\n    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)\n\n    # \u6784\u5efa\u7ef4\u5ea6\u4e3ak\u7684embedding\u5c42\uff0c\u8fd9\u91cc\u4f7f\u7528\u5b57\u5178\u7684\u5f62\u5f0f\u8fd4\u56de\uff0c\u65b9\u4fbf\u540e\u9762\u642d\u5efa\u6a21\u578b\n    # embedding\u5c42\u7528\u6237\u6784\u5efaFM\u4ea4\u53c9\u90e8\u5206\u548cDNN\u7684\u8f93\u5165\u90e8\u5206\n    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)\n\n    # \u5c06\u8f93\u5165\u5230dnn\u4e2d\u7684sparse\u7279\u5f81\u7b5b\u9009\u51fa\u6765\n    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))\n\n    pooling_output = get_bi_interaction_pooling_output(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # B x (n(n-1)\/2)\n    \n    # \u8bba\u6587\u4e2d\u8bf4\u5230\u5728\u6c60\u5316\u4e4b\u540e\u52a0\u4e0a\u4e86BN\u64cd\u4f5c\n    pooling_output = BatchNormalization()(pooling_output)\n\n    dnn_logits = get_dnn_logits(pooling_output)\n    \n    # \u5c06linear,dnn\u7684logits\u76f8\u52a0\u4f5c\u4e3a\u6700\u7ec8\u7684logits\n    output_logits = Add()([linear_logits, dnn_logits])\n\n    # \u8fd9\u91cc\u7684\u6fc0\u6d3b\u51fd\u6570\u4f7f\u7528sigmoid\n    output_layers = Activation(\"sigmoid\")(output_logits)\n\n    model = Model(input_layers, output_layers)\n    return model","516c2177":"# \u8bfb\u53d6\u6570\u636e\ndata = pd.read_csv('..\/input\/criteo-sample\/criteo_sample.txt')\n\n# \u5212\u5206dense\u548csparse\u7279\u5f81\ncolumns = data.columns.values\ndense_features = [feat for feat in columns if 'I' in feat]\nsparse_features = [feat for feat in columns if 'C' in feat]\n\n# \u7b80\u5355\u7684\u6570\u636e\u9884\u5904\u7406\ntrain_data = data_process(data, dense_features, sparse_features)\ntrain_data['label'] = data['label']\n\n# \u5c06\u7279\u5f81\u5206\u7ec4\uff0c\u5206\u6210linear\u90e8\u5206\u548cdnn\u90e8\u5206(\u6839\u636e\u5b9e\u9645\u573a\u666f\u8fdb\u884c\u9009\u62e9)\uff0c\u5e76\u5c06\u5206\u7ec4\u4e4b\u540e\u7684\u7279\u5f81\u505a\u6807\u8bb0\uff08\u4f7f\u7528DenseFeat, SparseFeat\uff09\nlinear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n                        for feat in dense_features]\n\ndnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n                        for feat in dense_features]\n\n# \u6784\u5efaNFM\u6a21\u578b\nhistory = NFM(linear_feature_columns, dnn_feature_columns)\nhistory.summary()\nhistory.compile(optimizer=\"adam\", \n            loss=\"binary_crossentropy\", \n            metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n\n# \u5c06\u8f93\u5165\u6570\u636e\u8f6c\u5316\u6210\u5b57\u5178\u7684\u5f62\u5f0f\u8f93\u5165\ntrain_model_input = {name: data[name] for name in dense_features + sparse_features}\n# \u6a21\u578b\u8bad\u7ec3\nhistory.fit(train_model_input, train_data['label'].values,\n        batch_size=64, epochs=5, validation_split=0.2, )","684d67c7":"\u4e00.NFM(Neural Factorization Machines)  \n\u4f20\u7edf\u7684FM\u6a21\u578b\u4ec5\u5c40\u9650\u4e0e\u7ebf\u6027\u8868\u8fbe\u7684\u4e8c\u9636\u4ea4\u4e92\uff0c\u9488\u5bf9FM\u7684\u8fd9\u70b9\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e00\u79cd\u5c06FM\u878d\u5408\u8fdbDNN\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165**\u7279\u5f81\u4ea4\u53c9\u6c60\u5316\u5c42**\u7684\u7ed3\u6784\n\u8fd9\u6837\u7ec4\u5408\u4e86FM\u7684\u5efa\u6a21\u4f4e\u9636\u7279\u5f81\u4ea4\u4e92\u80fd\u529b\u548cDNN\u5b66\u4e60\u9ad8\u9636\u7279\u5f81\u4ea4\u53c9\u548c\u975e\u7ebf\u6027\u7684\u80fd\u529b\u3002\n$$ \\hat{y}{N F M}(\\mathbf{x})=w{0}+\\sum_{i=1}^{n} w_{i} x_{i}+f(\\mathbf{x}) $$ \n    $$ \\hat{y}{F M}(\\mathbf{x})=w{0}+\\sum_{i=1}^{n} w_{i} x_{i}+ \\sum_{i=1}^{N} \\sum_{j=i+1}^{N} v_{i}^{T} v_{j}\\cdot x_{i}x{j} $$ \nf(x)\u7684\u90e8\u5206\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc  \n![image.png](attachment:image.png)\n\u8fd9\u4e2a\u7ed3\u6784\u548cPNN\u975e\u5e38\u50cf\uff0c\u53ea\u4e0d\u8fc7\u90a3\u91cc\u662f\u4e00\u4e2aproduct_layer\uff0c \u800c\u8fd9\u91cc\u6362\u6210\u4e86Bi-Interaction Pooling\u4e86\uff0c \u8fd9\u4e2a\u4e5f\u662fNFM\u7684\u6838\u5fc3\u7ed3\u6784\u4e86\u3002"}}