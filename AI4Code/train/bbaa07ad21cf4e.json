{"cell_type":{"cfdb9fea":"code","096e32c8":"code","85729b6b":"code","3699f9ee":"code","d1a6aa22":"code","0d669c17":"code","144ce0b7":"code","e840e24d":"code","f812401a":"code","405c9e14":"code","ac0d5340":"code","bbee2470":"code","2c524446":"code","c925c466":"code","26b0d77c":"code","fac0d42b":"code","abcb7623":"code","025a243a":"code","c0803b31":"code","6b6118e4":"code","598948a8":"code","738f408b":"code","f1c70b10":"code","d86064ff":"code","75726c83":"code","0ee1d349":"code","c544e5fd":"code","8635d509":"code","8292ebae":"code","6ed0f404":"code","78c6b33f":"code","7ed17033":"code","c6b17ab0":"code","c32d98b4":"code","7fb49f5c":"code","21542494":"code","55ef2450":"code","7536fcdd":"code","aed94136":"code","f9d76d38":"code","9f544f9e":"code","c7e691c1":"code","a1c09ba2":"code","dd6a17aa":"code","19b4896c":"code","4f3bb8fb":"code","ea6269ba":"code","a085b034":"code","6c051975":"code","a7de9ce1":"code","f6841a22":"code","dfadba9d":"code","1e0c1f90":"code","8f12e9f9":"code","926efe20":"code","5d40d2db":"code","e42d1430":"code","4667aaa1":"code","0db56b39":"code","557f65d0":"code","24a443d9":"code","1e977fdf":"code","703a0f35":"code","28576e4d":"code","db1b9153":"code","069ec269":"code","73edd618":"code","764fc7d1":"code","a695898e":"code","b41b32f5":"code","d7ccedf0":"code","997e9126":"code","86f3f401":"code","d998b444":"code","edbc0c40":"code","6bdadc8b":"code","632801e7":"code","f357d9ab":"code","5fa3b105":"markdown","fa8a3d37":"markdown","b7bf909c":"markdown","ffdc72f6":"markdown","33a4540b":"markdown","bc9b316d":"markdown","180e5137":"markdown","68705973":"markdown","cc65a46c":"markdown","88d7a321":"markdown","ad3a1c76":"markdown","471e93e5":"markdown","29eb6624":"markdown","83942ddf":"markdown","849cc0a2":"markdown","5b62b315":"markdown","c6b5aec1":"markdown","017d4a0f":"markdown","460972ef":"markdown","706611ed":"markdown","57cfaec0":"markdown","56de41ad":"markdown","12f46210":"markdown","4bf28a65":"markdown","73d273c8":"markdown","b904c367":"markdown","bacc1c60":"markdown"},"source":{"cfdb9fea":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport nltk\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\n#from sklearn.metrics.classification import accuracy_score, log_loss\n# from sklearn.feature_extraction.text import Tfiall_dataVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n#from sklearn.cross_validation import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\n#from sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\nfrom IPython.display import Image\n\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom gensim import corpora, models, similarities \n\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn import tree\nfrom lightgbm import LGBMClassifier\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport torch\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\n\nfrom transformers import BertForSequenceClassification\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","096e32c8":"data_variants = pd.read_csv('..\/input\/cancerdata\/training_variants')\ndata_text =pd.read_csv(\"..\/input\/cancerdata\/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\ntest_variants = pd.read_csv('..\/input\/cancerdata\/test_variants')\ntest_text =pd.read_csv(\"..\/input\/cancerdata\/test_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\ncleaned_text_data = pd.read_csv('..\/input\/textcol-new\/textcol_new.csv')\ncleaned_test_text_data = pd.read_csv('..\/input\/textcol-test\/textcol_test.csv')","85729b6b":"all_data = pd.merge(data_variants, data_text,on='ID', how='left')\nall_data.head()","3699f9ee":"all_test = pd.merge(test_variants, test_text,on='ID', how='left')","d1a6aa22":"print('There are {} rows and {} columns in train'.format(all_data.shape[0],all_data.shape[1]))\nprint('There are {} rows and {} columns in test'.format(all_test.shape[0],all_test.shape[1]))","0d669c17":"del test_text\ndel test_variants\ndel data_variants\n#del data_text","144ce0b7":"all_data.info()","e840e24d":"all_data['Class'].value_counts()","f812401a":"plt.figure(figsize=(16,5))\nsns.countplot(y='Class',data=all_data)\nplt.gca().xaxis.tick_bottom()\nplt.title('Data count by Class')","405c9e14":"def create_corpus(data):\n    corpus=[]\n    \n    for x in data.dropna(subset=['TEXT'])['TEXT'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","ac0d5340":"stop=list(stopwords.words('english'))\nstop.extend(['The','the','In','also','fig','Figure','et','al.,','al', 'figure','also', 'et '])","bbee2470":"corpus=create_corpus(all_data)\n\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\n\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","2c524446":"plt.figure(figsize=(16,5))\nsns.barplot(x=y,y=x)","c925c466":"most=counter.most_common()\nx_stop=[]\ny_stop=[]\n\nfor word,count in most[:20]:\n    if (word in stop) :\n        x_stop.append(word)\n        y_stop.append(count)","26b0d77c":"plt.figure(figsize=(16,5))\nsns.barplot(x=y_stop,y=x_stop)","fac0d42b":"avg = 0\nfor i in all_data.index:\n    if i in all_data[(all_data['TEXT'].isnull())].index:\n        continue\n    else:\n        avg+=len(all_data['TEXT'][i].split(' '))\nprint('Average number of words in each text:',avg\/3316)","abcb7623":"def initial_clean(text):\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\n\ndef remove_stop_words(text):\n    return [word for word in text if word not in stop]\n\nlemmatizer = WordNetLemmatizer()\ndef stem_words(text):\n    try:\n        text = [lemmatizer.lemmatize(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: # the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    if type(text) is not float:\n        return ' '.join(stem_words(remove_stop_words(initial_clean(text))))\n    else:\n        return text","025a243a":"from tqdm import tqdm\ntqdm.pandas(desc=\"my bar!\")\nk = all_data['TEXT'].progress_apply(apply_all)\nall_data['TEXT'] = k\nk.to_csv('.\/textcol_new.csv')\ndel k","c0803b31":"cleaned_text_data = pd.read_csv('.\/textcol_new.csv')","6b6118e4":"print(\"cleaned: \\n\",cleaned_text_data['TEXT'][1][:257])\nprint(\"\\n\")\nprint(\"orignal: \\n\",all_data['TEXT'][1][:324])","598948a8":"all_data['TEXT'] = cleaned_text_data['TEXT']\nall_test['TEXT'] = cleaned_test_text_data['TEXT']","738f408b":"del cleaned_text_data\ndel cleaned_test_text_data","f1c70b10":"corpus=create_corpus(all_data)","d86064ff":"def get_top_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","75726c83":"top_bigrams=get_top_bigrams(all_data.dropna(subset=['TEXT'])['TEXT'])[:10]","0ee1d349":"plt.figure(figsize=(16,5))\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x)","c544e5fd":"def get_top_trigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","8635d509":"top_trigrams=get_top_trigrams(all_data.dropna(subset=['TEXT'])['TEXT'])[:10]","8292ebae":"plt.figure(figsize=(16,5))\nx,y=map(list,zip(*top_trigrams))\nsns.barplot(x=y,y=x)","6ed0f404":"def LDA(gen, var):\n    h = all_data[(all_data['Gene']==gen) | (all_data['Variation']==var)]['TEXT'].values\n    corpus = [x for x in h if str(x) != 'nan']\n    stoplist = stopwords.words('english') + list(punctuation)\n    texts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n    dictionary = corpora.Dictionary(texts)\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    tfidf = models.TfidfModel(corpus)\n    corpus_tfiall_data = tfidf[corpus] \n    total_topics = 2\n    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n    corpus_lda = lda[corpus_tfiall_data]\n    k = lda.show_topics(total_topics,100)\n    return k\ndef final_text(gen,var):\n    h = LDA(gen,var)\n    reg = re.compile('\"(.*?)\"')\n    ans = reg.findall(h[0][1])\n    ans1 = reg.findall(h[1][1])\n    ans.extend(ans1)\n    fin = ' '.join(ans)\n    return fin\n\nfor i in tqdm(all_data.loc[all_data['TEXT'].isnull()].index):\n    all_data['TEXT'][i] = final_text(all_data['Gene'][i], all_data['Variation'][i])\nfor i in tqdm(all_test.loc[all_test['TEXT'].isnull()].index):\n    all_test['TEXT'][i] = final_text(all_test['Gene'][i], all_test['Variation'][i])","78c6b33f":"all_data.isnull().sum()","7ed17033":"del all_test","c6b17ab0":"string = data_text['TEXT'].dropna().to_list()\nstring = ' '.join(string)","c32d98b4":"%%time\nfrom wordcloud import WordCloud, STOPWORDS\n# Generate word cloud\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', \n                      collocations=False, stopwords = STOPWORDS).generate(string)","7fb49f5c":"def plot_cloud(wordcloud):\n    plt.figure(figsize=(40, 30))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n    \nplot_cloud(wordcloud)","21542494":"del string","55ef2450":"y_true = all_data['Class'].values\nX_train, test_df, y_train, y_test = train_test_split(all_data, y_true, stratify = y_true, test_size=0.2)","7536fcdd":"train_df, cv_df, y_train, y_cv = train_test_split(X_train,y_train,stratify = y_train, test_size=0.2 )","aed94136":"print('Data points in train data:', train_df.shape[0])\nprint('Data points in test data:', test_df.shape[0])\nprint('Data points in cross validation data:', cv_df.shape[0])","f9d76d38":"vectorizer = TfidfVectorizer()\ntrain_gene_feature_onehotCoding =  vectorizer.fit_transform(train_df['Gene'])\ntest_gene_feature_onehotCoding  =  vectorizer.transform(test_df['Gene'])\ncv_gene_feature_onehotCoding    =  vectorizer.transform(cv_df['Gene'])","9f544f9e":"train_variation_feature_onehotCoding =  vectorizer.fit_transform(train_df['Variation'])\ntest_variation_feature_onehotCoding  =  vectorizer.transform(test_df['Variation'])\ncv_variation_feature_onehotCoding    =  vectorizer.transform(cv_df['Variation'])","c7e691c1":"text_vectorizer = TfidfVectorizer(min_df=3)\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\ntest_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\ncv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","a1c09ba2":"del vectorizer","dd6a17aa":"train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(cv_df['Class']))","19b4896c":"test_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs\/rand_probs.sum())[0])\n\ncv_log_loss = round(log_loss(y_cv,cv_predicted_y, eps=1e-15),2)\n\nprint(\"Log loss on Cross Validation Data using Random Model\",cv_log_loss)\n\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs\/rand_probs.sum())[0])\ntest_log_loss = round(log_loss(y_test,test_predicted_y, eps=1e-15),2)\n\nprint(\"Log loss on Test Data using Random Model\",test_log_loss)","4f3bb8fb":"def LoglossWithFeat(train_feat_hotencode,cv_feat_hotencode):\n    clf = SGDClassifier(alpha=0.1, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_feat_hotencode, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_feat_hotencode, train_y)\n    predict_y = sig_clf.predict_proba(cv_feat_hotencode)\n    return log_loss(cv_y, predict_y, labels=clf.classes_, eps=1e-15)","ea6269ba":"print(\"Log loss for Gene feature:\",LoglossWithFeat(train_gene_feature_onehotCoding, cv_gene_feature_onehotCoding))\nprint(\"Log loss for Text feature:\", LoglossWithFeat(train_text_feature_onehotCoding, cv_text_feature_onehotCoding))\nprint(\"Log loss for Variant feature:\", LoglossWithFeat(train_variation_feature_onehotCoding, cv_variation_feature_onehotCoding))\n","a085b034":"del train_gene_feature_onehotCoding,train_variation_feature_onehotCoding\ndel test_gene_feature_onehotCoding,test_variation_feature_onehotCoding\ndel cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding\ndel train_gene_var_onehotCoding, train_text_feature_onehotCoding\ndel test_gene_var_onehotCoding, test_text_feature_onehotCoding\ndel cv_gene_var_onehotCoding, cv_text_feature_onehotCoding","6c051975":"eval_dict = {}","a7de9ce1":"%%time\nclf = MultinomialNB(alpha=0.1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs_train = sig_clf.predict(train_x_onehotCoding)    \nsig_clf_probs_test = sig_clf.predict(test_x_onehotCoding)    \nsig_clf_probs_cv = sig_clf.predict(cv_x_onehotCoding)    \nsig_clf_probs_train_l = sig_clf.predict_proba(train_x_onehotCoding)    \nsig_clf_probs_test_l = sig_clf.predict_proba(test_x_onehotCoding)    \nsig_clf_probs_cv_l = sig_clf.predict_proba(cv_x_onehotCoding)    ","f6841a22":"print(classification_report(train_y,sig_clf_probs_train))\nprint(classification_report(test_y,sig_clf_probs_test))\nprint(classification_report(cv_y,sig_clf_probs_cv))\neval_dict['Naive Bayes'] = log_loss(train_y, sig_clf_probs_train_l)\n\ndel clf, sig_clf, sig_clf_probs_train, sig_clf_probs_test, sig_clf_probs_cv, sig_clf_probs_train_l, sig_clf_probs_test_l, sig_clf_probs_cv_l","dfadba9d":"%%time\nclf = RandomForestClassifier(n_estimators=1500, max_depth = 10)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs_train = sig_clf.predict(train_x_onehotCoding)    \nsig_clf_probs_test = sig_clf.predict(test_x_onehotCoding)    \nsig_clf_probs_cv = sig_clf.predict(cv_x_onehotCoding)    \nsig_clf_probs_train_l = sig_clf.predict_proba(train_x_onehotCoding)    \nsig_clf_probs_test_l = sig_clf.predict_proba(test_x_onehotCoding)    \nsig_clf_probs_cv_l = sig_clf.predict_proba(cv_x_onehotCoding)    ","1e0c1f90":"print(classification_report(train_y,sig_clf_probs_train))\nprint(classification_report(test_y,sig_clf_probs_test))\nprint(classification_report(cv_y,sig_clf_probs_cv))\neval_dict['Random Forest'] = log_loss(train_y, sig_clf_probs_train_l)\n\ndel clf, sig_clf, sig_clf_probs_train, sig_clf_probs_test, sig_clf_probs_cv, sig_clf_probs_train_l, sig_clf_probs_test_l, sig_clf_probs_cv_l","8f12e9f9":"%%time\nclf = SGDClassifier(class_weight='balanced', alpha=0.001, penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs_train = sig_clf.predict(train_x_onehotCoding)    \nsig_clf_probs_test = sig_clf.predict(test_x_onehotCoding)    \nsig_clf_probs_cv = sig_clf.predict(cv_x_onehotCoding)  \nsig_clf_probs_train_l = sig_clf.predict_proba(train_x_onehotCoding)    \nsig_clf_probs_test_l = sig_clf.predict_proba(test_x_onehotCoding)    \nsig_clf_probs_cv_l = sig_clf.predict_proba(cv_x_onehotCoding)    ","926efe20":"print(classification_report(train_y,sig_clf_probs_train))\nprint(classification_report(test_y,sig_clf_probs_test))\nprint(classification_report(cv_y,sig_clf_probs_cv))\neval_dict['Logistic Regression with weightbalanced'] = log_loss(train_y, sig_clf_probs_train_l)\n\ndel clf, sig_clf, sig_clf_probs_train, sig_clf_probs_test, sig_clf_probs_cv, sig_clf_probs_train_l, sig_clf_probs_test_l, sig_clf_probs_cv_l","5d40d2db":"%%time\nclf = tree.DecisionTreeClassifier()\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs_train = sig_clf.predict(train_x_onehotCoding)    \nsig_clf_probs_test = sig_clf.predict(test_x_onehotCoding)    \nsig_clf_probs_cv = sig_clf.predict(cv_x_onehotCoding)    \nsig_clf_probs_train_l = sig_clf.predict_proba(train_x_onehotCoding)    \nsig_clf_probs_test_l = sig_clf.predict_proba(test_x_onehotCoding)    \nsig_clf_probs_cv_l = sig_clf.predict_proba(cv_x_onehotCoding)    ","e42d1430":"print(classification_report(train_y,sig_clf_probs_train))\nprint(classification_report(test_y,sig_clf_probs_test))\nprint(classification_report(cv_y,sig_clf_probs_cv))\neval_dict['Decision Tree'] = log_loss(train_y, sig_clf_probs_train_l)\n\ndel clf, sig_clf, sig_clf_probs_train, sig_clf_probs_test, sig_clf_probs_cv, sig_clf_probs_train_l, sig_clf_probs_test_l, sig_clf_probs_cv_l","4667aaa1":"%%time\nclf = LGBMClassifier()\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs_train = sig_clf.predict(train_x_onehotCoding)    \nsig_clf_probs_test = sig_clf.predict(test_x_onehotCoding)    \nsig_clf_probs_cv = sig_clf.predict(cv_x_onehotCoding)    \nsig_clf_probs_train_l = sig_clf.predict_proba(train_x_onehotCoding)    \nsig_clf_probs_test_l = sig_clf.predict_proba(test_x_onehotCoding)    \nsig_clf_probs_cv_l = sig_clf.predict_proba(cv_x_onehotCoding)    ","0db56b39":"print(classification_report(train_y,sig_clf_probs_train))\nprint(classification_report(test_y,sig_clf_probs_test))\nprint(classification_report(cv_y,sig_clf_probs_cv))\neval_dict['LGBMClassifier'] = log_loss(train_y, sig_clf_probs_train_l)\n\ndel clf, sig_clf, sig_clf_probs_train, sig_clf_probs_test, sig_clf_probs_cv, sig_clf_probs_train_l, sig_clf_probs_test_l, sig_clf_probs_cv_l","557f65d0":"possible_labels = all_data.Class.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nlabel_dict","24a443d9":"all_data['label']=all_data.Class.replace(label_dict)","1e977fdf":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(all_data.index.values, \n                                                  all_data.Class.values, \n                                                  test_size=0.15, \n                                                  random_state=42, \n                                                  stratify=all_data.Class.values)\n\nall_data['data_type'] = ['not_set']*all_data.shape[0]\n\nall_data.loc[X_train, 'data_type'] = 'train'\nall_data.loc[X_val, 'data_type'] = 'val'\n\nall_data.groupby(['Class', 'data_type']).count()[['TEXT']]","703a0f35":"# %%time\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n                                          \n# encoded_data_train = tokenizer.batch_encode_plus(\n#     all_data[all_data.data_type=='train'].TEXT.values, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=256, \n#     return_tensors='pt',\n#     #truncation=False\n# )","28576e4d":"# %%time\n# encoded_data_val = tokenizer.batch_encode_plus(\n#     all_data[all_data.data_type=='val'].TEXT.values, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=256, \n#     return_tensors='pt'\n# )","db1b9153":"# %%time\n# encoded_data_test = tokenizer.batch_encode_plus(\n#     all_test.TEXT.values, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=256, \n#     return_tensors='pt'\n# )","069ec269":"# %%time\n# input_ids_train = encoded_data_train['input_ids']\n# attention_masks_train = encoded_data_train['attention_mask']\n# labels_train = torch.tensor(all_data[all_data.data_type=='train'].label.values)\n\n# input_ids_val = encoded_data_val['input_ids']\n# attention_masks_val = encoded_data_val['attention_mask']\n# labels_val = torch.tensor(all_data[all_data.data_type=='val'].label.values)\n","73edd618":"# input_ids_test = encoded_data_test['input_ids']\n# attention_masks_test = encoded_data_test['attention_mask']","764fc7d1":"# dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n# dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","a695898e":"# torch.save(dataset_train, '.\/train')\n# torch.save(dataset_val, '.\/val')\n","b41b32f5":"#torch.save(dataset_test, '.\/test')","d7ccedf0":"dataset_train=torch.load('..\/input\/bert-final\/train (1)')\ndataset_val=torch.load('..\/input\/bert-final\/val (1)')","997e9126":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","86f3f401":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 3\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=batch_size)","d998b444":"from transformers import AdamW, get_linear_schedule_with_warmup, Adafactor\n\n\n\noptimizer = AdamW(model.parameters(),\n                  lr=1e-10, \n                  eps=1e-8)\n                  \nepochs = 5\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","edbc0c40":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\ndef classification_report_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return classification_report(labels_flat, preds_flat)\n\n\n\ndef accuracy_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return accuracy_score(labels_flat, preds_flat)\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')","6bdadc8b":"device = torch.device('cuda')\n\n\nmodel = model.to(device)","632801e7":"import random\n\nseed_val = 7\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndef evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n    \nfor epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'.\/finetuned_BERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_clf = classification_report_func(predictions, true_vals)\n    val_acc = accuracy_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'Accuracy: {val_acc}')\n    tqdm.write(val_clf)\n    ","f357d9ab":"del model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)\n\nmodel.to(device)\n\nmodel.load_state_dict(torch.load('..\/input\/models\/finetuned_BERT_epoch_5.model', map_location=torch.device('cuda')))\n\n_, predictions, true_vals = evaluate(dataloader_validation)\naccuracy_per_class(predictions, true_vals)","5fa3b105":"## 4. WordCloud <a class=\"anchor\" id=\"6\"><\/a>\n\n","fa8a3d37":"### Common stopwords in text","b7bf909c":"### Missing Value imputation","ffdc72f6":"## 2. Download data <a class=\"anchor\" id=\"3\"><\/a>\n\n\n","33a4540b":"### Text","bc9b316d":"### Stacking all the 3 vectorized forms and making dataframes","180e5137":"## 9. BERT  <a class=\"anchor\" id=\"10\"><\/a>\n\n","68705973":"### Decision Tree","cc65a46c":"## 8. Modelling <a class=\"anchor\" id=\"8\"><\/a>\n","88d7a321":"### Gene","ad3a1c76":"### Stochastic Gradient Descent Classifier","471e93e5":"### LightGBM Classifier","29eb6624":"### BertTokenizer and Encoding the Data","83942ddf":"## 3. EDA <a class=\"anchor\" id=\"4\"><\/a>","849cc0a2":"### Variant","5b62b315":"## 6. Word Embedding <a class=\"anchor\" id=\"8\"><\/a>\n","c6b5aec1":"### Class distribution","017d4a0f":"##### We will do a bigram (n=2) analysis over the medical text. Let's check the most common bigrams in text.","460972ef":"## 1. Import libraries <a class=\"anchor\" id=\"2\"><\/a>","706611ed":"### Random forest","57cfaec0":"### Preprocessing and Cleaning of text","56de41ad":"## 5. Splitting dataset into Train, Test and Validation set<a class=\"anchor\" id=\"6\"><\/a>\n\n","12f46210":"### Common words in text","4bf28a65":"## 7. Log Loss evaluation for feature importance <a class=\"anchor\" id=\"8\"><\/a>\n","73d273c8":"### Naive Bayes","b904c367":"### N-gram analysis","bacc1c60":"##### We will do a trigram (n=3) analysis over the medical text. Let's check the most common trigrams in text."}}