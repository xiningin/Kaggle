{"cell_type":{"cb7c4d27":"code","f2e9e4e1":"code","f83ec803":"code","d4dd7549":"code","08af9e52":"code","43f0423f":"code","779b3fd7":"code","7042b148":"code","9d30bd37":"code","010b79a3":"code","6a366037":"code","5c226f02":"code","aa01fdaa":"code","0fee8543":"code","35d326a1":"code","6bbee3cb":"code","28246360":"code","68a4f415":"code","7dd6fe81":"code","643b7918":"code","e85792e1":"code","588c721f":"code","29a26efb":"code","f565a73b":"code","611667b4":"code","3f3a7276":"code","ea8a95a9":"code","e29b1e20":"code","39949be5":"code","0bc1debb":"code","9e3d21e5":"code","a3c9286c":"code","e212e540":"code","88902d14":"code","dbc0608d":"code","309e3a56":"code","fcde9dc2":"code","8dbf5bd9":"code","d85aaaeb":"code","26ada47c":"code","9e1a3db5":"code","aaf973e5":"code","392edd63":"code","025b9a61":"markdown","a47d2d63":"markdown","489ab744":"markdown","db1f0e96":"markdown","c6dff65f":"markdown","3273751c":"markdown","32e65bbe":"markdown","f8d30950":"markdown","594270ba":"markdown","25fb643f":"markdown","0fe34a28":"markdown","6545c984":"markdown","c4d25b6f":"markdown","d8e0a769":"markdown","83d2aadb":"markdown","f25fc436":"markdown","37944d7f":"markdown"},"source":{"cb7c4d27":"\n# importing visualization libraries \nimport pandas as pd\nimport plotly.express as px \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)    #THIS LINE IS MOST IMPORTANT AS THIS WILL DISPLAY PLOT ON \n#NOTEBOOK WHILE KERNEL IS RUNNING\n\ndata = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata.head(2)\niris = data\niris.sample(5)","f2e9e4e1":"import warnings \nwarnings.filterwarnings('ignore')\ndef central_tendency(col):\n    mean = data.describe()[col]['mean']\n    std = data.describe()[col]['std']\n    min_ = data.describe()[col]['min']\n    max_ = data.describe()[col]['max']\n    median = data.describe()[col]['50%']\n    fig, ax = plt.subplots(1, 1, figsize=(16, 7))\n    sns.distplot(data[col], bins=20, color='grey',kde=False)\n    ax.axvline(mean, color='r', linestyle='--', label='mean')\n    ax.axvline(median, color='g', linestyle='--', label='medain')\n    ax.axvline(max_, color='b', linestyle='-', label='max')\n    ax.axvline(min_, color='b', linestyle='--', label='min')\n    ax.set_title(str(col), size=22)\n\n    ax.legend()\n\n    plt.show()\n    print(f'Mean of {col} {mean}')\n    print(f'std of {col} {std}')\n    print(f'min of {col} {min_}')\n    print(f'max of {col} {max_}')\n    print(f'Median of {col} {median}')\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 7))\n    sns.boxplot(data[col],  color='grey', ax=ax)\n    ax.set_title(str(col), size=22)\n\n    plt.show()","f83ec803":"central_tendency('SepalLengthCm')","d4dd7549":"central_tendency('SepalWidthCm')","08af9e52":"central_tendency('PetalWidthCm')","43f0423f":"central_tendency('PetalLengthCm')","779b3fd7":"fig, ax = plt.subplots(1, 1, figsize=(15, 6))\nsns.countplot(data=data, x='Species', palette='Greys')\nax.set_title('Species', size=22)\n#ax.legend(['Setosa', \"Versicolor\", ''])\nplt.show()\nprint(\"\")\nprint(\"Count per species:\")\nprint(data['Species'].value_counts())\n","7042b148":"fig, ax = plt.subplots(1, 1, figsize=(15, 6))\nsns.kdeplot(data=iris['SepalLengthCm'], shade=True)\nsns.kdeplot(data=iris['SepalWidthCm'], shade=True)\nsns.kdeplot(data=iris['PetalLengthCm'], shade=True)\nsns.kdeplot(data=iris['PetalWidthCm'], shade=True)\nplt.legend(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\nplt.show()","9d30bd37":"# understanding the distribution \nfig, ax = plt.subplots(4, 2, figsize=(11, 16))\nsns.histplot(data=iris, x='SepalLengthCm', ax=ax[0][0], color='b')\nsns.kdeplot(data=iris, x='SepalLengthCm', ax=ax[0][1], shade=True, color='b')\nsns.histplot(data=iris, x='SepalWidthCm', ax=ax[1][0], color='b')\nsns.kdeplot(data=iris, x='SepalWidthCm', ax=ax[1][1], shade=True, color='b')\nsns.histplot(data=iris, x='PetalLengthCm', ax=ax[2][0], color='b')\nsns.kdeplot(data=iris, x='PetalLengthCm', ax=ax[2][1], shade=True, color='b')\nsns.histplot(data=iris, x='PetalWidthCm', ax=ax[3][0], color='b')\nsns.kdeplot(data=iris, x='PetalWidthCm', ax=ax[3][1], shade=True, color='b')\n#fig.set_title('T')\nplt.show()","010b79a3":"# distribution with categorical effect \"Species\"\nfig, ax = plt.subplots(4, 2, figsize=(11, 16))\nsns.histplot(data=iris, x='SepalLengthCm',hue='Species', ax=ax[0][0], color='b')\nsns.kdeplot(data=iris, x='SepalLengthCm',hue='Species', ax=ax[0][1], shade=True, color='b')\nsns.histplot(data=iris, x='SepalWidthCm', ax=ax[1][0],hue='Species', color='b')\nsns.kdeplot(data=iris, x='SepalWidthCm', ax=ax[1][1],hue='Species', shade=True, color='b')\nsns.histplot(data=iris, x='PetalLengthCm', ax=ax[2][0],hue='Species', color='b')\nsns.kdeplot(data=iris, x='PetalLengthCm', ax=ax[2][1], hue='Species',shade=True, color='b')\nsns.histplot(data=iris, x='PetalWidthCm', ax=ax[3][0],hue='Species', color='b')\nsns.kdeplot(data=iris, x='PetalWidthCm', ax=ax[3][1], hue='Species',shade=True, color='b')\n#fig.set_title('T')\nplt.show()","6a366037":"\nfig = px.scatter(data, x=\"SepalLengthCm\", y=\"SepalWidthCm\", color=\"Species\", \n                 marginal_x=\"box\", marginal_y=\"violin\",\n                  title=\"SepalLength vs Sepal width with species\")\nfig.show()","5c226f02":"fig, ax = plt.subplots(4, 3, figsize=(15, 20))\n\nsns.scatterplot(data=iris , x='PetalLengthCm', y='PetalWidthCm', hue='Species', ax=ax[0][0])\nsns.scatterplot(data=iris , x='SepalWidthCm', y='PetalWidthCm', hue='Species', ax=ax[0][1])\nsns.scatterplot(data=iris , x='SepalLengthCm', y='PetalWidthCm', hue='Species', ax=ax[0][2])\n\nsns.scatterplot(data=iris , x='PetalWidthCm', y='PetalLengthCm', hue='Species', ax=ax[1][0])\nsns.scatterplot(data=iris , x='SepalWidthCm', y='PetalLengthCm', hue='Species', ax=ax[1][1])\nsns.scatterplot(data=iris , x='SepalLengthCm', y='PetalLengthCm', hue='Species', ax=ax[1][2])\n\nsns.scatterplot(data=iris , x='PetalLengthCm', y='SepalWidthCm', hue='Species', ax=ax[2][0])\nsns.scatterplot(data=iris , x='PetalWidthCm', y='SepalWidthCm', hue='Species', ax=ax[2][1])\nsns.scatterplot(data=iris , x='SepalLengthCm', y='SepalWidthCm', hue='Species', ax=ax[2][2])\n\n\nsns.scatterplot(data=iris , x='PetalLengthCm', y='SepalLengthCm', hue='Species', ax=ax[3][0])\nsns.scatterplot(data=iris , x='SepalWidthCm', y='SepalLengthCm', hue='Species', ax=ax[3][1])\nsns.scatterplot(data=iris , x='PetalWidthCm', y='SepalLengthCm', hue='Species', ax=ax[3][2])","aa01fdaa":"data.Species.value_counts()","0fee8543":"data['Species'] = data['Species'].replace('Iris-versicolor', 0)\ndata['Species'] = data['Species'].replace('Iris-virginica', 1)\ndata['Species'] = data['Species'].replace('Iris-setosa', 2)","35d326a1":"import plotly.express as px\ncorr = data.drop('Id', axis=1).corr()\nfig = px.imshow(corr,)\nfig.show()","6bbee3cb":"from scipy import stats\nimport numpy as np\n\nprint(f\"Mean of SepalLengthCm {np.mean(iris['SepalLengthCm'])}\")","28246360":"h_null = 9\ntest_result, pval = stats.ttest_1samp(iris['SepalLengthCm'], h_null)\n\nif pval < 0.05: \n    print(f\"We reject null hypothesis & There is difference in mean of the sample distribution and  null hypothesis: {h_null}\")\nelse: \n    print(f\"Null hypothesis is true : The mean of distribition is {h_null}\")","68a4f415":"# let's now change the h_null to 5.79\nh_null = 5.79\ntest_result, pval = stats.ttest_1samp(iris['SepalLengthCm'], h_null)\n\nif pval < 0.05: \n    print(f\"We reject null hypothesis & There is difference in mean of the sample distribution and  null hypothesis: {h_null}\")\nelse: \n    print(f\"Null hypothesis is true : The mean of distribition is {h_null}\")","7dd6fe81":"# let's now change the h_null to 3.5\nh_null = 5.5\ntest_result, pval = stats.ttest_1samp(iris['SepalLengthCm'], h_null)\n\nif pval < 0.05: \n    print(f\"We reject null hypothesis & There is difference in mean of the sample distribution and  null hypothesis: {h_null}\")\nelse: \n    print(f\"Null hypothesis is true : The mean of distribition is {h_null}\")","643b7918":"# if we chcek the distributoin \nfig, ax = plt.subplots(1, 1, figsize=(15, 6))\n\nsns.distplot(data['SepalLengthCm'], color='r', ax=ax)\nax.axvline(5.79, color='r', linestyle='--', label='5.79')\nax.axvline(9, color='g', linestyle='--', label='9')\nax.axvline(5.5, color='b', linestyle='--', label='5.5')\nax.axvline(5.843333333333334, color='black', linestyle='--', label='5.843333333333334 real mean')\n\n\nax.legend()","e85792e1":"# for the dataset we are going to put null hypothesis as  as sepallenght and petal lenght have identical average i.e they are drawn from same population \n# HA  : Alternate Hypothesis : that 2 independent samples have different mean","588c721f":"#dummy = [1,6,6,6,6,5,5,5,5,5, 9]\ntest_result, pval = stats.ttest_ind(iris['SepalLengthCm'], iris['PetalLengthCm'])\nif pval < 0.05: \n    print(f\"WE  reject null hypothesis & Their is difference in mean of the sample distributions and  null hypothesis\")\nelse: \n    print(f\"Null hypothesis is true : The mean of distribition is identical \")\nprint(test_result, pval)","29a26efb":"fig, ax = plt.subplots(1,1,figsize=(16, 8))\nsns.kdeplot(iris['SepalLengthCm'], shade=True)\nsns.kdeplot(iris['PetalLengthCm'], shade=True)\nax.legend(['SepalLengthCm', 'PetalLengthCm'])\nplt.show()","f565a73b":"# let's take a dummy data and do the hypothesis testing\ndummy = [1,6,6,6,6,5,5,5,5,5, 9]\ntest_result, pval = stats.ttest_ind(iris['SepalLengthCm'], dummy)\nif pval < 0.05: \n    print(f\"WE  reject null hypothesis & Their is difference in mean of the sample distributions and  null hypothesis: {h_null}\")\nelse: \n    print(f\"Null hypothesis is true : The mean of distribition is identical \")\nprint(test_result, pval)","611667b4":"fig, ax = plt.subplots(1,1,figsize=(16, 8))\nsns.kdeplot(iris['SepalLengthCm'], shade=True)\nsns.kdeplot(dummy, shade=True)\nax.legend(['Runtime', 'dummy'])\nplt.show()","3f3a7276":"# ANOVA \n# CHI-SQUARE TEST \n# CORRELATION","ea8a95a9":"# here we don't have two categorical column so I will randomly create a new catgorical column of \"yes\" and \"no\"  to chech the test of independecy \n# so I will import titanic data set just to do this hypothesis testing \ntitanic = pd.read_csv('..\/input\/titanic-machine-learning-from-disaster\/train.csv')\ntitanic.sample(2)","e29b1e20":"# we will test Sex column with survived column \ntitanic_tab = pd.crosstab(titanic['Sex'], titanic['Survived'], margins = True)\n","39949be5":"titanic_tab","0bc1debb":"g, pval, dof, expctd = stats.chi2_contingency(titanic_tab)","9e3d21e5":"if pval < 0.05: \n    print(f\"WE  reject null hypothesis : is that there is relationship exist on the categorical variables in the population and they are dependent\")\nelse: \n    print(f\"Null hypothesis is true : is that no relationship exists on the categorical variables in the population; they are independent \")\nprint(g, pval)","a3c9286c":"# we will test EMbardked column with survived column \ntitanic_tab = pd.crosstab(titanic['SibSp'], titanic['Survived'], margins = True)\n","e212e540":"titanic_tab","88902d14":"g, pval, dof, expctd = stats.chi2_contingency(titanic_tab)","dbc0608d":"if pval < 0.05: \n    print(f\"WE  reject null hypothesis : is that there is relationship exist on the categorical variables in the population and they are dependent\")\nelse: \n    print(f\"Null hypothesis is true : is that no relationship exists on the categorical variables in the population; they are independent \")\nprint(g, pval)","309e3a56":"# ANOVA TEST \ntitanic.head()","fcde9dc2":"# Group age data by race\nvoter_frame = pd.DataFrame({\"SibSp\":titanic.SibSp,\"Fare\":titanic.Fare})\ngroups = voter_frame.groupby(\"SibSp\").groups","8dbf5bd9":"titanic.SibSp.unique()","d85aaaeb":"groups","26ada47c":"one = groups[1]\nzero = groups[0]\nthree = groups[3]\nfour = groups[4]\ntwo = groups[2]\nfive = groups[5]\neight = groups[8]","9e1a3db5":"# Perform the ANOVA\nstat, pval = stats.f_oneway(one, zero, three, four, two, five, eight)","aaf973e5":"if pval < 0.05: \n    print(f\"WE  reject null hypothesis : Atleast one mean of category group differs from atleast one other category group\")\nelse: \n    print(f\"Null hypothesis is true : There is no difference in the mean of categories\")\nprint(stat, pval)","392edd63":"stats.pearsonr(data['PetalLengthCm'], data['PetalWidthCm'])","025b9a61":"# One Way ANOVA \n    ANOVA, which stands for Analysis of Variance, is a statistical test used to analyze the difference between the means of more than two groups.\n\n    A one-way ANOVA uses one independent variable, while a two-way ANOVA uses two independent variables\n    \n    \n## Assumptions of ANOVA\n- The assumptions of the ANOVA test are the same as the general assumptions for any parametric test:\n\n- Independence of observations: the data were collected using statistically-valid methods, and there are no hidden relationships among observations. If your data fail to meet this assumption because you have a confounding variable that you need to control for statistically, use an ANOVA with blocking variables.\n- Normally-distributed response variable: The values of the dependent variable follow a normal distribution.\n- Homogeneity of variance: The variation within each group being compared is similar for every group. If the variances are different among the groups, then ANOVA probably isn\u2019t the right fit for the data.","a47d2d63":"## CHI-SQUARE TEST : Test of Independency\n\n - Independency is a key concept in probability that describes a situation where knowing the value of one variable tells you nothing about the value of another. For instance, the month you were born probably doesn't tell you anything about which web browser you use, so we'd expect birth month and browser preference to be independent. \n - On the other hand, your month of birth might be related to whether you excelled at sports in school, so month of birth and sports performance might not be independent.\n\n - The chi-squared test of independence tests whether two categorical variables are independent. The test of independence is commonly used to determine whether variables like education, political views and other preferences vary based on demographic factors like gender, race and religion. Let's generate some fake voter polling data and perform a test of independence.\n \n     \n     \n\n\n        H0 : Null Hypothesis : The null hypothesis of the Chi-Square test is that no relationship exists on the categorical variables in the population; they are independent\n        HA Alternate Hypothesis : The Alternate hypothesis of the Chi-Square test is that there is relationship exist on the categorical variables in the population and they are dependent\n \n \n\n","489ab744":"#### DO UPVOTE IF YOU FOUND IT HELPFUL","db1f0e96":"    when null hypothesis was 5.79 we accepted the null hypothesis. \n        - it is clear too since when we use 5.5 or 9 as null hypothesis mean it was not near to the mean of data sample  but 5.79 was the nearest so it has high probability that the mean we have is of the sample than the other sample \n        - also tried to plot the data you can see the real mean and 5.79 lie so close to eachother \n        ","c6dff65f":"    References : \n    1) Scipy Documentation \n    2) https:\/\/www.scribbr.com\/\n    3) https:\/\/www.kaggle.com\/hamelg\/python-for-data-26-anova\n    4)https:\/\/www.kaggle.com\/hamelg\/python-for-data-24-hypothesis-testing\n    5) https:\/\/www.kaggle.com\/hamelg\/python-for-data-25-chi-squared-tests\n    6) Udemy Course : https:\/\/www.udemy.com\/course\/statsml_x\/\n    7) google.com\n    8) https:\/\/www.wikipedia.org\/\n    9) youtube.com\n    \n    ","3273751c":"## Central tendency \n#### As per above theory \n- Here I created a function which help us to understand the data \n- It has 2 types of graphs, box plot and Histogram.\n- box plot helps to undertsand the IQR and data representation, while Histograms help to understand data distribution \n- I also included more info like max value, min value, std, variance, with mean median  etc. \n- this function will help to understand how the data is spread, what is the central tendency and also it shows outliers present in the data column ","32e65bbe":"# Correlation \n\n- The Pearson product-moment correlation coefficient (or Pearson correlation coefficient, for short) is a measure of the strength of a linear association between two variables and is denoted by r. Basically, a Pearson product-moment correlation attempts to draw a line of best fit through the data of two variables, and the Pearson correlation coefficient, r, indicates how far away all these data points are to this line of best fit (i.e., how well the data points fit this new model\/line of best fit)\n![image.png](attachment:4f177fb8-5907-42b1-8bd0-39e51cea5447.png)\n\n- The stronger the association of the two variables, the closer the Pearson correlation coefficient, r, will be to either +1 or -1 depending on whether the relationship is positive or negative, respectively. Achieving a value of +1 or -1 means that all your data points are included on the line of best fit \u2013 there are no data points that show any variation away from this line. Values for r between +1 and -1 (for example, r = 0.8 or -0.4) indicate that there is variation around the line of best fit. The closer the value of r to 0 the greater the variation around the line of best fit. Different relationships and their correlation coefficients are shown in the diagram below:\n\n![image.png](attachment:bed7209a-ca93-4286-9cec-9423c3f293a1.png)","f8d30950":"    outlier : \n        - here outlier is presetn ofter 4.0 and before 2.3  value \n        - box plot is best to see the outliers visualize. \n        there are two methods to remove\/detect outliers \n            1. z-score method\n            2. trimming data\n     \n     same follows below dataset...... ","594270ba":"- What is data distribution ? \n    - it is nothing but ploting kde plot or histogram according to the data. no the previous line is not definition it is in the way of visualization. but data distribution is more than just ploting. \n    \n    - A data distribution is a function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs. Often, the data in a distribution will be ordered from smallest to largest, and graphs and charts allow you to easily see both the values and the frequency with which they appear.\n    - For Categorical Data it is using bar\/hist plot and for numerical data it is kde\/hist plot. \n    \n    \n  \n- what is the normal distribution? \n    - Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that   data near the mean are more frequent in occurrence than data far from the mean\n   -  A normal distribution is the proper term for a probability bell curve.\n   -  In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3.\n   - Normal distributions are symmetrical, but not all symmetrical distributions are normal.\n   - In reality, most pricing distributions are not perfectly normal.\n\n\n- what is IQR ? \n- what is QQ plot ? \n- what is z-score ? \n- what is min_max scaling ? \n- Outlier and outlier detection ? ","25fb643f":"     we need to change to species data with numerical encoding as 0,1,2 for particular categories\n     ","0fe34a28":"## Dataset Description \n![IRIS](https:\/\/miro.medium.com\/max\/3500\/1*f6KbPXwksAliMIsibFyGJw.png)\n\n# IRIS \nThis is the most common and most visualization friendly dataset. it has both continous and categorical features. So doing analysis in this dataset will give you nothing but easiness.\n\n### I googled it : Why IRIS Data is famous? found this facts \n\n- The Iris dataset is deservedly widely used throughout statistical science, especially for illustrating various problems in statistical graphics, multivariate statistics and machine learning.\n\n- Containing 150 observations, it is small but not trivial.\n\n- The task it poses of discriminating between three species of Iris from measurements of their petals and sepals is simple but challenging.\n\n- The data are real data, but apparently of good quality. In principle and in practice, test datasets could be synthetic and that might be necessary or useful to make a point. Nevertheless, few people object to real data.\n\n- The data were used by the celebrated British statistician **Ronald Fisher in 1936**. (Later he was knighted and became Sir Ronald.) At least some teachers like the idea of a dataset with a link to someone so well known within the field. The data were originally published by the statistically-minded botanist Edgar Anderson, but that earlier origin does not diminish the association.\n\n- Using a few famous datasets is one of the traditions we hand down, such as telling each new generation that Student worked for Guinness or that many famous statisticians fell out with each other. That may sound like inertia, but in comparing methods old and new, and in evaluating any method, it is often considered helpful to try them out on known datasets, thus maintaining some continuity in how we assess methods.\n\n- Last, but not least, the Iris dataset can be enjoyably coupled with pictures of the flowers concerned, as from e.g. the useful Wikipedia entry on the dataset.\n\n","6545c984":"# Hypothesis Testing \n\n### T-Test \n    ONE SAMPLE T-TEST (column : SepalLengthCm)\n    \n        - Null : Hypothesis : The mean of distribution  is h_null {variables name }\n        - ALternate Hypothesis : There is difference in mean of the distribution","c4d25b6f":"# Two Sample \n\n    H0 : nullhypothesis : the null hypothesis that 2 independent samples have identical average (expected) values\n    HA : Alternate Hypothesis : that 2 independent samples have different mean","d8e0a769":"## Playgroud 1.0 \n\nSince I have started learning DataScience and Natural Language processing. I was doing data manipulation, data visualization & learning theory. In this notebook I am thinking to combine most of my learning so it will be easy to brush things up in future otherwise I have really worst memory of forgeting things. I am hoping this will help other too \n\n### Two things that I do less will be doing more in this notebook \n1. Writing theory as much as possible \n2. Data Visualization \n\n## Descriptive Analysis & Visualization \n- what is central tendency ? \n    - Central Tendency is single value that describes the data. It is the values which explains the data roughly that all data points converge to this point. It is part of Statisitical Descriptive analysis where we use Mean, Median & Mode to understand the data distribution.\n    - The central tendency is one of the most quintessential concepts in statistics. Although it does not provide information regarding the individual values in the dataset, it delivers a comprehensive summary of the whole dataset.\n\n\n- what is mean ? \n    - Mean is just the average of the data i.e sum the data value and divide it with number of data values we have and there is your mean ! (You're mean though ;p)\n    - One cartoon that I like about mean : We use \"myu\" to define mean for population and x_bar for sample mean.\n    \n![mean](https:\/\/i.pinimg.com\/originals\/23\/f9\/ff\/23f9ffa4213904056401f2845bef20c7.png)    \n\n- what is Median ?\n    - Median is just a middle point in the data points i.e sort the data and the data point which divides the data into two part is the our median \n    - Median of even numbers is mean of two middle numbers while for odd it is just one middle point. \n    \n    \n- what is mode ?\n    - Model is most frequent number of the data. i.e. if data has 4 apples 3 banana 2 orange the Apples are our mode \n\n![Skewness](https:\/\/methods.sagepub.com\/images\/virtual\/statistics-for-the-social-sciences\/image101.jpg)\n\n- why we use central tendency ? ( answer of stack exchange is really worth) \n\n   - You may find it more useful to think of \"central tendency\" as giving a sense of the distribution's location. This is in contrast to measures of spread (variance, range, etc.), which don't communicate location. From the wikipedia entry for central tendency:\n\n        In statistics, a central tendency (or, more commonly, a measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution.\n\n        For many examples, the mean, median or mode will communicate the location of the distribution well. If you want to know how much it costs to buy a houseboat in Amsterdam, for example, knowing the average price, the 50th percentile price, or the most common price will all give you a sense of what a houseboat there costs. Of course, there will be plenty of variability in that distribution, and knowing the mean (or median or mode) won't actually tell you how much any individual houseboat would cost. But it does give you an idea of the location of the distribution of prices of houseboats on the scale of $0 to $infinity (e.g. you'll have the sense that it costs more than a cup of coffee and less than buying a restaurant in Tokyo). Even for many discrete variables, the mean can be interesting and useful (for example, you may be curious about the average number of rooms per houseboat even though it's actually nonsensical to talk about a fraction of a room).\n\n        Because the mean is so useful and so widely applicable, I think many people conflate \"central tendency\" with \"average value\", but there's no real reason to do that. As you point out, there are plenty of situations where the mean (or median) is an unusual value (multimodal or nonsymmetric distributions) or even an impossible value (in the case of discrete distributions). Happily, there are plenty of ways to communicate location \/ central tendency. Just pick one that makes sense for your data\n\n- what is skewness ? \n    - Skewness is the distribution where the data is not normal and we get long tailed distribution in right or left side of our central tendency. \n    - Two types of skewness \"Negatively Skewed\" & \"Positively Skewed\"\n    - when is negatively Skewed ? : when mode is greater than the value of median and mean i.e number of occurence of some value in data is more than the data mean. \n    - when is positive skewed ? : when the mode is smaller than the mean and median of the data. \n\n","83d2aadb":"    From Above plot \n    - with less std we can see that data is not much spread \n    - max and min value help to understand the extreme data points in the dataset\n    - mean and median are so close that it is clear data is not much skewed but data is multi modal \n    - box plot give us information like where the 50% data lie, where is meadian, are there any outliers \n    \n    \n    ","f25fc436":"### Lets set the hypothesis \n    H0 : There is no difference in the mean of categories. \n    HA : Atleast one mean of category group differs from atleast one other category group","37944d7f":"### let's understand the data \n1. It has 5 features and 6 columns, with Species as Categorical column \n2. Species has 3 categories [Setosa , Virginica, Versicolor]\n3. Other 4 are Continous variables Of length & width of sepal and petals \n    ( If someone doesn't know Like me :p  about petal and sepal)\n        - Sepal: The outer parts of the flower (often green and leaf-like) that enclose a developing bud. \n        - Petal: The parts of a flower that are often conspicuously colored.\n        \n4. dataset has 150 rows and data is evenly divided between three categories \n"}}