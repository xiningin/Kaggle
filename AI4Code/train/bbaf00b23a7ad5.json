{"cell_type":{"539d6b04":"code","39aa3cb4":"code","55cdccf5":"code","f2678a42":"code","2a493f54":"code","b72f021c":"code","1f407b7c":"code","cff00325":"code","a598d3ff":"code","4c04ffb9":"code","4061ecba":"code","6b10381b":"code","f0f7553b":"code","9b2a3116":"code","471ee515":"code","3e122efe":"code","7bd7966b":"code","a18c41c4":"code","a0deaf18":"code","841771e7":"code","3b42f2e6":"code","f74a02f7":"code","f1950f1c":"code","56a0a2d6":"code","60136410":"code","4e6fa961":"code","e4989ca3":"code","ce7fe6ed":"code","da76ff59":"code","8989449f":"code","2067a7e3":"code","6412060c":"code","7c1c43f0":"code","216b145d":"code","35ad5551":"code","9182341c":"code","15235f16":"code","5d91382c":"code","c15c89ee":"code","7f0c2f16":"code","df72426b":"code","10ba28f0":"code","762092ad":"code","5782ceea":"code","e6e9e753":"code","e8fca22f":"code","0bbf9b94":"code","8b611566":"code","e2590dae":"code","fc1da4c2":"code","3ccde74b":"code","9ea6843a":"code","fd838bc6":"code","b485084f":"code","63625b99":"code","04c673e1":"code","de452e3e":"code","29843dfa":"code","588a026c":"code","5f6842a2":"code","72ea7c1d":"code","8ce78055":"code","5ce06054":"code","4a9c67fc":"code","03f38ab2":"code","9badd9fd":"code","0bfcf94c":"code","2576cb9e":"code","86d235a4":"code","278b8e01":"code","fcc2f893":"code","e77b237e":"code","f320ff84":"code","1e6c0a84":"code","cdc451fe":"code","095488cb":"code","5ce5822f":"code","07fa6d01":"code","ad7a9c7a":"code","b3f72352":"code","5a11f9ce":"code","1171dce1":"code","494a0b53":"code","21f82b36":"code","5c1e1a9c":"code","4b3a1f68":"code","caae6ba6":"markdown","f0ce7e18":"markdown","a748a531":"markdown","8dc65292":"markdown","287810dc":"markdown","215ba63e":"markdown","96435109":"markdown","ca7fdfee":"markdown","b17bdf3e":"markdown","e925f206":"markdown","8824fa73":"markdown","9af6b0dc":"markdown","5e872e4b":"markdown","afa57975":"markdown","6eb6d16c":"markdown","a14e3e23":"markdown","6888b0e8":"markdown","0619bfa0":"markdown","19050c5e":"markdown","d97ccaee":"markdown","cc358d58":"markdown","a9663e01":"markdown","719d5ddc":"markdown","5f048a49":"markdown","ea065ddc":"markdown","c4c5e135":"markdown","ee353332":"markdown","51f86d28":"markdown","47cc26e4":"markdown","3137acbc":"markdown","73edd9f1":"markdown","4162ffa6":"markdown","3ae3fd21":"markdown","6ce0c6cf":"markdown","268c790d":"markdown","5d2e8c37":"markdown","67165a36":"markdown","c8b55cb5":"markdown","54f426b0":"markdown","6455f8fa":"markdown","76c835dd":"markdown","379b5f99":"markdown","46f52f59":"markdown","86b0a5e0":"markdown","0ebbcb0e":"markdown","6a5d8bcf":"markdown","93937bd4":"markdown","c3ab8b99":"markdown","a6cb844a":"markdown","77030fac":"markdown","f6cbbe06":"markdown","660799ac":"markdown","084a2c6c":"markdown","473a63c2":"markdown","57f904b9":"markdown","7023341a":"markdown","52bdb8ed":"markdown","5769136e":"markdown","9119aa06":"markdown","88dc4025":"markdown","c8389d1a":"markdown","debf90bd":"markdown","c8e5e79f":"markdown","40e2852d":"markdown","aea1b653":"markdown","3b1f1cb7":"markdown","b79d3625":"markdown","36f68072":"markdown","d40543de":"markdown","66609d01":"markdown","9c6c27ed":"markdown","cab2a708":"markdown","434d0a59":"markdown","d5777d48":"markdown","0ad11399":"markdown","497178e5":"markdown","c1d72341":"markdown","0e05dadc":"markdown","19d66d81":"markdown","8f3b4e85":"markdown","fa5d3e66":"markdown","a5bf76a8":"markdown","34eb7669":"markdown","c8b07f4d":"markdown","d60be395":"markdown","8a42735c":"markdown","2c08fef1":"markdown","6fc11605":"markdown","e398182a":"markdown","2ec3686b":"markdown","b8d72b75":"markdown","99a17256":"markdown","af676109":"markdown","9f752b1a":"markdown","eb189717":"markdown","e645675c":"markdown","973e0f16":"markdown","be36bb31":"markdown","cc22c8f2":"markdown","89871900":"markdown","bd1ffc92":"markdown","b4323406":"markdown","fc70d368":"markdown","259b0748":"markdown","875b7515":"markdown","942b4910":"markdown","a97aefbf":"markdown","4c44b2be":"markdown","6f1dd440":"markdown","d20e354a":"markdown","68c08ac9":"markdown","af92a6b3":"markdown","b65ff384":"markdown","ef43be59":"markdown","b3f077a1":"markdown","e73178b1":"markdown","fe593f7c":"markdown","fddd4407":"markdown","2f8a9b62":"markdown","bb053c65":"markdown","fc637137":"markdown","9c73e343":"markdown","417e6f12":"markdown","0473f0fc":"markdown","7556f091":"markdown","7f414fb5":"markdown","d9203dd3":"markdown","75f9f23a":"markdown","85832e90":"markdown","508ea754":"markdown","556d79d5":"markdown","09b1faa1":"markdown","31f30409":"markdown","4a3cda91":"markdown","17b2a4ea":"markdown","b85403d5":"markdown","67c4d4bd":"markdown","3c8b77c2":"markdown","daf29cbd":"markdown","0285fba3":"markdown","077aac2f":"markdown"},"source":{"539d6b04":"import pandas as pd\nhousing_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhousing_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","39aa3cb4":"housing_train.head()","55cdccf5":"housing_train.info()","f2678a42":"nhousing_train = housing_train.shape[0]\nnhousing_test = housing_test.shape[0]\ny_train = housing_train['SalePrice'].values\ntest_ID = housing_test['Id'].values\nall_data = pd.concat((housing_train,housing_test),axis=0).reset_index(drop=True)\nall_data.drop(['Id','SalePrice'],axis=1,inplace=True)\nprint('all_data size is: {}'.format(all_data.shape))","2a493f54":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\ncols = all_data.columns\ncolours = ['#000099', '#ffff00']\nf, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(all_data[cols].isnull(), cmap=sns.color_palette(colours))","b72f021c":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na_values = all_data.isnull().sum()\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'N\u00ba Missing Values':all_data_na_values,'Missing Ratio' :all_data_na})\nprint(missing_data)","1f407b7c":"f, ax = plt.subplots(figsize=(8, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","cff00325":"missing_values_obs = pd.DataFrame(all_data.isnull().sum(axis=1).value_counts())\nmissing_values_obs.reset_index(inplace=True)\nmissing_values_obs.rename(columns={'index':'N\u00ba missing values',0:'N\u00ba observations'},inplace=True)\nmissing_values_obs.sort_values('N\u00ba missing values',axis=0,ascending=True,inplace=True)\nmissing_values_obs\n\nf, ax = plt.subplots(figsize=(8, 6))\nsns.barplot(x=missing_values_obs['N\u00ba missing values'], y=missing_values_obs['N\u00ba observations'])\nplt.xlabel('N\u00ba missing values', fontsize=15)\nplt.ylabel('N\u00ba observations', fontsize=15)\nplt.title('N\u00ba of observations with missing values', fontsize=15)","a598d3ff":"all_data[all_data.duplicated(keep=False)]","4c04ffb9":"import numpy as np\n\nall_data.drop([193,829],axis=0,inplace=True)\ny_train = np.delete(y_train,193)\ny_train = np.delete(y_train,829)\nall_data.reset_index(inplace=True)","4061ecba":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","6b10381b":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","f0f7553b":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","9b2a3116":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","471ee515":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","3e122efe":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","7bd7966b":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","a18c41c4":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","a0deaf18":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","841771e7":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('NoBsmt')","3b42f2e6":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","f74a02f7":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","f1950f1c":"all_data = all_data.drop(['Utilities'], axis=1)","56a0a2d6":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","60136410":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","4e6fa961":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","e4989ca3":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","ce7fe6ed":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","da76ff59":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","8989449f":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na_values = all_data.isnull().sum()\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'N\u00ba Missing Values':all_data_na_values,'Missing Ratio' :all_data_na})\nprint(missing_data)","2067a7e3":"nhousing_train = all_data.shape[0]-nhousing_test #Remember we removed two observations from the train set, so we need to update the length\nhousing_train = all_data[:nhousing_train]\nhousing_test = all_data[nhousing_train:]\nhousing_train['SalePrice'] = y_train #We have to add the SalePrice column removed before","6412060c":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nf, ax = plt.subplots(figsize=(8, 6))\nsns.distplot(housing_train['SalePrice'],fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(housing_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","7c1c43f0":"f, ax = plt.subplots(figsize=(8,6))\nsns.boxplot(housing_train['OverallQual'],housing_train['SalePrice'])","216b145d":"f, ax = plt.subplots(figsize=(15,7))\nsns.boxplot(housing_train['YearBuilt'],housing_train['SalePrice'])\nplt.xticks(rotation=90);","35ad5551":"f, ax = plt.subplots(figsize=(8,6))\nsns.regplot(housing_train['GrLivArea'],housing_train['SalePrice'],housing_train)","9182341c":"f, ax = plt.subplots(figsize=(8,6))\nsns.regplot(housing_train['TotalBsmtSF'],housing_train['SalePrice'],housing_train)","15235f16":"housing_train[housing_train['TotalBsmtSF']>6000]['GrLivArea']","5d91382c":"# Calculate first and third quartile\nfirst_quartile = housing_train['GrLivArea'].describe()['25%']\nthird_quartile = housing_train['GrLivArea'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nhousing_train = housing_train[(housing_train['GrLivArea'] > (first_quartile - 3 * iqr)) &\n            (housing_train['GrLivArea'] < (third_quartile + 3 * iqr))]","c15c89ee":"f, ax = plt.subplots(figsize=(8,6))\nsns.regplot(housing_train['GrLivArea'],housing_train['SalePrice'],housing_train)","7f0c2f16":"f, ax = plt.subplots(figsize=(8,6))\nsns.regplot(housing_train['TotalBsmtSF'],housing_train['SalePrice'],housing_train)","df72426b":"f, ax = plt.subplots(figsize=(8, 6))\nsns.distplot(housing_train['SalePrice'],fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(housing_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","10ba28f0":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = housing_train.iloc[:,1:].corr()\nplt.subplots(figsize=(22,15))\nsns.heatmap(corrmat, square=True,annot=True)","762092ad":"num_feat = housing_train.select_dtypes(include='number').columns\nfor i in num_feat:\n    if abs(housing_train[i].corr(housing_train['SalePrice'])) > 0.2:\n        print(i, '-', 'SalesPrice:', housing_train[i].corr(housing_train['SalePrice']))","5782ceea":"# Select the numeric columns\nnumeric_subset = housing_train.select_dtypes('number')\n\n# Create columns with square root and log of numeric columns\nfor col in numeric_subset.columns:\n    # Skip the SalePrice column\n    if col == 'SalePrice':\n        next\n    else:\n        numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])\n        numeric_subset['log_' + col] = np.log(numeric_subset[col]+1)\n\n# Select the categorical columns\ncategorical_subset = housing_train.select_dtypes(exclude='number')\n\n# One hot encode\ncategorical_subset = pd.get_dummies(categorical_subset)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\n# Find correlations with the score \ncorrelations = features.corr()['SalePrice'].sort_values()","e6e9e753":"# Display most negative correlations\ncorrelations.head(15)","e8fca22f":"# Display most positive correlations\ncorrelations.tail(15)","0bbf9b94":"nhousing_train = housing_train.shape[0]\nnhousing_test = housing_test.shape[0]\ny_train = housing_train['SalePrice'].values\nall_data = pd.concat((housing_train,housing_test),axis=0).reset_index(drop=True)\nall_data.drop(['SalePrice'],axis=1,inplace=True)","8b611566":"all_data = all_data.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"},\n                             'YrSold' : {2006 : '2006', 2007 : '2007', 2008 : '2008', 2009 : '2009', 2010 : '2010'}\n                      })","e2590dae":"all_data = all_data.replace({\"Alley\" : {'None' : 0, \"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"NoBsmt\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {'NoBsmt' : 0, \"No\" : 1, \"Mn\" : 2, \"Av\": 3, \"Gd\" : 4},\n                       \"BsmtFinType1\" : {\"NoBsmt\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"NoBsmt\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"NoBsmt\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"None\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2}})","fc1da4c2":"all_data['BsmtCond'] = pd.to_numeric(all_data['BsmtCond'])","3ccde74b":"# Overall quality of the house\nall_data[\"OverallGrade\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\n# Overall quality of the garage\nall_data[\"GarageGrade\"] = all_data[\"GarageQual\"] * all_data[\"GarageCond\"]\n# Overall quality of the exterior\nall_data[\"ExterGrade\"] = all_data[\"ExterQual\"] * all_data[\"ExterCond\"]\n# Overall kitchen score\nall_data[\"KitchenScore\"] = all_data[\"KitchenAbvGr\"] * all_data[\"KitchenQual\"]\n# Overall fireplace score\nall_data[\"FireplaceScore\"] = all_data[\"Fireplaces\"] * all_data[\"FireplaceQu\"]\n# Overall garage score\nall_data[\"GarageScore\"] = all_data[\"GarageArea\"] * all_data[\"GarageQual\"]\n# Overall pool score\nall_data[\"PoolScore\"] = all_data[\"PoolArea\"] * all_data[\"PoolQC\"]\n\n# Total number of bathrooms\nall_data[\"TotalBath\"] = all_data[\"BsmtFullBath\"] + (0.5 * all_data[\"BsmtHalfBath\"]) + all_data[\"FullBath\"] + (0.5 * all_data[\"HalfBath\"])\n# Total SF for house (incl. basement)\nall_data[\"AllSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\nall_data[\"AllFlrsSF\"] = all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n# Total SF for porch\nall_data[\"AllPorchSF\"] = all_data[\"OpenPorchSF\"] + all_data[\"EnclosedPorch\"] + all_data[\"3SsnPorch\"] + all_data[\"ScreenPorch\"]\n# Has masonry veneer or not\nall_data[\"HasMasVnr\"] = all_data.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \n                                               \"Stone\" : 1, \"None\" : 0})\n# House completed before sale or not\nall_data[\"BoughtOffPlan\"] = all_data.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})","9ea6843a":"numerical_features = all_data.select_dtypes('number')\ncategorical_features = all_data.select_dtypes(exclude='number')\nnumerical_features.drop(['index'],axis=1,inplace=True)\nfor col in numerical_features.columns:\n    numerical_features['sqrt_'+col] = np.sqrt(numerical_features[col])\n    numerical_features['log_'+col] = np.log(numerical_features[col]+1)\n    numerical_features['s2_'+col] = numerical_features[col]**2\n    numerical_features['s3_'+col] = numerical_features[col]**3\n\nall_data = pd.concat([categorical_features,numerical_features],axis=1)","fd838bc6":"categorical_features = all_data.select_dtypes(exclude='number').columns\nall_data_cat = all_data[categorical_features]\nall_data_cat_dumm = pd.get_dummies(all_data_cat)\nall_data_cat_dumm.shape\nall_data = pd.concat([all_data,all_data_cat_dumm],axis=1)\nall_data.drop(categorical_features,axis=1,inplace=True)\nall_data.head()","b485084f":"housing_train = all_data[:nhousing_train]\nhousing_test = all_data[nhousing_train:]\nhousing_train['SalePrice'] = y_train #We have to add the SalePrice column removed before\nhousing_train.info()","63625b99":"num_feat = housing_train.select_dtypes(include='number').columns\ndrop_cols = []\nfor i in num_feat:\n    if abs(housing_train[i].corr(housing_train['SalePrice'])) < 0.3:\n        drop_cols.append(i)\n        print(i, '-', 'SalesPrice:', housing_train[i].corr(housing_train['SalePrice']))\n\nhousing_train = housing_train.drop(columns = drop_cols)","04c673e1":"housing_test = housing_test.drop(columns = drop_cols)","de452e3e":"housing_test.shape","29843dfa":"corr_matrix = housing_train.corr()\nitem = corr_matrix.iloc[3:4, 9:10]\nitem.columns[0]","588a026c":"def remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model\n        to generalize and improves the interpretability of the model.\n        \n    Inputs: \n        threshold: any features with correlations greater than this value are removed\n    \n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n    \n    # Dont want to remove correlations between SalePrice\n    y = x['SalePrice']\n    x = x.drop(columns = ['SalePrice'])\n    \n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns)-1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n            \n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])   \n               \n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns = drops)\n    \n    # Add the SalePrice back in to the data\n    x['SalePrice'] = y\n               \n    return drops, x","5f6842a2":"# Remove the collinear features above a specified correlation coefficient\ndrops, housing_train = remove_collinear_features(housing_train, 0.6);","72ea7c1d":"housing_train.shape","8ce78055":"drops","5ce06054":"housing_train.corr()['SalePrice'].sort_values()","4a9c67fc":"housing_test = housing_test.drop(columns = drops)","03f38ab2":"X_train = housing_train.drop('SalePrice',axis=1)\ny_train = np.log1p(housing_train['SalePrice'])\nX_test = housing_test","9badd9fd":"# Scaling values\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Save de column names of the training set\ncolumns = X_train.columns\n\n# Create the scaler object with a range of 0-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Fit on the training data\nscaler.fit(X_train)\n\n# Transform both the training and testing data\nX_train = scaler.transform(X_train)\nX_train = pd.DataFrame(X_train)\nX_train.columns = columns\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test)\nX_test.columns = columns","0bfcf94c":"from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.svm import SVR","2576cb9e":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Validation function\nn_folds = 10\n\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse.mean())","86d235a4":"# Create the model to use for hyperparameter tuning\nmodel = Ridge(random_state=42)\n\n# Regularization strength; must be a positive float. Larger values specify stronger regularization.\nalpha = [0.0001, 0.0003, 0.0005, 0.001, 0.1, 0.3, 0.75, 1, 1.5]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'alpha': alpha}\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=9, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","278b8e01":"model_Ridge = Ridge(alpha=0.3, random_state=42)\n\nrmse_cv_Ridge = rmse_cv(model_Ridge)\nprint('Ridge Regressor RMSE is: {}'.format(rmse_cv_Ridge))","fcc2f893":"# Create the model to use for hyperparameter tuning\nmodel = Lasso(random_state=42)\n\n# Constant that multiplies the L1 term.\nalpha = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.001, 0.1, 0.3, 0.75, 1, 1.5]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'alpha': alpha}\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=11, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","e77b237e":"model_Lasso = Lasso(alpha=0.0002, random_state=42)\n\nrmse_cv_Lasso = rmse_cv(model_Lasso)\nprint('Lasso Regressor RMSE is: {}'.format(rmse_cv_Lasso))","f320ff84":"# Create the model to use for hyperparameter tuning\nmodel = ElasticNet(random_state=42)\n\n# Constant that multiplies the penalty terms\nalpha = [0.0001, 0.0003, 0.0005, 0.001, 0.1, 0.3, 0.75, 1, 1.5]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'alpha': alpha}\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=9, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","1e6c0a84":"model_ElasticNet = ElasticNet(alpha=0.0003, random_state=42)\n\nrmse_cv_ElasticNet = rmse_cv(model_ElasticNet)\nprint('ElasticNet Regressor RMSE is: {}'.format(rmse_cv_ElasticNet))","cdc451fe":"# The strategy used to choose the split at each node\nsplitter = ['best', 'random']\n\n# Maximum depth of the tree\nmax_depth = [2, 3, 5, 10, 15, None]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'splitter': splitter,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       'min_samples_leaf': min_samples_leaf,\n                       'max_features': max_features}\n\n# Create the model to use for hyperparameter tuning\nmodel = DecisionTreeRegressor(random_state = 42)\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=40, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","095488cb":"model_DecisionTree = DecisionTreeRegressor(splitter='best', max_depth=None, min_samples_split=6, min_samples_leaf=8, \n                                           max_features='auto', random_state=42)\n\nrmse_cv_DecisionTree = rmse_cv(model_DecisionTree)\nprint('Decision Tree Regressor RMSE is: {}'.format(rmse_cv_DecisionTree))","5ce5822f":"# The number of trees in the forest\nn_estimators = [100, 500, 900, 1100, 1500]\n\n# Maximum depth of the tree\nmax_depth = [2, 3, 5, 10, 15, None]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       'min_samples_leaf': min_samples_leaf,\n                       'max_features': max_features}\n\n# Create the model to use for hyperparameter tuning\nmodel = RandomForestRegressor(random_state = 42)\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=40, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","07fa6d01":"model_RandomForest = RandomForestRegressor(n_estimators=1500, max_depth=15, min_samples_split=4, min_samples_leaf=1, \n                                           max_features='sqrt', random_state=42)\n\nrmse_cv_RandomForest = rmse_cv(model_RandomForest)\nprint('Random Forest Regressor RMSE is: {}'.format(rmse_cv_RandomForest))","ad7a9c7a":"# Loss function to be optimized\nloss = ['ls', 'lad', 'huber']\n\n# Number of trees used in the boosting process\nn_estimators = [100, 500, 900, 1100, 1500]\n\n# Maximum depth of each tree\nmax_depth = [2, 3, 5, 10, 15]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'loss': loss,\n                       'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}\n\n# Create the model to use for hyperparameter tuning\nmodel = GradientBoostingRegressor(random_state = 42)\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=40, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","b3f72352":"model_GB = GradientBoostingRegressor(loss='huber', max_depth=3, max_features='sqrt', min_samples_leaf=4,\n                                     min_samples_split=10, n_estimators=100, random_state=42)\n\nrmse_cv_GB = rmse_cv(model_GB)\nprint('Gradient Boosted Regressor RMSE is: {}'.format(rmse_cv_GB))","5a11f9ce":"# Specifies the kernel type to be used in the algorithm\nkernel = ['linear', 'poly', 'rbf', 'sigmoid', 'rbf']\n\n# Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels\ndegree = [2, 3, 4, 5, 6]\n\n# Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019\ngamma = ['scale', 'auto']\n\n# Regularization parameter. The strength of the regularization is inversely proportional to C\nC = [0.1, 1, 1.5, 2]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'kernel': kernel,\n                       'degree': degree,\n                       'gamma': gamma,\n                       'C': C}\n\n# Create the model to use for hyperparameter tuning\nmodel = SVR()\n\n# Set up the random search with 10-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=10, n_iter=40, \n                               scoring = 'neg_mean_squared_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True)\n\n# Fit on the training data\nrandom_cv.fit(X_train, y_train)\n\n# Best Estimators\nbest_estimators = random_cv.best_estimator_\nprint(best_estimators)","1171dce1":"model_SVR = SVR(kernel='rbf', degree=6, gamma='auto', C=2)\n\nrmse_cv_SVR = rmse_cv(model_SVR)\nprint('Support Vector Regressor RMSE is: {}'.format(rmse_cv_SVR))","494a0b53":"# Dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['Ridge Regression', 'Lasso Regression', 'ElasticNet Regression', \n                                           'Decision Tree Regressor', 'Random Forest Regressor', 'Gradient Boosted Regressor',\n                                           'Support Vector Regressor'],\n                                 'rmse': [rmse_cv_Ridge, rmse_cv_Lasso, rmse_cv_ElasticNet, rmse_cv_DecisionTree, \n                                         rmse_cv_RandomForest, rmse_cv_GB, rmse_cv_SVR]})\n\n# Plot\nf, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(x='rmse', y='model', data=model_comparison.sort_values('rmse', ascending = True), color='red', edgecolor='black')\nplt.ylabel('')\nplt.yticks(size = 14)\nplt.xlabel('Root Mean Squared Error')\nplt.xticks(size = 14)\nplt.title('Model Comparison on CV RMSE', size = 20);","21f82b36":"model_Lasso.fit(X_train,y_train)\ncoef = pd.Series(model_Lasso.coef_, index = X_train.columns)\n\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n\nf, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(x=imp_coef[:], y=imp_coef.index, color='blue', edgecolor='black')\nplt.ylabel('')\nplt.yticks(size = 14)\nplt.xlabel('')\nplt.xticks(size = 14)\nplt.title('Coefficients in Lasso Model', size = 20)","5c1e1a9c":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" features and eliminated the other \" +  str(sum(coef == 0)) + \" features\")","4b3a1f68":"submission_Lasso = pd.DataFrame()\nsubmission_Lasso['Id'] = test_ID\nmodel_Lasso.fit(X_train,y_train)\nsubmission_Lasso['SalePrice'] = np.expm1(model_Lasso.predict(X_test))\nsubmission_Lasso.to_csv('submission_Lasso.csv',index=False)\n\nsubmission_Lasso.head()","caae6ba6":"##### Decision Tree Regressor","f0ce7e18":"Feature engineering and selection are iterative processes that will usually require several attempts to get right. Often we will use the results of modeling, such as the feature importances from a random forest, to go back and redo feature selection, or we might later discover relationships that necessitate creating new variables. Moreover, these processes usually incorporate a mixture of domain knowledge and statistical qualitites of the data.","a748a531":"#### Model Optimization","8dc65292":"We selected 1 hyperparameter to tune in the Lasso Regressor.","287810dc":"As before, it looks like we have one outlier when TotalBsmtSF > 600...Let's see if it has any relationship with the ouliers of GrLivArea.","215ba63e":"The best gradient boosted model has the following hyperparameters:\n\n- loss = huber\n- n_estimators = 100 \n- max_depth = 3\n- min_samples_leaf = 4\n- min_samples_split = 10\n- max_features = sqrt (This means that max_features=sqrt(n_features) according to the docs)","96435109":"#### Implementing Machine Learning Models in Scikit-Learn","ca7fdfee":"Let's take a look at how Lasso model picks the features:","b17bdf3e":"Now we have performed some transformation in our variables it's time to feature selection.","e925f206":"In the following code, we take log and square root transformations of the numerical variables, one-hot encode the categorical variables, calculate the correlations between all of the features and the SalePrice, and display the top 15 most positive and top 15 most negative correlations.","8824fa73":"- FireplaceQu: data description says NA means \"no fireplace\"","9af6b0dc":"- BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2: For all these categorical basement-related features, NaN means that there is no basement.","5e872e4b":"##### Hyperparameter Tuning with Random Search and Cross Validation","afa57975":"The Support Vector Regressor performs the best followed closely by the Gradient Boosted Regressor, ElasticNet, Lasso and Ridge Regresion. Especially with the Support Vector Regressor, the hyperparameters have a significant influence on performance. The Random Forest and Gradient Boosting methods are great for starting out because the performance is less dependent on the model settings.\n\nFrom here, we will choose the best model but taking into account interpretability, so we could choose between Elastic, Lasso or Ridge Regressor since they seem to be accurate and are the most interpretable. We will choose Lasso Regressor. Since it only has one hyperparameter it is not necessary to perform a GridSearch for optimizing again (actually, we could have performed GridSearchCV before instead of RandomizedSearchCV). In the case we had chosen a model with more than one hyperparameter we should concentrate on optimizing the best model using GridSearchCV. Grid search evaluates every single combination we specify. As discussed above, random search is better when we have limited knowledge of the best model hyperparameters and we can use random search to narrow down the options and then use grid search with a more limited range of options.","6eb6d16c":"First of all we need to get back to our train and test sets.","a14e3e23":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","6888b0e8":"We selected 6 different hyperparameters to tune in the Gradient Boosted Regressor.","0619bfa0":"We can see that the distribution:","19050c5e":"##### Lasso Regression","d97ccaee":"- Exterior1st and Exterior2nd: Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","cc358d58":"### Establish a Baseline","a9663e01":"#### Feature Scaling","719d5ddc":"Let's take a quick look at the data structure","5f048a49":"### FRAME THE PROBLEM\nIt is clearly a typical supervised learning task since we are given labeled training examples. Moreover, it is also a typical regression task, since we are asked to predict a value. More specifically, this is a multivariate regression problem since the system will use multiple features to make a prediction. Finally, there is no continuous flow of data coming in the system so there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.","ea065ddc":"##### Elastic Net Regressor","c4c5e135":"As we can see there are two pair of observations with identical attributes. Each pair of observations has one belonging to the training dataset (rows 193 and 829) and the other belonging to the test dataset (rows 2713 and 2865). We are going to delete the ones corresponding to the training dataset because it doesn't make sense to test our model with the observations that has been trained.","ee353332":"In the case of having so many features in the dataset we can make a list of missing data % for each feature:","51f86d28":"After this process we have over 1,454 observations with 530 columns (features). Not all of these features are likely to be useful for predicting the Sale Price, so now we will turn to feature selection to remove some of the variables.","47cc26e4":"## SUBMISSION","3137acbc":"The best Random Forest model has the following hyperparameters:\n\n- n_estimators = 1500 \n- max_depth = 15\n- min_samples_split = 4\n- min_samples_leaf = 1\n- max_features = sqrt (This means that max_features=sqrt(n_features) according to the docs)","73edd9f1":"- KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","4162ffa6":"We selected 5 different hyperparameters to tune in the Random Forest Regressor.","3ae3fd21":"The best Support Vector Regressor model has the following hyperparameters:\n\n- kernel = 'rbf'\n- degree = 6\n- gamma = 'auto'\n- C = 2","6ce0c6cf":"- Utilities: For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","268c790d":"The purpose is to to find anomalies, patterns, trends, or relationships. These may be interesting by themselves (for example finding a correlation between two variables) or they can be used to inform modeling decisions such as which features to use. In short, the goal of EDA is to determine what our data can tell us! EDA generally starts out with a high-level overview, and then narrows in to specific parts of the dataset once as we find interesting areas to examine.\n\nTo begin the EDA, we will focus on a single variable, the SalePrice, because this is the target for our machine learning models.","5d2e8c37":"- Electrical: It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","67165a36":"We also removed them from the test dataset.","c8b55cb5":"After removing the features that present collinearty we have 29 columns (features) (1 columns is the target). This is still quite a few, but mostly it is because we have one-hot encoded the categorical variables. Moreover, while a large number of features may be problematic for models such as linear regression, models such as the random forest perform implicit feature selection and automatically determine which features are important during traning. There are other feature selection steps to take, but for now we will keep all the features we have and see how the model performs.","54f426b0":"Many of the 170 features we have in our data are redundant because they are highly correlated with one another.","6455f8fa":"As well, some categorical features can be defined as numerical if there is information in the order, so we can encode them.","76c835dd":"- PoolQC: data description says NA means \"No Pool\"","379b5f99":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","46f52f59":"- LotFrontage: Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","86b0a5e0":"Before dealing wth the missing values we should concatenate the train and test data in the same dataframe.","0ebbcb0e":"We'll plot the probability distribution of our target variable.","6a5d8bcf":"#### Model Interpretation","93937bd4":"##### Support Vector Regressor","c3ab8b99":"As we expected, 'OverallQual' has a strong relationship with 'SalePrice'","a6cb844a":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","77030fac":"We selected 1 hyperparameter to tune in the Elastic Net Regressor.","f6cbbe06":"In machine learning, optimizing a model means finding the best set of hyperparameters for a particular problem.","660799ac":"### Feature Engineering and Selection","084a2c6c":"\nSince Kaggle doesn't provide the solutions ('SalePrice' column) of the test set of the problem we cannot establish a naive baseline before we making machine learning models. It would be important because if the models we build cannot outperform a naive guess then we might have to admit that machine learning is not suited for this problem. This could be because we are not using the right models, because we need more data, or because there is a simpler solution that does not require machine learning. Establishing a baseline is crucial so we do not end up building a machine learning model only to realize we can't actually solve the problem.\n\nFor a regression task, a good naive baseline is to predict the median value of the target on the training set for all examples on the test set. This is simple to implement and sets a relatively low bar for our models: if they cannot do better than guessing the median value, then we will need to rethink our approach.\n\nIn this case, we will apply cross validation to our machine learning models and apply the regression metric 'Root Mean Squared Log Error' stablished by Kaggle in order to find out which model performs the best.","473a63c2":"##### Random Forest Regressor","57f904b9":"Now we can create some new features, by for example combining existing features.","7023341a":"##### Gradient Boosted Regressor","52bdb8ed":"We selected 5 different hyperparameters to tune in the Decision Tree Regressor. These all will affect the model in different ways that are hard to determine ahead of time, and the only method for finding the best combination for a specific problem is to test them out.","5769136e":"We control a model by choosing the hyperparameters, and these choices can have a significant effect on the final performance of the model (although usually not as great of an effect as getting more data or engineering features).","9119aa06":"#### Model Comparison on Test RMSE","88dc4025":"Features that are strongly correlated with each other are known as collinear and removing one of the variables in these pairs of features can often help a machine learning model generalize and be more interpretable. (I should point out we are talking about correlations of features with other features, not correlations with the target, which help our model!)","c8389d1a":"#### Analysing 'SalePrice'","debf90bd":"- MSZoning (The general zoning classification): 'RL' is by far the most common value. So we can fill in missing values with 'RL'","c8e5e79f":"If plot an histogram:","40e2852d":"An histogram is a good choice when we have many features. To learn more about the missing value patterns among observations, we can visualize it by a histogram.","aea1b653":"- BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath: missing values are likely zero for having no basement","3b1f1cb7":"This histogram helps to identify the missing values situations among the 2919 observations. For example, there are around 1200 observations with 4 missing values. Notice that there is no observation with all of the values.","b79d3625":"### Dealing with Outliers","36f68072":"There are 1460 instances in the train dataset, which means that is fairly small by Machine Learning standards, but it's ok to get  started and make some predictions. Notice that we have some attributes with missing values, such as PoolQC, Fence, MiscFeature.... We will need to take care of this later.","d40543de":"After removing the outliers, we can get back to the analysis.\n\nThis plot looks a little less suspicious and is close to normally distributed with a long tail on the right side (it has a positive skew).","66609d01":"# My 1st Kaggle Competition - House Prices Advanced Regression Techniques","9c6c27ed":"- Is devated from the normal distribution.\n- Has positive skewness.\n- Shows peakedness.","cab2a708":"- Fence: data description says NA means \"no fence\"","434d0a59":"The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute's type and number of non-null values.","d5777d48":"It seems that 'SalePrice' and 'GrLivArea' have a linear relationship. It also seems like we have some outliers when GrLivArea > 4500...Let's now see 'TotalBsmtSF'.","0ad11399":"### Model Evaluation and Selection","497178e5":"- GarageYrBlt, GarageArea and GarageCars: Replacing missing data with 0 (Since No garage = no cars in such garage.)","c1d72341":"In order to quantify correlations between the features (variables) and the target, we can calculate the Pearson correlation coefficient. Although there can be non-linear relationships between the features and targets and correlation coefficients do not account for interactions between features, linear relationships are a good way to start exploring trends in the data. We can then use these values for selecting the features to employ in our model.","0e05dadc":"We're going to One-hot encode the rest of categorical variables, as there is no information in the order.","19d66d81":"Let's check if there is any remaining missing value:","8f3b4e85":"It's time to impute the missing values. We impute them by proceeding sequentially through features with missing values:","fa5d3e66":"This heatmap is the best way to get a quick overview of our variables and its relationships.\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'GarageX' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account.","a5bf76a8":"I must confess this is not a really good score but, since this is my first Kaggle competition the aim of this project wasn't be at the top of the leaderboard but learn and practice with a more realistic data science and machine learning problem and trust me I did!\n\nFor improving the results we could do more (and guess better) feature engineering, explore in deep the multilinearity of the problem and try ensemble models. Maybe apply some dimensionality reduction with Principal Component Analysis (PCA) may help as well. Anyway, I think this is a good step-by-step walkthrough of how a beginner data science and machine learning problem could be faced.\n\nHope you enjoy the reading and any suggestion is welcome!","34eb7669":"Before we get any further, we should define what feature engineering and selection are! These definitions are informal and have considerable overlap, but I like to think of them as two separate processes:\n\n- Feature Engineering: The process of taking raw data and extracting or creating new features that allow a machine learning model to learn a mapping beween these features and the target. This might mean taking transformations of variables, such as we did with the log and square root, or one-hot encoding categorical variables so they can be used in a model. Generally, I think of feature engineering as adding additional features derived from the raw data.\n- Feature Selection: The process of choosing the most relevant features in your data. \"Most relevant\" can depend on many factors, but it might be something as simple as the highest correlation with the target, or the features with the most variance. In feature selection, we remove features that do not help our model learn the relationship between features and the target. This can help the model generalize better to new data and results in a more interpretable model. Generally, I think of feature selection as subtracting features so we are left with only those that are most important.","c8b07f4d":"#### Imputing missing values","d60be395":"#### Feature Selection","8a42735c":"Let's now analyse the relationship between 'SalePrice' and other variables that seeem to play an important role in this problem:\n\n- OverallQual.\n- YearBuilt.\n- TotalBsmtSF.\n- GrLivArea.","2c08fef1":"As a reminder, Lasso Regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, L2 regularization (e.g. Ridge regression) doesn\u2019t result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge.\n\nSo, having said that, our Lasso model has picked 26 features and eliminated the other 3 features, being the most important features the GrLivArea, OverallQual and sqrt_LotArea. It catchs my eye not to see the GarageCars, TotalBsmtSF or YearBuilt features with more importance for the model as they are highly correlated with SalePrice.","6fc11605":"We selected 1 hyperparameter to tune in the Ridge Regressor. It will affect the model in different ways that are hard to determine ahead of time, and the only method for finding the best combination for a specific problem is to test them out.","e398182a":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","2ec3686b":"We can choose the best hyperparameters for a model through random search and cross validation.\n\n- Random search refers to the method in which we choose hyperparameters to evaluate: we define a range of options, and then randomly select combinations to try. This is in contrast to grid search which evaluates every single combination we specify. Generally, random search is better when we have limited knowledge of the best model hyperparameters and we can use random search to narrow down the options and then use grid search with a more limited range of options.\n- Cross validation is the method used to assess the performance of the hyperparameters. Rather than splitting the training set up into separate training and validation sets which reduces the amount of training data we can use, we use K-Fold Cross Validation. This means dividing the training data into K folds, and then going through an iterative process where we first train on K-1 of the folds and then evaluate performance on the kth fold. We repeat this process K times so eventually we will have tested on every example in the training data with the key that each iteration we are testing on data that we did not train on. At the end of K-fold cross validation, we take the average error on each of the K iterations as the final performance measure and then train the model on all the training data at once. The performance we record is then used to compare different combinations of hyperparameters.","b8d72b75":"#### A little bit of feature engineering","99a17256":"- MasVnrArea and MasVnrType: NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","af676109":"- SaleType: Fill in again with most frequent which is \"WD\"","9f752b1a":"- Alley: data description says NA means \"no alley access\"","eb189717":"Here we will implement random search with cross validation to select the optimal hyperparameters for the different machine learning models chosen before. We first define a grid then peform an iterative process of: randomly sample a set of hyperparameters from the grid, evaluate the hyperparameters using 10-fold cross-validation, and then select the hyperparameters with the best performance.","e645675c":"The best ElasticNet Regressor model has the following hyperparameter:\n\n- alpha = 0.0003","973e0f16":"Now we will check if there is any observation with repeated values.","be36bb31":"Scaling refers to the general process of changing the range of a feature. This is necessary because features are measured in different units, and therefore cover different ranges. Methods such as support vector machines that take into account distance measures between observations are significantly affected by the range of the features and scaling allows them to learn. While methods such as Ridge Regression and Random Forest do not actually require feature scaling, it is still best practice to take this step when we are comparing multiple algorithms.\n\nWe will scale the features by putting each one in a range between 0 and 1. This is done by taking each value of a feature, subtracting the minimum value of the feature, and dividing by the maximum minus the minimum (the range). This specific version of scaling is often called normalization and the other main version is known as standardization.\n\nWe can do it using a MinMaxScaler object in Scikit-Learn. Again, we make sure to train only using training data and then transform all the data.","cc22c8f2":"We want to develop a model that is both accurate \u2014 it can predict the 'Sale Price' close to the true value \u2014 and interpretable \u2014 we can understand the model predictions. Once we know the goal, we can use it to guide our decisions as we dig into the data and build models. As we also want to focus on interpretability it is important to understand at least some of the columns. In the 'data_description.txt' file provided by Kaggle there is a description of each feature and their possible values.","89871900":"### Correlations between Features and Target","bd1ffc92":"Now that we have explored the trends and relationships within the data, we can work on engineering a set of features for our models. We can use the results of the EDA to inform this feature engineering. In particular, we learned the following from EDA which can help us in engineering\/selecting features:\n\n- Taking the log transformation of features does not result in significant increases in the linear correlations between features and the score.","b4323406":"- MSSubClass: Na most likely means No building class. We can replace missing values with None","fc70d368":"Each row represents one observation. There are 80 attributes plus the Id column.","259b0748":"The best Lasso Regressor model has the following hyperparameter:\n\n- alpha = 0.0002","875b7515":"## SOLUTION","942b4910":"- MiscFeature: data description says NA means \"no misc feature\"","a97aefbf":"### Non-linear Relationships","4c44b2be":"The following code removes the collinear features based on a threshold we select for the correlation coefficients by removing one of the two features that are compared. It also prints the correlations that it removes so we can see the effect of adjusting the threshold. We will use a threshold of 0.6 which removes one of a pair of features if the correlation coefficient between the features exceeds this value.","6f1dd440":"Kaggle score is 0.14516.","d20e354a":"The best Ridge Regressor model has the following hyperparameter:\n\n- alpha = 0.3","68c08ac9":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","af92a6b3":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","b65ff384":"We will eliminate those variables with a correlation in absolute value lower than 0.3. By applying non-linear transformations to the variables we would already be considering the cases of non-linear relationships.","ef43be59":"##### Ridge Regression","b3f077a1":"To account for possible non-linear relationships, we can take square root and natural log transformations of the features and then calculate the correlation coefficients with the SalePrice. To try and capture any possible relationships between the categorical variables and the SalePrice we will have to one-hot encode these columns.","e73178b1":"Let's plot a heatmap to see the missing data of our complete dataset (train + test):","fe593f7c":"### DATA CLEANING","fddd4407":"- GarageType, GarageFinish, GarageQual and GarageCond: Replacing missing data with None","2f8a9b62":"##### Hyperparameters","bb053c65":"### GET THE DATA","fc637137":"Adding transformed features can help our model learn non-linear relationships within the data. Taking the square root, natural log, or various powers of features is common practice in data science and can be based on domain knowledge or what works best in practice. Here we will include the square root, natural log and power of 2 and 3 of all numerical features.","9c73e343":"#### Dealing with duplicated data","417e6f12":"Now we have computed the 7 different models with their associated RMSE we can visualize which one performs the best on our problem.","0473f0fc":"We will use the Scikit-Learn library in Python, which has great documentation and a consistent model building syntax. Once you know how to make one model in Scikit-Learn, you can quickly implement a diverse range of algorithms.","7556f091":"## COMPETITION DESCRIPTION\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nGOAL\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\nMETRIC\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","7f414fb5":"After transforming the features, the strongest relationships are still those related to OverallQual and GrLivArea. The log and square root transformations do not seem the have resulted in any stronger relationships. Related to negative linear relationships we can see that the top 15 are one-hot encoded representation of the categorical variables.\n\nWe can use these correlations in order to perform feature selection (coming up in a little bit).","d9203dd3":"When we remove outliers, we want to be careful that we are not throwing away measurements just because they look strange. They may be the result of actual phenomenon that we should further investigate. When removing outliers, I try to be as conservative as possible, using the definition of an extreme outlier:\n\n- On the low end, an extreme outlier is below $\\text{First Quartile} -3 * \\text{Interquartile Range}$\n- On the high end, an extreme outlier is above $\\text{Third Quartile} + 3 * \\text{Interquartile Range}$\n\nIn this case, We will only remove the single outlying point and see how the distribution looks.","75f9f23a":"Let\u2019s model the train dataset and generate prediction score by cross-validation.","85832e90":"It's not a strong tendency, but We'd say that the price of a house is higher if it's a new one.","508ea754":"### EXPLORATORY DATA ANALYSIS","556d79d5":"Generally, it\u2019s a good idea to start out with simple, interpretable models such as linear regression, and if the performance is not adequate, move on to more complex, but usually more accurate methods.\n\nWe will evaluate seven different models covering the complexity spectrum:\n- Ridge Regression\n- Lasso Regression\n- Elastic Net Regression\n- Decision Trees Regressor\n- Random Forest Regressor\n- Gradient Boosted Regressor\n- Support Vector Machine Regressor","09b1faa1":"There are a number of methods to calculate collinearity between features, with one of the most common the variance inflation factor. In this project, we will use the correlation coefficient to identify and remove collinear features. We will drop one of a pair of features if the correlation coefficient between them is greater than 0.6.","31f30409":"In the code below, we create the Randomized Search Object passing in the following parameters:\n\n- estimator: the model\n- param_distributions: the distribution of parameters we defined\n- cv the number of folds to use for k-fold cross validation\n- n_iter: the number of different combinations to try\n- scoring: which metric to use when evaluating candidates\n- n_jobs: number of cores to run in parallel (-1 will use all available)\n- verbose: how much information to display (1 displays a limited amount)\n- return_train_score: return the training score for each cross-validation fold\n- random_state: fixes the random number generator used so we get the same results every run\n\nThe Randomized Search Object is trained the same way as any other scikit-learn model. After training, we can compare all the different hyperparameter combinations and find the best performing one.","4a3cda91":"#### Looking for missing values","17b2a4ea":"Yes! both values correspond to the same observation, so it seems that we are in front of an outlier.","b85403d5":"We selected 4 different hyperparameters to tune in the Support Vector Regressor.","67c4d4bd":"- Functional: data description says NA means typical","3c8b77c2":"##### Define a Cross Validation Strategy","daf29cbd":"We have to apply the feature engineering to all of our data (train and test).","0285fba3":"The best Decision Tree model has the following hyperparameters:\n\n- splitter = best \n- max_depth = None\n- min_samples_split = 6\n- min_samples_leaf = 8\n- max_features = auto (This means that max_features=n_features according to the docs)\n\nUsing random search is a good method to narrow down the possible hyperparameters to try. Initially, we had no idea which combination would work the best, but this at least narrows down the range of options.","077aac2f":"Some numerical features of our problem are actually really categories, such as MSSubClass, YrSold and MoSold, so we can convert them to categorical."}}