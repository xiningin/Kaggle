{"cell_type":{"5b8b3a0c":"code","7975b383":"code","a9b0b9fc":"code","490db803":"code","ab7c8880":"code","d1a363ed":"code","8dca8731":"code","c2863382":"code","7b7ab566":"code","40342b6f":"code","89c75d71":"code","6e7046ac":"code","0e0352eb":"code","6d8bc809":"code","a42b46be":"code","c869405f":"code","4214d812":"code","d328a928":"code","f2ca05e7":"code","c1489e84":"code","073ce634":"code","1ef15ead":"code","f1a3a2cb":"code","2c0949de":"code","c0545878":"code","cb18d39f":"code","a56fd505":"code","316fa9f8":"markdown","8366ad83":"markdown","fb76256b":"markdown","58609673":"markdown","17dcd46b":"markdown","1275f05e":"markdown"},"source":{"5b8b3a0c":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import minmax_scale  \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom matplotlib.lines import Line2D\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nprint(os.listdir(\"..\/input\/dataset-wine\"))\n","7975b383":"# Functions\n\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \n# Showing Incorrect Classification\ndef plot_comp_test_data(X_test, y_test, y_head, title):\n    plt.figure(figsize=(12,6))\n    plt.scatter(X_test[:,0], X_test[:,6], c=[colors[i] for i in y_head], marker=\"*\", s=60)\n    n = np.size(y_head)\n    for i in range(0, n):\n        if y_head[i] != y_test[i]:\n            plt.scatter(X_test[i,0], X_test[i,6], c=[colors[y_test[i]]], marker=\"X\", s=120, alpha=.4)\n    legend_elements = [Line2D([0], [0], marker='*', color='w', label='Prediction', markerfacecolor='k', markersize=14),\n                       Line2D([0], [0], marker='X', color='w', label='Correct Class', markerfacecolor='k', markersize=12)]\n    plt.legend(handles=legend_elements)\n    plt.title(title)\n    plt.show()","a9b0b9fc":"# Defining dataset and browsing content\ndata = pd.read_csv('..\/input\/dataset-wine\/wine.csv')\ndata.info()","490db803":"# Show first 10 sample\ndata.head(10)","ab7c8880":"# Split data : X(feature) and y(target) \nX = np.array(data.drop(['Wine'],1))\ny = np.array(data['Wine'])","d1a363ed":"# Visualization Data\nfig = plt.figure(1, figsize=(12, 6))\nax = Axes3D(fig)\ncolors = {1:\"r\", 2:\"g\", 3:\"b\"}\nax.scatter(X[:,0], X[:,6], c=[colors[i] for i in y])\nax.set_xlabel(\"Alcohol\")\nax.set_ylabel(\"Flavanoids\")\nplt.title(\"Values of Wine Data\")\nax.legend()\nplt.show()","8dca8731":"# scaling each feature to a 0 to 1\nX = minmax_scale(X)","c2863382":"# Create Train and Test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","7b7ab566":"# Create k-NN classification model with GridSearchCV\n# n_neighbors: Number of neighbors to use\n# weights: weights is used for distance function (uniform: All points in each neighborhood are weighted equally ||\n#                                                 distance: Weight points by the inverse of their distance.)\n# p: Power parameter for calculate distance. (1: Manhattan Distance. 2: Eucledian Distance. 3 and upper: Minkowski Distance.)\nknn_grid = {\"n_neighbors\":np.arange(1,15), \"weights\":[\"uniform\", \"distance\"], \"p\":[1, 2, 3] }\nknn = GridSearchCV(KNeighborsClassifier(), knn_grid, cv=10, iid=False)\nknn.fit(X_train, y_train)\n\nprint(\"k-NN Tuned Hyperparameters\", knn.best_params_)\nprint(\"k-NN Tuned Best Score:\", round(knn.best_score_,3))","40342b6f":"# Use best classification model\nbest_clf_knn = knn.best_estimator_\nbest_clf_knn.fit(X_train, y_train)\nknn_y_head = best_clf_knn.predict(X_test)\nprint(\"k-Nearest Neighbors (k-NN) Classification Accuracy: {}%\" .format(round(best_clf_knn.score(X_test, y_test)*100,2)))","89c75d71":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, knn_y_head,title=\"k-Nearest Neighbors Classification\")","6e7046ac":"# Show Confusion Matrix\nplot_cm(y_test, knn_y_head, title=\"k-NN Confusion Matrix\")","0e0352eb":"# Create DT classification model with GridSearchCV\n# criterion: The function to measure the quality of a split. (Gini: Gini impurity. || Entropy: Information gain)\n# min_samples_split: The minimum number of samples required to split an internal node.\n# min_samples_leaf: The minimum number of samples required to be at a leaf node.\ndtree_grid = {\"criterion\":[\"gini\", \"entropy\"], \"min_samples_split\":[3,4,5,6,7,8,9,10], \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9]}\ndtree = GridSearchCV(DecisionTreeClassifier(random_state=42), dtree_grid, cv=10, iid=False)\ndtree.fit(X_train, y_train)\n\nprint(\"DT Tuned Hyperparameters\", dtree.best_params_)\nprint(\"DT Tuned Best Score:\", round(dtree.best_score_,3))","6d8bc809":"# Use best classification model\nbest_clf_dt = dtree.best_estimator_\nbest_clf_dt.fit(X_train, y_train)\ndtree_y_head = best_clf_dt.predict(X_test)\nprint(\"Decision Tree (DT) Classification Accuracy: {}%\" .format(round(best_clf_dt.score(X_test, y_test)*100,2)))","a42b46be":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, dtree_y_head, title=\"Decision Tree Classification\")","c869405f":"# Show Confusion Matrix \nplot_cm(y_test, dtree_y_head, title=\"DT Confusion Matrix\")","4214d812":"# Create RF classification model with GridSearchCV\n# n_estimators: The number of trees in the forest.\n# criterion, min_samples_split and min_samples_leaf are same to decision tree parameters.\nrf_grid = {\"n_estimators\": [50, 100, 150], \"criterion\": [\"gini\", \"entropy\"],\n           \"min_samples_split\":[3,4,5,6,7,8,9,10], \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9]}\nrforest = GridSearchCV(RandomForestClassifier(random_state=42), rf_grid, cv=10, iid=False)\nrforest.fit(X_train, y_train)\n\nprint(\"RF Tuned Hyperparameters\", rforest.best_params_)\nprint(\"RF Tuned Best Score:\", round(rforest.best_score_,3))","d328a928":"best_clf_rforest = rforest.best_estimator_\nbest_clf_rforest.fit(X_train, y_train)\nrforest_y_head = best_clf_rforest.predict(X_test)\nprint(\"Random Forest (RF) Classification Accuracy: {}%\" .format(round(best_clf_rforest.score(X_test, y_test)*100,2)))","f2ca05e7":"#Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, rforest_y_head, title=\"Random Forest Classification\")","c1489e84":"# Show Confusion Matrix\nplot_cm(y_test, rforest_y_head, title=\"RF Confusion Matrix\")","073ce634":"# Create SVM classification model with GridSearchCV\n# C: Penalty parameter C of the error term.\n# kernel: Specifies the kernel type to be used in the algorithm.\n# gamma: Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\nsvm_grid = {\"C\":[0.001, 0.01, 0.1, 1, 10, 100], \"kernel\":[\"rbf\", \"poly\", \"linear\"],\n        \"gamma\":[\"auto\", \"scale\"]}\n\nsvm = GridSearchCV(SVC(decision_function_shape='ovo'), svm_grid, cv=10, iid=False)\nsvm.fit(X_train, y_train)\n\nprint(\"SVM Tuned Hyperparameters\", svm.best_params_)\nprint(\"SVM Tuned Best Score:\", round(svm.best_score_,3))","1ef15ead":"# Use best classification model\nbest_clf_svm = svm.best_estimator_\nbest_clf_svm.fit(X_train, y_train)\nsvm_y_head = best_clf_svm.predict(X_test)\nprint(\"Support Vector Machine (SVM) Classification Accuracy: {}%\" .format(round(best_clf_svm.score(X_test, y_test)*100,2)))","f1a3a2cb":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, svm_y_head, title=\"Support Vector Machine Classification\")","2c0949de":"# Show Confusion Matrix \nplot_cm(y_test, svm_y_head, title=\"SVM Confusion Matrix\")","c0545878":"# Create Naive Bayes model GaussianNB classifier\n# priors: Prior probabilities of the classes.\nnb = GaussianNB(priors=None)\nnb.fit(X_train, y_train)\n\nprint(\"Gaussian Naive Bayes (NB) Classification Accuracy: {}%\" .format(round(nb.score(X_test, y_test)*100, 2)))\nnb_y_head = nb.predict(X_test)","cb18d39f":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, nb_y_head, title=\"Naive Bayes Classification\")","a56fd505":"# Show Confusion Matrix\nplot_cm(y_test, nb_y_head,title=\"NB Confusion Matrix\")","316fa9f8":"## ** Decision Tree (DT) Classification**","8366ad83":"## **Naive Bayes (NB) Classification**","fb76256b":"## **Preparing Dataset for Classification**","58609673":"## **k-Nearest Neighbors (k-NN) Classification**","17dcd46b":"## **Support Vector Machine (SVM) Classification**","1275f05e":"## ** Random Forest (RF) Classification**"}}