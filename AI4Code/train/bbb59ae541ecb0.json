{"cell_type":{"3e612316":"code","5fbaca2a":"code","20bb4c84":"code","2a8e3d28":"code","6b7c447c":"code","8e9ae448":"code","25bebfdf":"code","96accae3":"code","92913579":"code","ec2854d7":"code","a3886c77":"code","a3c58fcf":"code","83ea69e1":"code","ff635291":"code","3d6f707c":"code","da9671e1":"code","666c4d5f":"code","fe7c0087":"code","b66aec8b":"code","b192980e":"code","65c4f308":"code","9614368c":"code","b1e92493":"code","5d01373a":"code","7451f986":"code","2c558a86":"code","aefd2210":"code","6b1f1adf":"code","1999b6dc":"code","336e4457":"code","596b421d":"code","e380d213":"code","ec79aa49":"code","a0a5766f":"code","0cbb4c89":"code","c9ddf497":"code","a79d48e8":"code","51ce2f40":"code","4a1cd120":"markdown","786ac053":"markdown","ddadf6e1":"markdown","d5945119":"markdown","4bbe6231":"markdown","313f130c":"markdown","43458f68":"markdown","d39d37bc":"markdown","5c480216":"markdown","d8eb415c":"markdown","a384f5da":"markdown","e2c0c202":"markdown","103f7ad0":"markdown","47b4eace":"markdown","9444e933":"markdown","f55fa0c2":"markdown","27074dbf":"markdown","aae00780":"markdown","f76955bf":"markdown","da0fb4fe":"markdown","9a893820":"markdown","14d46bd4":"markdown","c4286f5f":"markdown"},"source":{"3e612316":"#%%\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n#import pymssql\nimport tqdm\n#from sklearn.ensemble import GradientBoostingClassifier\nimport sklearn.metrics as sm\nfrom sklearn.model_selection import KFold\nimport logging\nimport itertools\nfrom sklearn.ensemble import RandomForestRegressor\n#LGBM\nimport lightgbm as lgb\nfrom collections import defaultdict","5fbaca2a":"# run on all\/ any schema\ndef get_data_files(filePaths):\n    data=pd.DataFrame()\n    for csvfile in filePaths:\n        df = pd.read_csv(csvfile)\n        data=pd.concat([df,data],ignore_index=True)\n    return data","20bb4c84":"filePaths = ['..\/input\/churn-telco-europa\/test_churn_kg.csv','..\/input\/churn-telco-europa\/train_churn_kg.csv']\ndf = get_data_files(filePaths)","2a8e3d28":"df.head()","6b7c447c":"df.describe().T","8e9ae448":"df0 = pd.DataFrame(df.groupby(\"CNI_CUSTOMER\")[\"CETEL_NUMBER\"].count())\ndf = pd.merge(df,df0,on='CNI_CUSTOMER')\ndf.head()","25bebfdf":"df.rename(columns={'CETEL_NUMBER_y':'linesPerCst'}, inplace=True)","96accae3":"df['exceededMinutes'] = df['AVG_MIN_CALL_OUT_3'] - df['MIN_PLAN']\ndf['planRateMinute'] = df['PRICE_PLAN'] \/ (df['MIN_PLAN'] + 1)\ndf['effectiveRateMinute'] = df['PRICE_PLAN'] \/ (df['AVG_MIN_CALL_OUT_3'] + 1)\ndf['effectiveRateMinuteLastMonth'] = df['PRICE_PLAN'] \/ (df['TOT_MIN_IN_ULT_MES'] + 1)\ndf['planVSEffectiveRateMinute'] = df['planRateMinute'] - df['effectiveRateMinute']","92913579":"df0 = pd.DataFrame(df.groupby(\"CNI_CUSTOMER\")[\"effectiveRateMinute\"].mean())\ndf = pd.merge(df,df0,on='CNI_CUSTOMER')\ndf.head()","ec2854d7":"df.rename(columns={'effectiveRateMinute_y':'effectiveRatePerCst'}, inplace=True)","a3886c77":"df = df[df.CHURN == 1]\n\ndrop_cols = ['CNI_CUSTOMER','CETEL_NUMBER_x','TOT_MIN_CALL_OUT','CHURN'];\n#drop_cols.append(dropppers)\ndfjunk = df[drop_cols]\ndf = df.drop((drop_cols), axis = 1)\n\n","a3c58fcf":"corrdf = df.drop(['STATE_VOICE','STATE_DATA','CITY_VOICE','CITY_DATA','TEC_ANT_VOICE','TEC_ANT_DATA'], axis=1)","83ea69e1":"sns.set_theme(style=\"white\")\ncorr = corrdf.corr()\nf, ax = plt.subplots(figsize=(11, 9))\ncolormap = sns.diverging_palette(230, 20, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap=colormap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","ff635291":"plt.hist(df['DAYS_LIFE'], bins = 50)\nplt.suptitle(f'Life time in days')\nplt.show()","3d6f707c":"Customer distribution by Exceeded minutes","da9671e1":"plt.hist(df['exceededMinutes'], bins = 50)\nplt.suptitle(f'Exceeded minutes')\nplt.show()","666c4d5f":"tmp = df.groupby('DEVICE')['DAYS_LIFE']\nlbls = ['Sony','Motorola','Huawei','Apple','LG','Nokia','Samsung','Alcatel','Verykool','Lenovo','Mobiwire','ZTE','Sony Ericsson','BlackBerry','VSN Mobil','VSN','T610','Other']","fe7c0087":"tmp.count().head(50)","b66aec8b":"plt.pie(tmp.count(),labels = lbls)\nplt.show()","b192980e":"plt.figure()\nplt.barh(lbls,tmp.mean())\nplt.show()","65c4f308":"tmp = df.groupby('MIN_PLAN')['DAYS_LIFE']","9614368c":"plt.pie(tmp.count(),labels = df['MIN_PLAN'].unique())\nplt.show()","b1e92493":"plt.figure()\nplt.barh([\"%.0f\" % number for number in df['MIN_PLAN'].unique()],tmp.mean())\nplt.show()","5d01373a":"df.isnull().values.any()","7451f986":"df.info()","2c558a86":"for col in df.columns:\n    if df[col].isnull().values.any():\n            print(col)\n            #print (col + \": {}\".format(df[col].isnull.values.any()))\n            # after checking all values can receive a zero, uncomment the command below\n            df[col] = df[col].fillna(0)","aefd2210":"df[['STATE_DATA','CITY_DATA','STATE_VOICE','CITY_VOICE']].head()","6b1f1adf":"#outlier check and remove\nfor col in df.columns:\n        if(df[col].dtype == 'float64' or df[col].dtype == 'int64'):\n                print (col + \" q95: \" + str(round(df[col].quantile(0.95),2)) + \" q90: \" + str(round(df[col].quantile(0.91),2)) )","1999b6dc":"#to run only once !\nlist95cut = []\nfor col in list95cut:\n    df = df[df[col] < df[col].quantile(0.91)]","336e4457":"cats = []\nfor col in df.columns:\n    if df[col].dtype == 'O':\n        cats.append(col)\n\nmancats = ['DEVICE_TECNOLOGY','DEVICE','TEC_ANT_DATA','STATE_DATA','CITY_DATA','TEC_ANT_VOICE','STATE_VOICE','CITY_VOICE']\ncats = cats + mancats\nprint (cats)","596b421d":"logging.basicConfig(format='%(asctime)s [%(levelname)-8s] %(message)s')\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nlogger.info('Logging has been set up...')","e380d213":"# Encoding of the categorical variable\n#for f in tqdm.tqdm(cats):\n#    lbl = LabelEncoder()\n#    lbl.fit(list(df[f].values))\n#    df[f] = lbl.transform(list(df[f].values))\n## one version of it - the problem - the labels are not saved\n## no way to rerun the model on new data with the same labels like that","ec79aa49":"## running and saving the label encoders in a dictionary\nd = defaultdict(LabelEncoder)\nfit = df[cats].apply(lambda x: d[x.name].fit_transform(x))\nfor cat in cats:\n    df[cat] = fit[cat]","a0a5766f":"# Train test split\ndf_train, df_test = train_test_split(df, test_size=0.3) \n\n# split of the dependent and independent variables\ntarget = 'DAYS_LIFE' # set the target variable here\nrem_cols = []\n\nrem_cols.append(target)\nX = df_train.drop(rem_cols, axis=1)\nY = df_train[target]\nX_test = df_test.drop(rem_cols, axis=1)\nrem_cols.remove(target)\n\n\ndf_train['DAYS_LIFE'].value_counts(normalize=True) # with target variable\n\ndf_test['DAYS_LIFE'].value_counts() # target variable","0cbb4c89":"params = {\n    'boosting_type':'gbdt',\n    'n_estimators':600,\n    'num_leaves':32, \n    'objective':'regression_l2', \n    'colsample_bytree': 1.0, \n    'learning_rate': 0.05,\n    'max_bin':255, \n    'max_depth':9, \n    'metric':'rmse', \n    'min_child_samples':10,\n    'min_child_weight':5, \n    'min_split_gain':0.0,\n    'random_state':1234,\n    'lambda_l2': 1.5,\n    'verbose' : -1,\n    #'reg_alpha':0.0,\n    #'reg_lambda':0.0, \n    #'silent':True, \n    'subsample':1.0\n    #,\n    #'early_stopping_round' : 100\n}\nmdl = lgb.LGBMRegressor(**params)","c9ddf497":"# ######## folding\n\nn_folds = 5\nkf = KFold(n_splits = n_folds, random_state = 1, shuffle = True)\n\nscores = []\nsubmit_pred = np.zeros((X_test.shape[0],1))\nget_probab = False\n\nfor i, (train_index, test_index) in enumerate(kf.split(X, Y)):\n    # Create data for this fold\n    Y_train, Y_valid = Y.iloc[train_index].copy(), Y.iloc[test_index].copy()\n    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n                        \n    logger.info( f'Fold: {i}')\n\n    fit_model = mdl\n\n    fit_model.fit(X_train, Y_train)\n                    \n    if get_probab:\n        pred = fit_model.predict_proba(X_valid)[:,1]\n        submit_pred += fit_model.predict_proba(X_test)[:,1]\/n_folds\n\n    else:\n        full_fold_pred = fit_model.predict(X_test)\n        full_fold_pred = np.reshape(full_fold_pred, (full_fold_pred.shape[0],1))\n        pred = fit_model.predict(X_valid)\n        submit_pred += full_fold_pred\/n_folds\n                    \n    # Save validation predictions for this fold\n    score = sm.r2_score(Y_valid, pred)\n    \n    print(\"mean absolute error: \", round(sm.mean_absolute_error(Y_valid, pred),2))\n    print(\"mean squared error: \", round(sm.mean_squared_error(Y_valid, pred),2))\n    print(\"median absolute error: \", round(sm.median_absolute_error(Y_valid, pred),2))\n    print(\"explained variance score: \", round(sm.explained_variance_score(Y_valid, pred),2))\n    print(\"r2 score: \", round(sm.r2_score(Y_valid, pred),2))\n\n    logger.info( f'The valuation metric for the fold {i} is {score}')\n    scores.append(score)\n\n    plt.hist(pred, bins = 50)\n    plt.suptitle(f'Life time in months - prediction')\n    plt.show()\n    \n    plt.hist(Y_valid, bins = 50)\n    plt.suptitle(f'Life time in months - fold test data')\n    plt.show()\n\nave_score = np.mean(scores)\nlogger.info(f'The average score accross the folds is {ave_score}')","a79d48e8":"\n\nprint(\"r2 score, prediction on test data: \", round(sm.r2_score(df_test[target], submit_pred),2))\nplt.hist(submit_pred, bins = 50)\nplt.suptitle(f'Life time in months - prediction')\nplt.show()\n\nplt.hist(df_test[target], bins = 50)\nplt.suptitle(f'Life time in months - test data')\nplt.show()\n\nprint(\"Prediction description\")\npd.DataFrame(submit_pred).describe().T","51ce2f40":"# vars used\n\nplt.figure(figsize=(12,6))\nfeat_importances = pd.Series(mdl.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(25).sort_values().plot(kind='barh')\n\nplt.show()","4a1cd120":"**variables importance**","786ac053":"minutes plan","ddadf6e1":"# *missing values check*","d5945119":"***Data enriching from within the dataset itself***\n\n1st of all - we'll calculate the number of lines per customer, and add the result to each phone line's row - in essense we'll know how many \"siblings\" does each line have (including its own)\n\nNext - we'll calculate the usage vs minutes in plan, and the formal (by plan), and effective (by usage) rare per plan\n\nLastly - we'll calculate the average effective rate per *customer*, and add it to each row\n_____________________________________________________________________________________________________","4bbe6231":"# preparing the data ","313f130c":"3. the average effective rate per customer, and add it to each row","43458f68":"***K folding and running the model***","d39d37bc":"Price plan customer distibution","5c480216":"1. the number of lines per customer, and add the result to each phone line's row - in essense we'll know how many \"siblings\" does each line have (including its own)","d8eb415c":"2. The usage vs minutes in plan, and the formal (by plan), and effective (by usage) rare per plan","a384f5da":"**Setting the light GBM model **","e2c0c202":"Life time per device","103f7ad0":"after checking all values can receive a zero, we add the last row to the loop and rerun it\n```python\ndf[col] = df[col].fillna(0)\n```","47b4eace":"**correlation heat map**\n*only numeric varibles to check, city and state data are categories labeled*","9444e933":"**outlier check and removal**","f55fa0c2":"## Data in graphs","27074dbf":"# columns description","aae00780":"**Remove unnecessary data**\n\nIn order to calculate the lifetime, not only are the IDs irrelvant\nbut we need to remove all cases where customers did not churn: for the rest of customers the lifetime isn't final\nwe can estimate their future time of churn, but we won't really know it...\n\nwe also need to drop the total outgoing minutes field, since it's kind of letting the model cheat\n    ","f76955bf":"**find categorial variable to encode for the model**","da0fb4fe":"lifetime distribution","9a893820":"**a quick look into the raw data:**","14d46bd4":"set logging","c4286f5f":"**The columns description as were written by the contributor**\n\nDiccionario de Datos \/ Data Dictionary\n* CETEL_NUMBER : Corresponde al numero de telefono del cliente \/ phonenumber customer\n* CNI_CUSTOMER : Identificador del cliente \/ DNI \/ CNI \/ ID customer\n* DAYS_LIFE : Los d?as de vida del cliente en la compa?ia \/ Days of life customer\n* DEVICE_TECNOLOGY : referente a la gama del equipo telefonico (1 = AWS , 2 = No AWS, 3 = 3G Only, 4 = 4g Only, 5 sin clasificar \/ phone device tecnology\n* MIN_PLAN : Minutos incluidos en el plan contratado \/ minutes include contract\n* PRICE_PLAN : Dinero por el contrato del plan \/ money plan contracted\n* TOT_MIN_CALL_OUT : cantidad de minutos de Call OUT \/ count of minutes call out\n* AVG_MIN_CALL_OUT_3: Promedio de Call Out ultimos 3 meses \/ avg minutes call out last 3 months\n* ROA_LASTMONTH : Uso de datos en redes de otras compa?ias \/ Data used in networks of other companies\n* DEVICE: ( 1 Sony, 2 Motorola, 3 Huawei, 4 Apple, 5 LG 6 Nokia, 7 Samsung, 8 Alcatel, 9 Verykool, 10 Lenovo, 11 Mobiwire, 12 ZTE, 13 Sony Ericsson, 14 BlackBerry, 15 VSN Mobil, 16 VSN, 17 T610, 18 Other)\n* TEC_ANT_DATA: Tecnolog?a de la antena de datos \/ Tecnology data antenna\n* TEC_ANT_VOICE: Tecnolog?a de la antena de voz \/ Tecnology voice antenna\n* STATE\/CITY: Tecnolog?a de las antenas en estado y ciudad \/ Tecnology antenna in state and city (State 1,2...100), (comuna 1,2,3,....320)\n* CHURN: 0 Cliente fugadado, 1 cliente no fugado \/ 0 = Churn Yes, 1 Churn No"}}