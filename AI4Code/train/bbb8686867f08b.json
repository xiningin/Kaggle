{"cell_type":{"78e44dc0":"code","928e798d":"code","d6fb87f2":"code","5b77b6fc":"code","f5295383":"code","78e6ef80":"code","65da0cb3":"code","cbccd4f7":"code","0d990e2c":"code","4db5ee3f":"code","9572d6f9":"code","b9f1c54b":"code","1622b0fd":"code","6f9908eb":"code","be2b3d7e":"code","b33207f9":"code","81f9b5e6":"code","2f318a72":"code","23495aa7":"code","d31428ce":"code","ecb44f0b":"code","fc5c5528":"code","e44b7533":"code","1e599087":"code","a5667d39":"code","f87f5b0a":"code","5e85a93e":"code","e5a08954":"markdown","18354692":"markdown","ce75f6b3":"markdown","aa4ef755":"markdown","d318b728":"markdown","741fc486":"markdown","7ff21c5e":"markdown","4a94b3b3":"markdown","cd57f987":"markdown","b3a08cfa":"markdown","142435da":"markdown","6461a199":"markdown","306bfd1d":"markdown","5fba35bd":"markdown"},"source":{"78e44dc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.cluster import KMeans\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","928e798d":"data = pd.read_csv('\/kaggle\/input\/ultimate-spotify-tracks-db\/SpotifyFeatures.csv')\ndata.head()","d6fb87f2":"data.describe()","5b77b6fc":"# columns = data.columns.values\n# for col in columns:\n#     df_null = data.isnull().groupby(['genre']).size()\n#     print(df_null.head())\n\ndata.isnull().sum()","f5295383":"data.info()","78e6ef80":"for col in data.columns.values:\n    print(col,'\\t',data[col].nunique())\n\n","65da0cb3":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True)","cbccd4f7":"bar_cols = data[['genre','key','mode','time_signature']].columns.values\nfor col in bar_cols:\n    df_temp = data.groupby([col]).size().reset_index(name='count')\n    plt.figure(figsize=(18,8))\n    plt.xticks(rotation=45)\n    sns.set_style(\"ticks\")\n    sns.barplot(data = df_temp, x= col, y= 'count')","0d990e2c":"# 'popularity','acousticness','danceability','energy','instrumentalness','liveness','loudness','speechiness','tempo','valence'\n# dropping popularity adding key and mode\n# data = data[data['popularity'] > 50]\nX = data[['genre','acousticness','danceability','energy','instrumentalness','liveness','loudness','speechiness','tempo','valence']]\nX.head()\n# len(X)","4db5ee3f":"dummy_genre = pd.get_dummies(X['genre'],drop_first=True)\nX = pd.concat([dummy_genre, X.iloc[:,1:]], axis=1)\nX.head()\n\n","9572d6f9":"X['loudness'] = X['loudness']\/10.0\nX['tempo'] = X['tempo']\/100.0\nprint(X.columns.values)\nl_ofcols = ['acousticness','danceability','energy','instrumentalness','liveness','loudness','speechiness','tempo','valence']\nfor cols in l_ofcols:\n    X[cols] = X[cols] * 10.0\n    print(cols,'\\tMAX\\t',X[cols].max(),'\\tMIN\\t',X[cols].min())","b9f1c54b":"df_artist = data.groupby(['artist_name']).size().reset_index(name='count').sort_values(['count'],ascending = False)\nprint(df_artist.head(25))","1622b0fd":"# \n# import time\n# wcss = []\n# print('Started\\n')\n# for i in range(1,50):\n#     kmeans = KMeans(n_clusters = i,random_state = 0,n_jobs = -1)\n#     y = kmeans.fit(X)\n#     wcss_temp = kmeans.inertia_\n#     if(len(wcss)>0):\n#         print(time.ctime(),\" done for \\t\",i,\" with wcss as \\t\",wcss_temp,\" diff \\t\",(wcss[-1] - wcss_temp))\n#     else:\n#         print(time.ctime(),\" done for \\t\",i,\" with wcss as \\t\",wcss_temp,\" diff \\t\",(wcss_temp))\n#     wcss.append(wcss_temp)\n\n# plt.figure(figsize=(18,8))\n# plt.plot(range(1,50),wcss)\n\n# kmeans = KMeans(n_clusters = 10,random_state = 0,n_jobs = -1)\n# clusters = kmeans.fit_predict(X)\n# print(clusters)\n","6f9908eb":"# from sklearn.cluster import AgglomerativeClustering\n# x_limit = X.iloc[:50000,:]\n# # x_limit.head()\n# agg_clustering = AgglomerativeClustering(n_clusters = 2)\n# agg_clusters = agg_clustering.fit_predict(x_limit)\n# print(agg_clusters)","be2b3d7e":"# from sklearn.cluster import DBSCAN\n# dbs_clustering = DBSCAN(eps= 1.10)\n# dbs_clusters = dbs_clustering.fit_predict(X)\n# print(dbs_clusters)","b33207f9":"# from sklearn.cluster import OPTICS\n# import time\n\n# print('Start at ',time.ctime())\n# opt_clustering = OPTICS(min_samples = 2,max_eps= 12.0,xi = 0.05,min_cluster_size=5,n_jobs = -1)\n# # opt_clustering = OPTICS(n_jobs = -1)\n# opt_clusters = opt_clustering.fit_predict(X_temp)\n# print(np.unique(opt_clusters))\n# print('End at ',time.ctime())","81f9b5e6":"from sklearn.cluster import Birch\nimport time\n\nprint('Start at ',time.ctime())\nbirch_clustering = Birch(threshold = 3.0,branching_factor = 50,n_clusters = None ,compute_labels = True)\n# opt_clustering = OPTICS(n_jobs = -1)\nbirch_clusters = birch_clustering.fit_predict(X)\nprint(np.unique(birch_clusters))\nprint('End at ',time.ctime())","2f318a72":"# unique,counts = np.unique(birch_clusters,return_counts = True)\n# print(type(unique),type(counts))\n# plt.figure(figsize=(22,8))\n# plt.xticks(rotation=90)\n# sns.set_style(\"ticks\")\n# sns.barplot(x=unique,y=counts)","23495aa7":"# t_df = birch_clusters\n# plt.figure(figsize=(18,8))\n# plt.xticks(rotation=45)\n# sns.set_style(\"ticks\")\n# plot_cluster_df = pd.DataFrame({'cluster':t_df[:]})\n# plot_cluster_df = plot_cluster_df.groupby(['cluster']).size().reset_index(name = 'count')\n# sns.barplot(data = plot_cluster_df,x='cluster',y='count')\n","d31428ce":"t_clusters = birch_clusters\noutput_df = data[['artist_name','track_name','track_id']]\noutput_df['cluster'] = t_clusters.tolist()\ntemp = output_df.groupby(['cluster']).size().reset_index(name = 'count').sort_values(['count'],ascending = False)\ntemp.head(10)","ecb44f0b":"output_df[output_df['artist_name'] == 'Drake'].head(20)","fc5c5528":"cluster_look_up = 3084\noutput_df[output_df['cluster'] == cluster_look_up]['track_id'].nunique()","e44b7533":"s_ofsongs = output_df[output_df['cluster'] == cluster_look_up].iloc[:,2]\n# print(s_ofsongs)\ndata[data['track_id'].isin(s_ofsongs)].sort_values(by=['popularity'],ascending = False).head(10)","1e599087":"plot_data = data[data['track_id'].isin(s_ofsongs)][['genre','artist_name','track_name']]\n# sns.barplot(data = plot_data,X = 'genre', y = plot_data.value_counts())","a5667d39":"# unique,counts = np.unique(birch_clusters,return_counts = True)\n# print(type(unique),type(counts))\nplt.figure(figsize=(22,8))\nplt.xticks(rotation=90)\nsns.set_style(\"ticks\")\n# sns.barplot(x=unique,y=counts)\nsns.barplot(data = plot_data, x= plot_data['genre'].unique(), y=plot_data['genre'].value_counts() )","f87f5b0a":"# correct lable names\nplt.figure(figsize=(22,8))\nplt.xticks(rotation=90)\nsns.set_style(\"ticks\")\nsns.barplot(data = plot_data, x= plot_data['artist_name'].unique(), y=plot_data['artist_name'].value_counts() )","5e85a93e":"plot_data['List'] = plot_data['artist_name']+' -> '+plot_data['track_name']\nplot_data['List'].unique()","e5a08954":"Below are list of genres for this cluster\/playlist.","18354692":"Below are artist with similar songs","ce75f6b3":"Looking for nulls. Fortunately there aren't any.","aa4ef755":"Joining output of clusters to original dataset.","d318b728":"For getting the desired output. I tried a few clustering algorithms.\n* k-means -> I figured out it wouldnt work as the number of clusters will be low. If we go with it we could max generate 100 playlist\n* AgglomerativeClustering -> Memory issue had to reduce the dataset to half to make it work.\n* OPTICS -> It took way to long(6 hrs+) to process but output was not great.\n\nSo after trial and testing above algorithms I tried Birch\n","741fc486":"Below is unique list of all the sonns in this cluster","7ff21c5e":"Checking for correlation between the columns","4a94b3b3":"Scaling all attributes together. \nLoudness and tempo had high values so they would impact the dataset. Instead reduced them to be equal to other attributes. So all cols have similar equal values\n\nReason for multiplying all be 10 -> Most of the values were less than 1. Runnning clustering algorithms were giving memory issues when they were less so instead upscaled it. I assume its because in clustering algorithm all values will be closer at each neighoour and which would require more memory to compute","cd57f987":"Checking different genre present.\nand also looking for if all of genres are equal or not","b3a08cfa":"One hot encoding genre attribute","142435da":"Output of similar songs\nIf we observe God's plan fall under 3 genres hip-hop,rap and pop.","6461a199":"Hey, let me start by saying i am new to machine learning. So a constructive feedback would really be helpful.\nWhat i am trying to achieve here is create a playlist of similar songs together.\n","306bfd1d":"Picking attributes that can help find similar music","5fba35bd":"Let's look for similar songs to God's plan\n\nThere are 37 sons similar to it"}}