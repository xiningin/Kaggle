{"cell_type":{"a9e95c1f":"code","dd0d7c89":"code","c000a616":"code","4c3db2ad":"code","90ef0a4f":"code","cae59457":"code","aeaedb08":"code","c29da359":"code","6842b177":"code","949d00a2":"code","5d004687":"code","f560b8df":"code","c528e6c0":"code","372a79fc":"code","b69bcf28":"code","059fa2b2":"code","bbadaf03":"code","b3b3e630":"code","efe66011":"code","e9733362":"code","5b412633":"code","fc245633":"code","e102fcde":"code","7a14a437":"code","bdc5d086":"code","f7298d7c":"code","fa62b865":"code","34f1cf76":"code","f8a01fbc":"code","5ffd085d":"code","d77698cd":"code","88bfaa94":"code","addcab10":"code","fbb0626b":"code","adb7c829":"code","edf2259b":"code","008b29d0":"code","487061d7":"code","4a7534a9":"code","b5d5cf27":"markdown","e895fdae":"markdown","4898bcc6":"markdown","474fa40b":"markdown","9a96c4d4":"markdown","a9ffa93c":"markdown","d2a9e681":"markdown","0990de6e":"markdown","0e8f0037":"markdown","18740f11":"markdown","2555b23e":"markdown","1f36b396":"markdown","eb25a8e7":"markdown","148072cd":"markdown","2dab6165":"markdown","087fa71f":"markdown","f22af0e4":"markdown","ce1cf596":"markdown","928310ff":"markdown","77fcecef":"markdown","6004f345":"markdown","9eded81d":"markdown","d2ce8ffb":"markdown","7b17e798":"markdown","415a6814":"markdown","77f487e8":"markdown","544f9271":"markdown","bef1a393":"markdown","6cb8c15a":"markdown","8901778c":"markdown","a39fb515":"markdown"},"source":{"a9e95c1f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# sharper plots\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","dd0d7c89":"df = pd.read_csv('..\/input\/mediumpostfbprophet\/medium_posts.csv')","c000a616":"df.head()","4c3db2ad":"df = df[['published', 'url']].dropna().drop_duplicates()","90ef0a4f":"df['published'] = pd.to_datetime(df['published'])","cae59457":"df.sort_values(by=['published']).head(n=3)","aeaedb08":"df = df[(df['published'] > '2012-08-15') & (df['published'] < '2017-06-26')].sort_values(by=['published'])\ndf.head(n=3)","c29da359":"df.tail(n=3)","6842b177":"aggr_df = df.groupby('published')[['url']].count()\naggr_df.columns = ['posts']","949d00a2":"aggr_df.head(n=3)","5d004687":"daily_df = aggr_df.resample('D').apply(sum)\ndaily_df.head(n=3)","f560b8df":"from plotly.offline import init_notebook_mode, iplot\nfrom plotly import graph_objs as go\n\n# Initialize plotly\ninit_notebook_mode(connected=True)","c528e6c0":"def plotly_df(df, title=''):\n    \"\"\"Visualize all the dataframe columns as line plots.\"\"\"\n    common_kw = dict(x=df.index, mode='lines')\n    data = [go.Scatter(y=df[c], name=c, **common_kw) for c in df.columns]\n    layout = dict(title=title)\n    fig = dict(data=data, layout=layout)\n    iplot(fig, show_link=False)","372a79fc":"plotly_df(daily_df, title='Posts on Medium (daily)')","b69bcf28":"weekly_df = daily_df.resample('W').apply(sum)","059fa2b2":"plotly_df(weekly_df, title='Posts on Medium (weekly)')","bbadaf03":"daily_df = daily_df.loc[daily_df.index >= '2015-01-01']\ndaily_df.head(n=3)","b3b3e630":"from fbprophet import Prophet\n\nimport logging\nlogging.getLogger().setLevel(logging.ERROR)","efe66011":"df = daily_df.reset_index()\ndf.columns = ['ds', 'y']\n# converting timezones (issue https:\/\/github.com\/facebook\/prophet\/issues\/831)\ndf['ds'] = df['ds'].dt.tz_convert(None)\ndf.tail(n=3)","e9733362":"prediction_size = 30\ntrain_df = df[:-prediction_size]\ntrain_df.tail(n=3)","5b412633":"m = Prophet()\nm.fit(train_df);","fc245633":"future = m.make_future_dataframe(periods=prediction_size)\nfuture.tail(n=3)","e102fcde":"forecast = m.predict(future)\nforecast.tail(n=3)","7a14a437":"m.plot(forecast);","bdc5d086":"m.plot_components(forecast);","f7298d7c":"print(', '.join(forecast.columns))","fa62b865":"def make_comparison_dataframe(historical, forecast):\n    \"\"\"Join the history with the forecast.\n    \n       The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.\n    \"\"\"\n    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))","34f1cf76":"cmp_df = make_comparison_dataframe(df, forecast)\ncmp_df.tail(n=3)","f8a01fbc":"def calculate_forecast_errors(df, prediction_size):\n    \"\"\"Calculate MAPE and MAE of the forecast.\n    \n       Args:\n           df: joined dataset with 'y' and 'yhat' columns.\n           prediction_size: number of days at the end to predict.\n    \"\"\"\n    \n    # Make a copy\n    df = df.copy()\n    \n    # Now we calculate the values of e_i and p_i according to the formulas given in the article above.\n    df['e'] = df['y'] - df['yhat']\n    df['p'] = 100 * df['e'] \/ df['y']\n    \n    # Recall that we held out the values of the last `prediction_size` days\n    # in order to predict them and measure the quality of the model. \n    \n    # Now cut out the part of the data which we made our prediction for.\n    predicted_part = df[-prediction_size:]\n    \n    # Define the function that averages absolute error values over the predicted part.\n    error_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n    \n    # Now we can calculate MAPE and MAE and return the resulting dictionary of errors.\n    return {'MAPE': error_mean('p'), 'MAE': error_mean('e')}","5ffd085d":"for err_name, err_value in calculate_forecast_errors(cmp_df, prediction_size).items():\n    print(err_name, err_value)","d77698cd":"def show_forecast(cmp_df, num_predictions, num_values, title):\n    \"\"\"Visualize the forecast.\"\"\"\n    \n    def create_go(name, column, num, **kwargs):\n        points = cmp_df.tail(num)\n        args = dict(name=name, x=points.index, y=points[column], mode='lines')\n        args.update(kwargs)\n        return go.Scatter(**args)\n    \n    lower_bound = create_go('Lower Bound', 'yhat_lower', num_predictions,\n                            line=dict(width=0),\n                            marker=dict(color=\"gray\"))\n    upper_bound = create_go('Upper Bound', 'yhat_upper', num_predictions,\n                            line=dict(width=0),\n                            marker=dict(color=\"gray\"),\n                            fillcolor='rgba(68, 68, 68, 0.3)', \n                            fill='tonexty')\n    forecast = create_go('Forecast', 'yhat', num_predictions,\n                         line=dict(color='rgb(31, 119, 180)'))\n    actual = create_go('Actual', 'y', num_values,\n                       marker=dict(color=\"red\"))\n    \n    # In this case the order of the series is important because of the filling\n    data = [lower_bound, upper_bound, forecast, actual]\n\n    layout = go.Layout(yaxis=dict(title='Posts'), title=title, showlegend = False)\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig, show_link=False)\n\nshow_forecast(cmp_df, prediction_size, 100, 'New posts on Medium')","88bfaa94":"def inverse_boxcox(y, lambda_):\n    return np.exp(y) if lambda_ == 0 else np.exp(np.log(lambda_ * y + 1) \/ lambda_)","addcab10":"train_df2 = train_df.copy().set_index('ds')","fbb0626b":"train_df2.head()","adb7c829":"train_df2['y'], lambda_prophet = stats.boxcox(train_df2['y'])\ntrain_df2.reset_index(inplace=True)","edf2259b":"m2 = Prophet()\nm2.fit(train_df2)\nfuture2 = m2.make_future_dataframe(periods=prediction_size)\nforecast2 = m2.predict(future2)","008b29d0":"for column in ['yhat', 'yhat_lower', 'yhat_upper']:\n    forecast2[column] = inverse_boxcox(forecast2[column], lambda_prophet)","487061d7":"cmp_df2 = make_comparison_dataframe(df, forecast2)\nfor err_name, err_value in calculate_forecast_errors(cmp_df2, prediction_size).items():\n    print(err_name, err_value)","4a7534a9":"show_forecast(cmp_df, prediction_size, 100, 'No transformations')\nshow_forecast(cmp_df2, prediction_size, 100, 'Box\u2013Cox transformation')","b5d5cf27":"More info: [Box\u2013Cox transformation](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html)\n","e895fdae":"Data yang dihasilkan dari proses aggregasi ini bukan jumlah post per hari, mari kita check","4898bcc6":"Data yang kita butuhkan hanya published date dan url, duplicate dan na juga perlu di drop","474fa40b":"Kali ini kita ingin memprediksi jumlah postingan di medium, sehingga data yang kita punya harus kita aggregasikan dengan menggunakan count() berdasarkan published date","9a96c4d4":"Penulis Fb Prophet menyarankan untuk prediksi sebaiknnya menggunakan data minimum beberapa bulan atau lebih dari setahun historical data. Dalam kasus ini, kita punya data lebih dari setahun, jadi cukup untuk fit ke model. \n\nUntuk menghitung kualitas prediksi fb prophet, kita harus membagi dataset kita menjadi dua bagian, historical part, bagian terbesar dari data kita dan prediction part. Jadi kita akan menghapus data bulan terkahir dari dataset, bulan terkahir ini akan menjadi target prediksi, dan nanti kita bisa bandingan hasil prediksi dengan data yang sebenarnya. ","a9ffa93c":"### Mengevaluasi prediksi","d2a9e681":"Coba kita experiment pakai data dari mulai January 2015","0990de6e":"Cara simple fit model Fb prophet","0e8f0037":"Prosentase error MAPE = 22.6%, dan rata-rata model yang kita buat salah kurang lebih 70 posts (MAE).","18740f11":"Ketika kita lihat data diatas, ada banyak sekali kolom, termasuk trend dan seasonality dengan confidence intervals. Hasil prediksi disimpan di `yhat` column.\n\nFb prophet juga sudah menyediakan fitur untuk plot hasil prediksi","2555b23e":"### Prediksi dengan Fb Prophet","1f36b396":"# Coba plotting data","eb25a8e7":"Kalau kita lihat dari plot diatas, dengan menggunakan daily data, ternyata tampilannya cukup cluttered, coba kita ganti data post per weekly","148072cd":"Nah, kita bisa improve modelnya nih, sekarang percentage error MAPE 12%","2dab6165":"## Box-Cox Transformation","087fa71f":"Kalau dilihat dari hasil ini, merah (data aktual), biru adalah prediksi, secara mean sepertinya benar. Tetapi model kita ternyata tidak bisa menangkap peak dan dips weekly seasonality. \n\nTerlihat pula banyak data actual keluar dari confident interval fb prophet model. Ini mungkin karena unstable variance. Coba kita gunakan box-cox transformation.","f22af0e4":"Medium itu rilis tanggal August 15, 2012. Kalau dilihat dari data yang kita dapatkan diatas, sepertinya itu ada beberapa dummy data. Jadi kita pastikan saja data yang kita ambil mulai dari 15 Agustus 2021 sampai 26 July 2017","ce1cf596":"# Practice with Facebook Prophet\n\n### Installation in Python\n\n\nIn Python you can install Prophet using PyPI:\n```\n$ pip install fbprophet\n```\n","928310ff":"Compare prediksi result setelah box-cox dengan actual data","77fcecef":"Kalau kita amati ternyata FB prophet bagus juga fitting modelnya, jumlah post di Medium naik banget diakhir tahun 2016. Kemudian weekend cenderung sedkit post, dan public holiday juga cenderung sedikit post, chrismast dan new year","6004f345":"### Visualization","9eded81d":"Gunakan `Prophet.make_future_dataframe`, untuk generate prediksi","d2ce8ffb":"Check data, sort by published date","7b17e798":"Kita join hasil prediksi dengan data aktual bulan terakhir yang kita hapus sebelumnya","415a6814":"New Prophet model","77f487e8":"Published data masih dalam format string, jadi harus di convert ke tipe date terlebih dahulu","544f9271":"Evaluasi dengan MAPE dan MAE","bef1a393":"Gambar diatas sepertinya tidak memberikan kita banyak informasi. Sepertinya model nya menganggap banyak data outliers, yang keluar dari range\/confidence interval dari prediksi. \n\nFunction `Prophet.plot_components` mungkin lebih berguna di case ini. Kita bisa lihat trend secara umum,trend pekanan, dan trend tahunan ","6cb8c15a":"Convert data sesuai dengan format Fb prophet","8901778c":"Resources:\n1. https:\/\/towardsdatascience.com\/getting-started-with-facebook-prophet-20eccb25b06b\n2. https:\/\/medium.com\/analytics-vidhya\/forecasting-using-facebooks-prophet-library-ce628e76586b\n3. https:\/\/www.kaggle.com\/jagangupta\/time-series-basics-exploring-traditional-ts\n4. https:\/\/machinelearningmastery.com\/time-series-forecasting-with-prophet-in-python\/\n5. https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-2-time-series-with-facebook-prophet\n","a39fb515":"Nah agar datanya bisa jumlah post per day, kita bisa memanfaatkan fitur dari pandas, untuk resample bins sehingga kita bisa mendapatkan data jumlah post perday"}}