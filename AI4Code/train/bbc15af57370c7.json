{"cell_type":{"59899689":"code","75a6e7c6":"code","e96facd8":"code","09888344":"code","230423c7":"code","567b3722":"code","d30a17f0":"code","af6a96d5":"code","6130f8aa":"code","36189126":"code","a3b8cbba":"code","8477efa1":"code","e7d0fc19":"code","d3b6c12e":"code","98eb4000":"code","da2dd786":"code","81c6e50b":"code","7fe41402":"code","1930f5c5":"code","a2ae8415":"code","33332560":"code","3ab3abae":"code","1cdf0a79":"code","e1515d38":"code","898d750a":"code","6e28c315":"code","e2713853":"code","71d67757":"code","bc58ef5f":"code","ade3ddc2":"code","c54aec37":"code","2e0c4f91":"code","48011b87":"code","d9e91a1e":"code","6f5b8f41":"code","1514b8da":"code","f67a8b7e":"code","7435db3c":"code","806fff80":"code","0aa65d53":"code","595ef787":"code","5a994f03":"code","6d28d531":"code","6967552c":"code","94518dbf":"code","161c54c0":"code","d8215423":"code","f6035a17":"code","df79a2b4":"code","4ed93c51":"code","f1320722":"markdown","d8de4f11":"markdown","fb210c5a":"markdown","93de875a":"markdown","2c325cfb":"markdown","0b28ed52":"markdown","4b91b47a":"markdown","5146fac0":"markdown","7206f6be":"markdown","057bd550":"markdown","493c9de8":"markdown","cd187cb3":"markdown","c62592ee":"markdown","ba179f9f":"markdown","04504816":"markdown","983c361c":"markdown","8123c246":"markdown","1f29b11e":"markdown","963b7198":"markdown","dc8d3d53":"markdown","d1505d7c":"markdown"},"source":{"59899689":"              #Working on our Second dataset - Indian water quality data Prediction Model\nimport pandas as pd\nimport numpy as np\n              #importing both numpy and pandas.\ndataset2=pd.read_csv('..\/input\/data-setadjusted-headers\/water_dataX.csv', header=0) \n              # with header=0 is like the default value, treating firtst row as header. Already header fixed on csv file.\n\nprint('No. of Instances = %d' % (dataset2.shape[0]))\nprint('No. of Attributes = %d' % (dataset2.shape[1])) \n              #Printing no. of objects\/instances  and attributes,  shape[0]give us no. of raws, shape[1] no. of columns\ndataset2.head()\n              #Printing the first 5 records on the dataframe, We can notice that all the missing values left blanked in th original file.\n              #While uploading to dataset1 dataframe, it filled all blank with NaN. a standard missing value ( Pandas will recognize both empty cells and \u201cNA\u201d types as missing values)\n\n","75a6e7c6":"print('Number of missing values = ') \nfor col in dataset2.columns:\n    print('\\t%s:%d' % (col,dataset2[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset2.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows\/records\/objects that containn at least one missing value.\n\n","e96facd8":"#taking care of the empty cells and cells with one spaces\ndataset2.replace(' ', np.nan, inplace=True) # replacing one Spaces with NaN- there was a one space in empty cells\ndataset2.replace('', np.nan, inplace=True) # replacing empty Spaces with NaN\nprint('Number of missing values = ') \nfor col in dataset2.columns:\n    print('\\t%s:%d' % (col,dataset2[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset2.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows\/records\/objects that containn at least one missing value.\n\n","09888344":"#taking care of  cells with NAN and replace it with NaN\ndataset2.replace('NAN', np.nan, inplace=True) # replacing empty Spaces with NaN\nprint('Number of missing values = ') \nfor col in dataset2.columns:\n    print('\\t%s:%d' % (col,dataset2[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset2.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows\/records\/objects that containn at least one missing value.\n\n","230423c7":"#Dropping the attributes that will not be part of our calculation, Station Code, Location, State, and year.\ndataset2_copy=dataset2 #keeping a copy of the dataframe and working on dataset2 again\ndataset2=dataset2.drop(['STATION CODE','LOCATIONS','STATE','year'], axis = 1)\n","567b3722":"dataset2 #Showing the datafram details","d30a17f0":"#checking the datatype of the attributes\ndataset2.dtypes ","af6a96d5":"#Converting datatype for all attributes to float64 before dealing with imputation (NaN)\n\ndataset2['Temp']=pd.to_numeric(dataset2['Temp'],errors='coerce',) #for the attribute convert to numeric\ndataset2['DO']=pd.to_numeric(dataset2['DO'],errors='coerce',)\ndataset2['PH']=pd.to_numeric(dataset2['PH'],errors='coerce',)\ndataset2['CONDUCTIVITY']=pd.to_numeric(dataset2['CONDUCTIVITY'],errors='coerce',)\ndataset2['BOD']=pd.to_numeric(dataset2['BOD'],errors='coerce',)\ndataset2['NITRATENAN_N']=pd.to_numeric(dataset2['NITRATENAN_N'],errors='coerce',)\ndataset2['FECAL_COLIFORM']=pd.to_numeric(dataset2['FECAL_COLIFORM'],errors='coerce',)\ndataset2['TOTAL_COLIFORM']=pd.to_numeric(dataset2['TOTAL_COLIFORM'],errors='coerce',)\n","6130f8aa":"#checking the datatype of the attributes\ndataset2.dtypes ","36189126":"#dealing with missing no.\n#first showing the missing after dropping some attributes\ndataset2_noNaN=dataset2 #creat a copy to work on the imputation\n\nprint('Number of missing values = ') \nfor col in dataset2_noNaN.columns:\n    print('\\t%s:%d' % (col,dataset2_noNaN[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset2_noNaN.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows\/records\/objects that containn at least one missing value.\n","a3b8cbba":"\n#Handeling Missing data\n          #replacing with Mean vlaue\ndataset2_noNaN=dataset2_noNaN.fillna(dataset2_noNaN.mean()) #replacing al NaN with Mean of each column\n#dataset2_noNaN.head()\n\n# showing the missing after imputation\n\nprint('Number of missing values = ') \nfor col in dataset2_noNaN.columns:\n    print('\\t%s:%d' % (col,dataset2_noNaN[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset2_noNaN.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows\/records\/objects that containn at least one missing value.\n\n","8477efa1":"dataset2_noNaN #Showing the datafram details after imputation with mean","e7d0fc19":"#applying boxplot to detect outliers\n%matplotlib inline \n        # library to be used for plotting boxplot\n\ndataset2_noNaN.boxplot(figsize=(20,10))\n","d3b6c12e":"Z2 = (dataset2_noNaN-dataset2_noNaN.mean())\/dataset2_noNaN.std() \n          # new dataframe z2 for dataset1 no NaN\/replaced by mean\n\nZ2","98eb4000":"#printing no. of outliers \nprint('Number of rows before discarding outliers Z2 = %d' % (Z2.shape[0]))\n\n\nZ = Z2.loc[((Z2 > -3).sum(axis=1)==8) & ((Z2 <= 3).sum(axis=1)==8),:]\nprint('Number of rows after discarding outliers values = %d' % (Z.shape[0]))\nZ # Z is a dataframe of Zscoee after removimg outliers, Z2 before removing outliers\n","da2dd786":"#outliers with Z score\n#applying boxplot to detect outliers\n%matplotlib inline \n        # library to be used for plotting boxplot\n#print('Plotting after removing outliers')\n#Z.boxplot(figsize=(15,10))\nprint('Plotting before removing outliers')\nZ2.boxplot(figsize=(10,5))","81c6e50b":"#Now another way to create a new dataframe without outliers that we will rely on, is by using SciPy\n \n#SciPy is a free (as Sckitleaard) and open-source Python library used for scientific computing and technical computing. \n#SciPy contains modules for optimization, linear algebra, integration, interpolation, \n\n\n#using scipy libraby\nfrom scipy import stats\nz_scores = stats.zscore(dataset2_noNaN)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\ndata2 = dataset2_noNaN[filtered_entries]\n\nprint(data2)\n\n","7fe41402":"#Correlations\n#correlation Seaborn Heatmap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(data2.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n#save heatmap as .png file\n# dpi - sets the resolution of the saved image in dots\/inches\n# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\nplt.savefig('heatmap_dataset2.png', dpi=300, bbox_inches='tight')","1930f5c5":"#data2.dtypes\n#Calculation of some attribute (transfer the attribute to interval type) by assigning ratinng for each attribute that will \n#be part of the equation, which are 6 attributes :  DO, PH, CONDUCTIVITY, BOD, NITRATENAN_N, TOTAL_COLIFORM \n\n#calculation ratinng of dissolved oxygen- DO, new column ndo for the rating\ndata2['ndo']=data2.DO.apply(lambda x:(100 if (x>=6)  \n                                 else(80 if  (6>=x>=5.1) \n                                      else(60 if (5>=x>=4.1)\n                                          else(40 if (4>=x>=3) \n                                              else 0)))))\n#calulation ratinng of PH\ndata2['npH']=data2.PH.apply(lambda x: (100 if (8.5>=x>=7)  \n                                 else(80 if  (8.6>=x>=8.5) or (6.9>=x>=6.8) \n                                      else(60 if (8.8>=x>=8.6) or (6.8>=x>=6.7) \n                                          else(40 if (9>=x>=8.8) or (6.7>=x>=6.5)\n                                              else 0)))))\n#calculation of electrical conductivity - CONDUCTIVITY\ndata2['nec']=data2.CONDUCTIVITY.apply(lambda x:(100 if (75>=x>=0)  \n                                 else(80 if  (150>=x>=75) \n                                      else(60 if (225>=x>=150)\n                                          else(40 if (300>=x>=225) \n                                              else 0)))))\n\n#calculation of B.D.O - BOD\ndata2['nbod']=data2.BOD.apply(lambda x:(100 if (3>=x>=0)  \n                                 else(80 if  (6>=x>=3) \n                                      else(60 if (80>=x>=6)\n                                          else(40 if (125>=x>=80) \n                                              else 0)))))\n#Calulation of nitrate - NITRATENAN_N\ndata2['nna']=data2.NITRATENAN_N.apply(lambda x:(100 if (20>=x>=0)  \n                                 else(80 if  (50>=x>=20) \n                                      else(60 if (100>=x>=50)\n                                          else(40 if (200>=x>=100) \n                                              else 0)))))\n#calculation of total coliform - TOTAL_COLIFORM\ndata2['nco']=data2.TOTAL_COLIFORM.apply(lambda x:(100 if (5>=x>=0)  \n                                 else(80 if  (50>=x>=5) \n                                      else(60 if (500>=x>=50)\n                                          else(40 if (10000>=x>=500) \n                                              else 0)))))\n","a2ae8415":"data2\n\n\n","33332560":"#creating new columns to calculate the weight of each attribute\ndata2['wph']=data2.npH * 0.165\ndata2['wdo']=data2.ndo * 0.281\ndata2['wbod']=data2.nbod * 0.234\ndata2['wec']=data2.nec* 0.009\ndata2['wna']=data2.nna * 0.028\ndata2['wco']=data2.nco * 0.281\ndata2['WQI']=data2.wph+data2.wdo+data2.wbod+data2.wec+data2.wna+data2.wco","3ab3abae":"data2","1cdf0a79":"#Creating of Class\ndata2['Class']=data2.WQI.apply(lambda x: ('Excellent' if (100>=x>=90)  \n                                 else('Good' if  (90>x>=70)\n                                      else('Medium' if (70>x>=50) \n                                          else('Bad' if (50>x>=25)\n                                              else 'Poor')))))","e1515d38":"data2","898d750a":"# Get frequency count of values in column 'Class'\nfrequency = data2['Class'].value_counts()\nprint(\"Frequency of value in column 'Class' :\")\nfrequency","6e28c315":"#converting Categorical to integer Code.\n#Excellent 0, Good 1, Medium 2, Bad 3, Poor 4\n#Creating of Class_code\ndata2['Class_Code']=data2.WQI.apply(lambda x: (0 if (100>=x>=90)  \n                                 else(1 if  (90>x>=70)\n                                      else(2 if (70>x>=50) \n                                          else(3 if (50>x>=25)\n                                              else 4)))))","e2713853":"data2.dtypes\n","71d67757":"data2.head(10)","bc58ef5f":"# Get frequency count of values in column 'Class_code'\nfrequency2 = data2['Class_Code'].value_counts()\nprint(\"Frequency of value in column 'Class Code' :\")\nfrequency2","ade3ddc2":"#exporting data Frame to CSV\ndata2.to_csv('data2.csv', index=False)","c54aec37":"        #Load libraries-\n        #Scikit-learn is a free software machine learning library for the Python programming language. \n        #It features various classification, regression and clustering algorithms including support vector machines.\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n        \n        #split dataset in features and target variable\nfeature_cols = ['DO', 'PH', 'CONDUCTIVITY', \n                'BOD','NITRATENAN_N','TOTAL_COLIFORM']\nX = data2[feature_cols] # Features \ny = data2.Class_Code # Target variable\n\n     # Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n          # 70% training and 30% test\n          # Split dataset into training set and test set\n         # Create Decision Tree classifer object\ntree = DecisionTreeClassifier()\n         # criterion: default is Gini, max_depth: defualt is non,\n                              #splitter : default is best\n\n          # Train Decision Tree Classifer\ntree = tree.fit(X_train,y_train)\n\n          #Predict the response for test dataset\ny_pred = tree.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","2e0c4f91":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['Excellent','Good','Medium','Bad','Poor'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('water2.png')\nImage(graph.create_png())","48011b87":"# import the metrics class \n\nfrom sklearn import metrics\ncnf_matrix2_1 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix2_1","d9e91a1e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred)) #classification report showing the accuracy, recall and precision","6f5b8f41":"# import required modules\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1,2,3,4] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix2_1), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nprint(\"Precision Score : \",precision_score(y_test, y_pred, average=None))\nprint(\"Recall Score : \",recall_score(y_test, y_pred, average=None))\n                                        \n                                          ","1514b8da":"#Create Decision Tree classifer object\ntree2 = DecisionTreeClassifier(criterion=\"entropy\",max_depth=10)\n\n# Train Decision Tree Classifer\ntree2 = tree2.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = tree2.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","f67a8b7e":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(tree2, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['Excellent','Good','Medium','Bad','Poor'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('water22.png')\nImage(graph.create_png())","7435db3c":"# import the metrics class \n\nfrom sklearn import metrics\ncnf_matrix2_2 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix2_2","806fff80":"print(classification_report(y_test, y_pred)) #classification report showing the accuracy, recall and precision","0aa65d53":"# import required modules\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1,2,3,4] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix2_2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nprint(\"Precision Score : \",precision_score(y_test, y_pred, average=None))\nprint(\"Recall Score : \",recall_score(y_test, y_pred, average=None))","595ef787":"#testing the model for overfitting\n\n########################################\n# Training and Test set creation\n#########################################\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n#########################################\n# Model fitting and evaluation\n#########################################\n\nmaxdepths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n\ntrainAcc = np.zeros(len(maxdepths))\ntestAcc = np.zeros(len(maxdepths))\n\nindex = 0\nfor depth in maxdepths:\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    clf = clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc[index] = accuracy_score(y_train, y_predTrain)\n    testAcc[index] = accuracy_score(y_test, y_predTest)\n    index += 1\n    \n#########################################\n# Plot of training and test accuracies\n#########################################\n    \nplt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')","5a994f03":"#Cross-Validation\n#To eliminate over-fitting, we can apply cross-validation.\n#going to apply k-fold cross-validation.\n#split the original data set into k subsets and use one of the subsets as the testing set and the remaining as the training sets.\n#This process iterated k times until every subset have been used as the testing set.\n# 10-folds - k Folds cross validation cv=10 parameter for K=10 in cross_val_score function\n\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n\ndtc = DecisionTreeClassifier()\ncv_scores = cross_val_score(dtc, X, y, cv=10) \nsns.distplot(cv_scores)\nplt.title('Average score: {}'.format(np.mean(cv_scores)))","6d28d531":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\ndtc = DecisionTreeClassifier()\n#testing for multiple depth and features\nparameter_grid = {'criterion': ['gini', 'entropy'],\n                  'splitter': ['best', 'random'],\n                  'max_depth': [1, 5, 8, 10, 15, 20, 25, 30, 35, 40, 50, 60],\n                  'max_features': [1, 2, 3, 4, 5, 6]}\n\n\ncross_validation = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(dtc, param_grid=parameter_grid, cv=cross_validation)\n\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\n\ndtc = grid_search.best_estimator_\ndtc","6967552c":"#got the best parameters for this model. \n#We can directly assign it to the decision tree classifier and we can check all of its property. \n#checking it\u2019s accuracy.\ncv_scores = cross_val_score(dtc, X, y)\nsns.distplot(cv_scores)\nplt.title('Average score: {}'.format(np.mean(cv_scores)))\n","94518dbf":"#Load libraries-\n        #Scikit-learn is a free software machine learning library for the Python programming language. \n        #It features various classification, regression and clustering algorithms including support vector machines.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n        \n       \n      #split dataset in features and target variable\nfeature_cols = ['DO', 'PH', 'CONDUCTIVITY', \n                'BOD','NITRATENAN_N','TOTAL_COLIFORM']\nX = data2[feature_cols] # Features \ny = data2.Class_Code # Target variable\n\n\n     # Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n          # 70% training and 30% test\n          # Split dataset into training set and test set\n\n#KNN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=30) # no of neighbors\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\ncnf_matrix2_3 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix2_3\n","161c54c0":"# import required modules\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1,2,3,4] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix2_3), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nprint(\"Precision Score : \",precision_score(y_test, y_pred, average=None))\nprint(\"Recall Score : \",recall_score(y_test, y_pred, average=None))","d8215423":"##KNN  testing different K values and check the accuracy over the change in both test and train data\n\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnumNeighbors = [1, 5, 10, 15, 20, 25, 30,35,40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300]\ntrainAcc = []\ntestAcc = []\n\nfor k in numNeighbors:\n    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n    clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc.append(accuracy_score(y_train, y_predTrain))\n    testAcc.append(accuracy_score(y_test, y_predTest))\n\nplt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')","f6035a17":"from sklearn.model_selection import cross_val_score\n\n\nknn = KNeighborsClassifier(n_neighbors = 100)\n\nscores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    # X,y will automatically devided by 10 folder, the scoring I will still use the accuracy\n\nprint(scores)\n  # print all 5 times scores \n# then I will do the \nprint(scores.mean())\n#average about these  scores to get more accuracy score.\n","df79a2b4":"import matplotlib.pyplot as plt \n%matplotlib inline\n# choose k between 1 to 200\nk_range = range(1, 200)\nk_scores = []\n# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    k_scores.append(scores.mean())\n# plot to see clearly\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')\nplt.show()\n\n#the warning because the class Poor have onle one instance, we could remove that instance.\n#this instance is less than the no. of splits in kfold.\n# but no worry , the graph is working and the messages are warning only","4ed93c51":"#Not used\n#Ensemble Method\n#\nfrom sklearn import ensemble\nfrom sklearn.tree import DecisionTreeClassifier\n\nnumBaseClassifiers = 700\nmaxdepth = 10\ntrainAcc = []\ntestAcc = []\n\nclf = ensemble.RandomForestClassifier(n_estimators=numBaseClassifiers)\nclf.fit(X_train, y_train)\ny_predTrain = clf.predict(X_train)\ny_predTest = clf.predict(X_test)\ntrainAcc.append(accuracy_score(y_train, y_predTrain))\ntestAcc.append(accuracy_score(y_test, y_predTest))\n\nclf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=maxdepth),n_estimators=numBaseClassifiers)\nclf.fit(X_train, y_train)\ny_predTrain = clf.predict(X_train)\ny_predTest = clf.predict(X_test)\ntrainAcc.append(accuracy_score(y_train, y_predTrain))\ntestAcc.append(accuracy_score(y_test, y_predTest))\n\nclf = ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=maxdepth),n_estimators=numBaseClassifiers)\nclf.fit(X_train, y_train)\ny_predTrain = clf.predict(X_train)\ny_predTest = clf.predict(X_test)\ntrainAcc.append(accuracy_score(y_train, y_predTrain))\ntestAcc.append(accuracy_score(y_test, y_predTest))\n\nmethods = ['Random Forest', 'Bagging', 'AdaBoost']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\nax1.bar([1.5,2.5,3.5], trainAcc)\nax1.set_xticks([1.5,2.5,3.5])\nax1.set_xticklabels(methods)\nax2.bar([1.5,2.5,3.5], testAcc)\nax2.set_xticks([1.5,2.5,3.5])\nax2.set_xticklabels(methods)","f1320722":"Some calculation should be done to calculate the WQI.\n\nWQI  will be consider the class attribute, that we will build our models on.\n\n\n\n**Calculation of some attribute (transfer the attribute to interval type) by assigning ratinng for each attribute that is part of the equation, which are 6 attributes out of the 8:  DO, PH, CONDUCTIVITY, BOD, NITRATENAN_N, TOTAL_COLIFORM **\n\nThe rate will be givin a weight after that in the equation to calculate WQI\n\n","d8de4f11":"#Adding the Weight to each of the six attributes as per the equation.\nEquation: WQI = 0.165 * nph + 0.281 * ndo + 0.234 * nbod + 0.009 * nec + 0.028 * nna + 0.281 * nco\n","fb210c5a":"**Water Quality INDEX- Second Dataset**\n\n\n*Data Mining Project\n\nFiras K.\n\n","93de875a":"As we got the best parameters for this model. We can directly assign it to the decision tree classifier and we can check all of its property.\n\nchecking it\u2019s accuracy- with optimal parameter","2c325cfb":"We can see that a lot of outliers appears but not clear, a plot on the Zscore will be better. We will create new data frame with the standard Z score.\n","0b28ed52":"The dataframe to be work on now is (data2)\ncorrelation is plotted.\n ","4b91b47a":"#KNN Classifier \n\nKnn from sklearn","5146fac0":"Dealing with missing numbers","7206f6be":"![image.png](data:image\/png;base64,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)","057bd550":"Data Upload","493c9de8":"Checking Missing Values","cd187cb3":"we could choose differenct neighbors to see which K is the best K.","c62592ee":"Confusion Matrix for first DC tree module","ba179f9f":"Cross-Validation\n\n\nTo eliminate over-fitting, we can apply cross-validation. We are going to apply k-fold cross-validation by spliting the original data set into k subsets and use one of the subsets as the testing set and the remaining as the training sets.\n\nThis process iterated k times until every subset have been used as the testing set.\n\n10-folds - (k-Folds cross-validation)","04504816":"File is ready to check outliers\n","983c361c":"#K-Fold Cross-Validation for KNN Classification\n\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples.","8123c246":"\n\n##DATA MINING TASKs\nClassification by Decesion Tree using **scikit liabrary**\n\ndata2 dataframe is ready for Data Mining Tasks","1f29b11e":"Dropping unnecessary attributes","963b7198":"From the table below we will create the class attribute from the WQI attribute \nas a categorical attribute\n\nhttps:\/\/www.researchgate.net\/figure\/Classification-criteria-standards-based-on-NSF-WQI_tbl2_215581438\n","dc8d3d53":"###Optimizing Decision Tree Performance\nScikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning.","d1505d7c":"Parameter Tuning\n\nIn every classification technique, There are some parameters that can be tuned to optimize the classification. Some parameters that can be tuned in the decision tree is max depth (the depth of the tree), max feature (the feature used to classify), criterion, and splitter.\n\nTo tune parameter is to use Grid Search.\nBasically, it explores a range of parameters and finds the best combination of parameters. Then repeat the process several times until the best parameters are discovered.\n\nWe will also use Stratified k-fold cross-validation that will prevent a certain class only split them to the same subset"}}