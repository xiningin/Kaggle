{"cell_type":{"7cd6c214":"code","511efcef":"code","e1ea8d89":"code","03d1ff0d":"code","cf43afed":"code","8d63648d":"code","dfb49ede":"code","ebcacbd6":"code","dd07a487":"code","7600d981":"code","0c156efc":"code","17bbac77":"code","9a37128a":"code","7e51abf8":"code","7c36418e":"code","ef6f6526":"code","8193d420":"markdown","9e1beb3d":"markdown","c9cbe4b7":"markdown","c31a2b1e":"markdown","b47bed3f":"markdown"},"source":{"7cd6c214":"import numpy as np\nimport pandas as pd\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.optim import Optimizer\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import _Loss\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","511efcef":"path = '..\/input\/Kannada-MNIST\/'\ndf_train = pd.read_csv(path + 'train.csv')\ndf_test = pd.read_csv(path + 'test.csv')\ndf_train.head()","e1ea8d89":"labels = df_train['label'].values\nfeatures = df_train[list(df_train.columns)[1:]].values\nfeatures = features.reshape(-1, 28, 28, 1)\nfeatures = torch.tensor(features).type(torch.FloatTensor)\nfeatures = features.permute(0, 3, 1, 2)","03d1ff0d":"X_train, X_test, y_train, y_test = train_test_split(\n                                    features, labels, test_size=0.3, random_state=42)","cf43afed":"X_train_auto = (X_train - X_train.min()) \/ (X_train.max() - X_train.min()) * 2 - 1\nX_test_auto = (X_test - X_train.min()) \/ (X_train.max() - X_train.min()) * 2 - 1","8d63648d":"class Layer(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x: Tensor,\n                inference: bool = False) -> Tensor:\n        raise NotImplementedError()","dfb49ede":"class DeconvLayer(Layer):\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 filter_size: int,\n                 activation: nn.Module = None,\n                 dropout: float = 1.0,\n                 flatten: bool = False) -> None:\n        super().__init__()\n        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, filter_size, \n                                       padding=filter_size \/\/ 2)\n        self.activation = activation\n        self.flatten = flatten\n        if dropout < 1.0:\n            self.dropout = nn.Dropout(1 - dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        x = self.deconv(x)\n        if self.activation:\n            x = self.activation(x)\n        if self.flatten:\n            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n        if hasattr(self, \"dropout\"):\n            x = self.dropout(x)            \n            \n        return x","ebcacbd6":"class Model(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x: Tensor) -> Tuple[Tensor]:\n        raise NotImplementedError()","dd07a487":"class ConvLayer(Layer):\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 filter_size: int,\n                 activation: nn.Module = None,\n                 dropout: float = 1.0,\n                 flatten: bool = False) -> None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, filter_size, \n                              padding=filter_size \/\/ 2)\n        self.activation = activation\n        self.flatten = flatten\n        if dropout < 1.0:\n            self.dropout = nn.Dropout(1 - dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        x = self.conv(x)\n        if self.activation:\n            x = self.activation(x)\n        if self.flatten:\n            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n        if hasattr(self, \"dropout\"):\n            x = self.dropout(x)            \n            \n        return x","7600d981":"class DenseLayer(Layer):\n    def __init__(self,\n                 input_size: int,\n                 neurons: int,\n                 dropout: float = 1.0,\n                 activation: nn.Module = None) -> None:\n\n        super().__init__()\n        self.linear = nn.Linear(input_size, neurons)\n        self.activation = activation\n        if dropout < 1.0:\n            self.dropout = nn.Dropout(1 - dropout)\n\n    def forward(self, x: Tensor,\n                inference: bool = False) -> Tensor:\n        if inference:\n            self.apply(inference_mode)\n\n        x = self.linear(x)\n        if self.activation:\n            x = self.activation(x)\n        if hasattr(self, \"dropout\"):\n            x = self.dropout(x)\n\n        return x","0c156efc":"class Autoencoder(Model):\n    def __init__(self,\n                 hidden_dim: int = 28):\n        super(Autoencoder, self).__init__()\n        self.conv1 = ConvLayer(1, 14, 5, activation=nn.Tanh())\n        self.conv2 = ConvLayer(14, 7, 5, activation=nn.Tanh(), flatten=True)\n        \n        self.dense1 = DenseLayer(7 * 28 * 28, hidden_dim, activation=nn.Tanh())\n        self.dense2 = DenseLayer(hidden_dim, 7 * 28 * 28, activation=nn.Tanh())\n        \n        self.conv3 = ConvLayer(7, 14, 5, activation=nn.Tanh()) \n        self.conv4 = ConvLayer(14, 1, 5, activation=nn.Tanh())         \n\n    def forward(self, x: Tensor) -> Tensor:\n        assert_dim(x, 4)\n            \n        x = self.conv1(x)\n        x = self.conv2(x)\n\n        encoding = self.dense1(x)\n        \n        x = self.dense2(encoding)\n        \n        x = x.view(-1, 7, 28, 28)\n        \n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        return x, encoding","17bbac77":"def permute_data(X: Tensor, y: Tensor, seed=1) -> Tuple[Tensor]:\n    perm = torch.randperm(X.shape[0])\n    return X[perm], y[perm]\n\ndef assert_dim(t: Tensor,\n               dim: Tensor):\n    assert len(t.shape) == dim, \\\n        '''\n        Tensor expected to have dimension {0}, instead has dimension {1}\n        '''.format(dim, len(t.shape))\n    return None","9a37128a":"class Trainer(object):\n    def __init__(self,\n                 model: Model,\n                 optim: Optimizer,\n                 criterion: _Loss):\n        self.model = model\n        self.optim = optim\n        self.loss = criterion\n        self._check_optim_net_aligned()\n\n    def _check_optim_net_aligned(self):\n        assert self.optim.param_groups[0]['params']\\\n            == list(self.model.parameters())\n\n    def _generate_batches(self,\n                          X: Tensor,\n                          y: Tensor,\n                          size: int = 32) -> Tuple[Tensor]:\n\n        N = X.shape[0]\n\n        for ii in range(0, N, size):\n            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n\n            yield X_batch, y_batch\n\n    def fit(self, X_train: Tensor = None,\n            y_train: Tensor = None,\n            X_test: Tensor = None,\n            y_test: Tensor = None,\n            train_dataloader: DataLoader = None,\n            test_dataloader: DataLoader = None,\n            epochs: int=100,\n            eval_every: int=10,\n            batch_size: int=32,\n            final_lr_exp: int = None):\n\n        init_lr = self.optim.param_groups[0]['lr']\n        if final_lr_exp:\n            decay = (final_lr_exp \/ init_lr) ** (1.0 \/ (epochs + 1))\n            scheduler = lr_scheduler.ExponentialLR(self.optim, gamma=decay)\n        for e in range(epochs):\n\n            if final_lr_exp:\n                scheduler.step()\n\n            if not train_dataloader:\n                X_train, y_train = permute_data(X_train, y_train)\n\n                batch_generator = self._generate_batches(X_train, y_train,\n                                                         batch_size)\n\n                self.model.train()\n\n                for ii, (X_batch, y_batch) in enumerate(batch_generator):\n\n                    self.optim.zero_grad()\n\n                    output = self.model(X_batch)[0]\n\n                    loss = self.loss(output, y_batch)\n                    loss.backward()\n                    self.optim.step()\n\n                if e % eval_every == 0:\n                    with torch.no_grad():\n                        self.model.eval()\n                        output = self.model(X_test)[0]\n                        loss = self.loss(output, y_test)\n                        print(\"Epoch:\", e+1, \", Loss:\", loss.item())\n\n            else:\n                for X_batch, y_batch in train_dataloader:\n\n                    self.optim.zero_grad()\n\n                    output = self.model(X_batch)[0]\n\n                    loss = self.loss(output, y_batch)\n                    loss.backward()\n                    self.optim.step()\n\n                if e % eval_every == 0:\n                    with torch.no_grad():\n                        self.model.eval()\n                        losses = []\n                        for X_batch, y_batch in test_dataloader:\n                            output = self.model(X_batch)[0]\n                            loss = self.loss(output, y_batch)\n                            losses.append(loss.item())\n                        print(\"Epoch:\", e, \", Loss:\",\n                              round(torch.Tensor(losses).mean().item(), 4))","7e51abf8":"model = Autoencoder(hidden_dim=28)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\ntrainer = Trainer(model, optimizer, criterion)\n\ntrainer.fit(X_train_auto, X_train_auto,\n            X_test_auto, X_test_auto,\n            epochs=10,\n            batch_size=60)","7c36418e":"reconstructed_images, image_representations = model(X_test_auto)","ef6f6526":"def display_image(ax,\n    t: Tensor):\n    n = t.detach().numpy()\n    ax.imshow(n.reshape(28, 28))\n\nf, axarr = plt.subplots(5,8, figsize=(14,9))\n\nk = 0\nfor i in range(5):\n    \n    for j in range(7):\n        display_image(axarr[i][j + (j % 2)], X_test[k])\n        display_image(axarr[i][j + (j % 2 + 1)], reconstructed_images[k])\n        k += 1\n\n    for j in range(7):\n        axarr[i][j + (j % 2)].set_title(\"Original\")\n        axarr[i][j + (j % 2 + 1)].set_title(\"Reconstructed\")\n\n    for j in range(7):\n        axarr[i][j + (j % 2)].axis('off')\n        axarr[i][j + (j % 2 + 1)].axis('off')","8193d420":"# Prepare Data","9e1beb3d":"# Training","c9cbe4b7":"<pre>\n                       !\n                      \/^\\\n                    \/     \\\n |               | (       ) |               |\n\/^\\  |          \/^\\ \\     \/ \/^\\          |  \/^\\\n|O| \/^\\        (   )|-----|(   )        \/^\\ |O|\n|_| |-|    |^-^|---||-----||---|^-^|    |-| |_|\n|O| |O|    |\/^\\|\/^\\||  |  ||\/^\\|\/^\\|    |O| |O|\n|-| |-|    ||_|||_||| \/^\\ |||_|||_||    |-| |-|\n|O| |O|    |\/^\\|\/^\\||(   )||\/^\\|\/^\\|    |O| |O|\n|-| |-|    ||_|||_||||   ||||_|||_||    |-| |-|\n|O| |_|----|___|___|||___|||___|_|_|    |O| |O|\n|_|                                         |_|\n   \/_______________________________________\\\n__|_______________________________________|___|\n\n<b>Kannada - Unsupervised Learning Autoencoders<\/b>\nby Alin Cijov\n<\/pre>","c31a2b1e":"# Implementation","b47bed3f":"# Reconstruct image from trained model"}}