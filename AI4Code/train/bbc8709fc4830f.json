{"cell_type":{"c3754b4c":"code","263a3099":"code","db7800b5":"code","3d996b18":"code","83622cfd":"code","f096dc7c":"code","2b82cb47":"code","636b981f":"code","5275b5b9":"code","eeca9589":"code","24fc918e":"code","3c842852":"code","d183b858":"code","cb460bf2":"code","7194dbb9":"code","957516ba":"code","ae7c69e8":"code","673a2fa1":"code","57ab2361":"code","cc2537a8":"code","75255abf":"code","ff7a96a0":"code","c86b9164":"code","5a694c71":"code","311d0053":"code","3969154a":"code","74be7ab3":"code","a68105be":"code","e0738236":"code","1f4a00ee":"code","73dede3b":"code","a329138c":"code","4b59bf92":"code","f2605c51":"code","632ab8be":"code","5f3f3067":"code","d39b6273":"code","8910fac8":"code","009a9dc1":"code","e1c6e583":"code","6b5dad40":"code","5a7c117a":"code","9e07a16a":"code","f17e693f":"code","65c24832":"code","0ce91aad":"code","11911aaf":"code","e4aa3369":"code","30a1d057":"code","df384101":"code","21551809":"code","ac5ac028":"code","af228a3e":"code","de08faa1":"code","1344cd32":"code","022207ac":"code","ce6746a6":"code","254145c3":"code","d5b3b744":"code","4b7ebe29":"code","7ccf2163":"code","3a08adac":"code","74be9d1e":"code","5e832f09":"code","69616ebb":"code","f4f192e6":"code","1ca5e8a9":"code","bd34cf62":"code","33dcf9e8":"code","6735edb6":"code","0e42c048":"code","184f9774":"code","fda258ec":"code","a3d3bdfb":"code","6956f20b":"code","f364c772":"code","655a06ba":"code","92a0feec":"code","edc939f3":"code","9348f1a1":"code","85328640":"markdown","1d1bd46a":"markdown","d154886b":"markdown","0fb792f1":"markdown","08367948":"markdown","750bca5d":"markdown","ac8f4a25":"markdown","3629a60f":"markdown","8af17287":"markdown","c0b2419e":"markdown","15c540f6":"markdown","6a07ffc4":"markdown","f94c8c79":"markdown","2100cf8c":"markdown","5b79cac8":"markdown","78f2974e":"markdown","8a2ab700":"markdown","7f908445":"markdown","b3f70810":"markdown","307802d0":"markdown","7164b646":"markdown","4ffb8830":"markdown","42343085":"markdown","3952977f":"markdown","79d53cd8":"markdown","d77e574d":"markdown","5f0c0f11":"markdown","5c64f07c":"markdown","50a623d5":"markdown","4ec0649a":"markdown","ad3cffc9":"markdown","1fabb819":"markdown"},"source":{"c3754b4c":"#Importing DATA calucation and manipulation modules\nimport numpy as np\nimport pandas as pd\n\n#Importing neccessary plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#to filter all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","263a3099":"data = pd.read_csv('..\/input\/online_shoppers_intention.csv')\ndata.shape","db7800b5":"data.head()","3d996b18":"data.info()","83622cfd":"print('Descriptive statistics of Data')\ndata.describe().T","f096dc7c":"data.columns = ['admin_pages','admin_duration','info_pages','info_duration','product_pages', 'prod_duration',\n                'avg_bounce_rate', 'avg_exit_rate','avg_page_value','spl_day','month','os','browser','region',\n                'traffic_type','visitor_type','weekend','revenue']","2b82cb47":"data1 = data.copy()","636b981f":"#Replacing boolean values with binary values(1,0) \ndata1.weekend = np.where(data.weekend == True,1,0)\ndata1.revenue = np.where(data.revenue == True,1,0)","5275b5b9":"data.month.unique()","eeca9589":"#mapping months with numerical values\ndata1['month'] = data1['month'].map({'Feb':2,'Mar':3 ,'May':5,'June':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12})    ","24fc918e":"data['visitor_type'].value_counts()","3c842852":"#mapping months with numerical values\ndata1['visitor_type'] = data1['visitor_type'].map({'Returning_Visitor':0,'New_Visitor':1,'Other':2})","d183b858":"data1.head(10)","cb460bf2":"data1.info()","7194dbb9":"#Checking for missing values\npd.isnull(data1).sum()","957516ba":"cat = ['admin_pages','info_pages','spl_day', 'month','os', 'browser','region','traffic_type', \n       'visitor_type', 'weekend']\n\ncont = ['admin_duration', 'info_duration','product_pages','prod_duration','avg_bounce_rate', 'avg_exit_rate','avg_page_value']","ae7c69e8":"print('Correlation Heat map of the data')\nplt.figure(figsize=(15,10))\nmask = np.array(data1[cont].corr())\nmask[np.tril_indices_from(data1[cont].corr())] = False\nsns.heatmap(data1[cont].corr(),annot=True,mask = mask, fmt='.2f',vmin=-1,vmax=1)\nplt.show()","673a2fa1":"def cat_data(i):\n        sns.countplot(data[i])\n        print('--'*60)\n        plt.title(\"Count plot of \"+str(i))\n        plt.show()\n        \nfor i in cat:\n    cat_data(i)        ","57ab2361":"sns.countplot(data.revenue)","cc2537a8":"from scipy.stats import skew\nsns.set() #Sets the default seaborn plotting style\n\ndef continous_data(i):\n        sns.boxplot(data1[i])\n        print('--'*60)\n        plt.title(\"Boxplot of \"+str(i))\n        plt.show()\n        plt.title(\"histogram of \"+str(i))        \n        sns.distplot(data1[i],bins=40,kde=True,color='blue')\n        plt.show()\n        print('skewness :',skew(data1[i]))\n        \nfor i in cont:\n    continous_data(i)        ","75255abf":"for i in cont:\n    Q1 = data1[i].quantile(0.25)\n    Q3 = data1[i].quantile(0.75)\n    IQR = Q3 - Q1\n    upper = Q3 + 1.5*IQR\n    lower = Q1 - 1.5*IQR\n    outlier_count = data1[i][(data1[i] < lower) | (data1[i] > upper)].count()\n    total = data1[i].count()\n    percent = (outlier_count\/total)*100\n    print('Percentage of Outliers in {} column :: {}%'.format(i,np.round(percent,2)))","ff7a96a0":"def cat_bivar(i):\n        sns.barplot(data[i],data1.revenue)\n        print('--'*60)\n        plt.title(\"Bar-plot of Revenue against \"+str(i))\n        plt.show()\n        \nfor i in cat:\n    cat_bivar(i)        ","c86b9164":"sns.boxplot(x=data1.revenue, y=data1.avg_exit_rate)","5a694c71":"sns.scatterplot(data1.admin_duration,data1.admin_pages)","311d0053":"sns.scatterplot(data1.prod_duration,data1.product_pages)","3969154a":"sns.scatterplot(data1.avg_bounce_rate,data1.avg_exit_rate)","74be7ab3":"sns.barplot(data1.revenue,data1.admin_duration)","a68105be":"sns.barplot(data1.revenue,data1.info_duration)","e0738236":"sns.barplot(data1.revenue,data1.prod_duration)","1f4a00ee":"def f(x):\n    return pd.Series(dict(avg_time_on_admintrative_pages=x['admin_duration'].mean(),\n                         avg_time_on_info_pages=x['info_duration'].mean(),\n                         avg_time_on_products=x['prod_duration'].mean(),\n                         avg_bounce_rate =x['avg_bounce_rate'].mean(),\n                         avg_exit_rate =x['avg_exit_rate'].mean(),\n                         avg_page_value =x['avg_page_value'].mean()))\n\n\nby_visitor_type = data1.groupby('visitor_type').apply(f)\nby_visitor_type","73dede3b":"data1.pivot_table(index='revenue',columns = 'visitor_type',values ='product_pages', aggfunc='mean')","a329138c":"data1.groupby('revenue')[['admin_duration','info_duration','product_pages','prod_duration','avg_page_value']].mean()","4b59bf92":"data1.groupby('revenue')[['avg_bounce_rate','avg_exit_rate']].mean()","f2605c51":"# Importing train-test-split \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,confusion_matrix,classification_report","632ab8be":"df = data1.copy()\ny = df['revenue']\nx = df.drop(['revenue'],axis=1)\n# Splitting the data into train and test\nxtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.30, random_state = 127)","5f3f3067":"y_pred = []\nfor i in range(0,ytest.shape[0]):\n    y_pred.append(ytest.mode()[0])","d39b6273":"y_pred = pd.Series(y_pred)\nscore = accuracy_score(ytest,y_pred)\n\npd.DataFrame({'Classifier':['Baseline model'], 'Accuracy': [score]})","8910fac8":"df_cat = df.copy()","009a9dc1":"def categorize(col):\n    l = []\n    for i in col:\n        if i == 0:\n            l.append(0)\n        elif i > 0 and i < 300:\n            l.append(1)\n        else: \n            l.append(2)\n    return l               ","e1c6e583":"def cat_pd(col):\n    l = []\n    for i in col:\n        if i <= 300:\n            l.append(0)\n        elif i > 300 and i < 3000:\n            l.append(1)\n        else: \n            l.append(2)\n    return l        ","6b5dad40":"def cat_rates(col):\n    l = []\n    for i in col:\n        if i <= 0.05:\n            l.append(0)\n        elif i > 0.05 and i < 0.15:\n            l.append(1)\n        else: \n            l.append(2)\n    return l        ","5a7c117a":"def cat_pv(col):\n    l = []\n    for i in col:\n        if i == 0:\n            l.append(0)\n        elif i > 0 and i < 20:\n            l.append(1)\n        else: \n            l.append(2)\n    return l      ","9e07a16a":"cat_admin_duration = list(categorize(df.admin_duration)) \ncat_info_duration = list(categorize(df.info_duration))\ncat_prod_duration = list(cat_pd(df.prod_duration)) \ncat_bounce_rate = list(cat_rates(df.avg_bounce_rate)) \ncat_exit_rate = list(cat_rates(df.avg_exit_rate)) \ncat_avg_page_value = list(cat_pv(df.avg_page_value)) \n#cat_product_pages = list(cat_pp(df.product_pages)) ","f17e693f":"df_cat.admin_duration = cat_admin_duration\ndf_cat.info_duration = cat_info_duration \ndf_cat.prod_duration = cat_prod_duration\ndf_cat.avg_bounce_rate = cat_bounce_rate\ndf_cat.avg_exit_rate = cat_exit_rate\ndf_cat.avg_page_value = cat_avg_page_value\n#df_cat.product_pages = cat_product_pages","65c24832":"from scipy.stats import boxcox\n\ndt = boxcox(df_cat['product_pages']+1,lmbda = 0.01)\ndf_cat.product_pages = pd.Series(dt)\nsns.boxplot(df_cat.product_pages)\nplt.show()\nsns.distplot(df_cat.product_pages)\nplt.show()","0ce91aad":"#replacing outlier with the upper whisker values\nIQR = (np.percentile(df_cat.product_pages,75) - np.percentile(df_cat.product_pages,25))\nupper = (np.percentile(df_cat.product_pages,75) + 1.5*IQR)\ndf_cat.product_pages = np.where(df_cat.product_pages > upper,upper,df_cat.product_pages)\nsns.boxplot(df_cat.product_pages)","11911aaf":"df_cat.head()","e4aa3369":"#One hot encoding\ndf_cat1 = pd.get_dummies(df_cat,columns = ['spl_day','month','os','browser','traffic_type','visitor_type'],drop_first=True)\ndf_cat1.head()","30a1d057":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = df_cat1.drop('revenue',axis =1)\nscaler = MinMaxScaler()\nx_std = scaler.fit_transform(features)\nx_std = pd.DataFrame(x_std,columns=features.columns)\nx_std.head()","df384101":"cat1 = cat+['admin_duration','info_duration','prod_duration','avg_bounce_rate','avg_exit_rate','avg_page_value']","21551809":"from scipy.stats import chisquare,chi2_contingency\n\ncat_col = []\nchi_pvalue = []\nchi_name = []\n\ndef chi_sq(i):\n    ct = pd.crosstab(df_cat['revenue'],df_cat[i])\n    chi_pvalue.append(chi2_contingency(ct)[1])\n    chi_name.append(i)\n\nfor i in cat1:\n    chi_sq(i)\n\nchi_data = pd.DataFrame()\nchi_data['Pvalue'] = chi_pvalue\nchi_data.index = chi_name\n\nplt.figure(figsize=(11,8))\nplt.title('P-Values of Chisquare with ''REVENUE'' as Target Categorical Attribute',fontsize=16)\nx = chi_data.Pvalue.sort_values().plot(kind='barh')\nx.set_xlabel('P-Values',fontsize=15)\nx.set_ylabel('Independent Categorical Attributes',fontsize=15)\nplt.show()","ac5ac028":"from scipy.stats import ttest_ind\n\ncont_col = []\npvalue = []\nname = []\nt_stat = []\n\ndef t_test(i):\n    sample_0 = df[df.revenue==0][i]\n    sample_1 = df[df.revenue==1][i]\n    t_statistic , p_value  =  ttest_ind(sample_0,sample_1)\n    t_stat.append(t_statistic)\n    pvalue.append(p_value)\n    name.append(i)\n\nfor i in cont:\n    t_test(i)\n\nt_data = pd.DataFrame()\nt_data['Pvalue'] = pvalue\nt_data.index = name\n\nplt.figure(figsize=(10,7))\nplt.title('P-Values of T-test with ''REVENUE'' as Target Categorical Attribute',fontsize=16)\nx = t_data.Pvalue.sort_values().plot(kind='barh')\nx.set_xlabel('P-Values',fontsize=14)\nx.set_ylabel('Independent Continuous Attributes',fontsize=14)\nplt.show()","af228a3e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc","de08faa1":"X = df_cat1.drop(['revenue','region'],axis=1)\nY = df_cat1['revenue']\n\n# Splitting the data into train and test\nxtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.30, random_state = 25)\n\n# Splitting the STANDARDIZED data into train and test\nXtrain, Xtest, Ytrain, Ytest = train_test_split(x_std,Y,test_size=0.30, random_state = 25)","1344cd32":"from sklearn.model_selection import GridSearchCV\n#for decision tree: \n#parameter={'max_depth':np.arange(1,15),'criterion':['gini','entropy']}\n#gs = GridSearchCV(dt,parameter,cv=4)\n#gs.fit(xtrain,ytrain)\n#gs.best_params_","022207ac":"dt = DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=25)\ndt.fit(xtrain,ytrain)\ny_pred = dt.predict(xtest)\n\nprint(confusion_matrix(ytest,y_pred))\nprint(classification_report(ytest,y_pred))","ce6746a6":"importance =  pd.DataFrame()\nimportance['features'] =  xtrain.columns\nimportance['importance'] = dt.feature_importances_\nimportance[importance.importance>0].sort_values(by='importance',ascending = False)","254145c3":"training = dt.score(xtrain,ytrain)\nscore = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\n\nresults = pd.DataFrame({'Classifier':['Decision Tree'],'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[0])\nresults","d5b3b744":"#for KNN:\n#parameter={'n_neighbors':np.arange(1,16,2)}\n#gs = GridSearchCV(knn,parameter,cv=4,scoring='f1')\n#gs.fit(xtrain,ytrain)\n#gs.best_params_","4b7ebe29":"knn=KNeighborsClassifier(n_neighbors=11)\nknn.fit(xtrain,ytrain)\ny_pred = knn.predict(xtest)\n\nprint(classification_report(ytest, y_pred))\nprint(confusion_matrix(ytest,y_pred))","7ccf2163":"training = knn.score(xtrain,ytrain)\nscore = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\nknn_results = pd.DataFrame({'Classifier':['KNN'], 'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[1])\n\nresults = pd.concat([results,knn_results])\nresults","3a08adac":"logistic = LogisticRegression()\nlogistic.fit(xtrain,ytrain)\ny_pred = logistic.predict(xtest)\n\nprint(classification_report(ytest, y_pred))\nprint(confusion_matrix(ytest,y_pred))","74be9d1e":"score = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\ntraining = logistic.score(xtrain,ytrain)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\nlog_results = pd.DataFrame({'Classifier':['Logistic'], 'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[2])\n\nresults = pd.concat([results,log_results])\nresults","5e832f09":"gnb = GaussianNB()\nmnb = MultinomialNB()\nbnb = BernoulliNB()\n\ngnb.fit(xtrain,ytrain)\nmnb.fit(xtrain,ytrain)\nbnb.fit(xtrain,ytrain)\n\ny_pred1 = gnb.predict(xtest)\ny_pred2 = mnb.predict(xtest)\ny_pred3 = bnb.predict(xtest)","69616ebb":"score1 = accuracy_score(ytest,y_pred1)\nscore2 = accuracy_score(ytest,y_pred2)\nscore3 = accuracy_score(ytest,y_pred3)\n\nrecall1 = recall_score(ytest,y_pred1)\nrecall2 = recall_score(ytest,y_pred2)\nrecall3 = recall_score(ytest,y_pred3)\n\nprecision1 = precision_score(ytest,y_pred1)\nprecision2 = precision_score(ytest,y_pred2)\nprecision3 = precision_score(ytest,y_pred3)\n\nf1score1 = f1_score(ytest,y_pred1)\nf1score2 = f1_score(ytest,y_pred2)\nf1score3 = f1_score(ytest,y_pred3)\n\ntraining1 = gnb.score(xtrain,ytrain)\ntraining2 = mnb.score(xtrain,ytrain)\ntraining3 = bnb.score(xtrain,ytrain)\n\nfpr1,tpr1, _ = roc_curve(ytest, y_pred1)\nroc_auc1 = auc(fpr1,tpr1)\nfpr2,tpr2, _ = roc_curve(ytest, y_pred2)\nroc_auc2 = auc(fpr2,tpr2)\nfpr3,tpr3, _ = roc_curve(ytest, y_pred3)\nroc_auc3 = auc(fpr3,tpr3)\n\nnb_results = pd.DataFrame({'Classifier':['Gaussian NB','Multinomial NB','Bernoulli NB'], \n                           'Training Accuracy':[training1,training2,training3],\n                           'Test Accuracy': [score1,score2,score3],'Recall':[recall1,recall2,recall3],\n                           'Precision':[precision1,precision2,precision3],\n                           'F1 Score':[f1score1,f1score2,f1score3],'AUC':[roc_auc1,roc_auc2,roc_auc3]},index=[3,4,5])\n\nresults = pd.concat([results,nb_results])\nresults","f4f192e6":"### Class Imbalance in Training set\nplt.title('Class Imbalance in Target Variable-Revenue')\nplt.pie(ytrain.value_counts(),autopct='%.2f%%',labels=['Negative','Positive'])\nplt.show()\nprint(ytrain.value_counts())","1ca5e8a9":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before UpSampling, counts of label '1': {}\".format(sum(ytrain==1)))\nprint(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(ytrain==0)))\n\nsmt = SMOTE(random_state = 25)   #Synthetic Minority Over Sampling Technique\nxtrain_bal, ytrain_bal = smt.fit_sample(xtrain,ytrain)\n\nprint(\"After UpSampling, counts of label '1': {}\".format(sum(ytrain_bal==1)))\nprint(\"After UpSampling, counts of label '0': {} \\n\".format(sum(ytrain_bal==0)))\n\nplt.title('Class Imbalance in Target Variable-Revenue after over sampling')\nplt.pie(pd.Series(ytrain_bal).value_counts(),autopct='%.2f%%',labels=['Negative','Positive'])\nplt.show()","bd34cf62":"#for decision tree balanced: \n#parameter={'max_depth':np.arange(1,18),'criterion':['gini','entropy']}\n#gs = GridSearchCV(dt_bal,parameter,cv=4,scoring='f1')\n#gs.fit(xtrain_bal,ytrain_bal)\n#gs.best_params_","33dcf9e8":"dt_bal = DecisionTreeClassifier(criterion='entropy',max_depth = 5,random_state=25)\ndt_bal.fit(xtrain_bal,ytrain_bal)\ny_pred = dt_bal.predict(xtest)\n\nprint(confusion_matrix(ytest,y_pred))\nprint(classification_report(ytest,y_pred))","6735edb6":"score = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\ntraining = dt_bal.score(xtrain,ytrain)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\n\nbl_results = pd.DataFrame({'Classifier':['DT Balanced'], 'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[0])\n\nbl_results","0e42c048":"#for RF balanced: \n#parameter={'n_estimators':np.arange(1,101)}\n#gs = GridSearchCV(rfc,parameter,cv=3,scoring='roc_auc')\n#gs.fit(xtrain_bal,ytrain_bal)\n#gs.best_params_","184f9774":"from sklearn.ensemble import RandomForestClassifier \n\nrfc = RandomForestClassifier(n_estimators=51,random_state=25)\nrfc.fit(xtrain_bal,ytrain_bal)\ny_pred = rfc.predict(xtest)\n\nprint(confusion_matrix(ytest,y_pred))\nprint(classification_report(ytest,y_pred))","fda258ec":"score = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\ntraining = rfc.score(xtrain,ytrain)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\n\nrf_results = pd.DataFrame({'Classifier':['RF Balanced'], 'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[1])\n\nbl_results = pd.concat([bl_results,rf_results])\nbl_results","a3d3bdfb":"#parameter={'n_estimators':np.arange(1,101) }\n#gs= GridSearchCV(abc,parameter,cv=3,scoring='f1')\n#gs.fit(xtrain_bal,ytrain_bal)\n#gs.best_params_","6956f20b":"from sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier(n_estimators=71,random_state=25)\nabc.fit(xtrain_bal,ytrain_bal)\ny_pred = abc.predict(xtest)\n\nprint(confusion_matrix(ytest,y_pred))\nprint(classification_report(ytest,y_pred))","f364c772":"score = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\ntraining = abc.score(xtrain,ytrain)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\n\nabc_results = pd.DataFrame({'Classifier':['ADA-boost Balanced'], 'Training Accuracy':[training],\n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[2])\n\nbl_results = pd.concat([bl_results,abc_results])\nbl_results","655a06ba":"#parameter={'n_estimators':np.arange(1,101) }\n#gs= GridSearchCV(gbc,parameter,cv=3,scoring='f1')\n#gs.fit(xtrain_bal,ytrain_bal)","92a0feec":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=51,random_state=25)\ngbc.fit(xtrain_bal,ytrain_bal)\ny_pred = gbc.predict(xtest)\n\nprint(confusion_matrix(ytest,y_pred))\nprint(classification_report(ytest,y_pred))","edc939f3":"score = accuracy_score(ytest,y_pred)\nrecall = recall_score(ytest,y_pred)\nprecision = precision_score(ytest,y_pred)\nf1score = f1_score(ytest,y_pred)\ntraining = gbc.score(xtrain,ytrain)\nfpr,tpr, _ = roc_curve(ytest, y_pred)\nroc_auc = auc(fpr,tpr)\n\ngbc_results = pd.DataFrame({'Classifier':['Gradient-boost Balanced'],'Training Accuracy':[training], \n                          'Test Accuracy': [score],'Recall':[recall],\n                          'Precision':[precision],'F1 Score':[f1score],'AUC':[roc_auc]},index=[3])\n\nbl_results = pd.concat([bl_results,gbc_results])\nbl_results","9348f1a1":"print('Area under Curve :',roc_auc)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristics of Gradient boost classifier')\nplt.show()","85328640":"# Building Base Classification models","1d1bd46a":"## Gradient Boost","d154886b":"## Treatment of Outliers","0fb792f1":"### Inferences\n\n- We can observe that the features admin_duration,info_duration,product_duration are Pareto Distributed which tells us that most observations of online customers are spending very fews seconds online\n- Very few observations of online customers are spending huge time online\n- Histogram for product pages tells us that most customers are opening less than 100 Pages related to products\n- But there are few customers who are opening more than 100 product pages\n- There are many observations having Avg. Bounce rate below 0.05\n- The avg. page value is also Pareto distributed with mode 0","08367948":"A Baseline model is built using all the features in the data without any transformations and the accuracy of this model is taken as reference for the further improvements and model building ","750bca5d":"## 2.KNN classifier","ac8f4a25":"## Data Standardization","3629a60f":"### Bivariate or Multivariate Analysis","8af17287":"### Inferences\n- With increasing no. of openings of administrative and information pages results in increasing chances for a revenue \n- Chances of making transaction is increasing month by month from jan to november\n- Chance of making a transaction is high for a new visitor\n- transaction chances are high in weekend comparatively\n- avg. bounce rate and avg. exit rates are linear and positively correlated\n- avg. bounce rate and avg. exit rates are low for those resulting in a transaction\n- chances for making a transaction is increasing with increase in product pages visited\n- we can say confindently say that the customers who spent a longer administrative duration in a website are very less likely to bounce or exit from the website that is navigating away from the website just after navigating one page of that website.","c0b2419e":"Since, p-value for region is >0.05, hence it is not significant in predicting target revenue","15c540f6":"### Decision Tree","6a07ffc4":"# 1.Decison Tree","f94c8c79":"> #### So,far the best algorithm for the given dataset is Gradient boost which gave us highest recall score and AUC i.e.,power of predicting 1's(minority)","2100cf8c":"## Base models for Balanced","5b79cac8":"## Exploratory Data Analysis","78f2974e":"## Chi-square test for Independence","8a2ab700":"# Baseline Model ","7f908445":"## One-hot Encoding","b3f70810":"From t-test We can say that all the continuous variables are significant predictors of revenue","307802d0":"## Random Forest Classifier","7164b646":"## Univariate analysis","4ffb8830":"## Ada Boosting","42343085":"models = zip([dt,knn,gnb,mnb,bnb,logistic],['Decision Tree','KNN','Gaussian NB','Multinomial NB','Bernoulli NB','Logistic'])\nclassifier = []\naccuracy = []\nrecall = []\nprecision = []\nf1score = []\n\nfor model,name in models:\n    model.fit(xtrain,ytrain)\n    y_pred = model.predict(xtest)\n    classifier.append(name)\n    print('model :',name)\n    print(confusion_matrix(ytest,y_pred))\n    print(classification_report(ytest,y_pred))\n    print('\\n')\n    accuracy.append(accuracy_score(ytest,y_pred))\n    recall.append(recall_score(ytest,y_pred))\n    precision.append(precision_score(ytest,y_pred))\n    f1score.append(f1_score(ytest,y_pred))\n    \n    \n    \nresults = pd.DataFrame({'Classifier': classifier, 'Test Accuracy': accuracy,'Recall':recall,\n                          'Precision':precision,'F1 Score':f1score})\nresults","3952977f":"THERE ARE NO MISSING VALUES PRESENT IN THE DATASET","79d53cd8":"## Categorizing Duration columns","d77e574d":"### Class Imbalance in Training set","5f0c0f11":"### Inferences\n- we can observe that most observations for Admin and Info pages are at 0\n- Most online customers observations are in the months: March,May(Summer holidays),November and December(year ending)\n- Most of Online customers are Using OS:1,2,3 and Browser :1,2\n- Most of Online customers are from the Region 1 followed by region 3\n- Most of Online customers are Returning visitors\n- Most of the Observations are recorded in weekdays itself","5c64f07c":"### 3.Logistic Regression","50a623d5":"### 4.Naive Bayes","4ec0649a":"## Balancing the Training set using SMOTE\n\n- The class ratio after the over sampling of the minority is to be 1:1 .","ad3cffc9":"Base Models on imbalanced dataset","1fabb819":"## Two sample T-test of independence"}}