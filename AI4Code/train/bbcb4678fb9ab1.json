{"cell_type":{"6d223ccf":"code","a0898bbb":"code","07839659":"code","ea399a33":"code","c4de5453":"code","7265c8a6":"code","cfd6d47a":"code","5973dd61":"code","2b897a90":"code","665ab1a6":"code","47e276e3":"code","883c0995":"code","8725ac7b":"code","05817043":"code","669a81c2":"code","f94eac72":"code","e872ca94":"code","37a47141":"code","93a149a0":"code","a700cb8a":"code","d8ff4d3f":"code","21f57db0":"code","600dd509":"code","87481741":"code","7fe01756":"code","c5813394":"code","85a1045d":"code","137f232c":"markdown","82a1bcc9":"markdown","b9f7e63e":"markdown","b76319f5":"markdown","26a02eb1":"markdown","ce3f887a":"markdown","2a5e3311":"markdown"},"source":{"6d223ccf":"import pandas as pd\npd.options.display.max_columns = 500\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nimport warnings\nwarnings.filterwarnings('ignore')","a0898bbb":"XY = pd.read_csv('..\/input\/absenteeism-at-work-short-version\/Absenteeism_at_work.csv')","07839659":"XY.head(2)","ea399a33":"XY.info()","c4de5453":"XY.shape","7265c8a6":"XY.describe()","cfd6d47a":"# We will create the Y target columns, less than 4 hours of absenteeism = 0, else 1.\nXY['Target'] = np.where(XY['Absenteeism time in hours'] > 4, '1','0')","5973dd61":"XY = XY.drop(columns=['Absenteeism time in hours', 'ID'])\nXY.Target = XY.Target.astype('int')","2b897a90":"#DEfinicion de funciones\ndef relaciones_vs_target(X, Y, return_type='axes'):\n    '''\n    Funci\u00f3n que representa gr\u00e1ficos de dispersi\u00f3n de las variables\n    en X en funci\u00f3n a la variable Y\n    '''\n    fig_tot = (len(X.columns))\n    fig_por_fila = 4.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+5 ) )\n    c = 0 \n    for i, col in enumerate(X.columns):\n        plt.subplot(num_filas, fig_por_fila, i+1)\n        sns.scatterplot(x=X[col], y=Y)\n        plt.title( '%s vs %s' % (col, 'target') )\n        plt.ylabel('Target')\n        plt.xlabel(col)\n    plt.show()\n\ndef represento_doble_hist(x_1, x_0, n_bins=11, title='', label_1='Clase 1', \n                          label_0='Clase 0', density=0):\n    '''\n    Funci\u00f3n que recibe dos distribuciones de probabilidad y las representa\n    en el mismo gr\u00e1fico\n    '''\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='red')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='green')\n    plt.title(title)\n    plt.legend(loc='best') \n\ndef hist_pos_neg_feat(x, y, density=0, nbins=11, targets=(0,1)):\n    '''\n    Representa las variables en x divididas en dos distribuciones\n    seg\u00fan su valor de y sea 1 o 0\n    '''\n    fig_tot = len(x.columns)\n    fig_tot_fila = 4.; fig_tamanio = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_tot_fila) )\n    plt.figure( figsize=( fig_tot_fila*fig_tamanio+2, num_filas*fig_tamanio+2 ) )\n    target_neg, target_pos = targets\n    for i, feat in enumerate(x.columns):\n        plt.subplot(num_filas, fig_tot_fila, i+1);\n        plt.title('%s' % feat)\n        idx_pos = y == target_pos\n        idx_neg= y == target_neg\n        represento_doble_hist(x[feat][idx_pos].values, x[feat][idx_neg].values, nbins, \n                   density = density, title=('%s' % feat))","665ab1a6":"XY.info()","47e276e3":"XY_normalizado = (XY-XY.mean())\/XY.std()\n# This function, let as see a more ordered graph.","883c0995":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=XY_normalizado)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","8725ac7b":"X = XY.drop('Target', axis=1)\nY = XY['Target']\nrelaciones_vs_target(X, Y)","05817043":"matriz_correlaciones = XY.corr(method='pearson')\nn_ticks = len(XY.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), XY.columns, rotation='vertical')\nplt.yticks(range(n_ticks), XY.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Matriz de correlaciones de Pearson')","669a81c2":"correlaciones_target = matriz_correlaciones.values[ -1, : -1]\nindices_inversos =  abs(correlaciones_target[ : ]).argsort()[ : : -1]\ndiccionario = {}\nfor nombre, correlacion in zip( X.columns[indices_inversos], list(correlaciones_target[indices_inversos] ) ):\n    diccionario[nombre] = correlacion\npd.DataFrame.from_dict(diccionario, orient='index', columns=['Correlaci\u00f3n con la target'])","f94eac72":"obj_escalar = StandardScaler()\nX_estandarizado = obj_escalar.fit_transform(X)","e872ca94":"X_train, X_test, Y_train, Y_test = train_test_split(X_estandarizado, Y, test_size=0.2, random_state=0)","37a47141":"modelo = MLPClassifier()\nparametros = {'solver': ['lbfgs'], \n              'max_iter': [100,200,300,500], # Iteraciones m\u00e1ximas en cada red\n              'alpha': 10.0 ** -np.arange(0.5, 2), # Par\u00e1metro de regularizaci\u00f3n L2 para evitar sobreajuste\n              'hidden_layer_sizes':np.arange(10, 35), # N\u00famero de neuronas en cada capa\n              'random_state':[0]}","93a149a0":"modelo_gs = GridSearchCV(modelo, param_grid=parametros, cv = 3, \n                         scoring='roc_auc', n_jobs=-1, verbose=10)\nmodelo_gs.fit(X_train, Y_train)","a700cb8a":"print(modelo_gs.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs.best_score_,2)))","d8ff4d3f":"print(modelo_gs.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs.best_score_,2)))","21f57db0":"mejor_modelo = MLPClassifier(**modelo_gs.best_params_, verbose=10)","600dd509":"mejor_modelo.fit(X_train, Y_train)","87481741":"y_test_pred_prob = mejor_modelo.predict_proba(X_test) \ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]","7fe01756":"preds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c5813394":"umbral = 0.5\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","85a1045d":"print(u\"Matriz de confusi\u00f3n\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisi\u00f3n\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2))) ","137f232c":"# I threshold the predictions:","82a1bcc9":"# Predictions","b9f7e63e":"\n# Prior standardization of data:","b76319f5":"# graphics","26a02eb1":"# Data preprocessing | Preprocesamiento de datos","ce3f887a":"### Looking for the best parameters \u00a1","2a5e3311":"#                     Absenteeism\n\n### The human resources department of a company has analyzed the data of its employees and discovered that there is a problem with absenteeism.\n\n### They have collected data and want to make a model that correctly classifies employees based on whether they miss more than four hours a month or not. This will help them to make future hires, and even keep track of their current employees.\n\n### That is a nice workflow that you coud use like base for yours problems of clasification.\n\n### One of the big problem using Neural Networks for clasifications problems lies in that we cannot know the reason behind the problem, because NN are a black box sistem. \n"}}