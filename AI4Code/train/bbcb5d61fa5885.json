{"cell_type":{"a509470f":"code","10625361":"code","428c7bec":"code","62bf48e5":"code","9f9c327c":"code","adcbfaec":"code","635ce597":"code","bc56c7cf":"code","2d1869cf":"code","dbd3d743":"code","d1974bcb":"code","080d7654":"code","38a58f9a":"code","2abb98fe":"code","b0b5090a":"code","119b7b2a":"code","a6cdfc48":"code","69f1b30d":"code","878d67d0":"code","3906659d":"code","256d9e25":"code","d9284a0d":"code","dcf89280":"code","093f8f06":"code","3ea5000d":"code","03853e59":"code","bf89f2aa":"code","bde5e581":"markdown","0c47b496":"markdown"},"source":{"a509470f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","10625361":"def Score_data(pred, real):\n    # computing errors\n    errors = np.abs(pred - real).flatten()\n    # estimation\n    mean = sum(errors)\/len(errors)\n    cov = 0\n    for e in errors:\n        cov += (e - mean)**2\n    cov \/= len(errors)\n\n    print('mean : ', mean)\n    print('cov : ', cov)\n    return errors, cov, mean\n\n# calculate Mahalanobis distance\ndef Mahala_distantce(x,mean,cov):\n    return (x - mean)**2 \/ cov\n\n\ndef scale(A):\n    return (A-np.min(A))\/(np.max(A) - np.min(A))\n\n\ndef stats_dfs(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts()\/df.shape[0]*100)\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts()\/df.shape[0]*100)\n    return df\n\n\ndef stats_dfs_freeanomaly(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    return df\n","428c7bec":"list_df_1 = [\"\/kaggle\/input\/skoltech-anomaly-benchmark-skab\/SKAB\/valve2\/1.csv\"]\ndf = stats_dfs(list_df_1[0])\ndf.head()","62bf48e5":"# df = df.loc[:550]\ntest = df.loc[550:]","9f9c327c":"list_a_free = [\"\/kaggle\/input\/skoltech-anomaly-benchmark-skab\/SKAB\/anomaly-free\/anomaly-free.csv\"]\ndf_a_free = stats_dfs_freeanomaly(list_a_free[0])\ndf_a_free.head()","adcbfaec":"raw_data = pd.read_csv(\"..\/input\/benckmark-anomaly-timeseries-skab\/alldata_skab.csv\")\nprint(raw_data.columns)\nprint(raw_data.head())\n\nprint(\"anomaly \", raw_data.anomaly.value_counts())\nprint(\"changepoint \",raw_data.changepoint.value_counts())\n\n# # Plotting\npd.DataFrame(raw_data[['Volume Flow RateRMS', 'anomaly', 'changepoint']].values, columns=['Volume Flow RateRMS', 'anomaly', 'changepoint'], index = raw_data.index).plot(figsize=(12,6))\n\nplt.xlabel('Values')\nplt.ylabel('Values')\nplt.title('Residuals')\nplt.show()","635ce597":"raw_data = df.copy()\nraw_data.set_index('datetime')\n# # Plotting\npd.DataFrame(raw_data.values, columns=raw_data.columns, index = raw_data.index).plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals')\nplt.show()","bc56c7cf":"import matplotlib.pyplot as plt# Standardize\/scale the dataset and apply PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n# Extract the names of the numerical columns\n\n# x = df[['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', 'Thermocouple', 'Voltage', 'Volume Flow RateRMS']]\nx = df[['Volume Flow RateRMS']]","2d1869cf":"scaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler, pca)\n# pipeline.fit(x.values.reshape(-8, 8))\npipeline.fit(x.values.reshape(-1, 1))","dbd3d743":"# Plot the principal components against their inertia\nfeatures = range(pca.n_components_)\n_ = plt.figure(figsize=(15, 5))\n_ = plt.bar(features, pca.explained_variance_)\n_ = plt.xlabel('PCA feature')\n_ = plt.ylabel('Variance')\n_ = plt.xticks(features)\n_ = plt.title(\"Importance of the Principal Components based on inertia\")\nplt.show()","d1974bcb":"# # Calculate PCA with 8 components\n# pca = PCA(n_components=8)\n# principalComponents = pca.fit_transform(x.values.reshape(-8,8))\n# principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8'])\n\n# Calculate PCA with 1 components\npca = PCA(n_components=1)\nprincipalComponents = pca.fit_transform(x.values.reshape(-1,1))\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1'])\n","080d7654":"from statsmodels.tsa.stattools import adfuller\n# Run Augmented Dickey Fuller Test\nresult = adfuller(principalDf['pc1'])\n# Print p-value\nprint(result[1] >0.05, result[1])","38a58f9a":"from statsmodels.tsa.arima_model import ARIMA\n\n# follow lag\nmodel_ar = ARIMA(principalDf['pc1'].loc[550:], order=(1,1,0))  \nresults_ARIMA_ar = model_ar.fit(disp=-1)","2abb98fe":"# Forecast\nfc, se, conf = results_ARIMA_ar.forecast(513, alpha=0.05)  # 95% conf","b0b5090a":"# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(principalDf['pc1'].loc[550:], label='training') # 550, train\nplt.plot(principalDf['pc1'].loc[:550], label='actual')  # 513, test\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","119b7b2a":"errors, cov, mean = Score_data(fc_series.values , principalDf['pc1'].loc[550:].values)\n\nmahala_dist = []\nfor e in errors:\n    mahala_dist.append(Mahala_distantce(e, mean, cov))\n\n","a6cdfc48":"test['pca1_value'] = principalDf['pc1'].loc[550:]\ntest['pca1_scores'] = mahala_dist\n\ntest['pca1_scores_norm'] = scale(mahala_dist)\nplt.figure(figsize=(12, 8))\nplt.hist(test['pca1_scores_norm'], bins=50);","69f1b30d":"q1_pc1, q3_pc1 = test['pca1_scores'].quantile([0.10, 0.60])\niqr_pc1 = q3_pc1 - q1_pc1\n\n# Calculate upper and lower bounds for outlier for pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n# Filter out the outliers from the pc1\ntest['outlier_pca1'] = ((test['pca1_scores']>upper_pc1) | (test['pca1_scores']<lower_pc1)).astype('int')\ntest['outlier_pca1'].value_counts()","878d67d0":"# fig, axes = plt.subplots(nrows=2, figsize=(15,10))\n# axes[0].plot(test[['pca1_scores']], color='blue')\n# axes[1].plot(np.array(mahala_dist).ravel(), color='red')\n\n# axes[0].set_title('original data', fontsize=20)\n# axes[1].set_title('outlier score', fontsize=20)\n\n# # axes[0].grid()\n# # axes[1].grid()\n# plt.tight_layout()\n# plt.show()","3906659d":"# visualization\na = test.loc[test['anomaly'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('True Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","256d9e25":"# visualization\na = test.loc[test['outlier_pca1'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","d9284a0d":"N = test.shape[0]\nplt.scatter(range(N),test['pca1_scores_norm'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Readings')\nplt.ylabel('anomalies frequency')\nplt.legend()\nplt.show()","dcf89280":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positive','Negative']\nplt.hist(test[test['outlier_pca1']==1]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(test[test['outlier_pca1']==0]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='decision boundary')\n# plt.xlim([0,1])\nplt.title('Distributions', size=13)\nplt.xlabel('Norm values', size=13)\nplt.ylabel('Readings (norm.)', size=13)\nplt.legend(loc=\"upper right\")","093f8f06":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(test['anomaly'], test['outlier_pca1']))\nconfusion_matrix(test['anomaly'], test['outlier_pca1'])","3ea5000d":"print(classification_report(test['changepoint'], test['outlier_pca1']))\nconfusion_matrix(test['changepoint'], test['outlier_pca1'])","03853e59":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(test['outlier_pca1'], test['anomaly'])","bf89f2aa":"roc_auc_score(test['outlier_pca1'], test['changepoint'])","bde5e581":"## Using PCA1 component with AR model","0c47b496":"The test have value very small number (much smaller than 0.05). Thus, I will reject the Null Hypothesis and say the data is stationary"}}