{"cell_type":{"6488cf69":"code","2d0e1180":"code","a8275934":"code","c893d1c3":"code","d8e26e48":"code","2771dc55":"code","4e684e9f":"code","a30d5065":"code","8cdfbe3a":"code","5cda48f0":"code","cd480a31":"code","275bf6dd":"code","193d0714":"code","4b6f2a8d":"code","05192456":"code","65054757":"code","0daba19e":"code","043327c7":"code","1bae7713":"markdown","ae93fe04":"markdown","92068336":"markdown","3af0b8b7":"markdown","1ca03fad":"markdown","6a021074":"markdown","f4b865f9":"markdown","7aab46a0":"markdown","67acbf77":"markdown","e653d9c1":"markdown","e264241e":"markdown","6345c8d7":"markdown","03200120":"markdown","10fd9110":"markdown"},"source":{"6488cf69":"import random\nfrom random import sample\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torch\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import uniform_filter1d\nfrom scipy.interpolate import interp1d\nimport numpy as np \nimport pandas as pd  \nimport torch.nn as nn\nimport os\nimport time\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom collections import defaultdict\n\nEPOCH = 50\nBATCH_SIZE = 256\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMEMORY_LENGTH = 5 # max sequence length\nWIFI_NUM = 50\nFOLDS = 10\ninpath = '..\/input\/indoor-location-navigation\/'\nmetapath = inpath + 'metadata'\ntrainpath = inpath + 'train'\ntestpath = inpath + 'test'\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed()","2d0e1180":"sample_submission = pd.read_csv('..\/input\/indoor-location-navigation\/sample_submission.csv')","a8275934":"floors = pd.read_csv('\/kaggle\/input\/indoor-xy-floor\/result_floor_feb22.csv',index_col=0)\nfloors.head()","c893d1c3":"train_data = pd.read_csv('..\/input\/time-series-unified-wifi\/train_all.csv',index_col=0)\ntest_data = pd.read_csv('..\/input\/time-series-unified-wifi\/test_all.csv',index_col=0)\ntrain_data","d8e26e48":"del_path = []\nnew_trains = []\nfor i, (p, g) in enumerate(train_data.groupby('path_id')):\n    if len(g) > MEMORY_LENGTH:\n        del_path.append(p)\n        for j in range((len(g) \/\/ MEMORY_LENGTH) + 1):\n            if j == (len(g) \/\/ MEMORY_LENGTH):\n                tmp = g.iloc[j*MEMORY_LENGTH:]\n            else:\n                tmp = g.iloc[j*MEMORY_LENGTH:(j+1)*MEMORY_LENGTH]\n            tmp.loc[:, 'path_id'] = p + '_' + str(j)\n            new_trains.append(tmp)\n        \ntrain_data.drop(train_data[train_data['path_id'].isin(del_path)].index, inplace=True)\ntrain_data = pd.concat([train_data, pd.concat(new_trains)]).reset_index(drop=True)\nprint(f\"previous path len:{i+1}, current path len:{len(train_data.groupby('path_id'))}\")\ndel new_trains\ngc.collect()","2771dc55":"BSSID_FEATS = [f'wb_{i}' for i in range(WIFI_NUM)]\nRSSI_FEATS = [f'wr_{i}' for i in range(WIFI_NUM)]\nX_train = train_data.loc[:, ['t1_wifi', 'building',\n                             'path_id'] + BSSID_FEATS + RSSI_FEATS]\ny_train = train_data.loc[:, ['t1_wifi', 'path_id', 'x', 'y', 'building']]\nX_test = test_data.loc[:, ['t1_wifi', 'building',\n                           'path_id'] + BSSID_FEATS + RSSI_FEATS]","4e684e9f":"test_building_weight = defaultdict(int)\ntrain_building_weight = defaultdict(int)\nbuilding_weight = dict()\n\nfor building in [x.split('_')[0] for x in sample_submission['site_path_timestamp'].values]:\n    test_building_weight[building] += 1 * 24\/len(sample_submission)\n    \nfor building in train_data['building'].values:\n    train_building_weight[building] += 1\ntrain_building_weight = dict((k, len(train_data)\/24\/v) for k, v in train_building_weight.items())\n\nfor building in list(train_building_weight.keys()):\n    building_weight[building] = train_building_weight[building] * test_building_weight[building]\n    \ny_train['weight'] = [building_weight[x] for x in y_train['building'].values]","a30d5065":"le = LabelEncoder()\nunique_bssids = np.unique(X_train.loc[:, BSSID_FEATS].values.tolist(\n) + X_test.loc[:, BSSID_FEATS].values.tolist())\nwifi_bssids_size = len(unique_bssids)\nprint(f\"bssids_size: {wifi_bssids_size}\")\nle.fit(unique_bssids)\nle_site = LabelEncoder()\nle_site.fit(list(set(X_train.loc[:, 'building'].values.tolist())))\n\nx_min = np.min(y_train.loc[:, 'x'].values)\ny_min = np.min(y_train.loc[:, 'y'].values)\nnorm_x = np.max(y_train.loc[:, 'x'].values) - x_min\nnorm_y = np.max(y_train.loc[:, 'y'].values) - y_min\n\nX_train.loc[:, RSSI_FEATS] = (X_train.loc[:, RSSI_FEATS] + 99) \/ (np.max(X_train.loc[:, RSSI_FEATS].values) + 99)\nX_test.loc[:, RSSI_FEATS] = (X_test.loc[:, RSSI_FEATS] +99) \/ (np.max(X_train.loc[:, RSSI_FEATS].values) + 99)\ny_train.loc[:, 'x'] = (y_train.loc[:, 'x'] - x_min) \/ norm_x\ny_train.loc[:, 'y'] = (y_train.loc[:, 'y'] - y_min) \/ norm_y\n\n\nfor i in BSSID_FEATS:\n    X_train.loc[:, i] = le.transform(X_train.loc[:, i])\n    X_test.loc[:, i] = le.transform(X_test.loc[:, i])\nX_train.loc[:, 'building'] = le_site.transform(X_train.loc[:, 'building'])\nX_test.loc[:, 'building'] = le_site.transform(X_test.loc[:, 'building'])\n\ntest_data.set_index(['path_id', 't1_wifi'], inplace=True)\nresult = pd.DataFrame(np.zeros([len(test_data), 2]),\n                     index=test_data.index, columns=['x', 'y'])\ndel train_data, test_data\ngc.collect()","8cdfbe3a":"def make_data(data, col):\n    train = []\n    for _, group in data.groupby('path_id'):\n        group = group.sort_values('t1_wifi')\n        train.append(group[col])\n    return train\n\ntrain_cols = ['building'] + BSSID_FEATS + RSSI_FEATS \ny_train = make_data(y_train, col=['x', 'y', 'weight'])\nX_train = make_data(X_train, col=train_cols)","5cda48f0":"class CustomDataset(Dataset):\n    def __init__(self, x_train, y_train, transform, inverse_ratio=0.5, combine_ratio=0.2):\n        self.transform = transform\n        self.x_train = x_train\n        self.y_train = y_train\n        self.inverse_ratio = inverse_ratio\n        self.combine_ratio = combine_ratio\n\n    def __getitem__(self, index):\n        x = self.x_train[index]\n        y = self.y_train[index]\n        mask = torch.tensor(np.full(MEMORY_LENGTH, True, dtype=bool))\n        if len(x) < MEMORY_LENGTH:\n            x_out = torch.tensor(\n                np.pad(x.to_numpy(), ([(0, MEMORY_LENGTH - len(x)), (0, 0)]), 'edge'))\n            y_out = torch.tensor(np.pad(\n                y.to_numpy(), ([(0, MEMORY_LENGTH - len(x)), (0, 0)]), 'edge'))\n            mask[len(x):] = False\n        else:\n            x_out = torch.tensor(x.to_numpy())\n            y_out = torch.tensor(y.to_numpy())\n\n        if self.transform:\n            # inverse trajectory\n            if np.random.rand() < self.inverse_ratio:\n                tmp = torch.arange(MEMORY_LENGTH-1, -1, -1)\n                x_out = x_out[tmp, :]\n                y_out = y_out[tmp, :]\n                mask = mask[tmp]\n\n            # # combine trajectory\n            if np.random.rand() < self.combine_ratio:\n                p = torch.randperm(MEMORY_LENGTH)\n                x_out = x_out[p]\n                y_out = y_out[p]\n                mask = mask[p]\n\n        return x_out, y_out, mask\n\n    def __len__(self):\n        return len(self.x_train)","cd480a31":"class ManyToMany(nn.Module):\n    def __init__(self, wifi_bssids_size, input_dim, hidden_dim):\n        super(ManyToMany, self).__init__()\n        self.emb_dim = 256\n        self.entitity_dim = 128\n        self.bssi = nn.Embedding(wifi_bssids_size, self.emb_dim)\n        self.building = nn.Embedding(24, 2)\n\n        self.entity1 = nn.Sequential(\n            nn.Linear(self.emb_dim + 3, self.entitity_dim),\n            nn.Tanh()\n        )\n        self.entity2 = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.entitity_dim*WIFI_NUM, input_dim),\n        )\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=1,batch_first=True)\n\n        self.main = nn.Sequential(\n            nn.Linear(hidden_dim, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 2),\n        )\n\n    def forward(self, x):\n        bssids = self.bssi(x[:, :, 1:int(WIFI_NUM+1)].long()).float()\n        buildings = self.building(x[:, :, 0].long()).unsqueeze(2).expand(-1, -1, WIFI_NUM, -1).float()\n        rssis = x[:, :, int(WIFI_NUM+1):int(WIFI_NUM*2+1)].unsqueeze(3).float()\n        \n        x = torch.cat((bssids,  rssis, buildings), axis=3)\n        #(batch, memory_length, wifi_num, self.emb_dim+2+1)\n        x = self.entity1(x)\n        #(batch, memory_length, wifi_num, self.entity1_dim)\n        x = x.flatten(start_dim=2)\n        #(batch, memory_length, wifi_num * self.entity1_dim)\n        x = self.entity2(x)\n        #(batch, memory_length, input_dim)\n        output, _ = self.gru(x)\n        output = self.main(output)\n        #(batch, memory_length, 2)\n        return output\n","275bf6dd":"def train_manytomany(t_loader, v_loader, optimizer, criterion, model,norms):\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n    model.train()\n    early_stopping = 0\n    loss_pred = 100000\n    for e in range(EPOCH):\n        for (x, y, m) in t_loader:\n            optimizer.zero_grad()\n            x, y = x.to(DEVICE), y.to(DEVICE).float()\n            output = model(x)\n            loss = criterion(output[m], y[m])\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        if (e+1) % 1 == 0:\n            losses = []\n            losses2 = []\n            total_point = 0\n            with torch.no_grad():\n                for (x, y, m) in v_loader:\n                    x, y = x.to(DEVICE), y.to(DEVICE).float()\n                    output = model(x)\n                    loss = comp_metric_xy(output[m], y[m], norms)\n                    loss2 = comp_metric_weighted_xy(output[m], y[m], norms)\n                    losses.append(loss.item())\n                    losses2.append(loss2.item())\n                    total_point += torch.count_nonzero(m)\n                tmp = np.sum(losses)\/total_point\n                tmp2 = np.sum(losses2)\/total_point\n                print(\n                    f\"epoch: {e}, loss: {tmp:.5f}, weight_loss: {tmp2:.5f} lr:{scheduler.get_last_lr()[0]:.5f}\")\n                if loss_pred < tmp2:\n                    early_stopping += 1\n                else:\n                    early_stopping = 0\n                    loss_pred = tmp2\n                    model_pred = model\n                if early_stopping > 5:\n                    return model_pred, loss_pred    \n    return model_pred, loss_pred\n\ndef comp_metric_xy(output, y, norms):\n    return torch.sqrt(((norms*(output - y[:,:2]))**2).sum())\ndef weighted_mse_loss(output, y):\n    return (y[:, 2].reshape(-1,1)*(output - y[:,:2])**2).sum()\ndef comp_metric_weighted_xy(output, y, norms):\n    return torch.sqrt(((y[:, 2].reshape(-1,1)*(norms * (output - y[:,:2]))**2)).sum())","193d0714":"buildings = []\nfor x in X_train:\n    buildings.append(x.iloc[0, 0])\n\nloss_folds = []\nskf = StratifiedKFold(n_splits=FOLDS)\ncriterion = weighted_mse_loss\nnorms = torch.tensor([[norm_x, norm_y]]).to(DEVICE)\nstart = time.time()\nfor fold, (idt, idv) in enumerate(skf.split(X_train, buildings)):\n    print('\\r', f'{fold}', end='\\t')\n    mtrain, mvalid = [X_train[i] for i in idt], [y_train[i] for i in idt]\n    ltrain, lvalid = [X_train[i] for i in idv], [y_train[i] for i in idv]\n    t_loader = DataLoader(CustomDataset(\n        mtrain, mvalid, transform=True), batch_size=BATCH_SIZE, shuffle=True)\n    v_loader = DataLoader(CustomDataset(\n        ltrain, lvalid, transform=False), batch_size=BATCH_SIZE, shuffle=False)\n\n    model = ManyToMany(wifi_bssids_size, 64, 128).to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n    \n    model, loss = train_manytomany(\n        t_loader, v_loader, optimizer, criterion, model, norms)\n\n    # prediction by sliding window averaging. see https:\/\/arxiv.org\/pdf\/1903.11703.pdf\n    prediction = []\n    with torch.no_grad():\n        for p, x in X_test.groupby('path_id'):\n            window_score = defaultdict(list)\n            x = x.sort_values('t1_wifi')\n            ts = x['t1_wifi'].to_numpy()\n            x = x[train_cols].reset_index(drop=True)\n            \n            for window in range(len(ts)):\n                if MEMORY_LENGTH + window > len(ts):\n                    break\n                \n                x_out = torch.tensor(\n                    x.iloc[window:(window+MEMORY_LENGTH)].to_numpy())\n                x_out = x_out.unsqueeze(0)\n                x_out = model(x_out.to(DEVICE))\n                for i in range(MEMORY_LENGTH):\n                    window_score[ts[window + i]].append(x_out.squeeze().cpu().detach().numpy()[i,:])\n                \n            #  sort by time and get average \n            window_score = sorted(window_score.items(), key=lambda x:x[0])\n            prediction.extend(list(map(lambda x: np.mean(x[1],axis=0), window_score)))\n\n    prediction = np.array(prediction)\n    result['x'] += (prediction[:, 0] * norm_x + x_min) \/ FOLDS\n    result['y'] += (prediction[:, 1] * norm_y + y_min) \/ FOLDS\n    loss_folds.append(loss)\n    print(f'fold: {fold}, loss: {loss}')\n    \n    del mtrain, mvalid, ltrain, lvalid, t_loader, v_loader, model, prediction\n    gc.collect()\n    \n\nprint(f\"mean loss: {np.mean(loss_folds)}, time:{(start -time.time())\/60:.1f} min.\")","4b6f2a8d":"sample_submission['building'] = [x.split('_')[0] for x in sample_submission['site_path_timestamp']]\nsample_submission['path_id'] = [x.split('_')[1] for x in sample_submission['site_path_timestamp']]\nsample_submission['timestamp'] = [x.split('_')[2] for x in sample_submission['site_path_timestamp']]\nsamples = pd.DataFrame(sample_submission.groupby(['building','path_id'])['timestamp'].apply(lambda x: list(x)))\nbuildings = np.unique([x[0] for x in samples.index])\nsamples.head()","05192456":"result.reset_index(inplace=True)\nresult.set_index('path_id', inplace=True)\nresult","65054757":"from scipy.spatial.transform import Rotation as R\nfrom PIL import Image\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.graph_objs as go\nfrom pathlib import Path\nimport scipy.signal as signal\nimport json\nimport seaborn as sns # visualization\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt  # visualization\nimport numpy as np  # linear algebra\nimport random\nimport pandas as pd\nfrom collections import Counter, defaultdict\n\nplt.rcParams.update({'font.size': 14})\n\ndef split_ts_seq(ts_seq, sep_ts):\n    \"\"\"\n\n    :param ts_seq:\n    :param sep_ts:\n    :return:\n    \"\"\"\n    tss = ts_seq[:, 0].astype(float)\n    unique_sep_ts = np.unique(sep_ts)\n    ts_seqs = []\n    start_index = 0\n    for i in range(0, unique_sep_ts.shape[0]):\n        end_index = np.searchsorted(tss, unique_sep_ts[i], side='right')\n        if start_index == end_index:\n            continue\n        ts_seqs.append(ts_seq[start_index:end_index, :].copy())\n        start_index = end_index\n\n    # tail data\n    if start_index < ts_seq.shape[0]:\n        ts_seqs.append(ts_seq[start_index:, :].copy())\n\n    return ts_seqs\n\n\ndef correct_trajectory(original_xys, end_xy):\n    \"\"\"\n\n    :param original_xys: numpy ndarray, shape(N, 2)\n    :param end_xy: numpy ndarray, shape(1, 2)\n    :return:\n    \"\"\"\n    corrected_xys = np.zeros((0, 2))\n\n    A = original_xys[0, :]\n    B = end_xy\n    Bp = original_xys[-1, :]\n\n    angle_BAX = np.arctan2(B[1] - A[1], B[0] - A[0])\n    angle_BpAX = np.arctan2(Bp[1] - A[1], Bp[0] - A[0])\n    angle_BpAB = angle_BpAX - angle_BAX\n    AB = np.sqrt(np.sum((B - A) ** 2))\n    ABp = np.sqrt(np.sum((Bp - A) ** 2))\n\n    corrected_xys = np.append(corrected_xys, [A], 0)\n    for i in np.arange(1, np.size(original_xys, 0)):\n        angle_CpAX = np.arctan2(original_xys[i, 1] - A[1], original_xys[i, 0] - A[0])\n\n        angle_CAX = angle_CpAX - angle_BpAB\n\n        ACp = np.sqrt(np.sum((original_xys[i, :] - A) ** 2))\n\n        AC = ACp * AB \/ ABp\n\n        delta_C = np.array([AC * np.cos(angle_CAX), AC * np.sin(angle_CAX)])\n\n        C = delta_C + A\n\n        corrected_xys = np.append(corrected_xys, [C], 0)\n\n    return corrected_xys\n\n\ndef correct_positions(rel_positions, reference_positions):\n    \"\"\"\n\n    :param rel_positions:\n    :param reference_positions:\n    :return:\n    \"\"\"\n    rel_positions_list = split_ts_seq(rel_positions, reference_positions[:, 0])\n    if len(rel_positions_list) != reference_positions.shape[0] - 1:\n        # print(f'Rel positions list size: {len(rel_positions_list)}, ref positions size: {reference_positions.shape[0]}')\n        del rel_positions_list[-1]\n    assert len(rel_positions_list) == reference_positions.shape[0] - 1\n\n    corrected_positions = np.zeros((0, 3))\n    for i, rel_ps in enumerate(rel_positions_list):\n        start_position = reference_positions[i]\n        end_position = reference_positions[i + 1]\n        abs_ps = np.zeros(rel_ps.shape)\n        abs_ps[:, 0] = rel_ps[:, 0]\n        # abs_ps[:, 1:3] = rel_ps[:, 1:3] + start_position[1:3]\n        abs_ps[0, 1:3] = rel_ps[0, 1:3] + start_position[1:3]\n        for j in range(1, rel_ps.shape[0]):\n            abs_ps[j, 1:3] = abs_ps[j-1, 1:3] + rel_ps[j, 1:3]\n        abs_ps = np.insert(abs_ps, 0, start_position, axis=0)\n        corrected_xys = correct_trajectory(abs_ps[:, 1:3], end_position[1:3])\n        corrected_ps = np.column_stack((abs_ps[:, 0], corrected_xys))\n        if i == 0:\n            corrected_positions = np.append(corrected_positions, corrected_ps, axis=0)\n        else:\n            corrected_positions = np.append(corrected_positions, corrected_ps[1:], axis=0)\n\n    corrected_positions = np.array(corrected_positions)\n\n    return corrected_positions\n\n\ndef init_parameters_filter(sample_freq, warmup_data, cut_off_freq=2):\n    order = 4\n    filter_b, filter_a = signal.butter(order, cut_off_freq \/ (sample_freq \/ 2), 'low', False)\n    zf = signal.lfilter_zi(filter_b, filter_a)\n    _, zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n    _, filter_zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n\n    return filter_b, filter_a, filter_zf\n\n\ndef get_rotation_matrix_from_vector(rotation_vector):\n    q1 = rotation_vector[0]\n    q2 = rotation_vector[1]\n    q3 = rotation_vector[2]\n\n    if rotation_vector.size >= 4:\n        q0 = rotation_vector[3]\n    else:\n        q0 = 1 - q1*q1 - q2*q2 - q3*q3\n        if q0 > 0:\n            q0 = np.sqrt(q0)\n        else:\n            q0 = 0\n\n    sq_q1 = 2 * q1 * q1\n    sq_q2 = 2 * q2 * q2\n    sq_q3 = 2 * q3 * q3\n    q1_q2 = 2 * q1 * q2\n    q3_q0 = 2 * q3 * q0\n    q1_q3 = 2 * q1 * q3\n    q2_q0 = 2 * q2 * q0\n    q2_q3 = 2 * q2 * q3\n    q1_q0 = 2 * q1 * q0\n\n    R = np.zeros((9,))\n    if R.size == 9:\n        R[0] = 1 - sq_q2 - sq_q3\n        R[1] = q1_q2 - q3_q0\n        R[2] = q1_q3 + q2_q0\n\n        R[3] = q1_q2 + q3_q0\n        R[4] = 1 - sq_q1 - sq_q3\n        R[5] = q2_q3 - q1_q0\n\n        R[6] = q1_q3 - q2_q0\n        R[7] = q2_q3 + q1_q0\n        R[8] = 1 - sq_q1 - sq_q2\n\n        R = np.reshape(R, (3, 3))\n    elif R.size == 16:\n        R[0] = 1 - sq_q2 - sq_q3\n        R[1] = q1_q2 - q3_q0\n        R[2] = q1_q3 + q2_q0\n        R[3] = 0.0\n\n        R[4] = q1_q2 + q3_q0\n        R[5] = 1 - sq_q1 - sq_q3\n        R[6] = q2_q3 - q1_q0\n        R[7] = 0.0\n\n        R[8] = q1_q3 - q2_q0\n        R[9] = q2_q3 + q1_q0\n        R[10] = 1 - sq_q1 - sq_q2\n        R[11] = 0.0\n\n        R[12] = R[13] = R[14] = 0.0\n        R[15] = 1.0\n\n        R = np.reshape(R, (4, 4))\n\n    return R\n\n\ndef get_orientation(R):\n    flat_R = R.flatten()\n    values = np.zeros((3,))\n    if np.size(flat_R) == 9:\n        values[0] = np.arctan2(flat_R[1], flat_R[4])\n        values[1] = np.arcsin(-flat_R[7])\n        values[2] = np.arctan2(-flat_R[6], flat_R[8])\n    else:\n        values[0] = np.arctan2(flat_R[1], flat_R[5])\n        values[1] = np.arcsin(-flat_R[9])\n        values[2] = np.arctan2(-flat_R[8], flat_R[10])\n\n    return values\n\n\ndef compute_steps(acce_datas):\n    step_timestamps = np.array([])\n    step_indexs = np.array([], dtype=int)\n    step_acce_max_mins = np.zeros((0, 4))\n    sample_freq = 50\n    window_size = 22\n    low_acce_mag = 0.6\n    step_criterion = 1\n    interval_threshold = 250\n\n    acce_max = np.zeros((2,))\n    acce_min = np.zeros((2,))\n    acce_binarys = np.zeros((window_size,), dtype=int)\n    acce_mag_pre = 0\n    state_flag = 0\n\n    warmup_data = np.ones((window_size,)) * 9.81\n    filter_b, filter_a, filter_zf = init_parameters_filter(sample_freq, warmup_data)\n    acce_mag_window = np.zeros((window_size, 1))\n\n    # detect steps according to acceleration magnitudes\n    for i in np.arange(0, np.size(acce_datas, 0)):\n        acce_data = acce_datas[i, :]\n        acce_mag = np.sqrt(np.sum(acce_data[1:] ** 2))\n\n        acce_mag_filt, filter_zf = signal.lfilter(filter_b, filter_a, [acce_mag], zi=filter_zf)\n        acce_mag_filt = acce_mag_filt[0]\n\n        acce_mag_window = np.append(acce_mag_window, [acce_mag_filt])\n        acce_mag_window = np.delete(acce_mag_window, 0)\n        mean_gravity = np.mean(acce_mag_window)\n        acce_std = np.std(acce_mag_window)\n        mag_threshold = np.max([low_acce_mag, 0.4 * acce_std])\n\n        # detect valid peak or valley of acceleration magnitudes\n        acce_mag_filt_detrend = acce_mag_filt - mean_gravity\n        if acce_mag_filt_detrend > np.max([acce_mag_pre, mag_threshold]):\n            # peak\n            acce_binarys = np.append(acce_binarys, [1])\n            acce_binarys = np.delete(acce_binarys, 0)\n        elif acce_mag_filt_detrend < np.min([acce_mag_pre, -mag_threshold]):\n            # valley\n            acce_binarys = np.append(acce_binarys, [-1])\n            acce_binarys = np.delete(acce_binarys, 0)\n        else:\n            # between peak and valley\n            acce_binarys = np.append(acce_binarys, [0])\n            acce_binarys = np.delete(acce_binarys, 0)\n\n        if (acce_binarys[-1] == 0) and (acce_binarys[-2] == 1):\n            if state_flag == 0:\n                acce_max[:] = acce_data[0], acce_mag_filt\n                state_flag = 1\n            elif (state_flag == 1) and ((acce_data[0] - acce_max[0]) <= interval_threshold) and (\n                    acce_mag_filt > acce_max[1]):\n                acce_max[:] = acce_data[0], acce_mag_filt\n            elif (state_flag == 2) and ((acce_data[0] - acce_max[0]) > interval_threshold):\n                acce_max[:] = acce_data[0], acce_mag_filt\n                state_flag = 1\n\n        # choose reasonable step criterion and check if there is a valid step\n        # save step acceleration data: step_acce_max_mins = [timestamp, max, min, variance]\n        step_flag = False\n        if step_criterion == 2:\n            if (acce_binarys[-1] == -1) and ((acce_binarys[-2] == 1) or (acce_binarys[-2] == 0)):\n                step_flag = True\n        elif step_criterion == 3:\n            if (acce_binarys[-1] == -1) and (acce_binarys[-2] == 0) and (np.sum(acce_binarys[:-2]) > 1):\n                step_flag = True\n        else:\n            if (acce_binarys[-1] == 0) and acce_binarys[-2] == -1:\n                if (state_flag == 1) and ((acce_data[0] - acce_min[0]) > interval_threshold):\n                    acce_min[:] = acce_data[0], acce_mag_filt\n                    state_flag = 2\n                    step_flag = True\n                elif (state_flag == 2) and ((acce_data[0] - acce_min[0]) <= interval_threshold) and (\n                        acce_mag_filt < acce_min[1]):\n                    acce_min[:] = acce_data[0], acce_mag_filt\n        if step_flag:\n            step_timestamps = np.append(step_timestamps, acce_data[0])\n            step_indexs = np.append(step_indexs, [i])\n            step_acce_max_mins = np.append(step_acce_max_mins,\n                                           [[acce_data[0], acce_max[1], acce_min[1], acce_std ** 2]], axis=0)\n        acce_mag_pre = acce_mag_filt_detrend\n\n    return step_timestamps, step_indexs, step_acce_max_mins\n\n\ndef compute_stride_length(step_acce_max_mins):\n    K = 0.4\n    K_max = 0.8\n    K_min = 0.4\n    para_a0 = 0.21468084\n    para_a1 = 0.09154517\n    para_a2 = 0.02301998\n\n    stride_lengths = np.zeros((step_acce_max_mins.shape[0], 2))\n    k_real = np.zeros((step_acce_max_mins.shape[0], 2))\n    step_timeperiod = np.zeros((step_acce_max_mins.shape[0] - 1, ))\n    stride_lengths[:, 0] = step_acce_max_mins[:, 0]\n    window_size = 2\n    step_timeperiod_temp = np.zeros((0, ))\n\n    # calculate every step period - step_timeperiod unit: second\n    for i in range(0, step_timeperiod.shape[0]):\n        step_timeperiod_data = (step_acce_max_mins[i + 1, 0] - step_acce_max_mins[i, 0]) \/ 1000\n        step_timeperiod_temp = np.append(step_timeperiod_temp, [step_timeperiod_data])\n        if step_timeperiod_temp.shape[0] > window_size:\n            step_timeperiod_temp = np.delete(step_timeperiod_temp, [0])\n        step_timeperiod[i] = np.sum(step_timeperiod_temp) \/ step_timeperiod_temp.shape[0]\n\n    # calculate parameters by step period and acceleration magnitude variance\n    k_real[:, 0] = step_acce_max_mins[:, 0]\n    k_real[0, 1] = K\n    for i in range(0, step_timeperiod.shape[0]):\n        k_real[i + 1, 1] = np.max([(para_a0 + para_a1 \/ step_timeperiod[i] + para_a2 * step_acce_max_mins[i, 3]), K_min])\n        k_real[i + 1, 1] = np.min([k_real[i + 1, 1], K_max]) * (K \/ K_min)\n\n    # calculate every stride length by parameters and max and min data of acceleration magnitude\n    stride_lengths[:, 1] = np.max([(step_acce_max_mins[:, 1] - step_acce_max_mins[:, 2]),\n                                   np.ones((step_acce_max_mins.shape[0], ))], axis=0)**(1 \/ 4) * k_real[:, 1]\n\n    return stride_lengths\n\n\ndef compute_headings(ahrs_datas):\n    headings = np.zeros((np.size(ahrs_datas, 0), 2))\n    for i in np.arange(0, np.size(ahrs_datas, 0)):\n        ahrs_data = ahrs_datas[i, :]\n        rot_mat = get_rotation_matrix_from_vector(ahrs_data[1:])\n        azimuth, pitch, roll = get_orientation(rot_mat)\n        around_z = (-azimuth) % (2 * np.pi)\n        headings[i, :] = ahrs_data[0], around_z\n    return headings\n\n\ndef compute_step_heading(step_timestamps, headings):\n    step_headings = np.zeros((len(step_timestamps), 2))\n    step_timestamps_index = 0\n    for i in range(0, len(headings)):\n        if step_timestamps_index < len(step_timestamps):\n            if headings[i, 0] == step_timestamps[step_timestamps_index]:\n                step_headings[step_timestamps_index, :] = headings[i, :]\n                step_timestamps_index += 1\n        else:\n            break\n    assert step_timestamps_index == len(step_timestamps)\n\n    return step_headings\n\n\ndef compute_rel_positions(stride_lengths, step_headings):\n    rel_positions = np.zeros((stride_lengths.shape[0], 3))\n    for i in range(0, stride_lengths.shape[0]):\n        rel_positions[i, 0] = stride_lengths[i, 0]\n        rel_positions[i, 1] = -stride_lengths[i, 1] * np.sin(step_headings[i, 1])\n        rel_positions[i, 2] = stride_lengths[i, 1] * np.cos(step_headings[i, 1])\n\n    return rel_positions\n\n\ndef compute_step_positions(acce_datas, ahrs_datas, posi_datas):\n    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n    headings = compute_headings(ahrs_datas)\n    stride_lengths = compute_stride_length(step_acce_max_mins)\n    step_headings = compute_step_heading(step_timestamps, headings)\n    rel_positions = compute_rel_positions(stride_lengths, step_headings)\n    step_positions = correct_positions(rel_positions, posi_datas)\n\n    return step_positions\n","0daba19e":"# Extract testing files, buildings and sites:\nos.system(f'grep SiteID {testpath}\/* > test_buildings.txt' )\ntest_buildings = pd.read_csv('test_buildings.txt',sep='\\t',header=None,names=['file','building','site'])\ntest_buildings['file'] = test_buildings['file'].apply(lambda x: x[:-2])\ntest_buildings['building'] = test_buildings['building'].apply(lambda x: x[7:])\n\n# How many buildings in the testing set?\nbuildings = np.unique(test_buildings['building'])\nprint('There are',len(buildings),'buildings in the testing set.')\n\ntest_buildings.head()\n# Compile C++ pre-processing code:\ner=os.system(\"g++ \/kaggle\/input\/indoor-cpp\/1_preprocess.cpp -std=c++11 -o preprocess\")\nif(er): print(\"Error\")\n\n# Reformat the testing set:\nos.system('mkdir test')\nfor i,(path_filename,building) in enumerate(zip(test_buildings['file'],test_buildings['building'])):\n    er=os.system(f'.\/preprocess {path_filename} test {building} {0}') #since we do not know the floor, I put 0.\n    if(er): print(\"Error:\",path_filename)\n# Acceleration, magnetic and orientation testing data:\nos.system('mkdir indoor_testing_accel')\nos.system(\"g++ \/kaggle\/input\/indoor-cpp\/2_preprocess_accel.cpp -std=c++11 -o preprocess_accel\")\nfor building in buildings:\n    os.system(f'.\/preprocess_accel {building}')\n# Wifi testing data:\nos.system('mkdir test_wifi')\nos.system(\"g++ \/kaggle\/input\/indoor-cpp\/2_preprocess_wifi.cpp -std=c++11 -o preprocess_wifi\")\nfor building in buildings:\n    os.system(f'.\/preprocess_wifi {building}')","043327c7":"from scipy.interpolate import interp1d\nfrom scipy.ndimage.filters import uniform_filter1d\n\ncolacce = ['xyz_time','x_acce','y_acce','z_acce']\ncolahrs = ['xyz_time','x_ahrs','y_ahrs','z_ahrs']\n\nfor building in buildings:\n    print(building)\n    paths = samples.loc[building].index\n    # Acceleration info:\n    tfm = pd.read_csv(f'indoor_testing_accel\/{building}.txt',index_col=0)\n    for path_id in paths:\n        # Original predicted values:\n        xy = result.loc[building+'_'+path_id]\n        tfmi = tfm.loc[path_id]\n        acce_datas = np.array(tfmi[colacce],dtype=np.float)\n        ahrs_datas = np.array(tfmi[colahrs],dtype=np.float)\n        posi_datas = np.array(xy[['t1_wifi','x','y']],dtype=np.float)\n        # Outlier removal:\n        xyout = uniform_filter1d(posi_datas,size=3,axis=0,mode='reflect')\n        xydiff = np.abs(posi_datas-xyout)\n        xystd = np.std(xydiff,axis=0)*3\n        posi_datas = posi_datas[(xydiff[:,1]<xystd[1])&(xydiff[:,2]<xystd[2])]\n        # Step detection:\n        step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n        stride_lengths = compute_stride_length(step_acce_max_mins)\n        # Orientation detection:\n        headings = compute_headings(ahrs_datas)\n        step_headings = compute_step_heading(step_timestamps, headings)\n        rel_positions = compute_rel_positions(stride_lengths, step_headings)\n        # Running average:\n        posi_datas = uniform_filter1d(posi_datas,size=3,axis=0,mode='reflect')[0::3,:]\n        # The 1st prediction timepoint should be earlier than the 1st step timepoint.\n        rel_positions = rel_positions[rel_positions[:,0]>posi_datas[0,0],:]\n        # If two consecutive predictions are in-between two step datapoints,\n        # the last one is removed, causing error (in the \"split_ts_seq\" function).\n        posi_index = [np.searchsorted(rel_positions[:,0], x, side='right') for x in posi_datas[:,0]]\n        u, i1, i2 = np.unique(posi_index, return_index=True, return_inverse=True)\n        posi_datas = np.vstack([np.mean(posi_datas[i2==i],axis=0) for i in np.unique(i2)])\n        # Position correction:\n        step_positions = correct_positions(rel_positions, posi_datas)\n        # Interpolate for timestamps in the testing set:\n        t = step_positions[:,0]\n        x = step_positions[:,1]\n        y = step_positions[:,2]\n        fx = interp1d(t, x, kind='linear', fill_value=(x[0],x[-1]), bounds_error=False) #fill_value=\"extrapolate\"\n        fy = interp1d(t, y, kind='linear', fill_value=(y[0],y[-1]), bounds_error=False)\n        # Output result:\n        t0 = np.array(samples.loc[(building,path_id),'timestamp'],dtype=np.float64)\n        sample_submission.loc[(sample_submission.building==building)&(sample_submission.path_id==path_id),'x'] = fx(t0)\n        sample_submission.loc[(sample_submission.building==building)&(sample_submission.path_id==path_id),'y'] = fy(t0)\n        sample_submission.loc[(sample_submission.building==building)&(sample_submission.path_id==path_id),'floor'] = floors.loc[building+'_'+path_id,'floor']\n#         break\n#     break\n\nsample_submission[['site_path_timestamp','floor','x','y']].to_csv('submission.csv',index=False)","1bae7713":"Procedure to produce floor results through KMeans+GBM @ https:\/\/www.kaggle.com\/oxzplvifi\/indoor-kmeans-gbm-floor-prediction","ae93fe04":"Post processing is same as [Indoor GBM+postprocessing XY prediction](https:\/\/www.kaggle.com\/oxzplvifi\/indoor-gbm-postprocessing-xy-prediction) by [@Oscar Villarreal Escamilla](https:\/\/www.kaggle.com\/oxzplvifi)<br>","92068336":"group by path_id and sort by t1_wifi","3af0b8b7":"train","1ca03fad":"### Dataset\nI used inverse trajectory and random trajectory as augmentation according to https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7374298\/","6a021074":"### model\nmany to many","f4b865f9":"### building weight\ntest_data frequency \/ train_data frequency ","7aab46a0":"## Overview\n\nThanks for reading. I would like to share time series RNN approach.<br>\nThis notebook is almost implementaion of [Recurrent Neural Networks For Accurate RSSI Indoor Localization](https:\/\/arxiv.org\/pdf\/1903.11703.pdf).\n![image.png](attachment:image.png)\n### dataset\nMy [time sequence unified  wifi dataset](https:\/\/www.kaggle.com\/ebinan92\/time-sequence-unified-wifi\/metadata) is made from two exellent notebooks.\n* [LSTM by Keras with Unified Wi-Fi Feats](https:\/\/www.kaggle.com\/kokitanisaka\/lstm-by-keras-with-unified-wi-fi-feats) by [@Kouki](https:\/\/www.kaggle.com\/kokitanisaka)<br>\n* [Indoor GBM+postprocessing XY prediction](https:\/\/www.kaggle.com\/oxzplvifi\/indoor-gbm-postprocessing-xy-prediction) by [@Oscar Villarreal Escamilla](https:\/\/www.kaggle.com\/oxzplvifi)<br>\n\nThe main difference between my dataset and [@kouki's dataset](https:\/\/www.kaggle.com\/kokitanisaka\/indoorunifiedwifids) is t1_wifi column.\nCan be treated as time series data by this column. I'll upload dataset notebook if really needed.\n\n### model\n* One hot encoding for bssi features of all test's building is intractable. So I used entity embedding approch followed by @kouki.\n* Many models are possible, but here for simplicity. I used many to many rnn model.\n","67acbf77":"preparation","e653d9c1":"Long sequence is not learnable. make sure each paths length is shorter than memory_length. ","e264241e":"Thank you for reading! \nI'm new to kaggle and RNN, and this is my first published notebook.\nSo please let me know if you notice any mistakes or have suggestions.","6345c8d7":"### normalization","03200120":"The main difference between my dataset and [@kouki's dataset](https:\/\/www.kaggle.com\/kokitanisaka\/indoorunifiedwifids) is t1_wifi column.\nWe can treat as time series data by this column.\nI haven't deleted bssids by the frequency of bssids. ","10fd9110":"utils"}}