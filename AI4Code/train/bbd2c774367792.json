{"cell_type":{"a7407882":"code","9e405c47":"code","abdf434e":"code","b40a9bbd":"code","f36449b6":"code","7893a772":"code","f40961c2":"code","c1abcb58":"code","1a86808b":"code","071ae2ae":"code","7006e6af":"code","6acbc6f7":"code","83d613fd":"code","87f63703":"code","ec9e8df4":"code","00764033":"code","81d4dcf5":"code","6af6ccb2":"code","13b964e5":"code","2ae3b88c":"code","76d54986":"code","9ac1c2a0":"code","e9aec67a":"code","d70b6fc9":"code","de7b801c":"code","630eee8a":"code","41b9da50":"code","14d568ca":"code","4f6fefd5":"code","8dfaf5a7":"code","e6518a98":"code","60e5ff1e":"code","8445784b":"code","02deecaa":"code","97bc9bca":"code","fbf079ce":"code","18387136":"code","60f4a324":"code","b0aae155":"code","a3a39835":"code","3740199e":"code","9b91e812":"code","805f4b67":"code","cb41c3a1":"code","8414b26a":"code","a96fe7f4":"code","0a483abb":"code","553a3095":"code","741b6a52":"code","ea6773fe":"code","b348f61f":"code","d21c1482":"code","dc15075c":"code","f08992b1":"code","f071f2d1":"code","186b7ae9":"code","0f6769cc":"code","8ce43c4a":"code","ea5b15b4":"code","fe356904":"code","1c02de09":"code","cf106a4d":"code","f9a20a7b":"code","c10cdcde":"code","5ad4a1f4":"code","11145456":"code","2f647dcb":"code","64cbfdf7":"code","38ad78bd":"code","5bcba66d":"code","285a3f10":"code","647def88":"code","3e6601b1":"code","776070df":"code","f8c94be3":"code","969adc93":"code","9ebd6dbf":"code","3dc641c2":"code","adedd384":"code","3a0239eb":"code","38df4ba8":"code","0d2bade3":"code","4bbd394f":"code","7f826bda":"markdown","6aa41ffe":"markdown","1351240b":"markdown","14911f1e":"markdown","fcf6a5a1":"markdown","3fa17053":"markdown","e280e3c6":"markdown","76f302ab":"markdown","77877fae":"markdown","00c1dc39":"markdown","7e0980b4":"markdown","3cce8717":"markdown","0832e7bf":"markdown","4dc224f2":"markdown","b9915b53":"markdown","3d15f334":"markdown","0751163f":"markdown","5624e693":"markdown","04f43895":"markdown","44c379d7":"markdown","aa4109ea":"markdown"},"source":{"a7407882":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\nfrom matplotlib.patches import ConnectionPatch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime\nimport warnings\n\n\nfrom scipy.stats import expon, reciprocal\nfrom scipy.stats import randint\nfrom scipy import stats\n\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import SpectralClustering #kernel\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.cluster import AffinityPropagation\n\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.preprocessing import StandardScaler # mean0 std1\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.exceptions import ConvergenceWarning\n\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\n%matplotlib inline\n\nfrom  warnings import simplefilter\nsimplefilter(\"ignore\", category=UserWarning)","9e405c47":"main_test = pd.read_csv('..\/input\/titanic\/test.csv')\nmain_test.info()","abdf434e":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndata.info()","b40a9bbd":"cluster_data_c = data.copy()\ncluster_main_test = main_test.copy()","f36449b6":"cluster_data = pd.concat([cluster_data_c, cluster_main_test], axis=0)\ncluster_data.describe().T.style.bar(subset=['mean'], color='#606ff2').background_gradient(subset=['std'], cmap='mako_r').background_gradient(subset=['50%'], cmap='mako_r')","7893a772":"eda_data = data.reset_index().copy()","f40961c2":"with plt.style.context('fivethirtyeight'): # background color set rcPram\n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n    eda_data['Survived'].value_counts().plot.pie(explode=[0, 0.18], autopct='%1.1f%%',\n                                                 shadow=True, colors=['#682F2F', '#F3AB60'], startangle=70, ax=ax[0])\n    \n    age_bin = pd.qcut(eda_data['Age'], 10)\n    age_counts = sns.barplot(x=age_bin.sort_index().value_counts().index, y=age_bin.value_counts().values,\n                             linewidth=0.5, ec='black', zorder=3, palette='rocket', ax=ax[1])\n    for i in age_counts.patches:\n        values = f'{i.get_height():,.0f} | {i.get_height() \/ age_bin.shape[0]:,.1%}'\n        x = i.get_x() + i.get_width() \/ 2\n        y = i.get_y() + i.get_height() + 5\n        age_counts.text(x, y, values, ha='center', va='center', fontsize=10, \n                        bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n        ax[1].set_xlabel('ALL Age Bins')\n        ax[1].set_ylabel('Counts')","c1abcb58":"with plt.style.context('fivethirtyeight'): # background color set\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    fig.subplots_adjust(wspace=0)\n    eda_data['Survived'].value_counts().plot.pie(explode=[0, 0.2], autopct='%1.1f%%',\n                                                 shadow=True, colors=['#faf0e6', '#F3AB60'], startangle=70, ax=ax1)\n    \n    colors = ['#682f2f', '#774343', '#865858', '#956D6D', '#A48282',\n              '#B39797', '#C2ABAB', '#D1C0C0', '#E0D5D5', '#EFEAEA']\n    bar_per = eda_data['Survived'].groupby(age_bin).count()[::-1]\n    bottom = 0\n    for i in range(len(bar_per.values)):\n        height = bar_per.values[i] \/ bar_per.values.sum()\n        ax2.bar(-0.2, height=height, width=.2 ,bottom=bottom, color=colors[i], edgecolor='black')\n        y = bottom + ax2.patches[i].get_height() \/ 2.5\n        bottom += height\n        values = f'{ax2.patches[i].get_height(): ,.0%} | {bar_per.values[i]: ,.0f}' # \"%d%%\" % (ax2.patches[i].get_height() *100)\n        ax2.text(-0.2, y, values, ha='center',\n                 bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n\n    ax2.set_title('Surviver Age Per')\n    # reversed legend\n    ax2.legend(('0-13', '14-18', '19-21', '22-24', '25-27','28-30', '31-35', '36-40', '41-49', '50-80'), \n               title='38.4% -> 100%', loc='center', prop={'size':15})\n    ax2.axis('off')\n    ax2.set_xlim(-2.0 * .2, 2.0 * .2)\n\n    theta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\n    center, r = ax1.patches[0].center, ax1.patches[0].r\n    \n    x = r * np.cos(np.pi \/ 173 * theta2) + center[0]\n    y = r * np.sin(np.pi \/ 180 * theta2) + center[1]\n    con = ConnectionPatch(xyA=(-.3, 0.01), xyB=(x, y), coordsA=ax2.transData, coordsB=ax1.transData, shrinkA=1, arrowstyle='<-')\n    con.set_color('#e9967a')#e9967a\n    con.set_linewidth(2)\n    ax2.add_artist(con)\n\n    x = r * np.cos(np.pi \/ 216 * theta1) + center[0]\n    y = r * np.sin(np.pi \/ 180 * theta1) + center[1]\n    con = ConnectionPatch(xyA=(-.3, 0.998), xyB=(x, y), coordsA=ax2.transData, coordsB=ax1.transData, shrinkA=1, arrowstyle='<-')\n    con.set_color('#e9967a')#e9967a\n    ax2.add_artist(con)\n    con.set_linewidth(2)","1a86808b":"def ms_pair_plot(values, hue='Survived'):\n    with plt.style.context('fivethirtyeight'):\n        fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n        for i in range(len(values)):\n            #fig, ax = plt.subplots(1, 2, figsize=(18, 6)) when plot_buplication is performed, inclusion \n            value = values[i]\n            sns.histplot(x=value, hue=hue, data=eda_data, kde=True, palette='rocket', ax=ax[i])\n            ax[i].axvline(x=eda_data[value].mean(), color='g', linestyle='--', linewidth=3)\n            ax[i].axvline(x=eda_data[value].std(), color='c', linestyle=':', linewidth=3)\n            ax[i].text(eda_data[value].mean(), eda_data[value].mean(), \"<--Mean\", horizontalalignment='left', size='small', color='black', weight='semibold')\n            ax[i].text(eda_data[value].std(), eda_data[value].std(), \"Std-->\", horizontalalignment='right', size='small', color='black', weight='semibold')\n            sns.despine()","071ae2ae":"values = ['Age', 'Fare']\nms_pair_plot(values)","7006e6af":"values = ['SibSp', 'Parch']\nms_pair_plot(values)","6acbc6f7":"with plt.style.context('fivethirtyeight'):\n    fig = plt.figure(figsize=(18, 6))\n    sns.violinplot(x=\"Sex\", y=\"Survived\", data=eda_data, palette=['#682F2F', '#F3AB60'])\n    #plt.ylim(-50, 200)","83d613fd":"survived_count = pd.crosstab(eda_data['Sex'], eda_data['Survived'])\nsurvived_pct = survived_count.div(survived_count.sum(1), axis=0)\nwith plt.style.context('fivethirtyeight'):\n    survived_pct.plot.barh(stacked=True, figsize=(19, 6), alpha=0.9, grid=False, color=['#682F2F', '#F3AB60'])","87f63703":"survived_count = pd.crosstab(eda_data['SibSp'], eda_data['Survived'])\nsurvived_pct = survived_count.div(survived_count.sum(1), axis=0)\nwith plt.style.context('fivethirtyeight'):\n    survived_pct.plot.barh(stacked=True, figsize=(19, 6), alpha=0.9, grid=False, color=['#682F2F', '#F3AB60'])","ec9e8df4":"survived_count = pd.crosstab(eda_data['Parch'], eda_data['Survived'])\nsurvived_pct = survived_count.div(survived_count.sum(1), axis=0)\nwith plt.style.context('fivethirtyeight'):\n    survived_pct.plot.barh(stacked=True, figsize=(19, 6), alpha=0.9, grid=False, color=['#682F2F', '#F3AB60'])","00764033":"survived_count = pd.crosstab(eda_data['Fare'], eda_data['Survived'])\nsurvived_pct = survived_count.div(survived_count.sum(1), axis=0)\nwith plt.style.context('fivethirtyeight'):\n    survived_pct.plot.bar(stacked=True, alpha=0.9, figsize=(19, 6),\n                          grid=False, use_index=None, logy=False,\n                          rot=60, yticks=[], xticks=range(0, 251, 50),\n                          title='Surviver Fare Pattern', color=['#682F2F', '#F3AB60'])","81d4dcf5":"def trimming_ax(ax, N):\n    f_axs = ax.flat\n    for ax in f_axs[N:]:\n        ax.remove()\n    return f_axs[:N]\n\ndef cluster_bar_plot(data, product_list, cols=3, figsize=(19, 6)):\n    product_list.append('')\n    length = len(product_list)\n    product_list.remove('')\n    if length % 2 == 0:\n        rows = length \/\/ cols\n    else:\n        rows = length \/\/ cols + 1\n    with plt.style.context('fivethirtyeight'):\n        \n        ax = plt.figure(figsize=figsize, constrained_layout=True).subplots(rows, cols)\n        ax = trimming_ax(ax, length)\n        for i, product in enumerate(product_list):\n            cluster = data.query(\"Survived == {}\".format(i))\n            \n            sns.barplot(x=\"Survived\", y=product, data=data, palette='rocket', ax=ax[i])\n            ax[i].legend(labels=['{}'.format(product)], title='P', loc=2, bbox_to_anchor=(1,1))\n        \n            sns.boxenplot(x=\"Survived\", y=\"Fare\", data=data, palette='rocket', ax=ax[-1])\n            ax[-1].legend(labels=['Survived'], title='Survived_Number', loc=2, bbox_to_anchor=(1,1))","6af6ccb2":"product_List = ['Sex','Survived', 'Age', 'Fare', 'SibSp','Parch']\n\n\ncluster_bar_plot(eda_data, product_List, figsize=(19, 7))","13b964e5":"def Survived_hist_plot(data, columns, cols=4, figsize=(10, 5)):\n    #cols = cols\n    rows = len(np.unique(data['Survived'])) \/\/ cols + 1\n    bins = np.round(np.log(len(data)) + 1).astype(int) # Sturgess Formula : k=log2N+1\n    ax = plt.figure(figsize=figsize, constrained_layout=True).subplots(rows, cols)\n    ax = trimming_ax(ax, len(np.unique(data['Survived'])))\n    with plt.style.context('fivethirtyeight'):\n        for i in np.unique(data['Survived']):\n            cluster = data.query(\"Survived == {}\".format(i))\n            # replace plot\n            sns.histplot(x=columns, data=cluster.reset_index(), bins=bins, ax=ax[i])\n            #sns.countplot(x='Age', data=cluster.reset_index(), ax=ax[i])\n            ax[i].legend(labels=['{}'.format(i)], title='Survived', loc=2, bbox_to_anchor=(1,1))","2ae3b88c":"Survived_hist_plot(eda_data, columns='Age', figsize=(19, 7))","76d54986":"Survived_hist_plot(eda_data, columns='Sex', figsize=(19, 7))","9ac1c2a0":"def density_plot(data):\n    density_per_col = [stats.gaussian_kde(col) for col in data.values.T]\n    x = np.linspace(np.min(data.values), np.max(data.values), 100)\n    \n    with plt.style.context('fivethirtyeight'):\n        fig, ax = plt.subplots(figsize=(18, 6))\n        for density in density_per_col:\n            ax.plot(x, density(x))\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Density')\n        fig.legend(data, loc='right')","e9aec67a":"counts = data.copy()\ncounts.reset_index(inplace=True)\ncounts.drop(['Name', 'Cabin', 'Ticket', 'PassengerId', 'Sex', 'Embarked'], axis=1, inplace=True)\ncounts.fillna(0, inplace=True)\ncounts_nd = counts.values\nlog_counts = np.log(counts_nd + 1)\nlog_counts_pd = pd.DataFrame(log_counts, columns=counts.columns)","d70b6fc9":"density_plot(log_counts_pd)","de7b801c":"def quantile_norm(X):\n    quantile = np.mean(np.sort(X, axis=0), axis=1) # quantile calculus\n    rank = np.apply_along_axis(stats.rankdata, 0, X)\n    rank_indices = rank.astype(np.int) - 1\n    X_index = quantile[rank_indices]\n    return X_index\n\ndef quantile_log(X):\n    X_log = np.log(X + 1)\n    Xi_log = quantile_norm(X_log)\n    Xi_log_pd = pd.DataFrame(Xi_log, columns=X.columns)\n    return Xi_log_pd","630eee8a":"count_normalized = quantile_log(counts)","41b9da50":"density_plot(count_normalized)","14d568ca":"def most_variable_rows(data, *arg):\n    rowvar = np.var(data, axis=1)\n    sort_indices = np.argsort(rowvar)\n    variable_data = data[sort_indices, :]\n    return variable_data\n\nfrom scipy.cluster.hierarchy import linkage\n\ndef bicluster(data, linkage_method='average', distance_metric='correlation'):\n    y_rows = linkage(data, method=linkage_method, metric=distance_metric)\n    y_cols = linkage(data.T, method=linkage_method, metric=distance_metric)\n    return y_rows, y_cols","4f6fefd5":"from scipy.cluster.hierarchy import dendrogram, leaves_list\n\ndef clear_spines(axes):\n    for loc in ['left', 'right', 'top', 'bottom']:\n        axes.spines[loc].set_visible(False)\n    axes.set_xticks([])\n    axes.set_yticks([])\n\ndef plot_bicluster(data, row_linkage, col_linkage, row_nclusters=10, col_nclusters=5):\n    fig = plt.figure(figsize=(10, 10))\n    \n    ax1 = fig.add_axes([0.09, 0.1, 0.2, 0.6])\n    \n    threshold_r = (row_linkage[-row_nclusters, 2] +\n                   row_linkage[-row_nclusters+1, 2]) \/ 2\n    with plt.rc_context({'lines.linewidth': 0.75}):\n        dendrogram(row_linkage, orientation='left',\n                   color_threshold=threshold_r, ax=ax1)\n    clear_spines(ax1)\n    \n    ax2 = fig.add_axes([0.3, 0.71, 0.6, 0.2]) \n    threshold_c = (col_linkage[-col_nclusters, 2] +\n                   col_linkage[-col_nclusters+1, 2]) \/ 2\n    with plt.rc_context({'lines.linewidth': 0.75}):\n        dendrogram(col_linkage,\n                   color_threshold=threshold_c, ax=ax2)\n    clear_spines(ax2)\n    \n    ax = fig.add_axes([0.3, 0.1, 0.6, 0.6])\n    \n    idx_rows = leaves_list(row_linkage)\n    data = data[idx_rows, :]\n    idx_cols = leaves_list(col_linkage)\n    data = data[:, idx_cols]\n    \n    im = ax.imshow(data, aspect='auto', origin='lower', cmap='YlGnBu_r')\n    clear_spines(ax)\n    \n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Index', labelpad=150)\n    axcolor = fig.add_axes([0.91, 0.1, 0.02, 0.6])\n    plt.colorbar(im, cax=axcolor)","8dfaf5a7":"count_log = np.log(counts_nd + 1)\ncount_var = most_variable_rows(count_log)\n\nyr, yc = bicluster(count_var, linkage_method='ward', distance_metric='euclidean')\n\nplot_bicluster(count_var, yr, yc)","e6518a98":"data.isnull().sum(), main_test.reset_index().isnull().sum()","60e5ff1e":"cluster_data['FamilySize'] = cluster_data['SibSp'] + cluster_data['Parch'] + 1\ncluster_data['IsAlone'] = np.where(cluster_data['FamilySize'] <= 1, 1, 0)\ncluster_data.loc[cluster_data['FamilySize'] > 0, 'travelled_alone'] = 'No'\ncluster_data.loc[cluster_data['FamilySize'] == 0, 'travelled_alone'] = 'Yes'\n\ncluster_data['Honorific'] = cluster_data.Name.str.split(',', -1,\n                                                        expand=True)[1].str.split('.', 1,\n                                                                                  expand=True)[0].str.strip().replace({'Mlle': 'Miss','Ms': 'Miss', 'Lady': 'Noble', 'Don': 'Noble',\n                                                                                                                       'Jonkheer': 'Noble', 'the Countess': 'Noble', 'Sir': 'Noble',\n                                                                                                                       'Countess': 'Noble',\n                                                                                                                       'Mme': 'Mrs', 'Capt': 'Soldier', 'Major': 'Soldier', 'Col': 'Soldier', \n                                                                                                                       'Rev': 'Mr', 'Dr': 'Mr', 'Dona': 'Noble', 'Master': 'Soldier'})\ncluster_data['FamilyName'] = cluster_data.Name.str.split(',', -1, expand=True)[0]\ncluster_data['FullName'] = cluster_data.Name.str.split(',', -1, expand=True)[1].str.split('.', 1, expand=True)[1].str.strip()\ncluster_data['FirstName'] = cluster_data.FullName.str.split(' ', 1, expand=True)[0].str.strip('(').str.strip(')')\ncluster_data['NameLength'] = cluster_data.FullName.apply(lambda x: len(x))\ncluster_data['Surname'] = cluster_data.Name.str.extract(r'([A-Za-z]+),', expand=False)\ncluster_data['TicketPrefix'] = cluster_data.Ticket.str.extract(r'(.*\\d)', expand=False)\ncluster_data['SurnameTicket'] = cluster_data['Surname'] + cluster_data['TicketPrefix']\ncluster_data['IsFamily'] = cluster_data.SurnameTicket.duplicated(keep=False).astype(int)\ncluster_data['Child'] = cluster_data.Age.map(lambda x: 1 if x <=16 else 0)\n\n\nbins = [0, 2, 12, 17, 60, np.inf]\nlabels = ['baby', 'child', 'teenager', 'adult', 'elderly']\nage_groups = pd.cut(cluster_data.Age, bins, labels=labels)\ncluster_data['AgeGroup'] = age_groups\n\n#Cont_Features = ['Age', 'Fare']\n#num_bins = 5\n#for feature in Cont_Features:\n#    bin_feature = feature + 'Bin'\n#    cluster_data[bin_feature] = pd.qcut(cluster_data[feature], num_bins)\n#    label = LabelEncoder()\n#    cluster_data[bin_feature] = label.fit_transform(cluster_data[bin_feature])\n\n\ncluster_data['Age*Class'] = cluster_data.Age * cluster_data.Pclass\n\n\nFamilyWithChild = cluster_data[(cluster_data.IsFamily == 1) & (cluster_data.Child == 1)]['SurnameTicket'].unique()\ncluster_data['FamilyId'] = 0\nfor ind, identifier in enumerate(FamilyWithChild):\n    cluster_data.loc[cluster_data.SurnameTicket == identifier, ['FamilyId']] = ind + 1\n\ncluster_data['FamilySurvival'] = 1\nSurvived_by_FamilyId = cluster_data.groupby('FamilyId').Survived.sum()\nfor i in range(1, len(FamilyWithChild)+1):\n    if Survived_by_FamilyId[i] >= 1:\n        cluster_data.loc[cluster_data.FamilyId == i, ['FamilySurvival']] = 2\n    elif Survived_by_FamilyId[i] == 0:\n        cluster_data.loc[cluster_data.FamilyId == i, ['FamilySurvival']] = 0\n\n\ncluster_data.drop('Name', axis=1, inplace=True)\ncabin_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 1, 'M': 8}\ncluster_data['Deck'] = cluster_data['Cabin'].str[0].fillna('M').replace(cabin_map)\ncluster_data['Ticket'] = cluster_data['Ticket'].str.split(' ').str.get(-1).str.get(0).str.replace('L', '2').astype(np.int64)\n\n\ncat_features = cluster_data.columns[~cluster_data.columns.isin(['Survived', 'Fare', 'FamilySize', 'Pclass',\n                                                                'PassengerId', 'Age', 'Ticket', 'Parch', 'Cabin',\n                                                                'Embarked'])]\ncluster_data = pd.get_dummies(cluster_data, columns=cat_features)\ndummie_data = cluster_data.reset_index()\n\ndummie = pd.get_dummies(dummie_data['Cabin'])\ndata_dummie = dummie_data.drop(['Cabin'], axis=1).join(pd.DataFrame(dummie.sum(axis=1), columns=['Cabin']))\ndata_rep = data_dummie.fillna({'Age': data_dummie['Age'].mean(), 'Embarked': data_dummie['Embarked'].fillna(method='ffill'), 'Fare': data_dummie['Fare'].mean()})\ndata_rep['Fare'] = data_rep['Fare'].round().astype(np.int64)\ndata_rep['Age'] = data_rep['Age'].astype(np.int64)\ndata_rep['Fare'].where((np.abs(data_rep['Fare']) < data_rep['Fare'].quantile(0.997, )), 500, inplace=True) # 300\n\ndata_rep_cluster = data_rep.drop('Survived', axis=1)\ndata_rep.drop(['index', 'PassengerId'], axis=1,  inplace=True)","8445784b":"train = data_rep.iloc[cluster_data_c.index]\ntest = data_rep.iloc[cluster_main_test.index+cluster_data_c.shape[0]].drop(columns=['Survived'])\nX = train.drop(columns='Survived', axis=1) # .to_numpy()\ny = train['Survived'] # .to_numpy()\n\ndata_labels = y.to_numpy()","02deecaa":"data_num = X.drop([\"Embarked\"], axis=1)\nnumber_attribs = list(data_num)","97bc9bca":"categorie_attribs = [\"Embarked\"] \n\nnumber_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n])\n\ncluster_test_pipeline = ColumnTransformer([\n    (\"number\", number_pipeline, number_attribs),\n    (\"categorie\", OneHotEncoder(), categorie_attribs),\n])","fbf079ce":"cluster_data_prepared = cluster_test_pipeline.fit_transform(X)\ncluster_main_test = cluster_test_pipeline.transform(test)","18387136":"X_train, X_val, y_train, y_val = train_test_split(cluster_data_prepared, data_labels, test_size=0.25, stratify=data_labels)","60f4a324":"#gb_model = GradientBoostingClassifier()\n#gb_param_grid = {'learning_rate':[0.1, 0.01, 0.001], 'max_depth':[5, 10], 'n_estimators':[10, 100, 200, 300]}\n#gb_model.get_params().keys()","b0aae155":"#gb_s_model = GridSearchCV(gb_model, gb_param_grid, cv=10, scoring='accuracy')\n#gb_s_model.fit(X_train, y_train)","a3a39835":"#gb_s_model.best_params_\n#{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100} samples=10000\ngb_model = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=100) # warm_start=True\ngb_model.fit(X_train, y_train)","3740199e":"#gb = gb_s_model.best_estimator_\ny_pred_gb = gb_model.predict(X_val)","9b91e812":"print('Precision : {} \/ Recall : {}'.format(precision_score(y_val, y_pred_gb), recall_score(y_val, y_pred_gb)))\nprint(classification_report(y_val, y_pred_gb))\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val, y_pred_gb), display_labels=gb_model.classes_)\ndisp.plot(cmap='Blues');","805f4b67":"main_pred_gb = gb_model.predict(cluster_main_test)","cb41c3a1":"#pd.DataFrame({'PassengerId': np.arange(892, 1310, 1), 'Survived': main_pred_gb.astype(int)}).to_csv('submission_test.csv', index=0) # 0.80143","8414b26a":"kfold_1 =StratifiedKFold(n_splits=5,shuffle=True) #random_state=42","a96fe7f4":"y_score = cross_val_predict(gb_model, X_train, y_train, cv=kfold_1, method='decision_function')","0a483abb":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\") # [:-1]\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\") # [:-1]\n    plt.legend()","553a3095":"precision, recall, thresholds = precision_recall_curve(y_train, y_score)","741b6a52":"plot_precision_recall_vs_threshold(precision, recall, thresholds)","ea6773fe":"threshold_90_precision = thresholds[np.argmax(precision >= 0.80)] # 0.85 # 0.80\nthreshold_90_precision","b348f61f":"y_train_pred_90 = (y_score >= threshold_90_precision)","d21c1482":"y_train_pred_90.shape # y_train","dc15075c":"gb_model.fit(X_train, y_train_pred_90)","f08992b1":"y_pred_90_gb = gb_model.predict(X_val)","f071f2d1":"print('Precision : {} \/ Recall : {}'.format(precision_score(y_val, y_pred_90_gb), recall_score(y_val, y_pred_90_gb)))\nprint(classification_report(y_val, y_pred_90_gb))\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val, y_pred_90_gb), display_labels=gb_model.classes_)\ndisp.plot(cmap='Blues');","186b7ae9":"main_pred_90_gb = gb_model.predict(cluster_main_test)","0f6769cc":"#pd.DataFrame({'PassengerId': np.arange(892, 1310, 1), 'Survived': main_pred_90_gb.astype(int)}).to_csv('submission_best.csv', index=0)","8ce43c4a":"errors = [precision_score(y_val, y_pred)\n          for y_pred in gb_model.staged_predict(X_val)]","ea5b15b4":"bst_n_estimators = np.argmax(errors) + 1","fe356904":"bst_n_estimators","1c02de09":"gb_best = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=bst_n_estimators)","cf106a4d":"gb_best.fit(X_train, y_train_pred_90)","f9a20a7b":"y_pred_90_gb_best = gb_model.predict(X_val)","c10cdcde":"print('Precision : {} \/ Recall : {}'.format(precision_score(y_val, y_pred_90_gb_best), recall_score(y_val, y_pred_90_gb_best)))\nprint(classification_report(y_val, y_pred_90_gb_best))\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val, y_pred_90_gb_best), display_labels=gb_best.classes_)\ndisp.plot(cmap='Blues');","5ad4a1f4":"main_pred_90_gb_best = gb_best.predict(cluster_main_test)","11145456":"#pd.DataFrame({'PassengerId': np.arange(892, 1310, 1),\n#              'Survived': pd.DataFrame(main_pred_90_gb_best).replace({False: 0, True :1}).to_numpy().T[0]}).to_csv('submission_01.csv', index=0)","2f647dcb":"param_rand = {'learning_rate': reciprocal(0.001, 1), \n              'max_depth': randint(low=3, high=10)}","64cbfdf7":"gb_best_rd = GradientBoostingClassifier(subsample=0.25, n_estimators=bst_n_estimators, warm_start=True)","38ad78bd":"rnd_search = RandomizedSearchCV(gb_best_rd, param_rand, n_iter=10, cv=kfold_1, scoring='f1')","5bcba66d":"rnd_search.fit(X_train, y_train_pred_90)","285a3f10":"gb_best_p = GradientBoostingClassifier(learning_rate=0.12727932524008223, max_depth=3, subsample=0.25, n_estimators=bst_n_estimators, warm_start=True)","647def88":"gb_best_p.fit(X_train, y_train_pred_90)","3e6601b1":"y_pred_90_gb_best_rnd = gb_best_p.predict(X_val)","776070df":"print('Precision : {} \/ Recall : {}'.format(precision_score(y_val, y_pred_90_gb_best_rnd), recall_score(y_val, y_pred_90_gb_best_rnd)))\nprint(classification_report(y_val, y_pred_90_gb_best_rnd))\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val, y_pred_90_gb_best_rnd), display_labels=gb_best_p.classes_)\ndisp.plot(cmap='Blues');","f8c94be3":"main_pred_90_gb_best_rnd = gb_best.predict(cluster_main_test)","969adc93":"#pd.DataFrame({'PassengerId': np.arange(892, 1310, 1),'Survived': pd.DataFrame(main_pred_90_gb_best_rnd).replace({False: 0, True :1}).to_numpy().T[0]}).to_csv('submission_02.csv', index=0)","9ebd6dbf":"precision_score(y_train, y_train_pred_90)","3dc641c2":"recall_score(y_train, y_train_pred_90)","adedd384":"fpr, tpr, thresholds_r = roc_curve(y_train_pred_90, y_score)","3a0239eb":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')","38df4ba8":"plot_roc_curve(fpr, tpr)","0d2bade3":"roc_auc_score(y_train_pred_90, y_score)","4bbd394f":"y_train_pred = cross_val_predict(gb_model, X_train, y_train_pred_90, cv=3)","7f826bda":"- One way to regularize a model\n- In stochastic gradient descent and mini-batch gradient descent methods, the curve is not as smooth as in batch gradient descent, and there is a risk of being caught in a local minimum. Therefore, if the verification error keeps rising above the minimum for a while, we can construct a syntax to roll back to the last minimum.\n- Early termination can also be used in boosting algorithms to find the optimal number of decision trees.","6aa41ffe":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:300%;text-align:center;border-radius:10px 10px;\">Pre-Processing & Feature Enginiering<\/p>","1351240b":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">DATA Concatenation<\/p>","14911f1e":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:200%;text-align:center;border-radius:10px 10px;\">early stopping<\/p>","fcf6a5a1":"### PR_corve","3fa17053":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:200%;text-align:center;border-radius:10px 10px;\">Other Models<\/p>\n\n- Publish to another notebook\n- Thank you for Reading. \n- Please give me an UPVOTE if you can. Your UPVOTE will be a great encouragement to me","e280e3c6":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:300%;text-align:center;border-radius:10px 10px;\">Other Data Plot<\/p>\n\n#### \u30fbData Plot & Normalization\n#### \u30fbDendrogram & HeatMap","76f302ab":"## Train & Test\n\n>Divided into Train set used for Model training and data to make predictions.","77877fae":"# Pre-Processing & Feature Enginiering\n1. **Data cleaning**\n    - Fix or remove outliers\n    - Fill missing values (0, mean, median)\n3. **Feature engineering**\n    - Discretization of continuous value features\n    - Decomposition of features (categorical)\n    - Add transformations that are expected to be effective on features (log(x), sqrt(x), x**2)\n    - Aggregate the features to create new features\n4. **Feature scaling**\n    - Standardization and normalization of features","00c1dc39":">Merge the pre-divided data sets. This operation makes it possible to make the same changes during data conversion.","7e0980b4":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">DATA Observation<\/p>","3cce8717":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:300%;text-align:center;border-radius:10px 10px;\">EDA<\/p>","0832e7bf":"1. Make a copy of the data for observation, or reduce it to the required size if the data is large.\n2. examine the attributes of the data and their characteristics\n    - Missing values\n    - Categorical\/textual\/integer\/floating point numbers\n    - Noise presence and type (stochastic, outlier, rounding error)\n    - Type of distribution (Gaussian, Uniform, Logarithmic)\n3. visualize the data\n4. examine correlation of features\n5. Identify the transformation to be applied","4dc224f2":"## RandomizedSearchCV","b9915b53":"## GridSearchCV","3d15f334":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:200%;text-align:center;border-radius:10px 10px;\">Data Splitting<\/p>","0751163f":"<p style= \"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:300%;text-align:center;border-radius:10px 10px;border-style:solid;border-width:3px;border-color:#000000;\"><b>Titanic Data Science<\/b><\/p>\n\n## Please give me an UPVOTE if you can. Your UPVOTE will be a great encouragement to me\n\n- The score is the result of combining multiple pipelines in the following code. We have removed some of the pipelines in the following code because we think it is not good for learning to put the score answers as they are.","5624e693":"<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:150%;text-align:center;border-radius:10px 10px;\">Data Science Workflow<\/p>\n\n1. Understand the framework and big picture of the problem.\n2. Analyze the data to develop a personal understanding.\n3. Prepare data to make it easier for machine learning algorithms to find patterns in the data.\n4. Try different models and narrow it down to the best few.\n5. Fine-tune the models, use them as an ensemble, and combine them into a solution.\n6. Present the solution.","04f43895":"- A grid search is used to narrow down the rough range, and a random search is used to further understand the detailed values and obtain the hyperparameters of the model.","44c379d7":"-----\n<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:200%;text-align:center;border-radius:10px 10px;\">Best Score Model<\/p>\n\n# **GradientBoosting** ","aa4109ea":"-----\n<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:300%;text-align:center;border-radius:10px 10px;\">Model<\/p>\n\n<p style=\"background-color:#000000;font-family:Georgia;color:#FFFFFF;font-size:200%;text-align:center;border-radius:10px 10px;\">GridSearch RandomizedSearch<\/p>"}}