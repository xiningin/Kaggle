{"cell_type":{"3f3f5e2a":"code","5a62b2da":"code","0f450809":"code","da717f51":"code","05ce69c6":"code","c10c11ef":"code","654bc762":"code","44fb8984":"code","e2a3be01":"code","386a5b88":"code","24182a26":"code","f088cba1":"code","6f882738":"code","9142a5c1":"code","a27f1363":"code","3eee9578":"code","580cf975":"code","2b9d8dcc":"code","f4e88a38":"code","0d99a997":"code","ba70eda3":"code","11fa36d3":"code","c13443f8":"code","995a4b43":"code","849fc4cf":"code","aacb7c0e":"code","213b624c":"code","ed22443f":"code","4b97ed91":"code","a62038e5":"markdown","0f798750":"markdown","f51321ce":"markdown","bae86383":"markdown","70006f1e":"markdown","b2c04248":"markdown","8d3857af":"markdown","f5bfd29e":"markdown","f3f64ca8":"markdown","21c1e408":"markdown","543c49c9":"markdown","7dcbf0a5":"markdown","52d73987":"markdown"},"source":{"3f3f5e2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a62b2da":"#Ignoring the warning that are obtained\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0f450809":"df=pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head(10)","da717f51":"#getting an over-view about all columns present in the dataset\ndf.info()","05ce69c6":"#Total number of data records that have a null value in them\ndf.isnull().sum()","c10c11ef":"count=0\nfor i in df['review']:\n    if count==5:\n        break\n    print(i)\n    print()\n    count+=1","654bc762":"#converting the sentiment column which of type object to integer to perform machine Learning algorithms\ndf['sentiment']=df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)","44fb8984":"#Finding out the composition of both positive and the negative reviews.\ndf['sentiment'].value_counts()","e2a3be01":"import regex as re\nprint(df['review'][1000])\nre.sub('(<[\\w\\s]*\/?>)',\"\",df['review'][1000])","386a5b88":"!pip3 install contractions","24182a26":"import contractions\nfrom tqdm import tqdm\n#tqdm package is used to track the progress of work. It displays the percentage of loop done.","f088cba1":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n#donwloadin the stopwords of english language\nstopwords=stopwords.words('english')\n#Removing stopwords 'no','nor' and 'not'\nprint('not' in stopwords)\nstopwords.remove('no')\nstopwords.remove('nor')\nstopwords.remove('not')\nprint('not' in stopwords)","6f882738":"processed_reviews=[]\nfor i in tqdm(df['review']):\n    #Regular expression that removes all the html tags pressent in the reviews\n    i=re.sub('(<[\\w\\s]*\/?>)',\"\",i)\n    #Expanding all the contractions present in the review to is respective actual form\n    i=contractions.fix(i)\n    #Removing all the special charactesrs from the review text\n    i=re.sub('[^a-zA-Z0-9\\s]+',\"\",i)\n    #Removing all the digits present in the review text\n    i=re.sub('\\d+',\"\",i)\n    #Making all the review text to be of lower case as well as remvoing the stopwords and words of length less than 3\n    processed_reviews.append(\" \".join([j.lower() for j in i.split() if j not in stopwords and len(j)>=3]))","9142a5c1":"#Creating a new datafram using the Processed Reviews\nprocessed_df=pd.DataFrame({'review':processed_reviews,'sentiment':list(df['sentiment'])})","a27f1363":"processed_df.head()","3eee9578":"#Splitting the data into dependent and independent variables i.e, features and the target columns\nX=processed_df['review']\nY=processed_df['sentiment']","580cf975":"#Splitting the data such that 33% will be used for testing and the remaining 67% will be used for training. \nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,stratify=Y,test_size=0.33)\n#when stratify is provided the splitting of data into train and test datasets agree with the composition of actual possitive and negative reviews present in the dataset\n","2b9d8dcc":"def print_shape(a,b):\n    \"\"\"\n    Function that prints the shape of the numpy arrays passed as arguments\n    \"\"\"\n    print(\"Size of Training Samples\")\n    print(\"=\"*30)\n    print(a.shape)\n    print(\"Size of Testing Samples\")\n    print(\"=\"*30)\n    print(b.shape)\nprint_shape(x_train,x_test)","f4e88a38":"from sklearn.feature_extraction.text import CountVectorizer\n#CountVectorizer is used to implement BagOfWords techniqe using which the textual data can be converted to numerical data so that it can be used for computation.\nvectorizer=CountVectorizer()\nx_train_bow=vectorizer.fit_transform(x_train)\nx_test_bow=vectorizer.transform(x_test)\nprint_shape(x_train_bow,x_test_bow)","0d99a997":"#The total number of features that are recognized by Count Vectorizer. In other words this also refers to total number of unique words present in the corpus.\nlen(list(vectorizer.get_feature_names()))","ba70eda3":"#Displaying the first 100 features of BagofWords Vector\ncount=0\nfor i in list(vectorizer.get_feature_names()):\n    if count==100:\n        break\n    print(i,end=\",\")\n    count+=1","11fa36d3":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid_params = { 'n_neighbors' : [40,50,60,70,80,90],\n               'metric' : ['manhattan']}\nknn=KNeighborsClassifier()\nclf = RandomizedSearchCV(knn, grid_params, random_state=0,n_jobs=-1,verbose=1)\nclf.fit(x_train_bow,y_train)","c13443f8":"#The parameters of the best model\nclf.best_params_","995a4b43":"#The Score of the best model\nclf.best_score_","849fc4cf":"#Displaying the complete result of RandomizedSearchCV\nclf.cv_results_","aacb7c0e":"from sklearn.metrics import roc_curve, auc\ntrain_fpr,train_tpr,thresholds=roc_curve(y_train,clf.predict_proba(x_train_bow)[:,1])\ntest_fpr,test_tpr,thresholds=roc_curve(y_test,clf.predict_proba(x_test_bow)[:,1])","213b624c":"import matplotlib.pyplot as plt\nplt.plot(train_fpr,train_tpr,label=\"Training Accuracy=\"+str(round(auc(train_fpr, train_tpr),2)))\nplt.plot(test_fpr,test_tpr,label=\"Testing Accuracy =\"+str(round(auc(test_fpr, test_tpr),2)))\nplt.legend()\nplt.xlabel(\"Thresholds\")\nplt.ylabel(\"ACCURACY\")\nplt.title(\"Training and Testing ROC Curves\")\nplt.show()","ed22443f":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nsns.heatmap(confusion_matrix(y_train,clf.predict(x_train_bow)),annot=True)","4b97ed91":"sns.heatmap(confusion_matrix(y_test,clf.predict(x_test_bow)),annot=True)","a62038e5":"# <center> Hyper Parameter Tuning<\/center>","0f798750":"#### Stopwords: Stopwords are those words that do not provide any useful information to decide in which category a text should be classified. Such words can be removed from the reviews present in the dataset as they do not contribute to the final prediction. Stopwords exist for all languages. we can download stopwords of a specific language from the nltk library. The nltk library identifies the word \"not\" also as a stopword. Removing the not from the review may lead to some inconisten results.\nExample:\n\"I am not happy with the movie\" will become \"I am happy with the movie\" if 'not' is considered as a stopword.","f51321ce":"## Plotting Confustoin Matrix","bae86383":"# <center> Data Preparation<\/center>","70006f1e":"## Using Regex module to remove the html tags that are present in the reviews.","b2c04248":"# <center> Model Evaluation<\/center>","8d3857af":"#### From the above reviews, it is clear that the reviews has been scraped from the site. The reviews contain some html tags which must be cleaned. Stopwords must be removed and the reviews must be cleaned","f5bfd29e":"### Observing the Reviews in the dataset","f3f64ca8":"### contractions is the module that can be used to fix the contractions that are generally used in the english language. Replacing I'll with \"I will \" can be done using this module.","21c1e408":"## Plotting ROC Curve","543c49c9":"# <center> Data Analysis <\/center>","7dcbf0a5":"### Hyper Parameter Tuning for finding the correct value for n_neighbors. Since the data contains large number of features, euclidean distance may not work well and hence I have manhattan as the distance metric. Using RandomizedSearchCV to find out the best value for \"KneighborsClassifier\"","52d73987":"# <center> Data Cleaning <\/center>"}}