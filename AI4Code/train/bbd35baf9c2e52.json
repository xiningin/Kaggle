{"cell_type":{"8f553513":"code","f0a0787d":"code","969348bd":"code","22387ce2":"code","f39ac60b":"code","51ba809f":"code","e5966c5e":"code","ccbe89dd":"code","3a91fdca":"code","f522ea27":"code","1a4bdaa6":"code","8e8375f1":"code","8bba99a3":"code","b20a997a":"code","0e7d5465":"code","67066814":"code","9ab4a100":"markdown","f4c166aa":"markdown","ad5e334e":"markdown","0fe43435":"markdown","f78702f0":"markdown","d4ed9aec":"markdown","8f6288c6":"markdown","0772d05e":"markdown","4803d13e":"markdown","b7cba944":"markdown"},"source":{"8f553513":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0a0787d":"from keras.datasets import mnist","969348bd":"(train_img, train_labels), (test_img, test_labels) = mnist.load_data()","22387ce2":"train_img.shape","f39ac60b":"len(train_labels)","51ba809f":"train_labels","e5966c5e":"digit = train_img[4]\n\nimport matplotlib.pyplot as plt\nplt.imshow(digit, cmap=plt.cm.binary)\nplt.show()","ccbe89dd":"print(test_img.shape)\nprint(len(test_labels))\nprint(test_labels)","3a91fdca":"from keras import models\nfrom keras import layers","f522ea27":"\"\"\"It consists of two Dense layers, which are densly connected neural layers.\nsecond layer is 10-way softmax--> it will return array of 10 probability scores. \"\"\"\n\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape = (28*28,)))\nnetwork.add(layers.Dense(10,activation='softmax'))","1a4bdaa6":"network.compile(optimizer='rmsprop',\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])","8e8375f1":"train_img = train_img.reshape((60000, 28*28))\ntrain_img = train_img.astype('float32')\/255\n\ntest_img = test_img.reshape((10000, 28*28))\ntest_img = test_img.astype('float32')\/255","8bba99a3":"from keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","b20a997a":"network.fit(train_img, train_labels, epochs =5, batch_size =128)","0e7d5465":"test_loss, test_acc = network.evaluate(test_img, test_labels)","67066814":"test_acc","9ab4a100":"# Train the network","f4c166aa":"To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n1. A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction. \n2. An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function. \n3. Metrics to monitor during training and testing. Here we will only care about accuracy\n","ad5e334e":"WOOOHOOOO!!! We got 97% accuracy so far. But it is bit lower than our training accuracy.\nGap between training and testing acccuracy is \"Overfitting\" if trainAcc > testAcc.","0fe43435":"# compilation step","f78702f0":"# Preparing the image data\nScale image to [0,1] interval from [0,255].","d4ed9aec":"# Preparing the labels","8f6288c6":"# Training Data","0772d05e":"# Evaluate the network","4803d13e":"# The Network Architecture","b7cba944":"# Test Data"}}