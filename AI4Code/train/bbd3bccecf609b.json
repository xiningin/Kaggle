{"cell_type":{"a9af60bf":"code","0ade8992":"code","38b5b8ed":"code","3213736c":"code","4ac73128":"code","486c5ea0":"code","c1d643fd":"code","4d25cf10":"code","a3b857c1":"code","8c3e46c7":"code","b28fa40b":"code","9fae4dbf":"code","70743fa7":"code","5f3859b3":"code","4eaa029e":"code","d144d55f":"code","7928e061":"code","57f32096":"code","621d0432":"code","7aa1d9ec":"code","e610fd1f":"code","83ce611e":"code","5864a587":"code","64c08d9c":"code","8a14158e":"code","72974fa7":"code","9a678fe2":"code","947c8172":"code","cc22087c":"code","2a08fafe":"code","ea8bf646":"markdown","71e50e5f":"markdown","b5c297df":"markdown","5130ba2c":"markdown","77630fb9":"markdown","565633ac":"markdown","af72b1de":"markdown","c1e6fb0a":"markdown","2fa49100":"markdown","2ff915dd":"markdown","fcbe589b":"markdown","274525bb":"markdown","005d9e34":"markdown","49827cf6":"markdown","86f00eb3":"markdown","576858ff":"markdown","e86c63c0":"markdown"},"source":{"a9af60bf":"!ls ..\/input\/sarcasm\/","0ade8992":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","38b5b8ed":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","3213736c":"train_df.head()","4ac73128":"train_df.info()","486c5ea0":"train_df.dropna(subset=['comment'], inplace=True)","c1d643fd":"train_df['label'].value_counts()","4d25cf10":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","a3b857c1":"train_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).plot(kind='hist', label='sarcastic', alpha=0.5)\ntrain_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).plot(kind='hist', label='normal', alpha=0.5)\nplt.legend()","8c3e46c7":"from wordcloud import WordCloud, STOPWORDS","b28fa40b":"wordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)","9fae4dbf":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 1, 'comment']))\nplt.imshow(wordcloud);","70743fa7":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df.loc[train_df['label'] == 0, 'comment']))\nplt.imshow(wordcloud);","5f3859b3":"sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='size', ascending=False).head(10)","4eaa029e":"sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)","d144d55f":"sub_df = train_df.groupby('author')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)","7928e061":"# build bigrams, put a limit on maximal number of features and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=1)\n\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","57f32096":"%%time\ntfidf_logit_pipeline.fit(train_texts, y_train)","621d0432":"%%time\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)","7aa1d9ec":"accuracy_score(y_valid, valid_pred)","e610fd1f":"def plot_confusion_matrix(actual, predicted, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(actual, predicted).T\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","83ce611e":"plot_confusion_matrix(y_valid, valid_pred,\n                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))","5864a587":"from eli5.sklearn.explain_weights import explain_linear_classifier_weights\n\nexplain_linear_classifier_weights(clf=tfidf_logit_pipeline.named_steps['logit'],\n                                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","64c08d9c":"X_train, X_valid = train_test_split(train_df, random_state=17)","8a14158e":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion\n\ntf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\ntf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))\n\nFeatureUnioner = FeatureUnion([\n                ('tf_idf_texts', \n                  Pipeline([('extract_field',\n                              FunctionTransformer(lambda x: x['comment'], \n                                                  validate=False)),\n                            ('tfidf', \n                              tf_idf_texts)])),\n                ('tf_idf_subreddits', \n                  Pipeline([('extract_field', \n                              FunctionTransformer(lambda x: x['subreddit'], \n                                                  validate=False)),\n                            ('tfidf', \n                              tf_idf_subreddits)]))])\n\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('FeatureUnioner', FeatureUnioner), \n                                 ('logit', logit)])","72974fa7":"%%time\ntfidf_logit_pipeline.fit(X_train, y_train)","9a678fe2":"%%time\nvalid_pred = tfidf_logit_pipeline.predict(X_valid)","947c8172":"accuracy_score(y_valid, valid_pred)","cc22087c":"plot_confusion_matrix(y_valid, valid_pred,\n                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))","2a08fafe":"FeatureUnioner = tfidf_logit_pipeline.named_steps['FeatureUnioner']\n\ntf_idf_texts = FeatureUnioner.transformer_list[0][1].named_steps['tfidf']\ntf_idf_subreddits = FeatureUnioner.transformer_list[1][1].named_steps['tfidf']\n\nfeature_names = tf_idf_texts.get_feature_names() + tf_idf_subreddits.get_feature_names()\n\nexplain_linear_classifier_weights(clf=tfidf_logit_pipeline.named_steps['logit'],\n                                  feature_names=feature_names)","ea8bf646":"We split data into training and validation parts.","71e50e5f":"Let's analyze whether some subreddits are more \"sarcastic\" on average than others.","b5c297df":"Some comments are missing, so we drop the corresponding rows.","5130ba2c":"Word cloud are nice, but not very useful.","77630fb9":"## Part 3. Explaining the model","565633ac":"We notice that the dataset is indeed balanced","af72b1de":"## Part 2. Training the model","c1e6fb0a":"## Part 4. Improving the model","2fa49100":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions","2ff915dd":"Distribution of lengths for sarcastic and normal comments is almost the same.","fcbe589b":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","274525bb":"Train the same logistic regression.","005d9e34":"As we can see, accuracy slightly increased.","49827cf6":"## Part 1. Exploratory data analysis","86f00eb3":"We'll have separate Tf-Idf vectorizers for comments and for subreddits and stack all features together.","576858ff":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","e86c63c0":"The same for authors doesn't yield much insight. Except for the fact that somebody's comments were sampled - we can see the same amounts of sarcastic and non-sarcastic comments."}}