{"cell_type":{"5fb1d8ee":"code","d57e04e9":"code","72119a89":"code","4a53a14d":"code","08d2ab8f":"code","e5a27e1c":"code","be1995cd":"code","a562590c":"code","223b6e4e":"code","ac77fd44":"code","73b2fbf5":"code","2a1829ef":"code","0223a41e":"code","19371ba3":"code","8c2d14b4":"code","39e10f14":"code","1e03ede0":"code","626d42eb":"code","152b656d":"code","2273f5d1":"code","767a0d99":"code","5082c973":"code","60d46dd8":"code","a07ae69d":"code","1ece5c4f":"code","7fd8a7f4":"code","3ed78806":"code","a87a1b4e":"code","7a379c1e":"code","3117c803":"code","c6aa6328":"code","d2d6f856":"code","00b0d0a0":"code","262e9a73":"code","aaa8145a":"code","2b6a768e":"code","3193d562":"code","0717f0f0":"code","700f25d4":"code","865b669e":"code","a963a334":"markdown","0bf72380":"markdown","29376193":"markdown","8f0843a4":"markdown","ac17c0f9":"markdown","dcc612de":"markdown","ea57f4b2":"markdown","98b5ec3b":"markdown","61cc7ed7":"markdown","6eb58965":"markdown","1e3b33f7":"markdown","7b87101b":"markdown","e917ce5b":"markdown","906ef423":"markdown","49f81221":"markdown","d7e5e041":"markdown","1efce65e":"markdown","ff94a53b":"markdown","e3dfa33a":"markdown","48b9e4ba":"markdown","a3a797a4":"markdown"},"source":{"5fb1d8ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import IsolationForest\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d57e04e9":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv\", low_memory=False)#, nrows=10000)\ntrain[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv\", low_memory=False)\ntest[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","72119a89":"test.info(memory_usage=\"deep\")","4a53a14d":"train.head(10)","08d2ab8f":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\ntarget_names = [\"Carbon monoxide\", \"Benzene\", \"Nitrogen oxides\"]","e5a27e1c":"def add_new_plot_features(df):\n    \"\"\"\n    Adds new features to a given dataset for plotting\n    \"\"\"\n    df[\"month\"] = df[\"date_time\"].dt.month\n    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n    df[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"quarter\"] = df[\"date_time\"].dt.quarter\n    df[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\n#     df[\"is_winter\"] = df[\"month\"].isin([1, 2, 12])\n#     df[\"is_sprint\"] = df[\"month\"].isin([3, 4, 5])\n#     df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8])\n#     df[\"is_autumn\"] = df[\"month\"].isin([9, 10, 11])\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    return df\n\ndef add_new_ml_features(df, i=3): # i=3 is for heatmap plot\n    \"\"\"\n    Adds new features to a given dataset for training\n    \"\"\"\n\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"maximum_hours\"] =  df[\"hour\"].isin([8, 9, 17, 18, 19, 20]).astype(\"int\")\n    \n    # Marking weekends because they usually have lower target values\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) \/ df[\"relative_humidity\"]\n    \n    # A list of features to generate shifted and lagged values\n    shift_features = [[\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\", \"target_benzene_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_5\"],\n                      # Features for heatmap plot\n                      [\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]]\n    \n    # Amounts of hour shifts and lags\n    shifts = [1, 2, 3, 4, 5, 6, 12, 24]\n    \n    for feature in shift_features[i]:\n        for shift in shifts:\n            df[feature+\"-\"+str(shift)+\"abs_shfit\"] = df[feature] - df[feature].shift(periods=shift, fill_value=0)\n            df[feature+\"+\"+str(shift)+\"abs_shfit\"] = df[feature] - df[feature].shift(periods=-shift, fill_value=0)\n\n    # Replacing infinity values as a result of devision by zero at the end of a dataset\n    df.replace(to_replace=np.inf, value=0, inplace=True)\n    \n    return df.drop(\"hour\", axis=1)\n","be1995cd":"train_copy = train.copy()\ntest_copy = test.copy()\ntrain = add_new_plot_features(train)\ntest = add_new_plot_features(test)","a562590c":"# Plot dataframe\ndf = pd.concat([train[\"date_time\"], test[\"date_time\"]], axis=0).reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(16, 1.5))\nbar1 =  ax.barh(0, 7111+2247, color=\"salmon\", height=0.2)\nbar2 =  ax.barh(0, 7111, color=\"teal\", height=0.2)\nax.set_title(\"Train and test datasets size comparison\", fontsize=20, pad=5)\nax.bar_label(bar1, [\"Test dataset\"], label_type=\"edge\", padding=-170,\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.bar_label(bar2, [\"Train dataset\"], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.set_xticks([0, 7111, 7111+2247])\nax.set_xticklabels([\"2010-03-10\", \"2011-01-01\", \"2011-04-04\"])\nax.set_yticks([])\nplt.show();","223b6e4e":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    axs[i].plot(train[\"date_time\"], train[targets[i]], color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","ac77fd44":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\ndf[\"day\"] = df[\"date_time\"].dt.dayofyear\ndf[\"weekday\"] = df[\"date_time\"].dt.dayofweek\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\n# An array of number of days of year (i.e. from 1 to 365) which are mondays to mark week starts\nmondays = df.loc[df[\"weekday\"] == 0][\"day\"].value_counts(sort=False).index\n# An array of number of weeks of year to be used as label ticks\nweeks = df[\"date_time\"].dt.isocalendar().week.unique()[1:]\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\nfor i in [0, 1, 2]:\n    axs[i].plot(df.groupby(\"day\")[targets[i]].mean().index,\n                df.groupby(\"day\")[targets[i]].mean().values, color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) mean levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Week starts\", fontsize=14, labelpad=5)\n    axs[i].set_xticks(mondays)\n    axs[i].set_xticklabels(weeks)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","73b2fbf5":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[0], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[0]], color=\"steelblue\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","2a1829ef":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[1], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[1]], color=\"palevioletred\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","0223a41e":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[2], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[2]], color=\"goldenrod\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","19371ba3":"fig, axs = plt.subplots(figsize=(15, 6), ncols=3, nrows=1, sharey=False)\n\nfig.suptitle(\"Target values distribution\", fontsize=20)\n\ncolors = [\"mediumorchid\", \"lightseagreen\", \"cornflowerblue\"]\n\nfor i in [0, 1, 2]:\n    axs[i].hist(train[targets[i]], bins=60, edgecolor=\"black\", color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=5)\n    axs[i].set_ylabel(\"Amount of values\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(f\"{target_names[i]} level\", fontsize=13, labelpad=5)\n    axs[i].grid(axis=\"y\")\n\nplt.show();","8c2d14b4":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.35\nx = train.groupby(\"hour\")[\"target_carbon_monoxide\"].mean().index\n\nfor i in np.arange(3):\n    bars1 = axs[i].bar(x-width\/2, train.groupby(\"hour\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"cornflowerblue\")\n    bars2 = axs[i].bar(x+width\/2, train.groupby(\"hour\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"palevioletred\")\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(\"Day hours\", fontsize=13, labelpad=5)\n    axs[i].set_xticks(x)\n    axs[i].grid(axis=\"y\")\n    axs[i].legend(fontsize=13)\n","39e10f14":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 19), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.25)\nwidth=0.35\nx = df.groupby(\"day_of_week\")[\"target_carbon_monoxide\"].mean().index + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width\/2, df.groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x+width\/2, df.groupby(\"day_of_week\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=13)\n\nx = df.groupby(\"month\")[\"target_carbon_monoxide\"].mean().index\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width\/2, df.groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x+width\/2, df.groupby(\"month\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=13)","1e03ede0":"# Day hours which will be used for plotting data\nhours = [0, 5, 8, 14, 19]\n# Dataframe copy excluding the last row which is the only one representing January\ndf = train.loc[train[\"hour\"].isin(hours)].drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week at given hours\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.15\nx = np.sort(df[\"day_of_week\"].unique()) + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 0].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 0].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 0].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=10)\n\nx = df[\"month\"].unique()\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 1].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 1].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 1].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=10)","626d42eb":"# Lists of feature names to be used for plots below\nall_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\", \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nall_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\", \"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]\n\nweather_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\"]\nweather_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\"]\n\nsensor_features = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nsensor_feature_names = [\"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]","152b656d":"fig, axs = plt.subplots(figsize=(16, 30), ncols=1, nrows=8, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\nfor i in np.arange(8):\n    legend_lines = [Line2D([0], [0], color=colors[i], lw=10),\n                    Line2D([0], [0], color=\"black\", lw=10)]\n    axs[i].plot(train[\"date_time\"], train[all_features[i]], color=colors[i], label=\"Train data\")\n    axs[i].plot(test[\"date_time\"], test[all_features[i]], color=\"black\", label=\"Test data\")\n    axs[i].set_title(f\"{all_feature_names[i]} levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{all_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].legend(legend_lines, [\"Train data\", \"Test data\"], fontsize=12, loc=1)\n    axs[i].grid(axis=\"both\")","2273f5d1":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    # New year days start from 7110th row\n    data = df.iloc[:7110].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in [0, 1, 2]:\n    # New year weeks start from 7159th row. \n    # Because of Jan 1st and 2nd from the test dataset are counted as 52nd week of 2010,\n    # the colored plotline contains some test data. \n    data = df.iloc[:7159].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean weekly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","767a0d99":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 30), ncols=2, nrows=5, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\", \"goldenrod\", \"indianred\"]\n\nfor i in np.arange(5):\n    data = df.iloc[:7110].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in np.arange(5):\n    data = df.iloc[:7159].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","5082c973":"# Plot dataframe\ndf = train_copy.copy()\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(12,12))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", linewidth=1,\n                 annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Original dataset correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"bold\")\nplt.setp(ax.get_yticklabels(), weight=\"bold\")\nplt.show();","60d46dd8":"# Plot dataframe\ndf = add_new_ml_features(train_copy.copy())\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,12))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\": 9})\nax.set_title(\"Original and engineered features correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\")\nplt.show();","a07ae69d":"def prepare_dataset(train_copy, test_copy, i):\n    \"\"\"\n    Preprocesses given train and test datasets and returns X, y, X_test \n    \"\"\"\n\n    X = add_new_ml_features(train_copy.copy(), i)\n\n    # Dropping the last row which is 2011-01-01 00:00:00\n    if X.index[-1] == 7110:\n        X.drop([7110], axis=0, inplace=True)\n\n    # Resetting dataframe index\n    X.reset_index(drop=True, inplace=True)\n    # Adding 168 last train set rows to the head of test set in order to get shifting feature values\n    X_test_temp = pd.concat([train_copy.iloc[-169:-1].drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1), test_copy], axis=0)\n    X_test_temp.reset_index(inplace=True, drop=True)\n    X_test = add_new_ml_features(X_test_temp.copy(), i)\n    # Deleting added train set rows\n    X_test.drop(X_test.loc[:167].index, axis=0, inplace=True)\n    X_test.reset_index(inplace=True, drop=True)\n\n    y = np.log1p(X[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]])\n    X.drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1, inplace=True)\n    \n    X['date_time'] = X['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n    X_test['date_time'] = X_test['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n\n#     display(X.head())\n#     display(X_test.head())\n#     display(y.head())\n\n    return X, X_test, y","1ece5c4f":"# Sets of base models hyperparameters optimized by Optuna for each target with various depth\ncb_params = [\n                [{'iterations': 1000,\n                  'learning_rate': 0.03298118997883894,\n                  'l2_leaf_reg': 13.866168815111331,\n                  'bagging_temperature': 3.598456367521501,\n                  'random_strength': 1.1810169295356476,\n                  'depth': 16,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.03298118997883894,\n                  'l2_leaf_reg': 13.866168815111331,\n                  'bagging_temperature': 3.598456367521501,\n                  'random_strength': 1.1810169295356476,\n                  'depth': 15,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.03298118997883894,\n                  'l2_leaf_reg': 13.866168815111331,\n                  'bagging_temperature': 3.598456367521501,\n                  'random_strength': 1.1810169295356476,\n                  'depth': 14,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 20000,\n                  'learning_rate': 0.023621545862016827,\n                  'l2_leaf_reg': 19.030604497176707,\n                  'bagging_temperature': 1.6654461295318517,\n                  'random_strength': 1.3014359165957727,\n                  'depth': 4,\n                  'grow_policy': 'SymmetricTree',\n                  'leaf_estimation_method': 'Gradient'},\n                 {'iterations': 16427,\n                  'learning_rate': 0.011270680049200347,\n                  'l2_leaf_reg': 2.1567741919720036,\n                  'bagging_temperature': 2.179300367880761,\n                  'random_strength': 1.4991607805892369,\n                  'depth': 4,\n                  'grow_policy': 'Depthwise',\n                  'leaf_estimation_method': 'Gradient'}],\n    \n    \n                [{'iterations': 1000,\n                  'learning_rate': 0.02721102734588763,\n                  'l2_leaf_reg': 17.373548837368403,\n                  'bagging_temperature': 5.321778377898784,\n                  'random_strength': 1.442010850601074,\n                  'depth': 16,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Newton',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.02721102734588763,\n                  'l2_leaf_reg': 17.373548837368403,\n                  'bagging_temperature': 5.321778377898784,\n                  'random_strength': 1.442010850601074,\n                  'depth': 15,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Newton',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.02721102734588763,\n                  'l2_leaf_reg': 17.373548837368403,\n                  'bagging_temperature': 5.321778377898784,\n                  'random_strength': 1.442010850601074,\n                  'depth': 14,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Newton',\n                  'max_leaves': 64},\n                 {'iterations': 10035,\n                  'learning_rate': 0.010293242258606604,\n                  'l2_leaf_reg': 1.00856046822379,\n                  'bagging_temperature': 2.0084905523023475,\n                  'random_strength': 1.0989704542465142,\n                  'depth': 4,\n                  'grow_policy': 'SymmetricTree',\n                  'leaf_estimation_method': 'Newton'},\n                 {'iterations': 7686,\n                  'learning_rate': 0.01018369258714962,\n                  'l2_leaf_reg': 16.303503907195584,\n                  'bagging_temperature': 6.0525965608860615,\n                  'random_strength': 1.7916045776213503,\n                  'depth': 4,\n                  'grow_policy': 'Depthwise',\n                  'leaf_estimation_method': 'Newton'}],\n    \n    \n                [{'iterations': 1000,\n                  'learning_rate': 0.04162551441945943,\n                  'l2_leaf_reg': 0.6161561831774424,\n                  'bagging_temperature': 8.70578164844492,\n                  'random_strength': 1.033027101582271,\n                  'depth': 16,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.04162551441945943,\n                  'l2_leaf_reg': 0.6161561831774424,\n                  'bagging_temperature': 8.70578164844492,\n                  'random_strength': 1.033027101582271,\n                  'depth': 15,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 1000,\n                  'learning_rate': 0.04162551441945943,\n                  'l2_leaf_reg': 0.6161561831774424,\n                  'bagging_temperature': 8.70578164844492,\n                  'random_strength': 1.033027101582271,\n                  'depth': 14,\n                  'grow_policy': 'Lossguide',\n                  'leaf_estimation_method': 'Gradient',\n                  'max_leaves': 64},\n                 {'iterations': 8859,\n                  'learning_rate': 0.049661025418408236,\n                  'l2_leaf_reg': 19.999333247464577,\n                  'bagging_temperature': 9.584826983465707,\n                  'random_strength': 1.8532006283239564,\n                  'depth': 4,\n                  'grow_policy': 'SymmetricTree',\n                  'leaf_estimation_method': 'Newton'},\n                 {'iterations': 19895,\n                  'learning_rate': 0.010495629145711569,\n                  'l2_leaf_reg': 18.350392839340135,\n                  'bagging_temperature': 2.2661957751297495,\n                  'random_strength': 1.449631439165256,\n                  'depth': 4,\n                  'grow_policy': 'Depthwise',\n                  'leaf_estimation_method': 'Newton'}]\n            ]\n\n# Sets of blender models hyperparameters optimized by Optuna for each target\nxgb_params = [{'n_estimators': 78,\n               'learning_rate': 0.07527947300777532,\n               'max_depth': 2,\n               'booster': 'gbtree',\n               'tree_method': 'auto',\n               'reg_lambda': 0.6378105258790215,\n               'reg_alpha': 0.0220722729914309,\n               'random_state': 42,\n               'n_jobs': 4},\n              {'n_estimators': 333,\n               'learning_rate': 0.025432541067690877,\n               'max_depth': 2,\n               'booster': 'gbtree',\n               'tree_method': 'auto',\n               'reg_lambda': 0.21247269011384323,\n               'reg_alpha': 0.3029339596513587,\n               'random_state': 42,\n               'n_jobs': 4},\n              {'n_estimators': 140,\n               'learning_rate': 0.0633431641773825,\n               'max_depth': 2,\n               'booster': 'gbtree',\n               'tree_method': 'auto',\n               'reg_lambda': 0.1995327745605569,\n               'reg_alpha': 0.7236251161640519,\n               'random_state': 42,\n               'n_jobs': 4}]","7fd8a7f4":"def get_oof_rmsle(X, y, params, months):\n    \"\"\"\n    Splits given train dataset then trains a model with given parameters on a train set and returns valid set RMSLE score\n    \"\"\"\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n    for train_idx, valid_idx in split.split(X, months):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n        model = CatBoostRegressor(random_state=42,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 **params)\n        model.fit(X_train, y_train,\n                  verbose=False,\n                  cat_features=[\"working_hours\", \"is_weekend\", \"maximum_hours\"])\n        oof_preds = np.expm1(model.predict(X_valid))\n        oof_preds[oof_preds < 0] = 0\n        oof_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds))\n        \n    return oof_rmsle","3ed78806":"def train_without_folds(X, y, X_test, params):\n    \"\"\"\n    Trains one model on a given dataset and returns its test data predictions and eature importances\n    \"\"\"\n    \n    model = CatBoostRegressor(random_state=42,\n                             thread_count=4,\n                             verbose=False,\n                             loss_function='RMSE',\n                             eval_metric='RMSE',\n                             **params,\n                            iterations=10)\n    model.fit(X, y,\n              verbose=False,\n              cat_features=[\"working_hours\", \"is_weekend\", \"maximum_hours\"])\n    model_preds = np.expm1(model.predict(X_test))\n    model_fi = model.feature_importances_\n        \n    return model_preds, model_fi","a87a1b4e":"def train_with_folds(X, y, X_test, months, params):\n    \"\"\"\n    The function splits given train data into 10 folds and trains each model on each fold.\n    Each model makes test predictions. Mean test predictions as vell as valid data predictions\n    and score and feature importances are returned.\n    \"\"\"\n    \n    \n    splits = 10\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros((X.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X, months)):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n        model = CatBoostRegressor(random_state=42,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 od_type=\"Iter\",\n                                 early_stopping_rounds=500,\n                                 use_best_model=True,\n    #                                      iterations=10000,\n                                 **params)\n        model.fit(X_train, y_train,\n                  eval_set=(X_valid, y_valid),\n                  verbose=False,\n                  cat_features=[\"working_hours\", \"is_weekend\", \"maximum_hours\"])\n        model_preds += np.expm1(model.predict(X_test)) \/ splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n        oof_preds[oof_preds < 0] = 0\n        print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n    #         print(f\"Trees: {model.tree_count_}\")\n    model_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y), oof_preds))\n    \n    \n    return model_rmsle, model_preds, model_fi, oof_preds","7a379c1e":"def get_blender_preds(X, X_test, y, months, params):\n    \"\"\"\n    Splits the given dataset of base model predictions into folds and trains\n    blender models. Test data predictions are returned.\n    \"\"\"\n    \n    splits = 10\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros((X.shape[0],))\n    model_preds = 0\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X, months)):\n        X_train, X_valid = np.ascontiguousarray(X.loc[train_idx]), np.ascontiguousarray(X.loc[valid_idx])\n        y_train, y_valid = np.ascontiguousarray(y.loc[train_idx]), np.ascontiguousarray(y.loc[valid_idx])    \n        \n        blender = XGBRegressor(**params, verbosity=0)\n        blender.fit(X_train, y_train,\n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    early_stopping_rounds=100,\n                    verbose=0)\n        model_preds += np.expm1(blender.predict(np.ascontiguousarray(X_test))) \/ splits\n        oof_preds[valid_idx] = np.expm1(blender.predict(X_valid))\n        oof_preds[oof_preds < 0] = 0\n        print(f\"Blender fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n    blender_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y), oof_preds))\n    print(f\"Blender oof RMSLE is {blender_rmsle}\")\n    \n    return model_preds, oof_preds","3117c803":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Log loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n       \n    #A set of hyperparameters to optimize by optuna\n    #First set deep model\n#     xgb_params = {\n#                  \"n_estimators\": trial.suggest_categorical('n_estimators', [2000]),\n#                  \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.2),\n#                  \"booster\": trial.suggest_categorical('booster', [\"gblinear\"]),\n#                  \"reg_lambda\": trial.suggest_float('reg_lambda', 0.00001, 0.9),\n#                  \"reg_alpha\": trial.suggest_float('reg_alpha', 0.00001, 0.9),\n#                  \"base_score\": trial.suggest_float('base_score', 0.1, 0.9),\n#                  \"random_state\": trial.suggest_categorical('random_state', [42]),\n#                  \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n#                     }\n\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [4000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.8),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"auto\"]),\n        \n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 0.00001, 0.9),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 0.00001, 0.9),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n                    }\n\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n\n    oof = np.expm1(model.predict(X_valid))\n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    \n    return np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof))","c6aa6328":"def optimize_params(X, y, months):\n    # Splitting data into train and valid folds using target bins for stratification\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    for train_idx, valid_idx in split.split(X, months):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n        \n    # Setting optuna verbosity to show only warning messages\n    # If the line is uncommeted each iteration results will be shown\n    # optuna.logging.set_verbosity(optuna.logging.WARNING)\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n                                                        y_train, y_valid),\n                   n_trials = 500)\n\n    # Showing optimization results\n    print('Number of finished trials:', len(study.trials))\n    print('Best trial parameters:', study.best_trial.params)\n    print('Best score:', study.best_value)","d2d6f856":"%%time\n\n# A list of feature importances\nall_fi = []\n\n# Initializing and filling predictions dataframe with datetime values\npreds = pd.DataFrame()\npreds[\"date_time\"] = test_copy[\"date_time\"].copy()\n\n# The months will be used for folds split\nmonths = train_copy.drop(7110, axis=0)[\"date_time\"].dt.month\n\ntotal_mean_rmsle = 0\n\nX = pd.DataFrame()\n\ntrain_preds_df = pd.DataFrame(index=np.arange(7110))\ntest_preds_df = pd.DataFrame(index=test.index)\n\nfor i, target in enumerate(targets):\n# for i in [2]:\n    target = targets[i]\n    print(f\"\\nTraining for {target}...\")\n    \n    group_fi = 0\n    # Getting dataset for a current target in case there is need for different feature set per target\n    X, X_test, y = prepare_dataset(pd.concat([train_copy, train_preds_df], axis=1), pd.concat([test_copy, test_preds_df], axis=1), i)\n    oof_preds_for_blender = pd.DataFrame()\n    test_preds_for_blender = pd.DataFrame()\n    \n    # Train base models with folds\n    for num, params in enumerate(cb_params[i]):\n        print(f\"Training with this params: {params}\")\n        model_rmsle, model_preds, model_fi, oof_preds = train_with_folds(X, y[target], X_test, months, params)\n        print(f\"\\nOverall {target} RMSLE: {model_rmsle}\")\n        total_mean_rmsle += model_rmsle \/ (len(cb_params[i]) * len(targets))\n        all_fi.append(dict(zip(X_test.columns, model_fi)))\n        oof_preds_for_blender[str(num)] = oof_preds\n        test_preds_for_blender[str(num)] = model_preds\n    display(oof_preds_for_blender)\n#     optimize_params(oof_preds_for_blender, y[target], months)\n    preds[target], blender_oof_preds = get_blender_preds(oof_preds_for_blender, test_preds_for_blender, y[target], months, xgb_params[i])\n    train_preds_df[target+\"_preds\"] = np.log1p(blender_oof_preds)\n    test_preds_df[target+\"_preds\"] = np.log1p(preds[target])\n    \n    \n#     # Train base models without folds\n#     for num, params in enumerate(cb_params[i]):\n#         print(f\"Training with this params: {params}\")\n#         model_rmsle = get_oof_rmsle(X, y[target], params, months)\n#         print(f\"Model's RMSLE is {model_rmsle}\")\n#         total_mean_rmsle += model_rmsle \/ (len(cb_params[i]) * len(targets))\n        \n#         model_preds, model_fi = train_without_folds(X, y[target], X_test, params)\n#         group_fi += model_fi \/ len(cb_params[i])\n        \n# #         preds_for_blender[str(i)+\"_\"+str(num)] = model_preds\n#         preds[target] += model_preds \/ len(cb_params[i])\n#     all_fi.append(dict(zip(X_test.columns, group_fi)))\n\nprint(f\"\\n\\nTotal RMSLE is {total_mean_rmsle}\\n\")","00b0d0a0":"# Creating feature list from feature importance dictionaries\nfeature_list = set()\nfor i in np.arange(len(all_fi)):\n    feature_list = set.union(feature_list, set(all_fi[i].keys()))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","262e9a73":"# Combining feature importances of different models into one dataframe\ndf = pd.DataFrame(columns=[\"Feature\"])\ndf[\"Feature\"] = list(feature_list)\nfor i in np.arange(len(all_fi)):\n    for key in all_fi[i].keys():\n        df.loc[df[\"Feature\"] == key, \"Importance_\" + str(i+1)] = all_fi[i][key] \/ 1000\ndf.fillna(0, inplace=True)\ndf[\"Overall_importance\"] = df[\"Importance_1\"] + df[\"Importance_2\"] + df[\"Importance_3\"]\ndf.sort_values(\"Overall_importance\", ascending=False, inplace=True)\ndf.reset_index(drop=True, inplace=True)","aaa8145a":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 75))\nbars1 = ax.barh(x-height, df[\"Importance_1\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Importance_2\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Importance_3\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\n# ax.set_xlim(0, 0.25)\n# ax.set_xticks(np.arange(0, 0.275, 0.025))\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\n# ax2 = ax.secondary_xaxis('top')\n# ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n# # ax2.set_xlim(0, 0.25)\n# # ax2.set_xticks(np.arange(0, 0.275, 0.025))\n# ax2.tick_params(axis=\"x\", labelsize=15)\n# ax.legend(legend_lines, [\"Original features\", \"Custom features\"], fontsize=15, loc=1, bbox_to_anchor=(0, 0, 1, 0.97))\nax.legend(fontsize=13, loc=\"lower right\")\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","2b6a768e":"preds.to_csv('submission.csv', index=False)\npreds.head()","3193d562":"targets = train_copy[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]]","0717f0f0":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 744, 1), targets.loc[train[\"month\"]==12, target], label=\"Train, 12th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","700f25d4":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 720, 1), targets.loc[train[\"month\"]==11, target], label=\"Train, 11th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","865b669e":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 598, 1), targets.loc[:597, target], label=\"Train, from 10.3 to 4.4\")\n    axs[i].plot(np.arange(0, 596, 1), preds.loc[1651: , target],\n                label=\"Test, from 10.3 to 4.4\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","a963a334":"Let's compare predictions with the closest months from the train datasets.","0bf72380":"# **Predictions analysis and submission**","29376193":"Let's compare our train and test feature data.","8f0843a4":"As you can see, the predictions are the closest to the training set in the overlapping months (from March 10 to April 4). ","ac17c0f9":"Let's see mean target values per day of year.","dcc612de":"## **Data import**","ea57f4b2":"As you can see, all target values usually go down at the end of each week (i.e. during weekends). \n\nLet's check targets distribution along each month.","98b5ec3b":"There are some near zero flat areas at 4th, 6th, 8th, 12th month plots. Need to figure out what is so special about these days. It also may be a garbage data which sould be deleted before machine learning.","61cc7ed7":"Let's check feature correlation.","6eb58965":"# **Feature importances**","1e3b33f7":"## Target plots","7b87101b":"The datasets have timestamps. Let's compare which dates are in each dataset.","e917ce5b":"The idea of SMC feature below was taken from this [notebook](https:\/\/www.kaggle.com\/junhyeok99\/automl-pycaret).\n\nTaking into account temperature changes was suggested by [@lukaszborecki](https:\/\/www.kaggle.com\/lukaszborecki) [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/discussion\/250931#1380107).","906ef423":"The datasets also have three target columns that the model have to predict. Let's see how each target is changing in time.","49f81221":"Let's check how each target value chenges depending on the time of day, day of week, and month.","d7e5e041":"The code below runs hyperparameters optimization. It is commeted to save runtime.","1efce65e":"# **Models training**","ff94a53b":"Let's check each target value distribution.","e3dfa33a":"The datetime to int conversion shown below was found in this [notebook](https:\/\/www.kaggle.com\/jarupula\/eda-rf-model-tps-july-21).","48b9e4ba":"## Feature plots","a3a797a4":"# **EDA**"}}