{"cell_type":{"bacb424b":"code","38a9bb3c":"code","cc06e2df":"code","1b51d7fe":"code","76a895ef":"code","7c1273ad":"code","c667b8da":"code","328c7db8":"code","c573d92a":"code","993dd94e":"code","ad056a2b":"code","83898960":"code","542346cc":"code","e83f105e":"code","280cfc88":"code","a8b5d598":"markdown","b3bdb770":"markdown"},"source":{"bacb424b":"import tensorflow as tf\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization, RandomFlip, RandomRotation, RandomCrop\nfrom tensorflow.keras.layers.experimental.preprocessing import CenterCrop, RandomZoom, RandomContrast\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nimport matplotlib.pyplot as plt","38a9bb3c":"train_dataset = image_dataset_from_directory('..\/input\/labeled-chest-xray-images\/chest_xray\/train',\n                                             subset='training',\n                                             seed=42,\n                                             validation_split=0.1,\n                                             batch_size=128,\n                                             image_size=(256, 256))","cc06e2df":"validation_dataset = image_dataset_from_directory('..\/input\/labeled-chest-xray-images\/chest_xray\/train',\n                                                  subset='validation',\n                                                  seed=42,\n                                                  validation_split=0.1,\n                                                  batch_size=128,\n                                                  image_size=(256, 256))","1b51d7fe":"class_names = train_dataset.class_names\nclass_names","76a895ef":"plt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","7c1273ad":"test_dataset = image_dataset_from_directory('..\/input\/labeled-chest-xray-images\/chest_xray\/test',\n                                             shuffle=True,\n                                             label_mode='int',\n                                             batch_size=128,\n                                             image_size=(256, 256))","c667b8da":"data_augmentation = Sequential()\ndata_augmentation.add(RandomFlip(\"horizontal\"))\ndata_augmentation.add(RandomRotation(0.1))\ndata_augmentation.add(RandomZoom(0.1))\ndata_augmentation.add(RandomContrast(0.1))","328c7db8":"base_inception = InceptionV3(weights='imagenet', \n                  include_top=False, \n                  input_shape=(256, 256, 3))","c573d92a":"model = Sequential()\nmodel.add(data_augmentation)\nmodel.add(Normalization())\nmodel.add(base_inception)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(class_names), activation='softmax'))","993dd94e":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nmodel_save_path = '.\/best_model.h5'\n\ncheckpoint_callback = ModelCheckpoint(model_save_path, \n                                      monitor='val_accuracy',\n                                      save_best_only=True,\n                                      verbose=1)","ad056a2b":"model.compile(loss='sparse_categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","83898960":"history = model.fit(train_dataset, \n                    validation_data=validation_dataset,\n                    epochs=10,\n                    callbacks=[checkpoint_callback])","542346cc":"plt.plot(history.history['accuracy'], \n         label='Test correct answers percent')\nplt.plot(history.history['val_accuracy'], \n         label='Validation correct answers percent')\nplt.xlabel('Epoch')\nplt.ylabel('Correct answers percent')\nplt.legend()\nplt.show()","e83f105e":"plt.plot(history.history['loss'], \n         label='Train loss')\nplt.plot(history.history['val_loss'], \n         label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","280cfc88":"scores = model.evaluate(test_dataset, verbose=1)\nprint(\"The percent of correct answers:\", round(scores[1] * 100, 4))","a8b5d598":"Let's try to use an **InceptionV3** as our base model.\n\nInception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years and is widely used nowadays.\n\nThe model itself is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, concats, dropouts, and fully connected layers. Batchnorm is used extensively throughout the model and applied to activation inputs. Loss is computed via Softmax.\n\n![InceptionV3](https:\/\/www.apriorit.com\/images\/articles\/applying_inception_v3\/figure-1.jpg)","b3bdb770":"**Tensorflow** has a very convinient tool **image_dataset_from_directory** whenever you have an image structure FOLDER_NAME(class_name) --> images. Don't forget to use a subset and a validation_split parameter.\n\n"}}