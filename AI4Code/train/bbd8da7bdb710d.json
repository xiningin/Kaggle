{"cell_type":{"5d8d6401":"code","43496278":"code","c92177ff":"code","ebdb584f":"code","2d3a5a99":"code","42ccd38e":"code","26bf8ee8":"code","cf330f7c":"code","ec4ff407":"code","f69cb6e3":"code","d9c1b6ea":"code","8bfa727e":"code","08cea225":"code","25a1eff1":"code","e9fecc5c":"code","649c0100":"code","4c5c99d9":"code","1d7ca58d":"code","7e96053a":"code","95e74c51":"code","028f0ca3":"code","899d40f2":"code","960d4de1":"code","b1c0fcbd":"code","ea63aa33":"code","523ec10a":"code","a4985440":"code","8d440493":"code","2b6accc2":"code","2480f654":"code","d7867aaf":"code","4879502d":"code","502c6f8e":"code","f4b76139":"code","9d37ddb4":"code","ebdda986":"code","fa90eb3b":"code","57264f05":"code","4d177675":"code","a24d0daf":"code","d15e2c99":"code","951ffc75":"code","f20bc0b9":"code","f1607166":"code","8261edf2":"code","b7cade6d":"code","a0d11f68":"code","fc86cac6":"code","a5ec6169":"code","ed65f28e":"code","c31ea321":"code","99d6cdee":"code","ad7ec75c":"code","034fa655":"code","d09fc3c1":"code","74c4e14f":"code","7cb2d2c5":"code","e88164ed":"code","577b60a7":"code","01e47f1c":"code","75479258":"markdown","a020a0ed":"markdown","f6cf5bb8":"markdown","8f13944a":"markdown","43fe94c0":"markdown","31c65fb2":"markdown","2c5ee7ac":"markdown","4c122851":"markdown","74d352e8":"markdown","e8785541":"markdown","492e823a":"markdown","49482d94":"markdown","f3c2bcaf":"markdown","d0c269a9":"markdown","50121191":"markdown","1ea22da1":"markdown","42d9cffc":"markdown","b28ec27c":"markdown","812d2f68":"markdown","554d3078":"markdown","19645f4d":"markdown","3b4b9cd2":"markdown","9df06417":"markdown","612ff71c":"markdown","35d72abb":"markdown","36d22948":"markdown"},"source":{"5d8d6401":"import math, re, os, json\nimport seaborn as sn\nimport cv2\nimport itertools\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import plot_model,to_categorical\nfrom tensorflow.keras.models import load_model\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nprint(\"Tensorflow version \" + tf.__version__)","43496278":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","c92177ff":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nBASE_DIR = '..\/input\/cassava-leaf-disease-classification\/'\nIMAGE_SIZE = [512, 512]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 50\nPROBA_CONTRAST=1.","ebdb584f":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(y_true * y_pred,axis=0)\n    possible_positives = K.sum(y_true,axis=0)\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(y_true * y_pred,axis=0)\n    predicted_positives = K.sum(y_pred,axis=0)\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    y_pred = tf.one_hot(tf.argmax(y_pred,axis=-1),len(CLASSES))\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*K.mean((precision*recall)\/(precision+recall+K.epsilon()))","2d3a5a99":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, np.round(1000*cm[i, j])\/1000,\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","42ccd38e":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","26bf8ee8":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","cf330f7c":"def load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","ec4ff407":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/ld_train*.tfrec'),\n    test_size=0.35, random_state=5\n)\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test_tfrecords\/ld_test*.tfrec')","f69cb6e3":"# One hot label and float images.\ndef data_treat(image,label):\n    label = tf.one_hot(label,len(CLASSES))\n    image = tf.cast(image, tf.float32)\n    return image,label","d9c1b6ea":"# Values going from 0 to 255.\ndef data_treat_test(image,label):\n    image = tf.cast(image, tf.float32)\n    image = image-tf.math.reduce_min(image)\n    image = image\/tf.math.reduce_max(image)\n    image = image*255\n    return image,label","8bfa727e":"def data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    return image, label","08cea225":"def func_standard(p=PROBA_CONTRAST):\n    def data_standard(image, label):\n        if tf.random.uniform(shape=(), minval=0, maxval=1)<p:\n            image = image-tf.math.reduce_min(image)\n            image = image\/tf.math.reduce_max(image)\n            image = tf.math.round(image*255)\n        return image, label\n    return data_standard","25a1eff1":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","e9fecc5c":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training images, {} validation images, {} (unlabeled) test images'.format(\n    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","649c0100":"with open(os.path.join(BASE_DIR, \"label_num_to_disease_map.json\")) as file:\n    map_classes = json.loads(file.read())\n    map_classes = {int(k) : v for k, v in map_classes.items()}\n    \nprint(json.dumps(map_classes, indent=4))","4c5c99d9":"input_files = os.listdir(os.path.join(BASE_DIR, \"train_images\"))\nprint(f\"Number of train images: {len(input_files)}\")","1d7ca58d":"df_train = pd.read_csv(os.path.join(BASE_DIR, \"train.csv\"))\ndf_train[\"class_name\"] = df_train[\"label\"].map(map_classes)\nplt.figure(figsize=(8, 4))\nsn.countplot(y=\"class_name\", data=df_train);","7e96053a":"# This function returns the labels weights, compounded by a coefficient n.\ndef c_weights(labels,n=3\/4):\n    c_labels = Counter(labels)\n    A=len(c_labels)\/np.sum([x**-n for x in c_labels.values()])\n    cw = {i:A*c_labels[i]**-n for i in range(5)}\n    return cw","95e74c51":"cw = c_weights(df_train[\"label\"],n=3\/4)","028f0ca3":"lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-4, \n    decay_steps=1000, \n    decay_rate=0.9)","899d40f2":"def create_histogram(x,l):\n    Z=tf.split(x, 3, axis=-1)\n    Z=[tf.histogram_fixed_width(z,[0., 255.], nbins=32) for z in Z]\n    Z=tf.concat(Z, -1)\n    return Z,l","960d4de1":"def get_training_histogram(ordered=False):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)  \n    dataset = dataset.map(data_treat, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(func_standard(p=1),num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(create_histogram, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","b1c0fcbd":"def get_test_histogram(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.map(data_treat_test, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(create_histogram, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","ea63aa33":"def get_validation_histogram(ordered=False):\n    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n    dataset = dataset.map(data_treat, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(func_standard(p=1),num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(create_histogram, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","523ec10a":"p = 0.1","a4985440":"def create_model():\n    A2=tf.keras.layers.Input(shape=(32*3,))\n    X=tf.keras.layers.Dense(2048,activation='relu')(A2)\n    X=tf.keras.layers.Dropout(p)(X)\n    X=tf.keras.layers.Dense(128,activation='relu')(X)\n    X=tf.keras.layers.Dropout(p)(X)\n    X=tf.keras.layers.Dense(len(CLASSES),activation='softmax')(X)\n    model = tf.keras.Model(inputs=A2, outputs=X)\n    return model","8d440493":"with strategy.scope():       \n    model = create_model()\n    if os.path.exists('pierre_color.h5'):\n        print('loading')\n        modelp = tf.keras.models.load_model('pierre_color.h5',compile=False)\n        model.set_weights(modelp.get_weights())\n    else:\n        print('creating')\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n        loss='categorical_crossentropy',  metrics=['categorical_accuracy',f1_m],\n        )","2b6accc2":"model.summary()","2480f654":"# load data\ntrain_dataset = get_training_histogram()\nvalid_dataset = get_validation_histogram()","d7867aaf":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE\n\nhistory = model.fit(train_dataset, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    epochs=EPOCHS,#class_weight=c_weigths,\n                    validation_data=valid_dataset,\n                    validation_steps=VALID_STEPS)","4879502d":"model.save('pierre_color.h5')","502c6f8e":"# this code will convert our test image data to a float32 \ndef to_float32(image, label):\n    return tf.cast(image, tf.float32), label","f4b76139":"train_dataset = get_training_histogram(ordered=True)\nvalid_dataset = get_validation_histogram(ordered=True)","9d37ddb4":"dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=True)\n#dataset = tf.data.TFRecordDataset(TRAINING_FILENAMES[0], num_parallel_reads=AUTOTUNE)\nvalid_labels = []\nfor images, labels in dataset.take(-1):  # only take first element of dataset\n    valid_labels.append(labels.numpy())","ebdda986":"train_ds = train_dataset.map(to_float32)\nfit_train_label = model.predict(train_ds,steps=STEPS_PER_EPOCH)\nfit_train_label = np.argmax(fit_train_label,axis=1)","fa90eb3b":"valid_ds = valid_dataset.map(to_float32)\nfit_valid_label = model.predict(valid_ds)\nfit_valid_label = np.argmax(fit_valid_label,axis=1)","57264f05":"cm = confusion_matrix(valid_labels[:len(fit_valid_label)],fit_valid_label)","4d177675":"plot_confusion_matrix(cm, [0,1,2,3,4],normalize=True)","a24d0daf":"with strategy.scope():       \n    model = create_model()\n    if os.path.exists('pierre_color_W.h5'):\n        print('loading')\n        modelp = tf.keras.models.load_model('pierre_color_W.h5',compile=False)\n        model.set_weights(modelp.get_weights())\n    else:\n        print('creating')\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001),\n        loss='categorical_crossentropy',  metrics=['categorical_accuracy',f1_m],\n        )","d15e2c99":"train_dataset = get_training_histogram(ordered=False)\nvalid_dataset = get_validation_histogram(ordered=False)","951ffc75":"# First weight : same weights per class.\ncw = c_weights(df_train[\"label\"],n=1)","f20bc0b9":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE\n\nhistory = model.fit(train_dataset, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    epochs=EPOCHS,class_weight=cw,\n                    validation_data=valid_dataset,\n                    validation_steps=VALID_STEPS)","f1607166":"model.save('pierre_color_W.h5')","8261edf2":"train_dataset = get_training_histogram(ordered=True)\nvalid_dataset = get_validation_histogram(ordered=True)","b7cade6d":"train_ds = train_dataset.map(to_float32)\nfit_train_label = model.predict(train_ds,steps=STEPS_PER_EPOCH)\nfit_train_label = np.argmax(fit_train_label,axis=1)","a0d11f68":"valid_ds = valid_dataset.map(to_float32)\nfit_valid_label = model.predict(valid_ds)\nfit_valid_label = np.argmax(fit_valid_label,axis=1)","fc86cac6":"cm = confusion_matrix(valid_labels[:len(fit_valid_label)],fit_valid_label)","a5ec6169":"plot_confusion_matrix(cm, [0,1,2,3,4],normalize=True)","ed65f28e":"with strategy.scope():       \n    model = create_model()\n    if os.path.exists('pierre_color_34.h5'):\n        print('loading')\n        modelp = tf.keras.models.load_model('pierre_color_34.h5',compile=False)\n        model.set_weights(modelp.get_weights())\n    else:\n        print('creating')\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001),\n        loss='categorical_crossentropy',  metrics=['categorical_accuracy',f1_m],\n        )","c31ea321":"train_dataset = get_training_histogram(ordered=False)\nvalid_dataset = get_validation_histogram(ordered=False)","99d6cdee":"# Second weight : n=3\/4.\ncw = c_weights(df_train[\"label\"],n=3\/4)","ad7ec75c":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE\n\nhistory = model.fit(train_dataset, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    epochs=EPOCHS,class_weight=cw,\n                    validation_data=valid_dataset,\n                    validation_steps=VALID_STEPS)","034fa655":"model.save('pierre_color_34.h5')","d09fc3c1":"train_dataset = get_training_histogram(ordered=True)\nvalid_dataset = get_validation_histogram(ordered=True)","74c4e14f":"train_ds = train_dataset.map(to_float32)\nfit_train_label = model.predict(train_ds,steps=STEPS_PER_EPOCH)\nfit_train_label = np.argmax(fit_train_label,axis=1)","7cb2d2c5":"valid_ds = valid_dataset.map(to_float32)\nfit_valid_label = model.predict(valid_ds)\nfit_valid_label = np.argmax(fit_valid_label,axis=1)","e88164ed":"cm = confusion_matrix(valid_labels[:len(fit_valid_label)],fit_valid_label)","577b60a7":"plot_confusion_matrix(cm, [0,1,2,3,4],normalize=True)","01e47f1c":"#print('Generating submission.csv file...')\n#test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n#test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n#np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n#!head submission.csv","75479258":"## Last model n=3\/4. ","a020a0ed":"# TPU\n","f6cf5bb8":"# Set up environment","8f13944a":"## Define data loading methods\nThe following functions will be used to load our `training`, `validation`, and `test` datasets, as well as print out the number of images in each dataset.","43fe94c0":"# Set up variables\nWe'll set up some of our variables for our notebook here. ","31c65fb2":"# Creating a submission file\nNow that we've trained a model and made predictions we're ready to submit to the competition! You can run the following code below to get your submission file.","2c5ee7ac":"Once again, only one class is found. We can conclude that colour distribution is not a good parameter to determine the illness of cassava leaf. A model based on correlation seems needed.","4c122851":"We first train without class weights.","74d352e8":"## Simple color model.\nWe want to check the importance of colors to the illness detection. We create a color only model.","e8785541":"# Brief exploratory data analysis (EDA)\nFirst we'll print out the shapes and labels for a sample of each of our three datasets:","492e823a":"## Adding in augmentations ","49482d94":"Model based on the colours distribution.","f3c2bcaf":"### Confusion matrix.","d0c269a9":"Counting the labels","50121191":"# Building the model\n## Learning rate schedule\nWe learned about learning rates in the **[Intro to Deep Learning: Stochastic Gradient Descent](https:\/\/www.kaggle.com\/ryanholbrook\/stochastic-gradient-descent)** lesson, and here I've created a learning rate schedule mostly using the defaults in the **[Keras Exponential Decay Learning Rate Scheduler](https:\/\/keras.io\/api\/optimizers\/learning_rate_schedules\/exponential_decay\/)** documentation (I did change the `initial_learning_rate`. You can adjust the learning rate scheduler below, and read more about the other types of schedulers available to you in the **[Keras learning rate schedules API](https:\/\/keras.io\/api\/optimizers\/learning_rate_schedules\/)**.","1ea22da1":"# Introduction\n**This notebook is based on  [Jesse Mostipak\u2019s Tutorial](https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease)**  \nIn this notebook we check the importance of colours in the classification process.  \nWe study a baseline model.","42d9cffc":"## Visualizing the new results.\nLet's see if the distribution has changed.","b28ec27c":"## Checking the results.","812d2f68":"The following code chunk sets up a series of functions that will print out a grid of images. The grid of images will contain images and their corresponding labels.","554d3078":"# Load the data","19645f4d":"### F1 score.","3b4b9cd2":"This model simply return the most common class (here 3). We will compare with the same model with weights added.","9df06417":"## Visualizing the last results.\nLet's see if the distribution has changed.","612ff71c":"Now, all entries goes to the smaller class.  \nChecking the litterature, a power law of 3\/4 is often prefered. We try it as our last model.","35d72abb":"Be aware that because this is a code competition with a hidden test set, internet and TPUs cannot be enabled on your submission notebook. Therefore TPUs will only be available for training models. For a walk-through on how to train on TPUs and run inference\/submit on GPUs, see our [TPU Docs](https:\/\/www.kaggle.com\/docs\/tpu#tpu6).","36d22948":"## Decode the data"}}