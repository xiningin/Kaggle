{"cell_type":{"ed949d5c":"code","32750a76":"code","ebc1a40f":"code","b80c3db0":"code","61931909":"code","6b70c43f":"code","d85d66e2":"code","f735006a":"code","7b35ac94":"code","05040b02":"code","52a8764a":"code","29ebb543":"code","b2a70f27":"code","c2b08fa8":"code","da78959b":"code","b2796a06":"code","fa5242b3":"markdown","2b8c6318":"markdown","45d68455":"markdown","3dc79ae3":"markdown","b7a3187c":"markdown","2275598d":"markdown","e742a3fd":"markdown","c8c471cd":"markdown","043803d2":"markdown","442c3d34":"markdown","6822ba64":"markdown","334d51d3":"markdown","8f01c77a":"markdown","111d65ee":"markdown","5b5b0780":"markdown","3e2d2478":"markdown"},"source":{"ed949d5c":"import numpy\nimport seaborn\nimport torch\nimport matplotlib.pyplot as plt","32750a76":"BATCH_SIZE = 64\nEPOCHS = 25\nLEARNING_RATE = 3.5e-3\nSTEP_SIZE = 5\nGAMMA = 0.075","ebc1a40f":"RANDOM_STATE = 1234\n\ntorch.manual_seed(RANDOM_STATE)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(RANDOM_STATE)","b80c3db0":"if torch.cuda.is_available() and torch.version.cuda:\n    device = torch.device(\"cuda\")\nelif torch.cuda.is_available() and torch.version.hip:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","61931909":"from pandas import read_csv\n\ndf_train = read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv\")\ndf_test = read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv\")\n\nX_train, y_train = df_train.iloc[:, 1:], df_train.iloc[:, 0]\nX_evaluate, y_evaluate = df_test.iloc[:, 1:], df_test.iloc[:, 0]","6b70c43f":"from sklearn.preprocessing import LabelEncoder\n\ny_train = LabelEncoder().fit_transform(y_train.values.astype(numpy.int64))\ny_evaluate = LabelEncoder().fit_transform(y_evaluate.values.astype(numpy.int64))","d85d66e2":"from sklearn.preprocessing import MinMaxScaler\n\nX_train = MinMaxScaler().fit_transform(X_train.values.astype(numpy.float32)).reshape(-1, 1, 28, 28)\nX_evaluate = MinMaxScaler().fit_transform(X_evaluate.values.astype(numpy.float32)).reshape(-1, 1, 28, 28)","f735006a":"train_dataset = torch.utils.data.TensorDataset(\n    torch.from_numpy(X_train).to(device),\n    torch.from_numpy(y_train).to(device)\n)\n\nevaluate_dataset = torch.utils.data.TensorDataset(\n    torch.from_numpy(X_evaluate).to(device),\n    torch.from_numpy(y_evaluate).to(device)\n)","7b35ac94":"train_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n)\n\nevaluate_loader = torch.utils.data.DataLoader(\n    evaluate_dataset, batch_size=BATCH_SIZE, shuffle=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    evaluate_dataset, batch_size=1, shuffle=False\n)","05040b02":"class CNN(torch.nn.Module):\n    def __init__(self, classes: int = 10):\n        super(CNN, self).__init__()\n\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 256, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(512),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n            torch.nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        for child in self.features.children():\n            if isinstance(child, torch.nn.Conv2d):\n                n = (child.kernel_size[0] * child.kernel_size[1] * child.out_channels)\n                child.weight.data.normal_(0, numpy.sqrt(2.0 \/ n, dtype=float))\n            elif isinstance(child, torch.nn.BatchNorm2d):\n                child.weight.data.fill_(1)\n                child.bias.data.zero_()\n\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(1024 * 7 * 7, 512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, 1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(1024, classes)\n        )\n\n        for child in self.classifier.children():\n            if isinstance(child, torch.nn.Linear):\n                torch.nn.init.xavier_uniform_(child.weight)\n            elif isinstance(child, torch.nn.BatchNorm1d):\n                child.weight.data.fill_(1)\n                child.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, start_dim=1)\n\n        return self.classifier(x)","52a8764a":"from torch.optim import Adam, lr_scheduler\n\nmodel = CNN().to(device)\n\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)","29ebb543":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\n\nprogress = tqdm(total=EPOCHS * (len(train_loader) + len(evaluate_loader)))\n\nfor epoch in range(1, EPOCHS + 1):\n    train_accuracy, evaluation_accuracy = 0, 0\n    train_loss, evaluation_loss = 0, 0\n    with torch.enable_grad():\n        model.train()\n        for X, y in train_loader:\n            optimizer.zero_grad()\n            output = model(X)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # Log performance\n            train_accuracy += accuracy_score(\n                output.cpu().argmax(dim=1), y.cpu().data\n            )\n            train_loss += loss.data\n\n            progress.update(1)\n\n    with torch.no_grad():\n        model.eval()\n        for X, y in evaluate_loader:\n            output = model(X)\n\n            # Log performance\n            evaluation_accuracy += accuracy_score(\n                output.cpu().argmax(dim=1), y.cpu().data\n            )\n            evaluation_loss += criterion(output, y).data\n\n            progress.update(1)\n\n    scheduler.step()\n\nprogress.close()","b2a70f27":"y_preds = []\n\nwith torch.no_grad():\n    model.eval()\n    for X, y in tqdm(test_loader):\n        y_preds.append(model(X).argmax(dim=1).item())","c2b08fa8":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_evaluate, y_preds))","da78959b":"from pandas import DataFrame\nfrom sklearn.metrics import confusion_matrix\n\ndf = DataFrame(\n    confusion_matrix(y_evaluate, y_preds),\n    index=numpy.unique(y_evaluate),\n    columns=numpy.unique(y_evaluate)\n)\n\n_, ax = plt.subplots()\nax = seaborn.heatmap(\n    df,\n    cmap=\"rocket\",\n    square=True,\n    annot=True,\n    fmt=\".4g\",\n    annot_kws={\"size\": \"xx-small\"},\n    linewidths=1\n)","b2796a06":"import os\n\nif not os.path.exists(\"model\/\"):\n    os.makedirs(\"model\/\")\n\ntorch.save(\n    model.cpu().state_dict(), f\"model\/CIFAR_10_CNN_{LEARNING_RATE}_{BATCH_SIZE}.pickle\"\n)","fa5242b3":"The last step in dataloading is to wrap the `torch.utils.data.dataset.TensorDataset` in the `torch.utils.data.DataLoader` class to assist in the iteration of the data when training, testing and validating.","2b8c6318":"### Classification summary","45d68455":"## Train\n\nThe following codeblock contain the code for creating the data to the CNN model. It loops through a specific number of iterations of epochs, where each epoch both is trained and evaluated. This is then logged to a tensorboard to evaluate the training performance of the model.","3dc79ae3":"Then, define the optimizer, criterion and learning rate scheduler for model training.","b7a3187c":"An integer is set as a random state used in Numpy, Scikit-learn, PyTorch, etc. Furthermore, `manual_seed` set to make independent sessions as reproducible as possible using this random state. PyTorch does not guarantee reproducibility between PyTorch commits, different platforms, individual commits or between GPU and CPU executions despite using identical seeds.","2275598d":"At this point the Fashion MNIST dataset is seperatated into features and labels as well as train and test. For this data to be used in PyTorch it has to be converted into *torch.Tensor*. And then wrap the train and test datasets into `torch.utils.data.dataset.TensorDataset`.","e742a3fd":"## Preprocessing\n\nThe labels are provided as strings in an array. Using `sklearn.preprocessing.LabelEncoder` these labels are reduced to integer values. This function reduce labels from unique string objects to integers.","c8c471cd":"### Confusion Matrix","043803d2":"## Validation\n\nBenchmark the model using different metrics and methods.\n\n### Generate prediction\n\nFirst step include generating *y_preds* for furher use.","442c3d34":"## Save model\n","6822ba64":"# Classification of objects\n\nThis notebook is an attempt to classify objects in Fashion MNIST.","334d51d3":"## Loading data","8f01c77a":"Defining some constants used to split the data and train the model. These are related to the data loading process in PyTorch, the optimizer, the loss function and the learning rate scheduler.","111d65ee":"Futhermore, the data currently values between 0 and 255 is scaled using `sklearn.preprocessing.MinMaxScale` to values between 0.0 and 1.0. The scaling is mathematically represented by\n\n$$\nX_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n$$","5b5b0780":"Checking GPU availability to utilize Compute Unified Device Architecture (CUDA) or Heterogeneous Interface for Portability (HIP) if available for GPGPU acceleration of training else the process fall back on CPU. The current model in this notebook is complex and may take days to run on a consumer CPU. Use of GPU acceleration is highly encouraged.","3e2d2478":"## Architecture\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural network, most commonly applied to analyze visual imagery. CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer."}}