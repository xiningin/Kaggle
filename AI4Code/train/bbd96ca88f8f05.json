{"cell_type":{"509f057b":"code","0a8d58b3":"code","7b97c44f":"code","8278ad10":"code","56c418cf":"code","73b7b1e2":"code","9292ad40":"code","6c19e7f8":"code","559967b7":"code","c308de91":"code","20ca23b1":"code","bec30e72":"code","4d54ec91":"code","d150da6e":"code","bb1eea04":"code","6cb6adda":"code","9f1c96de":"code","f0e5e626":"code","586a0607":"code","489d5195":"code","36ec534f":"code","9290f7ca":"code","6fad2fb6":"markdown","f86db7af":"markdown","0e1f4baf":"markdown","0a7be318":"markdown","20119d3f":"markdown","cdd0d165":"markdown","6f138a2e":"markdown","5c5fe426":"markdown","e918d53f":"markdown"},"source":{"509f057b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a8d58b3":"import numpy as np # linear algebra\nimport pandas as pd ","7b97c44f":"df_sa = pd.read_csv('..\/input\/articles-sharing-reading-from-cit-deskdrop\/shared_articles.csv')\ndf_sa.head()","8278ad10":"df_ui = pd.read_csv('..\/input\/articles-sharing-reading-from-cit-deskdrop\/users_interactions.csv')\ndf_ui","56c418cf":"df = df_ui","73b7b1e2":"df['COUNTER'] =1       #initially, set that counter to 1.\ngroup_data = df.groupby(['contentId','eventType'])['COUNTER'].sum().reset_index() #sum function\nprint(group_data)","9292ad40":"events_df = group_data.pivot_table('COUNTER', ['contentId'], 'eventType')","6c19e7f8":"events_df = events_df.fillna(0)\n","559967b7":"events_df","c308de91":"def label(row):\n   return (1* row['VIEW']) + (4*row['LIKE']) + (10*row['COMMENT CREATED']) +( 25*row['FOLLOW'] )+ (100*row['BOOKMARK'])\n\nevents_df['label'] = events_df.apply (lambda row: label(row), axis=1)\n      ","20ca23b1":"events_df","bec30e72":"events_df.describe()","4d54ec91":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(events_df, test_size=0.2)","d150da6e":"import seaborn as sns\nimport matplotlib.pyplot as plt \n\nsns.pairplot(train)","bb1eea04":"plt.figure(figsize=(16,12))\nsns.heatmap(train.corr(),annot=True,fmt=\".2f\")\n","6cb6adda":"train_X, train_Y = train.drop('label',axis = 1), train['label']\ntest_X, test_Y = test.drop('label',axis = 1), test['label']","9f1c96de":"from sklearn.linear_model import LinearRegression \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\n\n\nlr = LinearRegression()\nlr.fit(train_X,train_Y)\nlr.score(train_X,train_Y)\npredict_test = lr.predict(test_X)\n\n\nres = dict()\nmetrics = dict()\n\nres['lr'] = lr.coef_\nmetrics['lr'] = r2_score(test_Y, predict_test),explained_variance_score(test_Y,predict_test)\n\n","f0e5e626":"from sklearn import linear_model\nclf = linear_model.Lasso(alpha=0.1)\nclf.fit(train_X, train_Y)\npredict_clf = clf.predict(test_X)\nprint(clf.coef_)\nres['lasso'] = clf.coef_\nmetrics['lasso'] = r2_score(test_Y, predict_clf),explained_variance_score(test_Y,predict_clf)\n\n","586a0607":"from sklearn.linear_model import ElasticNetCV\n\nregr.fit(train_X, train_Y)\nElasticNetCV(cv=3, random_state=0)\nprint(regr.coef_)\npredict_regr = regr.predict(test_X)\n\nres['cv'] = regr.coef_\nmetrics['cv'] = r2_score(test_Y, predict_regr),explained_variance_score(test_Y,predict_regr)\n","489d5195":"print (\"Comparing coefficients of the features with Ground truth : array [100,10,25,4,1]\")\nfor r in res.items():\n    print(r)","36ec534f":"print (\"Comparing r2 and explained variance score of models with 1 as max value\")\nfor m in metrics.items():\n    print(m)","9290f7ca":"\nnew_post = [2,3,78,4,23]\n\nnew_post_f = np.array(new_post).reshape(1,-1)\n\npredict_new_post = clf.predict(new_post_f)\nprint(predict_new_post)","6fad2fb6":"The label\/ viralty score takes values from the range(1 to 7907) ","f86db7af":"Tring different models and comparing performace :\n\n- Linear Regression (without regularization)\n\n- Linear regression lasso regularization\n\n- Linear regresion k-fold cross validation","0e1f4baf":"\n\u25cb What features did you consider?\n\n\n- The features considered for the given problem include **number of bookmarks, comments created, follow, likes and view**s for given posts. The correlation of different features with target were analyzed. Since the number of features are smaller in number, there wasn't a need to compress the features.\n\n\u25cb What model did you use and why?\n\n- Different models including **linear regression(without regularization), lasso regression, k-fold elastic regression** were tested on a test dataset with 20% split of data. The linear regression gave a 100% of training and testing accuracy, 100% of r-squared and explained variance metric. The model was doubted to overfit the data and hence other regularization mrthods were used as lasso (99% of r-squared and explained variance), k-fold elastic CV (61% of r-squared and explained variance) with 3-fold cross validation. \n    The **lasso model is the most generalizable** among all the tested models\n    \n\u25cb What was your evaluation metric for this?\n\n- The data was preprocessed using Pandas to get count of features as like, follow, etc for the posts and generate labels using given function of 1* VIEW + 4*LIKE + 10*COMMENT + 25*FOLLOW + 100*BOOKMARK . The data was split into 80-20 train,test. Evaluation metrics used were **testing accuracy, r-squared\/explained variance**.\n\n\u25cb What features would you like to add to the model in the future if you had more\ntime?\n\n- If there was more time to work, I'd like to play around with compressed features and reduce the number of features used through PCA (pricipal component analysis)\n\n\u25cb What other things would you want to try before deploying this model in\nproduction.\n\n- I'd like to consider the **frequency of model calls, latency requirements, computing capabilities and costs to deploy and mantain**. I'd also like to think about the results **being individually or batch processed**. A\/B testing on a smaller group for early results before deploying to test with user engagement results against the predictions will also be helpful for more insights. ","0a7be318":"Modifying the table to expand for the features and their count through pivot table","20119d3f":"Generating label column as ground truth values for our training data","cdd0d165":"80-20 train-test split, features EDA analysis ","6f138a2e":"Label is correlated with : bookmark, like, view, follow and comment created in decreasing order. \n","5c5fe426":"Making prediction on new posts","e918d53f":"Comparing model performace "}}