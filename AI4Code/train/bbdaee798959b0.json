{"cell_type":{"79327726":"code","0539807d":"code","d2686d4b":"code","79f62e4d":"code","4d8bf7ef":"code","91d6d866":"code","64e1661f":"code","c54f881c":"code","77ed3cfa":"code","66b0162d":"code","b6822041":"code","83b33dff":"code","a6dad668":"code","f93d558f":"code","82aca025":"code","bf4c6d35":"code","20883ff1":"code","5cadad52":"code","67ba4def":"code","54aa122f":"code","ddf80cbf":"code","84b77812":"code","909152a3":"code","31cc4301":"code","c0d4fe48":"code","cf2ecf12":"code","f4970ae4":"code","e264424c":"code","59f1a9b7":"code","52ae8200":"code","a4d92f0f":"code","d53b679f":"code","788795e4":"code","c8cdf636":"code","7563d2c1":"code","672192fe":"code","5ac36e5b":"code","fe88be00":"code","00809220":"code","8c14ee2a":"code","9447b933":"code","3c0ef1f0":"code","87ee62ab":"code","dbb9b8ad":"code","112b2e2d":"code","4c4d57e5":"code","d0e26a15":"code","2cd61749":"code","fdaa89c3":"code","96b83740":"code","6b58f26a":"code","350c3b13":"code","a4d9ea68":"code","5343c332":"markdown","22202efd":"markdown","49dd81c7":"markdown","feded083":"markdown","5e630abb":"markdown","ec74b5ba":"markdown","afafa428":"markdown","db358e9b":"markdown","10238c5f":"markdown","1337b8ad":"markdown","0ab54b6d":"markdown","4cfdd98b":"markdown","4d7dc0d9":"markdown","6580c7a7":"markdown","1e512463":"markdown","4e7c1caf":"markdown","0a377aaf":"markdown","2f369669":"markdown","2b71a008":"markdown","fc2c1b91":"markdown","1143e3de":"markdown","c0372cd7":"markdown"},"source":{"79327726":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#modeling\nfrom subprocess import check_output\nfrom sklearn.svm import SVC\nfrom sklearn import svm, neighbors\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomForestClassifier, VotingClassifier, AdaBoostClassifier,\nGradientBoostingClassifier,ExtraTreesClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nimport xgboost as xbg\n\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n# Any results you write to the current directory are saved as output.","0539807d":"nulls=df_train.isnull().sum()\nprint(nulls)\n","d2686d4b":"print(df_test.isnull().sum())\n#(\ucd94\ud6c4 fare \ud55c\uac1c\ub9cc \ub530\ub85c \ucc44\uc6cc\uc8fc\uae30)","79f62e4d":"length = len(df_train)\npercentage = (nulls\/length)*100\nprint (percentage)","4d8bf7ef":"#plotting\nx = percentage.values\ny = np.array(percentage.index)\n\nplt.figure(figsize=(16, 5))\nsns.set(font_scale=1.2)\nax = sns.barplot(y, x, palette='hls', log=False)\nax.set(xlabel='Feature', ylabel='(Percentage of Nulls)', title='Number of Nulls')","91d6d866":"df_train.head()","64e1661f":"for i in np.array(df_train.columns):\n    print (\"{0} has {1} attributes\".format(i, len(df_train[i].unique())))","c54f881c":"df = pd.concat([df_train.drop(\"Survived\", axis = 1), df_test], ignore_index = True)\nlabel = df_train[\"Survived\"]\nindex = df_train.shape[0]","77ed3cfa":"df[\"Ticket_number\"] = df[\"Ticket\"].apply(lambda x: x.split()[-1])\ndf[\"Ticket_code\"] = df[\"Ticket\"].apply(lambda x: x.split()[0] if len(x.split())!= 1 else \"No Code\")\ndf[\"Ticket_code\"].unique()","66b0162d":"df['Ticket_code'].value_counts()","b6822041":"df[\"Ticket_code\"]= df[\"Ticket_code\"].apply(lambda a: a[:-1] if a[-1]==\".\" else a)\ndf['Ticket_code'].value_counts()","83b33dff":"#\/\uc640 . \ubaa8\ub450 \ubd84\ub9ac\nimport re\ncodes= [i for i in df[\"Ticket_code\"].unique() if i!= \"No Code\"]\ndef split_codes(code):\n    return re.split('[^a-zA-Z0-9]+', code)\n    \nnew_codes = []\nfor i in codes:\n    for j in  split_codes(i):\n        new_codes.append(j)\n        \npd.Series(new_codes).value_counts()","a6dad668":"#\/\ub9cc \ubd84\ub9ac\ndef split_codes2(code):\n    return re.split('\/+', code)\n    \nnew_codes2 = []\nfor i in codes:\n    for j in  split_codes2(i):\n        new_codes2.append(j)\n        \npd.Series(new_codes2).value_counts()","f93d558f":"#.\ub9cc \ubd84\ub9ac\ndef split_codes3(code):\n    return re.split('\\.+', code)\n    \nnew_codes3 = []\nfor i in codes:\n    for j in  split_codes3(i):\n        new_codes3.append(j)\n        \npd.Series(new_codes3).value_counts()","82aca025":"df[\"Has_ticket_codes\"] = df[\"Ticket_code\"].apply(lambda x: 0 if x==\"No Code\" else 1)\ndf[\"Has_only_1_codes\"] = df[\"Ticket_code\"].apply(lambda x: 0 if len(re.split('[^a-zA-Z0-9]+', x)) !=1 else 1)\ndf[\"Number_of_codes\"] = df[\"Ticket_code\"].apply(lambda x: 0 if x==\"No Code\" else len(re.split('[^a-zA-Z0-9]+', x)))\n\ndf[\"Ticket_code_HEAD\"] = df[\"Ticket_code\"].apply(lambda x: \"No Code\" if x == \"No Code\" else re.split('[^a-zA-Z0-9]+', x)[0])\ndf[\"Ticket_code_TAIL\"] = df[\"Ticket_code\"].apply(lambda x: \"No Code\" if x == \"No Code\" else re.split('[^a-zA-Z0-9]+', x)[-1])\n\ndf['Ticket_code_HEAD'].value_counts()","bf4c6d35":"df['Ticket_code_TAIL'].value_counts()","20883ff1":"df[\"Name\"]\ndf[\"Initial\"] = df.Name.str.extract('([A-Za-z]+)\\.')","5cadad52":"pd.crosstab(df[\"Initial\"], df[\"Sex\"]).T.style.background_gradient(cmap='summer_r')","67ba4def":"df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)","54aa122f":"train = df[:length]\ntrain[\"Survived\"] = label\ntrain.groupby('Initial').mean()[\"Survived\"].plot.bar()","ddf80cbf":"train.groupby('Initial').mean()","84b77812":"print (\"{} percent of Cabin data is null\".format(df[\"Cabin\"].isnull().sum()\/len(df)*100))","909152a3":"train_cabin = train[train[\"Cabin\"].notnull()]\ntrain_cabin[\"Cabin_Initial\"] = train_cabin[\"Cabin\"].apply(lambda x: x[0])\ntrain_cabin[\"Cabin_Number\"] = train_cabin[\"Cabin\"].apply(lambda x: (x[1:].split(\" \")[0]))\ntrain_cabin[\"Cabin_Number\"].replace(\"\", -1, inplace = True)\ntrain_cabin[\"Cabin_Number\"] = train_cabin[\"Cabin_Number\"].apply(lambda x: int(x))\n\ntrain_cabin[\"Cabin_Initial\"].value_counts()","31cc4301":"train_cabin.groupby(\"Cabin_Initial\").mean()","c0d4fe48":"train_cabin.groupby(\"Cabin_Initial\").mean()[\"Survived\"].plot.bar()","cf2ecf12":"df_cabin = df[df[\"Cabin\"].notnull()]\ndf_cabin[\"Cabin_Initial\"] = df_cabin[\"Cabin\"].apply(lambda x: x[0])\ndf_cabin[\"Cabin_Number\"] = df_cabin[\"Cabin\"].apply(lambda x: (x[1:].split(\" \")[0]))\ndf_cabin[\"Cabin_Number\"].replace(\"\", -1, inplace = True)\ndf_cabin[\"Cabin_Number\"] = df_cabin[\"Cabin_Number\"].apply(lambda x: int(x))\n\ndf_cabin.groupby(\"Cabin_Initial\").mean()","f4970ae4":"#Heatmap \uadf8\ub824\ubcf4\uae30\ndf_cabin_heatmap = df_cabin[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin_Number\", \"Cabin_Initial\"]]\ndf_cabin_heatmap['Cabin_Initial'] = df_cabin_heatmap['Cabin_Initial'].map({'A': 0, 'B': 1, \"C\":2, \"D\":3, \"E\":4, \"F\":5, \"G\":7, \"H\":8})\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14, 12))\nplt.title('Correlation, y=1.05, size=15')\nsns.heatmap(df_cabin_heatmap.astype(float).corr(), linewidths=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n","e264424c":"df[\"Embarked\"].isnull().sum()","59f1a9b7":"df['Embarked'].fillna('S', inplace=True)","52ae8200":"from sklearn import preprocessing\n#Drop non-using columns\ndf = df.drop([\"Name\", \"Ticket\"], axis = 1)\ncategorical = [\"Sex\", \"Embarked\",\"Ticket_code\", \"Ticket_code_HEAD\", \"Ticket_code_TAIL\", \"Initial\"] \n\nlbl = preprocessing.LabelEncoder()\nfor col in categorical:\n    df[col].fillna('Unknown')\n    df[col] = lbl.fit_transform(df[col].astype(str))\ndf.head()","a4d92f0f":"df.groupby(\"Initial\").mean()","d53b679f":"#Initial \uc744 \uac00\uc9c0\uace0 Fillna\ndf[\"Age\"] = df.groupby(\"Initial\").transform(lambda x: x.fillna(x.mean()))[\"Age\"]","788795e4":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf[\"Isalone\"] = df[\"FamilySize\"].apply(lambda x: 0 if x!=1 else 1)\ndf= df.drop(\"Ticket_number\", axis = 1)\ndf['Fare'].fillna((df['Fare'].median()), inplace=True)\n\n\n#Age \/ Fare Category \ucd94\uac00\ud574\ubcf4\uae30\n\ndf.head()","c8cdf636":"#Cabin Initial imputation\ndf[\"Has_Cabin\"] = df[\"Cabin\"].apply(lambda x: 0 if type(x)==float else 1)\ndf[\"Cabin_Initial\"] = df[\"Cabin\"].apply(lambda x: x[0] if pd.notnull(x) else x)\ndf[\"Cabin_number\"] = df[\"Cabin\"].apply(lambda x: x[-1] if pd.notnull(x) else x)\ndf = df.drop(\"Cabin\", axis = 1)","7563d2c1":"#Imputation of Cabin initial\ntrain_cabin = df[df[\"Cabin_Initial\"].notnull()]\ntest_cabin = df[df[\"Cabin_Initial\"].isnull()]\n\ntr = train_cabin.drop([\"Cabin_Initial\", \"Cabin_number\"], axis = 1)\nla = train_cabin[\"Cabin_Initial\"]\nte = test_cabin.drop([\"Cabin_Initial\", \"Cabin_number\"], axis = 1)\nclf = VotingClassifier([('lsvc',svm.LinearSVC()),\n                        ('knn',neighbors.KNeighborsClassifier()),\n                        ('rfor',RandomForestClassifier()),\n                        ('lr',LogisticRegression()),\n                        ('LDA',LinearDiscriminantAnalysis()),\n                        ('DC',DecisionTreeClassifier()),\n                        ])\n\n\nclf.fit(tr, la)\npredicted_initial = clf.predict(te)\nte[\"predicted_inital\"] = predicted_initial\n\nfor i in range(0, len(df)):\n    if type(df[\"Cabin_Initial\"].iloc[i])==float:\n        df.ix[i, \"Cabin_Initial\"] = te[\"predicted_inital\"].ix[i]\n\ndf[\"Cabin_Initial\"] = lbl.fit_transform(df[col].astype(str))\ndf = df.drop([\"Cabin_number\",\"PassengerId\"], axis = 1)\n","672192fe":"colormap = plt.cm.RdBu\nplt.figure(figsize = (14, 12))\nplt.title(\"Correlation- Pearson\")\nsns.heatmap(df.corr(), square = True, cmap = colormap, annot=True)","5ac36e5b":"ntrain = length\nntest = len(df)-length\n\ny_train = label.ravel()\nx_train  = df[:length].values\nx_test = df[length:].values\n\nSEED = 0\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS)","fe88be00":"#\ub0b4\uac00 \ucc38\uace0\ud55c \ucee4\ub110\uc740 \uac01 \ubaa8\ub378\uc744 \ub354 \uc6a9\uc774\ud558\uac8c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud558\uc600\ub2e4\nclass SklearnHelper(object):\n    def __init__(self, clf, seed = 0, params=None):\n        params[\"random_state\"] = seed\n        self.clf = clf(**params)\n        \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n    \n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self, x, y):\n        return self.clf.fit(x, y)\n        \n    def feature_importances(self, x, y):\n        importance = []\n        for i in self.clf.fit(x, y).feature_importances_:\n            importance.append(i)\n        return importance\n        ","00809220":"def get_oof(clf, X, y, X_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(X)):\n        print('\\nFold {}'.format(i))\n        x_tr = X[train_index]\n        y_tr = y[train_index]\n        x_te = X[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(X_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n#Out of Fold \ub97c \uac00\uc9c0\uace0 KFold\ub85c Predict\ud55c \ub4a4 \uadf8\uac83\ub4e4\uc758 \ud3c9\uade0\uc744 \ud1b5\ud574 OOF Predicion\uc744 \uc0b0\ucd9c\ud568","8c14ee2a":"#Parameter \uc124\uc815\n#Random Forest\nrf_params = {\n    \"n_jobs\": -1,\n    \"n_estimators\": 500,\n    \"warm_start\": True,\n    #\"max_features\":0.2,\n    \"max_depth\":6,\n    \"min_samples_leaf\": 2, \n    \"max_features\": \"sqrt\",\n    \"verbose\":0\n}\n\n#Extra Trees\net_params = {\n    \"n_jobs\": -1,\n    \"n_estimators\": 500,\n    #\"max_features\":0.5,\n    \"max_depth\":8,\n    \"min_samples_leaf\": 2, \n    \"verbose\":0\n}\n\n#AdaBoost\nada_params = {\n    \"n_estimators\" : 500,\n    \"learning_rate\" : 0.75\n}\n\n#Gradient Boosting\ngb_params = {\n    \"n_estimators\":500,\n    #\"max_features\" : 0.2\n    \"max_depth\" : 5,\n    \"min_samples_leaf\" : 2,\n    \"verbose\" : 0\n}\n\n#SVC -Support Vector Classifier\n\nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","9447b933":"rf = SklearnHelper(clf = RandomForestClassifier, seed = SEED, params = rf_params)\net = SklearnHelper(clf = ExtraTreesClassifier, seed = SEED, params = et_params)\nada = SklearnHelper(clf = AdaBoostClassifier, seed = SEED, params = ada_params)\ngb = SklearnHelper(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)\n\nsvc = SklearnHelper(clf = SVC, seed = SEED, params = svc_params)","3c0ef1f0":"#First Level Prediction - OOF train and test\nprint (\"Generating OOFs\")\n\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","87ee62ab":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","dbb9b8ad":"cols = df[:length].columns.values\nfeature_df = pd.DataFrame({\n    \"features\":cols,\n    'Random Forest feature importances': rf_feature,\n     'Extra Trees  feature importances': et_feature,\n      'AdaBoost feature importances': ada_feature,\n    'Gradient Boost feature importances': gb_feature\n})","112b2e2d":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_df['Random Forest feature importances'].values,\n    x = feature_df['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_df['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_df['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_df['Extra Trees  feature importances'].values,\n    x = feature_df['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_df['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_df['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_df['AdaBoost feature importances'].values,\n    x = feature_df['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_df['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_df['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_df['Gradient Boost feature importances'].values,\n    x = feature_df['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_df['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_df['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","4c4d57e5":"feature_df[\"mean\"] = feature_df.mean(axis = 1)\nfeature_df.head()","d0e26a15":"\ndata =[\n    go.Bar(\n    x =feature_df[\"features\"].values,\n    y = feature_df[\"mean\"].values\n    )\n]\n\nlayout = go.Layout(\n    title = \"Feature Importance-Mean\",\n    yaxis =dict(\n        title = \"Importance\", \n    )\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = \"BAR\")","2cd61749":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train[:,0],\n     'ExtraTrees': et_oof_train[:,0],\n     'AdaBoost': ada_oof_train[:,0],\n      'GradientBoost': gb_oof_train[:,0]\n    })\nbase_predictions_train.head(10)\n\ndf.head()","fdaa89c3":"#Train\uc5d0 ADD\ud558\uae30\nx_train = np.concatenate((x_train, et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis = 1)\nx_test = np.concatenate((x_test, et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","96b83740":"from sklearn.model_selection import train_test_split\nxtrain, xvalid, ytrain, yvalid = train_test_split(\n      x_train, y_train, test_size=0.30, random_state=5)","6b58f26a":"#VotingClasifier\nfrom sklearn.linear_model import LogisticRegression\nfrom subprocess import check_output\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm, neighbors\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nclf = VotingClassifier([('lsvc',svm.LinearSVC()),\n                        ('knn',neighbors.KNeighborsClassifier()),\n                        ('rfor',RandomForestClassifier()),\n                        ('lr',LogisticRegression()),\n                        ('LDA',LinearDiscriminantAnalysis()),\n                        ('DC',DecisionTreeClassifier()),\n                        ('GB',GradientBoostingClassifier()),\n                        ('XGB',XGBClassifier()),\n                        ('Ada', AdaBoostClassifier()),\n                        ('GNB', GaussianNB())\n                        ])\nclf.fit(xtrain, ytrain)\nconfidence = clf.score(xvalid, yvalid)\nprint('Confidence: ',confidence)\n","350c3b13":"predictions = clf.predict(x_test)","a4d9ea68":"submission = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n                    'Survived': predictions})\nsubmission.to_csv('Ensemble_with_OOF_190131_ver7.csv', index=False)","5343c332":"\uc774\uc678\uc5d0\ub3c4 SibSp, Parch \ub294 \uc774\ub2c8\uc15c\uacfc \uc5b4\ub290\uc815\ub3c4\uc758 \uad00\uacc4\ub97c \uac00\uc9c0\ub294 \uac83\uc73c\ub85c \ubcf4\uc784","22202efd":"Modeling","49dd81c7":"Null Value\uac00 Cabin\uc758 \uacbd\uc6b0 \uc57d 77\ud504\ub85c\ub85c \uc0c1\ub2f9\ud55c \uc591\uc758 \ub370\uc774\ud130\uac00 \uc720\uc2e4 (\ud639\uc740 \uc5c6\uc74c) \ub418\uc5c8\uc74c\uc744 \uc54c \uc218 \uc788\uace0, age\uc758 \uacbd\uc6b0 \uc57d 20\ud504\ub85c\uc784. Embarked\uc758 \uacbd\uc6b0 0.22\ud504\ub85c\ub85c \ubbf8\ubbf8\ud55c \uc218\uc900\uc784\uc744 \uc54c \uc218 \uc788\uc73c\uba70 \ub098\uba38\uc9c0 column\uc5d4 \ubaa8\ub4e0 \ub370\uc774\ud130\uac00 \uc874\uc7ac. ","feded083":"**\uae30\ud0c0 Atribute \uc0dd\uc131**\n(\ucd94\ud6c4 \ucd94\uac00\uc608\uc815)","5e630abb":"**Cabin Initial** \uacfc **Fare**\uc758 \uad00\uacc4\uac00 \uc2ec\uc0c1\uce58 \uc54a\uc544\ubcf4\uc778\ub2e4. Cabin number\ub294 \ud638\uc2e4 \ubc88\ud638\uc774\uc9c0\ub9cc Cabin Initial \uc740 \uc120\ubc15 \ub0b4 \uc120\uc2e4\uc758 \uc704\uce58, \ub4f1\uae09 \ub4f1\uacfc \uad00\ub828\uc774 \uc788\uc744 \uac00\ub2a5\uc131\uc774 \ud06c\ubbc0\ub85c Null valu\ub97c \uaf2d \ucc44\uc6cc Attribute\uc73c\ub85c \uc0ac\uc6a9\ud558\uba74 \uc88b\uc744 \ub4ef \ud558\ub2e4. ","ec74b5ba":"**Name**\n\ub2e4\uc74c\uc73c\ub85c \uc774\ub984\uc5d0 \ud3ec\ud568\ub41c \uc815\ubcf4\ub97c \ucd94\ucd9c\ud574\ubcf4\uaca0\ub2e4. \n\uc0dd\uc874\uacfc \uad00\ub828\ud574\uc11c \uc774\ub984\uc73c\ub85c\ubd80\ud130 \ucd94\ucd9c\ud560 \uc218 \uc788\uc744 \ubc95 \ud55c \uc815\ubcf4\ub294 \uc6b0\uc120 \ud638\uce6d (\uc131\ubcc4 \ub4f1\uc758 \uc815\ubcf4 \ud3ec\ud568 \uac00\ub2a5)\uc774 \uc788\uc744 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.\n*\uc774 \ubd80\ubd84\uc740 Baseine kernal : https:\/\/www.kaggle.com\/youhanlee\/youhan-s-baseline \uc744 \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4. ","afafa428":"- \ud558\ub098\uc758 \ucf54\ub4dc\ub9cc \uae30\uc7ac\ub41c \uacbd\uc6b0\uc640 \ub458 \uc774\uc0c1\uc758 \ucf54\ub4dc\uac00 \uae30\uc7ac\ub418\uc5b4\uc788\ub294 \uacbd\uc6b0\ub85c \ub098\ub20c \uc218 \uc788\ub2e4. \n- \ub458 \uc774\uc0c1\uc758 \ucf54\ub4dc\uac00 \uae30\uc7ac\ub41c \uacbd\uc6b0, \/ \ud639\uc740 . \ub97c \uae30\uc900\uc73c\ub85c \uc55e\/\ub4a4\ub85c \uc9c0\uba85\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc73c\ub85c \ucd94\uc815\ub418\ub294 \ucf54\ub4dc\uac00 \ub098\ub258\uc5b4 \uc788\uc73c\uba70,  \/ \uc640 . \uac00 \ud63c\uc6a9\ub418\uc5b4 \uc788\ub294 \ub4ef \ubcf4\uc778\ub2e4. \n- \uac19\uc740 \uc9c0\uba85\uc774\ub77c\ub3c4 \ud45c\uae30\uac00 \ub2e4\ub974\uac8c \ub41c \uacbd\uc6b0\uac00 \uc788\ub294 \ub4ef \ud558\ub2e4. SOTON, STON \ub4f1\uc774 \uadf8 \uc608\uc774\uba70, C.A.\uc640 \uac19\uc740 \uacbd\uc6b0 CA \uc640 \uac19\uc740 \ucf54\ub4dc\uc778\uc9c0, \uc544\ub2c8\uba74 C\uc640 A\ub77c\ub294 \ub450 \uac1c\uc758 \ucf54\ub4dc\ub97c '.' \uad6c\ubd84\uc790\ub97c \uc0ac\uc6a9\ud574 \ub098\ub208\uac83\uc778\uc9c0 \uc54c\uae30 \uc5b4\ub835\ub2e4. \n\n\uc123\ubd80\ub978 \ud310\ub2e8\uc744 \ud558\uae30\uc5d0 \uc815\ubcf4\uac00 \ubd80\uc871\ud558\ub2c8 \uc77c\ub2e8 \uba85\ud655\ud558\uac8c **\uc77c\uce58\ud558\ub294 \ucf54\ub4dc\uba85\uc774\ub77c \uc5ec\uaca8\uc9c0\ub294 \uac83\ub4e4\uc744 \ucc98\ub9ac. \uc6b0\uc120 \ucf54\ub4dc\uba85\uc758 \ub9e8 \ub4a4\uc5d0 \ubd99\ub294 '.'\ub294 \uc81c\uac70\ud574\ub3c4 \ub420 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. **","db358e9b":"**Cabin \uc0b4\ud3b4\ubcf4\uae30**\n\nCabin\uc740 \ub9ce\uc740 \uc218\uc758 \ub370\uc774\ud130\uac00 \uc5c6\uc9c0\ub9cc, \ud0d1\uc2b9\uc790\uac00 \uc704\uce58\ud574\uc788\ub358 \uc88c\uc11d\/\ud638\uc2e4\uc758 \uc704\uce58\uc640 \uc0dd\uc874 \uc5ec\ubd80\uac00 \uad00\ub828\uc774 \uc788\uc744 \uc218 \uc788\uc73c\ubbc0\ub85c \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uace0, Null Data\ub97c \ucc44\uc6b8 \uc218 \uc788\ub294 \uc2e4\ub9c8\ub9ac\ub97c \ucc3e\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4. ","10238c5f":"\uc5ec\uae30\uc11c Survived\ub294 label\uc774\uba70\n- **Pclass, Sex, Embarked**\ub294 \ud655\uc2e4\ud788 **Categorical feature**\ub77c \ubcfc \uc218 \uc788\uc74c. \n- **Name** \uacfc **Ticket, Cabin**\uc740 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcfc \ud544\uc694\uac00 \uc788\uc744 \uac83\uc73c\ub85c \ubcf4\uc774\uba70\n- **Age, SibSp, Parch, Fare**\ub294 \uc77c\ub2e8 **numeric value **\ubd10\ub3c4 \ub420 \uac83\uc774\ub77c \ubcf4\uc784. \n\n\uba3c\uc800 Ticket \uacfc Name\uc744 \uc0b4\ud3b4\ubcf8 \ud6c4\n Null value\ub4e4\uc744 \ucd94\uc815\ud558\uc5ec \ucc44\uc6b0\uace0, \ucd94\uac00\uc801\uc73c\ub85c Add\ud560 \uc218 \uc788\ub294 Feature \uac00 \uc788\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uace0,\nCategorical feature\ub97c encoding \ud55c \ud6c4 \ubaa8\ub378\ub9c1\uc744 \uc9c4\ud589\ud558\ub3c4\ub85d \ud558\uaca0\ub2e4. ","1337b8ad":"**\ub9c8\uc9c0\ub9c9\uc73c\ub85c Age \uc758 \uacb0\uce21\uce58\ub97c \ucc44\uc6cc\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4. Age\ub97c \uac00\ub2a0\ud560 \uc218 \uc788\ub294 \uc694\uc18c\ub294 \uac00\uc871\uc758 \uc218 \ub610\ub294 Initial\uc774 \uc788\ub2e4. **","0ab54b6d":"\ub2e4\uc74c\uc73c\ub85c, '\/'\uc640 '.' \uac01\uac01, \ub610 \ub458 \ubaa8\ub450\ub97c \uae30\uc900\uc73c\ub85c \ucf54\ub4dc\ub97c \ub098\ub204\uc5c8\uc744 \ub54c \uac1c\ubcc4\uc801\uc73c\ub85c \uc874\uc7ac\ud558\ub294 \ucf54\ub4dc\uac00 \ubb34\uc5c7\uc774 \uc788\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\ub2e4. (Ex: SC\/A.3  -> SC\uc640 A.3, SC\uc640 A \uc640 3, \ub4f1)\n**\uc774 \ubd80\ubd84\uc744 \ub354 \ud6a8\uc728\uc801\uc774\uace0 \uc815\ud655\ud558\uac8c \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc544\uc774\ub514\uc5b4\uac00 \uc788\ub2e4\uba74 \uacf5\uc720\ubc14\ub78d\ub2c8\ub2e4**","4cfdd98b":"\uc815\ubcf4\uac00 \ubd80\uc871\ud574 \uc123\ubd88\ub9ac \ud310\ub2e8\ud558\uae30 \uc5b4\ub835\uc9c0\ub9cc, \uc77c\ub2e8 \uac00\uc7a5 \uacf5\ud1b5\uc801\uc73c\ub85c \ub098\ud0c0\ub098\ub294 \ucf54\ub4dc\uac00 \ub9ce\uc740 \ubc29\ubc95\uc744 \ud1b5\ud574 (\/\uc640 . \ubaa8\ub450 \ubd84\ub9ac) Code\ub97c \ubd84\ub9ac\ud574\ubcf4\uaca0\ub2e4. \/ \ub610\ub294 . \ub97c \ud1b5\ud574 \ub450 \uac00\uc9c0 \uc774\uc0c1\uc758 \ucf54\ub4dc\uac00 \uae30\uc7ac\ub418\uc5b4\uc788\ub294 \uacbd\uc6b0 \uccab \ubc88\uc9f8 \ucf54\ub4dc\ub9cc\uc744 \ucd94\ucd9c\ud574 \ud558\ub098\uc758 Attribute\uc73c\ub85c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\ub2e4. ","4d7dc0d9":"Cabin \uc758 Initial\uc740 \uc774\ud6c4 imputation\uc744 \ud1b5\ud574 \uc608\uce21\ud558\uc5ec \ucc44\uc6cc Attribute\uc73c\ub85c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud558\uaca0\ub2e4.","6580c7a7":"\ud2f0\ucf13 \ubc88\ud638 \uc55e\uc758 \ucf54\ub4dc\ub294 \ucd9c\ubc1c\uc9c0, \ub3c4\ucc29\uc9c0\ub098 \uae30\ud0c0 \ud0d1\uc2b9\uc790\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ub2f4\uace0\uc788\uc744 \ud655\ub960\uc774 \ud06c\ub2e4.  \ud55c\ubc88 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\ub2e4","1e512463":"**Ticket**\n\ud2f0\ucf13\uc758 \uc55e\ubd80\ubd84\uc5d0 \ucf54\ub4dc\uba85\uc73c\ub85c \ud45c\uc2dc\ub41c \uac83\uacfc \ud2f0\ucf13 \ubc88\ud638\ub97c \ubd84\ub9ac\ud574 \uc0b4\ud3b4\ubcf4\uaca0\ub2e4.","4e7c1caf":"**\uba3c\uc800 \uac00\uc7a5 Null Value\uac00 \uc801\uc740 embarked\ubd80\ud130. **\ucd1d 2\uac1c\ubfd0\uc774 Null value\uc774\ubbc0\ub85c \uac00\uc7a5 \ub9ce\uc740 \uc218\ub97c \ucc28\uc9c0\ud558\ub294 \"S\" \ub85c \ub2e4\uccb4\ud558\uaca0\ub2e4. ","0a377aaf":"\uc0c1\ub2f9\uc218\uc758 \ub370\uc774\ud130\uac00 Miss \/ Mrs, MR\/Master\uc5d0 \uce58\uc911\ub418\uc5b4\uc788\uc74c\uc744 \ubcfc \uc218 \uc788\uc73c\uba70 \ub098\uba38\uc9c0\uc758 \uacbd\uc6b0\uc5d4 \uadf9\uc18c\uc218\ub85c, \uacb0\uacfc\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc744 \uac83\uc73c\ub85c \ubcf4\uc784. ","2f369669":"\ub2e8, Cabin Initial \uc774 \ud655\ubcf4\ub41c \uc804\uccb4 \ub370\uc774\ud130\uc758 \uc591\uc774 \ud06c\uc9c0 \uc54a\ub2e4\ub294\uc810\uc744 \uace0\ub824\ud574\uc57c \ud568. (\ud2b9\ud788 G, T\uc758 \uacbd\uc6b0)","2b71a008":"Nans (Null Data)\ucc3e\uc544\ubcf4\uae30","fc2c1b91":"**Null Value estimation**","1143e3de":"\ubcf8 \ucee4\ub110\uc740 \uacfc\uac70\uc5d0 \uae30\uc874 Kaggle Titanic Competition\uc5d0 \uc5f0\uc2b5\uc744 \uc704\ud574 \uc791\uc131\ud574\ubcf8 \ucee4\ub110\uc744 \ubc14\ud0d5\uc73c\ub85c Building\ud574\uc11c \uc791\uc131\ud588\uc2b5\ub2c8\ub2e4. \n\ud0c0\uc774\ud0c0\ub2c9 competition\uc758 \uacf5\uac1c\ub41c \ucee4\ub110\uc744 \ucc38\uace0\ud55c \ubd80\ubd84\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \n\n**Stacking \uc758 \ubaa8\ub378\ub9c1\uc5d0 \ub300\ud574\uc11c\ub294 :https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python \ub2e4\uc74c \ucee4\ub110\uc744 \ucc38\uace0\ud558\uc600\uace0, \uc0c1\ub2f9\ubd80\ubd84 \uc778\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4. **","c0372cd7":"**Categorical Data \ub97c \ub2e4\ub4ec\uace0 Label encoding \ud558\uae30**"}}