{"cell_type":{"59c9d533":"code","9096ebf8":"code","34286e69":"code","a8a6cf77":"code","99276202":"code","83618398":"code","eaf29d98":"code","5489dc47":"code","a8b972e5":"code","43dccd41":"code","9a1d3e2b":"code","83212ac8":"code","809d06f2":"code","8d7facc5":"code","2d3bd178":"code","603f6cd0":"code","da1d648c":"code","6754d83c":"code","3113370c":"code","40c11174":"code","a8d9a9b6":"code","c591c6fe":"code","5ed61f20":"code","79739fb3":"code","bf613309":"markdown","c07f0b22":"markdown","938b0e45":"markdown","b782d05f":"markdown","26279e7c":"markdown","d8f3b7a1":"markdown","fef4fde0":"markdown","0ed14f51":"markdown","d7c6ab95":"markdown","8d836fdf":"markdown","feaa286f":"markdown","87091ba1":"markdown","ab115818":"markdown"},"source":{"59c9d533":"import os, json\nimport pandas as pd\nimport re\nimport nltk","9096ebf8":"path_to_json = '..\/input\/web-scraped-text'\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]","34286e69":"#LIST OF .JSON FILES\njson_files","a8a6cf77":"text = []\nfor index, js in enumerate(json_files):\n    with open(os.path.join(path_to_json, js)) as json_file:\n        json_text = json.load(json_file)\n    text.append(json_text['text'])","99276202":"len(text)","83618398":"#STORING ORIGINAL TEXT\nraw_text = []\nfor i in range(10):\n    string = ' '.join(str(item) for item in text[i])\n    raw_text.append(string.strip().split('\\n'))   ","eaf29d98":"raw_text[0]","5489dc47":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.append('n')\nstop_words.append('r')","a8b972e5":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","43dccd41":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","9a1d3e2b":"from nltk.util import bigrams","83212ac8":"filtered_text = []     #TO STORE THE PROCESSED DATA\n\nbigrams = []    \nfor t in raw_text:\n    filtered_sentence = \"\"\n    stemmed_list = []\n    lemmatized_list = []\n    \n    sentence = str(t)\n    \n    #Data Cleansing\n    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n    \n    #Tokenization\n    words = nltk.word_tokenize(sentence)\n    \n    #Stop words removal\n    words = [w for w in words if not w in stop_words]\n    \n    #Stemming\n    for word in words:\n        stemmed_word = stemmer.stem(word)\n        stemmed_list.append(stemmed_word)\n        \n    #Lemmatization\n    for s_word in stemmed_list:\n        lemmatized_word = lemmatizer.lemmatize(s_word)\n        lemmatized_list.append(lemmatized_word)\n    \n    #Bigram Formation\n    bigrams = list(nltk.bigrams(lemmatized_list))\n    bigram_text=[\" \".join(t) for t in bigrams]\n    \n    filtered_text.append(bigram_text)  ","809d06f2":"len(filtered_text)","8d7facc5":"len(filtered_text[9])","2d3bd178":"table_head = [\"Raw Data\", \"Processed Data\"]","603f6cd0":"df_data = pd.DataFrame(zip(raw_text,filtered_text), columns = table_head)\ndf_data","da1d648c":"from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer()\nBoW_list = []\nfor i in range(10):\n    x = vec.fit_transform(filtered_text[i])\n    BoW_list.append(x)","6754d83c":"BoW_list","3113370c":"len(BoW_list)","40c11174":"table_head.append(\"Bag of Words\")\ndf_data = pd.DataFrame(zip(raw_text,filtered_text, BoW_list), columns = table_head)\ndf_data","a8d9a9b6":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nTfidf_list = []\nfor i in range(10):\n    x = vectorizer.fit_transform(filtered_text[i])\n    Tfidf_list.append(x)","c591c6fe":"Tfidf_list","5ed61f20":"len(Tfidf_list)","79739fb3":"table_head.append(\"Tfidf\")\ndf_data = pd.DataFrame(zip(raw_text,filtered_text, BoW_list, Tfidf_list), columns = table_head)\ndf_data","bf613309":"Term Frequency Inverse Document Frequency (TF-IDF)","c07f0b22":"CONVERTING DATA TO PANDAS DATAFRAME","938b0e45":"**FOR STEMMING","b782d05f":"**FOR BIGRAM FORMATION","26279e7c":"**FOR LEMMATIZATION","d8f3b7a1":"## EMBEDDING TECHNIQUES","fef4fde0":"EXTRACTING TEXT FROM EACH .JSON FILE","0ed14f51":"BAG OF WORDS","d7c6ab95":"### TEXT PROCESSING","8d836fdf":"### COLLECTING .JSON FILES","feaa286f":"### NLP PIPELINE","87091ba1":"### IMPORTING THE LIBRARIES","ab115818":"**TO REMOVE STOP WORDS"}}