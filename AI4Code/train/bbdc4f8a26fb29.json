{"cell_type":{"d22501eb":"code","c1eb66d3":"code","0a9c06f2":"code","3c148db3":"code","d9a64c3f":"code","cdf436e9":"code","e6b3f516":"code","2ec21b52":"code","3363f5a5":"code","7714859c":"code","b76d9d86":"code","744fb8e8":"code","b6715189":"code","b4ee029f":"code","7fe8a3bf":"code","73bce13d":"code","ac38184c":"code","ef5ef8dc":"code","edaaa95e":"code","68393478":"code","ae1addf7":"code","be318e78":"code","b2bce13d":"code","69f3dc64":"code","193b8da6":"code","bc20f1b8":"code","d768f877":"code","3597263c":"code","070dafbb":"code","4a2d575e":"code","73574c27":"code","29f7f39a":"code","f21ed9e9":"code","87f77650":"code","e4987cc1":"code","997871a8":"code","71834c6b":"code","0d92d9f3":"code","2c7a7fd9":"code","3ca3015b":"code","d7616882":"code","f00705d6":"code","95032bc5":"code","43fa5b1b":"code","bc89d855":"code","4bfec8cf":"markdown","a58e4949":"markdown","657bfd5b":"markdown","a32e75cf":"markdown","e6ab22a0":"markdown","9814f9bb":"markdown","9d2feb7e":"markdown","1ff5ecf0":"markdown"},"source":{"d22501eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport regex as re\nimport gc\n# Any results you write to the current directory are saved as output.","c1eb66d3":"baseline_tree_score = 0.23092278864723115\nbaseline_neuralnetwork_score = 0.5480561937041435","0a9c06f2":"train = pd.read_csv('..\/input\/kaggletutorial\/covertype_train.csv')\ntest = pd.read_csv('..\/input\/kaggletutorial\/covertype_test.csv')","3c148db3":"train_index = train.shape[0]","d9a64c3f":"lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.06,\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.7,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}","cdf436e9":"def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef keras_history_plot(history):\n    plt.plot(history.history['loss'], 'y', label='train loss')\n    plt.plot(history.history['val_loss'], 'r', label='val loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper right')\n    plt.show()","e6b3f516":"def baseline_tree_cv(train):\n    train_df = train.copy()\n    y_value = train_df[\"Cover_Type\"]\n    del train_df[\"Cover_Type\"], train_df[\"ID\"]\n    \n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_iteration = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        evals_result_dict = {} \n        dtrain = lgbm.Dataset(train_x, label=train_y)\n        dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n        clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=3000, valid_sets=[dtrain, dvalid],\n                               early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n        predict = clf.predict(valid_x)\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_iteration = max(best_iteration, clf.best_iteration)\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        lgbm.plot_metric(evals_result_dict)\n        plt.show()\n        \n    print(\"Best Iteration\", best_iteration)\n    print(\"Total LogLoss\", total_score \/ NFOLD)\n    print(\"Baseline model Score Diff\", total_score \/ NFOLD - baseline_tree_score)\n    \n    del train_df\n    \n    return best_iteration\n\ndef baseline_keras_cv(train):\n    train_df = train.copy()\n    y_value = train_df['Cover_Type']\n    del train_df['Cover_Type'], train_df['ID']\n    \n    model = keras_model(train_df.shape[1])\n    callbacks = [\n            EarlyStopping(\n                patience=10,\n                verbose=10)\n        ]\n\n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_epoch = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                            verbose=1, callbacks=callbacks)\n\n        keras_history_plot(history)\n        predict = model.predict(valid_x.values)\n        null_count = np.sum(pd.isnull(predict) )\n        if null_count > 0:\n            print(\"Null Prediction Error: \", null_count)\n            predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_epoch = max(best_epoch, np.max(history.epoch))\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        \n    print(\"Best Epoch: \", best_epoch)\n    print(\"Total LogLoss\", total_score\/NFOLD)\n    print(\"Baseline model Score Diff\", total_score\/NFOLD - baseline_neuralnetwork_score)","2ec21b52":"def outlier_binary(frame, col, outlier_range):\n    outlier_feature = col + '_Outlier'\n    frame[outlier_feature] = 0\n    frame.loc[frame[col] > outlier_range, outlier_feature] = 1\n    return frame\n\ndef outlier_divide_ratio(frame, col, outlier_range):\n    outlier_index = frame[col] >= outlier_range\n    outlier_median =  frame.loc[outlier_index, col].median()\n    normal_median = frame.loc[frame[col] < outlier_range, col].median()\n    outlier_ratio = outlier_median \/ normal_median\n    \n    frame.loc[outlier_index, col] = frame.loc[outlier_index, col]\/outlier_ratio\n    return frame\n\ndef frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()\/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequncy'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')","3363f5a5":"def tree_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n\n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n    # all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n\n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n\n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n\n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n    \n    del all_data, predict, aspect_train, aspect_test\n    gc.collect()\n    \n    return train_df, test_df","7714859c":"train_df, test_df = tree_data_preprocessing(train, test)","b76d9d86":"all_data = pd.concat([train_df, test_df])","744fb8e8":"distance_feature = [col for col in train.columns if col.find('Distance') != -1 ]\ndistance_feature","b6715189":"all_data[distance_feature].head()","b4ee029f":"all_data['Horizontal_Distance_To_Hydrology'] = all_data['Horizontal_Distance_To_Hydrology']\/1000","7fe8a3bf":"sns.pairplot(all_data[distance_feature + ['Cover_Type']], hue='Cover_Type', x_vars=distance_feature, y_vars=distance_feature, size=3)\nplt.show()","73bce13d":"all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\nall_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\nall_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\nall_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Fire_Points']\n\nall_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\nall_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\nall_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\nall_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Roadways']\n\nall_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\nall_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\nall_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\nall_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Vertical_Distance_To_Hydrology']\n\nall_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\nall_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\nall_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\nall_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] \/ all_data['Horizontal_Distance_To_Roadways']\n\nall_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5","ac38184c":"np.isinf(all_data).sum()","ef5ef8dc":"all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\nall_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\nall_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\nall_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0","edaaa95e":"np.isinf(all_data).sum().sum()","68393478":"all_data.isnull().sum()","ae1addf7":"all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)","be318e78":"def target_disturibution(frame, col):\n    sns.distplot(frame.loc[frame['Cover_Type']==0, col])\n    sns.distplot(frame.loc[frame['Cover_Type']==1, col])\n    plt.title(col)\n    plt.show()","b2bce13d":"train_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","69f3dc64":"for col in train_df.columns:\n    if col.find('HF') != -1 or col.find('HH') != -1 or col.find('FR') != -1 or col.find('HR') != -1:\n        target_disturibution(train_df,col)","193b8da6":"baseline_tree_cv(train_df)","bc20f1b8":"other_numerical_feature = ['Aspect', 'Elevation', 'Slope', 'Hillshade_3pm', 'Hillshade_9am', 'Hillshade_Noon']","d768f877":"sns.pairplot(all_data[other_numerical_feature + ['Cover_Type']], hue='Cover_Type', x_vars=other_numerical_feature, y_vars=other_numerical_feature, size=3)\nplt.show()","3597263c":"sns.scatterplot('Elevation', 'Aspect', hue='Cover_Type',data=all_data)\nplt.show()","070dafbb":"def scatter_quantile_graph(frame, col1, col2):\n    col1_quantile = np.arange(0,1.1,0.1)\n    col2_quantile = np.arange(0,1.1,0.2)\n    \n    sns.scatterplot(col1, col2, hue='Cover_Type',data=frame)\n    for quantile_value in frame[col1].quantile(col1_quantile):\n        plt.axvline(quantile_value, color='red')\n    for quantile_value in frame[col2].quantile(col2_quantile):\n        plt.axhline(quantile_value, color='blue')\n\n    plt.title('{} - {}'.format(col1,col2))\n    plt.show()","4a2d575e":"scatter_quantile_graph(all_data, 'Elevation', 'Aspect')\nscatter_quantile_graph(all_data, 'Elevation', 'Slope')\nscatter_quantile_graph(all_data, 'Elevation', 'Hillshade_3pm')\nscatter_quantile_graph(all_data, 'Elevation', 'Hillshade_9am')\nscatter_quantile_graph(all_data, 'Elevation', 'Hillshade_Noon')","73574c27":"all_data_binning = all_data.copy()","29f7f39a":"quantile_10 = np.arange(0,1.1,0.1)\nquantile_5 = np.arange(0,1.1,0.2)\n    \nall_data_binning['Elevation_quantile_label'] = pd.qcut(\n                                            all_data_binning['Elevation'], \n                                            q=quantile_10, labels = ['Ele_quantile_{:.1f}'.format(col) for col in quantile_10][1:])\n\nall_data_binning['Aspect_quantile_label'] = pd.qcut(\n                                            all_data_binning['Aspect'], \n                                            q=quantile_5, labels = ['Aspect_quantile_{:.1f}'.format(col) for col in quantile_5][1:])","f21ed9e9":"all_data_binning['Ele_Asp_Combine'] = all_data_binning[['Elevation_quantile_label','Aspect_quantile_label']].apply(lambda row: row['Elevation_quantile_label'] +'_'+ row['Aspect_quantile_label'] ,axis=1)","87f77650":"all_data_binning['Ele_Asp_Combine'].nunique()","e4987cc1":"for col in ['Elevation_quantile_label','Aspect_quantile_label','Ele_Asp_Combine']:\n    all_data_binning[col] = all_data_binning[col].factorize()[0]","997871a8":"train_df_binning = all_data_binning.iloc[:train_index]\ntest_df_binning = all_data_binning.iloc[train_index:]","71834c6b":"baseline_tree_cv(train_df_binning)","0d92d9f3":"def binning_category_combine_feature(frame, col1, col2, col1_quantile, col2_quantile):\n    print(col1, ' ', col2, 'Bining Combine')\n    col1_quantile = np.arange(0,1.1,col1_quantile)\n    col2_quantile = np.arange(0,1.1,col2_quantile)\n    \n    col1_label = '{}_quantile_label'.format(col1)\n    frame[col1_label] = pd.qcut(frame[col1], q=col1_quantile, labels = ['{}_quantile_{:.1f}'.format(col1, col) for col in col1_quantile][1:])\n    \n    col2_label = '{}_quantile_label'.format(col2)\n    frame[col2_label] = pd.qcut(frame[col2], q=col2_quantile, labels = ['{}_quantile_{:.1f}'.format(col2, col) for col in col2_quantile][1:])\n    \n    combine_label = 'Binnig_{}_{}_Combine'.format(col1, col2)\n    frame[combine_label] = frame[[col1_label, col2_label]].apply(lambda row: row[col1_label] +'_'+ row[col2_label] ,axis=1)\n    for col in [col1_label, col2_label, combine_label]:\n        frame[col] = frame[col].factorize()[0]\n    \n    # del frame[col1_label], frame[col2_label]\n    gc.collect()\n    return frame, [col1_label, col2_label, combine_label]","2c7a7fd9":"all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1)\n\n\"\"\" \ub098\uba38\uc9c0 \ubd80\ubd84\uc740 Feature \ucd94\uac00\ud558\uba74\uc11c \uc131\ub2a5 \uac80\ud1a0 \ubc14\ub78d\ub2c8\ub2e4~!\nall_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Slope', 0.1, 0.2)\nfor col in new_col:\n    all_data = frequency_encoding(all_data, col)\n\nall_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Hillshade_3pm', 0.1, 0.2)\nfor col in new_col:\n    all_data = frequency_encoding(all_data, col)\n\nall_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Hillshade_9am', 0.1, 0.2)\nfor col in new_col:\n    all_data = frequency_encoding(all_data, col)\n\nall_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Hillshade_Noon', 0.1, 0.2)\nfor col in new_col:\n    all_data = frequency_encoding(all_data, col)\n\"\"\"","3ca3015b":"all_data.head()","d7616882":"train_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","f00705d6":"baseline_tree_cv(train_df)","95032bc5":"def nn_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n\n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n    \n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n\n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n\n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n\n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    before_one_hot = set(all_data.columns)\n    for col in category_feature:\n        all_data = pd.concat([all_data,pd.get_dummies(all_data[col],prefix=col)],axis=1)\n        del all_data[col]\n    one_hot_feature = set(all_data.columns) - before_one_hot\n    \n    all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\n    all_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Fire_Points']\n\n    all_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\n    all_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Vertical_Distance_To_Hydrology']\n\n    all_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] \/ all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\n    all_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\n    all_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\n    all_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0\n    \n    all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)\n    \n    all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1)\n    \n    for col in new_col:\n        all_data = frequency_encoding(all_data, col)\n        \n    all_data.drop(columns=new_col,axis=1,inplace=True)\n    \n    scale_feature = list(set(all_data.columns)-one_hot_feature-set(['Cover_Type','ID']))\n    \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n\n    sc = StandardScaler()\n    train_df[scale_feature] = sc.fit_transform(train_df[scale_feature])\n    test_df[scale_feature] = sc.transform(test_df[scale_feature] )\n    \n    return train_df, test_df","43fa5b1b":"nn_train_df, nn_test_df = nn_data_preprocessing(train, test)","bc89d855":"baseline_keras_cv(nn_train_df)","4bfec8cf":"### \uc9c0\uae08\uae4c\uc9c0 \uc218\ud589\ud588\ub358 \uc804\ucc98\ub9ac \uc785\ub2c8\ub2e4.","a58e4949":"Tree Model\uacfc NeuralNetwork Model\uc758 \uc804\ucc98\ub9ac\uac00 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0 Tree\uc6a9 Data, NN\uc6a9 \ub370\uc774\ud130\ub97c \ubd84\ub9ac\ud569\ub2c8\ub2e4.<br>\nFE \uc2dc\uc5d0\ub294 \uc218\ud589\uc2dc\uac04\uc774 \ube60\ub978 LightGBM\uc744 \uae30\uc900\uc73c\ub85c \ud558\uaca0\uc2b5\ub2c8\ub2e4.","657bfd5b":"## Tree Model\uc5d0\uc11c \uc131\ub2a5\ud5a5\uc0c1\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. ","a32e75cf":"# \uc9c0\uae08\uae4c\uc9c0 \ucd94\uac00\ud55c Feature\ub97c \uc815\ub9ac\ud558\uc5ec Neural Network\uc5d0\uc11c \ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","e6ab22a0":"## Utility Function \uc785\ub2c8\ub2e4.","9814f9bb":"## Elevation\uacfc Aspect\ub97c Adaptive Binning \ud558\uc5ec Category Combine \ud569\ub2c8\ub2e4.","9d2feb7e":"Elevation \/ quantitative \/meters \/ Elevation in meters <br>\nAspect \/ quantitative \/ azimuth \/ Aspect in degrees azimuth <br>\nSlope \/ quantitative \/ degrees \/ Slope in degrees <br>\nHorizontal_Distance_To_Hydrology \/ quantitative \/ meters \/ Horz Dist to nearest surface water features <br>\nVertical_Distance_To_Hydrology \/ quantitative \/ meters \/ Vert Dist to nearest surface water features <br>\nHorizontal_Distance_To_Roadways \/ quantitative \/ meters \/ Horz Dist to nearest roadway <br>\nHillshade_9am \/ quantitative \/ 0 to 255 index \/ Hillshade index at 9am, summer solstice <br>\nHillshade_Noon \/ quantitative \/ 0 to 255 index \/ Hillshade index at noon, summer soltice <br>\nHillshade_3pm \/ quantitative \/ 0 to 255 index \/ Hillshade index at 3pm, summer solstice <br>\nHorizontal_Distance_To_Fire_Points \/ quantitative \/ meters \/ Horz Dist to nearest wildfire ignition points <br>\nWilderness_Area (4 binary columns) \/ qualitative \/ 0 (absence) or 1 (presence) \/ Wilderness area designation <br>\nSoil_Type (40 binary columns) \/ qualitative \/ 0 (absence) or 1 (presence) \/ Soil Type designation <br>\nCover_Type (7 types) \/ integer \/ 1 to 7 \/ Forest Cover Type designation<br>","1ff5ecf0":"# Add New Feature"}}