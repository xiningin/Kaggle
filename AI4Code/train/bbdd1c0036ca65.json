{"cell_type":{"355df91c":"code","8e62423a":"code","2c23a869":"code","1175ea0f":"code","cfea6a62":"code","b22a0341":"code","fee4d166":"code","20306039":"code","3247feda":"code","2c54b0bc":"code","58cd63f9":"code","73e53b03":"code","228444ac":"code","417d4887":"code","86b000df":"code","db72e570":"code","2a4c76e7":"code","f40aba2c":"code","2199721a":"code","0cc77af0":"code","1986f9a7":"code","6f44a3ee":"code","0a1ba69a":"code","74e30963":"code","74ff446f":"code","7ff3fa89":"code","6c6976e2":"code","bfcc7d81":"code","8bc781c5":"code","a7cbc4de":"code","abd6c99f":"code","c85e4198":"code","0b4b31d0":"code","d28f3506":"code","b7e3be74":"code","37a230c5":"code","6cc2fb47":"code","019ff027":"code","427e3894":"code","7a365d8e":"code","6fd2e078":"code","10f75b0d":"markdown","e32b45dc":"markdown","3618c980":"markdown","fd2fec49":"markdown","9e4d958c":"markdown","30047802":"markdown","21f0bedd":"markdown","278a04dc":"markdown","4bc071be":"markdown","294c4312":"markdown","aaffe68a":"markdown","925e58b3":"markdown","0cf25013":"markdown","5e3b1790":"markdown","6d8a8acb":"markdown","c8c8b451":"markdown","9e3cf90e":"markdown","aa45409d":"markdown","6d8b9b20":"markdown","1175fb1e":"markdown","741f321b":"markdown","5573dbc9":"markdown","7618fe2e":"markdown","de136e36":"markdown","160a1106":"markdown","a7aac669":"markdown","a9f92bfa":"markdown","0b886681":"markdown","9377485b":"markdown","bf740126":"markdown","e29ab231":"markdown","547a090d":"markdown","fc8209c3":"markdown","4a6b68c3":"markdown","42c13105":"markdown","c696d4c2":"markdown","ecb230f2":"markdown","3b1df0aa":"markdown","3a7971bd":"markdown","8f2edf10":"markdown","07fba0bd":"markdown","04ff983a":"markdown","b57a9a74":"markdown","83445261":"markdown","c6f73465":"markdown","97c1b8b3":"markdown","140a02b3":"markdown","ad84de8c":"markdown","f90ecc39":"markdown","04f504a3":"markdown","98689746":"markdown","364138ed":"markdown","fc31aac8":"markdown","7b6e206c":"markdown"},"source":{"355df91c":"import numpy as np \nimport pandas as pd \nimport datetime\nimport requests\nimport warnings\nimport random\nimport squarify\nimport matplotlib\nimport seaborn as sns\nimport matplotlib as mpl\nimport plotly.offline as py\nimport plotly_express as px\nfrom sklearn.svm import SVR\nimport statsmodels.api as sm\nfrom functools import partial\nfrom fbprophet import Prophet\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom lightgbm import LGBMRegressor\nfrom scipy.optimize import minimize\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom fbprophet.plot import plot_plotly, add_changepoints_to_plot\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n\nfrom IPython.display import Image\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e62423a":"## Weights -> In the previous weeks (wk 2\/3\/4) of the competition phase weight was assigned according to the population of the region specified\n\nconfirmed_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv')\ndeaths_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv')\nrecovered_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv')\nlatest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\/04-04-2020.csv')\n\nworld_confirmed = confirmed_df[confirmed_df.columns[-1:]].sum()\nworld_recovered = recovered_df[recovered_df.columns[-1:]].sum()\nworld_deaths = deaths_df[deaths_df.columns[-1:]].sum()\nworld_active = world_confirmed - (world_recovered - world_deaths)\n\nlabels = ['Active','Recovered','Deceased']\nsizes = [world_active,world_recovered,world_deaths]\ncolor= ['red','green','black']\nexplode = []\n\nfor i in labels:\n    explode.append(0.05)\n\nplt.figure(figsize= (15,10))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=8, explode =explode,colors = color)\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\n\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.title('World COVID-19 Cases',fontsize = 20)\nplt.axis('equal')  \nplt.tight_layout()","2c23a869":"## DATA READING\n\ndf_deaths = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv')\ndf_covid19 = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/web-data\/data\/cases_country.csv\")\ndf_confirmed = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv')\n\n## PRE-PROCESSING\n\ndf_confirmed = df_confirmed.rename(columns={\"Province\/State\":\"state\",\"Country\/Region\": \"country\"})\ndf_covid19 = df_covid19.drop([\"People_Tested\",\"People_Hospitalized\",\"UID\",\"ISO3\",\"Mortality_Rate\"],axis =1)\ndf_deaths = df_deaths.rename(columns={\"Province\/State\":\"state\",\"Country\/Region\": \"country\"})\ndf_covid19 = df_covid19.rename(columns={\"Country_Region\": \"country\"})\ndf_covid19[\"Active\"] = df_covid19[\"Confirmed\"]-df_covid19[\"Recovered\"]-df_covid19[\"Deaths\"]\n\n# Changing the conuntry names as required by pycountry_convert Lib\ndf_deaths.loc[df_deaths['country'] == \"US\", \"country\"] = \"USA\"\ndf_deaths.loc[df_deaths['country'] == 'Korea, South', \"country\"] = 'South Korea'\ndf_deaths.loc[df_deaths['country'] == 'Taiwan*', \"country\"] = 'Taiwan'\ndf_deaths.loc[df_deaths['country'] == 'Congo (Kinshasa)', \"country\"] = 'Democratic Republic of the Congo'\ndf_deaths.loc[df_deaths['country'] == \"Cote d'Ivoire\", \"country\"] = \"C\u00f4te d'Ivoire\"\ndf_deaths.loc[df_deaths['country'] == \"Reunion\", \"country\"] = \"R\u00e9union\"\ndf_deaths.loc[df_deaths['country'] == 'Congo (Brazzaville)', \"country\"] = 'Republic of the Congo'\ndf_deaths.loc[df_deaths['country'] == 'Bahamas, The', \"country\"] = 'Bahamas'\ndf_deaths.loc[df_deaths['country'] == 'Gambia, The', \"country\"] = 'Gambia'\n\ncountries = np.asarray(df_confirmed[\"country\"])\ncountries1 = np.asarray(df_covid19[\"country\"])\n# Continent_code to Continent_names\ncontinents = {\n    'NA': 'North America',\n    'SA': 'South America', \n    'AS': 'Asia',\n    'OC': 'Australia',\n    'AF': 'Africa',\n    'EU' : 'Europe',\n    'na' : 'Others'}\n\n\n# Defininng Function for getting continent code for country.\ndef country_to_continent_code(country):\n    try:\n        return pc.country_alpha2_to_continent_code(pc.country_name_to_country_alpha2(country))\n    except :\n        return 'na'\n\n#Collecting Continent Information\ndf_deaths.insert(2,\"continent\",  [continents[country_to_continent_code(country)] for country in countries[:]])\ndf_covid19.insert(1,\"continent\",  [continents[country_to_continent_code(country)] for country in countries1[:]])\n\ndf_deaths[df_deaths[\"continent\" ]== 'Others']\ndf_deaths = df_deaths.replace(np.nan, '', regex=True)\n\ndf_countries_cases = df_covid19.copy().drop(['Lat','Long_','continent','Last_Update'],axis =1)\ndf_countries_cases.index = df_countries_cases[\"country\"]\ndf_countries_cases = df_countries_cases.drop(['country'],axis=1)\n\ndf_countries_cases.fillna(0,inplace=True)\n\n## VISUALIZATION\n\ntemp_df = pd.DataFrame(df_countries_cases['Deaths'])\ntemp_df = temp_df.reset_index()\nfig = px.choropleth(temp_df, locations=\"country\",\n                    color=np.log10(temp_df[\"Deaths\"]+1), \n                    hover_name=\"country\", \n                    hover_data=[\"Deaths\"],\n                    color_continuous_scale=px.colors.sequential.Plasma,locationmode=\"country names\")\nfig.update_geos(fitbounds=\"locations\", visible=False)\nfig.update_coloraxes(colorbar_title=\"Deaths (Log Scale)\",colorscale=\"Reds\")\n\nfig.show()","1175ea0f":"f = plt.figure(figsize=(10,5))\nf.add_subplot(111)\n\nplt.axes(axisbelow=True)\nplt.barh(df_countries_cases.sort_values('Deaths')[\"Deaths\"].index[-10:],df_countries_cases.sort_values('Deaths')[\"Deaths\"].values[-10:],color=\"crimson\")\nplt.tick_params(size=5,labelsize = 13)\nplt.xlabel(\"Deaths Cases\",fontsize=18)\nplt.title(\"Top 10 Countries (Deaths Cases)\",fontsize=20)\nplt.grid(alpha=0.3,which='both')","cfea6a62":"hotspots = ['China','Germany','Iran','Italy','Spain','US','Korea, South','France','Turkey','United Kingdom','India']\ndates = list(confirmed_df.columns[4:])\ndates = list(pd.to_datetime(dates))\ndates_india = dates[8:]\n\ndf1 = confirmed_df.groupby('Country\/Region').sum().reset_index()\ndf2 = deaths_df.groupby('Country\/Region').sum().reset_index()\ndf3 = recovered_df.groupby('Country\/Region').sum().reset_index()\n\nglobal_confirmed = {}\nglobal_deaths = {}\nglobal_recovered = {}\nglobal_active= {}\n\nfor country in hotspots:\n    k =df1[df1['Country\/Region'] == country].loc[:,'1\/30\/20':]\n    global_confirmed[country] = k.values.tolist()[0]\n\n    k =df2[df2['Country\/Region'] == country].loc[:,'1\/30\/20':]\n    global_deaths[country] = k.values.tolist()[0]\n\n    k =df3[df3['Country\/Region'] == country].loc[:,'1\/30\/20':]\n    global_recovered[country] = k.values.tolist()[0]\n    \nfor country in hotspots:\n    k = list(map(int.__sub__, global_confirmed[country], global_deaths[country]))\n    global_active[country] = list(map(int.__sub__, k, global_recovered[country]))\n    \nfig = plt.figure(figsize= (15,25))\nplt.suptitle('Active, Recovered, Deaths in Hotspot Countries and India as of May 15',fontsize = 13,y=1.0)\n#plt.legend()\nk=0\nfor i in range(1,12):\n    ax = fig.add_subplot(6,2,i)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))\n    ax.bar(dates_india,global_active[hotspots[k]],color = 'red',alpha = 0.6,label = 'Active');\n    ax.bar(dates_india,global_recovered[hotspots[k]],color='green',label = 'Recovered');\n    ax.bar(dates_india,global_deaths[hotspots[k]],color='black',label = 'Death');   \n    plt.title(hotspots[k])\n    handles, labels = ax.get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper left')\n    k=k+1\n\nplt.tight_layout(pad=3.0)","b22a0341":"us = latest_data.loc[latest_data['Country_Region'] == 'US']\nus.drop('Admin2', axis=1, inplace=True)\n\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https:\/\/raw.githubusercontent.com\/plotly\/datasets\/master\/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n\nus_min = us[\"Confirmed\"].min()\nus_mean = us[\"Confirmed\"].mean()\nus_max = us[\"Confirmed\"].max()\nus_med = us[\"Confirmed\"].median()\n\nfig = px.choropleth_mapbox(us, geojson=counties, locations=\"FIPS\", color='Confirmed',\n                           hover_name=\"Province_State\",\n                           color_continuous_scale=\"OrRd\",\n                           range_color=(us_med,us_mean),\n                           mapbox_style=\"carto-positron\",\n                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n                           opacity=0.4,\n                           labels={'Confirmed':'Confirmed Case Number'}\n                          )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","fee4d166":"cluster_data = pd.read_csv(\"..\/input\/covid19-useful-features-by-country\/Countries_usefulFeatures.csv\")\nage_df = cluster_data[[\"Country_Region\",\"Mean_Age\"]]\nsns.distplot(a=age_df['Mean_Age'], kde=False)","20306039":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-5\/test.csv\")\n\ntrain_copy = train.copy()\ntest_copy = test.copy()\n\ntrain['day']=pd.to_datetime(train.Date,format='%Y-%m-%d').dt.day\ntrain['month']=pd.to_datetime(train.Date,format='%Y-%m-%d').dt.month\n\ntest['day']=pd.to_datetime(test.Date,format='%Y-%m-%d').dt.day\ntest['month']=pd.to_datetime(test.Date,format='%Y-%m-%d').dt.month\n\ntrain.columns = map(str.lower, train.columns)\ntrain = train.rename(columns = {'county': 'country', 'province_state': 'state', 'country_region': 'region', 'target': 'case', 'targetvalue':'case_value'}, inplace = False)\n\ntc_data = pd.read_csv(\"..\/input\/number-of-cases-in-the-city-covid19-turkey\/number_of_cases_in_the_city.csv\")","3247feda":"tc_list = list(range(1, 82))\ntc_data.insert(0, \"id\", tc_list, True) \n\nimport plotly.express as px\n\nmore_case = tc_data.sort_values(by='Number of Case', ascending=False)\n\nfig = px.pie(\n    more_case.head(5),\n    values = \"Number of Case\",\n    names = \"Province\",\n    color_discrete_sequence = px.colors.sequential.RdBu)\n\nfig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\nfig.show()","2c54b0bc":"import plotly.express as px\n\n# loading Turkey's geoplot json file\nfrom urllib.request import urlopen\nimport json\nwith open(\"..\/input\/geoplot\/tr-cities-utf8.json\") as f:\n    cities = json.load(f)\n\nmini = tc_data[\"Number of Case\"].min()\naverage = tc_data[\"Number of Case\"].mean()\n#tc_data.drop('id', axis=1, inplace=True)\n    \nfig = px.choropleth_mapbox(tc_data, geojson=cities, locations=tc_data.id, color=(tc_data[\"Number of Case\"]),\n                           hover_name=\"Province\",\n                           range_color= (mini,average),\n                           color_continuous_scale='amp',\n                           mapbox_style=\"carto-positron\",\n                           zoom=4, opacity=0.7,center = {\"lat\": 38.963745, \"lon\": 35.243322},\n                           labels={'color':'Number of Case'})\n\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","58cd63f9":"less_case = tc_data.sort_values(by='Number of Case', ascending=True)\n\nfig = px.bar(\n    less_case.head(10),\n    x = \"Province\",\n    y = \"Number of Case\")\nfig.update_layout(barmode=\"group\")\nfig.update_traces(marker_color='rosybrown')\nfig.show()","73e53b03":"import matplotlib as mpl\n\ntc = train.loc[train.region == 'Turkey']\n\ntc.drop('country', axis=1, inplace=True)\ntc.drop('state', axis=1, inplace=True)\ntc.drop('region', axis=1, inplace=True)\ntc.drop('population', axis=1, inplace=True)\n\ntc_1=tc['case_value'].groupby(tc['case']).sum()\n\nfatal_tc=tc[tc['case']=='Fatalities']\nconf_tc=tc[tc['case']=='ConfirmedCases']\n\nlabels =[tc_1.index[0],tc_1.index[1]]\nsizes = [tc_1[0],tc_1[1]]\nexplode = (0, 0.08)  \nplt.figure(figsize = (8,8))\n\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',textprops={'fontsize': 14},startangle=90)\nplt.show()","228444ac":"px.line(data_frame=conf_tc, x=\"date\", y='case_value',hover_name=\"case\")","417d4887":"fig = px.line(data_frame=fatal_tc, x=\"date\", y='case_value',hover_name=\"case\", color_discrete_map={'case_value': 'red'})\nfig.show()","86b000df":"with plt.style.context('fivethirtyeight'):\n    dategroup=tc.groupby('month').mean()\n    fig, ax = plt.subplots(figsize=(20,6))\n    ax.xaxis.set(ticks=range(0,13)) # Manually set x-ticks\n    dategroup['case_value'].plot(x=tc.month)","db72e570":"from pandas import Series\nfrom math import sqrt\n\n# metrics\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\n\n# forecasting model\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA\n\n# for analysis\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom shapely.geometry import LineString\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import plot\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 7\n\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","2a4c76e7":"train_original=pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/train.csv')\ntest_original=pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/test.csv')\ntrain_original.sample(3)","f40aba2c":"# Train Data Cleaning\ntrain_original=train_original.drop([\"County\"], axis=1)\ntrain_original=train_original.drop([\"Province_State\"], axis=1)\ntrain_original=train_original.drop([\"Population\"], axis=1)\ntrain_original=train_original.drop([\"Weight\"], axis=1)\ntrain_original=train_original.drop([\"Id\"], axis=1)\n\ntrain_original = pd.DataFrame(train_original[(train_original['Country_Region'] == 'Turkey') & (train_original['Target'] == 'ConfirmedCases')])\ntrain_original=train_original.drop([\"Country_Region\"], axis=1)\ntrain_original=train_original.drop([\"Target\"], axis=1)\n\n# Test Data Cleaning\ntest_original=test_original.drop([\"County\"], axis=1)\ntest_original=test_original.drop([\"Province_State\"], axis=1)\ntest_original=test_original.drop([\"Population\"], axis=1)\ntest_original=test_original.drop([\"Weight\"], axis=1)\n\ntest_original = pd.DataFrame(test_original[(test_original['Country_Region'] == 'Turkey') & (test_original['Target'] == 'ConfirmedCases')])\ntest_original=test_original.drop([\"Country_Region\"], axis=1)\ntest_original=test_original.drop([\"Target\"], axis=1)\n\ntest_original.dropna(inplace=True)\ntest_original.dropna(inplace=True)\ntest_original.drop(test_original.tail(1).index, inplace=True)\n\ntrain_df=train_original.copy()\ntest_df=test_original.copy()\n\ntrain_original['Date']=pd.to_datetime(train_original.Date, format='%Y\/%m\/%d')\ntest_original['Date']=pd.to_datetime(test_original.Date, format='%Y\/%m\/%d')\ntrain_df['Date']=pd.to_datetime(train_df.Date, format='%Y\/%m\/%d')\ntest_df['Date']=pd.to_datetime(test_df.Date, format='%Y\/%m\/%d')\n\n# generate day, month, year feature\nfor i in (train_original, test_original, train_df, test_df):\n    i['year']=i.Date.dt.year\n    i['month']=i.Date.dt.month\n    i['day']=i.Date.dt.day\n    i['hour']=i.Date.dt.hour\n    \n# sampling for daily basis\ntrain_df.index=train_df.Date\ntest_df.index=test_df.Date\n\ntrain_df=train_df.resample('D').mean()\ntest_df=test_df.resample('D').mean()","2199721a":"train_df.head(3) #Last Version","0cc77af0":"train=train_df.loc['2020-04-11':'2020-05-16']\nvalid=train_df.loc['2020-05-17':'2020-07-10']\nplt.figure(figsize=(20,7))\n\ntrain.TargetValue.plot(label='Train data')\nvalid.TargetValue.plot(label='Valid data')\nplt.legend(loc='best')","1986f9a7":"rolmean=train.TargetValue.rolling(window=7).mean() #for 7 days -> roll.mean: pencere gezdirip ortalama alma\nrolstd=train.TargetValue.rolling(window=7).std()\nrolmean.dropna(inplace=True)\nrolstd.dropna(inplace=True)\n\nplt.figure(figsize=(17,7))\nrolmean.plot(label='Rolmean', color='green')\nrolstd.plot(label='rolstd')\ntrain.TargetValue.plot(label='Train')\nplt.legend(loc='best')","6f44a3ee":"dftest=adfuller(train.TargetValue, autolag='AIC')\ndfout=pd.Series(dftest[0:4], index=['Test statistics', 'p-value', '#Lags used', 'Number of observation used'])\nfor key, val in dftest[4].items():\n    dfout['Critical value (%s)'%key]=val\n\nprint(dfout)","0a1ba69a":"# estimating trend\ntrain_count_log=np.log(train.TargetValue)\n\n# make TS to be stationary\nmoving_avg=train_count_log.rolling(window=7).mean()\nmoving_std=train_count_log.rolling(window=7).std()\nplt.figure(figsize=(17,7))\n\ntrain_count_log.plot(label='Log Scale')\nmoving_avg.plot(label='moving_avg')\nmoving_std.plot(label='moving_std')\n\nplt.legend(loc='best')","74e30963":"# Varyasyonun y\u00fcksek oldu\u011fu yer.\ndif_log=train_count_log-moving_avg\ndif_log.dropna(inplace=True)\ndif_log.plot()","74ff446f":"def test_stationary(timeseries):\n    # determine roling stats\n    mov_avg=timeseries.rolling(window=7).mean()\n    mov_std=timeseries.rolling(window=7).std()\n    #plot rolling stats\n    plt.figure(figsize=(12,7))\n    timeseries.plot(label='Original')\n    mov_avg.plot(label='Mov avg')\n    mov_std.plot(label='Mov std')\n    plt.legend(loc='best')\n    plt.title('Rolling mean & standard deviation')\n    \n    # dickey-fuller test\n    print('Result of Dickey-fuller test')\n    dftest=adfuller(timeseries, autolag='AIC')\n    dfout=pd.Series(dftest[:4], index=['Test stats', 'p-value', '#Lag used', 'Number of observation used'])\n    for key, val in dftest[4].items():\n        dfout['Critical value (%s)'%key]=val\n    print(dfout)","7ff3fa89":"test_stationary(dif_log)","6c6976e2":"plt.figure(figsize=(12,7))\nedw_avg=train_count_log.ewm(halflife=7, min_periods=0, adjust=True).mean()\ntrain_count_log.plot(label='Log scale')\nedw_avg.plot(label='Exponential Decay Weight MA')","bfcc7d81":"dif_edw=train_count_log-edw_avg\ndif_edw = dif_edw.replace([np.inf, -np.inf], np.nan)\ndif_edw.dropna(inplace=True)\ntest_stationary(dif_edw)","8bc781c5":"dif_shift=train_count_log-train_count_log.shift()\ndif_shift = dif_shift.replace([np.inf, -np.inf], np.nan)\ndif_shift.dropna(inplace=True)\ntest_stationary(dif_shift)","a7cbc4de":"decom=seasonal_decompose(dif_edw, freq=3)\n\ntrend=decom.trend\nseasonal=decom.seasonal\nresidual=decom.resid\n\nfig=plt.figure(figsize=(15,8))\nplt.subplot(211)\ntrain_count_log.plot(label='Original')\nplt.title(\"Original\")\nplt.subplot(212)\ntrend.plot(label='Trend')\nplt.title(\"Trend\")\n\n'''\nplt.subplot(413)\nseasonal.plot(label='Seasonal')\nplt.title(\"Seasonal\")\nplt.subplot(414)\nresidual.plot(label='Residual')\nplt.title(\"Residual\")\nfig.tight_layout()\n'''\n\ndecom_log_data=residual\ndecom_log_data = decom_log_data.replace([np.inf, -np.inf], np.nan)\ndecom_log_data.dropna(inplace=True)\n#test_stationary(decom_log_data)","abd6c99f":"## AR MODEL:\n\ntrain_count_log = train_count_log.replace([np.inf, -np.inf], np.nan)\ntrain_count_log.dropna(inplace=True)\nmodel=ARIMA(train_count_log, order=(4,1,0))\nresults_AR=model.fit(disp=0)\n\n'''\nThe Residual sum of Squares (RSS) is defined as below and is used in the Least Square Method \nin order to estimate the regression coefficient.\nThe smallest residual sum of squares is equivalent to the largest r squared.\nThe deviance calculation is a generalization of residual sum of squares.\nSquared loss = (y\u2212y^)2\n'''\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_AR.fittedvalues.dropna(inplace=True)\nresults_AR.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_AR.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('AR MODEL \/RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","c85e4198":"## MA MODEL\nmodel=ARIMA(train_count_log, order=(2,1,1))\nresults_MA=model.fit(disp=0)\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_MA.fittedvalues.dropna(inplace=True)\nresults_MA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_MA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('MA MODEL \/RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","0b4b31d0":"## ARIMA MODEL\nmodel=ARIMA(train_count_log, order=(4,1,2))\nresults_ARIMA=model.fit(disp=0)\n\nplt.figure(figsize=(18,6))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_ARIMA.fittedvalues.dropna(inplace=True)\nresults_ARIMA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_ARIMA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('ARIMA MODEL \/RSS: %.4f'%sum((df[0]-df['TargetValue'])**2))","d28f3506":"# using AR model\npred_ar_dif=pd.Series(results_AR.fittedvalues, copy=True)\npred_ar_dif_cumsum=pred_ar_dif.cumsum()\n\npred_ar_log=pd.Series(train_count_log.iloc[0], index=train_count_log.index)\npred_ar_log=pred_ar_log.add(pred_ar_dif_cumsum, fill_value=0)\npred_ar_log.head()\n\n# inverse of log is exp\npred_ar=np.exp(pred_ar_log)\nplt.figure(figsize=(12,7))\ntrain.TargetValue.plot(label='Train')\npred_ar.plot(label='Pred')","b7e3be74":"def validation(order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.TargetValue).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n    print('Test MSE: %.3f' % mse)\n    print('Test RMSE: %.3f' % rmse)\n    \n    fig=plt.figure(figsize=(12,7))\n    # reverse transform\n    pred=np.exp(output[0])\n    pred=pd.Series(pred, index=valid.index)\n    valid.TargetValue.plot(label='Valid')\n    pred.plot(label='Pred')\n    plt.legend(loc='best')\n    \n    fig=plt.figure(figsize=(18,7))\n    train.TargetValue.plot(label='Train')\n    valid.TargetValue.plot(label='Valid')\n    pred.plot(label='Pred', color='black')","37a230c5":"validation((2,1,2))","6cc2fb47":"'''def arima_predict_hourly(data, arima_order):\n    # forecasting for testing (Daily based forecasting)\n    \n    history = data\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(test_original))\n\n    submit=test_original.copy()\n    submit.index=submit.ID\n    submit['Count']=np.exp(output[0])\n    submit.drop(['Unnamed: 0','ID','Datetime','year','month','day','hour'], axis=1, inplace=True)\n    \n    # plot result\n    plt.figure(figsize=(12,7))\n    train_original.index=train_original.Datetime\n    submit.index=test_original.Datetime\n\n    train_original.TargetValue.plot(label='Train')\n    submit.TargetValue.plot(label='Pred')\n    return submit'''\n\nfrom pandas import DataFrame\n# forecasting for testing (daily based forecasting)\n\nh = list(np.log(train_original.TargetValue).values)\nhistory = DataFrame(h,columns=['values'])\nhistory = history.replace([np.inf, -np.inf], np.nan)\nhistory.fillna(0, inplace=True)\n\nmodel = ARIMA(history, order=(2,0,1))\nmodel_fit = model.fit(disp=0)\noutput = model_fit.forecast(steps=len(test_original))\n\nsubmit=test_original.copy()\nsubmit.index=submit.ForecastId\nsubmit['TargetValue']=np.exp(output[0])\nsubmit.drop(['ForecastId','Date','year','month','day','hour'], axis=1, inplace=True)","019ff027":"# plot result\nplt.figure(figsize=(18,7))\ntrain_original.index=train_original.Date\nsubmit.index=test_original.Date\n\ntrain_original.TargetValue.plot(label='Train')\nsubmit.TargetValue.plot(label='Pred')","427e3894":"# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(arima_order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.TargetValue).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n#     print('Test MSE: %.3f' % mse)\n#     print('Test RMSE: %.3f' % rmse)\n    return mse\n\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))","7a365d8e":"# evaluate parameters\np_values = [0, 1, 2, 4, 6, 8]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)","6fd2e078":"# ARIMA PDQ Param Tuning said that BEST ARIMA -> (4,1,0)\nvalidation((4,1,0))","10f75b0d":"<a id=\"section-two\"><\/a>\n## Global Deaths Heat Map","e32b45dc":"<a id=\"section-six-four-6\"><\/a>\n### Log scale tranformation\n\n* Plot your graph of the data against time. If it looks like the variation increases with the level of the series, take logs. Otherwise model the original data.\n* If it seems to be linear, than no need for logs.","3618c980":"<a id=\"section-five-three\"><\/a>\n### 10 Cities with the Lowest Number of Cases","fd2fec49":"<a id=\"section-six-four-8\"><\/a>\n### ADCF Test","9e4d958c":"#### What does it mean for data to be stationary?\n\n- The mean of the series should not be a function of time. We will see if this series is stationary by examining the Decomposition result in Trend.\n\n#### Why is this important? \n- When running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.\n\n- There are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the Dickey-Fuller test. I won\u2019t go into the specifics of this test, but if the \u2018Test Statistic\u2019 is greater than the \u2018Critical Value\u2019 than the time series is stationary. Below is code that will help you visualize the time series and test for stationarity.\n-----\n- Veriyi dura\u011fan(stationary) hale getirerek, regresyon tekniklerini verilen zamana ba\u011fl\u0131 de\u011fi\u015fkene ger\u00e7ekten de uygulayabiliriz.\n<img src=\"https:\/\/miro.medium.com\/max\/390\/0*3XXCQed3bPHrD1lt.png\">","30047802":"* The smaller p-value, the more likely it's stationary. Here our p-value is 0.603053. It's actually not that bad, if we use a 5% Critical Value(CV), this series would be considered stationary. But as we just visually found an upward trend, we want to be more strict, we use 1% CV.\n* To get a stationary data, there's many techiniques. We can use log, differencing etc...","21f0bedd":"### Fatalities vs Confirmed Cases","278a04dc":"<a id=\"section-six-four-7\"><\/a>\n### Exponential Decay Transformation","4bc071be":"<a id=\"section-six\"><\/a>\n# Turkey COVID-19 Forecasting","294c4312":"<a id=\"section-five\"><\/a>\n# Turkey","aaffe68a":"<a id=\"section-six-one\"><\/a>\n### Confirmed Case in Time Intervals","925e58b3":"### Plotting the Features to see trends\n* Covid cases have strong daily and monthly properties.\n\n#### Concepts:\n* Trend: As the name suggests trend depicts the variation in the output as time increases.It is often non-linear. Sometimes we will refer to trend as \u201cchanging direction\u201d when it might go from an increasing trend to a decreasing trend.\n\n* Level: It basically depicts baseline value for the time series.\n\n* Seasonal: As its name depicts it shows the repeated pattern over time. In layman terms, it shows the seasonal variation of data over time.\n\n* Noise: It is basically external noises that vary the data randomly.","0cf25013":"* Exponential Decay Transformation is a time series forecasting method for univariate data that can be extended to support data with a systematic trend or seasonal component. Specifically, past observations are weighted with a geometrically decreasing ratio.\n\n------\n\n* Exponential Decay Transformation, verileri sistematik bir e\u011filim veya mevsimsel bile\u015fenle desteklemek i\u00e7in geni\u015fletilebilen tek de\u011fi\u015fkenli veriler i\u00e7in bir zaman serisi tahmin y\u00f6ntemidir. Spesifik olarak, ge\u00e7mi\u015f g\u00f6zlemler geometrik olarak azalan bir oranla a\u011f\u0131rl\u0131kland\u0131r\u0131l\u0131r.","5e3b1790":"<a id=\"section-five-one\"><\/a>\n### Top 5 Cities with the Highest Number of Cases","6d8a8acb":"* In this graph, we ca see that the cases peaked in April.","c8c8b451":"# The Story of COVID-19 in World and Time Forecasting in Turkey","9e3cf90e":"<a id=\"section-six-four-15\"><\/a>\n### ARIMA PDQ Param Tuning","aa45409d":"<a id=\"section-six-four-10\"><\/a>\n### Decomposition\n\n- To start with, we want to decompose the data to seperate the trend. Since we have 3 months of confirmed case data, we would expect there's a  monthly or weekly pattern. Let's use a function in statsmodels to help us find it.","6d8b9b20":"<a id=\"section-six-four-3\"><\/a>\n### Spliting Data for Training and Validation","1175fb1e":"### TO SEE GLOBAL FORECASTING, PLEASE GO PART.2 -> https:\/\/www.kaggle.com\/thepinokyo\/esin-part-2-covid19","741f321b":"#### Data Cleaning and Generating Date:","5573dbc9":"<a id=\"section-six-four-12\"><\/a>\n### Prediction & Reverse Transformations","7618fe2e":"## Top 10 countries (Deaths)","de136e36":"<a id=\"section-six-four\"><\/a>\n# TIME SERIES MODEL","160a1106":"<a id=\"section-four-1\"><\/a>\n## Average Age Distribution of Cases in Countries","a7aac669":"<a id=\"section-six-two\"><\/a>\n### Fatalities Case in Time Intervals","a9f92bfa":"<a id=\"section-six-four-5\"><\/a>\n### Check for Stationary","0b886681":"<a id=\"section-five-two\"><\/a>\n### Turkey Heatmap (Number of Case)","9377485b":"<a id=\"section-one\"><\/a>\n# World COVID-19 Cases","bf740126":"<a id=\"section-six-four-11\"><\/a>\n### Building Model","e29ab231":"<a id=\"section-six-four-1\"><\/a>\n### Importing Libraries","547a090d":"<a id=\"section-six-four-4\"><\/a>\n### Determine Rolling Stats","fc8209c3":"<a id=\"section-six-four-9\"><\/a>\n### Time Shift Transformation","4a6b68c3":"* Augmented Dickey\u2013Fuller test is the most accepted determination of stationarity in the literature and it is accepted as the most valid test in determining stationarity in time series.\n\n------\n\n* Augmented Dickey\u2013Fuller testi, literat\u00fcrde en \u00e7ok kabul g\u00f6ren dura\u011fanl\u0131k tespitidir ve zaman serisi konusunda da dura\u011fanl\u0131\u011f\u0131n tespitinde en ge\u00e7erli test olarak kabul edilmi\u015ftir (Enders, 1995).","42c13105":"### Changes in the Number of Cases by Months","c696d4c2":"<a id=\"section-four\"><\/a>\n## US Heatmap (Confirmed Cases)","ecb230f2":"# Table of Contents\n\n* [World COVID-19 Cases](#section-one)\n* [Global Deaths Heat Map](#section-two)\n* [Active, Recovered, Deaths in Hotspot Countries](#section-three)\n* [US Heatmap(Confirmed Cases)](#section-four)\n* [Average Age Distribution of Cases in Countries](#section-four-1)\n* [Turkey](#section-five)\n    * [Top 5 Cities with the Highest Number of Cases](#section-five-one)\n    * [Turkey Heatmap (Number of Case)](#section-five-two)\n    * [10 Cities with the Lowest Number of Cases](#section-five-three)\n* [Turkey COVID-19 Forecasting](#section-six)\n     * [Confirmed Case in Time Intervals](#section-six-one)\n     * [Fatalities Case in Time Intervals](#section-six-two)\n     * [Time Series Model](#section-six-four)\n         * [Importing Libraries](#section-six-four-1)\n         * [Prepearing Data](#section-six-four-2)\n         * [Spliting Data for Training and Validation](#section-six-four-3)\n         * [Determine Rolling Stats](#section-six-four-4)\n         * [Check for Stationary](#section-six-four-5)\n         * [Log scale tranformation](#section-six-four-6)\n         * [Exponential Decay Transformation](#section-six-four-7)\n         * [ADCF Test](#section-six-four-8)\n         * [Time Shift Transformation](#section-six-four-9)\n         * [Decomposition](#section-six-four-10)\n         * [Building Model](#section-six-four-11)\n         * [Prediction & Reverse Transformations](#section-six-four-12)\n         * [Validation](#section-six-four-13)\n         * [Test Forecasting](#section-six-four-14)\n         * [ARIMA PDQ Param Tuning](#section-six-four-15) \n* [REFERENCES](#section-five)","3b1df0aa":"**Coronavirus is a large family of viruses. This is a disease that was detected in 1960, with several varieties. The virus, which is seen mostly in animals, has also been seen in humans for the first time. The current outbreak first appeared in Wuhan, China, in December 2019.The best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face.**\n\n**The COVID-19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes, so it\u2019s important that you also practice respiratory etiquette (for example, by coughing into a flexed elbow).At this time, there are no specific vaccines or treatments for COVID-19. However, there are many ongoing clinical trials evaluating potential treatments. WHO will continue to provide updated information as soon as clinical findings become available.**\n\n<img src=\"https:\/\/pbs.twimg.com\/media\/EWjM_DBWAAESbWc.jpg\">","3a7971bd":"<a id=\"section-three\"><\/a>\n## Active, Recovered, Deaths in Hotspot Countries","8f2edf10":"#### Further steps:\n* Improving the current model by using different techniques and based on different metrics.\n* To make longer term predictions with a better understanding of certain concepts.\n* With the arrival of new data, to make more detailed estimates based on city \/ region.\n-----\n#### \u0130leriki ad\u0131mlar:\n* Daha farkl\u0131 teknikler kullan\u0131l\u0131p ve farkl\u0131 metrikler baz al\u0131narak \u015fuanki modelin iyile\u015ftirilmesi.\n* Belli kavramlar\u0131n daha iyi anla\u015f\u0131lmas\u0131yla birlikte daha uzun vadeli tahminler yapmak.\n* Yeni datalar\u0131n gelmesiyle de birlikte \u015fehir\/b\u00f6lge bazl\u0131 daha detayl\u0131 tahminler yapmak.","07fba0bd":"<a id=\"section-six-four-2\"><\/a>\n### Prepearing Data","04ff983a":"### Implications and Causation:\n* The first confirmed cases in Turkey were found Covidien-19 on March 11.\n* Before that, on February 3, Turkey has announced that it stop all flights from China.\n* Turkey on February 29, announced that flights with Italy, South Korea and Iraq were mutually suspended.\n* Soon, the Iraqi border was also closed. The ministry also established field hospitals close to the Iraqi and Iranian borders.\n* On March 11, Health Minister Fahrettin Koca announced that a Turkish man caught the virus while traveling to Europe was the country's first coronavirus case. The patient was isolated to a hospital and family members of the patient were observed.\n* During the month of March, many activities such as sports leagues, horse races, barbecues in gardens, parks and recreation areas were stopped.<br>\n\n#### Why It Increased?\n* It peaked on April 11.\n* As a result of incomplete reporting of case numbers, people did not take the disease seriously.\n* The fact that the virus load is very high indicates that these people stay together in large numbers and for a long time in closed spaces and therefore are exposed to very intense \/ large amount of virus attacks and that there is a lot of inter-human contact in these places. The amount of virus you come into contact with is important in terms of how you will overcome the disease.\n* People with chronic diseases (obesity, inactivity is also considered a chronic disease) could not protect themselves and got infected.\n* Effective drugs were not in use.\n* The increased hospital load could not be balanced.\n* People did not listen to the Stay At Home call and continued on domestic travel. Necessary measures could not be taken early for jobs and schools, the interaction continued for a long time.\n------------------------------------------\n\n### \u00c7\u0131kar\u0131mlar ve Nedensellik:\n* T\u00fcrkiye'de ilk teyit edilen covid-19 vakas\u0131 11 Mart'ta bulundu.\n* Ondan \u00f6nce 3 \u015eubat'ta T\u00fcrkiye, \u00c7in'den gelen t\u00fcm u\u00e7u\u015flar\u0131 durdurdu\u011funu a\u00e7\u0131klad\u0131. \n* 29 \u015eubatta T\u00fcrkiye; \u0130talya, G\u00fcney Kore ve Irak ile u\u00e7u\u015flar\u0131n kar\u015f\u0131l\u0131kl\u0131 olarak durduruldu\u011funu a\u00e7\u0131klad\u0131.\n* K\u0131sa s\u00fcre sonra Irak s\u0131n\u0131r\u0131 da kapat\u0131ld\u0131. Bakanl\u0131k ayr\u0131ca Irak ve \u0130ran s\u0131n\u0131rlar\u0131na yak\u0131n saha hastaneleri kurdu.\n* 11 Mart'ta Sa\u011fl\u0131k Bakan\u0131 Fahrettin Koca, Avrupa'ya seyahat ederken vir\u00fcse yakalanan bir T\u00fcrk erke\u011fin \u00fclkenin ilk koronavir\u00fcs vakas\u0131 oldu\u011funu a\u00e7\u0131klad\u0131. Hasta bir hastaneye tecrit edildi ve hastan\u0131n aile \u00fcyeleri g\u00f6zlem alt\u0131na al\u0131nd\u0131.\n* Mart ay\u0131 s\u00fcresince spor ligleri, at yar\u0131\u015flar\u0131, bah\u00e7e, park ve mesire alanlar\u0131nda mangal yak\u0131lmas\u0131 gibi bir\u00e7ok aktivite durduruldu.<br>\n\n#### Neden Artt\u0131? \n* **11 Nisan'da** pik yapt\u0131.\n* **Vaka say\u0131lar\u0131n\u0131n eksik bildirimi** sonucunda,insanlar hastal\u0131\u011f\u0131 ciddiye almad\u0131.\n* **Vir\u00fcs y\u00fck\u00fcn\u00fcn \u00e7ok y\u00fcksek olmas\u0131,** bu insanlar\u0131m\u0131z\u0131n kapal\u0131 mek\u00e2nlarda \u00e7ok say\u0131da bir arada ve uzun s\u00fcre kald\u0131klar\u0131n\u0131 ve bu nedenle \u00e7ok yo\u011fun \/ \u00e7ok miktarda vir\u00fcs sald\u0131r\u0131s\u0131na maruz kald\u0131klar\u0131n\u0131 ve bu mek\u00e2nlarda insanlar aras\u0131 temas\u0131n \u00e7ok oldu\u011funu g\u00f6steriyor. Temasa geldi\u011finiz vir\u00fcs miktar\u0131 hastal\u0131\u011f\u0131 nas\u0131l atlataca\u011f\u0131n\u0131z a\u00e7\u0131s\u0131ndan \u00f6nemli.\n* **Kronik hastal\u0131klar\u0131** olan (obezlik, hareketsizlik de bir kronik hastal\u0131k say\u0131l\u0131r) insanlar\u0131n kendilerini koruyamay\u0131p vir\u00fcs kapt\u0131lar.\n* **Etkili ila\u00e7lar** devrede de\u011fildi.\n* **Artan hastane y\u00fck\u00fc** dengelenemedi.\n* \u0130nsanlar **Evde Kal\u0131n** \u00e7a\u011fr\u0131s\u0131n\u0131 dinlemedi ve yurti\u00e7i seyehatlare devam ettiler. \u0130\u015f ve okullar i\u00e7in erkenden gerekli \u00f6nlem al\u0131namad\u0131, uzun bir s\u00fcre etkile\u015fim devam etti.\n","b57a9a74":"#### ARIMA Pros:\n\n* Intepretability: Each coefficient means a specific thing ts key elements understanding: the concept of lags, and error lag terms are very unique, ARIMA gave a comprehensive cover on them. So even in the future I want to try some other regression model. I would add the lag terms and consider the error term.\n\n#### ARIMA Cons:\n\n* Inefficiency: ARIMA needs to be run on each time series, since we have 500 store\/item combinations, it needs to run 500 times. Every time we want to forecast the future, say on Jan 2, 2018, we want to forecast next 90 days. We need to re-run ARIMA.\n\n-----\n\n#### ARIMA Art\u0131lar\u0131:\n\n* Anla\u015f\u0131labilirlik: Her bir katsay\u0131, belirli bir \u015fey anlam\u0131na gelir ve temel unsurlar\u0131n anla\u015f\u0131lmas\u0131, gecikme kavram\u0131 ve hata gecikme terimleri \u00e7ok benzersizdir, ARIMA bunlar\u0131 kapsaml\u0131 bir \u015fekilde ele al\u0131r. Bu y\u00fczden gelecekte bile ba\u015fka bir regresyon modeli denemek istiyorsak gecikme katsay\u0131lar\u0131n\u0131 ekler ve hata metriklerini dikkate alabiliriz.\n\n#### ARIMA Eksileri:\n\n* Verimsizlik: ARIMA'n\u0131n her zaman serisinde \u00e7al\u0131\u015ft\u0131r\u0131lmas\u0131 gerekir, \u00e7\u00fcnk\u00fc \u00f6rne\u011fin 500 ma\u011faza \/ \u00fcr\u00fcn kombinasyonumuz oldu\u011fundan, 500 kez \u00e7al\u0131\u015ft\u0131r\u0131lmas\u0131 gerekir. Gelece\u011fi tahmin etmek istersek, \u00f6rne\u011fin \u00f6n\u00fcm\u00fczdeki 90 g\u00fcn\u00fc tahmin etmek istedi\u011fimizde ARIMA'y\u0131 yeniden \u00e7al\u0131\u015ft\u0131rmam\u0131z gerekiyor.","83445261":"#### Multiple Data Source:\n\n* COVID19 Global Forecasting (Week 5)\n* COVID-19 in Turkey\n* COVID-19 useful features by country\n* COVID19 Daily Updates\n* Novel Corona Virus 2019 Dataset\n* Number of Covid-19 cases in the cities of Turkey\n* Python Folium Country Boundaries\n* Turkey Geoplot\n\nI received the \"World COVID-19 Cases\" from the following github links to study its worldwide spread and effects. It contains all cases until 23\/9\/2020.\nhttps:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data","c6f73465":"<a id=\"section-six-four-14\"><\/a>\n### Test Forecasting","97c1b8b3":"<a id=\"section-six-four-13\"><\/a>\n### Validation ","140a02b3":"The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n\n#### Autoregression Intuition\n- Consider a time series that was generated by an autoregression (AR) process with a lag of k.\n\n- We know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information.\n\n- This means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.\n\n- We know that the PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.\n\n- This is exactly the expectation of the ACF and PACF plots for an AR(k) process.","ad84de8c":"#### How to determine p, d, q\n- In our case, we see the first order differencing make the ts stationary. \n\n- AR model might be investigated first with lag length selected from the PACF or via empirical investigation. In our case, it's clearly that within 4 lags the AR is significant. Which means, we can use AR = 4\n\n- To avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0), it may often make sense to extend the lag observed from the last significant term in the PACF.\n\n- What is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.\n\n- Assuming an AR(s) model were computed, then I would suggest that the next step in identification is to estimate an MA model with s-1 lags in the uncorrelated errors derived from the regression. The parsimonious MA specification might be considered and this might be compared with a more parsimonious AR specification. Then ARIMA models might also be analysed.\n-----\n#### \u00d6zba\u011flan\u0131ml\u0131 Model (Autoregressive model):\n- \u0130statistik, ekonometri ve sinyal i\u015flemede, otoregresif bir model bir t\u00fcr rasgele s\u00fcrecin temsilidir; bu haliyle, do\u011fa, ekonomi, vb. zamana g\u00f6re belirli zamanla de\u011fi\u015fen s\u00fcre\u00e7leri tan\u0131mlamak i\u00e7in kullan\u0131l\u0131r\n\n#### Moving-average model\n* Zaman serisi analizinde, hareketli ortalama s\u00fcreci olarak da bilinen hareketli ortalama modeli, tek de\u011fi\u015fkenli zaman serilerini modellemek i\u00e7in yayg\u0131n bir yakla\u015f\u0131md\u0131r. Hareketli ortalama modeli, \u00e7\u0131kt\u0131 de\u011fi\u015fkeninin do\u011frusal olarak stokastik bir terimin mevcut ve \u00e7e\u015fitli ge\u00e7mi\u015f de\u011ferlerine ba\u011fl\u0131 oldu\u011funu belirtir. ARIMA ise ikisinin birle\u015fimidir.","f90ecc39":"* The first fatalities case of covid-19 in Turkey was found on 17 March","04f504a3":"After log transformation, our p-value is extremely small. Thus this series is very likely to be stationary.","98689746":"### REFERENCES\n* https:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data\n* https:\/\/www.who.int\/health-topics\/coronavirus#tab=tab_1\n* https:\/\/www.stat.tamu.edu\/~jnewton\/stat626\/topics\/topics\/topic5.pdf\n* https:\/\/tr.wikipedia.org\/wiki\/T\u00fcrkiye%27de_COVID-19_pandemisi_zaman_\u00e7izelgesi\n* https:\/\/www.herkesebilimteknoloji.com\/yazarlar\/orhan-bursali\/turkiyede-covid-19-olum-oranlari-neden-artti\n* https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/\n* https:\/\/www.researchgate.net\/post\/How_does_one_determine_the_values_for_ARp_and_MAq\n* https:\/\/stats.stackexchange.com\/questions\/281666\/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms\/281726#281726\n* https:\/\/stats.stackexchange.com\/questions\/134487\/analyse-acf-and-pacf-plots?rq=1","364138ed":"# STAY AT HOME AND DO KAGGLE\n<img src=\"https:\/\/i.hizliresim.com\/dOUeA3.jpg\">","fc31aac8":"1. How can techniques we will use to create the model help us deepen the understandings from EDA?\n    - While our human eyes used to focus more on what happened already,machine may look beyond to the future objectively and systematically.\n2. How much worthy or useful is it to predict the number of infections or deceased victims?\n    - It could help decision makers select more efficient and timely measures to tackle the new cases\n    - It might be helpful for individuals to know how much we need to be serious on preventing efforts\n3. What could be fundamental or neccessary too for anyone in the storm of pandemic?\n    - A general forecasting model not only for Turkey but for all countries (especially for the ones showing the early pattern of spreading.)\n    - Ways of allocating necessary medical resources nationwide or even worldwide (e.g. ventilator) (from the places with the decreasing trend to those with the increasing one.)\n-----\n1. Modeli olu\u015ftururken kullanaca\u011f\u0131m\u0131z teknikler, EDA'y\u0131 derinle\u015ftirmemize nas\u0131l yard\u0131mc\u0131 olabilir?\n     - \u0130nsan g\u00f6z\u00fc \u00f6nceden olanlara daha \u00e7ok odaklan\u0131rken, olu\u015fturaca\u011f\u0131m\u0131z modelle gelece\u011fin \u00f6tesine nesnel ve sistematik olarak bakabiliriz.\n2. Enfektelerin veya \u00f6len kurbanlar\u0131n say\u0131s\u0131n\u0131 tahmin etmek ne kadar de\u011ferli veya yararl\u0131d\u0131r?\n     - Karar vericilerin yeni vakalar\u0131n \u00fcstesinden gelmek i\u00e7in daha verimli ve zaman\u0131nda \u00f6nlemler se\u00e7melerine yard\u0131mc\u0131 olabilir.\n     - Bireylerin hastal\u0131\u011f\u0131 \u00f6nleme konusundaki \u00e7abalar\u0131n\u0131n ne kadar ciddi olmas\u0131 gerekti\u011fini bilmelerinde faydal\u0131 olabilir.\n3. Pandemi s\u00fcrecinde olan herhangi biri i\u00e7in temel veya gerekli olanlar nedir, ne olabilir?\n     - Sadece T\u00fcrkiye i\u00e7in de\u011fil, t\u00fcm \u00fclkeler i\u00e7in (\u00f6zellikle erken yay\u0131lma modelini g\u00f6steren \u00fclkeler i\u00e7in) genel bir tahmin modeli sunar.\n     - Ulusal ve hatta d\u00fcnya \u00e7ap\u0131nda gerekli t\u0131bbi kaynaklar\u0131 tahsis etme yollar\u0131 konusunda yard\u0131mc\u0131 olabilir. (\u00f6rn. test kiti)\n    ","7b6e206c":"* A rolling analysis of a time series model is often used to assess the model's stability over time. \n\n-----\n\n* Bir zaman serisi modelinin rolling analizi, genellikle modelin zaman i\u00e7indeki kararl\u0131l\u0131\u011f\u0131n\u0131 de\u011ferlendirmek i\u00e7in kullan\u0131l\u0131r."}}