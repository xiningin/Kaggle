{"cell_type":{"c32ec1f9":"code","d21c43cb":"code","a9c0e3f1":"code","8fb4ece2":"code","c6e9b596":"code","98c79676":"code","ded897e2":"code","2bd6b097":"code","50c68206":"code","83d7fcef":"code","82a12e12":"code","a96f2473":"code","c33ffd66":"code","a0fdc63e":"code","1ba6246c":"code","641b5f74":"code","813ebde8":"code","1b0d2f6d":"code","e225e7e5":"code","bdc8d5cf":"code","3ec476cf":"code","f49bcca9":"code","9cc5e5ba":"code","74752f04":"code","5ab8e2f9":"code","e9140e86":"code","614abf90":"code","be32832e":"markdown","3eecc7a4":"markdown","90220967":"markdown","0241e2e1":"markdown"},"source":{"c32ec1f9":"!pip install wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport os\nimport re\nimport nltk","d21c43cb":"train = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')","a9c0e3f1":"print(train.shape,test.shape)","8fb4ece2":"train.head()","c6e9b596":"print(train.isnull().sum())\nprint('------------------')\nprint(test.isnull().sum())","98c79676":"test=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+' '+test['text']\ntrain['total']=train['title']+' '+train['author']+' '+train['text']","ded897e2":"train.head()","2bd6b097":"#WORDCLOUD\nreal_words = ''\nfake_words = ''\nstopwords = set(STOPWORDS)\n\n#iterate through the csv file\nfor val in train[train['label']==0].total:\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    real_words += \" \".join(tokens)+\" \"\nfor val in train[train['label']==1].total:\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    fake_words += \" \".join(tokens)+ \" \"","50c68206":"wordcloud = WordCloud(width = 1000,height=1000,\n                      background_color = 'white',\n                      stopwords = stopwords,\n                      min_font_size = 10).generate(real_words)\n\n#Plot the real words wordcloud image\nplt.figure(figsize = (10,10), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.show()","83d7fcef":"wordcloud2 = WordCloud(width = 1000, height = 1000,\n                      background_color = 'white',\n                      stopwords = stopwords,\n                      min_font_size = 10).generate(fake_words)\n\n#Plotting the Fakewords wordcloud image\nplt.figure(figsize = (10,10),facecolor =  None)\nplt.imshow(wordcloud2)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","82a12e12":"#TOKENIZATION\n#Downloading nltk data\nnltk.download('punkt')\nnltk.word_tokenize(\"Hello how are you\")","a96f2473":"#STOPWORDS\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\nprint(stop_words)","c33ffd66":"stop_words.append('')\nprint(stop_words)","a0fdc63e":"#LEMMATIZATION\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n","1ba6246c":"nltk.download('wordnet')","641b5f74":"#APPLICATION\nlemmatizer = WordNetLemmatizer()\nfor index,row in train.iterrows():\n    filter_sentence = ''\n    \n    sentence = row['total']\n    sentence = re.sub(R'[^\\w\\s]','',sentence)  #cleaning\n    \n    words = nltk.word_tokenize(sentence)  #tokenization\n    \n    words = [w for w in words if not w in stop_words] #stopwords removal\n    \n    for word in words:\n        filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()\n        \n    train.loc[index,'total'] = filter_sentence","813ebde8":"train.head()","1b0d2f6d":"train = train[['total','label']]","e225e7e5":"train.head()","bdc8d5cf":"from nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score","3ec476cf":"\nTrain_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(train['total'],train['label'],test_size=0.3)\n","f49bcca9":"from sklearn.preprocessing import LabelEncoder\nEncoder = LabelEncoder()\nTrain_Y = Encoder.fit_transform(Train_Y)\nTest_Y = Encoder.fit_transform(Test_Y)","9cc5e5ba":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nTfidf_vect = TfidfVectorizer(max_features=5000)\nTfidf_vect.fit(train['total'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)","74752f04":"print(Tfidf_vect.vocabulary_)","5ab8e2f9":"print(Train_X_Tfidf)","e9140e86":"# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(Train_X_Tfidf,Train_Y)\n# predict the labels on validation dataset\npredictions_NB = Naive.predict(Test_X_Tfidf)\n# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)","614abf90":"# Classifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\nSVM.fit(Train_X_Tfidf,Train_Y)\n# predict the labels on validation dataset\npredictions_SVM = SVM.predict(Test_X_Tfidf)\n# Use accuracy_score function to get the accuracy\nprint(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)","be32832e":"SVM","3eecc7a4":"#Cleaning and Preprocessing\n#REGEX","90220967":"Naive Bayes","0241e2e1":"models"}}