{"cell_type":{"c1a2d3e8":"code","d962a256":"code","af321778":"code","4cea29fa":"code","01b5ec58":"code","5660f8ab":"code","91244036":"code","d11ca89b":"code","3a0cfe83":"code","864e624f":"code","fd9bb074":"code","9d29ec51":"code","e44b038f":"code","da34fa44":"code","c098b986":"code","28693936":"code","6e0a114d":"code","e60846b4":"code","9a06558b":"code","510b6170":"code","4b9ed395":"code","4a4db40e":"code","3070206f":"code","4aaface3":"code","4a273c7d":"code","e7967242":"code","c4765efd":"code","117f2fb5":"code","04493b5b":"code","9667fb86":"code","35e09ce9":"code","e9f58e9c":"code","82ebd4fa":"code","9215c1e9":"code","44132787":"code","32d2245f":"code","aabba75f":"code","23df0afb":"code","e3ae2bae":"code","bfdadc6c":"code","34e75547":"markdown","8fa0ac95":"markdown","cb7aea2c":"markdown","86b7a932":"markdown","13294337":"markdown","083c650a":"markdown","bd7e2968":"markdown","a5bc184b":"markdown","f2d9bcc4":"markdown","34ff79d6":"markdown","97c81dbd":"markdown","b4fd82c3":"markdown","f3af57e5":"markdown","dda66123":"markdown","3069efef":"markdown","e4f9956f":"markdown","5dce3650":"markdown","f738bd67":"markdown","925f5352":"markdown","ec9a6345":"markdown","85d41bb1":"markdown","3335d688":"markdown","99ab4eca":"markdown","7d2a472d":"markdown","254276aa":"markdown","e5348a61":"markdown","7c5fd98d":"markdown","1e1be9f6":"markdown","7fb0839d":"markdown","e525f2ee":"markdown","f7b2a8c3":"markdown","a588dc6b":"markdown","c6c020af":"markdown","1220c2c7":"markdown","63b9b380":"markdown","6914ed4a":"markdown"},"source":{"c1a2d3e8":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h2 {\n    background-color: #E87D23; \n    color: white; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 30px; \n    max-width: 1500px; \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n<\/style>","d962a256":"from collections import Counter, defaultdict\n\nimport matplotlib.pyplot as plt  # basic plotting\nimport nltk #NLP tasks\nimport numpy as np  # numerical computation\nimport pandas as pd  # for handling and cleaning data\nimport plotly.express as px\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\n\nimport gensim #for topic modelling\nfrom gensim import corpora\n\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","af321778":"DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n# since dataset is not having column and is encoded, pass both DATASET_columns and encoding\ndf = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names=DATASET_COLUMNS)","4cea29fa":"df.head()","01b5ec58":"print(f\"No of rows: {df.shape[0]} \\nNo of columns: {df.shape[1]}\")","5660f8ab":"df.dtypes","91244036":"# mapping sentiment data to classes 0 and 4 and convert 4 to 1 to be more intutive\ndf = pd.concat([df.query(\"sentiment==0\"), df.query(\"sentiment==4\")])\ndf.sentiment = df.sentiment.map({0:0, 4:1})\ndf =  shuffle(df).reset_index(drop=True)","d11ca89b":"def missing_value_of_data(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percentage=round(total\/ data.shape[0]*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n\n\nmissing_value_of_data(df)","3a0cfe83":"users = df['user'].value_counts()[:10]\nusers.plot(kind='bar', color='red')","864e624f":"x = df.sentiment.value_counts()\nx.plot(kind='bar')","fd9bb074":"from nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","9d29ec51":"corpus = []\n\nword = df['text'].str.split()\nnew = word.values.tolist()\ncorpus=[word for i in new for word in i]","e44b038f":"from collections import Counter\n\ncounter=Counter(corpus)\nmost=counter.most_common(100)\n\nx, y= [], []\nfor word,count in most[:50]:\n    # can avoid stop words too\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n        \nplt.bar(x,y)","da34fa44":"import plotly.express as px\n\ntemp = pd.DataFrame(most)\n\n\nfig = px.treemap(temp, path=[0], values=1,title='Tree of Most Common Tweeted words')\nfig.show()","c098b986":"from collections import defaultdict\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, )","28693936":"temp = pd.DataFrame(top)\nfig = px.treemap(temp, path=[0], values=1,title='Tree of Most Common Tweeted words in stop word list')\nfig.show()","6e0a114d":"df['text'].str.len().hist()","e60846b4":"min(df['text'].str.len()), max(df['text'].str.len())","9a06558b":"df['text'].str.split().map(lambda x: len(x)).hist()","510b6170":"min(df['text'].str.split().map(lambda x: len(x))), max(df['text'].str.split().map(lambda x: len(x)))","4b9ed395":"df['text'].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x)).hist()","4a4db40e":"avg = df['text'].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\navg.value_counts()","3070206f":"from nltk.util import ngrams\nlist(ngrams(['I' ,'went','to','the','river','bank'],2))","4aaface3":"import seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n## visualising top n-grams\n\ndef get_top_word_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_word_trigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","4a273c7d":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_word_bigrams(df['text'])[:20]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","e7967242":"plt.figure(figsize=(10,5))\ntop_tweet_trigrams=get_top_word_trigrams(df['text'])[:20]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","c4765efd":"## lemmatization and stemming\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","117f2fb5":"copy_df = df.copy()\n# taking a fraction of dataset because lemmatization takes a lot of time\ndf = df.sample(frac=0.1)","04493b5b":"def preprocess_corpus(df):\n    corpus = []\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    \n    for tweet in df['text']:\n        tweets = [t for t in word_tokenize(tweet) if (t not in stop)]\n        tweets = [lem.lemmatize(t) for t in tweets if len(t)>2]\n        corpus.append(tweets)\n        \n    return corpus\n\ncorpus = preprocess_corpus(df)","9667fb86":"from gensim import corpora\nimport gensim\n\ndic=gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]","35e09ce9":"lda_model = gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 4, \n                                   id2word = dic,                                    \n                                   passes = 10,\n                                   workers = 2)\nlda_model.show_topics()","e9f58e9c":"import pyLDAvis.gensim","82ebd4fa":"def plot_lda_vis(lda_model, bow_corpus, dic):\n    pyLDAvis.enable_notebook()\n    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n    return vis","9215c1e9":"plot_lda_vis(lda_model, bow_corpus, dic)","44132787":"from nlp_profiler_class import NLPProfiler \nnew = df.sample(n=200)","32d2245f":"%%timeit\nprofiled_text_dataframe = NLPProfiler().apply_text_profiling(new, 'text')","aabba75f":"profiled_text_dataframe.head(15)","23df0afb":"profiled_text_dataframe['sentiment_polarity'].hist(xlabelsize=5, ylabelsize=15)","e3ae2bae":"profiled_text_dataframe['sentiment_subjectivity'].hist(xlabelsize=5,ylabelsize=15)","bfdadc6c":"profiled_text_dataframe['spelling_quality'].hist(xlabelsize=8, ylabelsize=10)","34e75547":"- In each word the tweet word length ranges 0 to even 140 \n- Does it mean that people are using really short words in their tweets?","8fa0ac95":"- Luckily for us there are no missing values in this dataset","cb7aea2c":"A quick look at the processed data with NLP Profiler library gives a lot of insights on various aspects mentioned belows. Almost 23 parameters are analysed from the dataset\nwith this library.\n\nIt returns returns either high-level insights or low-level\/granular statistical information about the text when given a dataset and a column name containing text data, in that column. In short: Think of it as using the pandas.describe() function or running Pandas Profiling on your data frame, but for datasets containing text columns rather than the usual columnar datasets.","86b7a932":"### Spelling quality\n\nThe word quality of the typed tweets.","13294337":"### Checking for class distribution\n\nBefore we begin with any visualisation,let's check the class distribution.There are only two classes 0(negative) and 1(positive).","083c650a":"### Users who tweet the most\n\n- Lets checkout which all users tweet the most in the given dataset","bd7e2968":"<a id=ac><\/a>\n<div class=h2>Acknowledgment<\/div>\n<i><p style=\"font-size:16px; background-color: #FFF1D7; border: 2px dotted black; margin: 20px; padding: 20px;\">Credits: Thank you <a href=\"https:\/\/www.kaggle.com\/shahules\/\">Shahul ES<\/a> for your <a href=\"https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools\">excellent article on EDA with NLP<\/a> to provide as a basis to my kernel for this dataset and other kernels.<\/p>\n\n- Thanks to Abhishek Thakur for his awesome book and videos\n- Thanks to Mani Sarkar for his suggestions and NLP Profiler library\n\nThanks for reading, if you liked it give it a upvote!","a5bc184b":"Can't say anything about sentiment subjectivity from the insights","f2d9bcc4":"- As you can see from the above graph, we are able to see the most commonly occuring words in the dataset of reviews\n- Both in bar chart and tree chart form","34ff79d6":"The order of sorting packages is using done with a python package called `isort` which helps in keeping the PEP8 rules intact.","97c81dbd":"- Do you know the difference between **lemattization and stemming**, check out Abhishek thakurs video?\n\n[![IMAGE ALT TEXT](http:\/\/img.youtube.com\/vi\/OQxi-d5C9j8\/0.jpg)](http:\/\/www.youtube.com\/watch?v=OQxi-d5C9j8 \"Video Title\")","b4fd82c3":"Code snippet that generates this chart:\n\n- On the left side, the area of each circle represents the importance of the topic relative to the corpus. As there are four topics, we have four circles.\n- The distance between the center of the circles indicates the similarity between the topics. Here you can see that the topic 3 and topic 4 overlap, this indicates that the topics are more similar.\n- On the right side, the histogram of each topic shows the top 30 relevant words. For example, in topic 1 the most relevant words and get a sense what it talks about.","f3af57e5":"<div class=h2> Analysing the word level data<\/div>","dda66123":"### Sentiment polarity","3069efef":"Now, let\u2019s create the bag of words model using gensim and then create a LDA model.","e4f9956f":"<a id='in'><\/a>\n<div class=h2> Peeking at data<\/div>\n\n**Importing Libraries**\n\nIf you want to follow the analysis step-by-step you may want to install the following libraries:","5dce3650":"<a id='ToC'><\/a>\n\n-----\n\n\n<div class=\"h2\">Table of Contents<\/div>\n<ul>\n    <li><a href=\"#in\">Peeking at Data<\/a><\/li>\n    <li><a href=\"#mpi\">Commonly occuring words and stop words in dataset<\/a><\/li>\n    <li><a href=\"#dk\">Analysing Text Statistics<\/a><\/li>\n    <li><a href=\"#mt\">N Gram exploration<\/a><\/li>\n    <li><a href=\"#tp\"> Topic modelling and visualisation<\/a><\/li>\n    <li><a href=\"#nlp\">Analysis with NLP profiler library<\/a><\/li>\n    <li><a href=\"#ac\">Acknowledgement<\/a><\/li>\n<\/ul>","f738bd67":"- The histogram shows each word in tweets that ranging 1 to 58 characters\n- There maybe a few long hashtags in the dataset","925f5352":"You can print all the topics and try to make sense of them but there are tools that can help you run this data exploration more efficiently. One such tool is pyLDAvis which visualizes the results of LDA interactively.","ec9a6345":"<a id=mt><\/a>\n<div class=h2>N-Gram exploration<\/div>\n\nNgrams are simply contiguous sequences of n words. For example \u201criverbank\u201d,\u201d The three musketeers\u201d etc.If the number of words is two, it is called bigram. For 3 words it is called a trigram and so on.\n\nLooking at most frequent n-grams can give you a better understanding of the context in which the word was used.\n\nLet's explore some of the trigrams in the dataset","85d41bb1":"### For our analysis we are using certain high level features like:","3335d688":"### **Range of length of words**","99ab4eca":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)\n\nExploratory data analysis is one of the most important parts of any machine learning workflow and Natural Language Processing is no different. But which tools you should choose to explore and visualize text data efficiently?\n\n<h2> Why EDA?<\/h2>\n\nMajority of the beginners think that being a Data Scientist is all about building machine learning models and appearning geeky.\nAfter all it's called the [sexiest job of 21st Century](https:\/\/www.hiringlab.org\/2019\/01\/17\/data-scientist-job-outlook\/). \nYet in reality only 10-20 percent of the time is gone in building ML models usually. We spend a lot of time in collecting data,\ncleaning it(not in Kaggle which provides clean data), doing EDA. The below diagram is a time depicted for actual data scientist in \nIndustry.\n\nAlmost all the Kaggle grandmasters do their own EDA for any competition they are solving first before diving into buidling models.\nExploring the data helps in identifying heuristics is always very important for building good ML models. \n\n**Always remember your Machine learning models is always as good as your training data only**\n\n\n\n\n[Image source](https:\/\/course.create.ml\/ian)","7d2a472d":"### Sentiment subjectivity","254276aa":"- Most of the tweets belong to being strongly Positive, negative about anything they tweet. Is it a sign of more polarised communities?","e5348a61":"- No missing values are there in our data\n- This shows our data is a perfectly balanced dataset with both same number of both positive and negative reviews\n","7c5fd98d":"<a id=mpi><\/a>\n<div class=h2>Commonly occuring words and stop words in dataset<\/div>\n\nSince we are exploring a text dataset it's always necessary to look at some of the most commonly occuring words. Consider you are listening to a \nspeech by US President, how will you understand the contents of the full speech without listening to the entire speech. One way to look and find\nsolution for this problem is using finding the most common words used by the president and understand the main things he was talking about.","1e1be9f6":"### **Checking for missing values**\n\nIs it a clean dataset? Are there missing values are first thing to check while doing EDA","7fb0839d":"### **average word length duration**","e525f2ee":"So this is as expected. Almost 60% of tweets are suffering from poor grammar. In social media do you really need to care about your grammar?","f7b2a8c3":"- The histogram shows tweets that ranging 7 to 313 characters\n- It's quite evident from dataset, long tweets are an outlier in a platform like twitter where the word limit is usually 240","a588dc6b":"### Length of tweets","c6c020af":"<a id=\"tp\"><\/a>\n<div class=h2> Topic modelling with LDA and visualization<\/div>\n\n**Topic modeling** is the process of using unsupervised learning techniques to extract the main topics that occur in a collection of documents.\n\n**Latent Dirichlet Allocation (LDA)** is an easy to use and efficient model for topic modeling. Each document is represented by the distribution of topics and each topic is represented by the distribution of words.\n\nOnce we categorize our documents in topics we can dig into further data exploration for each topic or topic group.\n\nBut before getting into topic modeling we have to pre-process our data a little. We will:\n\n- tokenize: the process by which sentences are converted to a list of tokens or words.\n- remove stopwords\n- lemmatize: reduces the inflectional forms of each word into a common base or root.","1220c2c7":"<center><h1>Exploratory Data Analsis<\/h1><\/center>\n\nWith this data you are challenged to build complex NLP solutions for doing Sentiment Analysis\/Text classification problems in NLP. Before you get started, always remember to do your EDA!\n\n\n","63b9b380":"<a id='nlp'><\/a>\n<div class=h2> Analysis using NLP Profiler library<\/div>\n\nIt's a library to quickly analyse the given text data and understand valuable insights from the dataset like:\n- grammar checks\n- sentiment analysis\n- character or word count\n- Sentiment subjectivity\/objectivity\n- Spelling quality and much more\n\nIt's written by Mani Sarkar! \n\n> Please note we are using just a sample of 200 tweets for analysis with the data","6914ed4a":"- As you can see from the above graph, we are able to see the most commonly occuring stop words in the dataset\n- Both in bar chart and tree chart form"}}