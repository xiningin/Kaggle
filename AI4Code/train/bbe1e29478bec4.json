{"cell_type":{"37da2d88":"code","9c035ce4":"code","cbd77833":"code","2d8a2d1a":"code","34cb166f":"code","b661610c":"code","25ce70e0":"code","b04e5d14":"code","e4281e23":"code","0abd289b":"code","e1fb5bba":"code","46c5cf5c":"code","a4a33cd6":"code","2b2bce6c":"code","2f1f910f":"code","4d76e819":"code","c1e31761":"code","6d94af0d":"code","f4613f96":"markdown","2011dc48":"markdown","f63cfe82":"markdown","853b86d1":"markdown","eb195f7f":"markdown","b2daca0a":"markdown","686336a0":"markdown","26927820":"markdown","28261fd4":"markdown","8a61edfd":"markdown","327ca496":"markdown","18f24bc5":"markdown","195673c0":"markdown","fb443b38":"markdown","389f04ad":"markdown","b4947c64":"markdown","df95724d":"markdown","9b6d23ab":"markdown","64fb9a80":"markdown","3bd7ef11":"markdown","d2945e09":"markdown","4ff599b9":"markdown","c55a2a53":"markdown","a3e7eca8":"markdown"},"source":{"37da2d88":"! apt install libgeos-dev\n! pip uninstall -y shapely; pip install --no-binary :all: shapely==1.6.4\n! pip uninstall -y cartopy; pip install --no-binary :all: cartopy==0.17.0\n! pip install geoviews==1.6.6 hvplot==0.5.2 panel==0.8.0 bokeh==1.4.0","9c035ce4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nfrom operator import add, mul\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport hvplot.pandas\nimport holoviews as hv\nimport cartopy.crs as ccrs\nimport geopandas as gpd\nfrom toolz.curried import map, partial, pipe, reduce\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","cbd77833":"countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')).replace('United States of America', 'US')\ncovid = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv', parse_dates=['Date'], index_col='Id')\nindicators = pd.read_csv('\/kaggle\/input\/countries-of-the-world\/countries of the world.csv', decimal=',').replace('United States', 'US')\n\ncountry_indicators = (countries.assign(name = lambda df: df.name.astype(str).str.strip())\n                     .merge(indicators.assign(Country = lambda df: df.Country.astype(str).str.strip()), \n                            left_on = 'name', right_on='Country', how='inner'))\nweeks = (covid\n         .assign(dayofweek = lambda df: df.Date.dt.dayofweek)\n         .set_index('Date')\n         .drop(columns=['Province_State'])\n         .groupby(['Country_Region', pd.Grouper(freq='W')]).agg({'ConfirmedCases':'sum', 'Fatalities':'sum', 'dayofweek':'max'})\n         .reset_index()\n         .where(lambda df: df.ConfirmedCases > 0)\n         .dropna(0)\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .sort_values('Date')\n                            .assign(week_of_infection = lambda df: pd.np.arange(df.shape[0]))))\n         .where(lambda df: df.dayofweek >= 6)\n         .drop(columns=['dayofweek'])\n         .dropna(0)\n         .reset_index(drop=True)\n         .merge(country_indicators, left_on='Country_Region', right_on='name', how='inner')\n         .pipe(lambda df: gpd.GeoDataFrame(df, geometry='geometry'))\n         .assign(ConfirmedCases_per_capita = lambda df: (df.ConfirmedCases \/ df.pop_est),\n                 Fatalities_per_capita= lambda df: (df.Fatalities \/ df.pop_est),\n                 land_area = lambda df: df.area.astype('float'),\n                 week_of_infection_exp = lambda df: df.week_of_infection.apply(np.exp))\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .assign(week_since_first_death = lambda x: (x.week_of_infection - x.where(lambda y: y.Fatalities > 0)\n                                                                        .week_of_infection.min())\n                                                                        .clip(lower=0)\n                                                                        .fillna(0))))\n         .assign(week_since_first_death_exp = lambda df: df.week_since_first_death.apply(np.exp))\n         .drop(columns = 'gdp_md_est'))\nweeks","2d8a2d1a":"X, y_cases = (weeks\n     .select_dtypes(include=['number'])\n     .drop(columns=['ConfirmedCases', 'Fatalities', 'ConfirmedCases_per_capita', 'Fatalities_per_capita'])\n     .replace(0, 1e-8)# add jitter\n     .transform(np.log)\n     .pipe(lambda df: df.fillna(df.mean()))\n     .rename(columns = lambda name: '%\u0394 ' + name)\n     .rename(columns = {'%\u0394 week_of_infection_exp': 'week_of_infection'})\n     .rename(columns = {'%\u0394 week_since_first_death_exp': 'week_since_first_death'})\n     .pipe(lambda df: pd.concat([df, pd.get_dummies(weeks.name, drop_first=True).rename(columns =lambda s: 'is_'+s)], axis=1))\n     .assign(const = 1),\n        \n    weeks\n    .loc[:, ['ConfirmedCases_per_capita']]\n    .rename(columns={'ConfirmedCases_per_capita': 'Cases\/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%\u0394 ' + name)\n    )\n\nX.head()","34cb166f":"y_cases.hvplot.kde(title='Kernel Density Estimation of %\u0394 Confirmed Cases Response')","b661610c":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.015, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\nparams_cases = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_cases, threshold_in=0.015)\n\nmodel_cases = OLS(y_cases, X.loc[:, params_cases])\nresults_cases = model_cases.fit()","25ce70e0":"results_cases.summary()","b04e5d14":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(17, 5))\nfig = sm.graphics.influence_plot(results_cases, ax=axes[0], criterion=\"cooks\")\nfig = sm.graphics.plot_leverage_resid2(results_cases, ax=axes[1])\nres = results_cases.resid # residuals\nfig = sm.qqplot(res, ax=axes[2])\nfig.tight_layout()\n","e4281e23":"fig = plt.figure(figsize=(30,60))\nsm.graphics.plot_partregress_grid(results_cases, fig=fig)","0abd289b":"y_fatalities = (weeks\n    .loc[:, ['Fatalities_per_capita']]\n    .rename(columns={'Fatalities_per_capita': 'Fatalities\/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%\u0394 ' + name))\n\ny_fatalities.hvplot.kde(title='Kernel Density Estimation of %\u0394 Fatalities Response')","e1fb5bba":"params_fatalities = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_fatalities,  threshold_in=0.025)\n\nmodel_fatalities = OLS(y_fatalities, X.loc[:, params_fatalities])\nresults_fatalities = model_fatalities.fit()","46c5cf5c":"results_fatalities.summary()","a4a33cd6":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(17, 5))\nfig = sm.graphics.influence_plot(results_fatalities, ax=axes[0], criterion=\"cooks\")\nfig = sm.graphics.plot_leverage_resid2(results_fatalities, ax=axes[1])\nres = results_fatalities.resid # residuals\nfig = sm.qqplot(res, ax=axes[2])\nfig.tight_layout()\n","2b2bce6c":"fig = plt.figure(figsize=(30,60))\nsm.graphics.plot_partregress_grid(results_fatalities, fig=fig)","2f1f910f":"(pd.concat([results_cases.params.to_frame(name='Coefficient').assign(Response = '%\u0394 Cases\/capita'),\n            results_fatalities.params.to_frame(name='Coefficient').assign(Response = '%\u0394 Fatalities\/capita')], axis=0)\n .drop(index=['const'])\n .reset_index().rename(columns={'index': 'Covariate'})\n .where(lambda s: ~s.Covariate.str.startswith('is_')).dropna().set_index('Covariate')\n .hvplot.bar(title='COVID-19: Coefficients on (%\u0394) Covariate against (%\u0394) Response', by='Response', rot=90)\n .opts(width=1200, height=400))","4d76e819":"coefficients = (results_cases.params.to_frame('Cases')\n                .join(results_fatalities.params.to_frame('Fatalities'), how='outer')\n                .fillna(0))","c1e31761":"formula = (coefficients\n .Cases\n .loc[results_fatalities.params.index]\n .reset_index()\n .rename(columns={'index':'Name'})\n .assign(formula = lambda df: df.Name.astype(str) + ' = ' + df.Cases.astype(str) + ' ,')\n .formula\n .sum())[:-1]\n\nT_test = results_fatalities.t_test(formula)\nT_test.summary_frame().assign(names = model_fatalities.exog_names).set_index('names').round(3)","6d94af0d":"formula = (coefficients\n .Fatalities\n .loc[results_cases.params.index]\n .reset_index()\n .rename(columns={'index':'Name'})\n .assign(formula = lambda df: df.Name.astype(str) + ' = ' + df.Fatalities.astype(str) + ' ,')\n .formula\n .sum())[:-1]\n\nT_test = results_cases.t_test(formula)\nT_test.summary_frame().assign(names = model_cases.exog_names).set_index('names').round(3)","f4613f96":"# %\u0394 Fatalities Model","2011dc48":"In order to extend on our visual analysis of these coefficients, we can test if the coefficients of one of our models are statistically different from the estimated of the model, under the t-distribution. This is different than identifying whether these coefficients are statistically non-zero, as many of these coefficients we are comparing against can take on positive and negative values.  ","f63cfe82":"# Contents\n1. [Data](#Data)  \n2. [%\u0394 ConfirmedCases Model](#%\u0394-ConfirmedCases-Model)  \n3. [%\u0394 Fatalities Model](#%\u0394-Fatalities-Model)  \n4. [Model Comparison](#Model-Comparison)  \n5. [Conclusion](#Conclusion)","853b86d1":"# %\u0394 ConfirmedCases Model","eb195f7f":"**Regression Estimates**  \nOur final model includes many of our country-specific effects, which may be interesting to analyze. What is fascinating to investigate in our model is the inclusion of significant %\u0394 Industry and %\u0394 Agriculture features.  This may suggest that countries with largely service-based economies have lower growth-rates in infection controlling for our other variables.  ","b2daca0a":"**Partial Regression Plots**  \nWhat should raise concern is the correlation many variables have with the errors, and the presence of heteroskedasticity in our data, which may be a function of the number of the transformations on our data or omitted variables. ","686336a0":"To construct our design matrix for our experiment, we opted to include all our numeric columns and, to account for country-specific effects, we opted to include dummy variables for countries.  This design matrix is reused in both models, to estimate covariates for %\u0394 Cases\/capita and %\u0394 Fatalities\/capita.  ","26927820":"\nTo perform feature selection, we opted to use forward-backwards stepwise feature selection, with an input threshold of 0.015 and removal threshold at the 5% level of significance.  In order to ensure this procedure was not bias to the order of the columns, the columns were randomly shuffles, and the selection procedure was rerun multiple times. ","28261fd4":"Secondly, we will check if our estimates for our %\u0394 ConfirmedCases\/capita model is the same as those for our %\u0394 Fatalities\/capita model.  ","8a61edfd":"Similar to our first model, we chose a forward-backwards stepwise method of feature selection, but with a threshold of 0.025 for variables entering our model. This higher value was chosen after analyzing our models under a number of different hyperparameters and comparing the variables entering the model against our %\u0394 Cases\/capita model. ","327ca496":"Aggregates Statistics are COVID-19 cases dominate the news, they dominate the conversation, and they dominate Kaggle. One of the most watch Aggregates Statistics is the number of Confirmed Cases in each country.  The challenge we face when thinking about the number of Confirmed Cases is whether these figures are accurate.  Some countries have taken heroic steps to testing, and others remain slow to roll-out tests.  This begs the question: what can we rely on?  While the number of confirmed may vary greatly between countries based on testing policy, the number of fatalities I expect to be far more faithful.  The problem with comparing these figures is the 1. most people don't die of the disease, and 2. countries can observe fatalities at a lag to their number of Confirmed Cases.  The question then remains: can we use Fatalities to verify the consistency of Confirmed Cases, and if the factors driving them do differ, why?","18f24bc5":"**Regression Estimates**  \nOur estimated model appears to have far fewer features included and a far lower Adjusted R-squared. While it may be difficult to explain why our %\u0394 Fatalities\/capita model is explained worse by its covariates than the %\u0394 Cases\/capita model, this may be due to the fact that many countries are too early on in their infection rate to recognise deaths, making estimation more challenging. ","195673c0":"# Data","fb443b38":"Firstly we will check if estimates for our %\u0394 Fatalitlies\/capita model the same as those for %\u0394 ConfirmedCases per capita model.   ","389f04ad":"I chose to draw on a number of data sources on not only COVID cases but country indicators on GDP, infant mortality, etc., as well as data on population estimates and land size.  In order to better control for the variance in Fatalities and Confirmed Cases as a result of country sizes, I opted to look at Fatalities and Confirmed Cases per Capita.  As, at this stage in the virus, the pandemic is still dominated by the exponential growth in new cases, I opted to analyze the relationship between the percent change in Fatalities or Confirmed Cases per Capita, against percent changes in our factors.  Two interesting exceptions to this were variables representing the weeks since the first case in the country and the first case death, where I included both the log of the weeks since this event, to represent percent change, and the original value. This is used to model any effects relating to the logarithmic flattening of the curve late in the infection in a given country.  ","b4947c64":"Our final response variable for %\u0394 Cases\/capita looks approximately symmetric, which should make our assumption of conditional normality in our models better motivated. ","df95724d":"I would love to hear your feedback on this notebook and any suggestions on how I may improve the analysis in anyway by included new data sources or new methodologies.  Please, if you liked this kernel, please give it a vote and check our some of my other intesting kernels on COVID-19 Survival Analysis.  ","9b6d23ab":"**Influence + Leverage against Squared Residuals + QQ-plot**  \nOur final model includes many of our country-specific effects, which may be interesting to analyze. What is fascinating\nA major concern for our analysis is the clear structure in our leverage and residuals, suggesting there may be an omitted variable not included in our design matrix or by our selection procedure.  Despite this structure, the distribution of our errors appears to strongly follow our assumptions of normality, which is promising for the later tests on our model.  g to investigate in our model is the inclusion of significant %\u0394 Industry and %\u0394 Agriculture features.  This may suggest that countries with largely service-based economies have lower growth-rates in infection controlling for our other variables.  ","64fb9a80":"**Partial Regression Plots**  \nYet again, our model does suffer from string correlation against our residuals which may be a function of either omitted variables or poor transformations of our feature-space.  ","3bd7ef11":"# Model Comparison","d2945e09":"For our second model, we will investigate the covariates on %\u0394 Fatalities Response.  Looking at the Kernel Density Estimation of our response variable, we can see clearly a mixture of two- possibly three- symmetric distributions, which may have interesting covariates in our data. ","4ff599b9":"\nThe main aim of this analysis has been to compare the coefficients across our two models to identify where and why they differ. The conjecture I present in this notebook is that if these coefficients differ this may be an indication that either Fatalities are driven by other factors which do not influence the number of Confirmed Cases, or that the number of Confirmed Cases is a function of factors which lead to better testing and thus higher rates of Confirmed Cases.  What is interesting here is to observe where these may be either omitted variables or change in the sign of a coefficient between the two models. ","c55a2a53":"**Influence + Leverage against Squared Residuals + QQ-plot**  \nThis model appears far more dominated by points of high leverage, and our residuals seem to exhibit much fatter tails to the model. ","a3e7eca8":"# Conclusion\nWhat appears interesting from our analysis, is that apart from some country-specific estimtes. The estimates of our coefficients do seem to differ between our two models. The structure of the economies of countries appears a credible factor to investigate why these estimates vary so much which may have far reaching implications as the virus spreads. "}}