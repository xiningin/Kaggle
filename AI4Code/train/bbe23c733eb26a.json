{"cell_type":{"1ef185bf":"code","077b3c48":"code","afba42b5":"code","bdb90529":"code","2e472a26":"code","55074961":"code","e6434d0c":"code","0d8a6b50":"code","ced44275":"code","13924da0":"code","5dac7d86":"code","5ac2854b":"code","f915f52f":"code","33310b0b":"code","d14624b6":"code","4aecd357":"code","57fd6d9b":"code","c9e45b21":"code","8050385f":"code","8c86496d":"code","e33d2d9b":"code","8c37e67a":"code","304b5688":"code","2e59021c":"code","5d40c563":"code","3eb95127":"code","2b12d423":"code","cabbbc37":"code","d1131420":"code","3c06ccc3":"code","5359aa3b":"code","9591114a":"code","6ccdb7b0":"code","171c046d":"code","742129cb":"code","94c09347":"code","3e0d24b2":"code","f9258080":"code","eb54a4e8":"code","05b66e5d":"code","2701330d":"code","ed8c5c42":"code","7d3652a4":"code","ad414068":"code","18779aed":"code","4d861382":"code","9940bcea":"code","609eba86":"code","a4d0ecc9":"code","e905fbf3":"code","dc74f4ae":"code","28cc98d1":"code","6efedbb2":"code","01b5e11e":"code","42db7cd9":"code","cca9239f":"code","d7dd332a":"code","c9e63138":"code","1290f819":"code","b34a58d5":"markdown","676b7fff":"markdown","2af54cd1":"markdown","dafe8ef0":"markdown","75b19ac7":"markdown","3f623cf5":"markdown","08fc9928":"markdown","28868e05":"markdown","22213e6a":"markdown","abb605bd":"markdown","2d65fe57":"markdown","2bac99fd":"markdown","cef7d8a1":"markdown","81ab953a":"markdown","4bf3a675":"markdown","538fa582":"markdown","2c53bd6b":"markdown","ab1a126b":"markdown","0362a752":"markdown","a8657d63":"markdown","95a80c09":"markdown","bd41c1ef":"markdown","7a05efd3":"markdown","87b3afce":"markdown","697e2442":"markdown","36a4852a":"markdown","e844532e":"markdown","23ffdb5c":"markdown","3eea20fc":"markdown","df0959c5":"markdown","06d06c93":"markdown","2b531398":"markdown","83ab5240":"markdown","51ba542d":"markdown","1f450517":"markdown","0d855742":"markdown","152c15ea":"markdown","3317ddbd":"markdown","c7c18b32":"markdown","9e2b2930":"markdown","34ebf95d":"markdown","a8e93065":"markdown","c4527d1d":"markdown","5c44d7b6":"markdown","c2a7f8dd":"markdown","b9f09c74":"markdown"},"source":{"1ef185bf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings('ignore')","077b3c48":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","afba42b5":"train_df","bdb90529":"test_df","2e472a26":"train_df.describe()","55074961":"plt.figure(figsize=(18,18))\nsns.heatmap(train_df.corr(),annot=True,cmap=\"Blues\",fmt='.1f',square=True)","e6434d0c":"train_df.drop(['Id'],axis=1,inplace=True)\ntest_df.drop(['Id'],axis=1,inplace=True)","0d8a6b50":"# View features that are highly correlated with SalePrice\ncorrs = train_df.corr()[['SalePrice']]\ncorrs = corrs[corrs['SalePrice']>0.5]\ncorrs = corrs.sort_values(by='SalePrice',ascending=False)\n\nhigh_corr_feats = corrs.index[1:]\n\nfig, axes = plt.subplots(5,2,figsize=(13,16))\n\nfor i, ax in enumerate(axes.flatten()):\n    feat = high_corr_feats[i]\n    sns.scatterplot(x=train_df[feat], y=train_df['SalePrice'], ax=ax)\n    plt.xlabel(feat)\n    plt.ylabel('Sale Price')\nplt.tight_layout()","ced44275":"train_df.shape","13924da0":"# Drop GrLivArea outliers\ntrain_df.drop(train_df[(train_df['SalePrice'] < 300000) & \n                       (train_df['GrLivArea'] > 4000)].index,\n                       inplace=True)\n\n# Drop TotalBsmtSF and 1stFlrSF outliers\ntrain_df.drop(train_df[(train_df['TotalBsmtSF'] > 6000) | \n                       (train_df['1stFlrSF'] > 4000)].index,\n                       inplace=True)","5dac7d86":"train_df.shape","5ac2854b":"fig, axes = plt.subplots(1,3,figsize=(14,4))\nfeats = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF']\n\nfor i, ax in enumerate(axes.flatten()):\n    feat = feats[i]\n    sns.scatterplot(x=train_df[feat], y=train_df['SalePrice'], ax=ax)\n    plt.xlabel(feat)\n    plt.ylabel('Sale Price')\n    \nplt.tight_layout()","f915f52f":"df = pd.concat([train_df.drop(['SalePrice'],axis=1),\n                test_df]).reset_index(drop=True)\ndf.shape","33310b0b":"msno.matrix(train_df)","d14624b6":"msno.matrix(test_df)","4aecd357":"df_na = 100 * df.isnull().sum() \/ len(df)\ndf_na = pd.DataFrame(df_na,columns=['%NA'])\ndf_na = df_na.sort_values('%NA', ascending=False)\ndf_na = df_na[df_na['%NA']>0]\n\nplt.figure(figsize=(14,6))\nsns.barplot(x=df_na.index,y=df_na['%NA'],)\nplt.xticks(rotation = '90')\nplt.title('Feature Missing Value Percentage',fontsize=20,fontweight='bold')","57fd6d9b":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(df)","c9e45b21":"# 'None' if NA\nfor i in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n         'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'GarageType', 'GarageFinish',\n         'GarageQual', 'GarageCond', 'MasVnrType', 'FireplaceQu', 'MSSubClass']:\n    df[i] = df[i].fillna('None')\n\n\n# 0 if NA\nfor i in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n          'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']:\n    df[i] = df[i].fillna(0)\n\n    \n# Exterior1st, Exterior2nd - mode if NA\nfor i in ['Exterior1st', 'Exterior2nd', 'KitchenQual', 'Electrical', 'MSZoning',\n         'SaleType', 'Functional']:\n    df[i] = df[i].fillna(df[i].mode()[0])\n\n    \n# LotFrontage - Take median of neighborhood\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n\n# Utilities - Drop, as all are 'AllPub', except one 'NoSeWa in training data.\ndf.drop(['Utilities'],inplace=True,axis=1)","8050385f":"missing_percentage(df)","8c86496d":"df['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)","e33d2d9b":"from sklearn.preprocessing import LabelEncoder\n\nvar = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', \n        'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', \n        'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape',\n        'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass',\n        'OverallCond', 'YrSold', 'MoSold']\n\nfor i in var:\n    mdl = LabelEncoder().fit(list(df[i].values))\n    df[i] = mdl.transform(list(df[i].values))\n\ndf[var].head()","8c37e67a":"df['Total_SF_Main'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n#df['Total_Porch_SF'] = df['WoodDeckSF'] + df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n#df['Total_Bathrooms'] = df['BsmtFullBath'] + df['FullBath'] + 0.5*(df['HalfBath'] + df['BsmtHalfBath'])\n#df['YrBltRemod'] = df['YearBuilt'] + df['YearRemodAdd']\n#df['Total_sqr_footage'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF']\n#df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n#df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n#df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n#df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n#df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","304b5688":"sns.distplot(train_df['SalePrice'])","2e59021c":"mu = train_df['SalePrice'].mean()\nmed = train_df['SalePrice'].median()\nstd = train_df['SalePrice'].std()\nskew = train_df['SalePrice'].skew()\nkurt = train_df['SalePrice'].kurt()\n\nprint('SalePrice \\n mean = {:.2f} \\n median = {:.2f} \\n standard deviation = {:.2f} \\n skew = {:.2f} \\n kurtosis = {:.2f}'.format(mu, med, std, skew, kurt))","5d40c563":"stats.probplot(train_df['SalePrice'], plot=plt)","3eb95127":"sns.residplot('GrLivArea', 'SalePrice', data=train_df)","2b12d423":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\nmu = train_df['SalePrice'].mean()\nmed = train_df['SalePrice'].median()\nstd = train_df['SalePrice'].std()\nskew = train_df['SalePrice'].skew()\nkurt = train_df['SalePrice'].kurt()\n\nprint('SalePrice \\n mean = {:.2f} \\n median = {:.2f} \\n standard deviation = {:.2f} \\n skew = {:.2f} \\n kurtosis = {:.2f}'.format(mu, med, std, skew, kurt))\n\nsns.distplot(train_df['SalePrice'])\nplt.figure()\nstats.probplot(train_df['SalePrice'], plot=plt)","cabbbc37":"sns.residplot('GrLivArea', 'SalePrice', data=train_df)","d1131420":"numeric_var_skews = pd.DataFrame(df.dtypes[df.dtypes != 'object'].index,columns=['Numeric_Variables'])\nnumeric_var_skews['Skew'] = numeric_var_skews['Numeric_Variables'].apply(lambda x: df[x].skew())\nnumeric_var_skews.sort_values('Skew',ascending=False,inplace=True)\nnumeric_var_skews.reset_index(inplace=True,drop=True)\ndisplay(numeric_var_skews)","3c06ccc3":"high_skew = numeric_var_skews[abs(numeric_var_skews['Skew']) > 0.75]\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew_vars = high_skew['Numeric_Variables']\nfor var in high_skew_vars:\n    df[var] = boxcox1p(df[var], 0.15, #boxcox_normmax(df[var] + 1)\n                      )","5359aa3b":"# Interestingly, not removing the first dummy variable actually improved\n# the final test score, thus we keep drop_first=False. Normally, one \n# would want to remove one of the dummy variables to avoid collinearity\n# in situations where the dummies represent all possible scenarios.\ndf_dummy = pd.get_dummies(df, #drop_first = True\n                         )\ndf_dummy.shape","9591114a":"def overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99.94:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\noverfitted_features = overfit_reducer(df_dummy[:train_df.shape[0]])\n\ndf_dummy = df_dummy.drop(overfitted_features, axis=1)","6ccdb7b0":"# Remove additional outliers\ntrain = df_dummy[:train_df.shape[0]]\nY_train = train_df['SalePrice'].values\n\nimport statsmodels.api as sm\nols = sm.OLS(endog = Y_train,\n             exog = train)\nfit = ols.fit()\ntest2 = fit.outlier_test()['bonf(p)']\n\noutliers = list(test2[test2<1e-2].index)\n\nprint('There were {:.0f} outliers at indices:'.format(len(outliers)))\nprint(outliers)\n\ntrain_df = train_df.drop(train_df.index[outliers])\ndf_dummy = df_dummy.drop(df_dummy.index[outliers])\ndf_dummy.shape","171c046d":"# Helpful imports\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn import metrics\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, LassoLarsIC, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVR\n\n# Designate preprocessed train and test data\ntrain = df_dummy[:train_df.shape[0]]\ntest = df_dummy[train_df.shape[0]:]\nY_train = train_df['SalePrice'].values\n\n# Cross validation strategy\ndef rmsle_cv(model):\n    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, Y_train,\n            scoring='neg_mean_squared_error', cv=kf))\n    return(rmse)","742129cb":"models = pd.DataFrame([],columns=['model_name','model_object','score_mean','score_std'])","94c09347":"knr = KNeighborsRegressor(9, weights='distance')\nscore = rmsle_cv(knr)\nprint('KNN Regression score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['knr',knr,score.mean(),score.std()]","3e0d24b2":"from sklearn.linear_model import SGDRegressor\nsgd = make_pipeline(RobustScaler(), SGDRegressor(alpha=1000000000000000,l1_ratio=1))\nscore = rmsle_cv(sgd)\nprint('SGD score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['sgd',sgd,score.mean(),score.std()]","f9258080":"rfr = RandomForestRegressor()\nscore = rmsle_cv(rfr)\nprint('Random Forest score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['rfr',rfr,score.mean(),score.std()]","eb54a4e8":"lnr = LinearRegression()\nscore = rmsle_cv(lnr)\nprint('Linear Regression score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['lnr',lnr,score.mean(),score.std()]","05b66e5d":"ridg = make_pipeline(RobustScaler(), Ridge(alpha = .17,normalize=True, random_state=4))\nscore = rmsle_cv(ridg)\nprint('Ridge score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['ridg',ridg,score.mean(),score.std()]","2701330d":"svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.02, gamma=0.00046))\nscore = rmsle_cv(svr)\nprint('SVR score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['svr',svr,score.mean(),score.std()]","ed8c5c42":"lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.00042, max_iter=100000, random_state=1))\nscore = rmsle_cv(lasso)\nprint('Lasso Score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['lasso',lasso,score.mean(),score.std()]","7d3652a4":"e_net = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.00045, l1_ratio=0.9, random_state=1))\nscore = rmsle_cv(e_net)\nprint('Elastic Net score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['e_net',e_net,score.mean(),score.std()]","ad414068":"kr = make_pipeline(RobustScaler(), KernelRidge(alpha=0.04, kernel='polynomial', degree=1, coef0=2.5))\nscore = rmsle_cv(kr)\nprint('Kernel Ridge score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['kr',kr,score.mean(),score.std()]","18779aed":"dtr = make_pipeline(RobustScaler(), DecisionTreeRegressor(random_state=0, max_depth=20))\nscore = rmsle_cv(dtr)\nprint('Decision Tree score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['dtr',dtr,score.mean(),score.std()]","4d861382":"gbr = GradientBoostingRegressor(n_estimators=3000, \n            learning_rate=0.05, max_depth=4, max_features='sqrt',\n            min_samples_leaf=1, min_samples_split=2, loss='huber',\n            random_state=5,)\nscore = rmsle_cv(gbr)\nprint('Gradient Boosting score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['gbr',gbr,score.mean(),score.std()]","9940bcea":"lgbr = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n        learning_rate=0.05, n_estimators=720, max_bin = 55,\n        bagging_fraction = 0.8, bagging_freq = 5, \n        feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9,\n        min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(lgbr)\nprint('LightGBM score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['lgbr',lgbr,score.mean(),score.std()]","609eba86":"xgbr = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n        learning_rate=0.05, max_depth=3, min_child_weight=1.7817,\n        n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571,\n        subsample=0.5213, silent=True, random_state =7, nthread = -1)\nscore = rmsle_cv(xgbr)\nprint('XGBoost score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))\nmodels.loc[len(models)] = ['xgbr',xgbr,score.mean(),score.std()]","a4d0ecc9":"models.sort_values(by='score_mean',inplace=True)\nmodels.reset_index(inplace=True,drop=True)\nmodels","e905fbf3":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","dc74f4ae":"# First, we create a list of all model combinations\nfrom itertools import combinations \n\ndef subset(lst, count): \n    return list(set(combinations(lst, count)))\n\nmodel_list = list(models[models['score_mean']<0.11]['model_name'])\ncombo = list()\n\nfor i in range(1,len(model_list)):\n    combo = combo + subset(model_list, i)\n\nprint('There are {:.0f} combinations. First 20 include:'.format(len(combo)))\ncombo[:20]","28cc98d1":"# Now, we'll apply AveragingModels to every combination. Note, this may take a while.\n\n# Commenting out this section for purposes of posting on Kaggle.\n\n'''\nmodel_scores = pd.DataFrame([],columns=['models_averaged','score_mean','score_std'])\n\nfor i in range(len(combo)):\n    mods = list()\n    for j in range(len(combo[i])):\n        mods = mods + list(models[models['model_name']==list(combo[i])[j]]['model_object'])\n    avg = AveragingModels(models = mods)\n    score = rmsle_cv(avg)\n    model_scores.loc[len(model_scores)] = [combo[i],score.mean(),score.std()]\n\nmodel_scores = model_scores.sort_values(by='score_mean')\nmodel_scores.head(25)\n'''","6efedbb2":"simple_avg_final = AveragingModels(models = (lasso, gbr, lgbr, kr))\nscore = rmsle_cv(simple_avg_final)\nprint('Simple Average score = {:.4f}  (std = {:.4f})'.format(score.mean(), score.std()))","01b5e11e":"from mlxtend.regressor import StackingCVRegressor\n\nstacked = StackingCVRegressor(regressors=(lasso, gbr, lgbr, kr),\n                                meta_regressor=lasso,\n                                use_features_in_secondary=True)\n\nscore = rmsle_cv(stacked)\nprint('Stacked score = {:.8f}  (std = {:.4f})'.format(score.mean(), score.std()))","42db7cd9":"class StackingCVRegressor_Scratch(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_])\n        return self.meta_model_.predict(meta_features)","cca9239f":"stacked_scratch = StackingCVRegressor_Scratch(base_models = (lasso, gbr, lgbr, kr),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_scratch)\nprint('Stacked (Scratch) score = {:.8f}  (std = {:.4f})'.format(score.mean(), score.std()))","d7dd332a":"stacked_final = StackingCVRegressor(regressors=(svr, ridg, xgbr),\n                                meta_regressor=e_net,\n                                use_features_in_secondary=True)\n\nscore = rmsle_cv(stacked_final)\nprint('stacked_final score = {:.8f}  (std = {:.4f})'.format(score.mean(), score.std()))","c9e63138":"model_1 = simple_avg_final\nmodel_2 = stacked_final\nmod_1_share = .5\nmod_2_share = .5\n\nmodel_1.fit(train.values, Y_train)\nmodel_1_test_predictions = np.expm1(model_1.predict(test.values))\n\nmodel_2.fit(train.values, Y_train)\nmodel_2_test_predictions = np.expm1(model_2.predict(test.values))\n\ntest_predictions = mod_1_share * model_1_test_predictions + mod_2_share * model_2_test_predictions","1290f819":"test_id = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')[['Id']]\ntest_id['SalePrice'] = np.round(test_predictions,2)\ntest_id.to_csv('out51_simple(lasso,gbr,lgbr,kr)_meta(e_net,svr,ridg,xgbr).csv',index=False)","b34a58d5":"### Kernel Ridge Regression","676b7fff":"### Null Values\n\nNow, let's get an idea of the null values in our data, and let's figure out how best to replace them. First, we'll concatenate the train and test data into one df.","2af54cd1":"### Linear Regression","dafe8ef0":"# Stacking Models: Meta-Model\nLet's see if we can improve our predictions even further through applying a meta-model atop our base model predictions. Keeping consistent with our cross-validation strategy, we'll use StackingCVRegressor to train our meta-model (as opposed to StackingRegressor, which does not train the meta-model using the out-of-fold cross-validation predictions from the base models).","75b19ac7":"Awesome, we've addressed all our null values.\n# Additional Preprocessing\n\n### Numerical --> Categorical Variables\nNext, let's change datatype on a few numerical variables that would be better represented categorically.","3f623cf5":"Let's also remove any additional outliers we may have missed.","08fc9928":"# Data Preprocessing\n\n### Removing Unnecessary Columns\nFirst, let's drop the ID column. There may be others, but for now ID is an obvious choice.","28868e05":"# Baseline Model Performance\nAwesome! We're finally done cleaning up our data, and we're ready to start making predictions! First we'll define a cross-validation strategy, and then we'll proceed with testing a variety of different base models to see which perform best.","22213e6a":"### Lasso Regression","abb605bd":"### XGBoost Regression","2d65fe57":"### Gradient Boosting Regression","2bac99fd":"### LightGBM Regression","cef7d8a1":"Awesome, now let's visualize our null values in a few different ways: msno matrices, a bargraph showing feature null-value percentages, and a table showing null-value totals & percentages.","81ab953a":"# Stacking Models II: Meta-Model From Scratch\nFor fun, let's see if we can build from scratch a stacked regression with a meta-model.","4bf3a675":"Great! Looks like these outliers boiled down to just two points. Let's visualize the graphs again to ensure all outliers were removed.","538fa582":"Thank you so much for going on this journey with me! If you found this notebook helpful, please upvote or shoot me a comment. And please let me know if you have any questions or if you have suggestion for improving upon my approach - having a conversation is the best way to learn & grow :)","2c53bd6b":"### Random Forest Regression","ab1a126b":"Now, we'll apply \"AveragingModels\" to every combination of models with score_mean < 0.11.","0362a752":"Great! Now let's fill our null values. We'll take a specific approach for each variable, depending on the context:","a8657d63":"### Correlation of Numerical Variables","95a80c09":"### Outliers\nNow let's look for potential outliers and address them.","bd41c1ef":"Great! We were able to improve our score using a stacked model approach. In particular, defining our base models to be the same set of models for which we received the best simple-average test results above (lasso, gbr, lgbr, kr), we were able to marginally improve our cross-validation score by applying the lasso meta-model.","7a05efd3":"### KNN Regression","87b3afce":"On GrLivArea, it looks like those two points on the bottom right are outliers, given they have such high GrLivArea and low SalePrice. Same for the points on the bottom right of TotalBsmtSF and 1stFlrSF. Let's drop these for now.","697e2442":"### Categorical --> Numerical Variables (Label Encoding)\nNow, let's go the other way -- let's change datatype on a few categorical variables that would be better represented numerically. Here, we use Label Encoding. Interestingly, Label Encoding outperformed One Hot Encoding on the final test submissions, which is surprising... usually we would expect the opposite to be true.","36a4852a":"Looks like SalePrice is positively skewed. Let's quantify this further...","e844532e":"Success! There are likely other outliers, but we will address these later on in our analysis in a more automated way using outlier_test() from statsmodels.api.","23ffdb5c":"Awesome! We have some pretty strong predictive models so far. Let's see if we can improve our predictions through ensembling.\n# Ensemble Models: Simple Average\nFirst we'll create the class \"AveragingModels\" that calculates the simple average prediction of a basket of models.","3eea20fc":"Success! Note that the scores differ slightly between our scratch model and StackingCVRegressor.","df0959c5":"### SGD Regression","06d06c93":"Interesting... We can see that OverallQual and many of the area\/sqrft-related variables are highly correlated with our SalePrice target variable. Furthermore, notice that many independent variables are correlated with each other... it's important to keep in mind that linear regression models (like the ones we'll be using for predictive purposes later on in this notebook) require independent variables to have little to no collinearity. We'll keep these variables for now, however, as we can account for collinearity through regularization (i.e. Lasso, Ridge) later on.","2b531398":"# Overfitted Variables & Other Outliers\nIn general, it's a good idea to consider removing variables where the vast majority of values are the same, as this can cause overfitting. ","83ab5240":"Awesome! If you were able to run the above code, you should see the top 25 model combinations by cross validation score. Note, after testing many of the top combinations, we saw the best performance in our final test submission from (lasso, gbr, lgbr, kr):","51ba542d":"### Support Vector Regressor","1f450517":"### Engineering New Features\nBelow are a variety of different features introduced to try to improve prediction accuracy in our final models. Interestingly, only 'Total_SF_Main' improved our final test score (which is why the others are commented out).","0d855742":"### Elastic Net Regression","152c15ea":"### Ridge","3317ddbd":"SalePrice has a positive skew of 1.88 and a positive kurtosis of 6.52 (meaning it's vulnerable to outliers). Further evidence of skew can be seen in the probability plot above. Finally, we see a heteroscedastic relationship between certain independent variables (i.e. GrLivArea) and our target variable. Let's see if we can normalize SalePrice a bit.","c7c18b32":"### All Models Ranked:","9e2b2930":"# Dummy Variables\nGreat! Now that we've tackled skewness, we're ready to create dummy variables.","34ebf95d":"### Decision Tree Regression","a8e93065":"# Initial EDA","c4527d1d":"Great! SalePrice is now much less skewed, more homoscedastic, and more normally distributed. Let's adjust our other highly skewed variables as well, but in a more automated way.\n\n### Independent Variables","5c44d7b6":"# Stacking & Ensembling - Top 8%\n## Housing Price Prediction - Regression, Stacking, Ensembling\n\nIn this project, we'll explore housing data in Ames, Iowa, with the goal of developing the best predictive model on final sale price. We'll take a systematic approach to do so, which includes:\n\n    1.) Initial Exploratory Data Analysis\n        - Variable Statistics\n        - Correlation of Numerical Variables\n    2.) Data Preprocessing\n        - Removing Unnecessary Columns\n        - Outliers\n        - Null Values\n    3.) Additional Preprocessing\n        - Numerical to Categorical Variables\n        - Categorical to Numerical Variables (Label Encoding)\n        - Engineering New Features\n    4.) Adjusting Skewed Variables\n        - Target Variable (SalePrice)\n        - Independent Variables\n    5.) Dummy Variables\n    6.) Overfitted Variables and Other Outliers\n    7.) Baseline Model Performance\n        - KNN Regression\n        - SGD Regression\n        - Random Forest Regression\n        - Linear Regression\n        - Ridge Regression\n        - Support Vector Regression\n        - Lasso Regression\n        - Elastic Net Regression\n        - Kernel Ridge Regression\n        - Gradient Boosting Regression\n        - LightGBM Regression\n        - XGBoost Regression\n    8.) Ensemble Models: Simple Average\n    9.) Stacking Models: Meta-Model\n    10.) Stacking Models II: Meta-Model From Scratch\n    11.) Final Predictions","c2a7f8dd":"# Adjusting Skewed Variables\nAlright, now let's address skew in our variables. The more skewed our numeric variables (especially our target variable), the worse our linear regression models will perform. Let's see if we can identify these highly skewed variables and attempt to normalize them through log & boxcox transformations. Let's start with our target variable, SalePrice.\n### Target Variable","b9f09c74":"# Final Predictions\n\nYahoo! We made it! For our final prediction, we'll create an ensemble model that includes both a simple average of lasso, kr, gbr, lgbr and a stacked meta model with base svr, ridg, and xgbr and meta regressor e_net. \n\nInterestingly, while incorporating the stacked meta-model approach into our final prediction ensemble did improve predictive power in a variety of cases, my strongest test result (.11997) came from a simple average of Lasso, Kernel Ridge, Gradient Boost, and LightGBM."}}