{"cell_type":{"0faf333a":"code","988cb0de":"code","e3d87455":"code","be3b1d2e":"code","35d85694":"code","c89d8013":"code","dbbd495e":"code","eacf8bf8":"code","3350bcd3":"code","90f296df":"code","cf08d3e3":"code","db217208":"code","fabb7273":"code","b689b044":"code","db7d6476":"code","f58b807e":"code","0028def0":"code","c1d6f90c":"code","f08835a9":"code","9e2f13a9":"code","4c27901d":"code","d1721fd0":"code","9fa01eb0":"code","df31d619":"code","d1cb64c4":"code","bc6519ce":"code","d4550d1e":"code","e14a09d5":"code","0a3786d3":"code","f16bec00":"code","8d56aa22":"code","6f64d4c2":"code","44bb8d9d":"code","711a4b64":"code","fb93b4ea":"code","e8ab5665":"code","5b6258e7":"code","20464408":"code","d13ff5c5":"markdown","6c64a3a3":"markdown","ec629418":"markdown","54b46f92":"markdown","4a524883":"markdown","71a523ec":"markdown","483c14c6":"markdown","258ce8d0":"markdown"},"source":{"0faf333a":"#import the libraries\nimport os \nimport sys\nimport io\nimport pickle\nfrom sys import path\nassert sys.version_info >= (3,5)\n\n#data manipulation\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.core.display import display\nfrom IPython.core.display import HTML\n\n#consistent sized plot\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,5\nrcParams['xtick.labelsize'] = 12\nrcParams['ytick.labelsize'] = 12\nrcParams['axes.labelsize'] = 12\n\n#display options for dataframe\npd.options.display.max_columns = None\n\n#text processing\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom fuzzywuzzy import process\nfrom fuzzywuzzy import fuzz\n\n#text feature engineering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n#models\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\n\n#regular expressions\nimport re\n\n#string operations\nimport string\nfrom string import punctuation\nfrom string import digits\n\n#compute articles similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\n\n#for file extraction\nfrom zipfile import ZipFile\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='')","988cb0de":"# load the spam collection sms file\nsms =  pd.read_table('\/kaggle\/input\/d\/datajameson\/smsspamcollection\/SMSSpamCollection.csv',names=['label','message'])\nsms.head()","e3d87455":"sms.info()","be3b1d2e":"sms['label'].value_counts()","35d85694":"#countplot of the categories of sentiments\nsns.countplot(sms['label'])\nplt.title('Countplot of the SMS labels (ham or spam)')\nplt.show()","c89d8013":"sms.isna().sum()","dbbd495e":"#check for any empty string\nblanks = []\nfor idx,rev,lab in sms.itertuples():\n    if type(rev) == 'str':\n        if rev.isspace():\n            blanks.append(idx)\nprint(blanks)  ","eacf8bf8":"X = sms.drop('label',axis=1)\ny = sms['label']\ntest_size = 0.3\nrandom_state = 42\n#split the dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_size,random_state=random_state,stratify=sms['label'])\nX_train.shape,X_test.shape","3350bcd3":"def simplify(text):\n    '''Function to handle the diacritics in the text'''\n    import unicodedata\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError:\n        pass\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    return str(text)","90f296df":"stemmer = PorterStemmer()\ndef text_cleaning(text):\n    '''Function to clean the text'''\n    #convert to lower case\n    text = text.lower()\n    #remove the email address\n    text = text.replace(r'[a-zA-z0-9._]+@[a-zA-z0-9._]+','emailaddr')\n    #replace the URL's\n    text = text.replace(r'(http[s]?\\S+)|(\\w+\\.[a-zA-Z]{2,4}\\S*)','httpaddr')\n    #replace currency symbol with moneysymb\n    text = text.replace(r'\u00a3|\\$', 'moneysymb')\n    # Replace phone numbers with 'phonenumbr'\n    text = text.replace(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b','phonenumbr')\n    #remove the ip address\n    text = text.replace(r'((2[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}','')\n    #remove the user handles\n    text = text.replace(r'@\\w+','')\n    #remove the string punctuation\n    text = ' '.join([txt for txt in word_tokenize(text) if txt not in string.punctuation])\n    #replace the digits from the text with numbr\n    text = text.replace(r'\\d','numbr')    \n    #remove all non alphabetical characters\n    text = ' '.join([txt for txt in word_tokenize(text) if txt.isalpha()])\n    #replace multiple white space with a single one\n    text = text.replace(r'\\s+', ' ')\n    #remove the leading and trailing whitespaces\n    text = text.replace(r'^\\s+|\\s+?$', '')\n    #remove the stop words\n    text = ' '.join([txt for txt in word_tokenize(text) if txt not in stopwords.words('english')])\n    #apply stemmer\n    text = ' '.join([stemmer.stem(txt) for txt in word_tokenize(text)])\n    \n    #return the cleaned text\n    return text","cf08d3e3":"'''\nClean the text values in train and test\n'''\npreprocesses = [simplify,text_cleaning]\n\nfor preprocess in preprocesses:\n    X_train['message'] = X_train['message'].apply(preprocess)\n    X_test['message'] = X_test['message'].apply(preprocess)","db217208":"# Construct a design matrix using an n-gram model and a tf-idf statistics\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\ncounts = vectorizer.fit_transform(X_train['message'])\nvocab = vectorizer.vocabulary_","fabb7273":"test_counts = vectorizer.transform(X_test['message'])","b689b044":"#label encode the target features\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(y_test)","db7d6476":"# Train SVM with a linear kernel on the training set\nclf = LinearSVC(loss='hinge')\nclf.fit(counts, y_train)","f58b807e":"train_predictions = clf.predict(counts)","0028def0":"test_predictions = clf.predict(test_counts)","c1d6f90c":"print('Accuracy Score on the Train and Test Set')\nprint('Train Accuracy = {}'.format(accuracy_score(y_train,train_predictions)))\nprint('Test Accuracy = {}'.format(accuracy_score(y_test,test_predictions)))","f08835a9":"print('Classification Report on Test Set')\nprint(classification_report(y_test,test_predictions))","9e2f13a9":"print('F-1 score on the test set')\nprint('Test Set F1 Score = {}'.format(f1_score(y_test,test_predictions)))","4c27901d":"print('Confusion Matrix on the Test Set')\nprint(confusion_matrix(y_test,test_predictions))","d1721fd0":"# Display a confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,test_predictions),index=[['actual', 'actual'], ['spam', 'ham']],\n             columns=[['predicted', 'predicted'], ['spam', 'ham']])","9fa01eb0":"# select 10 different sizes of the entire training dataset. The test set will still be kept separate an an unseen data\nraw_text = X_train['message']\nsample_space = np.linspace(500,len(raw_text)*.80,10,dtype='int')\n\n# Compute learning curves without regularization for the SVM model\ntrain_size,train_scores,valid_scores = learning_curve(estimator=LinearSVC(loss='hinge',C=1e10),\n                                                      X=counts,y=y_train,\n                                                      train_sizes=sample_space,\n                                                      cv=StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42),\n                                                      scoring='f1',\n                                                      n_jobs=-1)","df31d619":"train_size","d1cb64c4":"#check the train scores\ntrain_scores","bc6519ce":"#checl the test scores \nvalid_scores","d4550d1e":"def make_tidy(sample_space, train_scores, valid_scores):\n    # Join train_scores and valid_scores, and label with sample_space\n    messy_format = pd.DataFrame(np.stack((sample_space, train_scores.mean(axis=1),valid_scores.mean(axis=1)), axis=1),\n                                columns=['# of training examples', 'Training set', 'Validation set'])\n     # Re-structure into into tidy format\n    return pd.melt(messy_format,id_vars='# of training examples',value_vars=['Training set', 'Validation set'],\n                   var_name='Scores',value_name='F1 score')","e14a09d5":"# Initialize a FacetGrid object using the table of scores and facet on the type of score\ng = sns.FacetGrid(make_tidy(sample_space, train_scores, valid_scores), hue='Scores', size=5)\n# Plot the learning curves and add a legend\ng.map(plt.scatter, '# of training examples', 'F1 score')\ng.map(plt.plot, '# of training examples', 'F1 score').add_legend()\nplt.show()","0a3786d3":"''' Grid Search for the best hyperparameter '''\nfrom scipy.stats import loguniform\nspace = dict()\nspace['penalty'] = ['l1', 'l2', 'elasticnet']\nspace['loss'] = ['squared_hinge','hinge']\nspace['C'] = [1e10,100]\nprint(space)","f16bec00":"clf = LinearSVC()\nfolds = StratifiedKFold(n_splits=10,random_state=random_state)\ngrid_search = GridSearchCV(estimator=clf,param_grid=space,scoring='f1',\n                           n_jobs=-1, cv=folds)\ngrid_result = grid_search.fit(counts,y_train)","8d56aa22":"grid_result.best_params_","6f64d4c2":"#clf = LinearSVC(loss='hinge',penalty='l2',C=100)\nclf = grid_result.best_estimator_\nclf.fit(counts,y_train)\ntrain_predictions = clf.predict(counts)\ntest_predictions = clf.predict(test_counts)","44bb8d9d":"print('Accuracy Score on the Train and Test Set')\nprint('Train Accuracy = {}'.format(accuracy_score(y_train,train_predictions)))\nprint('Test Accuracy = {}'.format(accuracy_score(y_test,test_predictions)))","711a4b64":"print('Classification Report on Test Set')\nprint(classification_report(y_test,test_predictions))","fb93b4ea":"# Display a confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,test_predictions),index=[['actual', 'actual'], ['spam', 'ham']],\n             columns=[['predicted', 'predicted'], ['spam', 'ham']])","e8ab5665":"# Display the features with the highest weights in the SVM model\npd.Series(clf.coef_.T.ravel(),index=vectorizer.get_feature_names()).sort_values(ascending=False)[:20]","5b6258e7":"# function to decide whether a string is spam or not, and apply it on the hypothetical message from earlier.\n\ndef spam_filter(message):\n    '''Classify whether spam or ham for an unseen text message'''\n    text = simplify(message)\n    text_clean = text_cleaning(text)\n    if clf.predict(vectorizer.transform([text_clean])):\n        return 'spam'\n    else:\n        return 'not spam'   \n\nspam_filter('Ohhh, but those are the best kind of foods')","20464408":"spam_filter('You have won 10 million as an award. Send me your personal details on this website http:\/\/www.pprdww.com')","d13ff5c5":"## _Import Libraries and Load the Data_","6c64a3a3":"## _Text Preprocessing - Cleaning & Preparing for Modeling_","ec629418":"- <b> _Grid search fine tuned hyperparameters does not outperform the f1 score achieved using the base model. Other combinations of hyper parameters can also be tried_ <\/b>","54b46f92":"## _Target Spam Words and Patterns_\n***\nDESCRIPTION\n\nHow to build a spam filter for text messages using Natural Language Processing.\n\nWe'll make use of some of the NLP concepts and combine them with machine learning to build a spam filter for SMS text messages.      \n\n\n","4a524883":"## _Model Fine Tuning_","71a523ec":"- <b> _The data is highly imbalanced. There are a lot more ham messages than the spam messages_ <\/b>","483c14c6":"- <b> _There are no blank messages in the dataset. Also, there are no null or missing values in the dataset_ <\/b>","258ce8d0":"## _Split the dataset into train and test set_"}}