{"cell_type":{"26980fd3":"code","c32d807b":"code","5307038a":"code","9f08b688":"code","72bbb979":"code","77350148":"code","198be6f9":"code","f7a8f7c3":"code","edd05ce4":"markdown","349fe842":"markdown","75717013":"markdown","716ffbd4":"markdown","86e82e3c":"markdown"},"source":{"26980fd3":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.simplefilter('ignore')","c32d807b":"INPUT = \"..\/input\/tabular-playground-series-feb-2022\/\"\n\ndf_train = pd.read_csv(INPUT + \"train.csv\")\ndf_test = pd.read_csv(INPUT + \"test.csv\")\ndf_submission = pd.read_csv(INPUT + \"sample_submission.csv\")","5307038a":"train = df_train.drop([\"row_id\"],axis=1)\ntest = df_test.drop([\"row_id\"],axis=1)\n\nprint(f'Size of train data: {train.shape}')\nprint(f'Size of test data: {test.shape}')\n\nTARGET = 'target'\nFEATURES = [col for col in train.columns if col not in ['row_id', TARGET]]\nSEED = 2022\ntrain.target.value_counts()","9f08b688":"lb = LabelEncoder()\ntrain[TARGET]  = lb.fit_transform(train['target'])","72bbb979":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ntrain[FEATURES] = scale.fit_transform(train[FEATURES])\ntest[FEATURES] = scale.transform(test[FEATURES])","77350148":"fit_params = { 'objective' : 'multiclass',\n                'metric' : 'multi_logloss',\n               }","198be6f9":"# Run CV\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Lets split the data into 5 folds.  \nK = 5\n\n# We will use this 'kf'(KFold splitting stratergy) \nkf = KFold(n_splits = K, shuffle = True, random_state = SEED)\n\nacc = []\nlgb_predictions = []\nlgb_scores = []\n\n# split()  method generate indices to split data into training and test set. \nfor i, (train_index, test_index) in enumerate(kf.split(train[FEATURES], train[TARGET])):\n    \n    # Create data for this fold\n    y_train, y_valid = train[TARGET].iloc[train_index], train[TARGET].iloc[test_index]\n    X_train, X_valid = train.iloc[train_index][FEATURES], train.iloc[test_index][FEATURES]\n    print( \"\\nFold \", i)\n    \n    lgb_model = LGBMClassifier(**fit_params)\n    \n    lgb_model.fit(X_train, y_train, eval_set = [(X_valid,y_valid)], early_stopping_rounds=150)\n    \n    # Generate validation predictions for this fold\n    lgb_predict = lgb_model.predict(X_valid)\n    acc = accuracy_score(y_valid, lgb_predict)\n    lgb_scores.append(acc)    \n        \n     # Accumulate test set predictions\n    y_test_pred = lgb_model.predict(test[FEATURES])\n    lgb_predictions.append(y_test_pred)\n    \n    print(\"Mean Accuracy :\", np.mean(lgb_scores))\n","f7a8f7c3":"from scipy.stats import mode\n\nlgb_submission = df_submission.copy()\nlgb_submission[\"target\"] = lb.inverse_transform(np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int'))\nlgb_submission.to_csv(\"submission.csv\",index=False)\nlgb_submission.head()","edd05ce4":"# Load Datasets \ud83d\uddc3\ufe0f","349fe842":"# Libraries  \ud83d\udcda","75717013":"# Model Training \ud83c\udfcb\ufe0f","716ffbd4":"# Submit To Kaggle \ud83c\uddf0","86e82e3c":"# Data Manipulation \u2699\ufe0f"}}