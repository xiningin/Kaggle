{"cell_type":{"b129c4af":"code","52e8449f":"code","7ae65ff2":"code","770206eb":"code","51540764":"code","244ba2e1":"code","f2b3c6be":"code","8a5c4585":"code","740f2b41":"code","fc89a5b3":"code","ff273e37":"code","71355b9c":"code","74467b27":"code","a8965d52":"code","e6d411f4":"code","0ab35a52":"code","65071699":"code","6eeaf6bf":"code","0053ca7f":"code","d0e25168":"code","1e2d83b4":"code","a4864580":"code","84a14754":"code","9068249d":"code","5621d692":"code","e7330cbc":"code","9cd28220":"code","35156ed6":"code","2438c5ff":"code","6e18f8a9":"markdown","2431ed38":"markdown","f3ce2eb3":"markdown","c8e18567":"markdown","525f5667":"markdown","0c8f9c05":"markdown","3f5c23a5":"markdown","a9a159b8":"markdown","f5c6c5cf":"markdown","f01b0c3f":"markdown","f3a078ff":"markdown","088dc2d1":"markdown","4bc7e2d6":"markdown","b8c90100":"markdown","1b53ec99":"markdown","311e4b73":"markdown","2059da94":"markdown","0bd6d20a":"markdown","d92526d5":"markdown"},"source":{"b129c4af":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport \nfrom pycaret.classification import *","52e8449f":"from pycaret.datasets import get_data\ndiabetes = get_data('diabetes')","7ae65ff2":"df = pd.read_csv('..\/input\/churn_data_st.csv')","770206eb":"profile = ProfileReport(df,title='Pandas Profiling Report')\nprofile","51540764":"print(df.shape)\ndf.head()","244ba2e1":"df_setup = setup(df, target='Churn',ignore_features=['customerID'])","f2b3c6be":"compare_models(fold=5)","8a5c4585":"xgb_model=create_model('xgboost')","740f2b41":"tuned_xgb_model=tune_model('xgboost', optimize = 'AUC')","fc89a5b3":"# creating a decision tree model\ndt = create_model('dt')# ensembling a trained dt model\ndt_bagged = ensemble_model(dt)","ff273e37":"plot_model(tuned_xgb_model, plot = 'auc')","71355b9c":"plot_model(tuned_xgb_model, plot = 'pr')","74467b27":"plot_model(tuned_xgb_model,plot= 'boundary')","a8965d52":"plot_model(tuned_xgb_model, plot='feature')","e6d411f4":"plot_model(tuned_xgb_model, plot = 'confusion_matrix')","0ab35a52":"predict_model(tuned_xgb_model)","65071699":"xgb_final = finalize_model(tuned_xgb_model)\nxgb_model","6eeaf6bf":"interpret_model(tuned_xgb_model)","0053ca7f":"interpret_model(tuned_xgb_model, plot= 'correlation')","d0e25168":"predict_model(xgb_final)","1e2d83b4":"save_model(tuned_xgb_model,'xgb_model')","a4864580":"ls","84a14754":"load_xgb=load_model('xgb_model')","9068249d":"load_xgb","5621d692":"evaluate_model(tuned_xgb_model)","e7330cbc":"\nfrom pycaret.datasets import get_data\ndf = get_data('diabetes')\n\nfrom pycaret.classification import *\nclf = setup(data= df, target ='Class variable')\n\nlr = create_model('lr')\n\nplot_model(lr) # other parameter plot='auc'\/'boundary'\/'pr'\/'vc'","9cd28220":"# Importing dataset\nfrom pycaret.datasets import get_data\nboston = get_data('boston')\n# Importing module and initializing setup\nfrom pycaret.regression import *\nreg1 = setup(data = boston, target = 'medv')\n# creating a model\nlr = create_model('lr')\n# plotting a model\nplot_model(lr)","35156ed6":"# Importing dataset\nfrom pycaret.datasets import get_data\njewellery = get_data('jewellery')\n# Importing module and initializing setup\nfrom pycaret.clustering import *\nclu1 = setup(data = jewellery)\n# creating a model\nkmeans = create_model('kmeans')\n# plotting a model\nplot_model(kmeans)","2438c5ff":"\n# Importing dataset\nfrom pycaret.datasets import get_data\nanomalies = get_data('anomaly')\n# Importing module and initializing setup\nfrom pycaret.anomaly import *\nano1 = setup(data = anomalies)\n# creating a model\niforest = create_model('iforest')\n# plotting a model\nplot_model(iforest)","6e18f8a9":"**9. Predict Model**\n \nSo far the results we have seen are based on k-fold cross validation on training dataset only (70% by default). In order to see the predictions and performance of the model on the test \/ hold-out dataset, the predict_model function is used.\n\n\n\npredict_model function is also used to predict unseen dataset. For now, we will use the same dataset we have used for training as a proxy for new unseen dataset. In practice, predict_model function would be used iteratively, every time with a new unseen dataset.\n\n\ud83d\udca1 predict_model function can also predict a sequential chain of models which are created using stack_models and create_stacknet function.\n\n\ud83d\udca1 predict_model function can also predict directly from the model hosted on AWS S3 using deploy_model function.","2431ed38":"**8. Interpret Model**\n \nWhen the relationship in data is non-linear which is often the case in real life we invariably see tree-based models doing much better than simple gaussian models. However, this comes at the cost of losing interpretability as tree-based models do not provide simple coefficients like linear models. PyCaret implements SHAP (SHapley Additive exPlanations using interpret_model function.](http:\/\/)","f3ce2eb3":"**2. setting Up env. **\n\nThe first step of any machine learning experiment in PyCaret is setting up the environment by importing the required module and initializing setup( ). The module used in this example is pycaret.classification.\n\nOnce the module is imported, setup() is initialized by defining the dataframe (\u2018diabetes\u2019) and the target variable (\u2018Class variable\u2019).\n\n\n\nPyCaret creates a transformation pipeline based on the parameters defined in setup function. It automatically orchestrates all dependencies in a pipeline so that you don\u2019t have to manually manage the sequential execution of transformations on test or unseen dataset. PyCaret\u2019s pipeline can easily be transferred across environments to run at scale or be deployed in production with ease. Below are preprocessing features available in PyCaret as of its first release.\n\n\n![](http:\/\/www.kdnuggets.com\/wp-content\/uploads\/pycaret-intro-5.png)\n\n\nData Preprocessing steps that are compulsory for machine learning such as missing values imputation, categorical variable encoding, label encoding (converting yes or no into 1 or 0), and train-test-split are automatically performed when setup() is initialized","c8e18567":"**11. Save Model \/ Save Experiment**\n \nOnce training is completed the entire pipeline containing all preprocessing transformations and trained model object can be saved as a binary pickle file.","525f5667":"**Regression **","0c8f9c05":"source : https:\/\/pycaret.org\/\n\nhttps:\/\/www.kdnuggets.com\/2020\/04\/announcing-pycaret.html\n\n\nplease upvote if you like the kernel . ","3f5c23a5":"PyCaret is simple and easy to use. All the operations performed in PyCaret are sequentially stored in a Pipeline that is fully orchestrated for deployment. Whether its imputing missing values, transforming categorical data, feature engineering or even hyperparameter tuning, PyCaret automates all of it.\n\n\npip install PyCaret","a9a159b8":"**4. Create Model**\n \nCreating a model in any module of PyCaret is as simple as writing create_model. It takes only one parameter i.e. the model name passed as string input. This function returns a table with k-fold cross validated scores and a trained model object\n\n\nlist of open source algorithms . \n\nhttps:\/\/pycaret.org\/create-model\/","f5c6c5cf":"**Anomaly **","f01b0c3f":"**10. Deploy Model**\n \nOne way to utilize the trained models to generate predictions on an unseen dataset is by using the predict_model function in the same notebooks \/ IDE in which model was trained. However, making the prediction on an unseen dataset is an iterative process; depending on the use-case, the frequency of making predictions could be from real time predictions to batch predictions. PyCaret\u2019s deploy_model function allows deploying the entire pipeline including trained model on cloud from notebook environment.","f3a078ff":"**1. Getting Data**\n\nThe dataset is available on PyCaret\u2019s github repository. Easiest way to import dataset directly from repository is by using get_data function from pycaret.datasets modules.","088dc2d1":"**Classification **","4bc7e2d6":"**7. Plot Model**\n \nPerformance evaluation and diagnostics of a trained machine learning model can be done using the plot_model function. It takes a trained model object and the type of plot as a string input within the plot_model function.\n\nAlternatively, you can use evaluate_model function to see plots via user interface within notebook.\n\n","b8c90100":"**3. Compare Models**\n \nThis is the first step recommended in supervised machine learning experiments (classification or regression). This function trains all the models in the model library and compares the common evaluation metrics using k-fold cross validation (by default 10 folds). The evaluation metrics used are:\n\nFor Classification: Accuracy, AUC, Recall, Precision, F1, Kappa\n\nFor Regression: MAE, MSE, RMSE, R2, RMSLE, MAPE\n\n\n\ud83d\udca1 Metrics are evaluated using 10-fold cross validation by default. It can be changed by changing the value of fold parameter.\n\n\ud83d\udca1 Table is sorted by \u2018Accuracy\u2019 (Highest to Lowest) value by default. It can be changed by changing the value of sort parameter.","1b53ec99":"**6 .Ensemble Model**\n \nThe ensemble_model function is used for ensembling trained models. It takes only one parameter i.e. a trained model object. This functions returns a table with k-fold cross validated scores and a trained model object.\n\n\n\u2018Bagging\u2019 method is used for ensembling by default which can be changed to \u2018Boosting\u2019 by using the method parameter within the ensemble_model function.","311e4b73":"**Clustering **","2059da94":"**5. Tune Model**\n \nThe tune_model function is used for automatically tuning hyperparameters of a machine learning model. PyCaret uses random grid search over a predefined search space. This function returns a table with k-fold cross validated scores and a trained model object.\n\n\n\ud83d\udca1 The tune_model function in unsupervised modules such as pycaret.nlp, pycaret.clustering and pycaret.anomaly can be used in conjunction with supervised modules. For example, PyCaret\u2019s NLP module can be used to tune number of topics parameter by evaluating an objective \/ cost function from a supervised ML model such as \u2018Accuracy\u2019 or \u2018R2\u2019.","0bd6d20a":"This is the first step recommended in supervised machine learning experiments (classification or regression). This function trains all the models in the model library and compares the common evaluation metrics using k-fold cross validation (by default 10 folds). The evaluation metrics used are:\n\n**For Classification:** Accuracy, AUC, Recall, Precision, F1, Kappa\n\n**For Regression:** MAE, MSE, RMSE, R2, RMSLE, MAPE\n\n\nCreating a model in any module of PyCaret is as simple as writing create_model('model_name'). It takes only one parameter\n\n**Model tunning**\n\ntune_model('model_name')\n\n **Ensemble Model**\n\ndt = create_model('model_name')# ensembling a trained dt model\n\n**Plot Model** \n\nplot_model('model_name, plot = 'auc')# Decision Boundary\n\nEvalute model \n\nevaluate_model(model_name)\n\nInterpret Model \n\ninterpret_model(model_name)# correlation plot\n","d92526d5":"Here  I am using churn_dataset "}}