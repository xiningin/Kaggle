{"cell_type":{"033ab061":"code","b1823802":"code","7d043651":"code","4fcce85a":"code","cb94b605":"code","0443ed4a":"code","d093b5c3":"code","9a23022a":"code","811f9d82":"code","48d88236":"code","55e09569":"code","6c399406":"code","ed8772c5":"code","ee4eda1c":"code","5d494e8f":"code","4535749a":"code","b1cd2891":"code","495d3ce3":"code","4bf5c587":"code","358754a4":"code","c74f2ecf":"code","5c88ee2c":"code","bf9ca0c2":"code","96bd9a1e":"code","ed439988":"code","f65da790":"code","f2b2b89d":"code","db22f606":"code","39e47a79":"code","a7316e4f":"code","1e8d4134":"code","0616a675":"code","2b1bce79":"code","66979064":"code","22876293":"code","2188d2d1":"code","3359fcf7":"code","fadae188":"code","1689411b":"code","d44ddf05":"code","6eddd185":"code","f9a0c17b":"code","1c829ca0":"code","88a95017":"code","434ccf4d":"code","eb9dcb18":"code","406a8f85":"markdown","68028740":"markdown","02ec9fd9":"markdown","759eae16":"markdown","3ab412de":"markdown","4a0de8bd":"markdown","db1ed48d":"markdown","b2da695a":"markdown","fa01f48e":"markdown","75645eb6":"markdown","5b5229f1":"markdown","7fbe8b5a":"markdown","4a01e955":"markdown","f8ccf24d":"markdown","f1868245":"markdown","a3326990":"markdown","1622745c":"markdown","a98471a0":"markdown","2029e05c":"markdown","2b5a2959":"markdown","28ca724f":"markdown","1457b30e":"markdown","a7b610dd":"markdown","dcfb8782":"markdown","a6a1411a":"markdown","4ceafff0":"markdown","b3c56e9e":"markdown","dfceac0d":"markdown","5d9e8beb":"markdown","edae4a20":"markdown","cb71aab2":"markdown","bda94b26":"markdown","3c196bd5":"markdown"},"source":{"033ab061":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1823802":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import cm\nimport seaborn as sns","7d043651":"file_path = \"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\"","4fcce85a":"exam_perf = pd.read_csv(file_path)","cb94b605":"exam_perf.head()","0443ed4a":"exam_perf.shape","d093b5c3":"# changement des noms des colonnes (certaines)\nexam_perf.rename(columns = {\"race\/ethnicity\": \"race_ethnicity\",\n                            \"parental level of education\": \"parental_lev_education\",\n                            \"test preparation course\": \"test_preparation\",\n                            \"math score\": \"math_score\",\n                            \"reading score\": \"reading_score\",\n                            \"writing score\": \"writing_score\"}, inplace = True)","9a23022a":"exam_perf.describe()","811f9d82":"exam_perf.dtypes","48d88236":"categorial_features = [\"gender\", \"race_ethnicity\", \"parental_lev_education\", \n                       \"lunch\", \"test_preparation\"]\nexam_perf[categorial_features] = exam_perf[categorial_features].astype(\"category\")","55e09569":"# On v\u00e9rifie s'il y a des donn\u00e9es a partir du tracage d'un boxplot\n# La v\u00e9rification se fera par genre\nsns.set_theme(rc = {\"axes.spines.right\": False, \"axes.spines.top\": False})","6c399406":"# Voyons les relations entre les targets avec seaborn\nsns.pairplot(data = exam_perf, hue=\"gender\")\nplt.title(\"relational plot between math_score, writing_score and reading_score\")\nplt.savefig(\"pairplot_score_features.png\")","ed8772c5":"# Tracons les lmplots pour voir la linearite des relations entre\n# les variables\nvariables = [\"math_score\", \"reading_score\", \"writing_score\"]\nfig, ax = plt.subplots(3, 1, figsize = (15, 8))\nax = ax.flat\nfig.tight_layout(pad = 4.5)\nsns.regplot(x = variables[0], y = variables[1], data = exam_perf, ax = ax[0])\nax[0].set_title(\"Linear model of reading_score\/mah_score\", fontsize = 16)\nsns.regplot(x = variables[0], y = variables[2], data = exam_perf, ax = ax[1])\nax[1].set_title(\"Linear model of writing_score\/mah_score\", fontsize = 16)\nsns.regplot(x = variables[2], y = variables[1], data = exam_perf, ax = ax[2])\nax[2].set_title(\"Linear model of reading_score\/writing_score\", fontsize = 16)\nplt.savefig(\"linear_models.png\")","ee4eda1c":"# Tracons le heatmap entre nos trois variables\nsns.heatmap(exam_perf[variables].corr(), annot = True)\nplt.title(\"Correlation between math_score, reading_score and writing_score\")\nplt.savefig(\"Heatmap_score_features\")","5d494e8f":"exam_perf.isnull().sum()","4535749a":"exam_perf['average'] = exam_perf.mean(axis = 1)","b1cd2891":"def make_classes(average):\n  \"\"\"\n  Cette fonction va permettre d\u00e9terminer la classe de chaque valeur de la colonne average\n  \"\"\"\n  classes = ['A', 'B', 'C', 'D', 'E']\n  i = 0\n  for value in range(100, 9, -20):\n      if (average <= value) and\\\n         (average > value-20):\n         classe = classes[i]\n      i += 1\n  if average == 0:\n    return 'E'\n  return classe","495d3ce3":"exam_perf['classes'] = exam_perf['average'].map(make_classes)","4bf5c587":"exam_perf.head()","358754a4":"# Verifions cela en prenant pour attribut hue de notre fonction boxplot \n# les differentes colonnes categorielles de notre dataset\nfig, ax = plt.subplots(5, 1, figsize = (16, 14))\nax = ax.flat\nfor i, column in enumerate(categorial_features):\n  fig.tight_layout()\n  sns.boxplot(x = column, y = \"average\", data = exam_perf, ax = ax[i])\n  ax[i].set_title(\"Boxplot for {}\".format(column))\nplt.savefig(\"boxplots.png\")","c74f2ecf":"descriptions = exam_perf['average'].describe()\ndescriptions = descriptions['25%':'75%']\ninterquartil = descriptions['75%'] - descriptions['25%']\nlimitation = descriptions['25%'] - 1.5*interquartil\nexam_perf.drop(index = exam_perf[exam_perf['average']<= limitation].index.values, inplace=True)","5c88ee2c":"# Verifions cela en prenant pour attribut hue de notre fonction boxplot \n# les diff\u00e9rentes colonnes categorielles de notre dataset\nfig, ax = plt.subplots(5, 1, figsize = (16, 14))\nax = ax.flat\nfor i, column in enumerate(categorial_features):\n  fig.tight_layout()\n  sns.boxplot(x = column, y = \"average\", data = exam_perf, ax = ax[i])\n  ax[i].set_title(\"Boxplot for {}\".format(column))\nplt.savefig(\"boxplots2.png\")","bf9ca0c2":"exam_perf.shape","96bd9a1e":"exam_perf['classes'] = exam_perf['classes'].astype('category')\nexam_perf.dtypes","ed439988":"categorial_features.append('classes')","f65da790":"exam_perf_prepared = exam_perf.copy()\nfor column in categorial_features:\n  exam_perf_prepared[column] = exam_perf_prepared[column].cat.codes","f2b2b89d":"target = \"classes\"\ncategorial_features.pop()\nfeatures = categorial_features","db22f606":"features","39e47a79":"# Separons les donnees d'entrainement des donnees de test\nX_train, X_test, y_train, y_test = train_test_split(exam_perf_prepared[features], exam_perf_prepared[target])","a7316e4f":"neighbors_model = KNeighborsClassifier(n_neighbors=1)\nneighbors_model.fit(X_train, y_train)\npredictions = neighbors_model.predict(X_test)","1e8d4134":"print(\"Score {}\".format(neighbors_model.score(X_test, y_test)))","0616a675":"def get_score(X_train, X_test, y_train, y_test):\n  scores = np.array([])\n  # best_number_of_neighbors = []\n  for i in range(1, 11):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(X_train, y_train)\n    scores = np.concatenate((scores, [model.score(X_test, y_test), i]), axis = 0)\n  scores = scores.reshape(10, 2)\n  print(\"La meilleure prediction est de {}\".format(scores[scores[:, 0] == np.max(scores[:, 0])]))","2b1bce79":"get_score(X_train, X_test, y_train, y_test)","66979064":"def train(X_train, X_test, y_train, n):\n  neighbors_model = KNeighborsClassifier(n_neighbors=n)\n  neighbors_model.fit(X_train, y_train)\n  predictions = neighbors_model.predict(X_test)\n  print(\"Score {}\".format(round(neighbors_model.score(X_test, y_test)*100, 2)))\n  return neighbors_model","22876293":"train(X_train, X_test, y_train, 9)","2188d2d1":"features.append('math_score')","3359fcf7":"# S\u00e9parons les donn\u00e9es d'entrainement des donn\u00e9es de test\nX_train, X_test, y_train, y_test = train_test_split(exam_perf_prepared[features], exam_perf_prepared[target])","fadae188":"print(\"En rajoutant le score feature math_score nous avons :\")\ntrain(X_train, X_test, y_train, 9)","1689411b":"for score_feature in ['writing_score', 'reading_score']:\n  features.pop()\n  features.append(score_feature)\n  # Separons les donnees d'entrainement des donnees de test\n  X_train, X_test, y_train, y_test = train_test_split(exam_perf_prepared[features], exam_perf_prepared[target])\n  print(\"En rajoutant le score feature {} nous obtenons\".format(score_feature))\n  model = train(X_train, X_test, y_train, 9)\n  print(\"---------------------\")","d44ddf05":"# d\u00e9finissons les param\u00e8tres \u00e0 tester\nparams = {\n    \"n_neighbors\": np.arange(1, 20),\n    \"metric\": [\"manhattan\", \"euclidean\", \"minkowski\"]\n}\n\n# cross validation et entra\u00eenement \ngrid = GridSearchCV(KNeighborsClassifier(), params, cv = 5, scoring = \"accuracy\")\ngrid.fit(X_train, y_train)","6eddd185":"# meilleurs param\u00e8tres \ngrid.best_params_","f9a0c17b":"model = grid.best_estimator_\nprint(\"Le meilleur score obtenu est de : {}\".format(model.score(X_test, y_test)))","1c829ca0":"def predire(gender = 0, race_ethnicity = 1, parental_lev_education = 2, lunch = 0, test_prep = 1, reading_score = 81):\n  x = np.array([gender, race_ethnicity, parental_lev_education, lunch, test_prep, reading_score])\n  x = x.reshape(1, x.shape[0])\n  print(\"model prediction {}\".format(model.predict(x)))\n  print(\"model prediction probabilities {}\".format(model.predict_proba(x)))","88a95017":"predire()","434ccf4d":"print(\"Matrice de confusion \\n{}\".format(confusion_matrix(y_test, model.predict(X_test))))","eb9dcb18":"predictions = pd.DataFrame({\"Id\": X_test.index,\n                            \"predictions\": model.predict(X_test)})\npredictions.to_csv(\"students_performances_predict.csv\")","406a8f85":"### Matrice de confusion","68028740":"Donc 9 est le meilleur nombre de voisins pour notre mod\u00e8le","02ec9fd9":"### D\u00e9terminons les moyennes obtenues dans une nouvelle colonne","759eae16":"A \u00e9tant la classe la plus forte et E la classe la plus faible","3ab412de":"### D\u00e9terminons s'il y a des donn\u00e9es aberrantes ","4a0de8bd":"### Essayons avec les deux autres features reading_score et writing_score","db1ed48d":"Nous voyons que les trois variables suivent des distributions normales et que les scatterplots entre les variables nous montrent qu'il existe une corr\u00e9lation entre les diff\u00e9rentes variables","b2da695a":"### Nous allons concevoir une fonction qui va classer les moyennes par pas de 20 :\n- classe A : $average \\in ]80, 100]$\n- classe B : $average \\in ]60, 80]$\n- classe C : $average \\in ]40, 60]$\n- classe D : $average \\in ]20, 40]$\n- classe E : $average \\in ]0, 20]$\n","fa01f48e":"### Cross validation avec le feature reading_score","75645eb6":"### Rev\u00e9rifions s'il y a des donn\u00e9es aberrantes avec le boxplot","5b5229f1":"#### nous devons copier le dataset original et encoder les colonnes categorielles\n","7fbe8b5a":"### Concevons une fonction qui nous permet de pr\u00e9dire le niveau auquel appartient notre candidat ","4a01e955":"### Le meilleur score obtenu ","f8ccf24d":"### R\u00e9entrainons le mod\u00e8le avec 9 pour n_neighbors cette fois ci","f1868245":"Donc notre mod\u00e8le a pr\u00e9dit que je suis de la classe B ","a3326990":"### D\u00e9terminons si les colonnes math_score, reading_score et writing_score sont bien corr\u00e9l\u00e9es \u00e0 travers quatre tests","1622745c":"#### Commencons par le math_score target","a98471a0":"### R\u00e9cup\u00e9rons le dataset","2029e05c":"Donc uniquement avec le reading_score nous obtenons une meilleure pr\u00e9diction qu'avec les deux autres","2b5a2959":"#### Parfait !!","28ca724f":"### exportation des r\u00e9sultats ","1457b30e":"Notre dataset est plus propre \u00e0 pr\u00e9sent","a7b610dd":"### A present indrosuisons le math_score dans les features pour voir si la pr\u00e9diction sera plus pr\u00e9cise","dcfb8782":"Pas de traitement \u00e0 faire pour des donn\u00e9es manquantes","a6a1411a":"\nOn note une forte corr\u00e9lation entre les variables","4ceafff0":"Donc on a \u00e9limin\u00e9 7 lignes qui contenaient des donn\u00e9es aberrantes","b3c56e9e":"### On passe \u00e0 l'entrainement de notre mod\u00e8le\n","dfceac0d":"## Essayons de r\u00e9duire le nombre de donn\u00e9es aberrantes","5d9e8beb":"#### Transformons le type de notre variable classes en 'category'","edae4a20":"###Verifions le nombre de voisins pour notre model nous donnant le score le plus \u00e9lev\u00e9","cb71aab2":"### Avant de passer aux pr\u00e9-traitements importons les librairies n\u00e9cessaires","bda94b26":"### V\u00e9rifions les types des donn\u00e9es et transformons les types des donn\u00e9es categorielles en 'category'","3c196bd5":"### V\u00e9rifions si le dataset contient des donn\u00e9es aberrantes ou des valeurs manquantes\n "}}