{"cell_type":{"2e92ee7b":"code","3fed9b20":"code","f3a61804":"code","66e0cca3":"code","94ebf068":"code","f7af30ab":"code","810d5790":"code","a7c1d979":"code","af82ccbd":"code","30b9afbe":"code","e86f2345":"code","a4565236":"code","ca41989e":"code","4f7ca8b7":"code","85df3464":"code","0e8a2230":"code","a8b2a8a0":"code","00c3c9c2":"code","82811c0d":"code","9ecf2233":"code","966bcd55":"code","57448eb6":"code","a04860a1":"code","4f43adb4":"markdown","7e1c2c37":"markdown","85d100aa":"markdown","3c1ca8eb":"markdown","eaf14a28":"markdown","3164e276":"markdown","740a6007":"markdown","4d9faa8d":"markdown","44df745c":"markdown","05a78881":"markdown","c768ba17":"markdown","701f88d6":"markdown","44735795":"markdown","880a1adf":"markdown","27097785":"markdown"},"source":{"2e92ee7b":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport random as rn\n\n# Define plot style\nplt.style.use('ggplot')\n%matplotlib inline","3fed9b20":"# Read train and test data\ntrain = pd.read_csv('..\/input\/data-science-london-scikit-learn\/train.csv', header=None)\ntest = pd.read_csv('..\/input\/data-science-london-scikit-learn\/test.csv', header=None)\n\n# Read train labels\ntrain_label = pd.read_csv('..\/input\/data-science-london-scikit-learn\/trainLabels.csv', header=None)\n\n# Check the data dimensions\nprint('Train data:', train.shape)\nprint('Test data:', test.shape)\nprint('Train label', train_label.shape)","f3a61804":"# Print out data information\nprint(train.info())","66e0cca3":"# Print out summary statistics\nprint(train.describe())\n\n# Visualize summary statistics\nfig, ax = plt.subplots(figsize=[13,5])  # default figsize = [6,4, 4.8]\nplt.boxplot(train)\nplt.show()","94ebf068":"# Let's split train data into train and validadtion set\nfrom sklearn.model_selection import train_test_split\ntrain_label = np.ravel(train_label)\nXtrain, Xval, ytrain, yval = train_test_split(train, train_label, random_state=42, test_size=0.2)\nprint(Xtrain.shape)\nprint(Xval.shape)","f7af30ab":"# Import classifiers from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n","810d5790":"num_neighs = np.arange(1,26)\ntrain_accuracy = []\nval_accuracy = []\nbest_accuracy_knn = 0.0\n\n# Run kNN over different hyperparameter values\nfor i, n in enumerate(num_neighs):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    # Fit kNN\n    knn.fit(Xtrain, ytrain)\n    # Train accuracy\n    train_accuracy.append(knn.score(Xtrain, ytrain))\n    # Validation accuracy\n    val_accuracy.append(knn.score(Xval, yval))\n    print(\"kNN (k={}): train accuracy={:.5f}, validation accuracy={:.5f}\"\n          .format(n, train_accuracy[i], val_accuracy[i]))\n    if val_accuracy[i] > best_accuracy_knn:\n        best_accuracy_knn = val_accuracy[i]\n        best_knn = knn\n        best_num_neighs = n\n\nplt.plot(num_neighs, train_accuracy, label='Train')\nplt.plot(num_neighs, val_accuracy, label='Validation')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors')\nplt.legend()\nplt.show()\n\nprint('Best validation accuracy (k={}): {:.5f}'.format(best_num_neighs, best_accuracy_knn))","a7c1d979":"estimators = [25, 50, 75, 100]  # number of estimators \nmax_depths = [20, 30, 40, 50]\nsplits = [5, 10, 15]\nparam = {'n_estimators':estimators, \n         'max_depth':max_depths, \n         'min_samples_split':splits}\ntrain_accuracy = []\nval_accuracy = []\n\nrf = RandomForestClassifier(random_state=100)\nRandomForestCV = GridSearchCV(estimator=rf, param_grid=param, cv=10)\nRandomForestCV.fit(Xtrain, ytrain)\ntrain_accuracy = RandomForestCV.score(Xtrain, ytrain)\nbest_accuracy_rf = RandomForestCV.score(Xval, yval)\n\n# Print results\nprint(RandomForestCV.best_params_)\nprint(\"RF: train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_rf))","af82ccbd":"svm = SVC(kernel='linear', random_state=100, C=1)\nsvm.fit(Xtrain, ytrain)\n# Train accuracy\ntrain_accuracy = svm.score(Xtrain, ytrain)\n# Validation accuracy\nbest_accuracy_svml = svm.score(Xval, yval)\nprint(\"SVM (linear): train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_svml))\n","30b9afbe":"svm = SVC(kernel='rbf', gamma='auto', random_state=100, C=1)\nsvm.fit(Xtrain, ytrain)\n# Train accuracy\ntrain_accuracy = svm.score(Xtrain, ytrain)\n# Validation accuracy\nbest_accuracy_svmr = svm.score(Xval, yval)\nprint(\"SVM (rbf): train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_svmr))","e86f2345":"print('KNN: {:.5f}'.format(best_accuracy_knn))\nprint('RF: {:.5f}'.format(best_accuracy_rf))\nprint('SVM (linear): {:.5f}'.format(best_accuracy_svml))\nprint('SVM (rbf): {:.5f}'.format(best_accuracy_svmr))","a4565236":"from sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\n\npca = PCA(0.95,whiten=True)\ntrain_pc = pca.fit_transform(train)\ntest_pc = pca.transform(test)\nprint(train_pc.shape)\nprint(test_pc.shape)","ca41989e":"# Let's fit GMM with various number of components\nn_components = np.arange(1,11)\nmodels = [GaussianMixture(n, covariance_type='full', random_state=100) \n          for n in n_components]\naics = [model.fit(train_pc).aic(train_pc) for model in models]\nbics = [model.fit(train_pc).bic(train_pc) for model in models]\n\nplt.plot(n_components, aics, label='AIC')\nplt.plot(n_components, bics, label='BIC')\nplt.xlabel('Number of Gaussians')\nplt.legend()\nplt.show()","4f7ca8b7":"best_gmm = GaussianMixture(n_components=4, covariance_type='full').fit(train_pc)\n\n# Let's apply the model to get probabilities\ntrain_gmm = best_gmm.predict_proba(train_pc)\ntest_gmm = best_gmm.predict_proba(test_pc)\nprint(train_gmm.shape)\nprint(test_gmm.shape)\n\n# Split the new data into train and validadtion set\nXtrain_gmm, Xval_gmm, ytrain_gmm, yval_gmm = train_test_split(train_gmm, train_label, random_state=42, test_size=0.2)","85df3464":"num_neighs = np.arange(2,26)\ntrain_accuracy = []\nval_accuracy = []\nbest_accuracy_knn_gmm = 0.0\n\n# Run kNN over different hyperparameter values\nfor i, n in enumerate(num_neighs):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    # Fit kNN\n    knn.fit(Xtrain_gmm, ytrain_gmm)\n    # Train accuracy\n    train_accuracy.append(knn.score(Xtrain_gmm, ytrain_gmm))\n    # Validation accuracy\n    val_accuracy.append(knn.score(Xval_gmm, yval_gmm))\n    print(\"kNN (k={}): train accuracy={:.5f}, validation accuracy={:.5f}\"\n          .format(n, train_accuracy[i], val_accuracy[i]))\n    if val_accuracy[i] > best_accuracy_knn_gmm:\n        best_accuracy_knn_gmm = val_accuracy[i]\n        best_knn = knn\n        best_num_neighs = n\n\nplt.plot(num_neighs, train_accuracy, label='Train')\nplt.plot(num_neighs, val_accuracy, label='Validation')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors')\nplt.legend()\nplt.show()\n\nprint('Best validation accuracy (k={}): {:.5f}'.format(best_num_neighs, best_accuracy_knn_gmm))","0e8a2230":"# Random Forest\nestimators = [25, 50, 75, 100]  # number of estimators \nmax_depths = [20, 30, 40, 50]\nsplits = [5, 10, 15]\nparam = {'n_estimators':estimators, \n         'max_depth':max_depths, \n         'min_samples_split':splits}\n\nrf = RandomForestClassifier(random_state=100)\nRandomForestCV = GridSearchCV(estimator=rf, param_grid=param, cv=10)  # cv=5 in default\nRandomForestCV.fit(Xtrain_gmm, ytrain_gmm)\ntrain_accuracy = RandomForestCV.score(Xtrain_gmm, ytrain_gmm)\nbest_accuracy_rf_gmm = RandomForestCV.score(Xval_gmm, yval_gmm)\n\n# Print results\nprint(RandomForestCV.best_params_)\nprint(\"RF: train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_rf_gmm))","a8b2a8a0":"# SVM (linear)\nsvm = SVC(kernel='linear', random_state=100, C=1)\nsvm.fit(Xtrain_gmm, ytrain_gmm)\n# Train accuracy\ntrain_accuracy = svm.score(Xtrain_gmm, ytrain_gmm)\n# Validation accuracy\nbest_accuracy_svml_gmm = svm.score(Xval_gmm, yval_gmm)\nprint(\"SVM (linear): train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_svml_gmm))","00c3c9c2":"# SVM (rbf)\nsvm = SVC(kernel='rbf', gamma='auto', random_state=100, C=1)\nsvm.fit(Xtrain_gmm, ytrain_gmm)\n# Train accuracy\ntrain_accuracy = svm.score(Xtrain_gmm, ytrain_gmm)\n# Validation accuracy\nbest_accuracy_svmr_gmm = svm.score(Xval_gmm, yval_gmm)\nprint(\"SVM (rbf): train accuracy={:.5f}, validation accuracy={:.5f}\"\n      .format(train_accuracy, best_accuracy_svmr_gmm))","82811c0d":"acc_raw = [best_accuracy_knn, best_accuracy_rf, best_accuracy_svml, best_accuracy_svmr]\nacc_gmm = [best_accuracy_knn_gmm, best_accuracy_rf_gmm, best_accuracy_svml_gmm, best_accuracy_svmr_gmm]\n\nres = pd.DataFrame({'Raw': acc_raw, 'GMM': acc_gmm}, index=['kNN','RF','SVM (linear)','SVM (rbf)'])\nprint(res)","9ecf2233":"best_knn.fit(train_gmm, train_label)\nypred = best_knn.predict(test_gmm)\nsubmission = {'Id': np.arange(1, ypred.shape[0]+1),\n             'Solution': ypred}\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv('submission_knn.csv', index=False)","966bcd55":"rf = RandomForestClassifier(max_depth=20, min_samples_split=5, n_estimators=25)\nrf.fit(train_gmm, train_label)\nypred = rf.predict(test_gmm)\nsubmission = {'Id': np.arange(1, ypred.shape[0]+1),\n             'Solution': ypred}\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv('submission_rf.csv', index=False)","57448eb6":"svm = SVC(kernel='linear', random_state=100, C=1)\nsvm.fit(train_gmm, train_label)\nypred = svm.predict(test_gmm)\nsubmission = {'Id': np.arange(1, ypred.shape[0]+1),\n             'Solution': ypred}\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv('submission_svm_linear.csv', index=False)","a04860a1":"svm = SVC(kernel='rbf', random_state=100, C=1)\nsvm.fit(train_gmm, train_label)\nypred = svm.predict(test_gmm)\nsubmission = {'Id': np.arange(1, ypred.shape[0]+1),\n             'Solution': ypred}\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv('submission_svm_rbf.csv', index=False)","4f43adb4":"## kNN","7e1c2c37":"# Option 1: Raw data\n## kNN","85d100aa":"## SVM (linear)","3c1ca8eb":"# Submission\nFrom the above, we can see that applying GMM increased the classification accuracy for all the four classifiers. We will re-train the four classifiers using entire samples and submit the results.","eaf14a28":"# Data Science London + Scikit-learn\nThis is a synthetic data set of 40 features, representing objects from two classes (labeled as 0 or 1). The training set has 1000 samples and the testing set has 9000.\nMore information can be found [here](https:\/\/www.kaggle.com\/c\/data-science-london-scikit-learn\/data)","3164e276":"# Import various classification models\nIn this notebook, we will try 5 different classifiers:\n* k-neighest neighbors (KNN)\n* Random forest (RF)\n* Support vector machine (SVM) with linear and rbf kernels\n\nWe will first train the four classifiers on the raw data. Then, we will do some feature engineering using Gaussian Mixture Models (GMM) to obtain new data, and re-trained the four classifiers on the new data.","740a6007":"# Option 2: Feature engineering\n## Gaussian mixture model (GMM)\nGMMs can have difficulty converging in a high dimensional space, so we will do dimensionality reduction on the data. Here, we will use a straightforward PCA, asking it to preserve 95% of the variance in the data","4d9faa8d":"It appears that around 4 components minimize both AIC and BIC. We can use the four cluster probabilities as new features.","44df745c":"# Summary\nLet's compare all the results we have gotten so far","05a78881":"## SVM (rbf)","c768ba17":"## RF","701f88d6":"Let's compare the final results when the classifiers were trained on the raw data","44735795":"## SVM (rbf)","880a1adf":"## RF","27097785":"## SVM (linear)"}}