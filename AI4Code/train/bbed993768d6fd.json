{"cell_type":{"fa0c7433":"code","0e718eae":"code","63c4c357":"code","430fa3d0":"code","1d456c4b":"code","6b092e26":"code","71700c9c":"code","654a3bca":"code","262d0c07":"code","0bf3e468":"code","20553fc2":"code","56403c9a":"code","9074a308":"code","0015ef4a":"code","353a218d":"markdown","a5757663":"markdown","d75918eb":"markdown","5032f4f8":"markdown","263d929f":"markdown","c708b431":"markdown","f8b5e97c":"markdown","c9ccde09":"markdown","566bc8c4":"markdown","cfdbbc36":"markdown","3a2dfb33":"markdown","0223508c":"markdown"},"source":{"fa0c7433":"import random\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport category_encoders as ce\nfrom xgboost import XGBClassifier\nimport h2o\nfrom h2o.automl import H2OAutoML\n\n# set seed for reproducability\nrandom.seed()\n\ntraining_data = pd.read_csv(\"..\/input\/practical-model-evaluation-day-2\/train_data_2018.csv\")\ntesting_data = pd.read_csv(\"..\/input\/practical-model-evaluation-day-2\/test_data_2018.csv\")\n\n# save out copy of testing data to use w\/ GCP\nwith open(\"test_data_2018.csv\", \"+w\") as file:\n    testing_data.to_csv(file, index=False, na_rep='NA')\n\n# split into predictors & target variables\nX_training = training_data.drop(\"job_title\", axis=1)\ny_training = training_data[\"job_title\"]\n\nX_testing = testing_data.drop(\"job_title\", axis=1)\ny_testing = testing_data[\"job_title\"]\n\n# encoded copy of our training data for training TPOT model\nencoder_X = ce.OrdinalEncoder()\nX_encoded = encoder_X.fit_transform(X_training)\nX_testing_encoded = encoder_X.transform(X_testing)\n\nencoder_y = ce.OrdinalEncoder()\ny_encoded = encoder_y.fit_transform(y_training)","0e718eae":"# load our saved XGBoost model\nxgboost_model = XGBClassifier()\nxgboost_model.load_model(\"..\/input\/practical-model-evaluation-day-2\/xgboost_baseline.model\")\nxgboost_model._le = LabelEncoder().fit(training_data[\"job_title\"])\n\n# initilaize H2o instance & load winning AutoML model\nh2o.init()\nh2o_model = h2o.load_model(\"..\/input\/practical-model-evaluation-day-2\/GBM_5_AutoML_20191205_060406\")\n\n# convert our data to h20Frame, an alternative to pandas datatables\n# (required for h20 AutoMl)\ntrain_data = h2o.H2OFrame(X_testing)\ntest_data = h2o.H2OFrame(list(y_testing))\ntest_data_h2o = train_data.cbind(test_data)\n\n# train new model using the pipeline generated by TPOT \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(X_encoded.values, y_encoded.values, random_state=None)\n\nexported_pipeline = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, max_features=0.7500000000000001, min_samples_leaf=3, min_samples_split=2, n_estimators=100, subsample=0.45)\nexported_pipeline.fit(training_features, training_target)","63c4c357":"from google.cloud import automl_v1beta1 as automl\nfrom kaggle.gcp import KaggleKernelCredentials\nfrom kaggle_secrets import GcpTarget\nfrom google.cloud import storage\n\n# don't change this value!\nREGION = 'us-central1' # don't change: this is the only region that works currently\n\n# these you'll change based on your GCP project\/data\nPROJECT_ID = 'kaggle-automl-example' # this will come from your specific GCP project\nDATASET_DISPLAY_NAME = 'data_jobs_info_2018' # name of your uploaded dataset (from GCP console)\nTARGET_COLUMN = 'job_title' # column with feature you're trying to predict\n\n# these can be whatever you like\nMODEL_DISPLAY_NAME = 'kaggle_automl_example_model' # what you want to call your model\nTRAIN_BUDGET = 1000 # max time to train model in milli-hours, from 1000-72000\n\nstorage_client = storage.Client(project=PROJECT_ID, credentials=KaggleKernelCredentials(GcpTarget.GCS)) \ntables_gcs_client = automl.GcsClient(client=storage_client, credentials=KaggleKernelCredentials(GcpTarget.GCS)) \ntables_client = automl.TablesClient(project=PROJECT_ID, region=REGION, gcs_client=tables_gcs_client, credentials=KaggleKernelCredentials(GcpTarget.AUTOML))","430fa3d0":"%%time\n\ntpot_predictions = exported_pipeline.predict(X_testing_encoded)","1d456c4b":"%%time\n\nxgb_predictions = xgboost_model.predict(X_testing_encoded)","6b092e26":"xgb_predictions","71700c9c":"%%time\n\nh20_predictions = h2o_model.predict(test_data_h2o)","654a3bca":"def download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","262d0c07":"%%time\n# name of the bucket to store your results & data in\nBUCKET_NAME = \"kaggle-automl-example\"\n# url of the data you're using to test\ngcs_input_uris = \"gs:\/\/kaggle-automl-example\/test_data_2018.csv\"\n# folder to store outputs in (you should create this folder)\ngcs_output_uri_prefix = 'gs:\/\/kaggle-automl-example\/predictions'\n\n# predict\ncloud_predictions = tables_client.batch_predict(\n    model_display_name=MODEL_DISPLAY_NAME, \n    gcs_input_uris=gcs_input_uris,\n    gcs_output_uri_prefix=gcs_output_uri_prefix\n)","0bf3e468":"# from here we need to download our result file\n# you can find the file path in the GCP console in the buckets for your project\nRESULT_FILE_PATH = \"gs:\/\/kaggle-automl-example\/predictions\/prediction-kaggle_automl_example_model-2019-12-05T05:07:28.873Z\/tables_1.csv\"\n\n# save to working directory\nwith open('cloud_automl_results.csv', \"wb\") as file_obj:\n     storage_client.download_blob_to_file(RESULT_FILE_PATH,\n                                  file_obj)\n        \n# load predictions into dataframe\ncloud_predictions_df =  pd.read_csv(\"cloud_automl_results.csv\")","20553fc2":"# TPOT Accuracy\ntpot_predictions_df = pd.DataFrame(data= {'job_title': tpot_predictions})\ntpot_predictions_unencoded = encoder_y.inverse_transform(tpot_predictions_df)\nprint(\"TPOT: \" + str(accuracy_score(y_testing, tpot_predictions_unencoded)))\n\n# H2O accuracy\nh20_predictions_df = h20_predictions.as_data_frame()\nprint(\"H2O: \" + str(accuracy_score(y_testing, h20_predictions_df.predict)))\n\n# XGBoost accuracy\nprint(\"XGBoost: \" + str(accuracy_score(y_testing, xgb_predictions)))\n\n# Cloud AutoML accuracy\nprediction_probs = cloud_predictions_df[cloud_predictions_df.columns[pd.Series(cloud_predictions_df.columns).str.startswith('job_title_')]]\ntitles_uncleaned = prediction_probs.idxmax(axis=1)\n\npredicted_titles_cloud = titles_uncleaned.str.replace(r'job_title_', '')\npredicted_titles_cloud = predicted_titles_cloud.str.replace(r'_score', '')\n\nprint(\"Cloud AutoML*: \" + str(accuracy_score(cloud_predictions_df.job_title, predicted_titles_cloud)))\nprint(\"* some rows missing from Cloud AutoML predictions\")","56403c9a":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.multiclass import unique_labels\n\n# function based one from SciKitLearn documention (https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html)\n# and is modified and redistributed here under a BSD liscense, https:\/\/opensource.org\/licenses\/BSD-2-Clause\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    \n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n    return ax","9074a308":"plot_confusion_matrix(xgb_predictions, testing_data[\"job_title\"], \n                      classes=unique_labels(testing_data[\"job_title\"]),\n                      normalize=True,\n                      title='XGBoost Confusion Matrix')\n\nplot_confusion_matrix(tpot_predictions_unencoded, testing_data[\"job_title\"], \n                      classes=unique_labels(testing_data[\"job_title\"]),\n                      normalize=True,\n                      title='TPOT Confusion Matrix')\n\nplot_confusion_matrix(h20_predictions_df[\"predict\"], testing_data[\"job_title\"], \n                      classes=unique_labels(testing_data[\"job_title\"]),\n                      normalize=True,\n                      title='H2O AutoML Confusion Matrix')\n\nplot_confusion_matrix(predicted_titles_cloud, cloud_predictions_df.job_title, \n                      classes=unique_labels(cloud_predictions_df[\"job_title\"]),\n                      normalize=True,\n                      title='Cloud AutoML Confusion Matrix')","0015ef4a":"# your code here!","353a218d":"# Comparing time\n\nFirst we'll think about how much time each of these models took.\n\n## Training\/retraining time\n\nFor each of these four types of models, you'll probably have to retrain from scratch if you want to do something like add a new class. Here are the training times for each of the models we trained yesterday:\n\n|   | Model        | Time to Train                      |\n|---|--------------|------------------------------------|\n| 1 | XGBoost      | 10.2 s \u00b1 71.7 ms (using %%timeit)  |\n| 2 | TPOT         | 10 - 15 minutes (depending on run) |\n| 3 | H2o AutoML   | 36 minutes (HT Erin LeDell)     |\n| 3 | Cloud AutoML | 1 Hour (user-specified)            |\n\nSo, if what you really care about is training a model as fast a possible, the XGBoost baseline is probably your best bet. But what about inference time?\n\n## Inference time\n\nBut what about how quickly each model can be used to make predictions? To figure this out, I'm going to be using the `%%time` magic, which runs a cell and reports how long it took to run.","a5757663":"Now that we're set up, let's get to evaluating models! ","d75918eb":"# Error analysis\n\nFor our error analysis, we're going to be using confusion matrices. The idea of a confusion matrix is that you have the actual labels on on axis, the predicted labels on the other axis and then the count or proportion of classifications in the matrix itself. They're mostly handy for quickly comparing performance across multiple classes, which is how we'll use them here.\n\nI've written a custom function, based on one\u00a0[from the SciKitLearn documentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html), to plot confusion matrices for us and I'm going to use it to compare classifications from the four different models. (I've collapsed the code for this so we can focus on the matrices themselves.","5032f4f8":"Today we're going to be evaluating our models using several different metrics: \n\n1. How long did they take to train?\n2. How long does it take them to do batch inference on the held out data?\n3. What's their overall accuracy? \n4. How well do they perform across cases?\n5. How does their performance change if we change one of our input features?\n\nLet's get coding! First, we'll need to load in all the libraries and data we'll need.","263d929f":"These notebooks are part of Kaggle\u2019s [Practical Model Evaluation](https:\/\/www.kaggle.com\/practical-model-evaluation) event, which ran from December 3-5 2019. You can find the [livestreams for this event here](https:\/\/youtu.be\/7RdKnACscjA?list=PLqFaTIg4myu-HA1VGJi_7IGFkKRoZeOFt).\n\n* Day 1 Notebook: [Figuring out what matters for you](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-1)\n* Day 2 Notebook: [Training models with automated machine learning](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-2)\n* Day 3 Notebook: [Evaluating our models](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-3)\n\n***","c708b431":"If you're using Cloud AutoML, you'll also need to connect your GCP account to this notebook ([this video show you how](https:\/\/www.youtube.com\/watch?v=xP99eh6nQN0)) and update the information below:","f8b5e97c":"# Counterfactuals\n\nThis one I'm going to have you do as an exercise. You'll need to fork this notebook and run the cells above this one first. Remember to remove the Cloud AutoML bits if you're not using it!\n\nLet's say one of my stakeholders has a background as a researcher. Based on their experience with the field, they think that someone who says that their job duties include \"do_research_that_advances_the_state_of_the_art_of_machine_learning\" should be classified as researcher, no matter what else they do. They want to see if our models agree with them.\nTo check this, there are several steps you'll need to follow:\n\nTo check this, there are several steps you'll need to follow:\n\n* Create a copy of the `testing_data` where every row in the column \"do_research_that_advances_the_state_of_the_art_of_machine_learning\" is \"Do research that advances the state of the art of machine learning\"\n* Use each of the three (or four if you're using Cloud AutoML) models to predict the job titles for this new testing dataset\n* Use the `plot_confusion_matrix` function to compare these new classifications to the classifications each model made when the correct information was in that column (this will show you how the model classifications changed rather than how the new classifications compare to the ground truth)\n* What results do you see? What does this suggest about how each model ","c9ccde09":"# Final Exercise!\n\nIf you were a data scientist working on this problem at a company, which model would you pick to put into production? You might want to consider:\n\n1. How long did they take to train?\n2. How long does it take them to do batch inference on the held out data?\n3. What's their overall accuracy? \n4. How well do they perform across cases?\n5. How does their performance change if we change one of our input features?\n6. Other factors not included here: what else do you think is important to consider?\n\nFeel free to share your answers in the comments of this notebook.","566bc8c4":"So, based on just inference time, it looks the TPOT model is the fastest of the four, followed by XGBoost, AutoML and then Cloud AutoML.","cfdbbc36":"# Load in our models\n\nNext we need to load in all our models that we trained yesterday.\n\n> **Tip:** If you want to use your own models, you can add your notebook from yesterday by clicking on [+ Add Data] in the upper right hand corner, then [Kernel Output Files] and search for your notebook. You'll need to have committed your notebook first, though!\n\nFor the TPOT model, we're training a new version of the winning pipeline.","3a2dfb33":"Looking at these confusion matrices, a few things jump out at me. First, that some classes seem to be more difficult than others, particularly \"Consultant\" and \"Data Engineer\". Secondly, that there's a lot of variation between models in how well they're handling specific classes. For example, the H2O model is more accurate than the TPOT model in identifying \"Data Analyst\" roles but less accurate at identifying \"Business Analyst\" roles. If one of those classes is more important to me, that's probably something I should consider when picking a model.","0223508c":"# Comparing metrics\n\nNow that we've got our predictions, let's compare the performance of these models in terms of metrics. For this example, I'm just going to look at raw accuracy: what proportion of job titles did each model assign correctly? (If we were looking at probabilities per class instead of predicted category we could use [log loss](https:\/\/www.kaggle.com\/dansbecker\/what-is-log-loss) instead, but let's just use accuracy here for simplicity.)\n"}}