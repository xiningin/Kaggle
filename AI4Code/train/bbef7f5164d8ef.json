{"cell_type":{"e22c5f32":"code","beb2d651":"code","23fc99f7":"code","5903200d":"code","55acc798":"code","6779bd47":"code","28f74f6c":"code","0cc7a48b":"code","a87779f5":"code","aff6e18a":"code","52d05fe2":"code","746e42c3":"code","dbddf6e0":"code","2dcaa429":"code","333acf28":"code","e3652526":"code","bf233250":"code","9d626160":"code","2907e983":"code","28a2ae03":"code","2e264362":"code","6b3c210d":"code","1554c34b":"code","6d71bd2e":"code","a3c35cdc":"code","77f6a54c":"code","99e91ac4":"code","781ca89f":"code","e6e30df2":"markdown","6e03f69d":"markdown","865c2fca":"markdown","6f8ef7ef":"markdown","33d3ac14":"markdown","0e13d326":"markdown","c31680b3":"markdown","0959b92a":"markdown","b05c6af8":"markdown","3b3cf5f0":"markdown","f8de516a":"markdown","969dece4":"markdown","577aa1d8":"markdown","fc9209b5":"markdown","929f9805":"markdown","2c5ced20":"markdown","1e3443ab":"markdown","c659519e":"markdown"},"source":{"e22c5f32":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom multiprocessing import cpu_count\n\nimport sys\nimport cv2\nimport imageio\nimport joblib\nimport pickle\n\n# Activate pandas progress apply bar\ntqdm.pandas()\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","beb2d651":"# Smaller side of the image, can be adjusted\nIMG_SIZE = 384\nN_CHANNELS = 3\nVERSION = '1A'\n\n# Total number of images\nN_ROWS = 1580470\n# Higher resolution will require more splits due to the 20GB dataset limit\nN_SPLITS = 3\nPART_N = 1","23fc99f7":"# Read train CSV with correct data types\ndtype = { 'id': 'string', 'landmark_id': np.uint32 }\ntrain = pd.read_csv('\/kaggle\/input\/landmark-recognition-2021\/train.csv', dtype=dtype)","5903200d":"print(f'First 10 landmark ids: {train[\"landmark_id\"].unique()[:10]}')","55acc798":"train['label'] = train['landmark_id'].astype('category').cat.codes","6779bd47":"label2landmark_id = train[['label', 'landmark_id']].drop_duplicates().set_index('label').squeeze().to_dict()\nlandmark_id2label = {v: k for k, v in label2landmark_id.items()}","28f74f6c":"with open('label2landmark_id.pkl', 'wb') as f:\n    pickle.dump(label2landmark_id, f)\n\nwith open('landmark_id2label.pkl', 'wb') as f:\n    pickle.dump(landmark_id2label, f)","0cc7a48b":"display(train.head())","a87779f5":"display(train.info())","aff6e18a":"# Get the Google Cloud Storage path for a given image\ndef to_gcs_file_path(i):\n    return f'{GCS_DS_PATH}\/train\/{i[0]}\/{i[1]}\/{i[2]}\/{i}.jpg'\n\n# Get Google Cloud Path to dataset\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('landmark-recognition-2021')\n\n# Assign Google Cloud Path\ntrain['gcs_file_path'] = train['id'].progress_apply(to_gcs_file_path).astype('string')","52d05fe2":"def to_file_path(i):\n    return f'\/kaggle\/input\/landmark-recognition-2021\/train\/{i[0]}\/{i[1]}\/{i[2]}\/{i}.jpg'\n\ntrain['file_path'] = train['id'].progress_apply(to_file_path).astype('string')","746e42c3":"display(train.head())","dbddf6e0":"# Save the updated train DataFrame\ntrain.to_pickle('train.pkl.xz')","2dcaa429":"START_IDX = int(N_ROWS * ((PART_N - 1) \/ N_SPLITS))\nEND_IDX = int(N_ROWS * (PART_N \/ N_SPLITS))\n\nprint(f'START_IDX: {START_IDX}, END_IDX: {END_IDX}')","333acf28":"# Drop all indices which do note belong to this part\nDROP_IDXS = train.loc[(train.index < START_IDX) | (train.index >= END_IDX)].index\ntrain.drop(DROP_IDXS, inplace=True)\n\nprint(f'DataFrame idx min: {train.index.min()}, idx max: {train.index.max()}')\n\n# Reset index\ntrain.reset_index(drop=True, inplace=True)","e3652526":"# Shuffle DataFrame\ntrain = train.sample(frac=1, random_state=42)","bf233250":"display(train.head())","9d626160":"display(train.info())","2907e983":"def process_image(file_path):\n    img = imageio.imread(file_path)\n    h, w, _ = img.shape\n\n    r = IMG_SIZE \/ min(w, h)\n    # Check whether image is bigger than IMG_SIZE\n    if min(h,w) > IMG_SIZE:\n        w_resize = int(w * r)\n        h_resize = int(h * r)\n        # Resize using LANCZOS algorithm\n        img = cv2.resize(img, (w_resize, h_resize), interpolation=cv2.INTER_LANCZOS4)\n        # Save as JPEG with quality set to 70, just as original images\n        img_jpeg = tf.io.encode_jpeg(img, quality=70, optimize_size=True).numpy()\n        return img_jpeg, h_resize, w_resize\n    # Otherwise use original image\n    else:\n        with open(file_path, 'rb') as f:\n            img_jpeg = f.read()\n        return img_jpeg, h, w","28a2ae03":"def output_size(N):\n    mean_size = 0\n    for fp in tqdm(train['file_path'].sample(N, random_state=42)):\n        img_jpeg, h, w = process_image(fp)\n        mean_size += len(img_jpeg) \/ N\n        \n    print(f'Estimated TFRecord output size: {len(train) * mean_size \/ 2**30:.2f}GB')\n\noutput_size(int(1e3))","2e264362":"def split_in_chunks(data, chunk_size):\n    return [data[:, i:i + CHUNK_SIZE] for i in range(0, len(data[1]), CHUNK_SIZE)]","6b3c210d":"CHUNK_SIZE = int(3e3)\n\ntrain_split = split_in_chunks(np.array((train['file_path'], train['label'])), CHUNK_SIZE)\n\nprint(f'train_split chunks: {len(train_split)}')","1554c34b":"def to_tf_records(data_split, name):\n    for idx, (fps, lbls) in enumerate(tqdm(data_split)):\n        \n        # Create image processing jobs and execute them in parallel\n        jobs = [joblib.delayed(process_image)(fp) for fp in fps]\n        imgs_resized = joblib.Parallel(\n            n_jobs=cpu_count(),\n            verbose=0,\n            batch_size=64,\n            pre_dispatch=64*cpu_count(),\n            require='sharedmem'\n        )(jobs)\n        tfrecord_name = f'{VERSION}_PART_{PART_N}_{name}_batch_{idx}.tfrecords'\n        \n        # Create the actual TFRecords\n        with tf.io.TFRecordWriter(tfrecord_name) as file_writer:\n            for (img, h, w), lbl in zip(imgs_resized, lbls):\n                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n                    # Image as JPEG bytes\n                    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n                    # Label of image\n                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(lbl)])),\n                    # Height of image\n                    'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(h)])),\n                    # Width of image\n                    'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(w)])),\n                })).SerializeToString()\n                file_writer.write(record_bytes)\n\n# Create TFRecords\nto_tf_records(train_split, 'train')","6d71bd2e":"# Imagenet mean and standard deviation per channel\nIMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\nIMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n\n# Number of channels, 3 for RGB images\nN_CHANNELS = tf.constant(3, dtype=tf.int64)","a3c35cdc":"# Function to decode the TFRecords\ndef decode_tfrecord(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n        'width': tf.io.FixedLenFeature([], tf.int64),\n        'height': tf.io.FixedLenFeature([], tf.int64),\n    })\n\n    image = tf.io.decode_jpeg(features['image'])\n    label = features['label']\n    height = features['height']\n    width = features['width']\n    \n    # Cutout Random Square if image is not square\n    if height != width:\n        if height > width:\n            offset = tf.random.uniform(shape=(), minval=0, maxval=height-width, dtype=tf.int64)\n            image = tf.slice(image, [offset, 0, 0], [width, width, N_CHANNELS])\n        else:\n            offset = tf.random.uniform(shape=(), minval=0, maxval=width-height, dtype=tf.int64)\n            image = tf.slice(image, [0, offset, 0], [height, height, N_CHANNELS])\n    \n    # Reshape and Normalize\n    size = tf.math.reduce_min([height, width])\n    # Explicit reshape needed for TPU, tell cimpiler dimensions of image\n    image = tf.reshape(image, [size, size, N_CHANNELS])\n    # Some images are smaller than 384x384 and need to be upscaled\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    # Convert to float32 and normalize to range 0-1\n    image = tf.cast(image, tf.float32)  \/ 255.0\n    # Normalize according to ImageNet mean and standard deviation\n    image = (image - IMAGENET_MEAN) \/ IMAGENET_STD\n    \n    return image, label","77f6a54c":"# Shows a batch of images\ndef show_batch(dataset, rows=4, cols=3):\n    imgs, lbls = next(iter(dataset))\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*4, rows*4))\n    for r in range(rows):\n        for c in range(cols):\n            img = imgs[r*cols+c].numpy().astype(np.float32)\n            img += abs(img.min())\n            img \/= img.max()\n            axes[r, c].imshow(img)\n            axes[r, c].set_title(f'Label: {lbls[r*cols+c]}')","99e91ac4":"def get_train_dataset():\n    FNAMES_TRAIN_TFRECORDS = tf.io.gfile.glob('.\/*.tfrecords')\n    train_dataset = tf.data.TFRecordDataset(FNAMES_TRAIN_TFRECORDS, num_parallel_reads=1)\n    train_dataset = train_dataset.map(decode_tfrecord, num_parallel_calls=1)\n    train_dataset = train_dataset.batch(32)\n    \n    return train_dataset","781ca89f":"train_dataset = get_train_dataset()\nshow_batch(train_dataset)","e6e30df2":"# Google Cloud File Paths","6e03f69d":"This helper function computer the estimated part size, this should be less than 20GB due to disk space limitations.","865c2fca":"Shuffling the DataFrame is import, as otherwise batches could consists of a single class. Shuffling the DataFrame makes sure every TFRecord will consists of a random set of classes.","6f8ef7ef":"# Split in Data Chunks","33d3ac14":"This next function creates the actual TFrecords. The images are processed using my favourite parallelism package [joblib](https:\/\/joblib.readthedocs.io\/en\/latest\/#).","0e13d326":"The dataset has to be split into multiple parts due to limited disk space. In the next cell the start and end index of the DataFrame are computed.","c31680b3":"# Check TFRecords","0959b92a":"Create mappings between the labels and landmark ids. This can be needed when converting predicted model labels back to landmark ids for example.","b05c6af8":"The landmark ids are not continious, as shown below. To get continous labels in a single line of code the landmark ids are converted to categories, where the label is the ordinal encoded category.","3b3cf5f0":"# Part Selection","f8de516a":"# Make TFRecords","969dece4":"Hello fellow Kagglers,\n\nThis notebook demonstrates how to create TFRecords for the Google Landmark Recognition 2021 competition. Since the dataset is huge with over 1.5 million images the dataset has to be split in 3 parts.\nThe datasets are made public and can be found here:\n\n[Part 1](https:\/\/www.kaggle.com\/markwijkhuizen\/landmark-recognition-2021-tfrecords-384-part-1)\n\n[Part 2](https:\/\/www.kaggle.com\/markwijkhuizen\/landmark-recognition-2021-tfrecords-384-part-2)\n\n[Part 3](https:\/\/www.kaggle.com\/markwijkhuizen\/landmark-recognition-2021-tfrecords-384-part-3)\n\nThe resolution is set to 384 with 384 being the smaller side of the image, thus images can for example have a resolution of 384x512 or 512x384.","577aa1d8":"Check whether the TFRecords are succesfully created","fc9209b5":"Split the dataset in chunks of 3000 images to get TFRecords of approximately 100MB","929f9805":"# Process Image","2c5ced20":"Processing the image is rather easy. The image is resized to have a smaller side of size 384. The computing intensize LANCZOS algorithm is used for resizing. I am not an expert on resizing algorithms, however the [PIL documentation](https:\/\/pillow.readthedocs.io\/en\/stable\/handbook\/concepts.html#filters-comparison-table) on filters state this algorithm performs the best.","1e3443ab":"# File Path","c659519e":"# Sample Size"}}