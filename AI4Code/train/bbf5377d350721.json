{"cell_type":{"94f05416":"code","036c448a":"code","0f938ed5":"code","018589c6":"code","cc3c8160":"code","0860d287":"code","c6397abe":"code","da710fa8":"code","8d965568":"code","e2d0bb67":"code","b668a538":"code","d633fb22":"code","46f68db3":"code","8c99debb":"code","83501af9":"code","66515c77":"code","968d7c91":"code","490d8268":"code","fe1a6a1a":"code","eadc631c":"code","c6de4ba3":"code","0b217a72":"code","df292891":"code","3f5279ea":"code","820e01a9":"code","8019103c":"code","2967074b":"code","bee354ed":"code","f8dd07c0":"code","31119627":"code","0552b8c6":"code","4dd16c46":"code","73229671":"code","5ce836d6":"code","2028ae72":"code","7bf9b178":"code","f4506f39":"code","5b7b678d":"code","617b3190":"code","da53cba1":"code","b828edb3":"code","525e3182":"code","f2f5a1b3":"code","2557b6c5":"code","aa6b8b66":"code","f226a972":"code","9886059e":"code","f8a9fd14":"code","be57452f":"code","c07b6fdc":"code","858ab3ce":"markdown","32f4e283":"markdown","e97ed64f":"markdown","f47c7c3e":"markdown","a02b95ea":"markdown","e5735a07":"markdown","dd8a7f61":"markdown","850d0eb8":"markdown","78f65064":"markdown","ef684ea0":"markdown","ba01cc34":"markdown","8d68d527":"markdown","abd0ba06":"markdown","64de19b7":"markdown","54dafce4":"markdown","fee0abb5":"markdown","27ad5bf7":"markdown","19524945":"markdown","f915f238":"markdown","f3c2eb15":"markdown","40780e1b":"markdown","4fbd68f5":"markdown"},"source":{"94f05416":"# data processing\nimport pandas as pd\n\n## linear algebra\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n \nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","036c448a":"df=pd.read_csv('..\/input\/titanic-data\/titanic.csv')","0f938ed5":"df.head()","018589c6":"df.shape","cc3c8160":"df.describe()","0860d287":"df.info()","c6397abe":"missing=df.isnull().sum().sort_values(ascending=False)\nmissing.head()","da710fa8":"percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n","8d965568":"missing_percentage=pd.concat([missing, percent], axis=1, keys=['Missing','Percent'])","e2d0bb67":"missing_percentage.head(15)","b668a538":"drop_column = ['Body','Cabin',]\ndf.drop(drop_column, axis= 1, inplace = True)","d633fb22":"df.head(5)\n","46f68db3":"drop_column = ['Lifeboat']\ndf.drop(drop_column, axis= 1, inplace = True)","8c99debb":"df['Age'].fillna(df['Age'].median(), inplace = True)\ndf['Survived'].fillna(df['Survived'].mode()[0], inplace = True)","83501af9":"df.isnull().sum()","66515c77":"df1 = df.dropna()\nprint(df1)","968d7c91":"df1.isnull().sum()","490d8268":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph= sns.countplot(x='Survived', hue=\"Survived\", data=df1)\n","fe1a6a1a":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Sex\", hue =\"Survived\", data = df1)\n","eadc631c":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4)) \nx = sns.countplot(df1['Pclass'], ax=ax[0])\ny = sns.countplot(df1['Embarked'], ax=ax[1])\n\nfig.show()","c6de4ba3":"drop_column = ['Embarked']\ndf1.drop(drop_column, axis=1, inplace = True)","0b217a72":"df1.head()","df292891":"plt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Pclass\", hue =\"Survived\", data = df1)\n","3f5279ea":"plt.figure(figsize = (8, 5))\nsns.barplot(x='Pclass', y='Survived', data=df1)","820e01a9":"axes = sns.factorplot('SibSp','Survived', \n                      data=df1, aspect = 2.5, )","8019103c":"axes = sns.factorplot('Parch','Survived', \n                      data=df1, aspect = 2.5, )","2967074b":"df1['Age_bin'] = pd.cut(df1['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n    \nplt.figure(figsize = (8, 5))\nsns.barplot(x='Age_bin', y='Survived', data=df1)","bee354ed":"plt.figure(figsize = (8, 5))\nsns.countplot(x='Age_bin', hue='Survived', data=df1)\n","f8dd07c0":"df1['Fare_bin'] = pd.cut(df1['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','median_fare','Average_fare','high_fare'])\nplt.figure(figsize = (8, 5))\nsns.countplot(x='Pclass', hue='Fare_bin', data=df1)\n","31119627":"sns.barplot(x='Fare_bin', y='Survived', data=df1)","0552b8c6":"df1.head()","4dd16c46":"pd.DataFrame(abs(df1.corr()['Survived']).sort_values(ascending = False))","73229671":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = df1.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax,annot=True)","5ce836d6":"df1.info()","2028ae72":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\ndf1['Sex'] = df1['Sex'].map(genders)\ndf1['Sex'].value_counts()","7bf9b178":"drop_column = ['Age_bin','Fare','Name','Ticket', 'PassengerId','WikiId','Name_wiki','Age_wiki','Hometown','Boarded','Destination','Fare_bin']\ndf1.drop(drop_column, axis=1, inplace = True)","f4506f39":"df1.head()","5b7b678d":"all_features = df1.drop(\"Survived\",axis=1)\nTargete = df1[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targete,test_size=0.3,random_state=0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","617b3190":"# standard scaling \nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","da53cba1":"# Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state = 22)\nlogreg.fit(X_train, y_train)\n\n","b828edb3":"# Support Vector Classifier Algorithm\nfrom sklearn.svm import SVC\nsvc = SVC(kernel = 'linear', random_state = 22)\nsvc.fit(X_train, y_train)","525e3182":"# Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n","f2f5a1b3":"# Decision tree Algorithm\nfrom sklearn.tree import DecisionTreeClassifier\ndectree = DecisionTreeClassifier(criterion = 'entropy', random_state = 22)\ndectree.fit(X_train, y_train)\n","2557b6c5":"# Random forest Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nranfor = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', random_state = 22)\nranfor.fit(X_train, y_train)","aa6b8b66":"# Making predictions on test dataset\nY_pred_logreg = logreg.predict(X_test)\n#Y_pred_knn = knn.predict(X_test)\nY_pred_svc = svc.predict(X_test)\nY_pred_nb = nb.predict(X_test)\nY_pred_dectree = dectree.predict(X_test)\nY_pred_ranfor = ranfor.predict(X_test)","f226a972":"# Evaluating using accuracy_score metric\nfrom sklearn.metrics import accuracy_score\naccuracy_logreg = accuracy_score(y_test, Y_pred_logreg)\n#accuracy_knn = accuracy_score(y_test, Y_pred_knn)\naccuracy_svc = accuracy_score(y_test, Y_pred_svc)\naccuracy_nb = accuracy_score(y_test, Y_pred_nb)\naccuracy_dectree = accuracy_score(y_test, Y_pred_dectree)\naccuracy_ranfor = accuracy_score(y_test, Y_pred_ranfor)","9886059e":"# Accuracy on test set\nprint(\"Logistic Regression: \" + str(accuracy_logreg * 100))\n#print(\"K Nearest neighbors: \" + str(accuracy_knn * 100))\nprint(\"Support Vector Classifier: \" + str(accuracy_svc * 100))\nprint(\"Naive Bayes: \" + str(accuracy_nb * 100))\nprint(\"Decision tree: \" + str(accuracy_dectree * 100))\nprint(\"Random Forest: \" + str(accuracy_ranfor * 100))","f8a9fd14":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, Y_pred_logreg)\ncm","be57452f":"# Heatmap of Confusion matrix\nsns.heatmap(pd.DataFrame(cm), annot=True)","c07b6fdc":"# Classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, Y_pred_ranfor))","858ab3ce":"**If you liked please upvote. Thank you**","32f4e283":"## Data Visualization","e97ed64f":"##### people in Pclass 1 with high fare had a higher survival chance, and people with low to average fare had a very low survival rate","f47c7c3e":"### Predictive modelling","a02b95ea":"##### Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person was in class 1, and class 3 proved the least chances of survival. We will create another pclass plot below","e5735a07":"### Feature engineering","dd8a7f61":"##### Children below 12 ears of age had higher chances of survival,as we can assume that parents and siblings might have saved the younger ones before themselves.","850d0eb8":"##### Both the above plots describe that the chances of survival goes down as the member in a family increases","78f65064":"##### Logistic regression gives the best accuracy score","ef684ea0":"### Age vs survived","ba01cc34":"### Sibsp and parch vs survived","8d68d527":"## Loading Data","abd0ba06":"### Evaluation","64de19b7":"# A detailed statistical analysis of Titanic data set along with Machine learning model implementation","54dafce4":"## Data Cleaning","fee0abb5":"#### Since it does not matter from where didi the passenger board ,as it is more important that the passenger was currently on Titanic,as we know that At 2:20 a.m. on April 15, 1912, the British ocean liner Titanic sinks into the North Atlantic Ocean.We can use Embarked as feature here for getting high accuracy but logically its doesn't matter. so we drop it out.","27ad5bf7":"### Correlation matrix","19524945":"#### The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\n#### On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n#### While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. Here I am doing an attempt to explore the same.","f915f238":"### Male and female survival count","f3c2eb15":"### Passenger class impact on survival","40780e1b":"### Embarked and p-class vs survival\n\n#### Embarked: From which location passenger go on board to Titanic.\n\nC = Cherbourg\nQ = Queenstown\nS = Southampton\n\n\n#### PClass: Passenger belongs to which class.\n\n1st = Upper\n2nd = Middle\n3rd = Lower","4fbd68f5":"### Fare vs survived"}}