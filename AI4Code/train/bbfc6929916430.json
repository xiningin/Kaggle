{"cell_type":{"476111fa":"code","85e7ff85":"code","1e2b5eeb":"code","8427028b":"code","030303f6":"code","cc31d4b1":"code","e1e33175":"code","b64577d0":"code","6524105c":"code","8a97b696":"code","024eb479":"code","f884bd74":"code","db3d8581":"markdown","f5096674":"markdown","fb115af8":"markdown","8949dd99":"markdown","668c3b24":"markdown","c992616e":"markdown","44a52b06":"markdown","8fb80efc":"markdown","196fdcfc":"markdown","8f944559":"markdown","69fc836f":"markdown"},"source":{"476111fa":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","85e7ff85":"import torch\nimport torch.nn as nn\n\nimport requests\nimport urllib.request\n\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom torchvision import transforms","1e2b5eeb":"class ResNetBlock(nn.Module): # <1>\n\n    def __init__(self, dim):\n        super(ResNetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim)\n\n    def build_conv_block(self, dim):\n        conv_block = []\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim),\n                       nn.ReLU(True)]\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x) # <2>\n        return out\n\n\nclass ResNetGenerator(nn.Module):\n\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): # <3> \n\n        assert(n_blocks >= 0)\n        super(ResNetGenerator, self).__init__()\n\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n                 nn.InstanceNorm2d(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=True),\n                      nn.InstanceNorm2d(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResNetBlock(ngf * mult)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult \/ 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=True),\n                      nn.InstanceNorm2d(int(ngf * mult \/ 2)),\n                      nn.ReLU(True)]\n\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input): # <3>\n        return self.model(input)","8427028b":"netG = ResNetGenerator()","030303f6":"model_path = '\/kaggle\/input\/horse2zebra-tensor-parameters\/horse2zebra_0.4.0.pth'\nmodel_data = torch.load(model_path)\nnetG.load_state_dict(model_data)","cc31d4b1":"netG.eval()","e1e33175":"preprocess = transforms.Compose([transforms.Resize(256),\ntransforms.ToTensor()])","b64577d0":"# img = Image.open(\"\/kaggle\/input\/cgan-data\/horse2zebra\/horse2zebra\/trainA\/n02381460_901.jpg\")  # \u043f\u043e\u043b\u043e\u0441\u0430\u0442\u0430\u044f \u0433\u0440\u0438\u0432\u0430 (=\n# img = Image.open(\"\/kaggle\/input\/cgan-data\/horse2zebra\/horse2zebra\/trainA\/n02381460_1766.jpg\")  # not bad\n# img = Image.open(\"\/kaggle\/input\/cgan-data\/horse2zebra\/horse2zebra\/trainA\/n02381460_2594.jpg\")\n\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FPkiJUQIBLI0%2Fmaxresdefault.jpg&f=1&nofb=1\"  # arabic black zebra (=\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmNFPs0KVu6Q%2Fmaxresdefault.jpg&f=1&nofb=1\"\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.PcV9jVT4-lrLK_tFtLuj_wHaEr%26pid%3DApi&f=1\"\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.smvnnog_xSu4b9jfT7fi_gHaEK%26pid%3DApi&f=1\"\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fimg-fotki.yandex.ru%2Fget%2F4713%2F656708.d%2F0_785ec_f45c557d_XXL&f=1&nofb=1\"\nurl = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Flifeglobe.net%2Fmedia%2Fentry%2F6445%2F1a.jpg&f=1&nofb=1\"\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fblog.luxvacation.com%2Fcontent%2Fimages%2F2015%2F12%2FRacing_at_Arlington_Park.jpg&f=1&nofb=1\"\n# url = \"https:\/\/raw.githubusercontent.com\/deep-learning-with-pytorch\/dlwpt-code\/master\/data\/p1ch2\/bobby.jpg\"\n# url = \"https:\/\/raw.githubusercontent.com\/deep-learning-with-pytorch\/dlwpt-code\/master\/data\/p1ch2\/horse.jpg\"\n\n\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\n\n\nimg","6524105c":"img_t = preprocess(img)\nbatch_t = torch.unsqueeze(img_t, 0)","8a97b696":"batch_out = netG(batch_t)","024eb479":"out_t = (batch_out.data.squeeze() + 1.0) \/ 2.0\nout_img = transforms.ToPILImage()(out_t)","f884bd74":"out_img","db3d8581":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","f5096674":"Let\u2019s\npass it through preprocessing and turn it into a properly shaped variable:","fb115af8":"https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-dog-detection","8949dd99":"At this point, netG has acquired all the knowledge it achieved during training. Note\nthat this is fully equivalent to what happened when we loaded resnet101 from torch-\nvision in section 2.1.3; but the torchvision.resnet101 function hid the loading\nfrom us.\nLet\u2019s put the network in eval mode, as we did for resnet101 :","668c3b24":"batch_out is now the output of the generator, which we can convert back to an image:","c992616e":"We\u2019re ready to load a random image of a horse and see what our generator pro-\nduces.","44a52b06":"Then we define a few input transformations to make sure data enters the network with\nthe right shape and size:","8fb80efc":"The netG model has been created, but it contains random weights. We mentioned ear-\nlier that we would run a generator model that had been pretrained on the horse2zebra\ndataset, whose training set contains two sets of 1068 and 1335 images of horses and\nzebras, respectively. The dataset be found at http:\/\/mng.bz\/8pKP. The weights of the\nmodel have been saved in a .pth file, which is nothing but a pickle file of the model\u2019s tensor parameters. We can load those into ResNetGenerator using the model\u2019s load\n_state_dict method:","196fdcfc":"Playing with a pretrained CycleGAN will give us the opportunity to take a step\ncloser and look at how a network\u2014a generator, in this case\u2014is implemented. We\u2019ll\nuse our old friend ResNet. We\u2019ll define a ResNetGenerator class offscreen. The code\nis in the first cell of the 3_cyclegan.ipynb file, but the implementation isn\u2019t relevant\nright now, and it\u2019s too complex to follow until we\u2019ve gotten a lot more PyTorch experi-\nence. Right now, we\u2019re focused on what it can do, rather than how it does it. Let\u2019s\ninstantiate the class with default parameters (code\/p1ch2\/3_cyclegan.ipynb):","8f944559":"We shouldn\u2019t worry about the details right now. The important thing is that we follow\nfrom a distance. At this point, batch_t can be sent to our model:","69fc836f":"Let\u2019s open a horse file:"}}