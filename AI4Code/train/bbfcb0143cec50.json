{"cell_type":{"6404fa11":"code","fa7a4686":"code","c7d08331":"code","25f323ea":"code","6022cead":"code","221c67ea":"code","151f62c1":"code","81a3d278":"code","93bda900":"code","6e4a3858":"code","7303fcd3":"code","78ffbe41":"code","8cd3deca":"code","7f7def96":"code","8282268a":"code","cc61d2bc":"code","31f985ef":"code","772cc123":"code","85964b36":"code","2961dd4a":"code","4135137e":"code","8ddc2d3e":"code","450c2e64":"code","544de49e":"code","84b6a5a1":"code","03db7ac7":"code","981b3401":"code","c3891f87":"code","8b65f40c":"code","51b2a57a":"code","e84ad346":"code","5851b1a2":"code","08c6e72f":"code","b6d051d0":"code","df3cba87":"code","01c660ff":"code","26fcc1bd":"code","84b090d5":"code","81fc1411":"code","1238370b":"code","e0ca4414":"code","8101f7b9":"code","ce2c99c0":"code","785178ed":"code","d0a98c8c":"code","a6f8b355":"code","4f8e4cff":"code","26640205":"code","112eab71":"code","8e54f4f9":"code","924f87f9":"code","cdcee716":"code","a3dd3504":"code","739a6bac":"code","67027b11":"code","ac3ad538":"code","997e22a9":"code","2d7752af":"code","649b8ea7":"code","1aa1e2e0":"code","14ca9eda":"code","f86dd64f":"code","1f07c2e2":"code","30df432a":"markdown","57bb5ac0":"markdown","c6c81029":"markdown","84995c39":"markdown","ed8b140b":"markdown","d36c4f1b":"markdown","dd083ce4":"markdown","7834e0d2":"markdown","51951b8c":"markdown","3839babb":"markdown","200bfff1":"markdown","abd9aff4":"markdown","2b3d5eab":"markdown","52374a60":"markdown","c38251d3":"markdown","fec2b3ce":"markdown","36a9c14e":"markdown","a657b661":"markdown","25224719":"markdown","656f9c9c":"markdown","08b2c45c":"markdown","5515e979":"markdown","2ceb9491":"markdown","36b51590":"markdown","d857f545":"markdown","9e04a074":"markdown","79547ee9":"markdown","bceb1aff":"markdown","1a2ed806":"markdown","6273df08":"markdown","171e4034":"markdown","9af6245b":"markdown","32c1a93f":"markdown","bf227378":"markdown","14b9d7be":"markdown","fc183c0e":"markdown","73873b5e":"markdown","def8d9bd":"markdown","5989cf9c":"markdown","794eebda":"markdown","3d92723c":"markdown","05239ded":"markdown","bfcf3512":"markdown","9a2b1ca8":"markdown","cb03be4a":"markdown","596b96a6":"markdown","d463d623":"markdown","c93232cb":"markdown","c55e8518":"markdown"},"source":{"6404fa11":"# Just because I have a conflicting Python 3.6 installation at root\n# Comment this when uploading to kaggle\n# try:\n#     sys.path.remove('C:\/Python36\/Lib\/site-packages')\n# except:\n#     print(\"sys.path already ok\")\n    \nimport sys\nimport copy\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing","fa7a4686":"%matplotlib inline\nsns.set()\n\n# Loading data\n# adultTrain = pd.read_csv(\n#     \"C:\/Users\/bruno\/Desktop\/kaggle-adult-comp-knn\/data\/train_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# For uploading to kaggle\nadultTrain = pd.read_csv(\n    \"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\n# adultTest = pd.read_csv(\n#     \"C:\/Users\/bruno\/Desktop\/kaggle-adult-comp-knn\/data\/test_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# For uploading to kaggle\nadultTest = pd.read_csv(\n    \"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\nmodifyNames = {\n    \"fnlwgt\": \"weight\",\n    \"education.num\": \"educationNum\", \n    \"marital.status\": \"maritalStatus\",\n    \"capital.gain\": \"capitalGain\", \n    \"capital.loss\": \"capitalLoss\",\n    \"hours.per.week\": \"hoursPerWeek\", \n    \"native.country\": \"country\",\n    \"income\": \"target\"\n}\n\n# Changing columns names\nadultTrain.rename(columns=modifyNames, inplace=True)\nadultTest.rename(columns=modifyNames, inplace=True)\n\n# Casting appropriate datatypes\ndtypes = {\n    \"age\": int,\n    \"workclass\": str,\n    \"weight\": int,             \n    \"education\": str,\n    \"educationNum\": int,\n    \"maritalStatus\": str,\n    \"occupation\": str,\n    \"relationship\": str,\n    \"race\": str,\n    \"sex\": str,\n    \"capitalGain\": int,\n    \"capitalLoss\": int,\n    \"hoursPerWeek\": int,\n    \"country\": str,\n    \"target\": str\n}\n\nadultTrain.astype(dtypes, copy=False)\nadultTest.astype(dtypes.pop(\"target\"), copy=False)\n\n# Id is not relevant, so it is dropped\nadultTrain.pop(\"Id\")\nidTest = adultTest.pop(\"Id\")\n\n# weight is not important for testing\nweightTrain = adultTrain[\"weight\"]\nadultTest.pop(\"weight\")\n\n# IMPORTANT: need to take levararge of weights by scaling datapoint features!\n\nprint(\"\\n\\n#### TRAIN DATASET ####\")\n# (32560, 16)\nprint('\\nshape: ', adultTrain.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTrain.dtypes)\n# max of 4000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTrain.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTrain.duplicated().sum()) \n\nprint(\"\\n\\n#### TEST DATASET ####\")\n# (16280, 15)\nprint('\\nshape: ', adultTest.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTest.dtypes)\n# max of aprox 2000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTest.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTest.duplicated().sum()) ","c7d08331":"# education can be dropped, since educationNum is givving all the information we want\n# there is notinh specific about a certain degree that will affect the target\nadultTrain.head(20)","25f323ea":"adultTest.head(10)","6022cead":"adultTrain.describe()","221c67ea":"# aprox 25 000 datapoints <= 50K and 7 500 < 50K -> relatively imbalanced dataset\n# most simple baseline is prediciting always <= 50K -> gives 0.76% accuracy\ncounts = adultTrain[\"target\"].value_counts().values\nimbalanceRatio = counts[0]\/counts[1]\nprint(imbalanceRatio)\nadultTrain[\"target\"].value_counts().plot(kind=\"bar\")\n","151f62c1":"# capitalGain and capitalLoss have very few examples\n# ideas\n    # 1. exclude these festures\n    # 2. cluster them in two bins -> will become boolean variables\nprint(adultTrain.astype(bool).sum(axis=0))","81a3d278":"# hoursPerWeek could be dividid in three bins:  <30, 30-50, >50\n# educationNUm could be dividid in four bins: <8, 8-10, 10-12, >13\n# capitalGains and capitalLoss needs to actuallt only form one feature \n# that is capitalLiquid = capitalGains - capitalLoss. \n# The effect of this feature will be almost as of a imbalanced binary variable since almost all values are zero\n# and the other are in a small range\nadultTrain.hist(bins=30, figsize=(15, 10))","93bda900":"# Private is way bigger than the rest (therefore the rest of the classes have little data)\n# Without pay and never work have very few examples (14) but these examples guarantee we know the target\n# Ideas:\n    # 1. Cluster into 3 bins: private, {without pay + ever worked},  and rest -> \n    # but need to see if private and rest have distinct relatinships with target\nprint('\"Without-pay\" or \"Never-worked\" datapoints: ', adultTrain[adultTrain[\"workclass\"] == (\"Without-pay\" or \"Never-worked\")].shape[0])\nadultTrain[\"workclass\"].value_counts().plot(kind=\"bar\")","6e4a3858":"# This feature will be excluded, educationNum already gives us the info we need. There is nothing specific to a \n# certain category that would be relevant for predicting the target\nadultTrain[\"education\"].value_counts().plot(kind=\"bar\")","7303fcd3":"# A priori a would think only having a present spouse or not is important\n# So this could be cluster into two groups: present spouse and not present spouse\nadultTrain[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","78ffbe41":"# Each of the categories seem to be very important\nadultTrain[\"occupation\"].value_counts().plot(kind=\"bar\")","8cd3deca":"# This feature seem a little weird, it doest provide mmuch new info, \n# and the categories dont seem to be mutually exclusive\n# Idea: exclude this feature\nadultTrain[\"relationship\"].value_counts().plot(kind=\"bar\")","7f7def96":"# this could be divided into two bins: white and black \n# because the rest doesnt have data and my guess they would be very similar to white\nadultTrain[\"race\"].value_counts().plot(kind=\"bar\")","8282268a":"# a priori seems to be important\nadultTrain[\"sex\"].value_counts().plot(kind=\"bar\")","cc61d2bc":"# Surely maintaning all these low data categories will fit statistical noise and ruin the accuracy\n# Ideas: \n    # 1. divide in two bins: developed and not developed ccontries\n    # 2. divide in two bins: USA and rest\nadultTrain[\"country\"].value_counts().plot(kind=\"bar\")","31f985ef":"# no surprises here\nadultTest.hist(bins=30, figsize=(15, 10))","772cc123":"# no surprises here\nadultTest[\"workclass\"].value_counts().plot(kind=\"bar\")","85964b36":"# no surprises here\nadultTest[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","2961dd4a":"# no surprises here\nadultTest[\"occupation\"].value_counts().plot(kind=\"bar\")","4135137e":"# no surprises here\nadultTest[\"race\"].value_counts().plot(kind=\"bar\")","8ddc2d3e":"# no surprises here\nadultTest[\"country\"].value_counts().plot(kind=\"bar\")","450c2e64":"## OBS: the plots below dont consider the dataset imbalaca, therefore, all ratios are essentially multiplied\n# by a factor of 2.3 in favour of <50K.","544de49e":"# for <30 it is almost certain that wage <50K; 30-40 roughly the same; 40-50 >50K has good advantage\n# <50K decays linearly with age, while 50K is like a normal function centered in 43\nsns.catplot(x=\"target\", y=\"age\", kind=\"violin\", inner=None, data=adultTrain)","84b6a5a1":"# for <10 <50K has a good advantage; 10-12.5 same; >12.5 >50K has very good advantage\nsns.catplot(x=\"target\", y=\"educationNum\", kind=\"violin\", inner=None, data=adultTrain)","03db7ac7":"# for <40 <50K has a good advantage; for >40 >50K has a good advantage\nsns.catplot(x=\"target\", y=\"hoursPerWeek\", kind=\"violin\", inner=None, data=adultTrain)","981b3401":"sns.catplot(x=\"target\", y=\"capitalGain\", kind=\"violin\", inner=None, data=adultTrain)","c3891f87":"sns.catplot(x=\"target\", y=\"capitalLoss\", kind=\"violin\", inner=None, data=adultTrain)","8b65f40c":"# OBS: I am multiplying the counts of >50K by the imbalaceRatio to decouple \n# the fact that the dataset is imbalaced from differences in distribution of the feature","51b2a57a":"# Private & Self-empinc differ a little, the rest is roughly the same, so can be grouoed \n# into a single category called other\ncountsDf = adultTrain[[\"target\",\"workclass\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","e84ad346":"# all ctegories are different, thus maintaining all of them seems the way to go\ncountsDf = adultTrain[[\"target\",\"maritalStatus\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","5851b1a2":"# tranposrt moving, tech support, sales, creaf repair dont seem to help distringuish, so could be grouped\n# into a single category named rest\ncountsDf = adultTrain[[\"target\",\"occupation\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","08c6e72f":"# black and other dimishes for over >50K but white dominates in both\n# I think grouping into white and non-white is a valid approach here\ncountsDf = adultTrain[[\"target\",\"race\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","b6d051d0":"# i think this can be mexico and non-mexico because the rest of the categories have so little data\n# that it is likely that we are fittng statistical noise\ncountsDf = adultTrain[[\"target\",\"country\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(15, 10))","df3cba87":"# age limmits >50K even with high education and hours per week\n# age < 35 seems to be good indicator -> could maybe be binary variable\n\n# capitalGain > aprox 5 000 seems to be a great separator \n# capitalGain > 50 000 guarantees >50K \n# could be categorical variable\n\n# educationNum > 10 seems to be good indicator also\n \n# 1 000 < capital loss < 3 000 can be good \n\n# hours per week < 50 good\nsns.pairplot(adultTrain, hue=\"target\")","01c660ff":"# all numerical features with very low correlation\nsns.heatmap(adultTrain.corr())","26fcc1bd":"adultTrainDummies = pd.get_dummies(adultTrain[[\"workclass\", \"maritalStatus\", \"occupation\", \"race\", \"country\"]])\ndummy_features = adultTrainDummies.columns.values\npivots = []\nfor feature in dummy_features:\n    rest_of_features = dummy_features[dummy_features != feature]\n    new_pivot = adultTrainDummies.groupby(feature)[rest_of_features].sum().fillna(0)\n    pivots.append(new_pivot)\n\nfullPivot = pd.concat(pivots)[dummy_features]\nfullPivotOnes = fullPivot.iloc[lambda x: x.index > 0]\nfullPivotOnes.set_index(adultTrainDummies.columns, inplace=True)\n\ndef normalize_pivot_tables(fullPivot):\n    vec = np.array(fullPivot.sum(axis=1).values)\n    sizeDummies = vec.size\n    normMatrix = np.zeros((sizeDummies, sizeDummies))\n    for i, element in enumerate(vec):\n        for j, element2 in enumerate(vec):\n            normMatrix[i][j] = element + element2\n                        \n    normDf = pd.DataFrame(normMatrix, columns=fullPivot.columns)\n    normDf.set_index(fullPivot.columns, inplace=True)\n    fullPivotNorm = fullPivot.div(normDf)\n    return fullPivotNorm\n\nfullPivotNorm = normalize_pivot_tables(fullPivotOnes) # P(X1 = 1, X2 = 1)\n\n# dataset is too big, so will divide in two for plotting heatmaps\n#fullPivot2 = fullPivot.iloc[37:, :37] # down left -> not useful\n#fullPivot4 = fullPivot.iloc[37:, 37:] # down right -> country vs country -> not useful\nfullPivotNorm1 = fullPivotNorm.iloc[:37, :37] # top left\nfullPivotNorm3 = fullPivotNorm.iloc[:37:, 37:] # top right\n\n#OBS: dummy features with same prefix are mutually exclsusive, \n# therefore they will have joint prob equal to zero \n\n# max joint probability is aprox 0.1 in entire categorical combinations dataset, \n# therefore all categorical features are relatively independent from each other\n","84b090d5":"fullPivotNorm.describe()","81fc1411":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm1,ax=ax)","1238370b":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm3, ax=ax)","e0ca4414":"numColumns = [\"age\", \"capitalGain\", \"capitalLoss\", \"educationNum\", \"hoursPerWeek\"] # obs: left weight out\ncatColumns = [\"country\", \"education\", \"maritalStatus\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"] # obs: left target out\ntargetTrain = adultTrain[\"target\"]\nadultTrainNum = adultTrain[numColumns]\nadultTrainCat = adultTrain[catColumns]","8101f7b9":"adultTrainNum.head()","ce2c99c0":"adultTrainCat.head()","785178ed":"adultTrainNum = (adultTrainNum-adultTrainNum.mean())\/adultTrainNum.std()\nadultTrainNum.head()","d0a98c8c":"# Target-encoding \n# encoder = TargetEncoder()\n# encoder.fit_transform(adultTrainCat, adultTrain[\"target\"])\n# Simple one-hot encoding (this will be chosen one for now)\nadultTrainCat = pd.get_dummies(adultTrainCat)\nadultTrainCat.head()","a6f8b355":"adultTrain = pd.concat([adultTrainNum, adultTrainCat], axis=1)\nadultTrain.head()","4f8e4cff":"# Two main options\n# 1. Just thorw away rows with missing values\n# 2. Replace with mean of colummn (this will be chosen one for now)\nadultTrain.fillna(adultTrain.mean(), inplace=True)","26640205":"# Outliers don't mess much with KNN, since we we only looking to locally close points, so won't be necessary","112eab71":"# knn classiffier distance metric and algorithm can do this job already","8e54f4f9":"adultTestNum = adultTest[numColumns]\nadultTestCat = adultTest[catColumns]\n\nadultTestNum = (adultTestNum-adultTestNum.mean())\/adultTestNum.std() # broadcasts to columns by default\n\nadultTestCat = pd.get_dummies(adultTestCat)\nadultTestCat = adultTestCat.reindex(columns = adultTrainCat.columns, fill_value=0) # equivalent to fit transform\n\nadultTest = pd.concat([adultTestNum, adultTestCat], axis=1)\n\nadultTest.fillna(adultTest.mean(), inplace=True)\n\nadultTest.head()","924f87f9":"# Primising Engineered Datasets (v0)\n# Disct which will hold different feature engineered candidate datsets\npromisingDatasets = {}","cdcee716":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # with the given weights, the rows can be resampled according to their weight\n# # number of rows = weight_factor*minMaxNormalized_weight where the weight factor is large enough so that the sampling\n# # can give different integer values for most of the rows, \n# # but if it is too large becomes computationally heavy\n# weightTrainNorm = ((weightTrain) - weightTrain.min())\/(weightTrain.max() - weightTrain.min())\n# weightFactor = 50 # (can be altered later)\n# knnNeighboursFactor = weightTrainNorm.mean()*50 # expected value of number of columns added\n# print('knnNeighboursFactor:', knnNeighboursFactor)\n\n# # adultTrain70Importance will contain replicas only of row in it, will used to train KNN, \n# # that will be tested in adultTrain30ImportanceCV, which wasnt replicated\n# adultTrainShuffled = adultTrain.sample(frac=1)\n# adultTrain70Importance, adultTrain30Importance = \\\n#     np.split(adultTrainShuffled, [int(.7*len(adultTrain))])\n# adultTrain30ImportanceCopy = adultTrain30Importance.copy()\n# # putting back target I removed earlier\n# adultTrain70Importance[\"target\"] = targetTrain[:len(adultTrain70Importance)]\n\n# for idx, row in adultTrain70Importance.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain70Importance = adultTrain70Importance.append([df]*numReplicatedRows, ignore_index=True)\n    \n# for idx, row in adultTrain30ImportanceCopy.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain30ImportanceCopy = adultTrain30ImportanceCopy.append([df]*numReplicatedRows, ignore_index=True)\n\n# #### IMPORTANT: this is 100% of the actual training dataset -> only used if survived CV\n# adultTrainImportance = pd.concat([adultTrain70Importance, adultTrain30ImportanceCopy])\n# promisingDatasets[\"importanceSampling\"] = adultTrainImportance","a3dd3504":"# Didn't have time to do, will finish later","739a6bac":"# Didn't have time to do, will finish later","67027b11":"# Label encoder\nle = preprocessing.LabelEncoder()\n# Test data\nXtest = adultTest.values","ac3ad538":"#### Baseline dataset\nXtrain = adultTrain.values\nYtrain = le.fit_transform(targetTrain)\n\n# shape check\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(Ytrain.shape)","997e22a9":"baselineKnn = KNeighborsClassifier()\nbaselineKnnAcc = cross_val_score(baselineKnn, Xtrain, Ytrain, cv=10, scoring='accuracy')\nbaselineKnnAccMean = baselineKnnAcc.mean()\nprint('mean accuracy for baselineKnnMean: ', baselineKnnAccMean)\n\ncurrentBestModel = {\n    'model': copy.deepcopy(baselineKnn), \n    'cv': baselineKnnAccMean, \n    'X': Xtrain,\n    'Y': Ytrain\n}","2d7752af":"neighbours3Knn = KNeighborsClassifier(n_neighbors=3)\nneighbours3KnnAcc = cross_val_score(neighbours3Knn, Xtrain, Ytrain, cv=10, scoring='accuracy')\nneighbours3KnnAccMean = neighbours3KnnAcc.mean()\nprint('mean accuracy for neighbours3KnnMean: ', neighbours3KnnAccMean)\n\nif neighbours3KnnAccMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': neighbours3Knn,\n        'cv': neighbours3KnnAccMean\n    }","649b8ea7":"distanceManhattanKnn = KNeighborsClassifier(metric=\"manhattan\")\ndistanceManhattanKnnAcc = cross_val_score(distanceManhattanKnn, Xtrain, Ytrain, cv=10, scoring='accuracy')\ndistanceManhattanKnnAccMean = distanceManhattanKnnAcc.mean()\nprint('mean accuracy for distanceManhattanKnnMean: ', distanceManhattanKnnAccMean)\n\nif distanceManhattanKnnAccMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': distanceManhattanKnn,\n        'cv': distanceManhattanKnnAccMean\n    }","1aa1e2e0":"weightsDistanceKnn = KNeighborsClassifier(weights=\"distance\")\nweightsDistanceKnnAcc = cross_val_score(weightsDistanceKnn, Xtrain, Ytrain, cv=10, scoring='accuracy')\nweightsDistanceKnnAccMean = weightsDistanceKnnAcc.mean()\nprint('mean accuracy for weightsDistanceKnnMean: ', weightsDistanceKnnAccMean)\n\nif weightsDistanceKnnAccMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model':weightsDistanceKnn,\n        'cv': weightsDistanceKnnAccMean\n    }","14ca9eda":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # 1. Importance Sampling dataset\n# Ytrain70ImportanceNotEncoded = adultTrain70Importance.pop(\"target\").values\n# print('Ytrain70ImportanceNotEncoded:', Ytrain70ImportanceNotEncoded) \n# Ytrain70Importance = le.fit_transform(Ytrain70ImportanceNotEncoded)\n# print('Ytrain70Importance:', Ytrain70Importance) ## remove afterwards\n# Xtrain70Importance = adultTrain70Importance.values\n\n# # for cross validation \n# # k-fold cross validation is not done here because, the duplicated rows would leak to the cv sets\n# #Ytrain30ImportanceNotEncoded = adultTrain30Importance.pop(\"target\")\n# Ytrain30Importance = le.fit_transform(Ytrain30ImportanceNotEncoded)\n# Xtrain30Importance = adultTrain30Importance.values\n\n# # shape check\n# print(Xtrain70Importance.shape)\n# print(Xtest.shape) \n# print(Ytrain70Importance.shape)\n\n# # 5 is the deafult n_neighbors\n# importanceSamplingKnn = KNeighborsClassifier(n_neighbors=int(5*knnNeighboursFactor))\n# importanceSamplingKnn.fit(Xtrain70Importance, Ytrain70Importance)\n# Ytrain30Prediction = importanceSamplingKnn.predict(Xtrain30Importance)\n# print('Ytrain30Prediction', Ytrain30Prediction) ### remove afterwards\n# importanceSamplingKnnAccMean = accuracy_score(Ytrain30Importance, Ytrain30Prediction)\n\n# print('mean accuracy for importanceSamplingKnnAccMean: ', importanceSamplingKnnAccMean)\n# if importanceSamplingKnnAccMean > currentBestModel['cv']:  \n#     # get whole dataset fro promisingDatasets\n#     adultTrainImportance = promisingDatasets[\"importanceSampling\"] \n    \n#     YtrainImportance = le.fit_transform(adultTrainImportance.pop(\"target\").values)\n#     XtrainImportance = adultTrainImportance.values\n        \n#     currentBestModel = {\n#         'model': importanceSamplingKnn,\n#         'cv': importanceSamplingKnnAccMean,\n#         'X': XtrainImportance,\n#         'Y': YtrainImportance\n#     }","f86dd64f":"model = currentBestModel['model']\nX = currentBestModel['X']\nY = currentBestModel['Y']\nmodel.fit(X, Y)\npredictions = model.predict(Xtest) # numpy array\nprint(predictions)","1f07c2e2":"# going back to array of strings <=50 K and >50K\npredictions = le.inverse_transform(predictions)\nsubmissionDf = pd.DataFrame({'Id': idTest.values, 'income': predictions})\n\nsubmissionDf.to_csv(\"submission.csv\", index=False)","30df432a":"## Joining numerical and categorical dfs back","57bb5ac0":"## Glance at data","c6c81029":"##  Target histogram","84995c39":"## Engineered datasets","ed8b140b":"## Summary statistics","d36c4f1b":"# Experiments\n### Train on different datasets, try different hyperparameters for KNN.","dd083ce4":"## Non zero counts","7834e0d2":"# Final model\n### Trained on entire train dataset.","51951b8c":"### Test dataset","3839babb":"## Mirror on test dataset","200bfff1":"### # Neighbors parameter","abd9aff4":"## Baseline","2b3d5eab":"### Test dataset","52374a60":"## Plots of target vs features","c38251d3":"## Correlation plot","fec2b3ce":"### Categorical features","36a9c14e":"### Distance type parameter","a657b661":"# Submission\n### Save to csv in the required format.","25224719":"## Divide dataset into numerical and categorical subdatasets","656f9c9c":"## Treat missing values","08b2c45c":"## Libraries","5515e979":"# Setup and imports\n### Setup environment and import libraries.","2ceb9491":"## Treat categorical features","36b51590":"## Importance sampling","d857f545":"## Base dataset","9e04a074":"## Normalize features","79547ee9":"### Distance weights parameters","bceb1aff":"### Train dataset","1a2ed806":"#### Bar plots for categorical features","6273df08":"#### Histograms of numerical features","171e4034":"## Create new features","9af6245b":"# Table of contents\n1. [Setup and imports](#Setup-and-imports)\n    1. [Libraries](##Libraries)\n    2. [Setup](##Setup)\n2. [EDA (Exploratory Data Analysis)](#EDA(Exploratory-Data-Analysis))\n    1. [Glance at data](##Glance-at-data)\n        1. [Train dataset](###Train-dataset)\n        2. [Test dataset](###Test-dataset)\n    2. [Summary statistics](##Summary-statistics)\n    3. [Target histogram](##Target-histogram)\n    4. [Non zero counts](##Non-zero-counts)\n    5. [Empirical distribution of features](##Empirical-distribution-of-features)\n        1. [Train dataset](###Train-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        2. [Test dataset](###Test-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        3. [Plots of target vs features](###Plots-of-target-vs-features)\n            1. [Numerical features](####Numerical-features)\n            2. [Categorical features](####Categorical-features)\n        4. [Pairwise plots](###Pairwise-plots)\n            1. [Scatter plot](####Numerical-vs-numerical)\n            2. [Correlation heatmap](####Correlation-heatmap)\n            3. [Categorical heatmap](####Categorical-heatmap)\n3. [Data engineering](#Data-engineering)\n    1. [Divide dataset into numerical and categorical subdatasets](##Divide-dataset-into-numerical-and-categorical-subdatasets)\n    1. [Normalize features](##Normalize-features)\n    2. [Treat categorical features](##Treat-categorical-features)\n    3. [Joining numerical and categorical dfs back](##Joining-numerical-and-categorical-dfs-back)\n    4. [Treat missing values](##Treat-missing-values)\n    5. [Treat outliers](##Treat-outliers)\n    6. [Feature tranformations](##Feature-tranformations)\n    7. [Mirror on test dataset](##Mirror-on-testdataset)\n4. [Feature Engineering](#Featur-engineering)\n    1. [Importance sampling](##Importance-sampling)\n    2. [Select features](##Select-features)\n    3. [Create new features](##Create-new-features)\n5. [Experiments](#Experiments)\n    1. [Base dataset](##Base-dataset)\n    2. [Baseline](##Baseline)\n    3. [Hyperparameter tuning](##Hyperparameter-tuning)\n        1. [Neighbors parameter](###Neighbors-parameter)\n        2. [Distance type parameter](###Distance-type-parameter)\n        3. [Distance weights parameters](###Distance-weights-parameters)\n        4. [Neighbors parameter](###Neighbors-parameter)\n    4. [Engineered datasets](##Engineered-datasets)\n6. [Final model](#Final-model)\n7. [Submission](#Submission)","32c1a93f":"## Plot empirical distribution of each feature","bf227378":"## Setup","14b9d7be":"## Treat outliers","fc183c0e":"### Numerical vs numerical","73873b5e":"# Data engineering\n### Prepare data for algorithm.","def8d9bd":"#### Bar plots for categorical features","5989cf9c":"### Numerical features","794eebda":"- University: University of S\u00e3o Paulo (USP) \n\n- Class: PMR3508 (2021) - Fundamentals of Machine Learning\n\n- Kaggle Competition: Adult","3d92723c":"#### Histograms of numerical features","05239ded":"# Feature Engineering\n### Select and\/or create new features. Non-linear transformations affect more KNN performance","bfcf3512":"## Categorical heatmap","9a2b1ca8":"### Train dataset","cb03be4a":"## Hyperparameter tuning","596b96a6":"# EDA (Exploratory Data Analysis)\n### Get to know data and draw insights on the problem of classifying income as > 50K.","d463d623":"## Pairwise plots","c93232cb":"## Select features","c55e8518":"## Feature tranformations"}}