{"cell_type":{"4014834a":"code","accc7d60":"code","054e82ac":"code","a97726a8":"code","53999b1a":"code","ae58c651":"code","02dea368":"code","d667ec62":"code","2371df0c":"code","cffc3101":"code","798cd919":"code","a621a360":"code","43c95c27":"code","d147ff3e":"code","52140cda":"code","45ab6d09":"code","a1886f47":"code","773b25ae":"code","bd693412":"code","390cc5f2":"code","2eb226f7":"code","2ec3a0d0":"code","e3d72ca0":"code","0f189065":"code","825a6de6":"code","30f1c271":"code","83f08af9":"code","db7f3d79":"code","9416495d":"code","b08d3e74":"code","f0400f1d":"code","f7e9752c":"code","c38e2258":"code","ed17a710":"code","75b54125":"code","be51b25a":"code","1118c38a":"code","3e6f436e":"code","ea9de24a":"code","c8b87338":"code","dce62e16":"code","1e96fa56":"markdown","200a6a68":"markdown","3eb4bdf1":"markdown","afe48314":"markdown","5b6d8dcd":"markdown","abedb06a":"markdown","9883049e":"markdown","18ccf51f":"markdown"},"source":{"4014834a":"pip install eli5","accc7d60":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom eli5.sklearn import PermutationImportance","054e82ac":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a97726a8":"train['week'] = pd.to_datetime(train['week'])\ntest['week'] = pd.to_datetime(test['week'])","53999b1a":"train.info()","ae58c651":"train.isnull().sum()","02dea368":"train.describe()","d667ec62":"train","2371df0c":"train.drop(train[train['units_sold']>1500].index,inplace=True)","cffc3101":"#Distributions of numerical variables\nnum_cols = ['total_price','base_price','units_sold']\nfig,ax = plt.subplots(3,1,figsize=(8,8),squeeze=False)\nr=0\nc=0\nfor i in num_cols:\n  sns.distplot(train[i],ax=ax[r][c])\n  r+=1\n","798cd919":"train['units_sold']=np.log1p(train['units_sold'])\n","a621a360":"train['total_price']=np.log(train['total_price'])\ntest['total_price']=np.log(test['total_price'])","43c95c27":"train['base_price']=np.log(train['base_price'])\ntest['base_price']=np.log(test['base_price'])","d147ff3e":"#Distributions of numerical variables after transformation\n\nfig,ax = plt.subplots(3,1,figsize=(8,8),squeeze=False)\nr=0\nc=0\nfor i in num_cols:\n  sns.distplot(train[i],ax=ax[r][c])\n  r+=1\n","52140cda":"train['IsDiscounted'] = np.where(train['total_price']<train['base_price'],1,0)\ntest['IsDiscounted'] = np.where(test['total_price']<test['base_price'],1,0)","45ab6d09":"train['Discount'] = train['base_price'] - train['total_price']\ntest['Discount'] = test['base_price'] - test['total_price']\n\ntrain['Discount']=np.where(train['Discount'] > 0 ,np.log1p(train['Discount']),0)\ntest['Discount']=np.where(test['Discount'] > 0 ,np.log1p(test['Discount']),0)\n","a1886f47":"train['Premium'] =  train['total_price'] - train['base_price']\ntest['Premium'] =  test['total_price'] - test['base_price']\ntrain['Premium']=np.where(train['Premium'] > 0 ,np.log1p(train['Premium']),0)\ntest['Premium']=np.where(test['Premium'] > 0 ,np.log1p(test['Premium']),0)","773b25ae":"#Extracting only Month component from datetime as other components did not have much afffect on target variable\nfrom pandas import datetime\ntrain['Month'] = [date.month for date in train.week]\ntest['Month'] = [date.month for date in test.week]\n","bd693412":"#function for converting datetime object to seconds format so that it can be processed by the model\nfrom datetime import datetime\nimport time\ndef datetounix(df):\n    # Initialising unixtime list\n    unixtime = []\n    \n    # Running a loop for converting Date to seconds\n    for date in df['week']:\n        unixtime.append(time.mktime(date.timetuple()))\n    \n    # Replacing Date with unixtime list\n    df['week'] = unixtime\n    return(df)","390cc5f2":"train = datetounix(train)\ntest = datetounix(test)","2eb226f7":"X = train.drop(['units_sold'],axis=1)\ny = train['units_sold']","2ec3a0d0":"test_final = test","e3d72ca0":"train.head().append(train.tail())","0f189065":"test.head().append(test.tail())","825a6de6":"#function for creating k-fold like validation sets for TS data. Since there is autocorrelation among datapoints we cannot use classical K-Fold approach\ndef train_test_split(k):\n   train_size = int(len(train) * k)\n   x_tr, y_tr = X[0:train_size], y[0:train_size]\n   x_v,y_v = X[train_size:len(X)], y[train_size:len(X)]\n   return x_tr,y_tr,x_v,y_v\n\n\n","30f1c271":"#ratios in which train data will be split\nratio = [0.88,0.9,0.92,0.94,0.96]","83f08af9":"from lightgbm import LGBMRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error","db7f3d79":"#Applying K-Fold validation(K=5)\nrmsle = 0\nfor i in ratio:\n  x_train,y_train,x_val,y_val = train_test_split(i)\n  lgbc=LGBMRegressor(boosting_type='gbdt',n_estimators=800, learning_rate=0.12,objective= 'regression',n_jobs=-1,random_state=100)\n  model=lgbc.fit(x_train,y_train)\n  pred_val = model.predict(x_val)\n   \n  print(np.sqrt(mean_squared_log_error(np.exp(y_val), np.exp(pred_val))))\n  rmsle+=np.sqrt(mean_squared_log_error(np.exp(y_val),np.exp(pred_val)))\n    \n\n\n\nprint(rmsle\/5)","9416495d":"lgbc.fit(X,y)","b08d3e74":"import eli5\nperm = PermutationImportance(lgbc,random_state=100).fit(x_val, y_val)\neli5.show_weights(perm,feature_names=x_val.columns.tolist())","f0400f1d":"pred = lgbc.predict(test_final)\npred = np.exp(pred)\n# pred = np.abs(pred)","f7e9752c":"pred","c38e2258":"pip install catboost","ed17a710":"from catboost import CatBoostRegressor   \ncb = CatBoostRegressor(\n    n_estimators = 1000,\n    learning_rate = 0.11,\n    #iterations=1000,\n    loss_function = 'RMSE',\n    eval_metric = 'RMSE',\n    verbose=0)\n","75b54125":"rmsle = 0\nfor i in ratio:\n  x_train,y_train,x_val,y_val = train_test_split(i)\n\n  model=cb.fit(x_train,y_train)\n  pred_val = model.predict(x_val)\n   \n  print(np.sqrt(mean_squared_log_error(np.exp(y_val), np.exp(pred_val))))\n  rmsle+=np.sqrt(mean_squared_log_error(np.exp(y_val),np.exp(pred_val)))\n\n\n\nprint(\"Test set RMSE:\",100*rmsle\/5)","be51b25a":"cb.fit(X,y)","1118c38a":"import eli5\nperm = PermutationImportance(cb,random_state=100).fit(x_val, y_val)\neli5.show_weights(perm,feature_names=x_val.columns.tolist())","3e6f436e":"pred_cb = cb.predict(test_final)\npred_cb = np.exp(pred_cb)","ea9de24a":"pred_cb","c8b87338":"#Averaging out predictions from both the models.\npred_final =0.75*pred + 0.25*pred_cb\npred_final","dce62e16":"df_solution = pd.DataFrame()\ndf_solution['record_ID'] = test.record_ID\ndf_solution['units_sold'] = pred_final\ndf_solution","1e96fa56":"# Problem Statement\n\n**One of the largest retail chains in the world wants to use their vast data source to build an efficient forecasting model to predict the sales for each SKU in its portfolio at its 76 different stores using historical sales data for the past 3 years on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise.** \n\n**However, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product\/SKU-store combination for the next 12 weeks accurately? If yes, then dive right in!**","200a6a68":"\n**Its always a good strategy to reduce the variance and skewness from the variable distributions(including target variable)**\n\n","3eb4bdf1":"# Model Building ","afe48314":"# Data Analysis and Feature Engineering","5b6d8dcd":"## CatBoost","abedb06a":"## Light-GBM","9883049e":"**Making some extra features**","18ccf51f":"**Hackathon 7:Demand Forecasting**\n\n**Platform: Analytics Vidya**\n\n**Link:[https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-demand-forecasting\/](http:\/\/)**\n\n**Final Rank:13**\n\n"}}