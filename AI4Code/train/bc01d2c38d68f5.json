{"cell_type":{"f063d056":"code","feeec7cb":"code","b61a2e54":"code","0a261187":"code","0b9d6cc8":"code","35a97a5c":"code","887676d6":"code","29189e2f":"code","40517074":"code","8b662667":"code","434924f7":"code","3592a20c":"code","05bc3346":"code","dd116ab8":"code","869da9c9":"code","b77eb5d6":"code","b3cd4fdc":"code","f5bfd15d":"code","382edd48":"code","376301b0":"code","d170119f":"code","f8b0c9e7":"code","3c02fd8f":"code","b8d3c3fc":"code","a792a61e":"code","3fb4807d":"code","f12c5a01":"markdown","c041797c":"markdown","22801eb5":"markdown","8463b6bc":"markdown","25fdee6a":"markdown","1c4931d2":"markdown","e63ec1bd":"markdown","8ea7c4c8":"markdown","68b97666":"markdown","8681cacb":"markdown","d88ed18f":"markdown","8e9213b1":"markdown","c08286bc":"markdown","4b4fb50f":"markdown","8aaae5ba":"markdown","33c3bfa6":"markdown","8e559a18":"markdown","f48191b7":"markdown","ad2ac777":"markdown","652a952a":"markdown","8f1ccf9b":"markdown","9ca43b23":"markdown","a04fee88":"markdown","a8174be7":"markdown","2967f023":"markdown"},"source":{"f063d056":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport time\nimport datetime as dt\nfrom datetime import date\nimport warnings\nwarnings.filterwarnings('ignore')","feeec7cb":"covdata_filepath = '\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv'\ncovdata = pd.read_csv(covdata_filepath)\n#covdata.head()\ncovdata['Last Update'] = covdata['Last Update'].apply(pd.to_datetime)\ncovdata.drop(['SNo'],axis=1,inplace=True)\ncovdata.tail()","b61a2e54":"countries = covdata['Country\/Region'].unique().tolist()\nprint(countries)\nprint(\"\\nTotal countries affected by virus: \",len(countries))","0a261187":"covdata_china = covdata.loc[covdata['Country\/Region'] == 'Mainland China']\ncovdata_china.tail()","0b9d6cc8":"Mainland_China_provinces = covdata_china['Province\/State'].unique().tolist()\nprint(Mainland_China_provinces)\ncases = 0\nfor province in Mainland_China_provinces:\n    province = covdata_china.loc[covdata_china['Province\/State'] == province]\n    cases += province['Confirmed'].iloc[[-1][0]]\nprint('Mainland China total cases number is : ', int(cases))\n\nhubei = covdata_china.loc[covdata_china['Province\/State'] == 'Hubei']\n#hubei.tail()\nhubei_cases = hubei['Confirmed'].iloc[[-1][0]]\nprint('Hubei confirmed cases: ', int(hubei_cases))\n\nprint('Mainland China cases w\/ Hubei: ', int(cases - hubei_cases))","35a97a5c":"df_hubei = pd.DataFrame(hubei)\ndates = df_hubei.ObservationDate\nx = [dt.datetime.strptime(d,'%m\/%d\/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df_hubei.Confirmed, '.y')\nax.plot(x, df_hubei.Deaths, '.r')\nax.plot(x, df_hubei.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","887676d6":"covdata_US = covdata.loc[covdata['Country\/Region'] == 'US']\nUS_states = covdata_US['Province\/State'].unique().tolist()\n#print(US_states)\ncases = 0\ncases_max = cases\nfor state in US_states:\n    state = covdata_US.loc[covdata_US['Province\/State'] == state]\n    cases_state = state['Confirmed'].iloc[[-1][0]]\n    cases += cases_state\n    if cases_state > cases_max:\n        cases_max = cases_state\n        state_with_max_number = state['Province\/State'].iloc[-1]\n        #print('in {} there are {} cases.'.format(state['Province\/State'].iloc[-1], cases_max))\nprint('US total cases number is: ', int(cases))\nprint('The US state\/province with the max number of cases is: {}, having {} cases'.format(state_with_max_number, int(cases_max)))\nNY = covdata_US.loc[covdata_US['Province\/State'] == 'New York']","29189e2f":"df_NY = pd.DataFrame(NY)\ndates = df_NY.ObservationDate\nx = [dt.datetime.strptime(d,'%m\/%d\/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df_NY.Confirmed, '.y')\nax.plot(x, df_NY.Deaths, '.r')\nax.plot(x, df_NY.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","40517074":"covdata_new = covdata[covdata['Last Update'] > pd.Timestamp(date(2020,2,15))]\ncovdata_new.head()","8b662667":"df = pd.DataFrame(covdata_new)\ndf = df.loc[df['Country\/Region'] == 'Italy']\ndf.tail()","434924f7":"dates = df.ObservationDate\nx = [dt.datetime.strptime(d,'%m\/%d\/%Y').date() for d in dates]\n\nf, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.plot(x, df.Confirmed, '.y')\nax.plot(x, df.Deaths, '.r')\nax.plot(x, df.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","3592a20c":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_yscale('log')\nax.plot(x, df.Confirmed, '.y')\nax.plot(x, df.Deaths, '.r')\nax.plot(x, df.Recovered, '.g')\nplt.gcf().autofmt_xdate()\nax.xaxis.set_major_locator(ticker.AutoLocator())\nplt.show()","05bc3346":"new_Confirmed = [0]\nnew_Deaths = [0]\nnew_Recovered = [0]\nfor i in range(1, (df.Confirmed).size):\n    new_Confirmed.append(df.Confirmed.tolist()[i]-df.Confirmed.tolist()[i-1])\n    new_Deaths.append(df.Deaths.tolist()[i]-df.Deaths.tolist()[i-1])\n    new_Recovered.append(df.Recovered.tolist()[i]-df.Recovered.tolist()[i-1])","dd116ab8":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y', label = 'italian confirmed')\nax.plot(df.Deaths, new_Deaths,  '.r', label = 'italian dead')\nax.plot(df.Recovered, new_Recovered,  '.g', label = 'italian recovered')\n\nax.legend(loc = 'best')\nplt.show()","869da9c9":"y = np.log10(new_Confirmed) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df.Confirmed.tolist()[i]))\n    i +=1    \nx_confirmed = np.asarray(cleanx); y = np.asarray(cleany)\na_confirmed, b_confirmed = np.polyfit(x_confirmed, y, 1)\n\ny = np.log10(new_Deaths) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df.Deaths.tolist()[i]))\n    i +=1\nx_deaths = np.asarray(cleanx); y = np.asarray(cleany)\na_deaths, b_deaths = np.polyfit(x_deaths, y, 1)\n\ny = np.log10(new_Recovered) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf:\n        if np.isnan(j) != True:\n            if df.Recovered.tolist()[i] != 0:\n                cleany.append(j)        \n                cleanx.append(np.log10(df.Recovered.tolist()[i]))\n    i +=1\nx_recovered = np.asarray(cleanx); y = np.asarray(cleany)\na_recovered, b_recovered = np.polyfit(x_recovered, y, 1)","b77eb5d6":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y')\nax.plot(10**(x_confirmed), 10**(a_confirmed*x_confirmed + b_confirmed), 'y', label = 'italian confirmed')\nax.plot(df.Deaths, new_Deaths,  '.r')\nax.plot(10**(x_deaths), 10**(a_deaths*x_deaths + b_deaths), 'r', label = 'italian dead')\nax.plot(df.Recovered, new_Recovered,  '.g')\nax.plot(10**(x_recovered), 10**(a_recovered*x_recovered + b_recovered), 'g', label = 'italian recovered')\n\nax.legend(loc = 'best')\nplt.show()","b3cd4fdc":"new_cases_hubei = [0]; new_cases_NY = [0]\nfor i in range(1, (df_hubei.Confirmed).size):\n    new_cases_hubei.append(df_hubei.Confirmed.tolist()[i]-df_hubei.Confirmed.tolist()[i-1])\nfor i in range(1, (df_NY.Confirmed).size):\n    new_cases_NY.append(df_NY.Confirmed.tolist()[i]-df_NY.Confirmed.tolist()[i-1])","f5bfd15d":"y = np.log10(new_cases_NY) ; cleany = []; cleanx = []\ni = 0\nfor j in y:\n    if j != -np.inf: \n        cleany.append(j); \n        cleanx.append(np.log10(df_NY.Confirmed.tolist()[i]))\n    i +=1    \nx_confirmed_NY = np.asarray(cleanx); y = np.asarray(cleany)\na_confirmed_NY, b_confirmed_NY = np.polyfit(x_confirmed_NY, y, 1)","382edd48":"f, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nax.plot(df.Confirmed, new_Confirmed, '.y')\nax.plot(10**(x_confirmed), 10**(a_confirmed*x_confirmed + b_confirmed), 'y', label = 'italian confirmed')\n\nax.plot(df_hubei.Confirmed, new_cases_hubei, '.k', label = 'Hubei_cases')\nax.plot(df_NY.Confirmed, new_cases_NY, '.b')\nax.plot(10**(x_confirmed_NY), 10**(a_confirmed_NY*x_confirmed_NY + b_confirmed_NY), 'b', label = 'NY confirmed')\n\nax.legend(loc = 'best')\nplt.gcf().autofmt_xdate()\nplt.show()","376301b0":"from numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout","d170119f":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequence)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the sequence\n\t\tif end_ix > len(sequence)-1:\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)","f8b0c9e7":"raw_seq = np.array(new_Confirmed)\nn_steps = 5\nX, y = split_sequence(raw_seq, n_steps)\n\n# summarize the data\n#for i in range(len(X)):\n\t#print(X[i], y[i])\n    \n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))","3c02fd8f":"# define model\nmodel1 = Sequential()\nmodel1.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel1.add(Dropout(0.1))\nmodel1.add(LSTM(50, activation='relu', return_sequences=True))\nmodel1.add(Dropout(0.1))\nmodel1.add(LSTM(50, activation='relu'))\nmodel1.add(Dense(1))\n\nstart = time.time()\nmodel1.compile(optimizer='adam', loss='mse')          \nprint('compilation time : ', time.time() - start)\nprint('\\n')\nmodel1.summary()","b8d3c3fc":"model1.fit(X, y, epochs=300, verbose=0)\nx_input = np.array(new_Confirmed)[-n_steps:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model1.predict(x_input, verbose=0)\ncases_forecast = int(round(yhat[0][0]))\nprint('cases forecast for today: {}'.format(cases_forecast))\n\naccuracy = 100*(1-np.abs(cases_forecast - new_Confirmed[-1])\/new_Confirmed[-1])\nprint('forecast accuracy: {:.2f} %'.format(accuracy))\n\nprint('and for tomorrow?')\nx_input2 = np.array(new_Confirmed)[(-n_steps+1):]\nx_input2 = np.concatenate((x_input2, yhat), axis=None)\nx_input2 = x_input2.reshape((1, n_steps, n_features))\nyhat2 = model1.predict(x_input2, verbose=0)\nprint('tomorrow there might be {:.0f} cases'.format(int(yhat2[0][0])))","a792a61e":"raw_seq = np.array(new_Deaths)\nn_steps = 4\nX, y = split_sequence(raw_seq, n_steps)\n\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n\n# define model\nmodel2 = Sequential()\nmodel2.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel2.add(LSTM(50, activation='relu', return_sequences=True))\nmodel2.add(LSTM(50, activation='relu'))\nmodel2.add(Dense(1))\n\nstart = time.time()\nmodel2.compile(optimizer='adam', loss='mse')          \nprint('compilation time : ', time.time() - start)\nprint('\\n')\nmodel2.summary()","3fb4807d":"model2.fit(X, y, epochs=300, verbose=0)\n\nx_input = np.array(new_Deaths)[-n_steps:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model2.predict(x_input, verbose=0)\n\ndeaths_forecast = int(round(yhat[0][0]))\nprint('deaths forecast for today: {}'.format(deaths_forecast))\n\naccuracy = 100*(1-np.abs(deaths_forecast - new_Deaths[-1])\/new_Deaths[-1])\nprint('forecast accuracy: {:.2f} %'.format(accuracy))\n\nprint('and for tomorrow?')\nx_input2 = np.array(new_Deaths)[(-n_steps+1):]\nx_input2 = np.concatenate((x_input2, yhat), axis=None)\nx_input2 = x_input2.reshape((1, n_steps, n_features))\nyhat2 = model2.predict(x_input2, verbose=0)\nprint('tomorrow, in total, there might be {:.0f} deaths'.format(int(yhat2[0][0])))","f12c5a01":"This is a little notebook for anyone who wants to practice his\/her Python skills trying to tackle a real world problem.\nA very simple machine learning model is at the bottom, for the most patient of you. So let's begin!","c041797c":"Step 1: The various necessary imports:","22801eb5":"...defining the model...","8463b6bc":"Plotting with a logarithmic x axis, to get a more mathematical view. ","25fdee6a":"Plotting everything together: it is quite apparent that the NY curve has a greater growth rate than the italian one. NY is unfortunately going to outpace the italian cases in the very near future, so Cuomo wake up! Hubei has the descending trend that everybody in the world looks forward to (trusting official data). We hope that it will soon come for everyone else, the sooner the better.","1c4931d2":"Then we can plot everything. Now we have more information to look at.","e63ec1bd":"Step 2: Importing the data! Then: dropping unnecessary column and having a quick look.","8ea7c4c8":"To gain a little more insight, we do the same thing for Hubei and NY. ","68b97666":"Step 4 (final step): the machine learnig part. Just a simple LSTM (stacked means with more than 2 LSTM layers). Importing the models from keras...","8681cacb":"Now jump the Pacific and look at the USA data. Very sad! Same procedure as before. NY the most affected.. ","d88ed18f":"Closer look at NY state. We plot confirmed cases, deaths & recovered to get the full picture. Unfortunately we still have exponential increases :(","8e9213b1":"Now I must divert your attention to Italy, my beloved home Country. We simplify taking the most meaningful data, starting from 15\/02\/2020.","c08286bc":"Closer look at Hubei province. We create a pandas dataframe, to handle the data in a nicer way. Then we plot confirmed cases, deaths & recovered to get the full picture. Nice (but also very sad) logistical curves!","4b4fb50f":"Doing the same for the deaths prediction. If you want, you can change n_steps, add other LSTM layers or Dense layers as you please. The more you play around, the more you'll learn. \nFor this forecast, I used the simplest LSTM possible. As you can see, the results are still very grim. The accuracy is generally worse than with the previous model.","8aaae5ba":"...fitting the model! Not so joyfully :(\nAccuracy is demonstrated by testing the model on the last established data. It can vary a little bit if you run the code multiple times.","33c3bfa6":"Step 3: Beginning to look closely at the data. We start from China, for logical reasons. Quick look...","8e559a18":"... defining input sequence, choosing a number of time steps, splitting into samples, reshaping...","f48191b7":"..and plotting, same as before! Still a very steep increase, but nowhere near an exponential (Gott sei Dank!). I think we are near the change of curvature of the function, or just past it.","ad2ac777":"Another quick look: list of countries affected by the virus","652a952a":"... then dive in! Summing up all the cases of the various provinces, and comparing with the Hubei province, the biggest player for China.","8f1ccf9b":"...and plotting everything. Sadly the deaths curve still grows more rapidly than the recovered one.","9ca43b23":"But this still look messy. So we fit the data points, in order to have more insight on the growths. We do this in a very ugly way: don't do this at home. I'm going to fix this mess one day. The important part is to discard the infinities and nans. Saving our fit parameters a & b.. ","a04fee88":"But, as the virus cannot care less of the dates, we need something more meaningful on the graph. For an exponential phenomenon, the natural scale is log - log. Most interesting are the new cases, so we want to plot them on the y-axis. But first we need to calculate them. We do this very simply:","a8174be7":"Creating a pandas dataframe for Italy...","2967f023":"...definig a function, used to split the data in sequences (with input and output part of the pattern)..."}}