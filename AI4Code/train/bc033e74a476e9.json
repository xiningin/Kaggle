{"cell_type":{"924cf4ec":"code","6a627d2b":"code","8022db6b":"code","991a2a32":"code","be05b6c0":"code","4731e49b":"code","700bfcc3":"code","f63e3f4d":"code","7895b84c":"code","e03fb7d2":"code","e379612f":"code","2cb74451":"code","f29f66f6":"code","11df87f2":"code","2381386b":"code","1a5f0bd1":"code","d5f5f3cd":"code","32929f6c":"code","766bf72e":"code","19f09a5d":"code","3ad86673":"code","2a419f39":"code","a6ed8fdf":"code","2a798b5c":"code","d732d401":"code","e6f431aa":"code","30dd1881":"code","dd7bec39":"code","90dd0787":"code","d418e525":"code","0e1eae26":"code","173a809a":"code","51712c66":"code","318e2009":"code","fc385142":"code","1e821eb6":"code","d5314adb":"code","3bf0f045":"code","ad387f1e":"code","bd389c6c":"code","5fc52f74":"code","96005ed8":"code","499cb200":"code","308fe03d":"code","1b8baa00":"markdown","6c726e2f":"markdown","77d256a1":"markdown","e1a0d5da":"markdown","b5d1b903":"markdown","f3e075a0":"markdown","7ebf6ec5":"markdown","9430f3b8":"markdown","e3ca7ceb":"markdown","0814a7b9":"markdown","d4a54892":"markdown","2386d7b8":"markdown","41b470a6":"markdown","db1fefda":"markdown","1631613a":"markdown","ae8879ee":"markdown","4c56db12":"markdown","e44117b4":"markdown","1e140704":"markdown","d992890e":"markdown","164c9dff":"markdown","3a0fb917":"markdown","142ed035":"markdown","e3ba6c79":"markdown","8170d314":"markdown","6906dd8c":"markdown","dfe97aed":"markdown","850ba23e":"markdown","e38ab2ce":"markdown","c7e80c47":"markdown","02ae5087":"markdown","b82ff573":"markdown","6dcf0903":"markdown","23ee8dec":"markdown","edaf994d":"markdown","a857ebef":"markdown","1857a94c":"markdown","85171e52":"markdown","27fdfe5b":"markdown","29f71663":"markdown","d12ef061":"markdown","317c6414":"markdown","379c180b":"markdown","06127d0a":"markdown"},"source":{"924cf4ec":"!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","6a627d2b":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport re\nimport h5py\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nimport lightgbm as lgb\n\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\n\nSEED = 1337","8022db6b":"df_fnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv')\ndf_loading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv')\ndf_train_scores = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv')\n\nfnc_features = list(df_fnc.columns[1:])\nloading_features = list(df_loading.columns[1:])\ntarget_features = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\ndf_train_scores['is_train'] = 1\n\ndf = df_fnc.merge(df_loading, on='Id')\ndf = df.merge(df_train_scores, how='left', on='Id')\n\ndf.loc[df['is_train'].isnull(), 'is_train'] = 0\ndf['is_train'] = df['is_train'].astype(np.uint8)\ndf['Id'] = df['Id'].astype(np.uint16)\n\nprint(f'Static FNC Correlation Shape = {df_fnc.shape}')\nprint(f'Static FNC Correlation Memory Usage = {df_fnc.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'sMRI SBM Loadings Shape = {df_loading.shape}')\nprint(f'sMRI SBM Loadings Memory Usage = {df_loading.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Train Scores Shape = {df_train_scores.shape}')\nprint(f'Train Scores Memory Usage = {df_train_scores.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint('-------------------------------------')\nprint(f'Train & Test Set Shape = {df.shape}')\nprint(f'Train & Test Set Memory Usage = {df.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n\ndel df_fnc, df_loading, df_train_scores","991a2a32":"def plot_target(target_feature): \n    \n    if target_feature == 'age':\n        print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * 39}')\n    else:\n        print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * 48}')\n        \n    print(f'Mean: {df[target_feature].mean():.4}  -  Median: {df[target_feature].median():.4}  -  Std: {df[target_feature].std():.4}')\n    print(f'Min: {df[target_feature].min():.4}  -  25%: {df[target_feature].quantile(0.25):.4}  -  50%: {df[target_feature].quantile(0.5):.4}  -  75%: {df[target_feature].quantile(0.75):.4}  -  Max: {df[target_feature].max():.4}')\n    print(f'Skew: {df[target_feature].skew():.4}  -  Kurtosis: {df[target_feature].kurtosis():.4}')\n    missing_values_count = df[(df['is_train'] == 1) & (df[target_feature]).isnull()].shape[0]\n    training_samples_count = df[df['is_train'] == 1].shape[0]\n    print(f'Missing Values: {missing_values_count}\/{training_samples_count} ({missing_values_count * 100 \/ training_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(18, 6), dpi=100)\n\n    sns.distplot(df[target_feature], label=target_feature, ax=axes[0])\n    stats.probplot(df[target_feature], plot=axes[1])\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12)\n        axes[i].tick_params(axis='y', labelsize=12)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n    axes[0].set_title(f'{target_feature} Distribution in Training Set')\n    axes[1].set_title(f'{target_feature} Probability Plot')\n    plt.show()\n    \nfor target_feature in target_features:\n    plot_target(target_feature)","be05b6c0":"fig = plt.subplots(figsize=(12, 6), dpi=100)\n\nsns.countplot(y=df['age'], label=target_feature)\n\nplt.xlabel('Value Counts', size=12)\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title(f'age Value Counts in Training Set', size=12, pad=12)\n\nplt.show()","4731e49b":"df_domain1_anomaly = df[(df['domain1_var1'] == 47.7159246378591) |\n                        (df['domain1_var1'] == 40.0354975451987)][target_features].sort_values(by='domain1_var1')\ndf_domain1_anomaly.style.background_gradient(subset=['age', 'domain1_var1', 'domain1_var2'], cmap='viridis')","700bfcc3":"df['domain1'] = df['domain1_var1'] + df['domain1_var2']\n\nfor target_feature in ['domain1_var1', 'domain1_var2', 'domain1']:\n    print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * (36 + len(target_feature))}')\n    print(f'Mean: {df[target_feature].mean():.4}  -  Median: {df[target_feature].median():.4}  -  Std: {df[target_feature].std():.4}')\n    print(f'Min: {df[target_feature].min():.4}  -  25%: {df[target_feature].quantile(0.25):.4}  -  50%: {df[target_feature].quantile(0.5):.4}  -  75%: {df[target_feature].quantile(0.75):.4}  -  Max: {df[target_feature].max():.4}')\n    print(f'Skew: {df[target_feature].skew():.4}  -  Kurtosis: {df[target_feature].kurtosis():.4}\\n')\n\nfig, axes = plt.subplots(ncols=3, figsize=(18, 6), dpi=100)\nplt.tight_layout()\n\nsns.distplot(df['domain1_var1'], ax=axes[0])\nsns.distplot(df['domain1_var2'], ax=axes[1])\nsns.distplot(df['domain1'], ax=axes[2])\n\nfor i in range(3):\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=10)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    \naxes[0].set_title(f'domain1_var1 Distribution', size=15, pad=15)\naxes[1].set_title(f'domain1_var2 Distribution', size=15, pad=15)\naxes[2].set_title(f'domain1_var1 + domain1_var2 Distribution', size=15, pad=15)\n\nplt.show()\n\ndf.drop(columns=['domain1'], inplace=True)","f63e3f4d":"df_domain2_anomaly = df[(df['domain2_var1'] == 39.350140365296397) |\n                        (df['domain2_var1'] == 46.244030185775202) | \n                        (df['domain2_var1'] == 43.239463678844999)][target_features].sort_values(by='domain2_var1')\n\ndf_domain2_anomaly.style.background_gradient(subset=['age', 'domain2_var1', 'domain2_var2'], cmap='viridis')","7895b84c":"df['domain2'] = df['domain2_var1'] + df['domain2_var2']\n\nfor target_feature in ['domain2_var1', 'domain2_var2', 'domain2']:\n    print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * (36 + len(target_feature))}')\n    print(f'Mean: {df[target_feature].mean():.4}  -  Median: {df[target_feature].median():.4}  -  Std: {df[target_feature].std():.4}')\n    print(f'Min: {df[target_feature].min():.4}  -  25%: {df[target_feature].quantile(0.25):.4}  -  50%: {df[target_feature].quantile(0.5):.4}  -  75%: {df[target_feature].quantile(0.75):.4}  -  Max: {df[target_feature].max():.4}')\n    print(f'Skew: {df[target_feature].skew():.4}  -  Kurtosis: {df[target_feature].kurtosis():.4}\\n')\n\nfig, axes = plt.subplots(ncols=3, figsize=(18, 6), dpi=100)\nplt.tight_layout()\n\nsns.distplot(df['domain2_var1'], ax=axes[0])\nsns.distplot(df['domain2_var2'], ax=axes[1])\nsns.distplot(df['domain2'], ax=axes[2])\n\nfor i in range(3):\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=10)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    \naxes[0].set_title(f'domain2_var1 Distribution', size=15, pad=15)\naxes[1].set_title(f'domain2_var2 Distribution', size=15, pad=15)\naxes[2].set_title(f'domain2_var1 + domain2_var2 Distribution', size=15, pad=15)\n\nplt.show()\n\ndf.drop(columns=['domain2'], inplace=True)","e03fb7d2":"fig = plt.figure(figsize=(10, 10), dpi=100)\n\nsns.heatmap(df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 14}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=14, rotation=45)\nplt.tick_params(axis='y', labelsize=14, rotation=0)\n    \nplt.title('Target Features Correlations', size=18, pad=18)\nplt.show()","e379612f":"def plot_loading(loading_feature):\n    \n    df_train = df.loc[df['is_train'] == 1]\n    df_test = df.loc[df['is_train'] == 0]\n        \n    print(f'Loading feature {loading_feature} Statistical Analysis\\n{\"-\" * 42}')\n\n    print(f'Training Mean: {float(df_train[loading_feature].mean()):.4}  - Training Median: {float(df_train[loading_feature].median()):.4} - Training Std: {float(df_train[loading_feature].std()):.4}')\n    print(f'Test Mean: {float(df_test[loading_feature].mean()):.4}  - Test Median: {float(df_test[loading_feature].median()):.4} - Test Std: {float(df_test[loading_feature].std()):.4}')\n    print(f'Training Min: {float(df_train[loading_feature].min()):.4}  - Training Max: {float(df_train[loading_feature].max()):.4}')\n    print(f'Test Min: {float(df_test[loading_feature].min()):.4}  - Training Max: {float(df_test[loading_feature].max()):.4}')\n    print(f'Training Skew: {float(df_train[loading_feature].skew()):.4}  - Training Kurtosis: {float(df_train[loading_feature].kurtosis()):.4}')\n    print(f'Test Skew: {float(df_test[loading_feature].skew()):.4}  - Test Kurtosis: {float(df_test[loading_feature].kurtosis()):.4}')\n    training_missing_values_count = df_train[df_train[loading_feature].isnull()].shape[0]\n    test_missing_values_count = df_test[df_test[loading_feature].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    test_samples_count = df_test.shape[0]\n    print(f'Training Missing Values: {training_missing_values_count}\/{training_samples_count} ({training_missing_values_count * 100 \/ training_samples_count:.4}%)')\n    print(f'Test Missing Values: {test_missing_values_count}\/{test_samples_count} ({test_missing_values_count * 100 \/ test_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(25, 12), dpi=100, constrained_layout=True)\n    title_size = 18\n    label_size = 18\n\n    # Loading Feature Training and Test Set Distribution\n    sns.distplot(df[df['is_train'] == 1][loading_feature], label='Training', ax=axes[0][0])\n    sns.distplot(df[df['is_train'] == 0][loading_feature], label='Test', ax=axes[0][0])\n    axes[0][0].set_xlabel('')\n    axes[0][0].tick_params(axis='x', labelsize=label_size)\n    axes[0][0].tick_params(axis='y', labelsize=label_size)\n    axes[0][0].legend()\n    axes[0][0].set_title(f'{loading_feature} Distribution in Training and Test Set', size=title_size, pad=title_size)\n    \n    # Loading Feature vs age\n    sns.scatterplot(df[loading_feature], df['age'], ax=axes[0][1])\n    axes[0][1].set_title(f'{loading_feature} vs age', size=title_size, pad=title_size)\n    axes[0][1].set_xlabel('')\n    axes[0][1].set_ylabel('')\n    axes[0][1].tick_params(axis='x', labelsize=label_size)\n    axes[0][1].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain1_var1\n    sns.scatterplot(df[loading_feature], df['domain1_var1'], ax=axes[0][2])\n    axes[0][2].set_title(f'{loading_feature} vs domain1_var1', size=title_size, pad=title_size)\n    axes[0][2].set_xlabel('')\n    axes[0][2].set_ylabel('')\n    axes[0][2].tick_params(axis='x', labelsize=label_size)\n    axes[0][2].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain1_var2\n    sns.scatterplot(df[loading_feature], df['domain1_var2'], ax=axes[1][0])\n    axes[1][0].set_title(f'{loading_feature} vs domain1_var2', size=title_size, pad=title_size)\n    axes[1][0].set_xlabel('')\n    axes[1][0].set_ylabel('')\n    axes[1][0].tick_params(axis='x', labelsize=label_size)\n    axes[1][0].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain2_var1\n    sns.scatterplot(df[loading_feature], df['domain2_var1'], ax=axes[1][1])\n    axes[1][1].set_title(f'{loading_feature} vs domain2_var1', size=title_size, pad=title_size)\n    axes[1][1].set_xlabel('')\n    axes[1][1].set_ylabel('')\n    axes[1][1].tick_params(axis='x', labelsize=label_size)\n    axes[1][1].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain2_var2\n    sns.scatterplot(df[loading_feature], df['domain2_var2'], ax=axes[1][2])\n    axes[1][2].set_title(f'{loading_feature} vs domain2_var2', size=title_size, pad=title_size)\n    axes[1][2].set_xlabel('')\n    axes[1][2].set_ylabel('')\n    axes[1][2].tick_params(axis='x', labelsize=label_size)\n    axes[1][2].tick_params(axis='y', labelsize=label_size)\n    \n    plt.show()\n    \nfor loading_feature in sorted(loading_features):\n    plot_loading(loading_feature)\n","2cb74451":"loading_target_features = sorted(loading_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(30, 30), dpi=100)\n\nsns.heatmap(df[loading_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=45)\nplt.tick_params(axis='y', labelsize=18, rotation=45)\n\nplt.title('Target and Loading Features Correlations', size=25, pad=25)\nplt.show()","f29f66f6":"scn_pattern = r'SCN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nscn_features = [col for col in fnc_features if re.match(scn_pattern, col)]\nscn_target_features = sorted(scn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(25, 25), dpi=100)\n\nsns.heatmap(df[scn_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=75)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\n\nplt.title('Target and SCN Features Correlations', size=25, pad=25)\nplt.show()","11df87f2":"adn_pattern = r'ADN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nadn_features = [col for col in fnc_features if re.match(adn_pattern, col)]\nadn_target_features = sorted(adn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(25, 25), dpi=100)\n\nsns.heatmap(df[adn_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15},\n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=75)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\n\nplt.title('Target and ADN Features Correlations', size=25, pad=25)\nplt.show()","2381386b":"smn_pattern = r'SMN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nsmn_features = [col for col in fnc_features if re.match(smn_pattern, col)]\nsmn_target_features = sorted(smn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(40, 40), dpi=100)\n\nsns.heatmap(df[smn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and SMN Features Correlations', size=50, pad=50)\nplt.show()","1a5f0bd1":"vsn_pattern = r'VSN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nvsn_features = [col for col in fnc_features if re.match(vsn_pattern, col)]\nvsn_target_features = sorted(vsn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(40, 40), dpi=100)\n\nsns.heatmap(df[vsn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and VSN Features Correlations', size=50, pad=50)\nplt.show()","d5f5f3cd":"con_pattern = r'CON\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ncon_features = [col for col in fnc_features if re.match(con_pattern, col)]\ncon_target_features = sorted(con_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[con_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and CON Features Correlations', size=50, pad=50)\nplt.show()","32929f6c":"dmn_pattern = r'DMN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ndmn_features = [col for col in fnc_features if re.match(dmn_pattern, col)]\ndmn_target_features = sorted(dmn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[dmn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and DMN Features Correlations', size=50, pad=50)\nplt.show()","766bf72e":"cbn_pattern = r'CBN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ncbn_features = [col for col in fnc_features if re.match(cbn_pattern, col)]\ncbn_target_features = sorted(cbn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[cbn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and CBN Features Correlations', size=50, pad=50)\nplt.show()","19f09a5d":"fig = plt.figure(figsize=(60, 60), dpi=100)\n\nsns.heatmap(df[fnc_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and All FNC Features Correlations', size=60, pad=60)\nplt.show()","3ad86673":"site2_ids = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv').values.flatten()\n\ndf['site'] = 0\ndf.loc[df['is_train'] == 1, 'site'] = 1\ndf.loc[df['Id'].isin(site2_ids), 'site'] = 2\ndf['site'] = df['site'].astype(np.uint8)\n\ndel site2_ids","2a419f39":"fig = plt.figure(figsize=(10, 6), dpi=100)\nsns.barplot(x=df['site'].value_counts().index, y=df['site'].value_counts())\n\npercentages = [(count \/ df['site'].value_counts().sum() * 100).round(2) for count in df['site'].value_counts()]\nplt.ylabel('')\nplt.xticks(np.arange(3), [f'No Site (%{percentages[1]})', f'Site 1 (%{percentages[0]})', f'Site 2 (%{percentages[2]})'])\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Site Counts', size=15, pad=15)\n\nplt.show()","a6ed8fdf":"def plot_site_distribution(feature):    \n    \n    print(f'{feature} Site Distribution Analysis\\n{\"-\" * 32}')\n        \n    print(f'Site 1 Mean: {df[df[\"site\"] == 1][feature].mean():.4}  -  Median: {df[df[\"site\"] == 1][feature].median():.4}  -  Std: {df[df[\"site\"] == 1][feature].std():.4}')\n    print(f'Site 2 Mean: {df[df[\"site\"] == 2][feature].mean():.4}  -  Median: {df[df[\"site\"] == 2][feature].median():.4}  -  Std: {df[df[\"site\"] == 2][feature].std():.4}')\n    print(f'\\nSite 1 Min: {df[df[\"site\"] == 1][feature].min():.4}  -  25%: {df[df[\"site\"] == 1][feature].quantile(0.25):.4}  -  50%: {df[df[\"site\"] == 1][feature].quantile(0.5):.4}  -  75%: {df[df[\"site\"] == 1][feature].quantile(0.75):.4}  -  Max: {df[df[\"site\"] == 1][feature].max():.4}')\n    print(f'Site 2 Min: {df[df[\"site\"] == 2][feature].min():.4}  -  25%: {df[df[\"site\"] == 2][feature].quantile(0.25):.4}  -  50%: {df[df[\"site\"] == 2][feature].quantile(0.5):.4}  -  75%: {df[df[\"site\"] == 2][feature].quantile(0.75):.4}  -  Max: {df[df[\"site\"] == 2][feature].max():.4}')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(20, 6), dpi=100)\n\n    sns.distplot(df[df['site'] == 1][feature], label='Site 1', ax=axes[0])\n    sns.distplot(df[df['site'] == 2][feature], label='Site 2', ax=axes[0])\n    sns.distplot(df[df['is_train'] == 1][feature], label='Training', ax=axes[1])\n    sns.distplot(df[df['is_train'] == 0][feature], label='Test', ax=axes[1])\n    \n    for i in range(2):\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n        axes[i].legend()\n    axes[0].set_title(f'{feature} Distribution in Site 1 and 2', size=18, pad=18)\n    axes[1].set_title(f'{feature} Distribution in Training and Test Set', size=18, pad=18)\n    \n    plt.show()\n    \nfor loading_feature in sorted(loading_features):\n    plot_site_distribution(loading_feature)","2a798b5c":"def get_distribution_difference(feature):\n    site1_values = df[df['site'] == 1][feature].values\n    site2_values = df[df['site'] == 2][feature].values\n    return feature, stats.ks_2samp(site1_values, site2_values)\n\nks_threshold = 0.125\nshifted_features = [fnc_feature for fnc_feature in fnc_features if get_distribution_difference(fnc_feature)[1][0] > ks_threshold]        ","d732d401":"for feature in sorted(shifted_features):\n    plot_site_distribution(feature)","e6f431aa":"site_predictors = shifted_features + loading_features\n\nX_train = df.loc[df['site'] > 0, site_predictors]\ny_train = df.loc[df['site'] > 0, 'site']\nX_test = df.loc[df['site'] == 0, site_predictors]","30dd1881":"df['site_predicted'] = 0\n\nK = 2\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n\noof_scores = []\nfeature_importance = pd.DataFrame(np.zeros((X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=X_train.columns)\n\nsite_model_parameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 5, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.9,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': SEED,\n    'feature_fraction_seed': SEED,\n    'bagging_seed': SEED,\n    'drop_seed': SEED,\n    'data_random_seed': SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,   \n}\n\nprint('Running LightGBM Site Classifier Model\\n' + ('-' * 38) + '\\n')\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx, :], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx, :], label=y_train.iloc[val_idx])                \n    site_model = lgb.train(site_model_parameters, trn_data, valid_sets=[trn_data, val_data], verbose_eval=50)\n    feature_importance.iloc[:, fold - 1] = site_model.feature_importance(importance_type='gain')\n\n    site_oof_predictions = site_model.predict(X_train.iloc[val_idx, :], num_iteration=site_model.best_iteration)\n    df.loc[X_train.iloc[val_idx, :].index, 'site_predicted'] = site_oof_predictions\n    \n    site_test_predictions = site_model.predict(X_test, num_iteration=site_model.best_iteration)\n    df.loc[X_test.index, 'site_predicted'] += site_test_predictions \/ K\n\n    oof_score = f1_score(y_train.iloc[val_idx], np.clip(np.round(site_oof_predictions), 1, 2))\n    oof_scores.append(oof_score)            \n    print(f'\\nFold {fold} - F1 Score {oof_score:.6}\\n')\n    \ndf['site_predicted'] = df['site_predicted'].astype(np.float32)\noof_f1_score = f1_score(df.loc[df['site'] > 0, 'site'], np.clip(np.round(df.loc[df['site'] > 0, 'site_predicted']), 1, 2))\n\nprint('-' * 38)\nprint(f'LightGBM Site Classifier Model Mean F1 Score {np.mean(oof_scores):.6} [STD:{np.std(oof_scores):.6}]')\nprint(f'LightGBM Site Classifier Model OOF F1 Score {oof_f1_score:.6}')\n\nplt.figure(figsize=(20, 20))\nfeature_importance['Mean_Importance'] = feature_importance.sum(axis=1) \/ K\nfeature_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=feature_importance.index, data=feature_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('LightGBM Site Classifier Model Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()","dd7bec39":"print('Site Classifier expected values are rounded with .5 threshold\\n')\nprint(df.loc[df['is_train'] == 1, 'site_predicted'].round(0).astype(np.uint8).value_counts())\n\nprint('\\nSite Classifier expected values are rounded with .2 threshold\\n')\ndf.loc[df['site_predicted'] > 1.2, 'site_predicted'] = 2\ndf['site_predicted'] = df['site_predicted'].round().astype(np.uint8)\nprint(df.loc[df['is_train'] == 1, 'site_predicted'].round(0).astype(np.uint8).value_counts())","90dd0787":"print(f'Train fMRI Samples in fMRI_train: {len(os.listdir(\"..\/input\/trends-assessment-prediction\/fMRI_train\"))}')\nprint(f'Test fMRI Samples in fMRI_test: {len(os.listdir(\"..\/input\/trends-assessment-prediction\/fMRI_test\"))}')","d418e525":"matlab_file = h5py.File(f'..\/input\/trends-assessment-prediction\/fMRI_train\/10001.mat', 'r')\nmatlab_file","0e1eae26":"print(f'.keys() -> {matlab_file.keys()}')\nprint(f'.values() -> {matlab_file.values()}')\n\nprint('\\nmatlab_file[\"SM_feature\"] ->', matlab_file['SM_feature'])","173a809a":"# Loading reference image\nfmri_mask = nl.image.load_img('..\/input\/trends-assessment-prediction\/fMRI_mask.nii')\n\n# Reorienting the axis of 3D spatial map\nspatial_maps = np.moveaxis(matlab_file['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0]) \n\n# Loading 3D spatial maps\nspatial_maps_niimg = nl.image.new_img_like(ref_niimg=fmri_mask,\n                                           data=spatial_maps,\n                                           affine=fmri_mask.affine,\n                                           copy_header=True)","51712c66":"fig, axes = plt.subplots(nrows=53, figsize=(30, 300))\n\nfor i, img in enumerate(list(nl.image.iter_img(spatial_maps_niimg))):\n    nlplt.plot_stat_map(stat_map_img=img,\n                        bg_img='ch2better.nii',\n                        title=f'10001.mat Spatial Map {i + 1} plot_stat_map',\n                        axes=axes[i],\n                        threshold=1,\n                        display_mode='ortho',\n                        annotate=False,\n                        draw_cross=True,\n                        colorbar=False)","318e2009":"img = list(nl.image.iter_img(spatial_maps_niimg))[0]\n\nfig, axes = plt.subplots(nrows=5, figsize=(30, 50))\n\nnlplt.plot_glass_brain(img,\n                       title='10001.mat Spatial Map 1 plot_glass_brain',\n                       threshold=3,\n                       black_bg=True,\n                       axes=axes[0])\nnlplt.plot_roi(img,\n               title='10001.mat Spatial Map 1 plot_roi',\n               threshold=3,\n               black_bg=True,\n               axes=axes[1])\n\nnlplt.plot_anat(img,\n                title='10001.mat Spatial Map 1 plot_anat',\n                axes=axes[2])\n\nnlplt.plot_epi(img,\n               title='10001.mat Spatial Map 1 plot_epi',\n               axes=axes[3])\n\nnlplt.plot_img(img,\n               title='10001.mat Spatial Map 1 plot_img',\n               axes=axes[4])\n\nplt.show()","fc385142":"view = nlplt.view_img_on_surf(img,\n                              title='10001.mat Spatial Map 1 view_img_on_surf',\n                              title_fontsize=20,\n                              threshold=1,\n                              black_bg=False)\nview.open_in_browser()\nview","1e821eb6":"df_icn = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\ndf_icn['ICN_number'] = df_icn['ICN_number'].astype(np.uint8)\n\nunique_components = [feature.split('_')[0] for feature in fnc_features] + [feature.split('_')[2] for feature in fnc_features]\ndf_icn['component'] = df_icn['ICN_number'].apply(lambda x: [c for c in unique_components if f'({x})' in c][0])\n\nicn_order = {index: value for index, value in enumerate(df_icn['component'].values)}\nicn_order","d5314adb":"age_min_id = 10721\nage_max_id = 11685\n\nspatial_data_age_min = h5py.File(f'..\/input\/trends-assessment-prediction\/fMRI_train\/{age_min_id}.mat', 'r')\nspatial_data_age_max = h5py.File(f'..\/input\/trends-assessment-prediction\/fMRI_train\/{age_max_id}.mat', 'r')\n\nspatial_data_age_min = np.moveaxis(spatial_data_age_min['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0])\nspatial_data_age_max = np.moveaxis(spatial_data_age_max['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0])\n\nspatial_data_age_min_niimg = nl.image.new_img_like(ref_niimg=fmri_mask, data=spatial_data_age_min, affine=fmri_mask.affine, copy_header=True)\nspatial_data_age_max_niimg = nl.image.new_img_like(ref_niimg=fmri_mask, data=spatial_data_age_max, affine=fmri_mask.affine, copy_header=True)\n\nspatial_data_age_min_images = list(nl.image.iter_img(spatial_data_age_min_niimg))\nspatial_data_age_max_images = list(nl.image.iter_img(spatial_data_age_max_niimg))\n\nfig, axes = plt.subplots(nrows=53, ncols=2, figsize=(30, 300))\n\nfor i, (img_age_min, img_age_max) in enumerate(zip(spatial_data_age_min_images, spatial_data_age_max_images)):\n\n    nlplt.plot_stat_map(stat_map_img=img_age_min,\n                        bg_img='ch2better.nii',\n                        title=f'{age_min_id}.mat (Youngest Age) Component:{icn_order[i]}',\n                        axes=axes[i][0],\n                        threshold=1,\n                        display_mode='ortho',\n                        annotate=False,\n                        draw_cross=True,\n                        colorbar=False)\n    \n    nlplt.plot_stat_map(stat_map_img=img_age_max,\n                        bg_img='ch2better.nii',\n                        title=f'{age_max_id}.mat (Oldest Age) Component:{icn_order[i]}',\n                        axes=axes[i][1],\n                        threshold=1,\n                        display_mode='ortho',\n                        annotate=False,\n                        draw_cross=True,\n                        colorbar=False)","3bf0f045":"component = 0\nimg = list(nl.image.iter_img(spatial_data_age_min_niimg))[component]\n\nfig, axes = plt.subplots(nrows=6, ncols=1, figsize=(30, 50))\n\nfor i, fwhm in enumerate(np.arange(0, 30, 5)):\n    \n    smoothed_img = nl.image.smooth_img(img, fwhm)\n    \n    nlplt.plot_stat_map(stat_map_img=smoothed_img,\n                        bg_img='ch2better.nii',\n                        title=f'{age_min_id}.mat (Youngest Age) Component:{icn_order[component]} - Smoothing FWHM {fwhm}',\n                        axes=axes[i],\n                        threshold=1,\n                        display_mode='ortho',\n                        annotate=False,\n                        draw_cross=True,\n                        colorbar=False)","ad387f1e":"age_min_id = 19531\nage_max_id = 17104\n\nfmri_mask = nl.image.load_img('..\/input\/trends-assessment-prediction\/fMRI_mask.nii')\nprint(f'fMRI Mask Shape: {fmri_mask.shape}')\n\nspatial_data_age_min = h5py.File(f'..\/input\/trends-assessment-prediction\/fMRI_train\/{age_min_id}.mat', 'r')['SM_feature'][()]\nspatial_data_age_max = h5py.File(f'..\/input\/trends-assessment-prediction\/fMRI_train\/{age_max_id}.mat', 'r')['SM_feature'][()]\nprint(f'Spatial Data Initial Shape: {spatial_data_age_min.shape}')\n\nspatial_data_age_min = np.moveaxis(spatial_data_age_min, [0, 1, 2, 3], [0, 3, 2, 1])\nspatial_data_age_max = np.moveaxis(spatial_data_age_max, [0, 1, 2, 3], [0, 3, 2, 1])\nprint(f'Spatial Data Reoriented Axes Shape: {spatial_data_age_min.shape}')\n\nmasked_spatial_data_age_min = spatial_data_age_min[:, fmri_mask.get_data() == 1].astype(np.float32)\nmasked_spatial_data_age_max = spatial_data_age_max[:, fmri_mask.get_data() == 1].astype(np.float32)\nprint(f'Spatial Data Masked Shape: {masked_spatial_data_age_min.shape}')","bd389c6c":"fig, axes = plt.subplots(nrows=53, figsize=(30, 400), dpi=100)\n\nfor i in range(53):\n    \n    axes[i].plot(masked_spatial_data_age_min[i], label=f'{age_min_id}.mat (Youngest Age)', alpha=0.5)\n    axes[i].plot(masked_spatial_data_age_max[i], label=f'{age_max_id}.mat (Oldest Age)', alpha=0.5)\n    \n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=15)\n    axes[i].legend(prop={'size': 15})\n    axes[i].set_title(f'Component {i + 1} {icn_order[i]} Signal', size=20, pad=15)\n    \nplt.show()","5fc52f74":"class SpatialDataFeaturizer:\n    \n    def __init__(self, fwhm=0, visualize=False):\n        \n        self.binary_mask = nib.load('..\/input\/trends-assessment-prediction\/fMRI_mask.nii').get_fdata()\n        self.n_components = 53\n        self.networks = {\n            'ADN': np.array([5, 6]),\n            'CBN': np.array([49, 50, 51, 52]),\n            'CON': np.array([25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]),\n            'DMN': np.array([42, 43, 44, 45, 46, 47, 48]),\n            'SCN': np.array([0, 1, 2, 3, 4]),\n            'SMN': np.array([7,  8,  9, 10, 11, 12, 13, 14, 15]),\n            'VSN': np.array([16, 17, 18, 19, 20, 21, 22, 23, 24])\n        }\n        \n        self.fwhm = fwhm\n        self.visualize = visualize\n        \n        self.df_train_features = pd.DataFrame()\n        self.df_test_features = pd.DataFrame()       \n                \n    def _read_file(self, file_path):\n        \n        \"\"\"\n        Read the given .mat file with h5py\n        Switch second and fourth axes (53, 52, 63, 53) -> (53, 53, 63, 52)\n        Smooth images by applying a Gaussian filter with a smoothing factor of given fwhm\n        Reduce dimensions with binary mask (53, 53, 63, 52) -> (53, 58869)\n\n        Parameters\n        ----------\n        file_path : string\n        path of the MATLAB file\n\n        Returns\n        -------\n        masked_spatial_data : numpy array, shape = (53, 58869)\n        numpy array after reducing dimensions\n        \"\"\"\n        \n        spatial_data = h5py.File(file_path, 'r')['SM_feature'][()] \n        spatial_data = np.moveaxis(spatial_data, [0, 1, 2, 3], [0, 3, 2, 1])\n        \n        if self.fwhm > 1:\n            for i in range(self.n_components):\n                img = nl.image.new_img_like(ref_niimg=fmri_mask, data=spatial_data[i], affine=fmri_mask.affine, copy_header=True)\n                smoothed_img = nl.image.smooth_img(img, self.fwhm)\n                spatial_data[i] = smoothed_img.get_data()\n        \n        masked_spatial_data = spatial_data[:, self.binary_mask == 1].astype(np.float32)\n        return masked_spatial_data\n    \n    def _create_features(self, Id, masked_spatial_data):\n        \n        \"\"\"\n        Create features from the spatial data\n\n        Parameters\n        ----------\n        masked_spatial_data : numpy array, shape = (53, 58869)\n        numpy array after reducing dimensions\n\n        Returns\n        -------\n        features : pandas dataframe, shape = (1, n_features + 1)\n        features created for a single sample\n        \"\"\"\n        \n        features = {}\n        \n        for i in range(self.n_components):\n            \n            component = masked_spatial_data[i]\n            component_mean = component.mean()\n            component_std = component.std()\n\n            active = component[component > component_mean + component_std]\n            inactive = component[component < component_mean - component_std]            \n            more_active = component[component > component_mean + (2 * component_std)]\n            more_inactive = component[component < component_mean - (2 * component_std)]\n                    \n            component_skew = stats.skew(component)\n            component_kurtosis = stats.kurtosis(component)\n            active_skew = stats.skew(active)\n            inactive_skew = stats.skew(inactive)\n            \n            component_diff = np.diff(component)\n            \n            features['Id'] = Id,\n            features[f'{icn_order[i]}_skew'] = component_skew,\n            features[f'{icn_order[i]}_kurtosis'] = component_kurtosis,\n            features[f'{icn_order[i]}_diff_skew'] = stats.skew(component_diff),\n            features[f'{icn_order[i]}_active_spread'] = active.shape[0],\n            features[f'{icn_order[i]}_active_skew'] = active_skew,\n            features[f'{icn_order[i]}_inactive_spread'] = inactive.shape[0],\n            features[f'{icn_order[i]}_inactive_skew'] = inactive_skew,\n            features[f'{icn_order[i]}_active_inactive_mean_ratio'] = active.mean() \/ inactive.mean(),\n            features[f'{icn_order[i]}_active_inactive_std_ratio'] = active.std() \/ inactive.std(),\n            features[f'{icn_order[i]}_component_active_skew_difference'] = component_skew - active_skew,\n            features[f'{icn_order[i]}_component_inactive_skew_difference'] = component_skew - inactive_skew,\n            features[f'{icn_order[i]}_more_active_spread'] = more_active.shape[0],\n            features[f'{icn_order[i]}_more_inactive_spread'] = more_inactive.shape[0],\n                       \n            box_count = 15\n            for b, box in enumerate(np.array_split(component, box_count)):\n                features[f'{icn_order[i]}_box{b}_mean'] = box.mean()\n                box_active = box[box > component_mean + component_std]\n                features[f'{icn_order[i]}_box{b}_active_spread'] = box_active.shape[0],\n                \n        return pd.DataFrame(features)      \n    \n    def visualize_features(self):\n        \n        \"\"\"\n        Plot distributions of newly created features in training and test set\n        Plot scatter plots of newly created features vs every target features\n        \"\"\"\n    \n        for feature in self.df_train_features.columns.tolist()[1:]:\n            \n            print(f'New Feature {feature} Statistical Analysis\\n{\"-\" * 42}')\n\n            print(f'Training Mean: {float(self.df_train_features[feature].mean()):.4}  - Training Median: {float(self.df_train_features[feature].median()):.4} - Training Std: {float(self.df_train_features[feature].std()):.4}')\n            print(f'Test Mean: {float(self.df_test_features[feature].mean()):.4}  - Test Median: {float(self.df_test_features[feature].median()):.4} - Test Std: {float(self.df_test_features[feature].std()):.4}')\n            print(f'Training Min: {float(self.df_train_features[feature].min()):.4}  - Training Max: {float(self.df_train_features[feature].max()):.4}')\n            print(f'Test Min: {float(self.df_test_features[feature].min()):.4}  - Training Max: {float(self.df_test_features[feature].max()):.4}')\n            print(f'Training Skew: {float(self.df_train_features[feature].skew()):.4}  - Training Kurtosis: {float(self.df_train_features[feature].kurtosis()):.4}')\n            print(f'Test Skew: {float(self.df_test_features[feature].skew()):.4}  - Test Kurtosis: {float(self.df_test_features[feature].kurtosis()):.4}')\n            training_missing_values_count = self.df_train_features[self.df_train_features[feature].isnull()].shape[0]\n            test_missing_values_count = self.df_test_features[self.df_test_features[feature].isnull()].shape[0]\n            training_samples_count = self.df_train_features.shape[0]\n            test_samples_count = self.df_test_features.shape[0]\n            print(f'Training Missing Values: {training_missing_values_count}\/{training_samples_count} ({training_missing_values_count * 100 \/ training_samples_count:.4}%)')\n            print(f'Test Missing Values: {test_missing_values_count}\/{test_samples_count} ({test_missing_values_count * 100 \/ test_samples_count:.4}%)')\n\n            fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(25, 12), dpi=100, constrained_layout=True)\n            title_size = 18\n            label_size = 18\n\n            # New Feature Training and Test Set Distribution\n            sns.distplot(self.df_train_features[feature], label='Training', ax=axes[0][0])\n            sns.distplot(self.df_test_features[feature], label='Test', ax=axes[0][0])\n            axes[0][0].set_xlabel('')\n            axes[0][0].tick_params(axis='x', labelsize=label_size)\n            axes[0][0].tick_params(axis='y', labelsize=label_size)\n            axes[0][0].legend()\n            axes[0][0].set_title(f'{feature} Distribution in Training and Test Set', size=title_size, pad=title_size)\n\n            # New Feature vs age\n            sns.scatterplot(self.df_train_features[feature], self.df_train_features['age'], ax=axes[0][1])\n            axes[0][1].set_title(f'{feature} vs age', size=title_size, pad=title_size)\n            axes[0][1].set_xlabel('')\n            axes[0][1].set_ylabel('')\n            axes[0][1].tick_params(axis='x', labelsize=label_size)\n            axes[0][1].tick_params(axis='y', labelsize=label_size)\n\n            # New Feature vs domain1_var1\n            sns.scatterplot(self.df_train_features[feature], self.df_train_features['domain1_var1'], ax=axes[0][2])\n            axes[0][2].set_title(f'{feature} vs domain1_var1', size=title_size, pad=title_size)\n            axes[0][2].set_xlabel('')\n            axes[0][2].set_ylabel('')\n            axes[0][2].tick_params(axis='x', labelsize=label_size)\n            axes[0][2].tick_params(axis='y', labelsize=label_size)\n\n            # New Feature vs domain1_var2\n            sns.scatterplot(self.df_train_features[feature], self.df_train_features['domain1_var2'], ax=axes[1][0])\n            axes[1][0].set_title(f'{feature} vs domain1_var2', size=title_size, pad=title_size)\n            axes[1][0].set_xlabel('')\n            axes[1][0].set_ylabel('')\n            axes[1][0].tick_params(axis='x', labelsize=label_size)\n            axes[1][0].tick_params(axis='y', labelsize=label_size)\n\n            # New Feature vs domain2_var1\n            sns.scatterplot(self.df_train_features[feature], self.df_train_features['domain2_var1'], ax=axes[1][1])\n            axes[1][1].set_title(f'{feature} vs domain2_var1', size=title_size, pad=title_size)\n            axes[1][1].set_xlabel('')\n            axes[1][1].set_ylabel('')\n            axes[1][1].tick_params(axis='x', labelsize=label_size)\n            axes[1][1].tick_params(axis='y', labelsize=label_size)\n\n            # New Feature vs domain2_var2\n            sns.scatterplot(self.df_train_features[feature], self.df_train_features['domain2_var2'], ax=axes[1][2])\n            axes[1][2].set_title(f'{feature} vs domain2_var2', size=title_size, pad=title_size)\n            axes[1][2].set_xlabel('')\n            axes[1][2].set_ylabel('')\n            axes[1][2].tick_params(axis='x', labelsize=label_size)\n            axes[1][2].tick_params(axis='y', labelsize=label_size)\n\n            plt.show()        \n                \n    def run(self):\n                \n        fMRI_train = [f'..\/input\/trends-assessment-prediction\/fMRI_train\/{file}' for file in sorted(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train'))]\n        fMRI_test = [f'..\/input\/trends-assessment-prediction\/fMRI_test\/{file}' for file in sorted(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_test'))]\n        \n        print('Creating spatial features from fMRI_train')\n\n        for file_path in tqdm(fMRI_train):\n            Id = file_path.split('\/')[-1].strip('.mat')\n\n            masked_spatial_data = self._read_file(file_path)\n            df_features = self._create_features(Id, masked_spatial_data)\n            \n            self.df_train_features = pd.concat([self.df_train_features, df_features], axis=0) \n        \n        self.df_train_features.reset_index(inplace=True, drop=True)\n        self.df_train_features['Id'] = self.df_train_features['Id'].astype(np.uint32) \n        self.df_train_features = self.df_train_features.merge(df[['Id'] + target_features], on='Id', how='left')\n        \n        print('Creating spatial features from fMRI_test')\n            \n        for file_path in tqdm(fMRI_test):\n            Id = file_path.split('\/')[-1].strip('.mat')\n\n            masked_spatial_data = self._read_file(file_path)\n            df_features = self._create_features(Id, masked_spatial_data)\n\n            self.df_test_features = pd.concat([self.df_test_features, df_features], axis=0)  \n                       \n        self.df_test_features.reset_index(inplace=True, drop=True)\n        self.df_test_features['Id'] = self.df_test_features['Id'].astype(np.uint32) \n        self.df_test_features = self.df_test_features.merge(df[['Id'] + target_features], on='Id', how='left')\n        \n        if self.visualize:\n            self.visualize_features()\n","96005ed8":"featurizer = SpatialDataFeaturizer(fwhm=0, visualize=False)\nfeaturizer.run()","499cb200":"print(f'Loading and FNC features {df.shape} are merged with spatial features {pd.concat([featurizer.df_train_features, featurizer.df_test_features], axis=0).shape}')\ndf = df.merge(pd.concat([featurizer.df_train_features, featurizer.df_test_features], axis=0).drop(columns=target_features), on='Id', how='left')","308fe03d":"utility_features = ['is_train', 'site', 'site_predicted']\n\nspatial_features = [feature for feature in df.columns.tolist()[1:] if feature not in fnc_features and \n                                                                      feature not in loading_features and\n                                                                      feature not in target_features and \n                                                                      feature not in utility_features]\n\nbounding_box_features = [feature for feature in spatial_features if 'box' in feature]\nstatistical_features = [feature for feature in spatial_features if 'box' not in feature]\n\nprint(f'Number of Target Features: {len(target_features)}')\nprint(f'Number of Loading Features: {len(loading_features)}')\nprint(f'Number of FNC Features: {len(fnc_features)}')\nprint(f'Number of Utility Features: {len(utility_features)}')\nprint(f'Number of Bounding Box Features: {len(bounding_box_features)}')\nprint(f'Number of Statistical Features: {len(statistical_features)}')\n\ndf.to_pickle('trends_tabular_data.pkl')\n\nprint(f'TReNDS Tabular Data Shape = {df.shape}')\nprint(f'TReNDS Tabular Data Memory Usage = {df.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","1b8baa00":"### **5.2 Visualizing 3D Data on 2D Space**\n\nThose **53** 3D spatial maps can be visualized with `nilearn` package. However, 3D spatial maps have to be drawn on a reference image and the reference image is `fMRI_mask.nii` file that is shared in the competition data. Niimg-like objects (`.nii` files) can be load from filenames or list of filenames with `nilearn` as well.\n\nBefore the 3D spatial maps are loaded, it's necessary to reorient the axis, since h5py flips axis order. The axis of data is reoriented in reverse order.\n\nFinally, 3D spatial maps can be loaded as a Niimg-like object with `nilearn.image.new_img_like` by using the `fMRI_mask.nii` as a reference image.","6c726e2f":"### **1.1 Target Distributions**\n\nEven though some of target features are slightly tailed, all of them follow a normal distribution. Their descriptive statistical summary are very similar to each other.\n\nA small percentage of values are missing in target features except `age`. Target features in the same domain have same number of missing values and they are missing in same samples.  Those are skipped in the score calculation. However, every row should be predicted in the submission file.","77d256a1":"After the 3D spatial maps are loaded as Niimg-like objects, they can be iterated with `nilearn.image.iter_img` and visualized one by one with `nilearn.plotting.plot_stat_map`. The **53** 3D spatial maps that are visualized below, belong to a single sample, `10001.mat`. There are **5877** samples in both `fMRI_train` and `fMRI_test`.\n\n`nilearn` is very useful to visualize 3D spatial maps because `plot_stat_map` plots the data from three different angles which makes it easier to see the correlated parts of brain.","e1a0d5da":"### **6.1 ICN Numbers**\n\nThe last file shared in the competition data is `ICN_numbers.csv`. It has intrinsic connectivity network numbers for each fMRI spatial map. Those numbers can be mapped to **53** 3D spatial maps.\n\nValues in `ICN_number` represent the number that comes after the network names in parentheses. After the network names are connected with their numbers, they can be mapped to 3D spatial maps directly because index is the order of the 3D spatial maps. It means the first 3D spatial map in every sample is `SCN(69)`, the second 3D spatial map in every sample is `SCN(53)` and etc.","b5d1b903":"### **6.5 Spatial Data Featurizer**\n\n`SpatialDataFeaturizer` is a preprocessing and feature engineering pipeline. It executes two main steps for every .mat file in `fMRI_train` and `fMRI_test`. Those two main steps are:\n\n1. Read file into memory and process it for feature engineering\n  * Read the .mat file with `h5py`\n  * Switch second and fourth axes\n  * Smooth images by applying a Gaussian filter (This step is not used because smoothing loses too much information)\n  * Reduce dimensions with fMRI mask (`fMRI_mask.nii`)\n2. Create Features\n  * Skew and kurtosis of the components\n  * Skew of the components' difference\n  * Shapes of active\/inactive and more active\/inactive regions\n  * Skew of the active\/inactive regions\n  * Mean of active region \/ mean of inactive region\n  * Std of active region \/ std of inactive region\n  * Skew of the component - skew of the active\/inactive regions\n  * Means and shapes of active regions in 15 equally divided bounding boxes","f3e075a0":"## **4. Sites**\n\n`reveal_ID_site2.csv` a list of subject IDs whose data was collected with a different scanner than the train samples.\n\n> Models are expected to generalize on data from a different scanner\/site (site 2). All subjects from site 2 were assigned to the test set, so their scores are not available. While there are fewer site 2 subjects than site 1 subjects in the test set, the total number of subjects from site 2 will not be revealed until after the end of the competition. To make it more interesting, the IDs of some site 2 subjects have been revealed below. Use this to inform your models about site effects. Site effects are a form of bias. To generalize well, models should learn features that are not related to or driven by site effects.\n\nThis paragraph is taken from the data tab. It means data is collected from two different sources, site 1 and 2. This is the reason of train and test set feature distribution discrepancies.\n\n* All of the data in training set is collected from site 1.\n* All of the data collected from site 2 are in test set but the entire test set is not collected from site 2.\n\nIn this case, there is no way to find out site 1 samples in test set perfectly, but they can be predicted with a classifier by looking at feature distributions.","7ebf6ec5":"### **3.7. Cerebellar Network (CBN) Features** \n\nSeventh FNC feature sub-group is CBN features. This is a large sub-group and there are **202** features in it. This sub-group has cross-correlations with SCN (20), ADN (8), SMN (36), VSN (36), CON (68), DMN (28) and itself (6).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN, VSN, CON and DMN features can be seen on CBN features as well.\n\nEvery cross-correlation of CBN have very strong correlations inside the second network groups. This explains the bright red and blue blocks near the diagonal axis.\n\n* `CBN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `CBN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `CBN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `CBN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `CBN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `CBN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n* `CBN(X)_vs_CBN(Y)`: `X` is strongly correlated with every different `Y` value for `CBN`\n\nEvery cross-correlation of CBN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis, and repeating bright red and orange diagonal lines.\n\n* `CBN(X)_vs_ADN(Y)`: `Y` is strongly correlated with every different `X` value for `ADN` \n* `CBN(X)_vs_SCN(Y)`: `Y` is strongly correlated with every different `X` value for `SCN` \n* `CBN(X)_vs_SMN(Y)`: `Y` is strongly correlated with every different `X` value for `SMN`\n* `CBN(X)_vs_VSN(Y)`: `Y` is strongly correlated with every different `X` value for `VSN`\n* `CBN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `CBN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n* `CBN(X)_vs_CBN(Y)`: `X` is strongly correlated with every different `Y` value for `CBN`\n\nCorrelations between target and CBN features are very weak which can be seen at the bottom and right end.","9430f3b8":"### **1.5 Target Correlations**\n\nTarget features are not correlated with each other too much. The strongest correlations are between `age` and `domain1_var1` (**0.34**), and between  `age` and `domain2_var1` (**0.23**). There might be a relationship between  `age` and var1 features.\n\nCorrelation between `domain1_var1` and `domain1_var2` is **0** which strongly indicates that Domain 1 is a single variable and it has two independent components.\n\nCorrelation between `domain2_var1` and `domain2_var2` is **0.18** which It is too high for both PCA and ICA, but they are probably two independent components of a single variable as well.","e3ca7ceb":"### **6.4 Masker**\n\nData in `fMRI_train` and `fMRI_test` are represented in 4 dimensional space: 3 spatial dimensions and one component dimension. The 4th dimension is time in most cases but it is 53 different components in this dataset. It is convenient to apply a brain mask in order to convert the 4D data into a restructured 2D data representation, **voxel x time**, as seen below:\n\n![masker](https:\/\/nilearn.github.io\/_images\/masking1.jpg)\n\nComponent dimension can be treated as time dimension, and thus 4D data can be transformed with `fMRI_mask.nii` into 2D **voxel x component** structure. Transformed data of every subject will have the shape of **53 timesteps (components) x 58869 voxels**, so the whole dataset's shape will be `(11754, 53, 58869)`. Based on that information, the data can be modeled with both 2D and 3D CNNs.","0814a7b9":"The `.mat` files are using dictionary interface so data inside the file can be accessed with `keys()` and `values()` methods. `.keys()` method shows us there is only one key in the file and it is `SM_feature`. That is where the data is stored.\n\nAccessing the data using `matlab_file[\"SM_feature\"]` shows us, each `.mat` file is a 4D HDF5 dataset with the shape of `(53, 52, 63, 53)`. Those are the **53** 3D spatial maps.","d4a54892":"## **7. Conclusion**\n\nColumns in `fnc.csv`, `loading.csv` and `train_scores.csv` are merged on `Id`. Three features `is_train`, `site` and `site_predicted` are created for labeling different datasets and sites. Bounding box features and statistical features are extracted from `fMRI_train` and `fMRI_test`. Data is saved in pickle format for faster save and load time.","2386d7b8":"## **2. Source-based Morphometry Loadings**\n\nThe first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray matter concentration maps from structural MRI (sMRI) scans. Those features are for both training and test samples.\n\nThere are **26** features in `loading.csv` file without `Id`. Those features are named from `IC_01` to `IC_30`, but `IC_19`, `IC_23`, `IC_25` and `IC_27` don't exist. They are parts of brain and their explanations are listed below:\n\n`\nIC_01 - Cerebellum\nIC_02 - ACC+mpfc\nIC_03 - Caudate\nIC_04 - Cerebellum\nIC_05 - Calcarine\nIC_06 - Calcarine\nIC_07 - Precuneus+PCC\nIC_08 - Frontal\nIC_09 - IPL+AG\nIC_10 - MTG\nIC_11 - Frontal\nIC_12 - SMA\nIC_13 - Temporal Pole\nIC_14 - Temporal Pole + Fusiform\nIC_15 - STG\nIC_16 - Middle Occipital?\nIC_17 - Cerebellum\nIC_18 - Cerebellum\nIC_20 - MCC\nIC_21 - Temporal Pole + Cerebellum\nIC_22 - Insula + Caudate\nIC_24 - IPL+Postcentral\nIC_26 - Inf+Mid Frontal\nIC_28 - Calcarine\nIC_29 - MTG\nIC_30 - Inf Frontal\n`","41b470a6":"### **3.2. Auditory Network (ADN) Features**\n\nSecond FNC feature sub-group is ADN features. This is also the second smallest sub-group of FNC features and there are **11** features in it. This sub-group has cross-correlations with SCN (10) and itself (1).\n\nADN features are strongly correlated with each other if only ADN is cross-correlated with SCN. There are 10 ADN(2) vs SCN(5) features and they have very strong correlations. There is only one ADN vs ADN (`ADN(56)_vs_ADN(21)`) feature and it doesn't have any significant correlation with anything. Correlations between target and ADN features are very weak which can be seen at the bottom and right end.","db1fefda":"### **3.8. Inference**\nFNC features from the same group are more likely to be correlated with each other. Correlations from different groups doesn't seem to be very strong. Even in the same groups, there are lots of weak correlations. FNC features from the same groups must be in the same network in order to have strong correlation. FNC features in the same network don't have strong correlations if they are not in the same group.\n\nThe entire heatmap of FNC features is rendered below but it is very hard to derive any meaning from it.","1631613a":"Target features `domain2_var1` and `domain2_var2` are independent, and adding them to each other creates a new target which has a higher predictive power and stronger linear relationships as well. The distribution of the new target becomes more bumpy and tailed.","ae8879ee":"### **6.3 Noise Comparison**\n\nThere are lots of noise in all components. Inactive and more inactive regions appear to be more noisy and scattered around. The noise can be removed with `image.smooth_img` function. It smoothes images by applying a Gaussian filter and it takes two parameters; the image and smoothing factor (`fwhm`).\n\nAs the FWHM increases, the amount of noise decreases along with loose spatial details. There is a trade off between noise and information. In general, the best amount of smoothing for a given analysis depends on the spatial extent of the effects that are expected. In this case, the best amount of smoothing should be between 0 and 5. FWHM greater than 5 loses too much information.","4c56db12":"## **1. Train Scores (Targets)**\n`train_scores.csv` is the file which consists of target features. Those features are `age`, `domain1_var1`, `domain1_var2`, `domain2_var1` and `domain2_var2`. There are 5 target features to predict and submissions are scored using feature-weighted, normalized absolute errors.\n\n**score** ${= \\Large \\sum\\limits_{f} w_{f} \\Big( \\frac{\\sum_{i} \\big| y_{f,i}  - \\hat{y}_{f,i}\\big| }{\\sum_{i} \\hat{y}_{f,i}} \\Big) }$\n\nThe weights are `[.3, .175, .175, .175, .175]` corresponding to features `[age, domain1_var1, domain1_var2, domain2_var1, domain2_var2]`. This means every targets normalized absolute error is independent from each other. They can be trained and predicted with a single model or 5 different models.\n\nAnother important thing to consider is, `train_scores.csv` are not the original age and raw assessment values. They have been transformed and de-identified to help protect subject identity and minimize the risk of unethical usage of the data. Nonetheless, they are directly derived from the original assessment values and, thus, associations with the provided features is equally likely.\n\n**Before transformation, the age in the training set is rounded to nearest year for privacy reasons. However, age is not rounded to year (higher precision) in the test set. Thus, heavily overfitting to the training set age will very likely have a negative impact on your submissions.**","e44117b4":"### **1.3 Domain 1 Variables**\nSecond group of target variables are `domain1_var1` and `domain1_var2`. They are dependent to each other and both of them have **438** missing values in the same samples.\n\nThere are anomalies in `domain1_var1` and `domain1_var2` values. Every unique value in `domain1_var1` can be another unique value in `domain1_var2`, and vice versa. Both of those target features have **5328** unique values. At first this could be related to high precision in target features, but there are duplicate samples which can be seen below. Thus, `domain1_var1` and `domain1_var2` look like unique value pairs rather than continuous data.","1e140704":"### **3.5. Cognitive-control Network (CON) Features** \n\nFifth FNC feature sub-group is CON features. This is the largest sub-group and there are **561** features in it. This sub-group has cross-correlations with SCN (85), ADN (34), SMN (153), VSN (153) and itself (136).\n\nIt is even hard to identify correlations at block level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN and VSN features can be seen on CON features as well.\n\nEvery cross-correlation of CON have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `CON(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `CON(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `CON(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `CON(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `CON(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n\nEvery cross-correlation of CON have weak correlations inside the first network groups. This explains the light orange and light blue color blocks everywhere except near of diagonal axis.\n\nCorrelations between target and CON features are very weak which can be seen at the bottom and right end.","d992890e":"The same things in loading features can be observed in FNC features as well. Slightly shifted train\/test feature distributions are shifted even more in site 1\/2. However, none of the feature shifts are as severe as `IC_20` in loading features.\n\n**0.125** is an arbitrary number and distribution difference can be measured with other ways as well, but feature selection should definitely be based on site 1 and 2 feature distribution difference.","164c9dff":"### **4.2. FNC Features Site Distributions**\n\nThere are 1378 FNC features so they can't be analyzed one by one. Kolmogorov\u2013Smirnov test calculated on site 1 and site 2 feature distribution can be used for feature selection. If the KS statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same. I used **0.125** for KS statistic threshold and selected features that are exceeding the threshold.","3a0fb917":"Extracted signals may vary from component to component and subject to subject. The youngest subject (`19531.mat`) and the oldest subject (`17104.mat`) are selected again and their extracted signals are drawn. There are both structural and random differences on those signals and they are listed below.\n\n* Different subjects have their peaks aligned in all components but the height of the peaks are different. This shows the activation intensity difference between different subjects. The peak alignment means that the activation of certain locations are consistent between different subjects.\n* Components' standard deviations are very similar to each other between different subjects, but rarely they can be different in some components.","142ed035":"The youngest subject (`10721.mat`) and the oldest subject (`11685.mat`) are selected and plotted side by side. The differences of those fMRIs are listed below.\n\n* The locations of the active\/inactive regions are similar for both fMRIs, but they may shift, expand or shrink slightly. The locations of the more active\/inactive regions are less similar for both fMRIs, and they may shift, expand or shrink more significantly. It looks like the changes in more active region is dependent to target, but changes in more inactive region is random. Active regions are more dense and inactive regions are more scattered around. Based on that observation, features that are capturing active\/inactive regions and more active\/inactive regions could be useful.\n\n* Active\/inactive intensity and balance is another phenomenon that has effects on targets. In almost every component, active\/inactive intensity changes between different subjects. The change in active\/inactive intensity may vary from component to component as well. Active\/inactive balance also changes between different subjects due to intensity change. Based on that observation, features that are capturing active\/inactive regions' distributions could be useful.\n\n* Active\/inactive spread and intensity in specific locations (region of interests) has major effects on targets. Featurizing the change in those particular locations could be useful.\n\nFeatures should be created separately for every different component because patterns change a lot between different components. One newly created feature can have positive linear relationship in one component, but at the same time, it can have negative relationship in another component.","e3ba6c79":"### **1.4 Domain 2 Variables**\n\nThird group of target variables are `domain2_var1` and `domain2_var2`. They are dependent to each other as well and both of them have **39** missing values in the same samples.\n\nThe anomalies in `domain1_var1` and `domain1_var2` values exist in `domain2_var1` and `domain2_var2` and it is more severe. Both of those target features have **2038** unique values. They also look like unique value pairs rather than continuous data. ","8170d314":"### **6.2 fMRI Comparison**\n\nFeature engineering should be based on the differences of fMRIs between different target groups. In order to make the differences more clear, fMRIs of the lowest value and the highest value are selected for every target feature and plotted side by side.\n\n* **Active\/Inactive Regions:** Yellow and light blue areas\n* **More Active\/Inactive Regions:** Red and dark blue areas\n\nActive\/Inactive Regions and patterns vary a lot from component to component. They also vary a lot from target to target, so one feature could be useful for one target, but it could be useless for another target at the same time.","6906dd8c":"### **3.6. Default-mode Network (DMN) Features** \n\nSixth FNC feature sub-group is DMN features. This is the second largest sub-group and there are **315** features in it. This sub-group has cross-correlations with SCN (35), ADN (14), SMN (63), VSN (63), CON (119) and itself (21).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN, VSN and CON features can be seen on DMN features as well.\n\nEvery cross-correlation of DMN have very strong correlations inside the second network groups. This explains the bright red and blue blocks near the diagonal axis.\n\n* `DMN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `DMN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `DMN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `DMN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `DMN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `DMN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n\nEvery cross-correlation of DMN have weak correlations inside the first network groups. This explains the light orange and light blue color blocks everywhere except near of diagonal axis. There are also repeating light orange and light blue diagonal lines. \n\nCorrelations between target and DMN features are very weak which can be seen at the bottom and right end.","dfe97aed":"Ridge regression is used in this simulation and the results are listed below. The gap between training and validation NAE looks very realistic based on the train\/test discrepancies, especially for `age`.\n\n`\nage Train NAE 0.129934 - Validation NAE 0.154733\ndomain1_var1 Train NAE 0.143931 - Validation NAE 0.147178\ndomain1_var2 Train NAE 0.150529 - Validation NAE 0.146966\ndomain2_var1 Train NAE 0.176281 - Validation NAE 0.189226\ndomain2_var2 Train NAE 0.171305 - Validation NAE 0.171517\n`\n\nEven though Ridge regression scores **0.159** on public leaderboard, it scored **0.161** on slightly different samples. This means the private leaderboard score will be even worse.\n\n`Train Weighted NAE 0.151338 - Validation Weighted NAE 0.161025`","850ba23e":"### **4.3. Site Classification**\n\nEven though it is impossible to predict the sites of unlabeled samples perfectly, shifted features can be used in a classifier and predict the sites to some degree. Loading features should be used in this classifier along with shifted FNC features.","e38ab2ce":"### **2.1 Source-based Morphometry Loadings' Distributions**\n\nAll of the loading features follow a normal distribution. Their distributions and descriptive statistical summary in training and test samples are very similar except `IC_20`. It is an exception because the distribution of `IC_20` in test samples is shifted. This feature may require some preprocessing.\n\nNone of the loading features have a visible relationship with any of the targets. Data points of loading features are scattered around the means of `domain1_var1`, `domain1_var2`, `domain2_var1`, `domain2_var2`, however `age` has a tiny relationship with loading features. This relationship is not easy to detect but it looks like `age` is easier to predict than other target features.\n\nThere are some data points which can be classified as mild outliers in all loading features. Models used for predicting targets, should be robust to outliers. The most extreme outlier in loading features belongs to `IC_02`. It is a single data point and it's locations are very close in every target.","c7e80c47":"### **1.2 Age**\n\nFirst and the most important target variable is `age`. It has the highest weight in the competition metric. It is different than other target variables because `age` in training set is rounded to nearest year for privacy reasons. However, `age` in test set is not rounded. Therefore, overfitting to those rounded values might be disastrous.\n\nCardinality of `age` in training set is very low due to rounding. There are **33** unique values in `age`, so adding random noise to it might help models to generalize better.","02ae5087":"## **5. Component Spatial Maps**\n\nThe third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI). Those features are contained under the directories below:\n\n* `fMRI_train` - a folder containing 53 3D spatial maps for train samples in .mat format\n* `fMRI_test` - a folder containing 53 3D spatial maps for test samples in .mat format\n\nThose explanations are written under the Data tab on the competition page, but they are not clear. `fMRI_train` and `fMRI_test` are directories in which there are **5877** samples. Each sample is a `.mat` file that contains **53** 3D spatial maps.\n\nThis means there are **311481** 3D spatial maps in both `fMRI_train` and `fMRI_test`, and this is the reason why competition data is **166.59 GB**.","b82ff573":"Even though `domain1_var1` and `domain1_var2` are independent, adding them to each other creates a new target feature which has higher predictive power and stronger linear relationships. The distribution of the new target is smooth and less tailed.","6dcf0903":"### **2.2 Source-based Morphometry Loadings' Correlations**\n\nThere are strong correlations between `age` and some loading features that exceed **-0.4**, but none of the loading features are correlated with other targets.\n\nLoading features have decent correlations between themselves that exceed **0.5** and **-0.5**. Some of those high positive correlations belong to feature groups which are from the same part of the brain. However, all features from the same part of the brain are not necessarily correlated with each other, so there is no pattern here.","23ee8dec":"### **4.4. Private Leaderboard Simulation**\nIf the expected values predicted by the site classifier are rounded, there are **32** samples classified as site 2. This operation ceils the values greater than **.5** and floors the values lesser than **.5**. It yields very few samples for models to use as a validation set.\n\nIf this threshold is set to **.2**, a 0.9 train\/test split can be achieved with samples which slightly look like they are from site 2. I tried to simulate the private leaderboard by:\n\n* Training on **5342** samples which are classified as site 1 (`site_predicted` expected value < 1.2)\n* Validate on **535** samples which slightly look like site 2 (`site_predicted` expected value >= 1.2)","edaf994d":"### **3.3. Sensorimotor Network (SMN) Features**\n\nThird FNC feature sub-group is SMN features. This is a large sub-group compared to previous ones and there are **99** features in it. This sub-group has cross-correlations with SCN (45), ADN (18) and itself (36).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect.\n\nEvery cross-correlation of SMN have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `SMN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `SMN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `SMN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n\nEvery cross-correlation of SMN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis.\n\n* `SMN(X)_vs_ADN(Y)`: `Y` is moderately correlated with every different `X` value for `ADN` \n* `SMN(X)_vs_SCN(Y)`: `Y` is moderately correlated with every different `X` value for `SCN` \n* `SMN(X)_vs_SMN(Y)`: `Y` is moderately correlated with every different `X` value for `SMN`\n\nCorrelations between target and SMN features are very weak which can be seen at the bottom and right end.","a857ebef":"## **6. Feature Engineering**","1857a94c":"### **5.1 MATLAB Files**\n\nThe `.mat` files in this competition can be read in Python using `h5py.File`. HDF5 files work generally like standard Python file objects. They support standard modes like r\/w\/a, and should be closed when they are no longer in use. For an example, the first file (`10001.mat`) inside the `fMRI_train` is read in the cell below.","85171e52":"There are various ways to visualize 3D spatial maps other than `plot_stat_map`. 5 different plotting functions used in the visualizations below in every row. Those plotting functions are `plot_glass_brain`, `plot_roi`, `plot_anat`, `plot_epi` and `plot_img`. Some of those visualizations are more detailed and some of them are faster.\n\n`threshold` parameter is used for reducing the noise in visualizations. If `None` is given, the image is not thresholded. If a number is given, it is used to threshold the image: values below the threshold (in absolute value) are plotted as transparent.\n","27fdfe5b":"Expected values closer to **1** and **2** in `site_predicted`, could possible be added to the original `site` feature since they are confident predictions.\n\n`IC_20`, `IC_18`, `IC_11`, `IC_21` and `IC_05` are at the top of feature importance in site classifier model. It looks like FNC features don't contribute site classifier model as much as loading features. `IC_20` is the most dangerous feature without any doubt but other loading features should be selected very carefully in main models.","29f71663":"### **3.4. Visual Network (VSN) Features** \n\nFourth FNC feature sub-group is VSN features. This is a very large sub-group compared to previous ones and there are **180** features in it. This sub-group has cross-correlations with SCN (45), ADN (18), SMN (81) and itself (36).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN features can be seen on VSN features as well.\n\nEvery cross-correlation of VSN have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `VSN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `VSN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `VSN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `VSN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n\nEvery cross-correlation of VSN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis.\n\n* `VSN(X)_vs_ADN(Y)`: `Y` is moderately correlated with every different `X` value for `ADN` \n* `VSN(X)_vs_SCN(Y)`: `Y` is moderately correlated with every different `X` value for `SCN` \n* `VSN(X)_vs_SMN(Y)`: `Y` is moderately correlated with every different `X` value for `SMN`\n* `VSN(X)_vs_VSN(Y)`: `Y` is moderately correlated with every different `X` value for `VSN`\n\nCorrelations between target and VSN features are very weak which can be seen at the bottom and right end.","d12ef061":"## **3. Static Functional Network Connectivity**\nThe second set of features are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nThere are **1378** features in `fnc.csv` file without `Id`. Those features are named as `Network1(X)_vs_Network2(Y)` and there are **7** different networks. Network names and abbreviations are listed below:\n\n`\nSCN - Sub-cortical Network\nADN - Auditory Network\nSMN - Sensorimotor Network\nVSN - Visual Network\nCON - Cognitive-control Network    \nDMN - Default-mode Network\nCBN - Cerebellar Network\n`\n\nThose groups might be useful to analyze **1378** features part by part.","317c6414":"### **5.3 Visualizing 3D Data on 3D Space**\n3D spatial maps can also be visualized on 3D space with `nilearn` package. `view_img_on_surf` used as the plotting function and it renders the 3D data onto a 3D brain figure. 3D visualizations can be thresholded with `threshold` parameter as well. ","379c180b":"### **4.1. Loading Features Site Distributions**\n\nIt is hard to make an accurate analysis on feature distributions for different sites because sample sizes are different for site 1 and 2. There are **5877** site 1 samples (training set) and **510** verified site 2 samples. That's why train\/test distribution of the feature should be plotted as well in order to measure the shift in distribution.\n\nThere are three types of shifts in the distributions ploted below:\n\n* Mild shift in site 1\/2 feature distribution but it can't be seen on train\/test feature distribution (Mean shift close to \u00b10.001). Those shifts can be observed due to small sample size of site 2 subjects, but they are suspicious.\n* Severe shift in site 1\/2 feature distribution that it can also be seen on train\/test feature distribution (Mean shift greater than \u00b10.002). Those features should be eliminated.\n* No shift in both site 1\/2 and train\/test feature distributions. Those features can be trusted. ","06127d0a":"### **3.1. Sub-cortical Network (SCN) Features**\n\nFirst FNC feature sub-group is SCN features. This is the smallest sub-group of fnc features and there are **10** features in it. This sub-group has cross-correlations with only itself.\n\nSCN features are strongly correlated with each other, but none of the SCN features are correlated with targets. Correlations between target and SCN features are very weak which can be seen at the bottom and right end."}}