{"cell_type":{"3a45d8b8":"code","21ae2c7c":"code","0441c6d5":"code","5c36f35c":"code","f37f4169":"code","c6f88397":"code","d0451a84":"code","17ec3034":"code","3b4f21a2":"code","9c3e2fc7":"code","361d281c":"code","478920e3":"code","a1d7506f":"code","3b079fed":"code","f9eccf0f":"code","32c96492":"code","3062dc72":"code","4bc0a451":"code","32471d8f":"code","8b2e8f9b":"markdown"},"source":{"3a45d8b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","21ae2c7c":"train = pd.read_csv('..\/input\/covid19-global-forecasting-week-2\/train.csv')\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-2\/test.csv')\nsubmission = pd.read_csv('..\/input\/covid19-global-forecasting-week-2\/submission.csv')","0441c6d5":"test['Date']=test.Date.astype('datetime64[ns]')","5c36f35c":"train['key']=train['Province_State'].astype('str')+ \" \" + train['Country_Region'].astype('str')\n\ntest['key']=test['Province_State'].astype('str')+ \" \" + test['Country_Region'].astype('str')\n\ntrain.describe()","f37f4169":"train=train[train['Date']<='2020-03-19']","c6f88397":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\ntrain_lag_1=train.groupby(['key']).shift(periods=1)\ntrain_lag_2=train.groupby(['key']).shift(periods=2)\ntrain_lag_3=train.groupby(['key']).shift(periods=3)\ntrain['lag_1_ConfirmedCases']=train_lag_1['ConfirmedCases']\ntrain['lag_1_Fatalities']=train_lag_1['Fatalities']\ntrain['lag_2_ConfirmedCases']=train_lag_2['ConfirmedCases']\ntrain['lag_2_Fatalities']=train_lag_2['Fatalities']\ntrain['lag_3_ConfirmedCases']=train_lag_3['ConfirmedCases']\ntrain['lag_3_Fatalities']=train_lag_3['Fatalities']\ntrain","d0451a84":"def pred_ets(fcastperiod,fcastperiod1,actual,ffcast,type_ck='ConfirmedCases',verbose=False):\n    \n    actual=actual[actual[type_ck]>0]\n    index=pd.date_range(start=ffcast.index[0], end=ffcast.index[-1], freq='D')\n    data=ffcast[type_ck].values\n    ffcast1 = pd.Series(data, index)\n    index=pd.date_range(start=actual.index[0], end=actual.index[-1], freq='D')\n    data=actual[type_ck].values\n    daily_analysis_dat = pd.Series(data, index)\n    livestock2=daily_analysis_dat\n    fit=[]\n    fcast=[]\n    fname=[]\n    try:\n        fit1 = SimpleExpSmoothing(livestock2).fit()\n        fcast1 = fit1.forecast(fcastperiod1).rename(\"SES\")\n        fit.append(fit1)\n        fcast.append(fcast1)\n        fname.append('SES')\n    except:\n        1==1\n    try:\n        fit2 = Holt(livestock2).fit()\n        fcast2 = fit2.forecast(fcastperiod1).rename(\"Holt\")\n        fit.append(fit2)\n        fcast.append(fcast2)\n        fname.append('Holt')\n    except:\n        1==1\n    try:\n        fit3 = Holt(livestock2, exponential=True).fit()\n        fcast3 = fit3.forecast(fcastperiod1).rename(\"Exponential\")\n        fit.append(fit3)\n        fcast.append(fcast3)\n        fname.append('Exponential')\n    except:\n        1==1\n    try:\n        fit4 = Holt(livestock2, damped=True).fit(damping_slope=0.98)\n        fcast4 = fit4.forecast(fcastperiod1).rename(\"AdditiveDamped\")\n        fit.append(fit4)\n        fcast.append(fcast4)\n        fname.append('AdditiveDamped')\n    except:\n        1==1\n    try:\n        fit5 = Holt(livestock2, exponential=True, damped=True).fit()\n        fcast5 = fit5.forecast(fcastperiod1).rename(\"MultiplicativeDamped\")\n        fit.append(fit5)\n        fcast.append(fcast5)\n        fname.append('MultiplicativeDamped')\n    except:\n        1==1\n    try:\n        fit6 = Holt(livestock2, damped=True).fit()\n        fcast6 = fit6.forecast(fcastperiod1).rename(\"AdditiveDampedC\")\n        fit.append(fit6)\n        fcast.append(fcast6)\n        fname.append('AdditiveDampedC')\n    except:\n        1==1\n\n\n    def RMSLE(pred,actual):\n        return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\n    \n    pred_all_result=pd.concat([pd.DataFrame(k.fittedvalues) for k in fit],axis=1)\n    pred_all_result.columns=fname\n    all_result=pd.concat([pd.DataFrame(k) for k in fcast],axis=1)\n    col_chk=[]\n    vvvl=ffcast[type_ck].values.shape[0]\n    for k in all_result.columns:\n        if verbose: print(\"actual value for method %s  is = %s\" % (k,RMSLE(all_result[k].values,ffcast[type_ck].values)))\n        if RMSLE(all_result[k].values[:vvvl],ffcast[type_ck].values) is not np.nan:\n            col_chk.append(k)\n    col_chk_f=[]\n    min_acc=-1\n    for k in col_chk:\n        acc=RMSLE(pred_all_result[k].values,actual[type_ck].values)\n        #if k =='Exponential' and acc>0.01:\n                #acc=acc-0.01\n        if verbose: print(\"pred value for method %s  is = %s\" % (k,acc))\n        if acc is not np.nan:\n            col_chk_f.append(k)\n            if min_acc==-1:\n                min_acc=acc\n                model_select=k\n            elif acc<min_acc:\n                min_acc=acc\n                model_select=k\n    all_result=all_result.append(pred_all_result,sort=False)\n\n    all_result['best_model']=model_select\n    all_result['best_pred']=all_result[model_select]\n    return all_result","17ec3034":"import sys\norig_stdout = sys.stdout\n\nFatalities_all_result_final=pd.DataFrame()\nConfirmedCases_all_result_Final=pd.DataFrame()\nfor keys in train['key'].unique():\n    chk=train[train['key']==keys]\n    chk.index=chk.Date\n    fcastperiod=0\n    fcastperiod1=35\n    actual=chk[:chk.shape[0]-fcastperiod]\n    ffcast=chk[chk.shape[0]-fcastperiod-1:]\n    ffcast\n    try:\n        Fatalities_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'Fatalities').reset_index()\n        \n        \n    except:\n        Fatalities_all_result_1=pd.DataFrame(pd.date_range(start=chk.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        Fatalities_all_result_1.columns=['index']\n        Fatalities_all_result_1['best_model']='naive'\n        Fatalities_all_result_1['best_pred']=0\n        \n    Fatalities_all_result_1['key']=keys\n    Fatalities_all_result_final=Fatalities_all_result_final.append(Fatalities_all_result_1,sort=True)\n    try:\n        ConfirmedCases_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'ConfirmedCases').reset_index()\n\n        \n    except:\n        ConfirmedCases_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        ConfirmedCases_all_result_1.columns=['index']\n        ConfirmedCases_all_result_1['best_model']='naive'\n        ConfirmedCases_all_result_1['best_pred']=1\n    \n    ConfirmedCases_all_result_1['key']=keys\n    ConfirmedCases_all_result_Final=ConfirmedCases_all_result_Final.append(ConfirmedCases_all_result_1,sort=True)\n    print( ' done for %s' % keys)\nsys.stdout = orig_stdout","3b4f21a2":"ConfirmedCases_all_result_Final.rename(columns={'index':'Date'},inplace=True)\nFatalities_all_result_final.rename(columns={'index':'Date'},inplace=True)\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['AdditiveDamped'] is np.nan , ConfirmedCases_all_result_Final['best_pred'] ,\n                                                       ConfirmedCases_all_result_Final['AdditiveDamped'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['AdditiveDamped'] is np.nan , Fatalities_all_result_final['best_pred'] ,\n                                                       Fatalities_all_result_final['AdditiveDamped'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )","9c3e2fc7":"test['Date']=test.Date.astype('datetime64[ns]')","361d281c":"eval1 = ConfirmedCases_all_result_Final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\neval1.rename(columns={'best_pred':'ConfirmedCases'},inplace=True)\neval1['ConfirmedCases']=eval1['ConfirmedCases'].fillna(0)\neval1","478920e3":"eval2 = Fatalities_all_result_final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\n\neval2.rename(columns={'best_pred':'Fatalities'},inplace=True)\neval2['Fatalities']=eval2['Fatalities'].fillna(0)\neval2","a1d7506f":"sub_prep = eval1[['ForecastId','ConfirmedCases','key']].merge(eval2[['ForecastId','Fatalities']], on=['ForecastId'],  how='left')\nsub_prep","3b079fed":"sub = sub_prep.merge(submission['ForecastId'], on=['ForecastId'],  how='right')\nsub","f9eccf0f":"sub=sub[['ForecastId','ConfirmedCases','Fatalities']]\nsub=sub.sort_values('ForecastId')\nsub","32c96492":"sub.to_csv('submission.csv',header=['ForecastId','ConfirmedCases','Fatalities'],index=False)","3062dc72":"train['Date']=train.Date.astype('datetime64[ns]')\nverify=train[['key','Date','ConfirmedCases','Fatalities']].merge(test[['key','Date','ForecastId']], how='inner', on=['key','Date'])\npred=verify[['ForecastId']].merge(sub, how='inner', on=['ForecastId'])","4bc0a451":"RMSLE(pred['Fatalities'].values,verify['Fatalities'].values)","32471d8f":"RMSLE(pred['ConfirmedCases'].values,verify['ConfirmedCases'].values)","8b2e8f9b":"# Actual complete code is at week 1 comp\nhttps:\/\/www.kaggle.com\/yatinece\/exp-model-to-state-of-city-or-country-all-data \n"}}