{"cell_type":{"7a36781e":"code","2e9784ad":"code","6cb2fb36":"code","0951dfd8":"code","156ba81a":"code","6217be24":"code","8ef5bc34":"code","71c0d2ad":"code","3d52fd58":"code","06a74e47":"code","6ea499bf":"code","02b0dd67":"code","688cf754":"code","ffea4791":"code","c9ab54c7":"code","1b839164":"code","361e324c":"code","e8461846":"code","3b74cc0c":"code","20e081b0":"code","a3392444":"code","74776661":"code","c576c8be":"code","9cfef7e0":"code","60a91f5c":"code","3155e693":"code","2513210b":"code","36269e64":"code","624ac75a":"code","180f2d01":"code","7e88cd7f":"code","897106cb":"code","29a732c4":"code","3a2082b7":"code","27a16eb6":"code","6a13db2b":"code","5cf05d7d":"code","596abf6a":"code","c076ddc9":"code","6e8f656a":"code","c494f955":"code","00ebf05b":"code","ccef562c":"code","b1bb498f":"code","5b6ac5c5":"code","044c3eee":"code","b1c92bc4":"code","df9ab7c9":"code","c5e8c266":"code","3b675478":"code","1abd6e34":"code","e339e612":"markdown","373f5fe4":"markdown","05b7210e":"markdown","d9bb660e":"markdown","1112d7e5":"markdown","21e1c546":"markdown","8145de75":"markdown","f19e1e2b":"markdown","c4f1ed42":"markdown","5a15fbe4":"markdown","73f9341f":"markdown","b5ddb831":"markdown","c3450605":"markdown","701b5e15":"markdown","456098d1":"markdown"},"source":{"7a36781e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport cv2\nfrom IPython.display import display\nimport zipfile\n\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler","2e9784ad":"sales_df = pd.read_csv(\"..\/input\/sample-sales-data\/sales_data_sample.csv\", encoding='unicode_escape')\n# MSRP is the manufacturer's suggested retail price (MSRP) or sticker price represents the suggested retail price of products. \n# MSRP is used to standardize the price of products over multiple company store locations.\nsales_df","6cb2fb36":"sales_df.info()","0951dfd8":"# Convert order date to datetime format\nsales_df['ORDERDATE'] = pd.to_datetime(sales_df['ORDERDATE'])\n# Check the type of data of ORDERDATE\nsales_df.dtypes","156ba81a":"# checking for null values\nsales_df.isnull().sum()","6217be24":"# since there are lot of Null values in 'addressline2', 'state', 'postal code' and 'territory' we can drop them. \n# Country & City would represent the order grographical information.\n# Also we can drop city, address1, phone number, contact_name, contact last_name and contact first_name since they are not required for the analysis\n\nto_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']\nsales_df = sales_df.drop(to_drop, axis = 1)\nsales_df.head()","8ef5bc34":"#checking again for null values\nsales_df.isnull().sum().sum()","71c0d2ad":"#number of unique values\nsales_df.nunique()","3d52fd58":"sales_df.COUNTRY.unique()","06a74e47":"sales_df.COUNTRY.value_counts()","6ea499bf":"def barplot_visualization(x):\n    '''\n    Function to visulize the count of items in a given column\n    '''\n    #fig = plt.figure(figsize=(12,6))\n    fig = px.bar(x=sales_df[x].unique(), y=sales_df[x].value_counts(), height=600, color=sales_df[x].unique(),\n                 labels={x:x}\n                )\n    fig.update_layout(yaxis=dict(title_text='Count', titlefont=dict(size=20)), \n                      xaxis=dict(title_text=x, titlefont=dict(size=20)),\n                      title_text=x[0]+ x[1:].lower() +' Bar Plot'\n                     )\n    fig.show()\n\ng = barplot_visualization('COUNTRY')","02b0dd67":"barplot_visualization('STATUS')","688cf754":"barplot_visualization('DEALSIZE')","ffea4791":"barplot_visualization('PRODUCTLINE')","c9ab54c7":"status_dict = {'Shipped':1, 'Cancelled':2, 'On Hold':2, 'Disputed':2, 'In Process':0, 'Resolved':0}\nsales_df['STATUS'].replace(status_dict, inplace=True)","1b839164":"sales_df = pd.get_dummies(data=sales_df, columns=['PRODUCTLINE', 'DEALSIZE', 'COUNTRY'])\nsales_df.shape","361e324c":"sales_df.head()","e8461846":"pd.Categorical(sales_df['PRODUCTCODE'])","3b74cc0c":"pd.Categorical(sales_df['PRODUCTCODE']).codes","20e081b0":"# Since the number unique product code is 109, if we add one-hot variables, there \n# would be additional 109 columns, we can avoid that by using categorical encoding\n# This is not the optimal way of dealing with it but it's important to avoid curse of dimensionality\nsales_df['PRODUCTCODE'] = pd.Categorical(sales_df['PRODUCTCODE']).codes","a3392444":"date_group = sales_df.groupby('ORDERDATE').sum()\ndate_group","74776661":"fig = px.line(x = date_group.index, y = date_group.SALES, title = 'Sales vs Date')\nfig.update_layout(yaxis=dict(title_text='Sales', titlefont=dict(size=15)), \n                  xaxis=dict(title_text='Date', titlefont=dict(size=15))\n                 )\nfig.show()","c576c8be":"# We can drop 'ORDERDATE' and keep the rest of the date-related data such as 'MONTH'\nsales_df.drop(\"ORDERDATE\", axis = 1, inplace = True)\nsales_df.shape","9cfef7e0":"plt.figure(figsize = (20, 20))\ncorr_matrix = sales_df.iloc[:, :10].corr()\nsns.heatmap(corr_matrix, annot=True);","60a91f5c":"# It looks like the Quarter ID and the monthly IDs are highly correlated as they will produce nearly same results\n# Let's drop 'QTR_ID' (or 'MONTH_ID') \nsales_df.drop(\"QTR_ID\", axis = 1, inplace = True)\nsales_df.shape","3155e693":"\n# Distplot shows the (1) histogram, (2) kde plot and (3) rug plot.\n# (1) Histogram: it's a graphical display of data using bars with various heights. Each bar groups numbers into ranges and taller bars show that more data falls in that range.\n# (2) Kde Plot: Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable.\n# (3) Rug plot: plot of data for a single quantitative variable, displayed as marks along an axis (one-dimensional scatter plot). \nimport plotly.figure_factory as ff\n\n#fig = plt.figure(figsize=(10,10));\nfor i in range(8):\n    if sales_df.columns[i]!='ORDERLINENUMBER':\n        fig = ff.create_distplot([sales_df[sales_df.columns[i]].apply(lambda x: float(x))], ['distplot']);\n        fig.update_layout(title_text=sales_df.columns[i]);\n        fig.show();","2513210b":"# Visualize the relationship between variables using pairplots\n\nfig = px.scatter_matrix(sales_df, \n                        dimensions=sales_df.columns[:8], color='MONTH_ID')# fill color by months\nfig.update_layout(title_text='Sales Data',\n                  width=1100,\n                  height=1100\n                 )\nfig.show()","36269e64":"# Scale the data\nscaler = StandardScaler()\nsales_df_scaled = scaler.fit_transform(sales_df)","624ac75a":"wcss = []\nfor i in range(1,15):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(sales_df_scaled)\n    wcss.append(kmeans.inertia_) # intertia is the Sum of squared distances of samples to their closest cluster center (WCSS)\n\nplt.plot(wcss, marker='o', linestyle='--')\nplt.title('The Elbow Method (Finding right number of clusters)')\nplt.xlabel('Number of CLusters')\nplt.ylabel('WCSS')\nplt.show()","180f2d01":"#applying k-means with 5 clusters\nkmeans = KMeans(n_clusters=5, init='k-means++')\nkmeans.fit(sales_df_scaled)\nlabels = kmeans.labels_\nlabels","7e88cd7f":"kmeans.cluster_centers_.shape","897106cb":"cluster_centers = pd.DataFrame(data=kmeans.cluster_centers_, columns=sales_df.columns)\ncluster_centers","29a732c4":"# In order to understand what these numbers mean, let's perform inverse transformation\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data=cluster_centers, columns=sales_df.columns)\ncluster_centers","3a2082b7":"sales_df['ORDERLINENUMBER'] = sales_df['ORDERLINENUMBER'].apply(lambda x: float(x))","27a16eb6":"# Add a label (which cluster) corresponding to each data point\nsales_df_cluster = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)\nsales_df_cluster","6a13db2b":"# plot histogram for each feature based on cluster \nfor i in sales_df.columns[:8]:\n    plt.figure(figsize=(30,6))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        cluster = sales_df_cluster[sales_df_cluster['cluster']==j]\n        cluster[i].hist()\n        plt.title('{} \\ncluster {}'.format(i,j))\nplt.show()","5cf05d7d":"pca = PCA(n_components=3)\nprincipal_comp = pca.fit_transform(sales_df_scaled)\nprincipal_comp","596abf6a":"pca_df = pd.DataFrame(data=principal_comp, columns=['pca1', 'pca2', 'pca3'])\npca_df.head()","c076ddc9":"pca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis=1)\npca_df.head()","6e8f656a":"fig = px.scatter_3d(pca_df, x='pca1', y='pca2', z='pca3', \n                    color='cluster', symbol='cluster', size_max=18, opacity=0.7)\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","c494f955":"sales_df.shape","00ebf05b":"\ninput_df = Input(shape = (38,))\nx = Dense(50, activation = 'relu')(input_df)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nencoded = Dense(8, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\ndecoded = Dense(38, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n# encoder - used for dimensionality reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer = 'adam', loss='mean_squared_error')","ccef562c":"autoencoder.fit(sales_df, sales_df, batch_size=128, epochs=500, verbose=3)","b1bb498f":"encoded_df = autoencoder.predict(sales_df_scaled)","5b6ac5c5":"wcss = []\nfor i in range(1,15):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(encoded_df)\n    wcss.append(kmeans.inertia_) # intertia is the Sum of squared distances of samples to their closest cluster center (WCSS)\n\nplt.plot(wcss, marker='o', linestyle='--')\nplt.title('The Elbow Method (Finding right number of clusters)')\nplt.xlabel('Number of CLusters')\nplt.ylabel('WCSS')\nplt.show()","044c3eee":"# from the above layer 3 clusters seems best choice\nkmeans = KMeans(3)\nkmeans.fit(encoded_df)\nlabels = kmeans.labels_\ny = kmeans.fit_predict(sales_df_scaled)","b1c92bc4":"df_cluster_dr = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)\ndf_cluster_dr.head()","df9ab7c9":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [sales_df.columns])\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [sales_df.columns])\ncluster_centers","c5e8c266":"# plot histogram for each feature based on cluster \nfor i in sales_df.columns[:8]:\n  plt.figure(figsize = (30, 6))\n  for j in range(3):\n    plt.subplot(1, 3, j+1)\n    cluster = df_cluster_dr[df_cluster_dr['cluster'] == j]\n    cluster[i].hist()\n    plt.title('{}    \\nCluster - {} '.format(i,j))\n  \n  plt.show()","3b675478":"# Reduce the original data to 3 dimension using PCA for visualize the clusters\npca = PCA(n_components = 3)\nprin_comp = pca.fit_transform(sales_df_scaled)\npca_df = pd.DataFrame(data = prin_comp, columns = ['pca1', 'pca2', 'pca3'])\npca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","1abd6e34":"# Visualize clusters using 3D-Scatterplot\nfig = px.scatter_3d(pca_df, x = 'pca1', y = 'pca2', z = 'pca3',\n              color='cluster', symbol = 'cluster', size_max = 10, opacity = 0.7)\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","e339e612":"**OBESRVATIONS**\n- There is a high co-relation in Quarter ID and the monthly IDs\n- MSRP is +velly correlated to PRICEEACH and SALES\n- PRODUCTCODE is -velly correlated with MSRP, PRICEEACH and SALES\n- +ve correlation btw SALES, PRICEEACH, QUANTITYORDERED","373f5fe4":"# 2: PERFORM EXPLORATORY DATA ANALYSIS AND DATA CLEANING ","05b7210e":"**FINAL OBESERVATIONS:**\n* Cluster 0 - This group represents customers who buy items in high quantity(47), they usually buy items with high prices(99). They bring-in more sales than other clusters. They are mostly active through out the year. They usually buy products corresponding to product code 10-90. They buy products with high mrsp(158).\n* Cluster 1 - This group represents customers who buy items in average quantity(37) and they buy tend to buy high price items(95). They bring-in average sales(4398) and they are active all around the year.They are the highest buyers of products corresponding to product code 0-10 and 90-100.Also they prefer to buy products with high MSRP(115) .\n* Cluster 2 - This group represents customers who buy items in small quantity(30), they tend to buy low price items(69). They correspond to the lowest total sale(2061) and they are active all around the year.They are the highest buyers of products corresponding to product code 0-20 and 100-110  they then to buy products with low MSRP(77).","d9bb660e":"**OBESRVATIONS**\n* A trend exists between 'SALES' and 'QUANTITYORDERED'  \n*  A trend exists between 'MSRP' and 'PRICEEACH' (there are some outlaiers)  \n* A trend exists between 'PRICEEACH' and 'SALES'\n* It seems that sales growth exists as we move from 2013 to 2014 to 2015 ('SALES' vs. 'YEAR_ID')","1112d7e5":"# 4: APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS","21e1c546":"# 1. IMPORTING LIBRARIES AND DATASET","8145de75":"### Contents\n* 1. IMPORTING LIBRARIES AND DATASET\n* 2. PERFORM EXPLORATORY DATA ANALYSIS AND DATA CLEANING\n* 3. FIND THE OPTIMAL NUMBER OF CLUSTERS USING ELBOW METHOD\n    * Apply k-Means\n* 4. APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS\n* 5. APPLY AUTOENCODERS (PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)\n    * Apply K-Means again after obtaining results from encoders\n    * Final Observations","f19e1e2b":" From this we can observe that, 5th cluster seems to be forming the elbow of the curve. after that we will apply auto encoders to solve this problem","c4f1ed42":"# 3: FIND THE OPTIMAL NUMBER OF CLUSTERS USING ELBOW METHOD","5a15fbe4":"we are good to go now","73f9341f":"we have `ADDRESSLINE2`, `STATE`, `POSTALCODE` and `TERRITORY` these columns have null values","b5ddb831":"**OBERSVATIONS:**\n* CLUSTER 0 (highest) - customer in this group buy item in high quantity, price of each item ~ 99, they also corresponds to highest total sales of ~ 8293. They are the highest buyers of products with high MSRP ~158.\n* CLUSTER 1 - This cluster is nearly close to cluster 4 with MSRP around 94 and average quantity ordered ~34, average piced ~ 83 and sales to 3169\n* CLUSTER 2 (lowest) - This group represents customers who buy items in varying quantity ~30, they tend to low price items ~68. Their sales is ~ 2061, they buy products with lowert MSRP of ~62.\n* CLUSTER 3 - This is the second highest cluster, this group buy in medium quantity ~38, wwith total sales upto ~ 4405 with average price of ~ 95. The MSRP is around 115\n* CLUSTER 4 - This group represents customers who are only active during the holidays. they buy in lower quantity ~35, but they tend to buy average price items around ~87. They also correspond to lower total sales around ~3797, they tend to buy items with MSRP around 116.\n\n**NOTE:** the KMeans result in the final (save version run) might be different with cluster number and values, but the obervations will be simillar","c3450605":"some cluster seems to overlap each other, this issue will be solved by auto encoders","701b5e15":"# 5: APPLY AUTOENCODERS (PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)\n\n* auto encoders are a type of Artificial Neural Netwirk that are used to perform data encoding or representation learning\n* auto encoders use the input and give the same output\n* auto encoders works by adding a bottle neck in network\n* this bottleneck g]forces the network to create a compressed (encoded) version of the original input\n* auto encoders works well if there is correlation between inputs","456098d1":"### Encoding Categorical Variables"}}