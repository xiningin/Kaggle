{"cell_type":{"fccf58a8":"code","58c25888":"code","c9e79d86":"code","5ea296af":"code","17e31a11":"code","f8edad71":"code","a2ac7705":"code","b76c252d":"code","717c45c2":"code","1417ab90":"markdown","09ae57ef":"markdown","f5c1a5e4":"markdown","6f3c478f":"markdown","26abae1e":"markdown","ffe5d81e":"markdown","5948d0a2":"markdown","dfdf4a52":"markdown","3645dbb3":"markdown","a4476a83":"markdown","07869496":"markdown","9b7566a5":"markdown","3318cb2b":"markdown","2256f879":"markdown","c17caf87":"markdown","326dcae2":"markdown","039de179":"markdown","68353022":"markdown"},"source":{"fccf58a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint(\"done!\")","58c25888":"data = pd.DataFrame(np.random.randint(0,2,size=(1000, 6)), columns=['ACC\/mPFC', 'PFC\/OFC', 'Striatum', 'Parietal & Sensory cortices', 'HPC', 'Thalamus'])\ndata.head(20) # see what we made, the first 20 rows","c9e79d86":"labels = pd.DataFrame(np.zeros(shape=(1000, 2)), columns=[\"yes\", \"no\"])\n\n# check all the data\nfor i, row in data.iterrows():\n    # find 0 1 0 * * *\n    if row['PFC\/OFC'] == 0 and row['Parietal & Sensory cortices'] == 1 and row['Thalamus'] == 0:\n        # set the label to 1, 0 (yes)\n        labels[\"yes\"][i] = 1\n        labels[\"no\"][i] = 0\n    # find 1 0 1 * * *\n    elif row['PFC\/OFC'] == 1 and row['Parietal & Sensory cortices'] == 0 and row['Thalamus'] == 1:\n        # set the label to 1, 0 (yes)\n        labels[\"yes\"][i] = 1\n        labels[\"no\"][i] = 0\n    # find 1 1 1 * * *\n    elif row['PFC\/OFC'] == 1 and row['Parietal & Sensory cortices'] == 1 and row['Thalamus'] == 1:\n        # set the label to 1, 0 (yes)\n        labels[\"yes\"][i] = 1\n        labels[\"no\"][i] = 0\n    # find 0 0 0 * * *\n    elif row['PFC\/OFC'] == 0 and row['Parietal & Sensory cortices'] == 0 and row['Thalamus'] == 0:\n        # set the label to 1, 0 (yes)\n        labels[\"yes\"][i] = 1\n        labels[\"no\"][i] = 0\n    else:\n        # anything brain scan not these 4 patterns is not ADHD, set the label to 0, 1 (no)\n        labels[\"yes\"][i] = 0\n        labels[\"no\"][i] = 1\n\n\nlabels.head(20) # see what we made, the first 20 rows","5ea296af":"print(\"times yes:\", labels[\"yes\"].sum(), \"times no:\", labels[\"no\"].sum())","17e31a11":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel = tf.keras.Sequential([\n  layers.Dense(7),\n  layers.Dense(2)\n])\nmodel.compile(loss = tf.losses.MeanSquaredError(),\n                      optimizer = tf.optimizers.Adam())\nprint(\"done!\")","f8edad71":"model.fit(data, labels, epochs=5)","a2ac7705":"output = model(np.array([[1,1,1,1,1,1]]), training=False)\nprint(np.around(output, 2))","b76c252d":"model2 = tf.keras.Sequential([\n  layers.Dense(6),\n  layers.Dense(10, activation='sigmoid'),\n  layers.Dense(2)\n])\nmodel2.compile(loss = tf.losses.MeanSquaredError(),\n                      optimizer = tf.optimizers.Adam())\n\nmodel2.fit(data, labels, epochs=100)","717c45c2":"#output = model(np.array([[1,0,1,0,1,0]]), training=False)\noutput = model2.predict(np.array([[0,0,0,1,1,1]]))\nprint(np.around(output, 2))","1417ab90":"## 6.1) How many layers does this network have?\n\n## 6.2) How many hidden units does this network have?\n\n## 6.3) Try to find the minimum number of hidden units and training epochs that can solve this task.\n\n## 6.4) Try to make a bigger network with 3 hidden layers.","09ae57ef":"# 8) Bonus\n\n## 8.1) Try to make the network learn that 0 1 0 1 0 1 is ADHD and all the rest are not. Write down the general steps you took to make this work.","f5c1a5e4":"Use a Neural Network to predict if someone has ADHD from a brain scan\n===","6f3c478f":"## 1.1) How many brain areas did we measure in our fake dataset?\n\n## 1.2) How many subjects did we generate?","26abae1e":"# 6) But you said deep learning has super human performance?\n\nDeep learning, yes. The single layer perceptron that we made earlier, no.\n\nLets add another (hidden) layer to our network.\n\nNow our network is not a perceptron anymore, it is a proper neural network with multiple layers.\n\nThe hidden layer uses a Sigmoid activation function, so neurons can switch either on or off.\nYou can try removing the Sigmoid activation function, the network will then not be able to solve the task anymore.\nMany layers with switching neurons is what gives the network its computing power.\n","ffe5d81e":"## 3.1) How many layers does this network have?\n\n## 3.2) How many output nodes does this network have?","5948d0a2":"# 1) Lets create some random data\nHere we create 6 brain areas, which we may have scanned somehow.\nWe'll just create some random readings from this hypothetical brainscan for 10 thousand subjects.\n\nWhy do we need so many subjects? Because for AI to work properly you need A LOT OF DATA!! Always remember that.","dfdf4a52":"# 2) Label the data, with some obscure pattern\n\nFor AI you do not just need a lot of data, you need a lot of LABELED data.\n\nHere we create some labels that we just made up out of thin air, well not totally out of thin air, they are based on the brain areas found to be linked to ADHD.\n\nBut the pattern is very weird, almost random. Is higher better? Sometimes. Is lower better? Sometimes. We have no idea what's happening here??!?!\n\nWe'll just let the ai do all difficult work for us. Here we create some obscure pattern of malfunctioning brain areas that cause ADHD.\nNormally you would not know this, you would only have the brain readings, and if somebody has ADHD or not.","3645dbb3":"# 4) Train the network on our data","a4476a83":"## 4.1) What do you think 'loss' means?\n\n## 4.2) The network now trains for 5 epochs, try to train the network for more epochs to see if it can solve the task","07869496":"# Lets see how many of our test subjects got the label ADHD\n\nFor ai it is best if a dataset is balanced. Meaning all categories have about the same number of subjects.\n\nThe ai is very keen on cutting corners, if only a few (maybe 1%) people answer \"yes\" to ADHD, the ai will just always answer \"no\" and get a 99% correct score.\nThe computer would think it is very accurate, but it will not help us much.\n\nSo always make sure you have about an equal number of subjecs in the \"yes\" and in the \"no\" group, or in any number of groups you happen to have.","9b7566a5":"# 5) Test if the network works\n\nTry to create some subjects that should and should not have ADHD, and see if the network got the score correct.\n\nHint: It does not! You can see that the 'loss' never reaches zero. This network cannot solve our riddle.","3318cb2b":"# 7) Now try again if the network can predict if somebody has ADHD","2256f879":"## 2.1) What are all the patterns that cause ADHD in our fake dataset?","c17caf87":"From: [https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5744617\/](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5744617\/)","326dcae2":"## 5.1) What output would indicate someone has ADHD?\n\n## 5.2) What is an input where the network gives the wrong diagnosis?","039de179":"ADHD is a complex disorder, with many contributing factors. In this machine learning tutorial we will try to make a neural network predict if somebody has ADHD based on fake brainscan data.\n\n![neuromodulators](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5744617\/bin\/fncir-11-00108-g0001.jpg)","68353022":"# 3) Now lets create the network\n\nTensorFlow, made by Google, makes it very easy to create huge complex neural network with insane performance.\n\nWhile in the past you would have to build the whole network yourself, now you can just ask TensorFlow for a network of a specific size ready to go.\n\nThe past, would here mean only 5 years ago, TensorFlow was created in 2015.\n\nDeep learning, which is huge neural networks with many layers and often super-human performance, is a very new art, only in 2012 was the first neural network made that could beat hand-made algorithms at computer vision."}}