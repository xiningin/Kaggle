{"cell_type":{"48ab30e5":"code","11c7aea4":"code","8dfef92f":"code","4614430a":"code","def1a6a2":"code","1e8087f9":"code","a9491d0a":"markdown","abdc8248":"markdown","dc093360":"markdown","877e1b2a":"markdown","f89309d9":"markdown"},"source":{"48ab30e5":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)","11c7aea4":"PATH = '..\/input\/google-quest-challenge\/'\nBERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","8dfef92f":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","4614430a":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback\n","def1a6a2":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","1e8087f9":"\nhistories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold in [0, 1, 2, 3, 4]:\n        K.clear_session()\n        model = bert_model()\n\n        train_inputs = [inputs[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n\n        # history contains two lists of valid and test preds respectively:\n        #  [valid_predictions_{fold}, test_predictions_{fold}]\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs, \n                          learning_rate=1e-5, epochs=4, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n\n        histories.append(history)\n        model.save_weights(f'bert_lr_1e5{fold}.h5')\n","a9491d0a":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 5 epochs --- with a learning rate of 1e-5 and batch_size of 8. A simple binary crossentropy is used as the objective-\/loss-function. ","abdc8248":"#### 4. Obtain inputs and targets, as well as the indices of the train\/validation splits","dc093360":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","877e1b2a":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`CustomCallback()` is a class which inherits from `tf.keras.callbacks.Callback` and will compute and append validation score and validation\/test predictions respectively, after each epoch.\n<br><br>\n`bert_model()` contains the actual architecture that will be used to finetune BERT to our dataset. It's simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)\n<br><br>\n`train_and_predict()` this function will be run to train and obtain predictions","f89309d9":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs."}}