{"cell_type":{"7f64e13f":"code","eca4f2b6":"code","d3a089a3":"code","f146910e":"code","2d9066d2":"code","3d5711a7":"code","c9ae3410":"code","939712fc":"code","cfb3bfc9":"code","28eac950":"code","e2c469d9":"code","21243525":"code","60088c72":"code","8feadc36":"code","8d7478b1":"code","7de35c7b":"code","46742336":"code","2db0ba96":"code","867632c9":"code","4ff91bcc":"code","7be09cb4":"code","5c90d0ec":"code","9770dd34":"code","f6449ce5":"code","979ae466":"code","587e4662":"code","3960bdd0":"code","77483bc2":"code","c9206ece":"code","00c8466a":"code","79da8f3e":"code","6e46c25f":"code","68f36085":"code","c4b076ee":"code","0c3eca45":"code","d7626bae":"code","b0e78dc3":"code","94a6aed1":"code","63c1cfec":"code","e5f0162b":"code","e6f82548":"code","a30f01cf":"code","6a5b706c":"code","973c7de2":"code","da160ed0":"code","e2a83d23":"code","84e296c2":"markdown","7f21b2dc":"markdown","f4167ad8":"markdown","e6093d27":"markdown","113b68e9":"markdown","2fe52bce":"markdown","9840fe5e":"markdown","60a9bb2d":"markdown","4017b9a1":"markdown","c66dee9d":"markdown","5e8245f9":"markdown","945a07f0":"markdown","5ec03ea8":"markdown","ce2be66a":"markdown","e02cd58b":"markdown","e107f1fe":"markdown","74bfc710":"markdown","ed297d81":"markdown","ba0574e0":"markdown","3ef660d5":"markdown","36fe9c00":"markdown","b7e07704":"markdown","8de97961":"markdown"},"source":{"7f64e13f":"#------------------------------------------Libraries---------------------------------------------------------------#\n####################################################################################################################\n#-------------------------------------Boiler Plate Imports---------------------------------------------------------#\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#---------------------------------------Text Processing------------------------------------------------------------#\nimport regex\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\n#------------------------------------Metrics and Validation---------------------------------------------------------#\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n#-------------------------------------Models to be trained----------------------------------------------------------#\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost\n#####################################################################################################################","eca4f2b6":"train = pd.read_csv('\/kaggle\/input\/ireland-historical-news\/ireland-news-headlines.csv')\ntest = pd.read_csv('\/kaggle\/input\/ireland-historical-news\/w3-latnigrin-tokens.csv')","d3a089a3":"train.head()","f146910e":"train.info()","2d9066d2":"train.isna().sum()","3d5711a7":"train.headline_category.value_counts()","c9ae3410":"year = [] \nmonth = [] \nday = [] \n\ndates = train.publish_date.values\n\nfor date in dates:\n    str_date = list(str(date))\n    year.append(int(\"\".join(str_date[0:4]))) \n    month.append(int(\"\".join(str_date[4:6])))\n    day.append(int(\"\".join(str_date[6:8])))","939712fc":"train['year'] = year\ntrain['month'] = month\ntrain['day'] = day\n\ntrain.drop(['publish_date'] , axis=1,inplace=True) \ntrain = train[train['headline_category'] != 'removed']\ntrain.head()","cfb3bfc9":"print('Unique Headlines Categories: {}'.format(len(train.headline_category.unique())))","28eac950":"set([category for category in train.headline_category if \".\" not in category] ) ","e2c469d9":"train['headline_category'] = train.headline_category.apply(lambda x: x.split(\".\")[0]) \ntrain = train.loc[train.headline_category != 'removed']","21243525":"wordnet_lemmatizer = WordNetLemmatizer()\n\nstop = stopwords.words('english')\n\nfor punct in punctuation:\n    stop.append(punct)\n\ndef filter_text(text, stop_words):\n    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha()]\n    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n    return \" \".join(filtered_text)","60088c72":"train[\"filtered_text\"] = train.headline_text.apply(lambda x : filter_text(x, stop)) \ntrain.head()","8feadc36":"plt.figure(figsize=(10,5))\nax = sns.countplot(train.headline_category, palette = sns.color_palette(\"mako\"))","8d7478b1":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.year.value_counts().index.values,y=train.year.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Year')","7de35c7b":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.month.value_counts().index.values,y=train.month.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Month')","46742336":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.day.value_counts().index.values,y=train.day.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Day')","2db0ba96":"def make_wordcloud(words,title):\n    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n    plt.figure(figsize=(20,20))\n    plt.imshow(cloud, interpolation=\"gaussian\")\n    plt.axis(\"off\") \n    plt.title(title, fontsize=60)\n    plt.show()","867632c9":"all_text = \" \".join(train[train.headline_category == \"news\"].filtered_text) \nmake_wordcloud(all_text, \"News\")","4ff91bcc":"all_text = \" \".join(train[train.headline_category == \"culture\"].filtered_text) \nmake_wordcloud(all_text, \"Culture\")","7be09cb4":"all_text = \" \".join(train[train.headline_category == \"opinion\"].filtered_text) \nmake_wordcloud(all_text, \"Opinion\")","5c90d0ec":"all_text = \" \".join(train[train.headline_category == \"business\"].filtered_text) \nmake_wordcloud(all_text, \"Business\")","9770dd34":"all_text = \" \".join(train[train.headline_category == \"sport\"].filtered_text) \nmake_wordcloud(all_text, \"Sport\")","f6449ce5":"all_text = \" \".join(train[train.headline_category == \"lifestyle\"].filtered_text) \nmake_wordcloud(all_text, \"Lifestyle\")","979ae466":"tfidf = TfidfVectorizer(lowercase=False)\ntrain_vec = tfidf.fit_transform(train['filtered_text'])","587e4662":"train_vec.shape","3960bdd0":"train['classification'] = train['headline_category'].replace(['news','culture','opinion','business','sport','lifestyle'],[0,1,2,3,4,5])","77483bc2":"x_train, x_val, y_train, y_val = train_test_split(train_vec,train['classification'], stratify=train['classification'], test_size=0.2)","c9206ece":"#C = np.arange(0, 1, 0.001)\n#l1_ratio = np.ratio(0, 1, 0.01)\n#max_iter = range(100, 500)\n#warm_start = [True, False]\n#solver = ['lbfgs', 'newton-cg']\n#penalty = ['l2', 'l1']\n\n#params = {\n#    'C' : C,\n#    'l1_ratio' : l1_ratio,\n#    'max_iter' : max_iter,\n#    'warm_start' : warm_start,\n#    'solver' : solver,\n#    'penalty' : penalty\n#}\n#\n#random_search = RandomizedSearchCV(\n#    estimator = LogisticRegression(),\n#    param_distributions = params,\n#    n_iter = 100,\n#    cv = 3,\n#    n_jobs = -1,\n#    random_state = 1\n#).fit(x_train, y_train)\n#\n#random_search.best_params_","00c8466a":"model_lr = LogisticRegression(\n    C=0.98, \n    l1_ratio=0.23, \n    max_iter=430, \n    random_state=1,\n    warm_start=True\n).fit(x_train, y_train)\n\nmodel_lr.score(x_train, y_train)","79da8f3e":"predicted = model_lr.predict(x_val)\n\nlr_acc = accuracy_score(y_val,predicted)\nlr_cop = cohen_kappa_score(y_val,predicted)\nlr = pd.DataFrame([lr_acc, lr_cop], columns = ['Logistic Regression with RandomizedSearchCV'])\n\nprint(\"Test score: {:.2f}\".format(lr_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(lr_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","6e46c25f":"#alpha = np.arange(0, 1, 0.001)\n#fit_prior = [True, False]\n\n#params = {\n#    'alpha' : alpha,\n#    'fit_prior' : fit_prior\n#}\n#\n#random_search = RandomizedSearchCV(\n#    estimator = MultinomialNB(),\n#    param_distributions = params,\n#    n_iter = 100,\n#    cv = 3,\n#    n_jobs = -1,\n#    random_state = 1\n#).fit(x_train, y_train)\n#\n#random_search.best_params_","68f36085":"model_mnb = MultinomialNB(alpha=1.9000000000000001, fit_prior=False).fit(x_train, y_train)\n\nmodel_mnb.score(x_train, y_train)","c4b076ee":"predicted = model_mnb.predict(x_val)\n\nmnb_acc = accuracy_score(y_val,predicted)\nmnb_cop = cohen_kappa_score(y_val,predicted)\nmnb = pd.DataFrame([mnb_acc, mnb_cop], columns = ['MultinomialNB with RandomizedSearchCV'])\n\nprint(\"Test score: {:.2f}\".format(mnb_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(mnb_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","0c3eca45":"model_sgd_hinge = SGDClassifier(\n    loss='squared_hinge',\n    penalty='l2',\n    alpha=0.0001,\n    l1_ratio=0.15,\n    fit_intercept=True,\n    max_iter=1000,\n    tol=0.001,\n    shuffle=True,\n    verbose=0,\n    epsilon=0.1,\n    n_jobs=-1,\n    random_state=1,\n    learning_rate='optimal',\n    eta0=0.0,\n    power_t=0.5,\n    early_stopping=False,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    class_weight=None,\n    warm_start=False,\n    average=False).fit(x_train, y_train)\n\nmodel_sgd_hinge.score(x_train, y_train)","d7626bae":"predicted = model_sgd_hinge.predict(x_val)\n\nsgd_hinge_acc = accuracy_score(y_val,predicted)\nsgd_hinge_cop = cohen_kappa_score(y_val,predicted)\nsgd_hinge = pd.DataFrame([sgd_hinge_acc, sgd_hinge_cop], columns = ['SGDClassifier with Squared Hinge Loss'])\n\nprint(\"Test score: {:.2f}\".format(sgd_hinge_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(sgd_hinge_cop))\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","b0e78dc3":"estimators = [\n    ('svm', model_sgd_hinge),\n    ('mnb', model_mnb),\n    ('lr', model_lr)\n]\n\nestimators","94a6aed1":"model_voting = VotingClassifier(\n    estimators = estimators,\n    voting='hard', \n    n_jobs=-1,\n    flatten_transform=True, \n    verbose=1).fit(x_train, y_train)\n\nmodel_voting.score(x_train, y_train)","63c1cfec":"predicted = model_voting.predict(x_val)\n\nvoting_acc = accuracy_score(y_val,predicted)\nvoting_cop = cohen_kappa_score(y_val,predicted)\nvoting = pd.DataFrame([voting_acc, voting_cop], columns = ['Hard Voting Classifier'])\n\nprint(\"Test score: {:.2f}\".format(voting_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(voting_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","e5f0162b":"xgc = xgboost.XGBClassifier()\n\nmodel_stack = StackingClassifier(\n    estimators=estimators,\n    final_estimator=xgc,\n    n_jobs = -1,\n    verbose = 1\n)\n\nmodel_stack.fit(x_train, y_train)\n\nmodel_stack.score(x_train, y_train)","e6f82548":"predicted = model_stack.predict(x_val)\n\nstack_acc = accuracy_score(y_val,predicted)\nstack_cop = cohen_kappa_score(y_val,predicted)\nstack = pd.DataFrame([stack_acc, stack_cop], columns = ['Stacking Classifier'])\n\nprint(\"Test score: {:.2f}\".format(stack_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(stack_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","a30f01cf":"model_comp = pd.concat([lr, mnb, sgd_hinge, voting, stack], axis = 1)\nmodel_comp","6a5b706c":"test_vec = tfidf.transform(test.iloc[:, -1])\ntest_vec.shape","973c7de2":"cat = ['news','culture','opinion','business','sport','lifestyle']\ncode = [0,1,2,3,4,5]\ndic = dict([(code[x], cat[x])for x in range(6)])\ndic","da160ed0":"pred = model_stack.predict(test_vec)\n\npredictions = []\nfor i in pred:\n    predictions.append(dic[i])\ntest['Category'] = predictions\n\ntest.head()","e2a83d23":"plt.figure(figsize=(10,5))\nax = sns.countplot(test.Category, palette = sns.color_palette(\"mako\"))","84e296c2":"## Exploring Training Data","7f21b2dc":"### Stacking Classifier","f4167ad8":"## Inferences and Future Works\n\n* **Though Naive Bayes based models are considered to be the go-to models for Text Classification, they couldn't perform well as the linear model because of the size of the dataset. Same goes for SVM, these models are suitable for small to medium sized datasets.**\n\n* **Tree Based Models such as Decision Trees and other ensemble methods such as GradientBoost and AdaBoost could also be used but require much more computational power and proper hyperparameter tuning.**\n\n* **For a huge dataset like this, traditional ML models can prove short. Hence artificial neural network architectures using LSTMs and also CNNs can be proved as a step up. They would also require an extra computational capacity with GPUs.**","e6093d27":"* We could clearly make out that for every category there are multiple sub-categories which only increases the complexity of our predictions and increases variance as the sub-categories are less in number. \n\n* One way to tackle this, is to merge the sub-categories into their respective 'super class'. This wouldn't effect the predictions much as the sub-categories are clearly out-numbered.","113b68e9":"### Multinomial Naive Bayes with RandomizedSearchCV\nCross-Validation takes typically 10min (due to the size of dataset) on an 8 core CPU for Multinomial Naive Bayes.<br>\nThe parameters used were obtained by already performing Cross Validation.","2fe52bce":"## Vectorizing Cleaned Text For Modelling","9840fe5e":"### Importing libraries needed","60a9bb2d":"## Predict the Test Data","4017b9a1":"### SGDClassifier with Squared Hinge Loss\n* Support Vector Classifiers can be implemented by using the SGDClassifier with **'hinge'** as the loss function.<br><br>\n* Since the **'squared hinge loss'** removes the possibility of negative class in the range, makes it the optimal choice for the loss function.<br><br>\n* Cross Validation **failed** to yeild better results than the base model defined, hence skipped.","c66dee9d":"## Loading Data","5e8245f9":"## Modelling\n\n### Base Models\n\n* Logistic Regression with RandomizedSearchCV.\n* Multinomial Naive Bayes with RandomizedSearchCV.\n* SDGClassifier with Squared Hinge Loss -> SVM.\n\n### Ensemble Methods\n\n* Hard Voting Classifier.\n* Stacking Classifier with XGBClassifier as the \"final estimator\" or \"meta learner\".","945a07f0":"## Clean the textual data\n\n* Normalize the data by converting to lower case.\n* Remove punctuations, stopwords and special characters.\n* Tokenize the text, i.e segment the text into sentences and further into words.\n* Extract the root word by 'Lemmatizing' each word. Ex: Lemmatize('swimming') = 'swim'","5ec03ea8":"## Peek Training Data","ce2be66a":"# Headline Classification using Stacking Ensemble method","e02cd58b":"### Basic Visualization","e107f1fe":"## Model Comparision","74bfc710":"## Credits : \n\n* [EDA and Data Preparation](https:\/\/www.kaggle.com\/darkrubiks\/classifying-irish-times-headlines-categories)\n* [SVC implemented with SGDClassifier](https:\/\/www.kaggle.com\/flubber\/text-classification-irish-times-news-dataset)","ed297d81":"## Ensemble Methods","ba0574e0":"### Hard Voting Classifier","3ef660d5":"## Base Models\n\n### Logistic Regression with RandomizedSearchCV\n\nCross-Validation takes a lot of time, typically 1hr 30min (due to the size of dataset) on an 8 core CPU for Logistic Regression.<br>\nThe parameters used were obtained by already performing Cross Validation.","36fe9c00":"### Splitting into training and validation sets","b7e07704":"**Helper Function For Data Preparation**","8de97961":"### Word Clouds\n\n**Helper Function for word clouds**"}}