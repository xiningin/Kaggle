{"cell_type":{"7ca1fc0a":"code","cdd1550d":"code","36f69162":"code","8f89ca7b":"code","b90a519a":"code","0ad1f36a":"code","b950b0bf":"code","82c3a156":"code","5423af6c":"code","79eeb7fc":"code","f547c79f":"code","85bf1cba":"code","f6c284df":"code","09a909b4":"code","1f1af2b6":"code","97374636":"code","2289ce68":"code","fefb67fc":"code","b6a92cb3":"code","6d0bd636":"code","d836b1b8":"markdown","d38e80a0":"markdown","2b8f9ebe":"markdown","92a2ce5e":"markdown","824c6fc5":"markdown","548a82d1":"markdown","f94878e0":"markdown","c87e20b5":"markdown","c68e9613":"markdown","60fa6a89":"markdown","d29b7e7e":"markdown","01305f02":"markdown","9e4dee8d":"markdown","16d5e1bd":"markdown"},"source":{"7ca1fc0a":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n","cdd1550d":"train=pd.read_csv('..\/input\/learn-together\/train.csv')\ntest=pd.read_csv('..\/input\/learn-together\/test.csv')\nsample_submission=pd.read_csv('..\/input\/learn-together\/sample_submission.csv')","36f69162":"colormap = plt.cm.RdBu\nplt.figure(figsize=(30,30))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","8f89ca7b":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def featureimportances(self,x,y):\n        return(self.clf.fit(x,y).featureimportances)","b90a519a":"def get_oof(clf, x_train, y_train, x_test,train):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","0ad1f36a":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}","b950b0bf":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)","82c3a156":"# # Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Cover_Type'].ravel()\ntrain = train.drop(['Cover_Type'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","5423af6c":"# # Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test,train) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test,train) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test,train) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test,train) # Gradient Boost\n\nprint(\"Training is complete\")","79eeb7fc":"rffeatures = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","f547c79f":"rf_features=[0.0653386317, 0.268108074, 1.42839657e-02, 7.19156381e-03,\n 4.13514491e-02, 2.41487372e-02, 6.32447442e-02, 2.51975425e-02,\n 7.10712493e-03 ,1.29448946e-02, 2.84739582e-02, 1.51107746e-02,\n 4.83119888e-03 ,3.39534421e-02, 1.13941100e-01, 1.95066524e-04,\n 4.00883743e-03 ,4.32800819e-02, 1.76773018e-02, 3.25226534e-04,\n 1.49393911e-03 ,0.00000000e+00, 0.00000000e+00, 5.88897147e-06,\n 5.66192773e-02 ,1.26682285e-03, 7.09571857e-04, 1.62340618e-02,\n 3.83372651e-04 ,0.00000000e+00 ,1.28555088e-04, 5.86794893e-03,\n 8.11840877e-05 ,4.31002213e-06, 1.28590880e-04, 2.02550994e-05,\n 6.26636751e-03 ,5.47858778e-03 ,8.78188005e-04, 0.00000000e+00,\n 2.31388696e-05 ,1.33793100e-05 ,0.00000000e+00, 3.06472875e-03,\n 1.67556242e-03 ,5.34528513e-04 ,3.71206392e-03, 6.74646390e-04,\n 1.18542434e-05 ,1.18870469e-03 ,3.03438386e-06, 2.56667624e-04,\n 4.42542736e-02 ,3.66349294e-02 ,2.16718502e-02]\n\net_features=[2.00323300e-02, 1.57974391e-01, 1.04367874e-02, 8.01935244e-03,\n 1.54490349e-02, 8.94297793e-03, 3.69506432e-02, 1.65402604e-02,\n 7.05963184e-03, 1.16087339e-02, 1.63213070e-02, 5.41784033e-02,\n 7.22718969e-03, 4.36806516e-02, 1.70251320e-01, 1.24754074e-03,\n 8.38209284e-03, 5.98490030e-02, 2.49438533e-02, 1.19938161e-03,\n 4.40891025e-03, 0.00000000e+00, 0.00000000e+00, 6.62546072e-05,\n 7.16434246e-02, 1.54439192e-03, 1.00454046e-02, 9.62085947e-03,\n 2.13425345e-03, 0.00000000e+00, 3.22089792e-04 ,1.49918988e-02,\n 1.14671967e-03, 7.53632614e-05 ,4.61637387e-04, 6.34530298e-06,\n 1.05107416e-02, 6.16968222e-03 ,1.43969050e-03, 0.00000000e+00,\n 7.92748272e-05, 1.09216273e-05 ,1.11310231e-05, 1.20959440e-02,\n 2.47280244e-02, 1.03838327e-03 ,4.89115919e-03, 1.75953346e-03,\n 1.82531122e-05, 2.33314207e-03, 9.98047251e-06, 7.45852092e-04,\n 5.56504812e-02, 5.04751490e-02, 3.12702164e-02]\n\nada_features=[0. ,   0.334, 0. ,    0. ,    0.328, 0.004 ,0. ,   0. ,   0. ,   0. ,   0. ,   0.002,\n 0.  ,  0.  ,  0.332, 0.   , 0. ,   0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0. ,   0. ,\n 0.  ,  0.  ,  0.  ,  0.  ,  0. ,   0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0. ,\n 0.  ,  0.  ,  0.  ,  0.  ,  0. ,   0.  ,  0.  ,  0.  ,  0.  ,  0. ,   0.  ,  0. ,\n 0.  ,  0.  ,  0.  ,  0.  ,  0. ,   0.  ,  0.   ]\n\ngb_features=[9.23807651e-02, 4.53059872e-01 ,1.82701273e-02, 7.15936708e-03,\n 5.05055314e-02, 2.21329794e-02, 5.68438923e-02, 4.63737905e-02,\n 1.72796716e-02 ,1.45278433e-02, 5.35590597e-02, 3.81380933e-03,\n 1.24769898e-03 ,8.56185829e-03, 2.13915431e-03, 8.41919839e-04,\n 5.54011455e-03 ,1.14838423e-02, 1.22423982e-02, 1.98441022e-03,\n 2.45840472e-03 ,0.00000000e+00, 0.00000000e+00, 1.79202952e-05,\n 3.52959393e-02 ,2.16911650e-03, 4.42834495e-03, 1.41830077e-02,\n 1.17709301e-04 ,0.00000000e+00, 4.82907945e-04, 3.92275992e-03,\n 4.94074158e-05 ,2.09058102e-04, 1.21503285e-03, 1.30829819e-04,\n 3.78783475e-03 ,3.28079497e-03, 1.26110953e-03, 0.00000000e+00,\n 3.19075903e-04 ,2.61379053e-04, 2.08531237e-04, 1.95439744e-03,\n 2.03778112e-02 ,1.75494613e-03, 7.67160302e-03, 3.38316878e-03,\n 3.73420741e-04 ,7.60061654e-04, 2.02655172e-04, 6.42735625e-07,\n 2.46647368e-03, 6.51226716e-03, 7.95281876e-04]","85bf1cba":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","f6c284df":"feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","09a909b4":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='basic-scatter')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='basic-scatter')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='basic-scatter')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(data, filename='basic-scatter')","1f1af2b6":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(data, filename='grouped-bar-direct-labels')","97374636":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","2289ce68":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='pandas-heatmap')","fefb67fc":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)","b6a92cb3":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","6d0bd636":"output = pd.DataFrame({'Id': test.Id,\n                      'Cover_Type': predictions})\noutput.to_csv('sample_submission.csv', index=False)","d836b1b8":"**Take away from plots::**\n\nit shows that thier are not many features which are strongly correlated so it is good to feed in our model because there isn't much redundant or superfluous data in our training set and we are happy that each feature but soil type7 and soil type15 has nothing to do with other features so we will remove it from train and test set","d38e80a0":"## Generating our Base First-Level Models\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\nRandom Forest classifier\nExtra Trees classifier\nAdaBoost classifer\nGradient Boosting classifer\nSupport Vector Machine\n\n\nParameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\nn_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n\nn_estimators : Number of classification trees in your learning model ( set to 10 per default)\n\nmax_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n\nverbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.","2b8f9ebe":"**i have tried various methods to convert features into list like tolist() function but nothing has worked me because it is of None type so i have manually pasted the result from above with commas**","92a2ce5e":"Plotly Barplot of Average Feature Importances\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows","824c6fc5":"#### Second level learning model via XGBoost\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:","548a82d1":"### Interactive feature importances via Plotly scatterplots\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers via a plotly scatter plot by calling \"Scatter\" as follows:","f94878e0":"<html><font size=3 color='red'>If you find this kernel interesting, please drop an upvote. It motivates me to produce more quality content :)<\/font><\/html>","c87e20b5":"Creating NumPy arrays out of our train and test sets\n\nGreat. Having prepared our first layer base models as such, we can now ready the training and test test data for input into our classifiers by generating NumPy arrays out of their original dataframes as follows:","c68e9613":"# Second-Level Predictions from the First-level Output\n\n#### First-level output as new features\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.","60fa6a89":"# Correlation heatMap","d29b7e7e":"## Feature importances generated from the different classifiers\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .featureimportances. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such","01305f02":"# Ensembling & Stacking models\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n","9e4dee8d":"Output of the First level Predictions\n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.","16d5e1bd":"### Correlation Heatmap of the Second Level Training set"}}