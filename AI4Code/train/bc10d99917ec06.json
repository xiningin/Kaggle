{"cell_type":{"03c6046e":"code","88fc5a18":"code","5ee01351":"code","abefb124":"code","48686346":"code","1d1bf88b":"code","7081a571":"code","80009212":"code","137c0421":"code","da394374":"code","d80741a4":"code","b16fa6f0":"code","8b6af096":"code","34696c5b":"code","ef24c395":"code","d8c729f3":"code","11ff5c46":"code","0fdbed84":"code","b2fecad1":"code","688f9b0e":"code","8009b4bc":"code","e6cb23b3":"code","6c988b7b":"code","efa5d3cb":"markdown","e28d4770":"markdown","7f4438d8":"markdown","2c3620ac":"markdown","d8394f88":"markdown","e03c7c4a":"markdown","a4659d0a":"markdown","9e14c7c4":"markdown","50c154e5":"markdown","7e607041":"markdown","38c9fd5c":"markdown","109e6f41":"markdown","70923b11":"markdown","59042b66":"markdown","98f6826f":"markdown","a37d737c":"markdown","b821f765":"markdown"},"source":{"03c6046e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88fc5a18":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")\n\ntrain_df.head()","5ee01351":"train_df.describe()","abefb124":"def show_training_example(df, train=True):\n    index = np.random.randint(len(df))\n    example = df.iloc[index]\n    if train:\n        label = example[0]\n        img = example[1:].values.reshape((28, 28))\n    else:\n        img = example.values.reshape((28, 28))\n        label = \"Unknown\"\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(label)\n    plt.show()","48686346":"show_training_example(train_df)","1d1bf88b":"# Now we are going to get images of only zeros and ones to do binar classification\nbinary_df = train_df.loc[train_df['label'] < 2]\nshow_training_example(binary_df)\nprint(len(binary_df))","7081a571":"def sigmoid(z):\n    return 1. \/ (1. + np.exp(-z))\n\ndef hypothesis(X, W, b):\n    return sigmoid(np.dot(X, W) + b)","80009212":"def cost(h, y):\n    return (-y * np.log(h + 0.001) - (1 - y) * np.log(1 - h + 0.001)).mean()\n\n## the small number in the log is to prevent log(0) which would overflow","137c0421":"# now let's start training\nfrom sklearn.model_selection import train_test_split\nX = binary_df.drop(columns=['label']).values \/ 255.\ny = binary_df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","da394374":"W = np.random.rand(X_train.shape[1])\nb = np.random.rand(1)\nalpha = 0.1\nepochs = 200\ncosts = []\nfor i in range(epochs):\n    h = hypothesis(X_train, W, b)\n    costs.append(cost(h, y_train))\n    W -= alpha * np.dot(X_train.T, (h - y_train))\n    b -= alpha * (h - y_train).mean()\n    if i % 10 == 0:\n        print(costs[-1])\n    ","d80741a4":"plt.plot(list(range(epochs-2)), costs[2:])\nplt.show()","b16fa6f0":"from sklearn.metrics import accuracy_score\nh_test = hypothesis(X_test, W, b)\nfor i in range(h_test.size):\n    h_test[i] = int(h_test[i] >= 0.5)","8b6af096":"accuracy_score(y_test, h_test)","34696c5b":"model = LogisticRegression(penalty=\"none\")\n\nmodel.fit(X_train, y_train)\n\naccuracy_score(y_test, model.predict(X_test))","ef24c395":"X = train_df.drop(columns=[\"label\"]).values \/ 255.\ny = train_df[\"label\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","d8c729f3":"# let's try crossentropy\nmodel = LogisticRegression(penalty=\"none\")\nmodel.fit(X_train, y_train)\n\naccuracy_score(y_test, model.predict(X_test))","11ff5c46":"accuracy_score(y_train, model.predict(X_train))","0fdbed84":"model = LogisticRegression(penalty='l2', C=0.01)\nmodel.fit(X_train, y_train)\n\ntrain_score = accuracy_score(y_train, model.predict(X_train))\ntest_score = accuracy_score(y_test, model.predict(X_test))\n\nprint(\"Training Accuracy: {}, Testing Accuracy: {}\".format(train_score, test_score))","b2fecad1":"X_train = train_df.drop(columns=[\"label\"]).values \/ 255.\ny_train = train_df[\"label\"]\nX_test = test_df.values \/ 255.\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\ny_test = model.predict(X_test)\nsubmission[\"Label\"] = y_test\n\nsubmission.head()","688f9b0e":"submission.to_csv(\"result.csv\", index=False)","8009b4bc":"X = train_df.drop(columns=[\"label\"]).values \/ 255.\ny = train_df[\"label\"]\nX_test = test_df.values \/ 255.\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2)\n\nmodel = LinearSVC()\n\nmodel.fit(X_train, y_train)\npreds_validate = model.predict(X_validate)\n\nprint(accuracy_score(preds_validate, y_validate))\n","e6cb23b3":"X = train_df.drop(columns=[\"label\"]).values \/ 255.\ny = train_df[\"label\"]\nX_test = test_df.values \/ 255.\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2)\n\nmodel = SVC(kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\npreds_validate = model.predict(X_validate)\n\nprint(accuracy_score(preds_validate, y_validate))\n","6c988b7b":"# later we will descuss the kernel trick\n# now let's submit\npreds_test = model.predict(X_test)\nsubmission[\"Label\"] = preds_test\n\nsubmission.to_csv(\"results2.csv\", index=False)","efa5d3cb":"### Let's try SVMs and see if we get better results\n> **Note** SVMs are going to be explained in details in another notebook","e28d4770":"## Let's make a submission","7f4438d8":"## Logistic Regression for binary classification\n\n> **Note** some basic linear algebra ahead\n\nLet's assume the input features make a vector $X$ where the dimension of $X$ is $(n, 1)$ where $n$ is the number of features.  \nRight now we have only one output which takes values 0 or 1 let's call it the hypothesis $h$ of shape $(c, 1)$ where $c$ the number of classes (one in this case). we need some tranformation matrix that takes us from $X$ to $O$, we will call this matrix the weights matrix $W$ where $W$ is of shape $(n, c)$.  \nOur hypothesis $h = X \\odot W$ however there are two problems with this hypothesis:\n- The separating line between the two classes always (aka decision boundary) passes through the origin\n- It outputs values from $-\\infty$ to $\\infty$ and we need numbers between 0 and 1\n\nto solve the first problem we add the bias vector $b$ of shape $(1, 1)$ per example  \nTo solve the second we use the logistic sigmoid function which takes the real number line and squeezes it between 0 and 1.  \n$$ h = \\sigma(X \\odot W + b) $$\nWhere\n$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n\n\n![With and without Bias term](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/01\/Screen-Shot-2017-06-29-at-5.34.44-PM.png)","2c3620ac":"### Multi class classification\nIf you read the docs for the LogisticRegression classifier, it will tell you that for multiclass classification (which means you have more than two classes) it can use the one-vs-Rest method (OvR) if you set the `multi_class` parameter to 'ovr', however, by default it uses crossentropy loss for multiclass problems which is more complex than the simple log loss function.  \nOvR means that for the mnist problem it would train one classifier that can say whether a digit is 0 or not, and another for ones and another for twos and so on, so it would train 10 models, one for each digit.","d8394f88":"### Gradient Descent\n![Gradient Descent](https:\/\/cdn-images-1.medium.com\/max\/600\/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)  \n\nFrom vector analysis we know that the gradient of a function is the direction at which the function increases the most denoted by the symbol $\\nabla$.  \nIf we can calculate the gradient of the cost function $\\nabla L$ at any point and move in the opposite direction updating the weights and bias this is going to slowly decrease the loss until we reach a local minimum.  \nUpdate equations\n$$ W := W - \\alpha * \\nabla_{W} L $$\n$$ b := b - \\alpha * \\nabla_{b} L $$\n$ \\nabla_{W} L = X^T \\odot (h - y) $  \n$ \\nabla_{b} L = (h - y) $","e03c7c4a":"Not too far","a4659d0a":"The loss is decreasing which is good!  \nNow let's make predictions using our weight and bias","9e14c7c4":"As you can see, there is a label column in the training dataframe which is the target output and you have pixel values from pixel0 all the way to pixel783 which is basically a flattened 28 * 28 image. each pixel has a value between 0 and 255, 0 being black and 255 being white.  \nLet's take a look at some of these images","50c154e5":"We will write the loss function for one example and then we will write the general cost function and then explain both.\n- Loss function\n$$ l = - y\\log(h(x)) - (1 - y)\\log(1 - h(x)) $$\nwhere $x$ is the feature vector for one example and $y$ is the target label.  \n\n- Cost function over all examples\n$$ L = \\frac{1}{m}\\sum_{i=1}^m - y_i\\log(h(x_i)) - (1 - y_i)\\log(1 - h(x_i)) $$\nSo the cost function is simply the mean of the loss over all training examples.  \n\nNow let's see why this works.  \nWhen $y_i == 1$ and $h(x_i) \\simeq 0$ this means we got it wrong, this will cause the second term of the equation to equal 0 however the first term is goint to be $\\log(~0)$ which is going to be a very big negative number.  \nWhen $y_i == 1$ and $h(x_i) \\simeq 1$ this means we got it right, this will cause the second term of the equation to equal 0 however the first term is goint to be $\\log(~1)$ which is going to be a very small negative number.  \n\nNow this proves that the Loss function we wrote earlier is a good indicator for how bad the model is which means we need to run some sort of optimization algorithm on this function to minimize it with respect to $W$ and $b$.","7e607041":"just out of curiousity, what is the accuracy score on the training data","38c9fd5c":"If you look at the documentation you will see that the smaller the `C` parameter the stronger the regularization, which means it is the inverse of the $\\lambda$ in the equation. try different values of `C` and see which results in the least overfitting","109e6f41":"# Introduction\n\n## Types of machine learning problems\n- Supervised learning\n    - Classification\n    - Regression\n    - ...\n- Unsupervised learning\n    - Clustering\n    - ...\n    \n### Supervised learning\nSupervised learning is a type of machine learning algorithms that is used when you have both the data and the target output and your goal is to learn the correlation between the data and the target output.  \nOne example is Classification which is going to be this notebook's example. You will be given 28 * 28 pixel data for a hand written digit from 0 to 9 and the goal is to know only from the pixel data what digit the data represents. So you should classify the data, hence the name.\n\n### Unsupervised learning\nUnsupervised learning algorithms are used when you don't have the target output for the training data. \nOne example for clustering is when you have data of your ecommerce website users and you want to figure out how may sigments of users exists and what are there interests. In this case you're going to use the user features to cluster similar users together, however, notice that you don't know the number of clusters nor the names of the clusters.","70923b11":"## Classification\nThis problem is a classification problem which means we need to categorize our data to descrete known categories, in todays problem the data is pixel data of 28 x 28 images of hand written digits and we need to classify each entry to one of 10 categories (0, 1, 2, etc...). The data is flattened to 784 feature for simplicity","59042b66":"It seems to be much higher, this happened because the mode memorized the training data and failed to generalize to new data, this phenominon is called overfitting and can be solved using regularization\n### Regularization\nregularization is so simple, all we need to do is add a term to the loss function that penalizes the model for having high weights.\n$$ L = \\frac{1}{m}\\sum_{i=1}^m - y_i\\log(h(x_i)) - (1 - y_i)\\log(1 - h(x_i)) + \\lambda \\|{W}\\|^2 $$  \nIn scikit learn you just need to set the `penalty` parameter to 'l2' which is the default value.  \nLet's try it.","98f6826f":"The reason why the data is provided as a flattened array instead of an image is that almost all machine learning algorithms require a flat array (if we exclude deep learning).  \n\nNow we are going to take only zeros and ones to do binary classification","a37d737c":"As you can see we can now get a model that can classify images of hand written zeros and ones with an accuracy of 99.7% which is not bad at all.  \nNow we will use [sklearn.linear_model.LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) model","b821f765":"What the hypothesis represents is the probability of the data example being of class 1, however, if we use it as is it will give random results simply because $W$ and $b$ are randomly initialized.  \nWe need a way to iteratively improve the values of $W$ and $b$ so that they would describe the data better.  \nTo do so we need to know how bad the current values are, and this is measured using the cost function."}}