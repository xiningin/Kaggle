{"cell_type":{"2ccd62dd":"code","cf082a84":"code","eb3e7f95":"code","c79d5f09":"code","129df9cd":"code","c4993262":"code","4658168b":"code","90a83d77":"code","6f38a0ba":"code","09ccb88a":"code","8c273606":"code","cb06fd05":"code","b3300fb8":"code","bfa985ee":"code","aec29070":"code","b04deb89":"code","933fd9bb":"code","110a4e37":"code","f1588feb":"code","4bdb9efb":"code","a963941a":"code","e64675c0":"code","41a15c21":"code","f0d7e01e":"code","6206658a":"code","e325af0e":"code","825337a1":"code","378edd6b":"code","e38eada1":"code","c51f1d11":"code","c14c8523":"code","4bc47708":"code","2560c0df":"code","d3044306":"code","8f6efe6b":"code","bb7c12e3":"code","daebc3c2":"code","3453f335":"code","17fc0eb3":"code","7388e546":"code","6b60b73f":"code","2e632d10":"code","27cb042e":"code","993b9916":"code","0a591d38":"code","b66fc480":"code","45a2f5ce":"code","44961007":"code","c40d52b0":"code","0079f21c":"code","f7495a6a":"code","5e0fbc21":"code","4fec7309":"code","8755fb62":"code","3f352fb7":"code","b2f2c770":"code","8314322b":"code","de820342":"code","0703d19e":"code","4dfe2998":"code","e4ce29eb":"markdown","a838a5de":"markdown","68ee1d74":"markdown","6928f75a":"markdown","4d7c6e29":"markdown","6d1d103d":"markdown","edc54307":"markdown","76255d8d":"markdown","5e36414e":"markdown"},"source":{"2ccd62dd":"# Imports\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt","cf082a84":"# Extracting the wave, \"y\", and sampling rate, \"sr\", of the audio file\ny, sr = librosa.load('..\/input\/datawavfiles\/blues.00000.wav')","eb3e7f95":"print(y.shape)\nsr","c79d5f09":"# Plotting the wave\nplt.plot(y);\nplt.title('Signal');\nplt.xlabel('Time (samples)');\nplt.ylabel('Amplitude');","129df9cd":"# Computing the fast Fourier transform on a single short time window of length 2048 (standard for music audio)\nn_fft = 2048\nft = np.abs(librosa.stft(y[:n_fft], hop_length = n_fft+1))","c4993262":"# Plotting the signal after applying the FFT\nplt.plot(ft);\nplt.title('spectrum');\nplt.xlabel('Frequency Bin');\nplt.ylabel('Amplitude');","4658168b":"# Computing the spectrogram\nspec = np.abs(librosa.stft(y, hop_length=512))\nspec = librosa.amplitude_to_db(spec, ref=np.max) # converting to decibals\n\n# Plotting the spectrogram\nplt.figure(figsize=(8,5));\nlibrosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log');\nplt.colorbar(format='%+2.0f dB');\nplt.title('Spectrogram');","90a83d77":"# Computing the mel spectrogram\nspect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\nspect = librosa.power_to_db(spect, ref=np.max) # Converting to decibals\n\n# Plotting the mel spectrogram\nplt.figure(figsize=(8,5))\nlibrosa.display.specshow(spect, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram');\nplt.colorbar(format='%+2.0f dB');","6f38a0ba":"# Extracting mfccs from the audio signal\nmfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=512, n_mfcc=13)\n# Displaying the mfccs\nplt.figure(figsize=(8,5));\nlibrosa.display.specshow(mfcc, x_axis='time');\nplt.title('MFCC');","09ccb88a":"# Scaling the mfccs\nmfccscaled = np.mean(mfcc.T, axis=0)\nmfccscaled\n","8c273606":"# Creating an empty list to store sizes in\nsizes = []\n\n# Looping through each audio file\nfor file in os.scandir('..\/input\/datawavfiles'):\n        \n    # Loading in the audio file\n    y, sr = librosa.core.load(file)\n        \n    # Computing the mel spectrograms\n    spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n    spect = librosa.power_to_db(spect, ref=np.max)\n    \n    # Adding the size to the list\n    sizes.append(spect.shape)\n    \n# Checking if all sizes are the same\nprint(f'The sizes of all the mel spectrograms in our data set are equal: {len(set(sizes)) == 1}')\n\n# Checking the max size\nprint(f'The maximum size is: {max(sizes)}')","cb06fd05":"def extract_mel_spectrogram(directory):\n    \n    # Creating empty lists for mel spectrograms and labels\n    labels = []\n    mel_specs = []\n    \n    # Looping through each file in the directory\n    for file in os.scandir(directory):\n        \n        # Loading in the audio file\n        y, sr = librosa.core.load(file)\n        \n        # Extracting the label and adding it to the list\n        label = str(file).split('.')[0][11:]\n        labels.append(label)\n        \n        # Computing the mel spectrograms\n        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n        spect = librosa.power_to_db(spect, ref=np.max)\n        \n        # Adjusting the size to be 128 x 660\n        if spect.shape[1] != 660:\n            spect.resize(128,660, refcheck=False)\n            \n        # Adding the mel spectrogram to the list\n        mel_specs.append(spect)\n    # Converting the list or arrays to an array\n    X = np.array(mel_specs)\n    \n    # Converting labels to numeric values\n    labels = pd.Series(labels)\n    label_dict = {\n        'jazz': 1,\n        'reggae': 2,\n        'rock': 3,\n        'blues': 4,\n        'hiphop': 5,\n        'country': 6,\n        'metal': 7,\n        'classical': 8,\n        'disco': 9,\n        'pop': 10\n    }\n    y = labels.map(label_dict)\n    \n    # Returning the mel spectrograms and labels\n    return X, y","b3300fb8":"# Using the function to read and extract mel spectrograms from the GTZAN Genre Dataset audio files\nX, y = extract_mel_spectrogram('..\/input\/datawavfiles')","bfa985ee":"def make_mel_spectrogram_df(directory):\n    \n    \n    # Creating empty lists for mel spectrograms and labels\n    labels = []\n    mel_specs = []\n    \n    # Looping through each file in the directory\n    for file in os.scandir(directory):\n        \n        # Loading in the audio file\n        y, sr = librosa.core.load(file)\n        \n        # Extracting the label and adding it to the list\n        label = str(file).split('.')[0][11:]\n        labels.append(label)\n        \n        # Computing the mel spectrograms\n        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n        spect = librosa.power_to_db(spect, ref=np.max)\n        \n        # Adjusting the size to be 128 x 660\n        if spect.shape[1] != 660:\n            spect.resize(128,660, refcheck=False)\n        \n        # Flattening to fit into dataframe and adding to the list\n        spect = spect.flatten()\n        mel_specs.append(spect)\n    # Converting the lists to arrays so we can stack them\n    mel_specs = np.array(mel_specs)\n    labels = np.array(labels).reshape(300,1)\n    \n    # Create dataframe\n    df = pd.DataFrame(np.hstack((mel_specs,labels)))\n    \n    # Returning the mel spectrograms and labels\n    return df","aec29070":"# Using the above function to create a dataframe with all of the flattened mel spectrograms and genre labels\ndf = make_mel_spectrogram_df('..\/input\/datawavfiles')","b04deb89":"df.to_csv('.\/genre_mel_specs.csv', index=False)","933fd9bb":"def extract_audio_features(directory):\n    \n    # Creating an empty list to store all file names\n    files = []\n    labels = []\n    zcrs = []\n    spec_centroids = []\n    spec_rolloffs = []\n    mfccs_1 = []\n    mfccs_2 = []\n    mfccs_3 = []\n    mfccs_4 = []\n    mfccs_5 = []\n    mfccs_6 = []\n    mfccs_7 = []\n    mfccs_8 = []\n    mfccs_9 = []\n    mfccs_10 = []\n    mfccs_11 = []\n    mfccs_12 = []\n    mfccs_13 = []\n    # Looping through each file in the directory\n    for file in os.scandir(directory):\n        \n        # Loading in the audio file\n        y, sr = librosa.core.load(file)\n        \n        # Adding the file to our list of files\n        files.append(file)\n        \n        # Adding the label to our list of labels\n        label = str(file).split('.')[0]\n        labels.append(label)\n        \n        # Calculating zero-crossing rates\n        zcr = librosa.feature.zero_crossing_rate(y)\n        zcrs.append(np.mean(zcr))\n        \n        # Calculating the spectral centroids\n        spec_centroid = librosa.feature.spectral_centroid(y)\n        spec_centroids.append(np.mean(spec_centroid))\n        \n        # Calculating the spectral rolloffs\n        spec_rolloff = librosa.feature.spectral_rolloff(y)\n        spec_rolloffs.append(np.mean(spec_rolloff))\n        \n        # Calculating the first 13 mfcc coefficients\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=512, n_mfcc=13)\n        mfcc_scaled = np.mean(mfcc.T, axis=0)\n        mfccs_1.append(mfcc_scaled[0])\n        mfccs_2.append(mfcc_scaled[1])\n        mfccs_3.append(mfcc_scaled[2])\n        mfccs_4.append(mfcc_scaled[3])\n        mfccs_5.append(mfcc_scaled[4])\n        mfccs_6.append(mfcc_scaled[5])\n        mfccs_7.append(mfcc_scaled[6])\n        mfccs_8.append(mfcc_scaled[7])\n        mfccs_9.append(mfcc_scaled[8])\n        mfccs_10.append(mfcc_scaled[9])\n        mfccs_11.append(mfcc_scaled[10])\n        mfccs_12.append(mfcc_scaled[11])\n        mfccs_13.append(mfcc_scaled[12])\n        # Creating a data frame with the values we collected\n    df = pd.DataFrame({\n        'files': files,\n        'zero_crossing_rate': zcrs,\n        'spectral_centroid': spec_centroids,\n        'spectral_rolloff': spec_rolloffs,\n        'mfcc_1': mfccs_1,\n        'mfcc_2': mfccs_2,\n        'mfcc_3': mfccs_3,\n        'mfcc_4': mfccs_4,\n        'mfcc_5': mfccs_5,\n        'mfcc_6': mfccs_6,\n        'mfcc_7': mfccs_7,\n        'mfcc_8': mfccs_8,\n        'mfcc_9': mfccs_9,\n        'mfcc_10': mfccs_10,\n        'mfcc_11': mfccs_11,\n        'mfcc_12': mfccs_12,\n        'mfcc_13': mfccs_13,\n        'labels': labels\n    })\n    \n    # Returning the data frame\n    return df","110a4e37":"# Using the function to read and extract the audio files from the GTZAN Genre Dataset\ndf = extract_audio_features('..\/input\/datawavfiles')","f1588feb":"df.to_csv('.\/genre.csv', index=False)","4bdb9efb":"# Reading in the data\ngenre = pd.read_csv('.\/genre.csv')\ngenre.head()","a963941a":"# Fixing the file names and labels\ngenre['files'] = genre['files'].map(lambda x: x[11:-2])\ngenre['labels'] = genre['labels'].map(lambda x: x[11:])","e64675c0":"# Mapping the labels to numeric values\nlabel_map = {\n    'blues': 1,\n    'classical': 2,\n    'country': 3,\n    'disco': 4,\n    'hiphop': 5,\n    'jazz': 6,\n    'metal': 7,\n    'pop': 8,\n    'reggae': 9,\n    'rock': 10\n}\n\ngenre['y'] = genre['labels'].map(label_map)\ngenre.head()","41a15c21":"genre.to_csv('.\/genre_clean.csv', index=False)","f0d7e01e":"# Reading in the data\nmel_specs = pd.read_csv('.\/genre_mel_specs.csv')\n# Taking a look at the data\nmel_specs.head()","6206658a":"# Renaming the label column and mapping them to numeric values using the same map as above\nmel_specs = mel_specs.rename(columns={'84480': 'labels'})\nmel_specs['y'] = mel_specs['labels'].map(label_map)","e325af0e":"mel_specs.to_csv('.\/genre_mel_specs_clean.csv', index=False)","825337a1":"# Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display","378edd6b":"# Reading in the data\ndf = pd.read_csv('.\/genre_clean.csv')\ndf.head()","e38eada1":"# Looking at some descriptive statistics\ndf.describe()","c51f1d11":"def plot_spectrogram(genre):\n    '''\n    This function takes in a list of genres and plots a mel spectrogram for one song \n    per genre.\n    '''\n    \n    # Loading in the audio file\n    y, sr = librosa.core.load(f'..\/input\/datawavfiles\/{genre}.00000.wav')\n    \n    # Computing the spectrogram and transforming it to the decibal scale\n    spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n    spect = librosa.power_to_db(spect, ref=np.max) # Converting to decibels\n    \n    # Plotting the transformed spectrogram\n    plt.figure(figsize=(10,7))\n    librosa.display.specshow(spect, y_axis='mel', fmax=8000, x_axis='time')\n    plt.title(str(genre))\n    plt.show()","c14c8523":"# Creating a list of all the genres\ngenres = list(df['labels'].unique())\n\n# Plotting spectrogram for each genre\nfor genre in genres:\n    plot_spectrogram(genre)","4bc47708":"def spectrogram_subplots(genre):\n  \n    # Defining the subplots\n    fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = (25,10))\n    ax = ax.ravel() # Turning ax into a matrix to make it easier to work with\n\n    # Looping through the list of genres\n    for i, kind in enumerate(genre):\n        \n        # Reading in the first file from each genre\n        y, sr = librosa.core.load(f'..\/input\/datawavfiles\/{kind}.00000.wav')\n        \n        # Computing the mel spectrogram\n        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n        spect = librosa.power_to_db(spect, ref=np.max)\n        \n        # Displaying the mel spectrogram \n        librosa.display.specshow(spect, y_axis = 'mel', fmax = 8000, x_axis = 'time', ax = ax[i])\n        ax[i].set_title(str(kind))","2560c0df":"spectrogram_subplots(genres)","d3044306":"# Checking correlations\nplt.figure(figsize=(10,7))\nsns.heatmap(df.corr()[['y']].sort_values('y', ascending=False), annot=True);","8f6efe6b":"import os\nimport librosa\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.python.keras import utils\nfrom keras.utils.np_utils import to_categorical","bb7c12e3":"def extract_mel_spectrogram(directory):\n\n    # Creating empty lists for mel spectrograms and labels\n    labels = []\n    mel_specs = []\n    \n    \n    # Looping through each file in the directory\n    for file in os.scandir(directory):\n        \n        # Loading in the audio file\n        y, sr = librosa.core.load(file)\n        \n        # Extracting the label and adding it to the list\n        label = str(file).split('.')[0][11:]\n        labels.append(label)\n        \n        # Computing the mel spectrograms\n        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n        spect = librosa.power_to_db(spect, ref=np.max)\n        \n        # Adjusting the size to be 128 x 660\n        if spect.shape[1] != 660:\n            spect.resize(128,660, refcheck=False)\n        # Adding the mel spectrogram to the list\n        mel_specs.append(spect)\n        \n    # Converting the list or arrays to an array\n    X = np.array(mel_specs)\n    \n    # Converting labels to numeric values\n    labels = pd.Series(labels)\n    label_dict = {\n        'jazz': 0,\n        'reggae': 1,\n        'rock': 2,\n        'blues': 3,\n        'hiphop': 4,\n        'country': 5,\n        'metal': 6,\n        'classical': 7,\n        'disco': 8,\n        'pop': 9\n    }\n    y = labels.map(label_dict).values\n    \n    # Returning the mel spectrograms and labels\n    return X, y","daebc3c2":"# Using the function to read and extract mel spectrograms from the GTZAN Genre Dataset audio files\nX, y = extract_mel_spectrogram('..\/input\/datawavfiles')","3453f335":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)\n# Checking the minimum value (the scale ranges from zero to some negative value) to see how we should scale the data\nX_train.min()","17fc0eb3":"# Scaling our data to be between 0 and 1 using the minimum value from above\nX_train \/= -80\nX_test \/= -80","7388e546":"# Reshaping images to be 128 x 660 x 1, where the 1 represents the single color channel\nX_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\nX_test = X_test.reshape(X_test.shape[0], 128, 660, 1)","6b60b73f":"# One hot encoding our labels\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)","2e632d10":"# Initializing a random seed for replication purposes\nnp.random.seed(23456)\n\n# Initiating an empty neural network\nmodel = Sequential()\n\n# Adding a flattened layer to input our image data\nmodel.add(Flatten(input_shape = (128, 660, 1)))\n\n# Adding a dense layer with 128 neurons\nmodel.add(Dense(128, activation='relu'))\n\n# Adding a dense layer with 128 neurons\nmodel.add(Dense(128, activation='relu'))\n\n# Adding a dense layer with 64 neurons\nmodel.add(Dense(64, activation='relu'))\n\n# Adding an output layer\nmodel.add(Dense(10, activation='softmax'))\n\n# Compiling our neural network\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# Fitting our neural network\nhistory = model.fit(X_train,\n                    y_train, \n                    batch_size=16,\n                    validation_data=(X_test, y_test),\n                    epochs=40)","27cb042e":"import tensorflow as tf\n# Initializing a random seed for replication purposes\nnp.random.seed(23456)\ntf.random.set_seed(123)\n\n# Initiating an empty neural network\ncnn_model = Sequential(name='cnn_1')\n\n# Adding convolutional layer\ncnn_model.add(Conv2D(filters=16,\n                     kernel_size=(3,3),\n                     activation='relu',\n                     input_shape=(128,660,1)))\n\n# Adding max pooling layer\ncnn_model.add(MaxPooling2D(pool_size=(2,4)))\n\n# Adding convolutional layer\ncnn_model.add(Conv2D(filters=32,\n                     kernel_size=(3,3),\n                     activation='relu'))\n\n# Adding max pooling layer\ncnn_model.add(MaxPooling2D(pool_size=(2,4)))\n\n# Adding a flattened layer to input our image data\ncnn_model.add(Flatten())\n\n# Adding a dense layer with 64 neurons\ncnn_model.add(Dense(64, activation='relu'))\n\n# Adding a dropout layer for regularization\ncnn_model.add(Dropout(0.25))\n\n# Adding an output layer\ncnn_model.add(Dense(10, activation='softmax'))\n\n# Compiling our neural network\ncnn_model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n# Fitting our neural network\nhistory = cnn_model.fit(X_train,\n                        y_train, \n                        batch_size=16,\n                        validation_data=(X_test, y_test),\n                        epochs=15)","993b9916":"# Checking the model summary\ncnn_model.summary()","0a591d38":"# The code in this cell was adapted from a lecture at General Assembly\n\n# Check out our train loss and test loss over epochs.\ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Set figure size.\nplt.figure(figsize=(8, 4))\n\n# Generate line plot of training, testing loss over epochs.\nplt.plot(train_loss, label='Training Loss', color='blue')\nplt.plot(test_loss, label='Testing Loss', color='red')\n\n# Set title\nplt.title('Training and Testing Loss by Epoch', fontsize = 14)\nplt.xlabel('Epoch', fontsize = 10)\nplt.ylabel('Categorical Crossentropy', fontsize = 10)\nplt.xticks(range(1,16), range(1,16))\n\nplt.legend(fontsize = 12);","b66fc480":"# The code in this cell was adapted from a lecture at General Assembly\n\n# Check out our train accuracy and test accuracy over epochs.\ntrain_loss = history.history['accuracy']\ntest_loss = history.history['val_accuracy']\n\n# Set figure size.\nplt.figure(figsize=(8, 4))\n\n# Generate line plot of training, testing loss over epochs.\nplt.plot(train_loss, label='Training Accuracy', color='blue')\nplt.plot(test_loss, label='Testing Accuracy', color='red')\n\n# Set title\nplt.title('Training and Testing Accuracy by Epoch', fontsize = 14)\nplt.xlabel('Epoch', fontsize = 10)\nplt.ylabel('Accuracy', fontsize = 10)\nplt.xticks(range(1,21), range(1,21))\n\nplt.legend(fontsize = 12);\n","45a2f5ce":"# Making predictions from the cnn model\npredictions = cnn_model.predict(X_test, verbose=1)","44961007":"# Checking the number of targets per class\nfor i in range(10): \n    print(f'{i}: {sum([1 for target in y_test if target[i] == 1])}')","c40d52b0":"# Checking the number of predicted values in each class\nfor i in range(10): \n    print(f'{i}: {sum([1 for prediction in predictions if np.argmax(prediction) == i])}')","0079f21c":"# Calculating the confusion matrix \n# row: actual\n# columns: predicted\nconf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\nconf_matrix","f7495a6a":"# Creating a dataframe of the confusion matrix with labels for readability \nconfusion_df = pd.DataFrame(conf_matrix)\nconfusion_df","5e0fbc21":"# Creating a dictionary of labels\nlabels_dict = {\n    0: 'jazz',\n    1: 'reggae',\n    2: 'rock',\n    3: 'blues',\n    4: 'hiphop',\n    5: 'country',\n    6: 'metal',\n    7: 'classical',\n    8: 'disco',\n    9: 'pop'\n}","4fec7309":"# Renaming rows and columns with labes\nconfusion_df = confusion_df.rename(columns=labels_dict)\nconfusion_df.index = confusion_df.columns\nconfusion_df","8755fb62":"# Creating a heatmap for the confusion matrix for display\nplt.figure(figsize= (10,6))\nsns.set(font_scale = 1);\nax = sns.heatmap(confusion_df, annot=True, cmap=sns.cubehelix_palette(50));\nax.set(xlabel='Predicted Values', ylabel='Actual Values');","3f352fb7":"# Imports\nimport os\nimport librosa\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.python.keras import utils\nfrom keras.utils import to_categorical","b2f2c770":"mel_specs = pd.read_csv('.\/genre_mel_specs_clean.csv')\nmel_specs.head()","8314322b":"def get_genre_subset(data, genre_subset):\n  \n    # Filtering the dataframe for the subset of the genres and resetting the index\n    df = data.loc[data['labels'].isin(genre_subset)]\n    df = df.reset_index().drop(columns=['index'])\n    \n    # Creating a new label dictionary\n    new_label_dict = {}\n    for i in range(len(genre_subset)):\n        new_label_dict[genre_subset[i]] = i\n    \n    # Changing labels to be the new labels\n    df['y'] = df['labels'].map(new_label_dict)\n\n    return df","de820342":"def preprocess_mel_spec_data(data, genre_subset):\n    # Getting a subset of the genres using our genre_subset function\n    subset = get_genre_subset(data, genre_subset)\n    \n    # Dropping label columns to prepare our feature vector\n    specs = subset.drop(columns=['labels', 'y'])\n    \n    # Reshaping the arrays to their original \"image\" form\n    X = []\n    for i in range(len(genre_subset)*100):\n        X.append(np.array(specs.iloc[i]).reshape(128,660))\n        \n    # Converting list X to an array\n    X = np.array(X)\n    # Defining our targets\n    y = subset.loc[subset['labels'].isin(genre_subset), 'y'].values\n    \n    # train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)\n    \n    # Scaling our data to be between 0 and 1\n    X_train \/= -80\n    X_test \/= -80\n    \n    # Reshaping images to be 128 x 660 x 1\n    X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n    X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)\n    \n    # One hot encoding our labels\n    y_train = to_categorical(y_train, len(genre_subset))\n    y_test = to_categorical(y_test, len(genre_subset))\n    \n    return X_train, X_test, y_train, y_test","0703d19e":"# List of all the genres\ngenre_list = [ \n    'jazz',\n    'reggae',\n    'rock',\n    'blues',\n    'hiphop',\n    'country',\n    'metal',\n    'classical',\n    'disco',\n    'pop'\n]\n# List of a subset of the genres\ngenre_subset = [\n    'jazz',\n    'hiphop',\n    'country',\n    'metal',\n    'classical',\n    'disco',\n    'pop'\n]","4dfe2998":"# Using our function to get our features and targets\nX_train, X_test, y_train, y_test = preprocess_mel_spec_data(mel_specs, genre_subset)","e4ce29eb":"Constructing a CNN\n","a838a5de":"confusion matrix","68ee1d74":"Plotting mel spectograms","6928f75a":"**CNN**","4d7c6e29":"**DATA ANALYSIS**","6d1d103d":"Function to Preprocess the Features and Target","edc54307":"**DATA PREPROCESSING**","76255d8d":"Constructing a Feed Forward Neural Network (FFNN)","5e36414e":"**CNN EXPLORATION**"}}