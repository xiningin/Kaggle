{"cell_type":{"06665dce":"code","999c301b":"code","935a064d":"code","234c37c3":"code","f3c86c14":"code","b69a38ef":"code","e9a7de6f":"code","3f02c556":"code","7f3d17c8":"code","fddd08d4":"code","0706666e":"code","82fe3719":"code","68a3fac3":"code","f88bd80f":"code","4ffc1aa8":"markdown","79be03ff":"markdown","d1394c29":"markdown","4010793c":"markdown","7a213d22":"markdown","d5f2bc80":"markdown","3abed8c2":"markdown","76a3f5ac":"markdown","32292aaa":"markdown","f2330f3e":"markdown"},"source":{"06665dce":"import torch\nimport torchvision\nimport math\nimport matplotlib.pyplot as plt","999c301b":"class CosineDecayWithWarmUpScheduler(object):\n    def __init__(self,optimizer,step_per_epoch,init_warmup_lr=1e-5,warm_up_steps=1000,max_lr=1e-4,min_lr=1e-6,num_step_down=2000,num_step_up=None,\n                T_mul=1,max_lr_decay=None, gamma=1,min_lr_decay=None,alpha=1):\n        self.optimizer = optimizer\n        self.step_per_epoch = step_per_epoch\n        if warm_up_steps != 0:\n            self.warm_up = True\n        else:\n            self.warm_up = False  \n        self.init_warmup_lr = init_warmup_lr\n        self.warm_up_steps = warm_up_steps\n        self.max_lr = max_lr\n        if min_lr == 0:\n            self.min_lr = 0.1 * max_lr\n            self.alpha = 0.1\n        else:\n            self.min_lr = min_lr\n        self.num_step_down = num_step_down\n        if num_step_up == None:\n            self.num_step_up = num_step_down\n        else:\n            self.num_step_up = num_step_up    \n        self.T_mul = T_mul\n        if max_lr_decay == None:\n            self.gamma = 1\n        elif max_lr_decay == 'Half':\n            self.gamma = 0.5\n        elif max_lr_decay == 'Exp':\n            self.gamma = gamma\n        \n        if min_lr_decay == None:\n            self.alpha = 1\n        elif min_lr_decay == 'Half':\n            self.alpha = 0.5\n        elif min_lr_decay == 'Exp':\n            self.alpha = alpha\n\n\n        self.num_T = 0\n        self.iters = 0\n        self.lr_list = []\n        \n        \n    def update_cycle(self, lr):\n        old_min_lr = self.min_lr\n        if lr == self.max_lr or (self.num_step_up == 0 and lr == self.min_lr):\n            if self.num_T == 0:\n                self.warm_up = False\n                self.min_lr \/= self.alpha\n            self.iters = 0\n            self.num_T += 1\n            self.min_lr *= self.alpha\n\n        if lr == old_min_lr and self.max_lr * self.gamma >= self.min_lr:\n            self.max_lr *= self.gamma\n            \n    \n    def step(self):\n        self.iters += 1\n        if self.warm_up:\n            lr = self.init_warmup_lr + (self.max_lr-self.init_warmup_lr) \/ self.warm_up_steps * self.iters\n        else:\n            T_cur = self.T_mul**self.num_T\n            if self.iters <= self.num_step_down*T_cur:\n                lr = self.min_lr + (self.max_lr-self.min_lr) * (1 + math.cos(math.pi*self.iters\/(self.num_step_down*T_cur)))\/2\n                if lr < self.min_lr:\n                    lr = self.min_lr\n            elif self.iters > self.num_step_down*T_cur:\n                lr = self.min_lr + (self.max_lr-self.min_lr) \/ (self.num_step_up * T_cur) * (self.iters-self.num_step_down*T_cur)\n                if lr > self.max_lr:\n                    lr = self.max_lr\n\n        self.update_cycle(lr)\n                \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n            self.lr_list.append(lr)","935a064d":"model = torchvision.models.resnet18(pretrained=False)","234c37c3":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=4e-6,num_step_down=6000,\n                                          num_step_up = 6000)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","f3c86c14":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=4e-6,num_step_down=6000,\n                                          num_step_up = 6000,T_mul=1,max_lr_decay='Half')\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","b69a38ef":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=4e-6,num_step_down=6000,\n                                          num_step_up = 6000,T_mul=1,max_lr_decay='Exp',gamma=0.9)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","e9a7de6f":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=2e-5,warm_up_steps=4000,max_lr=4e-5,min_lr=1e-5,num_step_down=4000,\n                                          num_step_up = 4000,min_lr_decay='Exp',alpha=0.1)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","3f02c556":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=4e-6,num_step_down=6000,\n                                          num_step_up = 0)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","7f3d17c8":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=1e-6,num_step_down=6000,\n                                          num_step_up = 0, max_lr_decay='Half')\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","fddd08d4":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=2e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=2e-5,num_step_down=6000,\n                                          num_step_up = 0, max_lr_decay='Exp',gamma=0.9, min_lr_decay='Exp', alpha=0.5)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","0706666e":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=2e-5,num_step_down=6000,\n                                          num_step_up = 0, max_lr_decay='Half', min_lr_decay='Exp', alpha=0.5)\nfor epoch in range(35):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","82fe3719":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=3000,max_lr=4e-5,min_lr=4e-6,num_step_down=3000,\n                                          num_step_up = 3000,T_mul=2)\nfor epoch in range(60):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","68a3fac3":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=1e-5,warm_up_steps=3000,max_lr=4e-5,min_lr=4e-6,num_step_down=3000,\n                                          num_step_up = 3000,T_mul=2,max_lr_decay='Half')\nfor epoch in range(60):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","f88bd80f":"optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = CosineDecayWithWarmUpScheduler(optimizer,step_per_epoch=1000,init_warmup_lr=2e-5,warm_up_steps=5000,max_lr=4e-5,min_lr=4e-6,num_step_down=5000,\n                                          num_step_up = 5000, max_lr_decay='Exp',gamma=0.5, min_lr_decay='Exp', alpha=0.5)\nfor epoch in range(30):\n    for i in range(1000):\n        optimizer.step()\n        scheduler.step()\nplt.figure(figsize=(24,8))\nplt.plot(scheduler.lr_list)","4ffc1aa8":"Warm Up Cosine Decay With Restart ","79be03ff":"T_mul = 2","d1394c29":"# CosineDecayWithWarmUp","4010793c":"max_lr_decay = 'Half'","7a213d22":"min_lr_decay = 'Exp'","d5f2bc80":"*  My first Notebook. \n*  Plot kinds of learning rate. \n*  Hope you could find the one you like.\n*  Welcome to improve it.\n*  Thanks for watching","3abed8c2":"max_lr_decay='Half'","76a3f5ac":"Welcome to make comments on which learning rate scheduler do you think work really well?","32292aaa":"T_mul = 2, max_lr_decay = 'Half'","f2330f3e":"max_lr_decay = 'Exp' \ngamma =0.9"}}