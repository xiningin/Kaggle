{"cell_type":{"b7b2d0a9":"code","fe20ae6c":"code","d3eb14af":"code","c0f3f020":"code","871ccc4f":"code","6bcb3bf0":"code","acf6a3d8":"code","648c50c1":"code","27299ce2":"code","ef464d9a":"code","41ab2ae5":"code","ee0c25a8":"code","a8130b54":"code","aeab7365":"code","c68df657":"code","f8c6e414":"code","7cd519f0":"code","6dcf0080":"code","b1f64153":"code","c80cb1a0":"code","3296efd1":"code","7ee1eb87":"code","49dfee94":"markdown","1a5574f9":"markdown","151748dd":"markdown","6b088e07":"markdown","a03b8b1c":"markdown"},"source":{"b7b2d0a9":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport sklearn.model_selection as ms\nimport sklearn.metrics as m\nimport sklearn.tree as tree\nimport sklearn.ensemble as ensemble\nimport sklearn.svm as svm\nimport sklearn.linear_model as lm\nimport sklearn.preprocessing as pp\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe20ae6c":"data = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')","d3eb14af":"data.info()","c0f3f020":"data.sample(5)","871ccc4f":"data.iloc[:, 0:10]","6bcb3bf0":"data.iloc[:, 10:]","acf6a3d8":"data.isnull().sum() # check for blank values","648c50c1":"data['stalk-root'].unique()","27299ce2":"tol_label = 0\nfor col in data.columns:\n    print(col)\n    print(data[col].unique())\n    tol_label += len(data[col].unique())\n    print()\n# seems that only the column - \"stalk-root\" contain ?","ef464d9a":"tol_label","41ab2ae5":"data['stalk-root'].value_counts() # we have about 2480 row items with ?","ee0c25a8":"data[data['stalk-root'] == '?']['class'].value_counts() # breakdown of class where stalk-root == ?","a8130b54":"data['class'].value_counts() # about 45% of the positive class (p) has rows where stalk-root == ?","aeab7365":"data['class'].replace({'p': 1, 'e': 0}, inplace=True)\n\nX = data.copy()\nX.drop('class', inplace=True, axis=1)\n\ny = data['class'].copy()","c68df657":"one_hot = pp.OneHotEncoder()\none_hot.fit(X)\nX_transform = one_hot.transform(X)\nX_transform.shape","f8c6e414":"X_train_val, X_test, y_train_val, y_test = ms.train_test_split(X_transform, y, train_size=0.75, shuffle= True, stratify= y, random_state= 42)\nX_train_val.shape","7cd519f0":"X_train, X_validation, y_train, y_validation = ms.train_test_split(\\\n                                                X_train_val, y_train_val, train_size=0.75, shuffle= True, stratify= y_train_val, random_state= 42)","6dcf0080":"print(f'Size of training set: {X_train.shape[0]}')\nprint(f'Size of validation set: {X_validation.shape[0]}')\nprint(f'Size of testing set: {X_test.shape[0]}')","b1f64153":"rf_clf = ensemble.RandomForestClassifier(random_state=42)\ndt_clf = tree.DecisionTreeClassifier(random_state=42)\next_clf = ensemble.ExtraTreesClassifier(random_state=42)\nsvc_clf = svm.LinearSVC(random_state=42)\nlog_clf = lm.LogisticRegression(random_state=42)\ngb_clf = ensemble.GradientBoostingClassifier(random_state=42)\n\nvoting_classifier = ensemble.VotingClassifier([\n                    ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                    ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                    ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                    ('svc_clf', svm.LinearSVC(random_state=42)),\n                    ('log_clf', lm.LogisticRegression(random_state=42)),\n                    ('gb_clf', ensemble.GradientBoostingClassifier(random_state=42))\n                    ], voting='hard')\n\nestimators = [rf_clf, dt_clf, ext_clf, svc_clf, log_clf, gb_clf, voting_classifier]","c80cb1a0":"cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n\nfor estimator in estimators:\n    estimator.fit(X_train, y_train)\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    cv_f1 = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    \n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {np.mean(cv_accuracy) * 100}')\n    print(f'Std Accuracy: {np.std(cv_accuracy) * 100}')\n    print(f'Avg F1: {np.mean(cv_f1) * 100}')\n    print(f'Std F1: {np.std(cv_f1) * 100}')\n    print()","3296efd1":"# Most likely, we have overfitted the models\n# Let's see how it perform on the validation set\n\nfor estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_validation, y_validation) * 100)\n    print()","7ee1eb87":"for estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_test, y_test) * 100)\n    print()","49dfee94":"# Performance on Testing Set\n* Seems like all of the individual models performed just as well as the voting model (voting classifier)","1a5574f9":"# Performance on Training Set","151748dd":"# Candidates Models & Ensemble (BASELINE)\n* My approach will be to train individual classifier models as well as an ensemble model (voting classifier)\n* I will assess the individual classifier models based on the default hyperparameter values\n* The voting classifier will be based on all the individual classifier models with their default hyperparameter values\n* ALL features will be used for this baseline models","6b088e07":"# Performance on Validation Set","a03b8b1c":"# Data Processing\n* One-hot enncoding\n* Data is splitted such that BOTH the training and the testing dataset contain the same proportion of positive and negative class"}}