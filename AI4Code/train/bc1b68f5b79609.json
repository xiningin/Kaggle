{"cell_type":{"981a2c54":"code","8333a4df":"code","2d4810da":"code","3707062f":"code","83bf05c0":"code","224d9287":"code","abf97414":"code","be17b118":"code","6e08c3fa":"code","ab39345b":"code","b520ca07":"code","2f1f3a97":"code","9cd368d4":"code","69456ca2":"code","6f3f38b8":"code","fb2804ea":"code","56a8ba2c":"code","e835ec49":"code","9567678c":"code","005a7f2a":"code","5ff0f7f4":"code","bc23f377":"code","c21b422a":"code","d0bf1a3c":"code","ef3c4987":"code","b63b9ef9":"code","0e739514":"code","d817a215":"code","f67be02e":"code","55e94ddc":"code","c8f77999":"code","ec1c0282":"code","8df486c8":"code","8cfffae6":"code","71bcbe23":"code","2124e6f9":"code","cdd6c84e":"code","d278c911":"code","b9a5c9a0":"code","53cc57c1":"code","2b865334":"code","78053ac3":"code","b5d53031":"code","c156d79e":"markdown","5f2223e4":"markdown","0ba723ff":"markdown","a38eedc1":"markdown","f5f8e379":"markdown","f013d5ea":"markdown","fcd5cb67":"markdown","60b01e57":"markdown","257abb6e":"markdown","c6276adb":"markdown","2c7e171f":"markdown","ea2274af":"markdown","bfa9e3d8":"markdown","080d9b84":"markdown","d029973b":"markdown","da4ca798":"markdown","88f40fb8":"markdown","eebe978b":"markdown","ff341e54":"markdown","8036c950":"markdown","bbef73e8":"markdown","1ae992d9":"markdown","5315cbf0":"markdown","736a50dd":"markdown","514ff92d":"markdown","523b16d4":"markdown","2388e03f":"markdown","514f85ef":"markdown","18b48422":"markdown","b9289a2a":"markdown","6a7ed9aa":"markdown"},"source":{"981a2c54":"import numpy as np\nimport pandas as pd \nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot","8333a4df":"df = pd.read_csv(\"\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv\")","2d4810da":"df.head()","3707062f":"df.info()","83bf05c0":"missed = pd.DataFrame()\nmissed['column'] = df.columns\n\nmissed['percent'] = [round(100* df[col].isnull().sum() \/ len(df), 2) for col in df.columns]\nmissed = missed.sort_values('percent')\nmissed = missed[missed['percent']>0]\n\nfig = px.bar(\n    missed, \n    x='percent', \n    y=\"column\", \n    orientation='h', \n    title='Missed values percent for every column (percent > 0)', \n    height=400, \n    width=600\n)\nfig.show()","224d9287":"ds = df['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"tweets_count\", \n    y=\"user_name\", \n    orientation='h', \n    title='Top 40 users by number of tweets', \n    width=800, \n    height=800\n)\nfig.show()","abf97414":"df = pd.merge(df, ds, on='user_name')","be17b118":"data = df.sort_values('user_followers', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweets_count']]\ndata = data.sort_values('user_followers')\nfig = px.bar(\n    data.tail(40), \n    x=\"user_followers\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users by number of followers', \n    width=800, \n    height=800\n)\nfig.show()","6e08c3fa":"data = df.sort_values('user_friends', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_friends', 'tweets_count']]\ndata = data.sort_values('user_friends')\nfig = px.bar(\n    data.tail(40), \n    x=\"user_friends\", \n    y=\"user_name\", \n    color = 'tweets_count',\n    orientation='h', \n    title='Top 40 users by number of friends', \n    width=800, \n    height=800\n)\nfig.show()","ab39345b":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['year_created'] = df['user_created'].dt.year\ndata = df.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[data['year_created']>1970]\n\ndata = data['year_created'].value_counts().reset_index()\ndata.columns = ['year', 'number']\n\nfig = px.bar(\n    data, \n    x=\"year\", \n    y=\"number\", \n    orientation='v', \n    title='User created year by year', \n    width=800, \n    height=600\n)\nfig.show()","b520ca07":"df.head(10)","2f1f3a97":"ds = df['user_location'].value_counts().reset_index()\nds.columns = ['user_location', 'count']\nds = ds[ds['user_location']!='NA']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"user_location\", \n    orientation='h', title='Top 40 user locations by number of tweets', \n    width=800, \n    height=800\n)\nfig.show()","9cd368d4":"def pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]\/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n\n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(df, 'user_location', 0.5, 'Number of tweets per location')","69456ca2":"ds = df['source'].value_counts().reset_index()\nds.columns = ['source', 'count']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"source\", \n    orientation='h', \n    title='Top 40 user sources by number of tweets', \n    width=800, \n    height=800\n)\nfig.show()","6f3f38b8":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\ndf.head(10)","fb2804ea":"df['hashtags_count'].describe()","56a8ba2c":"ds = df['hashtags_count'].value_counts().reset_index()\nds.columns = ['hashtags_count', 'count']\nds = ds.sort_values(['count'])\nds['hashtags_count'] = ds['hashtags_count'].astype(str) + ' tags'\nfig = px.bar(\n    ds, \n    x=\"count\", \n    y=\"hashtags_count\", \n    orientation='h', \n    title='Distribution of number of hashtags in tweets', \n    width=800, \n    height=600\n)\nfig.show()","e835ec49":"ds = df[df['tweets_count']>10]\nds = ds.groupby(['user_name', 'tweets_count'])['hashtags_count'].mean().reset_index()\nds.columns = ['user', 'tweets_count', 'mean_count']\nds = ds.sort_values(['mean_count'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_count\", \n    y=\"user\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with higher mean number of hashtags (at least 10 tweets per user)', \n    width=800, \n    height=800\n)\nfig.show()","9567678c":"df['date'] = pd.to_datetime(df['date']) \ndf = df.sort_values(['date'])\ndf['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\ndf['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\ndf.head()","005a7f2a":"ds = df.groupby(['day', 'user_name'])['hashtags_count'].count().reset_index()\nds = ds.groupby(['day'])['user_name'].count().reset_index()\nds.columns = ['day', 'number_of_users']\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='day', \n    y=\"number_of_users\", \n    orientation='v',\n    title='Number of unique users per day', \n    width=800, \n    height=800\n)\nfig.show()","5ff0f7f4":"ds = df['day'].value_counts().reset_index()\nds.columns = ['day', 'count']\nds = ds.sort_values('count')\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='count', \n    y=\"day\", \n    orientation='h',\n    title='Tweets distribution over days present in dataset', \n    width=800, \n    height=800\n)\nfig.show()","bc23f377":"df['hour'] = df['date'].dt.hour\nds = df['hour'].value_counts().reset_index()\nds.columns = ['hour', 'count']\nds['hour'] = 'Hour ' + ds['hour'].astype(str)\nfig = px.bar(\n    ds, \n    x=\"hour\", \n    y=\"count\", \n    orientation='v', \n    title='Tweets distribution over hours', \n    width=800\n)\nfig.show()","c21b422a":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ntweets_df = df.copy()\ntweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\ntweets_df = tweets_df.explode('hashtag')\ntweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ntweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\ntweets_df","d0bf1a3c":"ds = tweets_df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(20), \n    x=\"count\", \n    y='hashtag', \n    orientation='h', \n    title='Top 20 hashtags', \n    width=800, \n    height=700\n)\nfig.show()","ef3c4987":"df['tweet_length'] = df['text'].str.len()","b63b9ef9":"fig = px.histogram(\n    df, \n    x=\"tweet_length\", \n    nbins=80, \n    title='Tweet length distribution', \n    width=800,\n    height=700\n)\nfig.show()","0e739514":"ds = df[df['tweets_count']>=10]\nds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'tweets_count', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.tail(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    color='tweets_count',\n    orientation='h', \n    title='Top 40 users with the longest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","d817a215":"ds = df[df['tweets_count']>=10]\nds = ds.groupby('user_name')['tweet_length'].mean().reset_index()\nds.columns = ['user_name', 'mean_length']\nds = ds.sort_values(['mean_length'])\nfig = px.bar(\n    ds.head(40), \n    x=\"mean_length\", \n    y=\"user_name\", \n    orientation='h', \n    title='Top 40 users with the shortest average length of tweet (at least 10 tweets)', \n    width=800, \n    height=800\n)\nfig.show()","f67be02e":"def build_wordcloud(df, title):\n    wordcloud = WordCloud(\n        background_color='gray', \n        stopwords=set(STOPWORDS), \n        max_words=50, \n        max_font_size=40, \n        random_state=666\n    ).generate(str(df))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","55e94ddc":"build_wordcloud(df['text'], 'Prevalent words in tweets for all dataset')","c8f77999":"test_df = df[df['user_name']=='GlobalPandemic.NET']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for GlobalPandemic.NET')","ec1c0282":"test_df = df[df['user_name']=='covidnews.ch']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for covidnews.ch')","8df486c8":"test_df = df[df['user_name']=='Open Letters']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Open Letters')","8cfffae6":"test_df = df[df['user_name']=='Hindustan Times']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Hindustan Times')","71bcbe23":"test_df = df[df['user_name']=='Blood Donors India']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Blood Donors India')","2124e6f9":"vec = TfidfVectorizer(stop_words=\"english\")\nvec.fit(df['text'].values)\nfeatures = vec.transform(df['text'].values)","cdd6c84e":"kmeans = KMeans(n_clusters=2, random_state=0)\nkmeans.fit(features)","d278c911":"res = kmeans.predict(features)\ndf['Cluster'] = res\ndf","b9a5c9a0":"df[df['Cluster'] == 0].head(20)['text'].tolist()","53cc57c1":"df[df['Cluster'] == 1].head(20)['text'].tolist()","2b865334":"df['location'] = df['user_location'].str.split(',', expand=True)[1].str.lstrip().str.rstrip()\nres = df.groupby(['day', 'location'])['text'].count().reset_index()","78053ac3":"country_dict = {}\nfor c in countries:\n    country_dict[c.name] = c.alpha3\n    \nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nres = res.groupby(['day', 'location', 'alpha3'])['text'].sum().reset_index()\nres","b5d53031":"fig = px.choropleth(\n    res, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"text\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Tweets from different countries for every day',\n    width=800, \n    height=600\n)\nfig.show()","c156d79e":"### And see the values for new created column.","5f2223e4":"### Lets split hashtags into separate column.","0ba723ff":"### Now we will see top 40 users that like to use hashtags a little bit more than others. ","a38eedc1":"### And most friendly users","f5f8e379":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h3>\n\n* [1. Dataset Quick Overview](#1)\n* [2. Data Visualization](#2)\n* [3. Additional features analysis](#3)\n* [4. Tweets text analysis](#4)\n* [5. Simple sentiment analysis](#5)\n* [6. Animation with geographical distribution of tweets](#6)\n","f013d5ea":"<a id=\"2\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Data Visualization<\/center><\/h2>","fcd5cb67":"<a id=\"5\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Simple sentiment analysis<\/center><h2>\n\n### Lets do simple version of sentiment analysis. We just use Tfidf Vectorizer to get features and use Kmeans clustering algotithm to split data into 2 clusters.","60b01e57":"### Here I am going to show approach how to use plotly world map to demonstrate geographical distribution of tweets.","257abb6e":"### Lets see world clouds for top 5 users.","c6276adb":"### And show top 20 hashtags on tweets.","2c7e171f":"<a id=\"6\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Animation with geographical distribution of tweets<\/center><h2>","ea2274af":"### Let's see most popular users","bfa9e3d8":"<a id=\"1\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Dataset Quick Overview<\/center><h2>","080d9b84":"### Lets see top 40 most popular locations by the number of tweets","d029973b":"<a id=\"3\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Additional features analysis<center><h2>","da4ca798":"### Lets do the same but for hours","88f40fb8":"### As we can see from chart coronavirus increases the number of new twitter users ","eebe978b":"### Lets do a first quick check of our dataset","ff341e54":"### And also we can see the pie plot for the full picture about users locations","8036c950":"### Lets create new feature - `hashtags_count` that will show us how many hashtags in the current tweet.","bbef73e8":"### Lets see percent of NaNs for every column.","1ae992d9":"### Number of unique users per day","5315cbf0":"<h1><center>Covid19 tweets. EDA. Visualization. Insides.<\/center><\/h1>\n\n<center><img src=\"https:\/\/ichef.bbci.co.uk\/news\/1024\/cpsprodpb\/031C\/production\/_112869700_gettyimages-1209519827-1.jpg\"><\/center>","736a50dd":"<a id=\"4\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>Tweets text analysis<\/center><h2>\n\n### Here we are going to check the `text` feature of the dataset.\n### Lets see general wordcloud for this column.","514ff92d":"### Let's see how coronavirus affect to new users creation","523b16d4":"### Now we are going to calculate the length for every tweet in dataset.","2388e03f":"### Distribution of new feature over the number of tweets is expected - a lot of tweets with few number of hashtags and few tweets with huge number of hashtags.","514f85ef":"### Just split day and time into separate columns","18b48422":"### Now we are going to check how many tweets were for every day in our dataset.","b9289a2a":"### Now it's time to check last one categorical feature - `source`. Lets see top 40 sources by the number of tweets.","6a7ed9aa":"### Lets see top 40 users by number of tweets."}}