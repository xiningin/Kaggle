{"cell_type":{"8dee679d":"code","5756b907":"code","78af864c":"code","6561a3be":"code","ce87c59e":"code","219910d9":"code","000a8ed4":"code","c09eca65":"code","58e7a680":"code","b5fae301":"code","a74ab832":"code","e43d60dc":"code","94c9a795":"code","f46cba0b":"code","ae070dfb":"code","05df2e21":"code","76907d15":"code","ed0a990a":"code","273c8552":"code","a12d1ae9":"code","80edbb52":"code","2be1932d":"code","baecce4d":"code","a52e419f":"code","4236017a":"code","0cad7788":"code","33beb44a":"code","18191b47":"code","2a258b4d":"code","fb1b672b":"code","16545661":"code","6b488f3b":"code","412036eb":"code","f89c9421":"code","805a2877":"code","64e88a65":"code","0d3df75a":"code","6a0258ee":"code","b536995d":"code","41f5575f":"markdown","237d44cd":"markdown","1a0b17ab":"markdown","64751674":"markdown","1819935b":"markdown","2e441438":"markdown","f65e48af":"markdown","c8f67213":"markdown","3321cbc7":"markdown","0e858b70":"markdown","1d603d86":"markdown","62a5c6b9":"markdown","3186bafe":"markdown","f1c47f45":"markdown"},"source":{"8dee679d":"!pip install -q sentence_transformers","5756b907":"!pip install -q faiss-cpu","78af864c":"import pandas as pd\nimport numpy as np\nimport string\nimport random\nfrom collections import Counter\n\nfrom sentence_transformers import SentenceTransformer, util\nfrom fastai.collab import CollabDataLoaders, collab_learner\nimport faiss\n\nimport matplotlib.pyplot as plt","6561a3be":"# Display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 14})","ce87c59e":"# Variable to store title embeddings for repeated queries\ntitle_embeddings = None","219910d9":"# File paths\nbooks_path = '..\/input\/book-recommendation-dataset\/Books.csv'\nratings_path = '..\/input\/book-recommendation-dataset\/Ratings.csv'\nusers_path = '..\/input\/book-recommendation-dataset\/Users.csv'","000a8ed4":"def get_data() -> pd.DataFrame:\n    \"\"\"Function extracts data for books, ratings and users\n    from csv files, cleans data and joins data on ISBN and user ID.\n    Creates a global variable \"books\", that contains lookup dictionary\n    to find author and book title by content ID.\n    :return: Combined pd.DataFrame with all available data\n    \"\"\"\n    global books  # Global dict to look up authors and titles by content IDs\n\n    # Original data\n    books_df = pd.read_csv(books_path, low_memory=False)\n    books_df = clean_book_data(books_df)\n    print(f'Books data extracted: {len(books_df)} rows')\n\n    # Various editions of the same book have different ISBNs.\n    # We reindex the books so that identical title-author pairs\n    # get unique key, which will be used to look up content for users\n    # and estimate ratings and number of readings.\n    books_df = reindex_books(books_df)\n\n    ratings_df = pd.read_csv(ratings_path, low_memory=False)\n    print(f'Ratings data extracted: {len(ratings_df)} rows')\n\n    users_df = pd.read_csv(users_path, low_memory=False)\n    users_df = clean_users_data(users_df)\n    print(f'Users data extracted: {len(users_df)} rows')\n\n    df = pd.merge(books_df, ratings_df, on='ISBN')\n    print(f'Books and ratings data combined: {len(df)} rows')\n    df = pd.merge(df, users_df, on='User-ID')\n    print(f'Books, ratings and users data combined: {len(df)} rows')\n\n    # Assign values to the lookup dictionary\n    # (keys = content IDs, values = {\"Book-Title\": ..., \"Book-Author\": ...})\n    books_df = books_df.drop_duplicates(subset=['Content_ID']).set_index('Content_ID', drop=True)\n    books = books_df[['Book-Title', 'Book-Author']].to_dict('index')\n\n    return df\n\n\ndef clean_book_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function corrects errors and dtypes in books DataFrame\n    and fills in missing values.\n    :param df: Original DataFrame extracted from csv file\n    :return: Updated DataFrame\n    \"\"\"\n    # In several rows title column contains both title and author\n    # while next columns are shifted one step to the left.\n    errors_idx = df[df['Year-Of-Publication'].apply(len) > 4].index  # Filter by long string in year column\n    values_to_move = df.loc[errors_idx, 'Book-Author':'Image-URL-M'].values\n    df.loc[errors_idx, 'Year-Of-Publication':'Image-URL-L'] = values_to_move  # Move values one step to the right\n    splitted_text = df.loc[errors_idx, 'Book-Title'].str.split(';').apply(pd.Series)\n    df.loc[errors_idx, 'Book-Title':'Book-Author'] = splitted_text.values  # Split title and author\n    # Convert year column from object to integer\n    df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n    # For some books year of publication is incorrect (2030, 2050, etc.).\n    # Assume that it should be 1930, 1950, etc. Zero values are replaced by 2000.\n    df['Year-Of-Publication'] = df['Year-Of-Publication'].apply(\n        lambda x: x - 100 if x > 2020 else (2000 if x == 0 else x)\n    )\n    # Several missing values for author and publisher\n    df.fillna('N\/A', inplace=True)\n    # Transform each author name to capitalized format\n    df['Book-Author'] = df['Book-Author'].apply(lambda x: x.lower().title())\n    # Replace ampersands and capitalize book titles\n    df['Book-Title'] = df['Book-Title'].str.replace('&amp;', 'and').apply(lambda x: x.lower().title())\n\n    return df\n\n\ndef clean_users_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function adds country column to users data\n    and groups users by age.\n    :param df: Original DataFrame extracted from csv file\n    :return: Updated DataFrame\n    \"\"\"\n    # Extract the last value, which in most cases is the country name.\n    df['Country'] = df['Location'].apply(lambda x: x.split(',')[-1].strip())\n    # Country name contains misspellings and errors.\n    # We assume that names repeating 10 time and more\n    # are correct country identifiers.\n    countries = df['Country'].value_counts()\n    countries = set(countries[countries >= 10].index)\n    df['Country'] = df['Country'].apply(lambda x: x if x in countries else 'other')\n    df['Country'] = df['Country'].fillna('other')\n    # Remove punctuation\n    df['Country'] = df['Country'].apply(\n        lambda x: x.translate(str.maketrans('', '', string.punctuation))\n    )\n    df['Country'] = df['Country'].replace(' na', 'other').replace('na', 'other').replace('', 'other')\n\n    # Age column contains large number of outliers and missing values.\n    # Group users by age assuming that extreme values on both sides\n    # will merge into respective groups and will not affect the accuracy.\n    # Introduce a separate group for users of unknown age.\n    df['Age'] = df['Age'].fillna(-1)\n    df['Age_group'] = pd.cut(df['Age'], bins=[-2, 0, 6, 10, 15, 20, 35, 55, 250],\n                             labels=['unknown', 'preschool', 'primary_school',\n                                     'secondary_school', 'teenager', 'young_adult',\n                                     'adult', 'senior'])\n\n    return df\n\n\ndef reindex_books(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function creates 'Content_ID' column with a unique numeric key\n    for all identical author-title pairs in the books DataFrame.\n    'Content_ID' helps to identify identical content (same books\n    published in different years or by different publishers and having different ISBNs).\n    :param df: DataFrame with all available data\n    :return: Updated DataFrame\n    \"\"\"\n    # Drop author-title duplicates from the original DataFrame\n    # and create a dictionary where each author-title string\n    # corresponds to a unique numeric value.\n    unique = df[['Book-Title', 'Book-Author']].drop_duplicates().reset_index(drop=True).copy()\n    unique['pair'] = unique['Book-Author'] + unique['Book-Title']\n    print(f'Books reindexed: {len(unique)} unique content IDs created')\n    unique = dict(zip(unique['pair'], unique.index))\n    # Add a new column with unique key to the original DataFrame\n    df['Content_ID'] = (df['Book-Author'] + df['Book-Title']).apply(lambda x: unique[x])\n    return df\n\n\ndef recommend_by_ratings(df: pd.DataFrame,\n                         countries=None,\n                         age_groups=None,\n                         n_books: int = 10,\n                         threshold: int = 10) -> list:\n    \"\"\"Function estimates average ratings per book\n    and produces recommendations based on book ratings\n    taking into account the threshold - minimum number of readers\n    necessary to trust the average rating.\n    :param df: DataFrame with all available data\n    :param countries: Optional argument to select geographic location\n    (a list of countries should be passed)\n    :param age_groups: Optional argument to select age groups\n    (a list of groups should be passed)\n    :param n_books: Maximum number of books to recommend\n    :param threshold: Minimum required number of ratings per book to consider\n    :return: List of recommended content IDs (if both threshold and n_books are high,\n    total number of recommended books could be smaller than n_books)\n    \"\"\"\n    # If list of countries is passed, limit the database to selected region\n    if countries:\n        df = df[df['Country'].isin(countries)]\n\n    # If list of age groups is passed, limit the database to selected demographics\n    if age_groups:\n        df = df[df['Age_group'].isin(age_groups)]\n\n    # Drop all rows where rating=0 (user did not rate the book)\n    df = df[df['Book-Rating'] > 0]\n\n    # Calculate average rating and number of ratings per book\n    avg_ratings = df.groupby(by='Content_ID')['Book-Rating'].agg(['mean', 'count'])\n\n    # Drop books with small number of ratings\n    avg_ratings = avg_ratings[avg_ratings['count'] >= threshold]\n\n    # Get content IDs for books with the highest average ratings\n    avg_ratings.sort_values(by='mean', ascending=False, inplace=True)\n    recommendations = avg_ratings.head(n_books).index.to_list()\n    display_recommendations(recommendations)\n\n    return recommendations\n\n\ndef recommend_by_readings(df: pd.DataFrame,\n                          countries=None,\n                          age_groups=None,\n                          n_books: int = 10) -> list:\n    \"\"\"Function estimates total number of reading per book\n    and returns a list of content IDs for books with largest readership base.\n    :param df: DataFrame with all available data\n    :param countries: Optional argument to select geographic location\n    (a list of countries should be passed)\n    :param age_groups: Optional argument to select age groups\n    (a list of groups should be passed)\n    :param n_books: Maximum number of books to recommend\n    :return: List of recommended content IDs\n    \"\"\"\n    # If list of countries is passed, limit the database to selected region\n    if countries:\n        df = df[df['Country'].isin(countries)]\n\n    # If list of age groups is passed, limit the database to selected demographics\n    if age_groups:\n        df = df[df['Age_group'].isin(age_groups)]\n\n    # Count total number of readings per book\n    # regardless of whether the user gave any rating or not.\n    readings = df.groupby(by='Content_ID')['Book-Rating'].count()\n    recommendations = readings.sort_values(ascending=False).head(n_books).index.to_list()\n    display_recommendations(recommendations)\n\n    return recommendations\n\n\ndef display_recommendations(content: list):\n    \"\"\"Function prints a list of recommended books.\n    :param content: List of content IDs\n    \"\"\"\n    print('Recommended books:')\n    for i, ID in enumerate(content):\n        print(f'{i + 1}. {books[ID][\"Book-Author\"]}: {books[ID][\"Book-Title\"]}')\n\n\ndef select_user(df: pd.DataFrame):\n    \"\"\"Function selects a random user.\n    :param df: DataFrame with all available data\n    :return: User ID\n    \"\"\"\n    user = random.choice(df['User-ID'])\n    print(f'Selected user: ID = {user}')\n    return user\n\n\ndef get_user_data(df: pd.DataFrame, user, titles_only=False) -> tuple:\n    \"\"\"Function extracts user's data by user ID, filters out disliked books\n    and returns either a list of book titles or a tuple with authors names\n    and content IDs.\n    :param df: DataFrame with all available data\n    :param user: User ID\n    :param titles_only: Boolean parameter specifying if the function\n    should return only a list of book titles\n    :return: Tuple with 2 elements (array of all authors read by user,\n    set of all books IDs) or a list of all book titles sorted by ratings\n    \"\"\"\n    user_data = df[df['User-ID'] == user].copy()\n    print(f'User activity: {len(user_data)} logs')\n\n    # Drop obviously disliked items rated between 1 and 4\n    user_data = user_data[(user_data['Book-Rating'] == 0)\n                          | (user_data['Book-Rating'] > 4)]\n    print(f'Without disliked books user history contains {len(user_data)} items')\n\n    # Return only book titles\n    if titles_only:\n        user_data.sort_values(by='Book-Rating', ascending=False, inplace=True)\n        return user_data['Book-Title'].to_list()\n\n    # Books IDs and authors read by user\n    book_ids = set(user_data['Content_ID'])\n    authors = user_data['Book-Author'].unique()\n\n    return authors, book_ids\n\n\ndef recommend_by_activity(df: pd.DataFrame, user_id=None, limit=10):\n    \"\"\"Function selects books by authors most read by the user,\n    with the exception of books user already read.\n    :param df: DataFrame with all available data\n    :param user_id: Optional argument for user ID (if not provided, will be randomly selected)\n    :param limit: Optional argument to limit total number of recommendations\n    :return: List of recommended content IDs\n    \"\"\"\n    user = user_id or select_user(df)\n    # Get all previous logs for the user\n    user_authors, user_books = get_user_data(df, user)\n    # Query book IDs by authors and drop all books the user already read\n    recommendations = set(books_by_authors(df, user_authors)).difference(user_books)\n    recommendations = list(recommendations)[:limit]\n    display_recommendations(recommendations)\n    return recommendations\n\n\ndef books_by_authors(df: pd.DataFrame, authors: list) -> list:\n    \"\"\"Function selects content IDs by authors names.\n    :param df: DataFrame with all available data\n    :param authors: List of authors names\n    :return: List of content IDs in random order\n    \"\"\"\n    df = df[df['Book-Author'].isin(authors)].drop_duplicates(subset=['Content_ID']).sample(frac=1.)\n    recommendations = df['Content_ID'].to_list()\n    return recommendations\n\n\ndef get_user_attributes(df: pd.DataFrame, user) -> tuple:\n    \"\"\"Function finds basic user attributes:\n    indicated country of residence and age group.\n    :param df: DataFrame with all available data\n    :param user: User ID\n    :return: Tuple with 2 values (country, age group)\n    \"\"\"\n    country, age_group = df[df['User-ID'] == user].iloc[0, :][['Country', 'Age_group']]\n    print(f'User region: {country}')\n    print(f'User age group: {age_group}')\n    return country, age_group\n\n\ndef recommend_by_user_attributes(df: pd.DataFrame, user_id=None, n_books=10) -> tuple:\n    \"\"\"Function produces generic recommendations\n    based on user location and age group.\n    :param df: DataFrame with all available data\n    :param user_id: User ID\n    :param n_books: Maximum number of books to recommend\n    :return: Tuple with 2 lists of recommended content IDs (by readings, by ratings)\n    \"\"\"\n    user = user_id or select_user(df)\n    country, age = get_user_attributes(df, user)\n\n    # Get generic recommendations based on user location and age group\n    recommendations_1 = recommend_by_readings(\n        df, countries=[country], age_groups=[age], n_books=n_books)\n    recommendations_2 = recommend_by_ratings(\n        df, countries=[country], age_groups=[age], n_books=n_books)\n\n    return recommendations_1, recommendations_2\n\n\ndef recommend_by_title_similarity(df: pd.DataFrame,\n                                  user_id=None,\n                                  n_titles: int = 5,\n                                  top_n: int = 10):\n    \"\"\"Function produces embeddings for book titles\n    and performs semantic similarity search using a list\n    of book titles passed as a query.\n    :param df: DataFrame with all available data\n    :param user_id: User ID\n    :param n_titles: Number of titles to select from user history\n    :param top_n: Number of similar titles to search for\n    :return: Map object containing recommended book titles\n    \"\"\"\n    global title_embeddings\n\n    user = user_id or select_user(df)\n\n    query = get_user_data(df, user, titles_only=True)\n    print(f'Top-5 books with highest ratings:')\n    query = query[:n_titles]\n    for title in query:\n        print(title)\n\n    # All unique content IDs and titles\n    df = df[['Content_ID', 'Book-Title']].drop_duplicates().reset_index(drop=True).copy()\n\n    # Pretrained NLP model\n    model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n\n    # If the function was called for the 1st time,\n    # we have to convert all titles into embeddings.\n    if title_embeddings is None:\n        title_embeddings = model.encode(df['Book-Title'].to_list(), convert_to_tensor=True)\n        print(f'Book titles converted to embeddings. Shape: {title_embeddings.shape}')\n        query_embeddings = model.encode(query, convert_to_tensor=True)\n        print(f'Query titles converted to embeddings. Shape: {query_embeddings.shape}')\n\n    # For repeated calls when embeddings are already available\n    else:\n        indexes = df[df['Book-Title'].isin(query)].index\n        query_embeddings = title_embeddings[indexes]\n        print(f'Embedding for query titles selected. Shape: {query_embeddings.shape}')\n\n    # For every title in the original query extract indexes of similar titles\n    recommendations = []\n    result = util.semantic_search(query_embeddings, title_embeddings, top_k=top_n)\n    for query in result:\n        for similar in query:\n            # Do not recommend identical titles\n            # and title with low similarity score.\n            if 0.75 < similar['score'] < 0.95:\n                recommendations.append(similar['corpus_id'])\n\n    # Replace indexes by respective titles\n    recommendations = map(lambda x: df.loc[x, 'Book-Title'], recommendations)\n    print('Semantically similar titles:')\n    for title in recommendations:\n        print(title)\n\n    return recommendations\n\n\ndef vectorize_users(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function produces users embeddings\n    based on reading history, age and location.\n    Readings of popular authors are used as preferences features.\n    Age group and location are one-hot-encoded and added to preferences.\n    :param df: DataFrame with all available data\n    :return: DataFrame with users embeddings of shape n_users x n_features\n    \"\"\"\n    # Select 2000 most popular authors\n    popular_authors = df['Book-Author'].value_counts().nlargest(2000)\n    popular_authors = set(popular_authors.index)\n    print(f'Identified {len(popular_authors)} popular authors')\n\n    # Group less popular authors in a separate category and create a pivot table\n    df['Book-Author'] = df['Book-Author'].apply(lambda x: x if x in popular_authors else 'other')\n    users = df.pivot_table(index='User-ID', columns='Book-Author',\n                           values='Book-Rating', aggfunc='count')\n    users = users.fillna(0)\n\n    # To get a normalized representation of users' preferences\n    # we divide number of readings per popular author\n    # by the total number of reading by each user.\n    shape = users.shape\n    total = users.sum(axis=1)\n    total_arr = np.repeat(total.values, shape[1]).reshape(shape)\n    index = users.index\n    columns = users.columns\n    users = pd.DataFrame(users.values \/ total_arr, index=index, columns=columns)\n    print(f'User embeddings based on authors extracted. Shape: {users.shape}')\n\n    # Drop duplicate rows from the original DataFrame\n    # and sort users in the same order as in embeddings array\n    df = df.drop_duplicates(subset=['User-ID']).sort_values(by='User-ID').set_index('User-ID')\n    # Encode age categories and countries\n    users = pd.concat(\n        (users, pd.get_dummies(df['Age_group']), pd.get_dummies(df['Country'])),\n        axis='columns'\n    )\n    print(f'Age and location added. Shape: {users.shape}')\n\n    return users\n\n\ndef select_unread_books(df: pd.DataFrame,\n                        sim_users: np.array,\n                        read_books: set,\n                        limit: int = 100) -> list:\n    \"\"\"Function selects books that were read and were not\n    explicitly disliked by 'sim_users', which are not present\n    in 'read_books'.\n    :param df: DataFrame with all available data\n    :param sim_users: Array of IDs representing similar users\n    :param read_books: Set of content IDs from the history of query user\n    :param limit: Maximum number of books to select from similar users logs\n    :return: List of recommended content IDs\n    \"\"\"\n    # Select books read by similar users, except the books rated between 1 and 4\n    # (obviously disliked)\n    u_books = df[\n        (df['User-ID'].isin(sim_users))\n        & ((df['Book-Rating'] >= 5) | (df['Book-Rating'] == 0))\n        ].copy()\n    print(f'Selected {len(u_books)} logs except books rated 1 through 4')\n\n    # Add a frequency column for read authors\n    u_books['Readings_per_author'] = (\n        u_books.groupby('User-ID')  # for each user\n        ['Book-Author'].transform('count')  # count number of readings per each author\n    )\n\n    # Filter out books with 0 rating if the user read only one book of this author\n    drop_idx = u_books[(u_books['Readings_per_author'] == 1) & (u_books['Book-Rating'] == 0)].index\n    u_books.drop(drop_idx, inplace=True)\n    print(f'Selected {len(u_books)} logs except 0-rated books with one reading per author')\n    # Count readings per unique 'Content_ID'\n    u_books = Counter(u_books['Content_ID'])\n    n_items = len(u_books)\n    print(f'Total number of unique content IDs: {n_items}')\n    # Reduce total number of selected books if necessary\n    if n_items > limit:\n        u_books = pd.DataFrame({'Content_ID': u_books.keys(), 'n_users': u_books.values()})\n        u_books.sort_values(by='n_users', ascending=False, inplace=True)\n        u_books = u_books.head(limit)['Content_ID'].values\n        print(f'Reduced the number of unique content IDs to {limit}')\n    else:\n        u_books = u_books.keys()\n\n    # Select the books the query user did not read\n    recommendations = list(set(u_books).difference(read_books))\n    print(f'Total number of recommended content IDs after dropping read books: {len(recommendations)}')\n\n    return recommendations\n\n\ndef recommend_by_similar_users(user_id: int,\n                               df: pd.DataFrame,\n                               embeddings: pd.DataFrame,\n                               top_k: int = 11) -> list:\n    \"\"\"Function searches 'embeddings' for vectors most similar\n    to the 'user_id', selects books that were read and were not\n    explicitly disliked by similar users, which are not present\n    in the 'user_id' history.\n    :param user_id: Integer ID for the user\n    :param df: DataFrame with all available data\n    :param embeddings: DataFrame of embeddings for all users\n    :param top_k: Maximum number of similar user vectors to search\n    :return: List of recommended content IDs\n    \"\"\"\n    # Array representing the query user\n    query = embeddings.loc[user_id, :].values\n    # Information about the query user\n    authors, read_books = get_user_data(df, user_id)\n    print(f'User {user_id} read authors:', authors)\n\n    # Search for top_k vectors most similar to the query\n    # (returns a pd.DataFrame with 2 columns: 'corpus_id' and 'score')\n    similar_users = pd.DataFrame(\n        util.semantic_search(query, embeddings.values, top_k=top_k)[0]\n    )\n    # Add users IDs finding them by row indexes\n    similar_users['User-ID'] = embeddings.iloc[similar_users['corpus_id'], :].index\n    print(f'Similar users:\\n{similar_users}')\n\n    # Drop 1st row which represents the user that was used as a query\n    # (self-match with similarity score=1.0)\n    similar_users = similar_users.iloc[1:, :]['User-ID']\n\n    # Select books from similar users histories except disliked books,\n    # one-off accidental readings and books present in current user's logs\n    recommendations = select_unread_books(df, similar_users, read_books)\n    display_recommendations(recommendations)\n\n    return recommendations\n\n\ndef recommend_by_similar_users_faiss(index,\n                                     user_id: int,\n                                     df: pd.DataFrame,\n                                     embeddings: pd.DataFrame,\n                                     top_k: int = 11) -> list:\n    \"\"\"\n    :param index: FAISS index of type IndexFlatL2\n    :param user_id: Integer ID for the user\n    :param df: DataFrame with all available data\n    :param embeddings: DataFrame of embeddings for all users\n    :param top_k: Maximum number of similar user vectors to search\n    :return: List of recommended content IDs\n    \"\"\"\n    # Information about the query user\n    authors, read_books = get_user_data(df, user_id)\n    print(f'User {user_id} read authors:', authors)\n\n    # User vector\n    query_vector = embeddings.loc[user_id, :].values.astype('float32').reshape(1, -1)\n    # Search for 10 most similar indexes for the user's vector (+self-match)\n    matched_emb, matched_indexes = index.search(query_vector, top_k)  # Row indexes of similar users\n    # Get users IDs by row indexes\n    similar_ids = embeddings.iloc[matched_indexes[0], :].index\n    similar_ids = similar_ids[1:]  # Drop query user (self-match)\n    print(f'Similar users:\\n{similar_ids}')\n\n    # Select books from similar users histories except disliked books,\n    # one-off accidental readings and books present in current user's logs\n    recommendations = select_unread_books(df, similar_ids, read_books)\n    display_recommendations(recommendations)\n\n    return recommendations","c09eca65":"# Extract all data and combine into a single DataFrame\ndata = get_data()","58e7a680":"# Content analysis\nfor column in ('ISBN', 'Book-Author', 'Publisher', 'Book-Title'):\n    print('-' * 100)\n    print(f'{column}: {data[column].nunique()} unique values')\n    counts = data[column].value_counts().nlargest(20)\n    plt.bar(counts.index, counts.values)\n    plt.xticks(rotation=90)\n    plt.title(f'{column} Most Frequent Values')\n    plt.tight_layout()\n    plt.show()\n    print('Top-20 most frequent values:')\n    print(counts)","b5fae301":"# Distribution of publication year for unique ISBN\nplt.hist(data.drop_duplicates(subset=['ISBN'])['Year-Of-Publication'],\n         bins=[1900, 1950, 1960, 1970, 1980, 1990, 1995, 2000, 2005])\nplt.title('Book Novelty: Year of Publication')\nplt.show()","a74ab832":"# Ratings are from 1 to 10. 0 means the user did not rate the book.\nplt.hist(data['Book-Rating'], bins=11)\nplt.title('Ratings Distribution')\nplt.show()","e43d60dc":"# Number of ratings per user\nplt.hist(data.groupby(by='User-ID')['Book-Rating'].count(),\n         bins=30, log=True)\nplt.title('Number of Ratings per User')\nplt.show()","94c9a795":"# Users by country\nprint(f'{data[\"Country\"].nunique()} countries')\ncounts = data.drop_duplicates(subset=['User-ID'])['Country'].value_counts().nlargest(20)\nplt.bar(counts.index, counts.values)\nplt.xticks(rotation=90)\nplt.title(f'Users Locations (Most Frequent)')\nplt.tight_layout()\nplt.show()","f46cba0b":"# Users by age\ncounts = data.drop_duplicates(subset=['User-ID'])['Age_group'].value_counts()\ngroups = ['unknown', 'preschool', 'primary_school', 'secondary_school',\n          'teenager', 'young_adult', 'adult', 'senior']\nplt.bar(groups, counts[groups].values)\nplt.xticks(rotation=90)\nplt.title(f'Age Groups')\nplt.tight_layout()\nplt.show()","ae070dfb":"# 10 most popular books, i.e. books with the largest readership base\nresult = recommend_by_readings(data)","05df2e21":"# 10 most popular books in Germany\nresult = recommend_by_readings(data, countries=['germany'])","76907d15":"# 50 most popular books\nresult = recommend_by_readings(data, n_books=50)","ed0a990a":"# 10 most popular books for senior age group\nresult = recommend_by_readings(data, age_groups=['senior'])","273c8552":"# Get recommendation by highest average book ratings.\n# Default settings: 10 books with at least 10 ratings.\nresult = recommend_by_ratings(data)","a12d1ae9":"# 10 highest rated books with at least 100 ratings.\nresult = recommend_by_ratings(data, threshold=100)","80edbb52":"# When both threshold and required number of recommendations are set too high,\n# the function returns as many books as it can find.\n# Query for 30 highest rated books with at least 500 ratings.\nresult = recommend_by_ratings(data, n_books=30, threshold=500)","2be1932d":"# Top-10 most highly rated books in Spain and Italy\nresult = recommend_by_ratings(data, countries=['spain', 'italy'])","baecce4d":"# Top-10 most highly rated books for teenagers and school age children\nresult = recommend_by_ratings(\n    data, age_groups=['primary_school', 'secondary_school', 'teenager']\n)","a52e419f":"# Top-10 most highly rated books for 20-35 year-olds in France\nresult = recommend_by_ratings(\n    data, age_groups=['young_adult'], countries=['france']\n)","4236017a":"# Identify user's location and age group and produce\n# generic recommendations based on these attributes.\n# 1st list is based on the total readings in the same geography and age group.\n# 2nd list is based on average ratings in the same geography and age group.\nres_1, res_2 = recommend_by_user_attributes(data)","0cad7788":"# For other random user\nres_1, res_2 = recommend_by_user_attributes(data)","33beb44a":"# Select random user and produce recommendations\n# querying most read and liked authors\nresult = recommend_by_activity(data)","18191b47":"# Other random user\nresult = recommend_by_activity(data)","2a258b4d":"# Select random user and produce recommendations based on title similarity\nresult = recommend_by_title_similarity(data)","fb1b672b":"# Recommendations for other random user\nresult = recommend_by_title_similarity(data)","16545661":"# Vector representation for all users\nusers_vectors = vectorize_users(data)","6b488f3b":"# Select random user\nuser = select_user(data)\n\n# Select 10 most similar users (search for 11 most similar vectors\n# because the result includes the user in question - self match)\nresult = recommend_by_similar_users(user, data, users_vectors, top_k=11)","412036eb":"# Recommendations for other random user\nuser = select_user(data)\nresult = recommend_by_similar_users(user, data, users_vectors, top_k=11)","f89c9421":"# Create FAISS index from all user embeddings\nn_dimensions = users_vectors.shape[1]\nfast_index = faiss.IndexFlatL2(n_dimensions)\nfast_index.add(users_vectors.values.astype('float32', order='C'))","805a2877":"# Select 10 most similar users (default top_k)\nresult = recommend_by_similar_users_faiss(fast_index, user, data, users_vectors)","64e88a65":"# Select 3 most similar users (+self-match)\nresult = recommend_by_similar_users_faiss(fast_index, user, data, users_vectors, top_k=4)","0d3df75a":"data_col = data[data['Book-Rating'] > 0]\nprint(f'Number of samples before dropping 0 values: {len(data)}\\n'\n      f'Number of samples after dropping 0 values: {len(data_col)}')","6a0258ee":"# Prepare data for the model\ndls = CollabDataLoaders.from_df(data_col[['User-ID', 'ISBN', 'Book-Rating']],\n                                bs=64, valid_pct=0.1)\n\n# Create a collaborative model and train for 5 epochs\nlearner = collab_learner(dls, y_range=(1, 10))\nlearner.fine_tune(5)","b536995d":"# Display predicted and actual ratings for user-book pairs\nlearner.show_results()","41f5575f":"## Recommendation algorithm based on user activity\n\nRecommendations based on user history:\n- Query by author: identify authors that the user previously read and did not\n  explicitly disliked and recommend other books by the same authors\n\nThis rules based approach takes into account user behaviour and is the most personalized.","237d44cd":"After 5 epochs of training validation loss stalls. If training continues, the model just overfits on the train data without any considerable improvement on the validation set.\n\nValidation error is about 3.8, which is high taking into account that ratings vary between 1 and 10. This magnitude of errors could easily result in recommending the books that the user wouldn't like and not recommending books that in reality would get high ratings.","1a0b17ab":"# Recommendation Algorithms for Books\nData is loaded from 3 csv files (for books, users and ratings). Original data includes user ID, age and location, book author, title, publisher, year of publication and ISBN, book ratings by users (1 to 10 or 0 if user did not rate the book),\nURLs to cover images in 3 sizes.\n\nSeveral algorithms are applied to produce recommendations:\n- Generic recommendations based on the majority vote (number of readings and average rating per book)\n- Cohorts based recommendations taking into account user age and location\n- Recommendations based on user activity logs (select unread books of previously read authors)\n- Recommendations based on users vectors and querying logs of similar users\n- Collaborative filtering using fastai library to predict book ratings","64751674":"## Imports and settings","1819935b":"### Simple majority vote","2e441438":"### Cohorts + majority vote","f65e48af":"### Use faiss library for faster index search","c8f67213":"## Exploratory Data Analysys","3321cbc7":"## Generic recommendation algorithms\n\nGeneric recommendations algorithms:\n- By book popularity: recommend books with largest number of readings\n- By book ratings: recommend books with highest average ratings\n\nGeneric recommendations augmented by user attributes:\n- Cohorts based approach: identify user location and age group\n  and obtain generic recommendation for this geography and demographics\n\nGeneric and cohorts based algorithms could be marked as \"popular\" or \"trending\"\nto inform the user that they are not actually personalized.","0e858b70":"## Collaborative recommendation algorithm\nCollaborative filtering based on fastai.collab library. Model predicts book ratings in the range between 1 and 10.\n\nSamples with 0 ratings where user did not rate the book are not used in training and testing.\n\nRatings histogram show that 0 ratings prevail in the dataset. Absence of a rating does not mean that the user did not like the book. Actual ratings values vary between 1 and 10. Assigning average or median rating values to the majority of samples distorts the data and leads to inaccurate and unreliable recommendation model.","1d603d86":"## Functions","62a5c6b9":"## Data extraction and preprocessing\nData preprocessing steps:\n- Correction of technical errors and misspellings in the original data (incorrect year of publication, misplaced column values, outlier values in user age, nonconventional descriptions of user location).\n- Reindexing of books. Various editions of the same book have different ISBNs. Unique content IDs help to avoid recommending the same content (same books published in different periods and by different publishers) to users who already rated or read it.\n- Merging all data into a single DataFrame.","3186bafe":"## Similarity search based on user vectors\nRecommendation algorithm based on vectorization of users and querying reading history of similar users.\n\nAlgorithm steps:\n1. Transform information about the users into 1D vectors:\n   - Count the number of times each user read most popular authors (2000 most popular authors are counted, all other authors are counted together in a joint feature-group)\n   - Normalize this data by total number of readings by each user to get a distribution for each user (vectors contain values between 0 and 1).\n   - Add one-hot-encoded data on user age group ('unknown', 'preschool', 'primary_school', 'secondary_school', 'teenager', 'young_adult', 'adult', 'senior') and location (country)\n2. Search for similar users based on cosine similarity of vectors.\n3. Select books that similar users read and did not explicitly disliked.\n4. Discard accidental items (books with 0 ratings where similar user read the author only once).\n5. To limit total number of recommendations sort all previously selected books by the number of readings by similar users and select top-100.\n6. Drop the books that the user in question had already read.\n\nThis algorithm produces a version of collaborative filtering recommendations.\n\nAdvantages:\n- Interpretability: selection rules are explicitly defined and could be fine-tuned\n- Speed of search: in production environment vectors matrix for all users could be\n  updated once a day or once a week and stored in a database or in cache.\n  For larger databases faiss library could be used to index user vectors and search\n  faster using CPU or GPU.\n\nDisadvantages:\n- Poor quality of recommendations for users with limited reading history.\n  New users or users with preferences not matching popular list of authors\n  could get irrelevant recommendations.\n- Limited information about users in the original data and small number of meaningful attributes.","f1c47f45":"## Content based recommendation algorithm\nBased on the user history the algorithm selects most highly rated books\nand searches for semantically similar titles using pretrained NLP model\n(Distil RoBERTa from sentence_transformers library).\n\nIf the original data contained features like book genre, language or text length,\nit would be possible to produce more meaningful recommendations based on content clustering.\nSemantic similarity search could produce diversifying but unexpected recommendations.\n\nDisadvantages:\n- Algorithm often matches identical books with minor differences in titles (punctuation or description of cover type added to the title in parenthesis).\n- Depending on the degree of text similarity results can be irrelevant and annoying to the user.\n- Algorithm does not take into account that for some types of specialized content (like language dictionaries) user who already obtained one item does not need similar items of the same type."}}