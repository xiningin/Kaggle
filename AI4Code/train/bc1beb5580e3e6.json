{"cell_type":{"1413184b":"code","23ee2cd4":"code","d1a146f8":"code","bcef2907":"code","3b967aa2":"code","4013d5f1":"code","b3b7e272":"code","c9e35c8e":"code","960929b5":"code","64677073":"code","3b5e1927":"code","77e5d223":"markdown","9caffa57":"markdown","9048a5ea":"markdown","35b30736":"markdown","f2b82ea6":"markdown","9f9794dd":"markdown","e5f5d852":"markdown"},"source":{"1413184b":"!pip install beautifulsoup4 # Run this command to install beautifulsoup","23ee2cd4":"# Load python packages\nfrom urllib.request import urlopen\nfrom urllib.error import HTTPError\nfrom bs4 import BeautifulSoup\nimport json\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt","d1a146f8":"# Define functions for parsing and collecting data\n\ndef get_listings(url):\n    \"Get a list of auction results\"\n    try:\n        html = urlopen(url)\n    except HTTPError as e:\n        return None    \n    try:\n        # Parsing data\n        bs = BeautifulSoup(html.read(), 'html.parser')\n        articles = bs.findAll('article', {'class':'css-3xqrp1'})\n        for atc in articles:\n            for c in atc.children:\n                if c.name == 'header':\n                    suburb = c.h3.text\n                if c.name == 'ul':\n                    getL_listing_suburb(c, suburb)\n    except AttributeError as e:\n        return None\n    \ndef getL_listing_suburb(tag, suburb=None):\n    \" Get a list in each suburb \"\n    ladd,lagen,htype,hInfo,soldInfo,price = tuple([\"Unknown\" for i in range(6)])\n    listing = list(tag.children)\n    ladd  = listing[0].text\n    if listing[1].name == 'li':\n        htype,hInfo = get_house_info(listing[1])\n    if listing[2].name == 'li':\n        soldInfo,price = get_sold_info(listing[2])\n    if listing[3].name == 'li':\n        lagen = listing[3].text\n    listings.append( {'suburb':suburb, 'street':ladd, 'agent':lagen, 'type': htype, 'info': hInfo, \n                      'sold':soldInfo, 'price':price})\n    \ndef get_sold_info(tag):\n    sold = list(tag.children)\n    if len(sold) >= 2: return sold[0].text, sold[1].text\n    else: return sold[0].text, \"Unknown\"\n\ndef get_house_info(tag):\n    house = list(tag.children)\n    if len(house) >= 2: return house[0].text, house[1].text\n    else: return house[0].text, \"Unknown\"","bcef2907":"listings = [] # To store all listings\nurl = \"https:\/\/www.domain.com.au\/auction-results\/melbourne\/\"\nget_listings(url)","3b967aa2":"len(listings) # There are 1622 listings","4013d5f1":"# Show first five listings\nlistings[1:5]","b3b7e272":"# Store into a json file for further analysis\nwith open('listings.json', 'w') as f:\n    json.dump(listings, f)","c9e35c8e":"df = pd.read_json('listings.json')\ndf.shape","960929b5":"# Check first 10 rows\ndf.head(10)","64677073":"df['type'].value_counts().plot(kind='bar')","3b5e1927":"df.to_csv(\"listings.csv\", index=False)","77e5d223":"#### Required python packages\n> We need urllib for connect to the website, and BeautifulSoup for parsing html sources.\n> After that, the data can be stored in a json file.","9caffa57":"### Check results","9048a5ea":"### Get listings from Melbourne and store it","35b30736":"### Save into .csv file","f2b82ea6":"### Read .json file for further analysis","9f9794dd":"# Scraping Melbourne housing prices by Beautiful Soup\n> In this tutorial we will scrap housing price in Melbourne.","e5f5d852":"### Save results into a .json file"}}