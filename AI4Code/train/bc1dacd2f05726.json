{"cell_type":{"d1cc011a":"code","d80bfacb":"code","b279732d":"code","5297b85e":"code","95bd90bc":"code","c71782c1":"code","6825e2f1":"code","ba85c654":"code","ae5341ec":"code","e293064e":"code","27d26108":"code","e35a6988":"code","2601f680":"code","13db3eb9":"code","81084585":"code","042eab6d":"code","51c2dd78":"code","5268ebdb":"code","f452dd41":"code","11f9cae8":"code","1c994dc5":"code","95d2d304":"code","e1d2e17e":"code","f2ee4531":"code","faaa8a2e":"code","5bb45748":"code","e4b312c9":"code","cc1ceda5":"markdown","fa453123":"markdown","dcd3c700":"markdown","21e3418f":"markdown","4b383e3f":"markdown","972fab0a":"markdown","e1c4c0c5":"markdown","2fa6bce8":"markdown","7f30e0d8":"markdown","29ac6771":"markdown","c7582cdb":"markdown","ef07b7f5":"markdown","43c95ef9":"markdown","69d122a3":"markdown","cdd9cb22":"markdown"},"source":{"d1cc011a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d80bfacb":"train_df = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-job-a-thon-may-2021\/train_s3TEQDk.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-job-a-thon-may-2021\/test_mSzZ8RL.csv\")","b279732d":"train_df.info()","5297b85e":"summary = pd.DataFrame(train_df.dtypes)\nsummary[\"unique\"] = train_df.nunique()\nsummary[\"null_vals\"] = train_df.isnull().sum()\nsummary[\"first\"] = train_df.loc[0,:]\nsummary[\"second\"] = train_df.loc[1,:]\nsummary[\"third\"] = train_df.loc[2,:]\nsummary","95bd90bc":"train_df[\"Credit_Product\"] = train_df[\"Credit_Product\"].fillna(\"No\")\ntest_df[\"Credit_Product\"] = test_df[\"Credit_Product\"].fillna(\"No\")","c71782c1":"train_df = train_df.drop(\"ID\", axis = 1)\ntest_df = test_df.drop(\"ID\", axis = 1)","6825e2f1":"cat_vals = train_df.select_dtypes(include=\"object\").columns.to_list()\nnum_vals = [cols for cols in train_df.columns.to_list() if cols not in cat_vals]","ba85c654":"def get_row_col_idx(idx):\n    \"\"\"get the row and column index from the index for plots\"\"\"\n    row_idx = idx\/\/2\n    col_idx = [0 if idx%2 == 0 else 1]\n    return (row_idx, col_idx[0])","ae5341ec":"def  write_percent(ax):\n    \"\"\"Writes the percentage on top of the bar\"\"\"\n    total_size = len(train_df)\n    for patch in ax.patches:\n        height = patch.get_height()\n        width = patch.get_width()\n        x_loc = patch.get_x()\n        percent = (height\/total_size) * 100\n        ax.text(x_loc+ width\/2.0, height, '{:1.1f}%'.format(percent), ha = \"center\")","e293064e":"target = train_df.pop(\"Is_Lead\")","27d26108":"fig, axes = plt.subplots(3, 2, figsize = (20,20))\nfor idx, val in enumerate(train_df[cat_vals].columns.to_list()):\n    row_idx, col_idx = get_row_col_idx(idx)\n    sns.countplot(data = train_df,\n                palette=\"Set3\",\n                x = val,\n                ax = axes[row_idx, col_idx] )\n    ax = axes[row_idx, col_idx]\n    write_percent(ax)","e35a6988":"fig, axes = plt.subplots(3, 2, figsize = (20,20))\nfor idx, val in enumerate(test_df[cat_vals].columns.to_list()):\n    row_idx, col_idx = get_row_col_idx(idx)\n    sns.countplot(data = test_df,\n                palette=\"Set3\",\n                x = val,\n                ax = axes[row_idx, col_idx] )\n    ax = axes[row_idx, col_idx]\n    write_percent(ax)","2601f680":"sns.catplot(x = target, data = train_df, kind = \"count\", height = 4, palette=\"Set3\") ","13db3eb9":"fig, axes = plt.subplots(1,3, figsize=(30, 10))\nfor idx, val in enumerate([\"Age\", \"Vintage\", \"Avg_Account_Balance\"]):\n    sns.histplot(x=val, data = test_df, bins=200, color = \"red\", ax = axes[idx])","81084585":"fig, axes = plt.subplots(1,3, figsize=(30, 10))\nfor idx, val in enumerate([\"Age\", \"Vintage\", \"Avg_Account_Balance\"]):\n    sns.histplot(x=val, data = train_df, bins=200, color = \"red\", ax = axes[idx])","042eab6d":"fig, axes = plt.subplots(3, 2, figsize = (20,20))\nfor idx, val in enumerate(train_df[cat_vals].columns.to_list()):\n    row_idx, col_idx = get_row_col_idx(idx)\n    sns.countplot(data = train_df,\n                palette=\"Set3\",\n                x = val,\n                hue = target,\n                ax = axes[row_idx, col_idx] )\n    ax = axes[row_idx, col_idx]\n    write_percent(ax)","51c2dd78":"fig, axes = plt.subplots(1,3, figsize=(30, 10))\nfor idx, val in enumerate([\"Age\", \"Vintage\", \"Avg_Account_Balance\"]):\n    sns.histplot(x=val, data = train_df, bins=200, hue = target, color = \"Pink\", ax = axes[idx])","5268ebdb":"def numerisize(feats, df):\n    for feature in feats:\n        df[feature] = df[feature].cat.codes","f452dd41":"for cols in cat_vals:\n    train_df[cols] = train_df[cols].astype(\"category\")\n    test_df[cols] = test_df[cols].astype(\"category\")\n    \nnumerisize(cat_vals, train_df)\nnumerisize(cat_vals, test_df)","11f9cae8":"corrMatrix = train_df.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","1c994dc5":"# train_df = train_df.drop(\"Channel_Code\", axis = 1)","95d2d304":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_score, confusion_matrix, roc_auc_score, recall_score, plot_roc_curve\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(train_df, target, random_state=0, stratify=target, test_size = 0.2)","e1d2e17e":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ntest_df = scaler.transform(test_df)","f2ee4531":"model= LogisticRegression()\nmodel.fit(X_train, y_train)\n\nprint(roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]))\nprint(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n\n# plot_roc_curve(y_train, y_pred_train)\n# plot_roc_curve(y_test, y_pred)","faaa8a2e":"# from sklearn.model_selection import GridSearchCV\n\n# parameters = {\n#     \"n_estimators\": [100, 150, 200], \n#     \"max_depth\": [5, 10, 15, 25],\n#     \"min_samples_leaf\": [3, 5] \n# }\n\n# model_random_forest = RandomForestClassifier(\n#     random_state = 1,\n#     class_weight='balanced',\n# )\n\n# model_random_forest = GridSearchCV(\n#     model_random_forest, \n#     parameters, \n#     cv=5,\n#     scoring='roc_auc',\n#     n_jobs = -1\n# )\n\n# model_random_forest.fit(X_train, y_train)\n\n# print('-----')\n# print(f'Best parameters {model_random_forest.best_params_}')\n# print(\n#     f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n#     f'{model_random_forest.best_score_:.3f}'\n# )","5bb45748":"model2 = RandomForestClassifier(max_depth=10, min_samples_leaf= 5, random_state=0, n_estimators = 200, n_jobs=-1)\nmodel2.fit(X_train, y_train)\n\nprint(roc_auc_score(y_train, model2.predict_proba(X_train)[:, 1]))\nprint(roc_auc_score(y_test, model2.predict_proba(X_test)[:,1]))","e4b312c9":"result = model2.predict_proba(test_df)","cc1ceda5":"From the above visuals we can conclude following things:\n1. There are more number of males having the account\n2. There are certain regsions whose number are high in the data\n3. People are mostly self employed and very less number of people are enterpreneurs\n4. Channel code mostly being used is X4\n5. Very few customers(30%) have any active credit product (Home loan, Personal loan, Credit Card etc.)\n6. Around 30% customer are Active in last 3 Months","fa453123":"Enterpreneurs are having higher chances of being leads than other occupations while Salaried people have the lowest conversion rate to leads and it does make sense. People who are active are more likely to be the leads than the non active people. People with channel code X3 are having higher conversion rate to leads while X1 being the lowest. Also people having the credit products are likely to be the leads than the ones who are not having credit products.","dcd3c700":"Filled the Null values of column Cerdit product with zero because it makes sense for users who doesn't have credit product to leave the cell empty so filling it with 0","21e3418f":"##### Dropping ID column","4b383e3f":"#### Test and Train data both are having same distribution","972fab0a":"#### Splitting the data","e1c4c0c5":"We can see that the account balance is skewed towards right and mostly the people with age between 25 to 35 are there in the data and people above 60 are not too much in the data. We can see how vintage is varying.","2fa6bce8":"# Reading Datatrain_df","7f30e0d8":"##### Getting Categorical and Numerical features","29ac6771":"#### Dealing with non numeric values","c7582cdb":"Although the people between age 25 and 35 are having more records but rate of them being leads is pretty less while people above that age have high chances of being lead. People with around vintage 90 are having the highest chances of being leads. People with avg_account_balance between 1 million to 2 million are having higher chances of being leads than others.","ef07b7f5":"# Multivariate Analysis","43c95ef9":"# Summary of Data","69d122a3":"#### Test data Distribution","cdd9cb22":"### Univariate Analysis"}}