{"cell_type":{"eefc0ed7":"code","d9e7c72e":"code","7c5bab6e":"code","8cfe8e03":"code","fbe2b114":"code","bbb1a854":"code","4ec54a89":"code","56513116":"code","29c25647":"code","799f1a19":"code","c10451f5":"code","7929cc7c":"code","e7381f4a":"code","b30039fe":"code","642f8fb7":"code","c83f1ac8":"code","0877eafa":"code","ebd000e3":"code","34f8bd3b":"code","3e1410e4":"code","d891e088":"code","4faf47ca":"code","b8239431":"code","bbc5eb3d":"code","3e47cb54":"code","5553f52b":"markdown","9670c250":"markdown","e86b326f":"markdown","9e9a51a2":"markdown","bb87aceb":"markdown","1a1dc148":"markdown","64e51947":"markdown","044ad915":"markdown","ebc5c8f9":"markdown","933b4a38":"markdown","1321e4d3":"markdown","2c888353":"markdown"},"source":{"eefc0ed7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9e7c72e":"df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf.head()","7c5bab6e":"data = df['text']\ndata.shape","8cfe8e03":"labels = df['target']\nlabels.shape","fbe2b114":"cls = np.unique(labels,return_counts=True)\ncls","bbb1a854":"from matplotlib import pyplot as plt\nimport seaborn as sns\ndt = df['target'].value_counts()\nchart = sns.barplot(x = dt.index,y = dt)\nplt.show()","4ec54a89":"test_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_data.head()","56513116":"test_data.shape","29c25647":"x_test = test_data['text']","799f1a19":"import nltk\nfrom nltk import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","c10451f5":"tokenizer = RegexpTokenizer(r'[a-z]\\w+')\nps = PorterStemmer()\nsw = set(stopwords.words('english'))","7929cc7c":"def cleaned_text(text):\n    text = text.lower()\n    text = text.replace(\"<br \/><br \/>\",\" \")\n    tokenized_text = tokenizer.tokenize(text)\n    filtered_text = [words for words in tokenized_text if words not in sw]\n    stemmed_text = [ps.stem(word) for word in filtered_text]\n    cleaned_text = ' '.join(stemmed_text)\n    return cleaned_text","e7381f4a":"x_train = list(data)\nx_test = list(x_test)","b30039fe":"x_train = [cleaned_text(i) for i in x_train]\nx_train[:10]","642f8fb7":"x_test = [cleaned_text(i) for i in x_test]\nx_test[:10]","c83f1ac8":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(1,2))\n","0877eafa":"vocab = cv.fit_transform(x_train)\nx_train = vocab.toarray()\nx_test = cv.transform(x_test)\nx_test = x_test.toarray()\nprint(x_train.shape)\nprint(x_test.shape)","ebd000e3":"from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, roc_curve, precision_recall_curve, auc","34f8bd3b":"x_train,val_x,y_train,val_y = train_test_split(x_train,labels,test_size=0.3,random_state=0)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(val_x.shape)\nprint(val_y.shape)","3e1410e4":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\npred_tr1 = lr.predict(x_train)\npred1 = lr.predict(val_x)\n\nconf_matrix = confusion_matrix(val_y,pred1)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(val_y,pred1,average='weighted')\nrecall = recall_score(val_y, pred1,average='weighted')\n\nprint(\"Results on training data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(y_train, pred_tr1)))\nprint(\"Precision = \"+str(precision_score(y_train,pred_tr1,average='weighted')))\nprint(\"Recall = \"+str(recall_score(y_train,pred_tr1,average='weighted')))\n\nprint()\n\nprint(\"Results on validation data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(val_y, pred1)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n        \n\n","d891e088":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(x_train,y_train)\n\npred_tr2 = mnb.predict(x_train)\npred2 = mnb.predict(val_x)\n\nconf_matrix = confusion_matrix(val_y,pred2)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(val_y,pred2,average='weighted')\nrecall = recall_score(val_y, pred2,average='weighted')\n\nprint(\"Results on training data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(y_train, pred_tr2)))\nprint(\"Precision = \"+str(precision_score(y_train,pred_tr2,average='weighted')))\nprint(\"Recall = \"+str(recall_score(y_train,pred_tr2,average='weighted')))\n\nprint()\n\nprint(\"Results on validation data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(val_y, pred2)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n        \n\n","4faf47ca":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\n\npred_tr3 = mnb.predict(x_train)\npred3 = mnb.predict(val_x)\n\nconf_matrix = confusion_matrix(val_y,pred3)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(val_y,pred3,average='weighted')\nrecall = recall_score(val_y, pred3,average='weighted')\n\nprint(\"Results on training data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(y_train, pred_tr3)))\nprint(\"Precision = \"+str(precision_score(y_train,pred_tr3,average='weighted')))\nprint(\"Recall = \"+str(recall_score(y_train,pred_tr3,average='weighted')))\n\nprint()\n\nprint(\"Results on validation data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(val_y, pred3)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n        \n","b8239431":"rfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)\npred4 = rfc.predict(val_x)\npred_tr4 = rfc.predict(x_train)\nconf_matrix = confusion_matrix(val_y,pred4)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(val_y,pred4,average='weighted')\nrecall = recall_score(val_y, pred4,average='weighted')\n\nprint(\"Results on training data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(y_train, pred_tr4)))\nprint(\"Precision = \"+str(precision_score(y_train,pred_tr4,average='weighted')))\nprint(\"Recall = \"+str(recall_score(y_train,pred_tr4,average='weighted')))\n\nprint()\n\nprint(\"Results on validation data:\\n\")\nprint(\"Accuracy = \"+str(accuracy_score(val_y,pred4)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n        ","bbc5eb3d":"test_pred = lr.predict(x_test)\ntest_pred = np.array(test_pred)","3e47cb54":"df = pd.DataFrame({'id':test_data['id'],\n                   'target':test_pred})\n\ndf.to_csv(\".\/predictions.csv\",index=None)","5553f52b":"Cleaning the data by removing uncessary words.","9670c250":"Predictions through random forest classifier.","e86b326f":"Predictions through logistic regression.","9e9a51a2":"Importing data","bb87aceb":"Importing test data.","1a1dc148":"Count vectorizer transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.","64e51947":"Predictions through mutlinomial naive bayes.","044ad915":" **Accuracy given by different models:-**\n* Logistic regression-82%\n* Multinomial naive bayes-79.8%\n* Gaussian naive bayes-79.8\n* Random Forest-78.8%","ebc5c8f9":"Making predictions on test data","933b4a38":"Predictions through gaussian naive bayes.","1321e4d3":"Bar plot of value counts for both the classes.","2c888353":"Splitting data into training and validation data."}}