{"cell_type":{"54798c44":"code","c93403c5":"code","1915f207":"code","fa274d90":"code","91ac56ae":"code","1fee3a3c":"code","e3bd5c6f":"code","006ca952":"code","27211532":"code","788128c9":"code","b0fad5cb":"code","63cfb823":"code","8b0bf14c":"code","6c9fa1f3":"code","7502e9aa":"code","bf24fc08":"code","564cfe24":"code","708243dc":"code","953ce8f5":"code","a769ed0f":"code","a0dd6152":"code","f69a9361":"code","1cead55b":"code","7d17382c":"code","62e5f7c5":"code","6581d09e":"markdown","15764951":"markdown","c46f271e":"markdown","b5939813":"markdown","3d466868":"markdown","b387884a":"markdown","1c57a9f3":"markdown","874f384e":"markdown","3e0d7b92":"markdown"},"source":{"54798c44":"import os\n\nimport numpy as np \nimport pandas as pd\nfrom sklearn import model_selection, preprocessing, impute\nfrom sklearn import pipeline, base, compose\nfrom sklearn import feature_selection, ensemble, metrics\n\nimport xgboost  \n\nimport category_encoders as ce  # feature engineering\n\nimport optuna  # hyperparameter optimization\n\nimport shap  # model explanations\n\nimport matplotlib.pyplot as plt  # plotting\n\n# Adjust plot sizes\nplt.rcParams[\"figure.figsize\"] = (15,10)","c93403c5":"class AttributeAdder(base.BaseEstimator, base.TransformerMixin):\n    \"\"\"\n    Adds age of car and mile per year columns to the dataset.\n    Assumes that columns year and mileage are the first two numeric columns in the dataset.\n    \"\"\"\n    def __init__(self):\n        self.max_year = 2020\n        self.column_year = 0\n        self.column_mileage = 1\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        age_of_car = self.max_year - X[:, self.column_year]\n        mile_per_year = X[:, self.column_mileage] \/ (age_of_car + 1)\n        return np.c_[X[:, 0:self.column_year], age_of_car, \n                     X[:, self.column_year+1:], mile_per_year]","1915f207":"def return_mean_model_prices(x):\n    \"\"\"\n    Returns the mean model prices for error analysis\n    Used with dataframe.apply\n    \"\"\"\n    subset = df[np.logical_and(df.file == x['file'], df.model.str.contains(x['model']))]\n    return subset.price.mean()","fa274d90":"def model_prediction(X):\n    \"\"\"\n    Return model predictions\n    \n    Arguments:\n    ---------\n        X (dict): A dictionary with all values\n    \n    Returns:\n    --------\n        float: rescaled prediction\n    \"\"\"\n    df_pred = pd.DataFrame(X, index=range(len(X['model'])))\n    df_pred.mpg = df_pred.mpg.map(lambda x: 200 if x > 200 else x)\n    df_pred.model = df_pred.model.map(lambda x: 'rare' if x in rare_models else x)\n    \n    return np.exp(final_pipeline.predict(df_pred))","91ac56ae":"df = pd.DataFrame()\nheaders = ['model', 'year', 'price', 'transmission', 'mileage', 'fuel_size', 'tax', 'mpg', 'engine_size', 'file']\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if not 'unclean' in filename:\n            df_new = pd.read_csv(os.path.join(dirname, filename),\n                                 names=headers,\n                                 skiprows=lambda x: x in [0])\n            df_new['file'] = filename[:-4]\n            df = df.append(df_new)\n\n# remove the years with 2060 and 1970\n# they are either faulty or to different from dataset \n# to include\ndf = df[~df.year.isin([2060, 1970])]\n\n# Limit the outliers in mpg\ndf.mpg = df.mpg.map(lambda x: 200 if x > 200 else x)\n\n# map rare models into small groups\nrare_models = df.model.value_counts()[df.model.value_counts() <= 20].index.to_list()\ndf.model = df.model.map(lambda x: 'rare' if x in rare_models else x)\n\n# to have unique indices, appending files repeat indices\ndf = df.reset_index(drop=True)\n\nX = df.drop('price', axis=1)\n\n# the data is hard to train on since there a lot of sales with exact same \n# price, some noise is added to help the models converge without problems\ny = np.log(df[['price']]) + np.random.randn(df.shape[0], 1) * 0.00001\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n                                                                    test_size=0.4,\n                                                                    random_state=710,\n                                                                    stratify=X['file'])\n\nnum_types = X_train.select_dtypes('number').columns.to_list()\ncat_types = X_train.select_dtypes(exclude='number').columns.to_list()","1fee3a3c":"def model_pipeline(config):\n    \"\"\"\n    Create model pipeline according to config dictionary.\n    Should include imputer, scaler, n_estimators, learning_rate, reg_alpha, reg_lambda\n    \n    Designed to accomodate choices made in hyperparameter opt.\n    \"\"\"\n    \n    # more complicated strategeies yielded worse results\n    if config['imputer'] == 'mean':\n        imputer = impute.SimpleImputer(strategy='mean')\n    elif config['imputer'] == 'median':\n        imputer = impute.SimpleImputer(strategy='median')\n    else:\n        raise ValueError('Wrong Imputer Value')\n\n    # choose scaler, non scaled variables had worse results in previous experiments\n    if config['scaler'] == 'standard':\n        scaler = preprocessing.StandardScaler()\n    elif config['scaler'] == 'power':\n        scaler = preprocessing.PowerTransformer()\n    else:\n        raise ValueError('Wrong Scaler Value')    \n    \n    \n    estimator = xgboost.XGBRegressor(n_estimators=config['n_estimators'],\n                                     max_depth=config['max_depth'],\n                                     learning_rate=config['learning_rate'],\n                                     reg_alpha=config['reg_alpha'],\n                                     reg_lambda=config['reg_lambda'],\n                                )\n    \n    # column-wise pipelines for numeric variables\n    num_pipeline = pipeline.Pipeline([\n        ('imputer', imputer),\n        ('attr_adder', AttributeAdder()),\n        ('scaler', scaler)\n    ])\n\n    # column-wise pipelines for categorical variables\n    cat_pipeline = pipeline.Pipeline([\n        ('enc', ce.TargetEncoder(handle_unknown='value',\n                                 smoothing=config['smoothing'])),\n    ])\n    \n    # combine pipelines\n    mid_pipeline = compose.ColumnTransformer([\n        ('num', num_pipeline, num_types),\n        ('cat', cat_pipeline, cat_types)\n    ])\n    \n    # final pipeline\n    full_pipeline = pipeline.Pipeline([\n        ('col_trans', mid_pipeline),\n        ('regressor', estimator)\n    ])\n    \n    return full_pipeline","e3bd5c6f":"def objective(trial):\n    \"\"\"\n    Optuna trial function\n    \"\"\"\n    X_trial = X_train.copy(deep=True)\n    y_trial = y_train.copy(deep=True)\n    \n    config = {}\n    \n    config['imputer'] = trial.suggest_categorical('imputer', ['median', 'mean'])\n    \n    config['smoothing'] = trial.suggest_loguniform('smoothing', 0.01, 1)\n    \n    config['scaler'] = trial.suggest_categorical('scaler', ['standard', 'power'])\n\n    config['regressor'] = trial.suggest_categorical('regressor', ['RandomForest', 'XGBoost'])\n    \n    config['n_estimators'] = trial.suggest_int('n_estimators', 350, 500)\n    config['max_depth'] = trial.suggest_int('max_depth', 10, 30)        \n    config['learning_rate'] = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n    config['reg_alpha'] = trial.suggest_uniform('reg_alpha', 0.5, 1)\n    config['reg_lambda'] = trial.suggest_uniform('reg_lambda', 0.5, 1)\n        \n        \n    model = model_pipeline(config)\n    \n    return model_selection.cross_val_score(model, \n                                           X_trial, \n                                           y_trial, \n                                           n_jobs=-1, \n                                           cv=5).mean()\n","006ca952":"study = optuna.create_study(direction='maximize')\n\n# higher job numbers crashed the kaggle notebook\nstudy.optimize(objective, n_trials=20, n_jobs=2)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","27211532":"# retrain with full dataset\nfinal_pipeline = model_pipeline(study.best_params)\nfinal_pipeline.fit(X_train, y_train)","788128c9":"test_preds = final_pipeline.predict(X_test)\n\nprint('r2', final_pipeline.score(X_test, y_test))\nprint('mse', metrics.mean_squared_error(y_test, test_preds))  # please note that the data here is in log scale","b0fad5cb":"# https:\/\/github.com\/slundberg\/shap\/issues\/1215\n# latest shap library has issues with new byte string of the booster\nbooster = final_pipeline.named_steps['regressor'].get_booster()\n\nmodel_bytearray = booster.save_raw()[4:]\ndef replace_booster(self=None):\n    return model_bytearray\n\nbooster.save_raw = replace_booster","63cfb823":"# initiate shap explorer\nexplainer = shap.TreeExplainer(final_pipeline.named_steps['regressor'])","8b0bf14c":"# get the transformed dataframe for predictions\nX_shap = final_pipeline.named_steps['col_trans'].transform(X_train)\nsamples = np.random.choice(X_shap.shape[0], 500)","6c9fa1f3":"shap_values = explainer.shap_values(X_shap[samples])\n\n# add name the name of the new variable to the list\nnames_features = X_train.columns.tolist() + ['mile_per_year']","7502e9aa":"shap.initjs()","bf24fc08":"shap.force_plot(explainer.expected_value, shap_values[1,:], X_shap[samples[1],:], feature_names=names_features)","564cfe24":"shap.force_plot(explainer.expected_value, shap_values, X_shap[samples], feature_names=names_features)","708243dc":"shap.dependence_plot(6, shap_values, X_shap[samples], feature_names=names_features)","953ce8f5":"shap.dependence_plot(0, shap_values, X_shap[samples], feature_names=names_features)","a769ed0f":"shap.summary_plot(shap_values, X_shap[samples], feature_names=names_features)","a0dd6152":"shap.summary_plot(shap_values, X_shap[samples], feature_names=names_features, plot_type='bar')","f69a9361":"# Calculate the error breakdown\nanalysis_df = pd.DataFrame({'actuals': np.exp(y_test.values).ravel(),\n                            'preds': np.exp(test_preds).ravel()}, \n                          index=y_test.index)\nanalysis_df['error'] = np.abs(analysis_df['preds'] - analysis_df['actuals'])\npd.qcut(analysis_df.error, q=[0, 0.7, 0.95, 0.99, 1]).value_counts()","1cead55b":"top_error = analysis_df.sort_values('error', ascending=False).head(10)\ndf_top_errors = pd.merge(top_error, df, left_index=True, right_index=True)\ndf_top_errors['mean_model_prices'] = df_top_errors.apply(lambda x: return_mean_model_prices(x), axis=1)\ndf_top_errors","7d17382c":"# totally nonsense models sent\nnew_input = {'model': ['2 Series', 'A8'],\n             'year': [2017, 2019], \n             'transmission': ['Automatic', 'Semi-Auto'],\n             'mileage': [2000, 30000], \n             'fuel_size': ['Diesel', 'Petrol'],\n             'tax': [20, 145],\n             'mpg': [33.2, 11.0],\n             'engine_size': [3.0, 5.5],\n             'file': ['audi', 'merc']}","62e5f7c5":"model_prediction(new_input)","6581d09e":"# Output Analysis","15764951":"There have been a lot of amazing EDAs for this task, so I didn't want to spend more time doing that. Instead, this notebook focuses on creating a good model.\n\nEDA has mostly been skipped, I left some comments on some of the choices. There are obvious problems such as the file 'focus' having a lot of of the mpg and engineSize columns null. I assume those high number of missing fields is why simple imputer methods have been more successful in general.\n\nFor modelling, after trying a bunch of models, XGBoost consistently yielded the best results and for explainability I didn't want to train an ensemble model.\n\nThis notebook covers:\n- Model Pipelines\n- Hyperparameter Optimisation Using Optuna\n- Model Explainability with Shap\n- Basic Error Analysis\n- End to end prediction using Pipelines","c46f271e":"# New Value Prediction","b5939813":"# Training","3d466868":"Top 3-4 errors seem more like a data error or there are some additional characteristics about the data that is not captured in the dataset that makes them so expensive. e.g. on row 3, the average sale price of that car is 7.7k while that model is sold for 92k.\n\nFor the other errors, more analysis is required to pin point the reasons.","b387884a":"# Setup","1c57a9f3":"# Model Pipeline Definition","874f384e":"# Highest Error Analysis","3e0d7b92":"From all the 40k test cases, more than 30k has less than 1.2k errors. \n\nHowever 435 cases seem to have very high errors. Let's look at the results."}}