{"cell_type":{"a648e690":"code","967096db":"code","969174a3":"code","2e0e2821":"code","9a642db6":"code","31f915e6":"code","58b8818d":"code","d18cb8c3":"code","1d450a41":"code","7f05c3b9":"code","62e275bf":"code","839c1b2e":"code","7a7e00e2":"code","9d9cb764":"code","45a19be6":"code","7d8c4038":"code","08bb9a42":"code","c081f5a2":"code","6ec63125":"markdown","26cdda2a":"markdown","f2d155cb":"markdown","119c129a":"markdown","a191066b":"markdown","90ba0086":"markdown","3bb384ab":"markdown","07b4a4da":"markdown"},"source":{"a648e690":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","967096db":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/XGBoost_logo.png\")\n\n","969174a3":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https:\/\/i.ytimg.com\/vi\/dMulLZKm_pg\/maxresdefault.jpg\")\n\n\n","2e0e2821":"from xgboost import XGBRegressor \nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import KFold,cross_val_score,train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom time import time \nimport datetime ","9a642db6":"##\u5bfc\u5165\u6570\u636e \ndata = load_boston()\n\nx = data.data  #\u7279\u5f81 \ny = data.target  #\u76ee\u6807","31f915e6":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3 ,random_state = 400)\n\nreg = XGBRegressor(n_estimators = 100).fit(x_train,y_train)\n\nreg.score(x_test,y_test)\n\ny_pre = reg.predict(x_test) \n\nmean_squared_error(y_test,y_pre)\n","58b8818d":"#\u67e5\u770b\u6a21\u578b\u91cd\u8981\u5206\u6570\nreg.feature_importances_","d18cb8c3":"#XGBRegressor\nreg = XGBRegressor(n_estimators = 100)\nprint(cross_val_score(reg,x_train,y_train,cv = 5).mean())  ##\u4ea4\u53c9\u9a8c\u8bc1\u53d6\u5e73\u5747\u503c\nprint(cross_val_score(reg,x_train,y_train,cv = 5,scoring = 'neg_mean_squared_error').mean()) #\u4ea4\u53c9\u9a8c\u8bc1\u4e3e\u5e73\u5747\u503c \u7528\u4e0d\u540c\u65b9\u6cd5\n","1d450a41":"#RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=100)\nprint(cross_val_score(rfr,x_train,y_train,cv = 5).mean())\nprint(cross_val_score(rfr,x_train,y_train,cv = 5,scoring = 'neg_mean_squared_error').mean())","7f05c3b9":"Lr = LinearRegression()\nprint(cross_val_score(Lr,x_train,y_train,cv = 5).mean())\nprint(cross_val_score(Lr,x_train,y_train,cv = 5,scoring = 'neg_mean_squared_error').mean())","62e275bf":"##\u5f00\u542f\u53c2\u6570\u67e5\u770b\nreg = XGBRegressor(n_estimators = 100,silent = False)\ncross_val_score(reg,x_train,y_train,cv = 5,scoring = 'neg_mean_squared_error').mean()\n\n\n","839c1b2e":"def plot_learning_curve(estimator,title,x,y\n                       ,ax = None      #\u5b50\u56fe\n                       ,ylim = None    #y\u7684\u8303\u56f4\n                       ,cv = None      \n                       ,n_jobs = None  #\u8bbe\u5b9a\u6240\u8981\u4f7f\u7528\u7684\u7ebf\u7a0b\n                       ):\n    from sklearn.model_selection import learning_curve\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    train_sizes,train_scores,test_scores = learning_curve(estimator,x,y\n                                                         ,shuffle=True\n                                                         ,cv=cv\n                                                         ,random_state=400\n                                                         ,n_jobs = n_jobs)\n    if ax == None:\n        ax = plt.gca()\n    else :\n        ax = plt.figure()\n    ax.set_title(title)\n    if ylim is not None:\n        ax.set_ylim(*ylim)\n    ax.set_xlabel('Training examples')\n    ax.set_ylabel('score')\n    ax.grid()\n    ax.plot(train_sizes,np.mean(train_scores,axis = 1),'o-'\n    ,color = 'r',label = 'Training score')\n    ax.plot(train_sizes,np.mean(test_scores,axis = 1),'o-'\n    ,color = 'g',label = 'Test score')\n    ax.legend(loc = 'best')\n    return ax \n    ","7a7e00e2":"cv = KFold(n_splits = 5,shuffle = True,random_state = 400)\nplot_learning_curve(XGBRegressor(n_estimators = 100,random_state = 400)\n                   ,'XGBRegressor',x_train,y_train,ax=None,cv=cv)\nplt.show()","9d9cb764":"axisx = range(10,1010,50)\nrs = []\nfor i in axisx :\n    reg = XGBRegressor(n_estimators = i , random_state = 400)\n    rs.append(cross_val_score(reg,x_train,y_train,cv=cv).mean())\nprint(axisx[rs.index(max(rs))],max(rs))\nplt.figure(figsize = (20,5))\nplt.plot(axisx,rs,c = 'red',label =\"XGBRegressor\")\nplt.legend()\nplt.show()\n\n","45a19be6":"axisx = range(50,1050,10) \nrs = [] \nvar = [] \nge = [] \nfor i in axisx:\n    reg = XGBRegressor(n_estimators=i,random_state=400)\n    cvresult = cross_val_score(reg,x_train,y_train,cv=cv)\n    \n    #\u8bb0\u5f551-\u504f\u5dee \u00a0    \n    \n    rs.append(cvresult.mean())\n    #\u8bb0\u5f55\u65b9\u5dee \u00a0    \n    var.append(cvresult.var())\n   \n    #\u8ba1\u7b97\u6cdb\u5316\u8bef\u5dee\u7684\u53ef\u63a7\u90e8\u5206 \u00a0 \u00a0    \n    ge.append((1 - cvresult.mean())**2+cvresult.var()) \n    \n#\u6253\u5370R2\u9ad8\u6240\u5bf9\u5e94\u7684\u53c2\u6570\u53d6\u503c\uff0c\u5e76\u6253\u5370\u8fd9\u4e2a\u53c2\u6570\u4e0b\u7684\u65b9\u5dee     \nprint(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))]) \n#\u6253\u5370\u65b9\u5dee\u4f4e\u65f6\u5bf9\u5e94\u7684\u53c2\u6570\u53d6\u503c\uff0c\u5e76\u6253\u5370\u8fd9\u4e2a\u53c2\u6570\u4e0b\u7684R2 \nprint(axisx[var.index(min(var))],rs[var.index(min(var))],min(var)) \n#\u6253\u5370\u6cdb\u5316\u8bef\u5dee\u53ef\u63a7\u90e8\u5206\u7684\u53c2\u6570\u53d6\u503c\uff0c\u5e76\u6253\u5370\u8fd9\u4e2a\u53c2\u6570\u4e0b\u7684R2\uff0c\u65b9\u5dee\u4ee5\u53ca\u6cdb\u5316\u8bef\u5dee\u7684\u53ef\u63a7\u90e8\u5206 \nprint(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge)) \n\n\nplt.figure(figsize=(20,5)) \nplt.plot(axisx,rs,c=\"red\",label=\"XGBRegressor\") \nplt.legend() \nplt.show()\n","7d8c4038":"axisx = range(0,90,2)\nrs = []\nvar = [] \nge = []\nfor i in axisx:\n    reg = XGBRegressor(n_estimators=i,random_state=400)\n    \n    cvresult = cross_val_score(reg,x_train,y_train,cv=cv)\n    rs.append(cvresult.mean())\n    var.append(cvresult.var())\n    ge.append((1 - cvresult.mean())**2+cvresult.var()) \nprint(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])\nprint(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))\nprint(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))\nrs = np.array(rs)\nvar = np.array(var)*0.01\nplt.figure(figsize=(20,5))\nplt.plot(axisx,rs,c=\"black\",label=\"XGB\") \n\n#\u6dfb\u52a0\u65b9\u5dee\u7ebf \u5305\u56f4\u8d77\u6765!! \nplt.plot(axisx,rs+var,c=\"red\",linestyle='-.')\nplt.plot(axisx,rs-var,c=\"red\",linestyle='-.')\n\nplt.legend()\nplt.show()\n#\u770b\u770b\u6cdb\u5316\u8bef\u5dee\u7684\u53ef\u63a7\u90e8\u5206\u5982\u4f55\uff1f \nplt.figure(figsize=(20,5))\nplt.plot(axisx,ge,c=\"gray\",linestyle='-.')\nplt.show()","08bb9a42":"##\u68c0\u6d4b\u6a21\u578b\u6548\u679c\n\ntime0 = time()\nprint(XGBRegressor(n_estimators=60,random_state=400).fit(x_train,y_train).score(x_test,y_test))\nprint(time()-time0)\ntime0 = time()\nprint(XGBRegressor(n_estimators=6,random_state=400).fit(x_train,y_train).score(x_test,y_test))\nprint(time()-time0)\ntime0 = time()\nprint(XGBRegressor(n_estimators=32,random_state=400).fit(x_train,y_train).score(x_test,y_test))\nprint(time()-time0)","c081f5a2":"#\u91cd\u8981\u53c2\u6570subsample\n\naxisx = np.linspace(0,1,20)\nrs = []\nfor i in axisx:\n    reg = XGBRegressor(n_estimators=32,subsample=i,random_state=400)\n    rs.append(cross_val_score(reg,x_train,y_train,cv=cv).mean())\nprint(axisx[rs.index(max(rs))],max(rs))\nplt.figure(figsize=(20,5))\nplt.plot(axisx,rs,c=\"green\",label=\"XGBRegressor\")\nplt.legend()\nplt.show()\n#\u7ec6\u5316\u5b66\u4e60\u66f2\u7ebf\naxisx = np.linspace(0.05,1,20)\nrs = []\nvar = []\nge = []\nfor i in axisx:\n    reg = XGBRegressor(n_estimators=180,subsample=i,random_state=400)\n    cvresult = cross_val_score(reg,x_train,y_train,cv=cv)\n    rs.append(cvresult.mean())\n    var.append(cvresult.var())\n    ge.append((1 - cvresult.mean())**2+cvresult.var())\n    \nprint(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])\nprint(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))\nprint(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))\nrs = np.array(rs)\nvar = np.array(var)\nplt.figure(figsize=(20,5)) \n\nplt.plot(axisx,rs,c=\"black\",label=\"XGB\")\nplt.plot(axisx,rs+var,c=\"red\",linestyle='-.')\nplt.plot(axisx,rs-var,c=\"red\",linestyle='-.')\nplt.legend()\nplt.show()\n#\u7ee7\u7eed\u7ec6\u5316\u5b66\u4e60\u66f2\u7ebf \naxisx = np.linspace(0.75,1,25)\n#\u4e0d\u8981\u76f2\u76ee\u627e\u5bfb\u6cdb\u5316\u8bef\u5dee\u53ef\u63a7\u90e8\u5206\u7684\u4f4e\u503c\uff0c\u6ce8\u610f\u89c2\u5bdf\u7ed3\u679c\n#\u770b\u770b\u6cdb\u5316\u8bef\u5dee\u7684\u60c5\u51b5\u5982\u4f55\nreg = XGBRegressor(n_estimators=180,subsample=0.7708333333333334 ,random_state=420).fit(x_train,y_train)\nreg.score(Xtest,Ytest)\nmean_squared_error(y_test,reg.predict(x_test))","6ec63125":"#  \u8fdb\u5316\u7684\u5b66\u4e60\u66f2\u7ebf\uff1a\u65b9\u5dee\u4e0e\u6cdb\u5316\u8bef\u5dee ","26cdda2a":"# \u4f7f\u7528\u53c2\u6570\u5b66\u4e60\u66f2\u7ebf\u89c2\u5bdfn_estimators\u5bf9\u6a21\u578b\u7684\u5f71\u54cd ","f2d155cb":"# \u5efa\u6a21\u4e0e\u63a5\u53e3\u5c5e\u6027","119c129a":"# \u753b\u51fa\u5b66\u4e60\u66f2\u7ebf","a191066b":"\u770b\u51fa => \u8fc7\u62df\u5408 ","90ba0086":"# \u4ea4\u53c9\u9a8c\u8bc1\u3001\u4e0eRandomForestRegressor \u3001LinearRegression \u5bf9\u6bd4","3bb384ab":"# \u7ec6\u5316\u5b66\u4e60\u66f2\u7ebf\uff0c\u627e\u51fa\u6700\u4f73n_estimators\n","07b4a4da":"# \u5b9a\u4e49\u5b66\u4e60\u66f2\u7ebf"}}