{"cell_type":{"b877e08f":"code","8544275f":"code","8dd97ca2":"code","bd886474":"code","8da84253":"code","cf1df352":"code","2ae37c37":"code","02c02b21":"markdown","81ae9467":"markdown","d9b12ac7":"markdown","86e67ffd":"markdown","b1278a2e":"markdown","f92f4304":"markdown","350c3ae1":"markdown","0fb6817c":"markdown","fed47582":"markdown","6ec06d45":"markdown"},"source":{"b877e08f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split # utils\nfrom sklearn.metrics import mean_absolute_error # eval metric\n\n# data processing\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import ElasticNet # machine learning\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8544275f":"# using pandas to read CSV files\ndf = pd.read_csv('..\/input\/X_train.csv')\ny = pd.read_csv('..\/input\/y_train.csv')['PRP']\ndf_test = pd.read_csv('..\/input\/X_test.csv')\ndf.head()","8dd97ca2":"fig, ax = plt.subplots(3,2,figsize=(10,8))\nfor i,c in enumerate(df):\n    sns.distplot(df[c], ax=ax[i \/\/ 2][i % 2], kde=False)\nfig.tight_layout()","bd886474":"sns.distplot(y)\nprint(y.head())","8da84253":"# split the data\nX, y = df.values, y.values\nX_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=2018)\n\n# data preprocessing using sklearn Pipeline\npipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=2, interaction_only=True)), # multiply features together\n    ('scale', StandardScaler()), # scale data\n])\n\n# fit and apply transform\nX_train = pipeline.fit_transform(X_train)\n# transform the validation set\nX_val = pipeline.transform(X_val)\nprint('train shape:', X_train.shape, 'validation shape:', X_val.shape)","cf1df352":"reg = ElasticNet(alpha=1.7)\nreg.fit(X_train, y_train) # magic happens here\ny_pred = reg.predict(X_val)\ny_pred[y_pred < 0] = 0\nprint('Model MAE:', mean_absolute_error(y_val, y_pred))\nprint('Mean  MAE:', mean_absolute_error(y_val, np.full(y_val.shape, y.mean())))","2ae37c37":"# refit and predict submission data\nX_train = pipeline.fit_transform(X)\nX_test = pipeline.transform(df_test.values)\nreg.fit(X_train, y)\ny_pred = reg.predict(X_test)\ny_pred[y_pred < 0] = 0\n\ndf_sub = pd.DataFrame({'Id': np.arange(y_pred.size), 'PRP': y_pred})\ndf_sub.to_csv('submission.csv', index=False)","02c02b21":"# Introduction to machine learning with scikit-learn\nIn this kernel, I will demonstrate how to use existing libraries to build a model for a regression task. You don't need to copy paste the code, just fork this notebook and run in the browser. The kernel is organized as follows. First, I do some very simple exploratory data analysis to understand the data better. Next, I will show some preprocessing steps and model training. Finally, I give some ideas on how to improve the model performance.","81ae9467":"Very good, we achieved better results than the dummy mean prediction. Out final step is to make predictions with the model. For that we use pandas dataframe.","d9b12ac7":"Looks like all variables have discrete and positive values. Let's do some simple univariate visualization.","86e67ffd":"## Exploratory data analysis\nExploratory data analysis means visualizing the data from different perspectives to see some patterns and make assumptions about the data. It is often said to be the most important step in Data Science process. I'll keep it very simple visualizing only histograms, but there are many possibilities what to do.","b1278a2e":"After expanding our dataset into 22 features, we can now train our model and evaluate it on the validation set. It is a good practice to compare the results with some very poor prediction such as the average of the target variable, so we know how good we are. We use a linear model with a fancy name *Elastic Net*.  This model is like a linear regression but it adds a penalty for each feature (l1 and l2 norm) to the loss function. This is mainly used to reduce model complexity and address A Nightmare on Machine Learning Street called overfitting.","f92f4304":"The distribution of the target variable. All values are non-negative  as well.","350c3ae1":"## Conclusion\nThis kernel serves only as an introduction to machine learning with Python. I hope the kernel was useful, leave comments if you have any questions. The model does not guarantee the first place on the leaderboard and there are many things, which can be done next, for example:\n  - multivariate visualization to see patterns in data\n  - apply some transformations to the data\n  - use Binomial Regression - this is often used when the target variable is non-negative and discrete (our case)\n  - better preprocessing\n  - try various models (Decision Trees, Support Vector Machines, etc.)\n  - use cross-validation for model evaluation and hyper-parameters tunning","0fb6817c":"After importing the packages, let's read the dataset (using pandas) and show the first 5 rows.","fed47582":"## Data preprocessing and modeling\nSo far, we have a very rough idea about the dataset. Before we run any machine learning algorithm, we should preprocess our data, otherwise, the model performance can significantly suffer. The scikit-learn API offers many preprocessing and machine learning tools. Most of the models have two main methods. The first one is **fit** which learns from data and the second is **predict** or **transform** which applies the model on new datasets (without learning). Here we use the class **PolynomialFeatures** to create new variables and **StandardScaler** to normalize the data. Moreover, we use a class **Pipeline** to concisely combine these two operations.","6ec06d45":"None of the features are normaly distributed. Perhaps, we should transform them by some nonlinear function such as $log$ or $x^{-1}$."}}