{"cell_type":{"b8abd13f":"code","a919cd38":"code","f9c28e7c":"code","76d500b3":"code","e15f1711":"code","3fb3a974":"code","9aaad3fa":"code","79d88c59":"code","48e883c8":"code","b5b659e9":"code","1681914c":"code","3e853683":"code","e3f9ca01":"code","ee89432b":"code","74644eb6":"code","361e746d":"code","3beaf886":"code","aba744b1":"code","1c3012b8":"code","21decff3":"code","687bfd91":"code","5f7e2078":"markdown","a2d5c73e":"markdown","772eedf3":"markdown","670720e9":"markdown","a0f03439":"markdown","ee0e48df":"markdown","754e3438":"markdown","e81effa1":"markdown","cd26861f":"markdown","77808f6a":"markdown","6e267957":"markdown","e8b1f83a":"markdown","204e1f73":"markdown","80a1e89f":"markdown","025410ac":"markdown","d483a15c":"markdown","d0151eea":"markdown","d40f5700":"markdown","14cd657e":"markdown","3db3ce46":"markdown","4edb535a":"markdown","5b290747":"markdown","c51b63a2":"markdown","bc74afd2":"markdown","3f61dbb0":"markdown","fe384853":"markdown","bf233bad":"markdown","3360f1e7":"markdown","9cb1fde9":"markdown","b1a24f8b":"markdown","8367e10a":"markdown","99a3175e":"markdown","88552e01":"markdown","71770820":"markdown","c8e2ab33":"markdown","fb761fad":"markdown","001c8821":"markdown","d112b324":"markdown","d08321b8":"markdown"},"source":{"b8abd13f":"# ------------- General libraries -------------- #\nimport os\nimport numpy as np  \nimport pandas as pd \nfrom math import pi\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# ------------- sklearn : ML tools ------------- #\nfrom sklearn import svm\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder #, label_binarize, LabelBinarizer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import RFECV, RFE\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (5.0, 5.0)  # Set default plot's sizes\nplt.rcParams['figure.dpi'] = 90  # Set default plot's dpi (increase fonts' size)\nplt.rcParams['axes.grid'] = True  # Show grid by default in figures\n\n## Loading the data\nurl = '..\/input\/breast-cancer-wisconsin-data\/data.csv'\ndf = pd.read_csv( url )\nrndperm = np.random.permutation(df.shape[0])\nD = df.iloc[rndperm, :-1]\n\n## Print the number of rows in the data set\ndf_rows, df_cols = df.shape\nprint('Table size : {} x {}'.format(df_rows, df_cols) )\n\nix_mn, ix_se, ix_wt = [*range(0, 10, 1)], [*range(10, 20, 1)], [*range(20, 30, 1)]\n\ndef arr_shift(arr, shift):\n    return [i+shift for i in arr]\n\nclass_feat = ['Radius', 'Texture', 'Perimeter', 'Area', 'Smoothness', 'Compactness', 'Concavity', 'Concave points', 'Symmetry', 'Fractal dim.']\nX_class = pd.DataFrame(class_feat, columns = ['Class'])\nnull_df1 = pd.DataFrame(df.iloc[:, arr_shift(ix_mn, 1)].isnull().sum(), columns = ['NaN'])\nnull_df2 = pd.DataFrame(df.iloc[:, arr_shift(ix_se, 1)].isnull().sum(), columns = ['NaN'])\nnull_df3 = pd.DataFrame(df.iloc[:, arr_shift(ix_wt, 1)].isnull().sum(), columns = ['NaN'])\n\nle = LabelEncoder()                 # label encoding\nX, y = D.iloc[:, 2:], D[['diagnosis']]\ny = y.rename(columns={'diagnosis': 'Diagnosis'})\ny['Diagnosis'] = le.fit_transform( y['Diagnosis'] )\n\ndf_NaN = X_class\ndf_NaN['Mean'], df_NaN['SE'], df_NaN['Worst'] = null_df1.to_numpy(), null_df2.to_numpy(), null_df3.to_numpy()\n\n# ---- Missing Values Warning ---- #\nif (df_NaN.iloc[:, 1:4].sum() != 0).any():\n    print('Warning ! deteced missing values !')\n\n\n# ----------------------------------------------------- #\n# ----------- Import from Drive environment ----------- #\n# from google.colab import drive\n# PATH_mount = '\/content\/gdrive'\n# drive.mount(PATH_mount)\n\n# # ------- Change to working directory ------- #\n# PATH_dir = '\/My Drive\/Colab Notebooks\/Proj-ML'\n# os.chdir(PATH_mount + PATH_dir)\n\n# url = 'https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Machine-Learning-Project\/wdbc.csv'\n# df = pd.read_csv('Wisconsin Breast Cancer Dataset.csv')\n# ----------------------------------------------------- #","a919cd38":"df_NaN","f9c28e7c":"D.head(10)        # Show ten first samples (after random shuffling)","76d500b3":"sum_D = len(y)\nsum_M = (y.values==1).sum()\nsum_B = sum_D - sum_M\nfrac_B = sum_B\/sum_D\nfrac_M = 1 - frac_B\nlabels = 'Benign ('+str(sum_B)+')', 'Malignant ('+str(sum_M)+')'\nsizes = [frac_B, frac_M]\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots(figsize=(4, 4))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 12})\n\nax1.set_title('Prevalence of diagnosis ('+str(sum_D)+')', fontsize=14)\nax1.axis('equal')\nax1.legend(fontsize=12, loc='best')\nplt.show()","e15f1711":"from sklearn.model_selection import train_test_split\n\n# Train\/Test : { 80 [%], 20 [%] }\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nix_train, ix_test = X_train.index, X_test.index\n\ny_train = y_train.values.ravel()\ny_test = y_test.values.ravel()\n\nsum_tr  = len(y_train)\nsum_tr_M = (y_train==1).sum()\nsum_tr_B = sum_tr - sum_tr_M\nfrac_tr_B = sum_tr_B\/sum_tr\nfrac_tr_M = 1 - frac_tr_B\nsizes_tr = [frac_tr_B, frac_tr_M]\nexplode_tr = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\nlabels_tr = 'Benign ('+str(sum_tr_B)+')', 'Malignant ('+str(sum_tr_M)+')'\n\nsum_ts   = len(y_test)\nsum_ts_M = (y_test==1).sum()\nsum_ts_B = sum_ts - sum_ts_M\nfrac_ts_B = sum_ts_B\/sum_ts\nfrac_ts_M = 1 - frac_ts_B\nsizes_ts = [frac_ts_B, frac_ts_M]\nexplode_ts = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\nlabels_ts = 'Benign ('+str(sum_ts_B)+')', 'Malignant ('+str(sum_ts_M)+')'\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\naxes[0].pie(sizes_tr, explode=explode_tr, labels=labels_tr, autopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 11})\naxes[0].set_title('Training set ('+str(sum_tr)+')', fontsize=12)\n# axes[0].legend(fontsize=10, loc=4)\n\naxes[1].pie(sizes_ts, explode=explode_ts, labels=labels_ts, autopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 11})\naxes[1].set_title('Test set ('+str(sum_ts)+')', fontsize=12)\n# axes[1].legend(fontsize=10, loc=4)\n\nplt.tight_layout()\nplt.show()\n\n","3fb3a974":"# Logical statement for diagnosis\ny_B, y_M = (y.values==0), (y.values==1)           \n# Partition the dataset\nD_full = D.iloc[:, 2:]\nX_mn, X_se, X_wt = D_full.iloc[:, ix_mn], D_full.iloc[:, ix_se], D_full.iloc[:, ix_wt]\nX_mn.columns, X_se.columns, X_wt.columns = class_feat, class_feat, class_feat\n\ndf_NaN = pd.DataFrame(class_feat, columns=['Aspcet'])\ndf_NaN['Mean_a'] = (X_mn.mean(axis=0)).values\ndf_NaN['Mean_b'] = (X_se.mean(axis=0)).values\ndf_NaN['Mean_c'] = (X_wt.mean(axis=0)).values\n\ndf_NaN['std_a'] = (X_mn.std(axis=0)).values\ndf_NaN['std_b'] = (X_se.std(axis=0)).values\ndf_NaN['std_c'] = (X_wt.std(axis=0)).values\n\ndf_NaN['Min_a'] = (X_mn.min(axis=0)).values\ndf_NaN['Min_b'] = (X_se.min(axis=0)).values\ndf_NaN['Min_c'] = (X_wt.min(axis=0)).values\n\ndf_NaN['Max_a'] = (X_mn.max(axis=0)).values\ndf_NaN['Max_b'] = (X_se.max(axis=0)).values\ndf_NaN['Max_c'] = (X_wt.max(axis=0)).values\n\ndf_NaN.transpose()","9aaad3fa":"dx_mn = X_mn.max(axis=0) - X_mn.min(axis=0)\n\nA_B = (( X_mn[y_B] - X_mn.min() )\/dx_mn).mean(axis=0)\nA_M = (( X_mn[y_M] - X_mn.min() )\/dx_mn).mean(axis=0)\n\n\ndef plot_data(B, M):\n    # Using Panda dataframe\n    df = pd.DataFrame({\n    'Diagnosis': ['Benign', 'Malignant'],\n    class_feat[0] : [ B[0] , M[0] ],\n    class_feat[1] : [ B[1] , M[1] ],\n    class_feat[2] : [ B[2] , M[2] ],\n    class_feat[3] : [ B[3] , M[3] ],\n    class_feat[4] : [ B[4] , M[4] ],\n    class_feat[5] : [ B[5] , M[5] ],\n    class_feat[6] : [ B[6] , M[6] ],\n    class_feat[7] : [ B[7] , M[7] ],\n    class_feat[8] : [ B[8] , M[8] ],\n    class_feat[9] : [ B[9] , M[9] ], })\n\n    # General details\n    N = len(B)\n    angles = [n \/ float(N) * 2 * pi for n in range(N)]\n    angles += angles[:1]\n    values=df.loc[0].drop('Diagnosis').values.flatten().tolist()\n    values += values[:1]\n    ax = plt.subplot(111, polar=True)\n    ax.set_theta_offset(pi \/ 2)\n    ax.set_theta_direction(-1)\n    ax.set_thetagrids(angles, fontsize=14)\n    plt.xticks(angles[:-1], class_feat, fontsize=15)\n    ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Benign\")\n    ax.fill(angles, values, 'b', alpha=0.1)\n\n    values=df.loc[1].drop('Diagnosis').values.flatten().tolist()\n    values += values[:1]\n    ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Malignant\")\n    ax.fill(angles, values, 'r', alpha=0.1) \n    plt.legend(bbox_to_anchor=(0.05, 0.05), fontsize=14)\n    fig = plt.gcf()\n    fig.set_size_inches(4.5, 4.5)\n\n# ----------------------- Case A ----------------------- #\nplot_data(A_B, A_M)\n","79d88c59":"# Add diagnosis label to the dataframe\nX_mn_y = pd.concat([X_mn, y], axis=1)\n\ndef PairGrid_func( D ):\n    g = sns.PairGrid(D, hue='Diagnosis', diag_sharey=False) # use diagnosis as sub-group criterion\n    g.map_lower(sns.scatterplot)\n    g.map_upper(sns.kdeplot)\n    g.map_diag(sns.kdeplot)\n    g.fig.set_size_inches(8, 8)\n\nPairGrid_func(X_mn_y)","48e883c8":"plt.rcParams['figure.figsize'] = (12, 6)\nclass_diag =  class_feat + ['Diagnosis']\n\ncorr_full = X_mn_y.corr(method='pearson')\nmask = np.triu( np.ones_like(corr_full, dtype=np.bool) )\nax = sns.heatmap(corr_full, cmap='RdBu_r', vmin=-1.0, vmax=1.0, mask=mask, annot=True, square=True, linewidths=1.0, annot_kws={\"size\": 9}) #, cbar_kws={\"shrink\": 1.0})\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nax.set_xticklabels(class_diag)\nax.set_yticklabels(class_diag)\nplt.title('Correlation Matrix', fontsize = 15)\nplt.show()\n","b5b659e9":"# ----------- Dataset Normlization ----------- #\n\ndef self_Normalize( X ):\n    X_n = (X-X.mean())\/(X.max(axis=0)-X.min(axis=0))\n    return X_n\n\nX_N = self_Normalize( X )\nX1, X2, X3 = X_N.iloc[:, ix_mn], X_N.iloc[:, ix_se], X_N.iloc[:, ix_wt]\nmodel_1 = ExtraTreesClassifier(); model_1.fit(X1, y.values.ravel())\nmodel_2 = ExtraTreesClassifier(); model_2.fit(X2, y.values.ravel())\nmodel_3 = ExtraTreesClassifier(); model_3.fit(X3, y.values.ravel())\n\nbars1 = pd.Series(model_1.feature_importances_, index=X1.columns)\nbars2 = pd.Series(model_2.feature_importances_, index=X2.columns)\nbars3 = pd.Series(model_3.feature_importances_, index=X3.columns)\n\nbarWidth = 0.2\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \n# Make the plot\nplt.bar(r1, bars1, color='b', width=barWidth, edgecolor='white', label='var1')\nplt.bar(r2, bars2, color='#7f6d5f', width=barWidth, edgecolor='white', label='var2')\nplt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='var3')\n \n# Add xticks on the middle of the group bars\nplt.title('Feature importance', fontsize = 16)\nplt.xlabel('Feature', fontsize = 14)\nplt.xticks([r + barWidth for r in range(len(class_feat))], [r for r in class_feat])\nplt.rcParams['figure.figsize'] = (15, 6)\nplt.legend(['Mean', 'SE', 'Worst'])\nplt.show()\n","1681914c":"X_all_y = pd.concat([X, y], axis=1)\n\ncorr_full = X_all_y.corr(method='pearson')\ncorr_full = corr_full.round(1)\nmask = np.triu( np.ones_like(corr_full, dtype=np.bool) )\nax = sns.heatmap(corr_full, cmap='RdBu_r', vmin=-1.0, vmax=1.0, mask=mask, annot=True, square=True, linewidths=1.0, annot_kws={\"size\": 8}, cbar_kws={\"shrink\": 0.7})\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n# plt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Correlation Matrix', fontsize = 15)\nfig = plt.gcf()\nfig.set_size_inches(16, 16)\nplt.show()\n","3e853683":"model_full = RandomForestRegressor(); model_full.fit(X_N, y.values.ravel())\nvec_importance = model_full.feature_importances_\n\nbarWidth = 0.5\nbars = pd.Series(vec_importance, index=X_N.columns)\nr = np.arange(len(bars))\nplt.bar(r, bars, width=barWidth, edgecolor='white', label='var1')\nplt.xticks(rotation=45, ha=\"right\")\n \n# Add xticks on the middle of the group bars\nplt.title('Feature-Target relative importance', fontsize = 14)\nplt.xticks([r + barWidth for r in range(len(X.columns))], [r for r in X.columns], fontsize=10, weight=None)\nplt.ylabel('Feature', fontsize = 14)\nplt.rcParams['figure.figsize'] = (12, 3)\nplt.show()","e3f9ca01":"svc = SVC(kernel=\"linear\")\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), scoring='accuracy')\n\n# Xt = self_Normalize( X )\nrfecv.fit( self_Normalize( X ), y.values.ravel() )\nfeat_opt = rfecv.n_features_\n\nplt.figure(figsize=(10, 4))\nplt.xlabel(\"Number of features selected\", fontsize = 14)\nplt.xticks(fontsize=12)\nplt.ylabel(\"Correct classifications [%]\", fontsize = 14)\nplt.yticks(fontsize=12)\nplt.title(\"Recursive Feature Elimination\", fontsize = 14)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, lw=2.5, linestyle='-')\n\ny_0, y_max = np.min(rfecv.grid_scores_), np.max(rfecv.grid_scores_)\ny_rfe = np.arange(y_0, y_max, 0.0025)\nx_rfe = feat_opt*np.ones(len(y_rfe))\nplt.plot(x_rfe, y_rfe, c='black', lw=3, linestyle='--')\nplt.show()\n\n# Get N highest numbers' indices \nix_imp = sorted(range(len(vec_importance)), key = lambda sub: vec_importance[sub])[-feat_opt:] \n","ee89432b":"plt.rcParams['figure.figsize'] = (10, 5)\nclass_full =  class_feat + ['Diagnosis']\n\nX_train_N = X_N\n\n# Initialize the PCA method\npca_mn, pca_all = PCA(), PCA()\n\n# All Datasets\npca_all.fit(X_train_N)\npca_exp_all = pca_all.explained_variance_ratio_\n\n# # Mean Dataset\npca_mn.fit(X_train_N.iloc[:, ix_mn])\npca_exp_mn = pca_mn.explained_variance_ratio_\n\nt_all  = [*range(1, D.shape[1]-1)]\nt_solo = [*range(1, X_mn.shape[1]+1)]\n\ndef PCA_plot(t, pca_exp):\n    # ---------------------------------------------- #\n    # Instantiate the prinicipal (LHS) plot\n    pca_cum = np.cumsum(pca_exp)\n    fig, ax1 = plt.subplots()\n    color = 'tab:blue'\n\n    ax1.set_xlabel('n-th component', fontsize=16)\n    ax1.set_ylabel('Explained Variance Ratio (EVR)', color=color, fontsize=17)\n    ax1.plot(t, pca_exp, 'bo', color=color, markersize=7)\n    ax1.plot(t, pca_exp, '--', color=color, linewidth=2.5)\n    ax1.tick_params(axis=\"x\", labelsize=12)\n    ax1.tick_params(axis=\"y\", labelsize=12)\n\n    # ---------------------------------------------- #\n    # Instantiate a second axes that shares the same x-axis\n    ax2 = ax1.twinx()  \n    color = 'tab:green'\n\n    ax2.set_ylabel('Cumulative EVR', color=color, fontsize=17)  # we already handled the x-label with ax1\n    ax2.plot(t, pca_cum, 'go', color=color, markersize=7)\n    ax2.plot(t, pca_cum, '--', color=color, linewidth=2.5)\n    ax2.tick_params(axis=\"y\", labelsize=12)\n    t_score, t_loc = pca_cum[2], pca_cum[2]*1.025\n    ax2.annotate('%.2f '%(t_score*100)+'[%]', fontsize=18, xy =(3, t_loc), xytext =(3, t_loc*1.06), arrowprops = dict(facecolor ='green', shrink = 0.05),) \n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.xticks(t)\n    plt.show()\n\n\nPCA_plot(t_solo, pca_exp_mn)\n# PCA_plot(t_all,  pca_exp_all)\n","74644eb6":"# **************************************************************** #\n# ------------------------ 3D PCA scatter ------------------------ #\n# **************************************************************** #\n\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib.text import Annotation\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nfrom matplotlib.patches import FancyArrowPatch\nfrom mpl_toolkits.mplot3d import proj3d\nimport pdb\n\n\nclass Arrow3D(FancyArrowPatch):\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n\ndef PCA_scatter_3D(X_pca, max_Var, y, labels):\n    \n    y_M, y_B = y==1, y==0           # Logical statement for Benign indication\n    xs, ys, zs = X_pca[:,0], X_pca[:,1], X_pca[:,2]\n    s_x, s_y, s_z = 1.0\/(xs.max() - xs.min()), 1.0\/(ys.max() - ys.min()), 1.0\/(zs.max() - zs.min())\n    \n    # ---------- Scatter color by class ----------- #\n    # pdb.set_trace()\n    ax.scatter(xs[y_M]*s_x, ys[y_M]*s_y, zs[y_M]*s_z, s=42, c='orange', alpha=0.5) \n    ax.scatter(xs[y_B]*s_x, ys[y_B]*s_y, zs[y_B]*s_z, s=42, c='blue',   alpha=0.5)\n    n = max_Var.shape[0]\n\n    for i in range(n):\n        mean_x, mean_y, mean_z = max_Var[i,0], max_Var[i,1], max_Var[i,2]\n        a = Arrow3D([mean_x, 0.0], [mean_y, 0.0], [mean_z, 0.0], mutation_scale=15, lw=3, arrowstyle=\"<|-\", color=\"r\")\n        ax.add_artist(a)\n\n        if labels is None:\n            ax.text(max_Var[i,0]* 1.15, max_Var[i,1] * 1.15, max_Var[i,2] * 1.15, \"Var\"+str(i+1), color = 'g', fontsize=14, ha = 'center', va = 'center')\n        else:\n            ax.text(max_Var[i,0]* 1.15, max_Var[i,1] * 1.15, max_Var[i,2] * 1.15, labels[i],      color = 'g', fontsize=14, ha = 'center', va = 'center')\n\n\ndef PCA_reduction(X, PC_num):\n    scaler = StandardScaler()\n    scaler.fit(X)\n    X = scaler.transform(X)\n    pca = PCA()         # Perform PCA transformation\n    X_pca = pca.fit_transform(X)[:, 0:PC_num]             # Low dim : Projected  instances\n    max_Var = np.transpose(pca.components_[0:PC_num, :])  # Direction of maximum variance\n    return X_pca, max_Var\n\n\n\n# ------------- UNCOMMENT this section for execution ------------- #\n\n# # ----------------- Visualization ----------------- #\n# PC_num = 3          # Dimensionallity reduction to 3D\n# plt.rcParams['figure.figsize'] = (5, 3)\n# ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n# X_pca, max_Var = PCA_reduction(X_mn, PC_num)            #\n# PCA_scatter_3D(X_pca, max_Var, y['Diagnosis'], class_feat)   # Call PCA function\n\n# ax.legend(['Malignant', 'Benign'], fontsize=15, loc='best')\n# pc_1, pc_2, pc_3 = pca_exp_mn[0]*100, pca_exp_mn[1]*100, pca_exp_mn[2]*100\n# ax.set_xlabel('Dim-1 : %.2f [%%]'%pc_1, fontsize=13)\n# ax.set_ylabel('Dim-2 : %.2f [%%]'%pc_2, fontsize=13)\n# ax.set_zlabel('Dim-3 : %.2f [%%]'%pc_3, fontsize=13)\n# ax.xaxis.pane.fill, ax.yaxis.pane.fill, ax.zaxis.pane.fill = False, False, False\n# ax.view_init(elev=45, azim=-120) # PC1 || PC2\n# plt.show()\n\n# ----------- UNCOMMENT the above section for execution ---------- #\n# ---------------------------------------------------------------- #\n# ---------------------------------------------------------------- #\n# ---------------------------------------------------------------- #\n# ---------------------------------------------------------------- #\n# ---------------------------------------------------------------- #\n# ---------------------------------------------------------------- #\n\n# **************************************************************** #\n# ------------------------ 2D PCA scatter ------------------------ #\n# **************************************************************** #\n\n# from sklearn.preprocessing import StandardScaler\n\n# # In general, it's a good idea to scale the data prior to PCA.\n# scaler = StandardScaler()\n# # xx = x_train_N.iloc[:, 2:]\n# xt = x_train_N.iloc[:, ix_mn]\n# scaler.fit(xt)\n# xt = scaler.transform(xt)\n# pca = PCA()\n# x_new = pca.fit_transform(xt)\n\n# def PCA_scatter(X, coeff, y_M, labels):\n    \n#     n = coeff.shape[0]\n#     xs, ys = X[:,0], X[:,1]             # zs = X[:,2]\n#     scalex, scaley = 1.0\/(xs.max() - xs.min()) , 1.0\/(ys.max() - ys.min())\n#     # scalez = 1.0\/(zs.max() - zs.min()) <-- 3D of Z\n    \n#     # ---------- Scatter color by class ----------- #\n#     plt.scatter(xs[y_M] * scalex, ys[y_M] * scaley, c = 'orange', alpha=0.5) \n#     plt.scatter(xs[1-y_M==True] * scalex, ys[1-y_M==True] * scaley, c = 'blue', alpha=0.5)\n#     # plt.scatter(xs * scalex, ys * scaley, c = 'blue')\n\n#     for i in range(n):\n#         plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'r',alpha = 0.5)\n#         if labels is None:\n#             plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n#         else:\n#             plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', fontsize=12, ha = 'center', va = 'center')\n    \n#     plt.xlabel(\"PC{}\".format(1))\n#     plt.ylabel(\"PC{}\".format(2))\n#     plt.grid(linestyle='-', linewidth=0.5)\n\n# #Call the function. Use only the 2 PCs\n# y_M, y_B = y_train, 1-y_train\n# label_diag = pd.DataFrame(columns=['M', 'B'])\n# label_diag['M'], label_diag['B'] = y_train, 1-y_train\n# y_M = label_diag.iloc[:, 0]==True\n\n# pca_i = 2\n# # pca_exp = pca.explained_variance_ratio_\n# PCA_scatter(x_new[:, 0:pca_i], np.transpose(pca.components_[0:pca_i, :]), y_M, class_feat)\n# # plt.xlabel('Dim-1 : %.2f [%%]'%pca_exp[0], fontsize=12)\n# # plt.ylabel('Dim-2 : %.2f [%%]'%pca_exp[1], fontsize=12)\n# plt.rcParams['figure.figsize'] = (10, 6)\n# plt.show()\n","361e746d":"# **************************************************************** #\n# ----------------- Import Classification Tools ------------------ #\n# **************************************************************** #\n\n# Feature Extraction to k-dim (k < 30)\nX_reduced = X.iloc[:, ix_imp]\n# Feature Compression to 3-dim\nx_pca, _ = PCA_reduction(X_reduced, PC_num=3)\n\nif not isinstance(x_pca, pd.DataFrame):\n    x_pca = pd.DataFrame(x_pca, index=y.index) # Note that indices should be coherent\nX_pca = self_Normalize(x_pca)\n\nXX_tr, yy_tr = X_pca.loc[ix_train], y.loc[ix_train]['Diagnosis'].values\nXX_ts, yy_ts = X_pca.loc[ix_test],  y.loc[ix_test]['Diagnosis'].values\n\n# **************************************************************** #\n# ---------------- Comparison Between Classifiers ---------------- #\n# **************************************************************** #\n\ndef Classification(model, X_train, y_train, X_test, y_test, clf_name, print_result):\n    random_state = np.random.RandomState(0)\n    model.fit(X_train, y_train)   \n\n    score_train, score_test = model.score(X_train, y_train)*100, model.score(X_test, y_test)*100\n    # Predicting on Test-set\n    y_pred = model.predict(X_test)\n\n    # confusion matrix \n    cm = confusion_matrix(y_test, y_pred)\n\n    # classification report\n    cr = classification_report(y_test, y_pred, digits=4)\n    \n    if print_result:\n        # Calculating the accuracies\n        print('--- '+clf_name+' ---')\n        print('Training : %.2f [%%] \\nTest :     %.2f [%%]\\n'%(score_train, score_test))\n\n        # Print confusion matrix\n        plt.rcParams['figure.figsize'] = (3, 3)\n        sns.heatmap(cm, annot = True, cmap = 'winter')\n        plt.title('Confusion Matrix', fontsize = 14)\n        plt.show()\n        print(cr)\n        \n    return model, cr\n    \ndef Report(model, X_test, y_test, cr):\n    model.fit(X_train, y_train) \n    clf_probs = model.predict_proba(X_test)\n    # keep probabilities for the positive outcome only\n    clf_probs = clf_probs[:, 1]\n    # calculate scores\n    clf_auc = roc_auc_score(y_test, clf_probs)\n    # calculate roc curves\n    fpr, tpr, _ = roc_curve(y_test, clf_probs)\n    return cr, fpr, tpr, clf_auc\n","3beaf886":"# -------------- List of Classifier -------------- #\nClf_dict = dict()               # Create a dicitionary of several classifiers \nClf_dict[0] = AdaBoostClassifier(n_estimators=100, random_state=0)\nClf_dict[1] = DecisionTreeClassifier(random_state=0)\nClf_dict[2] = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\nClf_dict[3] = GaussianNB()\nClf_dict[4] = GradientBoostingClassifier(random_state=0)\nClf_dict[5] = KNeighborsClassifier()\nClf_dict[6] = LogisticRegression()\nClf_dict[7] = RandomForestClassifier(n_estimators=30, max_depth=None, min_samples_split=2, random_state=0)\nClf_dict[8] = svm.SVC(probability=True)\n\n\nclf_names = ['Adaboost', 'Decision Tree', 'Extra Tree', 'Gaussian Naive Bayes', 'Gradient Boosting',\n                      'KNN', 'Logistic Regression', 'Random Forest', 'SVM']\n\n\ni = np.random.randint(0, len(clf_names))\n_, _ = Classification(Clf_dict[i], XX_tr, yy_tr, XX_ts, yy_ts, clf_names[i], print_result=1)\n\n","aba744b1":"def extract_values( Str ): \n    # ---------- Extract meterics from string ---------- #\n    get_acc = Str.split('accuracy')[1].split('\\n')[0]\n    Acc = float(get_acc.strip()[0:6])           # Remove irrelevant chars\n\n    get_line = Str.split('weighted avg')[1].split('\\n')[0]\n    get_line = get_line.replace(\" \", \"\")        # remove irrelevant chars\n\n    Pr = float(get_line[0:6])\n    Rc = float(get_line[6:12])\n    F1 = float(get_line[12:18])\n    return [Acc, Pr, Rc, F1]\n\nclf_names_n = ['Adaboost', 'Decision \\nTree', 'Extra Tree', 'Gaussian \\nNaive Bayes', 'Gradient \\nBoosting',\n                      'KNN', 'Logistic \\nRegression', 'Random \\nForest', 'SVM']\n\nc_report, fpr, tpr, auc = dict(), dict(), dict(), dict()\nfor i in range(len(Clf_dict)):\n    model, cr = Classification(Clf_dict[i], XX_tr, yy_tr, XX_ts, yy_ts, clf_names[i], print_result=0)\n    c_report[i], fpr[i], tpr[i], auc[i] = Report(model, X_test, y_test, cr)\n    \n\nlen_X = len(Clf_dict)\nAcc, Pr, Rc, F1 = np.zeros(len_X), np.zeros(len_X), np.zeros(len_X), np.zeros(len_X)\n\nfor i in range(len_X):\n    Acc[i], Pr[i], Rc[i], F1[i] = np.asarray( extract_values( c_report[i] ) )\n    \n\nbarWidth = 0.125\nr1 = np.arange(len_X)\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\n","1c3012b8":"# Make the plot\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.bar(r1, Acc, color='#3498db', width=barWidth, edgecolor='white', label='Accuracy')\nplt.bar(r2, Pr,  color='#e74c3c', width=barWidth, edgecolor='white', label='Precision')\nplt.bar(r3, Rc,  color='#2d7f5e', width=barWidth, edgecolor='white', label='Recall')\nplt.bar(r4, F1,  color='yellow', width=barWidth, edgecolor='white', label='F1 score')\n \n# Add xticks on the middle of the group bars\nplt.title('Performance comparison', fontsize = 16)\nplt.xlabel('Classifier', fontsize=14)\nplt.xticks([r + barWidth for r in range(len(clf_names))], [r for r in clf_names_n], fontsize=11, weight='bold')\nplt.yticks(fontsize=13)\n\nmin_value = np.concatenate( (Acc, Pr, Rc, F1) ).min()\nplt.ylim([0.95*min_value, 1.0]) \nplt.rcParams['figure.figsize'] = (18, 4)\nplt.legend(['Accuracy', 'Precision', 'Recall', 'F1 score'], fontsize=12, loc='best')\nplt.show()","21decff3":"# No skill classifier (random choice)\nns_probs = [0 for _ in range(len(y_test))]\nns_auc = roc_auc_score(y_test, ns_probs)\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n\nplt.rcParams['figure.figsize'] = (9, 5)\nplt.plot(ns_fpr, ns_tpr, lw=3, linestyle='--', label='No Skill')\nclf_names = [item.replace('\\n', '') for item in clf_names] # Remove down-line\n\nLegend = []\nLegend.append('No Skill  (AUC = 0.500)')\nfor i in range(len(c_report)):\n    plt.plot(fpr[i], tpr[i], lw=3, alpha=.8, marker='.')\n    Legend.append(clf_names[i]+'  (AUC = '+'%.3f' % auc[i]+')')\n\nplt.title('Receiver operating characteristic', fontsize = 16)\nplt.legend(Legend, loc='best', fontsize=10)\nplt.xlabel('False Positive Rate', fontsize=15)\nplt.xticks(fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=15)\nplt.yticks(fontsize=12)\nplt.show()\n","687bfd91":"from mlxtend.plotting import plot_decision_regions\nimport matplotlib.gridspec as gridspec\nimport itertools\n\n# Loading some example data\nx_pca, _ = PCA_reduction(X_reduced, PC_num=2)\nX_pca = self_Normalize(x_pca)\n\npx0, px1 = 1.25, 0.66\ngs = gridspec.GridSpec(3, 3)\nfig = plt.figure(figsize=(11, 11))\nx_min, x_max, y_min, y_max = X_pca[:, 0].min(), X_pca[:, 0].max(), X_pca[:, 1].min(), X_pca[:, 1].max()\n\nCLF = [Clf_dict[i] for i in range(9)]\nyy = y.values.ravel()\n\nfor clf, lab, grd in zip(CLF, clf_names, itertools.product([0, 1, 2], repeat=2)):\n    clf.fit(X_pca, yy)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_pca, y=yy, clf=clf, legend=2)\n    plt.xlim([x_min*px0, x_max*px1])\n    plt.ylim([y_min, y_max]) \n    plt.title(lab, fontsize=16)\n\nplt.show()\n","5f7e2078":"<a id=\"9\"><\/a>\n## $\\text{Discussion}$\nIn this project I examined the use of several classification techniques for breast cancer diagnosis, after reducing the raw data to a lower representation form. All ML algorithms exhibited high performance on the binary classification, as measured by the chosen metrics.\n","a2d5c73e":"After found valid, that dataset is shuffled randomly :\n<!-- Consider ten random rows of the raw-data : -->","772eedf3":"<a id=\"7\"><\/a>\n## $\\text{Results}$\n\nLet us start off with the binary classification restults using confusion matrix. \n\nDue to Kaggle limitation to Widget-tabs, **each execution** presents a single **random** classifiers :","670720e9":"### $\\text{Principal component analysis (PCA)}$\n\nPCA is a practical step in dimensionality reduction, by projecting the data from a high-dimensional space onto a lower one. The transformation reveals the internal structure that explains most (maximizes) the data variance :","a0f03439":"Interestingly, several suspicious differences were found, thereby encouraging researchers to further understand what dominant features may affect the decision rule :\n \n<center><img src=\"https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Latex\/Images\/diagnosis.png?raw=true\" width=\"800px\"\/><\/center>\n","ee0e48df":"### $\\text{Metrics}$\n\nAn evaluation metric is a function that measures a classifier's performance, thus allows comparison between several models. From left is a confusion matrix, which defines different combinations for each indication : \n \n<center><img src= https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Latex\/Images\/Mat_conf.png?raw=true width=\"630px\" \/><\/center>\n\n$\\hspace{11.5cm}$ ( **T** - True, **F** - False, **P** - Positive, **N** - Negative )\n\nCombining the metrics improves understanding of the big picture, and enables to overcome data imbalance. Optimality is obtained by minimizing false indications (**FP, FN**)","754e3438":"<a id=\"5\"><\/a>\n## $ \\text{Feature engineering} $\n\n$(i) \\ \\underline{\\text{Rows}} \\ 1 : (n-1) : \\,$ Based on the linear regression assumptions, the **independent variables** \n should not be correlated with each other (AKA **Multicollinearity**).\n  As can be seen, the Radius ($\\, r \\,$) and the Perimeter ($\\, P\\, $) have almost $\\, 100 \\% \\,$ correlation score, apparently since most samples are circular and $\\,P = f(r) \\,$. Therefore,\n  one of the feature is considered **redundant** and can be removed in order to improve later the model accuracy and reduce overfitting.\n\n$(ii)  \\ \\underline{n\\text{-th Row}} : \\,$ Contrarily to $(i)$, here the correlation expresses the feature importance relevance to the **target**. For example, the Radius, Texture, Area, Concavity and Concave points exhibit ($\\geq 70 \\% $) , while Fractal dimension $(\\approx 1 \\%)$. \n\nUsing random forest regressor [[$2$](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest%20regressor#sklearn.ensemble.RandomForestRegressor)], the $(ii)$ findings can be validated as follows :\n\n","e81effa1":"### $\\text{Classifiers}$\nThe following list of classifiers was utilized in search of an optimal candidate : \n\n$\\quad$ | $\\text{ML Algorithm}$ | $\\quad$\n---        |         ---          |     ---\n$\\text{AdaBoost} \\quad$ | $\\text{Gaussian Naive Bayes } \\ $| $\\text{KNN}$\n$\\text{Decision Trees}$ | $\\text{Gradient Boosting }$      | $\\text{SVM}$\n$\\text{Extra Trees }$   | $\\text{Logistic Regressio }$     | $\\text{Random Forest}$\n","cd26861f":"First of all, the data integrity should be checked for NaN or missing values :","77808f6a":"The figure above presents PCA on case (**a**) ( $X^{(a)}_{\\text{Mean}} \\in \\mathbb{R}^{n \\times 10}$ ). From left is the PCA's ability to capture proportional variance (EVR), and from right is the cumulative sum.\n\nLet $\\, \\mathbf{W} \\in \\mathbb{R}^{10\\times10} \\, $ be a weights matrix whose columns are the eigenvectors of $X^T X$. By choosing $\\mathbf{W}$'s top 3 eigenvectors, the observations can be projected onto a **3D** space :\n","6e267957":"This analysis shows not only the $(B\/M)$ differences but also the way each feature distributes. Smoothness and Fractal dimension distributes similarly, hence helpless in the classification attempt. Contrarily, Concavity and Radius show significant difference.","e8b1f83a":"And by using FPR-TPR values, compare the obtained **AUC-ROC** :","204e1f73":"<a id=\"1\"><\/a>\n## $\\text{Introduction}$\n\nModern AI developments constantly reduce the gap between ML algorithms and humans. Back in the 90's, the \"Wisconsin Breast Cancer Dataset\" ([WBCD](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)) )) was released, and soon gained a recognition as one of the first milestones of AI. In the absence of any advanced computational equipment, scientists were focused on extraction of meaningful insights, in attempt to distinguish between the samples :\n\n<center><img src=\"https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Latex\/Images\/Benign_Malignant.png?raw=true\" width=\"700px\" \/><\/center>","80a1e89f":"[ <img src='https:\/\/github.com\/Daniboy370\/Uploads\/blob\/master\/logo-Medium.png?raw=true' width=175 \/> ](https:\/\/medium.com\/@danielengelsman\/say-hello-to-doctor-ai-b32448ed5d52)","025410ac":"<a id=\"6\"><\/a>\n## $\\text{Modelling}$\n\nThe learning procedure defines a model which associates the correct label for each input sample - $\\, y_i = f(x_i) \\,$. The performance metric on the output space denotes the cost of wrong labeling.","d483a15c":"### $ \\text{Correlation Matrix} $\n\nIn order to better understand which features contribute most, an efficient method can be calculating the Pearson correlation coefficients to obtain a correlation matrix. Thereby, the strength of linear relationships are obtained in two aspects [[$1$](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient)] :\n\n* $+1 :\\ $ perfect **positive** linear correlation\n* $\\ \\ \\, 0 :\\ $ **no** linear correlation\n* $-1 :\\ $ perfect **negative** linear correlation\n\n$(i) \\hspace{2mm} $ **feature vs. feature** - Every pair of two independent variables :\n\n$$ \n\\text{corr}(X) = \\begin{bmatrix}\n1 & \\frac{\\operatorname{E}[(X_1 - \\mu_1)(X_2 - \\mu_2)]}{\\sigma(X_1)\\sigma(X_2)} & \\cdots & \\frac{\\operatorname{E}[(X_1 - \\mu_1)(X_n - \\mu_n)]}{\\sigma(X_1)\\sigma(X_n)} \\\\ \\\\\n \\frac{\\operatorname{E}[(X_2 - \\mu_2)(X_1 - \\mu_1)]}{\\sigma(X_2)\\sigma(X_1)} & 1 & \\cdots & \\frac{\\operatorname{E}[(X_2 - \\mu_2)(X_n - \\mu_n)]}{\\sigma(X_2)\\sigma(X_n)} \\\\ \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n \\frac{\\operatorname{E}[(X_n - \\mu_n)(X_1 - \\mu_1)]}{\\sigma(X_n)\\sigma(X_1)} & \\frac{\\operatorname{E}[(X_n - \\mu_n)(X_2 - \\mu_2)]}{\\sigma(X_n)\\sigma(X_2)} & \\cdots & 1\n\\end{bmatrix} \\hspace{4cm}\n$$\n\nUnlike the off-diagonal entries, the principal diagonal denotes the correlation of each random variable with itself $(= 1)$.\n\n$(ii)$ **feature vs. target** - Every pair of independent and target variable (*diagnosis*) :\n\n$$ \\rho_{X,Y} = \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y} \\hspace{6cm} $$\n\nThe following heatmap presents both calculations but $(ii)$ appears only on the buttom row. \n>Note that due to the symmetry, it is suffice to present the lower triangular.\n","d0151eea":"### $\\text{Key takeaways}$ :\n\n**True to all** : \n\nThe highest values at **any** parameter is obtained from the Malignant diagnosis.\n In addition, the following feature seem to exhibit an almost negligible difference : \n \n$\\quad \\bullet$ similarity - $d(B,M) \\approx 0$ : Fractal dimension. \n\n**c** $:=$ Worst (mean of 3 largest values) :\n\nAny point of dataset ('c') is greater than any of the original ('a') dataset.\nThat is not surprising as dataset ('c') consists of ('a')'s three largest values. \n\n**b** $:=$ SE $(\\frac{\\sigma}{\\sqrt{n}})$ : \n\nThe differences in the SE comparison are significantly smaller, which imply that the SE group distributes uniformly.\nNonetheless, it makes sense as both $\\ (B\/M) \\ $ are calculated over a common normalized standard deviation.\n\n**a** $:=$ Mean $(\\mu)$ :\n\nThis is the main comparison as it manages to exhibit the main dissimilarities between $\\ (B\/M) \\ $ :\n\n$\\quad \\bullet$ Proximity - $d(B,M) \\leq 0.1$ :$\\ $ Symmetry, Smoothness\n\n$\\quad \\bullet$ Dissimilarity - $d(B,M) > 0.1$ : $\\ $ Radius, Perimeter, Perimeter, Area, Compactness, Concavity, Concave points\n\n","d40f5700":"Concentrate it in one table for comparative analysis :","14cd657e":"#### $\\text{ROC-AUC}$\n\nAn **ROC** reflects a binary classifier ability to discriminate classes, using a probabilistic analysis. Each threshold is point on the **ROC** graph, denoting the TPR\/FPR tradeoff.\n \n* What would happen if we took features that scored poorly in the correlation matrix ? \n\n<center><img src= https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Latex\/Images\/ROC_bad.png?raw=true  width=\"575px\" \/><\/center>\n\nFeatures with low-ranked contribution to the explained variable perform poorly, slightly above a random decision (**AUC**=0.5). The AUC is the area under the ROC, which expresses the prediction success rate from 0-1.\n\n**Ideally**, the perfect classification will exhibit a $ \\Gamma$-shape that crosses the (0, 1) point in the FPR-TPR plane. Meaning that there exists a threshold with $100 \\%$ correct indications.","3db3ce46":"Similarly as before, the feature importance will be examined now the most important features on the entire dataset :","4edb535a":"### $\\text{Table of Contents}$\n\n* [1. Introduction](#1)\n* [2. Exploratory Data Analysis (EDA)](#2)\n* [3. Data composition](#3)\n* [4. Statistical analysis](#4)\n* [5. Feature engineering](#5)\n* [6. Modelling](#6)\n* [7. Results](#7)\n* [8. Decision Boundary](#8)\n* [9. Discussion](#9)","5b290747":"<a id=\"3\"><\/a>\n### $\\text{Data composition}$\n\nThe binary target variable $\\big($ <font color='blue'>  B <\/font> $\\ \/ \\ $ <font color='orange'>  M <\/font> $\\big)$ can act as a helpful tool to better understand the data distribution :\n","c51b63a2":"### $ \\text{Biplot} $\n\nEach variable (feature) that went into PCA has an associated red <font color='red'> arrow <\/font> (after scaling factor), in the directions that maximize each of the principal component's variance. \nHere, the Concave points feature (strong correlation), maximizes the **1st PC**. Contrarily, Fractal dimension and Symmetry (weak correlation), contribute poorly to the **3rd PC**. Therefore, the **full** feature space will undergo :\n \n$$ \nX_0 \\in \\mathbb{R}^{n \\times 30} \\quad \\underset{\\text{Feature selection}}{\\Rightarrow} \\quad X_1 \\in \\mathbb{R}^{n \\times d_{sel}} \\quad \\underset{\\text{Dim. compression}}{\\Rightarrow} \\quad X_2 \\in \\mathbb{R}^{n \\times 3}\n$$\n\nSuch that all that is left to do, is to train a prediction function $\\, f : X_2 \\rightarrow Y \\,$ that will be able to classify the compressed data correctly, in terms of selected metrics.","bc74afd2":"<img src=\"https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Animation\/PCA_vid.gif?raw=true\" width=\"700px\">","3f61dbb0":"It is defenitely interesting to see how different decision boundaries are obtained by different classifiers.","fe384853":"<a id=\"8\"><\/a>\n### $\\text{Bonus : Decision boundary}$\n\nConsider an additional PCA, this time after projecting the full data to a 2D plane. Using the amazing [[mlxtend](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/plotting\/plot_decision_regions\/)] library, the decision boundary obtained by each one of the classifiers, can be demonstrated as :  : \n","bf233bad":"### $\\text{Limitation of the study}$\n\n**Prior engineering** : The dataset as it is publicly available, is already after analysis with specific features, chosen by the researchers. Thereby, the user has no access to the raw image scans at full dimensionality. Thus leaving fewer options for action.\n\n\n**Low-dimensional representation** : The process of extraction (by correlation), and then projection (by PCA), ends up with 10 times smaller feature space. Both operations rely on existence of linear relationships between the features. This hidden presumption may not always be true, and it might ignore important features that simply fail to score well.\n\n**Dataset aspect** : The dataset contains a modest amount of samples, distributed at an imbalanced $(B\/M)$ ratio of $1.684$. During training, most classifiers reached an accuracy of $100 \\%$ (unlike test results), probably due to overfitting. However, rebalance technique that involved up-sampling of the malignant class, did not show any dramatic improvement.","3360f1e7":"The above table is only a short list of functions that can be helpful in providing early evaluation of the data.\nI chose to focus on the most informative one - the **mean** function, as it delivers a clear comparison between the **centres**. \nHowever, the **range** of the raw data varies widely thanks to several elements, so appropriate normalization is required.\n\n### $\\text{Rescaling (min-max normalization)}$\n\nFor convenience the dataset will be shifted to the minimum value, and then normalized by its own range.\nThis will remove dimensional units, and will ease the $\\ (B\/M) \\ $ comparison with respect to a common mean :\n\n$$ \\hat{X} = \\frac{ X - \\text{min}(X) }{ \\text{max}(X) - \\text{min}(X)} \\ \\in \\ [0, 1]^{n} $$\n\nConsider the mean of case **a** $\\big( \\ \\hat{X}_{Mean}^{(a)} \\ \\big)$, and compare between the two subgroups $\\ $:  $\\ \\hat{\\overline{X}^a}_{\\{ y=M \\}} \\ \\text{vs.} \\ \\hat{\\overline{X}^a}_{\\{ y=B \\}} \\ $ :","9cb1fde9":"### $ \\text{Feature elimination} $\nOut of 30 features, many of them seem to be redundant. As seen before, the Concave points and the perimeter contribute by far more than any other feature. Note that five features manage to contribute up to $80 \\%$ of the importance. Using recursive feature elimination (RFE), the features are sorted such that high-ranked (informative) features **increase** the model accuracy (**---** line). ","b1a24f8b":"#### $\\text{The workflow}$\n\n<center><img src=\"https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Latex\/Images\/pipeline.png?raw=true\" width=\"800px\" style=\"width:800px\" \/><\/center>","8367e10a":"After applying a random training\/test split ratio of  $\\ 80 \/ 20 \\ [\\%]$ , the target distribution becomes :","99a3175e":"<center><img src=\"https:\/\/media.giphy.com\/media\/STajXA50HIixG\/giphy.gif\" width=\"300px\"\/><\/center>\n\n$\\quad$\n\n$$ \n\\text{Namaste ! }\n$$","88552e01":"Despite not being perfectly equal, the data imbalance can be said to be reasonable.Let us analyze \nthe **three** different cases $\\, \\big( \\text{a} $ = mean,$\\ \\text{b}$ = SE$, \\text{c}$ = worst $\\big)$ with respect to **four** aspects $\\big( \\ \\mu, \\sigma, \\underset{x}{\\min}(), \\underset{x}{\\max}() \\ \\big)$ :","71770820":"It can be seen, that all of the classifiers succeeded well on the discrimination task.","c8e2ab33":"Across three cases, same features seem to exhibit similar behavior, in a manner that coincides with each correlation matrix. However, in order to extract maximum relevance from entire $X$, the correlation matrix should be applied on **all 30 features** :","fb761fad":"Contrastingly, further addition of low-ranked features **does not** contribute any improvement.","001c8821":"$\\underline{\\text{Data specification}}$ \n\n${\\text{Number of samples }} : n = 569  \\quad {\\text{Full dataset}} : \\ D = \\{ X, Y \\} = \\{ x_i, y_i \\}_{i=1}^{n}$\n\n$ \\text{Input space : }  \\dim(X) = ( \\underset{\\text{case :}}{n \\times 10}) \\times \\underset{\\text{a, b, c}}{3} \\quad \\text{Output space : }\\dim(Y) = n $ \n\n$\\underline{\\text{Dependent variable \/ Target}} \\ \\big( \\ y_i = \\{B, \\, M  \\} \\ \\big)$\n\n* **Diagnosis** - Binary indication whether a sample is Benign $(0)$ or Malignant $(1)$.\n\nFeature space $X$ consists of three different analysis cases (same features) : \n\n**a** $:=$ Mean $(\\mu) =  X_{[:, \\, 1:10]} \\quad$ \n**b** $:=$ SE $(\\frac{\\sigma}{\\sqrt{n}})  = X_{[:, \\, 11:20]} \\quad$\n**c** $:=$ Worst (3 max-values) $ = X_{[:, \\, 21:30]}$\n\nFor example, let us examine the $i$-th sample from the mean case (**a**) :\n\n$\\underline{\\text{Independent variable \/ Feature}} \\,\\ \\big( \\ x_i \\in \\mathbb{R}^{10} \\ \\big)$\n* **Radius** - mean of distances from center to points on the perimeter.\n* **Texture** - standard deviation of gray-scale value.\n* **Perimeter** - $P = \\sum_{i=1}^n l_i \\quad $ (Given  $n$  vertices)\n* **Area** - $\\quad A = \\frac{1}{2} \\big( \\sum_j w_j z_{j+1} - z_{j} w_{j+1} \\big) \\quad$ (Given  $n$  vertices on WZ plane)\n* **Smoothness** - Local variation in radius lengths.\n* **Compactness** - ($P^2$ \/ $A$ - 1.0)\n* **Concavity** - Severity of concave portions of the contour.\n* **Concave points** - Number of concave portions of the contour.\n* **Symmetry** - Relative difference of two half-planes.\n* **Fractal dimension** - (\"coastline approximation\" - 1)\n\nTo sum up, we can say that the dataset fulfills - $X = [X^{(a)}_{\\text{Mean}}, \\, X^{(b)}_{\\text{SE}}, \\, X^{(c)}_{\\text{Worst}}] \\in \\mathbb{R}^{(n \\times 10) \\times 3}$ :\n\nFeature | Mean | SE | Worst\n--- | --- | --- | ---\n1.Radius | $\\quad \\vdots$ |  $\\ \\vdots$ |  $\\quad \\vdots$\n$\\quad \\cdots$ | $\\quad \\vdots$ |  $\\ \\vdots$ |  $\\quad \\vdots$\n10.Fractal dim. | $\\quad \\vdots$ |  $\\ \\vdots$ |  $\\quad \\vdots$\n","d112b324":"<a id=\"2\"><\/a>\n## $\\text{Exploratory Data Analysis (EDA)}$\n\nIn this section the dataset will be analyzed to come up with the main characteristics that could shed light on the nature of the data. The analysis will assist to find out which features are more crucial in predicting whether a sample is cancerous or not. \n\n$\\bullet \\ $ **NOTE :** Along the notebook the code is hidden for aesthetic reasons (*double click to open*).","d08321b8":"<a id=\"4\"><\/a>\n### $ \\text{Statistical analysis} $\n\nThe following table maps each independent variable onto a row and column in order to explore bivariate relationships\n between every feature pair. The **lower** triangular presents a raw scatter plot of the joint probability distribution $P_{XY}(x, y)$. \n\n* The **upper** triangular presents the same, but using smoothed contour lines by kernel density estimation (KDE).\n\n* The **diagonal** exhibits the marginal distribution of both variables - $P_{X}(x) \\, , \\, P_{Y}(y)$ .\n"}}