{"cell_type":{"ece57729":"code","0acfb8ef":"code","223f6c74":"code","f63f5c2c":"code","7f7aff94":"code","063f8b27":"code","6f8cc978":"code","1a00f8c0":"code","ecd1157f":"code","7270b00d":"code","17369bec":"code","76490cce":"markdown","d7db2ea7":"markdown","4d0edad0":"markdown","a12c75c8":"markdown","fca047c5":"markdown","8bb9f77f":"markdown"},"source":{"ece57729":"import datatable as dt\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom torch.nn import CrossEntropyLoss\nfrom sklearn.metrics import log_loss, accuracy_score","0acfb8ef":"print('Loading data...')\n\ntrain_datatable = dt.fread('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ndf = train_datatable.to_pandas()\ndel train_datatable\n\ndisplay(df)","223f6c74":"def utility_score_bincount(date, weight, resp, action):\n    '''\n    Credits to Kaggle user Lindada with this implementation of the utility score for the competition\n    '''\n    \n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","f63f5c2c":"def preprocessing(df):\n    \n    # Someone observe that it is better using train.date > 85\n    # train = train.loc[train.date > 85].reset_index(drop=True)\n    \n    # Add action column based on the resp\n    df['action'] = (df['resp'] > 0).astype('int')\n    \n    # NaN values: fill with mean\n    fill_val = df.mean()\n    df = df.fillna(fill_val)\n    \n    # Split the training and validation data, leave the last 50 dates for validation\n    valid = df.loc[(df.date >= 450) & (df.date < 500)].reset_index(drop=True)\n    train = df.loc[df.date < 450].reset_index(drop=True)\n    \n    # Save validation set for testing\n    valid.to_csv('\/kaggle\/working\/val.csv')\n    \n    return train, valid\n    ","7f7aff94":"class Dataset:\n    \n    def __init__(self, data):\n        feat_cols = [f'feature_{i}' for i in range(130)]\n        self.features = data[feat_cols].values\n        self.label = data['action'].values\n        \n    def __getitem__(self, idx):\n        return {\n                'features': torch.tensor(self.features[idx], dtype=torch.float),\n                'label': torch.tensor(self.label[idx], dtype=torch.float)\n                }\n    \n    def __len__(self):\n        return len(self.features)","063f8b27":"class Model(nn.Module):\n    \n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.linear1 = nn.Linear(129, 256)\n        self.dropout1 = nn.Dropout(p=0.1)\n        self.BatchNorm1d_1 = nn.BatchNorm1d(256)\n        \n        self.linear2 = nn.Linear(256, 128)\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.BatchNorm1d_2 = nn.BatchNorm1d(128)\n        \n        self.linear3 = nn.Linear(128, 64)\n        self.dropout3 = nn.Dropout(p=0.1)\n        self.BatchNorm1d_3 = nn.BatchNorm1d(64)\n        \n        self.linear4 = nn.Linear(65,1)\n        \n        self.sigmoid = nn.Sigmoid()\n        \n        self.LeakyReLU = nn.LeakyReLU(inplace=True)\n  \n\n    def forward(self,x):\n        \n        x_continuous = x[:,1:]\n        x_binary = x[:,0] # Take out the only binary variable\n        \n        out = self.linear1(x_continuous)\n        out = self.LeakyReLU(out)\n        out = self.dropout1(out)\n        out = self.BatchNorm1d_1(out)\n        \n        out = self.linear2(out)\n        out = self.LeakyReLU(out)\n        out = self.dropout2(out)\n        out = self.BatchNorm1d_2(out)\n        \n        out = self.linear3(out)\n        out = self.LeakyReLU(out)\n        out = self.dropout3(out)\n        out = self.BatchNorm1d_3(out)\n        \n        \n        out = torch.cat((out, x_binary.unsqueeze(1)),dim=1) # Combine the binary variable back\n        out = self.linear4(out)\n        out = self.sigmoid(out)\n        \n        return out","6f8cc978":"def train_one_epoch(model, optimizer, dataloader, loss_fn, device):\n    '''\n    Train for one epoch and returns training loss\n    '''\n    \n    model.train()\n    total_loss = 0\n    \n    for batch in dataloader:\n        optimizer.zero_grad()\n        x = batch['features'].to(device)\n        y = batch['label'].to(device)\n        \n        output = model(x)\n        loss = loss_fn(output.squeeze(),y.squeeze())\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss\n        \n    return total_loss\/len(dataloader)\n\n\ndef validation(model, dataloader, device, valid):\n    \"\"\"\n    Function for validation at the end of each epoch\n    \"\"\"\n    \n    model.eval()\n    prediction = []\n    ground_truth = []\n    \n    for batch in dataloader:\n        x = batch['features'].to(device)\n        y = batch['label'].to(device)\n        with torch.no_grad():\n            output = model(x)\n        prediction.append(output.squeeze().detach().cpu().numpy())\n        ground_truth.append(y.squeeze().detach().cpu().numpy())\n        \n    prediction = np.concatenate(prediction).reshape(-1,1).squeeze()\n    ground_truth = np.concatenate(ground_truth).reshape(-1,1).squeeze()\n    \n    logloss = log_loss(ground_truth, prediction) # Calculate log loss of the predicted probability\n    \n    pred_label = np.where(prediction >= 0.5, 1, 0).astype(int) # Assign label based on predicted probability\n    \n    accuracy = accuracy_score(ground_truth, pred_label) # Calculate accuracy based on predicted label\n    \n    # Calculate utility score (cumulative returns)\n    utility = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,\n                                                   resp=valid.resp.values, action=pred_label)\n        \n    return logloss, accuracy, utility\n    \n\n    \ndef train(df, n_epochs):\n    '''\n    Function for training\n    '''\n    \n    train, valid = preprocessing(df)\n    \n    train_set = Dataset(train)\n    val_set = Dataset(valid)\n    \n    train_loader = DataLoader(train_set, batch_size=4096, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_set, batch_size=4096, shuffle=False, num_workers=4)\n    \n    device = torch.device(\"cuda\")\n    model = Model()\n    model.to(device)\n    \n    #optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)\n    optimizer = torch.optim.SGD(model.parameters(),0.05,0.9) # Use SGD because it empirically performs better according to my experiments\n    loss_fn = nn.BCELoss() # Binary cross entropy loss\n    \n    for epoch in range(n_epochs):\n        epoch_loss = train_one_epoch(model, optimizer, train_loader, loss_fn, device)\n        logloss, accuracy, utility = validation(model, val_loader, device, valid)\n        print(f\"EPOCH: {epoch}\")\n        print(f\"Training loss: {epoch_loss: .5f}  \" \n               f\"Validation logloss: {logloss: .3f}  \"\n                f\"Validation accuracy: {accuracy: .2f}  \"\n                 f\"Utility score: {utility: .2f}\")\n        print('\\n')\n    \n    torch.save(model.state_dict(), '\/kaggle\/working\/model.pt')\n    print('Training finished, model saved!')\n    \n    backtest_set = val_set\n    \n    return backtest_set # Return the validation dataset as the backtest dataset\n    ","1a00f8c0":"backtest_set = train(df, n_epochs=8)","ecd1157f":"def inference(backtest_data, model_path):\n    device = torch.device(\"cuda\")\n    model = Model()\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n    \n    inf_sampler = SequentialSampler(backtest_data)\n    inf_loader = DataLoader(backtest_data, sampler = inf_sampler)\n    \n    prediction = []\n    ground_truth = []\n    \n    for data in inf_loader:\n        x = data['features'].to(device)\n        y = data['label'].to(device)\n        with torch.no_grad():\n            output = model(x)\n        prediction.append(output.squeeze().detach().cpu().numpy())\n        ground_truth.append(y.squeeze().detach().cpu().numpy())\n        \n    prediction = np.array(prediction).reshape(-1,1).squeeze()\n    pred_label = np.where(prediction >= 0.5, 1, 0).astype(int)\n    \n    return pred_label","7270b00d":"pred_label = inference(backtest_set, '\/kaggle\/working\/model.pt')","17369bec":"np.savetxt('\/kaggle\/working\/prediction_NN.csv', pred_label, delimiter=',')","76490cce":"Preprocessing data","d7db2ea7":"Define function to do inference. Predict the label on the validation dataset by sequence.","4d0edad0":"Save results for back testing","a12c75c8":"Construct dataset for PyTorch training","fca047c5":"Training","8bb9f77f":"Define the neural network I use."}}