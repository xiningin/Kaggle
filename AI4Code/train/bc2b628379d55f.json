{"cell_type":{"6ff31e64":"code","88689691":"code","c76a1184":"code","7489ccae":"code","d2b6c4c2":"code","1e220602":"code","f5156fb5":"code","bac9b524":"code","a32968d4":"code","fdffd3ec":"code","8723f059":"code","15e9e3dd":"markdown","f1137aa9":"markdown","d13844d2":"markdown"},"source":{"6ff31e64":"#We want our model to learn to understand semantic correlations between words. Like for example, \"Mathematician\" and \"Physicist\" are both humans and scientists. ","88689691":"#For a word can create a vecot of semantic attributes. q(mathematician) = ( \"can run\": 2.3, \"likes coffee\":9.1, ...)\n#Similarity (mathematician, physicist) = normalized( q(mathematician)*q(physicist) )\n#Let your neural nettwork learn the attributes itself. However, they will not be interpretable.","c76a1184":"#Having vocabulary v, store embeddings in |v| x D matrix, where D is the dimensionality of the attributes. So the ith word has its attributes in ith row.\n#We map the words to indices with a dictionary word_to_ix","7489ccae":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1)","d2b6c4c2":"word_to_ix = {\"hello\":0, \"word\":1}\nembeds = nn.Embedding(2, 5) #embedding is not a tensor; it is like a function which stores value; refer to rows with tensors\nlookup_tensor = torch.tensor( [word_to_ix[\"hello\"] ], dtype = torch.long )\nprint( embeds(lookup_tensor))","1e220602":"def relu(tensor):\n    return tensor * torch.tensor( ten>0, dtype=torch.float)","f5156fb5":"#We want to compute P(w.i) given a sequence (w.i-n+1, w.i-n+2, ..., w.i-1)","bac9b524":"CONTEXT_SIZE = 2\nEMBEDDING_DIM = 10\n\ntest_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".split()","a32968d4":"#Build a list of tuples ([word.i-2, word.i-1], target_word)\ntrigrams = [ ( [test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range( len(test_sentence) - 2) ]\nprint(trigrams[:3])\n\nvocab = set(test_sentence)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\nprint(word_to_ix)\n\nclass NGramLanguageModeler(nn.Module):\n    def __init__( self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size*embedding_dim, 128) #Receives context_size words, each having embedding_dim size representation\n        self.linear2 = nn.Linear(128, vocab_size)\n    \n    def forward(self, inputs):\n        embeds = self.embeddings( inputs).view(1, -1) #treat context as one word - that's why input is a row vector\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        log_probs = F.log_softmax( out, dim= 1)\n        return log_probs\n    \nlosses = []\nloss_function = nn.NLLLoss()\nmodel = NGramLanguageModeler( len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\noptimizer = optim.SGD( model.parameters(), lr=0.01)\n    \nfor epoch in range(200):\n    total_loss = 0.\n    for context, target in trigrams:\n        context_idxs = torch.tensor( [word_to_ix[w] for w in context], dtype=torch.long )\n        target_id = torch.tensor( [word_to_ix[target] ], dtype=torch.long)\n            \n        model.zero_grad()\n            \n        log_probs = model( context_idxs )\n        loss = loss_function( log_probs, target_id)\n            \n        loss.backward()\n        optimizer.step()\n            \n        total_loss += loss.item()\n        losses.append(loss.item())","fdffd3ec":"plt.plot(losses)","8723f059":"#This work is based on the code from https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html and contains my interpretation of the knowledge given there.","15e9e3dd":"**ENCODING LEXICAL SEMANTICS**","f1137aa9":"**WORD EMBEDDINGS IN PYTORCH**","d13844d2":"**AN EXAMPLE: N-GRAM LANGUAGE MODELLING**"}}