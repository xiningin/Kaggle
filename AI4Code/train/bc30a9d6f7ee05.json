{"cell_type":{"3cf501fb":"code","6ffd2a74":"code","d56db0b6":"code","b604973a":"code","dc1f5c71":"code","db0c6f5d":"code","f4c8c776":"code","b116ece8":"code","ba83a73d":"code","0c1aa1b0":"code","c41b6274":"code","238bb9b4":"code","9241ad68":"code","0e96ef97":"code","53a240b9":"code","655dc057":"code","1e87e9fc":"code","6efd37a6":"code","08f4a7aa":"code","cab41743":"code","3cb8f4e2":"code","ef93f92b":"code","4e438534":"code","3f851337":"code","8700e753":"code","5b98654e":"code","eeee404f":"code","a2730edc":"code","91f1d877":"code","4ac77747":"code","dfc0a119":"markdown","b837552d":"markdown","04d553ec":"markdown","bbd2d20a":"markdown","ebcba5b2":"markdown","13e2b184":"markdown","7a582b5c":"markdown","184d2ee1":"markdown","6a64d895":"markdown","18f0d3e3":"markdown","ffc269a9":"markdown"},"source":{"3cf501fb":"import numpy as np\nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport itertools\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\nimport warnings\n#https:\/\/www.kaggle.com\/datatattle\/predicting-loan-default-classification","6ffd2a74":"train = pd.read_csv(\"..\/input\/dish-network-hackathon\/Train_Dataset.csv\", \n                    low_memory=False)\n        \ntest = pd.read_csv(\"..\/input\/dish-network-hackathon\/Test_Dataset.csv\",\n                  low_memory=False)","d56db0b6":"#Shape of dataframe\nprint(\" Shape of training dataframe: \", train.shape)\nprint(\" Shape of testing dataframe: \", test.shape)\n\n# Drop duplicates\ntrain.drop_duplicates()\ntest.drop_duplicates()\nprint(train.shape)\nprint(test.shape)","b604973a":"train.head()","dc1f5c71":"\nnull= train.isnull().sum().sort_values(ascending=False)\ntotal =train.shape[0]\npercent_missing= (train.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, \n                        keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", \n       missing_data.sort_values(by ='Total missing', ascending = False))","db0c6f5d":"class_df = train.groupby('Default').count()['ID'].reset_index().sort_values(by='ID',ascending=False)\nclass_df.style.background_gradient(cmap='winter')","f4c8c776":"train_null_unique= train.Own_House_Age.unique()\ntest_null_unique= test.Own_House_Age.unique()\nprint(train_null_unique)\nprint (test_null_unique)","b116ece8":"#plt.figure(figsize = (20, 12))\n#sns.countplot(train.Own_House_Age);","ba83a73d":"#Graph\nmy_pal = {0: 'deepskyblue', 1: 'deeppink'}\n\nplt.figure(figsize = (12, 6))\nax = sns.countplot(x = 'Default', data = train, palette = my_pal)\nplt.title('Class Distribution')\nplt.show()\n\n# Count and %\nCount_Normal_transacation = len(train[train['Default']==0])\nCount_Fraud_transacation = len(train[train['Default']==1]) \nPercentage_of_Normal_transacation = Count_Normal_transacation\/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of no defaults       :', Percentage_of_Normal_transacation*100)\nprint('Number of no defaults     :', Count_Normal_transacation)\nPercentage_of_Fraud_transacation= Count_Fraud_transacation\/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of defaults         :',Percentage_of_Fraud_transacation*100)\nprint('Number of defaults    :', Count_Fraud_transacation)","0c1aa1b0":"# Plot distribution of one feature\ndef plot_distribution(feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(train[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()\n    \n# Plot distribution of multiple features, with TARGET = 1\/0 on the same graph\ndef plot_distribution_comp(var,nrow=2):\n    \n    i = 0\n    t1 = train.loc[train['Default'] != 0]\n    t0 = train.loc[train['Default'] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"Default = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"Default = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","c41b6274":"def plot_bar_comp(var,nrow=2):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.countplot(train[feature])\n        sns.countplot(train[feature])\n        plt.ylabel('Count plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","238bb9b4":"# Box Plot for one feature\ndef plot_box(feature, color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Box Plot of %s\" % feature)\n    sns.boxplot(y=train[feature].dropna(),x= train.Default,color=color)\n    plt.show()","9241ad68":"# Bar Plot for one feature\ndef plot_bar(feature):\n    plt.figure(figsize=(10,50))\n    sns.catplot(y=feature, hue=\"Default\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=train);","0e96ef97":"print(train.Own_House_Age.describe())\nplot_distribution('Own_House_Age','green')","53a240b9":"plot_box(\"Own_House_Age\", \"green\")","655dc057":"#Number of observations in column\nobs = len(train.Own_House_Age)\nprint(\"No. of observations in column: \",obs)\n\n# calculate summary statistics\ndata_mean, data_std = mean(train.Own_House_Age), std(train.Own_House_Age)\nprint('Statistics: Mean=%.3f, Std dev=%.3f' % (data_mean, data_std))\n# identify outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n# identify outliers\noutliers = [x for x in train.Own_House_Age if x < lower or x > upper]\nprint('Identified outliers: %d' % len(outliers))","1e87e9fc":"def impute_outlier(x):\n    if x <= lower:\n        return(data_mean)\n    elif x>= (upper):\n        return(data_mean)\n    else:\n        return(x)\ntrain[\"Own_House_Age_new\"]= train[\"Own_House_Age\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.Own_House_Age_new))","6efd37a6":"def impute_missing(x):\n    if x == np.nan:\n        return(median(x))\n    else:\n        return(x)\n\ntrain[\"Own_House_Age_new\"]= train[\"Own_House_Age\"].apply(impute_missing)\nprint(\"No. of observations in column: \",len(train.Own_House_Age_new))\n    ","08f4a7aa":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['Own_House_Age_bins'] = pd.qcut(train['Own_House_Age'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['Own_House_Age_bins'].value_counts()","cab41743":"plot_bar(\"Own_House_Age_bins\")","3cb8f4e2":"print(train.Score_Source_1.describe().astype(str))\nplot_distribution('Score_Source_1','tomato')","ef93f92b":"plot_box(\"Score_Source_1\", \"tomato\")","4e438534":"#def impute_missing(x):\n#    if x == np.nan:\n#        return(median(x))\n#    else:\n#        return(x)\ntrain[\"Score_Source_1_new\"]= train[\"Score_Source_1\"].apply(impute_missing)\nprint(\"No. of observations in column: \",len(train.Score_Source_1_new))","3f851337":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['Score_Source_1_bins'] = pd.qcut(train['Score_Source_1'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['Score_Source_1_bins'].value_counts()","8700e753":"plot_bar(\"Score_Source_1_bins\")","5b98654e":"print(train.Social_Circle_Default.describe().astype(str))\nplot_distribution('Social_Circle_Default','tomato')","eeee404f":"plot_box(\"Social_Circle_Default\", \"tomato\")","a2730edc":"train[\"Social_Circle_Default_new\"]= train[\"Social_Circle_Default\"].apply(impute_missing)\nprint(\"No. of observations in column: \",len(train.Social_Circle_Default_new))","91f1d877":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['Social_Circle_Default'] = pd.qcut(train['Social_Circle_Default'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['Social_Circle_Default'].value_counts()","4ac77747":"plot_bar(\"Social_Circle_Default\");","dfc0a119":"## 3. Check some sample rows","b837552d":"### Lower social circle score has more probability of default","04d553ec":"### Almost all the variables except ID, some client variables, phone variables and default have missing values","bbd2d20a":"### Highly skewed distribution with 8% defaults and 92% no defaults, so a naive model which predicts no default for all records will have 92% accuracy","ebcba5b2":"### There is a tendency to default more at lower scores than higher scores","13e2b184":"### There are no outliers, so we just need to replace the missing values with median as the distribution is nearly normal","7a582b5c":"### We can see that there are lot of outliers, let us find how many outliers are there in total","184d2ee1":"## 2. Read in the train and test data","6a64d895":"#### There are lot of NAN values, so we would need to impute those before modeling\n#### Let us see the total NAN values for each variable in the dataset","18f0d3e3":"### There is a tendency to default more of the age of the house is more based on above chart","ffc269a9":"## 1. Import the necessary libraries\n\n"}}