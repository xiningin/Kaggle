{"cell_type":{"8c7f5be6":"code","492b4736":"code","533ece7e":"code","90990435":"code","ee26242e":"code","d217504e":"code","3a950061":"code","87f4e45d":"code","aeef1c4e":"code","c513ce83":"code","356f8406":"code","e576f2e9":"code","8b0832eb":"code","c0d37bc3":"code","c5000470":"markdown","f642a6cb":"markdown"},"source":{"8c7f5be6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","492b4736":"train_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsubmission_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\nprint('The train dataset contains {} rows and {} columns.'.format( len(train_df.index), len(train_df.columns)))\nprint('The test dataset contains {} rows and {} columns.'.format( len(test_df.index), len(test_df.columns)))","533ece7e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\nimport matplotlib\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","90990435":"input_cols =list(train_df.columns)[1:-1]\ntarget_cols='target'\ninputs_df = train_df[input_cols].copy()\ntargets = train_df[target_cols]\ninputs_df","ee26242e":"train_df.corr()","d217504e":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\n# Extracting Numeric and Categorical cols from data\nnumeric_cols=inputs_df.select_dtypes(exclude=['object']).columns.tolist()\ncategorical_cols = inputs_df.select_dtypes('object').columns.tolist()\n# Splitting data into training and validation\ntrain_inputs, val_inputs, train_targets, val_targets =train_test_split(inputs_df,targets,train_size=0.8,test_size=0.2,random_state=0)","3a950061":"%%time\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\n# Encode cstegorical columns with OneHotEncoder\nencoder=OneHotEncoder(sparse=True,handle_unknown='ignore').fit(inputs_df[categorical_cols])\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\n#print('encoded columns are:'+str(encoded_cols))\n#print()\n#print('categorical columns are:'+str(categorical_cols))\n#print()\n#print('These are the categories which we are going to encode and then add it to our data'+str(encoder.categories_))\ntrain_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols]).toarray()\nval_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols]).toarray()\ntest_df[encoded_cols] = encoder.transform(test_df[categorical_cols]).toarray()\n# Scale numeric columns with standard scalar    \n#scaler = StandardScaler()\n#train_inputs[numeric_cols] = scaler.fit_transform(train_inputs[numeric_cols])\n#val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n#test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\ntest_df[encoded_cols+numeric_cols]","87f4e45d":"%%time\n# Introducing Model and Predicting values\nrf1 = RandomForestRegressor(n_estimators=10,n_jobs=-1, random_state=7).fit(train_inputs[encoded_cols+numeric_cols],train_targets)\npreds_valid = rf1.predict(val_inputs[encoded_cols+numeric_cols])\npreds_valid","aeef1c4e":"mean_squared_error(preds_valid,val_targets,squared=False)","c513ce83":"def test_params(**params):\n    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params).fit(train_inputs[encoded_cols+numeric_cols], train_targets)\n    train_rmse = mean_squared_error(model.predict(train_inputs[encoded_cols+numeric_cols]), train_targets, squared=False)\n    val_rmse = mean_squared_error(model.predict(val_inputs[encoded_cols+numeric_cols]), val_targets, squared=False)\n    return train_rmse, val_rmse","356f8406":"def test_param_and_plot(param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, val_rmse = test_params(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(val_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","e576f2e9":"%%time\ntest_params(max_depth=10)","8b0832eb":"%%time\ntest_params(n_estimators=100,max_depth=10)","c0d37bc3":"%%time\n#Training the Best Model\nrf2= RandomForestRegressor(random_state=42,n_jobs=-1,n_estimators=100,max_depth=10).fit(train_inputs[encoded_cols+numeric_cols], train_targets)\n#Making Predictions on the Test Set\ntest_preds=rf2.predict(test_df[encoded_cols+numeric_cols])\nsubmission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","c5000470":"# Hyperparameter Tuning\nLet us now tune the hyperparameters of our model. You can find the hyperparameters for RandomForestRegressor here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\n![image.png](attachment:556da391-a31c-475d-a825-9672418b9b32.png)","f642a6cb":"Use the `test_params` and `test_param_and_plot` functions to experiment with different values of the hyperparmeters like `n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split etc`. You can learn more about the hyperparameters here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html"}}