{"cell_type":{"809464ce":"code","52cc19fb":"code","c50984c9":"code","2e450165":"code","4f3ec06c":"code","1863dfa8":"code","fe4d0bce":"code","c6706594":"code","4f383530":"code","d0132b0d":"code","55fd6a18":"code","ffe42d80":"code","493c1ef5":"code","2cba420e":"code","7c6767e1":"code","dd4ef68e":"code","4d1afc01":"code","7245989b":"code","9fe7ac0b":"code","a380e0ef":"code","891a20a3":"code","bc6d546b":"code","86f18d67":"code","5cc7d4b9":"markdown","edbf2376":"markdown","f14cde6b":"markdown","7e87a4c8":"markdown","36bac7c3":"markdown","8dc38786":"markdown","bd753c19":"markdown","9c224fe7":"markdown","7a78d134":"markdown","dac57fa9":"markdown","116e8130":"markdown","93b91b75":"markdown"},"source":{"809464ce":"#### CODE IN THE THIS CELL WAS CREATED BY SAKAMI\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm \n\n\nclass WRMSSEEvaluator(object):\n    \"\"\"\n    Example usage:\n    train_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_df.iloc[:, -28:]\nvalid_preds = valid_fold_df.copy() + np.random.randint(100, size=valid_fold_df.shape)\n\nevaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, prices)\nevaluator.score(valid_preds)\n\"\"\"\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        \n        # I am adding just this one line to this implementation to avoid changing the train_df globally, which happened to be in my backtest object\n        train_df = train_df.copy()\n        # And this to keep track of scores by level\n        self.all_scores = []\n        \n        \n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n        \n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            setattr(self, f'lv{i + 1}_train_df', train_df.groupby(group_id)[train_target_columns].sum())\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        train_y = getattr(self, f'lv{lv}_train_df')\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = ((train_y.iloc[:, 1:].values - train_y.iloc[:, :-1].values) ** 2).mean(axis=1)\n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        \n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            self.all_scores.append(lv_scores.sum())\n\n        return np.mean(self.all_scores)","52cc19fb":"import pandas as pd\nimport numpy as np \n\nclass Backtest:\n    \"\"\"\n    Validates your prediction process by running it on different time periods, \n    with the last 28 days of the time period used as the validation set.\n    This object will store the final scores in the .scores attribute\n    \"\"\"\n    def __init__(self, cal, stv, s_p, process: list, process_names=['process'], verbose=False): \n        self.cal = cal\n        self.stv = stv\n        self.s_p = s_p\n        self.process = process\n        self.process_names = process_names\n        self.verbose = verbose\n        self.valid_end_days = []\n        self.scores = {name: [] for name in process_names} \n        self.preds = [[] for name in process_names]\n        self.evaluator = None\n        \n\n    def score(self, days_back=0): \n        \"\"\"\n        gives the score for your predictions if the data ended days_back[n] days ago\n        \"\"\"\n        if days_back != 0: \n            stv = self.stv.iloc[:, :-days_back]\n        else:   \n            stv = self.stv\n        train_df = stv.iloc[:, :-28]\n        valid_df = stv.iloc[:, -28:]\n        self.valid_end_days.append(1913-days_back)\n        \n        self.evaluator = WRMSSEEvaluator(train_df, valid_df, self.cal, self.s_p)\n        \n        for i in range(len(self.process)):\n            valid_preds = self.process[i](self.cal, train_df, self.s_p)\n            score = self.evaluator.score(valid_preds)\n            self.scores[self.process_names[i]].append(score)\n            self.preds[i].append(valid_preds)\n            \n            \n            if self.verbose == True: \n                print(f'{self.process_names[i]} had a score of {score} on validation period {1913-days_back-28} to {1913-days_back}')\n        \n    def score_all(self, days_back=[0,308]):\n        for i in range(len(days_back)): \n            self.score(days_back[i])\n            \n    \n            \n    def scores_df(self):\n        return pd.DataFrame(self.scores, index=self.valid_end_days)\n    \n    \n            \n    ","c50984c9":"# for plotting later\nimport matplotlib.pyplot as plt","2e450165":"PATH = '\/kaggle\/input\/m5-forecasting-accuracy\/'\ncal = pd.read_csv(f'{PATH}calendar.csv')\nsell_prices = pd.read_csv(f'{PATH}sell_prices.csv')\nss = pd.read_csv(f'{PATH}sample_submission.csv')\nstv = pd.read_csv(f'{PATH}sales_train_validation.csv')","4f3ec06c":"def add_snap_col(df_in):\n    \"\"\"adds a 'snap_day' column to a dataframe that contains a state_id column and the columns 'snap_CA', 'snap_TX', \n    and snap_WI\"\"\"\n    df = pd.get_dummies(df_in, columns=['state_id'])\n    df['snap_day'] = (df.snap_CA * df.state_id_CA) + (df.snap_WI * df.state_id_WI) + (df.snap_TX * df.state_id_TX)\n    del df['state_id_WI'], df['state_id_CA'], df['state_id_TX']\n    return df\n\n\n\ndef melt_merge_snap(df):\n    df = df.melt(['id', 'state_id'], var_name='d', value_name='demand')\n    df = df.merge(cal)\n    df = add_snap_col(df)\n    return df\n\ndef get_valid(stv, n):\n    \"\"\"gets a df for the next 28 days to give predictions\"\"\"\n    valid = stv.iloc[:, pd.np.r_[0,5,-28:0]]\n    valid.columns = ['id', 'state_id'] + ['d_' + str(n+x) for x in range(1,29)]\n    return valid\n    \ndef join_valid_groupby(valid, group):\n    \"\"\" \n    Joins the sub dataframe created by get_valid to a groupby series\n    \"\"\"\n    return valid.join(group, on=group.index.names)\n\ndef reshape_valid(df): \n    \"\"\"Takes a dataframe in the form given by join_valid_groupby, or any dataframe with the proper index and and 'd' colums.\n    returns a prediction dataframe that can be input into an evaluator\n    \"\"\"\n    # pivot df to get it into the proper format for submission\n    df = df.pivot(index='id', columns='d', values='demand')\n    # need to reset index to take care of columns. comment next line out to see what i mean \n    #df.set_index('id',inplace=True)\n    return df#.iloc[:, -28:]","1863dfa8":"def process_00(cal, stv, sell_prices, days_to_agg=28):\n    \"\"\"a process that produces predictions for the next 28 days following the end of stv\"\"\"\n    \n    d = 1913 - (1919 - stv.shape[1])\n    last_90 = stv.iloc[:, pd.np.r_[0,5,-days_to_agg:0]].copy() # we include 0, and 5 to get the id and state id columns\n    last_90 = melt_merge_snap(last_90)\n    by_weekday_snap_90 = last_90.groupby(['id', 'wday', 'snap_day'])['demand'].mean()\n    valid = get_valid(stv, d)\n    valid_id = valid[['id']]\n    valid = melt_merge_snap(valid)\n    valid.drop('demand', axis='columns', inplace=True)\n    valid = join_valid_groupby(valid, by_weekday_snap_90)\n    preds = reshape_valid(valid).reset_index()\n    return valid_id.merge(preds).iloc[:, 1:]","fe4d0bce":"def process_01(cal, stv, sell_prices):\n    return process_00(cal, stv, sell_prices, days_to_agg=60)\n\ndef process_02(cal, stv, sell_prices):\n    return process_00(cal, stv, sell_prices, days_to_agg=90)\n\ndef process_03(cal, stv, sell_prices):\n    return process_00(cal, stv, sell_prices, days_to_agg=120)\n\ndef process_04(cal, stv, sell_prices):\n    return process_00(cal, stv, sell_prices, days_to_agg=150)","c6706594":"backtest = Backtest(cal, stv, sell_prices, \n                    process=[process_02], \n                    process_names=['agg_90'])","4f383530":"backtest = Backtest(cal, stv, sell_prices, \n                    process=[process_00, process_01, process_02, process_03, process_04], \n                    process_names=['agg_28', 'agg_60', 'agg_90', 'agg_120', 'agg_150'])","d0132b0d":"%%time\nbacktest.score_all([0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280, 308, 336, 364,\n                   392, 420, 448, 476, 504, 532, 560, 588, 616, 644, 672, 700, 728])","55fd6a18":"backtest.score(0)","ffe42d80":"scores_df = backtest.scores_df()\nscores_df.head()","493c1ef5":"e = backtest.evaluator","2cba420e":"preds = backtest.preds","7c6767e1":"preds[0][0] * 1.04","dd4ef68e":"levels = [\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n]","4d1afc01":"level_scores = zip(levels, e.all_scores)","7245989b":"list(level_scores)","9fe7ac0b":"e1 = backtest.evaluator","a380e0ef":"e1.all_scores = []","891a20a3":"e.score(preds[0][0])","bc6d546b":"import matplotlib.pyplot as plt\nscores_df.plot(figsize=(20,7))\nplt.xlabel('Validation end day', fontsize=20)\nplt.ylabel('WRMSSEE score', fontsize=20)\nplt.title('Process performance over time', fontsize=26)\nplt.show()","86f18d67":"scores_df.to_csv('backtest_results_' + '_'.join(backtest.process_names) + '.csv')\n","5cc7d4b9":"## Plot the performance accross time","edbf2376":"## Define helper functions for your process","f14cde6b":"# Example of using Backtest for a simple model: Group by id, weekday, snap_day and take the mean looking at the previous 90 days before the validation period.","7e87a4c8":"## Instantiate a Backtest object with the differenct processes youd like to compare","36bac7c3":"## Backtest object","8dc38786":"## Define process functions:","bd753c19":"## Backtest all processess and time frames at once using .score_all()\n\nIn this case will use every 28 day period going backward 2 years","9c224fe7":"## Get the scores as a nice pandas dataframe","7a78d134":"[Sakami's](https:\/\/www.kaggle.com\/sakami) implementation of the [evaluation metric](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834): I have added \"train_df = train_df.copy()\" as the first line in the __init__ function because it was changing the local variable train_df I have in my backtest object.","dac57fa9":"## Read scores_df to a csv file to save your backtest information easily ","116e8130":"# Backtest multiple processes easily, using [Sakami's](https:\/\/www.kaggle.com\/sakami) implementation of the [evaluation metric](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834)\n* Create process functions (e.g. *process(sales_train_validation, calendar, sell_prices)*) that take in the 3 pandas dataframes created from the data available, and returns predictions for the following 28 days\n* Then instantiate Backtest() with a list of process functions you'd like to compare over different time periods\n* Call the method backtest.score_all() with a list: days_back\n* Backtest() will run your process and give a WRMSSEE score as if the training data ended at day (1913 - days_back[i] - 28) and the validation was the following 28 days","93b91b75":"## Load data"}}