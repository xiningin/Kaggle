{"cell_type":{"00d3f483":"code","a0edd6b5":"code","e8608417":"code","7cbd7db2":"code","79d9886b":"code","76f6766d":"code","d49bcaaa":"code","f821c597":"code","5f1ddfd1":"code","319fea6a":"code","82e785ba":"code","a92735fd":"code","1315dc55":"code","25856329":"code","4d379f0c":"code","529604ac":"code","8d23a815":"code","0ae6160d":"code","efac4bb3":"code","b088ebd5":"code","285c1a43":"code","64690578":"code","3889545e":"code","4343813f":"code","d5a77005":"code","7ab45db5":"code","fcbcc46d":"code","d2942fc1":"code","c0a5f1dc":"code","1ca1cbd4":"code","16a7168f":"code","6e898bd9":"code","d0e3a7bb":"code","050d1fc2":"code","1fcb9da2":"code","6c9e8b68":"code","ce640a76":"code","b8350189":"code","d638f586":"code","79a6d528":"code","cfe35441":"code","46d48ec2":"code","9685f83e":"code","84feb2aa":"code","bc97fa2a":"code","a6e5ddfc":"code","70a0c50f":"code","5ca71633":"code","f36a27aa":"code","0b7610dd":"code","d63cc7e8":"markdown","12b31f1a":"markdown","d6dff8a3":"markdown","ba0054d9":"markdown","0b74402b":"markdown","c31b1c44":"markdown","b8b1daea":"markdown","4c80ba36":"markdown","56c47003":"markdown","c9266795":"markdown","822dfd16":"markdown","abbdc0fa":"markdown","3ff3e0c6":"markdown","c0e35498":"markdown","edc8d457":"markdown"},"source":{"00d3f483":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.model_selection import train_test_split\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0edd6b5":"train=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\nprint(\"Shape of Train Data \",train.shape)\ntest=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(\"Shape of Test Data \",test.shape)","e8608417":"train.head()","7cbd7db2":"!pip install nlp\nimport nlp","79d9886b":"snli= nlp.load_dataset(\"snli\")","76f6766d":"snli","d49bcaaa":"print(\"Type of SNLI Dataset \",type(snli))","f821c597":"print(snli['train'])\nprint(\"Type of SNLI Train \",type(snli['train']))","5f1ddfd1":"snli_train_df = pd.DataFrame(snli['train'])\nsnli_train_df.head()","319fea6a":"test_dup=pd.merge(test[['premise','hypothesis']],snli_train_df[['premise','hypothesis']],how=\"outer\",indicator=True)\ntest_dup.head()","82e785ba":"test_dup['_merge'].unique()\n","a92735fd":"snli_train_df['lang_abv']=\"en\"\ntrain_df=train[['premise','hypothesis','lang_abv','label']]\nsnli_train_df['dataset']=\"snli\"\ntrain_df['dataset']=\"train\"\ntrain_df=pd.concat([train_df,snli_train_df])\nprint(\"Shape of Original Train Data \",train.shape)\nprint(\"Shape of SNLI Train Data \",snli_train_df.shape )\nprint(\"Shape after merging Original Train Data With SNLI \",train_df.shape)","1315dc55":"train_df.head()","25856329":"xnli=nlp.load_dataset(\"xnli\")\nxnli","4d379f0c":"for idx, elt in enumerate(xnli['validation']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', xnli['validation'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 3:\n        break","529604ac":"buffer = {\n    'premise': [],\n    'hypothesis': [],\n    'label': [],\n    'lang_abv': []\n}\n\n\n\nfor x in xnli['validation']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buffer['premise'].append(premise)\n        buffer['hypothesis'].append(hypothesis)\n        buffer['label'].append(label)\n        buffer['lang_abv'].append(lang)\n        \n# convert to a dataframe and view\nxnli_valid_df = pd.DataFrame(buffer)\nxnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","8d23a815":"xnli_valid_df.shape","0ae6160d":"test_dup=pd.merge(test[['premise','hypothesis']],xnli_valid_df[['premise','hypothesis']],how=\"outer\",indicator=True)\ntest_dup.head()","efac4bb3":"test_dup['_merge'].value_counts()","b088ebd5":"dup_pairs=test_dup[test_dup['_merge']==\"both\"]\ndup_pairs.head()","285c1a43":"dup_pairs['combo']=dup_pairs['premise']+\" \"+dup_pairs['hypothesis']\ndup_pairs.head()","64690578":"xnli_valid_df['combo']=xnli_valid_df['premise']+\" \"+xnli_valid_df['hypothesis']","3889545e":"xnli_valid_without_dups=xnli_valid_df[(~xnli_valid_df['combo'].isin(dup_pairs['combo'].tolist()))]","4343813f":"xnli_valid_without_dups.drop(['combo'],axis=1,inplace=True)","d5a77005":"xnli_valid_without_dups.shape","7ab45db5":"xnli_valid_without_dups['dataset']=\"xnli\"","fcbcc46d":"train_df=pd.concat([train_df,xnli_valid_without_dups])\ntrain_df.shape","d2942fc1":"mnli=nlp.load_dataset(path='glue', name='mnli')\nmnli","c0a5f1dc":"mnli_train=pd.DataFrame(mnli['train'])\nmnli_train.head()","1ca1cbd4":"mnli_train.shape","16a7168f":"test_dup=pd.merge(test[['premise','hypothesis']],mnli_train[['premise','hypothesis']],how=\"outer\",indicator=True)\ntest_dup['_merge'].unique()","6e898bd9":"mnli_train.drop(['idx'],axis=1,inplace=True)","d0e3a7bb":"mnli_train['lang_abv']=\"en\"\nmnli_train['dataset']=\"mnli\"","050d1fc2":"train_df=pd.concat([train_df,mnli_train])\ntrain_df.shape","1fcb9da2":"pd.isnull(train_df).sum()","6c9e8b68":"from transformers import BertTokenizer,TFBertModel\nimport tensorflow as tf\nfrom transformers import AutoTokenizer\n","ce640a76":"MODEL_NAME=\"jplu\/tf-xlm-roberta-large\"\nMAX_LEN=64\nBATCH_SIZE=64","b8350189":"tokeniser=AutoTokenizer.from_pretrained(MODEL_NAME) ## Autokeniser will initialise the tokeniser based on the model name\n","d638f586":"from tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom transformers import TFAutoModel","79a6d528":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"In TPU STRATERGY\")\n    print('Number of replicas:', strategy.num_replicas_in_sync)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","cfe35441":"def createModel():\n    with strategy.scope():\n        model=TFAutoModel.from_pretrained(MODEL_NAME)\n        input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_token', dtype='int32')\n        #input_mask=tf.keras.layers.Input(shape=(MAX_LEN,), name='input_mask', dtype='int32')\n        #input_token_ids=tf.keras.layers.Input(shape=(MAX_LEN,), name='input_token_ids', dtype='int32')\n        ### From the Model, we need to extract the Last Hidden Layer - this is the first element of the model output\n        embedding=model(input_ids)[0]\n        ### Extract the CLS Token from the Embedding Layer. CLS Token is aggregate of the entire sequence representation. It is the first token\n        cls_token=embedding[:,0,:] ## embedding is of the size batch_size*MAX_LEN*768\n    \n        ### Add a Dense Layer, with three outputs \n        output_layer = Dense(3, activation='softmax')(cls_token)\n    \n        classification_model= Model(inputs=input_ids, outputs = output_layer)\n    \n        classification_model.compile(Adam(lr=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    \n        #classification_model.summary()\n    \n    return classification_model\n    \n    \n    \n    ","46d48ec2":"'''\nThis function will take the complete data, with option to select certain external datasets. It will also split the data into train and validation Split,\nencode the data and create a TF.Data.DataSet object for the train and validation data\n'''\ndef createTFDataSet(data,external_dataset=None,test_size=0.2,padding=True,max_length=MAX_LEN,truncation=True,batch_size=BATCH_SIZE):\n    tokeniser=AutoTokenizer.from_pretrained(MODEL_NAME) \n    if external_dataset==None:\n        train_data=data[data['dataset']==\"train\"]\n    else:\n        dat=data[data['dataset']==\"train\"]\n        external_data=data[data['dataset'].isin(external_dataset)]\n        train_data=pd.concat([dat,external_data])\n        assert dat.shape[0]+external_data.shape[0]==train_data.shape[0]\n    ### Split the Data into Train and Validation Split\n    X=train_data[['hypothesis','premise']]\n    y=train_data['label']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    print(\"Shape of Train Data \",X_train.shape)\n    print(\"Shape of Test Data \",X_test.shape)\n    \n    ### Encode the Training and Validation Data\n    train_encoded=tokeniser.batch_encode_plus(X_train[['hypothesis','premise']].values.tolist(),pad_to_max_length=padding,max_length=max_length,truncation=True)\n    val_encoded=tokeniser.batch_encode_plus(X_test[['hypothesis','premise']].values.tolist(),pad_to_max_length=padding,max_length=max_length,truncation=True)\n    \n    ### Convert the Encoded Train and Validation data into TF Dataset\n    auto = tf.data.experimental.AUTOTUNE\n    train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encoded['input_ids'], y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto))\n    \n    \n    valid_dataset = (tf.data.Dataset\n    .from_tensor_slices((val_encoded['input_ids'], y_test))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto))\n    \n    return tokeniser,train_dataset,valid_dataset,X_train.shape[0]\n    \n    \n    ","9685f83e":"tokeniser,train_dataset,valid_dataset,train_rows=createTFDataSet(train_df,external_dataset=['xnli','mnli'])","84feb2aa":"with strategy.scope():\n    model=createModel()\n    model.summary()","bc97fa2a":"n_steps =  train_rows\/\/BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5\n)","a6e5ddfc":"test_encoded=tokeniser.batch_encode_plus(test[['hypothesis','premise']].values.tolist(),pad_to_max_length=True,max_length=MAX_LEN,truncation=True)\n\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded['input_ids'])\n    .batch(BATCH_SIZE)\n)\n","70a0c50f":"test_preds = model.predict(test_dataset, verbose=1)\npredictions = test_preds.argmax(axis=1)","5ca71633":"submission = pd.DataFrame()\nsubmission['id']=test['id'].tolist()\nsubmission['prediction'] = predictions","f36a27aa":"submission.to_csv(\"submission.csv\",index=False)","0b7610dd":"model.save_weights(\"XLM_R_MNLI_XNLI.h5\",overwrite=True)","d63cc7e8":"## Getting the Standford Natural Language Inference(SNLI) Data","12b31f1a":"## Install the NLP Library of Hugging Face and import it","d6dff8a3":"SNLI Train contains three features : premise, hypotheasis and label. The label consists of either of the three class. We can iterate over this to get the data. We will convert this to a dataframe and have a look at the data. \n\n**Also, we need to check if any of the premise,hypothesis pair is in the test data and remove it - to avoid any data leakage**","ba0054d9":"### To get Cross Lingual Data, let us load the XNLI Dataset as well","0b74402b":"XNLI Data structure is slightly different from SNLI. Here we have a dictionary of premise and hypothesis across different languages.","c31b1c44":"SNLI data has three datasets - train, calidation and test data. The train data consists of around 550152 records and the three classes - entailment, neutral and contradiction","b8b1daea":"XNLI contains only Test and Validation data across 15 languages. The Validation Data is 2590 hypothesis premise pair converted to each of the 15 languages and even this contains, three classes of labels","4c80ba36":"There are 746 rows in the XNLI valid dataset, that is there is the test data. We have to remove those rows","56c47003":"There are no duplicates between test data and the SNLI Train Data. So we can safely use the SNLI data, without having to worry about data leakage\n\n","c9266795":"## Load the MNLI Dataset ","822dfd16":"After merging all three dataset, we have arounf 991578 rows in the data... ","abbdc0fa":"To detect duplicates between the Test Data and SNLI data, we will use the merge function with **indicator=True** , This will create an additional column \"**_merge**\" which will indicate where the row is present in both tables or only on the right or left table. We can then identify the duplicate rows.","3ff3e0c6":"There are no rows of MNLI present in the test data. So we do not worry about data leakage from the MNLI Dataset","c0e35498":"### Encode the Data Using batch_encode_plus.\n\nThis will tokenise and encode the data, and we can also allow for padding. XLM-R has been shown to significantly outperform multilingual BERT. This has same architecture as BERT, but tokenisation is based on BPE and we no longer need token_type_ids.","edc8d457":"## Introduction\n\n*\u201cIt is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.\u201d* - Sir Arthur Conan Doyle, Sherlock Holmes\n\nThe data given in this dataset consists of only 12K records. One way for deep learning models to work, is to use more data. Thanks to [Yih-Dar SHIEH](https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets-hugging-face-nlp-library)  for excellent Kernel on how to get more Datasets for NLI and using [hugging face nlp ](https:\/\/huggingface.co\/nlp\/) library to easily share and access datasets and evaluation metrics for Natural Language Processing.\n\nIn This Kernel, we will augment our training data with other datasets for NLI. Few NLI Datasets that we can get from Hugging Face nlp library are :\n\n1. [SNLI](https:\/\/huggingface.co\/datasets\/snli)\n2. [Adversarial Natural Language Inference](https:\/\/huggingface.co\/datasets\/anli)\n3. [Multi-Genre Natural Language Inference](https:\/\/huggingface.co\/datasets\/multi_nli)\n4. [XNLI](https:\/\/huggingface.co\/datasets\/xnli) : This is a subset of MNLI, hich has been translated into a 14 different languages . While others are purely english, this is cross lingual\n\n"}}