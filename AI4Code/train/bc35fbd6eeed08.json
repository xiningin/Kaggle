{"cell_type":{"ef0dffb2":"code","27588d34":"code","03be6968":"code","8c87e486":"code","c3e377bf":"code","8c4c02d4":"code","99edb96d":"code","f56d2f31":"code","cd225056":"code","c8c4861a":"code","569e619b":"code","f413d620":"code","2e3b12c2":"code","6b8ae4e8":"code","4f88515e":"code","d0846573":"code","a13971e6":"code","44091544":"code","30cb4db1":"code","2a62e1bb":"code","3ff61a0a":"code","468784f8":"code","4e9b2fb4":"code","41751ad2":"code","29ea7fce":"code","b33f887b":"code","ec8bb818":"code","e463d615":"code","bc48fb53":"code","5712aaa3":"code","65e48a7a":"code","2a0d2dbb":"code","5ea618c0":"code","f5b0c0fb":"code","b7d60149":"code","34cae838":"code","3ac8665e":"code","ad4336e4":"code","640266f1":"code","ea8f79f5":"code","20f976f2":"code","8be5f27f":"code","243da4fc":"code","cd2e83e9":"code","b7c5f647":"code","8c1030f4":"markdown","4d709337":"markdown","45ae90af":"markdown","dc3b0cf7":"markdown","b069b7eb":"markdown","b090f22b":"markdown","93259f83":"markdown"},"source":{"ef0dffb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27588d34":"!pip install -q fastai","03be6968":"from fastai.tabular.all import *","8c87e486":"path = '..\/input\/adult-census-income\/adult.csv'\ndf = pd.read_csv(path)","c3e377bf":"df.tail()","8c4c02d4":"df.columns","99edb96d":"cat_names = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race','sex','native.country']\ncont_names = ['age', 'fnlwgt', 'education.num','hours.per.week']\n\nprocs = [Categorify,FillMissing,Normalize]\ny_names = 'income'\ny_block = CategoryBlock()\nsplits = RandomSplitter()(range_of(df))","f56d2f31":"to = TabularPandas(df,procs=procs, cat_names=cat_names,cont_names=cont_names, y_names=y_names,y_block=y_block, splits=splits)","cd225056":"to.show()","c8c4861a":"import xgboost as xgb","569e619b":"to.train.ys, to.train.ys.values.ravel() # gives you an array","f413d620":"X_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","2e3b12c2":"model = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)","6b8ae4e8":"xgb_model = model.fit(X_train, y_train)","4f88515e":"xgb_preds = xgb_model.predict_proba(X_test)","d0846573":"xgb_preds","a13971e6":"accuracy(tensor(xgb_preds), tensor(y_test))","44091544":"from xgboost import plot_importance","30cb4db1":"plot_importance(xgb_model)","2a62e1bb":"dls = to.dataloaders()","3ff61a0a":"learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)","468784f8":"learn.fit(5, 1e-2)","4e9b2fb4":"nn_preds = learn.get_preds()[0]","41751ad2":"nn_preds","29ea7fce":"# from :https:\/\/github.com\/muellerzr\/Practical-Deep-Learning-for-Coders-2.0\/blob\/master\/Tabular%20Notebooks\/02_Ensembling.ipynb\nclass PermutationImportance():\n  \"Calculate and plot the permutation importance\"\n  def __init__(self, learn:Learner, df=None, bs=None):\n    \"Initialize with a test dataframe, a learner, and a metric\"\n    self.learn = learn\n    self.df = df if df is not None else None\n    bs = bs if bs is not None else learn.dls.bs\n    self.dl = learn.dls.test_dl(self.df, bs=bs) if self.df is not None else learn.dls[1]\n    self.x_names = learn.dls.x_names.filter(lambda x: '_na' not in x)\n    self.na = learn.dls.x_names.filter(lambda x: '_na' in x)\n    self.y = dls.y_names\n    self.results = self.calc_feat_importance()\n    self.plot_importance(self.ord_dic_to_df(self.results))\n\n  def measure_col(self, name:str):\n    \"Measures change after column shuffle\"\n    col = [name]\n    if f'{name}_na' in self.na: col.append(name)\n    orig = self.dl.items[col].values\n    perm = np.random.permutation(len(orig))\n    self.dl.items[col] = self.dl.items[col].values[perm]\n    metric = learn.validate(dl=self.dl)[1]\n    self.dl.items[col] = orig\n    return metric\n\n  def calc_feat_importance(self):\n    \"Calculates permutation importance by shuffling a column on a percentage scale\"\n    print('Getting base error')\n    base_error = self.learn.validate(dl=self.dl)[1]\n    self.importance = {}\n    pbar = progress_bar(self.x_names)\n    print('Calculating Permutation Importance')\n    for col in pbar:\n      self.importance[col] = self.measure_col(col)\n    for key, value in self.importance.items():\n      self.importance[key] = (base_error-value)\/base_error #this can be adjusted\n    return OrderedDict(sorted(self.importance.items(), key=lambda kv: kv[1], reverse=True))\n\n  def ord_dic_to_df(self, dict:OrderedDict):\n    return pd.DataFrame([[k, v] for k, v in dict.items()], columns=['feature', 'importance'])\n\n  def plot_importance(self, df:pd.DataFrame, limit=20, asc=False, **kwargs):\n    \"Plot importance with an optional limit to how many variables shown\"\n    df_copy = df.copy()\n    df_copy['feature'] = df_copy['feature'].str.slice(0,25)\n    df_copy = df_copy.sort_values(by='importance', ascending=asc)[:limit].sort_values(by='importance', ascending=not(asc))\n    ax = df_copy.plot.barh(x='feature', y='importance', sort_columns=True, **kwargs)\n    for p in ax.patches:\n      ax.annotate(f'{p.get_width():.4f}', ((p.get_width() * 1.005), p.get_y()  * 1.005))","b33f887b":"imp = PermutationImportance(learn)","ec8bb818":"avgs = (nn_preds + xgb_preds) \/2","e463d615":"avgs","bc48fb53":"argmax = avgs.argmax(dim=1)","5712aaa3":"argmax","65e48a7a":"accuracy(tensor(nn_preds),tensor(y_test))","2a0d2dbb":"accuracy(tensor(xgb_preds),tensor(y_test))","5ea618c0":"accuracy(tensor(avgs),tensor(y_test))","f5b0c0fb":"from sklearn.ensemble import RandomForestClassifier","b7d60149":"tree = RandomForestClassifier(n_estimators=100)","34cae838":"tree.fit(X_train,y_train)","3ac8665e":"!pip install rfpimp","ad4336e4":"from rfpimp import *","640266f1":"imp = importances(tree, X_test,to.valid.ys)","ea8f79f5":"plot_importances(imp)","20f976f2":"forest_preds = tree.predict_proba(X_test)","8be5f27f":"forest_preds","243da4fc":"avgs = (nn_preds+ xgb_preds + forest_preds)\/3","cd2e83e9":"accuracy(tensor(forest_preds), tensor(y_test))","b7c5f647":"accuracy(tensor(avgs), tensor(y_test))","8c1030f4":"## Fastai to the fold","4d709337":"## Ensembling of the two","45ae90af":"from walkwith fastai: https:\/\/github.com\/muellerzr\/Practical-Deep-Learning-for-Coders-2.0\/blob\/master\/Tabular%20Notebooks\/02_Ensembling.ipynb","dc3b0cf7":"Even though accuracy is higher than Random forest alone, it wasn't bigger than neural network. Happy Experimenting!","b069b7eb":"## Using XG Boost","b090f22b":"## Adding Random Forests","93259f83":"The ensembled models accuracy came out to be lower in this case."}}