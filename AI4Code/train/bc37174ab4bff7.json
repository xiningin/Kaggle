{"cell_type":{"a18cedbd":"code","c88cb054":"code","1074e3fb":"code","fd02c166":"code","f41231f6":"code","201e8983":"code","f6821253":"code","97cb01db":"code","64f33711":"code","6dca01a8":"code","a2503dfb":"code","d1712f43":"code","ccbba99c":"code","cced5d21":"code","2352ba3c":"code","60b41b67":"code","737d0b69":"code","808a9c79":"code","f8174a00":"code","cbf24ace":"code","556498e2":"code","89243e00":"code","1680cb48":"markdown","16b610ad":"markdown","c74b4516":"markdown","b5eb8e29":"markdown","ed507288":"markdown","10615781":"markdown","9e14e081":"markdown","53599674":"markdown","a3636ce3":"markdown","5e3b36f0":"markdown","e4b4efb5":"markdown","7ee82312":"markdown","d50c0aa9":"markdown","8bd12cd3":"markdown"},"source":{"a18cedbd":"import gc\nimport os\nimport copy\nimport math\nimport time\nimport numba\nimport warnings\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom contextlib import contextmanager\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import (\n    train_test_split,\n    GroupKFold\n)\nfrom kaggle.competitions import nflrush","c88cb054":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","1074e3fb":"class Config():\n    def __init__(self):\n        self.IS_KERNEL = True\n        self.DEBUG = True\n        # path\n        self.DATA_DIR = \"..\/input\/nfl-big-data-bowl-2020\/\"\n        self.OUT_PATH = \"log\"\n        self.PROJECT_PATH = \".\/\"\n        self.INITIAL_CHECKPOINT = None\n        # dataset\n        self.N_WORKERS = 0\n        self.BATCH_SIZE = 256\n        # epoch\n        self.N_EPOCH = 50\n        # model\/train\n        self.N_COMPONENTS = 3\n        self.N_EMB_OUT = 8\n        self.USE_CAT = True\n        # learning rate\n        self.LEARNING_RATE = 0.01\n        self.LR_DECAY = 0.9\n        # other\n        self.SEED = 2019\n        # validation display\n        self.VAL_PERIOD = 1\n        # early stopping\n        self.N_EARLY_STOPPPING_PATIENCE = 3\n        # validation\n        self.VALID_TYPE = \"GroupKFold\"\n        self.N_FOLDS = 5\n        self.TEST_SIZE = 0.15\n        self.TARGET_NORM = False","fd02c166":"config = Config()","f41231f6":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","201e8983":"logging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(filename)s[line:%(lineno)d] \"\n           + \"%(levelname)s %(message)s\",\n    datefmt='%a, %d %b %Y %H:%M:%S',\n    filename=\".\/train.log\",\n    filemode=\"w\")\n\n# Print to console\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nformatter = logging.Formatter('%(message)s')\nconsole.setFormatter(formatter)\nlogging.getLogger('').addHandler(console)","f6821253":"class Validation():\n    def __init__(self, config):\n        self.valid_type = config.VALID_TYPE\n        self.config = config\n        self.valid_list = []\n\n    def make_validation(self):\n        df = pd.read_csv(Path(self.config.DATA_DIR) \/ \"train.csv\")\n        # random split\n        if self.valid_type == \"random_split\":\n            game_ids = df[\"GameId\"].unique()\n            trn_game_ids, val_game_ids = \\\n                train_test_split(\n                    game_ids,\n                    test_size=self.config.TEST_SIZE,\n                    random_state=self.config.SEED\n                )\n            trn_play_ids = \\\n                df[df[\"GameId\"].isin(trn_game_ids)][\"PlayId\"].unique()\n            val_play_ids = \\\n                df[df[\"GameId\"].isin(val_game_ids)][\"PlayId\"].unique()\n            self.valid_list.append((trn_play_ids, val_play_ids))\n        elif self.valid_type == \"time_split\":\n            game_ids = df[\"GameId\"].unique()\n            trn_game_ids, val_game_ids = \\\n                train_test_split(\n                    game_ids,\n                    test_size=self.config.TEST_SIZE\n                )\n            trn_play_ids = \\\n                df[df[\"GameId\"].isin(trn_game_ids)][\"PlayId\"].unique()\n            val_play_ids = \\\n                df[df[\"GameId\"].isin(val_game_ids)][\"PlayId\"].unique()\n            self.valid_list.append((trn_play_ids, val_play_ids))\n\n        # group kfold\n        elif self.valid_type == \"GroupKFold\":\n            game_ids = df[\"GameId\"]\n            folds = GroupKFold(n_splits=self.config.N_FOLDS)\n            for trn_, val_ in folds.split(df, groups=game_ids):\n                trn_play_ids = df.loc[trn_, \"PlayId\"].unique()\n                val_play_ids = df.loc[val_, \"PlayId\"].unique()\n                self.valid_list.append((trn_play_ids, val_play_ids))\n        else:\n            raise(\"Implemented Error (No such valid type)\")\n\n    def get_split(self, fold_idx=None):\n        if fold_idx is None:\n            return self.valid_list[0]\n        else:\n            return self.valid_list[fold_idx]\n","97cb01db":"def read_train(config):\n    def read_data(csv_path, pkl_path):\n        if os.path.exists(pkl_path):\n            df = pd.read_pickle(pkl_path)\n        else:\n            df = pd.read_csv(csv_path)\n            if not config.IS_KERNEL:\n                df.to_pickle(pkl_path)\n        return df\n    pkl_path = os.path.join(config.DATA_DIR, \"train.pkl\")\n    csv_path = os.path.join(config.DATA_DIR, \"train.csv\")\n    df = read_data(csv_path, pkl_path)\n    return df","64f33711":"class NFLDataSet(Dataset):\n    def __init__(self,\n                 df,\n                 play_ids,\n                 numerical_features,\n                 cat_features,\n                 mode=\"train\",\n                 preprocessing=None,\n                 except_features=[]):\n        self.play_ids = play_ids\n        self.numerical_features = numerical_features\n        self.cat_features = cat_features\n        self.df = df[df[\"PlayId\"].isin(play_ids)].reset_index(drop=True)\n        self.mode = mode\n        self.preprocessing = preprocessing\n        self.target_col = \"Yards\"\n        self.use_cols = \\\n            [c for c in df.columns if c not in except_features]\n        self.retype()\n\n    def retype(self):\n        for c in self.numerical_features:\n            self.df[c] = self.df[c].fillna(0).astype(np.float32)\n\n    def __len__(self):\n        return len(self.play_ids)\n\n    def __getitem__(self, idx):\n        return \\\n            self.df.loc[idx, self.numerical_features].values, \\\n            self.df.loc[idx, self.cat_features].values, \\\n            self.df.loc[idx, self.target_col]\n","6dca01a8":"def adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef get_learning_rate(optimizer):\n    lr = []\n    for param_group in optimizer.param_groups:\n        lr += [param_group['lr']]\n    assert(len(lr) == 1)\n    lr = lr[0]\n    return lr\n\n\nclass NullScheduler():\n    def __init__(self, lr=0.01):\n        super(NullScheduler, self).__init__()\n        self.lr = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = \"NullScheduler\\n\" \\\n            + \"lr={0:0.5f}\".format(self.lr)\n        return string\n\n\nclass ManualScheduler():\n    def __init__(self, lr=0.01, lr_decay=0.9):\n        super(ManualScheduler, self).__init__()\n        self.lr_list = [lr * (lr_decay ** i) for i in range(100)]\n        self.cycle = 0\n\n    def __call__(self, time):\n        if time < len(self.lr_list):\n            return self.lr_list[time]\n        else:\n            return self.lr_list[-1]\n\n    def __str__(self):\n        string = \"ManualScheduler\\n\" \\\n            + \"lr={0:0.5f}\".format(self.lr_list[0])\n        return string","a2503dfb":"class EarlyStopping:\n    \"\"\"\n    ref: https:\/\/github.com\/Bjarten\/early-stopping-pytorch\n    \"\"\"\n    def __init__(self, patience=2, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.best_model = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model, save_name):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.best_model = copy.deepcopy(model)\n            self.save_checkpoint(val_loss, model, save_name)\n        elif score < self.best_score:\n            self.counter += 1\n            # print(f'EarlyStopping counter: {self.counter} '\n            #      'out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.best_model = copy.deepcopy(model)\n            self.save_checkpoint(val_loss, model, save_name)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, save_name):\n        if self.verbose:\n            print(f'Validation loss decreased ('\n                  '{self.val_loss_min:.5f} --> {val_loss:.5f}'\n                  ').  Saving model ...')\n            print(\"Save model: {}\".format(save_name))\n        torch.save(model.state_dict(), save_name)\n        self.val_loss_min = val_loss\n\n    def get_best_model(self):\n        return self.best_model","d1712f43":"# \u30b3\u30b9\u30c8\u95a2\u6570\u306e\u30af\u30e9\u30b9\nclass GaussianMixture(torch.autograd.Function):\n    \"\"\"Negative Log Likelihood of Gaussian Mixture model\"\"\"\n    def __init__(self, n_components):\n        # \u30ac\u30a6\u30b9\u5206\u5e03\u306e\u500b\u6570\u3001\u4eca\u307e\u3067\u306e\u4f8b\u3060\u30683\n        self.n_components = n_components\n\n    # \u30b3\u30b9\u30c8\u95a2\u6570\u306e\u5024\u3092\u8a08\u7b97\u3059\u308b\u30e1\u30bd\u30c3\u30c9\n    def __call__(self, pred, targets):\n        # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529bX\u3092\u6d3b\u6027\u5316\u95a2\u6570\u3067\u5909\u63db\u3057\u3066\u3001\u6a19\u6e96\u504f\u5dee\u3001\u6df7\u5408\u4fc2\u6570\u3001\u5e73\u5747\u3092\u8a08\u7b97\n        sigma, weight, mu = pred\n        # \u30ac\u30a6\u30b9\u95a2\u6570N(t|mu,sigma^2)\u306e\u5024\u3092\u8a08\u7b97\n        gauss = self.gauss(mu, sigma, targets) + 1e-8\n        # \u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6E(w):PRML(\u5f0f5.153)\n        return -torch.log(torch.sum(weight * gauss, dim=1)).sum()\n\n    # \u30ac\u30a6\u30b9\u95a2\u6570N(target|mu,sigma^2)\u3092\u8a08\u7b97\n    def gauss(self, mu, sigma, targets):\n        tmp = (-0.5 * (mu - targets.view(-1, 1))**2) \/ sigma**2\n        return torch.exp(tmp) \/ torch.sqrt(2 * np.pi * sigma**2)\n\n    # \u30b3\u30b9\u30c8\u95a2\u6570\u3092\u6d3b\u6027\u3067\u5fae\u5206(\u4eca\u306e\u3068\u3053\u4f7f\u3063\u3066\u306a\u3044)\n    def backward(self, X, targets):\n        sigma, weight, mu = self.activate(X)\n        var = sigma**2\n        gamma = weight * self.gauss(mu, sigma, targets)\n        gamma \/= torch.sum(gamma, axis=1, keepdims=True)\n        # \u305d\u308c\u305e\u308c\u306e\u5fae\u5206\u3092\u8a08\u7b97\n        delta_mu = gamma * (mu - targets) \/ var\n        delta_sigma = gamma * (1 - (mu - targets) ** 2 \/ var)\n        delta_weight = weight - gamma\n        # \u9023\u7d50\u3055\u305b\u3066\u304b\u3089\u8fd4\u3059\n        delta = torch.concat([delta_sigma, delta_weight, delta_mu], dim=1)\n        return delta","ccbba99c":"@numba.njit(cache=True)\ndef crps(y_true, y_pred_cdf):\n    # target_thre = target_std + target_mean\n    # y_true = y_true * target_std + target_mean\n    N = len(y_pred_cdf)  # Number of plays\n    total = 0.\n    for m in range(N):\n        for i, n in enumerate(range(-99, 100)):\n            total += (\n                y_pred_cdf[m][i] -\n                1 * (n - y_true[m] >= 0)  # Heavyside\n            ) ** 2\n    return total \/ (199 * N)","cced5d21":"class LinearBn(nn.Module):\n    def __init__(self, in_channel, out_channel, act=None):\n        super(LinearBn, self).__init__()\n        self.linear = nn.Linear(in_channel, out_channel, bias=False)\n        self.bn = nn.BatchNorm1d(out_channel, eps=1e-05, momentum=0.1)\n        self.act = act\n\n    def forward(self, x):\n        x = self.linear(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass NFLNet(nn.Module):\n    def __init__(self, numerical_features, cat_features,\n                 cat_nuniques, output_dim, target_mean, target_std,\n                 n_components=3, internal_dim=128, n_emb_out=4, target_norm=True):\n        super(NFLNet, self).__init__()\n        # features\n        self.numerical_features = numerical_features\n        self.cat_features = cat_features\n        self.numerical_input_dim = len(numerical_features)\n        self.cat_input_dim = len(cat_features)\n        self.cat_nuniques = cat_nuniques\n        # components for gmm\n        self.n_components = n_components\n        # target info\n        self.target_mean = target_mean\n        self.target_std = target_std\n        self.target_norm = target_norm\n        # model setting\n        input_dim = self.numerical_input_dim + self.cat_input_dim*n_emb_out\n        self.process = nn.Sequential(\n            LinearBn(input_dim, internal_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            LinearBn(internal_dim, internal_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            LinearBn(internal_dim, internal_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            LinearBn(internal_dim, internal_dim),\n            nn.ReLU(inplace=True)\n         )\n        self.cat_embedding = {}\n        for c in self.cat_features:\n            self.cat_embedding[c] = \\\n                nn.Embedding(self.cat_nuniques[c], n_emb_out)\n        self.output_layer = nn.Linear(internal_dim, output_dim)\n        self.weight_activate = nn.Softmax(dim=1)\n        self.pred_x = np.linspace(-99, 99, 199, endpoint=True)\n\n    def forward(self, numerical_x, cat_x):\n        emb_out = []\n        for i, c in enumerate(self.cat_features):\n            emb_out.append(self.cat_embedding[c](cat_x[:, i]))\n        emb_h = torch.cat(emb_out, dim=1)\n        h = torch.cat([numerical_x, emb_h], dim=1)\n        h = self.process(h)\n        h = self.output_layer(h)\n        X_sigma, X_weight, X_mu = \\\n            torch.split(h, self.n_components, dim=1)\n        # \u6a19\u6e96\u504f\u5dee\u3092\u6d3b\u6027\u5316\u95a2\u6570\u3067\u5909\u63db\n        X_sigma = torch.exp(X_sigma)\n        # \u6df7\u5408\u4fc2\u6570\u3092\u6d3b\u6027\u5316\u95a2\u6570\u3067\u5909\u63db\u3001\u6841\u304c\u3042\u3075\u308c\u306a\u3044\u3088\u3046\u306b\u6700\u5927\u5024\u3067\u5f15\u304f\n        X_weight = self.weight_activate(X_weight)\n        return X_sigma, X_weight, X_mu\n\n    def predict(self, numerical_x, cat_x):\n        # model forward\n        h_sigma_batch, h_weights_batch, h_mu_batch = \\\n            self.forward(numerical_x, cat_x)\n        # to numpy\n        h_sigma_arr = h_sigma_batch.numpy()\n        h_weights_arr = h_weights_batch.numpy()\n        h_mu_arr = h_mu_batch.numpy()\n        # make pred arr\n        pred_arr = np.zeros([len(numerical_x), 199])\n        for data_i, (h_sigma, h_weights, h_mu) in enumerate(\n                zip(h_sigma_arr, h_weights_arr, h_mu_arr)):\n            pred = np.zeros([self.n_components, 199])\n            # gmm\u306ecomponents\u6bce\u306e\u5206\u5e03\u3092\u7b97\u51fa\n            for comp_i in range(self.n_components):\n                if self.target_norm:\n                    pred[comp_i, :] = norm.cdf(\n                        self.pred_x, h_mu[comp_i]*self.target_std+self.target_mean,\n                        h_sigma[comp_i] * self.target_std\n                    ) * h_weights[comp_i]\n                else:\n                    # print(h_mu[comp_i], h_sigma[comp_i], h_weights[comp_i])\n                    pred[comp_i, :] = norm.cdf(\n                        self.pred_x, h_mu[comp_i], h_sigma[comp_i]\n                    ) * h_weights[comp_i]\n            pred_arr[data_i, :] = pred.sum(axis=0)\n        pred_arr = np.clip(pred_arr, 0., 1.)\n        return pred_arr","2352ba3c":"EXCEPT_FEATURES = [\"PlayId\", \"GameId\", \"Yards\"]\ncat_uniques = {}","60b41b67":"def create_features(df, deploy=False, outcomes=None):\n    def new_X(x_coordinate, play_direction):\n        if play_direction == 'left':\n            return 120.0 - x_coordinate\n        else:\n            return x_coordinate\n\n    def new_line(rush_team, field_position, yardline):\n        if rush_team == field_position:\n            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n            return 10.0 + yardline\n        else:\n            # half the field plus the yards between midfield and the line of scrimmage\n            return 60.0 + (50 - yardline)\n\n    def new_orientation(angle, play_direction):\n        if play_direction == 'left':\n            new_angle = 360.0 - angle\n            if new_angle == 360.0:\n                new_angle = 0.0\n            return new_angle\n        else:\n            return angle\n\n    def euclidean_distance(x1,y1,x2,y2):\n        x_diff = (x1-x2)**2\n        y_diff = (y1-y2)**2\n\n        return np.sqrt(x_diff + y_diff)\n\n    def back_direction(orientation):\n        if orientation > 180.0:\n            return 1\n        else:\n            return 0\n\n    def update_yardline(df):\n        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n\n        return new_yardline\n\n    def update_orientation(df, yardline):\n        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n\n        df = df.drop('YardLine', axis=1)\n        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n\n        return df\n\n    def back_features(df):\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n        carriers = carriers.rename(columns={'X':'back_X',\n                                            'Y':'back_Y'})\n        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n\n        return carriers\n\n    def features_relative_to_back(df, carriers):\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n                                         .reset_index()\n        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n                                   'min_dist','max_dist','mean_dist','std_dist']\n\n        return player_distance\n\n    def defense_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n\n        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        defense = defense.groupby(['GameId','PlayId'])\\\n                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n                         .reset_index()\n        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n\n        return defense\n\n    def static_features(df):\n        static_features = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n                                                            'YardLine','Quarter','Down','Distance','DefendersInTheBox']].drop_duplicates()\n        static_features['DefendersInTheBox'] = static_features['DefendersInTheBox'].fillna(np.mean(static_features['DefendersInTheBox']))\n\n        return static_features\n    \n    def split_personnel(s):\n        splits = s.split(',')\n        for i in range(len(splits)):\n            splits[i] = splits[i].strip()\n\n        return splits\n\n    def defense_formation(l):\n        dl = 0\n        lb = 0\n        db = 0\n        other = 0\n\n        for position in l:\n            sub_string = position.split(' ')\n            if sub_string[1] == 'DL':\n                dl += int(sub_string[0])\n            elif sub_string[1] in ['LB','OL']:\n                lb += int(sub_string[0])\n            else:\n                db += int(sub_string[0])\n\n        counts = (dl,lb,db,other)\n\n        return counts\n\n    def offense_formation(l):\n        qb = 0\n        rb = 0\n        wr = 0\n        te = 0\n        ol = 0\n\n        sub_total = 0\n        qb_listed = False\n        for position in l:\n            sub_string = position.split(' ')\n            pos = sub_string[1]\n            cnt = int(sub_string[0])\n\n            if pos == 'QB':\n                qb += cnt\n                sub_total += cnt\n                qb_listed = True\n            # Assuming LB is a line backer lined up as full back\n            elif pos in ['RB','LB']:\n                rb += cnt\n                sub_total += cnt\n            # Assuming DB is a defensive back and lined up as WR\n            elif pos in ['WR','DB']:\n                wr += cnt\n                sub_total += cnt\n            elif pos == 'TE':\n                te += cnt\n                sub_total += cnt\n            # Assuming DL is a defensive lineman lined up as an additional line man\n            else:\n                ol += cnt\n                sub_total += cnt\n\n        # If not all 11 players were noted at given positions we need to make some assumptions\n        # I will assume if a QB is not listed then there was 1 QB on the play\n        # If a QB is listed then I'm going to assume the rest of the positions are at OL\n        # This might be flawed but it looks like RB, TE and WR are always listed in the personnel\n        if sub_total < 11:\n            diff = 11 - sub_total\n            if not qb_listed:\n                qb += 1\n                diff -= 1\n            ol += diff\n\n        counts = (qb,rb,wr,te,ol)\n\n        return counts\n    \n    def personnel_features(df):\n        personnel = df[['GameId','PlayId','OffensePersonnel','DefensePersonnel']].drop_duplicates()\n        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: split_personnel(x))\n        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: defense_formation(x))\n        personnel['num_DL'] = personnel['DefensePersonnel'].apply(lambda x: x[0])\n        personnel['num_LB'] = personnel['DefensePersonnel'].apply(lambda x: x[1])\n        personnel['num_DB'] = personnel['DefensePersonnel'].apply(lambda x: x[2])\n\n        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: split_personnel(x))\n        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: offense_formation(x))\n        personnel['num_QB'] = personnel['OffensePersonnel'].apply(lambda x: x[0])\n        personnel['num_RB'] = personnel['OffensePersonnel'].apply(lambda x: x[1])\n        personnel['num_WR'] = personnel['OffensePersonnel'].apply(lambda x: x[2])\n        personnel['num_TE'] = personnel['OffensePersonnel'].apply(lambda x: x[3])\n        personnel['num_OL'] = personnel['OffensePersonnel'].apply(lambda x: x[4])\n\n        # Let's create some features to specify if the OL is covered\n        personnel['OL_diff'] = personnel['num_OL'] - personnel['num_DL']\n        personnel['OL_TE_diff'] = (personnel['num_OL'] + personnel['num_TE']) - personnel['num_DL']\n        # Let's create a feature to specify if the defense is preventing the run\n        # Let's just assume 7 or more DL and LB is run prevention\n        personnel['run_def'] = (personnel['num_DL'] + personnel['num_LB'] > 6).astype(int)\n\n        personnel.drop(['OffensePersonnel','DefensePersonnel'], axis=1, inplace=True)\n        \n        return personnel\n\n    def combine_features(relative_to_back, defense, static, personnel, deploy=deploy):\n        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,personnel,on=['GameId','PlayId'],how='inner')\n\n        if not deploy:\n            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n\n        return df\n    \n    yardline = update_yardline(df)\n    df = update_orientation(df, yardline)\n    back_feats = back_features(df)\n    rel_back = features_relative_to_back(df, back_feats)\n    def_feats = defense_features(df)\n    static_feats = static_features(df)\n    personnel = personnel_features(df)\n    basetable = combine_features(rel_back, def_feats, static_feats, personnel, deploy=deploy)\n    return basetable","737d0b69":"def process_two(t_):\n    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n    t_['fe5'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values  # N\n    t_['fe7'] = np.arccos(np.clip(t_['X'].values \/ t_['Y'].values, -1, 1))  # N\n    t_['fe8'] = t_['S'].values \/ np.clip(t_['fe1'].values, 0.6, None)\n    radian_angle = (90 - t_['Dir']) * np.pi \/ 180.0\n    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n    return t_\n","808a9c79":"def make_train_feature():\n    train = pd.read_csv('..\/input\/nfl-big-data-bowl-2020\/train.csv', dtype={'WindSpeed': 'object'})\n    outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()\n    train_basetable = create_features(train, False, outcomes=outcomes)\n    X = train_basetable.copy()\n\n    X = process_two(X)\n\n    important = ['back_from_scrimmage', 'min_dist', 'max_dist', 'mean_dist', 'std_dist',\n           'def_min_dist', 'def_max_dist', 'def_mean_dist', 'def_std_dist', 'X',\n           'Y', 'S', 'A', 'Dis', 'Orientation', 'Dir', 'YardLine']\n\n    cat = ['back_oriented_down_field', 'back_moving_down_field']\n\n    num = ['back_from_scrimmage', 'min_dist', 'max_dist', 'mean_dist', 'std_dist', 'def_min_dist', 'def_max_dist', 'def_mean_dist', 'def_std_dist',\n           'X', 'Y', 'S', 'A', 'Dis', 'Orientation', 'Dir', 'YardLine', 'Distance'] + ['fe1', 'fe5', 'fe7','fe8', 'fe10', 'fe11']\n    num = [i for i in num if i in important]\n    print(len(cat))\n    print(len(num))\n    scaler = StandardScaler()\n    X[num] = scaler.fit_transform(X[num])\n    return X, cat, num, scaler","f8174a00":"def run_train(\n        train_df, numerical_cols, cat_cols, cat_nuniques,\n        target_mean, target_std, validation, fold_idx=None):\n    if fold_idx is not None:\n        logging.info(f\"======= {fold_idx}th fold training =======\")\n    trn_play_ids, val_play_ids = validation.get_split(fold_idx)\n    # dataset\n    train_dataset = NFLDataSet(\n        train_df,\n        numerical_features=numerical_cols,\n        cat_features=cat_cols,\n        play_ids=trn_play_ids,\n        mode=\"train\",\n        except_features=EXCEPT_FEATURES\n    )\n    valid_dataset = NFLDataSet(\n        train_df,\n        numerical_features=numerical_cols,\n        cat_features=cat_cols,\n        play_ids=val_play_ids,\n        mode=\"valid\",\n        except_features=EXCEPT_FEATURES\n    )\n    # data loader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=config.N_WORKERS\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.N_WORKERS\n    )\n    # early stopping\n    early_stopping = EarlyStopping(\n        patience=config.N_EARLY_STOPPPING_PATIENCE,\n        verbose=False)\n    # criterion\n    criterion = GaussianMixture(config.N_COMPONENTS)\n    # model\n    net = NFLNet(\n        numerical_features=numerical_cols,\n        cat_features=cat_cols,\n        cat_nuniques=cat_nuniques,\n        output_dim=config.N_COMPONENTS*3,\n        target_mean=target_mean,\n        target_std=target_std,\n        n_components=config.N_COMPONENTS,\n        n_emb_out=config.N_EMB_OUT,\n        target_norm=config.TARGET_NORM\n    )\n    net.to(device)\n    # lr scheduler\n    # scheduler = ManualScheduler(\n    #     lr=config.LEARNING_RATE,\n    #     lr_decay=config.LR_DECAY,\n    # )\n    scheduler = NullScheduler(lr=config.LEARNING_RATE)\n    optimizer = torch.optim.Adam(\n        filter(lambda p: p.requires_grad, net.parameters()),\n        lr=scheduler(0))\n    # dataloaders\n    dataloaders_dict = {\n        \"train\": train_loader,\n        \"valid\": valid_loader\n    }\n    # counter\n    iteration = 1\n    best_score = None\n    # epoch loss\n    epoch_train_loss = 0.0\n    epoch_val_loss = 0.0\n    num_epochs = config.N_EPOCH\n    valid_period = config.VAL_PERIOD\n    # log\n    logging.info(f\"Optimizer\\n  {optimizer}\")\n    logging.info(f\"Scheduler\\n  {scheduler}\")\n    logging.info(f\"Batchsize\\n  {config.BATCH_SIZE}\")\n    logging.info(\"** start training here! **\")\n    logging.info(\"                      |----- VALID -----|---- TRAIN -----\")\n    logging.info(\"rate     iter   epoch |  loss    CRPS   |  loss  | time  \")\n    logging.info(\"---------------------------------------------------------\")\n    # epoch loop\n    for epoch in range(num_epochs+1):\n        t_epoch_start = time.time()\n        val_pred_list = []\n        val_true_list = []        \n        # train\n        lr = scheduler(epoch)\n        if lr < 0:\n            break\n        adjust_learning_rate(optimizer, lr)\n        net.train()\n        for numerical_x, cat_x, targets in train_loader:\n            numerical_x = numerical_x.float()\n            cat_x = cat_x.long()\n            targets = targets.float()\n            numerical_x = numerical_x.to(device)\n            cat_x = cat_x.to(device)\n            targets = targets.to(device)\n            # optimizer zero grad\n            optimizer.zero_grad()\n            with torch.set_grad_enabled(True):\n                y_sigma, y_x_weight, y_mu = net(numerical_x, cat_x)\n                loss = criterion(\n                    (y_sigma, y_x_weight, y_mu),\n                    targets)\n                loss.backward()\n                optimizer.step()\n                epoch_train_loss += loss.item()\n                iteration += 1\n        ######################\n        # valid\n        ######################\n        if((epoch+1) % valid_period == 0):\n            net.eval()\n        else:\n            continue\n        for numerical_x, cat_x, targets in valid_loader:\n            numerical_x = numerical_x.float()\n            cat_x = cat_x.long()\n            targets = targets.float()\n            numerical_x = numerical_x.to(device)\n            cat_x = cat_x.to(device)\n            targets = targets.to(device)\n            with torch.set_grad_enabled(False):\n                y_sigma, y_x_weight, y_mu = net(numerical_x, cat_x)\n                loss = criterion(\n                    (y_sigma, y_x_weight, y_mu),\n                    targets)\n                epoch_val_loss += loss.item()\n                pred = net.predict(numerical_x, cat_x)\n                val_pred_list.append(pred)\n                val_true_list.append(targets.numpy())\n        # valid score\n        val_preds = np.concatenate(val_pred_list, axis=0)\n        val_true = np.concatenate(val_true_list, axis=0)\n        if config.TARGET_NORM:\n            val_true = val_true * target_std + target_mean\n        val_crps = crps(val_true, val_preds)\n        t_epoch_finish = time.time()\n        elapsed_time = t_epoch_finish - t_epoch_start\n        lr_rate = get_learning_rate(optimizer)\n        logging.info(\n            \"{0:1.5f}  {1:4d}    {2:3d}  | {3:4.1f}   {4:1.5f}  {5:4.1f}   {6:4.1f}\"\n            .format(\n                lr_rate,\n                iteration,\n                epoch,\n                epoch_val_loss,\n                val_crps,\n                epoch_train_loss,\n                elapsed_time)\n        )\n        t_epoch_start = time.time()\n        epoch_train_loss = 0.0\n        epoch_val_loss = 0.0\n        # early stopping\n        model_save_path = \\\n            os.path.join(config.PROJECT_PATH, f\"checkpoint_{fold_idx}.pt\")\n        early_stopping(val_crps, net, model_save_path)\n        if best_score is None or val_crps < best_score:\n            best_score = val_crps\n        if early_stopping.early_stop:\n            best_score = early_stopping.best_score*(-1)\n            logging.info(\"******** Early stopping ********\")\n            logging.info(f\"Best Score: {best_score}\")\n            net = early_stopping.get_best_model()\n            break\n    return net, best_score","cbf24ace":"def prediction(models, numerical_cols, cat_cols, scaler):\n    env = nflrush.make_env()\n\n    pd.options.mode.chained_assignment = None\n    index = 0\n    test_feats = []\n    with torch.set_grad_enabled(False):\n        for (test_original_df, sample_prediction_df) in env.iter_test():\n            test_df = create_features(test_original_df, deploy=True)\n            test_df = process_two(test_df)\n            test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n            cat_x = test_df[cat_cols].values\n            numerical_x = test_df[numerical_cols].values\n            cat_x = torch.from_numpy(cat_x).to(device).long()\n            numerical_x = torch.from_numpy(numerical_x).to(device).float()\n            test_feats.append(test_df)\n            y_pred = np.zeros([len(models), 199])\n            for i, model in enumerate(models):\n                y_pred[i, :] = model.predict(numerical_x, cat_x)\n            y_pred = y_pred.mean(axis=0)\n            # y_pred = pred.numpy()\n            pred_df = pd.DataFrame(\n                data=y_pred.reshape([1, -1]),\n                columns=sample_prediction_df.columns)\n            env.predict(pred_df)\n            index += 22\n    env.write_submission_file()\n    if config.DEBUG:\n        all_test_feats = pd.concat(test_feats, axis=0)\n        all_test_feats.to_csv(\"test_df.csv\", index=False)","556498e2":"def main():\n    with timer(\"make feature...\"):\n        train_df, cat_cols, numerical_cols, scaler = make_train_feature()\n    logging.info(\"train shape: {}\".format(train_df.shape))\n    logging.info(\"numerical cols: {}\".format(len(numerical_cols)))\n    logging.info(\"categorical cols: {}\".format(len(cat_cols)))\n    if config.DEBUG:\n        train_df.to_csv(\"train_df.csv\", index=False)\n        pd.Series(cat_cols).to_csv(\"cat_cols.csv\", index=False)\n        pd.Series(numerical_cols).to_csv(\"numerical_cols.csv\", index=False)\n    cat_nuniques = {}\n    for c in cat_cols:\n        cat_nuniques[c] = train_df[c].max() + 1\n    # normalizing target\n    target_std = train_df[\"Yards\"].std()\n    target_mean = train_df[\"Yards\"].mean()\n    if config.TARGET_NORM:\n        train_df[\"Yards\"] = (train_df[\"Yards\"] - target_mean).values \/ target_std\n    # validation\n    validation = Validation(config)\n    validation.make_validation()\n    # training\n    models = []\n    cv_scores = []\n    if config.VALID_TYPE == \"GroupKFold\":\n        for fold_idx in range(config.N_FOLDS):\n            # if fold_idx > 0:\n            #     break\n            model, best_score = run_train(\n                train_df,\n                numerical_cols=numerical_cols,\n                cat_cols=cat_cols,\n                cat_nuniques=cat_nuniques,\n                target_mean=target_mean,\n                target_std=target_std,\n                validation=validation,\n                fold_idx=fold_idx\n            )\n            models.append(model)\n            cv_scores.append(best_score)\n    else:\n        model, best_score = run_train(\n            train_df,\n            numerical_cols=numerical_cols,\n            cat_cols=cat_cols,\n            cat_nuniques=cat_nuniques,\n            target_mean=target_mean,\n            target_std=target_std,\n            validation=validation,\n        )\n        models.append(model)\n        cv_scores.append(best_score)\n    for i, score in enumerate(cv_scores):\n        logging.info(f\"Fold {i}: {score}\")\n    logging.info(\"Average Score: {}\".format(np.mean(cv_scores)))\n    if config.IS_KERNEL:\n        prediction(models, numerical_cols, cat_cols, scaler)","89243e00":"if __name__ == \"__main__\":\n    main()","1680cb48":"# Implementation(Pytorch)\nIn this kernel, features are used from these kernels.\n- https:\/\/www.kaggle.com\/ryancaldwell\/location-eda\n- https:\/\/www.kaggle.com\/coolcoder22\/nn-19-features","16b610ad":"# EarlyStopping","c74b4516":"# LRScheduler","b5eb8e29":"# Train","ed507288":"# Prediction","10615781":"# DataSet","9e14e081":"# Loss","53599674":"# ReadData","a3636ce3":"# Model","5e3b36f0":"# Feature\n- https:\/\/www.kaggle.com\/ryancaldwell\/location-eda\n- https:\/\/www.kaggle.com\/coolcoder22\/nn-19-features","e4b4efb5":"# Metric","7ee82312":"# Mixture Density Networks(MDN)\n\nMixture density networks(MDN)(Bishop, 1994) is a neural network with outputs of Gaussian Mixuture Model parameter.  \n\nMDN have 3types of output, and the output number is changed by parameter N(the number of gaussian distrubution components).\n- \u03c0(k): a weight of (k)th gaussian distribution(\u2211\u03c0(k) = 1)\n- \u03bc(k): a mean of (k)th gaussian distribution\n- \u03c3(k): a standard deviation of (k)th gaussian distribution\n\nIf N = 3, there are 9 output(3x3components).\n\n## Why GMM?\nMy idea is near [This kernel](https:\/\/www.kaggle.com\/kenmatsu4\/nn-outputs-gaussian-distribution-directly) .\n\nI think the target distribution is more complicated than a single gaussian distribution looking at target distribution.  \n\n\n## Reference\n- thesis: https:\/\/publications.aston.ac.uk\/id\/eprint\/373\/1\/NCRG_94_004.pdf\n- imprement referencce: https:\/\/qiita.com\/ctgk\/items\/19c4a4f205b855cf6a05 (Japanese)","d50c0aa9":"# Config","8bd12cd3":"# Validation"}}