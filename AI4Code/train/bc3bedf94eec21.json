{"cell_type":{"23a1ab04":"code","0661e565":"code","d6b704b5":"code","88f857bb":"code","3b97f252":"code","f54abc1b":"code","eb5bc0d4":"code","b3d2f4d9":"code","8f89a5b8":"code","05ce97ff":"code","7acd52b6":"code","67ab9910":"code","8c20dc03":"code","60cd3a13":"code","d4e41339":"code","475fffa2":"code","d28c064f":"code","3ab55325":"code","267f3eda":"code","d4c9b983":"code","63f1c221":"code","f8204f13":"code","68860064":"code","803de6cc":"code","f845d3f5":"code","e6c9bd1b":"code","376aeb86":"code","6d846383":"code","fd5f4744":"code","aae07aa4":"code","8055a57d":"code","fd78f77f":"code","fa56c8f3":"code","dcc65887":"code","ff0c3be8":"markdown","2d7bd92f":"markdown","8513fe90":"markdown","e1e75108":"markdown","7a631500":"markdown","aa9cdaff":"markdown","2647c11e":"markdown","78688232":"markdown","f4ba3c3d":"markdown","8371ca51":"markdown","61806c7b":"markdown","dc7e32e4":"markdown","933979c9":"markdown","5a6a6156":"markdown","016ee7f4":"markdown","c7c57e5d":"markdown","d55c21c2":"markdown"},"source":{"23a1ab04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0661e565":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')","d6b704b5":"df.shape","88f857bb":"df.dtypes","3b97f252":"df.isnull().sum()","f54abc1b":"df.head()","eb5bc0d4":"df.describe()","b3d2f4d9":"df[df == 0].count()","8f89a5b8":"df[['RestingBP', 'Cholesterol']] = df[['RestingBP', 'Cholesterol']].replace(0, np.nan)","05ce97ff":"df.isnull().sum()","7acd52b6":"def load_data(df):\n    df = impute(df)\n    df = encode(df)\n    return df","67ab9910":"def impute(df):\n    df_1 = df[df['Cholesterol'].notna()]\n    df_1 = df_1[df_1['RestingBP'].notna()]\n    return df_1","8c20dc03":"def encode(df):\n    categorical_features = ['Sex', \n                        'ChestPainType',\n                        'RestingECG',\n                        'ExerciseAngina',\n                        'ST_Slope']\n    for name in categorical_features:\n        df[name] = df[name].astype(\"category\")\n    \n    return df","60cd3a13":"df = load_data(df)\ndf.head()","d4e41339":"df.dtypes","475fffa2":"model = XGBClassifier(use_label_encoder =False, random_state = 0, eval_metric = 'logloss', n_estimators = 500, learning_rate = 0.05)","d28c064f":"def score_dataset(X, y, model=XGBClassifier(use_label_encoder =False, random_state = 0, eval_metric = 'logloss')):\n    \n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"accuracy\",\n    )\n    score = score.mean()\n    return score","3ab55325":"X = df.copy()\ny = X.pop(\"HeartDisease\")\nprint(score_dataset(X, y))","267f3eda":"def max_heart_perc(df):\n    X = df.copy()\n    X['MaxHRPercentage'] = X['MaxHR'] \/ (205.8 - X['Age'] * 0.685) * 100\n    return X","d4c9b983":"df[['Cholesterol', 'Age', 'RestingBP', 'ExerciseAngina']].describe()","63f1c221":"sns.boxplot(x = 'HeartDisease', y = 'Cholesterol', data = df).set_title(\"Heart Disease vs Cholesterol\")","f8204f13":"sns.boxplot(x = 'HeartDisease', y = 'RestingBP', data = df).set_title(\"Heart Disease vs RestingBP\")","68860064":"sns.boxplot(x = 'HeartDisease', y = 'Age', data = df).set_title(\"Heart Disease vs Age\")","803de6cc":"def risk_zone(df):\n    X = df.copy()\n    risk = []\n    for i in X.index:\n        if all([X.loc[i, 'Age'] >= 58, X.loc[i, 'Cholesterol'] >= 250, X.loc[i, 'RestingBP'] >= 140]):\n            risk.append(1)\n        else:         \n            risk.append(0)\n    X['Risk'] = risk\n    return X\n","f845d3f5":"def clusterer(df):\n    X = df.copy()\n    features = [ 'MaxHRPercentage', 'ExerciseAngina', 'RestingECG']\n    kmeans = KMeans(n_clusters = 10, n_init = 50, random_state = 0)\n    scaler = StandardScaler()\n    cols = pd.DataFrame(scaler.fit_transform(X[features]))\n    X[\"Cluster\"] = kmeans.fit_predict(cols)\n    return X\n    \n    ","e6c9bd1b":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","376aeb86":"X = df.copy()\nfor colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\ny = X.pop(\"HeartDisease\")\npca, X_pca, loadings = apply_pca(X)\nprint(loadings)","6d846383":"def pca_inspired_features(df):\n    X = df.copy()\n    X['PCA'] = 0.20 * X.Age + 0.06 * X.Cholesterol + 0.16 * X.RestingBP\n    return X","fd5f4744":"def create_features(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Create the Max Heart Rate Percentage feature.\n    X = max_heart_perc(X)\n    \n    #Creates a cluster from Cholesterol, Age and RestingBP\n    X = clusterer(X)\n    #Creates a feature inspired by principal componenets\n    X = pca_inspired_features(X)\n    \n    X = risk_zone(X)\n    \n    return X","aae07aa4":"X = df.copy()\nX = create_features(X)\ny = X.pop(\"HeartDisease\")\n#xgb_params = {\n#   'max_depth': [2,3,4,5,6,7,8,9,10],           # maximum depth of each tree - try 2 to 10\n#    'learning_rate': list(np.arange(0.0001, 0.1)),    # effect of each tree - try 0.0001 to 0.1\n#   'n_estimators' : [1000, 2000, 3000,4000,5000,6000,7000,8000],     # number of trees (that is, boosting rounds) - try 1000 to 8000\n#    'min_child_weight': [1,2,3,4,5,6,7,8,9,10],    # minimum number of houses in a leaf - try 1 to 10\n#    'colsample_bytree': list(np.arange(0.2, 1, 0.05)),  # fraction of features (columns) per tree - try 0.2 to 1.0\n#    'subsample': list(np.arange(0.2, 1.0, 0.05)),         # fraction of instances (rows) per tree - try 0.2 to 1.0\n#    #'reg_alpha': list(np.arange(0.0, 10, 0.5)),         # L1 regularization (like LASSO) - try 0.0 to 10.0\n#    #'reg_lambda' : list(np.arange(0.0, 10, 0.5)),        # L2 regularization (like Ridge) - try 0.0 to 10.0\n#    'num_parallel_tree': [2],   # set > 1 for boosted random forests\n#    'use_label_encoder': [False], \n#    'random_state' : [0], \n#    'eval_metric' : ['logloss']\n#}\n#clf = RandomizedSearchCV(estimator = XGBClassifier(), param_distributions = xgb_params, n_iter = 10, random_state = 7)\n#clf.fit(X, y)","8055a57d":"#clf.best_params_\n#clf.best_score_","fd78f77f":"X = df.copy()\nX = create_features(X)\ny = X.pop(\"HeartDisease\")\nprint(score_dataset(X,y, model = XGBClassifier(**{'use_label_encoder': False,\n 'subsample': 0.5499999999999999,\n 'random_state': 0,\n 'num_parallel_tree': 2,\n 'n_estimators': 7000,\n 'min_child_weight': 1,\n 'max_depth': 10,\n 'learning_rate': 0.0001,\n 'eval_metric': 'logloss',\n 'colsample_bytree': 0.49999999999999994})))","fa56c8f3":"xgb_model = XGBClassifier(**{'use_label_encoder': False,\n 'subsample': 0.5499999999999999,\n 'random_state': 0,\n 'num_parallel_tree': 2,\n 'n_estimators': 7000,\n 'min_child_weight': 1,\n 'max_depth': 10,\n 'learning_rate': 0.0001,\n 'eval_metric': 'logloss',\n 'colsample_bytree': 0.49999999999999994})\nxgb_model.fit(X,y)","dcc65887":"data = {'feature_names':X.columns,\n       'feature_importance': xgb_model.feature_importances_}\nfi_df = pd.DataFrame(data)\nfi_df.sort_values(by = ['feature_importance'], ascending = False, inplace = True)\nsns.barplot(x = fi_df['feature_importance'], y = fi_df['feature_names']).set_title(\"XGBClassifier Feature Importance\")\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature Names\")","ff0c3be8":"# Loading the data\nFirst read all the data.","2d7bd92f":"Now we start doing a pipeline.\nFrom trying a few different methods for imputing(KNNImputer, SimpleImputer) I found that just removing all the missing values worked the best so that is what I am going to do.","8513fe90":"# Feature Engineering","e1e75108":"Now we put all the feature engineering functions together.","7a631500":"# Clustering\nLets create an extra feature by clustering variables using a KMeans","aa9cdaff":"The data has a column that contains the person's max heart rate lets see if we can get better results using a column that has the percentage of persons max heart rate relative to people in that age group. \\\nI will use the formula 205.8 - Age * 0.685 to calculate the max heart rate for an age group.","2647c11e":"# Post Modeling\nLets see what features the model valued the most.","78688232":"# Info\nThe goal of this project was mainly to practice feature engineering in Python using some recently learned methods.\\\nIn this kernel I have used mathematical column transformations, PCA inspired column transformations and clustering using KMeans. \\\nThe model used was XGBoost which I tuned using the Random Search Cross-validation.\nThe goal of the model is to accurately predict if someone has a heart disease.","f4ba3c3d":"# Tuning and Final Results\nNow when we have created a few extra predicitons lets tune the hyper parameters and get our final results.\nBe aware that the tuning takes a bit of time. But since I have already done the tuning before I kept results so that you need run through it again.","8371ca51":"After doing some research I found that there are 3 big risk factors for heart disease which are high blood pressure, high cholesterol levels and age.\nLets use these factors to create a risk zone feature that says if they are at high risk of heart disease or not.\nFirst lets look at the columns to try find some values for the risk zone.","61806c7b":"Here we see that most features were not very important for the model and that it mostly used the feature ST_Slope.","dc7e32e4":"The final result is 0.8579. Which is quite a small performance boost from our baseline and it is quite possibliy only due the tuning of hyperparameters.","933979c9":"# PCA\nLets if by using PCA we can get insprations for a new feature.","5a6a6156":"# Baseline Results\nLets see what our baseline results are without doing any feature engineering and tuning.","016ee7f4":"Lets change the 0 values to NaN.","c7c57e5d":"Here I am seeing problem in the RestingBP and Cholesterol having minimum values of 0 since that does not make any sense. \\","d55c21c2":"# Libraries"}}