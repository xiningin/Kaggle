{"cell_type":{"4b8424b3":"code","78bfa675":"code","835dd3be":"code","5c919dec":"code","c19fde26":"code","c703c903":"code","9b2ee214":"code","ba9b8e23":"code","39a352c8":"code","64cec64f":"code","152a8843":"code","e87ade50":"code","cacbe16d":"code","fa28a0cc":"code","c4c21c9d":"code","96f6af7a":"code","20da33dc":"code","fc798aac":"code","3093d4a8":"code","fbc486f1":"code","73977cf1":"code","418c7544":"code","7dc05c59":"code","dd1b8dcd":"code","a49fa075":"code","dadb1220":"code","c4bf23b3":"code","14985af0":"code","2187ab68":"code","8f3066d5":"code","15837b07":"code","bb26c8c3":"code","2353df37":"markdown","3320f7a4":"markdown","67b5f621":"markdown","3157a780":"markdown","144641b4":"markdown","3849952d":"markdown","8580da88":"markdown","dce021b6":"markdown","8bad6bb9":"markdown","6207e199":"markdown","b783453b":"markdown","4d3adbac":"markdown","ff5acb75":"markdown","40ec31de":"markdown","0ab85a6d":"markdown"},"source":{"4b8424b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78bfa675":"import pandas as pd \nimport numpy as np \nfrom pandas import DataFrame,Series \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \nfrom sklearn.preprocessing import StandardScaler ","835dd3be":"#read_csv file \n\ndf_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")","5c919dec":"#check columns , shpe ...\nprint(\"shape:\", df_train.shape)\nprint(\"columns\", df_train.columns)","c19fde26":"df_train[\"SalePrice\"].describe()\n\n#min is larger than zero ","c703c903":"#distributing \n\nsns.distplot(df_train[\"SalePrice\"])\n\n# we can see that this has normal distribution .\n# show peakedness ","9b2ee214":"var1 = \"TotalBsmtSF\"\nvar2 = \"GrLivArea\"\nplt.figure(figsize = (10,4))\nplt.subplot(1,3,1)\n\nplt.scatter(df_train[var1], df_train[\"SalePrice\"])\nplt.xlabel(\"TotalBsmtSF\")\nplt.ylabel(\"SalePrice\")\n\nplt.subplot(1,3,3)\nplt.scatter(df_train[var2], df_train[\"SalePrice\"])\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\n\n#both have powerfull positive linearity ","ba9b8e23":"var = \"YearBuilt\"\ndata = pd.concat([df_train[\"SalePrice\"], df_train[var]], axis =1)\nf , ax = plt.subplots(figsize= (16, 8))\n\nfig = sns.boxplot(x= var , y = \"SalePrice\" , data = data )\nplt.xticks(rotation =90)\n\n# new bubuilt has tendency that price has wide range","39a352c8":"var = \"OverallQual\"\n\ndata = pd.concat([df_train[\"SalePrice\"], df_train[var]], axis =1)\n\nf , ax = plt.subplots(figsize=(8,6))\nfig = sns.boxplot(x=var , y= \"SalePrice\" , data =data)\n\n\n#positive reration to sale price","64cec64f":"corrmatrix = df_train.corr()\nplt.figure(figsize = (12,9))\nsns.heatmap(corrmatrix , square =True)","152a8843":"k =10 \ncols = corrmatrix.nlargest(k,\"SalePrice\")[\"SalePrice\"].index\ncm =np.corrcoef(df_train[cols].values.T)\nhm = sns.heatmap(cm , cbar =True , annot=True , square =True,\n                annot_kws={\"size\":10},yticklabels=cols.values,\n                xticklabels =cols.values)\n\nplt.show()\n\n## plot heatmap  by top10 numericalfeature  related to \"SalePrice\" ","e87ade50":"#pairplot to see overview of corelationship \n\ncols = [\"SalePrice\",\"OverallQual\",\"GrLivArea\", \"GarageCars\",\n       \"TotalBsmtSF\",\"FullBath\", \"YearBuilt\"] #features which we adopt.\n\nsns.pairplot(df_train[cols])","cacbe16d":"#missing data \ntotal = df_train.isnull().sum().sort_values(ascending=False) \npercent = (df_train.isnull().sum()\/df_train.count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent], axis=1 )\nmissing_data.columns = [\"Total\", \"Percent\"]\nmissing_data.loc[\"GarageCars\",:]\nmissing_data.head(20)\n\n#sum()=>count True by columns \n#count()=> all caunt by columns","fa28a0cc":"delete_index = missing_data[missing_data[\"Total\"] >1].index\ndf_train= df_train.drop(delete_index.values , axis =1)\ndf_train =df_train.drop(df_train.loc[df_train[\"Electrical\"].isnull()].index)\n\ndf_train.isnull().sum().max()","c4c21c9d":"df_train.columns","96f6af7a":"#to define a threshhold to define an outlier , we use StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nsaleprice_scaled = StandardScaler().fit_transform(df_train[\"SalePrice\"][:,np.newaxis])\n\nsorted_price = np.sort(saleprice_scaled ,axis =0)\n#np.array(df_train[\"SalePrice\"]).reshape(-1,1)\n\nlow_range = sorted_price.flatten()[:10].reshape(-1,1)\nhigh_range = sorted_price.flatten()[-10:].reshape(-1,1)\n\nprint(\"out range (low) of distribution:\" )\nprint(low_range)\nprint(\"\\n\\n_____________________________________________\\n\\n\")\nprint(\"out range(high) of distribution:\")\nprint(high_range)","20da33dc":"var =\"GrLivArea\"\ndata = pd.concat([df_train[\"SalePrice\"],df_train[var]],axis=1)\ndata.plot.scatter(x=var , y = \"SalePrice\" , ylim=(0.8000000))\nplt.grid()","fc798aac":"#delete points \n#ascending => sort upper \n#axis =1\n\ndelete_index = df_train.sort_values(by = var ,ascending=False)[:2].index\ndf_train.drop(delete_index , axis =0)","3093d4a8":"var = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\nplt.grid()","fbc486f1":"import scipy.stats as stats\n\nplt.grid(True)\nplt.title(\"SalePrice-distribution\")\nsns.distplot(df_train[\"SalePrice\"]  ,bins =100)\nfig = plt.figure()\n\nres = stats.probplot(df_train[\"SalePrice\"] , plot=plt)","73977cf1":"# to nmalize , we use transformation to data by log.\ndf_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])","418c7544":"sns.distplot(df_train['SalePrice']);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","7dc05c59":"df_train","dd1b8dcd":"df_train[\"Electrical\"].value_counts() #categorical feature ","a49fa075":"df_test= pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\",\n                     \n                    )\n\ndf_train = pd.concat([df_train , pd.get_dummies(df_train[\"Electrical\"])], axis =1)\ndf_test = pd.concat([df_test , pd.get_dummies(df_test[\"Electrical\"])], axis =1) ","dadb1220":"from sklearn.linear_model import LinearRegression \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import Lasso\n\n#dummy_electrical = pd.get_dummies(df_train[\"Electrical\"],drop_first =True )\n#df_train = pd.concat([df_train , dummy_electrical] , axis =1)\n\n\nfeatures =  [\"OverallQual\",\"GrLivArea\", \"GarageCars\",\n                   \"TotalBsmtSF\",\"FullBath\", \"YearBuilt\",'1stFlrSF','FuseA', 'FuseF', 'FuseP', 'SBrkr']\n\ntrain_data = df_train[features]\nX = train_data.values\ny = df_train[\"SalePrice\"]\n\nX_train,X_val , y_train, y_val = train_test_split(X, y , random_state =0)\n\nmodel1 = RandomForestRegressor(random_state =0 , max_depth=1000)\nmodel2 = LinearRegression() \nmodel3 = Ridge()\nmodel4 = Lasso()\nmodels = [model1 , model2 , model3 , model4]\n\n\nfor i , model in enumerate(models):\n    model.fit(X_train , y_train )\n    y_pred_train = model.predict(X_train)\n    y_pred_val = model.predict(X_val)\n    print(\"Model{}'s RMSE for train:{}\".format(i+1 , np.sqrt(mean_squared_error(y_train, y_pred_train))))\n    print(\"Model{}'s RMSE for test:{}\".format(i+1 , np.sqrt(mean_squared_error(y_val, y_pred_val))))\n    print(\"\\n____________________\\n\")","c4bf23b3":"train_data","14985af0":"# we decide to use ridge_model , we shoud decide alpha \n\naccuracy_list = []\nalpha_range= np.arange(10,1000)\nfor alpha in alpha_range:\n    model = Ridge(alpha = alpha).fit(X_train, y_train)\n    y_pred = model.predict(X_val) \n    accuracy_list.append(np.sqrt(mean_squared_error(y_pred, y_val)))\n    \nplt.plot(alpha_range , accuracy_list , label= \"accuracy_test\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"RMSE\")\nplt.title(\"accuracy by alpha\")\nplt.legend()\nalpha_reasonable = accuracy_list.index(min(accuracy_list))","2187ab68":"# last model we use\n\nRidge_full_model = Ridge(alpha =alpha_reasonable).fit(X,y)","8f3066d5":"df_test[features].isnull().sum()  #there are missing data (\u00b4;\u03c9;\uff40)","15837b07":"X_test = df_test[features].fillna(df_test[features].mean())\nX_test = X_test.values","bb26c8c3":"prediction = Ridge_full_model.predict(X_test)\n\noutput = pd.DataFrame({\"Id\":df_test.Id , \"SalePrice\":prediction})\n\noutput.to_csv(\"Submission_Group6.csv\" , index =False)","2353df37":"**submission to csv**","3320f7a4":"**closs validation**","67b5f621":"analize with objective prespection\ncorrelation matrix\n\"SalePrice\"correlation matrix\nscatter plots between most colelated variable\ncorrelation matrix\nTo use correlation matrix , we can avoid malti colinearity\n\nwe use heatmap to check over view of relationship\n\nif there are strong relation between features , we can`t gain acurrate prediction.","3157a780":"* when more than 15% of data is missing , we should pretend it never exisit. => delete garageX\n* because low percent of missing , we delete missing data \"Electrical\"","144641b4":"**analysising for saleprice","3849952d":"check the categorical feature","8580da88":"**Missing data**","dce021b6":"how the set of dotsregarding last year has tendency to incresing faster. (like exponetial function)","8bad6bb9":"Relation to saleprice\n\"GrLivArea\" ,\"TotalBsmtSF\" indicate space of house. so it must ave positibe relation","6207e199":"**we decide to use ridge_model , we shoud decide alpha**","b783453b":"* \"GarageCars\" and \"GarageArea\" are close to.\n* \"TotalBsmtSF\" and\"1stFlrSF\" are close to .\n* \"YearBuilt\" has weak coleration to \"Sale Price\".","4d3adbac":"* two bigger val is defined outlier ,it is lelated to location?","ff5acb75":"**Out liars!**","40ec31de":"**make model to prediction**","0ab85a6d":"we can check relation between features. next , we are going to analise letationsip with taeget valuable\n\nnlargest... in SalesPrice columns,get the row has most large number top k"}}