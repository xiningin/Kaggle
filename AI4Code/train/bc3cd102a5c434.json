{"cell_type":{"5e1287de":"code","25469eb9":"code","0e6ecc84":"code","8ec56bec":"code","3d6e2959":"code","703e3695":"code","f4eee795":"code","acb9ee0a":"code","5a45e30f":"code","92db2d07":"code","799873b6":"code","9ba5c40f":"markdown","9c3d14b7":"markdown","8cbac7ea":"markdown","7e760426":"markdown","7761cf04":"markdown","5b21efd2":"markdown","f16d169f":"markdown","45367197":"markdown","e6ead7c4":"markdown","5befd079":"markdown","a7fe418d":"markdown","9dfe1314":"markdown","f6cc3907":"markdown","9a5830a5":"markdown","1f796d84":"markdown","78e1a24b":"markdown","6afcea3b":"markdown","65d5b10d":"markdown","10d514af":"markdown","459fe221":"markdown","d3ae2598":"markdown","678eaa24":"markdown"},"source":{"5e1287de":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics ","25469eb9":"#Function to calculate the entropy of a probability distribution\ndef calculateEntropy(prob_function):\n    entropy = 0\n    for i in range(len(prob_function)):\n        if(prob_function[i] != 0):\n            entropy += prob_function[i]*np.log2(prob_function[i])\n    return -entropy\n\n#We will use a Bernoulli's distribution where our random variable assumes the values 0 and 1\n#with the probabilities (1 - p) and p, respectively.\n#Note that Bernoulli's distribution, gives us a different distribution for each value of p, \n#consequently, Entropy will also be different for each of those distributions.\nx = np.linspace(0,1, 20)\ny = [calculateEntropy([1 - p, p]) for p in x]\nplt.plot(x, y, '-bo')\nplt.xlabel(\"Parameter p for Bernourlli's distribution\")\nplt.ylabel(\"Entropy\")\nplt.show","0e6ecc84":"col_names = ['Outlook','Temperature','Humidity','Wind','PlayTennis']\n\nexample = pd.read_csv(\"..\/input\/play-tennis\/exemplo_arvore.csv\", header=None, names=col_names)\nexample.head()","8ec56bec":"X = example.loc[:, example.columns != 'PlayTennis']\ny = example.PlayTennis","3d6e2959":"X_dummy = pd.get_dummies(X) #transforming the dataset\nX_dummy.head()","703e3695":"# creating the variable for the Decision Tree classifier.\nclf = DecisionTreeClassifier(criterion=\"entropy\")\n\n# We use the method fit to build the decision tree out of our data.\nclf = clf.fit(X_dummy, y)","f4eee795":"clf.predict(X_dummy)","acb9ee0a":"fig = plt.figure(figsize=(25,20))\n_ = plot_tree(clf, feature_names=X_dummy.columns, class_names=list(y.unique()), filled=True)\n","5a45e30f":"#Building the Random Forest classifier\n#The parameter n_estimators is how many trees are gonna be in the forest\n#The parameter bootstrap is telling the algorithm to execute the process of Bagging for each tree.\n#The parameter max_features is the number of features that can be chosen at each iteration, in the following case\n#the number is the square root of how many features are available.\n#The parameter criterion is the criteron used to build the tree, in this case Entropy\/Mutual Information.\nclf = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=\"sqrt\", criterion=\"entropy\")\n\n# We use the method fit to build the classifier from the data provided.\nclf = clf.fit(X_dummy, y)","92db2d07":"clf.predict(X_dummy)","799873b6":"fig = plt.figure(figsize=(25,20))\n_ = plot_tree(clf.estimators_[25], feature_names=X_dummy.columns, class_names=list(y.unique()), filled=True)","9ba5c40f":"It is possible to see a Decision Tree generated for this dataset in the figure below.<br> Note that we are deciding whether we will play tennis or not based off the wheather information we have.<br>\n","9c3d14b7":"We will use the method 'DecisionTreeClassifier' from the scikit-learn library on python, to classify our dataset of 'PlayTennis'.<br>","8cbac7ea":"Note that when we want to model a problem, where the column  <i>target<\/i>, in the example above 'PlayTennis', can get assigned only two values, we have a boolean function.<br>\nIn the case above it is possible to map:<br>\nNo $\\rightarrow$ False <br>\nYes $\\rightarrow$ True <br>\nAnd just like that, build a Truth Table for the Decision Tree:\n<table>\n  <tr>\n    <th>Outlook = Sunny<\/th>\n    <th>Outlook = Overcast<\/th>\n    <th>Outlook = Rain<\/th>\n    <th>Humidity = High<\/th>\n    <th>Humidity = Normal<\/th>\n    <th>Wind = Strong<\/th>\n    <th>Wind = Weak<\/th>\n    <th>PlayTennis<\/th>\n  <\/tr>\n  <tr>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n  <\/tr>\n  <tr>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n  <\/tr>\n  <tr>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n  <\/tr>\n  <tr>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n  <\/tr>\n  <tr>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>False<\/td>\n    <td>True<\/td>\n    <td>True<\/td>\n  <\/tr>\n<\/table>  \nMany lines of the table above were omitted, otherwise, such table would be humongous. More specifically, since we have 7 columns used for classification, and each of those can get assigned two values: True or False. We have $2^7 = 128$ possible combinations of values, that is, this Truth Table complete, would have 128 lines.","7e760426":"In order to start building a Random Forest, initially we shall decide how many trees there will be in that Forest.<br>\nAfter deciding the amount of trees in the Forest, we will make a new dataset from the original, selecting samples from the original dataset randomly, these selected samples will be present in this new dataset we are building. The technique is called <i>Bagging<\/i>.<br>\nFor each tree in the Random Forest there will be a new dataset generated from the <i>Bagging<\/i> technique, which will also be the dataset used to train the tree it is associated with.<br>\n\nThere are many different ways of choosing heuristics for building Decision Trees in a Random Forest. The default from the 'sklearn' library, is to select at each iteration a limited number of <i>features<\/i> that could be in the node of the current iteration.<br>\nIn 'sklearn', it is possible to define a constant $n$, in a way that in each iteration, the algorithm can choose only $n$ <i>features<\/i> selected randomly. The 'sklearn' library also provides the option of defining a function of the available number of <i>features<\/i>.<br>\nFor example, let $n$ be the number of available <i>features<\/i> in the current iteration, sklearn's default is to select $\\sqrt{n}$ random <i>features<\/i> that can be chosen for that node.","7761cf04":"# How to find a \"good\" tree\n\nNotice that we are making use of the word \"good\", in quotes, since we are building an algorithm for finding a tree that is good and not optimal (as seen before the optimal tree is a intractable problem).<br>\nThe idea is to put the most important attributes closer to the root of the tree, while the least important attributes get put in the lower levels of the tree. By \"most important attributes\" we refer to the attributes that better classify our <i>target<\/i>. <br>\n\nFor example, in the problem above, we see that the attribute 'Outlook = Overcast', classifies well 'PlayTennis', since  in every case where 'Outlook = Overcast' we see 'PlayTennis' as True, in other word, conditioning 'PlayTennis' on 'Outlook = Overcast', would reduce uncertainty (or <b>Entropy<\/b>) in 'PlayTennis'.<br>\nIn order to build the algorithm, instead of choosing the attributes that better classify the <i>target<\/i>, we shall choose the <i>features<\/i>, as seen previously, the <i>features<\/i> are the nodes of the trees and the attributes are the edges between the nodes.<br>\n\nFurthermore, we have a good way of determing how much uncertainty in a random variable is reduced, when we condition it on another one, and that is the <b>Mutual Information<\/b>.<br>\n\nIn the example of 'PlayTennis', in order to choose the <i>feature<\/i> that better classifies 'PlayTennis', we would calculate I(PlayTennis, Outlook), I(PlayTennis, Humidity), I(PlayTennis, Wind) and I(PlayTennis, Temperature) and choose the <i>feature<\/i> that maximizes the <b>Mutual Information<\/b>.<br>\n\nAfter we have decided the first <i>feature<\/i> that will be used to classify the <i>target<\/i>, we use such <i>feature<\/i>, to split our dataset into $n$ datasets, where $n$ is the number of attributes that can get assigned to the chosen <i>feature<\/i>.<br>\n\nAplying this idea to the example of 'PlayTennis', suppose that the chosen <i>feature<\/i> is 'Humidity', which has two attributes associated to it ('High' or 'Normal'), we build two dataframes que that together form the original dataframe, in one of them we will have every line of the original dataframe, where 'Humidity = High' e in the other one we will have every line of the original dataframe, where 'Humidity = Normal'.<br>\n\nUsing the generated dataframes, from the split of the original <i>dataframe<\/i>, we shall do the same process of classification, choosing the <i>feature<\/i> that best classifies 'PlayTennis' in each of the generated dataframes gerados. Notice that, as soon as the original dataframe is split into $n$ dataframes, each of these new <i>dataframes<\/i> is a new Decision Tree problem, with less examples (or lines) and minus one <i>feature<\/i>.\n\nWe shall do such process of splitting the dataframe, until we don't have any more <i>features<\/i> to use in the classification, or until the <b>Entropy<\/b> in <i>target<\/i> is 0, that is, all the values in the column <i>target<\/i> are the same.\n","5b21efd2":"# Scikit-learn's Decision Tree","f16d169f":"# Decision Trees and Random Forests","45367197":"The forest is composed by 100 trees (parameter that can be changed in 'n_estimators'), because of that it is not possible to visualize the entire forest, therefore, we pick one tree to be shown.","e6ead7c4":"What the Decision Tree above is telling us is the following:<br>\nIn case the Outlook is Sunny, we will play tennis if Humidity is Normal, in case it is High, we won't play.<br>\nIn case the Outlook is Overcast, we will play tennis.<br>\nIn case the Outlook is Rain, we will play tennis if Wind is Weak, in case it is Strong, we won't play.<br>","5befd079":"# Mutual Information\n\n<p>Beyond the concept of <b>Entropy<\/b>, we also need to address how <b> Mutual Information <\/b> works:<\/p>\n\\begin{equation}\nI(X,Y) = H(X) - H(X|Y)\n\\end{equation}\n<p>Notice that now we have introduced <b>Conditional Entropy<\/b> $H(X|Y)$, prior to addressing this matter properly, we shall understand the intuition behind Mutual Information, we are calculating the amount reduced in the Entropy of X when conditioned on Random Variable Y. In other words, how much conditioning X on Y, reduced the uncertainty of X.<\/p>\n<p>From this, it is possible to realize that Mutual Information is a measure of dependency between random variables, that is, if we have big reduction in the uncertainty of X, when conditioning the value of X on Y, it is possible to claim that Y and X are related to the deegre of how big that reduction is.<\/p>\n\n***\n\n<p> <b>Conditional Entropy<\/b> $H(X|Y)$, comes from the definition of <b>Entropy<\/b>, shown previously, from the expectation of a function of Random Variables:<\/p>\n\\begin{equation}\nH(X|Y) = E\\left[\\frac{1}{log_2(P(X = x | Y = y))} \\right] = - E\\left[log_2(P(X = x | Y = y)) \\right] \n\\end{equation}\n\\begin{equation}\n= - \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y)\\cdot log_2(P(X = x | Y = y))\n\\end{equation}\n<p>With $ \\mathcal{Y}$ e $\\mathcal{X}$ being the state spaces of the random variables Y and X, respectively.<\/p>\n\n<p>Now that we know <b>Conditional Entropy<\/b>, we can get a better equation for the <b>Mutual Information<\/b>:<\/p>\n\\begin{equation}\nI(X,Y) = H(X) - H(X|Y) = - \\sum_{x \\in \\mathcal{S}} P(X = x) log_2(P(X = x))  - \\left(- \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y)\\cdot log_2(P(X = x | Y = y)) \\right)\n\\end{equation}\n\\begin{equation}\n= - \\sum_{x \\in \\mathcal{X}} P(X = x) log_2(P(X = x))  + \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y)\\cdot log_2(P(X = x | Y = y))\n\\end{equation}\n\\begin{equation}\n= - \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y) \\cdot log_2(P(X = x))  + \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y)\\cdot log_2(P(X = x | Y = y))\n\\end{equation}\n\\begin{equation}\n= \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y) \\cdot log_2\\left(\\frac{P(X = x | Y = y)}{P(X = x)}\\right)\n\\end{equation}\n\\begin{equation}\nI(X, Y)= \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y) \\cdot log_2\\left(\\frac{P(X = x, Y = y)}{P(X = x)P(Y = y)}\\right)\n\\end{equation}\n\nWhere the second to third equality, comes from the fact that:\n\\begin{equation}\nP(X = x) = \\sum_{y \\in \\mathcal{Y}} P(X = x|Y = y)P(Y=y) = \\sum_{y \\in \\mathcal{Y}} P(X = x,Y = y)\n\\end{equation}\nWhich is the application of the Law of Total Probability\n\n<p>\nThat way, we have for <b>Mutual Information<\/b> $I(X, Y)$:\n\\begin{equation}\n    I(X, Y)= \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} P(X = x, Y = y) \\cdot log_2\\left(\\frac{P(X = x, Y = y)}{P(X = x)P(Y = y)}\\right)\n\\end{equation}\n<\/p>","a7fe418d":"# Random Forests","9dfe1314":"It's possible to use the method 'plot_tree', which is also from the scikit-learn library, to visualize the tree.","f6cc3907":"# The problem with the best Decision Tree\nHaving defined how a Decision Tree works, it is easy to observe that modifying the <i>features<\/i> we choose for each of the nodes gives us different trees as a result.<br><br>\nFrom this observation, one can easily infer that there are many different Decision Trees for the same problem, in the example above, the Decision Tree is built using 7 attributes ('Outlook = Sunny', 'Outlook = Overcast', 'Outlook = Rain', 'Humidity = High', 'Humidity = Normal', 'Wind = Strong' e 'Wind = Weak') and we saw that the Truth Table for this tree would have $2^7$ lines.<br>\nNotice that given a determined number of attributes, let's say $n$, for a single Decision Tree $T$, the Truth Table of $T$ would have $2^n$ lines.<br>\n\nIntuitively if we were to get only the target column 'PlayTennis', we would have a vector with $2^n$ elements, as observed below:\n<table>\n  <tr>\n    <th>PlayTennis<\/th>\n  <\/tr>\n  <tr>\n    <td>False<\/td>\n  <\/tr>\n  <tr>\n    <td>True<\/td>\n  <\/tr>\n  <tr>\n    <td>True<\/td>\n  <\/tr>\n  <tr>\n    <td>False<\/td>\n  <\/tr>\n  <tr>\n    <td>True<\/td>\n  <\/tr>\n  <tr>\n    <td>...<\/td>\n  <\/tr>\n<\/table>  \nDifferent Decision Trees, that solve the same problem, use the same attributes for classification, the only thing that changes in those trees, is the classification itself. That is, for every different Decision Tree possible, we have the same Truth Table, except for the <i>target<\/i> column, in the example above the column 'PlayTennis'.<br>\n\nFrom that, the total quantity of Decision Trees for a determined problem, is the total combination of values that each of the elements of the <i>target<\/i> columns in the Truth Table can get assigned to. Since the vector generated by the <i>target<\/i> column is a vector of $2^n$ elements (being $n$ the number of attributes), we have that each of the elements in this vector can get assigned a value (True or False), therefore, the total number of combinations is $2^{2^n}$.<br>\nThus, in the example above, since we are using 7 attributes, we would have $2^{2^7} = 3.4 \\cdot 10^{38}$ possible Decision Trees that can be built (and we didn't even consider the Temperature <i>feature<\/i>).<br>\n\nTo determine which Decision Tree is the best Decision Tree, it would be necessary to iterate over all $3.4 \\cdot 10^{38}$ possible trees and check which one best classifies our <i>target<\/i> 'PlayTennis'. It is, definitely impracticable to iterate over so many trees, and because of that we need a kind of heuristic that will help us estimate a tree that is good enough.","9a5830a5":"# Decision Trees\nA <b>Decision Tree<\/b> is a structure of relation between categories. Each of the nodes of a Decision Tree represents a kind of \"test\" in a determined <i>feature<\/i>, the relations between each of the nodes (\"branches\" of the tree) represent the values that can be assigned to such <i>feature<\/i>.\n\n---\n\nFor a better understanding, we will use the following dataset as an example, it contains data about how weather was in a specific day and, whether or not people played tennis that day.<br>\nThe variable Outlook contains the values <b>Sunny<\/b>, <b>Overcast<\/b> and <b>Rain<\/b>.<br>\nThe variable Temperature contains the values <b>Hot<\/b>, <b>Mild<\/b> and <b>Cold<\/b>.<br>\nThe variable Humidity contains the values <b>High<\/b> e <b>Normal<\/b>.<br>\nThe variable Wind contains the values <b>Strong<\/b> and <b>Weak<\/b>.<br>\nThe variable PlayTennis contains the values <b>Yes<\/b> and <b>No<\/b>, indicating whether or not tennis was played that day.<br>","1f796d84":"As said previously, Decision Trees are built based off an heuristic of what is the best tree. Since such process comes from an heuristic, there is no guarantee that the generated tree is, in fact, the best tree, as a matter of fact, there are other heuristics for the construction of a Decision Tree, what was presented previously is just one of them. <br>\nFrom the idea that we can have many different Decision Trees that classify the <i>target<\/i>, comes the notion of <b>Random Forests<\/b>.<br>\n\n---\nThe idea behind Random Forests, relies on generating several different Decision Trees, that would classify the <i>target<\/i> differently, and in that way, make all of those Decision Trees vote in the best classification.","78e1a24b":"# Scikit-learn's Random Forest","6afcea3b":"The data in our example is categorical and the Decision Tree classifier from scikit-lean does not work with this kind of data. We need to treat data before building our classifier. For this we will use the function from pandas \"pd.get_dummies\" that converts ctegorical attributes into attributes with values 0 and 1.<br>\nFor example, the attribute Humidity that can receive the values High and Normal, is now represented by attributes Humidity_High e Humidity_Normal, that can get assigned the values 0 and 1.<br>\nThose new created variables are called dummy variables, a categorical variable with $n$ attributes, leads to $n$  dummy variables, one for each attribute.<br>","65d5b10d":"We will use the method 'RandomForestClassifier' from scikit-learn library on python, to classify our 'PlayTennis' dataset.","10d514af":"![decision_tree.png](attachment:a858caa0-431a-455f-ade3-e86371185fbc.png)","459fe221":"<p>Above we see an exemple of an Entropy graph, for each different value of the parameter p in Bernoulli's distribution. It's easy to realize that when $p = 0$ or $p = 1$, Entropy is equal to 0, which intuitively, makes total sense, since in those cases Bernoulli's distribution would only have 0 or 1 as a result, that is, there is no uncertainty over the distribution.<\/p>\n<p>Furthermore, we have that Entropy is maximized when $p = 0.5$, which is exactly when values 0 and 1 have equal chances of being the outcome, in other words, the uncertainty about which of the two values we will get as a result is maximum.<\/p>","d3ae2598":"And in the following cell, we create the class for the Decision Tree classifier from scikit-learn, besides using the method 'fit' to create the tree for the data we have.","678eaa24":"<p>In this notebook, We will introduce the concept behind methods of Machine Learning for classification, named Decision Tree.<\/p> \n\n<p>The notebook assumes the reader has a previous knowledge of the main ideas behind Random Variables and Expectation of Random Variables.<\/p>\n\n****\n\n# Entropy\n<p>Prior to understading what Decisions Trees are, it is necessary to be acquainted with the concept of <b>Entropy<\/b>.<\/p>\n<p><b>Entropy<\/b> is a measure for the uncertainty of a determined random variable. Let $X$ be a random variable, with state space $\\mathcal{S}$, and probability density function $P(X = x), x \\in \\mathcal{S}$, we have that the Entropy for $X$, namely $H(X)$, is: <\/p>\n\n\\begin{equation}\nH(X) = - \\sum_{x \\in \\mathcal{S}} P(X = x) log_2(P(X = x))\n\\end{equation}\n<p>For convenience, we assume that $0\\cdot log_2(0) = 0$, which is explained by the fact that $\\lim_{x \\to 0} xlog_2(x) = 0$<\/p>\n<p>Entropy's unit of measurement are bits. The unit of measurement depends entirely on the base we chose for the logarithm, in the case we use $log_{10}$ or $log_3$, Entropy would be measured in dits and trits, respectively.<\/p>\n<p><b>Note:<\/b> Notice that the definition of Entropy $H(X)$, is similar to the expectation of the function $g(X) = \\frac{1}{log_2(P(X = X))}$ of a determined random variable $X$.<\/p>\n\n\\begin{equation}\nH(X) = E\\left[\\frac{1}{log_2(P(X = X))}\\right]\n\\end{equation}"}}