{"cell_type":{"e52d1db3":"code","765859a0":"code","6a33d771":"code","97cae929":"code","6090a7ab":"code","b23bfabd":"code","b610a31e":"code","4b080271":"code","da15e93f":"code","986a492f":"code","34e0fc32":"code","082b43bf":"code","9c763152":"code","788297c1":"code","b65ee71b":"code","f4d819fa":"code","8746bb4d":"markdown","99892c69":"markdown","0cece6e6":"markdown","2b72f6df":"markdown","50ec0a4f":"markdown","2984bb49":"markdown","6f56608e":"markdown","7f3bcb66":"markdown","a4f963db":"markdown","9f44ac2b":"markdown","826ff440":"markdown","2852bcb1":"markdown","204d128f":"markdown","ac28366d":"markdown","41dfa8b1":"markdown","63fe8915":"markdown","39bd36ba":"markdown","16ea2299":"markdown","d0128876":"markdown","aa5d7142":"markdown","42548218":"markdown","e27578ec":"markdown","a6a4edbf":"markdown","9c89da14":"markdown","bf8c47ef":"markdown","b6caa8d1":"markdown"},"source":{"e52d1db3":"#Add library that need to kernel \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\naccuracy_list = [] # accuracy list","765859a0":"x_l =np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Sign-language-digits-dataset\/X.npy\") # image\ny_l = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Sign-language-digits-dataset\/Y.npy\") # label\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')\nplt.show()","6a33d771":"z = np.zeros(205)\no = np.ones(205)\nt = [2 for i in range(205)]\nth = [3 for i in range(207)]\nf = [4 for i in range(206)]\nf5 = [5 for i in range(208)]\ns = [6 for i in range(207)]\ns7 = [7 for i in range(206)]\ne = [8 for i in range(206)]\nn = [9 for i in range(207)]\nY = np.concatenate((z, o,t,th,f,f5,s,s7,e,n), axis=0).reshape(x_l.shape[0],1)\nprint(\"Y shape:\",Y.shape)","97cae929":"X_flatten = x_l.reshape(x_l.shape[0],x_l.shape[1]*x_l.shape[2])\nprint(\"X train flatten\",X_flatten.shape)","6090a7ab":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_flatten, Y, test_size=0.2, random_state=42)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"Y_train shape: \",Y_train.shape)\nprint(\"X_test shape: \",X_test.shape)\nprint(\"Y_test shape: \",Y_test.shape)","b23bfabd":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state=42,multi_class =\"multinomial\",solver=\"lbfgs\")\nlogreg.fit(X_train, Y_train)\ny_head = logreg.predict(X_test)\nac_log = logreg.score(X_test,Y_test)\nprint(\"Accuracy: \",ac_log)\naccuracy_list.append(ac_log)","b610a31e":"from sklearn import metrics \n\ncnf_matrix = metrics.confusion_matrix(Y_test,y_head.reshape(-1,1)) \nclass_names=[0,1,2,3,4,5,6,7,8,9]\n\nfig, ax = plt.subplots(figsize=(10,10))\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"coolwarm\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion Matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","4b080271":"X_flatten = x_l.reshape(x_l.shape[0],x_l.shape[1]*x_l.shape[2])\nprint(\"X train flatten\",X_flatten.shape)","da15e93f":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_flatten, y_l, test_size=0.2, random_state=42)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"Y_train shape: \",Y_train.shape)\nprint(\"X_test shape: \",X_test.shape)\nprint(\"Y_test shape: \",Y_test.shape)","986a492f":"# Evaluating the ANN\n\nfrom keras.models import Sequential #initialize neural network library\nfrom keras.layers import Dense #build our layers library\n\nclassifier = Sequential() # initialize neural network\nclassifier.add(Dense(units = 160, activation = 'relu', input_dim = X_train.shape[1]))\nclassifier.add(Dense(units = 90, activation = 'relu'))\nclassifier.add(Dense(units = 80, activation = 'relu'))\nclassifier.add(Dense(units = 60, activation = 'relu'))\nclassifier.add(Dense(units = 40, activation = 'relu'))\nclassifier.add(Dense(units = 20, activation = 'relu'))\nclassifier.add(Dense(units = 10, activation = 'sigmoid')) #output layer\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel = classifier.fit(X_train,Y_train,epochs=400)\ny_head = classifier.predict(X_test)\nmean = np.mean(model.history['accuracy'])\naccuracy_list.append(mean)\nprint(\"Accuracy mean: \"+ str(mean))\n","34e0fc32":"from sklearn.metrics import confusion_matrix\n\ncnf_matrix= confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(y_head, axis=1))\nclass_names=[0,1,2,3,4,5,6,7,8,9]\n\nfig, ax = plt.subplots(figsize=(10,10))\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"coolwarm\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion Matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()\n","082b43bf":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x_l.reshape(-1,64,64,1), y_l, test_size=0.2, random_state=42)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"Y_train shape: \",Y_train.shape)\nprint(\"X_test shape: \",X_test.shape)\nprint(\"Y_test shape: \",Y_test.shape)","9c763152":"# Evaluating the CNN\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\nmodel = Sequential() # initialize neural network\n#layer 1\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (64,64,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n#layer 2\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n# fully connected\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\n# Compile the model\nmodel.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, Y_train,epochs=200)","788297c1":"y_head = model.predict(X_test)\nmean = np.mean(history.history['accuracy'])\naccuracy_list.append(mean)\nprint(\"Accuracy mean: \"+ str(mean))","b65ee71b":"from sklearn.metrics import confusion_matrix\n\ncnf_matrix= confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(y_head, axis=1))\nclass_names=[0,1,2,3,4,5,6,7,8,9]\n\nfig, ax = plt.subplots(figsize=(10,10))\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"coolwarm\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion Matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","f4d819fa":"#Accuracies\ncolumn = [\"Logistic Regression\",\"Artifical Neural Network\",\"Convolutional Neural Network\"]\nf,ax = plt.subplots(figsize = (15,7))\nsns.barplot(x=accuracy_list,y=column,palette = sns.cubehelix_palette(len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithms\")\nplt.title('Accuracy Values')\nplt.show()","8746bb4d":"<a id=\"3.4\"><\/a>\n### Model Evaluation using Confusion Matrix","99892c69":"<a id=\"6\"><\/a>\n### References\n\n* https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n* https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial\n* https:\/\/towardsdatascience.com\/real-world-implementation-of-logistic-regression-5136cefb8125","0cece6e6":"While the CNN model sets up, We decide to requirement of model. There is any rule about this. While our model sets up, We attent our data. \n\nI set up my model like following:\n\nconv -> max pool -> dropout -> (layer1) <br\/>\nconv -> max pool -> dropout -> (layer2) <br\/>\nfully connected \n\n* kernel_size - filter: feature detector\n* padding: same or valid \n* Conv2D: conv\n* MaxPool2D: Pooling\n* Dropout: Dropout (float between 0 and 1. Fraction of the input units to drop.)\n* Flatten: Flattening\n* strides: Stride","2b72f6df":"<a id=\"4.2\"><\/a>\n### Train and Test Split","50ec0a4f":"Confusion matrix displays number of true and false prediction. Diagonal displays true predictions. We can see Convolutional Neural Network algorithm is most successful algorithms. ","2984bb49":"While the ANN model sets up, We decide to number of layer and node ourselves. There isn't a any rule that determines number of layer and node. We decide by trying this.\n\n\n* Dense: the method is that determine hidden layer.\n* units: number of node.\n* activation: activation function.\n* input_dim: input dimension that is number of pixels in our images (4096 px)\n* optimizer: get as parameter of learning rate algorithms, we use adam optimizer\n* loss: cost function is the same. (categorical_crossentropy for multiclass)\n* metrics: it is accuracy.\n* epoch: number of iteration\n* validation_data: use cross validation\n* history: it is a dictionary that keep val_loss, val_accuracy, loss and accuracy values.\n\n### **What is the ADAM Algorithm?** \n\nADAM is Adaptive Moment Algorithm. It optimizes learning rate for neural network. ADAM is faster than other algorithms. Even if hyper parameters are tuned little. It displays good performance.","6f56608e":"<a id=\"2.2\"><\/a>\n### Train and Test Split","7f3bcb66":"<a id=\"3.2\"><\/a>\n### Train and Test Split","a4f963db":"<a id=\"2.4\"><\/a>\n### Model Evaluation using Confusion Matrix","9f44ac2b":"<a id=\"3\"><\/a>\n## Artificial Neural Networks\n\nArtifical Neural Network algorithm is the algorithm that had set on logic working of our brain cell. Between inputs and outpust are found a lot of hidden layer. You can think the hidden layers like brain cells. Synapses that connect two brain cells, are applied logistic regression. \n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*eBMwpBBboAXgqsawwOKkPw.png\" height=\"500\" width=\"500\"\/>\n\n* ANN is occured input, output and hidden layer.\n* Hidden layer don't see inputs.\n* Without the last logistic regression(in the last synaps), it is used tan(h) function or relu function as activation function.\n* As the number of hidden layers increases, complex learning increases.\n* ANN is applied n times of logistic regression algorithm.\n\n**What is the tan(h) Function?**\n\ntan(h) function is the function converting between -1 and 1 real value. It's more successful than sigmoid function for hidden layer. Because mean of tan(h) is closer than 0. This means that it keeps productivity more central.\n\n**What is the ReLU Function?**\n\nReLU function is the function that makes if input is small from 0, output makes equal 0 or if input is big from 0, output makes equal input. It's faster than sigmoid and tan(h) function. It can use instead of tan(h). \n","826ff440":"<a id=\"5\"><\/a>\n### Conclusion\n\n* Logistic Regression isn't more successful than ANN and CNN algorithms.\n* Logistic Regression isn't successful in multilabel problems.\n* In the same epoch, CNN is more successful than ANN. (I changed epoch value to create better model)\n* We can improve models different ways. \n* If epoch value increases, accuracy increases.","2852bcb1":"x_l categorical feature is the 3 dimentional array. We need to convert 2 dimentional array in order to use in artificial neural network algorithm.","204d128f":"x_l categorical feature is the 3 dimentional array. We need to convert 2 dimentional array in order to use in logistic regression algorithm.","ac28366d":"<a id=\"4.3\"><\/a>\n### Create Model\n\n","41dfa8b1":"<a id=\"2\"><\/a>\n## Logistic Regression\n\nLogistic regression is a special case of Lineer Regression. Its outcome is binary value. It predicts the probability of occurrence of variables using a logit function. \n\nLinear Regression:\n\n<img src=\"https:\/\/miro.medium.com\/max\/308\/0*c28OC-B--sPBd0wG.png\">\n\nSigmoid Function: \n\n<img src=\"https:\/\/miro.medium.com\/max\/121\/0*APSAu8-12IhJPIhF.png\">\n\nApply Sigmoid function on linear regression:\n\n<img src=\"https:\/\/miro.medium.com\/max\/263\/0*1-TM_M93_cM6xIe1.png\">\n\n\n**What is the Sigmoid Function?**\n\nSigmoid function is the function converting between 0 and 1 real value. It provide to predict as binary value. If outcome is more than 0.5 it predict 1 and If outcome is less than 0.5 it predict 0. \n\n**So, How to set weight values(\u00df1,\u00df2,... etc.) **\n\nAs first step, weights are assign randomly. Cost is calculated. Our goal is to decrease cost. Therefore we change weights using Gradient Descent Algorithm and we provide to reach the best cost. (The best cost is the lowest cost.)","63fe8915":"x_l is image pixel and y_l is label that image belong (0,1,2.. etc) we need to look data to set up true model. I'm visualizing using imshow method in the matplotlib library. I choosed two images. They have images of from 0 to 9 in our data.","39bd36ba":"<a id=\"1\"><\/a>\n## What is Deep Learning?\n\n\nDeep learning is technique that predict feature from the data. It is machine learning technique. It usually uses a huge amount of datas. Because machine learning techniques don't predict true in a huge amount of datas. Deep learning algorithms use in speech recognition, image classification, natural language procession and recommendation system problems.\n\n\n<img src=\"http:\/\/preview.ibb.co\/d3CEOH\/1.png\"\/>\n\n\n\n#### Deep Learning Vs. Machine Learning\n\n* Machine learning covers deep learning.\n* Deep learning predicts true in a big amount of data.\n* Features are given manually in machine learning however deep learning learns features from data itself. \n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/freeze\/max\/1000\/1*bB3FZmvTG_FhINMylXtAWQ.png?q=20\"\/>\n\n","16ea2299":"<a id=\"2.1\"><\/a>\n### Data Preparing ","d0128876":"<a id=\"3.3\"><\/a>\n### Create Model","aa5d7142":"Confusion matrix displays number of true and false prediction. Diagonal displays true predictions. For example, \"5\" image confuse \"4\" image. It has 4 faulty outcome.   ","42548218":"<a id=\"3.1\"><\/a>\n### Data Preparing ","e27578ec":"<a id=\"2.3\"><\/a>\n### Create Model","a6a4edbf":"# Deep Learning Tutorial on Sign Language Digits\n\nHi everbody! In this kernel, I'm going to evaluate image of Sign Language using Deep Learning Algorithms. This kernel will be beginner level in deep learning. I'm learning newly too. I hope it benefit for you. I'm going to use Keras library for Deep Learning algorithm. After I learn Pytorch library, I will write a kernel with Pytorch library. If my kernel benefit for you, please upvote it. If you have a questions, I pleasure to answer them. Let's start, Have fun :)\n\n\n## Content \n\n#### [**1.What is Deep Learning?**](#1)<br\/>\n#### [**2.Logistic Regression**](#2) <br\/>\n* [2.1.Data Preparing](#2.1) <br\/>\n* [2.2.Train and Test Split](#2.2) <br\/>\n* [2.3.Create Model](#2.3) <br\/>\n* [2.4.Model Evaluation using Confusion Matrix](#2.4) <br\/>\n\n#### [**3.Artificial Neural Networks**](#3) <br\/>\n* [3.1.Data Preparing](#3.1) <br\/>\n* [3.2.Train and Test Split](#3.2) <br\/>\n* [3.3.Create Model](#3.3) <br\/>\n* [3.4.Model Evaluation using Confusion Matrix](#3.4) <br\/>\n\n#### [**4.Convolutional Neural Networks**](#4) <br\/>\n* [4.1.Train and Test Split](#4.2) <br\/>\n* [4.2.Create Model](#4.3) <br\/>\n* [4.3.Model Evaluation using Confusion Matrix](#4.4) <br\/>\n\n#### [**5.Conclusion**](#5) \n#### [**6.References**](#6) ","9c89da14":"<a id=\"4\"><\/a>\n## Convolutional Neural Networks\n\nThe Convolutional Neural Network is the deep learning algortihm that can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. The pre-processing required in a CNN is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNN have the ability to learn these filters\/characteristics.\n\n<img src=\"https:\/\/preview.ibb.co\/nRkBpp\/gec2.jpg\"\/>\n\n* Feature Detection: It detects edges of image or complex figures.\n* Feature Map: It convolves feature detection with input matrix(image). All elements in the two matrix multipy and collect. It finds the new number. Feature map provide faster working by reducing image dimension. But it causes loss of information this translation.\n* Stride: It is the operation that is navigated step by step on input matrix.\n* Padding: Padding uses to estop loss of information while feature map creates. There are two padding method:\n    * Valid Padding: it means no padding and it assumes that all the dimensions are valid so that the input image gets fully covered by a filter and the stride specified by you.\n    * Some Padding: It is add 0 in the form of frame environment of input image. So it isn't loss information and it is protected image.\n* Max Pooling: It guarantees fast working by reducing number of parameter. It makes the detection of features invariant to scale or orientation changes. It blocks overfitting. There is no operation that overlap like stride. \n* Flattening: It is the process that feature maps translate to one dimension by holding corner of them. It creates input for ANN.\n* Fully Connected: It is the process that all inputs connect to all hidden layer. It creates ANN.\n* Dropout: It is the event that random neurals are closed throughout training model. It enstops overfitting.\n","bf8c47ef":"<a id=\"4.4\"><\/a>\n### Model Evaluation using Confusion Matrix","b6caa8d1":"Confusion matrix displays number of true and false prediction. Diagonal displays true predictions. We can see Artifical Neural Network algorithm more successful than Logistic Regression algorithm. Let see for \"3\" image. While the logistic regression has 4 fault outcome, the ann has no fault outcome. (6 instead of 3)"}}