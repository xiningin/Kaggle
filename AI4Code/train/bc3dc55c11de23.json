{"cell_type":{"0fbb9514":"code","cda9d831":"code","b1237e9e":"code","c024bf15":"code","8f862945":"code","595f1bd5":"code","d0ddb8a2":"code","ceb27d98":"code","5b8b8429":"code","059dbe19":"code","cd215ab6":"code","be1c9861":"code","c065ce95":"code","5ba9ebb2":"code","488cc40a":"markdown","08678bfa":"markdown","3fb31908":"markdown","05016220":"markdown","14dc90ba":"markdown","18dd5e1d":"markdown","bfa9acf5":"markdown","2ac3130b":"markdown","df4f498c":"markdown","d85979c0":"markdown","211561ec":"markdown","dc58f1b0":"markdown"},"source":{"0fbb9514":"import numpy as np\nimport pandas as pd\nimport gc  # Garbage Collector\nnumber_of_rows = 2000000 # We are loading 2 million rows and skip the others.\n\ndef unique_columns(dataframe): # A function to drop all columns where all rows are the same.\n    list_name = []\n    temp = list(dataframe.nunique().values)\n    for i in range(len(temp)):\n        if temp[i] == dataframe.shape[0] or temp[i] == 1:\n            list_name.append(dataframe.columns[i])\n    print('Columns that were dropped:',list_name)\n    return list_name\n\ndef get_top_features(fscore,df,model): # A function to get the features of a trained model with\n    temp=[]                            # fscore greater than the fscore parameter.\n    for i in range(len(model.feature_importances_)):\n        if model.feature_importances_[i]>=fscore:\n            temp.append(df.columns[i])\n    print('Features with fscore greater than ',fscore,':',temp)\n    return temp\n\ntrain = pd.read_csv( '..\/input\/train.csv' , skiprows =  lambda x: x > number_of_rows)\ntrain.tail() ","cda9d831":"train.drop(unique_columns(train),axis = 1,inplace = True)","b1237e9e":"train.select_dtypes(include = 'object').head()","c024bf15":"train.OsVer = train.OsVer.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.AppVersion = train.AppVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.EngineVersion = train.EngineVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.AvSigVersion = train.AvSigVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)","8f862945":"train.drop(['Census_OSEdition'],axis = 1, inplace = True)","595f1bd5":"for column in train.select_dtypes(include = 'object').columns:\n    print(column)\n    train[column]=train[column].astype('category')\n    train[column]=train[column].cat.codes","d0ddb8a2":"import seaborn as sns\ncorrelation_matrix = train.corr()\nmask = np.zeros_like(correlation_matrix)\nmask[np.triu_indices_from(mask)] = True \nsns.heatmap(correlation_matrix, mask = mask)","ceb27d98":"correlation_matrix","5b8b8429":"y_train = train['HasDetections']\nX_train = train.drop(['HasDetections'],axis=1)\ndel(train) # save some space\ngc.collect()","059dbe19":"import xgboost as xgb\nfrom xgboost import plot_importance\nxgbo = xgb.XGBClassifier()\nxgbo.fit(X_train,y_train)\nplot_importance(xgbo)","cd215ab6":"from matplotlib import pyplot\npyplot.bar(range(len(xgbo.feature_importances_)), xgbo.feature_importances_)\npyplot.show()\ncolumns_to_be_used = get_top_features(fscore = 0.04 , df = X_train, model = xgbo)","be1c9861":"del(X_train)\ndel(y_train)\ngc.collect()","c065ce95":"train = pd.read_csv( '..\/input\/train.csv' , usecols = columns_to_be_used)\n","5ba9ebb2":"train.shape","488cc40a":"We are only interested in the last row of the correlation matrix.","08678bfa":"The 'MachineIdentifier' feature is a unique ID for every machine so it's not going to add much information so we can drop this column.We can also drop all columns where all rows are the same.","3fb31908":"Census_OSEdition feature and ","05016220":"![](https:\/\/imgur.com\/J2wx5Pg)\n","14dc90ba":"Now we can load all rows and only the columns of the features with and fscore of 0.04 or greater.","18dd5e1d":"Columns AppVersion,EngineVersion, AvSigVersion and OsVersion can be turned into an meaningfull number if we take out the dot.","bfa9acf5":"We are training a simple XGBoost model to get the feature importances.","2ac3130b":"A more clean visualization.","df4f498c":"> Features Census_OSEdition and Census_OSSkuName contain the same information throughout the dataset so we can drop one and keep the other.","d85979c0":"One of the biggest problems for those who want to participate in Microsoft's competition is going to be the size of this massive dataset.With a train set of almost 9 million rows and 83 columns the first thing every Kaggler will notice is that we can't load the data in a single dataframe.While there are methods to get past this ( like using the chunksize option ) i thought it would be usefull to  load a part of the data and then decide which features we are going to load when we are going to train a model.","211561ec":"There are many categorical features in our dataset like 'ProductName' and 'EngineVersion'. We will have to use a Label Encoder hor those in order to be able to train a model.Before doing that let's see all the features that are of type 'object' (which means that are parsed as strings).","dc58f1b0":"We are going to turn the rest feature columns into categorical columns and let pandas encode them ( Label Encoder was slower)."}}