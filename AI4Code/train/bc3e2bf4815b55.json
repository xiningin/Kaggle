{"cell_type":{"8886e383":"code","2974c0d4":"code","7b102944":"code","3c82459c":"code","4c512bba":"code","4dd9d222":"code","b3c8452b":"code","93623fd0":"code","a2165315":"code","ad38b679":"code","35a155ce":"code","1694b24f":"code","fab7e4db":"code","7113c3fe":"code","8dcee8a7":"code","9702e375":"code","74682707":"code","63ce92e5":"code","a9772235":"code","ec271146":"code","0d8c8f49":"code","ea10c91b":"code","08c915d7":"code","b909c6e9":"code","6d81deaf":"code","4dd98cc8":"code","401d323f":"code","f0ffe231":"code","92440912":"code","25f9b4de":"code","5ba7e5fc":"code","e476099b":"code","415b396d":"code","b4092156":"code","71a2f730":"code","85c9ab81":"code","011d95ba":"code","14e4f46d":"code","9d62cd8b":"code","be746d4c":"code","7bf8af3e":"code","b7dc9cdc":"code","18cc0b66":"code","94591120":"code","7e959804":"code","b06ee9af":"code","81328a19":"code","a22f3939":"code","edf7da30":"markdown","f7bd4dc1":"markdown","87aa6737":"markdown","7324aef9":"markdown","03c0baf3":"markdown","23aebd36":"markdown","0d6c6668":"markdown","af66f990":"markdown","fd885c09":"markdown","90489637":"markdown","17660c49":"markdown","ce37575d":"markdown","418265ba":"markdown","b947889d":"markdown","1e658a08":"markdown","b1400e25":"markdown","84b9cd50":"markdown","58c2ca78":"markdown","72a1c869":"markdown","b402c871":"markdown","c04892e0":"markdown","b02b5313":"markdown","4d16adb9":"markdown","c3509af6":"markdown","fff172eb":"markdown","2a935784":"markdown","9784ba02":"markdown","9991550a":"markdown","6b59cc47":"markdown","1a9ab5b0":"markdown"},"source":{"8886e383":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split","2974c0d4":"#file=\"\/content\/gdrive\/My Drive\/Colab Notebooks\/train.csv\"\ntrain=pd.read_csv(\"..\/input\/ieee-pes-bdc-datathon-year-2020\/train.csv\")\ntest=pd.read_csv(\"..\/input\/ieee-pes-bdc-datathon-year-2020\/test.csv\")","7b102944":"train.drop([\"ID\"], axis=1, inplace=True)\n","3c82459c":"train.shape","4c512bba":"train.describe()","4dd9d222":"train.hist(bins=50,figsize=(20,15))","b3c8452b":"train.info()","93623fd0":"f, ax = plt.subplots(figsize=(15,8))\nsns.distplot(train['global_horizontal_irradiance']) #for checking distribution of GHI\nplt.xlim([-10,1602])","a2165315":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn')","ad38b679":"corr_matrix=train.corr()\nprint(corr_matrix['global_horizontal_irradiance'].sort_values(ascending=False))","35a155ce":"train_small=train[:200].copy()\nplt.style.use('ggplot')\ny=train_small['global_horizontal_irradiance']\nx=range(200)\n\nplt.plot(x,y,color='k')\nplt.xlabel(\"Index\")\nplt.ylabel(\"GHI\")\n#Give graph a title\nplt.title('GHI visualization')\nplt.show()","1694b24f":"train_label=train[\"global_horizontal_irradiance\"].copy()\ntrain.drop(['global_horizontal_irradiance'], axis=1, inplace=True)\nprint(train.shape,train_label.shape)","fab7e4db":"def processing(train_data):\n    from sklearn.preprocessing import StandardScaler\n    scaled_features = StandardScaler().fit_transform(train_data.values)\n    scaled_data=pd.DataFrame(scaled_features,index=train_data.index,columns=train_data.columns)\n\n    return scaled_data\n\nX_train_lstm=processing(train)","7113c3fe":"data_len = len(X_train_lstm)\npct = 0.98 \ntrain_len = int(pct*data_len)\ntrain_data_lstm = X_train_lstm[:train_len]\ntrain_label_lstm = train_label[:train_len].values\n\nX_valid_lstm = X_train_lstm[train_len:]\ny_valid_lstm = train_label[train_len:].values\n\ntrain_data_lstm = train_data_lstm.values.reshape((train_data_lstm.shape[0],1,train_data_lstm.shape[1])) #for the lstm shape (None,time step,features)\nX_valid_lstm = X_valid_lstm.values.reshape((X_valid_lstm.shape[0], 1, X_valid_lstm.shape[1]))\nprint(train_data_lstm.shape, train_label_lstm.shape, X_valid_lstm.shape,y_valid_lstm.shape) ","8dcee8a7":"train_data=processing(train)","9702e375":"train_data, X_valid, train_label, y_valid = train_test_split(train_data, train_label, train_size=0.95,test_size = 0.05, random_state = 42)\nprint(train_data.shape,train_label.shape,X_valid.shape,y_valid.shape)","74682707":"from tensorflow import keras\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.layers import AlphaDropout\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras import models\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Flatten","63ce92e5":"#reg = keras.regularizers.l1_l2(l1=0.01, l2=0.1) \nreg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \ndropout=0.2\nsnn_model = keras.models.Sequential([\n        \n        keras.layers.Input(shape=train_data.shape[1]),\n        keras.layers.Dense(600, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.2),\n    \n        keras.layers.Dense(600, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.2),\n    \n        keras.layers.Dense(300, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n     \n        keras.layers.Dense(300, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n    \n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n    \n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(100, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        keras.layers.Dense(1)\n])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-09\n)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])\n","a9772235":"reg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \ndropout=0.2\ninitializer = tf.keras.initializers.he_normal()\nfnn_model = keras.models.Sequential([\n        \n        keras.layers.Input(shape=train_data.shape[1]),\n        keras.layers.Dense(500, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(dropout),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(500, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(dropout),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(300, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(300, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.1),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(100, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        keras.layers.Dense(1)\n])\n# def exponential_decay(lr0, s):\n#     def exponential_decay_fn(epoch):\n#         return lr0 * 0.1**(epoch \/ s)\n#     return exponential_decay_fn\n# exponential_decay_fn = exponential_decay(lr0=0.01, s=100)\n#lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-06\n)\n#lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])","ec271146":"reg = keras.regularizers.l1_l2(l1=0.0001, l2=0.01) \n\nlstm_model = keras.models.Sequential([\n    keras.layers.Input(shape=(train_data_lstm.shape[1], train_data_lstm.shape[2])),\n    keras.layers.LSTM(500,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.Dropout(0.2),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.LSTM(300,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.1),\n    \n    keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.1),\n    \n    keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.LSTM(100,activation=\"selu\",kernel_initializer=\"lecun_normal\",kernel_regularizer=reg),\n    keras.layers.Dense(1)\n])\n\n\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-07)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])","0d8c8f49":"snn_model.summary()\nfnn_model.summary()\nlstm_model.summary()","ea10c91b":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"snn_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0) #For stucked at plateu problem\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=3, min_lr=0)","08c915d7":"history1 = snn_model.fit(train_data, train_label, epochs=130, batch_size=150, validation_data=(X_valid, y_valid), verbose=1) \nhistory2 = snn_model.fit(train_data, train_label, epochs=60, batch_size=512, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = snn_model.fit(train_data, train_label, epochs=40, batch_size=700, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr2]) ","b909c6e9":"pd.DataFrame(history1.history).plot(figsize=(8, 5)) #for checking learning_curve\nplt.grid(True)\nplt.gca() \nplt.show()","6d81deaf":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"fnn_he_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=3, min_lr=0)","4dd98cc8":"history1 = fnn_model.fit(train_data, train_label, epochs=130, batch_size=256, validation_data=(X_valid, y_valid), verbose=1) \nhistory2 = fnn_model.fit(train_data, train_label, epochs=60, batch_size=512, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = fnn_model.fit(train_data, train_label, epochs=40, batch_size=1024, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr2])","401d323f":"pd.DataFrame(history1.history).plot(figsize=(8, 5)) #for checking learning_curve\nplt.grid(True)\nplt.gca() \nplt.show()","f0ffe231":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"lstm_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=4, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=3, min_lr=0)","92440912":"history1 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=130, batch_size=256, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[reduce_lr1]) \nhistory2 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=60, batch_size=512, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=40, batch_size=1024, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[checkpoint_cb,reduce_lr2])","25f9b4de":"from sklearn.metrics import mean_squared_error\n\npred_snn = snn_model.predict(X_valid)\nbasic_mse=mean_squared_error(y_valid,pred_snn)\nbasic_mse_snn=np.sqrt(basic_mse)\nprint(basic_mse_snn) #117.67","5ba7e5fc":"pred_fnn = fnn_model.predict(X_valid)\nbasic_mse_fnn=np.sqrt(mean_squared_error(y_valid,pred_fnn))\nprint(basic_mse_fnn) #120.19","e476099b":"pred_lstm = lstm_model.predict(X_valid_lstm)\nbasic_mse_lstm=np.sqrt(mean_squared_error(y_valid_lstm,pred_lstm))\nprint(basic_mse_lstm) #119.01","415b396d":"all_models=list()\n\nmodel=keras.models.load_model(\"snn_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\nmodel=keras.models.load_model(\"lstm_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\nmodel=keras.models.load_model(\"fnn_he_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\n","b4092156":"def stacked_dataset(members, inputX):\n    \n    stackX=None\n    yhat=members[0].predict(inputX, verbose=0)\n    stackX=yhat\n\n    input_X_lstm=inputX.values.reshape((inputX.shape[0], 1, inputX.shape[1]))\n    yhat=members[1].predict(input_X_lstm, verbose=0)\n    stackX = np.concatenate((stackX,yhat),axis=1)\n\n    yhat=members[2].predict(inputX, verbose=0)\n    stackX = np.concatenate((stackX,yhat),axis=1)\n\n    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]))\n    return stackX    ","71a2f730":"def create_model(stackedX):\n    \n    \n    reg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \n    dropout=0.2\n    #initializer = tf.keras.initializers.he_normal()\n    model = keras.models.Sequential([\n                  \n      keras.layers.Input(shape=(stackedX.shape[1], stackedX.shape[2])),\n      keras.layers.LSTM(500,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.Dropout(0.2),\n      keras.layers.BatchNormalization(),\n    \n      keras.layers.LSTM(300,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n      keras.layers.Dropout(0.1),\n    \n      keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n      keras.layers.Dropout(0.1),\n    \n      keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n    \n      keras.layers.LSTM(100,activation=\"selu\",kernel_initializer=\"lecun_normal\",kernel_regularizer=reg),\n      keras.layers.Dense(1)\n\n             ])\n \n    early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n    optimizer=keras.optimizers.Nadam(\n        learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-06\n    )\n    model.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])\n    return model","85c9ab81":"def dividing_stacked(stackedX,inputy):\n    data_len = len(inputy)\n    pct = 0.90 # change it to 0.8~0.9\n    test_len = int(pct*data_len)\n    X_train = stackedX[:test_len,:]\n    y_train = inputy[:test_len]\n\n    X_valid = stackedX[test_len:,:]\n    y_valid = inputy[test_len:]\n\n    X_train=X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n    X_valid =X_valid .reshape((X_valid .shape[0], 1, X_valid .shape[1]))\n\n    print(stackedX.shape,inputy.shape,X_train.shape,y_train.shape,X_valid.shape,y_valid.shape)\n    return X_train,y_train,X_valid,y_valid","011d95ba":"def processing_stack(train_data):\n    \n    scaled_features = StandardScaler().fit_transform(train_data)\n    return scaled_features\n","14e4f46d":"def fit_stacked_model(members, inputX, inputy):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    \n    stackedX = processing_stack(stackedX)\n    #make a slice for validation info\n    stackedX,inputy,stackedX_slice,inputy_slice = dividing_stacked(stackedX,inputy)\n \n    nn_model = create_model(stackedX)\n\n    history1=nn_model.fit(stackedX, inputy, epochs=130, validation_data=(stackedX_slice, inputy_slice),batch_size=256,verbose=1,callbacks=[reduce_lr1])\n    history2=nn_model.fit(stackedX, inputy, epochs=50, validation_data=(stackedX_slice, inputy_slice),batch_size=512,verbose=1,callbacks=[checkpoint_cb,reduce_lr2])\n    history3=nn_model.fit(stackedX, inputy, epochs=40, validation_data=(stackedX_slice, inputy_slice),batch_size=800,verbose=1,callbacks=[checkpoint_cb,reduce_lr3])\n    \n    return nn_model,history1","9d62cd8b":"def stacked_prediction(members, model, inputX):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    #processing\n    stackedX = processing_stack(stackedX)\n    # make a prediction\n    yhat = model.predict(stackedX)\n    return yhat","be746d4c":"model,history = fit_stacked_model(all_models, train_data, train_label)\n# evaluate model on test set\nyhat = stacked_prediction(all_models, model, X_valid)","7bf8af3e":"stacked_mse=mean_squared_error(y_valid,yhat)  # mse on validation set\nstacked_mse=np.sqrt(stacked_mse)\nprint(stacked_mse)  #115.42","b7dc9cdc":"test=pd.read_csv(\"test.csv\")\ntest_ID = test['ID'].values.reshape(len(test))\ntest.drop(['ID'], axis=1, inplace=True)","18cc0b66":"X_test=processing(test)\nprint(X_test.columns,X_test.shape)","94591120":"X_test_lstm = X_test.values.reshape((X_test.shape[0],1,X_test.shape[1])) # preparing for lstm","7e959804":"test_pred_stacked = stacked_prediction(all_models, model, X_test) #prediction of stacked model (validation set rmse:)\n\ntest_pred_lstm = lstm_model.predict(X_test_lstm) #prediction of previously trained lstm\n\ntest_pred_snn = snn_model.predict(X_test) #prediction of previously trained snn","b06ee9af":"coefs = [0.60, 0.25, 0.15]\n\ntest_pred = test_pred_stacked * coefs[0] + test_pred_lstm * coefs[1] + test_pred_snn * coefs[2]\n","81328a19":"preds = [0 if p<0 else p for p in test_pred]","a22f3939":"zippedList =  list(zip(test_ID, preds))\nsubmission = pd.DataFrame(zippedList, columns = ['ID','global_horizontal_irradiance'])\nsubmission=submission.explode('global_horizontal_irradiance')\nsubmission.to_csv('submission_level5', index=False)  #PUBLIC leaderboard score: 112.67","edf7da30":"For SNN and FNN","f7bd4dc1":"*LSTM Model","87aa6737":"# **Visualization**","7324aef9":"***WEIGHTED AVERAGE On 3 PREDICTION***\n**from previous:\n*    stacked model rmse on validation set: 115.42\n*    lstm model rmse on validation set: 119.01\n*    snn model rmse on validation set: 117.67\n","03c0baf3":"**SNN","23aebd36":"*SNN model\nACTIVATION=\"SELU\",INIT=\"LECUN_NORMAL\"*","0d6c6668":"No null value :) We hate null value >_<","af66f990":"We will take the outpput from three model. The stacked model, SNN model and LSTM model, And apply weighted avg on them.","fd885c09":"**LSTM","90489637":"Lets check the learning_curve","17660c49":"***RMSE checking on validation set","ce37575d":"Validation error : 115.42","418265ba":"Train error : 104.21, validation error: 117.67","b947889d":"# TEST set prediction","1e658a08":"Most of the GHI value is distributed in zero.Lets see correlation between features for more insight.","b1400e25":"# Overall description:\n\n* *Visualization :* Observing dataset type,shape, data distribution type, correlation, GHI value over time.\n* *Preprocessing :* I used stadanrdscaling for removing the mean and scaling the data to unit variance. i split the dataset manually for lstm_input. For other model, I used sklearn train_test_split.\n* *Model training*: I trained three different NN model.SNN,FNN and LSTM. All hyperparameters were tuned by RANDPOMIZEDSEARCHCV. I haven't included that hyperparameter tunning part in this notebook. \n* *checking-error*: RMSE score\n* *Ensembling :* Stacking_ensemble on pretrained model.\n* *Weighted avg on prediction*: Gave weight to stacked_model,snn_model,lstm_model submission w.r.t to their accuracy on validation set.\n\n* ***My personal insight on increasing accuracy:*** \n> I should have used a fixed validation set for checking unbiased validation error of those model and regularize stacked model based on that unbiased error, instead i trained 3 model on different notebook,based on different train_validation set.I load those pretrained model,in stacked model and got biased result.And when i find this mistake, the comp was nearly at end,had no time for fixing that.This mistake resulted slightly overfit on test set. Public score:112.67(3rd) ,private score: 113.65(4th)\n\n","84b9cd50":"From above visualization and training on various model, I decided to train a \n1.SNN model(Selu+lecun_normal),as there is a great chance for exploding gradient\n 2.A FNN model(He+he_normal and \n 3.LSTM model for time series.\n\n**For LSTM, I didnt shuffle the dataset after preprocessing.","58c2ca78":"*FNN MODEL\nACTIVATION=\"ELU\",INIT=\"HE_NORMAL\" WITH BATCHNORMALIZATION*","72a1c869":"Normal distribution in air_temp,pressure.Poisson distribution in wind_speed. We will apply transformation to this later.","b402c871":"**I used \n1.log transformation\n2.Square root transformation\n3.Boxcox transformation\n4.Standardscaling on train_data. Among them, i find standardscaling does better then all of them.","c04892e0":"# ENSEMBLING MODEL","b02b5313":"Train error : 105.36, validation error: 119.01","4d16adb9":"Train error : 107.88, validation error: 120.19","c3509af6":"For lstm,(bcz we shouldnt feed shuffled data to lstm)","fff172eb":"This is a time series data, We can use RNN lstm on this data.","2a935784":"# Preprocessing & splitting the dataset","9784ba02":"***Model Training***","9991550a":"We can see ,wind_direction and precipitation is less correlated with GHI. But we cant drop those column bcz it has impact on solar prediction, And dropping columns hampers model accuracy also.","6b59cc47":"# ***MODEL TRAINING***\n***ALL HYPARPARAMETER WERE TUNED BY RANDOMIZEDSEARCHCV***","1a9ab5b0":"**FNN"}}