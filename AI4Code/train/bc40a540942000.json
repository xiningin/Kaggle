{"cell_type":{"ff04a5ab":"code","9edd00a5":"code","b9b44788":"code","f394ce39":"code","1eb42646":"code","967c5a13":"code","e2f4b450":"code","71a08a42":"code","ac8d5ae9":"code","3c138618":"code","c38f6267":"code","90a8d43c":"code","14d57927":"code","3f953fc5":"code","91bf0926":"code","c029e00a":"code","b5dcca06":"code","8759da2d":"code","77616224":"code","fd18aa60":"code","d85ccb03":"code","c3bf1515":"code","d70e3373":"code","f84143bb":"markdown","ef021b71":"markdown","5f640cda":"markdown","d844c8de":"markdown","e295c663":"markdown","201bba31":"markdown","dd7cd4cf":"markdown","0aa6f313":"markdown","a6b6761e":"markdown","2028a06e":"markdown","eb809e80":"markdown","ce0e1c69":"markdown","6c4839f6":"markdown","72e49f8b":"markdown"},"source":{"ff04a5ab":"import string, math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","9edd00a5":"print('Last update on', pd.to_datetime('now'))","b9b44788":"def load_sanitize(url):\n    df = pd.read_csv(url)\n    df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1}).astype(int)\n    #df['Age'] = df['Age'].fillna(df['Age'].mean()).astype(int)\n    df['Cabin'] = df['Cabin'].fillna('None')\n    df['Embarked'] = df['Embarked'].fillna(2).replace({'C': 0, 'Q': 1, 'S': 2}).astype(int)\n    df['Fare'] = df['Fare'].fillna(0)\n    return df","f394ce39":"train = load_sanitize('..\/input\/titanic\/train.csv')\ntrain.head()","1eb42646":"train.info()","967c5a13":"test = load_sanitize('..\/input\/titanic\/test.csv')\ntest.head()","e2f4b450":"train.dtypes","71a08a42":"def build_titles(row):\n    name = row['Name']\n    for title in ['Mrs','Mr','Master','Miss','Major','Rev','Dr', 'Ms','Mlle','Col','Capt','Mme','Countess','Don','Jonkheer']:\n        if str.find(name, title) != -1:\n            if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']: return 0\n            elif title in ['Mrs','Countess', 'Mme']: return 1\n            elif title in ['Miss','Mlle', 'Ms']: return 2\n            elif title in ['Master']: return 3\n            elif title == 'Dr':\n                if row['Sex'] == 'Male': return 0\n                else: return 1\n\ndef nan_ages(row):\n    age = row['Age']\n    if np.isnan(age):\n        if row['Title'] == 0: return train[(train['Title'] == 0)]['Age'].mean()\n        elif row['Title'] == 1: return train[(train['Title'] == 1)]['Age'].mean()\n        elif row['Title'] == 2: return train[(train['Title'] == 2)]['Age'].mean()\n        elif row['Title'] == 3: return train[(train['Title'] == 3)]['Age'].mean()\n        elif row['Title'] == 4: return train[(train['Title'] == 4)]['Age'].mean()\n    else: return age\n    \ndef build_deck(row):\n    cabin = str(row['Cabin'])\n    decks = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'None']\n    for i, deck in zip(range(0, len(decks)), decks):\n        if str.find(cabin, deck) != -1: return i\n        \ndef engineer_features(df):\n    df['Title'] = df.apply(build_titles, axis=1)\n    df['Deck'] = df.apply(build_deck, axis=1)\n    df['Age'] = df.apply(nan_ages, axis=1)\n    df['FamilySize'] = df['SibSp'] + df['Parch']\n    df['FarePerPassenger'] = df['Fare']\/(df['FamilySize'] + 1)\n    return df","ac8d5ae9":"train = load_sanitize('..\/input\/titanic\/train.csv')\ntrain, test = engineer_features(train), engineer_features(test)\ntrain.head(6)","3c138618":"test.head()","c38f6267":"def create_bins(df, columns, q=5):\n    for column in columns:\n        df[column] = pd.qcut(df[column], q).cat.codes\n\ndef normalize_data(df, columns):\n    minMaxScaler = MinMaxScaler()\n    for column in columns:\n        df[column] = minMaxScaler.fit_transform(df[[column]])","90a8d43c":"#create_bins(train, ['Age'])\n#create_bins(train, ['Fare','FarePerPassenger'], q=10)\nnormalize_data(train, ['Pclass','Age','SibSp','Parch','Fare','Embarked','Title','Deck','FamilySize','FarePerPassenger'])\ntrain.head()","14d57927":"#create_bins(test, ['Age'])\n#create_bins(test, ['Fare','FarePerPassenger'], q=10)\nnormalize_data(test, ['Pclass','Age','SibSp','Parch','Fare','Embarked','Title','Deck','FamilySize','FarePerPassenger'])\ntest.head()","3f953fc5":"fig, ax = plt.subplots(figsize=(18,12))\n\ns1 = plt.subplot(221)\nplt.hist(train['Age'], color='limegreen', alpha=0.75)\nplt.title('Passengers by Age', fontsize=14, fontweight='bold', color='#333333')\nplt.grid(which='major', axis='y', color='#CCCCCC')\nplt.xlabel('Ages')\n[s1.spines[spine].set_visible(False) for spine in ('top', 'right', 'left')]\ns1.set_axisbelow(True)\n\n\ngender = pd.DataFrame(data={'count': [train[(train['Sex'] == i)]['Sex'].count() for i in range(0,2)]}, index=['male','female'])\ns2 = plt.subplot(222)\nplt.bar(x=gender.index, height=gender['count'], color=['blue','red'], alpha=0.75)\nplt.title('Passengers by Gender', fontsize=14, fontweight='bold', color='#333333')\nplt.grid(which='major', axis='y', color='#CCCCCC')\nplt.xlabel('Gender')\n[s2.spines[spine].set_visible(False) for spine in ('top', 'right', 'left')]\ns2.set_axisbelow(True)\n\ntitle = pd.DataFrame(data={'count': [train[(train['Title'] == i)]['Title'].count() for i in range(0,4)]}, index=['Mr','Mrs','Miss','Master'])\ns3 = plt.subplot(223)\n#plt.bar(x=title.index, height=title['count'], color=['limegreen','blue','red','purple'], alpha=0.75)\nplt.hist(train['Title'], color='purple', alpha=0.75)\nplt.title('Passengers by Title', fontsize=14, fontweight='bold', color='#333333')\nplt.grid(which='major', axis='y', color='#CCCCCC')\nplt.xlabel('Titles')\n[s3.spines[spine].set_visible(False) for spine in ('top', 'right', 'left')]\ns3.set_axisbelow(True)\n\npclass = pd.DataFrame(data={'count': [train[(train['Pclass'] == i)]['Pclass'].count() for i in range(0,4)]}, index=range(0,4))\ns4 = plt.subplot(224)\nplt.hist(train['Pclass'], color='orange', alpha=0.75)\n#plt.bar(x=pclass.index, height=pclass['count'], color=['limegreen','blue','red','purple'], alpha=0.75)\nplt.title('Passengers by Class', fontsize=14, fontweight='bold', color='#333333')\nplt.grid(which='major', axis='y', color='#CCCCCC')\nplt.xlabel('Class')\n[s4.spines[spine].set_visible(False) for spine in ('top', 'right', 'left')]\n#s4.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n#s4.xaxis.set_ticks(np.arange(0, 4, 1))\n\ns4.set_axisbelow(True)\n\nplt.show();\n\n","91bf0926":"fig, ax = plt.subplots(figsize=(12,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), annot=True, vmin=0, cmap=plt.cm.YlGnBu)\nplt.title('Features Correlations', fontsize=14, fontweight='bold', color='#333333')\nplt.show();","c029e00a":"print('Train shape:', train.shape)\nprint('Test shape:', test.shape)","b5dcca06":"features = [f for f in test.columns if f not in ['PassengerId','Name','Ticket','Cabin']]\n\nx = train[features]\ny = train['Survived']\ntesting = test[features]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)","8759da2d":"svc = SVC(gamma='auto', probability=True)\nsvc.fit(x_train, y_train)\nyhat = svc.predict(x_test)\nprint('Train score: %.3f \\nTest score: %.3f ' % (svc.score(x_train,y_train), svc.score(x_test,yhat)))\nprint('Cross Validation:',cross_val_score(svc, x, y, cv=5).mean())\nprint('\\n',classification_report(y_test, yhat))","77616224":"logreg = LogisticRegression(max_iter=300)\nlogreg.fit(x_train, y_train)\nyhat = logreg.predict(x_test)\nprint('Train score: %.3f \\nTest score: %.3f ' % (logreg.score(x_train,y_train), logreg.score(x_test,yhat)))\nprint('Cross Validation:',cross_val_score(logreg, x, y, cv=5).mean())\nprint('\\n', classification_report(y_test, yhat))","fd18aa60":"knn = KNeighborsClassifier(n_neighbors=7)  \nknn.fit(x_train, y_train)\nyhat = knn.predict(x_test)\nprint('Train score: %.3f \\nTest score: %.3f ' % (knn.score(x_train,y_train), knn.score(x_test,yhat)))\nprint('Cross Validation:',cross_val_score(knn, x, y, cv=5).mean())\nprint('\\n', classification_report(y_test, yhat))","d85ccb03":"gpc = GaussianProcessClassifier(kernel=1.0 * RBF(1.0),random_state=0)\ngpc.fit(x_train, y_train)\nyhat = gpc.predict(x_test)\nprint('Train score: %.3f \\nTest score: %.3f ' % (gpc.score(x_train,y_train), gpc.score(x_test,yhat)))\nprint('Cross Validation:',cross_val_score(gpc, x, y, cv=5).mean())\nprint('\\n', classification_report(y_test, yhat))","c3bf1515":"forest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nyhat = forest.predict(x_test)\nprint('Train score: %.3f \\nTest score: %.3f ' % (forest.score(x_train,y_train), forest.score(x_test,yhat)))\nprint('Cross Validation:',cross_val_score(forest, x, y, cv=5).mean())\nprint('\\n', classification_report(y_test, yhat))","d70e3373":"for model, name in zip([svc, logreg, knn, gpc, forest],['svc','logreg','knn','gpc','forest']):\n    model.fit(x_train, y_train)\n    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': model.predict(test[features])})\n    submission.to_csv('%s.csv' % name, index=False)","f84143bb":"Based on the previous heatmap, we can notice a strong correlation on `Sex`, `Title` and `Fare`.","ef021b71":"First, let's split train and test datasets, for better model evaluation.","5f640cda":"# 3. Data Exploration <a id='exploration'><\/a>","d844c8de":"# 4. Methodology and results <a id='methodology'><\/a>\n\nIn the next section, the models were built and the prediction evaluated.\n\n1. [Support Vector Classifier](#svc)\n1. [Logistic Regression](#logreg)\n1. [K-neighbors Classifier](#kneighbors)\n1. [Gaussian Process Classifier](#gaussian)","e295c663":"## 4.3 K-neighbors Classifier <a id='kneighbors'><\/a>","201bba31":"## 4.1 Support Vector Classifier <a id='svc'><\/a>","dd7cd4cf":"## 4.2 Logistic Regression <a id='logreg'><\/a>","0aa6f313":"## 2.3 Normalize Data\nNow, I normalized the features creating bins and using the `MinMaxScaler`.","a6b6761e":"## 5.1 Generate CSV","2028a06e":"## 2.2 Feature Engineering\nNext, I'll create new features, based on the given dataset:\n* Create the `Title` feature, extracting substrings from the `Name`;\n* Create the `Deck` feature, extracting the floor level from the `Cabin`;\n* Create the `FamilySize`, aggregating `SibSp` and `Parch`;\n* Create the `FarePerPassenger`, dividing `Fare` by `FamilySize`.","eb809e80":"## 4.4 Gaussian Process Classifier <a id='gaussian'><\/a>","ce0e1c69":"# 5. Conclusion <a id='conclusion'><\/a>\nInitially, analyzing the output of each model, the Logistic Regression has the best score for precision, recall and f1-score. Therefore, the Logistic Regression model seems to be the best classifier to predict the surviving passengers.\n\n## Updates after submission\n* Logistic Regression, without binning and normalization, performed `0.76076` of precision;\n* Gaussian Process, without binning and normalization, performed `0.77511` of precision;\n* Logistic Regression, with binning and normalization, performed `0.76076` of precision (+0%);\n* Gaussian Process, with binning and normalization, performed `0.77033` of precision (-1.24%);\n* Gaussian Process, with train\/test rate ajusted to 80\/20, performed `0.78947` of precision (+1,81%).\n* Random Forest, with train\/test rate ajusted to 80\/20, performed `0.75119` of precision.\n* Gaussian Process and Random Forest, without binning, performed `0.78947` of precision.\n* Random Forest, with adjusted NaN ages, performed `0.78947` of precision.\n* Gaussian Process, with adjusted NaN ages, performed `0.78947` of precision.\n\nUntil the current time, *Gaussian Process Classifier* with binning and normalization performed best.","6c4839f6":"## 4.5 Random Forest Classifier <a id='forest'><\/a>","72e49f8b":"# Predicting Titanic Survivors\nThis notebook is part of the [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic\/notebooks) competition, that proposes a machine learning model that predicts if a given passenger whether to survive or not to the Titanic accident.\n\n[![Titanic Competition](https:\/\/storage.googleapis.com\/kaggle-media\/welcome\/video_thumbnail.jpg)](https:\/\/www.youtube.com\/watch?v=8yZMXCaFshs&feature=youtu.be)\n\n\n---\n\n### Table of contents\n1. [Introduction](#intro)\n1. [Data Process](#process)\n1. [Data Exploration](#exploration)\n1. [Methodology and results](#methodology)\n1. [Conclusion](#conclusion)\n\n---\n\n# 1. Introduction <a id='intro'><\/a>\nIn this notebook I'll try to build and evaluate the following models:\n\n1. [Support Vector Classifier](#svc)\n1. [Logistic Regression](#logreg)\n1. [K-neighbors Classifier](#kneighbors)\n1. [Gaussian Process Classifier](#gaussian)\n1. [Random Forest Classifier](#forest) (new!)\n\nThe main object of this project is try to answer the following problem:\n> What sorts of people were more likely to survive?\n\n# 2. Data Process <a id='process'><\/a>\n## 2.1 Import, load and sanitize data"}}