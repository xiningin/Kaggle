{"cell_type":{"8c0ef264":"code","aa083578":"code","88c377eb":"code","e40dd724":"code","b311a606":"code","449f58d0":"code","2ba533e9":"code","adb8af79":"code","0bdb6ef4":"code","fce44cf2":"code","3f0a9771":"code","42e7984e":"code","daa6d230":"code","82c0dbf8":"code","267d4bbc":"code","1262a1ca":"code","98857c2a":"code","0ace6436":"code","c7b2327b":"code","ffd0bce3":"code","8367f7b9":"markdown","50adb104":"markdown","3342c2d9":"markdown","91e4583f":"markdown","5f1d2d3d":"markdown","39ca48e3":"markdown","d58592dd":"markdown","06f47111":"markdown","72ac6e03":"markdown","31ca7e3c":"markdown"},"source":{"8c0ef264":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom itertools import combinations, permutations\n# Display all columns\npd.set_option('display.max_columns', None)\nplt.style.use('seaborn-colorblind')\n\ndf = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\ndf = df.set_index('PATIENT_VISIT_IDENTIFIER') ","aa083578":"w02 = df[df.WINDOW == '0-2']\nw24 = df[df.WINDOW == '2-4']\nw46 = df[df.WINDOW == '4-6']\nw612 = df[df.WINDOW == '6-12']\nwa12 = df[df.WINDOW == 'ABOVE_12']\n\nw02['ICU_W24'] = w24['ICU']\nw02['ICU_W46'] = w46['ICU']\nw02['ICU_W612'] = w612['ICU']\nw02['ICU_Wa12'] = wa12['ICU'] ","88c377eb":"w02.loc[:,['GENDER', 'AGE_ABOVE65', 'ICU','ICU_W24','ICU_W46','ICU_W612','ICU_Wa12']].head(10)","e40dd724":"# FIRST REMOVE ICU 1 FROM WINDOW 0-2\nw02 = w02[w02.ICU == 0]\n\n# NEW TARGET \"NOT ICU\"\nw02['temp'] = w02.loc[:,['ICU','ICU_W24','ICU_W46','ICU_W612','ICU_Wa12']].sum(axis=1) \ndef label_not_icu(x):\n    if (x['temp'] == 0):\n        val = 1 \n    elif (x['temp'] > 0):\n        val = 0\n    return val\n\nw02['NOT_ICU'] = w02.apply(label_not_icu, axis=1)\n\n# REMOVE UNWANTED COLUMNS \nw02 = w02.drop(['temp', 'ICU','ICU_W24','ICU_W46','ICU_W612','ICU_Wa12'], axis = 1)\nw02.shape","b311a606":"### Redundant Feature Check\ndef is_one_to_one(df, cols):\n    \"\"\"Check whether any number of columns are one-to-one match.\n\n    df: a pandas.DataFrame\n    cols: must be a list of columns names\n\n    Duplicated matches are allowed:\n        a - 1\n        b - 2\n        b - 2\n        c - 3\n    (This two cols will return True)\n    Source: [link](https:\/\/stackoverflow.com\/questions\/50643386\/easy-way-to-see-if-two-columns-are-one-to-one-in-pandas)\n\n    \"\"\"\n    if len(cols) == 1:\n        return True\n        # You can define you own rules for 1 column check, Or forbid it\n\n    # MAIN THINGs: for 2 or more columns check!\n    res = df.groupby(cols).count()\n    uniqueness = [res.index.get_level_values(i).is_unique\n                for i in range(res.index.nlevels)]\n    return all(uniqueness)\n\n# Getting combinations of all the colmns\ncombos = list(combinations(w02.columns,2))\n\n# Running to see if any of them are identical\nidentical_cols = []\n\nfor col in np.arange(0,len(combos),1):\n    x = [combos[col][0],combos[col][1]]\n    if is_one_to_one(w02,x) == True:\n         identical_cols.append(combos[col][0])\n            \nall_cols = [x for x in w02.columns if x not in identical_cols]\nw02 = w02.loc[:, all_cols]","449f58d0":"w02 = w02.drop('WINDOW', axis = 1)\nw02.info(verbose=True)","2ba533e9":"w02['NOT_ICU'].value_counts()","adb8af79":"from sklearn.datasets import load_breast_cancer\n!pip install optuna\n\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport lightgbm as lgb\nimport xgboost as xgb\n# sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\n\nimport sklearn.metrics\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nsns.set()\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\n\nimport operator as op \nfrom itertools import combinations","0bdb6ef4":"datacopy = w02\nx =[]\nfor col in w02.columns:\n    n = len(datacopy[col].unique())\n    if (15 < n):\n        continue\n    if (15 > n > 2 ): # making a list of columns that are greater than 2 levels\n        y = col\n        x.append(y)      \n#     elif (n == 2): # a categorical descriptive feature has only 2 levels\n#         datacopy[col] = pd.get_dummies(datacopy[col], drop_first=True)\n\ndatacopy = pd.get_dummies(datacopy, columns = x)\ndatacopy.shape\nX = datacopy.drop('NOT_ICU', axis = 1)\nDX =  datacopy.drop('NOT_ICU', axis = 1)\ny =  datacopy['NOT_ICU']\ndy =  datacopy['NOT_ICU']","fce44cf2":"def brute_force_feat(X,DX):\n  X = X.loc[:,~X.columns.duplicated()]\n\n  from sklearn.impute import SimpleImputer\n  imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n  imp_mean.fit(X)\n  X = imp_mean.transform(X)\n  DX = imp_mean.transform(DX)\n\n  print('Dimensionality Features')\n  # DIMENSIONALITY FEATURES\n  # 2 is usually a good size, however higher may result in less reward for time taken\n  X_TSNE = TSNE(n_components=2).fit_transform(X)\n  X_DBSCAN = DBSCAN(eps=3, min_samples=2).fit(X)\n  X_PCA = PCA(n_components=2).fit_transform(X)\n  # Depending on how large the dataset is increase or decrease n \n  X_KNN64, indices = NearestNeighbors(n_neighbors=64, algorithm='ball_tree').fit(X).kneighbors(X)\n  X = np.c_[X, X_KNN64] # NUMPY\n  X = np.c_[X, X_PCA]\n  X = np.c_[X, X_DBSCAN.labels_]\n  X = np.c_[X, X_TSNE]\n  X = np.where(np.isnan(X), 0, X)\n\n\n  # Selecting best 50 columns \n  from sklearn.ensemble import RandomForestClassifier\n\n  # NUMPY\n  print('No. Features:', np.size(X,1))\n  np.random.seed(42)\n  X = np.nan_to_num(X.astype(np.float32)) # prevents too large values error\n\n  # SX IS A SAMPLE OF X\n  SX = X\n  sy = dy # other methods if this gives error #.values.ravel() #np.array(dy)[idx.astype(int)] #dy[idx]\n  \n  model_rfi = RandomForestClassifier(n_jobs = -1, class_weight = 'balanced_subsample', random_state=42)\n\n  model_rfi.fit(SX, sy)\n  print('Done!')\n\n  return X, DX, model_rfi\n\n# CX is the new features data | DX is the original data  | Model_rfi contains the feature importance\nCX, DX, model_rfi = brute_force_feat(X,DX)","3f0a9771":"def scaler_fuc(scaler, f, CX, DX):\n\n  # order by most important features and take the f most important features\n  fs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:f]\n  CX = CX[:,fs_indices_rfi]\n  X = np.c_[DX, CX]\n  # delete duplicates\n  _, unq_col_indices = np.unique(X,return_index=True,axis=1)\n  X = X[:,unq_col_indices]\n\n  train_x, valid_x, train_y, valid_y = train_test_split(X, dy,\n                                              test_size=0.25, random_state = 123)\n  if (scaler == 'minmax'):\n    scaler = MinMaxScaler()\n    scaler.fit(train_x)\n    train_x = scaler.transform(train_x)\n    scaler.fit(valid_x)\n    valid_x = scaler.transform(valid_x)\n\n  if (scaler == 'stand'):\n    scaler = StandardScaler()\n    scaler.fit(train_x)\n    train_x = scaler.transform(train_x)\n    scaler.fit(valid_x)\n    valid_x = scaler.transform(valid_x)\n\n  if (scaler == 'log'):\n    transformer = FunctionTransformer(np.log1p, validate=True)\n    train_x = transformer.transform(train_x)\n    valid_x = transformer.transform(valid_x)\n    # prevent log 0 error\n    np.seterr(divide = 'ignore')  # invalid value encountered in log1p\n    train_x = np.where(np.isneginf(train_x), 0, train_x)\n    valid_x = np.where(np.isneginf(valid_x), 0, valid_x)\n    train_x = np.where(np.isinf(train_x), 0, train_x)\n    valid_x = np.where(np.isinf(valid_x), 0, valid_x)\n    train_x = np.where(np.isnan(train_x), 0, train_x)\n    valid_x = np.where(np.isnan(valid_x), 0, valid_x)\n\n  # turning these train\/tests into lgb\/xgb datasets\n  dtrain_gbm = lgb.Dataset(train_x, label=train_y)\n  dvalid_gbm = lgb.Dataset(valid_x, label=valid_y)\n\n  dtrain_xbg = xgb.DMatrix(train_x, label=train_y)\n  dvalid_xbg = xgb.DMatrix(valid_x, label=valid_y)\n  return train_x, valid_x, train_y, valid_y,  dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg","42e7984e":"models = ['gp', 'et', 'xgb', 'gbm'] # when you want to try all of them\/ iteration: 1 \ncomb = list(combinations(models, 3))","daa6d230":"\nclass Objective:\n\n    def __init__(self):\n        self.best_gbm = None\n        self._gbm = None\n        self.best_xgb = None\n        self._xgb = None\n        self.predictions = None\n        self.fpredictions = None\n\n    def __call__(self, trial):\n\n        i = trial.suggest_int(\"combos\", 0, (len(comb)-1))\n        gbm_preds = np.zeros((math.ceil(len(dy)*0.25),1), dtype=np.int)\n        xgb_preds = np.zeros((math.ceil(len(dy)*0.25),1), dtype=np.int)\n        et_preds = np.zeros((math.ceil(len(dy)*0.25),1), dtype=np.int)\n        gp_preds = np.zeros((math.ceil(len(dy)*0.25),1), dtype=np.int)\n\n\n   \n        ###############################################################################\n        #                 . GaussianProcess   +  Radial-Basis Function                #\n        ###############################################################################\n        if any(x == 'gp' for x in comb[i]):\n          gp_f = trial.suggest_int(\"gp_features\", 0, 20)\n          gp_scaler = trial.suggest_categorical(\"gp_Scaler\", ['minmax','stand','log'])\n          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(gp_scaler, gp_f, CX, DX)\n          a_gp = trial.suggest_loguniform(\"gp_a\", 0.001, 10)\n          gp_kern= trial.suggest_int(\"gp_kern\", 1, 15)\n          gpkernel = a_gp * RBF(gp_kern)\n          gp = GaussianProcessClassifier(kernel=gpkernel, random_state=0, n_jobs = -1).fit(train_x, train_y)\n          gp_preds = gp.predict_proba(valid_x)[:,1]\n\n        ###############################################################################\n        #                              . Extra Trees                                  #\n        ###############################################################################\n        if any(x == 'et' for x in comb[i]):\n          et_f = trial.suggest_int(\"et_features\", 0, 20)\n          et_scaler = trial.suggest_categorical(\"et_Scaler\", ['minmax','stand','log'])\n          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(et_scaler,et_f, CX, DX)\n          et_md = trial.suggest_int(\"et_max_depth\", 1, 100)\n          et_ne = trial.suggest_int(\"et_ne\", 1, 500) #1000\n          et = ExtraTreesClassifier(max_depth=et_md, n_estimators = et_ne,\n                                     random_state=0).fit(train_x, train_y)\n          et_preds = et.predict_proba(valid_x)[:,1]\n        ###############################################################################\n        #                                 . XGBoost                                   #\n        ###############################################################################\n        if any(x == 'xgb' for x in comb[i]): \n          xgb_f = trial.suggest_int(\"xgb_features2\", 0, 20)\n          xgb_scaler = trial.suggest_categorical(\"xgb_Scaler2\", ['minmax','stand','log'])\n          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(xgb_scaler,xgb_f, CX, DX)\n          xgb_param = {\n              \"silent\": 1,\n              \"objective\": \"binary:logistic\", # change for multiclass\n              \"eval_metric\": \"auc\",\n              # \"num_class\": 3, # change this up depending on multiclass | dont use with binary\n              \"booster\": trial.suggest_categorical(\"booster2\", [\"gbtree\", \"gblinear\", \"dart\"]),\n              \"lambda\": trial.suggest_loguniform(\"lambda2\", 1e-8, 1.0),\n              \"alpha\": trial.suggest_loguniform(\"alpha2\", 1e-8, 1.0),\n          }\n          if xgb_param[\"booster\"] == \"gbtree\" or xgb_param[\"booster\"] == \"dart\":\n              xgb_param[\"max_depth\"] = trial.suggest_int(\"max_depth2\", 1, 100)\n              xgb_param[\"eta\"] = trial.suggest_loguniform(\"eta2\", 1e-8, 1.0)\n              xgb_param[\"gamma\"] = trial.suggest_loguniform(\"gamma2\", 1e-8, 1.0)\n              xgb_param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy2\", [\"depthwise\", \"lossguide\"])\n          if xgb_param[\"booster\"] == \"dart\":\n              xgb_param[\"sample_type\"] = trial.suggest_categorical(\"sample_type2\", [\"uniform\", \"weighted\"])\n              xgb_param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type2\", [\"tree\", \"forest\"])\n              xgb_param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop2\", 1e-8, 1.0)\n              xgb_param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop2\", 1e-8, 1.0)\n          # Add a callback for pruning.\n          xgb_pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\" )\n          xgb_ = xgb.train(xgb_param, dtrain_xbg, evals=[(dvalid_xbg, \"validation\")], verbose_eval=False, callbacks=[xgb_pruning_callback])\n          xgb_preds = xgb_.predict(dvalid_xbg)\n          self._xgb = xgb_\n        ###############################################################################\n        #                          . Light Gradient Boosting                          #\n        ###############################################################################\n        if any(x == 'gbm' for x in comb[i]):\n          gbm_f = trial.suggest_int(\"gbm_features2\", 0, 20)\n          gbm_scaler = trial.suggest_categorical(\"gbm_Scaler2\", ['minmax','stand','log'])\n          train_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc(gbm_scaler,gbm_f, CX, DX)\n          gbm_param = {\n            'objective': 'binary', # change for multiclass\n            'metric': 'auc',\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"lambda_l1\": trial.suggest_loguniform(\"lambda_l12\", 1e-8, 10), \n              \"lambda_l2\": trial.suggest_loguniform(\"lambda_l22\", 1e-8, 10),\n              \"num_leaves\": trial.suggest_int(\"num_leaves2\", 2, 256), \n              \"feature_fraction\": trial.suggest_uniform(\"feature_fraction2\", 0.4, 1.0), \n              \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction2\", 0.4, 1.0),\n              \"bagging_freq\": trial.suggest_int(\"bagging_freq2\", 1, 7), \n              \"min_child_samples\": trial.suggest_int(\"min_child_samples2\", 2, 20), \n          }\n          # Add a callback for pruning.\n          gbm_pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n          gbm = lgb.train(gbm_param, dtrain_gbm, valid_sets=[dvalid_gbm], verbose_eval=False, callbacks=[gbm_pruning_callback])\n          gbm_preds = gbm.predict(valid_x)\n          self._gbm = gbm\n        ###############################################################################\n        #                            . Stacking Strategy                              #\n        ###############################################################################\n\n        preds = (gbm_preds + xgb_preds +  \\\n         + et_preds + \\\n         gp_preds ) \/ 3 \n        preds = preds[:1][0]\n\n\n        self.predictions = preds\n        auc = roc_auc_score(valid_y, preds)\n        return auc\n\n    def callback(self, study, trial):\n        if study.best_trial == trial:\n            self.best_gbm = self._gbm\n            self.best_xgb = self._xgb\n            self.fpredictions = self.predictions","82c0dbf8":"import warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) # for log error\n\nimport optuna\nobjective = Objective()\n\n# Setting SEED \nfrom optuna.samplers import TPESampler\nsampler = TPESampler(seed=10)\n\nstudy = optuna.create_study(\n    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\",\n    sampler=sampler\n)\nstudy.optimize(objective, n_trials=300, callbacks=[objective.callback]) # change this to 500 + \n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nbest_gbm = objective.best_gbm\nbest_xgb = objective.best_xgb","267d4bbc":"from sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve, roc_auc_score, precision_score, recall_score, average_precision_score\ntrain_x, valid_x, train_y, valid_y, dtrain_gbm, dvalid_gbm, dtrain_xbg, dvalid_xbg = scaler_fuc('log',3, CX, DX) # just taking the validation set","1262a1ca":"comb[2]","98857c2a":"predictions = objective.fpredictions\npredictions.shape","0ace6436":"len(train_x)","c7b2327b":"len(valid_x)","ffd0bce3":"accuracy  = accuracy_score(valid_y, predictions >= 0.5)\nroc_auc   = roc_auc_score(valid_y, predictions)\nprecision = precision_score(valid_y, predictions >= 0.5)\nrecall    = recall_score(valid_y, predictions >= 0.5)\npr_auc    = average_precision_score(valid_y, predictions)\n\n\nprint(f'Accuracy: {round(accuracy,4)}')\nprint(f'ROC AUC: {round(roc_auc,4)}')\nprint(f'Precision: {round(precision,4)}')\nprint(f'Recall: {round(recall,4)}')\nprint(f'PR Score: {round(pr_auc,4)}')","8367f7b9":"feature eng","50adb104":"Below we can see the first 10 patients. Patient 0 and 2 in window 0-2 (COLUMN ICU) show negative for ICU however after window 12 show positive. ","3342c2d9":"The task is to predict **NOT** admmited to ICU using window 0-2 of those who are not already in ICU. Lets make that dataframe. ","91e4583f":"after setting the patient visit identifier as the index, create a df for each window period. We will only be using data from window 0-2 which have UCI 0","5f1d2d3d":"a lot of the code is similar to my task 1 from here on in. feel free to check my task 1 EDA and Ensemble [here](https:\/\/www.kaggle.com\/harriwashere\/task-1-eda-3-model-ensemble-95-6-auc)","39ca48e3":"# 3 Model Ensemble | PR 83.63%\n\n\n## Aim\nThe aim is to provide tertiary and quarternary hospitals with the most **accurate** answer, so ICU resources can be arranged or patient transfer can be scheduled.\n\n\n## Task 2 \n\n*Based on the **subsample** of widely available data, is it feasible to predict which patients will need intensive care unit support?*\n\nGiven the explanation of the data [here](https:\/\/www.kaggle.com\/S%C3%ADrio-Libanes\/covid19). ICU admission data must be removed if the window is **greater than 2**. \n\n\n## Methodology\n\n### Task 2\n\n#### 1. Data Preparation\n\n- Data with a window greater than 2 is removed.\n- Target unit is made \"NOT_ICU\" \n- Reduntant\/ Duplicated features are removed. \n- Features are engineered using dimensionality techniques.\n\n#### 2. Model Creation\n\n- Train set is 264 samples and the test set is 89 samples. (75\/25)\n\n- Of 4 machine learning models 3 are self-chosen using the hyper-optimisation library Optuna. These models are uniquely scaled, uniquely featured engineered, uniquely undersampled and parametered tunned as normal.\n\n#### 3. Evaluation.\n\n- Using a callback, the predictions are analysied for Accuracy, Precision Recall & ROC AUC. \n\n## Results\n\n- Accuracy: 0.6517\n- ROC AUC: 0.7853\n- Precision: 0.7234\n- Recall: 0.6538\n- PR Score: 0.8363\n\n## Key Take Aways\n\n- Due to a less than ideal amount of data, building a robust model to new data is crucial. Hence, an ensemblment of models is more ideal.\n- Dimensionality features are experimental. My logic was to capture the data clusters making predictions easier for boosting models. This is mearly my hypothesis and not backed by any factual academia. \n- To ensure a more robust model. Instead of taking the mean of the 3 predictions. A formula such as one on my github displayed below could be used. \n\n\n\n## Acknowledgement & Improvements\n\nI've hidden the inputs of all the boring code to keep the notbook concise, feel free to click on show input\/output. \n\nIn this notebook i only used 4 models, and picked 3, however on my github you can pick from 11 for classification and 7 for regression. \nA fully detailed version of the ensemble is avaliable on my github: \n\nhttps:\/\/github.com\/CodeByHarri\/Stacking-Ensemble-Machine-Learning\n\n*feel free to comment below any questions or provide feedback :)*\n","d58592dd":"looks like a nice split of not icu and icu, undersampling may not be required like in task 1.","06f47111":"one-hotencode","72ac6e03":"perfect. this makes sense as there are only 353 patients that did not go to ICU in window 0-2. These are the patients we will predict to see if they NOT end up in ICU.\n\nJust because it's kinda confusing: \n- NOT_ICU = 1 . means the patient did not end up in ICU\n- NOT_ICU = 0 . mean that the patient ended up in ICU \n\n(this is done because the positive class usually has a bias, and we're predicting NOT ICU as per the task)","31ca7e3c":"```\nROC_AUC: 0.7853\n    combos: 2\n    gp_features: 11\n    gp_Scaler: minmax\n    gp_a: 0.03866013280015601\n    gp_kern: 7\n    xgb_features2: 11\n    xgb_Scaler2: log\n    booster2: gblinear\n    lambda2: 9.175533799894901e-07\n    alpha2: 2.078598952639305e-06\n    gbm_features2: 14\n    gbm_Scaler2: stand\n    lambda_l12: 0.014207818906099316\n    lambda_l22: 1.0967192744185169e-05\n    num_leaves2: 126\n    feature_fraction2: 0.5492187396683705\n    bagging_fraction2: 0.7154864203828437\n    bagging_freq2: 2\n    min_child_samples2: 6\n```"}}