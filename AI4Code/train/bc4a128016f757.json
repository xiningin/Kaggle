{"cell_type":{"cfb6e433":"code","ad3ceb3e":"code","8a8972f0":"code","fae78ba3":"code","6b787aca":"code","04509a40":"code","e7e52fb7":"code","abbec41f":"code","18d28155":"code","4f97196c":"code","c923e875":"code","cd906d63":"code","95b7411f":"code","b0f0642f":"code","952e77b9":"code","43174c8b":"code","99a789cd":"code","544c213a":"code","f12c9254":"code","fa735f4d":"code","46a392ec":"code","c43f979e":"code","48893ce3":"code","11480c6d":"code","d229a300":"code","e0a33791":"code","6d8683aa":"code","5be354e9":"code","33b7f6ac":"code","6bcee9ce":"code","cdef4ada":"code","b426c73c":"code","1a62ba42":"code","7833e4be":"code","4dceced9":"code","15059834":"markdown","323b58e2":"markdown","d832f094":"markdown","493f2fa8":"markdown","ff23c2c4":"markdown","d32b64a4":"markdown","dd4c51b9":"markdown","c11f9ea5":"markdown","8e7274cd":"markdown","f03d4eed":"markdown","220da780":"markdown","02f474cf":"markdown","b7b304ac":"markdown","89314b41":"markdown","4f7bf977":"markdown","a37d5b7d":"markdown"},"source":{"cfb6e433":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","ad3ceb3e":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","8a8972f0":"df.info()","fae78ba3":"df.describe().T","6b787aca":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness',\n                                                                      'Insulin','BMI']].replace(0, np.NaN)","04509a40":"df.isnull().sum()","e7e52fb7":"def median_target(var):   \n    temp = df[df[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","abbec41f":"cols = df.columns\ncols = cols.drop('Outcome')\n\nfor col in cols:\n    median_target(col)\n    \n    df.loc[(df['Outcome'] == 0) & (df[col].isnull()), col] = median_target(col)[col][0]\n    df.loc[(df['Outcome'] == 1) & (df[col].isnull()), col] = median_target(col)[col][1]","18d28155":"df.isnull().sum()","4f97196c":"sns.pairplot(df)","c923e875":"df.hist(bins=15,figsize=(20,20));","cd906d63":"sns.countplot(df['Outcome'])","95b7411f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfor col in df.columns[:-1]:\n    df[col] = scaler.fit_transform(df[[col]])","b0f0642f":"df.head()","952e77b9":"df.hist(bins=15,figsize=(20,20));","43174c8b":"from sklearn.model_selection import train_test_split\nX = df.drop('Outcome',axis=1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)","99a789cd":"def plt_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.tight_layout()\n    plt.axis([0,1,0,1])\n    plt.legend()\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")","544c213a":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel_rf = RandomForestClassifier(n_estimators=100)\nmodel_ext = ExtraTreesClassifier()\nmodel_ada = AdaBoostClassifier()\nmodel_grad = GradientBoostingClassifier()\nmodel_logis = LogisticRegression()\nmodel_dec = DecisionTreeClassifier()\n\nmodels = [model_rf, model_ext, model_ada, model_grad, model_logis, model_dec]","f12c9254":"from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, f1_score","fa735f4d":"def model_train(model):\n    model.fit(X_train,y_train)\n    \n    y_pred = model.predict(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n    print(f'model: {model}')\n    print(classification_report(y_test,y_pred))\n    plt_roc_curve(fpr,tpr,label=f'{model}')\n    \n    return f1_score(y_test,y_pred)","46a392ec":"scores = {'Random Forest':0,\n          'ExtraTress': 0,\n          'AdaBoost':0,\n          'Gradient Boosting':0,\n          'Logistic Regression':0,\n          'Decision Tree':0}\n\n\nscr = []\nfor i,model in enumerate(models):\n    score = model_train(model)\n    scr.append(score)","c43f979e":"from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline","48893ce3":"def ml_model(model, parameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n    random_search = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1, verbose=2 )\n    #pipe = make_pipeline(StandardScaler(),random_search)\n    random_search.fit(X_train, y_train)\n    y_pred_proba = random_search.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    print(\"ROC Score : \",roc_auc_score(y_test, y_pred_proba))\n    print(\"Accuracy for train: \", accuracy_score(y_train, random_search.predict(X_train)))\n    print(\"Accuracy for test: \" , accuracy_score(y_test, random_search.predict(X_test)))\n    print(\"Best params:\" + str(random_search.best_params_))\n    plt_roc_curve(fpr,tpr)\n    \n    return random_search\n    \nlog_reg_params = {\"C\" : [1,2,3,0.01,0.001, 2.5, 1.5],\n                  \"max_iter\" : range(100,800,100)}\nknn_params = {\"n_neighbors\" : np.arange(1,50),\n              \"leaf_size\" : np.arange(1,50)}\ndecTree_params = {\"max_depth\" : [5,10,15,20,25,30],\n                  \"min_samples_split\" : np.arange(2,50),\n                  \"min_samples_leaf\" : np.arange(1,50)}\nrandomForest_params = {\"n_estimators\" : [100,500, 1000],\n                       \"min_samples_split\" : np.arange(2,30),\n                       \"min_samples_leaf\" : np.arange(1,50),\n                       \"max_features\" : np.arange(1,7)}\ngrad_params = {\"n_estimators\" : [100,500,1000],\n               \"subsample\" : [0.6,0.8,1.0],\n               \"max_depth\" : [5,10,15,20,25,30],\n               \"learning_rate\" : [0.1, 0.01, 0.02, 0.5]\n               }\n\nsgd_params = {\"alpha\" : [0.0001, 0.1, 0.001, 0.01],\n              \"max_iter\" : [100,500,1000,2000],\n              \"loss\" : [\"log\",\"modified_huber\",\"perceptron\"]}","11480c6d":"ml_model(LogisticRegression(), log_reg_params)","d229a300":"ml_model(KNeighborsClassifier(), knn_params)","e0a33791":"ml_model(DecisionTreeClassifier(), decTree_params)","6d8683aa":"ml_model(RandomForestClassifier(), randomForest_params)","5be354e9":"ml_model(SGDClassifier(), sgd_params)","33b7f6ac":"ml_model(GradientBoostingClassifier(), grad_params)","6bcee9ce":"model_final_gradboost = GradientBoostingClassifier(subsample = 0.6,\n                                                n_estimators = 500, \n                                                max_depth = 20,\n                                                learning_rate = 0.01)","cdef4ada":"model_final_gradboost.fit(X_train, y_train)","b426c73c":"y_pred_proba = model_final_gradboost.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nprint(\"ROC Score : \",roc_auc_score(y_test, y_pred_proba))\nprint(\"Accuracy for train: \", accuracy_score(y_train, model_final_gradboost.predict(X_train)))\nprint(\"Accuracy for test: \" , accuracy_score(y_test, model_final_gradboost.predict(X_test)))\nplt_roc_curve(fpr,tpr)","1a62ba42":"y_pred = model_final_gradboost.predict(X_test)","7833e4be":"sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)","4dceced9":"import pickle\n\nwith open('model_gradboost.pkl','wb') as file:\n    pickle.dump(model_final_random,file)","15059834":"# Handling Missing Values\n\nIn this dataset missing data are filled with 0. First, we are gonna change zeros with NaN","323b58e2":"# Scaling the data","d832f094":"# Model Building","493f2fa8":"**We got highest roc_score of 95.4% for the `GradientBoosting Classifier`, we are storing the model with that best params and we are going to use that for the prediction**","ff23c2c4":"# Splitting data","d32b64a4":"In the above we are finding the median value the separate outcomes and then filling it in the missing values accordingly","dd4c51b9":"# Basic EDA","c11f9ea5":"From the above we can see that Gradient Boosting algorithm is performing well","8e7274cd":"# Data visualisation","f03d4eed":"# Storing the Best Model","220da780":"# K-Fold cross validated models with RandomSearchCV for best params","02f474cf":"**We have successfully stored our model in our storage**","b7b304ac":"# Diabetes prediction for the PIMA Indian Diabetes Database\n\n## Context\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Data Description\n\n- `Pregnancies` - Number of times pregnant\n- `Glucose` - Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- `Blood pressure` - Diastolic blood pressure (mm Hg)\n- `SkinThickness` - Triceps skin fold thickness (mm)\n- `Insulin` - 2-Hour serum insulin (mu U\/ml)\n- `BMI` - Body mass index (weight in kg\/(height in m)^2)\n- `DiabetesPedigreeFunction` - Diabetes pedigree function\n- `Age` - Age (years)\n- `Outcome` - Class variable (0 or 1) 268 of 768 are 1, the others are 0","89314b41":"**Filling the missing values with median values**","4f7bf977":"Without any feature engineering we got higher ROC","a37d5b7d":"Now as we can see all the missing values have been filled with their median values according to the target."}}