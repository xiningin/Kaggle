{"cell_type":{"32acdc00":"code","43fc6cf0":"code","9c3bd77e":"code","135a1923":"code","626fec44":"code","fbcb5d01":"code","630fe8e4":"code","95d776bd":"code","83af1370":"code","b6422742":"code","998e9f16":"code","000e8e81":"code","bcf517d2":"code","4c25853d":"code","d54d142d":"code","bed73e02":"code","773fd7ef":"code","7a8de614":"code","15f359ad":"code","d376538d":"code","87c3133e":"code","58affba9":"code","7e74067f":"code","1a9ab5fc":"code","538ca896":"code","af9907c5":"code","98818edb":"code","497ba002":"code","f8d06b9a":"code","b82c54b4":"code","b53033f0":"code","a73ff367":"code","265f91f4":"code","5ac9f6e7":"code","e9714a4c":"code","9133669e":"code","bea0d589":"code","bcd6a18f":"markdown","056d7781":"markdown","9d4f5a2d":"markdown","b75ed458":"markdown","c29bf13f":"markdown","28d2e691":"markdown","740bb285":"markdown","a9fde806":"markdown","75825c10":"markdown","ece34d34":"markdown","77c62b6d":"markdown","ccece013":"markdown","7ffb8fea":"markdown","d074b232":"markdown","5e59998a":"markdown","7ea7e17a":"markdown","960d021e":"markdown","bcd7add1":"markdown","60828c5f":"markdown","74247752":"markdown","33dbcfee":"markdown","0e216915":"markdown"},"source":{"32acdc00":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm as tq\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","43fc6cf0":"root = \"\/kaggle\/input\/toi-2018-news-articles\/data\/\"\nart_list = os.listdir(root)\nprint(f\"total articles {len(art_list)}\")\nprint(\"sample titles: \")\nprint(*art_list[:3], sep='\\n')","9c3bd77e":"contents = [open(root+i).read() for i in tq(art_list, leave=False)]\nprint(\"sample content :\")\nprint(contents[10])","135a1923":"df = pd.DataFrame(list(zip(art_list, contents)), columns=['title', 'content'])\ndf.head()","626fec44":"df['date'] = df.title.apply(lambda x: int(x.split('_')[0]))\ndf['tag'] = df.title.apply(lambda x: (\"_\".join(x.split('-')[0].split('_')[1:-1])))\ndf.head()","fbcb5d01":"months = [i for i in range(1, 13)]\ndays = [i for i in range(1, 32)]\n\ndef convert(x):\n    x = str(x)\n    splits = [(int(x[:k]), int(x[k:])) for k in range(1, len(x))]\n    for i, j in splits:\n        if i in months and j in days: \n            return i, j\n\ndf['year'] = df.title.apply(lambda x: int(x.split('_')[0][:4]))\nrest = df.title.apply(lambda x: int(x.split('_')[0][4:]))\na = rest.apply(convert)\ndf['month'] = [i[0] for i in a]\ndf['day'] = [i[1] for i in a]\ndf.head()","630fe8e4":"df['headline'] = df.title.apply(lambda x: (x.split('-')[0].split('_')[-1] + '-' + '-'.join(x.split('-')[1:])).replace(\"-\", \" \")[:-3])\ndf['content'] = df.content.apply(lambda x: x[1:-1])\ndf.head()","95d776bd":"def get_loc(x):\n    p = x.split(':')[0]\n    if len(p.split(\" \")) < 6:\n        return p\n    elif len(p.split('()')[0]) < 30:\n        return p.split(',')[0]\n    return \"\"\n\ndf['loc'] = df['content'].apply(get_loc)\ndf.head()","83af1370":"df = df[['date','year','month' ,'day', 'tag', 'loc', 'headline', 'title', 'content']]\ndf.head()","b6422742":"print(f\"Data from only one year is present : df.year.unique() = {df.year.unique()}\")","998e9f16":"fig, ax = plt.subplots(1,1, figsize=(9, 5))\nsns.countplot(ax=ax, x=\"month\", data=df)\nplt.show()","000e8e81":"fig, ax = plt.subplots(1,1, figsize=(9, 5))\nsns.countplot(ax=ax, x=\"day\", data=df)\nplt.show()","bcf517d2":"fig, ax = plt.subplots(1,1, figsize=(8, 10))\nsns.countplot(ax=ax, \n              y=\"tag\", \n              data=df, \n              order=list(df.tag.value_counts().sort_values(ascending=False).index)[:20], \n              orient='h')\nplt.title(\"Most Popular tags\")\nplt.show()","4c25853d":"fig, ax = plt.subplots(1,1, figsize=(8, 10))\nsns.countplot(ax=ax, \n              y=\"tag\", \n              data=df, \n              order=list(df.tag.value_counts().sort_values(ascending=True).index)[1:20], \n              orient='h')\nplt.title(\"least Popular tags\")\nplt.show()","d54d142d":"fig, ax = plt.subplots(1,1, figsize=(8, 10))\nsns.countplot(ax=ax, \n              y=\"loc\", \n              data=df, \n              order=list(df[\"loc\"].value_counts().sort_values(ascending=False).index)[:20], \n              orient='h')\nplt.title(\"Most Popular Locations\")\nplt.show()","bed73e02":"fig, ax = plt.subplots(1,1, figsize=(8, 10))\nsns.countplot(ax=ax, \n              y=\"loc\", \n              data=df, \n              order=list(df['loc'].value_counts().sort_values(ascending=True).index)[:20], \n              orient='h')\nplt.title(\"least Popular locations\")\nplt.show()","773fd7ef":"print(*list(df['headline'].sample(n=10, random_state=1)), sep='\\n\\n')","7a8de614":"fig, ax = plt.subplots(1, 1, figsize=(9, 6))\nsns.distplot(df['headline'].apply(lambda x: len(x)).values)\nplt.title(\"charecter length of headlines\")\nplt.show()","15f359ad":"from wordcloud import WordCloud, STOPWORDS\n\nwordcloud = WordCloud(\n    width = 800, \n    height = 800, \n    background_color ='white', \n    stopwords = set(STOPWORDS), \n    min_font_size = 10)\n\nwc_img = wordcloud.generate(' '.join(df.headline))\nplt.figure(figsize=(8, 8), facecolor = None) \nplt.imshow(wc_img) \nplt.axis(\"off\")\nplt.tight_layout(pad=0) \nplt.show()","d376538d":"fig, ax = plt.subplots(1, 1, figsize=(9, 6))\nsns.distplot(df['content'].apply(lambda x: len(x)).values)\nplt.title(\"charecter length of content\")\nplt.show()","87c3133e":"from wordcloud import WordCloud, STOPWORDS\n\nwordcloud = WordCloud(\n    width = 800, \n    height = 800, \n    background_color ='white', \n    stopwords = set(STOPWORDS), \n    min_font_size = 10)\n\nwc_img = wordcloud.generate(' '.join(df.sample(1000).content))\nplt.figure(figsize=(8, 8), facecolor = None) \nplt.imshow(wc_img) \nplt.axis(\"off\")\nplt.tight_layout(pad=0) \nplt.show()","58affba9":"df.to_csv(\"data.csv\", index=False)","7e74067f":"import re\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nfrom nltk.corpus import stopwords","1a9ab5fc":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","538ca896":"# Convert to list\n#df = df.sample(5000)\ndata = df.content.values.tolist()\n\npprint(data[0])","af9907c5":"def sent_to_words(sentence):\n        return gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations\n\ndata_words = [sent_to_words(i) for i in tq(data)]\n\nprint(data_words[0])","98818edb":"# # Build the bigram and trigram models\n# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n# trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# # Faster way to get a sentence clubbed as a trigram\/bigram\n# bigram_mod = gensim.models.phrases.Phraser(bigram)\n# trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# # See trigram example\n# print(trigram_mod[bigram_mod[data_words[0]]])\n\n# def make_bigrams(texts):\n#     return [bigram_mod[doc] for doc in texts]\n\n# def make_trigrams(texts):\n#     return [trigram_mod[bigram_mod[doc]] for doc in texts]","497ba002":"# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in tq(texts)]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in tq(texts):\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","f8d06b9a":"# Remove Stop Words\ndata_words = remove_stopwords(data_words)\n\n# Form Bigrams\n#data_words = make_bigrams(data_words)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[0])","b82c54b4":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\n# Convert document into the bag-of-words (BoW) format = \n# list of (token_id, token_count) tuples.\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[0])","b53033f0":"# Build LDA model\nNUM_TOPICS = 6\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=NUM_TOPICS, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","a73ff367":"# Compute Perplexity\n# a measure of how good the model is. lower the better.\nprint('Perplexity: ', lda_model.log_perplexity(corpus))  \n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, \n                                     texts=data_lemmatized, \n                                     dictionary=id2word, \n                                     coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)","265f91f4":"# Print the Keyword in the topics\nfor i, j in lda_model.print_topics():\n    print(\"-\"*80, \"\\n\", i)\n    pprint(j)","5ac9f6e7":"doc_lda = list(lda_model.get_document_topics(corpus))\nprint(*doc_lda[:5], sep='\\n'+('-'*80)+'\\n')","e9714a4c":"res = np.zeros((len(doc_lda), NUM_TOPICS))\nfor num, i in enumerate(doc_lda):\n    for p, q in i:\n        res[num, p-1] = q\n\ndf[\"topic\"] = np.argmax(res, axis=1)+1\nfor i in range(1, 1+NUM_TOPICS):\n    df[\"score_\"+str(i)] = res[:,i-1]\ndf","9133669e":"fig, ax = plt.subplots(1,1, figsize=(9, 5))\nsns.countplot(ax=ax, x=\"topic\", data=df)\nplt.show()","bea0d589":"df.to_csv(\"final.csv\", index=False)","bcd6a18f":"### imports","056d7781":"### Tags","9d4f5a2d":"## EDA","b75ed458":"### get the year month day from the date part","c29bf13f":"### get the headline from title and cleanup the content","28d2e691":"### Title wordcloud","740bb285":"### Location","a9fde806":"### dataframe of title and content","75825c10":"### Content Wordcloud","ece34d34":"### final dataframe","77c62b6d":"### try getting location from first few words of content","ccece013":"### Month","7ffb8fea":"### Day","d074b232":"## Given Data","5e59998a":"### Content","7ea7e17a":"### sample headlines","960d021e":"## Data Preparation","bcd7add1":"### Year","60828c5f":"### get the date_part and tag from title","74247752":"### Headline","33dbcfee":"**Print different topics with the weights of constituent words**","0e216915":"## Topic Modeling with LDA\nref : [link](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/)"}}