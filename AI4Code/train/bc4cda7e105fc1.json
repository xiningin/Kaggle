{"cell_type":{"5e9c017e":"code","6770b545":"code","b66d18c7":"code","3d6327d1":"code","3bf001fd":"code","9b8b79b2":"code","f3c241e4":"code","f84f3376":"code","91fa7067":"code","7b9cadfb":"code","bf0f3549":"code","40774ed2":"code","fe6989a6":"code","b73c9e01":"code","f79da044":"code","6b988f54":"code","15eaecf6":"code","8ce3eb95":"code","3b0a276b":"code","41141fe3":"code","a9b359ec":"code","3217cabf":"code","85a42744":"code","f503a6d6":"code","4daaafdd":"code","75e83bc9":"code","fa490ad0":"code","959d1ea6":"code","9641f6da":"code","6ec25eca":"code","23243db2":"code","f6fb0040":"code","4ad3801d":"code","fb8ee450":"code","b4bfef99":"code","9205768c":"code","8b9fbcbd":"code","afe4be65":"code","694dbf6c":"code","f32eb214":"code","ae2deb67":"code","0819b5c5":"code","fc04de08":"code","e120e1b1":"markdown","1d692253":"markdown","f5924a32":"markdown","f9af386f":"markdown","52890dcd":"markdown","30b69d75":"markdown","754f73d4":"markdown","6f002023":"markdown","7390dc5e":"markdown","b9210c88":"markdown","d1f17fde":"markdown","8b1afa9b":"markdown","7149c53e":"markdown"},"source":{"5e9c017e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6770b545":"traindf=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntestdf=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmissiondf=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nprint(traindf.shape)\ntraindf.head()","b66d18c7":"print(testdf.shape)\ntestdf.head()","3d6327d1":"submissiondf.head()","3bf001fd":"traindf.info()","9b8b79b2":"traindf.describe()","f3c241e4":"for col in traindf.columns:\n    if traindf[col].isnull().values.any():\n        print(col,traindf[col].isna().sum())","f84f3376":"traindf['Embarked'].value_counts()","91fa7067":"print(traindf.shape)\ntraindf=traindf.dropna(axis=0, subset=['Embarked'])\ntraindf.head()","7b9cadfb":"traindf = pd.get_dummies( traindf, columns = ['Embarked'],prefix=\"EM\" )\ntestdf=pd.get_dummies(testdf,columns = ['Embarked'],prefix=\"EM\")\n#traindf.drop('Embarked', axis=1, inplace=True)\ntraindf.head()","bf0f3549":"traindf.drop('Cabin', axis=1, inplace=True)\ntestdf.drop('Cabin', axis=1, inplace=True)\ntraindf.head()","40774ed2":"plt.figure(figsize=(20,5))\ng = sns.distplot(traindf['Age'], bins= 50,color='black')\ng.set_xlabel(\"Age\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\nplt.show()","fe6989a6":"print(traindf['Age'].mean())","b73c9e01":"traindf['Age'].fillna(traindf['Age'].mean(), inplace=True)","f79da044":"traindf.head()","6b988f54":"## Plotting the bar char to identify the frequnecy of values\nsns.countplot(traindf[\"Sex\"],color='black')\n##prinitng number of values for each type\nprint(traindf[\"Sex\"].value_counts())","15eaecf6":"Sex_binary = {'male': 1,'female': 0}\ntraindf[\"Sex\"]= [Sex_binary[item] for item in traindf[\"Sex\"]]\ntestdf[\"Sex\"]= [Sex_binary[item] for item in testdf[\"Sex\"]]\ntraindf.head()","8ce3eb95":"print(traindf.Ticket.value_counts().count())","3b0a276b":"traindf.drop('Ticket', axis=1, inplace=True)\ntraindf.drop('Name', axis=1, inplace=True)\ntraindf.drop('PassengerId', axis=1, inplace=True)\n\ntestdf.drop('Ticket', axis=1, inplace=True)\ntestdf.drop('Name', axis=1, inplace=True)\n#testdf.drop('PassengerId', axis=1, inplace=True)\n\ntraindf.head()","41141fe3":"# Map each Age value to a numerical value: \nbins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Youth', 'Student', 'Young Adult', 'Adult', 'Senior']\ntraindf['AgeGroup'] = pd.cut(traindf[\"Age\"], bins, labels = mylabels)\ntraindf[[\"AgeGroup\",\"Survived\"]].groupby(\"AgeGroup\").mean()\nage_mapping = {'Baby': 1, 'Child': 2, 'Youth': 3, 'Student': 4, 'Young Adult':5 , 'Adult': 6, 'Senior':7}\ntraindf['AgeGroup'] = traindf['AgeGroup'].map(age_mapping)\n\ntestdf['AgeGroup'] = pd.cut(testdf[\"Age\"], bins, labels = mylabels)\nage_mapping = {'Baby': 1, 'Child': 2, 'Youth': 3, 'Student': 4, 'Young Adult':5 , 'Adult': 6, 'Senior':7}\ntestdf['AgeGroup'] = testdf['AgeGroup'].map(age_mapping)","a9b359ec":"traindf.drop('Age', axis=1, inplace=True)\ntestdf.drop('Age', axis=1, inplace=True)\ntraindf.head()","3217cabf":"plt.figure(figsize=(20,5))\ng = sns.distplot(traindf['Fare'], bins= 50,color='black')\ng.set_xlabel(\"Fare\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\nplt.show()","85a42744":" sns.boxplot(traindf[\"Fare\"], color='black')","f503a6d6":"traindf['FareRange'] = pd.qcut(traindf['Fare'], 10, labels = [1, 2, 3, 4,5,6,7,8,9,10])\ntestdf['FareRange'] = pd.qcut(testdf['Fare'], 10, labels = [1, 2, 3, 4,5,6,7,8,9,10])\ntraindf.drop('Fare', axis=1, inplace=True)\ntestdf.drop('Fare', axis=1, inplace=True)\ntraindf.head()","4daaafdd":"plt.figure(figsize=(20,5))\ng = sns.distplot(traindf['FareRange'], bins= 50,color='black')\ng.set_xlabel(\"FareRange\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\nplt.show()","75e83bc9":"## Plotting the bar char to identify the frequnecy of values\nsns.countplot(traindf[\"Survived\"], color='black')\n##prinitng number of values for each type\nprint(traindf[\"Survived\"].value_counts())","fa490ad0":"f,ax = plt.subplots(figsize=(20, 5))\nsns.heatmap(traindf.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","959d1ea6":"traindf.head()","9641f6da":"testdf.head()","6ec25eca":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = traindf.drop(['Survived'],axis=1)\n# Putting response variable to y\ny = traindf['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","23243db2":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree","f6fb0040":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisonTree', DecisionTreeClassifier()))\nmodels.append(('NB', BernoulliNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('BaggingDecisonTree', BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))))\nmodels.append(('Adaboost',AdaBoostClassifier()))\nmodels.append(('Logistic', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\n# evaluate each model in turn\nresults = []\nnames = []\nperformance=[]\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, X_train,y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tperformance.append(msg)\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(20,8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results,widths = 0.5)\nax.set_xticklabels(names)\nplt.show()\n\nfor perf in performance:\n    print(perf)","4ad3801d":"from sklearn.linear_model import LogisticRegression\n\nLR= LogisticRegression(penalty='none')\nLR= LR.fit(X_train,y_train)\ny_predict = LR.predict(X_test)","fb8ee450":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import metrics\n\ndef Metrics_func( actual, probs ):\n    print(classification_report(actual,probs))\n    print(\"accuracy\", metrics.accuracy_score(actual, probs))\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return None\n\nMetrics_func(y_test, y_predict)","b4bfef99":"from sklearn.svm import SVC # \"Support Vector Classifier\" \nclf = SVC(kernel='linear') \nclf= clf.fit(X_train,y_train)\ny_predict = clf.predict(X_test)\nMetrics_func(y_test, y_predict)","9205768c":"from sklearn.ensemble import BaggingClassifier\ndtc = DecisionTreeClassifier(criterion=\"entropy\")\nbag_model=BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)\nbag_model=bag_model.fit(X_train,y_train)\nY_pred=bag_model.predict(X_test)","8b9fbcbd":"Metrics_func(y_test, Y_pred)","afe4be65":"traindf.head()","694dbf6c":"for col in testdf.columns:\n    if testdf[col].isnull().values.any():\n        print(col,testdf[col].isna().sum())","f32eb214":"testdf['FareRange'].fillna(3, inplace=True)","ae2deb67":"testdf['AgeGroup'].fillna(5, inplace=True)","0819b5c5":"x=testdf.drop('PassengerId', axis=1)\nsubmission_df=bag_model.predict(x)","fc04de08":"PassengerId = np.array(testdf[\"PassengerId\"])\nSurvived = np.array(submission_df)\nsubmission_dataset = pd.DataFrame({'PassengerId': PassengerId, 'Survived': Survived}, columns=['PassengerId', 'Survived'])\nsubmission_dataset.to_csv('submission.csv', header=True, index=False) ","e120e1b1":"<h4>Lets Handle Age Feature<\/h4>","1d692253":"Fare is a continous number and varied to huge extent, instead of having it a number let's group the fare intervals into Bins using Binning","f5924a32":"<h4>Lets Handle Cabin feature<\/h4>\nwith 775 of data missing in this column, Deleting the column all together\n","f9af386f":"<h1> Modeling, Evaluation and Model Tuning <\/h1>","52890dcd":"<ul>\n    <li>Over 20% of data in Age column is missing <\/li>\n<li>Over 77% of data in Cabin is missing<\/li>\n<li>Only 2 entries are Embarked is missing<\/li>\n<\/ul>\n\nLets handle Embarked feature Null entires first","30b69d75":"### Analysing and understanding data","754f73d4":"### 2.SVM","6f002023":"### 3. Bagging Classifer","7390dc5e":"**Target Variable is \"Survived\"**\n\n##### Basically the problem is a classification problem and we need to classify if the passenger survived(1) or not survived(0)","b9210c88":"#### As per above data, **Logistic Regression and SVM models** gave better results, so considering them as challening models and tuning them indivually to get champion model","d1f17fde":"<h1> Exploratory Data Analysis <\/h1>","8b1afa9b":"### 1. Logistic Regression ","7149c53e":"There are 680 unique values in Ticket column so may not be useful, so removing the column for now.<br\/>\nSimilarly deleting Name column too"}}