{"cell_type":{"643441d9":"code","0b23d7b6":"code","37be9833":"code","725ce83c":"code","20f72c5c":"code","5c11699b":"code","3f507c0c":"code","ad7a9bc2":"code","b20544bc":"code","2727ed78":"code","5cd80a5f":"code","c6312c1b":"code","15bd4596":"code","936b8f8b":"code","462138af":"code","7d997263":"code","c620b5bc":"code","93f50c3b":"code","fde4e161":"code","3832f23a":"code","056f111c":"code","6f09daac":"code","60597473":"code","24794095":"code","0a420c35":"code","4c0f92b8":"code","81c844ab":"code","09ecbbb8":"code","4afabec8":"code","59823e1c":"code","d86a929e":"code","db9d3ae6":"code","7be2bc8f":"code","7e7b661e":"code","4795a457":"code","e00d671b":"markdown","e7f21859":"markdown","caefd8ff":"markdown","ed64f16e":"markdown","85be18ae":"markdown","bc23618e":"markdown","f0533178":"markdown","a85968bd":"markdown","c9b2fb2f":"markdown","64a7e6b5":"markdown","2fece100":"markdown","8fb0466b":"markdown","8e3d384c":"markdown","0ff22775":"markdown","d7b0f846":"markdown","3a1d09a5":"markdown","1134cce1":"markdown","e3697528":"markdown","f4ea2718":"markdown","36f12110":"markdown","e7cfaa9d":"markdown","2a7e24b3":"markdown","f2e3a748":"markdown","6c2ce620":"markdown","56c3d3e1":"markdown"},"source":{"643441d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n#For Preprocessing\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport re #regex for removing non-letter characters\nimport nltk  #natural language processing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\n\n#For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#For training model\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\ngpu_devices=tf.config.experimental.list_physical_devices(\"GPU\")\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device,True)\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","0b23d7b6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","37be9833":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","725ce83c":"train_data.head(10)","20f72c5c":"train_data.info()","5c11699b":"# count null values\ntrain_data.isnull().sum()","3f507c0c":"train_data.describe()","ad7a9bc2":"for col in train_data.columns:\n    print(f\"{col}: {len(train_data[col].unique())}\")","b20544bc":"train_data['license'].value_counts()","2727ed78":"plt.figure(figsize=(12, 6))\nsns.countplot(data= train_data, y= 'license', saturation=0.2, color=\"blue\")\nplt.title('Types of License')\nplt.show();","5cd80a5f":"# Extract all url's\nurl_list = train_data['url_legal'].dropna().apply(lambda x : re.findall('https?:\/\/([A-Za-z_0-9.-]+).*',x)[0])\nurl_list = [url for url in url_list]\nurl_list[:10]\n# count url's and sort them descending order \nurls_counts = Counter(url_list)\nurls_counts_sorted = sorted(urls_counts.items(), key=lambda pair: pair[1], reverse=True)\nurls_counts_df = pd.DataFrame(urls_counts_sorted, columns=['sites', 'counts'])\nurls_counts_df","c6312c1b":"site = urls_counts_df['sites'].head(20)\ncount = urls_counts_df['counts'].head(20)\n \n# Figure Size\nfig, ax = plt.subplots(figsize =(16, 9))\n \n# Horizontal Bar Plot\nax.barh(site, count)\n \n# Remove axes splines\nfor s in ['top', 'bottom', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n \n\n# Remove x, y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n \n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad = 5)\nax.yaxis.set_tick_params(pad = 10)\n \n# Add x, y gridlines\nax.grid(b = True, color ='grey',\n        linestyle ='-.', linewidth = 0.5,\n        alpha = 0.2)\n \n# Show top values\nax.invert_yaxis()\n \n# Add annotation to bars\nfor i in ax.patches:\n    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n             str(round((i.get_width()), 2)),\n             fontsize = 10, fontweight ='bold',\n             color ='grey')\n \n# Add Plot Title\nax.set_title('Unique Sites count',\n             loc ='left', )\n \n# Add Text watermark\nfig.text(0.9, 0.15, 'kritanjalijain', fontsize = 12,\n         color ='grey', ha ='right', va ='bottom',\n         alpha = 0.7)\n \n# Show Plot\nplt.show()","15bd4596":"print(\"First example from train dataset: \\n\")\nprint(train_data.excerpt[0])","936b8f8b":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train_data['target'], kde= True, ax=ax[0])\nsns.histplot(train_data['standard_error'], kde= True, ax=ax[1])\nax[0].set_title(\"Target Distribution\")\nax[1].set_title(\"Standard Error Distribution\")\nplt.show();\nprint(train_data.target.describe())\nprint(\"-\"*50)\nprint(train_data.standard_error.describe())","462138af":"# Top 2 excerpts with lowest target\n\nmin_targets = sorted(train_data['target'])[:2]\nfor min_target in min_targets:\n    print(\"Target:\", train_data[train_data['target'] == min_target].iloc[0,4])\n    print(train_data[train_data['target'] == min_target].iloc[0,3])\n    print(\"-\" * 100)","7d997263":"# Top 2 excerpts with highest target\n\nmax_targets = sorted(train_data['target'])[-2:]\nfor max_target in max_targets:\n    print(\"Target:\", train_data[train_data['target'] == max_target].iloc[0,4])\n    print(train_data[train_data['target'] == max_target].iloc[0,3])\n    print(\"-\" * 100)","c620b5bc":"def wordcloud_draw(data, color = 'white'):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                          background_color = color,\n                          width = 3000,\n                          height = 2000\n                         ).generate(' '.join(data))\n    plt.figure(1, figsize = (12, 8))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nlowscore_excerpts_words = []\n\nfor _, row in train_data.sort_values('target').head(500).iterrows():\n    lowscore_excerpts_words.extend(row['excerpt'].split())\n\nprint(\"Wordcloud for 500 excerpts with lowest targets:\")\nwordcloud_draw(lowscore_excerpts_words, color='black')\n\nhighscore_excerpts_words = []\n\nfor _, row in train_data.sort_values('target').tail(500).iterrows():\n    highscore_excerpts_words.extend(row['excerpt'].split())\nprint(\"-\" * 100)\nprint(\"\\nWordcloud for 500 excerpts with highest targets:\")\nwordcloud_draw(highscore_excerpts_words, color='black')","93f50c3b":"from nltk.stem import PorterStemmer\ndef excerpt_to_words(excerpt):\n    ''' Convert excerpt text into a sequence of words '''\n    \n    # convert to lowercase\n    text = excerpt.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words\n\nwords_list = excerpt_to_words(''.join(sents for sents in train_data['excerpt']))\ns=words_list[:10]\n\n\nwords_list_freq = Counter(words_list)\nwords_list_freq_sorted = sorted(words_list_freq.items(), key=lambda pair: pair[1], reverse=True)\n\nwords_list_freq_sorted_df = pd.DataFrame(words_list_freq_sorted, columns=['words', 'counts'])[:30]\nwords_list_freq_sorted_df.head() ","fde4e161":"print(\"\\nOriginal excerpt ->\", train_data['excerpt'][0])\nprint(\"\\nProcessed excerpt ->\", excerpt_to_words(train_data['excerpt'][0]))","3832f23a":"X = list(map(excerpt_to_words, train_data['excerpt']))","056f111c":"word = words_list_freq_sorted_df['words'].head(20)\ncount = words_list_freq_sorted_df['counts'].head(20)\n \n# Figure Size\nfig, ax = plt.subplots(figsize =(16, 9))\n \n# Horizontal Bar Plot\nax.barh(word, count)\n \n# Remove axes splines\nfor s in ['top', 'bottom', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n \n\n# Remove x, y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n \n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad = 5)\nax.yaxis.set_tick_params(pad = 10)\n \n# Add x, y gridlines\nax.grid(b = True, color ='grey',\n        linestyle ='-.', linewidth = 0.5,\n        alpha = 0.2)\n \n# Show top values\nax.invert_yaxis()\n \n# Add annotation to bars\nfor i in ax.patches:\n    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n             str(round((i.get_width()), 2)),\n             fontsize = 10, fontweight ='bold',\n             color ='grey')\n \n# Add Plot Title\nax.set_title('Top 20 frequent words and no. of times they occured',\n             loc ='left', )\n \n# Add Text watermark\nfig.text(0.9, 0.15, 'kritanjalijain', fontsize = 12,\n         color ='grey', ha ='right', va ='bottom',\n         alpha = 0.7)\n \n# Show Plot\nplt.show()","6f09daac":"targets=np.array(train_data['target'])\nexcerpt_text=np.array(train_data['excerpt'])","60597473":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.excerpt)  \nvocab_size = len(tokenizer.word_index) + 1 \nmax_length = 200","24794095":"sequences_train = tokenizer.texts_to_sequences(excerpt_text) \n#sequences_test = tokenizer.texts_to_sequences(test_data.excerpt) \n\nX_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\n#X_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\ny_train = train_data.target.values\n#y_test = test_data.target.values","0a420c35":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector","4c0f92b8":"embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)","81c844ab":"num_epochs = 100\n#batch_size = 1000","09ecbbb8":"model=tf.keras.models.Sequential([\n\n    embedding_layer,\n    #tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(filters=64,kernel_size= 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    tf.keras.layers.Bidirectional(LSTM(64)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(16,activation='relu'),\n    tf.keras.layers.Dense(1,activation='linear'),\n\n])\nmodel.summary()","4afabec8":"import tensorflow as tf\ntf.keras.utils.plot_model(model, show_shapes=True)","59823e1c":"selected_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-05)\nselected_loss=tf.keras.losses.MeanSquaredError()\n\nmodel.compile(optimizer=selected_optimizer,loss=selected_loss)","d86a929e":"savedmodel_filepath='.\/model_1.h5'\nearly_stopping=EarlyStopping(patience=10,monitor='val_loss')\nreduce_lr=ReduceLROnPlateau(monitor='val_loss',min_lr=0.00001,patience=3,mode='min',verbose=1)\nmodel_checkpoint=ModelCheckpoint(monitor='val_loss',filepath=savedmodel_filepath,\n                                 save_best_only=True)\n\n\nselected_callbacks=[early_stopping]\n\nhistory=model.fit(X_train,y_train,epochs=num_epochs,verbose=1,callbacks=selected_callbacks)\n\n","db9d3ae6":"model.save('model_1.h5')\nprint(\"Model 1 saved\")\n","7be2bc8f":"excerpt_test=np.array(test_data['excerpt'])\ntest_data.head(10)","7e7b661e":"sequences_test = tokenizer.texts_to_sequences(excerpt_test) \n\nX_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\nprediction_testdata = model.predict(X_test)\n\ntest_data[\"target\"]= prediction_testdata\ntest_data.head(10)","4795a457":"submission[\"target\"]=prediction_testdata\nsubmission.to_csv('submission.csv',index=False)","e00d671b":"Let's convert the passages to a string of partially preprocessed words i.e. without punctuations, stopwords and in lowercase sorted by the number of times the word occured in ascending order.","e7f21859":"\nThere are three files have been provided with us in this competition: \n\n* `train.csv`: The CSV file containing all the training excerpts as well as their corresponding metadata, such as their ID and their target complexities.\n* `test.csv`: The CSV file containing the excerpts that will be used for testing purposes.\n* `sample_submission.csv`: The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column.\n\nLet's read these files.","caefd8ff":"Let's look at the distribution of the target and standard error.","ed64f16e":"Thus, we realise that there are 2834 entries in our dataset with no missing values except for legal and license information.","85be18ae":"## Loading and Exploring dataset","bc23618e":"## Understanding the License Distribution","f0533178":"## Preprocessing","a85968bd":"\n## Installing and importing dependencies\n\nFirst let us import all the modules and packages that will be required.","c9b2fb2f":"## Understanding the URLs distribution","64a7e6b5":"Words present in lower scoring excerpts such as government, light, matter and current stand out the most.\n\nWords present in higher scoring excerpts such as said, went, little and children stand out the most.","2fece100":"Now, let's look at an excerpt.","8fb0466b":"## Understanding the Excerpts","8e3d384c":"#### Visualizing wordcloud of the top 500 excerpts with the highest and lowest target scores:","0ff22775":"The standard deviation of `standard_error` variable is small, 0.034. Low standard error means that multiple rating systems mostly agreed regarding the ease of readability score and high standard error means that ratings from multiple rating systems are scattered.","d7b0f846":"Thus, we can conclude that the higher target scoring excerpts have a lower reading complexity i.e. difficult words and more complex sentences than excerpts with lower target scores.","3a1d09a5":"## Defining Model","1134cce1":"#### Let's look at the texts with highest and lowest readability.","e3697528":"#### Printing first 10 rows of the train data:","f4ea2718":"Let's see the most commonly occuring words in the excerpts (except for stopwords)","36f12110":"\n## Overview\n\nThis script performs EDA on `train.csv` and then preprocesses it to train a model which is in turn used to build algorithms to rate the complexity of reading passages for grade 3-12 classroom.\n","e7cfaa9d":"The dataset has the following columns:\n * `id` : Denotes the id of the excerpt\n * `legal_url` : Denotes the URL of the site from where the excerpt was taken\n * `license` : Denotes the License under which the excerpt lies\n * `excerpt` : The text data (readings)\n * `target` : Denotes the ease of readability score \n * `standard_error` : Denotes the corresponding standard error of the target measure across different rating scores","2a7e24b3":"Thus, we can conclude that all targets and standard errors are unique in the dataset.\n\nAlso, out of 2834 excepts, 830 excerpts have a license with only 15 unique licenses (16th value refers to NaN).\n","f2e3a748":"Targets follow a normal distribution centered at -1. It is apparent that negative targets are more common than positive ones, with the training range going from -3.676 up to 1.711 with 1.033 as standard deviation.","6c2ce620":"Let us jot down the different types of license and the number of excerpts they apply to.","56c3d3e1":"#### Printing the number of unique values of each column in the dataset:"}}