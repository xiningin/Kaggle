{"cell_type":{"216bffd3":"code","f1f23969":"code","900ad265":"code","c6b399a3":"code","2d54b647":"code","c8306cb1":"code","2d26a34b":"code","91b4cc6b":"code","8d585861":"code","d86e3f9c":"code","452444d4":"markdown","736f57da":"markdown","5954a12d":"markdown","e89eb4a8":"markdown","a42307ea":"markdown","028b1b01":"markdown","6cf35ad9":"markdown","38a579ea":"markdown","b21dd333":"markdown","e9958201":"markdown","d528fd27":"markdown"},"source":{"216bffd3":"!pip install -q nnAudio","f1f23969":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nimport torch\nfrom torch.utils.data import Dataset\nfrom nnAudio.Spectrogram import CQT # CQT is an alias of CQT1992v2\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","900ad265":"num_samples = 4 # first N samples to process","c6b399a3":"class G2NetDataset(Dataset):\n    def __init__(self, paths, targets, use_filter=True): \n        self.paths = paths\n        self.targets = targets\n        self.use_filter = use_filter\n        if self.use_filter:\n            self.bHP, self.aHP = signal.butter(8, (20, 500), btype='bandpass', fs=2048)\n\n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):      \n        waves = np.load(self.paths[index])\n        waves = np.concatenate(waves, axis=0)\n        if self.use_filter:\n            waves *= signal.tukey(4096*3, 0.2)\n            waves = signal.filtfilt(self.bHP, self.aHP, waves)\n        waves = waves\/np.max(waves)\n        targets = self.targets[index]\n                \n        return {\n            \"waves\": torch.tensor(waves, dtype=torch.float),\n            \"target\": torch.tensor(targets, dtype=torch.long),\n        }","2d54b647":"ROOT_DIR = '..\/input\/g2net-gravitational-wave-detection'\ndf = pd.read_csv(os.path.join(ROOT_DIR, 'training_labels.csv'))\ndf['path'] = df['id'].apply(lambda x: f'{ROOT_DIR}\/train\/{x[0]}\/{x[1]}\/{x[2]}\/{x}.npy')","c8306cb1":"transform = CQT(sr=2048,        # sample rate\n                fmin=20,        # min freq\n                fmax=1024,      # max freq (set to Nyquist frequency)\n                hop_length=64,  # hop length\n                verbose=False)  \n\nds = G2NetDataset(df['path'], df['target'], use_filter=False)\nds_f = G2NetDataset(df['path'], df['target'], use_filter=True)\n\nwaves = []\nwaves_f = []\ncqts = []\ncqts_f = []\nfor i in range(num_samples):\n    waves.append(ds.__getitem__(i)['waves'])\n    waves_f.append(ds_f.__getitem__(i)['waves'])\n    cqts.append(transform(waves[i]).squeeze())\n    cqts_f.append(transform(waves_f[i]).squeeze())","2d26a34b":"fig, axs = plt.subplots(num_samples)\nfig.set_figheight(15)\nfig.set_figwidth(15)\nfor i in range(num_samples):\n    nid = df['id'][i]\n    ntarget = df['target'][i]\n    axs[i].title.set_text(f'{nid}.npy, target: {ntarget}')\n    axs[i].plot(waves[i])","91b4cc6b":"fig, axs = plt.subplots(num_samples)\nfig.set_figheight(15)\nfig.set_figwidth(15)\nfor i in range(num_samples):\n    nid = df['id'][i]\n    ntarget = df['target'][i]\n    axs[i].title.set_text(f'{nid}.npy, target: {ntarget}')\n    axs[i].pcolormesh(cqts[i])","8d585861":"fig, axs = plt.subplots(num_samples)\nfig.set_figheight(15)\nfig.set_figwidth(15)\nfor i in range(num_samples):\n    nid = df['id'][i]\n    ntarget = df['target'][i]\n    axs[i].title.set_text(f'{nid}.npy, target: {ntarget}')\n    axs[i].plot(waves_f[i])","d86e3f9c":"fig, axs = plt.subplots(num_samples)\nfig.set_figheight(15)\nfig.set_figwidth(15)\nfor i in range(num_samples):\n    nid = df['id'][i]\n    ntarget = df['target'][i]\n    axs[i].title.set_text(f'{nid}.npy, target: {ntarget}')\n    axs[i].pcolormesh(cqts_f[i])","452444d4":"This notebook shows how Constant Q-transform from waves can be calculated with usage of nnAudio package (https:\/\/github.com\/KinWaiCheuk\/nnAudio). \n\nnnAudio is an audio processing toolbox using PyTorch convolutional neural network as its backend.\n\nHave any questions or suggestions? Please comment below.\n\n**<font color='red'>And if you liked this notebook, please upvote it!<\/font>**\n\n**Changelog**\n* v6 - number of processed samples can be now easily changed via num_samples variable\n* v5 - added Tukey window\n* v4 - added a bandpass filter (idea taken from https:\/\/www.kaggle.com\/c\/g2net-gravitational-wave-detection\/discussion\/261721#1458564) + wave plots\n* v3 - small markup fix :)\n* v2 - changed CQT1992v2 to CQT (alias)","736f57da":"Let's define a dataset to work with.","5954a12d":"## Demonstrate CQT usage","e89eb4a8":"You can use CQT() as your model block to convert waves to CQT on-the-fly.","a42307ea":"## Import packages","028b1b01":"nnAudio has several implementations of CQT; we will use the recommended one - CQT1992v2 (see https:\/\/github.com\/KinWaiCheuk\/nnAudio\/blob\/master\/Installation\/nnAudio\/Spectrogram.py for details). You can simply call CQT, since it's an alias of CQT1992v2. If you want other CQT version, you'll have to import it directly.\n\nLet's calculate CQT for 4 first signals with and without usage of a bandpass filter (20-500Hz), and plot results!","6cf35ad9":"### Without a filter","38a579ea":"### With a filter with Tukey window","b21dd333":"## Read training labels","e9958201":"Now we read training labels data, and get npy paths.","d528fd27":"## Define dataset"}}