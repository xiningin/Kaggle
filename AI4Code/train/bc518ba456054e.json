{"cell_type":{"2085a0a0":"code","b56f0833":"code","34ec76a7":"code","de7b7911":"code","7a437c33":"code","567a0170":"code","4381006e":"code","8aaa7eae":"code","ae4eca7f":"code","d49e8b3e":"code","10a72c9b":"code","4e18db8f":"code","aada4c1d":"code","673445c9":"code","f5ee8d00":"code","ce958036":"code","7fb34eaf":"code","c22fde37":"code","c5f91602":"code","1f29b55e":"code","1f995b4d":"code","b3bc0027":"code","7037bb83":"code","19d7abfa":"code","78dfa3f2":"code","2303ecc4":"code","89b9ae8f":"code","2b137511":"code","1fd1ce59":"code","81ae47ff":"code","9f8a5dbb":"code","c8a2348a":"code","05510aaa":"markdown","5298d9a7":"markdown","b2652975":"markdown","dcd3ca63":"markdown","14a0010d":"markdown","52faa686":"markdown","70c306a8":"markdown","640845ad":"markdown","04a85bf2":"markdown","5e296dbe":"markdown","0c9b6cdd":"markdown","311610f8":"markdown","e014c7d4":"markdown","ec0c3ef7":"markdown","10ec4218":"markdown","23fae3eb":"markdown","598fce20":"markdown","7206bd6d":"markdown","161f186c":"markdown","866ca18a":"markdown","1a2e4e27":"markdown","acdc2be0":"markdown","332304b6":"markdown","7d2c9862":"markdown","7d5d3103":"markdown","88746c89":"markdown","85d6fff3":"markdown","e88743dd":"markdown"},"source":{"2085a0a0":"# Load libraries\nimport sys\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle","b56f0833":"# Load CSV using Pandas from URL\nurl = \"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/pima-indians-diabetes.data.csv\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = pd.read_csv(url, names=names)","34ec76a7":"# dimension (rows, columns)\ndata.shape","de7b7911":"# head (first 10 rows)\ndata.head(10)","7a437c33":"data.info()","567a0170":"# descriptive stats\ndata.describe()","4381006e":"# data types\ndata.dtypes","8aaa7eae":"# distribution of an attribute (e.g. \"class\")\ndata.groupby('class').size()","ae4eca7f":"# pairwise correlation between attributes\ndata.corr()","d49e8b3e":"# missing values\ndata.isnull().sum()","10a72c9b":"# Dependent variable -- 'class'\nsns.countplot(data['class'])","4e18db8f":"# Distribution of attribute -- \"age\"\nf = plt.figure(figsize=(20,4))\nf.add_subplot(1,2,1)\nsns.distplot(data['age'])\nf.add_subplot(1,2,2)\nsns.boxplot(data['age'])","aada4c1d":"# distribution of 2 attributes to compare their shapes\nf = plt.figure(figsize=(20,4))\nf.add_subplot(1,3,1)\nsns.countplot(data['age'], color='red')\nf.add_subplot(1,3,2)\nsns.countplot(data['preg'], color='yellow')\nf.add_subplot(1,3,3)\nsns.countplot(data['pres'], color='green')","673445c9":"# histograms\ndata.hist()\nplt.show()","f5ee8d00":"# box and whisker plots\ndata.plot(kind = 'box')\nplt.show()","ce958036":"# scatter plot matrix\nscatter_matrix(data)\nplt.show()","7fb34eaf":"# Standardize data (0 mean, 1 stdev)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(x)\nrescaledX = scaler.transform(x)\n\n# Show head of transformed data\n(pd.DataFrame(rescaledX)).head(5)","c22fde37":"# Create x (independent, input) + y (dependent, output) variables\nx = data.drop(columns=['class'])\ny = data['class']\n\n# Split train\/validation datasets (80-20%)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=7)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","c5f91602":"x_test.shape, y_test.shape","1f29b55e":"# Prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RF', RandomForestClassifier(n_estimators=100, max_features=3)))","1f995b4d":"# Evaluate each model's accuracy on the validation set\nprint('Cross Validation Score: Mean accuracy & SD')\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\n\tcv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint('%s: %.2f%% (%.3f)' % (name, cv_results.mean()*100, cv_results.std()))\n    \n# Visualize model comparison\nplt.boxplot(results, labels=names)\nplt.title('Model Comparison: Cross Validation Score')\nplt.show()","b3bc0027":"# Evaluate 1 model using Cross Validation\nkfold = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\nKNNresults = cross_val_score(KNeighborsClassifier(), x_train, y_train, cv=kfold, scoring='accuracy')\nprint('KNN: %.2f%% (%.3f)' % (KNNresults.mean()*100, KNNresults.std()))\n\n# Visualize model\nplt.boxplot(KNNresults)\nplt.show()","7037bb83":"print('Train Set Performance Metrics: Accuracy & ROC')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_train_pred = trained_model.predict(x_train)\n    print('%s: %.2f%% (%.3f)' % (name, accuracy_score(y_train, y_train_pred)*100, (roc_auc_score(y_train, y_train_pred))))","19d7abfa":"print('Test Set Performance Metrics: Accuracy & ROC')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_test_pred = trained_model.predict(x_test)\n    print('%s: %.2f%% (%.3f)' % (name, accuracy_score(y_test, y_test_pred)*100, (roc_auc_score(y_test, y_test_pred))))","78dfa3f2":"# Train model\nknn = KNeighborsClassifier().fit(x_train, y_train)\n\n\n# Predict y on train set\ny_train_pred = knn.predict(x_train)\n\n# Train set performance metrics\nprint('Train Set Performance Metrics: Accuracy & ROC')\nprint('%.2f%% (%.3f)' % (accuracy_score(y_train, y_train_pred)*100, roc_auc_score(y_train, y_train_pred)))\n\n# Predict y on test set\ny_test_pred = knn.predict(x_test)\n\n# Test set performance metrics\nprint('Train Set Performance Metrics: Accuracy & ROC')\nprint('%.2f%% (%.3f)' % (accuracy_score(y_test, y_test_pred)*100, roc_auc_score(y_test, y_test_pred)))\n\n# Confusion matrix\nprint('Confusion Matrix: \\n %s' % (confusion_matrix(y_test, y_test_pred)))\n\n# Classification report\nprint(classification_report(y_test, y_test_pred))","2303ecc4":"# K-Nearest Neighbors (KNN)\n#Create dictionary of hyperparameters that we want to tune\nknn_params = {\n    'n_neighbors':[1,3,5,7,9,11,15,17,19],\n    'weights':['uniform', 'distance'],\n    'metric':['euclidean', 'manhattan'],\n    'leaf_size':list(range(1,50)),\n    'p':[1,2,3]\n}\n\n# Create new KNN object using GridSearch\ngrid_knn = GridSearchCV(KNeighborsClassifier(), knn_params, cv=10)\n\n#Fit the model\nbest_model_knn = grid_knn.fit(x_train, y_train)\n\n# Print the value of best hyperparameters\nprint('Best n_neighbors:', best_model_knn.best_estimator_.get_params()['n_neighbors'])\nprint('Best n_neighbors:', best_model_knn.best_estimator_.get_params()['weights'])\nprint('Best n_neighbors:', best_model_knn.best_estimator_.get_params()['metric'])\nprint('Best leaf_size:', best_model_knn.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model_knn.best_estimator_.get_params()['p'])\n#print(best_model_knn.best_params_)","89b9ae8f":"# Ridge Regression (RR)\n#List hyperparameters that we want to tune\nalpha = np.array([1,0.1,0.01,0.001,0.0001,0])\n\n#Convert to dictionary\nhyperparameters_ridge = dict(alpha=alpha)\n\n# Create new Ridge object using GridSearch\ngrid_ridge = GridSearchCV(Ridge(), hyperparameters_ridge, cv=kfold)\n\n#Fit the model\nbest_model_ridge = grid_ridge.fit(x,y)\n\n#Print the value of best hyperparameters\nprint('Best score:', best_model_ridge.best_score_)\nprint('Best alpha:', best_model_ridge.best_estimator_.get_params()['alpha'])","2b137511":"print(best_model_knn.best_params_)","1fd1ce59":"# Predict y on train set\ny_train_pred_2 = best_model_knn.predict(x_train)\n\n# Train set performance metrics\nprint('Train Set Performance Metrics: Accuracy & ROC')\nprint('%.2f%% (%.3f)' % (accuracy_score(y_train, y_train_pred_2)*100, roc_auc_score(y_train, y_train_pred_2)))\n\n# Predict y on test set\ny_test_pred_2 = best_model_knn.predict(x_test)\n\n# Test set performance metrics\nprint('Train Set Performance Metrics: Accuracy & ROC')\nprint('%.2f%% (%.3f)' % (accuracy_score(y_test, y_test_pred_2)*100, roc_auc_score(y_test, y_test_pred_2)))\n\n\n# Confusion matrix\nprint('Confusion Matrix: \\n %s' % (confusion_matrix(y_test, y_test_pred_2)))\n\n# Classification report\nprint(classification_report(y_test, y_test_pred_2))","81ae47ff":"# Save model to disk\nFinalModel_KNN = 'FinalModel.sav'\npickle.dump(best_model_knn, open(FinalModel_KNN, 'wb'))","9f8a5dbb":"# Load model from disk\nLoad_FinalModel = pickle.load(open(FinalModel_KNN, 'rb'))","c8a2348a":"# Apply model to new dataset to make predictions\nresult = Load_FinalModel.score(x_test, y_test)\nprint('Accuracy: %.3f%%' % (result*100))","05510aaa":"## Understand data with descriptive statistics","5298d9a7":"### Evaluate Model Performance (1)","b2652975":"# Import Libraries","dcd3ca63":"---","14a0010d":"**Model Types**\n\n**- Linear models:**\n* Logistic Regression (LR)\n* Linear Discriminant Analysis (LDA)\n\n**- Nonlinear models:**\n* K-Nearest Neighbors (KNN)\n* Classification and Regression Trees (CART)\n* Gaussian Naive Bayes (NB)\n* Support Vector Machines (SVM)\n* Ridge Regression (RR)\n\n**- Bagging ensemble models:**\n* Random Forest (RF)\n\n---","52faa686":"## Iteration (1)","70c306a8":"#### *EXTRA: optimize only one model*","640845ad":"Performance metrics on ***test set***:","04a85bf2":"#### *EXTRA: deep error check on individual model*","5e296dbe":"### Evaluate Model Performance (2)","0c9b6cdd":"#### *Additional Example for Tuning Hyperparameters*","311610f8":"# Modeling","e014c7d4":"### Optimize Models by Fitting Parameters (1)\n\nTrain models on train set to find the best parameters with cross validation & get the first performance measures on the validation set","ec0c3ef7":"For the purpose of understanding Hyperparameter Tuning, let's choose K-Nearest Neighbours (KNN).","10ec4218":"### Multivariate plots to understand relationship between attributes","23fae3eb":"# Save & Use Model","598fce20":"Load the Pima Indians onset of diabetes dataset using Pandas directly from the UCI Machine Learning Repository","7206bd6d":"https:\/\/machinelearningmastery.com\/python-machine-learning-mini-course\/","161f186c":"Performance metrics on ***train set***:","866ca18a":"Split dataset into train\/test set:","1a2e4e27":"### Optimize Models by Tuning Hyperparameters (2)","acdc2be0":"**5 Levels of ML Model Iteration:**\n1. Fitting Parameters\n2. Tuning Hyperparameters\n3. Feature Engineering\n\n---","332304b6":"## Understand data with visualization","7d2c9862":"# Preprocess Data","7d5d3103":"### Univariate plots to understand each individual attribute","88746c89":"# Load dataset","85d6fff3":"## Iteration (2)","e88743dd":"# Exploratory Data Analysis (EDA)"}}