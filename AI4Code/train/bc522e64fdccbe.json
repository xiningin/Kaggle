{"cell_type":{"2c9a5920":"code","69b9f2ed":"code","7fdd1526":"code","09fda6fb":"code","fa048598":"code","0a522189":"code","06944ae9":"code","cbd1bf75":"markdown","f389b4d5":"markdown","de462518":"markdown","f9349e0e":"markdown","193ca639":"markdown","0d4da265":"markdown","f71dd569":"markdown"},"source":{"2c9a5920":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss","69b9f2ed":"def PrepareFeatures(TestOrTrain):\n    source = f'..\/input\/{TestOrTrain}.json'\n    data = pd.read_json(source)\n    ulimit = np.percentile(data.price.values, 99)\n    data['price'][data['price']>ulimit] = ulimit\n    data['hasDesc'] = data['description'].apply(lambda x: len(x.strip())!=0)\n    data[\"nFeatures\"] = data[\"features\"].apply(len)\n    data[\"nDescWords\"] = data[\"description\"].apply(lambda x: len(x.split(\" \")))\n    data['nPhotos'] = data['photos'].apply(lambda x: min(10, len(x)))\n    data['created'] = pd.to_datetime(data['created'])\n    data['month'] = data['created'].dt.month\n    data['weekday'] = data['created'].apply(lambda x: x.weekday())\n    \n    if TestOrTrain == 'train':\n        interest_level_map = {'low': 0, 'medium': 1, 'high': 2}\n        data['interest_level'] = data['interest_level'].apply(lambda x: interest_level_map[x])\n    return data\n\ndef CreateCategFeat(data, features_list):\n    f_dict = {'hasParking':['parking', 'garage'], 'hasGym':['gym', 'fitness', 'health club'],\n              'hasPool':['swimming pool', 'pool'], 'noFee':['no fee', \"no broker's fees\"],\n              'hasElevator':['elevator'], 'hasGarden':['garden', 'patio', 'outdoor space'],\n              'isFurnished': ['furnished', 'fully  equipped'], \n              'reducedFee':['reduced fee', 'low fee'],\n              'hasAC':['air conditioning', 'central a\/c', 'a\/c', 'central air', 'central ac'],\n              'hasRoof':['roof', 'sundeck', 'private deck', 'deck'],\n              'petFriendly':['pets allowed', 'pet friendly', 'dogs allowed', 'cats allowed'],\n              'shareable':['shares ok'], 'freeMonth':['month free'],\n              'utilIncluded':['utilities included']}\n    for feature in features_list:\n        data[feature] = False\n        for ind, row in data.iterrows():\n            for f in row['features']:\n                f = f.lower().replace('-', '')\n                if any(e in f for e in f_dict[feature]):\n                    data.at[ind, feature]= True","7fdd1526":"data = PrepareFeatures('train')\ncat_features = ['hasParking', 'hasGym', 'hasPool', 'noFee', 'hasElevator',\n                'hasGarden', 'isFurnished', 'reducedFee', 'hasAC', 'hasRoof',\n                'petFriendly', 'shareable', 'freeMonth', 'utilIncluded']\nCreateCategFeat(data, cat_features)\nfeatures = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n             \"nPhotos\", \"hasDesc\", 'nFeatures', 'nDescWords', \"month\", 'weekday']\nfeatures.extend(cat_features)\nX = data[features]\ny = data[\"interest_level\"]","09fda6fb":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05)\nparam = {'eval_metric':\"mlogloss\", 'eta':0.02, 'objective':'multi:softprob', 'silent':1,\n        'max_depth':10, 'num_class':3, 'subsample':0.7, 'colsample_bytree':0.7}\nnum_rounds = 500\nxgtrain = xgb.DMatrix(X_train, label=y_train)\nmodel = xgb.train(param, xgtrain, num_rounds)","fa048598":"xgval = xgb.DMatrix(X_val)\ny_val_pred = model.predict(xgval) \nlog_loss(y_val, y_val_pred)","0a522189":"test = PrepareFeatures('test')\nCreateCategFeat(test, cat_features)\nX_test = test[features]\nxgtest = xgb.DMatrix(X_test)\ny = model.predict(xgtest)","06944ae9":"labels2idx = {'low': 0, 'medium': 1, 'high': 2}\nsub = pd.DataFrame()\nsub[\"listing_id\"] = test[\"listing_id\"]\nfor label in labels2idx.keys():\n    sub[label] = y[:, labels2idx[label]]\nsub.to_csv(\"submission_rf.csv\", index=False)","cbd1bf75":"Prepare the Test Set and predict final labels:","f389b4d5":"Prepare the submission file:","de462518":"<h2> Training data<\/h2>","f9349e0e":"Measure the Log Loss on Validation Data:","193ca639":"Split the training set to create a validation set for later log_loss measurement and train the model:","0d4da265":"<h2>Introduction<\/h2>\nFirst, import the required libraries. We will use XGBoost (Extreme Gradient Boost) Algorithm. \nIn addition we will need sklearn components as log_loss metric and train_test_split","f71dd569":"<h2>Auxilary Functions<\/h2>\nDefining two functions we used in Part2 (Random Forest)\nHere we organize the numerical and categorical features."}}