{"cell_type":{"d0e4cc39":"code","f8fce147":"code","d17258b0":"code","4cfe8e15":"code","f5c60722":"code","487fe823":"code","1b18e409":"code","5fcf45ac":"code","720f9cf3":"code","3078e073":"code","b82d29bf":"code","c8e189b0":"code","85e175e1":"code","7cf240e9":"code","0b906379":"code","777dd88d":"code","5d5373f8":"code","209c3bbc":"code","7244f0ac":"code","830e0cd4":"code","ff0030a2":"code","d27dbb33":"code","be4c9dd2":"code","e86a0d6d":"code","db7d7059":"code","c42ef83e":"code","f0470056":"markdown","c28c8441":"markdown","69216da4":"markdown","2d1567dc":"markdown","25dbff11":"markdown","dfaadadf":"markdown","878a0609":"markdown","f6c4e93b":"markdown","28f16615":"markdown","74d370d4":"markdown","307d9555":"markdown","9f4b5321":"markdown","ad64f3fa":"markdown","dcb42010":"markdown","c6906edd":"markdown","60c13aeb":"markdown","0a632276":"markdown","5ba48203":"markdown","64296391":"markdown","5212d141":"markdown","1a27f16a":"markdown","4ce301da":"markdown","7279397f":"markdown","e0ebf4b1":"markdown"},"source":{"d0e4cc39":"import gc\nimport random\nimport warnings\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, optimizers\nfrom tensorflow.keras.layers import Lambda, Input, Dense, Dropout, Concatenate, BatchNormalization, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom googleqa_utilityscript import *\n\nSEED = 0\nseed_everything(SEED)\nwarnings.filterwarnings(\"ignore\")\nsns.set(font_scale=1.5)\nplt.rcParams.update({'font.size': 16})","f8fce147":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\n\ntrain['set'] = 'train'\ntest['set'] = 'test'\ncomplete_set = train.append(test)\n\nprint('Train samples: %s' % len(train))\nprint('Test samples: %s' % len(test))\ndisplay(train.head())","d17258b0":"samp_id = 9\nprint('Question Title: %s \\n' % train['question_title'].values[samp_id])\nprint('Question Body: %s \\n' % train['question_body'].values[samp_id])\nprint('Answer: %s' % train['answer'].values[samp_id])","4cfe8e15":"question_target_cols = ['question_asker_intent_understanding','question_body_critical', 'question_conversational', \n                        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n                        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', \n                        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n                        'question_type_compare', 'question_type_consequence', 'question_type_definition', \n                        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n                        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written']\nanswer_target_cols = ['answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n                      'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', \n                      'answer_type_reason_explanation', 'answer_well_written']\ntarget_cols = question_target_cols + answer_target_cols\n\nprint('Question labels')\ndisplay(train.iloc[[samp_id]][question_target_cols])\nprint('Answer labels')\ndisplay(train.iloc[[samp_id]][answer_target_cols])","f5c60722":"train_users = set(train['question_user_page'].unique())\ntest_users = set(test['question_user_page'].unique())\n\nprint('Unique users in train set: %s' % len(train_users))\nprint('Unique users in test set: %s' % len(test_users))\nprint('Users in both sets: %s' % len(train_users & test_users))\nprint('What users are in both sets? %s' % list(train_users & test_users))","487fe823":"train_users = set(train['answer_user_page'].unique())\ntest_users = set(test['answer_user_page'].unique())\n\nprint('Unique users in train set: %s' % len(train_users))\nprint('Unique users in test set: %s' % len(test_users))\nprint('Users in both sets: %s' % len(train_users & test_users))","1b18e409":"question_gp = complete_set[['qa_id', 'question_user_name', 'question_user_page']].groupby(['question_user_name', 'question_user_page'], as_index=False).count()\nquestion_gp.columns = ['question_user_name', 'question_user_page', 'count']\ndisplay(question_gp.sort_values('count', ascending=False).head())\n\ntrain_question_gp = train[['qa_id', 'question_user_page']].groupby('question_user_page', as_index=False).count()\ntest_question_gp = test[['qa_id', 'question_user_page']].groupby('question_user_page', as_index=False).count()\ntrain_question_gp.columns = ['question_user_page', 'Question count']\ntest_question_gp.columns = ['question_user_page', 'Question count']\n\nsns.set(style=\"darkgrid\")\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question count\", data=train_question_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question count\", data=test_question_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","5fcf45ac":"answer_gp = complete_set[['qa_id', 'answer_user_name', 'answer_user_page']].groupby(['answer_user_name', 'answer_user_page'], as_index=False).count()\nanswer_gp.columns = ['answer_user_name', 'answer_user_page', 'count']\ndisplay(answer_gp.sort_values('count', ascending=False).head())\n\ntrain_answer_gp = train[['qa_id', 'answer_user_page']].groupby('answer_user_page', as_index=False).count()\ntest_answer_gp = test[['qa_id', 'answer_user_page']].groupby('answer_user_page', as_index=False).count()\ntrain_answer_gp.columns = ['answer_user_page', 'Answer count']\ntest_answer_gp.columns = ['answer_user_page', 'Answer count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Answer count\", data=train_answer_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Answer count\", data=test_answer_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","720f9cf3":"question_title_gp = complete_set[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\nquestion_title_gp.columns = ['question_title', 'count']\ndisplay(question_title_gp.sort_values('count', ascending=False).head())\n\ntrain_question_title_gp = train[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\ntest_question_title_gp = test[['qa_id', 'question_title']].groupby('question_title', as_index=False).count()\ntrain_question_title_gp.columns = ['question_title', 'Question title count']\ntest_question_title_gp.columns = ['question_title', 'Question title count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question title count\", data=train_question_title_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question title count\", data=test_question_title_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","3078e073":"question_body_gp = complete_set[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\nquestion_body_gp.columns = ['question_body', 'count']\ndisplay(question_body_gp.sort_values('count', ascending=False).head())\n\ntrain_question_body_gp = train[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\ntest_question_body_gp = test[['qa_id', 'question_body']].groupby('question_body', as_index=False).count()\ntrain_question_body_gp.columns = ['question_body', 'Question body count']\ntest_question_body_gp.columns = ['question_body', 'Question body count']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\nsns.countplot(x=\"Question body count\", data=train_question_body_gp, palette=\"Set3\", ax=ax1).set_title(\"Train\")\nsns.countplot(x=\"Question body count\", data=test_question_body_gp, palette=\"Set3\", ax=ax2).set_title(\"Test\")\nplt.show()","b82d29bf":"complete_set['question_title_len'] = complete_set['question_title'].apply(lambda x : len(x))\ncomplete_set['question_body_len'] = complete_set['question_body'].apply(lambda x : len(x))\ncomplete_set['answer_len'] = complete_set['answer'].apply(lambda x : len(x))\ncomplete_set['question_title_wordCnt'] = complete_set['question_title'].apply(lambda x : len(x.split(' ')))\ncomplete_set['question_body_wordCnt'] = complete_set['question_body'].apply(lambda x : len(x.split(' ')))\ncomplete_set['answer_wordCnt'] = complete_set['answer'].apply(lambda x : len(x.split(' ')))\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_title_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_title_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_title_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_title_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","c8e189b0":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_body_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_body_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['question_body_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['question_body_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","85e175e1":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['answer_len'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['answer_len'], ax=ax2).set_title(\"Test\")\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 7), sharex=True)\nsns.distplot(complete_set[complete_set['set'] == 'train']['answer_wordCnt'], ax=ax1).set_title(\"Train\")\nsns.distplot(complete_set[complete_set['set'] == 'test']['answer_wordCnt'], ax=ax2).set_title(\"Test\")\nplt.show()","7cf240e9":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 7), sharex=True)\nsns.countplot(complete_set[complete_set['set'] == 'train']['category'], ax=ax1).set_title(\"Train\")\nsns.countplot(complete_set[complete_set['set'] == 'test']['category'], ax=ax2).set_title(\"Test\")\nplt.show()","0b906379":"complete_set['host_first'] = complete_set['host'].apply(lambda x : x.split('.')[0])\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12), sharex=True)\nsns.countplot(y=complete_set[complete_set['set'] == 'train']['host_first'], ax=ax1, palette=\"muted\").set_title(\"Train\")\nsns.countplot(y=complete_set[complete_set['set'] == 'test']['host_first'], ax=ax2, palette=\"muted\").set_title(\"Test\")\nplt.show()","777dd88d":"f = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[:5]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[5:10]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[10:15]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in question_target_cols[15:]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()","5d5373f8":"f = plt.subplots(figsize=(24, 7))\nfor col in answer_target_cols[:5]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()\n\nf = plt.subplots(figsize=(24, 7))\nfor col in answer_target_cols[5:]:\n    sns.distplot(train[col], label=col, rug=True, hist=False)\nplt.show()","209c3bbc":"eng_stopwords = stopwords.words('english')\n\ncomplete_set['question_title'] = complete_set['question_title'].str.replace('[^a-z ]','')\ncomplete_set['question_body'] = complete_set['question_body'].str.replace('[^a-z ]','')\ncomplete_set['answer'] = complete_set['answer'].str.replace('[^a-z ]','')\ncomplete_set['question_title'] = complete_set['question_title'].apply(lambda x: x.lower())\ncomplete_set['question_body'] = complete_set['question_body'].apply(lambda x: x.lower())\ncomplete_set['answer'] = complete_set['answer'].apply(lambda x: x.lower())\n\nfreq_dist = FreqDist([word for comment in complete_set['question_title'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on question title').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()\n\nfreq_dist = FreqDist([word for comment in complete_set['question_body'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on question body').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()\n\nfreq_dist = FreqDist([word for comment in complete_set['answer'] for word in comment.split() if word not in eng_stopwords])\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on answer').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()","7244f0ac":"gc.collect()","830e0cd4":"EPOCHS = 12\nBATCH_SIZE = 32\nLEARNING_RATE = 3e-4\nEMBEDDDING_SIZE = 512\nN_CLASS = len(target_cols)\nES_PATIENCE = 3\nRLROP_PATIENCE = 2\nDECAY_DROP = 0.3\nmodule_url = \"..\/input\/universalsentenceencodermodels\/universal-sentence-encoder-models\/use-qa\"\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\nrlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)","ff0030a2":"use_embed = hub.load(module_url)\n\ndef USEEmbedding(x):\n    return use_embed(tf.squeeze(tf.cast(x, tf.string)))\n\ndef model_fn():\n    input_title = Input(shape=(1,), dtype=tf.string, name='input_title')\n    embedding_title = Lambda(USEEmbedding, output_shape=(EMBEDDDING_SIZE,))(input_title)\n\n    input_body = Input(shape=(1,), dtype=tf.string, name='input_body')\n    embedding_body = Lambda(USEEmbedding, output_shape=(EMBEDDDING_SIZE,))(input_body)\n\n    input_answer = Input(shape=(1,), dtype=tf.string, name='input_answer')\n    embedding_answer = Lambda(USEEmbedding, output_shape=(EMBEDDDING_SIZE,))(input_answer)\n\n    x = Concatenate()([embedding_title, embedding_body, embedding_answer])\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(N_CLASS, activation='sigmoid', name='output')(x)\n    model = Model(inputs=[input_title, input_body, input_answer], outputs=[output])\n\n    optimizer = optimizers.Adam(LEARNING_RATE)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n    \n    return model","d27dbb33":"feature_cols = ['question_title', 'question_body', 'answer']\nY_train = train[target_cols]\n\nNUM_FOLDS = 3\ntrain_rho_kfolds = []\nvalid_rho_kfolds = []\nmodel_path_list = []\nkf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n\nfor ind, (tr, val) in enumerate(kf.split(train[feature_cols], Y_train)):\n    print('FOLD', ind+1)\n    X_tr = train[feature_cols].loc[tr]\n    y_tr = Y_train.loc[tr].values\n    X_vl = train[feature_cols].loc[val]\n    y_vl = Y_train.loc[val].values\n\n    X_tr = [X_tr[col] for col in feature_cols]\n    X_vl = [X_vl[col] for col in feature_cols]\n    \n    \n    model = model_fn()\n    spearmanCallback = SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl))\n    callback_list = [es, rlrop, spearmanCallback]\n    history = model.fit(X_tr, y_tr, \n                        validation_data=(X_vl, y_vl), \n                        batch_size=BATCH_SIZE, \n                        callbacks=callback_list, \n                        epochs=EPOCHS, \n                        verbose=2).history\n    \n    preds_train = model.predict(X_tr)\n    preds_val = model.predict(X_vl)\n\n    rho_train = [spearmanr(y_tr[:, ind], preds_train[:, ind] + np.random.normal(0, 1e-7, preds_train.shape[0])).correlation for ind in range(preds_train.shape[1])]\n    rho_val = [spearmanr(y_vl[:, ind], preds_val[:, ind] + np.random.normal(0, 1e-7, preds_val.shape[0])).correlation for ind in range(preds_val.shape[1])]\n\n    train_rho_kfolds.append(rho_train)\n    valid_rho_kfolds.append(rho_val)\n    print('Train spearman-rho: %.3f' % np.mean(rho_train))\n    print('Validation spearman-rho: %.3f' % np.mean(rho_val))\n    \n    model_path = '..\/working\/use_baseline_fold_%d.h5' % (ind+1)\n    model.save_weights(model_path)\n    model_path_list.append(model_path)\n    print('Saved model at: %s' % model_path)","be4c9dd2":"sns.set(style=\"whitegrid\")\nfor key in spearmanCallback.history.keys():\n    history[key] = spearmanCallback.history[key]\n\nplot_metrics(history, metric_list=['loss', 'spearman'])","e86a0d6d":"print('Train')\nprint('Averaged spearman-rho: %.3f' % np.mean(train_rho_kfolds))\nprint('Averaged spearman-rho (nanmean): %.3f' % np.nanmean(train_rho_kfolds))\nprint('Averaged spearman-rho avg(regular and nanmean): %.3f +\/- %.3f'% (np.mean(train_rho_kfolds), np.std(np.mean(train_rho_kfolds))))\nprint('\\nValidation')\nprint('Averaged spearman-rho: %.3f' % np.mean(valid_rho_kfolds))\nprint('Averaged spearman-rho (nanmean): %.3f' % np.nanmean(valid_rho_kfolds))\nprint('Averaged spearman-rho avg(regular and nanmean): %.3f +\/- %.3f'% (np.mean(valid_rho_kfolds), np.std(np.mean(valid_rho_kfolds))))\n\nprint('\\nEach label :')\nspearman_avg_per_label = np.mean(valid_rho_kfolds, axis=0)\nspearman_std_per_label = np.std(valid_rho_kfolds, axis=0)\nfor ii in range(len(target_cols)):\n    print('%d - %.3f +\/- %.3f - %s' % (ii+1,spearman_avg_per_label[ii],spearman_std_per_label[ii],\n                                       target_cols[ii] ))","db7d7059":"# Test features\nX_test_title = test['question_title']\nX_test_body = test['question_body']\nX_test_answer = test['answer']\n\nX_test = [X_test_title, X_test_body, X_test_answer]\nY_test = np.zeros((len(test), len(target_cols)))\n\nfor model_path in model_path_list:\n    model = model_fn()\n    model.load_weights(model_path)\n    Y_test += model.predict(X_test) \/ NUM_FOLDS","c42ef83e":"submission = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/sample_submission.csv')\nsubmission[target_cols] = Y_test\nsubmission.to_csv(\"submission.csv\", index=False)\ndisplay(submission.head())","f0470056":"#### For the answers there are no duplicates\n\n## Now we will take a look at some statistics of the text data\n\n### Question title length and word count","c28c8441":"# EDA\n#### Each sample main features are the question and it's answer\n#### First let's take a look at one pair of question\/answer:","69216da4":"<h1><center> Google QUEST - EDA and USE Baseline <\/center><\/h1>\n<h2><center> Improving automated understanding of complex question answer content <\/center><\/h2>\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/google-research\/human_computable_dimensions_1.png\" width=\"800\">\n\n#### From the competition overview:\n>In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!\n\n>Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.\n\n##### Link for the dataset with the USE models: https:\/\/www.kaggle.com\/dimitreoliveira\/universalsentenceencodermodels","2d1567dc":"# Model\n## For the model we will use Universal Sentence Encoder (USE for short)\n#### From the [Tensorflow hub page](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4):\n>The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n\n>The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. The universal-sentence-encoder-large model is trained with a Transformer encoder.\n\n##### Link for the dataset with the USE models: https:\/\/www.kaggle.com\/dimitreoliveira\/universalsentenceencodermodels","25dbff11":"#### Answer user ranking","dfaadadf":"### Answer related labels","878a0609":"## Dependencies","f6c4e93b":"### Could be interesting to see the ranks of users that ask and answer more questions\n#### Question user ranking","28f16615":"#### How are distributed between sets users that answer questions?","74d370d4":"#### How are distributed between sets users that ask questions?","307d9555":"### The \"host\" column is related to where the questions and answers where published","9f4b5321":"# Train model (3-Fold)\n- I got the CV split and the evaluation from @ratthachat [awsome kernel](https:\/\/www.kaggle.com\/ratthachat\/quest-cv-analysis-on-different-splitting-methods), there is an explanation about why use this CV scheme checkout. ","ad64f3fa":"### Answer length and word count","dcb42010":"## Model loss graph (the last one) ","c6906edd":"# Make predictions on test","60c13aeb":"## Now for the question bodies","0a632276":"# Model parameters","5ba48203":"### We also have a \"category\" column that is related to the question\/answer\n- It seems that at least the \"Technology\" and \"SCIENCE\" categories have a very diferent distribution among sets","64296391":"#### Each question\/answer are given some ratings, those ratings are the competition labels, let's see this sample labels","5212d141":"# Evaluation","1a27f16a":"## Lastly let's look at the labels distribution\n### Question related labels","4ce301da":"### Question body length and word count","7279397f":"## Load data","e0ebf4b1":"## Let's do the same for the question titles"}}