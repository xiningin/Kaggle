{"cell_type":{"9bf201c7":"code","537cfed7":"code","d40530b4":"code","27796902":"code","b157780c":"code","68eb2183":"code","0a9bc44f":"code","b7dd4436":"code","c3d1c12a":"code","18fedd52":"code","1f5804aa":"code","f9034658":"code","afd7eef5":"code","74cf9b36":"code","c18165c6":"code","945d55d8":"code","91d49f43":"code","5368517b":"markdown","1707dd93":"markdown","fe1e800c":"markdown","35813854":"markdown","d099f207":"markdown","83959966":"markdown","046eac56":"markdown","8659e1df":"markdown","574a2523":"markdown","9590bd5e":"markdown","1fe2d8e4":"markdown","3b6c5246":"markdown"},"source":{"9bf201c7":"#Importing all the required libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt","537cfed7":"# Reading the train and test dataset\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d40530b4":"#Printing out the number of samples and features of Training as well as test dataset\n\nprint(\"{} no of features with {} numbers of samples in training\".format(train.shape[1],train.shape[0]))\nprint(\"{} no of features with {} numbers of samples in testing\".format(test.shape[1],test.shape[0]))","27796902":"train_id = train['Id']\ntest_id = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\nprint(\"{} no of features with {} numbers of samples in training\".format(train.shape[1],train.shape[0]))\nprint(\"{} no of features with {} numbers of samples in testing\".format(test.shape[1],test.shape[0]))","b157780c":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nfull_data = pd.concat((train, test)).reset_index(drop=True)\nfull_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"full_data size is : {}\".format(full_data.shape))","68eb2183":"full_data_null = (full_data.isnull().sum() \/ len(full_data)) * 100\nfull_data_null = full_data_null.drop(full_data_null[full_data_null == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Data Percentage' :full_data_null})\nmissing_data","0a9bc44f":"full_data[\"PoolQC\"] = full_data[\"PoolQC\"].fillna(\"None\")\nfull_data[\"MiscFeature\"] = full_data[\"MiscFeature\"].fillna(\"None\")\nfull_data[\"Alley\"] = full_data[\"Alley\"].fillna(\"None\")\nfull_data[\"Fence\"] = full_data[\"Fence\"].fillna(\"None\")\nfull_data[\"FireplaceQu\"] = full_data[\"FireplaceQu\"].fillna(\"None\")\nfull_data[\"MasVnrType\"] = full_data[\"MasVnrType\"].fillna(\"None\")\nfull_data[\"MasVnrArea\"] = full_data[\"MasVnrArea\"].fillna(0)\nfull_data['MSSubClass'] = full_data['MSSubClass'].fillna(\"None\")\nfull_data = full_data.drop(['Utilities'], axis=1)","b7dd4436":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    full_data[col] = full_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    full_data[col] = full_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    full_data[col] = full_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    full_data[col] = full_data[col].fillna('None')","c3d1c12a":"full_data['MSZoning'] = full_data.groupby(\"Neighborhood\")[\"MSZoning\"].transform(lambda x: x.fillna(x.mode()))\nfull_data[\"Functional\"] = full_data[\"Functional\"].fillna(\"Typ\")\nfull_data[\"LotFrontage\"] = full_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfull_data['Electrical'] = full_data.groupby(\"Neighborhood\")[\"Electrical\"].transform(lambda x: x.fillna(x.mode()[0]))\nfull_data['KitchenQual'] = full_data.groupby(\"Neighborhood\")[\"KitchenQual\"].transform(lambda x: x.fillna(x.mode()[0]))\nfull_data['Exterior1st'] = full_data.groupby(\"Neighborhood\")[\"Exterior1st\"].transform(lambda x: x.fillna(x.mode()[0]))\nfull_data['Exterior2nd'] = full_data.groupby(\"Neighborhood\")[\"Exterior2nd\"].transform(lambda x: x.fillna(x.mode()[0]))\nfull_data['SaleType'] = full_data.groupby(\"Neighborhood\")[\"SaleType\"].transform(lambda x: x.fillna(x.mode()[0]))","18fedd52":"#Check remaining missing values if any \nfull_data_null = (full_data.isnull().sum() \/ len(full_data)) * 100\nmissing_data = pd.DataFrame({'Missing Percentage' :full_data_null})\nmissing_data.head()","1f5804aa":"corr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncorr = corr.SalePrice\ndisplay(corr)","f9034658":"train.head()","afd7eef5":"plot = plt.subplot()\nplot.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","74cf9b36":"plot = plt.subplot()\nplot.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","c18165c6":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","945d55d8":"plot = plt.subplot()\nplot.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","91d49f43":"corrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","5368517b":"We can build scatter plots\/bar graph for the top N features and see if there some outliers.\n\nNext, I am plotting the heatmap for correlation between varibales. We may want to remove one of the variables from the pair of variables having large correclation. ","1707dd93":"Hello Kagglers,\n****\nThis is my first Kernel. The main purpose of this Kernel is to only do the Exploratory Data Analysis of the data. Please let me know if you find anything which does not make sense.","fe1e800c":"full data consists of 2919 samples and 79 features as we would have wanted. All good until now.\nNow, Lets, try to find the missing values in full data. The below code snippet will print out the percentage of the missing values for every features which have missing values.","35813854":"Since, \"Id\" column will not contribute in model building, I am dropping this column from both (training and test) datasets. We got one less feature for both the datasets.","d099f207":"The first feature which have the maximum correlation with the Sale Price is OverallQual, As we can see in the scatter plot, that when the OverallQual is increasing, the average Sale Price for each OverallQual is increasing which actually makes sense.","83959966":"We can see that we have completed imputing the missing values which could result in more accurate model.\n\nNow, we will try to see the correlation between the features and he target variable (Sale Price) and plot some scatter plots to see if we find any outlier or any interesting pattern.","046eac56":"As I mentioned before, this was my effort just to do some EDA and post my first Kernel.\n\nI will be looking forward to post more Kernels with more content as I continue learning.\n\nPlease feel free to comment for any suggestions\/\/improvements.\nCheers!","8659e1df":"Now, lets combine the train and test dataset to make the changes (if any) to the features in both the datasets together. Lets call the combined data set as **full data** now onwards.","574a2523":"For some categorical features we took the mode of that feature in that corresponding Neighbourhood and imputed the Mode values in place of null values. ","9590bd5e":"We can see in the previous dataframe that many variables have large number of null values (a couple have over 90%). Lets see why these have so many null values. We can see from the data description that if Pool Quality is NA, then there is no pool. This makes sense that most of the houses does not have pool. So, we will impute \"None\" with null values.\nSimilarly, there are other features which does not have Misc Features, does not have Alley, does not have Fence, does not have Fire Place , we will impute \"None\" for all the null values into these features.\n\nThere are some other features where we imputed with either \"0\" (if continuous) or \"None\" (if Categorical) for null values where those features were not installed in that home.","1fe2d8e4":"Now, the second most important feature is the GrLivArea. We can see in the graph that as the living area increases there is an increase in the SalePrice of the house. But we can also see that there are couple of outliers where GrLivArea>4500 but Sale Price<300000. We may want to remove this outlier for fitting the model.","3b6c5246":"Removing the outlier we have seen above. And checking the graph again for sanity check."}}