{"cell_type":{"f8520ebc":"code","6bcaac9d":"code","dbd7a4b2":"code","b4477ea2":"code","9c8fd233":"code","f7c6fdca":"code","9e7171a4":"code","1c426c54":"code","0be1b23a":"code","f8bb1d11":"code","222f6ae1":"code","6eff2998":"code","2fd3e2a8":"code","bdaef5c8":"code","7da04f00":"code","7d617d44":"code","a6f1e719":"code","cf09df6b":"code","80c77e8e":"code","741482d6":"code","6b8929cc":"code","b66decda":"code","f9cd35a9":"code","04e7aa31":"code","758d78b7":"code","94fa4f52":"code","9963a90a":"code","a154a451":"markdown","96c75799":"markdown","74d6809f":"markdown","adc5d275":"markdown","65fb42b4":"markdown","0223b113":"markdown","320e446d":"markdown","f1668281":"markdown","5c630af7":"markdown","f08d386d":"markdown","acd8fd34":"markdown","d3ce51d2":"markdown","0fa39600":"markdown","19058157":"markdown","80526fb9":"markdown","d7f35352":"markdown","2224360f":"markdown","7919d475":"markdown","cca6140d":"markdown","2fe3e692":"markdown","2bc42a65":"markdown","baa5e195":"markdown","0e0789dc":"markdown","5c2805ed":"markdown","9c7a8dcb":"markdown","375600bb":"markdown","6765da88":"markdown","ba3219b3":"markdown","699cc0ac":"markdown","f13dee10":"markdown","3fc9f983":"markdown"},"source":{"f8520ebc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('..\/input\/train.csv')\n\n# replace prices with logarithmic prices\ndata['logSalePrice'] = np.log10(data.SalePrice.values)\ndata.drop('SalePrice', axis=1, inplace=True)","6bcaac9d":"# automatically identify continuous features\ncontinuous_features = [col for col in data.columns if data[col].dtype != 'O']\n\ncontinuous_features.remove('Id')\ncontinuous_features.remove('logSalePrice') # remove the target feature\n\n# manually select ordinal features that can be ranked in some way\nordinal_features = ['Street', 'Alley', 'LotShape', 'Utilities', 'LandSlope', 'ExterQual', \n                    'LandContour', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                    'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', \n                    'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType',\n                    'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', \n                    'Fence'] \n\n# manually select categorical features that will be ranked based on median `logSalePrice`\ncategorical_features = ['LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n                        'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n                        'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'MiscFeature',\n                        'SaleType', 'SaleCondition', 'MSZoning', 'BldgType']","dbd7a4b2":"for col in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n            'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \n            'PoolQC', 'Fence', 'MiscFeature']:\n    data.loc[:, col].fillna('NA', inplace=True) ","b4477ea2":"data.fillna({col: data.loc[:, col].median() for col in continuous_features}, inplace=True)\ndata.fillna({col: data.loc[:, col].value_counts().index[0] for \n             col in categorical_features + ordinal_features}, inplace=True)","9c8fd233":"ordinal_transform = {'Street':  {'Grvl': 1, 'Pave': 2}, \n                     'Alley': {'NA': 0, 'Grvl': 1, 'Pave': 2}, \n                     'LotShape': {'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4}, \n                     'Utilities': {'ELO': 1, 'NoSeWa': 2, 'NoSewr': 3, 'AllPub': 4}, \n                     'LandSlope': {'Sev': 1, 'Mod': 2, 'Gtl': 3}, \n                     'LandContour': {'Low': 1, 'HLS': 1, 'Bnk': 2, 'Lvl': 3}, \n                     'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                     'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}, \n                     'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, \n                                      'GLQ': 6}, \n                     'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, \n                                      'GLQ': 6}, \n                     'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                     'CentralAir': {'N': 1, 'Y': 2}, \n                     'Electrical': {'Mix': 1, 'FuseP': 2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5}, \n                     'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, \n                                    'Min1': 7, 'Typ': 8}, \n                     'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'GarageType': {'NA': 0, 'Detchd': 1, 'CarPort': 2, 'BuiltIn': 3, \n                                    'Basment': 4, 'Attchd': 5, '2Types': 6},\n                     'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}, \n                     'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'PavedDrive': {'N': 1, 'P': 2, 'Y': 3}, \n                     'PoolQC': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}, \n                    }\n\n# apply transformations\nfor col in ordinal_features:\n    data.loc[:, col] = data.loc[:, col].map(ordinal_transform[col], na_action='ignore')\n    \n# move some features from continuous to ordinal feature list\nfor col in ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n            'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']:\n    continuous_features.remove(col)\n    ordinal_features.append(col)\n    \n# move one feature from continuous to categorial feature list\ncontinuous_features.remove('MSSubClass')\ncategorical_features.append('MSSubClass')","f7c6fdca":"ordinalized = []\nfor col in categorical_features:\n    sorted_labels = data[[col, 'logSalePrice']].groupby(col).logSalePrice.median().sort_values()\n    data.loc[:, col] = data.loc[:, col].map({sorted_labels.index.values[i]:i for i in range(len(sorted_labels))})\n    ordinalized.append(col)\n\nfor col in ordinalized:\n    categorical_features.remove(col)\n    ordinal_features.append(col)","9e7171a4":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharey=True)\nplt.subplots_adjust(hspace=0.5)\n\n# continuous features\ndata[continuous_features].corrwith(data.logSalePrice).agg('square').plot.bar(ax=ax1, alpha=0.5)\nax1.set_title('Coefficient of Determination: Continuous Features')\nax1.grid()\n\n# ordinal features\ndata[ordinal_features].corrwith(data.logSalePrice).agg('square').plot.bar(ax=ax2, alpha=0.5)\nax2.set_title('Coefficient of Determination: Ordinal Features')\nax2.grid()\n","1c426c54":"results = pd.DataFrame(data.drop('logSalePrice', axis=1).corrwith(data.logSalePrice).agg('square'), \n                       columns=['det_weight'])\n\nranks = np.zeros(len(results), dtype=np.int)\nfor i, j in enumerate(np.argsort(results.det_weight)[::-1]):\n    ranks[j] = i\nresults['det_rank'] = ranks\n\nresults.sort_values('det_rank').loc[:, ['det_rank', 'det_weight']].iloc[0:10]","0be1b23a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline([('model', RandomForestRegressor(n_jobs=-1, random_state=42))])\n\nparam_space = {'model__max_depth': [10, 15, 20],\n               'model__max_features': [10, 15, 20],\n               'model__n_estimators': [250, 300, 350]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=10, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1), data.logSalePrice)","f8bb1d11":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","222f6ae1":"weights = grid.best_estimator_.steps[0][1].feature_importances_\n\nresults['rf_weight'] = weights\n\nranks = np.zeros(len(results), dtype=np.int)\nfor i, j in enumerate(np.argsort(weights)[::-1]):\n    ranks[j] = i\nresults['rf_rank'] = ranks\n\nresults.sort_values('rf_rank').loc[:, ['rf_rank', 'rf_weight']].iloc[0:10]","6eff2998":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Ridge\n\npipe = Pipeline([('scaler', RobustScaler()), \n                 ('model', Ridge(random_state=42))])\n\nparam_space = {'model__alpha': [0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=10, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1), data.logSalePrice)","2fd3e2a8":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","bdaef5c8":"weights = grid.best_estimator_.steps[1][1].coef_\n\nresults['ridge_weight'] = weights\n\nranks = np.zeros(len(results), dtype=np.int)\nfor i, j in enumerate(np.argsort(weights)[::-1]):\n    ranks[j] = i\nresults['ridge_rank'] = ranks\n\nresults.sort_values('ridge_rank').loc[:, ['ridge_rank', 'ridge_weight']].iloc[0:10]","7da04f00":"from sklearn.linear_model import Lasso\n\npipe = Pipeline([('scaler', RobustScaler()), \n                 ('model', Lasso(random_state=42))])\n\nparam_space = {'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=10, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1), data.logSalePrice)","7d617d44":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","a6f1e719":"weights = grid.best_estimator_.steps[1][1].coef_\n\nresults['lasso_weight'] = weights\n\nranks = np.zeros(len(results), dtype=np.int)\nfor i, j in enumerate(np.argsort(weights)[::-1]):\n    ranks[j] = i\nresults['lasso_rank'] = ranks\n\nresults.sort_values('lasso_rank').loc[:, ['lasso_rank', 'lasso_weight']].iloc[0:10]","cf09df6b":"f, ax = plt.subplots(figsize=(17, 5))\n\nresults['mean_rank'] = results.loc[:, ['det_rank', 'rf_rank', 'ridge_rank', 'lasso_rank']].agg('abs').mean(axis=1)\nresults['mean_rank_std'] = results.loc[:, ['det_rank', 'rf_rank', 'ridge_rank', 'lasso_rank']].std(axis=1)\n\nresults.mean_rank.sort_values().plot.bar(width=0.5, color='orange', alpha=0.7, ax=ax)\nresults.sort_values(by='mean_rank').mean_rank_std.plot.bar(width=0.5, color='black', alpha=0.3, ax=ax)\n\nax.set_xlim([0,79.5])\nax.plot(ax.get_xlim(), [0, 80], color='red')\nax.set_ylabel('Average Rank')","80c77e8e":"from matplotlib import ticker\n\nf, ax = plt.subplots(figsize=(5, 17))\n\nfor idx, feature in enumerate(results.sort_values(by='mean_rank').index):\n    ax.plot(range(4), results.loc[feature, ['det_rank', 'rf_rank', 'ridge_rank', 'lasso_rank']]+0.5, marker='o',\n           alpha=np.clip(1\/(results.loc[feature, 'mean_rank_std']+0.001), 0.01, 1),\n           linewidth=np.clip(1\/(results.loc[feature, 'mean_rank_std']+0.1), 0.5, 1)*3)\nax.set_ylim([80, 0])\nax.yaxis.set_major_locator(ticker.LinearLocator(numticks=80))\nax.set_yticklabels(results.sort_values(by='mean_rank').index)\nax.xaxis.set_major_locator(ticker.LinearLocator(numticks=4))\nax.set_xticklabels(['det_rank', 'rf_rank', 'ridge_rank', 'lasso_rank'])\n","741482d6":"for col in ['det_weight', 'rf_weight', 'ridge_weight', 'lasso_weight']:\n    weightsum = results.loc[:, col].abs().sum(axis=0)\n    results.loc[:, col] = results.loc[:, col].apply(lambda x: np.abs(x)\/weightsum)","6b8929cc":"f, ax = plt.subplots(figsize=(17, 5))\nax.xaxis.set_major_locator(ticker.LinearLocator(numticks=80))\nresults.sort_values('mean_rank').loc[:, ['det_weight', 'rf_weight', 'ridge_weight', 'lasso_weight']].cumsum().plot.line(\n    rot='vertical', ax=ax)\nax.grid()","b66decda":"pipe = Pipeline([('scaler', RobustScaler()), \n                 ('model', Ridge(random_state=42))])\n\nparam_space = {'model__alpha': [1, 5, 10, 50, 100]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=10, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1).loc[:, ['OverallQual', 'Neighborhood', 'GrLivArea']], data.logSalePrice)","f9cd35a9":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","04e7aa31":"pipe = Pipeline([('model', RandomForestRegressor(n_jobs=-1, random_state=42))])\n\nparam_space = {'model__max_depth': [5, 7, 10],\n               'model__max_features': [1, 2],\n               'model__n_estimators': [70, 100, 120]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=5, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1).loc[:, ['OverallQual', 'Neighborhood', 'GrLivArea']], data.logSalePrice)","758d78b7":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","94fa4f52":"pipe = Pipeline([('model', RandomForestRegressor(n_jobs=-1, random_state=42))])\n\nparam_space = {'model__max_depth': [10, 15, 20, 25],\n               'model__max_features': [2, 4, 6],\n               'model__n_estimators': [100, 150, 200, 250, 300]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=5, scoring='neg_mean_squared_error')\n\ngrid.fit(data.drop('logSalePrice', axis=1).loc[:, results.sort_values('mean_rank').index[:15]], data.logSalePrice)","9963a90a":"print('best-fit parameters:', grid.best_params_)\nprint('prediction rms residuals:', 10**(np.sqrt(-grid.best_score_)))","a154a451":"# Comparing Feature Importance, Coefficient of Determination, and Linear Model Weights\n\nThis kernel compares a number of different methods to identfy the most important features of the [house prices dataset](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques). The accuracy of different regression models is compared for models trained on all available features and a model trained on only the $n$ most important features.\n\nI created this kernel out of curiosity while working on the [house prices dataset](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques). My main motivation was to figure out (1) how well rankings of feature importances based on different methods correlate with each other and (2) to quantify how leaving out less important features affects the model accuracy.\n\nThis kernel is based on the data preparations from [House Prices with simple Ridge Regression](https:\/\/www.kaggle.com\/mommermi\/house-prices-with-simple-ridge-regression).","96c75799":"Now we can plot a cumulative distribution of the weights derived from the different methods:","74d6809f":"We achieve a decent model accuracy. What are the most important features?","adc5d275":"## Impact of Feature Selection on Modeling Accuracy","65fb42b4":"We train the Ridge Regression model only on the three higest ranking features (`OverallQual`, `Neighborhood`, and `GrLivArea`) and compare the result to the model trained on all available features:","0223b113":"#### Lasso\n\nThe [Lasso](https:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics&#41;) is a linear model utilizing L1 regularization. \n\nWe take an approach similar to the Ridge Regression model above.","320e446d":"As expected from them previous plot, the Random Forest fares slightly better, as the three features combine more total weight in them. \n\nWhat if we include more features? The 15 highest-ranking features combine a cumulative weight of ~80% in them. Will that significantly improve the model accuracy?","f1668281":"## Preparation of Data","5c630af7":"For continuous features, fill in the median across the entire feature; for other ordinal\/categorical features, fill in the most common value across each feature.","f08d386d":"## Feature Importance Ranking\n\nIn the following, we apply different methods for estimating feature importance. ","acd8fd34":"Finally, we consider the derived weights instead of the ranks. In order to enable a comparison between the individual weights, we normalize the sum of all weights derived from each method to unity:","d3ce51d2":"Given the value of the regularization parameter $\\alpha$, the model is likely to over-fit the training data. Nevertheless, the accuracy of the model is very similar to the models used above. What are the ten most important features?","0fa39600":"### Feature Completeness","19058157":"Few features stick out with high coefficients of determination.\n\nWe list those ten features with the highest coefficients:","80526fb9":"### Random Forest Feature Importance\n\n[Decision Tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning)-based models provide information on the relative importance of each feature in the training data set. This *feature importance* is a diagnostic tool to identify features that carry crucial information.\n\nFeature importance is only available for Decision Tree-based model. In this case, we use a [Random Forest](https:\/\/en.wikipedia.org\/wiki\/Random_forest) model in combination with a grid search and cross validation in order to obtain a meaningful fit to the data and hence meaningful feature importances.","d7f35352":"### Feature Types\n\nIdentify the different types of features in the data set.","2224360f":"### Transforming Features\n\nThe following dictionary describes the ranking of those features that were previously classified as ordinal features from lowest rank (worst case, starting at `1` so that missing data can be ranked with zero where applicable) to highest rank (best case): ","7919d475":"## Comparison of Results\n\nEach of the methods used above allow us to rank the features based on their weights or importances (we will refer to them simply as *weights* in the remainder of this kernel).\n\nThe following plot shows the *mean rank* of each feature, which is defined as the average rank based across the four methods used above, sorted from the most important feature (highest rank) to the least important feature (lowest rank).","cca6140d":"Interestingly, all methods that are based on linear models (including the coefficient of determination, which is measuring the linear correlation between two features) follow a similar course, whereas the Random Forest cumulative weights follow a significantly steeper curve. \n\nThe three highest ranking features based on all four methods (`OverallQual`, `Neighborhood`, and `GrLivArea`) make up ~45% of the Random Forest cumulative weights, but only ~10% of the Ridge Regression cumulative weights.\n\nLet's see how these numers impact the modeling accuracy.","2fe3e692":"## Conclusions\n\nWe can conclude the following from this analysis:\n* The coefficient of determination, the Random Forest feature importance, and the weights of linear models provide very consistent results (if the models parameters are picked properly, leading to good generalization) in identifying the most important features; the ranking of less important features, however, can be highly variable.\n* Training the models only on the highest-ranking features greatly improves performance (runtime) but also reduces the accuracy of the models. Feature selection based on feature importance can lead to model accuracies comparable to accuracies achieved with the full feature set, but with a significantly smaller data set.\n* This type of comparison is useful for the interpretation of data sets and model outcomes. ","2bc42a65":"19% RMS loss is still pretty good given the fact that the entire feature palette results in 15% RMS loss.\n\nLet's redo the Random Forest with only three features:","baa5e195":"Fill those ordinal\/categorical features with `NA` for which this value is defined.","0e0789dc":"All features are listed based on their mean ranks from top (highest ranking) to bottom (lowest ranking). The abscissa shows the ranks based on the different methods. Each feature creates a line in this plot following the ranks through the different methods. \n\nIn cases in which the ranking across the methods is very similar, the plotted line should be more or less horizontal. If the ranking is subject to significant variance, the line has a slope or follows a zig-zag course. This information is coded into the lines: consistently ranked features have thick and solid lines, whereas inconsistently ranked features have thin and transparent lines.\n\nSimilar to the previous plot, three features stand out as important: `OverallQual`, `Neighborhood`, and `GrLivArea`. These feature have consistently high ranks across all four methods.","5c2805ed":"#### Ridge Regression\n\n[Ridge Regression](https:\/\/en.wikipedia.org\/wiki\/Tikhonov_regularization) is a linear regression model implementation using L2 regularization. Like in any other linear model, the weights derived for the different features during training are indicative of their relative importance.\n\nUsing a gridsearch and cross-validation, we find the best-fit parameter $\\alpha$ for the L2 regularization:","9c7a8dcb":"Now we have a complete data set consisting of purely numerical data.","375600bb":"Yes, the model fares significantly better: the resulting RMS loss is only 1.2% higher than that of the model using the entire feature set. This means in turn that 65 out of 80 features barely carry any useful information! ","6765da88":"### Coefficient of Determination\n\nWe derive the [coefficient of determination](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination), $r^2$, which is based on [Pearson's correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) $r$, for each feature with respect to `logSalePrice`. The higher $r^2$, the higher the correlation between the two features. \n\nThe coefficient of determination is typically considered a measure for the variance in a relation between two features that is [explained by the model](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination#Relation_to_unexplained_variance) and and does not rely on any machine learning model.","ba3219b3":"### Linear Model Coefficient Weights\n\nIn the following, we will fit two linear regression models to the entire training set and extract the weights of the individual features from these models a measure for feature importance.","699cc0ac":"Orange bars indicate the mean rank - grey bars indicate the standard deviation of the mean rank across the four methods. The red line symbolizes ideal behavior and unity line: the rank increases by one for each additional feature.\n\nThe mean rank distribution follows the ideal behavior only for the three highest ranked features (`OverallQual`, `Neighborhood`, and `GrLivArea`). For lower ranked features, the mean rank can be above or below the red line, symbolizing variation in the ranking. This behavior is supported by the standard deviations plotted here, and the top feature lists shown above for the individual methods.\n\nThe following plot visualizes the same data in a different way:","f13dee10":"The best-fit model accuracy is very similar to that obtained by the Random Forest model above. The ten most important features are:","3fc9f983":"In the case of categorical features, we rank each feature based on its median `logSalePrice`."}}