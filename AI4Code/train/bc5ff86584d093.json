{"cell_type":{"0e6d9784":"code","9c484c74":"code","8fd9584d":"code","d620856a":"code","c4f0ed2b":"code","484fca92":"code","a4e15d06":"code","297f2f85":"code","889b178a":"code","b7d7c547":"code","9a3ee57a":"code","d2ebcb45":"code","8f9ef97a":"code","091b7918":"code","62701d1c":"code","90dca356":"code","f019e07b":"markdown","3e31cc66":"markdown","f499f3b4":"markdown","4b5ea50f":"markdown","de4d78d9":"markdown","81b264aa":"markdown","d5098902":"markdown"},"source":{"0e6d9784":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c484c74":"from numpy import vstack\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n# from torch.nn import Module\nfrom torch import optim\nfrom torch import nn\n\n","8fd9584d":"class CSVDataset(Dataset):\n    def __init__(self, path):\n        # use a dataframe to load the CSV file\n        df = read_csv(path, header=None)\n        \n        # strore input (X) and output(y) tensor\n        self.X = df.values[:,:-1]\n        self.y = df.values[:,-1]\n        \n        # ensure input data is floats  \n        self.X = self.X.astype('float32')\n        \n        # label encode target (check online for meaning) \n        # and ensure the values are floats\n        self.y = LabelEncoder().fit_transform(self.y)\n        self.y = self.y.astype('float32')\n        self.y = self.y.reshape(len(self.y),1)\n        \n    # To determine the length of row \n    def __len__(self):\n        return len(self.X)\n    \n    # Call input and output row by index \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n    def get_splits(self, n_test=0.33):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])\n    \n    # get indexes for train and test rows\n#     def get_splits(self, n_test=0.2):\n#         # we calculate test and train sizes (return numbers only)\n#         test_size = round(len(self.X)*n_test)\n#         train_size = len(self.X) - test_size\n        \n#         # We split the dataset by randomly choosing the entire Dataset\/Mini-batch\n#         return random_split(self, [train_size, test_size])   \n         \n        ","d620856a":"class MLP(nn.Module):\n    # during initialization we define: for each layer, (a) the number of features \n    # (b) the weights (c) the type of activation\n    def __init__(self, n_input, n_hidden1, n_hidden_2, n_out):\n        super(MLP, self).__init__()\n        ###########################\n        # hidden layer 1\n        self.hidden1 = nn.Linear(n_input, n_hidden1)\n        nn.init.kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = nn.ReLU()\n        \n        # hidden layer 2\n        self.hidden2 = nn.Linear( n_hidden1, n_hidden_2)\n        nn.init.kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = nn.ReLU()\n        \n        # Output layer \n        self.output = nn.Linear(n_hidden_2, n_out)\n        nn.init.xavier_uniform_(self.output.weight)\n        self.actout = nn.Sigmoid()\n        \n     # Now do Forward propagation\n    def forward(self, X):\n        # hidden layer 1\n        X = self.hidden1(X)\n        X = self.act1(X)\n        # hidden layer 2\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # output layer \n        X = self.output(X)\n        X = self.actout(X)\n        return X","c4f0ed2b":"def prepare_data(path):\n    # load the dataset\n    dataset = CSVDataset(path)\n    # calculate split\n    train, test = dataset.get_splits()\n    # prepare data loaders\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl","484fca92":"# prepare the data\npath = 'https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/ionosphere.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# print(train_dl.dataset)","a4e15d06":"# train the model\ndef train_model(train_dl, model):\n    # define the optimization\n    criterion = nn.BCELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # enumerate epochs\n    for epoch in range(100):\n        # enumerate mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs)\n            # calculate loss\n            loss = criterion(yhat, targets)\n            # credit assignment\n            loss.backward()\n            # update model weights\n            optimizer.step()\n        if (epoch+1)%10 == 0:\n            print(\"Loss for epch \", epoch+1, \"is \", loss.detach().numpy())\n            ","297f2f85":"def evaluate_model(train_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):        \n        # Get output using model (defined as MLP above)\n        yhat = model(inputs)            \n        # retrieve numpy array and round it\n        yhat = yhat.detach().numpy()\n        yhat = yhat.reshape(len(yhat), 1)            \n        yhat = yhat.round()\n        # actual values provided by data\n        actual = targets.numpy()\n        actual = actual.reshape(len(actual), 1)\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate accuracy\n    acc = accuracy_score(predictions, actuals)  ","889b178a":"# make a class prediction for one row of data\ndef predict(row, model):\n    # convert to torch\n    row = torch.tensor([row])\n    # prediction\n    yhat = model(row)\n    # retrive to numpy\n    yhat = yhat.detach().numpy()\n    return yhat","b7d7c547":"# define online path from where data can be downloaded\npath = 'https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/ionosphere.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))","9a3ee57a":"# define the network\nmodel = MLP(n_input=34, n_hidden1=10, n_hidden_2=8, n_out=1 )","d2ebcb45":"# train model\ntrain_model(train_dl, model)","8f9ef97a":"# evaluate the model\nacc = evaluate_model(test_dl, model)\nprint(acc)","091b7918":"# make a single prediction (expect class=1)\nrow = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\nyhat = predict(row, model)\nprint('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))","62701d1c":"# Test the above code whie writing\ndef test():\n    torch.manual_seed(42)\n    X = torch.randint(0,10, size = (4, 10))\n    y = torch.randint(0,2, size = (1, 10))\n    return X, y, len(X)\nX, y = test()\nprint(X,y)","90dca356":"    torch.manual_seed(42)\n    a = torch.randint(0,10, size = (2,3,4))\n    a, a[:,:-1], a[:,-1], a[1]\n    ","f019e07b":"### Make a CNN step-by-step using pytorch:","3e31cc66":"### Step-2: Make model\nWe make class which takes input and produce output\n","f499f3b4":"### Step-4: Run\nIn each step, we have made a class and made functions which uses the class to produce our predicted output. In the next few lines we only will call the functions (i.e., no need to call classes as functions have already done that job)\n","4b5ea50f":"### Step-3: Train model\n","de4d78d9":"Use the CSVDatatset class to make a function which return train and test set","81b264aa":"### Other practice codes:","d5098902":"### Step-1: We import the data and make it ready for training\nSo basically we will make a class which will, (a) download the data (b) Transfer to tesor (c) make mini-batches for the train and test"}}