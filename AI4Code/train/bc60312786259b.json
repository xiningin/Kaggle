{"cell_type":{"cc09673b":"code","e5f63b19":"code","9101a351":"code","1761d586":"code","ac31263d":"code","be699d4f":"code","9de458d3":"code","e328b55d":"code","b68a215c":"code","5ab07287":"code","abef5ba9":"code","7fcdd0dd":"code","114c866c":"code","40197ea0":"code","55f0c7e0":"code","77c851fb":"code","55e37262":"code","5381bc1b":"code","0362d7e4":"code","9c7f48e6":"code","ed52ce97":"code","ba409f2a":"code","f79ba9d3":"code","3d133bfd":"code","b23dc71a":"code","75425c48":"code","29811770":"code","cc82559c":"code","026b002f":"code","8c6f4b9d":"code","82de9e1c":"code","779294a4":"code","bc985c71":"code","9077c60b":"code","8dda05cb":"code","6c5fe44f":"code","c23b206c":"code","6d9dbf90":"code","6e3ad65a":"code","c9b23321":"code","d9ad8a78":"code","4a88cc74":"code","106f5ade":"code","6e8bd772":"code","7f8f5429":"code","a8e4623d":"code","eb1aa1a9":"code","3ff776de":"code","9bdec68e":"code","4f34a103":"code","dd1a0d95":"code","9c8abb8e":"code","d0241244":"code","df802d09":"code","77ee2cf3":"code","c9818555":"code","f8df54ca":"code","4edc99a4":"code","f03659b5":"code","65243558":"code","2597349f":"markdown","41b13093":"markdown","4354cbb1":"markdown","dbcbca6e":"markdown","7ae70d67":"markdown","2af50ae9":"markdown","2c7e5737":"markdown","466804f6":"markdown","f3d70497":"markdown","a7ab179f":"markdown","d8e572c0":"markdown","9fc048c4":"markdown","703438ad":"markdown","bb68febe":"markdown","d9f35cdd":"markdown","f7ff58bf":"markdown","60ab9682":"markdown","868581ec":"markdown","3a1e01a9":"markdown","8fdf0d1d":"markdown","cbfc31c1":"markdown","c596540a":"markdown","868bf2e6":"markdown","a0b60c75":"markdown","1474ce60":"markdown","d138613b":"markdown","cbf59b6f":"markdown","3347a918":"markdown","1cb7324e":"markdown","c90464de":"markdown","a6bdd0e5":"markdown","f5881546":"markdown","4e70014e":"markdown","10a20f13":"markdown","05678753":"markdown","682f7f99":"markdown","3695f9cc":"markdown","f88548ae":"markdown","c4354943":"markdown","a45652e4":"markdown","5706a90d":"markdown","faabd33e":"markdown","63e49a04":"markdown","40e0edcf":"markdown","6476d2a4":"markdown","b975e3b8":"markdown","e5e47973":"markdown","970c7d33":"markdown","1edd3e73":"markdown","120a0cda":"markdown","fe544010":"markdown","c33103c8":"markdown","812444d5":"markdown","ec9d39f1":"markdown","1665486f":"markdown","a5ee3506":"markdown","dbc5ec53":"markdown","933a3a0e":"markdown","7bd8828b":"markdown","229bfc9e":"markdown","f6dd2339":"markdown","2af69902":"markdown","ad8e9912":"markdown"},"source":{"cc09673b":"import os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e5f63b19":"#Load the packages\nimport pandas as pd          \nimport numpy as np          # For mathematical calculations \nimport matplotlib.pyplot as plt  # For plotting graphs \nfrom datetime import datetime    # To access datetime \nfrom pandas import Series        # To work on series \n%matplotlib inline \nimport warnings                   # To ignore the warnings \nwarnings.filterwarnings(\"ignore\")","9101a351":"# Read the data and store the original as a copy:\n\ntrain=pd.read_csv(\"..\/input\/Train.csv\") \ntest=pd.read_csv(\"..\/input\/Test.csv\")\n\ntrain_original=train.copy() \ntest_original=test.copy()","1761d586":"train.columns, test.columns","ac31263d":"train.dtypes, test.dtypes","be699d4f":"train.shape, test.shape","9de458d3":"train.head(), test.head()","e328b55d":"train['Datetime'] = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntest['Datetime'] = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \ntest_original['Datetime'] = pd.to_datetime(test_original.Datetime,format='%d-%m-%Y %H:%M') \ntrain_original['Datetime'] = pd.to_datetime(train_original.Datetime,format='%d-%m-%Y %H:%M')","b68a215c":"#Split the data based on the year, month, date and time\nfor i in (train, test, test_original, train_original):\n    i['year']=i.Datetime.dt.year \n    i['month']=i.Datetime.dt.month \n    i['day']=i.Datetime.dt.day\n    i['Hour']=i.Datetime.dt.hour ","5ab07287":"train['day of week']=train['Datetime'].dt.dayofweek \ntemp = train['Datetime']","abef5ba9":"#Define the weekends\ndef applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0 \ntemp2 = train['Datetime'].apply(applyer) \ntrain['weekend']=temp2","7fcdd0dd":"train.index = train['Datetime'] # indexing the Datetime to get the time period on the x-axis. \ndf=train.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Passenger Count') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')","114c866c":"# Plotting traffic over the years\ntrain.groupby('year')['Count'].mean().plot.bar()","40197ea0":"# Plotting traffic over the average months\ntrain.groupby('month')['Count'].mean().plot.bar()","55f0c7e0":"temp=train.groupby(['year', 'month'])['Count'].mean() \ntemp.plot(figsize=(16,5), title= 'Passenger Count(Monthwise)', fontsize=14)","77c851fb":"train.groupby('day')['Count'].mean().plot.bar()","55e37262":"train.groupby('Hour')['Count'].mean().plot.bar()","5381bc1b":"train.groupby('weekend')['Count'].mean().plot.bar()","0362d7e4":"train.groupby('day of week')['Count'].mean().plot.bar()","9c7f48e6":"#Drop the ID variable \ntrain=train.drop('ID',1)","ed52ce97":"train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Hourly time series \nhourly = train.resample('H').mean() \n# Converting to daily mean \ndaily = train.resample('D').mean() \n# Converting to weekly mean \nweekly = train.resample('W').mean() \n# Converting to monthly mean \nmonthly = train.resample('M').mean()","ba409f2a":"fig, axs = plt.subplots(4,1) \nhourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0]) \ndaily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1])\nweekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) \nmonthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) \n\nplt.show()","f79ba9d3":"test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \ntest.index = test.Timestamp  ","3d133bfd":"# Converting to daily mean \ntest = test.resample('D').mean() \ntrain.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Converting to daily mean \ntrain = train.resample('D').mean()","b23dc71a":"Train=train.ix['2012-08-25':'2014-06-24'] \nvalid=train.ix['2014-06-25':'2014-09-25']","75425c48":"Train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train') \nvalid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')\nplt.show()","29811770":"# Naive Approach\ndd= np.asarray(Train.Count) \ny_hat = valid.copy() \ny_hat['naive'] = dd[len(dd)-1] \nplt.figure(figsize=(12,8)) \nplt.plot(Train.index, Train['Count'], label='Train') \nplt.plot(valid.index,valid['Count'], label='Valid') \nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \nplt.legend(loc='best') \nplt.title(\"Naive Forecast\") \nplt.show()","cc82559c":"from sklearn.metrics import mean_squared_error \nfrom math import sqrt \nrms = sqrt(mean_squared_error(valid.Count, y_hat.naive)) \nprint(rms)","026b002f":"# Moving Average\n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(50).mean().iloc[-1] # average of last 50 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations') \nplt.legend(loc='best') \nplt.show()\n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(20).mean().iloc[-1] # average of last 20 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations') \nplt.legend(loc='best') \nplt.show() \n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(10).mean().iloc[-1] # average of last 10 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations') \nplt.legend(loc='best') \nplt.show() \n","8c6f4b9d":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.moving_avg_forecast)) \nprint(rms)","82de9e1c":"# Simple Exponential Smoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_avg = valid.copy() \nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,optimized=False) \ny_hat_avg['SES'] = fit2.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SES'], label='SES') \nplt.legend(loc='best') \nplt.show()","779294a4":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SES)) \nprint(rms)","bc985c71":"!pip install statsmodels==0.5.0\nimport statsmodels.api as sm\n\nsm.tsa.seasonal_decompose(Train.Count).plot() \nresult = sm.tsa.stattools.adfuller(train.Count) \nplt.show()","9077c60b":"# Holt\u2019s Linear Trend Model\ny_hat_avg = valid.copy() \nfit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \nplt.legend(loc='best') \nplt.show()","8dda05cb":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_linear)) \nprint(rms)","6c5fe44f":"#Holt\u2019s Linear Trend Model on daily time series\npredict=fit1.forecast(len(test))\ntest['prediction']=predict","c23b206c":"# Calculating the hourly ratio of count\ntrain_original['ratio']=train_original['Count']\/train_original['Count'].sum() \n\n# Grouping the hourly ratio \ntemp=train_original.groupby(['Hour'])['ratio'].sum() \n\n# Groupby to csv format \npd.DataFrame(temp, columns=['Hour','ratio']).to_csv('GROUPby.csv') \n\ntemp2=pd.read_csv(\"GROUPby.csv\") \ntemp2=temp2.drop('Hour.1',1) \n\n# Merge Test and test_original on day, month and year \nmerge=pd.merge(test, test_original, on=('day','month', 'year'), how='left') \nmerge['Hour']=merge['Hour_y'] \nmerge=merge.drop(['year', 'month', 'Datetime','Hour_x','Hour_y'], axis=1) \n# Predicting by merging merge and temp2 \nprediction=pd.merge(merge, temp2, on='Hour', how='left') \n\n# Converting the ratio to the original scale \nprediction['Count']=prediction['prediction']*prediction['ratio']*24 \nprediction['ID']=prediction['ID_y']","6d9dbf90":"# Holt winter\u2019s model on daily time series\ny_hat_avg = valid.copy() \nfit1 = ExponentialSmoothing(np.asarray(Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter') \nplt.legend(loc='best') \nplt.show()","6e3ad65a":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_Winter))\nprint(rms)","c9b23321":"# Dickey-Fuller test.\n\nfrom statsmodels.tsa.stattools import adfuller \ndef test_stationarity(timeseries):\n        #Determing rolling statistics\n    rolmean = timeseries.rolling(window=24).mean() # 24 hours on each day\n    rolstd = timeseries.rolling(window=24).std()\n        #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n        #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\nfrom matplotlib.pylab import rcParams \nrcParams['figure.figsize'] = 20,10\ntest_stationarity(train_original['Count'])","d9ad8a78":"Train_log = np.log(Train['Count']) \nvalid_log = np.log(valid['Count'])\nmoving_avg = Train_log.rolling(24).mean() \nplt.plot(Train_log) \nplt.plot(moving_avg, color = 'red') \nplt.show()","4a88cc74":"train_log_moving_avg_diff = Train_log - moving_avg","106f5ade":"train_log_moving_avg_diff.dropna(inplace = True)\ntest_stationarity(train_log_moving_avg_diff)","6e8bd772":"train_log_moving_avg_diff.dropna(inplace = True) \ntest_stationarity(train_log_moving_avg_diff)","7f8f5429":"train_log_diff = Train_log - Train_log.shift(1) \ntest_stationarity(train_log_diff.dropna())","a8e4623d":"from statsmodels.tsa.seasonal import seasonal_decompose \ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24) \n\ntrend = decomposition.trend \nseasonal = decomposition.seasonal \nresidual = decomposition.resid \n\nplt.subplot(411) \nplt.plot(Train_log, label='Original') \nplt.legend(loc='best') \nplt.subplot(412) \nplt.plot(trend, label='Trend') \nplt.legend(loc='best') \nplt.subplot(413) \nplt.plot(seasonal,label='Seasonality') \nplt.legend(loc='best') \nplt.subplot(414) \nplt.plot(residual, label='Residuals') \nplt.legend(loc='best') \nplt.tight_layout() \nplt.show()","eb1aa1a9":"train_log_decompose = pd.DataFrame(residual) \ntrain_log_decompose['date'] = Train_log.index \ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace=True) \ntest_stationarity(train_log_decompose[0])","3ff776de":"from statsmodels.tsa.stattools import acf, pacf \nlag_acf = acf(train_log_diff.dropna(), nlags=25) \nlag_pacf = pacf(train_log_diff.dropna(), nlags=25, method='ols')","9bdec68e":"# ACF and PACF plot\nplt.plot(lag_acf) \nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.title('Autocorrelation Function') \nplt.show() \nplt.plot(lag_pacf) \nplt.axhline(y=0,linestyle='--',color='gray') \nplt.axhline(y=-1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \nplt.axhline(y=1.96\/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function') \nplt.show()","4f34a103":"# AR model\n\nfrom statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model \nresults_AR = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_AR.fittedvalues, color='red', label='predictions') \nplt.legend(loc='best') \nplt.show()","dd1a0d95":"AR_predict=results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\")\nAR_predict=AR_predict.cumsum().shift().fillna(0)\nAR_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nAR_predict1=AR_predict1.add(AR_predict,fill_value=0) \nAR_predict = np.exp(AR_predict1)\nplt.plot(valid['Count'], label = \"Valid\") \nplt.plot(AR_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['Count']))\/valid.shape[0]))\nplt.show()","9c8abb8e":"# MA model\n\nmodel = ARIMA(Train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model \nresults_MA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(), label='original') \nplt.plot(results_MA.fittedvalues, color='red', label='prediction') \nplt.legend(loc='best') \nplt.show()","d0241244":"MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\") \nMA_predict=MA_predict.cumsum().shift().fillna(0) \nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \nMA_predict1=MA_predict1.add(MA_predict,fill_value=0) \nMA_predict = np.exp(MA_predict1)\nplt.plot(valid['Count'], label = \"Valid\") \nplt.plot(MA_predict, color = 'red', label = \"Predict\") \nplt.legend(loc= 'best') \nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['Count']))\/valid.shape[0])) \nplt.show()","df802d09":"# Combined model\n\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='original') \nplt.plot(results_ARIMA.fittedvalues, color='red', label='predicted') \nplt.legend(loc='best') \nplt.show()","77ee2cf3":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n\n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()\n\ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))\/given_set.shape[0]))\n    plt.show()","c9818555":"ARIMA_predict_diff=results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\ncheck_prediction_diff(ARIMA_predict_diff, valid)","f8df54ca":"# SARIMAX Model \nimport statsmodels.api as sm\ny_hat_avg = valid.copy() \nfit1 = sm.tsa.statespace.SARIMAX(Train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SARIMA'], label='SARIMA') \nplt.legend(loc='best') \nplt.show()","4edc99a4":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SARIMA)) \nprint(rms)","f03659b5":"predict=fit1.predict(start=\"2014-9-26\", end=\"2015-4-26\", dynamic=True)","65243558":"test['prediction']=predict\n# Merge Test and test_original on day, month and year \nmerge=pd.merge(test, test_original, on=('day','month', 'year'), how='left')\nmerge['Hour']=merge['Hour_y'] \nmerge=merge.drop(['year', 'month', 'Datetime','Hour_x','Hour_y'], axis=1) \n\n# Predicting by merging merge and temp2 \nprediction=pd.merge(merge, temp2, on='Hour', how='left') \n\n# Converting the ratio to the original scale \nprediction['Count']=prediction['prediction']*prediction['ratio']*24","2597349f":"This model gives a much better indicator of the dataset.","41b13093":"An extension to ARIMA that supports the direct modeling of the seasonal component of the series is called SARIMAX.\n\nThis supports both an autoregressive and moving average elements.\n\nAn extension to ARIMA that supports the direct modeling of the seasonal component of the series is called SARIMA. on daily time series.\n","4354cbb1":"Since the test statistic is lesser than the critical value, the time series is stationary. However, there is an increase in the dataset. Therefore we will remove the seasonality and the trend.\n\nWe will take a rolling average of 24 hours to remove the trend","dbcbca6e":"We will now combine and compare the ARIMA and the MA models","7ae70d67":"This forecasting technique predicts the expected point to be the average of the last few observed points.\n\nDepending on the number of samples that we take, it can be inferred that the weights will differ as it still does not give an accurate estimation.\n","2af50ae9":"A seasonal pattern are periodic fluctuations of a fixed and known period that exist when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).","2c7e5737":"We will split the data into a more granular approach:","466804f6":"### Naive Approach","f3d70497":"An extension of the simple exponential smoothing, Holt\u2019s Linear Trend Model is used to forecast the time-series based on the seasonality of the data.\n\nBefore we deep dive into the model, there are a few factors that should be explained with respect to a time-series data. This can be broken down into 4 parts:\n\ni) Observed: The original time series.\n\nii) Trend: This shows the trend in the time series, i.e., increasing or decreasing nature of the time series.\n\niii) Seasonality: This shows the seasonality in the time series.\n\niv) Residual: This is obtained by removing any trend or seasonality in the time series.\n\nLet us visualize these features:\n","a7ab179f":"We plot the validataion curve of the AR model after scaling it.","d8e572c0":"Let\u2019s check the stationarity for residuals.","9fc048c4":"As we can see, this is unsuitable for datasets with high variablilty. let us look at out next forcasting model:","703438ad":"### Moving Average","bb68febe":"### SARIMAX Model.","d9f35cdd":"##  Exploratory Analysis","f7ff58bf":"As we can see, the predections are better for a smaller sample, but it still needs a long way  to go for a clearer forecasting.","60ab9682":"The monthy traffic analysis seems even.\n\nLet us look at the hourly analysis.","868581ec":"### Forecasting the time series using ARIMA","3a1e01a9":"### ARIMA model","8fdf0d1d":"### AR model\n","cbfc31c1":"From here, traffic starts increasing at 6AM, peaks at noon and decreases slighly till 5PM. From here, it peaks again at 7PM and sharply decreases with the cycle continuing again.\n\n\nLet's observe the traffic during the weekday and weekend ","c596540a":"We will stabilize the mean by differentiating.","868bf2e6":"### Removing Seasonality","a0b60c75":"## Time-series analysis","1474ce60":"## Feature Extraction","d138613b":"With this, we will fit the ARIMA model on our time series for that we have to find the optimized values for the p, d, q parameters.\n\nTo find the optimized values of these parameters, we will use ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) graph.\n\nACF is a measure of the correlation between the Time Series with a lagged version of itself.\n\nPACF measures the correlation between the Time Series with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons.\n","cbf59b6f":"## Conclusions ","3347a918":"The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.","1cb7324e":"## Introduction","c90464de":"### Holt\u2019s Linear Trend Model","a6bdd0e5":"The Auto Regression Integrated Moving Average (ARIMA) is specified by three ordered parameters (p, d, q) which are described as follows:\n\np: The order of the autoregressive model (number of time lags)\n\nd: The degree of differencing (number of times the data have had past values subtracted)\n\nq: The order of moving average model.\n","f5881546":"We will now scale the models to the original scale:","4e70014e":"## Do Reach Out!\n\nIf you found something that catches your eye or just have a chat about data science topics in general, I will be more than happy to connect with you on:\n\nLinkedIn: https:\/\/www.linkedin.com\/in\/shawn-dsouza\/\n\nMy Website: https:\/\/shawndsouza29.wixsite.com\/portfolio\n\nThis notebook will always be a work in progress. Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated. Thank you guys!\n","10a20f13":"After loading the train and test datasets, let\u2019s observe a basic highlight of our data:","05678753":"From this, the weekends have less traffic than the weekdays.","682f7f99":"### Simple Exponential Smoothing","3695f9cc":"From a quick glance, one can see the increasing traffic gowth over time with noticeable spikes.","f88548ae":"Let\u2019s look at the basic time series plot:","c4354943":"Since the error has reduced considerably, it would mean that this model offers a silght imporvement when compared to others. Then again, why stop here? Let's see if other models can best this.","a45652e4":"From the results, the residuals are stationary.","5706a90d":"It makes it simpler to figure the day of the week based on the dataset we have where 0 is Monday and 6 is Sunday","faabd33e":"Since we get the data in terms of daily data, we will break down into the hourly analysis.","63e49a04":"We will remove this increasing trend to make our time series stationary.","40e0edcf":"The autoregressive model specifies that the output variable depends linearly on its own previous values.","6476d2a4":"### Moving Average","b975e3b8":"We can find the difference in the predicton and actual validation using the Root Mean Square Error (RMSE). This is used to calculate the accuraccy of the prediction.","e5e47973":"This forecasting approach predicts the expected point to be the last observed point.\n\nThis is very rudimentary and will give a high bias to our prediction.\n","970c7d33":"# Gazing into the Data Abyss- Time Series Traffic Analysis","1edd3e73":"This project was an eye-opener in discovering time-series modelling and what it can build on, espectially us implications on a lot of continous data.\n\nUltimately, change is the only constant and we are closer than ever to predict what can occur next.\n","120a0cda":"The error has reduced, meaning that there is some improvement with the model.","fe544010":"With this, it would be a better idea to split the data in terms of day, date, month, year and time to give a granular analysis of what we are working with.\n\nLet\u2019s look at the basic time series plot:\n","c33103c8":"We can observe an increasing trend can be seen in the dataset, so now we will make a model based on the trend.","812444d5":"Now, let us observe the Holt's Model for forecasting the model ","ec9d39f1":"From the graph above, we can see that the traffic data coolected has stts in the middle of 2012 and ends 2014 is missing the last three months. This means the dataset is incomplete year-wise, but it will not affect the overall analysis.\n\nLet's look at the traffic analysis from each month. ","1665486f":"The basic requirement for an ARIMA estimator is that the time series should be stationary. i.e it should follow the following trends:\n\ni)The mean of the series should not be a function of time, rather should be a constant.\n\nii) The variance of the series should not a be a function of time.\n\niii) The covariance of the i th term and the (i + m) th term should not be a function of time.\n\n\n\nThe Dickey-Fuller test checks if the data is stationary.","a5ee3506":"Since the trend has an upward slope, it takes into factor the increasing trend. ","dbc5ec53":"Based on the dataset, there are a few forcasting methods that come to mind when it comes to time-series data. The most prominent ones are as follows:\n\ni) Naive Approach\n\nii) Moving Average\n\niii) Simple Exponential \nSmoothing\n\niv) Holt\u2019s Linear Trend Model\n\nLet us look into their functionalutes below:\n","933a3a0e":"## Understanding the dataset","7bd8828b":"From these graphs, the q value is the lag value where the ACF chart crosses the upper confidence interval for the first time. Here it is found at 1.\n\nThe p value is the lag value where the PACF chart crosses the upper confidence interval for the first time. Here it is found at 1.\n","229bfc9e":"Change. The one true constant. Often we see a flurry of events, through their small changes will alter the course of operations.\n\nOne of the most prominent metrics for a when analyzing change comes from a time-series analysis of data. This project is to delve deeper into the concept of changes that occur across time and the insights behind them.\n\nThis dataset looks at the traffic analysis of a rail road over 18 months. The dataset can be found at:\n\nhttps:\/\/courses.analyticsvidhya.com\/courses\/creating-time-series-forecast-using-python\n\nWith that, shall we dive into the data?\n","f6dd2339":"Since we took the average of 24 values, rolling mean is not defined for the first 23 values. So let\u2019s drop those null values.","2af69902":"From now on, we shall be focusing on the analyzing the time-series aspect of the data. We will split the training and validation parts, with the last 3 months reserved for validation testing","ad8e9912":"Here, larger weights are assigned to the more recent observations. This provides a more accurate portrayal of the features preset.\n"}}