{"cell_type":{"8c0356c3":"code","24d589bd":"code","29054f8d":"code","629c1d15":"code","21f1d65d":"code","d212b036":"code","363923d7":"code","2229f0d1":"code","a375eaca":"code","e874854f":"code","59b72651":"code","2ec12310":"code","ea8ec659":"code","c5494b54":"code","e42f57a8":"code","f8ab49d6":"code","9ec571ee":"code","515de665":"code","965d1829":"code","02c1fee2":"code","083455a4":"code","4e86c471":"code","36caa68b":"code","c73d33f4":"code","e56f8bd2":"code","11ec5e9b":"code","1f8c696b":"code","4f1fc564":"code","e7e19608":"code","74d8cb6a":"code","4ee97291":"code","982d90e0":"code","ccdfd87f":"code","47fcc061":"code","ca483f35":"code","0263afb7":"code","b64c660a":"code","6a0c2f95":"code","ecb648cc":"code","b06cd8d5":"code","8f29d91e":"code","003008c0":"code","6d1352ad":"code","95f04da3":"code","a4fc3c45":"code","f7b6139f":"code","7b892b68":"code","de22b919":"code","7fa32276":"code","aaedb3f3":"code","d8d9efce":"code","190ff197":"code","f39f9b67":"code","f38c3eed":"code","84b9660f":"code","ff4a165c":"code","24240623":"code","cfda645e":"code","6d44b776":"code","74630084":"code","fe2182a6":"markdown","303d8f14":"markdown","4f5ad7f1":"markdown","6c0abc0c":"markdown","cb783738":"markdown","1bed83a5":"markdown","76da5242":"markdown","f52e5577":"markdown","c0212fa1":"markdown","1eff3c7b":"markdown","4998e156":"markdown","5828b982":"markdown","83163d78":"markdown","079ef872":"markdown","54a0f51d":"markdown","7fe48f72":"markdown","e28f3473":"markdown","934dd35a":"markdown","b9eb72e1":"markdown","9c5d7baa":"markdown","2edb506e":"markdown","25f43dae":"markdown"},"source":{"8c0356c3":"\n#First we import all the libraries\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.utils import resample","24d589bd":"target = 'failure'   #defining a global variable","29054f8d":"\ndf_train = pd.read_csv(\"\/kaggle\/input\/hard-drive-data-and-stats\/data_Q3_2019\/data_Q3_2019\/2019-07-09.csv\")  #Training Dataset\n\ndf_test = pd.read_csv(\"\/kaggle\/input\/hard-drive-data-and-stats\/data_Q3_2019\/data_Q3_2019\/2019-07-10.csv\")    #Test Dataset\n","629c1d15":"df_train.head()","21f1d65d":"df_train.describe()","d212b036":"df_test.head()","363923d7":"df_test.describe()","2229f0d1":"print(df_train.shape)\nprint('*'*50)\nprint(df_test.shape)","a375eaca":"sns.countplot(df_train['failure'])    #Checking the distribution of the target variable","e874854f":"\nvalid = df_train[df_train['failure'] == 0]    #data of HDDs which do not indicate failure\nfailed = df_train[df_train['failure'] == 1]   #data of HDDs likely to fail\n\nprint(\"valid hdds:\",len(valid))      #storing the total number of valid HDDs\nprint(\"failing hdds:\",len(failed))    #storing the total number of HDDs likely to fail","59b72651":"#Since the number of HDDs indicating failure are too low, we proceed to upsample the minority class viz.'failure'\n\n# We perform this step to prevent our final model from being biased\n\n   #Resampling of the failure class to match the length of valid HDDs\n\nfailed_up = resample(failed,replace=True,n_samples=len(valid),random_state=27)  ","2ec12310":"#Finally we concatenate our newly resampled classes with our training data\n\ndf_train = pd.concat([valid,failed_up])\ndf_train.failure.value_counts()       #Levelling the count of both classes","ea8ec659":"sns.countplot(df_train['failure'])","c5494b54":"df_train","e42f57a8":"df_train.shape  # You can notice the dimensions have doubled for our training dataset","f8ab49d6":"df_train.isnull().sum()","9ec571ee":"# features which highly correlate to HDD failure as per BackBlaze \n\n# https:\/\/www.backblaze.com\/blog\/what-smart-stats-indicate-hard-drive-failures\/\n\n# SMART 5 \t\tReallocated Sectors Count\n# SMART 187 \t\tReported Uncorrectable Errors\n# SMART 188 \t\tCommand Timeout\n# SMART 197 \t\tCurrent Pending Sector Count\n# SMART 198 \t\tUncorrectable Sector Count","515de665":"features = ['date',\n 'serial_number',\n 'model',\n 'capacity_bytes',\n 'failure',\n'smart_5_raw','smart_187_raw','smart_188_raw','smart_197_raw','smart_198_raw']","965d1829":"misc_feat = [fname for fname in df_train if fname not in features]  #misc features to be dropped \nmisc_feat","02c1fee2":"df_train.drop(misc_feat,inplace=True,axis=1)  #Dropping the misc features","083455a4":"df_train","4e86c471":"# Since our model cannot proccess string values, we remove the columns which contain string values\/object values \n# to avoid errors\n\nobj = df_train.dtypes[df_train.dtypes == object ].index  \nobj","36caa68b":"df_train = df_train.drop(obj,axis=1)","c73d33f4":"df_train.isnull().sum()  #Total number of missing values ","e56f8bd2":"#After going through the dataset,i found that the drives which were missing values did not correlate to its failure\n\n#  i.e all drives indicating failure did not contain missing values\n\n# Hence i replaced them with the most commonly occuring values for the respective SMART attributes\n\ndf_train['smart_187_raw'] = df_train['smart_187_raw'].fillna(0)  ","11ec5e9b":"df_train['smart_5_raw'] = df_train['smart_5_raw'].fillna(0)","1f8c696b":"df_train['smart_188_raw'] = df_train['smart_188_raw'].fillna(0)","4f1fc564":"df_train['smart_197_raw'] = df_train['smart_197_raw'].fillna(0)","e7e19608":"df_train['smart_198_raw'] = df_train['smart_198_raw'].fillna(0)","74d8cb6a":"df_train.isnull().sum()","4ee97291":"df_train = df_train.drop('capacity_bytes',axis=1)","982d90e0":"df_train","ccdfd87f":"X_train = df_train.drop('failure',axis=1)\nY_train = df_train['failure']","47fcc061":"valid_test = df_test[df_test['failure'] == 0]\nfailed_test = df_test[df_test['failure'] == 1]\n\nprint(\"valid hdds:\",len(valid_test))\nprint(\"failing hdds:\",len(failed_test))","ca483f35":"failed_up_test = resample(failed,replace=True,n_samples=len(valid),random_state=27) #Same steps as in Training data","0263afb7":"df_test = pd.concat([valid_test,failed_up_test])\ndf_test.failure.value_counts()","b64c660a":"df_test.head()","6a0c2f95":"df_test.shape","ecb648cc":"df_test.drop(misc_feat,inplace=True,axis=1) #Since we have the imp features, we move ahead to drop the misc ones","b06cd8d5":"df_test","8f29d91e":"df_test['smart_187_raw'] = df_test['smart_187_raw'].fillna(0)","003008c0":"df_test['smart_5_raw'] = df_test['smart_5_raw'].fillna(0)","6d1352ad":"df_test['smart_188_raw'] = df_test['smart_188_raw'].fillna(0)","95f04da3":"df_test['smart_197_raw'] =df_test['smart_197_raw'].fillna(0)","a4fc3c45":"df_test['smart_198_raw'] = df_test['smart_198_raw'].fillna(0)","f7b6139f":"df_test.isnull().sum()","7b892b68":"df_test = df_test.drop(obj,axis=1)","de22b919":"df_test = df_test.drop('capacity_bytes',axis=1)","7fa32276":"df_test","aaedb3f3":"X_test = df_test.drop('failure',axis=1)\n","d8d9efce":"Y_test = df_test['failure']","190ff197":"df_test.shape","f39f9b67":" #Building the Random Forest Classifier (RANDOM FOREST) \nfrom sklearn.ensemble import RandomForestClassifier \n\n# random forest model creation \nrfc = RandomForestClassifier() \nrfc.fit(X_train, Y_train) \n\n# predictions(Notice the caps'P' of yPred to differentiate between model 1 and 2) \nyPred = rfc.predict(X_test) ","f38c3eed":"#Results of our predictions\n\nfrom sklearn.metrics import classification_report, accuracy_score  \nfrom sklearn.metrics import precision_score, recall_score \nfrom sklearn.metrics import f1_score, matthews_corrcoef \nfrom sklearn.metrics import confusion_matrix \n  \nn_outliers = len(failed) \nn_errors = (yPred != Y_test).sum()        #Notice the Y_test from iii) of Test Data\nprint(\"Model used is: Random Forest classifier\") \n  \nacc = accuracy_score(Y_test, yPred) \nprint(\"The accuracy is {}\".format(acc)) \n  \nprec = precision_score(Y_test, yPred) \nprint(\"The precision is {}\".format(prec)) \n  \nrec = recall_score(Y_test, yPred) \nprint(\"The recall is {}\".format(rec)) \n  \nf1 = f1_score(Y_test, yPred) \nprint(\"The F1-Score is {}\".format(f1)) \n  \nMCC = matthews_corrcoef(Y_test, yPred) \nprint(\"The Matthews correlation coefficient is {}\".format(MCC)) \n","84b9660f":" # confusion matrix \n\nLABELS = ['Healthy', 'Failed'] \nconf_matrix = confusion_matrix(Y_test, yPred) \nplt.figure(figsize =(12, 12)) \nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, fmt =\"d\"); \nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show() ","ff4a165c":"xData = X_train.values\nyData = Y_train.values","24240623":"from sklearn.model_selection import train_test_split \n\n# Splitting of data into training and testing sets \nxTrain, xTest, yTrain, yTest = train_test_split( \n        xData, yData, test_size = 0.2, random_state = 42) ","cfda645e":"\n#Random Forest Classifier \nfrom sklearn.ensemble import RandomForestClassifier \n\n# RF model creation \nrfc = RandomForestClassifier() \nrfc.fit(xTrain, yTrain) \n\n# predictions (notice the small 'p' to differentiate from model 1) \nypred = rfc.predict(xTest) \n","6d44b776":"from sklearn.metrics import classification_report, accuracy_score  \nfrom sklearn.metrics import precision_score, recall_score \nfrom sklearn.metrics import f1_score, matthews_corrcoef \nfrom sklearn.metrics import confusion_matrix \n  \nn_outliers = len(failed) \nn_errors = (ypred != yTest).sum()                             #yTest from the Train_Test_Split function\nprint(\"Model used is : Random Forest classifier\") \n  \nacc = accuracy_score(yTest, ypred) \nprint(\"The accuracy is {}\".format(acc)) \n  \nprec = precision_score(yTest, ypred) \nprint(\"The precision is {}\".format(prec)) \n  \nrec = recall_score(yTest, ypred) \nprint(\"The recall is {}\".format(rec)) \n  \nf1 = f1_score(yTest, ypred) \nprint(\"The F1-Score is {}\".format(f1)) \n  \nMCC = matthews_corrcoef(yTest, ypred) \nprint(\"The Matthews correlation coefficient is{}\".format(MCC)) \n","74630084":"# confusion matrix \n\nLABELS = ['Normal', 'Failed'] \nconf_matrix = confusion_matrix(yTest, ypred) \nplt.figure(figsize =(12, 12)) \nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, fmt =\"d\"); \nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show() ","fe2182a6":"\n# Step 2: Building the model using Random Forest:","303d8f14":"*  Handling missing values","4f5ad7f1":"## This notebook contains:\n1. Performing EDA on  'Hard Disk Drive Failure' dataset provided on Kaggle\n2. Using ML to build a model which can predict whether a HDD is likely to fail or not. ","6c0abc0c":"*   Splitting the values for X_train and Y_train ","cb783738":"*    Upsampling of test data to match the dimensionality of the test and train data (optional)\n####  Note : You can skip this step if using TrainTestSplit function","1bed83a5":" ## Model 1: RF Using X_test and Y_test\n### Note : Please refer Model 2 if using Train_Test_Split","76da5242":"*   Feature Selection for test data","f52e5577":"## That's it folks! Thank you for your time!\n## Would love to hear your comments and valuable feedback!  :)","c0212fa1":" ## Model 2: RF Using the Train_Test_Split Method ","1eff3c7b":"## Distribution of target variable (Failure)\n","4998e156":"*  Upsampling the minority class for train data","5828b982":"*  Selecting features with high correlation to hdd failure:","83163d78":"###  For the training data:","079ef872":"##   Balancing the dataset","54a0f51d":"##  Splitting values for X_test and Y_test (Optional)\n #### Note: Please skip this step if using TrainTestSpilt Method","7fe48f72":"# Step 1: Exploratory Data Analysis (EDA)","e28f3473":"### Dataset : https:\/\/www.kaggle.com\/jackywangkaggle\/hard-drive-data-and-stats","934dd35a":" ##   Feature Selection and filling of missing values","b9eb72e1":"# Predicting Hard Disk Drive Failures using Machine Learning","9c5d7baa":"###    Filling out missing values for test data","2edb506e":" ####  We perform this step as our model cannot use NaN data","25f43dae":"###  For Test data:"}}