{"cell_type":{"bd0d6a2b":"code","934ecb5a":"code","2e06e7c6":"code","2c307619":"code","c8f4033b":"code","6c7ef24d":"code","6e144d0d":"code","18d31347":"code","39798def":"code","056edb53":"code","492ad548":"code","30ba6f9b":"code","34864b86":"code","0be5d562":"code","5851f45d":"code","5f61933d":"code","c5d3e404":"code","88f05166":"code","5a156bfb":"code","7e8bc81c":"code","22b24d53":"code","db4842b9":"code","7091c468":"code","fec98f9e":"code","78b6bbc4":"code","39926ad6":"code","c2bf451d":"code","58e52a26":"code","bfc77abc":"code","f2cb0c81":"code","39602302":"code","8cdfdb5b":"code","e5c39e75":"code","dc330fbf":"code","5f13480d":"code","4bf12c8f":"code","9bd09cd9":"code","76bbe291":"code","16ae1c69":"code","760f3776":"code","f53ed7bc":"code","5b46c2ea":"markdown","e235a6d2":"markdown","a87e9dcc":"markdown","2662b335":"markdown","8189a008":"markdown","56188537":"markdown","da0a4602":"markdown","6e409de2":"markdown","7b985a3b":"markdown","9a0364e1":"markdown","e60dffe7":"markdown","54f61802":"markdown","69fab02b":"markdown","67156860":"markdown","1e758916":"markdown","8a5f96b3":"markdown","2dd36f3f":"markdown","cfefb093":"markdown","39cc91fe":"markdown","d1610e31":"markdown","5c7634dc":"markdown","a038a5a8":"markdown"},"source":{"bd0d6a2b":"import soundfile\nimport os, glob, pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom numpy.random import seed\nimport tensorflow \n# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","934ecb5a":"SAVEE = \"\/kaggle\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/\"\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"","2e06e7c6":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral_medium', 2:'neutral_medium', 3:'happy_low', 4:'sad_high', 5:'angry_high', 6:'fear_high', 7:'disgust_low', 8:'surprise_medium'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","2c307619":"# Pick a fearful track\nfname = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","c8f4033b":"# Pick a happy track\nfname = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","6c7ef24d":"df = pd.concat([ RAV_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","6e144d0d":"# Import our libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport os\nimport IPython.display as ipd  # To play sound in the notebook","18d31347":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook\nfrom numpy.random import seed\nimport tensorflow \n\nimport soundfile # to read audio file\nimport numpy as np\nimport librosa # to extract speech features\nimport glob\nimport os\nimport pickle # to save model after training\nfrom sklearn.model_selection import train_test_split # for splitting training and testing\nfrom sklearn.neural_network import MLPClassifier # multi-layer perceptron model\nfrom sklearn.metrics import accuracy_score # to measure how good we are","39798def":"ref = df\nref.head()","056edb53":"plt.figure(figsize=(20, 8))\nsns.countplot('labels', data=df)","492ad548":"# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","30ba6f9b":"# Now extract the mean bands to its own feature columns\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","34864b86":"# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]","0be5d562":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","5851f45d":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Check the dataset now \nX_train[150:160]","5f61933d":"\n# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n\n\n","c5d3e404":"\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","88f05166":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","5a156bfb":"# best model, determined by a grid search\nmodel_params = {\n    'alpha': 0.01,\n    'batch_size': 256,\n    'epsilon': 1e-08, \n    'hidden_layer_sizes': (300,), \n    'learning_rate': 'adaptive', \n    'max_iter': 500, \n}","7e8bc81c":"# building the model:\nmodel = Sequential()\nmodel.add(Conv1D(64, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(pool_size=(5)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(pool_size=(5)))\n#model.add(Dropout(0.2))\n\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-9)\nmodel.summary()","22b24d53":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n\n","db4842b9":"# Calculate pre-training accuracy \nscore = model.evaluate(X_test, y_test, verbose=1)\naccuracy = 100*score[1]\n\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy) ","7091c468":"\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(patience=2, verbose=1)\n\n","fec98f9e":"\n    model_history = model.fit(X_train, y_train,\n                               batch_size=64, epochs=32,\n                               validation_data=(X_test, y_test)), #callbacks=[early_stopping])\n","78b6bbc4":"# Evaluating the model on the training and testing set\nscore = model.evaluate(X_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1]*100)\n\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1]*100)","39926ad6":"score = model.evaluate(X_test, y_test, batch_size=64 ,verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))","c2bf451d":"pd.DataFrame(model_history.history).plot()","58e52a26":"   # Loss plotting\n        plt.plot(model_history.history['loss'])\n        plt.plot(model_history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n        plt.savefig('loss.png')\n        plt.close()\n","bfc77abc":" # Accuracy plotting\n        plt.plot(model_history.history['acc'],color='red',label='acc')\n        plt.plot(model_history.history['val_acc'], color='blue',label='val_acc')\n        plt.title('model accuracy')\n        plt.ylabel('acc')\n        plt.xlabel('epoch')\n        plt.legend(loc='best',shadow=True)\n        plt.show()      \n        plt.legend(['train', 'test'], loc='upper left')\n\n\n","f2cb0c81":"# Save model and weights\nmodel_name = 'Emotion_Priority_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","39602302":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Priority_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","8cdfdb5b":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds\n","e5c39e75":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","dc330fbf":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","5f13480d":"# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# emotion and priority recode function\ndef emotion(row):\n    if row == 'female_disgust_low' or 'male_disgust_low':  \n        return 'disgust'\n    elif row == 'male_angry_high' or 'female_angry_high': \n        return 'angry'\n    elif row=='female_fear_high' or 'male_fear_high':\n        return'fear'\n    elif row=='female_happy_low'or 'male_happy_low':\n        return'happy'\n    elif row=='female_sad_high'  or 'male_sad_high' :\n        return'sad'\n    elif row== 'female_surprise_medium' or 'male_surprise_medium' :\n        return'suprised'\n    elif row=='female_neutral_medium'or 'male_neutral_medium':\n        return'neutral'\ndef priority(row):\n    if row == 'female_disgust_low' or 'male_happy_low' or 'female_happy_low' or 'male_disgust_low':\n        return 'low'\n    elif row == 'male_angry_high' or 'male_fear_high' or 'male_sad_high'or 'female_fear_high' or 'female_sad_high'or 'female_angry_high':\n        return 'high'\n    elif row ==  'male_surprise_medium' or 'male_neutral_medium'or 'female_surprise_medium' or 'female_neutral_medium':\n        return 'medium'","4bf12c8f":"\n\n# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)\n\n","9bd09cd9":"\n\n# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))\n\n","76bbe291":"modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry_high':'high'\n                                       , 'female_disgust_low':'low'\n                                       , 'female_fear_high':'high'\n                                       , 'female_happy_low':'low'\n                                       , 'female_sad_high':'high'\n                                       , 'female_surprise_medium':'medium'\n                                       , 'female_neutral_medium':'medium'\n                                       , 'male_angry_high':'high'\n                                       , 'male_fear_high':'high'\n                                       , 'male_happy_low':'low'\n                                       , 'male_sad_high':'high'\n                                       , 'male_surprise_medium':'medium'\n                                       , 'male_neutral_medium':'medium'\n                                       , 'male_disgust_low':'low'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry_high':'high'\n                                       , 'female_disgust_low':'low'\n                                       , 'female_fear_high':'high'\n                                       , 'female_happy_low':'low'\n                                       , 'female_sad_high':'high'\n                                       , 'female_surprise_medium':'medium'\n                                       , 'female_neutral_medium':'medium'\n                                       , 'male_angry_high':'high'\n                                       , 'male_fear_high':'high'\n                                       , 'male_happy_low':'low'\n                                       , 'male_sad_high':'high'\n                                       , 'male_surprise_medium':'medium'\n                                       , 'male_neutral_medium':'medium'\n                                       , 'male_disgust_low':'low'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","16ae1c69":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","760f3776":"modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry_high':'angry'\n                                       , 'female_disgust_low':'disgust'\n                                       , 'female_fear_high':'fear'\n                                       , 'female_happy_low':'happy'\n                                       , 'female_sad_high':'sad'\n                                       , 'female_surprise_medium':'surprise'\n                                       , 'female_neutral_medium':'neutral'\n                                       , 'male_angry_high':'angry'\n                                       , 'male_fear_high':'fear'\n                                       , 'male_happy_low':'happy'\n                                       , 'male_sad_high':'sad'\n                                       , 'male_surprise_medium':'surprise'\n                                       , 'male_neutral_medium':'neutral'\n                                       , 'male_disgust_low':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry_high':'angry'\n                                       , 'female_disgust_low':'disgust'\n                                       , 'female_fear_high':'fear'\n                                       , 'female_happy_low':'happy'\n                                       , 'female_sad_high':'sad'\n                                       , 'female_surprise_medium':'surprise'\n                                       , 'female_neutral_medium':'neutral'\n                                       , 'male_angry_high':'angry'\n                                       , 'male_fear_high':'fear'\n                                       , 'male_happy_low':'happy'\n                                       , 'male_sad_high':'sad'\n                                       , 'male_surprise_medium':'surprise'\n                                       , 'male_neutral_medium':'neutral'\n                                       , 'male_disgust_low':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","f53ed7bc":"# Classification report \nclasses = modidf.actualvalues.unique()\n\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","5b46c2ea":" we're going to read each audio file, extract its mean across all MFCC bands by time, \n and just keep the extracted features, dropping the entire audio file data.","e235a6d2":"when changing the layers from decreasing to increasing layers the accuracy changed to better ","a87e9dcc":"**Now** predicting emotions on the test data. After serialising the model above, i'm going to just reload it into disk. Essentially to re-use the model without having to retrain by re-running the code, we just need to run this section of the code and apply the model to a new dataset. Since we used the same test set in the keras model, the result is essentially the same as the last epoch of 40 which is 42.50%","2662b335":"it's a 2D matrix of the number of bands by time. In order to optimise space and memory,\n\n","8189a008":"\n\nWith just priority we get a 57% accuracy. The model is especially precise in capturing medium priority .","56188537":"1. i tried batch size 16 and epoch 15\naccuracy=34.44%\n\n2. i tried batch size 32 and let the epoch stay the same 15 \naccuracy =33.33%\n\n3.  i tried batch size 32 and epoch 25 \naccuraccy =39.17%\n\n**i just want it to reach 50 %**\n\n4.  batch size 26 and epoch 25\naccuracy=38.61%\n\n5. try batch size 10 epoch 100\naccuracy=40.83 %\n\naccuracy wouldnt change after epoch 100\nso i lowered it\n\n6. batsh size =32 and epoch=35\naccuracy 41.39 %\n\n7. batch size=32 and epoch=40\naccuracy 42.50 % \nthe best accuracy i could reach \nthe accuracy changed after commit it droped to 36.5%\n8. batch size 16 , epoch=100\naccuracy =accuracy 40.28 %\n9. batch size 16 , epoch=100\naccuracy=42.50 %\n** optimizer Adam was used with all the above\n10. batch size 16, epoch=100 with optimizer=rmsprop\naccuracy=38.89 %\n11. it turns out the best is batsh size=64 and epochs =32\n\naccuracy= 51.11%","da0a4602":"# 6. Visualizing Data","6e409de2":"the accuracy of predicting priority (level of emergency ) is  high ","7b985a3b":"\n\nNow that looks alot better. Next step we will split the data into 2 parts, one for training and one for validation. This ensures we measure the model's performance at its true accuracy.\n","9a0364e1":"see the diffrenece in the number of bands by time , this will be the feature we use ","e60dffe7":"\n\nThe prediction is in the form of numbers, we'll need to append the labels to it before we run the accuracy measure...\n","54f61802":"# 1.Building model","69fab02b":"normalise the data. This is proven to improve the accuracy and speed up the training process.","67156860":"# 3.Evaluating the model","1e758916":"# 5. predictions","8a5f96b3":"# 4. Summarize result **","2dd36f3f":"now we will group them and check accuracy for priority and emotion ","cfefb093":"\n# 2.Training the model \n","39cc91fe":" CNN, we need to specify the 3rd dimension, which for us is 1.Its 1 because we're doing a 1D CNN","d1610e31":"Using MFCC is a good feature to differentiate the emotions as demonstrated above. Even thou we've ommited alot of good information by just taking the mean, it seems we still capture enough to be able to see some difference.","5c7634dc":"# Emotion accuracy\u00b6","a038a5a8":"# 7. priority accuracy "}}