{"cell_type":{"5ce4cc1d":"code","b82d6212":"code","670c3fed":"code","bc0ba50c":"code","03985216":"code","6942671f":"code","09f4c495":"code","c9b8f9f5":"code","e6d9708e":"code","3e060c3f":"code","cef41949":"code","a33a5d71":"code","b53a8b79":"code","ea9b7334":"code","673ea937":"code","dea1177c":"code","cdce6321":"code","3a1a2463":"code","69e4409a":"code","153acef6":"code","94284ece":"code","8fa471d7":"code","76efd722":"code","a632e99b":"code","16f03b16":"code","ef56c507":"markdown","277a3ed5":"markdown"},"source":{"5ce4cc1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b82d6212":"# Setup plotting\nimport matplotlib.pyplot as plt\nfrom learntools.deep_learning_intro.dltools import animate_sgd\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')","670c3fed":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/train.csv')\n\nX = train.copy()\n# Remove target\ny = X.pop('target')\nX = X.drop(['row_id'], axis=1)","bc0ba50c":"targets = y.unique()\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(targets)\ny = le.transform(y)","03985216":"X.head()","6942671f":"y[:5]","09f4c495":"preprocessor = make_column_transformer(\n    (StandardScaler(),\n     make_column_selector(dtype_include=np.number)),\n    (OneHotEncoder(sparse=False),\n     make_column_selector(dtype_include=object)),\n)\n\nX = preprocessor.fit_transform(X)\n\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","c9b8f9f5":"train.head()","e6d9708e":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(128, activation='relu'),    \n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation=\"softmax\"),\n])","3e060c3f":"model.compile(optimizer=\"rmsprop\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])","cef41949":"history = model.fit(\n    X, y,\n    batch_size=128,\n    epochs=200,\n)","a33a5d71":"import pandas as pd\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5. You can change this to get a different view.\nhistory_df.loc[5:, ['loss']].plot();","b53a8b79":"# Experiment with different values for the learning rate, batch size, and number of examples\nlearning_rate = 0.99\nbatch_size = 4096\nnum_examples = 8192\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","ea9b7334":"test = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/test.csv')\n\nx_test = test.copy()\nx_test = x_test.drop(['row_id'], axis=1)\nx_test = preprocessor.fit_transform(x_test)","673ea937":"predictions = model.predict(x_test)","dea1177c":"predictions[0].shape","cdce6321":"np.sum(predictions[0])","3a1a2463":"np.argmax(predictions[0])","69e4409a":"max_predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]","153acef6":"max_predictions[:5]","94284ece":"bacteria = le.inverse_transform(max_predictions)","8fa471d7":"bacteria[:5]","76efd722":"sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2022\/sample_submission.csv')\nsample_submission.head()","a632e99b":"sample_submission['target'] = bacteria\nsample_submission.head()","16f03b16":"sample_submission.to_csv('submission.csv', index=False)","ef56c507":"# 1) Add Loss and Optimizer","277a3ed5":"# 3) Evaluate Training"}}