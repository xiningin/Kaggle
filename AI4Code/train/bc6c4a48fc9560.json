{"cell_type":{"a22dc451":"code","ed53c2a9":"code","ab55a3ef":"code","fd54ca16":"code","ae8bcdac":"code","df429686":"code","97094a2b":"code","5e8502bc":"code","e09ac7c0":"code","5a11bbda":"code","735eacde":"code","935fe28e":"markdown","fd0fcc64":"markdown","3cdb8fc3":"markdown","43413316":"markdown","24648506":"markdown","2d01721d":"markdown","881760bb":"markdown","3995fa86":"markdown","dd16991e":"markdown"},"source":{"a22dc451":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\nimport random\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ed53c2a9":"rock_dir = os.path.join('..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/train\/rock')\npaper_dir = os.path.join('..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/train\/paper')\nscissors_dir = os.path.join('..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/train\/scissors')\n\nprint('Total training rock images:', len(os.listdir(rock_dir)))\nprint('Total training paper images:', len(os.listdir(paper_dir)))\nprint('Total training scissors images:', len(os.listdir(scissors_dir)))\n\nrock_files = os.listdir(rock_dir)\nprint(rock_files[:10])\n\npaper_files = os.listdir(paper_dir)\nprint(paper_files[:10])\n\nscissors_files = os.listdir(scissors_dir)\nprint(scissors_files[:10])","ab55a3ef":"pic_index = 2\n\nnext_rock = [os.path.join(rock_dir, fname) \n                for fname in rock_files[pic_index-2:pic_index]]\nnext_paper = [os.path.join(paper_dir, fname) \n                for fname in paper_files[pic_index-2:pic_index]]\nnext_scissors = [os.path.join(scissors_dir, fname) \n                for fname in scissors_files[pic_index-2:pic_index]]\n\nfor i, img_path in enumerate(next_rock+next_paper+next_scissors):\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\n    plt.axis('Off')\n    plt.show()","fd54ca16":"TRAINING_DIR = \"..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/train\/\"\ntraining_datagen = ImageDataGenerator(\n    rescale = 1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\nVALIDATION_DIR = \"..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/test\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = training_datagen.flow_from_directory(\n    TRAINING_DIR,\n    target_size=(150,150),\n    class_mode='categorical')\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    VALIDATION_DIR,\n    target_size=(150,150),\n    class_mode='categorical')","ae8bcdac":"model = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax')\n])","df429686":"model.summary()","97094a2b":"model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","5e8502bc":"history = model.fit_generator(train_generator, epochs=25, validation_data = validation_generator, verbose = 1)\n\nmodel.save(\"rps.h5\")","e09ac7c0":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","5a11bbda":"# load all images into a list\nimages = []\nimg_folder = os.path.join('..\/input\/rock-paper-scissors\/Rock-Paper-Scissors\/validation')\nimg_files = os.listdir(img_folder)\nimg_files = [os.path.join(img_folder, f) for f in img_files]\n# print(img_files)\nfor img in img_files:\n    img = load_img(img, target_size=(150, 150))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    images.append(img)\n\n# stack up images list to pass for prediction\nimages = np.vstack(images)\n# print(images)\nclasses = model.predict_classes(images, batch_size=10)\nprint(classes)","735eacde":"# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n\n# Let's prepare a random input image of a rock,paper or scissors from the training set.\nrock_img_files = [os.path.join(rock_dir, f) for f in rock_files]\npaper_img_files = [os.path.join(paper_dir, f) for f in paper_files]\nscissors_img_files = [os.path.join(scissors_dir, f) for f in scissors_files]\n\nimg_path = random.choice(rock_img_files + paper_img_files+scissors_img_files)\nimg = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n\nx   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\nx   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255.0\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers]\n\n# -----------------------------------------------------------------------\n# Now let's display our representations\n# -----------------------------------------------------------------------\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  \n  if len(feature_map.shape) == 4:\n    \n    #-------------------------------------------\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    #-------------------------------------------\n    n_features = feature_map.shape[-1]  # number of features in the feature map\n    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n    \n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    \n    #-------------------------------------------------\n    # Postprocess the feature to be visually palatable\n    #-------------------------------------------------\n    for i in range(n_features):\n        x  = feature_map[0, :, :, i]\n        x -= x.mean()\n        x \/= x.std ()\n        x *=  64\n        x += 128\n        x  = np.clip(x, 0, 255).astype('uint8')\n        display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n\n    #-----------------\n    # Display the grid\n    #-----------------\n\n    scale = 20. \/ n_features\n    plt.figure( figsize=(scale * n_features, scale) )\n    plt.title ( layer_name )\n    plt.grid  ( False )\n    plt.imshow( display_grid, aspect='auto', cmap='viridis' )","935fe28e":"## Training","fd0fcc64":"## Running the Model","3cdb8fc3":"## Explore the Data","43413316":"## Dataset Overview\nRock Paper Scissors is a dataset containing 2,892 images of diverse hands in Rock\/Paper\/Scissors poses.\nRock Paper Scissors contains images from a variety of different hands, from different races, ages and genders, posed into Rock \/ Paper or Scissors and labelled as such. These images have all been generated using CGI techniques as an experiment in determining if a CGI-based dataset can be used for classification against real images.\n\nNote that all of this data is posed against a white background.\n\nEach image is 300\u00d7300 pixels in 24-bit color","24648506":"## Visualizing Intermediate Representations","2d01721d":"## Evaluating Accuracy and Loss for the Model","881760bb":"## Import Packages","3995fa86":"## Data Preprocessing","dd16991e":"## Building Model"}}