{"cell_type":{"0d2d9320":"code","00c23bb7":"code","aa68353b":"code","40b80166":"code","34f1a1b1":"code","9c7b4461":"code","a0c0b052":"code","54fa5aeb":"code","2873d6a9":"markdown"},"source":{"0d2d9320":"import numpy as np\nimport pandas as pd\nimport os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.stats import pearsonr as p\nimport lightgbm as lgb\nimport joblib\n\nimport warnings\nwarnings.simplefilter('ignore')","00c23bb7":"#https:\/\/www.kaggle.com\/valleyzw\/ubiquant-lgbm-baseline\nparams = {\n        'learning_rate':0.1,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': -1,\n        'n_jobs': -1, \n        'seed': 21,\n        'lambda_l1': 1.1895057699067542, \n        'lambda_l2': 1.9079686837880768e-08, \n        'num_leaves': 112, \n        'subsample':None,\n        'feature_fraction': 0.6259927292757151, \n        'bagging_fraction': 0.9782210574588895, \n        'bagging_freq': 1, \n        'n_estimators': 306, \n        'max_depth': 12, \n        'max_bin': 494, \n        'min_data_in_leaf': 366,\n        'colsample_bytree': None,\n        'subsample_freq': None,\n        'min_child_samples': None,\n        'reg_lambda': None,\n        'reg_alpha': None,\n    }\ngc.collect()","aa68353b":"def run(train):\n    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train[feats], train.target, groups=train.time_id)):\n        X_train, y_train = train[feats].iloc[trn_idx], train.target.iloc[trn_idx]\n        X_val, y_val = train[feats].iloc[val_idx], train.target.iloc[val_idx]\n        \n        model = lgb.LGBMRegressor(**params)\n\n        model.fit(X_train, y_train, eval_metric='rmse', eval_set=[(X_val, y_val)], verbose=100, early_stopping_rounds=50)\n        joblib.dump(model, f'lgbm_fold_{fold}.pkl')\n\n        y_pred = model.predict(X_val)\n\n        score = p(y_pred, y_val)[0]\n        print(f\"Fold {fold + 1}: {score}\")\n\n        fold_scores.append(score)\n        models.append(model)\n        \n        del model, y_pred, score, X_train, y_train, X_val, y_val\n        gc.collect()\n\n    del train\n    gc.collect()\n    \n    print(f\"Overall score: {np.mean(fold_scores, axis=0)}\")","40b80166":"train = pd.read_pickle('..\/input\/ump195gb\/train.pkl')[-1000000:]\ngc.collect()","34f1a1b1":"#https:\/\/www.kaggle.com\/lucamassaron\/basic-eda-and-model-to-start\ndef feature_engineering(df, features):\n    \n    df['mean'] = df[features].mean(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['q01'] = df[features].quantile(q=0.01, axis=1)\n    df['q05'] = df[features].quantile(q=0.05, axis=1)\n    df['q10'] = df[features].quantile(q=0.10, axis=1)\n    df['q25'] = df[features].quantile(q=0.25, axis=1)\n    df['q75'] = df[features].quantile(q=0.75, axis=1)\n    df['q90'] = df[features].quantile(q=0.90, axis=1)\n    df['q95'] = df[features].quantile(q=0.95, axis=1)\n    df['q99'] = df[features].quantile(q=0.99, axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['min'] = df[features].min(axis=1)\n    \n    df['std'] = df[features].std(axis=1)\n    df['range'] = df['max'] - df['min']\n    df['iqr'] = df['q75'] - df['q25']\n    df['tails'] = df['range'] \/ df['iqr']\n    df['dispersion'] = df['std'] \/ df['mean']\n    df['dispersion_2'] = df['iqr'] \/ df['median']\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['median-max'] = df['median'] - df['max']\n    df['median-min'] = df['median'] - df['min']\n    df['q99-q95'] = df['q99'] - df['q95']\n    df['q99-q90'] = df['q99'] - df['q90']\n    df['q01-q05'] = df['q01'] - df['q05']\n    df['q01-q10'] =  df['q01'] - df['q10']\n    \n    gc.collect()\n    \n    return df\n\n","9c7b4461":"models = []\nfold_scores = []\nmodel_weights = []\nn_splits=10\nfeats = [f for f in train.columns if f not in ['time_id', 'row_id', 'target']]\n\ntrain = feature_engineering(train, feats)\n\nfeats_origin = feats\nfeats = [f for f in train.columns if f not in ['time_id', 'row_id', 'target']]\n\nkfold = GroupKFold(n_splits)\ngc.collect()","a0c0b052":"run(train)\ngc.collect()","54fa5aeb":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\n\nfor i, (test_df, sample_prediction_df) in enumerate(iter_test):\n    test_df = test_df[feats_origin]\n    \n    test_df = feature_engineering(test_df, feats_origin)\n    \n    pred = []\n    \n    for i in range(len(models)):\n        pred.append(models[i].predict(test_df))\n    \n    sample_prediction_df['target'] = np.mean(pred, axis=0)\n    gc.collect()\n    env.predict(sample_prediction_df)","2873d6a9":"Credit: \nhttps:\/\/www.kaggle.com\/nicohrubec\/lgb-baseline-with-groupkfold\nhttps:\/\/www.kaggle.com\/valleyzw\/ubiquant-lgbm-baseline"}}