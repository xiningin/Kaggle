{"cell_type":{"1e08e7be":"code","12cc0eeb":"code","d35a9e5a":"code","cd2a1583":"code","f58a5d51":"code","0f86d7f9":"code","979e6b92":"code","e48493a3":"code","df4ef303":"code","f9dd40a1":"code","2fbd4812":"code","e632396f":"code","6d950c9c":"code","9e8dce0b":"code","8051288c":"code","98ac9fc7":"code","1045bb34":"code","d429800c":"code","e37b28db":"code","ab2d941a":"code","c54a5455":"code","fcab28af":"code","30691081":"code","f20187a8":"code","45dff189":"code","6b7b605e":"code","480a6ac5":"code","dc4acd51":"code","d0c9cfd7":"code","9089555c":"code","3c92fc5a":"code","7dca4bfa":"code","29a2e8d8":"code","93fd596f":"code","fdcacdad":"code","621932e1":"code","fc6d3d8d":"code","e734e322":"code","d0a06e3a":"code","0338b790":"code","5d87b83e":"code","09770dc0":"code","e4918cab":"code","cad78d8b":"code","a8ef53f9":"code","ce9f5aff":"code","bfdae751":"code","e1452471":"code","ec054488":"code","1ee24373":"code","ffcd1026":"code","3f804d6c":"code","9dd62ec8":"code","65f65c4e":"code","09d2c07a":"code","24f561eb":"code","61d6e03b":"code","d9dd4a07":"code","3621c669":"code","de534e51":"code","d8fd823c":"code","6acfef29":"code","b0ac280a":"code","4211a9b3":"code","13e18248":"code","1619b188":"code","8c51827b":"code","9341585c":"code","cfdb866d":"code","57ca2990":"code","50da2fbc":"code","70e82912":"code","1f07ae81":"code","1f2e16f4":"markdown","49c84f07":"markdown","fa9f6299":"markdown","7b313798":"markdown","1bdbe4fb":"markdown","3aba3a79":"markdown","ea7061ee":"markdown","ac363d7b":"markdown","e959ad29":"markdown","26b29359":"markdown","2f62d041":"markdown","11fe3d02":"markdown","0224ba20":"markdown","ce319ede":"markdown","f52705ce":"markdown","34bbfe4d":"markdown","764471b2":"markdown","083a5ba1":"markdown","73718378":"markdown","1765289e":"markdown","2f71ff2b":"markdown","aff119d0":"markdown","6361717e":"markdown","35e4c40d":"markdown","daa0634d":"markdown","429095c0":"markdown","4ef1bdd0":"markdown","6707e5b0":"markdown","606821fd":"markdown","8e7622a5":"markdown","b006853d":"markdown","f7a266fa":"markdown","4ff7b4a5":"markdown","1b7d845a":"markdown","38f326d6":"markdown","6b837e83":"markdown","b2cf2584":"markdown","b00b35b9":"markdown","6397138c":"markdown","74c3c6fa":"markdown","b5eaddd3":"markdown","2764f0f2":"markdown","5f724cb4":"markdown","23a77828":"markdown","24cbe3e3":"markdown"},"source":{"1e08e7be":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.metrics import classification_report,f1_score\nstop_words = stopwords.words('english')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport unidecode\nimport re\nfrom skmultilearn.problem_transform import LabelPowerset# initialize label powerset multi-label classifier\n%matplotlib inline\n","12cc0eeb":"train=pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest=pd.read_csv('..\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')","d35a9e5a":"train.head(2)","cd2a1583":"test.head(2)","f58a5d51":"print('Train Shape: ',train.shape)\nprint('Test Shape: ',test.shape)","0f86d7f9":"train.info()","979e6b92":"test.info()","e48493a3":"print('As count:\\n')\nprint('Computer Science: ',train['Computer Science'].sum())\nprint('Physics: ',train['Physics'].sum())\nprint('Mathematics: ',train['Mathematics'].sum())\nprint('Statistics: ',train['Statistics'].sum())\nprint('Quantitative Biology: ',train['Quantitative Biology'].sum())\nprint('Quantiative Finance: ',train['Quantitative Finance'].sum())\n\nprint('\\nAs a percentage:\\n')\nprint('Computer Science: ',round(train['Computer Science'].sum()\/train.shape[0]*100))\nprint('Physics: ',round(train['Physics'].sum()\/train.shape[0]*100))\nprint('Mathematics: ',round(train['Mathematics'].sum()\/train.shape[0]*100))\nprint('Statistics: ',round(train['Statistics'].sum()\/train.shape[0]*100))\nprint('Quantitative Biology: ',round(train['Quantitative Biology'].sum()\/train.shape[0]*100))\nprint('Quantiative Finance: ',round(train['Quantitative Finance'].sum()\/train.shape[0]*100))\n","df4ef303":"train['TITLE_len']=train['TITLE'].apply(len) \ntest['TITLE_len']=test['TITLE'].apply(len) \n\ntrain['ABSTRACT_len']=train['ABSTRACT'].apply(len) \ntest['ABSTRACT_len']=test['ABSTRACT'].apply(len) \n\ntrain['cons']=train['TITLE']+train['ABSTRACT'] \ntest['cons']=test['TITLE']+test['ABSTRACT'] \n\ntrain['cons_len']=train['cons'].apply(len) \ntest['cons_len']=test['cons'].apply(len) \n","f9dd40a1":"sns.distplot(train['TITLE_len'])\nsns.distplot(test['TITLE_len'])","2fbd4812":"sns.distplot(train['ABSTRACT_len'])\nsns.distplot(test['ABSTRACT_len'])","e632396f":"sns.distplot(train['cons_len'])\nsns.distplot(test['cons_len'])","6d950c9c":"def remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. caf\u00e9\"\"\"\n    text = unidecode.unidecode(text)\n    return text","9e8dce0b":"def lower_(text):\n    return text.lower()","8051288c":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \ndef stop_words_removal(sentence):\n  \n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(sentence)\n  \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return (' '.join(filtered_sentence))","98ac9fc7":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming(sentence):\n    \n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n","1045bb34":"def remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    return text","d429800c":"#Removing Ascents\ntrain['TITLE']=train['TITLE'].apply(remove_accented_chars)\ntest['TITLE']=test['TITLE'].apply(remove_accented_chars)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(remove_accented_chars)\ntest['ABSTRACT']=test['ABSTRACT'].apply(remove_accented_chars)\n\ntrain['cons']=train['cons'].apply(remove_accented_chars)\ntest['cons']=test['cons'].apply(remove_accented_chars)","e37b28db":"train.head(2)","ab2d941a":"#Lower Casing the text\ntrain['TITLE']=train['TITLE'].apply(lower_)\ntest['TITLE']=test['TITLE'].apply(lower_)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(lower_)\ntest['ABSTRACT']=test['ABSTRACT'].apply(lower_)\n\ntrain['cons']=train['cons'].apply(lower_)\ntest['cons']=test['cons'].apply(lower_)","c54a5455":"train.head(2)","fcab28af":"#Removing Special Characters\ntrain['TITLE']=train['TITLE'].apply(remove_special_characters)\ntest['TITLE']=test['TITLE'].apply(remove_special_characters)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(remove_special_characters)\ntest['ABSTRACT']=test['ABSTRACT'].apply(remove_special_characters)\n\ntrain['cons']=train['cons'].apply(remove_special_characters)\ntest['cons']=test['cons'].apply(remove_special_characters)","30691081":"train.head(2)","f20187a8":"#Stopwords removal\ntrain['TITLE']=train['TITLE'].apply(stop_words_removal)\ntest['TITLE']=test['TITLE'].apply(stop_words_removal)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(stop_words_removal)\ntest['ABSTRACT']=test['ABSTRACT'].apply(stop_words_removal)\n\ntrain['cons']=train['cons'].apply(stop_words_removal)\ntest['cons']=test['cons'].apply(stop_words_removal)","45dff189":"train.head(2)","6b7b605e":"#Stemming\ntrain['TITLE']=train['TITLE'].apply(stemming)\ntest['TITLE']=test['TITLE'].apply(stemming)\n\ntrain['ABSTRACT']=train['ABSTRACT'].apply(stemming)\ntest['ABSTRACT']=test['ABSTRACT'].apply(stemming)\n\ntrain['cons']=train['cons'].apply(stemming)\ntest['cons']=test['cons'].apply(stemming)","480a6ac5":"train.head(2)","dc4acd51":"#Writing the pre-processed text data\ntrain.to_csv('new_train.csv')\ntest.to_csv('new_test.csv')","d0c9cfd7":"train=pd.read_csv('..\/input\/preprocessed-av-topic-modelling\/new_train.csv')\ntest=pd.read_csv('..\/input\/preprocessed-av-topic-modelling\/new_test.csv')","9089555c":"train['title_orig_len']=train['TITLE_len']\ntest['title_orig_len']=test['TITLE_len']\n\ntrain['abs_orig_len']=train['ABSTRACT_len']\ntest['abs_orig_len']=test['ABSTRACT_len']\n\ntrain['cons_orig_len']=train['cons_len']\ntest['cons_orig_len']=test['cons_len']","3c92fc5a":"train['TITLE_len']=train['TITLE'].apply(len) \ntest['TITLE_len']=test['TITLE'].apply(len) \n\ntrain['ABSTRACT_len']=train['ABSTRACT'].apply(len) \ntest['ABSTRACT_len']=test['ABSTRACT'].apply(len) \n\ntrain['cons_len']=train['cons'].apply(len) \ntest['cons_len']=test['cons'].apply(len) ","7dca4bfa":"train.describe()","29a2e8d8":"test.describe()","93fd596f":"sns.heatmap(train.drop(['ID','title_orig_len','abs_orig_len','cons_orig_len','TITLE_len','ABSTRACT_len'],axis=1).corr(),annot=True)","fdcacdad":"tr,ev = train_test_split(train,random_state=101,test_size=0.3, shuffle=True)","621932e1":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(train['cons'].values) + list(test['cons'].values))\n\n#Train\nxtrain_tfv =  tfv.transform(tr['cons']) \nxvalid_tfv = tfv.transform(ev['cons'])\n\n#Test\nxtest_tfv= tfv.transform(test['cons'])","fc6d3d8d":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(train['cons'].values) + list(test['cons'].values))\n\n#Train\nxtrain_ctv =  ctv.transform(tr['cons']) \nxvalid_ctv = ctv.transform(ev['cons'])\n\n#Test\nxtest_ctv= ctv.transform(test['cons'])","e734e322":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\nxtest_svd = svd.transform(xtest_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\nxtest_svd_scl = scl.transform(xtest_svd)","d0a06e3a":"#targets that need to be predicted\ntargets=['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']\n\n#Empty data frame for predictions\nev_pred=pd.DataFrame()\ntest_pred=pd.DataFrame()","0338b790":"#Using LogisticRegression one at a time on tf_idf\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using LogisticRegression\n    classifier = LogisticRegression()\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_logit.csv', index=False)\nprint(\"Your submission was successfully saved!\")","5d87b83e":"#Using LogisticRegression one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using LogisticRegression\n    classifier = LogisticRegression(max_iter=10000)\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_logit.csv', index=False)\nprint(\"Your submission was successfully saved!\")","09770dc0":"#Using MultinomialNB one at a time on tf_idf\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using MultinomialNB\n    classifier = MultinomialNB()\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_mnb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e4918cab":"#Using MultinomialNB one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using MultinomialNB\n    classifier = MultinomialNB()\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_mnb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","cad78d8b":"#Using SVC One at a time\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = SVC() \n    classifier.fit(xtrain_svd_scl, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd_scl)\n    test_pred[t] = classifier.predict(xtest_svd_scl)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svc.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a8ef53f9":"#Using XGBoost one at a time\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_tfv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_tfv)\n    test_pred[t] = classifier.predict(xtest_tfv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ce9f5aff":"#Using XGBClassifier one at a time on ctv\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGBClassifier\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_ctv, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_ctv)\n    test_pred[t] = classifier.predict(xtest_ctv)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('ctv_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","bfdae751":"#Using XGBoost on SVD components\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB789746001881468\n    classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    classifier.fit(xtrain_svd, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd)\n    test_pred[t] = classifier.predict(xtest_svd)\n\nfor t in targets:\n    print(t)\n    print(f1_score(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e1452471":"#Using XGBoost on SVD components\nfor t in targets:\n\n    y_train=tr[t]\n    y_test=ev[t]\n\n    #using XGB\n    classifier = xgb.XGBClassifier(nthread=10)\n    classifier.fit(xtrain_svd, y_train)\n    \n    ev_pred[t] = classifier.predict(xvalid_svd)\n    test_pred[t] = classifier.predict(xtest_svd)\n\nfor t in targets:\n    print(t)\n    print(classification_report(ev[t],ev_pred[t]))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_xgb2.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ec054488":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","1ee24373":"# Next we need a grid of parameters:\n\nparam_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","ffcd1026":"for t in targets:\n\n    y_train=tr[t]\n    \n    print('\\n For',t)\n    # Initialize Grid Search Model\n    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro',\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n    # Fit Grid Search Model\n    model.fit(xtrain_tfv, y_train)  # we can use the full data here but im only using xtrain\n    print(\"Best score: %0.3f\" % model.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n    \n'''\n    Results from GridSerach CV\n    \n    For Computer Science:\n    Best score: 0.857\n    Best parameters set:\n        lr__C: 0.1\n        lr__penalty: 'l2'\n        svd__n_components: 180\n\n    For Physics:\n    Best score: 0.932\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 120\n    \n    For Mathematics:\n    Best score: 0.902\n    Best parameters set:\n        lr__C: 0.1\n        lr__penalty: 'l2'\n        svd__n_components: 120\n    \n    For Statistics:\n    Best score: 0.881\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 180\n    \n    For Quantitative Biology:\n    Best score: 0.974\n    Best parameters set:\n        lr__C: 10\n        lr__penalty: 'l2'\n        svd__n_components: 180\n\n    For Quantitative Finance:\n    Best score: 0.990\n    Best parameters set:\n        lr__C: 1.0\n        lr__penalty: 'l2'\n        svd__n_components: 120\n\n'''","3f804d6c":"# Using Grid Search Results:\n\nsvd_comp={'Computer Science': 180, 'Physics': 120, 'Mathematics': 120, 'Statistics': 180, 'Quantitative Biology': 180, 'Quantitative Finance':120}\nlr_c={'Computer Science': 0.1, 'Physics': 1.0, 'Mathematics': 0.1, 'Statistics': 1, 'Quantitative Biology': 10, 'Quantitative Finance':1}\nlr_pen={'Computer Science': 'l2', 'Physics': 'l2', 'Mathematics': 'l2', 'Statistics': 'l2', 'Quantitative Biology': 'l2', 'Quantitative Finance':'l2'}\n\nfor t in targets:\n    \n    y_train=tr[t]\n    \n    # Initialize SVD\n    svd = TruncatedSVD(n_components=svd_comp[t])\n    \n    # Initialize the standard scaler \n    scl = preprocessing.StandardScaler()\n\n    # We will use logistic regression here..\n    lr_model = LogisticRegression(C=lr_c[t],penalty=lr_pen[t])\n    \n    svd.fit(xtrain_tfv)\n    xtrain_svd = svd.transform(xtrain_tfv)\n    xvalid_svd = svd.transform(xvalid_tfv)\n    xtest_svd = svd.transform(xtest_tfv)\n\n    # Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n    scl.fit(xtrain_svd)\n    xtrain_svd_scl = scl.transform(xtrain_svd)\n    xvalid_svd_scl = scl.transform(xvalid_svd)\n    xtest_svd_scl = scl.transform(xtest_svd)\n    \n    # Model Fit\n    lr_model.fit(xtrain_svd_scl, y_train)  \n    \n    ev_pred[t] = lr_model.predict(xvalid_svd_scl)\n    test_pred[t] = lr_model.predict(xtest_svd_scl)\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_logit_gsv.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9dd62ec8":"# Lightgbm\nimport lightgbm as lgb","65f65c4e":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n    \n    clf = lgb.LGBMClassifier(n_estimators=450,learning_rate=0.03,random_state=42,colsample_bytree=0.5,reg_alpha=2,reg_lambda=2)\n    \n    clf.fit(xtrain_svd_scl, y_train, early_stopping_rounds=100, eval_set=[(xtrain_svd_scl, y_train), (xvalid_svd_scl, y_test)], eval_metric='f1_micro', verbose=True)\n\n    eval_score = f1_score(y_test, clf.predict(xvalid_svd_scl))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(xvalid_svd_scl)\n    test_pred[t] = clf.predict(xtest_svd_scl)\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('tf_idf_svd_lgbm.csv', index=False)\nprint(\"Your submission was successfully saved!\")","09d2c07a":"embeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","24f561eb":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())\n","61d6e03b":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(tr['cons'])]\nxvalid_glove = [sent2vec(x) for x in tqdm(ev['cons'])]\nxtest_glove = [sent2vec(x) for x in tqdm(test['cons'])]\n","d9dd4a07":"# Fitting a simple xgboost on glove features\nfor t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    clf = xgb.XGBClassifier(nthread=10, silent=False)\n    clf.fit(np.array(xtrain_glove), y_train)\n    eval_score = f1_score(y_test, clf.predict(np.array(xvalid_glove)))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(np.array(xvalid_glove))\n    test_pred[t] = clf.predict(np.array(xtest_glove))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","3621c669":"# Fitting a simple xgboost on glove features\nfor t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n    clf.fit(np.array(xtrain_glove), y_train)\n    eval_score = f1_score(y_test, clf.predict(np.array(xvalid_glove)))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = clf.predict(np.array(xvalid_glove))\n    test_pred[t] = clf.predict(np.array(xtest_glove))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_xgb2.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","de534e51":"# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)\nxtest_glove_scl = scl.transform(xtest_glove)","d8fd823c":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units=1,activation='softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","6acfef29":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","b0ac280a":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(x=xtrain_glove_scl,\n              y=y_train, \n              batch_size=256, \n              epochs=500, \n              verbose=1, \n              validation_data=(xvalid_glove_scl, y_test),\n              callbacks=[early_stop]\n             )\n        \n    eval_score = f1_score(y_test, model.predict(xvalid_glove_scl).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_glove_scl))\n    test_pred[t] = np.array(model.predict(xtest_glove_scl))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('glove_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","4211a9b3":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(train['cons']) + list(test['cons']))\nxtrain_seq = token.texts_to_sequences(tr['cons'])\nxvalid_seq = token.texts_to_sequences(ev['cons'])\nxtest_seq = token.texts_to_sequences(test['cons'])\n\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\nxtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n\nword_index = token.word_index","13e18248":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n","1619b188":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","8c51827b":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","9341585c":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n\n        \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('lstm_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")","cfdb866d":"xtrain_pad.shape","57ca2990":"# A simple bidirectional LSTM with glove embeddings and two dense layers\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                         300,\n                         weights=[embedding_matrix],\n                         input_length=max_len,\n                         trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","50da2fbc":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100,verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n    \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('bilstm_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n","70e82912":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n","1f07ae81":"for t in targets:\n    \n    y_train=tr[t]\n    y_test=ev[t]\n\n    \n    model.fit(xtrain_pad, y=y_train, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, y_test), callbacks=[earlystop])\n    \n    eval_score = f1_score(y_test, model.predict(xvalid_pad).astype('int'))\n    \n    print('Eval ACC: {}'.format(eval_score))\n    \n    ev_pred[t] = np.array(model.predict(xvalid_pad))\n    test_pred[t] = np.array(model.predict(xtest_pad))\n\noutput = pd.DataFrame({'ID': test['ID'], 'Computer Science':test_pred['Computer Science'],'Physics':test_pred['Physics'],'Mathematics':test_pred['Mathematics'],'Statistics':test_pred['Statistics'],'Quantitative Biology':test_pred['Quantitative Biology'],'Quantitative Finance':test_pred['Quantitative Finance'] })\noutput.to_csv('gru_nn.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n","1f2e16f4":"### Checking Label Distribution","49c84f07":"# Proceeding to LSTMs","fa9f6299":"# Loading The Datasets","7b313798":"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.\n\nWiki Link - https:\/\/en.wikipedia.org\/wiki\/GloVe_(machine_learning)","1bdbe4fb":"##### Title and abstract in both train and test have very similar distribution","3aba3a79":"# Text Pre-Processing","ea7061ee":"##### Accuracy: 0.807033888436008","ac363d7b":"##### Accuracy: 0.7930549038010324","e959ad29":"# GRU with gloves","26b29359":"Janatahack: Independence Day 2020 ML Hackathon\n\nTopic Modeling for Research Articles\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set.\n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics:\n\n    Computer Science\n    Physics\n    Mathematics\n    Statistics\n    Quantitative Biology\n    Quantitative Finance\n\n","2f62d041":"##### Accuracy: 0.8100712807541964","11fe3d02":"### Getting Text Length & Creating Title+Abstract","0224ba20":"##### Accuracy: 0.7930022308324527","ce319ede":"# Using Grid Serach CV","f52705ce":"##### Accuracy: 0.8049348230912476","34bbfe4d":"<img src='https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/jantahack_i-day-thumbnail-1200x1200-90.jpg'>","764471b2":"# Model Building","083a5ba1":"##### Looks fairly balanced except for quantitative biology and finance","73718378":"# Loading Pre-processed Data","1765289e":"## Feel free to share your feedback, do Upvote if you like\/found the notebook useful!","2f71ff2b":"##### Accuracy: 0.821267230394996","aff119d0":"#### What is TFIDF, Count Vectorisation\n\nIn information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf\u2013idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf\u2013idf.\n\nVariations of the tf\u2013idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf\u2013idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n\nOne of the simplest ranking functions is computed by summing the tf\u2013idf for each query term; many more sophisticated ranking functions are variants of this simple model. \n\nWiki Link : https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf","6361717e":"#### What is grid search?\n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n\n#### Why should I use it?\n\nIf you work with ML, you know what a nightmare it is to stipulate values for hyper parameters. There are libraries that have been implemented, such as GridSearchCV of the sklearn library, in order to automate this process and make life a little bit easier for ML enthusiasts.","35e4c40d":"### Train & Test Split","daa0634d":"# Bi-directional LSTM","429095c0":"##### Accuracy: 0.768516313407954","4ef1bdd0":"### Simple Models using TFIDF & Vectors ","6707e5b0":"# Problem Statement","606821fd":"### Reference:\n\n1) https:\/\/www.kdnuggets.com\/2018\/08\/practitioners-guide-processing-understanding-text-2.html#:~:text=Removing%20Special%20Characters,be%20used%20to%20remove%20them.\n\n2) https:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff\n\n3) https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle","8e7622a5":"### Functions for Text Preprocessing","b006853d":"##### Accuracy: 0.34362045140488257","f7a266fa":"##### Accuracy: 0.8173573642520952","4ff7b4a5":"### Checking the change in length after pre-processing","1b7d845a":"##### Accuracy: 0.762793995981563","38f326d6":"##### Accuracy: 0.343620451404883","6b837e83":"##### Accuracy: 0.7993270848353762","b2cf2584":"##### Accuracy: 0.676923076923077","b00b35b9":"### Apply Text Pre-Processing","6397138c":"##### Accuracy: 0.812694032424974","74c3c6fa":"##### Accuracy: 0.8092842442259318","b5eaddd3":"# Deep Learning","2764f0f2":"# Word Vectors","5f724cb4":"### TFIDF & Count Vectorization  ","23a77828":"##### Accuracy: 0.34362045140488257","24cbe3e3":"#### What is LSTMs?\n\nLong short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data."}}