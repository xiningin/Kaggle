{"cell_type":{"ff3716ca":"code","917f4bf4":"code","02678ff1":"code","042c5af3":"code","f8ca258f":"code","6f8738aa":"code","fd9de176":"code","0c626c70":"code","64c00983":"code","ff96f270":"code","972fc21c":"code","228c7679":"code","d3837176":"code","dcc03c72":"code","0566b571":"code","b28f860e":"code","801b208a":"markdown","0dc0e5cb":"markdown","d8ce09d3":"markdown","9219f07e":"markdown","2064b215":"markdown","d3e130f4":"markdown","4dbe3b9d":"markdown","329115c2":"markdown","310c5036":"markdown","ad91be94":"markdown","2e8e5e66":"markdown","795f8f4b":"markdown","65d8fcea":"markdown","ae190305":"markdown"},"source":{"ff3716ca":"# install need package\n!pip install googletrans textAugment","917f4bf4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # lib for gsraph plot\nimport os\nimport re # Regular Exprexion lib\n\n\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\n# lib for Machine learning models (BERT)\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom transformers import TFRobertaModel, RobertaTokenizer\nimport tensorflow as tf\n\nfrom textaugment import EDA\nfrom googletrans import Translator\n\nimport multiprocessing as mp\nfrom tqdm import tqdm_notebook\n\nimport gc\nfrom sklearn.model_selection import train_test_split","02678ff1":"# TPU detection. No parameters necessary if TPU_NAME environment variable is\n# set: this is always the case on Kaggle.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","042c5af3":"# List of csv data files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f8ca258f":"df_train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')","6f8738aa":"df_train.head()","fd9de176":"labels, frequencies = np.unique(df_train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.title('language distribution in Training Set')\nplt.show()","0c626c70":"labels, frequencies = np.unique(df_test.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.title('language distribution in Testing Set')\nplt.show()","64c00983":"labels, freq_labels = np.unique(df_train.label.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(freq_labels,labels = labels, autopct = '%1.1f%%')\nplt.title('labels distribution in Training Set')\nplt.show()","ff96f270":"def clean_word(value):\n    language = value[0]\n    word = value[1]\n    if language != 'English':\n        word = word.lower()\n        return word\n    word = word.lower()\n    word = re.sub(r'\\?\\?', 'e', word)\n    word = re.sub('\\.\\.\\.', '.', word)\n    word = re.sub('\\\/', ' ', word)\n    word = re.sub('--', ' ', word)\n    word = re.sub('\/\\xad', '', word)\n    word = word.strip(' ')\n    return word\n\ndf_train['premise'] = df_train[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_train['hypothesis'] = df_train[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)\ndf_test['premise'] = df_test[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_test['hypothesis'] = df_test[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)\n\nlanguages = [ 'zh-cn' if lang == 'zh' else lang for lang in df_train['lang_abv'].unique()]","972fc21c":"seed = 42\ntf.random.set_seed(seed)\n\nmodel_name = 'jplu\/tf-xlm-roberta-large' # pretrained model' name\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name) # tokenizer init\n\n#model_name = 'roberta-large'\n#tokenizer = RobertaTokenizer.from_pretrained(model_name) # tokenizer init\n\ndef build_model():\n    with strategy.scope():\n        \n        bert_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n        #bert_encoder = TFRobertaModel.from_pretrained(model_name)\n        \n        # define tensors for inputs\n        input_word_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_mask\")\n        \n        # Define model for fine-tuning XLMRoberta\n        \n        ### Layer 1 is a pretrained XLMRoberta Transformer\n        embedding = bert_encoder([input_word_ids, input_mask])[0]\n        \n        ### 5 Layers before for Classification task\n        output_layer = tf.keras.layers.Dropout(0.25)(embedding)\n        output_layer = tf.keras.layers.GlobalAveragePooling1D()(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(64, activation='relu')(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(32, activation='relu')(output_dense_layer)\n        output = tf.keras.layers.Dense(3, activation='softmax')(output_dense_layer)\n\n        # Define Training parameters\n        ## Optimizer is ADAM\n        ## Function Loss is CrossEntropy\n        ## Metric for evaluation is a standard accuracy\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n        model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        return model\n\n# Init DeepLearning Model \nwith strategy.scope():\n    model = build_model()\n    model.summary() # this describe model architecture and layers","228c7679":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nbatch_size = 8 * strategy.num_replicas_in_sync\nnum_splits = 5\ntest_input = None","d3837176":"auto = tf.data.experimental.AUTOTUNE\n\ndef make_dataset(train_input, train_label):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            train_input,\n            train_label\n        )\n    ).repeat().shuffle(batch_size).batch(batch_size).prefetch(auto)\n    return dataset\n\n\ndef xlm_roberta_encode(hypotheses, premises, src_langs, augmentation=False):\n    num_examples = len(hypotheses)\n\n    sentence_1 = [tokenizer.encode(s) for s in premises]\n    sentence_2 = [tokenizer.encode(s) for s in hypotheses]\n    input_word_ids = list(map(lambda x: x[0]+x[1], list(zip(sentence_1,sentence_2))))\n    input_mask = [np.ones_like(x) for x in input_word_ids]\n    inputs = {\n        'input_word_ids': tf.keras.preprocessing.sequence.pad_sequences(input_word_ids, padding='post'),\n        'input_mask': tf.keras.preprocessing.sequence.pad_sequences(input_mask, padding='post')\n    }\n    return inputs\n\n# splite training data into train and valdiation\ntrain_df, validation_df = train_test_split(df_train, test_size=0.1)\n\ndf_train['prediction'] = 0\nnum_augmentation = 1\n\n# encoding training data\ntrain_input = xlm_roberta_encode(train_df.hypothesis.values,train_df.premise.values, train_df.lang_abv.values, augmentation=False)\ntrain_label = train_df.label.values\n\n# create data Iterator for training \ntrain_sequence = make_dataset(train_input, train_label)\n\n# encoding validation data\nvalidation_input = xlm_roberta_encode(validation_df.hypothesis.values, validation_df.premise.values,validation_df.lang_abv.values, augmentation=False)\nvalidation_label = validation_df.label.values\ntf.keras.backend.clear_session()","dcc03c72":"# splite training data into train and valdiation\ntrain_df, validation_df = train_test_split(df_train, test_size=0.1)\n\ndf_train['prediction'] = 0\nnum_augmentation = 1\n\n# encoding training data\ntrain_input = xlm_roberta_encode(train_df.hypothesis.values,train_df.premise.values, train_df.lang_abv.values, augmentation=False)\ntrain_label = train_df.label.values\n\n# create data Iterator for training \ntrain_sequence = make_dataset(train_input, train_label)\n\n# encoding validation data\nvalidation_input = xlm_roberta_encode(validation_df.hypothesis.values, validation_df.premise.values,validation_df.lang_abv.values, augmentation=False)\nvalidation_label = validation_df.label.values\ntf.keras.backend.clear_session()","0566b571":"n_steps = (len(train_label)) \/\/ batch_size\n\nwith strategy.scope():\n    history = model.fit(\n        train_sequence, shuffle=True, steps_per_epoch=n_steps, \n        validation_data = (validation_input, validation_label), epochs=50, verbose=1,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10),\n            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5),\n            tf.keras.callbacks.ModelCheckpoint(\n                'model.h5', monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n        ]\n    ) \n\n# save trained model\nmodel.load_weights('model.h5')\n\n# calcul of validation Accuracy\nvalidation_predictions = model.predict(validation_input)\nvalidation_predictions = np.argmax(validation_predictions, axis=-1)\nvalidation_df['predictions'] = validation_predictions\nacc = accuracy_score(validation_label, validation_predictions)\nprint('Accuracy: {}'.format(acc))","b28f860e":"# encoding test data for prediction and submission\nif test_input is None:\n    test_input = xlm_roberta_encode(df_test.hypothesis.values, df_test.premise.values, df_test.lang_abv.values,augmentation=False)\n\n# prediction using trained model\ntest_split_predictions = model.predict(test_input)\npredictions = np.argmax(test_split_predictions, axis=-1)\n\n# create submission file\nsubmission = df_test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.head()\n\n# submission to challenge\nsubmission.to_csv(\"submission.csv\", index = False)","801b208a":"## 4.3 Training the model","0dc0e5cb":"**Distribution of sentence's relations in training set**","d8ce09d3":"# 1. Import Needed Package","9219f07e":"**Distribution of languages in training and testing sets**","2064b215":"# **Detecting contradiction and entailment in multilingual text using TPUs**\n\nThe Challenge for this competition is to create an NLI model for contradiction of two sentences. However, NLI (for Natural Language Inferencing) is a NLP problem which analyse a relation between a pairs of sentences. In fact, There are three ways that a pair of sentences could be related: one could entail the other (= label equal to 0), they could be unrelated (= label equal to 1), or one could contradict the other (= label equal to 2).\n\nFor this competition, the train and test dataset include text in fifteen different languages !\n\n> Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.","d3e130f4":"**Data cleansing**","4dbe3b9d":"**A brief review of data structure :**","329115c2":"# 4. Building Model","310c5036":"# 2. Data Reviews\n\nTo better understand the issue, we first make a deep review of our input data","ad91be94":"# 5. Generate Prediction and Submission","2e8e5e66":"## 4.2 Data Preprocessing","795f8f4b":"## 4.1 initialize model architecture","65d8fcea":"**Load training and testing sets from each corresponding files :**","ae190305":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text."}}