{"cell_type":{"356fe496":"code","480bd126":"code","255c3066":"code","688bb29e":"code","79f6f12d":"code","d6a1c393":"code","8e6398cc":"code","5d02798b":"code","fc417142":"code","934e5b5a":"code","ce9179fe":"code","5aa358a4":"code","53b93379":"markdown","05deaa34":"markdown","4a024ca0":"markdown","fbb0ee5d":"markdown","bd647316":"markdown","39d40b4b":"markdown","ef91b4b1":"markdown","11d3e3ab":"markdown","539c8160":"markdown","c493a5e2":"markdown","fa6bbc21":"markdown"},"source":{"356fe496":"!pip install imutils","480bd126":"# DO NOT CHANGE ANYTHING IN THIS CELL\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nimport tensorflow.keras\nimport time\nimport pytz\nimport imutils\nimport hashlib\nimport pickle\nfrom datetime import datetime\n\n# scikit learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# tensorflow libraries\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Dense, Activation, Dropout, Flatten, Input\nfrom tensorflow.keras.metrics import categorical_accuracy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.applications import MobileNetV2\n\nfrom typing import List\nfrom pathlib import Path\nfrom imutils.video import VideoStream\nfrom PIL import Image\n\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']","255c3066":"npy_data = '..\/input\/ferdata\/numpy_images.npy'     #You have to attach the output of Preprocessing data\nlabel_data = '..\/input\/ferdata\/labels.pkl'\nif Path(npy_data).is_file() and Path(label_data).is_file():\n    print('Loading from previously processed training data X and labels Y')\n    with open(npy_data, 'rb') as f:\n        X = np.load(f)\n    with open(label_data, 'rb') as f:\n        Y = pickle.load(f)\n        print(Y)\n        labels = [label_map[e] for e in Y]\nelse:\n    print('No training numpy array saved previously. Please run the preprocessing notebook.')","688bb29e":"# your code here\na=np.random.choice(len(Y),20)\nprint(a)\n\n\n#plt.imshow(frame, cmap=plt.cm.binary_r)\nplt.figure()\nfor i in range(len(a)):\n    frame=X[a[i], :, :, :].reshape(48,48)\n    plt.subplot(4,5,i+1)\n    plt.title(labels[a[i]])\n    plt.imshow(frame, cmap=plt.cm.binary_r)\nplt.show()\n","79f6f12d":"# your code here\nfrom keras.utils import to_categorical\nY=to_categorical(Y)\nX_train,X_val,y_train,y_val=train_test_split(X, Y, test_size=0.1, random_state=42)\n","d6a1c393":"# your code here\nprint(X_train.shape)\nprint(Y.shape)","8e6398cc":"def simple_convnet():\n    model = Sequential()\n#   your code here\n    input_shape = (48,48,1)\n    model.add(Conv2D(64,(5,5), input_shape=input_shape,activation='relu',padding='same'))\n    model.add(Conv2D(64,(5,5),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(7))\n    model.add(Activation('softmax'))\n    \n    optim = tf.keras.optimizers.Adam(learning_rate=2e-4)\n    \n    \n    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=optim)\n    return model\n    \nmodel = simple_convnet()","5d02798b":"# check the summary of your model (you don't have to do anything in this cell)\nmodel.summary()","fc417142":"image_generator = ImageDataGenerator(\n#     \u6211\u61d2\u5f97\u5199\uff0c\u8c01\u7231\u5199\u8c01\u5199\u53bb\u5427\n        \n    \n    \n)","934e5b5a":"# DO NOT CHANGE ANYTHING IN THIS CELL\nutc_now = pytz.utc.localize(datetime.utcnow())\ndate_today = utc_now.astimezone(pytz.timezone('Asia\/Shanghai')).strftime('%Y-%m-%d')\npath_model=f'checkpoints\/convnet\/{date_today}\/' + '{epoch:02d}-{val_loss:.6f}.hdf5'\nPath(f'checkpoints\/convnet\/{date_today}').mkdir(parents=True, exist_ok=True)","ce9179fe":"# implement training of the model\n# your code here\nmodel.fit(X_train,y_train,epochs=10,batch_size=32)","5aa358a4":"# your code here\nmodel.evaluate(X_val,y_val)","53b93379":"### Step 2: train test split\nRerun train \/ test split before we being training\n","05deaa34":"Please check each of the variables shapes to ensure they are reaonsable. This is very important. Make sure everything is expected before moving on to the ConvNet part. ","4a024ca0":"### STEP 1\n\nLoad our preprocessed data back in.","fbb0ee5d":"### Step 3: Define Model and Train on a Conv-Net\nWe will implement the model in a function called `simple_convnet`. (Don't change the name of this!). It will achieve: \n1. model initialization\n2. model definition (whatever you want, layers, etc). \n3. model compilation","bd647316":"### Submission\n\n\u4f60\u4eec\u90fd\u662f\u725b\u903c\u7684CV\u4eba\u4e86\uff0c\u81ea\u5df1\u4ea4\u5427","39d40b4b":"### Step 4: Data augmentation\nImplement `image_generator` and `val_generator` here to do all the data augmentation tricks. We covered some data augmentation stuff and how to use it in our lab. You can choose some of it or all or some other things that are not covered in our lab. \nAfter that, you can use ","ef91b4b1":"### Step 6: Evaluate Model","11d3e3ab":"## Part 2: Training and Evaluation - ConvNet\nIn this section, you will perform the heavy lifting of the required task 1: build a CNN to detect facial expressions. You will be following these steps: \n1. Load the preprocess data and check the data validity\n2. Train-test split\n3. Define your CNN \n4. Data augmentation\n5. Train your CNN\n6. Evaluate your model\n7. Iterate 5) and 6) \n8. Predictions and visualize the results\n9. Error analysis\/Evaluations","539c8160":"### Step 5: Train your CNN\nHere you will learn how to save your checkpoint of your model. This is a crucial step for part 3 because you need to load the trained model locally in order to perform real time video detection with the camera of your laptop. \n\nYou don't have to change anything in the model checkpoint part. After that, you will implement the train\/fit part of your CNN. ","c493a5e2":"### Step 7: Iterate step 5 and 6. \nTry different ConvNets architecture to see whether you can improve your model performance. ","fa6bbc21":"Let's see some samples images again to remind ourselves what data we are working with. You can copy your code from part 1 here. This is a sanity check to make sure we are working with the right form of data. "}}