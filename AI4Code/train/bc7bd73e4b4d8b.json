{"cell_type":{"58489b4a":"code","69eda704":"code","5e18ff7d":"code","5210a90e":"code","58b864f7":"code","5428614f":"code","19598842":"code","41778872":"code","621a1d0d":"code","78bb67bc":"code","0b5a8176":"code","82c7a558":"code","3117473f":"code","f5398e8c":"code","ec312da7":"code","60dbccaf":"code","0a2de64f":"code","5af050c9":"code","43e26a5f":"code","5de1a8f8":"code","aff443f8":"code","456b1c38":"code","07fbfe84":"code","8dbf245b":"code","9d4d7c9d":"code","3d477271":"code","b8696ce7":"code","7ac5e11d":"code","46a45930":"code","621eb595":"code","f4045e19":"code","3040a9df":"code","077e65a4":"code","5d4530b0":"code","cb0bab7e":"code","660492f5":"code","1fc2d934":"code","67b424f5":"code","f7ef344d":"code","ad5724b3":"code","42a70e8b":"code","d33459ba":"code","b30bf737":"code","2a9524ad":"code","5bf3bf8b":"code","6e80f98a":"code","3d0e51f4":"code","b82ffa71":"code","0b9ccbec":"code","ff8a3850":"code","4fd324c2":"code","d9e85281":"code","39c2dba6":"code","399f1206":"code","0501e3a1":"code","ad729f21":"code","4b91f9ec":"markdown","46b0c975":"markdown","efc6adb1":"markdown","4288331f":"markdown","2cb68be4":"markdown","fa6fe8c3":"markdown","4cd6060f":"markdown","11cc06df":"markdown","e90cbf9f":"markdown","6421c388":"markdown","e1a7b7ae":"markdown","b5c7279f":"markdown","118d6df2":"markdown","44d3e446":"markdown","ec87af27":"markdown","0f021ce6":"markdown","d91ea8af":"markdown","fea255f4":"markdown","928921b2":"markdown","d02b6694":"markdown","15ed77e3":"markdown","6c5d2c21":"markdown","89e461bf":"markdown","e8b19ea5":"markdown","c4fd0137":"markdown","cae76cb4":"markdown","5bd358de":"markdown","5a8b7c3a":"markdown","ad27a7c3":"markdown","2fb0cd5e":"markdown","c1428528":"markdown","7c112d02":"markdown","74eecd1a":"markdown","4106b37a":"markdown","d0604375":"markdown","58237e89":"markdown","c5ade0b1":"markdown","4051d9c8":"markdown","6de1d256":"markdown","70ff9459":"markdown","8d1a6c2d":"markdown","476362b4":"markdown","8fab1d93":"markdown","4c3bf804":"markdown","f1d6d974":"markdown","ba298849":"markdown","65d4460f":"markdown","961c9bb8":"markdown","29b2674e":"markdown","f42e7cbf":"markdown","99d60fcb":"markdown","f1f6a9cf":"markdown"},"source":{"58489b4a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn')","69eda704":"adult = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\",\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")\nadult_test = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\",\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","5e18ff7d":"print('Formato do Dataset:', adult.shape)","5210a90e":"adult.head()","58b864f7":"adult.describe()","5428614f":"print(\"Workclass:\")\nprint(adult.Workclass.describe())\n\nprint(\"\\nOccupation:\")\nprint(adult.Occupation.describe())\n\nprint(\"\\nCountry:\")\nprint(adult.Country.describe())","19598842":"x = adult.Workclass.describe().top\nadult.Workclass =  adult.Workclass.fillna(x)\n\nx = adult.Country.describe().top\nadult.Country =  adult.Country.fillna(x)\n\nx = adult.Occupation.describe().top\nadult.Occupation =  adult.Occupation.fillna(x)","41778872":"adult.describe()","621a1d0d":"x = adult_test.Workclass.describe().top\nadult_test.Workclass =  adult_test.Workclass.fillna(x)\n\nx = adult_test.Country.describe().top\nadult_test.Country =  adult_test.Country.fillna(x)\n\nx = adult_test.Occupation.describe().top\nadult_test.Occupation =  adult_test.Occupation.fillna(x)","78bb67bc":"adult.head()","0b5a8176":"adult.describe()","82c7a558":"from sklearn import preprocessing\nLabelEncoder = preprocessing.LabelEncoder()\n\ncorrelationMap = adult.apply(preprocessing.LabelEncoder().fit_transform).corr()\ncorrelationMap.head()","3117473f":"plt.figure(figsize=(10,7))\nsns.heatmap(correlationMap, vmin=-0.3, vmax=0.3, cmap = 'mako')","f5398e8c":"def LabeledBarPlot(df, x, label):\n# plota um gr\u00e1fico de barra da coluna \"column\" enfatizando o r\u00f3tulo \"label\"\n\n    print('Graph of %s and %s' %(x,label))\n    \n    index = df[x].unique()\n    columns = df[label].sort_values().unique()\n    data_to_plot = pd.DataFrame({'index': index})\n    \n    for column in columns:\n        temp = []\n        for unique in index:\n            filtered_data = df[df[x] == unique]\n            filtered_data = filtered_data[filtered_data[label] == column]\n            \n            temp.append(filtered_data.shape[0])\n        data_to_plot = pd.concat([data_to_plot, pd.DataFrame({column: temp})], axis = 1)\n        \n    data_to_plot = data_to_plot.set_index('index', drop = True)\n    \n    ax = data_to_plot.plot.bar(rot=0, figsize = (14,7), alpha = 0.9, cmap = 'viridis')","ec312da7":"LabeledBarPlot(adult.drop('Id', axis=0), 'Sex', 'Target')","60dbccaf":"LabeledBarPlot(adult.drop('Id', axis=0), 'Sex', 'Education-Num')","0a2de64f":"adult['Race'].describe()","5af050c9":"LabeledBarPlot(adult.drop('Id', axis=0), 'Race', 'Target')","43e26a5f":"adult['Country'].describe()","5de1a8f8":"numericalColumns = ['Age', 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week']\n\nsns.set()\nsns.pairplot(adult.drop('Id'), vars = numericalColumns, hue = 'Target', palette = 'mako', height = 2.5, diag_kws={'bw':'1.0'})","aff443f8":"numericalFeatures = ['Age', 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week']\nadult = adult.drop(columns = ['fnlwgt'])\nadult_test = adult_test.drop(columns = ['fnlwgt'])\n\ncategoricalFeatures = ['Workclass', 'Martial Status', 'Occupation', 'Relationship', 'Race', 'Sex']\nadult = adult.drop(columns = ['Education', 'Country'])\nadult_test = adult_test.drop(columns = ['Education', 'Country'])\n\nprint(\"Atributos num\u00e9ricos: \",numericalFeatures)\nprint(\"Atributos categ\u00f3ricos: \", categoricalFeatures)","456b1c38":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()","07fbfe84":"# Vamos criar uma c\u00f3pia para conseguir reverter essa exclus\u00e3o posteriormente\n\nadultCopy = adult.drop('Id', axis=0)\n\nadult_testCopy = adult_test.drop('Id', axis=0)","8dbf245b":"adultCopy[numericalFeatures] = scaler.fit_transform(adultCopy[numericalFeatures])\n\nadult_testCopy[numericalFeatures] = scaler.fit_transform(adult_testCopy[numericalFeatures])","9d4d7c9d":"LabelEncoder = preprocessing.LabelEncoder()\n\n# training set\nadultCopy[categoricalFeatures] = adult[categoricalFeatures].apply(LabelEncoder.fit_transform)\n\n# testing set\nadult_testCopy[categoricalFeatures] = adult_test[categoricalFeatures].apply(LabelEncoder.fit_transform)","3d477271":"# Training set\nX_train = adultCopy[['Age','Workclass','Education-Num','Martial Status','Occupation','Relationship','Race','Sex','Capital Gain','Capital Loss','Hours per week']]\nY_train = adultCopy.Target\n\n# Test set\nX_test = adult_testCopy[['Age','Workclass','Education-Num','Martial Status','Occupation','Relationship','Race','Sex','Capital Gain','Capital Loss','Hours per week']]\n","b8696ce7":"adultCopy.head()","7ac5e11d":"adult_testCopy.head()","46a45930":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nlogreg = LogisticRegression(solver='liblinear',random_state=42)\n\n# Hiperpar\u00e2metros a serem otimizados: 'C' e tipo de regulariza\u00e7\u00e3o\nhyperparams = {'C': np.linspace(0,10,100), 'penalty': ['l2', 'l1']}\n\n# Busca de Hiperpar\u00e2metros:\nlogreg_clf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_logreg = logreg_clf.fit(X_train, Y_train)","621eb595":"print(\"Os melhores par\u00e2metros obtidos para a Regress\u00e3o Log\u00edsitica s\u00e3o:\", search_logreg.best_params_)\nprint(\"O score AUC para esse classificador \u00e9 de:\", search_logreg.best_score_)","f4045e19":"from sklearn.svm import SVC\nsvc = SVC(random_state=42, probability=True)\n","3040a9df":"from sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(svc, X_train, Y_train, cv = 5, scoring=\"roc_auc\")\nprint(\"Score AUC com cross validation:\", score.mean())","077e65a4":"# Hiperpar\u00e2metros a serem otimizados:\nhyperparams = {'C': np.linspace(0,20,100), 'gamma': ['scale', 'auto']}\n\n# Busca de Hiperpar\u00e2metros:\nsvc_clf = RandomizedSearchCV(svc, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_svc = svc_clf.fit(X_train, Y_train)","5d4530b0":"print(\"Os melhores par\u00e2metros obtidos para a SVM s\u00e3o:\", search_svc.best_params_)\nprint(\"O score AUC para esse classificador \u00e9 de:\", search_svc.best_score_)","cb0bab7e":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(random_state=42)","660492f5":"# Hiperpar\u00e2metros a serem otimizados:\nhyperparams = {'n_estimators': np.arange(100, 500), 'criterion': ['gini','entropy'], 'max_depth': np.arange(1,50)}\n# Busca de Hiperpar\u00e2metros:\nrfc_clf = RandomizedSearchCV(rfc, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_rfc = rfc_clf.fit(X_train, Y_train)","1fc2d934":"print(\"Os melhores par\u00e2metros obtidos para a Random Forest s\u00e3o:\", search_rfc.best_params_)\nprint(\"O score AUC para esse classificador \u00e9 de:\", search_rfc.best_score_)","67b424f5":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(random_state=42, early_stopping=True)","f7ef344d":"from scipy.stats import loguniform as sp_loguniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Hiperpar\u00e2metros a serem otimizados:\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros:\nmlp_clf = RandomizedSearchCV(mlp, hyperparams, scoring='accuracy', n_iter=25, cv=3, n_jobs=-1, random_state=42)\nsearch_mlp = mlp_clf.fit(X_train, Y_train)","ad5724b3":"print(\"Os melhores par\u00e2metros obtidos para a Rede Neural s\u00e3o:\", search_mlp.best_params_)\nprint(\"O score AUC para esse classificador \u00e9 de:\", search_mlp.best_score_)","42a70e8b":"print('AUC -------- Regress\u00e3o Log\u00edstica: {:.4f}'.format(search_logreg.best_score_))","d33459ba":"from sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(search_logreg, X_train, Y_train)\nplt.show()","b30bf737":"print('AUC -------- SVM: {:.4f}'.format(search_svc.best_score_))","2a9524ad":"plot_roc_curve(search_svc, X_train, Y_train)\nplt.show()","5bf3bf8b":"print('AUC -------- Random Forest: {:.4f}'.format(search_rfc.best_score_))","6e80f98a":"plot_roc_curve(search_rfc, X_train, Y_train)\nplt.show()","3d0e51f4":"print('AUC -------- Rede Neural: {:.4f}'.format(search_mlp.best_score_))","b82ffa71":"plot_roc_curve(search_mlp, X_train, Y_train)\nplt.show()","0b9ccbec":"modelos = ['Regress\u00e3o Log\u00edstica', 'SVM', 'Random Forest', 'Rede Neural']\nscores = [search_logreg.best_score_, search_svc.best_score_, search_rfc.best_score_, search_mlp.best_score_]\n\ncompara\u00e7\u00e3o = {'Modelo': modelos, \"AUC\": scores}\ncompara\u00e7\u00e3o = pd.DataFrame(compara\u00e7\u00e3o)\n\n# Colocando os scores em ordem crescente:\ncompara\u00e7\u00e3o = compara\u00e7\u00e3o.sort_values(\"AUC\", ascending = False)\ncompara\u00e7\u00e3o.reset_index(drop=True, inplace=True)","ff8a3850":"compara\u00e7\u00e3o.head()","4fd324c2":"predict = search_rfc.predict(X_test)","d9e85281":"predict","39c2dba6":"submission = pd.DataFrame(predict)","399f1206":"submission[1] = predict\nsubmission[0] = adult_testCopy.index\nsubmission.columns = ['Id','Income']","0501e3a1":"submission.head()","ad729f21":"submission.to_csv('submission.csv',index = False)","4b91f9ec":" - Curva ROC para a Regress\u00e3o Log\u00edstica:","46b0c975":"A nossa base de dados \u00e9 de certa forma complexa, apresentando 14 vari\u00e1veis diferentes, algumas com certa redund\u00e2ncia, como Education e Education-Num.","efc6adb1":"## 4. Aplica\u00e7\u00e3o de classificadores\nAgora que j\u00e1 fizemos o tratamento, an\u00e1lise e preprocessamento dos dados, podemos finalmente aplicar nossos classificadores.\n### 4.1 Regress\u00e3o Log\u00edstica\n- Vamos importar o classificador e otimizar os hiperpar\u00e2metros com o RandomizedSearch:","4288331f":"### 3.2 Processamento dos atributos num\u00e9ricos\nPara os atributos num\u00e9ricos, iremos aplicar uma normaliza\u00e7\u00e3o linear nos valores, j\u00e1 que nossas features apresentam intervalos muito diferentes de valores. O objetivo da normaliza\u00e7\u00e3o \u00e9 alterar os valores das colunas num\u00e9ricas no dataset para uma escala comum, sem distorcer as diferen\u00e7as nos intervalos de valores. Vamos precisar do m\u00f3dulo de pr\u00e9-processamento da biblioteca scikit-learn e do utilit\u00e1rio MinMaxScaler.\n\nFaremos isso tanto na base de treino como na de teste.","2cb68be4":"Agora sim podemos aplicar a normaliza\u00e7\u00e3o sobre os dados num\u00e9ricos.","fa6fe8c3":"### 4.3 Random Forest\n- Vamos importar a nossa Random Forest do scikit-learn e instanci\u00e1-la:","4cd6060f":"### 6.1 Submiss\u00e3o da predi\u00e7\u00e3o\nPrecisamos agora transformar esse array com as predi\u00e7\u00f5es em um data frame e em seguida export\u00e1-lo para o formato .csv.","11cc06df":"## 3. Preprocessamento dos atributos\n### 3.1 Escolha de atributos (*feature engineering*)\nNessa etapa, vamos levar em considera\u00e7\u00e3o o car\u00e1ter do atributo (qualitativo ou num\u00e9rico) e selecionar, um pouco pela intui\u00e7\u00e3o e um pouco pela an\u00e1lise dos dados que foi feita, aqueles atributos que seriam interessantes na determina\u00e7\u00e3o do fato da renda ser superior\/inferior a 50k, isto \u00e9, \u00fateis para o nosso classificador. ","e90cbf9f":"### 2.3 An\u00e1lise explorat\u00f3ria dos dados\nIremos explorar os dados da base Adult e obter algumas conclus\u00f5es acerca dos dados, principalmente usando gr\u00e1ficos. Espera-se que tanto essa etapa, como a busca por correla\u00e7\u00e3o seja \u00fatil na escolha dos atributos e cria\u00e7\u00e3o do nosso modelo.\nPara os gr\u00e1ficos, usaremos as bibliotecas Matplotlib e Seaborn, j\u00e1 importadas.\n\nPrimeiro, precisamos criar uma fun\u00e7\u00e3o que cria um gr\u00e1fico de barras rotulado, isso ser\u00e1 bastante \u00fatil nas nossas observa\u00e7\u00f5es.","6421c388":"### 2.2 Busca por correla\u00e7\u00f5es entre as vari\u00e1veis\nPrecisamos buscar **correla\u00e7\u00f5es** entre as vari\u00e1veis, verificando quais influem significativamente na renda anual e quais n\u00e3o, para montar um modelo adequado e eficiente. Al\u00e9m disso, \u00e9 valido enxergar tamb\u00e9m quais vari\u00e1veis estao ligadas entre si na base. \n\nPara fazer isso, vamos ter que aplicar o LabelEncoder nos dados, da biblioteca scikit-learn.","e1a7b7ae":"### 5.4 Rede Neural","b5c7279f":"- Assim, podemos ver que o modelo com melhor desempenho \u00e9 a Random Forest. Utilizaremos ela para fazer a predi\u00e7\u00e3o final.","118d6df2":"### 5.2 Support Vector Machinhe (SVM)","44d3e446":"- Aqui temos uma representa\u00e7\u00e3o de um trecho do dataset e de como ele se estrutura:","ec87af27":" - Agora, vamos juntar nossos resultados numa tabela para facilitar a visualiza\u00e7\u00e3o:","0f021ce6":"Vamos analisar agora a quest\u00e3o racial no nosso data set.","d91ea8af":"Percebemos que a base quase como um todo \u00e9 de americanos, o que torna pouco relevante para o modelo usar esse atributo.","fea255f4":"Vamos come\u00e7ar analisando a quest\u00e3o do g\u00eanero relacionada os ganhos e horas de trabalho como citado anteriormente.","928921b2":"- Definindo os conjuntos de treino e teste:","d02b6694":"Percebemos, ent\u00e3o, que \u00e9 bastante v\u00e1lido substituir os dados faltantes pela **moda** (observa\u00e7\u00e3o de maior frequ\u00eancia), j\u00e1 que esta observa\u00e7\u00e3o tem alta ocorr\u00eancia no dataset:\n\nEnt\u00e3o, vamos imputar os dados faltantes pela moda nesses atributos.","15ed77e3":"- Vamos otimizar os hiperpar\u00e2metros:","6c5d2c21":"O jeito mais simples de enxergar essas correla\u00e7\u00f5es \u00e9 plotando um **Heat Map** da biblioteca seaborn.\nNele, como podemos ver na legenda da direita, quanto mais clara \u00e9 a c\u00e9lula, maior \u00e9 a correla\u00e7\u00e3o, e quanto mais escura \u00e9 a c\u00e9lula, menor \u00e9 a correla\u00e7\u00e3o entre as vari\u00e1veis.","89e461bf":"## 5. Compara\u00e7\u00e3o dos classificadores\n- Vamos agora fazer uma compara\u00e7\u00e3o entre os resultados obtidos para cada classificador, inclusive plotando a curva ROC de cada um. Utilizaremos como m\u00e9trica de compara\u00e7\u00e3o a AUC (\u00e1rea abaixo da curva ROC):","e8b19ea5":"### 4.4 Rede Neural\n- Vamos importar a Rede Neural do scikit-learn e instanciar o classificador:","c4fd0137":"- Curva ROC para a Rede Neural:","cae76cb4":"Desconsiderando o desbalanceamento dos dados, o gr\u00e1fico nos mostra que o n\u00famero de brancos que ganham >50K \u00e9 muito maior do que as demais etnias. Isso indica tamb\u00e9m um problema de desigualdade racial.","5bd358de":"Vamos tamb\u00e9m acrescentar a coluna Id e renomear a coluna correspondente do data frame para Income, que \u00e9 o que estamos prevendo com o classificador.","5a8b7c3a":"### 5.3 Random Forest","ad27a7c3":"Esse \u00e9 o resultado final do nosso preprocessamento dos dados de treino e teste:\n\nObs: devido a normaliza\u00e7\u00e3o, os dados ficaram sem \"significado real\", ent\u00e3o n\u00e3o devemos usar esses data frames para an\u00e1lise explorat\u00f3ria, por exemplo, temos valores da idade negativos. Isso foi feito somente para melhorar os classificadores.","2fb0cd5e":"Podemos tirar algumas conclus\u00f5es interessantes dessa correla\u00e7\u00e3o:\n- Justificadamente, a vari\u00e1vel **Martial Status** est\u00e1 intimamente relacionada com a vari\u00e1vel **Relationship**\n    \n    Apesar do fato de os tempos modernos estarem desconstruindo a ideia de um padr\u00e3o de fam\u00edlia, ainda h\u00e1 grande rela\u00e7\u00e3o entre o estado civil (casado, solteiro, divorciado, etc) e o tipo de rela\u00e7\u00e3o familiar que existe (vive em fam\u00edlia, tem\/n\u00e3o tem filhos, etc).\n    \n    \n- Obtivemos uma alta correla\u00e7\u00e3o na matriz entre **Sex** e **Hours per week** e uma correla\u00e7\u00e3o m\u00e9dia entre **Sex** e **Capital Gain**\n    \n    O fato do g\u00eanero estar altamente relacionado com o ganho de capital e com o n\u00famero de horas trabalhadas evidencia alguma desigualdade de g\u00eanero. Esse fato ser\u00e1 melhor analisado na an\u00e1lise explorat\u00f3ria na sequ\u00eancia.\n    \n    \n- A vari\u00e1vel **Income** (dada por Target) est\u00e1 altamente ligada \u00e0s vari\u00e1veis **Age, Sex, Capital Gain, Capital Loss e Hours per week**\n    \n    A terceira conclus\u00e3o e, provavelmente, a mais importante para o nosso modelo nos indica quais s\u00e3o as principais vari\u00e1veis que influenciam a renda anual da pessoa, justamente o que estamos interessados em classificar.\n    \n    \n    \n- Por \u00faltimo, vemos que a vari\u00e1vel **fnlwgt** apresenta rela\u00e7\u00e3o aproximadamente nula com todas as demais vari\u00e1veis presentes, sendo pouco \u00fatil, portanto, para o nosso classificador.","c1428528":"- Atributos num\u00e9ricos: Age, fnlwgt, Education-Num, Capital Gain, Capital Loss e Hours per week\n\nQuanto aos atributos num\u00e9ricos, descartaremos a vari\u00e1vel \"fnlwgt\", que representa o n\u00famero de pessoas na popula\u00e7\u00e3o alvo que aquela unidade correspodente representa, n\u00e3o tendo influ\u00eancia sobre a classifica\u00e7\u00e3o do KNN; \n\n- Atributos qualitativos: Workclass, Education, Martial Status, Occupation, Relationship, Race, Sex e Country\n\nQuanto aos atributos qualitativos, descartaremos a vari\u00e1vel Education, pois j\u00e1 existe uma vari\u00e1vel num\u00e9rica correspondente no dataset (Education-Num) e usar ambas seria redundante; descartaremos tamb\u00e9m a vari\u00e1vel Country, pois cerca de 91,3% (28059) das observa\u00e7\u00f5es do dataset correspondem a americanos, representando um grande desbalanceamento nos dados.\n\nAgora, vamos subdividir as vari\u00e1veis que consideramos relevantes em qualitativas e num\u00e9ricas para que seja poss\u00edvel process\u00e1-las de maneira diferente.","7c112d02":"Antes vamos precisar remover a primeira linha do data frame, pois ela conflita com a normaliza\u00e7\u00e3o. Isto ocorre porque essa primeira linha n\u00e3o \u00e9 um dado em si e muito menos num\u00e9rico, n\u00e3o sendo poss\u00edvel normaliz\u00e1-lo.","74eecd1a":"- Curva ROC para a Random Forest:","4106b37a":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\n#### $\\textit{- An\u00e1lise do dataset Adult, aplica\u00e7\u00e3o de alguns classificadores e compara\u00e7\u00e3o dos resultados }$\n#### $\\textit{Autor: Victor Rocha da Silva - PMR3508-2020-177}$\n\n*Obs.: Todo o in\u00edcio do notebook a seguir ser\u00e1 uma c\u00f3pia direta da primeira atividade. Esse notebook ser\u00e1 diferente apenas nos m\u00e9todos de classifica\u00e7\u00e3o utilizados.*\n\n## 1. Preparando os dados (Data Prep)\n### 1.1 Importando bibliotecas e dados\n- Primeiramente, vamos importar algumas bibliotecas que ser\u00e3o \u00fateis ao trabalhar com esses dados e, logo em seguida, o dataset.","d0604375":"- Vamos otimizar os hiperpar\u00e2metros:","58237e89":" - Agora vamos otimizar nossos par\u00e2metros utilizando o RandomizedSearch. Esse algoritmo de otimiza\u00e7\u00e3o, por mais que n\u00e3o seja aquele que obt\u00e9m os melhores resultados, \u00e9 pouco custoso computacionalmente e consegue resultados decentes o suficiente.","c5ade0b1":"Nas tabela acima vemos o resultado do nosso dataset ap\u00f3s o tratamento dos dados faltantes.\n\nRepetiremos o processo agora para o set de teste.","4051d9c8":"Podemos observar que certas vari\u00e1veis num\u00e9ricas como Education-Num, Capital Gain, Capital Loss e Hours per week conseguem, de certa forma, evidenciar a separa\u00e7\u00e3o de r\u00f3tulos, sendo muito mais \u00fateis para treinar o modelo.","6de1d256":"Com os dois gr\u00e1ficos acima, fica claro uma quest\u00e3o de desigualdade de g\u00eanero. \u00c9 not\u00e1vel que h\u00e1 uma grande desproporcionalidade entre o n\u00famero de homens e mulheres que ganham >50K, por mais que isso tamb\u00e9m ocorra em menor escala para <=50K.\nOutro fato \u00e9 que as mulheres possuem menos anos de estudo quando comparado aos homens.","70ff9459":"## 2. Interpreta\u00e7\u00e3o e an\u00e1lise do data frame\nNessa etapa, temos como objetivo fazer uma an\u00e1lise geral do nosso dataframe, analisando a correla\u00e7\u00e3o entre as vari\u00e1veis e interpretando os dados que temos. Os gr\u00e1ficos ser\u00e3o uma ferramenta de grande import\u00e2ncia para fazer tal an\u00e1lise.\n\n### 2.1 Vis\u00e3o geral da base de dados\nCome\u00e7aremos nossa an\u00e1lise com alguma observa\u00e7\u00f5es simples e relevantes sobre a base de dados.","8d1a6c2d":"Feita essa an\u00e1lise de correla\u00e7\u00e3o, podemos partir agora para uma an\u00e1lise explorat\u00f3ria da base de dados.","476362b4":"A principio vemos que na nossa base, 27815 das pessoas s\u00e3o brancas, o que representa 85.4% dos dados. Logo, devemos levar em considera\u00e7\u00e3o que isso pode influenciar o plot do gr\u00e1fico e enfatizar desproporcionalmente algumas conclus\u00f5es.","8fab1d93":"A partir da descri\u00e7\u00e3o acima, notamos que os dados faltantes se concentram em tr\u00eas atributos diferentes: **Workclass, Occupation e Country**. Essa conclus\u00e3o foi tirada pelo fato de que o contador identificou menos observa\u00e7\u00f5es do que o total de 32561 dispon\u00edveis a priori. Temos:\n- 1836 (5,63%) faltando em Workclass, \n- 1843 (5,66%) faltando em Occupation\n- 583 (1,79%) faltando em Country\n\nVamos analisar, de maneira breve, cada um desses tr\u00eas atributos, para decidirmos o melhor tratamento poss\u00edvel:","4c3bf804":" - Vamos testar nosso SVM com os par\u00e2metros padr\u00f5es, para depois comparar com o modelo otimizado:","f1d6d974":"- Curva ROC para a SVM:","ba298849":" - Finalmente, temos nosso resultado:","65d4460f":"### 3.3 Processamento dos atributos categ\u00f3ricos (qualitativos)\nPara os atributos categ\u00f3ricos, iremos passar os dados para uma escala num\u00e9rica. Desse modo ser\u00e1 poss\u00edvel us\u00e1-los no nosso classificador K-NN. Para isso, podemos usar o utilit\u00e1rio LabelEncoder da biblioteca scikit-learn. \n\nFaremos isso tanto na base de treino como na de teste.","961c9bb8":"### 1.2 Lidando com dados faltantes (*missing data*)\n- Aqui faremos uma an\u00e1lise acerca dos dados faltantes, verificando onde se concentram e qual a melhor maneira de lidar com eles.","29b2674e":"Agora, s\u00f3 precisamos exportar o nosso data frame com a predi\u00e7\u00e3o para o formato .csv","f42e7cbf":"### 5.1 Regress\u00e3o Log\u00edstica","99d60fcb":"## 6. Predi\u00e7\u00e3o\n- Ent\u00e3o, vamos obter a predi\u00e7\u00e3o da nossa vari\u00e1vel de classe utilizando o classificador otimizado Random Forest:","f1f6a9cf":"### 4.2 Support Vector Machinhe (SVM)\n- Vamos importar o SVM do scikit-learn e instanciar o classificador:"}}