{"cell_type":{"e11d78a3":"code","491ae500":"code","684e869c":"code","7814de79":"code","6359ad2f":"code","756ff6c5":"code","0fd2b9db":"code","c867082a":"code","e57e27f1":"code","9a19bdfd":"code","4f5f2b86":"code","3777667b":"code","9a2958af":"code","dcf141f0":"code","8590314f":"code","ca473f8b":"code","241f5b8b":"code","ac8209af":"code","866eddc7":"code","4c5c79a7":"code","82b2a718":"code","5c4077c1":"code","5fc72828":"code","4d8bb46d":"code","c8dc5813":"code","075893ee":"code","c20f2ee4":"code","07340bbe":"code","2cbe5d1c":"code","974e570a":"code","bd83f01b":"code","07540974":"markdown","c5dae993":"markdown","6041ab33":"markdown","2315f70f":"markdown"},"source":{"e11d78a3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport subprocess\nimport gc","491ae500":"TRAIN_PATH = '..\/input\/train.csv'\nTEST_PATH = '..\/input\/test.csv'","684e869c":"# thanks to szelee for this quick loading method\np = subprocess.Popen(['wc', '-l', TRAIN_PATH], stdout=subprocess.PIPE, \n                                               stderr=subprocess.PIPE)\nresult, err = p.communicate()\nif p.returncode != 0:\n    raise IOError(err)\nn_rows = int(result.strip().split()[0])+1","7814de79":"# Params \nFARE_MIN = 2.00\nFARE_MAX = 150\nYEAR_MIN = 2000\nYEAR_MAX = 2018\nPASSENGER_MIN = 1\nPASSENGER_MAX = 6\nLAT_MIN  = 39.9\nLAT_MAX  = 41.3\nLONG_MIN = -74.4\nLONG_MAX = -72.5\nDIST_MIN = 0.05\nDIST_MAX = 35\n\n# Bin params\nNUM_LAT_BINS = 140*5\nNUM_LONG_BINS = 190*5\nNUM_TIME_BINS = 480\nNUM_DIST_BINS = 400\n\nlat_bins = np.linspace(LAT_MIN, LAT_MAX, NUM_LAT_BINS+1).tolist()\nlat_bins = [-90] + lat_bins + [90]\nlong_bins = np.linspace(LONG_MIN, LONG_MAX, NUM_LONG_BINS+1).tolist()\nlong_bins = [-180] + long_bins + [180]\ntime_bins = np.linspace(-1, 2400, NUM_TIME_BINS+1).tolist()\ndist_bins = np.linspace(DIST_MIN-1, DIST_MAX+1, NUM_DIST_BINS+1).tolist()","6359ad2f":"# thanks to madhurisivalenka for this function\ndef add_haversine_distance_feature(df, lat1='pickup_latitude', long1='pickup_longitude', lat2='dropoff_latitude', long2='dropoff_longitude'):\n    #R = 6371  # radius of earth in kilometers\n    R = 3959 # radius of earth in miles\n    phi1 = np.radians(df[lat1])\n    phi2 = np.radians(df[lat2])\n\n    delta_phi = np.radians(df[lat2]-df[lat1])\n    delta_lambda = np.radians(df[long2]-df[long1])\n\n    #a = sin\u00b2((\u03c6B - \u03c6A)\/2) + cos \u03c6A . cos \u03c6B . sin\u00b2((\u03bbB - \u03bbA)\/2)\n    a = np.sin(delta_phi \/ 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda \/ 2.0) ** 2\n\n    #c = 2 * atan2( \u221aa, \u221a(1\u2212a) )\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n\n    #d = R*c\n    d = (R * c)\n    df[\"distance\"] = d.astype('float32')\n    \n    return df","756ff6c5":"def drop_conditional(df):\n    # 1.\n    df = df.drop(df[df.isnull().any(1)].index, axis=0)\n    df = df.drop(df[df.isin([np.nan, np.inf, -np.inf]).any(1)].index, axis=0)\n    with pd.option_context('mode.use_inf_as_null', True):\n        df = df.dropna()\n\n    # 2.\n    df = df.drop(df[df.fare_amount > FARE_MAX].index, axis=0)\n    df = df.drop(df[df.fare_amount < FARE_MIN].index, axis=0)\n    \n    # 3.\n    df = df.drop(df[df.passenger_count > PASSENGER_MAX].index, axis = 0)\n    df = df.drop(df[df.passenger_count < PASSENGER_MIN].index, axis = 0)\n\n    # 4.\n    df = df.drop(df[df.pickup_latitude > LAT_MAX].index, axis=0)\n    df = df.drop(df[df.pickup_latitude < LAT_MIN].index, axis=0)\n    \n    df = df.drop(df[df.pickup_longitude > LONG_MAX].index, axis=0)\n    df = df.drop(df[df.pickup_longitude < LONG_MIN].index, axis=0)\n\n    df = df.drop(df[df.dropoff_latitude > LAT_MAX].index, axis=0)\n    df = df.drop(df[df.dropoff_latitude < LAT_MIN].index, axis=0)\n    \n    df = df.drop(df[df.dropoff_longitude > LONG_MAX].index, axis=0)\n    df = df.drop(df[df.dropoff_longitude < LONG_MIN].index, axis=0)\n    \n    # 5.\n    df = df.drop(df[df.distance > DIST_MAX].index, axis = 0)\n    df = df.drop(df[df.distance < DIST_MIN].index, axis = 0)\n    \n    df = df.drop(df[df.year > YEAR_MAX].index, axis = 0)\n    df = df.drop(df[df.year < YEAR_MIN].index, axis = 0)\n    \n    return df","0fd2b9db":"def add_date_features(df):\n    # 6.\n    # df.drop(columns=['key'], inplace=True)\n    df.pickup_datetime = df.pickup_datetime.str.slice(0, 16)\n    df.pickup_datetime = pd.to_datetime(df.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M')\n    # 7.\n    df['year'] = df.pickup_datetime.dt.year.astype('uint16')\n    df['month'] = df.pickup_datetime.dt.month.astype('uint8')\n    # df['day'] = df.pickup_datetime.dt.day.astype('uint8')\n    df['dayofweek'] = df.pickup_datetime.dt.dayofweek.astype('uint8')\n    hours = df.pickup_datetime.dt.hour.astype('uint8')\n    minutes = df.pickup_datetime.dt.minute.astype('uint8')\n    # combine the minutes and hours into a single variable\n    df['time_of_day'] = (hours*100.0) + (minutes*(5.0\/3)) # makes time [0 - 2399]\n    # don't need the pickup_datetime anymore since it's been divided into the above cols\n    df = df.drop(columns=['pickup_datetime'])\n    \n    return df","c867082a":"def bin_data(df):\n    df.pickup_latitude = pd.cut(df.pickup_latitude, precision=7, bins=lat_bins, labels=False).astype(\"uint16\")\n    df.pickup_longitude = pd.cut(df.pickup_longitude, precision=7, bins=long_bins, labels=False).astype(\"uint16\")\n    df.dropoff_latitude = pd.cut(df.dropoff_latitude, precision=7, bins=lat_bins, labels=False).astype(\"uint16\")\n    df.dropoff_longitude = pd.cut(df.dropoff_longitude, precision=7, bins=long_bins, labels=False).astype(\"uint16\")\n    df.time_of_day = pd.cut(df.time_of_day, bins=time_bins, precision=7, labels=False).astype(\"uint16\")\n    df.distance = pd.cut(df.distance, bins=dist_bins, precision=7, labels=False).astype(\"uint16\")\n    return df","e57e27f1":"def clean_data(df, test=False):\n    df = add_haversine_distance_feature(df)\n    df = add_date_features(df)\n    if not test:\n        df = drop_conditional(df) \n        df = bin_data(df)\n    return df","9a19bdfd":"traintypes = {'fare_amount': 'float16',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\ncols = list(traintypes.keys())\nchunksize = 2**20\ntotal_chunk = n_rows \/\/ chunksize + 1\n\ndef load_all_data(X):\n    df_list = [] # list to hold the batch dataframe\n    i = 0\n    limit = -1\n    for df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize):    \n        i = i+1\n        # Each chunk is a corresponding dataframe\n        print(f'DataFrame Chunk {i:02d}\/{total_chunk}')\n        df_chunk = clean_data(df_chunk)\n        # Alternatively, append the chunk to list and merge all\n        df_list.append(df_chunk)\n        if i == limit:\n            break\n    \n    X = pd.concat(df_list)\n    return X","4f5f2b86":"# Shuffle\ndef shuffle(X):\n    X = X.sample(frac=1).reset_index(drop=True)\n    return X","3777667b":"# Cut down on the data size\ndef reduce_data(X):\n    drop_portion = 0.25\n    X = X.drop(X.index[0:int(X.shape[0]*drop_portion)])\n    return X","9a2958af":"# Normalize\n# also change data into different float types to save on memory where we can\nminmax = pd.DataFrame()\nfloat32cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"distance\", \"time_of_day\"]\nfloat16cols = [\"passenger_count\", \"month\", \"dayofweek\", \"year\"]\n\ndef normalize(X):\n    for col in float32cols:\n        col_min = X[col].min()\n        col_max = X[col].max()\n        minmax[col] = (col_min, col_max)\n        X[col] = ((X[col] - col_min) \/ (col_max-col_min)).astype('float32')\n\n    for col in float16cols:\n        col_min = X[col].min()\n        col_max = X[col].max()\n        minmax[col] = (col_min, col_max)\n        X[col] = ((X[col] - col_min) \/ (col_max-col_min)).astype('float16')\n\n    minmax.time_of_day = (-1, 2400)\n    return X\n#     print(X.head())\n#     print(X.info())","dcf141f0":"# actually run everything\nX = pd.DataFrame()\nX = load_all_data(X)\nX.info()\nprint(\"Loading complete\")","8590314f":"X = shuffle(X)\nprint(\"Shuffle complete\")","ca473f8b":"X = reduce_data(X)\nprint(\"Reduction complete\")","241f5b8b":"X = normalize(X)\nprint(\"Normalization complete\")","ac8209af":"print(X.head())\nprint(X.info())","866eddc7":"# take out the answers\ny = []\ny = X.fare_amount\nX = X.drop(columns=\"fare_amount\")","4c5c79a7":"# Splitting off a validation set\nvalidation_portion = 1.0\/1001\nindex = int(X.shape[0]*validation_portion)\nprint(\"training:\\t%d\\nvalidation:\\t%d\" % (X.shape[0]-index, index))","82b2a718":"val_X = X[0:index]\nX = X.drop(X.index[0:index])","5c4077c1":"val_y = y[0:index]\ny = y.drop(y.index[0:index])","5fc72828":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras import metrics\nfrom keras import backend as K\nK.set_image_dim_ordering('tf')\n\n# Using a GPU for this kernel\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)","4d8bb46d":"model = Sequential()\n\nmodel.add(Dense(32, kernel_initializer=\"normal\", input_dim=X.shape[1], activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5));\nmodel.add(Dense(32, kernel_initializer=\"normal\", activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, kernel_initializer=\"normal\", activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, kernel_initializer=\"normal\", activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='relu'))\n\nmodel.compile(loss='mean_squared_error',\n              optimizer='nadam', \n              metrics=[metrics.mae])","c8dc5813":"num_epochs = 10\nbatch_size = 2**9\nhistory = model.fit(X.values, y.values, \n                    validation_data = (val_X, val_y),\n                    shuffle=True, \n                    epochs=num_epochs, \n                    batch_size=batch_size)","075893ee":"plt.figure()\nplt.plot(history.history['loss'], color=\"blue\")\nplt.plot(history.history['val_loss'], color=\"red\")\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\n\nplt.figure()\nplt.plot(history.history['mean_absolute_error'], color=\"blue\")\nplt.plot(history.history['val_mean_absolute_error'], color=\"red\")\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.ylabel(\"Mean Abs. Error\")\nplt.xlabel(\"epoch\")","c20f2ee4":"# a little test\nval_pred = model.predict(val_X[0:5]).flatten()\nprint(\"actual: \"+str(val_y[0:5].values))\nprint(\"pred:   \"+str(val_pred))","07340bbe":"traintypes = {'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\ncols = list(traintypes.keys())\ncols.append('key')\n\nX_test = pd.read_csv(TEST_PATH, usecols=cols, dtype=traintypes)\nX_test = clean_data(X_test, test=True)\nX_test_key = X_test['key']\nX_test.drop(columns=['key'], inplace=True)\n\n# normalize with the same values as the train data\n\nfor col in float16cols:\n    col_min, col_max = minmax[col]\n    X_test[col] = ((X_test[col] - col_min) \/ (col_max-col_min)).astype('float16')\n\nfor col in float32cols:\n    col_min, col_max = minmax[col]\n    X_test[col] = ((X_test[col] - col_min) \/ (col_max-col_min)).astype('float32')\n    \nX_test.head()","2cbe5d1c":"pred = model.predict(X_test).flatten()\npred = np.round(pred,2)","974e570a":"results = pd.DataFrame({'key': X_test_key, 'fare_amount': pred})\nresults.key = results.key.astype(str)\nresults.info()\nresults.to_csv('submission.csv', index=False)","bd83f01b":"print(results[0:5])","07540974":"# Create a Neural Net","c5dae993":"# Predict the Test Data\n\nThe test data will be loaded the same way that the train data was and prepared in the same fasion as well.","6041ab33":"# Train the Neural Net\nBecause this is not classification, we will not report accuracy because it will not be an accurate measure of performance. For example: if the fare is 15.25\\$ and the model predicts 15.297942 which gets rounded to 15.30\\$ the answer will be incorrect even though it was very close. Instead of accuracy, mean squared error will be used. A low value correlates to a low overall error.","2315f70f":"# Data Loading and Preparation\nThis dataset is so gigantic that we can afford to just drop rows we don't like. The effect will be very minimal.\n1. Remove the nulls.\n1. Remove any with a fare amount that's less than 0. Your taxi driver doesn't pay you!\n1. Remove any rides that have over 6 passengers, I don't think more than that really fit into a taxi... (also removing taxies that have less than 1 rider, obviously)\n1. Remove any rides that have lat\/long coordinates outside of the set bounds of NYC. \n1. Create new col of `distance` and remove any rides that go for more than 100 miles.\n1. `key` and `pickup_datetime` seem to both be dates, and also contain about the same time. So let's drop one to use less memory and convert the other to a more easily usable datetime.\n1. Just having a datetime isn't enough. Taxi rides are probably linked to the day of the week, the time of the day, and more. Let's make those data columns.\n\nNormalize after loading all the data into memory"}}