{"cell_type":{"83118778":"code","fb29cf5d":"code","d8fc1839":"code","72e02eb9":"code","40217b49":"code","da806a06":"code","d2cba60b":"code","4c898682":"code","93498249":"code","327d1f3d":"code","826e15a3":"code","217ae534":"code","6faadc10":"code","81f61b81":"code","99746413":"code","7bfeb74b":"code","cf69ac86":"code","db6f75ad":"markdown","eec7b223":"markdown"},"source":{"83118778":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fb29cf5d":"import pandas as pd\nimport numpy as np\nimport pyodbc\nfrom pandas import DataFrame\n\nimport string\nimport nltk\nfrom sklearn import re\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import FreqDist \nfrom nltk.stem.porter import *\nimport matplotlib.pyplot as plt \nlemma = WordNetLemmatizer()\n\nfrom fuzzywuzzy import process\nfrom fuzzywuzzy import fuzz\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\nimport csv\nimport datetime\nfrom datetime import datetime\nfrom datetime import date, timedelta\nimport os, re\n\n\n# vectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression #classification model\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score \n\n","d8fc1839":"df_train = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")\ndf_train","72e02eb9":"print(len(df_train[df_train.label == 0]), 'Non-Hatred Tweets')\nprint(len(df_train[df_train.label == 1]), 'Hatred Tweets')","40217b49":"df_combined = df_train.append(df_test, ignore_index=True, sort = False)\ndf_combined","da806a06":"def remove_pattern(input_text, pattern):\n    r = re.findall(pattern, input_text)\n    for i in r:\n        input_text = re.sub(i, '', input_text)\n        \n    return input_text   ","d2cba60b":"# remove twitter handles (@user)\ndf_combined['tidy_tweet'] = np.vectorize(remove_pattern)(df_combined['tweet'], \"@[\\w]*\")\ndf_combined","4c898682":"# remove special characters, numbers, punctuations\ndf_combined['tidy_tweet'] = df_combined['tidy_tweet'].str.replace('[^\\w\\d#\\s]',' ')\ndf_combined","93498249":"df_combined['tidy_tweet'] = df_combined['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf_combined","327d1f3d":"tokenized_tweet = df_combined['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","826e15a3":"stemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\ndf_combined['tidy_tweet_stemmed'] = tokenized_tweet\ndf_combined\n","217ae534":"def extract_hashtag(tweet):\n    tweets = \" \".join(filter(lambda x: x[0]== '#', tweet.split()))\n    tweets = re.sub('[^a-zA-Z]',' ',  tweets)\n    tweets = tweets.lower()\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \"\".join(tweets)\n    return tweets\n\ndf_combined['hashtag'] = df_combined.tweet.apply(extract_hashtag)\ndf_combined","6faadc10":"df_hashtag = df_combined['hashtag'].apply(lambda x: x.split())\ndf_hashtag = df_hashtag.to_list()\ndf_hashtag","81f61b81":"hash_list = [j for i in df_hashtag for j in i]\nhash_list = pd.DataFrame(hash_list)\nhash_list = hash_list.rename(columns = {0 : 'hashtags'})\nhash_list","99746413":"hash_list = hash_list.hashtags.str.split(expand=True).stack().value_counts()\nhash_list = pd.DataFrame(hash_list)\nhash_list = hash_list.reset_index()\nhash_list = hash_list.rename(columns = {'index' : 'hashtags', 0 : 'count'})\nhash_list","7bfeb74b":"import matplotlib.pyplot as plt; plt.rcdefaults()\nimport matplotlib.pyplot as plt\n\nx_pos = hash_list['hashtags']\ny_pos = hash_list['count']\n\nplt.bar(y_pos, x_pos, align='center', alpha=0.5)\nplt.xticks(y_pos, x_pos)\nplt.ylabel('Count')\nplt.title('HashTags')\n\nplt.show()\n\n","cf69ac86":"import plotly.graph_objects as go\n\nfig = go.Figure(go.Bar(\n            y=hash_list['hashtags'],\n            x=hash_list['count'],\n            orientation='h'))\n\nfig.show()","db6f75ad":"Basic Cleansing:","eec7b223":"To identify hastags:"}}