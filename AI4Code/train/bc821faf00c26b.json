{"cell_type":{"3bbfa574":"code","1b363c04":"code","0e8c2dd0":"code","5df36567":"code","9d99fade":"code","6165a133":"code","7afb0a5b":"code","cd657f3d":"code","a1dff669":"code","4f491351":"code","6578ad39":"code","c8601fde":"code","f9b3de85":"code","2392d09b":"code","a2ce92e7":"code","2ca20271":"code","b93a237b":"code","a0dfe054":"code","c5712c2b":"code","f36301b7":"code","2e8db660":"code","131f94ea":"code","b428638f":"code","3583ce4b":"code","f38aa3e6":"code","acf4d714":"code","61d183d9":"code","4de4ac84":"code","f4432d94":"code","24826d40":"code","2097e980":"code","9bcb203a":"code","e179d7ca":"code","c804a348":"code","41f00cf9":"code","27cd1215":"code","9d2df53c":"code","71e342eb":"code","78134187":"code","dead0c3e":"code","626173a5":"code","355edd48":"code","8daf5449":"code","44c33c40":"code","b6b98824":"code","5d89bf6f":"code","c9df69b6":"code","36f075de":"code","f445bda1":"code","8bc545a8":"code","24179f8b":"code","e6c86762":"code","503f51a2":"code","048e52fb":"code","7d0c92e2":"code","4bc569c2":"code","6535b0fd":"code","0b2df1e8":"code","7dadd1a2":"code","0fb56be4":"code","78d9cbe3":"code","4baf9de1":"code","84754db7":"markdown","b443d5a2":"markdown","c763a630":"markdown","a266e068":"markdown","68a3de9f":"markdown","49c8b935":"markdown","3570ded8":"markdown","5748b0be":"markdown","69c0d816":"markdown","44d77179":"markdown","6d099acc":"markdown","3415c8d6":"markdown","87d3f48f":"markdown","56e190fb":"markdown","f59067d6":"markdown","f1ee4fde":"markdown","d8ba1c8d":"markdown","6366eb42":"markdown","0786afa8":"markdown"},"source":{"3bbfa574":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd","1b363c04":"data = pd.read_csv('..\/input\/divorce-prediction\/divorce_data.csv', delimiter=';')\nreference = pd.read_csv('..\/input\/divorce-prediction\/reference.tsv', delimiter='|')","0e8c2dd0":"data","5df36567":"data_features = data.drop('Divorce', axis=1)\ndata_features.rename(columns=lambda x: x.replace('Q',\"\"), inplace=True)\ndata_features.columns = [int(i) for i in data_features.columns]","9d99fade":"positive = []\nnegative = []\nfor i in list(data.index):\n    if (data['Divorce'][i] == 0):\n        positive.append(i)\n    else:\n        negative.append(i)","6165a133":"data_features_positive = data_features.drop(positive, axis=0)\ndata_features_negative = data_features.drop(negative, axis=0)","7afb0a5b":"import matplotlib.pyplot as plt\nimport seaborn as sns","cd657f3d":"data_features_all = pd.DataFrame(columns=[], index=[])\n\nj = 1\nfor i in range(1,55,1):\n    data_features_all[j] = data_features[i]\n    j+=1\n    data_features_all[j] = data_features_positive[i]\n    j+=1\n    data_features_all[j] = data_features_negative[i]\n    j+=1","a1dff669":"fig, axes = plt.subplots(nrows = round(len(data_features_all.columns) \/ 3), ncols = 3, figsize=(12,160))\nd = 0\nc = 0\nfor i, ax in enumerate(fig.axes):\n    if i < len(data_features_all.columns):\n        \n        if i == 1: \n            ax.set_title(\"%s\"%(d+1) + \". \" + reference['description'][d])\n            c = 0\n            d+=1\n        elif (c % 3 == 0) and (c!=0) and (d <= 53):\n            ax.set_title(\"%s\"%(d+1) + \". \" + reference['description'][d])\n            d+=1\n        c+=1\n        \n        sns.countplot(x=data_features_all.columns[i], alpha=0.5, data=data_features_all, ax=ax)\n        ax.set_ylabel('')    \n        ax.set_xlabel('')\n\nfig.tight_layout()","4f491351":"from sklearn.model_selection import train_test_split","6578ad39":"X_training, X_test, y_training, y_test = train_test_split(data_features, data['Divorce'], test_size = 0.3, random_state = 1)","c8601fde":"print('TRAIN: ', y_training.value_counts())\nprint('\\nTEST: ', y_test.value_counts())","f9b3de85":"print('train: ', X_training.shape[0])\nprint('test: ', X_test.shape[0])","2392d09b":"!pip install xgboost\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline","a2ce92e7":"for classifier, pl in zip((LogisticRegression,\n                           RandomForestClassifier,\n                           SGDClassifier,\n                           DecisionTreeClassifier,\n                           CategoricalNB,\n                           KNeighborsClassifier,\n                           XGBClassifier,\n                           AdaBoostClassifier,\n                           SVC),\n                          ('Logistic regression',\n                           'Random forest', \n                           'Stochastic Gradient Descent',\n                           'Decision Tree',\n                           'Categorical Naive Bayes',\n                           'K-nearest neighbor Classifier',\n                           'XGBClassifier',\n                           'AdaBoostClassifier',\n                           'Support Vector Machines')):\n\n    pipe = Pipeline([('clf', classifier())])    \n    pipe.fit(X_training, y_training)\n    print(pl)\n    print(\"Training sample: \",pipe.score(X_training, y_training))\n    print(\"Testing sample:  \",pipe.score(X_test, y_test),'\\n')","2ca20271":"graph_importance = RandomForestClassifier()\ngraph_importance.fit(X_training,y_training)\nimportant_values = pd.DataFrame(graph_importance.feature_importances_, index=X_training.columns, columns=['importance'])\nimportant_values.sort_values('importance').plot(kind='barh', figsize=(15, 15))","b93a237b":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","a0dfe054":"SEED = 1","c5712c2b":"X_train, X_valid, y_train, y_valid = train_test_split(X_training, y_training, test_size = 0.3, random_state = 40)\nprint('TRAIN: ', y_train.value_counts())\nprint('\\nValid: ', y_valid.value_counts())","f36301b7":"print('train: ', X_train.shape[0])\nprint('valid: ', X_valid.shape[0])","2e8db660":"best_score = 0\ncounter = np.logspace(-2,2,5)\n\nfor C in counter:\n    logreg = LogisticRegression(C=C, penalty='l2', solver='liblinear')\n    logreg.fit(X_train, y_train)\n    score = logreg.score(X_valid, y_valid)\n    if (score > best_score) and (score != 1.0):\n        best_score = score\n        best_parameters = {'random_state':SEED,'C':C, 'penalty':'l2','solver':'liblinear'}\n\nbest_params_logreg = best_parameters\nlogreg = LogisticRegression(**best_parameters)\nlogreg.fit(X_training, y_training)\ntest_score = logreg.score(X_test, y_test)\n\nprint(\"best parameters\\n{}\".format(best_parameters))\nprint(\"\\nbest proper score\\non training:  {:.4f}\".format(best_score))\nprint(\"\\ntest's score: {:.4f}\".format(test_score))","131f94ea":"forest = RandomForestClassifier(random_state=SEED)\n\nforest_params = {\n    'criterion': ['gini','entropy'],\n    'n_estimators': [20,50,100,130],\n    'max_features': range(1,30,1),\n    'max_depth': range(1,20,1),\n}\n\nforest_search = RandomizedSearchCV(forest, forest_params, cv = 5)\naaa = forest_search.fit(X_training, y_training)","b428638f":"print(\"Best parameteres:\\n{:}\\n\".format(forest_search.best_params_)) \nprint(\"Best estimator:\\n{:}\\n\".format(forest_search.best_estimator_))","3583ce4b":"best_forest = forest_search.best_estimator_\nprint(\"Best score on training data: {:.4f}\".format(forest_search.best_score_))\nprint(\"Best score on testing data:  {:.4f}\".format(best_forest.score(X_test, y_test)))","f38aa3e6":"dtree = DecisionTreeClassifier(random_state=SEED)\n\ndtree_params = {\n    'criterion': ['gini','entropy'],\n    'max_features': range(1,30,1),\n    'max_depth': range(1,20,1),\n}\n\ndtree_search = GridSearchCV(dtree, dtree_params, cv = 5)\ndtree_search.fit(X_training, y_training)","acf4d714":"print(\"Best parameteres:\\n{:}\\n\".format(dtree_search.best_params_)) ","61d183d9":"print(\"Best value on cross-validation: {:}\".format(dtree_search.best_score_)) \nprint(\"testing sample: {:.4f}\".format(dtree_search.score(X_test, y_test)))","4de4ac84":"best_score = 0\ncounter_knn = range(1,10,1)\n\nfor neighbors in counter_knn:\n    knn = KNeighborsClassifier(n_neighbors=neighbors)\n    knn.fit(X_train, y_train)\n    score = knn.score(X_valid, y_valid)\n    \n    if (score > best_score) and (score != 1.0):\n        print(best_score)\n        best_score = score\n        best_parameters = {'n_neighbors':neighbors}\n        \nknn = KNeighborsClassifier(**best_parameters)\nknn.fit(X_training, y_training)\ntest_score = knn.score(X_test, y_test)\n\nprint(\"best parameters\\n{}\".format(best_parameters))\nprint(\"\\nbest proper score\\non training:  {:.4f}\".format(best_score))\nprint(\"\\ntest's score: {:.4f}\".format(test_score))","f4432d94":"best_score = 0\n\nfor C in np.logspace(-3,3,30):\n    for gamma in np.logspace(-2,2,6):\n        for kernel in ['rbf','linear']:\n            svc = SVC(gamma=gamma, kernel=kernel, C=C)\n            svc.fit(X_train, y_train)\n            score = svc.score(X_valid, y_valid)\n            if (score > best_score):\n                best_score = score\n                best_parameters = {\n                    'C':C,\n                    'gamma':gamma,\n                    'kernel':kernel\n                }\n\n                \nsvc = SVC(**best_parameters)\nsvc.fit(X_training, y_training)\ntest_score = svc.score(X_test, y_test)\ntest_score","24826d40":"print(\"best parameters\\n{}\".format(best_parameters))\nprint(\"\\nbest proper score\\non training:  {:.4f}\".format(best_score))\nprint(\"\\ntest's score: {:.4f}\".format(test_score))","2097e980":"from sklearn.metrics import roc_auc_score\n\nlogr = LogisticRegression(**best_params_logreg)\nrfor = RandomForestClassifier(**forest_search.best_params_)\n\ndef get_score(model, X, y, Xt, yt):\n  model.fit(X, y)\n  y_pred = model.predict_proba(Xt)[:,1]\n  score = roc_auc_score(yt, y_pred)\n  return score","9bcb203a":"from sklearn.feature_selection import VarianceThreshold","e179d7ca":"low_div = VarianceThreshold(threshold=0.9)\nX_trlow = low_div.fit_transform(X_training)\nX_trlow = pd.DataFrame(X_trlow, columns = list(X_training.columns[low_div.get_support()]))","c804a348":"X_trlow.shape","41f00cf9":"X_training.shape","27cd1215":"X_telow = low_div.transform(X_test)\nX_telow = pd.DataFrame(X_telow, columns = list(X_test.columns[low_div.get_support()]))","9d2df53c":"get_score(logr, X_trlow, y_training, X_telow, y_test)","71e342eb":"get_score(rfor,  X_trlow, y_training, X_telow, y_test)","78134187":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif","dead0c3e":"univar_div = SelectKBest(mutual_info_classif, 30)\nX_trunivar = univar_div.fit_transform(X_training, y_training)\nX_trunivar = pd.DataFrame(X_trunivar, columns = list(X_training.columns[univar_div.get_support()]))","626173a5":"X_trunivar.shape","355edd48":"sns.set(font_scale = 1.5)\nf, ax = plt.subplots(figsize=(15, 15))\nsns.barplot(y = X_training.columns, x = univar_div.scores_, palette = 'pastel', orient = 'h');","8daf5449":"X_trunivar.head()","44c33c40":"X_teunivar = univar_div.transform(X_test)\nX_teunivar = pd.DataFrame(X_teunivar, columns = list(X_test.columns[univar_div.get_support()]))","b6b98824":"univar_logr = get_score(logr, X_trunivar, y_training, X_teunivar, y_test)\nprint('Logistic Regression score:', univar_logr)","5d89bf6f":"univar_rfor = get_score(rfor, X_trunivar, y_training, X_teunivar, y_test)\nprint('Random Forest score:', univar_rfor)","c9df69b6":"from sklearn.preprocessing import StandardScaler","36f075de":"scaler = StandardScaler() \nscaler.fit(X_training)","f445bda1":"X_tr_standart = scaler.transform(X_training)\nX_tr_standart = pd.DataFrame(X_tr_standart, columns = X_training.columns)\nX_tr_standart.head()","8bc545a8":"X_te_standart = scaler.transform(X_test)\nX_te_standart = pd.DataFrame(X_te_standart, columns = X_test.columns)\nX_te_standart.head()","24179f8b":"from sklearn.decomposition import PCA","e6c86762":"pca = PCA(n_components=54)\npca.fit(X_tr_standart)","503f51a2":"variance = pca.explained_variance_ratio_\nvar = np.cumsum (np.round (variance, 3) * 100) \n\nsns.set(font_scale = 1.5)\nf, ax = plt.subplots(figsize=(15, 5))\nplt.ylabel('% Variance Explained') \nplt.xlabel('# of Features') \nplt.title('analysis of PCA') \nsns.lineplot(x = range(1, 55), y = var);","048e52fb":"sns.set(font_scale = 1.5)\nf, ax = plt.subplots(figsize=(15, 15))\nplt.ylabel('# of Features') \nplt.xlabel('% Variance Explained') \nplt.title('analysis of PCA') \nsns.barplot(y = list(range(1, 55)), x = pca.explained_variance_ratio_, palette = 'pastel', orient = 'h');","7d0c92e2":"pca = PCA(n_components = 0.99, svd_solver = 'full') \npca.fit(X_tr_standart)","4bc569c2":"pca.n_components_","6535b0fd":"sns.set(font_scale = 1.5)\nf, ax = plt.subplots(figsize=(15, 15))\nplt.ylabel('# of Features') \nplt.xlabel('% Variance Explained') \nplt.title('analysis of PCA') \nsns.barplot(y = list(range(1, 34)), x = pca.explained_variance_ratio_, palette = 'pastel', orient = 'h');","0b2df1e8":"X_train_pca = pca.transform(X_tr_standart)\nX_test_pca  = pca.transform(X_te_standart)","7dadd1a2":"X_train_pca = pd.DataFrame(X_train_pca, columns = [str(i) + ' component' for i in range(1, pca.n_components_ + 1)])\nX_test_pca  = pd.DataFrame(X_test_pca,  columns = [str(i) + ' component' for i in range(1, pca.n_components_ + 1)])","0fb56be4":"X_train_pca.head()","78d9cbe3":"pca_logreg_score = get_score(logr, X_train_pca, y_training, X_test_pca, y_test)\nprint('Logistic Regression score:', pca_logreg_score)","4baf9de1":"pca_rforest_score = get_score(rfor, X_train_pca, y_training, X_test_pca, y_test)\nprint('Random Forest score:', pca_rforest_score)","84754db7":"# Feature selection","b443d5a2":"# Learning","c763a630":"## Standardization","a266e068":"# Graphics","68a3de9f":"# Important features","49c8b935":"This dataset contains data about 150 couples with their corresponding Divorce Predictors Scale variables (DPS) on the basis of Gottman couples therapy.\nThe couples are from various regions of Turkey wherein the records were acquired from face-to-face interviews from couples who were already divorced or happily married.\nAll responses were collected on a 5 point scale\n\n(0=Never, 1=Seldom, 2=Averagely, 3=Frequently, 4=Always).","3570ded8":"## Univariate feature selection","5748b0be":"Attribute Information:\n\n1. If one of us apologizes when our discussion deteriorates, the discussion ends.\n2. I know we can ignore our differences, even if things get hard sometimes.\n3. When we need it, we can take our discussions with my spouse from the beginning and correct it.\n4. When I discuss with my spouse, to contact him will eventually work.\n5. The time I spent with my wife is special for us.\n6. We don't have time at home as partners.\n7. We are like two strangers who share the same environment at home rather than family.\n8. I enjoy our holidays with my wife.\n9. I enjoy traveling with my wife.\n10. Most of our goals are common to my spouse.\n11. I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.\n12. My spouse and I have similar values in terms of personal freedom.\n13. My spouse and I have similar sense of entertainment.\n14. Most of our goals for people (children, friends, etc.) are the same.\n15. Our dreams with my spouse are similar and harmonious.\n16. We're compatible with my spouse about what love should be.\n17. We share the same views about being happy in our life with my spouse\n18. My spouse and I have similar ideas about how marriage should be\n19. My spouse and I have similar ideas about how roles should be in marriage\n20. My spouse and I have similar values in trust.\n21. I know exactly what my wife likes.\n22. I know how my spouse wants to be taken care of when she\/he sick.\n23. I know my spouse's favorite food.\n24. I can tell you what kind of stress my spouse is facing in her\/his life.\n25. I have knowledge of my spouse's inner world.\n26. I know my spouse's basic anxieties.\n27. I know what my spouse's current sources of stress are.\n28. I know my spouse's hopes and wishes.\n29. I know my spouse very well.\n30. I know my spouse's friends and their social relationships.\n31. I feel aggressive when I argue with my spouse.\n32. When discussing with my spouse, I usually use expressions such as \u2018you always\u2019 or \u2018you never\u2019 .\n33. I can use negative statements about my spouse's personality during our discussions.\n34. I can use offensive expressions during our discussions.\n35. I can insult my spouse during our discussions.\n36. I can be humiliating when we discussions.\n37. My discussion with my spouse is not calm.\n38. I hate my spouse's way of open a subject.\n39. Our discussions often occur suddenly.\n40. We're just starting a discussion before I know what's going on.\n41. When I talk to my spouse about something, my calm suddenly breaks.\n42. When I argue with my spouse, \u0131 only go out and I don't say a word.\n43. I mostly stay silent to calm the environment a little bit.\n44. Sometimes I think it's good for me to leave home for a while.\n45. I'd rather stay silent than discuss with my spouse.\n46. Even if I'm right in the discussion, I stay silent to hurt my spouse.\n47. When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger.\n48. I feel right in our discussions.\n49. I have nothing to do with what I've been accused of.\n50. I'm not actually the one who's guilty about what I'm accused of.\n51. I'm not the one who's wrong about problems at home.\n52. I wouldn't hesitate to tell my spouse about her\/his inadequacy.\n53. When I discuss, I remind my spouse of her\/his inadequacy.\n54. I'm not afraid to tell my spouse about her\/his incompetence","69c0d816":"### Random Forest","44d77179":"### K-nearest neighbor Classifier","6d099acc":"### Support Vector Machines","3415c8d6":"Of the participants, 84 (49%) [Class=0] were divorced and 86 (51%) [Class=1] were married couples.","87d3f48f":"## Principal component analysis (PCA)","56e190fb":"### Decision Tree","f59067d6":"## Simple learning","f1ee4fde":"# Dimensionality reduction","d8ba1c8d":"## Configure Hyperparameteres","6366eb42":"## Removing features with low variance","0786afa8":"### Logistic Regression"}}