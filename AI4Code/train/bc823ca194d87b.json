{"cell_type":{"8e655179":"code","afe4ad5e":"code","731bf0c0":"code","7ef28b91":"code","c0ba284e":"code","4a21fe09":"code","b1a0d52e":"code","6758d8f2":"markdown","7593d284":"markdown","408c5223":"markdown","5de673cf":"markdown","83694d21":"markdown","574db0ff":"markdown","d73e7642":"markdown"},"source":{"8e655179":"#loading the outputs\nimport joblib\nwithoutshuffle = joblib.load('..\/input\/gptmodel-outputs\/results (4)\/withoutshuffle.pkl')\nwithshuffle = joblib.load('..\/input\/gptmodel-outputs\/results (3)\/withshuffle.pkl')\ndata29 = joblib.load('..\/input\/gptmodel-outputs\/results (5)\/data29.pkl')","afe4ad5e":"import matplotlib.pyplot as plt\nplt.plot(withshuffle[0],withshuffle[2],'r+')","731bf0c0":"plt.plot(withoutshuffle[0],withoutshuffle[2],'r+')","7ef28b91":"plt.plot(data29[0],data29[2],'r+')","c0ba284e":"buckets = []\nplt.rcParams[\"figure.figsize\"] = (25,3)\nimport numpy as np\nfor i in range(0,10000,10):\n    buckets.append(np.nanmean(withoutshuffle[2][i:i+10]))\nplt.plot(buckets)\nplt.savefig('data00.png')","4a21fe09":"buckets = []\nplt.rcParams[\"figure.figsize\"] = (25,3)\nimport numpy as np\nfor i in range(0,10000,10):\n    buckets.append(np.nanmean(data29[2][i:i+10]))\nplt.plot(buckets)\nplt.savefig('data29.png')","b1a0d52e":"#Finding means and variances\nprint(np.nanmean(withoutshuffle[2]),np.nanmean(data29[2]))\nprint(np.nanvar(withoutshuffle[2]),np.nanvar(data29[2]))","6758d8f2":"* Data @param withshuffle, @param withoutshuffle @param data29 are nested arrays with following structure\n\n> array[0] index of the document with respect to THE PILE dataset\n\n> array[1] length of the document\n\n> array[2] the score of the document (number of correctly predicted labels)","7593d284":"* Dividing the arrays of 0th and 29th shard into 1000 buckets and plotting their average score","408c5223":"* Atleast of gpt-neo-1.3B model, there doesn't seem to be any correlation between the way data is memorized and the position of data within training dataset","5de673cf":"* The folllowing two graphs compare the score of the model with and without shuffling the evaluation data\n* More information about shuffling can be found [here](https:\/\/www.kaggle.com\/usaiprashanth\/gpt-1-3b-model?scriptVersionId=72760342) and [here](https:\/\/www.kaggle.com\/usaiprashanth\/gpt-1-3b-model?scriptVersionId=72761073)","83694d21":"* Atleast for the first 10,000 samples, there doesn't seem to be any difference in the memorization of data with respect to it's position in the dataset.\n* However, It is worth noting that 10,000 samples is a very small sampling for a dataset as big as [The Pile](https:\/\/pile.eleuther.ai\/) and the results can significantly differ when evaluated with another shard of the dataset.\n* This can be generalized by plotting the bucketed version of data29 (outputs of 29th shard of THE PILE)","574db0ff":"# About This Notebook\n* The following notebooks utilizes the [generated outputs](https:\/\/www.kaggle.com\/usaiprashanth\/gptmodel-outputs) and performs some Exploratory Data Analysis","d73e7642":"* My original interpretation of this idea (which has been proved wrong) was that the order in which the data would be evaluated would effect the evaluation loss of model. Which is inherently false. The reasoning for this is due to the fact of there being randomness involved with the model."}}