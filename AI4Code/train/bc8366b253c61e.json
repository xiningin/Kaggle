{"cell_type":{"7d1d1d11":"code","f752e462":"code","a05ea103":"code","e5261113":"code","eb3d5a54":"code","351614ed":"code","c93db5a6":"code","5a6cf899":"code","889b72ae":"code","57e70b7c":"code","f4381fff":"code","a40777a9":"code","b32dba8d":"code","2951f8e3":"code","f7be18a8":"code","baed09ce":"code","cf741327":"code","91e3aa9d":"code","ddc24ae4":"code","328b4160":"code","ecd75ef9":"code","7a172d99":"code","63b3b785":"markdown","66eff346":"markdown","45bb1e9a":"markdown","e4139e04":"markdown","4d1cab06":"markdown","6ef1224f":"markdown","14d32019":"markdown","1321a35c":"markdown","dc18a48f":"markdown","160c595c":"markdown","f4dd1931":"markdown","2c7d0dd4":"markdown","603abde9":"markdown","5fe28de0":"markdown","2bec216f":"markdown","976b1d5f":"markdown","9f5103c1":"markdown","8b297b57":"markdown","3b33d964":"markdown","19d44413":"markdown","3ac1b52e":"markdown","41f2a3a3":"markdown","0d5f6fa4":"markdown","74e4462c":"markdown","d384861c":"markdown","32d01828":"markdown","ee17f615":"markdown","f2068503":"markdown","96dcd04d":"markdown","12a7d888":"markdown","bc4c50db":"markdown"},"source":{"7d1d1d11":"import os\nimport cv2\nimport ast\nimport json\nimport subprocess\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Video  \n","f752e462":"main_path= '..\/input\/tensorflow-great-barrier-reef'","a05ea103":"tdf= pd.read_csv(main_path + '\/train.csv')\ntdf.head()","e5261113":"tdf.info()","eb3d5a54":"tdf.describe()","351614ed":"Tdf=pd.read_csv(main_path + '\/test.csv')\nTdf.head()","c93db5a6":"tdf.annotations.iloc[133:200]","5a6cf899":"a=tdf.annotations.describe()\nprint(a)\ntdf.annotations.unique()","889b72ae":"for video_id in tdf['video_id'].unique():\n    print(f'video_id: {video_id}')\n    print(f'no. of images without annotations:  {sum(tdf[tdf[\"video_id\"]==video_id][\"annotations\"]== \"[]\")}')\n    print(f'no. of images with annotations:  {sum(tdf[tdf[\"video_id\"]==video_id][\"annotations\"] != \"[]\")}\\n')","57e70b7c":"a1=2143\/(4565 + 2143)*100 ; a2=2099\/(6133+2099)*100 ;a3=677\/(7884+677)*100 ; a=(2143 +2099+677)\/23501*100\nprint(f\"total % annotations in video0 : {a1}% \\ntotal % annotations in video1: {a2}% \\ntotal % annotations in video2 : {a3}% \\nToal % annotations  : {a}%\" )","f4381fff":"sdf=pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv\")\nsdf","a40777a9":"sample_test = np.load('..\/input\/tensorflow-great-barrier-reef\/example_test.npy') \n#sample_test","b32dba8d":"# Change the type of 'annotations' from str to list\ntdf[\"annotations\"] = tdf[\"annotations\"].apply(ast.literal_eval) # str -> list","2951f8e3":"\n# Add columns of image path and number of bboxes, and the difference.\ntdf['image_path'] = main_path + '\/train_images\/video_'+ tdf['video_id'].astype(str) + '\/' + tdf['video_frame'].astype(str) + \".jpg\"\ntdf['num_bboxes'] = tdf['annotations'].apply(lambda x: len(x))\ntdf.head()","f7be18a8":"tdf.sequence_frame.unique()","baed09ce":"a=tdf.num_bboxes.unique()\n#display(a)\ntdf['num_bboxes'][tdf['num_bboxes']==0].count()","cf741327":"fig ,ax= plt.subplots(3,1,figsize=(20,30))\nsns.set_style('darkgrid')\nsns.scatterplot(data=tdf , y='num_bboxes',x='video_frame' , hue ='video_id',ax=ax[0] ,size='num_bboxes', palette=['r','g','b'])\nsns.scatterplot(data=tdf , y='num_bboxes',x='sequence_frame' , hue ='video_id',ax=ax[1] ,size='num_bboxes', palette=['r','g','b'])\nsns.scatterplot(data=tdf , y='num_bboxes',x='sequence' , hue ='video_id',ax=ax[2] ,size='num_bboxes', palette=['r','g','b'])","91e3aa9d":"tdf.head(2)","ddc24ae4":"# splitting the annotation list into suitable parts \na= tdf.annotations.loc[134]\nprint(a ,type(a))\nprint(a[0] , type(a[0]))\nc= tdf.annotations[tdf.num_bboxes==18]\nprint(\"c :\\n\" ,c)","328b4160":"tdf.annotations.loc[12679]","ecd75ef9":"\ndef access_annotation(i ,df):  # i = index number\/ row number \/ loc value ,df = data\n    a= df.annotations.loc[i] #annotation number i \n    # accessing dict values\n    x=[];  y=[]; width =[] ;height =[]\n    for j in range(0,df.num_bboxes.loc[i]):\n        \n        x.append(a[j].get('x'))\n        y.append(a[j].get('y'))\n        width.append(a[j].get('width'))\n        height.append(a[j].get('height'))\n    \n    return x,y,width,height\nx,y,width,height=access_annotation(12679,tdf)\nprint(\"values of annotations :\\nx :\",x,\"\\ny : \",y,\"\\nwidth : \",width,\"\\nheight: \" ,height)\n","7a172d99":"# plot function \ndef plot_fig_with_annotation(i,df,figsize):\n    \n    fig ,ax = plt.subplots(1,1,figsize =figsize) \n    img = cv2.imread(df.image_path.loc[i])  # reading image\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # adding color\n    \n    if (df.annotations.loc[i]!=[]):\n        x,y,width,height = access_annotation(i,df)\n        for j in range(0,df.num_bboxes.loc[i]):\n           # plotting rectangles \n            cv2.rectangle(img,\n                  pt1=(x[j], y[j]),\n                  pt2=(x[j]+height[j], y[j]+width[j]),\n                  color=(0,299,0),\n                  thickness=3)\n            # adding text to each annatotation \n            ax.text(x[j],y[j]-5 ,\"label \" + str(j+1) ,color = 'black')  # adds text label at top of image\n        ax.set_axis_off()\n        ax.imshow(img)\n\n            \n        print(\"THERE EXISTS ANNATATIONS FOR BOTS IN THIS IMAGE AS SHOWN IN FIG\")\n    else :\n        plt.imshow(img)\n        print(\"THERE ARE NO ANNOTATIONS IN THIS IMAGE\")\n        \nplot_fig_with_annotation(12679,tdf,(20,20)) #try value of i = 10 , 100, 5474 , 9291 , 12679","63b3b785":"#### \u2b55annotations per video ","66eff346":"# \ud83d\udca5 Visualizing Image Data ","45bb1e9a":"## \ud83c\udf40 Conclusion of DATA analysis :\n1. we need to implement Object Detection algorithm \n2. model should be less heavy to train for creating real time application\n3. Data augmentation is sufficient but not necessary\n4. Transfer learning is to be used  training entire data is not feasible \n5. we should be using YOLO , RCNN ,or something like that...\n\n","e4139e04":"### \ud83d\udca0 Analysis for prediction Y\nlet's see what exactly we have to predict ","4d1cab06":" ok .. now we want to get values of x,y,width and heigth of each rectangle annotation in each image  <br>\n `note:` there are upto 18 rectangle (num_bboxes) annotations in each image , so we need upto  18 x,  y,  width and  height values each","6ef1224f":"#### 2. example_test.npy","14d32019":"so here for $0^{th}$ annotation :<br> \n`0.99` -- it's showing that there is bbox at given position and with given size confirming it's existence with CI of 99% or simply put it is probability of bbox being present there , <br>\n![image.png](attachment:b697bdcc-5315-46be-a44a-1a1e5b1512b3.png)<br>\n`123` -- is x co-ordnate it's  $b_x$,<br> \n`456` -- is y co-ordinate it's $b_y$,<br>\n`1` is weidth it's $b_w$'<br>\n`1` is height its $b_h$,\n\nthe image itself is solution but hard to implement \ud83d\ude02","1321a35c":"#### \ud83d\udd05Adding image path to dataset","dc18a48f":" In this notebook I tried to explain all the things that are needed for us to kick start our model building , I kept my comments as  begginer friendly as I can , explaining all facts like a conversation , hope it will help you to understand this interesting problem <br>\n So.. let's dive in...\ud83c\udfca\u200d\ud83c\udfca\u200d\n # \ud83d\udc20\ud83d\udc1f\ud83d\udc21\ud83e\udd91\ud83d\udc19\ud83e\udd88\ud83d\udc2c\ud83d\udc33\ud83d\udc0b\ud83e\udd80\ud83d\udc1a\ud83c\udfca\u200d\u2640\ufe0f\ud83c\udf40\u2618\ud83d\udcba\ud83d\udea4\u2693\ud83c\udfdd\ud83c\udf0a\ud83c\udf0a \ud83d\udc20\ud83d\udc1f\ud83d\udc21","160c595c":"so train.csv explain us the parameters to connect the images in the video_0,video_1 and video_2 folders, \n`video_id` -- explain us from which folder the file is , if video_id val ==0 then it belongs to video_0 and so on..\n`sequence` -- ","f4dd1931":"# \ud83d\udca5Preprocessing and feature engineering","2c7d0dd4":"# \ud83d\udca2 Basic info & requirements for this competition in simple language \ud83d\ude05\n<br>\n<br>\n\n\n![](https:\/\/505488.smushcdn.com\/2242995\/wp-content\/uploads\/site_assets\/crown_of_thorns_starfish_5457578925_.jpg?lossy=1&strip=1&webp=1)\n\n\n>**Goal** : there is one species of star fish which is eating corals and destroying the great barrier reef , we have to find that COTS named star fish (above fig. ) by creating an object detection model which can work in real time <br>\n**What we have with us to analyse and create model ?**  : well , in   our data we have <br>\n.  1. one API named inside folder greatbarrier reef <br>\n.  2. train_images : these are basically 3 videos whose images are splitted into video_0 , video_1 , video_2 folders <br>\n.  3. test, train, sample submission csvs are there <br>\n.  4. example test.npy is also there : <br>\n\n>**what we have to actually do here** : for training there is a image of corals and there is marking\/annotation ( a sqare bounding box ) is given in image at specific position and size , i.e. COTS fish is marked in bounding box using annotations , what we have to do is create a model which will train based on the given annotations and while testing it should create the annotations\/bounding boxes on it's own to find where the actual COTS fish is, there in the given image,\nfurther more the entire system should work in real time which means, when some sea diver is taking video clip of corals deep down sea our algorithm should detect the object from that video...this is what expected by user as per my understanding.\n\n>**what we have to submit** : as usual.. we have to predict y but this time y is  annotation and submit it in submission.csv file ..  [refer code requirements](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/overview\/code-requirements)<br>\n>**How our submission will be evaluated** : using F2 score , i.e False positivity rate is accepted , F2= 5PR\/(4P+R )\n\nif someone has any doubt or any other view regarding the question understanding or if something is missing , then please comment and share your views. because it will help me to explore the data more effectively.","603abde9":"hmm... annotations is the key here,\nlet's see what this feature's specifications are...\ud83e\udd14","5fe28de0":"ok.. let's see what we have in our sample submission example","2bec216f":"#### \ud83d\udca0importanting libraries ","976b1d5f":"ok now we got dimensions of each annotation , now let's plot it","9f5103c1":"#### \ud83c\udf40thank you  all helpers \n[@Yoshi_K ](https:\/\/www.kaggle.com\/yoshikuwano\/eda-non-annotated-starfish)  [@CAMARO](https:\/\/www.kaggle.com\/bamps53\/create-annotated-video#kln-23) \n[@tensor_choko](https:\/\/www.kaggle.com\/tensorchoko\/tensorflow-eda-for-beginner-jp-en\/notebook)","8b297b57":"we want to convert these values into one list ","3b33d964":"interesting, isn't it ?\ud83d\ude2f upvote if you like the simplicity in code \ud83d\ude09","19d44413":"### \ud83d\udca0Importing Data ","3ac1b52e":"#### \ud83d\udca2 plotting the images with annotations","41f2a3a3":"<br>\n<br>\n<br>","0d5f6fa4":"#### \ud83d\udca2converting annotations from [{'x': 666, 'y': 45, 'width': 32, 'height': 33}] to --> (666, 45, 32, 33)","74e4462c":"so I tried to get more insight about how the annotations are scattered among the data as per sequence and as per video frame :\nconclusion is  Video_id 1,2 has more annotations per frame than video_id 0 , there are upto 18 annotations in 1 and 2 while 0 has only upto 5 <br>\nwhat could be the use of this annotation scattering ? are they random ? --- Umm video_id 1 and 2 are normely distributed annotations while video 0 has damped annotations ... what is the meaning of this then ...  it is possible that video_id 0 has some annotations which are wrongly placed or something like outside the grid annnotations or someone disturbed annotations of  video 0 or someone trimmed the data of video 0. but simply putting we don't know yet how this happened , it could be just wrong interpretation as well.","d384861c":"ok..  we got something very bad here ..\nfor every annotation we have specific co-ordinates { x ,y, width and height} but lot's of annotations are empty `only 20.93%` images are annoted and that's not at all interesting \ud83d\ude02\ud83d\ude02 bcoz it's going to force us to use some advanced algorithms to create model from very less training dataset ... \n<br>\n![download.png](attachment:4c722f68-7f1c-4869-812a-12bba1433eff.png) <br>\njust kidding \ud83d\ude02","32d01828":"# \ud83d\udca5EDA for the <strong> Great barrier Reef <\/strong> dataset","ee17f615":"### \ud83d\udca0 Expected output annotation\n\n**What's there in the examples submission files ????**","f2068503":"`Note:` our train.csv only possess video_id , sequence, video frame , image sequence, image id but actual images are in video_0,1,2 folders we need to combine them , one way is copying path of images as per video_id ... thanks to Yoshi_k he has done it for us by writting below code    ","96dcd04d":"#### \ud83d\udd05 new features and their insights ","12a7d888":"#### 1. example_sample_submission.csv ","bc4c50db":"#### \u2b55percentage of annatotation "}}