{"cell_type":{"5bd5afbc":"code","4bcf1410":"code","8e146bf6":"code","cd0c33f2":"code","647e867e":"code","f75f2477":"code","758f196c":"code","4915a913":"code","7c336bf8":"code","7b60eeef":"code","89536b67":"code","d90e33e0":"code","3aec2a8f":"markdown"},"source":{"5bd5afbc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4bcf1410":"df=pd.read_csv('..\/input\/email-spam-classification-dataset-csv\/emails.csv')\nprint(df.head(5))\nprint(df.tail(5))","8e146bf6":"# df.corr(method='pearson')","cd0c33f2":"import matplotlib.pyplot as plt\nprint(\"Visualizing ratio Ham\/Spam:\\n\")\ncount_Class = pd.value_counts(df['Prediction'], sort=True)\ncount_Class.plot(kind = 'pie',labels=['Ham','Spam'], autopct='%1.0f%%,')\nplt.title('Ham vs Spam')\nplt.ylabel('')\nplt.show","647e867e":"# Splitting dataframe into features and target.\nX = df.iloc[:,1:-1].values\nY = df.iloc[:,-1].values\nprint(X.shape)\nprint(Y.shape)\nprint(X)","f75f2477":"#feat_cols = ['feature'+str(i) for i in range(X.shape[1])]\n#normalised_X = pd.DataFrame(X,columns=feat_cols)\n#n_pca_comp = 50\n#pca = PCA(n_components=n_pca_comp)\n#principalComponents_hamspam = pca.fit_transform(X)\n#scaler = MinMaxScaler()\n#scaler.fit(principalComponents_hamspam)\n#XX = scaler.transform(principalComponents_hamspam)\n\n#principal_hamspam_Df = pd.DataFrame(data = principalComponents_hamspam, columns = ['principal component '+str(i) for i in range(n_pca_comp)])\n#principal_hamspam_Df\n","758f196c":"# Splitting data into test and train\ntrain_x, test_x, train_y, test_y = train_test_split (X,Y, test_size=0.25, random_state=42)\nprint(train_x.shape)\nprint(test_x.shape)","4915a913":"mnb =  MultinomialNB ( alpha = 2 )\nmnb.fit(train_x, train_y)\ny_predNB = mnb.predict(test_x)\n\nprint(\"Accuracy score for Naive Bayes: \", accuracy_score(y_predNB, test_y))\n\ntarget_labels = ['Ham','Spam']\nprint(classification_report(test_y, y_predNB, target_names=target_labels))\nprint(\"Accuracy score for Naive Bayes: \", accuracy_score(y_predNB, test_y))\ncm = confusion_matrix(test_y, y_predNB)\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n\nlabels = [f\"{v1}\\n{v2}\" for v1, v2, in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm, annot=labels, fmt='')","7c336bf8":"#principal_hamspam_Df.head(5)\n#X_dim = principal_hamspam_Df.iloc[:,:].values\n#print(X_dim)","7b60eeef":"# Random Forest, an ensemble method fitted with cross validation function.\n\nrfc =  RandomForestClassifier(n_estimators=100, criterion=\"gini\")\n\n# ATTEMPT OF USING CROSS VALIDATION. COULD NOT UNDERSTAND WHAT IS REALLY HAPPENING. IN MY UNDERSTANDING, WITH CROSS VALIDATION THERE IS NO NEED TO SPLIT THE DATASET INTO\n# TRAIN AND TEST, AS THE SCI-KIT DOCUMENTATION STATES.\n\nscores = cross_val_score(rfc,X,Y,cv=10)\nprint(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores.mean(), scores.std()))\n\ny_pred_teste = cross_val_predict(rfc, test_x, test_y, cv=10, method='predict')\ny_pred_teste.shape","89536b67":"print(\"Accuracy Score of Random Forest Classifier : \", accuracy_score(test_y, y_pred_teste))\nprint(\"\\n\")\ntarget_labels = ['Ham','Spam']\nprint(classification_report(test_y,y_pred_teste, target_names=target_labels))\ncmm = confusion_matrix(test_y, y_pred_teste)\ntn, fp, fn, tp = cmm.ravel()\ngroup_names1 = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts1 = [\"{0:0.0f}\".format(value) for value in cmm.flatten()]\nlabels1 = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names1,group_counts1)]\nlabels1 = np.asarray(labels1).reshape(2,2)\nsns.heatmap(cmm, annot=labels1, fmt='')","d90e33e0":"# Random Forest classifier with the train test split function.\n\nrfc.fit(train_x,train_y)\ny_predRFC = rfc.predict(test_x)\nprint(classification_report(test_y,y_predRFC))\nprint(\"Accuracy Score of Random Forest Classifier : \", accuracy_score(y_predRFC,test_y))\ncm = confusion_matrix(test_y, y_predRFC)\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2, in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm, annot=labels, fmt='')","3aec2a8f":"**As we can see above, our dataframe is already in a word count, and each word is a feature (3000 features) in the dataframe.\n\nLooking at the dataframe we see that it is in a way that the word count has an effect on the outcome being spam or ham. In that way, thinking of the likelihood of a word (or a set of words) in a message and the outcome of that message being a spam or a ham, the Naive Bayes model could be employed here since it is about probabilities.\n\nSo that will be the first choice to train the model.**"}}