{"cell_type":{"b44db720":"code","e996166c":"code","ca79321a":"code","691fc3e6":"code","2133046c":"code","518f8fc8":"code","c3d0af22":"code","b214fd21":"code","f9dd9c2b":"code","c30a5f4d":"markdown"},"source":{"b44db720":"#import useful modules\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import cKDTree\nimport os\nimport gc\nimport json\nfrom gensim.models import Word2Vec\nimport tabulate\n\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport spacy\nfrom tqdm.notebook import tqdm\nimport glob\nimport re\nimport string\nfrom gensim.models import FastText\nfrom itertools import chain\nimport logging\nimport sys\nfrom gensim import utils, matutils \nfrom numpy import dot\nfrom IPython.core.display import display, HTML\nfrom IPython.display import display, Markdown, Latex\ntqdm.pandas()\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nnlp = spacy.load(\"en_core_web_sm\",disable=['parser','ner'])\nstopwords = spacy.lang.en.stop_words.STOP_WORDS\ntable = str.maketrans('', '', string.punctuation)\ncovid_synonyms = ['coronavirus-2019-ncov','covid','covid-19','corona','coronavirus','ncp']","e996166c":"# read the processed df from other kernel (this is done to save the time for redundant tasks and accelerate the experimentation)\nfull_df = pd.read_csv(\"\/kaggle\/input\/covid-data-preprocessing\/corona_research_articles.csv\")","ca79321a":"# read the trained word2vec model\nfull_df.fillna('',inplace=True)\nsimilarity_model = FastText.load(\"\/kaggle\/input\/covid-data-preprocessing\/fasttex_similarity_model.model\")\nsimilarity_model.init_sims(replace=True)","691fc3e6":"def get_must_have_words(query):\n    words = [z.lower().translate(table) for z in re.findall(r\"[\\w']+|[.,!?;]\",query) if z.lower() not in stopwords and not z.isnumeric() and len(z)>=3]\n    return words","2133046c":"#convert all sections to vector\ndoc_vec_full = np.concatenate(full_df['tokenized_text'].progress_apply(lambda x:matutils.unitvec(np.array([similarity_model.wv[word] for word in list(chain.from_iterable([y.split(\" \") for y in x.split(\".\")]))]).mean(axis=0))).values).reshape(-1,100)","518f8fc8":"queries = [\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery\",\n           \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children)\",\n           \"Seasonality of transmission\",\n           \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)\",\n           \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood)\",\n           \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic)\",\n           \"Natural history of the virus and shedding of it from an infected person\",\n           \"Implementation of diagnostics and products to improve clinical processes\",\n           \"Disease models, including animal models for infection, disease and transmission\",\n           \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n           \"Immune response and immunity\",\n           \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n           \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n           \"Role of the environment in transmission\"]\nquery_dict = {index:queries[index] for index in range(len(queries))}","c3d0af22":"# get the broad level similarity of each query with the text\nquery_words_list = []\nfor index, query in query_dict.items():\n    query_words = get_must_have_words(query)\n    query_vec = matutils.unitvec(np.array([similarity_model.wv[word] for word in query_words]).mean(axis=0).reshape(-1,100))\n    full_df['{}_query_similarity'.format(index)] = np.matmul(doc_vec_full,query_vec.reshape(-1,1))#full_df['tokenized_text'].progress_apply(lambda x: get_similarity(x,query_words_list))","b214fd21":"corona_vec = matutils.unitvec(np.array([similarity_model[word] for word in covid_synonyms]).mean(axis=0).reshape(-1,100))\nfull_df[\"corona_similarity\"] = np.matmul(doc_vec_full,corona_vec.reshape(-1,1))","f9dd9c2b":"for query_num in range(len(queries)):\n    query = query_dict[query_num]\n    query_words = get_must_have_words(query)\n    query_vec = matutils.unitvec(np.array([similarity_model.wv[word] for word in query_words]).mean(axis=0).reshape(-1,100))\n    query_df = full_df.loc[full_df[\"{}_query_similarity\".format(query_num)]>0.8]\n    query_df[\"weighted_similarity\"] = query_df[\"{}_query_similarity\".format(query_num)]*query_df[\"corona_similarity\"]\n    query_df = query_df.sort_values(by=[\"weighted_similarity\".format(query_num)],ascending=False).head(500)\n    full_text = \"\"\n    display(Markdown('<font color=green>**Query: {}**<\/font>'.format(query)))\n    sentences, scores, titles = [], [], []\n    for index,row in query_df.reset_index().iterrows():\n        title = row['title']\n        paper_id = row['paper_id']\n        section_code = row['section_code']\n        section_name = row['section_name']\n        section_text = row['section_text']\n        tokenized_text = row['tokenized_text']\n        para_vec_full = np.concatenate([matutils.unitvec(np.array([similarity_model.wv[word] for word in y.split(\" \")]).mean(axis=0).reshape(-1,100)) for y in tokenized_text.split(\".\")]).reshape(-1,100)\n        similarity_score_query = np.array([similarity_model.wv.wmdistance(y.split(\" \"),query_words) for y in tokenized_text.split(\".\")])#np.matmul(para_vec_full,query_vec.reshape(-1,1)).reshape(-1,)\n        similarity_score_corona = np.array([similarity_model.wv.wmdistance(y.split(\" \"),covid_synonyms) for y in tokenized_text.split(\".\")])#np.matmul(para_vec_full,corona_vec.reshape(-1,1)).reshape(-1,)\n        similarity_score = similarity_score_query*0.7 + similarity_score_corona*0.3 #np.multiply(similarity_score_query,similarity_score_corona)\n        n_len = len(similarity_score[np.where(similarity_score<=1.0)])\n        n_sentences = n_len#max(n_len,3)\n        arg_sort = np.argsort(similarity_score)\n        sentences = sentences + [section_text.split(\".\")[rank] for rank in arg_sort[:n_sentences]]\n        scores = scores + list(similarity_score[arg_sort[:n_sentences]])\n        titles = titles + [title]*len(similarity_score[arg_sort[:n_sentences]])\n        sorted_para = \".\".join([section_text.split(\".\")[rank] for rank in arg_sort[:n_sentences]])\n        full_text = full_text + sorted_para\n\n    args = np.argsort(np.array(scores))\n    sorted_sentences = [(titles[index], sentences[index],scores[index]) for index in np.argsort(np.array(scores))]\n    table = [[titles[index], sentences[index], scores[index]] for index in np.argsort(np.array(scores))[:10]]\n    display(HTML(tabulate.tabulate(table, [\"Title\",\"Relevant Text\",\"dissimilarity (lower the better)\"],tablefmt='html')))","c30a5f4d":"**The objective of this notebook is to find the relevant text, given a particular question\/query**.\n\nIn the previous versions, we appraoached at getting the more relevant sections given a query. We will now apply a further fine tuning to get the most relevant sentence from the paragraph. In the previous appraoch we used IoU as the similarity matrix, in here we will convert the section into a vector, and then use the dot product eith the query vector to get the similarity. We will be using a simpler way of getting the document vector from the word vectors, i.e. we will represent a document vector as the average of all the relevant words present in the section, and similaraly for "}}