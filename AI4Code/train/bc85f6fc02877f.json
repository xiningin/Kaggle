{"cell_type":{"ae23f293":"code","79051863":"code","7b479ba0":"code","c224fe56":"code","1015850d":"code","7a67ae04":"code","fbc76592":"code","4e982c94":"code","a80067be":"code","e56ddf16":"code","67213492":"code","59cec999":"code","95137f9a":"code","f7b13a26":"code","41741492":"code","03f41b13":"code","a505471b":"code","ff49ada3":"code","17ee5fc4":"code","8e5316f2":"code","aea77964":"code","21e2fa2f":"code","5ec7fb88":"code","62a4c600":"code","0ec33c35":"code","b5e7dae8":"code","3dc5ddaa":"code","85aff453":"code","7659ad31":"code","a9b375bd":"code","7173d00b":"code","b67c2d33":"code","74ee11fb":"code","e2eda517":"code","1e292b17":"code","fbdfaa3b":"code","f34b54e5":"code","d833316a":"code","8f104ba6":"code","cfe9d697":"code","5ec9563b":"code","ab4cb0be":"code","fd793f04":"code","2dfd43e5":"code","071db785":"code","e7773a7f":"code","4bf8bc9f":"code","31079f5d":"code","ab909ba8":"code","ba33b587":"code","bf6f305c":"code","4408b130":"code","7b1db64b":"code","1f76068d":"code","b8d5c173":"code","14f503e3":"code","651a8d98":"code","522c9f0c":"code","8128b31f":"code","80b3672a":"code","fb0fd629":"code","6548b3b4":"code","bb04767d":"code","33e5d907":"code","1415e2d5":"code","17536515":"code","ab5c66d1":"code","167b257c":"code","8f2397f3":"code","24ffe541":"code","f3096e92":"code","c05fd931":"code","4f8fc006":"code","11907f47":"code","4e97e2da":"code","60090238":"code","e8d2076a":"code","166d9cae":"code","57788503":"code","e9e02ee5":"code","3b5058e9":"markdown","3e8fec78":"markdown","8b008389":"markdown","6c50f628":"markdown","99408ef3":"markdown","eb46f230":"markdown","7105c9dc":"markdown","291c0291":"markdown","810fd629":"markdown","d7aae7ef":"markdown","9e58f623":"markdown","8921efca":"markdown","53b92e92":"markdown","1063f9d6":"markdown","f46846a8":"markdown","95087150":"markdown","4452807e":"markdown","a1d6f7de":"markdown"},"source":{"ae23f293":"#Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","79051863":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom datetime import datetime, timedelta","7b479ba0":"#for visualization\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\n","c224fe56":"import sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","1015850d":"# To perform Hierarchical clustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","7a67ae04":"data = pd.DataFrame(pd.read_csv('..\/input\/country-data\/Country-data.csv'))\ndata.head(5)","fbc76592":"#checking for duplicates\nsum(data.duplicated(subset = 'country')) == 0","4e982c94":"data.shape\n","a80067be":"data.info()","e56ddf16":"data.describe()\n","67213492":"data.isnull().sum()","59cec999":"\ndata.dtypes","95137f9a":"\ncat_col = data.select_dtypes(include = ['object']).columns\nnum_col = data.select_dtypes(exclude = ['object']).columns\n","f7b13a26":"fig, axs = plt.subplots(3,3,figsize = (15,15))\n\n\n# Child Mortality Rate : Death of children under 5 years of age per 1000 live births\ntop10_child_mort = data[['country','child_mort']].sort_values('child_mort', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='child_mort', data= top10_child_mort, ax = axs[0,0])\nplt1.set(xlabel = '', ylabel= 'Child Mortality Rate')\n\n# Fertility Rate: The number of children that would be born to each woman if the current age-fertility rates remain the same\ntop10_total_fer = data[['country','total_fer']].sort_values('total_fer', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='total_fer', data= top10_total_fer, ax = axs[0,1])\nplt1.set(xlabel = '', ylabel= 'Fertility Rate')\n\n# Life Expectancy: The average number of years a new born child would live if the current mortality patterns are to remain same\n\nbottom10_life_expec = data[['country','life_expec']].sort_values('life_expec', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='life_expec', data= bottom10_life_expec, ax = axs[0,2])\nplt1.set(xlabel = '', ylabel= 'Life Expectancy')\n\n# Health :Total health spending as %age of Total GDP.\n\nbottom10_health = data[['country','health']].sort_values('health', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='health', data= bottom10_health, ax = axs[1,0])\nplt1.set(xlabel = '', ylabel= 'Health')\n\n# The GDP per capita : Calculated as the Total GDP divided by the total population.\n\nbottom10_gdpp = data[['country','gdpp']].sort_values('gdpp', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='gdpp', data= bottom10_gdpp, ax = axs[1,1])\nplt1.set(xlabel = '', ylabel= 'GDP per capita')\n\n# Per capita Income : Net income per person\n\nbottom10_income = data[['country','income']].sort_values('income', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='income', data= bottom10_income, ax = axs[1,2])\nplt1.set(xlabel = '', ylabel= 'Per capita Income')\n\n# Inflation: The measurement of the annual growth rate of the Total GDP\n\ntop10_inflation = data[['country','inflation']].sort_values('inflation', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='inflation', data= top10_inflation, ax = axs[2,0])\nplt1.set(xlabel = '', ylabel= 'Inflation')\n\n# Exports: Exports of goods and services. Given as %age of the Total GDP\n\nbottom10_exports = data[['country','exports']].sort_values('exports', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='exports', data= bottom10_exports, ax = axs[2,1])\nplt1.set(xlabel = '', ylabel= 'Exports')\n\n\n# Imports: Imports of goods and services. Given as %age of the Total GDP\n\nbottom10_imports = data[['country','imports']].sort_values('imports', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='imports', data= bottom10_imports, ax = axs[2,2])\nplt1.set(xlabel = '', ylabel= 'Imports')\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 90)\n    \nplt.tight_layout()\nplt.savefig('eda')\nplt.show()\n    \n\n","41741492":" # Let's check the correlation coefficients\nplt.figure(figsize = (16, 10))\nsns.heatmap(data.corr(), annot = True,)\nplt.savefig('corrplot')\nplt.show()\n","03f41b13":"# pairplot of data\nsns.pairplot(data)","a505471b":"##outlier analysis\nfig, axs = plt.subplots(3,3, figsize = (15,7.5))\nplt1 = sns.boxplot(data['child_mort'], ax = axs[0,0])\nplt2 = sns.boxplot(data['health'], ax = axs[0,1])\nplt3 = sns.boxplot(data['life_expec'], ax = axs[0,2])\nplt4 = sns.boxplot(data['total_fer'], ax = axs[1,0])\nplt5 = sns.boxplot(data['income'], ax = axs[1,1])\nplt6 = sns.boxplot(data['inflation'], ax = axs[1,2])\nplt7 = sns.boxplot(data['gdpp'], ax = axs[2,0])\nplt8 = sns.boxplot(data['imports'], ax = axs[2,1])\nplt9 = sns.boxplot(data['exports'], ax = axs[2,2])\n\n\nplt.tight_layout()","ff49ada3":"#import standardscaler()\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nvarlist = ['child_mort', 'exports', 'health', 'imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']\ndata[varlist] = scaler.fit_transform(data[varlist])","17ee5fc4":"from sklearn.decomposition import PCA\n\npca = PCA(svd_solver='randomized', random_state=42)\n\n","8e5316f2":"data_drop_1 = data.copy()\ndata_drop_0 = data.copy()\n\ncountry = data_drop_0.pop('country')\n\n# Putting feature variable to X\nX = data_drop_1.drop(['country'],axis=1)\n\n#putting response variable to y\ny =  data_drop_1['country']\n\n \n","aea77964":"country\n\n\n\n","21e2fa2f":"##applying pca on train data\n\npca.fit(X)","5ec7fb88":"#getting components\n\npca.components_","62a4c600":"colnames = list(X.columns)\npca_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npca_df.head()\n","0ec33c35":"fig = plt.figure(figsize = (8,8))\nplt.scatter(pca_df.PC1, pca_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pca_df.Feature):\n    plt.annotate(txt, (pca_df.PC1[i],pca_df.PC2[i]))\nplt.tight_layout()\nplt.show()\n","b5e7dae8":"pca.explained_variance_ratio_\n","3dc5ddaa":"#Plotting cumulative variance against number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.savefig('pca_no')\nplt.show()","85aff453":"# Building the dataframe using Incremental PCA for better efficiency.\n\nfrom sklearn.decomposition import IncrementalPCA\n\npca_final = IncrementalPCA(n_components=4)","7659ad31":"# Fitting the scaled df on incremental pca\n\ndf_inc_pca = pca_final.fit_transform(X)\ndf_inc_pca.shape\n","a9b375bd":"df_inc_pca = pd.DataFrame(df_inc_pca)\ndf_inc_pca.head()\n","7173d00b":"#creating correlation matrix for the principal components\n\ncorrmat = np.corrcoef(df_inc_pca.transpose())\n\n#pplotting correlation matrix\n\n%matplotlib inline\nplt.figure(figsize = (8,5))\nsns.heatmap(corrmat,annot = True,cmap=\"YlGnBu\")\n\n","b67c2d33":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n\n \n\n","74ee11fb":"hopkins(df_inc_pca)","e2eda517":"## K-means Clustering\nfrom sklearn.cluster import KMeans \n","1e292b17":"## Elbow curve method to find the ideal number of clusters.\nssd = []\nfor num_clusters in list(range(1,8)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50,random_state= 100)\n    model_clus.fit(df_inc_pca)\n    ssd.append(model_clus.inertia_)\n    \n\nplt.plot(ssd)\n\n","fbdfaa3b":"# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(df_inc_pca)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_inc_pca, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","f34b54e5":"cluster3 = KMeans(n_clusters=3, max_iter=50, random_state= 100)\ncluster3.fit(df_inc_pca)\n\n\n","d833316a":"cluster3.labels_\n","8f104ba6":"df_inc_pca['Cluster_Id3'] = cluster3.labels_\ndf_inc_pca.head()\n\n","cfe9d697":"\ndf_inc_pca_final = pd.concat([country,df_inc_pca], axis=1)\ndf_inc_pca_final\n\n\n","5ec9563b":"# Number of countries in each cluster\n\ndf_inc_pca_final['Cluster_Id3'].value_counts()\n\n","ab4cb0be":"#K-means with k=4 clusters\n\ncluster4 = KMeans(n_clusters=4, max_iter=50, random_state= 100)\ncluster4.fit(df_inc_pca)\n","fd793f04":"\ncluster4.labels_\n","2dfd43e5":"df_inc_pca['Cluster_Id4'] = cluster4.labels_\ndf_inc_pca.head()\n","071db785":"\n\ndf_inc_pca_final = pd.concat([country,df_inc_pca], axis=1)\ndf_inc_pca_final\n","e7773a7f":"# Number of countries in each cluster\n\ndf_inc_pca_final['Cluster_Id4'].value_counts()\n","4bf8bc9f":"# Lets drop the Cluster Id created with 4 clusters and proceed with 5 clusters.\n\ndf_inc_pca_final = df_inc_pca_final.drop('Cluster_Id4',axis=1)\n","31079f5d":"\ncluster5 = KMeans(n_clusters=5, max_iter=50,random_state=100)\ncluster5.fit(df_inc_pca)","ab909ba8":"# Cluster labels\n\ncluster5.labels_\n","ba33b587":"\ndf_inc_pca['Cluster_Id5'] = cluster5.labels_\ndf_inc_pca.head()","bf6f305c":"df_inc_pca_final = pd.concat([country,df_inc_pca], axis=1)\ndf_inc_pca_final","4408b130":"# Number of countries in each cluster\n\ndf_inc_pca_final['Cluster_Id5'].value_counts()","7b1db64b":"#K-means with k=6 clusters\n\ncluster6 = KMeans(n_clusters=6, max_iter=50, random_state= 100)\ncluster6.fit(df_inc_pca)\n","1f76068d":"cluster6.labels_\n","b8d5c173":"df_inc_pca['Cluster_Id6'] = cluster5.labels_\ndf_inc_pca.head()","14f503e3":"df_inc_pca_final = pd.concat([country,df_inc_pca], axis=1)\ndf_inc_pca_final","651a8d98":"# Number of countries in each cluster\n\ndf_inc_pca_final['Cluster_Id6'].value_counts()","522c9f0c":"df_inc_pca_final\n","8128b31f":"# Lets drop the Cluster Id created with 4 clusters and  with 5 clusters and with 6 clusters.\n\ndf_inc_pca_final= df_inc_pca_final.drop('Cluster_Id4',axis=1)\n                    \n\n\n\n","80b3672a":"df_inc_pca_final = df_inc_pca_final.drop('Cluster_Id5',axis=1)\n\n\n","fb0fd629":"df_inc_pca_final = df_inc_pca_final.drop('Cluster_Id6',axis=1)\n\n","6548b3b4":"df_inc_pca_final\n\n\n","bb04767d":"df_inc_pca = df_inc_pca.drop('Cluster_Id6',axis=1)\n","33e5d907":"df_inc_pca = df_inc_pca.drop('Cluster_Id5',axis=1)","1415e2d5":"df_inc_pca = df_inc_pca.drop('Cluster_Id4',axis=1)\n","17536515":"df_inc_pca","ab5c66d1":"df_inc_pca_h = df_inc_pca.drop('Cluster_Id3',axis = 1)\n","167b257c":"df_inc_pca_h\n","8f2397f3":"mergings = linkage(df_inc_pca, method = \"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()\n\n","24ffe541":"clusterCut = pd.Series(cut_tree(mergings, n_clusters = 5).reshape(-1,))\ndf_pca_hc = pd.concat([df_inc_pca_h, clusterCut], axis=1)\ndf_pca_hc.columns = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"ClusterID\"]\ndf_pca_hc.head()","f3096e92":"pca_cluster_hc = pd.concat([country,df_pca_hc], axis=1, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True)\npca_cluster_hc.head()","c05fd931":"clustered_data_hc = pca_cluster_hc[['country','ClusterID']].merge(data, on = 'country')\nclustered_data_hc.head()","4f8fc006":"hc_clusters_child_mort = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).child_mort.mean())\nhc_clusters_exports = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).exports.mean())\nhc_clusters_health = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).health.mean())\nhc_clusters_imports = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).imports.mean())\nhc_clusters_income = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).income.mean())\nhc_clusters_inflation = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).inflation.mean())\nhc_clusters_life_expec = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).life_expec.mean())\nhc_clusters_total_fer = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).total_fer.mean())\nhc_clusters_gdpp = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).gdpp.mean())","11907f47":"df = pd.concat([pd.Series(list(range(0,5))), hc_clusters_child_mort,hc_clusters_exports, hc_clusters_health, hc_clusters_imports,\n               hc_clusters_income, hc_clusters_inflation, hc_clusters_life_expec,hc_clusters_total_fer,hc_clusters_gdpp], axis=1)\ndf.columns = [\"ClusterID\", \"child_mort_mean\", \"exports_mean\", \"health_mean\", \"imports_mean\", \"income_mean\", \"inflation_mean\",\n               \"life_expec_mean\", \"total_fer_mean\", \"gdpp_mean\"]\ndf","4e97e2da":"fig, axs = plt.subplots(3,3,figsize = (15,15))\n\nsns.barplot(x=df.ClusterID, y=df.child_mort_mean, ax = axs[0,0])\nsns.barplot(x=df.ClusterID, y=df.exports_mean, ax = axs[0,1])\nsns.barplot(x=df.ClusterID, y=df.health_mean, ax = axs[0,2])\nsns.barplot(x=df.ClusterID, y=df.imports_mean, ax = axs[1,0])\nsns.barplot(x=df.ClusterID, y=df.income_mean, ax = axs[1,1])\nsns.barplot(x=df.ClusterID, y=df.life_expec_mean, ax = axs[1,2])\nsns.barplot(x=df.ClusterID, y=df.inflation_mean, ax = axs[2,0])\nsns.barplot(x=df.ClusterID, y=df.total_fer_mean, ax = axs[2,1])\nsns.barplot(x=df.ClusterID, y=df.gdpp_mean, ax = axs[2,2])\nplt.tight_layout()","60090238":"clustered_data_hc[clustered_data_hc.ClusterID == 0].country.values\n\n","e8d2076a":"clustered_data_hc[clustered_data_hc.ClusterID == 1].country.values","166d9cae":"clustered_data_hc[clustered_data_hc.ClusterID == 2].country.values","57788503":"\nclustered_data_hc[clustered_data_hc.ClusterID == 3].country.values","e9e02ee5":"clustered_data_hc[clustered_data_hc.ClusterID == 4].country.values","3b5058e9":"###data cleaning\n","3e8fec78":"##heirarchal clustering","8b008389":"#inferences:\n#1.child _mort and total_fer are better explained by PC2\n#2.inflation and health are well explained by neither PC1 nor PC2\n#3.incomwe,gdpp and life_expec are better explained by PC1\n#4.imports and exports are well explained by both PC1 and PC2","6c50f628":"##Inferences:\n#as we can see from the above plot the correlation \n#is almost zero which means we can go ahead","99408ef3":"##hopkins statastics\n#If value is between 0.01 and 0.3 data is regularly spaced\n#If value is 0.5 data is random\n#If value is between 0.7 and 0.99 data has high tendancy to cluster\n\n\n","eb46f230":"##infernces:\n#as we can see the starts going downhill from 3 \n#therefore ideal no of clustres are 3 to 4","7105c9dc":"##inferences :\n#there seems to be no good no of countries in each cluster ecxept 3\n\n","291c0291":"##Inference:\n#child_mortality and life_expentency are highly correlated with correlation of -0.89\n#child_mortality and total_fertility are highly correlated with correlation of 0.85\n#imports and exports are highly correlated with correlation of 0.74\n#life_expentency and total_fertility are highly correlated with correlation of -0.76","810fd629":"#**** We will have a look on the lowest 10 countries for each factor.","d7aae7ef":"##Infernces :\n#As we can can see cluster 0 is the most backward cluster ","9e58f623":"##PCA on the data","8921efca":"HELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities. It runs a lot of operational projects from time to time along with advocacy drives to raise awareness as well as for funding purposes.\n\n \n\nAfter the recent funding programmes, they have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. The significant issues that come while making this decision are mostly related to choosing the countries that are in the direst need of aid. \n\n \n\nAnd this is where you come in as a data analyst. Your job is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.  The datasets containing those socio-economic factors and the corresponding data dictionary are provided below.","53b92e92":"##performing sillohete analysys","1063f9d6":"\n##sclaing the data\n\n","f46846a8":"##Data loading\n","95087150":"##Inferences:\n#Looks like 90% to 95% of variance can be explained by 4 components\n","4452807e":"##inferences:\n#0.80 implies a tendancy to cluster","a1d6f7de":"#Problem Statement"}}