{"cell_type":{"4fa8fd5e":"code","ac0972a2":"code","373dc153":"code","463eaf9b":"code","c169988c":"code","722bc536":"code","fd9861b0":"code","5c9f9b9b":"code","62cdde35":"code","fa629ad3":"code","b953f321":"code","693282ef":"code","cade778f":"code","b0e2a76d":"code","bc61cf53":"code","456aa130":"code","0df12b19":"code","41fb325a":"code","f3a2c25c":"code","768cf4fc":"code","92692ced":"markdown","3d4a534a":"markdown","9ee30a45":"markdown","6467da80":"markdown","ab0d3494":"markdown","896c9220":"markdown","0c5f50b7":"markdown","fd238b02":"markdown","78557e91":"markdown","f6e01761":"markdown","4ca9bd5e":"markdown","63b2c5fb":"markdown","9ac2b59d":"markdown","1150e8dc":"markdown","696c99a3":"markdown","7b13ca50":"markdown","8cc95d68":"markdown"},"source":{"4fa8fd5e":"# Importing the Required Library\nimport pandas as  pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Activation,Conv2D,MaxPooling2D,Flatten, BatchNormalization, MaxPool2D\nfrom tensorflow.keras.metrics import AUC\nfrom kerastuner.tuners import RandomSearch\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","ac0972a2":"train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\n# taking the independent feature and the dependent feature in xtrain and ytrain of the traning data\nx_train = train.drop(columns = 'label')\ny_train = train['label']\n\nx_test = test.drop(columns = 'label')\ny_test = test['label']\nclasses = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress',  'Coat',\n           'Sandal',      'Shirt',   'Sneaker',  'Bag',    'Ankle Boot'] ","373dc153":"x_train = x_train.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)\/255.0\nx_test =  x_test.values.reshape(-1, int(np.sqrt(784)), int(np.sqrt(784)), 1)\/255.0\ncounter = 0","463eaf9b":"rows = 5\ncols = 15\nfig = plt.figure(figsize=(15,7))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(x_train[counter + i-1]), cmap='gray')\n#     plt.title(classes[y_train[counter + i-1]], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot\ncounter += rows*cols","c169988c":"y_train = to_categorical(y_train, num_classes=10)\ny_test = to_categorical(y_test, num_classes=10)","722bc536":"# x_train_trim ,x_valid , y_train_trim, y_valid = train_test_split(x_train, y_train, test_size= 0.1 ,random_state = 1455)\n\n\n\nx_train_trim = x_train\ny_train_trim = y_train\nx_valid = x_test\ny_valid = y_test\n\nprint(f'Training Set size: {x_train_trim.shape[0]}')\nprint(f'Validation Set size: {x_valid.shape[0]}')","fd9861b0":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","5c9f9b9b":"def build_lrfn(lr_start=1e-4, lr_max=1e-3, \n               lr_min=0, lr_rampup_epochs=16, \n               lr_sustain_epochs=1, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=False)\n\n\n\n#Usually monitor='val_accuracy' should be tracked here. Since the training set is smaller let keep it limited to accuracy\ncheckpoint = ModelCheckpoint(\n    filepath='best_weights.hdf5',\n    save_weights_only=True,\n    monitor='accuracy',\n    mode='max',\n    save_best_only=True)\n\noptimizer_rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","62cdde35":"# With data augmentation to prevent overfitting (accuracy 0.99286)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.0, # Randomly zoom image \n        width_shift_range=0.0,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.0,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train_trim)\ndatagen.fit(x_valid)","fa629ad3":"model.compile(optimizer=optimizer_rmsprop, \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy'])\n\nbatch_size = 256\nepochs = 60\ntrain_history = model.fit_generator(datagen.flow(x_train_trim, y_train_trim, batch_size=batch_size),\n                                    epochs = epochs, validation_data = datagen.flow(x_valid, y_valid, batch_size=batch_size),\n                                    steps_per_epoch=x_train_trim.shape[0] \/\/ batch_size, \n                                    callbacks=[lr_schedule, checkpoint], \n                                    verbose = 0)","b953f321":"def visualize_training(history, lw = 3):\n    plt.figure(figsize=(10,10))\n    plt.subplot(2,1,1)\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.legend(fontsize = 'x-large')\n    \n\n    plt.subplot(2,1,2)\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Loss Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)\n    plt.title('Learning Rate')\n    plt.xlabel('Epochs')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.show()","693282ef":"visualize_training(train_history) ","cade778f":"model.load_weights('.\/best_weights.hdf5')\npredictions_probs  = model.predict(x_test)\npredictions = np.argmax(predictions_probs, axis = 1)\ncounter = 0","b0e2a76d":"rows = 5\ncols = 15\nfig = plt.figure(figsize=(15,7))\nfor i in range(1, rows*cols+1):\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(np.squeeze(x_test[counter + i-1]), cmap='gray')\n    plt.title(predictions[counter + i-1], fontsize=16)\n    plt.axis(False)\n    fig.add_subplot\ncounter += rows*cols","bc61cf53":"ImageId = np.arange(1,x_test.shape[0] + 1)\noutput = pd.DataFrame({'ImageId':ImageId, 'Label':predictions})\noutput.to_csv('submission.csv', index=False)\noutput['true'] = test['label']","456aa130":"y_true = output['true']\ny_pred = output['Label']","0df12b19":"accuracy_list = []\nfor class_index in range(len(classes)):\n    preds = (output[output['true'] == class_index]['Label'].values == class_index)\n    acc = sum(preds) \/ len(preds)\n    print(f'Accuracy of {classes[class_index]} is : {acc}')\n    accuracy_list.append(acc)\n    \nimport plotly.express as px\ndata_canada = px.data.gapminder().query(\"country == 'Canada'\")\nfig = px.bar(x=classes, y= accuracy_list, color = accuracy_list, \n             color_continuous_scale=px.colors.sequential.Viridis_r)\n\nfig.update_layout(title ='Accuracy of Individual Classes', xaxis_title = 'Classes', yaxis_title = 'Accuracy', title_x = 0.5)\n\nfig.show()\n    ","41fb325a":"from sklearn.metrics import accuracy_score, balanced_accuracy_score\nprint('Accuracy Score: ', accuracy_score(y_true, y_pred))\nprint('Balanced Accuracy Score: ', balanced_accuracy_score(y_true, y_pred))","f3a2c25c":"import seaborn as sns\n\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","768cf4fc":"from sklearn.metrics import confusion_matrix\n\ncnf_matrix = confusion_matrix(y_true, y_pred)\nmake_confusion_matrix(cnf_matrix, categories = classes, figsize=(15,10), cbar=False)","92692ced":"## With this model, I was able to get upto **0.99042** on the leaderboard\n```Python\nmodel  = Sequential([\n    Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = x_train_trim.shape[-3:]),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    Dropout(0.2),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n    BatchNormalization(),\n    MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n    Flatten(),\n    Dense(units = 512 , activation = 'relu'),\n    Dropout(0.3),\n    Dense(10, activation = 'softmax')])\n\n\n\nmodel.summary()\n\nmodel.compile(optimizer='adam', \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","3d4a534a":"The orginal dataset has the labels numbered all the way from 0 to 9. However, this is not convinent for the CNN target output. Therefore, for the ease of classification model,  let's convert them into one hot vectors for each labels. ","9ee30a45":"## 2. Loading Data","6467da80":"## 1. Loading Libraries\nLets load libraries first. Then read the csv files. ","ab0d3494":"## 4.4 Observe Training History\nNow the training is done, let's have a look at the training history. It is very crucial to determine whether the model is overfitting or not. Have a look [here](https:\/\/machinelearningmastery.com\/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error\/) to check if the model is overfitting. ","896c9220":"## 3. Splitting Up Train and Validation Set\nOriginal dataset doesn't have the train validation set separate. Therefore, in order to fine tune the model, we separate **10%** of the test data as hold out cross validation set. ","0c5f50b7":"## 4.3 Compilation and Training","fd238b02":"## Confusion Matrix:\nNow let us have a look at the confusion matrix to see the mis-classified objects. ","78557e91":"Thank you. ","f6e01761":"## Performance Evaluation on Test Set","4ca9bd5e":"## 4. Model Building & Compilation\n### 4.1 Creating Model\n","63b2c5fb":"### 4.2 Creating Callbacks\nThe learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process. Therefore let's take a learning rate scheduler from built in library then incorporate a scheduler to vary learning rate over time. For an in depth understanding of the effects of learning rate scheduler, you can have a look [here](https:\/\/machinelearningmastery.com\/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks\/)  \n\nAnd another thing is the saving the model after training. However, during the training process the accuarcy of the model can go down. But if we are able to save the best possible state if the network over time, then it is possible to get the maximum out of the model.  ","9ac2b59d":"## 5. Inference and Submission Generation\nLets generate the predictions from the test set and have a look at the performance. ","1150e8dc":"## Used it upto version 7\n```Python\nmodel.compile(optimizer=optimizer_rmsprop, \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","696c99a3":"## 2. Reshaping & Normalization  \nLet's separate the data and labels. It turns out that training data is of shape **(42000, 784)**. So the test set has 42000 inidividual images which are square image but unwinded to arrays of length 784. Lets make it into the image size of **28x28** so that we can plot and see. The pixel values are in th range of **[0, 255]**. Let's normalize them in the range **[0, 1]** for the convinence of CNN layers. \n","7b13ca50":"## With this model, I was able to get upto **0.99042** on the leaderboard\n```Python\nmodel = Sequential([\n          Conv2D(28, kernel_size = 3, padding='same', input_shape = x_train_trim.shape[-3:], activation ='relu'),\n          MaxPool2D(pool_size = (2,2)),\n          Conv2D(28, kernel_size=(2,2), padding='valid', activation='relu'),\n          MaxPool2D(pool_size =(2,2)),\n          Conv2D(28, kernel_size=(2,2), padding='valid', activation='relu'),\n          Dropout(0.2),\n          Flatten(),\n          Dense(512, activation='relu'),\n          Dropout(0.5),\n          Dense(256, activation='relu'),\n          Dense(10, activation = 'softmax')\n  ])\nmodel.summary()\n\nmodel.compile(optimizer='adam', \n              loss = 'categorical_crossentropy',  \n              metrics = ['accuracy', AUC()])\n\ntrain_history =  model.fit(x_train_trim, y_train_trim, \n                           epochs = 24, \n                           callbacks = [lr_schedule, checkpoint],\n                           validation_data = (x_valid, y_valid), \n                           batch_size = 1024, \n                           verbose = 2 )\n```","8cc95d68":"![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/03\/Fashion-MNIST.png)"}}