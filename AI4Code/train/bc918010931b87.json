{"cell_type":{"1f63e664":"code","04db3e14":"code","37f66998":"code","9f62922d":"code","214465a6":"code","71182947":"code","8cc88658":"code","aac129f6":"code","bc83561c":"code","4cfc8529":"code","31bd2d8d":"code","9a61ae4d":"code","1579799b":"code","dcf90c0b":"code","05c1a252":"code","37522c01":"markdown","a9f1d84a":"markdown","1439009b":"markdown","6fae3c47":"markdown","cfe18d54":"markdown","d7addfb2":"markdown","395d3b06":"markdown"},"source":{"1f63e664":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#import bq_helper\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n\n\n#hacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\",dataset_name = \"boneage-training-dataset\")\n#hacker_news.list_tables()\n\n# Any results you write to the current directory are saved as output.\n#dataset = pd.read_csv(\"..\/input\/boneage-training-dataset.csv\")\n#dataset = pd.read_csv(\"..\/input\/boneage-training-dataset.zip\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport os, sys\nfrom IPython.display import display\nfrom IPython.display import Image as _Imgdis\nfrom PIL import Image\nimport numpy as np\nfrom time import time\nfrom time import sleep\n\nfolder = \"..\/input\/boneage-training-dataset\/boneage-training-dataset\"\nfolder_test = \"..\/input\/boneage-test-dataset\/boneage-test-dataset\"\n#folder = \"..\/input\/regression_sample\"\n\nonlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\nonlyfiles_test = [f for f in os.listdir(folder_test) if os.path.isfile(os.path.join(folder_test, f))]\n\nprint(\"Working with {0} images\".format(len(onlyfiles)))\nprint(\"Image examples: \")\n\nfor i in range(40, 42):\n    print(onlyfiles[i])\n    display(_Imgdis(filename=folder + \"\/\" + onlyfiles[i], width=240, height=320))\n    \n    \nfrom scipy import ndimage\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ntrain_files = []\ntest_files = []\ni=0\n\nfor _file in onlyfiles:\n    train_files.append(_file)\n    \nfor _file in onlyfiles_test:\n    test_files.append(_file)\n    #print(train_files)\n    #label_in_file = _file.find(\"_\")\n    #y_train.append(int(_file[0:label_in_file]))\nprint(\"Files in train_files: %d\" % len(train_files))\nprint(\"Files in test_files: %d\" % len(test_files))\n#train_files[0]\n#print(train_files[0:])\nimg_df = pd.DataFrame(data = train_files,  # This converts Array to Dataframe, with Index and Column names\n                  index=None,\n                  columns = None)\nimg_df_test = pd.DataFrame(data = test_files,  # This converts Array to Dataframe, with Index and Column names\n                  index=None,\n                  columns = None)\n\ncsv_df = pd.read_csv(\"..\/input\/boneage-training-dataset.csv\")\ncsv_df_test = pd.read_csv(\"..\/input\/boneage-test-dataset.csv\")\n\ndf_train = pd.concat([img_df,csv_df],axis = 1)  # Join two Dataframes, Finally\ndf_test = pd.concat([img_df_test,csv_df_test],axis = 1)\n\nimg_df = []\nimg_df_test = []\n#pd.join(img_df,csv_df)\n#img_df\n#csv_df\ndf_train = df_train.rename(index=str, columns={0: \"file\"}) #Change name of Column from 0 to FIle\ndf_test = df_test.rename(index=str, columns={0: \"file\"})\n\ndf_y = df_train[['boneage']].copy()\n\ndf_train = df_train.drop(columns = ['boneage'],axis = 1)  # Dropped Y values from columns\n\n#df_train\n#df_test\n\nimage_width = 320\nimage_height = 240\nratio = 4\n\nimage_width = int(image_width \/ ratio)\nimage_height = int(image_height \/ ratio)\n\nchannels = 3\nnb_classes = 1\n\ndataset = np.ndarray(shape=(len(df_train), channels, image_height, image_width),dtype=np.float32)\ndataset_test = np.ndarray(shape=(len(df_test), channels, image_height, image_width),dtype=np.float32)\n","04db3e14":"# dataset = dataset[0:500,:]\n# dataset_test = dataset_test\nprint(\"Dataset Train Image                \"+str(dataset.shape))\nprint(\"Dataset Test Image                 \"+str(dataset_test.shape))\nprint(\"Dataframe Train Values             \"+str(df_train.shape))\nprint(\"Dataframe Test Values              \"+str(df_test.shape))\nprint(\"Dataset Y values                   \"+str(df_y.shape))\n\ndataset = dataset[0:500,:]\ndf_train = df_train.iloc[0:500,:]\ndf_y = df_y.iloc[0:500,:]\n\n# dataset = dataset[0:5000,:]\n# df_train = df_train.iloc[0:5000,:]\n# df_y = df_y.iloc[0:5000,:]\n\nprint(\"Dataset Train Image                \"+str(dataset.shape))\nprint(\"Dataset Test Image                 \"+str(dataset_test.shape))\nprint(\"Dataframe Train Values             \"+str(df_train.shape))\nprint(\"Dataframe Test Values              \"+str(df_test.shape))\nprint(\"Dataset Y values                   \"+str(df_y.shape))\n\ndf_train.iloc[0:500,:].shape","37f66998":"dataset.shape[0]","9f62922d":"i = 0\n#print(folder + \"\/\" + df_train['file'])\nfor _file in df_train['file']:\n    #print(folder + \"\/\" + _file)\n    img = load_img(folder + \"\/\" + _file,grayscale=False,target_size=[60,80],interpolation='nearest')  # this is a PIL image\n    #img = load_img(folder + \"\/\" + _file)  # this is a PIL image\n    #print(img)\n    img.thumbnail((image_width, image_height))\n    # Convert to Numpy Array\n    x = img_to_array(img)  \n    x = x.reshape((3, 60, 80))\n    #Normalize\n    x = (x - 128.0) \/ 128.0\n    dataset[i] = x\n    i += 1\n    if i % 250 == 0:\n        print(\"%d images to array\" % i)\nprint(\"All TRAIN images to array!\")\n\nj = 0\n#print(folder + \"\/\" + df_train['file'])\nfor _file in df_test['file']:\n    #print(folder + \"\/\" + _file)\n    img = load_img(folder_test + \"\/\" + _file,grayscale=False,target_size=[60,80],interpolation='nearest')  # this is a PIL image\n    #img = load_img(folder + \"\/\" + _file)  # this is a PIL image\n    #print(img)\n    img.thumbnail((image_width, image_height))\n    # Convert to Numpy Array\n    x = img_to_array(img)  \n    x = x.reshape((3, 60, 80))\n    #Normalize\n    x = (x - 128.0) \/ 128.0\n    dataset_test[j] = x\n    j += 1\n    if j % 250 == 0:\n        print(\"%d images to array\" % j)\nprint(\"All TEST images to array!\")\n\ndf_train = []\ndf_test = []\n\n#dataset_test.shape\n\n# This will flatten the entire array of [3,120,160], and also reshape it with right dimention\n# https:\/\/stackoverflow.com\/questions\/36967920\/numpy-flatten-rgb-image-array\nimg_flat = dataset.flatten().reshape((dataset.shape[0],3*60*80))   # dataset.shape[0] gives total training size\nimg_flat_test = dataset_test.flatten().reshape((dataset_test.shape[0],3*60*80))\n\nimg_flat.shape","214465a6":"img_flat_test.shape[1]","71182947":"#Feature Standardization\n# mean_px = img_flat.mean().astype(np.float32)\n# std_px = img_flat_test.std().astype(np.float32)\n\n# def Standardization(x):\n#     return (x-mean_px)\/std_px","8cc88658":"# DONOT RUN THIS, OUR PROBLEM IS NOT CLASIFICATION PROBLEM\n# from keras.utils.np_utils import to_categorical\n# df_y = to_categorical(df_y)\n# num_classes = df_y.shape[1]\n# num_classes","aac129f6":"df_y = df_y.values","bc83561c":"from keras.models import Sequential\nfrom keras.layers.core import Lambda, Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D, MaxPooling2D\nimport tensorflow as tf\nimport tensorboard as tb","4cfc8529":"model = Sequential()\n#model.add(Lambda(Standardization, input_shape =(60,80,3)))\n#model.add(Flatten())\n\n#THE INPUT LAYER\nmodel.add(Dense(128,kernel_initializer='normal',input_dim=img_flat.shape[1],  activation='relu'))\n\n#THE HIDDEN LAYER\nmodel.add(Dense(256, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n#THE OUTPUT LAYER\nmodel.add(Dense(1,kernel_initializer='normal', activation='linear'))\n\n\n\nprint(\"Input shape \",model.input_shape)\nprint(\"Output shape \",model.output_shape)","31bd2d8d":"# THIS IS FOR CLASSIFICATION PROBLEM\n# from keras.optimizers import RMSprop\n# model.compile(optimizer=RMSprop(lr=0.01)),\n# loss = categorical_crossentropy,\n# metrics=['accuracy']\n\nmodel.compile(loss= 'mean_absolute_error',optimizer = 'adam', metrics = ['mean_absolute_error'])\nmodel.summary()","9a61ae4d":"#DEFINE THE CHECKPOINT\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import TensorBoard\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n# checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncheckpoint = TensorBoard(log_dir='..\/output\/logs', histogram_freq=0, batch_size=1000, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\ncallbacks_list = [checkpoint]","1579799b":"model.fit(img_flat,df_y,epochs= 10,batch_size = 32, validation_split= 0.2, callbacks= callbacks_list)","dcf90c0b":"# wights_file = 'Weights-006--34.30850.hdf5' # choose the best checkpoint \n# model.load_weights(wights_file) # load it\n# model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])","05c1a252":"predictions = model.predict(img_flat_test)","37522c01":"**TIP**\n\nTo load th weights again into model","a9f1d84a":"***Compile network***\n-------------------\n\nBefore making network ready for training we have to make sure to add below things:\n\n 1.  A loss function: to measure how good the network is\n    \n 2.  An optimizer: to update network as it sees more data and reduce loss\n    value\n    \n 3.  Metrics: to monitor performance of network","1439009b":"*Linear Model*\n--------------","6fae3c47":"**Feature Standardization**\n-------------------------------------\n\nIt is important preprocessing step.\nIt is used to centre the data around zero mean and unit variance.","cfe18d54":"**ls ..\/**\n\nCOMAND TO GET ALL DIRECTORIES IN KAGGLE","d7addfb2":"Lets create a simple model from Keras Sequential layer.\n\n1. Lambda layer performs simple arithmetic operations like sum, average, exponentiation etc.\n\n In 1st layer of the model we have to define input dimensions of our data in (rows,columns,colour channel) format.\n (In theano colour channel comes first)\n\n\n2. Flatten will transform input into 1D array.\n\n\n3. Dense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer.\n In the last layer we have to specify output dimensions\/classes of the model.\n Here it's 10, since we have to output 10 different digit labels.","395d3b06":"*One Hot encoding of labels.*\n-----------------------------\n\nA one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. \n\nFor example, 3 would be [0,0,0,1,0,0,0,0,0,0].\n"}}