{"cell_type":{"f1c1400b":"code","ee1ed1bd":"code","a8a06583":"code","b5dcd427":"code","29bf788b":"code","7a0ec17c":"code","2da05419":"code","1a99b099":"code","f11f89af":"code","40461570":"code","235fe50f":"code","140464ad":"code","5316c155":"code","70e410dd":"code","673acee9":"code","fa91d34c":"markdown","35565f41":"markdown","2377f2d5":"markdown","b96f48b4":"markdown","985a783c":"markdown","1f3f171e":"markdown","2546cb63":"markdown"},"source":{"f1c1400b":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl","ee1ed1bd":"!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","a8a06583":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# treelite\nimport treelite\nimport treelite_runtime \n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport pathlib\nfrom tqdm import tqdm\nfrom random import choices\n\nimport operator\nimport xgboost as xgb\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5dcd427":"# tf setup\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nMIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","29bf788b":"SEED = 2021\nSTART_DATE = 86\nFOLDS = 5\nNN_NAME = 'mlp' # 1dcnn, resnet, mlp","7a0ec17c":"%%time\n\n# train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = pd.read_feather('..\/input\/janestreet-save-as-feather\/train.feather')\ntrain = train.query(f'date >= {START_DATE}').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain.fillna(train.mean(),inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n\nf_mean = np.mean(train[features[1:]].values,axis=0)","2da05419":"predictor = treelite_runtime.Predictor('..\/input\/janestreet-faster-inference-by-xgb-with-treelite\/mymodel.so', verbose=True)","1a99b099":"def create_autoencoder(input_dim,output_dim,noise=0.05):\n    i = tf.keras.layers.Input(input_dim)\n    encoded = tf.keras.layers.BatchNormalization()(i)\n    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n    encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n    x = tf.keras.layers.Dense(32,activation='relu')(decoded)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n    autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n                        loss={'decoded':'mse','label_output':'binary_crossentropy'})\n    return autoencoder, encoder","f11f89af":"autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\nencoder.load_weights('..\/input\/janestreet-1dcnn-for-feature-extraction-train\/encoder.hdf5') \nencoder.trainable = False","40461570":"def create_1dcnn(input_dim, output_dim, encoder):\n    # input\n    inputs = tf.keras.layers.Input(input_dim)\n    \n    x = encoder(inputs)\n    x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features\n    \n    # normalize\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    # 1dcnn\n    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n    x = tf.keras.layers.Reshape((256, 16))(x)\n    x = tf.keras.layers.Conv1D(filters=16,\n                      kernel_size=7,\n                      strides=1,\n                      activation='relu')(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n    x = tf.keras.layers.Flatten()(x)\n    \n    # ffn\n    for i in range(2):\n        x = tf.keras.layers.Dense(256 \/\/ (2 ** i), activation='relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.GaussianNoise(0.01)(x)\n        x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs=inputs,outputs=x)\n    \n    # compile\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=1e-03)\n    opt = tfa.optimizers.SWA(opt)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-02)\n    model.compile(optimizer=opt, \n                  loss=loss, \n                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model","235fe50f":"def create_resnet(n_features, n_labels, encoder, label_smoothing = 0.0005):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,))\n    input_2 = encoder(input_1)\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, activation=\"elu\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(512, \"elu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='l2_norm'),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n    model = tf.keras.models.Model(inputs = [input_1, ], outputs = output)\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=1e-03)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer=opt, \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model","140464ad":"%%time\n\nif NN_NAME == '1dcnn':\n    models = []\n\n    for fold in range(FOLDS):\n        # 1dcnn\n        tf.keras.backend.clear_session()\n        model = create_1dcnn(X.shape[-1], y.shape[-1], encoder)\n        model.load_weights(pathlib.Path(f'..\/input\/janestreet-1dcnn-for-feature-extraction-train\/model_{SEED}_{fold}.hdf5'))\n        models.append(model)\n        \n    models = [models[-1]]","5316c155":"%%time\n\nif NN_NAME == 'resnet':\n    models = []\n\n    for fold in range(FOLDS):\n        tf.keras.backend.clear_session()\n        model = create_resnet(X.shape[-1], y.shape[-1], encoder)\n        model.load_weights(pathlib.Path(f'..\/input\/janestreet-resnet-with-autoencoder-train\/model_{SEED}_{fold}.hdf5'))\n        models.append(model)\n        \n    models = [models[-1]]","70e410dd":"%%time\n\nif NN_NAME == 'mlp':\n    model = tf.keras.models.load_model('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n    models = [model]","673acee9":"f = np.median\nth = 0.500\n\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        \n        # GBDT inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n        xgb_pred = predictor.predict(batch)\n    \n        # NN inference\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        \n        # ensemble\n        pred_df.action = np.where(0.9*pred + 0.1*xgb_pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","fa91d34c":"# Loading the training data","35565f41":"# Treelite","2377f2d5":"# Libraries","b96f48b4":"## Submission","985a783c":"# Load NN","1f3f171e":"# Load XGB with Treelite","2546cb63":"Thought ensembling is not possible in this competiton due to the limited inference time?\n\nYes, it is! (But at most 2x shallow NN + 1x GBDT, in my experiment)\n\nHere I demonstrate it by simply ensembling the following public notebooks of mine:\n\nGBDT:\n\n- [[JaneStreet] Faster Inference by XGB with Treelite](https:\/\/www.kaggle.com\/code1110\/janestreet-faster-inference-by-xgb-with-treelite)\n\nNN (one of the following):\n\n- [[janestreet] 1DCNN for Feature Extraction (infer)](https:\/\/www.kaggle.com\/code1110\/janestreet-1dcnn-for-feature-extraction-infer)\n- [[janestreet] ResNet with AutoEncoder (infer)](https:\/\/www.kaggle.com\/code1110\/janestreet-resnet-with-autoencoder-infer)\n- [Jane Street with Keras NN overfit](https:\/\/www.kaggle.com\/code1110\/jane-street-with-keras-nn-overfit)\n\n\nThe key is to use [Treelite](https:\/\/treelite.readthedocs.io\/en\/latest\/) for a GBDT model to accelerate the inference speed."}}