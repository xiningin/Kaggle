{"cell_type":{"71d3502a":"code","e6b49b8f":"code","f927e892":"code","21e8799c":"code","e32c3c1a":"code","00f34fdb":"code","d244fdb9":"code","191accae":"code","db94071a":"code","93690adc":"code","654a2ee2":"code","2df9bcd4":"code","0cbd9fb1":"code","676e4666":"code","561bc2bb":"code","2fabc999":"code","c0c9a37f":"code","94aad5eb":"code","397cb804":"code","3ed182f6":"code","df3ea985":"code","2568c699":"code","f912fafa":"code","1c8641a8":"code","711f9256":"code","b541e6e2":"code","d040c44f":"code","80970c49":"code","ff32500c":"code","ae088542":"code","fc5b3b22":"code","9bd4a7ca":"code","db22888a":"code","c54bebcc":"code","fed4e715":"code","acf19637":"code","87351eb3":"code","4084425b":"code","5a786db0":"code","52853704":"code","97b7fe01":"code","b3cc9005":"code","8bbabb41":"code","8d562c36":"code","2add70d5":"markdown","2acf1425":"markdown","b91f4ffc":"markdown","d117edb5":"markdown","82c58548":"markdown"},"source":{"71d3502a":"import os\nfrom datetime import datetime \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.base import BaseEstimator, TransformerMixin","e6b49b8f":"control_1 = pd.read_csv('\/kaggle\/input\/the-depression-dataset\/data\/control\/control_1.csv')\ncondition_1 = pd.read_csv('\/kaggle\/input\/the-depression-dataset\/data\/condition\/condition_1.csv')\nprint(f'control_1.shape: {control_1.shape}')\nprint(f'condition_1.shape: {condition_1.shape}')","f927e892":"control_1.head()","21e8799c":"condition_1.head()","e32c3c1a":"condition_1.describe()","00f34fdb":"sns.histplot(x='activity', data=condition_1)","d244fdb9":"condition_1['activity'].skew()","191accae":"condition_1['log_activity'] = np.log(condition_1['activity'] + 1) # add + 1 because log(0) is infinity\ncondition_1['log_activity'].skew()","db94071a":"sns.displot(x='log_activity', data=condition_1, kind='kde', fill=True)","93690adc":"condition_1['sqrt_activity'] = np.sqrt(condition_1['activity'])\ncondition_1['sqrt_activity'].skew()","654a2ee2":"sns.displot(x='sqrt_activity', data=condition_1, kind='kde', fill=True)","2df9bcd4":"condition_1.describe()","0cbd9fb1":"control_1['activity'].skew()","676e4666":"control_1.describe()","561bc2bb":"control_1['log_activity'] = np.log(control_1['activity'] + 1)\ncontrol_1['log_activity'].skew()","2fabc999":"sns.displot(x='log_activity', data=control_1, kind='kde', fill=True)","c0c9a37f":"condition_1.head()","94aad5eb":"df = condition_1.groupby('date')['log_activity'].mean().reset_index()\ndf.head()","397cb804":"def combine_data(path):\n    dirs = os.listdir(path)\n    combine_df = []\n    \n    for filepath in dirs:\n        source = filepath.split('.')[0]\n        if filepath.endswith('.csv'):\n            X = pd.read_csv(path + filepath, parse_dates=['timestamp'], index_col='timestamp')\n            X['source'] = source\n            combine_df.append(X)\n        \n    return combine_df","3ed182f6":"combine_df = combine_data('\/kaggle\/input\/the-depression-dataset\/data\/condition\/')","df3ea985":"conditions = []\nfor condition in combine_df:\n    condition_df = pd.DataFrame(columns=['mean_activity', 'std_activity', 'zero_activity_proportion', 'source'])\n    condition_df['mean_activity'] = condition.activity.resample('H').mean()\n    condition_df['std_activity'] = condition.activity.resample('H').std()\n    condition_df['zero_activity_proportion'] = [data[1].tolist().count(0) for data in condition.activity.resample('H')]\n    condition_df['source'] = condition.source\n    conditions.append(condition_df)","2568c699":"combine_df = combine_data('\/kaggle\/input\/the-depression-dataset\/data\/control\/')","f912fafa":"controls = []\nfor control in combine_df:\n    control_df = pd.DataFrame(columns=['mean_activity', 'std_activity', 'zero_activity_proportion', 'source'])\n    control_df['mean_activity'] = control.activity.resample('H').mean()\n    control_df['std_activity'] = control.activity.resample('H').std()\n    control_df['zero_activity_proportion'] = [data[1].tolist().count(0) for data in control.activity.resample('H')]\n    control_df['source'] = control.source\n    controls.append(control_df)","1c8641a8":"fig, axes = plt.subplots(23, 1, figsize=(23, 30))\ncnt = 0\nfor i in range(23):\n    condition = conditions[cnt]\n    axes[i].plot(condition.index, condition.mean_activity, color='r')\n    axes[i].set_title(f'Mean activity for {condition.source[1]}', fontsize=18)\n    cnt += 1\n    \nplt.xlabel('Date', fontsize=14)\nfig.tight_layout(pad=1.0)\nfig.savefig('Mean activity of condition group.jpg', dpi=100)\nplt.show()","711f9256":"fig, axes = plt.subplots(32, 1, figsize=(23, 40))\ncnt = 0\nfor i in range(32):\n    control = controls[cnt]\n    axes[i].plot(control.index, control.mean_activity, color='g')\n    axes[i].set_title(f'Mean activity for {control.source[1]}', fontsize=18)\n    cnt += 1\n    \nplt.xlabel('Date', fontsize=14)\nfig.tight_layout(pad=1.0)\nfig.savefig('Mean activity of control group.jpg', dpi=100)\nplt.show()","b541e6e2":"def to_clock(x):\n    d = datetime.strptime(f'{x}:00', '%H:%M')\n    return d.strftime('%I:%M %p')","d040c44f":"# Draw Plot\nfig, axes = plt.subplots(23, 1, figsize=(23, 40))\n\ncnt = 0\nfor i in range(23):\n    df = conditions[i].reset_index()\n\n    # Prepare data\n    df['hour'] = [d.hour for d in df.timestamp]\n    df = df.sort_values('hour')\n    df['clock_hour'] = df['hour'].apply(lambda x: to_clock(x))\n    sns.boxplot(x='clock_hour', y='mean_activity', data=df, ax=axes[i])\n    axes[i].set_title(f'Box Plot of mean activity for {df.source[1]}', fontsize=18)\n    cnt += 1\n\nplt.xlabel('Date', fontsize=14)\nfig.tight_layout(pad=1.0)\nplt.show()","80970c49":"# Draw Plot\nfig, axes = plt.subplots(32, 1, figsize=(23, 50))\n\ncnt = 0\nfor i in range(32):\n    df = controls[i].reset_index()\n\n    # Prepare data\n    df['hour'] = [d.hour for d in df.timestamp]\n    df = df.sort_values('hour')\n    df['clock_hour'] = df['hour'].apply(lambda x: to_clock(x))\n    sns.boxplot(x='clock_hour', y='mean_activity', data=df, ax=axes[i])\n    axes[i].set_title(f'Box Plot of mean activity for {df.source[1]}', fontsize=18)\n    cnt += 1\n\nplt.xlabel('Date', fontsize=14)\nfig.tight_layout(pad=1.0)\nplt.show()","ff32500c":"fig, axes = plt.subplots(2, 1, figsize=(24, 10))\ndf = conditions[12].reset_index()\ndf['hour'] = [d.hour for d in df.timestamp]\ndf = df.sort_values('hour')\ndf['clock_hour'] = df['hour'].apply(lambda x: to_clock(x))\nsns.boxplot(x='clock_hour', y='zero_activity_proportion', data=df, ax=axes[0])\naxes[0].set_title('Zero Activity Count of a Depressed Patient', fontsize=18)\n\ndf = controls[2].reset_index()\ndf['hour'] = [d.hour for d in df.timestamp]\ndf = df.sort_values('hour')\ndf['clock_hour'] = df['hour'].apply(lambda x: to_clock(x))\nsns.boxplot(x='clock_hour', y='zero_activity_proportion', data=df, ax=axes[1])\naxes[1].set_title('Zero Activity Count of a Non-Depressed Patient', fontsize=18)\n\nfig.tight_layout(pad=1.0)\nplt.show()","ae088542":"## Clustering Analysis","fc5b3b22":"def nextday(dates):\n    for date in dates:\n        yield date","9bd4a7ca":"def zero_count(series):\n    return list(series).count(0)","db22888a":"def extractfeatures(X, date):\n    mask = X['date'] == date\n    d = {\n        'mean_log_activity': X[mask]['log_activity'].mean(),\n        'std_log_activity': X[mask]['log_activity'].std(),\n        'min_log_activity': X[mask]['log_activity'].min(),\n        'max_log_activity': X[mask]['log_activity'].max(),\n        'zero_proportion_activity': zero_count(X[mask]['log_activity'])\n    }\n    return d","c54bebcc":"class ExtractData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, path):\n        self.path = path\n        self.X = []\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        dirs = os.listdir(self.path)\n        \n        for filepath in sorted(dirs, key=lambda x: x.split('_')[0]):\n            condition = filepath.split('.')[0]\n            if filepath.endswith('.csv'):\n                X = pd.read_csv(self.path + filepath)\n                X['log_activity'] = np.log(X['activity'] + 1)\n                dates = X.date.unique()\n                \n                for date in nextday(dates):\n                    d = extractfeatures(X, date)\n                    d['source'] = condition\n                    self.X.append(d)\n                \n\n        return pd.DataFrame(self.X)","fed4e715":"e = ExtractData(path='\/kaggle\/input\/the-depression-dataset\/data\/condition\/')\nconditions = e.fit_transform(X=None, y=None)\nconditions['state'] = 1","acf19637":"conditions.tail()","87351eb3":"e = ExtractData(path='\/kaggle\/input\/the-depression-dataset\/data\/control\/')\ncontrols = e.fit_transform(X=None, y=None)\ncontrols['state'] = 0","4084425b":"full_df = controls.append(conditions, ignore_index=True)\nfull_df.head()","5a786db0":"full_df.shape","52853704":"full_df = full_df.sample(frac=1) # reshufle the dataset","97b7fe01":"def custom_train_test_split(train_set, test_set):\n    X_train = train_set.drop('label', axis=1)\n    y_train = train_set.label\n    X_test = test_set.drop('label', axis=1)\n    y_test = test_set.label\n    \n    return X_train, X_test, y_train, y_test","b3cc9005":"class CustomClassifierCV(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, base_clf):\n        self.base_clf = base_clf\n    \n    def fit(self, X, y=None):\n        X['label'] = y\n        participants = X.source.unique()\n        folds = []\n        \n        predictions = [] # predicted labels\n        actuals = [] # actual labels\n            \n        for p in participants:\n            folds.append(X[X['source'] == p])\n        \n        for i in range(len(folds)):   \n            test_set = folds[i]\n            train_fold = [elem for idx , elem in enumerate(folds) if idx != i]\n            \n            train_set = pd.concat(train_fold)\n            X_train, X_test, y_train, y_test = custom_train_test_split(train_set.drop(['source'], axis=1),\n                                    test_set.drop(['source'], axis=1))\n            \n            self.base_clf.fit(X_train, y_train)\n            predictions.append(self.predict(X_test))\n            actuals.append(test_set.label.iloc[0])\n            \n        self.score(predictions, actuals)\n        \n    def predict(self, X):\n        predictions = self.base_clf.predict(X)\n        ones = predictions.tolist().count(1)\n        zeroes = predictions.tolist().count(0)\n        \n        return 1 if ones > zeroes else 0\n    \n    def score(self, predictions, actuals):\n        print(classification_report(predictions, actuals))","8bbabb41":"X = full_df.drop(['state'], axis=1)\ny = full_df.state","8d562c36":"forest = RandomForestClassifier(n_estimators=100)\ncustom_clfCV = CustomClassifierCV(forest)\ncustom_clfCV.fit(X, y)","2add70d5":"## Modeling","2acf1425":"## Time Series Analysis","b91f4ffc":"Depression alone affects more than 300 million people worldwide and is one of the largest cause of disability worldwide, particularly for women.\n\nIt is characterized by sadness, loss of weight, loss of interest, suicide attempts etc.\n\nBody sensors have long being used to monitor personal health. This sensors store vast amount of data which holds the potential of measuring the quantity of daily steps, calories burned, continuous recordings of heart rate and activity level.\n\nDealing with depression can be very demanding and creates a lot of physical, economical and emotional problems. \n\nDepression is characterized by reduced day time activity and increased night time activity\n\nThe dataset collected used in this analysis is public available. The dataset was originally collected for the study of motor activity in schizophrenia and major depression. Motor activity was monitored with an actigraph watch worn at the right wrist (Actiwatch, Cambridge Neurotechnology Ltd, England, model AW4). The actigraph watch measures activity levels. The sampling frequency is 32Hz and movements over 0.05 g are recorded. A corresponding voltage is produced and is stored as an activity count in the memory unit of the actigraph watch.\n\nThe number of counts is proportional to the intensity of the movement. Total activity counts were continuously recorded in one minute intervals. This dataset consists of actigraphy data collected from 23 unipolar and bipolar depressed patients (condition group), 5 subjects were hospitalized during their data collection period,\nand 18 were outpatients. The severity level of the ongoing depression was rated by a clinician on the Montgomery- Asberg Depression Rating Scale (MADRS) at the beginning and conclusion of the motor-activity recordings. \n\nIn addition, the dataset contains actigraphy data from 32 non-depressed contributors (control group), consisting of 23 hospital employees, 5 students and 4 former patients without current psychiatric symptoms.","d117edb5":"# Depression Analysis","82c58548":"From the above graph, it can be seen that there is high level of skewness, An alternative is taking the log or square root of activity "}}