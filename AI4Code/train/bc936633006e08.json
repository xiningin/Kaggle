{"cell_type":{"07173071":"code","a8e3741b":"code","178bfd2a":"code","eda83ad9":"code","6e0f6373":"code","f84fec3c":"code","6c54643f":"code","5f2ddab9":"code","970c9f44":"code","0ec4b47a":"markdown","8f2b7813":"markdown","2cc5ea19":"markdown","8886a2e4":"markdown","a745d7b6":"markdown","89e16ad7":"markdown","fced91c5":"markdown","213aaab8":"markdown"},"source":{"07173071":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8e3741b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","178bfd2a":"dataset=pd.read_csv('\/kaggle\/input\/restaurant-reviews\/Restaurant_Reviews.tsv',quoting=3,delimiter='\\t')\ndataset.head()","eda83ad9":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus=[]\nfor i in range(0,1000):\n    reviews=re.sub('[^a-zA-Z]',' ',dataset.iloc[i,0])\n    reviews=reviews.lower()\n    reviews=reviews.split()\n    ps=PorterStemmer()\n    all_stopword=stopwords.words('english')\n    all_stopword.remove('not')\n   \n    \n\n    reviews=[ps.stem(word) for word in reviews if not word in set(all_stopword)]\n    reviews=' '.join(reviews)\n    corpus.append(reviews)\nprint(corpus)","6e0f6373":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=1500 )\nX=cv.fit_transform(corpus).toarray()\ny=dataset.iloc[:,-1].values\nprint(len(X[0]))","f84fec3c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","6c54643f":"from sklearn.svm import SVC\nclassifier=SVC(kernel='linear',random_state=0)\nclassifier.fit(X_train,y_train)","5f2ddab9":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","970c9f44":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","0ec4b47a":"# Creating Bags of words model","8f2b7813":"# Training the dataset on SVM","2cc5ea19":"# Predicting the Test set results","8886a2e4":"# spliting the dataset into train and testset","a745d7b6":"# Creating the confusion matrix","89e16ad7":"# Importing the dataset\n","fced91c5":"# Cleaning the dataset","213aaab8":"# Importing the dataset"}}