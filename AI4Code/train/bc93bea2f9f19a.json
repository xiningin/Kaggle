{"cell_type":{"64409265":"code","4daddb27":"code","c871acdb":"code","0367a087":"code","0b1b7fc4":"code","b950cea4":"code","1fc0cb4a":"code","f2d3a348":"code","3f230272":"code","d485f1d3":"code","cd8320a3":"code","32f3c38c":"code","723d6df1":"code","90ceb4d0":"code","b1747be3":"code","b8324d55":"code","2360ccd0":"code","1301ebf8":"code","c3406eb8":"code","e4369390":"code","edebeee9":"code","0669b3aa":"code","e1a73caa":"code","393911d9":"code","df964555":"code","7bcab361":"code","04a8a188":"code","96162328":"code","0d0850bd":"code","bcf568a0":"code","1ca7cf31":"code","8bb35edf":"code","74f97ed5":"code","333dee02":"code","d1c1e211":"code","1b9fcf2c":"code","538a406a":"code","940b81c4":"code","9fe095c1":"code","341ac55a":"code","b52efae7":"code","eed4b635":"code","1280c738":"code","c02becae":"markdown","8b375bfc":"markdown","ea381f10":"markdown"},"source":{"64409265":"#data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#machine learning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","4daddb27":"#access data and create pandas dataframes\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","c871acdb":"#see all of the types of data contained (print every column)\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(train_df.head())","0367a087":"#determine datatypes of the training data and see which have nulls\ntrain_df.info()","0b1b7fc4":"#determine datatypes of the testing data and see which have nulls\ntest_df.info()","b950cea4":"#see distribution statistics for numerical values\ntrain_df[['Age','Fare','SibSp','Parch']].describe()","1fc0cb4a":"#see distribution statistics for categorical values\ntrain_df.describe(include=['O'])","f2d3a348":"#analyze the features by pairing them together 'pivoting features'\n#Note: only do this with categorical, ordinal, or discrete variables\n\n#strong correlation between being Pclass: 3 and not surviving\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3f230272":"#strong correlation between being Sex: female and Survived\ntrain_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d485f1d3":"#SibSp 5 & 8 have zero correlation with Survived, while other groups do, indicating a\n#beneficial opportunity to utilize 'feature engineering' (deriving a new feature or set \n#of features from this one)\ntrain_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","cd8320a3":"#same scenario with the feature Parch, will also conduct feature engineering here\ntrain_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","32f3c38c":"#analyze the numerical features now (charts are useful for these)\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","723d6df1":"#can plot comparisons between continuous and categorical if the categorical is numeric\ng = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ng.map(plt.hist, 'Age', alpha=0.5, bins=20)\ng.add_legend()","90ceb4d0":"#plot the categorical features that indicated highest correlation, compare based upon where they embarked\ng = sns.FacetGrid(train_df, row='Embarked', height=2.2, aspect=1.6)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ng.add_legend()","b1747be3":"#plot the categorical features that indicated highest correlation, compare \n#based upon where they embarked and their fare price\ng = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=2.2, aspect=1.6)\ng.map(sns.barplot, 'Sex', 'Fare', alpha=0.5, ci=None)\ng.add_legend()","b8324d55":"#Wrangle the data\n\n#Correcting\n#start with correcting the data by dropping features that have no use\nprint('Before', train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint('After', train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","2360ccd0":"#Creating\n#create new feature by extracting from existing\n#pull title out of name feature\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","1301ebf8":"#wrangle the new Title feature, compress every row with low counts into 1 row\n#then compress variations into same row (Mlle == Miss, Ms == Miss, Mme == Mrs)\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","c3406eb8":"#compare the engineered feature Title with Survived to see correlation\ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","e4369390":"#convert the categorical titles to ordinal for future use in the ML model\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df[['Name', 'Title']].head()","edebeee9":"#now drop Name and PassengerId feature, since they are no \n#longer needed in the training set\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]","0669b3aa":"#convert the categorical feature Sex to numerical values \n#for future use in the ML model\nsex_mapping = {'male': 0, 'female': 1}\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n\ntrain_df[['Title', 'Sex']].head()","e1a73caa":"#Completing\n#now is time to complete a numerical continuous feature with \n#missing or null values\n#start with Age\n#guess ages by using the median values across Pclass and gender (Age \n#for Pclass=1 and Gender=0, Pclass=1 and Gender=1, ...)\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=0.5, bins=20)\ngrid.add_legend()","393911d9":"#start with an empty array to contain the guessed ages\nguess_ages = np.zeros((2,3))\n\n#now iterate over Sex (0,1) and Pclass(1,2,3) to calc the guessed ages\nfor dataset in combine:\n    for i in range(2):\n        for j in range(3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n            \n            age_guess = guess_df.median()\n            \n            #convert random age float to nearest .5 age\n            guess_ages[i,j] = int(age_guess\/0.5 + 0.5) * 0.5\n            \n\n    for i in range(2):\n        for j in range(3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1), 'Age'] = guess_ages[i,j]\n            \n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","df964555":"#create Age bands and determine correlations with Survived\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 7)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","7bcab361":"#replace Age with ordinal values based upon these bands\nfor dataset in combine:\n    dataset.loc[dataset['Age'] <= 11.429, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11.429) & (dataset['Age'] <= 22.857), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 22.857) & (dataset['Age'] <= 34.286), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 34.286) & (dataset['Age'] <= 45.714), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 45.714) & (dataset['Age'] <= 57.143), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 57.143) & (dataset['Age'] <= 68.571), 'Age'] = 5\n    dataset.loc[dataset['Age'] > 68.571, 'Age'] = 6\n    dataset['Age'] = dataset['Age'].astype(int)\n\n\ntrain_df[['Age', 'AgeBand']].head()","04a8a188":"#drop AgeBand, now that ages are placed in correct bands\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","96162328":"#Creating\n#create new feature called FamilySize which combines Parch and \n#SibSp, this will allow us to drop 2 columns and replace it with 1\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n#No correlation between FamilySize 8 & 11 and Survived, must engineer new feature\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","0d0850bd":"#Make a new feature called IsAlone to eliminate values of zero\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    #change value to 1 if family size is 1 person\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\n#Notice correlation between not being alone and surviving\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","bcf568a0":"#drop Parch, SibSp, and FamilySize and keep IsAlone\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","1ca7cf31":"#create artificial feature combining Pclass and Age\nfor dataset in combine:\n   dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df[['Age*Class', 'Survived']].groupby(['Age*Class'], as_index=False).mean()","8bb35edf":"#create artificial feature combining Title and Class\nfor dataset in combine:\n   dataset['Title*Class'] = dataset.Title * dataset.Pclass\n    \ntrain_df[['Title*Class', 'Survived']].groupby(['Title*Class'], as_index=False).mean()","74f97ed5":"#Completing\n#complete the embarked feature by find the mode, and \n#filling that value in all the null spots\nfreq_port = train_df.Embarked.dropna().mode()[0]\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n\n#Correlation between Embarked: C and Survived\ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","333dee02":"#map Embarked to numeric values for the ML model\nport_mapping = {'S': 0, 'C': 1, 'Q': 2}\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map(port_mapping)\n\ntrain_df.head()","d1c1e211":"#Completing\n#fill the missing fare values by finding the mode and \n#replacing the nulls with it\nfreq_fare = train_df.Fare.dropna().mode()[0]\n\nfor dataset in combine:\n    dataset['Fare'] = dataset['Fare'].fillna(freq_fare)","1b9fcf2c":"#create new feature called FareBand, just like AgeBand before\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","538a406a":"#convert Fare into ordinal values based upon results\nfor dataset in combine:\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31.0), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31.0, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n#remove FareBand feature\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","940b81c4":"#check test values too, make sure columns match up\ntest_df.head()","9fe095c1":"## Modeling\n#time to train a model and predict a solution\nX = train_df.drop('Survived', axis=1)\ny = train_df['Survived']\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.5, random_state=1)\nX_test = test_df.drop('PassengerId', axis=1).copy()","341ac55a":"#Model Selection\n#this is a linear problem, can we figure out a \n#function f(x) = y that predicts survivability?\n#spot check Linear ML algorithms to determine the \n#most accurate one\n#notice they are all close, but SVM is highest\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('PERC', Perceptron()))\nmodels.append(('LSVC', LinearSVC()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    score = model.score(X_train, Y_train)\n    print('Accuracy of {} on training set: {}'.format(name, score))\n\n    testScore = model.score(X_validation, Y_validation)\n    print('Accuracy of {} on testing set: {}'.format(name, testScore))","b52efae7":"#tune hyperparameters for SVM \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \nmodel = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \nmodel.fit(X_train, Y_train)\nhyperparams = model.best_params_\nprint(hyperparams)","eed4b635":"#best params: {'C': 100, 'gamma': 0.01, kernel': 'rbf'}\n#Original Train: 0.8674157303370786 \n#Tuned Train: 0.8674157303370786\n#Original Validation: 0.7959641255605381 \n#Tuned Validation: 0.7959641255605381\n#normally tuning hyperparameters will yield positive \n#results, but in this case it made no difference\ntuned_model = SVC(C=100, gamma=0.01, kernel='rbf')\ntuned_model.fit(X_train, Y_train)\ntrain_score = tuned_model.score(X_train, Y_train)\ntest_score = tuned_model.score(X_validation, Y_validation)\nprint(\"Train: {}\\nValidation: {}\".format(train_score, test_score))","1280c738":"#make predictions with model and save output data\npredictions = tuned_model.predict(X_test)\n\nsubmission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': predictions})\n\nprint('Printing Submission CSV')\n\nsubmission.to_csv('.\/submission.csv', index=False)","c02becae":"# Titanic: Top 5% on Leaderboard with SVM\n\n**By: Brian Rafferty**\n\nWelcome to my \"Titanic: Machine Learning from Disaster\" Jupyter Notebook! This notebook will highlight how I tackled the problem, and can act as a general blueprint to undertake most Data Science projects.","8b375bfc":"# Before moving forward, look at the data and answer these questions:\n\n**Which features are categorical?**\n     - Survived, Sex, and Embarked\n     - Pclass is ordinal\n**Which features are numerical?**\n     - Age and Fare are continuous\n     - SibSp and Parch are discrete\n**Which features are mixed data types?**\n     - Ticket and Cabin\n**Do any features possibly contain errors? If so, which ones?**\n     - Yes, Name has ambiguous data listed\n**Which features are missing values?**\n     - Cabin, Age, and Embarked","ea381f10":"# Final score with test.csv outputs 0.79665\n\n- Teams with this score start appearing at position 785. With around 17,000 total teams, that means this analysis and model results in the **top 5%** of all submissions! \n\n- Success aside, there is still much room to improve my skillset with feature engineering, which is the most likely candidate holding the model back from better predictions."}}