{"cell_type":{"cc751fbc":"code","eaf4ab9d":"code","75244dab":"code","dccd5fcd":"code","78657653":"code","373b8c9c":"code","02ebde2a":"code","f17bbc29":"code","ef9ac937":"code","873957e9":"code","42dba894":"code","996393a2":"code","55305faf":"code","5002bfd1":"code","a5bc807f":"code","735405e4":"code","961036f8":"code","1d1ec849":"code","49b22c2e":"markdown","736423f0":"markdown","3afbc146":"markdown","0b1a1cdc":"markdown","15e830f6":"markdown","cee7b1c1":"markdown"},"source":{"cc751fbc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport skimage.io\nimport os \nimport tqdm\nimport glob\nimport tensorflow \n\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage.color import grey2rgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.utils import to_categorical\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras.backend as K\n\n#import tensorflow_addons as tfa\n#from tensorflow.keras.metrics import Metric\n#from tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\nfrom typeguard import typechecked\nfrom typing import Optional","eaf4ab9d":"AUTOTUNE = tf.data.experimental.AUTOTUNE","75244dab":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split = 0.2,\n                                  \n        rotation_range=5,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        #zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                  validation_split = 0.2)\n\ntest_datagen  = ImageDataGenerator(rescale = 1.\/255\n                                  )","dccd5fcd":"train_dataset  = train_datagen.flow_from_directory(directory = '..\/input\/chest-ctscan-images\/Data\/train',\n                                                   target_size = (224,224),\n                                                   class_mode = 'categorical',\n                                                   batch_size = 64)","78657653":"valid_dataset = valid_datagen.flow_from_directory(directory = '..\/input\/chest-ctscan-images\/Data\/valid',\n                                                  target_size = (224,224),\n                                                  class_mode = 'categorical',\n                                                  batch_size = 64)","373b8c9c":"test_dataset = test_datagen.flow_from_directory(directory = '..\/input\/chest-ctscan-images\/Data\/test',\n                                                  target_size = (224,224),\n                                                  class_mode = 'categorical',\n                                                  batch_size = 64)","02ebde2a":"base_model = tf.keras.applications.VGG16(input_shape=(224,224,3),include_top=False,weights=\"imagenet\")","f17bbc29":"# Freezing Layers\n\nfor layer in base_model.layers[:-8]:\n    layer.trainable=False","ef9ac937":"# Building Model\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(4,activation='softmax'))","873957e9":"# Model Summary\n\nmodel.summary()","42dba894":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png')","996393a2":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","55305faf":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","5002bfd1":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 3,verbose = 1,factor = 0.50, min_lr = 1e-7)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=3)","a5bc807f":"model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)","735405e4":"%time\nhistory=model.fit(train_dataset,validation_data=valid_dataset,epochs = 20,verbose = 1,callbacks=[lrd,mcp,es])","961036f8":"model.evaluate(test_dataset, verbose=1)","1d1ec849":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n    \n    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(acc) + 1), acc)\n    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n    ax3.plot(range(1, len(auc) + 1), auc)\n    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n    ax3.set_title('History of AUC')\n    ax3.set_xlabel('Epochs')\n    ax3.set_ylabel('AUC')\n    ax3.legend(['training', 'validation'])\n    \n    ax4.plot(range(1, len(precision) + 1), precision)\n    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n    ax4.set_title('History of Precision')\n    ax4.set_xlabel('Epochs')\n    ax4.set_ylabel('Precision')\n    ax4.legend(['training', 'validation'])\n    \n    ax5.plot(range(1, len(f1) + 1), f1)\n    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n    ax5.set_title('History of F1-score')\n    ax5.set_xlabel('Epochs')\n    ax5.set_ylabel('F1 score')\n    ax5.legend(['training', 'validation'])\n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n               history.history['auc'],history.history['val_auc'],\n               history.history['precision'],history.history['val_precision'],\n               history.history['f1_score'],history.history['val_f1_score']\n              )","49b22c2e":"\n<h1 style='background-color:Green; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;' >  What is the VGG16 <\/h1>\n\n\n\nVGG16 is a convolutional neural network model proposed by K. ... Zisserman from the University of Oxford in the paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.\n\n\n\n","736423f0":"\n\n<h1 style='background-color:Green; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;' > Chest Cancer Classification - VVG16 <\/h1>","3afbc146":"### Description\n\n#### Data\n\nImages are not in dcm format, the images are in jpg or png to fit the model\nData contain 3 chest cancer types which are Adenocarcinoma,Large cell carcinoma, Squamous cell carcinoma , and 1 folder for the normal cell\nData folder is the main folder that contain all the step folders\ninside Data folder are test , train , valid\n\ntest represent testing set\n\ntrain represent training set\n\nvalid represent validation set\n\ntraining set is 70%\n\ntesting set is 20%\n\nvalidation set is 10%\n\n\n* Adenocarcinoma\n\nAdenocarcinoma of the lung: Lung adenocarcinoma is the most common form of lung cancer accounting for 30 percent of all cases overall and about 40 percent of all non-small cell lung cancer occurrences. Adenocarcinomas are found in several common cancers, including breast, prostate and colorectal.\n\nAdenocarcinomas of the lung are found in the outer region of the lung\nin glands that secrete mucus and help us breathe.\nSymptoms include coughing, hoarseness, weight loss and weakness.\n\n* Large cell carcinoma\n\nLarge-cell undifferentiated carcinoma: Large-cell undifferentiated carcinoma lung cancer grows and spreads quickly and can\nbe found anywhere in the lung. This type of lung cancer usually accounts for 10\nto 15 percent of all cases of NSCLC.\nLarge-cell undifferentiated carcinoma tends to grow and spread quickly.\n\n* Squamous cell carcinoma\n\nSquamous cell: This type of lung cancer is found centrally in the lung,\nwhere the larger bronchi join the trachea to the lung,\nor in one of the main airway branches.\nSquamous cell lung cancer is responsible for about 30 percent of all non-small\ncell lung cancers, and is generally linked to smoking.\n\nAnd the last folder is the normal CT-Scan images\n\n\n\n#### Dataset link :\n\n[Here](https:\/\/www.kaggle.com\/mohamedhanyyy\/chest-ctscan-images)\n","0b1a1cdc":"<img src=\"https:\/\/miro.medium.com\/max\/850\/1*_Lg1i7wv1pLpzp2F4MLrvw.png\" width=\"800px\">","15e830f6":"### Symptoms and Diagnosis\n\nBecause chest cancer encompasses a number of different cancers, symptoms will be different for different people. Common symptoms of chest cancer include:\n\n* Chest pain\n* Cough that brings up blood or hemoptysis\n* Painful coughing or a cough that doesn\u2019t go away\n* Shortness of breath\n* Hoarseness\n* Wheezing\n* Unusual lumps of tissue under the skin on the chest\n* Unexplained weight loss ","cee7b1c1":"<img src=\"https:\/\/cdn.images.express.co.uk\/img\/dynamic\/11\/750x445\/1321321.jpg\" width=\"800px\">"}}