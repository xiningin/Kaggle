{"cell_type":{"f8977319":"code","7d4a6996":"code","de1632f1":"code","3495aa66":"code","cbc82da6":"code","c43a80f9":"code","9f95f333":"code","e5590e2b":"code","c21d23e6":"code","c29e495e":"code","e00f1e07":"code","3fee088a":"code","134d8391":"code","d1f632aa":"code","52216d83":"code","ace3a1c0":"code","3ec3900d":"code","cbb70628":"code","a704b74a":"code","50d51073":"code","15b2eaa7":"code","e07949bc":"code","0c0602e3":"code","9972b422":"code","a1ec7582":"code","6545aa80":"code","1dd7984e":"code","7799f6aa":"code","0cf65853":"code","1ad6ca89":"code","92ace904":"code","939d474e":"code","402079a8":"code","7511a2a9":"code","f4c37856":"code","613f5ea1":"code","78b4522a":"code","a082c123":"code","7f6695f0":"code","84005e81":"code","c9ea4373":"code","55a4267c":"code","b260ecb4":"markdown","21b2eaef":"markdown","3d27a24d":"markdown","826d0bba":"markdown","fad15546":"markdown","7ec151a2":"markdown","ae2f5e24":"markdown","6c3cac75":"markdown","77d86486":"markdown","3ee41b94":"markdown","2f329498":"markdown","64e5f301":"markdown","0405342e":"markdown","9fd5c540":"markdown","d60908e5":"markdown","3054bdb7":"markdown","285ee345":"markdown","b6307028":"markdown","602e24a4":"markdown","96069dda":"markdown","b57236a8":"markdown","b88947ad":"markdown","6c1c8ed3":"markdown","f9b06333":"markdown","90d33605":"markdown","ce07b044":"markdown"},"source":{"f8977319":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d4a6996":"df=pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')","de1632f1":"df.head()","3495aa66":"df.shape","cbc82da6":"df.info()","c43a80f9":"print(df['Seller_Type'].unique())\nprint(df['Fuel_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())","9f95f333":"df.isnull().sum()","e5590e2b":"df.describe()","c21d23e6":"df.columns","c29e495e":"final_df=df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","e00f1e07":"final_df.head()","3fee088a":"final_df['Current_Year']=2021","134d8391":"final_df.head()","d1f632aa":"final_df['Number_of_Years']=final_df.Current_Year-final_df.Year","52216d83":"final_df.head()","ace3a1c0":"final_df.drop(['Year','Current_Year'],axis=1,inplace=True)","3ec3900d":"final_df.head()","cbb70628":"final_df=pd.get_dummies(final_df,drop_first=True)","a704b74a":"final_df.head()","50d51073":"final_df.corr()","15b2eaa7":"import seaborn as sns","e07949bc":"sns.pairplot(final_df)","0c0602e3":"import matplotlib.pyplot as plt\n%matplotlib inline","9972b422":"corrmat=final_df.corr()\ntop_corr_features=corrmat.index\nplt.figure(figsize=(10,10))\ng=sns.heatmap(final_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","a1ec7582":"X=final_df.iloc[:,1:]\ny=final_df.iloc[:,0]","6545aa80":"X.head()","1dd7984e":"y.head()","7799f6aa":"from sklearn.ensemble import ExtraTreesRegressor\nmodel=ExtraTreesRegressor()\nmodel.fit(X,y)","0cf65853":"print(model.feature_importances_)","1ad6ca89":"feat_importances=pd.Series(model.feature_importances_,index=X.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","92ace904":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","939d474e":"from sklearn.ensemble import RandomForestRegressor\nrf_random=RandomForestRegressor()","402079a8":"n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]\nmax_features=['auto','sqrt']\nmax_depth=[int(x) for x in np.linspace(5,30,num=6)]\nmin_samples_split= [2,5,10,15,200]\nmin_samples_leaf= [1,2,5,10]","7511a2a9":"from sklearn.model_selection import RandomizedSearchCV\nrandom_grid={\n    'n_estimators':n_estimators,\n    'max_features':max_features,\n    'max_depth':max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf\n    }\nprint(random_grid)","f4c37856":"rf = RandomForestRegressor()","613f5ea1":"rf_random=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,scoring='neg_mean_squared_error',n_iter=10,cv=5,verbose=2,random_state=42,n_jobs=1)","78b4522a":"rf_random.fit(X_train,y_train)","a082c123":"predictions=rf_random.predict(X_test)","7f6695f0":"predictions","84005e81":"sns.distplot(y_test-predictions)","c9ea4373":"plt.scatter(y_test,predictions)","55a4267c":"errors = abs(predictions - y_test)\nmape = 100 * np.mean(errors \/ y_test)\naccuracy = 100 - mape\nprint('Model Performance')\nprint('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\nprint('Accuracy = {:0.2f}%.'.format(accuracy))","b260ecb4":"Dropping the Car Name and Creating a final dataframe","21b2eaef":"![image.png](attachment:db83ef17-d8c9-4573-b1d5-141e3473c1ff.png)","3d27a24d":"### Plotting a graph of important features","826d0bba":"### Reading the Data","fad15546":"### Checking the Performance","7ec151a2":"### Checking for Unique values in Seller Type,Fuel Type,Transmission,Owner Column","ae2f5e24":"### Importing the dependecies","6c3cac75":"### Importing the Random Forest Regressor","77d86486":"### Splitting the Data into dependent and independent values","3ee41b94":"### Plotting a heatmap of Correleation Matrix","2f329498":"### Importing the dependencies","64e5f301":"### Encoding : One Hot Encoding\nOne Hot Encoding is a process in the data processing that is applied to categorical data, to convert it into a binary vector representation for use in machine learning algorithms.<br>\nFor more refer: https:\/\/www.geeksforgeeks.org\/python-pandas-get_dummies-method\/","0405342e":"How does Random Forest algorithm work?\nRandom Forest works in two-phase first is to create the random forest by combining N decision tree, and second is to make predictions for each tree created in the first phase.\n\nThe Working process can be explained in the below steps and diagram:\n\nStep-1: Select random K data points from the training set.\n\nStep-2: Build the decision trees associated with the selected data points (Subsets).\n\nStep-3: Choose the number N for decision trees that you want to build.\n\nStep-4: Repeat Step 1 & 2.\n\nStep-5: For new data points, find the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.","9fd5c540":"### Building the Model","d60908e5":"### The columns in the data frame","3054bdb7":"### Checking for Correleation among dependent and independent values","285ee345":"### Checking for Numeric and Categorical Variables","b6307028":"### Insights of the Data","602e24a4":"RandomizedSearchCV solves the drawbacks of GridSearchCV, as it goes through only a fixed number of hyperparameter settings. It moves within the grid in random fashion to find the best set hyperparameters. This approach reduces unnecessary computation.\n##### Credits: https:\/\/www.geeksforgeeks.org\/hyperparameter-tuning\/","96069dda":"### Checking for Null Values","b57236a8":"### Splitting the data into train and  test data","b88947ad":"##### Credits: https:\/\/www.javatpoint.com\/machine-learning-random-forest-algorithm <br>\nRandom Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.\n\nAs the name suggests, \"Random Forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset.\" Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.\n\nThe greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.","6c1c8ed3":"### Checking for the shape of the dataset","f9b06333":"### Hyperparameter Tunning","90d33605":"### Getting the important features","ce07b044":"### Creating a new feature my eliminating Year column"}}