{"cell_type":{"5ec7d30d":"code","edfe257c":"code","370fdef1":"code","9768c180":"code","c7c03f2a":"code","c128326f":"code","55afdc50":"code","d8239ca2":"code","546922dc":"code","d5de3c40":"code","1526da6e":"code","606b9207":"code","e6ca3a11":"code","60e3ea23":"code","5f9e5d0b":"code","66b22b46":"code","432b76c0":"code","f1145b54":"code","86407d7b":"code","deb23b97":"code","2be90add":"code","64f12c26":"code","d7f96f0a":"code","416d0646":"code","0d330dd0":"code","537eddbe":"code","a882a46f":"code","c240d719":"code","07fe5d09":"code","968bd4ac":"code","8fb7c18d":"code","4adb8cb5":"markdown","0a985e51":"markdown"},"source":{"5ec7d30d":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB","edfe257c":"# importing dataset and printing first 5 rows\ndf = pd.read_csv('..\/input\/spam-ham\/spam.csv', encoding = \"ISO-8859-1\")\ndf.head()","370fdef1":"# printing data information\ndf.info()","9768c180":"# dropping columns last 3 columns as it has more than 90% null values\ndf = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)","c7c03f2a":"# looking at the datatset again\ndf.head()","c128326f":"# mapping target variables with 1 and 0 as we have only 2 categories of ham and spam\ndf['v1'] = df['v1'].map({'ham': 1, 'spam': 0})\ndf.head()","55afdc50":"# Looking at value counts to see how much we have imbalance in our dataset\ndf.v1.value_counts() # 1 -> ham, 0-> spam","d8239ca2":"# Looking at the % of data of ham which is only 13.4%\n(df.v1.value_counts()[0]\/len(df.index))*100","546922dc":"# Dividing the data into X and y\nX = df.v2\ny = df.v1\ny = y.astype('int')","d5de3c40":"# splitting into test and train with random state as 1 and test size as 25%\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25)","1526da6e":"# lokking at the first 5 row of X_train\nX_train.head()","606b9207":"# Looking at the target variable\ny.head()","e6ca3a11":"# initializing countvectorizer including stopwords to analyze\nvect = CountVectorizer()\nvect.fit(X)","60e3ea23":"# Looking at the vocabulary of our dataset\nvect.vocabulary_","5f9e5d0b":"# total no of features or columns\nlen(vect.vocabulary_)","66b22b46":"# initializing again with excluding stopwords \nvect = CountVectorizer(stop_words='english')\nvect.fit(X)","432b76c0":"vect.vocabulary_","f1145b54":"# total no of features or columns\nlen(vect.vocabulary_)","86407d7b":"# Feature or column names\nprint(vect.get_feature_names())","deb23b97":"# transforming the train and test\nX_transform_train = vect.transform(X_train)\nX_transform_test = vect.transform(X_test)\nX_transform_train","2be90add":"# Looking at compressed sparse form of data\nprint(X_transform_train)","64f12c26":"# non compressed form\nX_transform_train.toarray()[:5]","d7f96f0a":"# converting non compressed form to dataframe\ndf = pd.DataFrame(X_transform_train.toarray(), columns=vect.get_feature_names())\ndf.head()","416d0646":"# converting test data to dataframe after transformation\ndf_test = pd.DataFrame(X_transform_test.toarray(), columns=vect.get_feature_names())","0d330dd0":"# shape of train data\ndf.shape","537eddbe":"# shape of test data\ndf_test.shape","a882a46f":"# defining alpha values to try on model and saving score of each model into respective arrays\nalpha_list = np.arange(1\/100000, 20, 0.11)\ntrain_score = np.zeros(len(alpha_list))\ntest_score = np.zeros(len(alpha_list))\nprecision_ = np.zeros(len(alpha_list))\nrecall_ = np.zeros(len(alpha_list))","c240d719":"# looping over alpha values in alpha list \ncount = 0\nfor alpha in alpha_list:\n    mnb = MultinomialNB(alpha=alpha) # Using MultinomialNB\n    mnb.fit(df, y_train)\n    train_score[count] = mnb.score(df, y_train)\n    test_score[count] = mnb.score(df_test, y_test)\n    conf = confusion_matrix(mnb.predict(df_test), y_test)\n    precision_[count] = conf[1][1]\/(conf[1][1]+conf[0][1])\n    recall_[count] = conf[1][1]\/(conf[1][0]+conf[1][1])\n    count = count + 1\n    ","07fe5d09":"# creating dataframe of all the scores defined above to analyse\nscore_matrix = np.matrix(np.c_[alpha_list, train_score, test_score, precision_, recall_])\nscore_df = pd.DataFrame(score_matrix, columns=['alpha', 'train', 'test', 'precision', 'recall'])\nscore_df.head(10)","968bd4ac":"# sorting with max precision\nscore_df.sort_values(by=['precision'], ascending=False)","8fb7c18d":"# choosing best alpha from above dataframe and finally fitting the model and getting the best score \n# we need 0 true positive to get best classifier for emails\n# I can manage some spam mails in my total mails but I can't manage my important mails going into spam folder\nalpha = 10.01\nmnb = MultinomialNB(alpha=alpha)\nmnb.fit(df, y_train)\nconf = confusion_matrix(mnb.predict(df_test), y_test)\nconf","4adb8cb5":"# Hope you found this notebook helpful.....Please give a feedback on this guys.....","0a985e51":"Although we have only 13 % of ham dataset but it's not the worst case like cancer dataset, where we have 0.1%\nof cancer patients out of thousands of patients, so we can go with the normal flow of building the model."}}