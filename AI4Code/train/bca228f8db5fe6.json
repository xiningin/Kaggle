{"cell_type":{"f73f1066":"code","6490b4c9":"code","fc12c299":"code","8fbabc0e":"code","34102174":"code","453b41c5":"code","53888cd0":"code","8dd05d25":"code","f79a54d6":"code","b5a0725c":"code","61a01550":"code","cda0356e":"code","bfd0060b":"code","63f8d7c9":"code","33af027e":"code","a8e362fd":"code","1046aa0d":"code","ad4a00d3":"code","a38e02ba":"code","308a67cd":"code","50b26b27":"code","f38d7088":"code","a9d92601":"code","5a04ec80":"code","5b6bee8d":"code","0f6e6d34":"code","535d890c":"code","e20176bc":"code","a07d4c06":"code","564fba6c":"code","7d6f53b9":"code","9059c5f5":"code","3fc5239b":"code","b184fc4d":"code","ef877faa":"code","92a5a439":"code","aee95179":"code","a4767192":"code","2a232ec8":"code","a589c971":"code","3f71dad3":"code","4ba4e6fa":"code","b1810473":"code","d05c31c1":"code","4e8e87cf":"code","4b74e453":"code","ef217ace":"code","b6c7d335":"code","3aa28ec1":"code","aac42864":"code","1ec10317":"code","3ce4f787":"code","d7a86168":"code","658d2109":"code","6e5e00a6":"markdown","b63bdc4f":"markdown","8f4f4be1":"markdown","1b77b773":"markdown","db3036d5":"markdown","a39a4fdc":"markdown","18a49610":"markdown","ae10092a":"markdown","2c594421":"markdown","8149a5d7":"markdown","01c4a6e7":"markdown","2e0bc71d":"markdown","50be977b":"markdown","e76bad57":"markdown","3a4466a4":"markdown","be218cd0":"markdown","ef27e0d2":"markdown","049a68f4":"markdown","a2eeb599":"markdown","b670e5dd":"markdown","31d3c8e5":"markdown","2a0641c9":"markdown","710aac9f":"markdown","173113d9":"markdown","e75f0e91":"markdown","d7499203":"markdown","11a6b22b":"markdown","40fa6ecd":"markdown","6a11a27d":"markdown","25ac68fa":"markdown","68b7c3fa":"markdown","c27376e5":"markdown","e67f725a":"markdown","73df2ab8":"markdown","1424c560":"markdown","bfbf3f8e":"markdown","d6f6778e":"markdown","2ad6b877":"markdown","324a9d4d":"markdown","9aed1c55":"markdown","eea57fbb":"markdown","3a205e76":"markdown","8bec87f0":"markdown","66d93fbe":"markdown","6a725332":"markdown","ad073351":"markdown"},"source":{"f73f1066":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nfrom matplotlib import pyplot as plt \n%matplotlib inline","6490b4c9":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndf.head()","fc12c299":"df.info()","8fbabc0e":"df1 = df.drop(columns=['salary','sl_no'], axis=1)","34102174":"dummy = pd.get_dummies(df['status'])\ndummy.drop(columns=['Not Placed'],axis=1, inplace=True)\ndf1 = pd.concat([df1,dummy],axis =1)\ndf1.head()","453b41c5":"df1['Placed'].value_counts().to_frame().style.bar()","53888cd0":"ax = sns.heatmap(df1.corr(),annot=True)","8dd05d25":"g = sns.factorplot(x=\"gender\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","f79a54d6":"g = sns.factorplot(x=\"ssc_b\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","b5a0725c":"g = sns.factorplot(x=\"hsc_b\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","61a01550":"g = sns.factorplot(x=\"hsc_s\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","cda0356e":"g = sns.factorplot(x=\"degree_t\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","bfd0060b":"g = sns.factorplot(x=\"workex\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","63f8d7c9":"g = sns.factorplot(x=\"specialisation\",y=\"Placed\",data= df1,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"placement probability\")","33af027e":"g = sns.kdeplot(df1[\"ssc_p\"][(df1[\"Placed\"] == 0) & (df1[\"ssc_p\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df1[\"ssc_p\"][(df1[\"Placed\"] == 1) & (df1[\"ssc_p\"].notnull())], color=\"Blue\", shade = True)\ng.set_xlabel(\"Marks in secondary school\")\ng.set_ylabel(\"Placement\")\ng = g.legend([\"Not Placed\",\"Placed\"])","a8e362fd":"g = sns.kdeplot(df1[\"hsc_p\"][(df1[\"Placed\"] == 0) & (df1[\"hsc_p\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df1[\"hsc_p\"][(df1[\"Placed\"] == 1) & (df1[\"hsc_p\"].notnull())], color=\"Blue\", shade = True)\ng.set_xlabel(\"Marks in high school\")\ng.set_ylabel(\"Placement\")\ng = g.legend([\"Not Placed\",\"Placed\"])","1046aa0d":"g = sns.kdeplot(df1[\"degree_p\"][(df1[\"Placed\"] == 0) & (df1[\"degree_p\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df1[\"degree_p\"][(df1[\"Placed\"] == 1) & (df1[\"degree_p\"].notnull())], color=\"Blue\", shade = True)\ng.set_xlabel(\"Marks in secondary school\")\ng.set_ylabel(\"Placement\")\ng = g.legend([\"Not Placed\",\"Placed\"])","ad4a00d3":"g = sns.catplot(x=\"Placed\",y= 'degree_p', order=[0, 1],data=df1)","a38e02ba":"g = sns.kdeplot(df1[\"etest_p\"][(df1[\"Placed\"] == 0) & (df1[\"etest_p\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df1[\"etest_p\"][(df1[\"Placed\"] == 1) & (df1[\"etest_p\"].notnull())], color=\"Blue\", shade = True)\ng.set_xlabel(\"Marks in secondary school\")\ng.set_ylabel(\"Placement\")\ng = g.legend([\"Not Placed\",\"Placed\"])","308a67cd":"g = sns.catplot(x=\"Placed\",y= 'etest_p', order=[0, 1],data=df1)","50b26b27":"g = sns.kdeplot(df1[\"mba_p\"][(df1[\"Placed\"] == 0) & (df1[\"mba_p\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df1[\"mba_p\"][(df1[\"Placed\"] == 1) & (df1[\"mba_p\"].notnull())], color=\"Blue\", shade = True)\ng.set_xlabel(\"Marks in secondary school\")\ng.set_ylabel(\"Placement\")\ng = g.legend([\"Not Placed\",\"Placed\"])","f38d7088":"g = sns.catplot(x=\"Placed\",y= 'mba_p', order=[0, 1],data=df1)","a9d92601":"df1.head()","5a04ec80":"df_feature = df1[['ssc_p','hsc_p','degree_p']]","5b6bee8d":"df1[\"workex\"] = df1[\"workex\"].astype('category')\ndf_feature[\"workex\"] = df1[\"workex\"].cat.codes\n#No=0, Yes=1","0f6e6d34":"df1[\"specialisation\"] = df1[\"specialisation\"].astype('category')\ndf_feature[\"specialisation\"] = df1[\"specialisation\"].cat.codes\n# Mkt&Fin=0 | Mkt&HR=1 ","535d890c":"df1[\"gender\"] = df1[\"gender\"].astype('category')\ndf_feature[\"gender\"] = df1[\"gender\"].cat.codes\n# F=0 | M=1 ","e20176bc":"hsc_dummy = pd.get_dummies(df1['hsc_s'], prefix='hsc')\ndf_feature = pd.concat([df_feature,hsc_dummy['hsc_Arts']],axis =1)\n#hsc_Arts - Yes=1, No=0","a07d4c06":"deg_dummy = pd.get_dummies(df1['degree_t'], prefix='degree_in')\ndf_feature = pd.concat([df_feature,deg_dummy['degree_in_Others']],axis =1)\n#degree_in_other - Yes=1, No=0","564fba6c":"df_feature.head()","7d6f53b9":"df_feature.dtypes","9059c5f5":"df_feature['ssc_p'] = df_feature['ssc_p']\/df_feature['ssc_p'].max()\ndf_feature['hsc_p'] = df_feature['hsc_p']\/df_feature['hsc_p'].max()\ndf_feature['degree_p'] = df_feature['degree_p']\/df_feature['degree_p'].max()","3fc5239b":"df_feature.head()","b184fc4d":"df_feature['Placed'] = df1['Placed']\nax = sns.heatmap(df_feature.corr(),annot=True)\ndf_feature.drop(columns='Placed',axis=1,inplace=True)","ef877faa":"df_feature.drop(columns=['gender','hsc_Arts'],axis=1,inplace=True)","92a5a439":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n","aee95179":"X_train, X_test, y_train, y_test = train_test_split(df_feature,df1['Placed'],test_size = 0.1,random_state=5)","a4767192":"clsf = LogisticRegression(C=3)\nclsf.fit(X_train, y_train)\nscore = clsf.score(X_test,y_test)\nprint (score)","2a232ec8":"print(cross_val_score(clsf, df_feature, df1['Placed'], cv=10))\nprint (cross_val_score(clsf, df_feature, df1['Placed'], cv=10).mean())","a589c971":"accuracy = []\nskf = StratifiedKFold(n_splits = 10,random_state=2)\nscore = cross_val_score(clsf, X_train, y = y_train, scoring = \"accuracy\", cv = skf, n_jobs=4)\nprint (score)","3f71dad3":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC","4ba4e6fa":"kfold = StratifiedKFold(n_splits = 10,random_state=2)\nclassifiers =[]\nr=2\n\nclassifiers.append(LogisticRegression(random_state=r))\nclassifiers.append(DecisionTreeClassifier(random_state=r))\nclassifiers.append(MLPClassifier(random_state=r))\nclassifiers.append(RandomForestClassifier(random_state=r))\nclassifiers.append(AdaBoostClassifier(RandomForestClassifier(random_state=r),random_state=r,learning_rate=0.1))\nclassifiers.append(ExtraTreesClassifier(random_state=r))\nclassifiers.append(GradientBoostingClassifier(random_state=r))\nclassifiers.append(SVC(random_state=r))\nclassifiers.append(KNeighborsClassifier())","b1810473":"cv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())","d05c31c1":"cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"LogisticRegression\",\"DecisionTree\",\"MultipleLayerPerceptron\",\n                                                                                       \"RandomForest\",\"Adaboost\",\"ExtraTree\",\"GradientBoosting\",\"SVC\",\"KNeighboors\"]})\ncv_res.head()","4e8e87cf":"g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","4b74e453":"RFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","ef217ace":"adaRFC = AdaBoostClassifier(RFC_best, random_state=7)\n\nada_param_grid = {\"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\ngsadaRFC = GridSearchCV(adaRFC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaRFC.fit(X_train,y_train)\nadaRFC_best = gsadaRFC.best_estimator_\ngsadaRFC.best_score_","b6c7d335":"ExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","3aa28ec1":"KNC = KNeighborsClassifier()\nkn_param_grid = {'n_neighbors':[4,5,6,7,8,9],\n                 'weights':[\"uniform\", \"distance\"],\n                 'algorithm':[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n                 'metric':['euclidean','manhattan']\n                }\n\ngsKNC = GridSearchCV(KNC, param_grid= kn_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n\ngsKNC.fit(X_train,y_train)\n\nKNC_best = gsKNC.best_estimator_\ngsKNC.best_score_","aac42864":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","1ec10317":"X = df_feature\ny = df1['Placed']\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaRFC.best_estimator_,\"AdaBoost learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsKNC.best_estimator_,\"KNeighbor learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"Gradient Boosting learning curves\",X_train,y_train,cv=kfold)","3ce4f787":"nrows = 1\nncols = 3\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(21,7))\n\nnames_classifiers = [(\"AdaBoosting\", adaRFC_best),(\"RandomForest\",RFC_best),(\"ExtraTrees\",ExtC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:6]\n        # [:6] is mentioned as there are only six feature, while the function can return more indexs than the feature. \n        print (name,indices)\n        x_ =  classifier.feature_importances_[indices][:6]\n        if name == \"GradientBoosting\": \n            print (x_)\n        g = sns.barplot(y=X_train.columns[indices][:6],x = x_ , orient='h',ax=axes[col])\n\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","d7a86168":"g = plot_learning_curve(gsadaRFC.best_estimator_,\"Adaboost RFC learning curves\",X_train,y_train,cv=kfold)\nprint (\"Score of the model\",adaRFC_best.score(X,y))","658d2109":"Placement_predict = pd.Series(adaRFC_best.predict(X), name=\"Survived\")\n\nPridiction = pd.concat([df[sl_no],Placement_predict],axis=1)\n\nresults.to_csv(\"python_ensemble_prediction.csv\",index=False)","6e5e00a6":"#### The best performance can be shown by Adaboosted Random Forest Classifier. This may be the most suitable algorithm. ","b63bdc4f":"MBA percentatges also don't show much relation with placement values","8f4f4be1":"### High School Percentage","1b77b773":"#### Observations- \nssc_p, hsc_p and degree_p are more correlated, while etest_p and mba_p are least related. \n**MBA percentage matters least for your placement as a MBA student, which is quite surprising.**","db3036d5":"### Secondary School Marks \n\nStudents with better marks in secondary schools have more placement chances.","a39a4fdc":"### Specialization","18a49610":"We will first get the correlation heat map. ","ae10092a":"### High School Board","2c594421":"## Cross Validation Of Models\n#### Modeling different classifiers gives an idea about more suitable algorithms to be used. Here about seven famous classifiers are used. \n1. Logistic Regression\n2. Decision Tree\n3. Random Forest\n4. Multi Layer Perceptron\n5. Support Vector Classifier \n6. Extra Tree Classifier\n7. KNeighbor","8149a5d7":"# Importance Of Features \n\nUnderstanding the feature importance on each classifier is helpful to understand data. For a student to be placed, she must have good marks in secondary school, above average degree percentage and high school marks are also important. ","01c4a6e7":"### Subject Bachelor Degree","2e0bc71d":"### Work Experience","50be977b":"Having more than 50% marks is important to get a placement. ","e76bad57":"### High School Subject","3a4466a4":"#### Since Gender and hsc_Arts features show very less coorelation, they can be removed to avoid any biasness in the model.","be218cd0":"Unfortuantely Females have slighly less probablity of getting the college placement then males, in this college. However such neglible variations can often be ignored.","ef27e0d2":"The training accuracy of KNeighbor and GradientBoosting model are decreasing with increasing data, hence won't be anymore effective on the data. \nThe Random Forest Classification Model gives good accuracy, but Adaboost Classifier is slighly better than RFC. ","049a68f4":"## Features Mapping\n\n> \n> 0. **Categories**       - 0  |  1\n> 1. **workex**           - N  |  Y\n> 2. **specialisation**   - N  |  Y\n> 3. **gender**           - F  |  M\n> 4. **hsc_Arts**         - N  |  Y\n> 5. **degree_in_Others** - N  |  Y\n> ","a2eeb599":"## Logistic Regression Model\nBy applying a simple linear regression model, one can understand the predictability of the data. ","b670e5dd":"## School Boards\n\n#### These school boards have negligible effect or correlation, with placement of the student. ","31d3c8e5":"The plausibility of placement is high, for Sci\/Tech and Comm\/Magmt student, while low for other degrees","2a0641c9":"#### Placement doesn't seeem to much affected by Etest, however large number of student with 60-70% seem to be unplaced. So if you are a MBA student at this college focus less on Etest.","710aac9f":"### Plotting The Learning Curve\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","173113d9":"### GENDER","e75f0e91":"### MBA Percentage","d7499203":"#### Science and commerece student have almost equal chances of placement, however for a arts student placement is less likely. ","11a6b22b":"![image.png](attachment:image.png)","40fa6ecd":"For placement Finance is more sutable than HR","6a11a27d":"# Campus Placement Prediction for MBA Students\n#### We have placement data of MBA students in at Jain University Bangalore. We will analyse the recriutment data and create a machine learning model to predict the chances of placement, based on student's academic details, given similar circumstances. ","25ac68fa":"Here, we need a dataset to train models. Hence all the object and categorical data will have to transformed.","68b7c3fa":"## Feature Engineering","c27376e5":"# Final Prediction Model","e67f725a":"### Senior School Board","73df2ab8":"### Prediction ","1424c560":"## First Glance at the Data","bfbf3f8e":"# HyperParamter Tuning","d6f6778e":"### Degree Percentage","2ad6b877":"## Features Analysis","324a9d4d":"### Employability test percentage","9aed1c55":"### Normalization\nThe three percentage variables are needed to be normalized.  ","eea57fbb":"## According to the cross validation score, best suitable algorithms are:\n1. RandomForests\n2. Adaboost With Random Forest\n3. Extra Tree \n4. KNeighbor\n4. Gradient Boosting","3a205e76":"Here we tend to have two target variable. \n1. status - Placed\/Not Placed\n2. salary \n\nWe are going to deal with, the status target i.e Placement Status. ","8bec87f0":"Obviouly a work experience gives more chances of placement. ","66d93fbe":"### Correlation Heatmap Of All Features\nAfter the final feature engineering, the heat ","6a725332":"#### Adaboost on Random Forest is the most accurate accurate classfier over other classifier. This model would provide better accuracy given more data. ","ad073351":"After overall analysis, the significant features are: \n1. ssc_p\n2. hsc_p \n3. degree_p\n4. Specialization\n5. Workex\n6. degree_t - other vs science\/commmerce\n7. hsc_s - arts vs other categories\n8. gender"}}