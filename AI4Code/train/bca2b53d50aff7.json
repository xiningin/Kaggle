{"cell_type":{"a5a53acc":"code","7123eeda":"code","f471ce89":"code","cfce9c9f":"code","cdce3452":"code","d2cef8ca":"code","ab2d1b96":"code","c0e08be0":"code","dc095a3d":"code","b3d3f2f7":"code","032d8f53":"code","e3b0f784":"code","c6346f70":"code","5be70b62":"code","c41ac3ad":"markdown","ad28c1d4":"markdown","22ba30c2":"markdown","cdd648e4":"markdown","210c03ca":"markdown","57d37aeb":"markdown","732a78ae":"markdown","dd20ce6b":"markdown","a84f0108":"markdown","fe683a40":"markdown","e4e6f9c7":"markdown"},"source":{"a5a53acc":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\n\nmodel = RandomForestRegressor(n_estimators=100, criterion=\"mse\")\n\nfeatures = ['Neighborhood', 'Condition1', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', 'CentralAir', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'KitchenAbvGr', 'TotRmsAbvGrd', 'KitchenQual', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'SaleType', 'SaleCondition']\n\ny = train['SalePrice']\nX = train[features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y)\nprint('settings completed')","7123eeda":"from sklearn.metrics import mean_squared_log_error\n\ndef cal_err(train_X, train_y, val_X, val_y):\n    model.fit(train_X, train_y)\n    preds = model.predict(val_X)\n    err = mean_squared_log_error(val_y, preds)\n    return err","f471ce89":"s = (train_X.dtypes == 'object')\nobj_cols = list(s[s].index)\nnum_cols = list(set(train_X.columns) - set(obj_cols))\n\nprint(\"Categorical variables:\")\nprint(obj_cols)\nprint(\"Numerical variabels:\")\nprint(num_cols)","cfce9c9f":"# Shape of training data (num_rows, num_columns)\nprint(train_X.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (train_X.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","cdce3452":"target_rm = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n\n# Make copy to avoid changing original data (when imputing)\nX_train_plus = train_X.copy()\nX_valid_plus = val_X.copy()\n\n# Make new columns indicating whether the house has garage\ntrain_X['hasGarage'] = X_train_plus['GarageType'].isnull() ^ 1\nval_X['hasGarage'] = X_valid_plus['GarageType'].isnull() ^ 1\n\ntrain_X['GarageYrBlt'].fillna(0, inplace=True)\nval_X['GarageYrBlt'].fillna(0, inplace=True)\n\ntrain_X = train_X.drop(target_rm, axis=1)\nval_X = val_X.drop(target_rm, axis=1)\n\nobj_cols = list(set(obj_cols) - set(target_rm))","d2cef8ca":"object_nunique = list(map(lambda col: train_X[col].nunique(), obj_cols))\nd = dict(zip(obj_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","ab2d1b96":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ntarget = []\nfor item in d:\n    if d[item] < 10:\n        target.append(item)\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[target]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(val_X[target]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_X.index\nOH_cols_valid.index = val_X.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_X.drop(target, axis=1)\nnum_X_valid = val_X.drop(target, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain_X = pd.concat([num_X_train, OH_cols_train], axis=1)\nval_X = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\ntrain_X['Neighborhood'] = label_encoder.fit_transform(train_X['Neighborhood'])\nval_X['Neighborhood'] = label_encoder.transform(val_X['Neighborhood'])","c0e08be0":"# Nothing to do yet","dc095a3d":"print(cal_err(train_X, train_y, val_X, val_y))","b3d3f2f7":"test[features].isnull().sum()","032d8f53":"test['TotalBsmtSF'].fillna(value=test['TotalBsmtSF'].mode()[0], inplace=True)\ntest['KitchenQual'].fillna(value=test['KitchenQual'].mode()[0], inplace=True)\ntest['SaleType'].fillna(value=test['SaleType'].mode()[0], inplace=True)\ntest['GarageCars'].fillna(value=test['GarageCars'].mean(), inplace=True)\ntest['GarageArea'].fillna(value=test['GarageArea'].mean(), inplace=True)","e3b0f784":"test_X = test[features]\n\ntarget_rm = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n\n# Make copy to avoid changing original data (when imputing)\nX_plus = X.copy()\ntest_X_plus = test_X.copy()\n\n# Make new columns indicating whether the house has garage\nX['hasGarage'] = X['GarageType'].isnull() ^ 1\ntest_X['hasGarage'] = test_X['GarageType'].isnull() ^ 1\n\nX['GarageYrBlt'].fillna(0, inplace=True)\ntest_X['GarageYrBlt'].fillna(0, inplace=True)\n\nX = X.drop(target_rm, axis=1)\ntest_X = test_X.drop(target_rm, axis=1)\n\nobj_cols = list(set(obj_cols) - set(target_rm))\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_X = pd.DataFrame(OH_encoder.fit_transform(X[target]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test_X[target]))\n\n# One-hot encoding removed index; put it back\nOH_cols_X.index = X.index\nOH_cols_test.index = test_X.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X = X.drop(target, axis=1)\nnum_X_test = test_X.drop(target, axis=1)\n\n# Add one-hot encoded columns to numerical features\nX = pd.concat([num_X, OH_cols_X], axis=1)\ntest_X = pd.concat([num_X_test, OH_cols_test], axis=1)\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nX['Neighborhood'] = label_encoder.fit_transform(X['Neighborhood'])\ntest_X['Neighborhood'] = label_encoder.transform(test_X['Neighborhood'])","c6346f70":"model.fit(X,y)\n\npreds = model.predict(test_X)","5be70b62":"submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\nsubmission.head()\n\nsubmission['SalePrice'] = preds\n\nsubmission.to_csv('submission_4.csv', index=False)\nprint('Completed')","c41ac3ad":"## Numerical Variables","ad28c1d4":"## Submission (Not yet)","22ba30c2":"## testing","cdd648e4":"## Categorical Variables","210c03ca":"## filling NaN","57d37aeb":"## Settings","732a78ae":"## Testing Function","dd20ce6b":"min2 = 100\nfor i in range(100,1001,100):\n    for j in range(10):\n        model = RandomForestRegressor(n_estimators=i, criterion=\"mse\")\n        score[i] += cal_err(train_X, train_y, val_X, val_y)\n    if score[min2] > score[i]:\n        min2 = i\n\nprint('The proper value is '+str(min2))\nprint('The score is '+str(score[min2] \/ 10))\n\nif score[min1] < score[min2]:\n    min_est = min1\nelse:\n    min_est = min2\n\nori_score = score[min_est]","a84f0108":"score3 = [0] * 600\nmin_leaf = 10\nmin_est = 900\nfor i in range(10,31):\n    for j in range(10):\n        model = RandomForestRegressor(n_estimators=min_est, criterion = \"mse\", max_leaf_nodes = i)\n        score3[i] += cal_err(train_X, train_y, val_X, val_y)\n    if score3[min_leaf] > score3[i]:\n        min3 = i\n    \nprint('The proper value is '+str(min_leaf))\nprint('The score is '+str(score3[min_leaf] \/ 10))\nprint('---------------------------------------------')\nif score3[min_leaf] < ori_score:\n    model = RandomForestRegressor(n_estimators=min_est, criterion = \"mse\", max_leaf_nodes = min_leaf)\n    print('The final score is' + str(score3[min_leaf] \/ 10))\nelse:\n    model = RandomForestRegressor(n_estimators=min_est, criterion = 'mse')\n    print('The final score is '+str(ori_score \/ 10))","fe683a40":"attributes of which have less than 10 unique values, do One Hot Encoding<br>\nand the others, Label Encoding","e4e6f9c7":"score = [0]*1001\nmin1 = 10\nfor i in range(10,101):\n    for j in range(10):\n        model = RandomForestRegressor(n_estimators=i, criterion=\"mse\")\n        score[i] += cal_err(train_X, train_y, val_X, val_y)\n    if score[min1] > score[i]:\n        min = i\n\nprint('The proper value is '+str(min1))\nprint('The score is '+str(score[min1] \/ 10))"}}