{"cell_type":{"6dfa1f28":"code","bc3c2f60":"code","aa928f6f":"code","cfc3e4aa":"code","d8a00afa":"code","9f8dfa74":"code","99fd7dfd":"code","d53638fa":"code","5b85cdaf":"code","d6b9e9ce":"code","c638e5a4":"code","9b5e328a":"code","46f6cef2":"markdown","f4f028b4":"markdown","f13a6ae1":"markdown","bd26095c":"markdown","7fa543d3":"markdown","1158a556":"markdown","7c0411d7":"markdown","1785ca37":"markdown","16621967":"markdown"},"source":{"6dfa1f28":"import numpy as np\nimport pickle\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom sklearn import linear_model\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.decomposition import TruncatedSVD, NMF\nimport matplotlib.gridspec as gridspec\n\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['svg.fonttype'] = 'none'","bc3c2f60":"#%% script params\n\nsave_figures = True\nsave_figures = False\nall_file_endings_to_use = ['.png', '.pdf', '.svg']\n\ndata_folder   = '\/kaggle\/input\/fiter-and-fire-paper\/results_data_capacity\/'\nfigure_folder = '\/kaggle\/working\/'","aa928f6f":"def create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):\n\n    safety_factor = 1.5\n    if tau_rise >= (tau_decay \/ safety_factor):\n        tau_decay = safety_factor * tau_rise\n\n    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)\n    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)\n\n    post_syn_potential = exp_d - exp_r\n    post_syn_potential \/= post_syn_potential.max()\n\n    return post_syn_potential\n\n\ndef construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):\n\n    num_synapses = tau_rise_vec.shape[0]\n    temporal_filter_length = int(7 * tau_decay_vec.max()) + 1\n\n    syn_filter = np.zeros((num_synapses, temporal_filter_length))\n\n    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):\n        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)\n\n    return syn_filter\n\n\ndef simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc local currents\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)\n    for k in range(normlized_syn_filter.shape[0]):\n        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]\n\n    # multiply by weights to get the somatic current\n    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\ndef add_offset_for_plotting(traces_matrix, offset_size=1.1):\n\n    traces_matrix_with_offset = offset_size * np.kron(np.arange(traces_matrix.shape[0])[:,np.newaxis], np.ones((1,traces_matrix.shape[1])))\n    traces_matrix_with_offset = traces_matrix_with_offset + traces_matrix\n\n    return traces_matrix_with_offset","cfc3e4aa":"#%% script params\n\n# input parameters\nnum_values_per_param = 12\n\n# neuron model parameters\nconnections_per_axon = 5\nnum_synapses = num_values_per_param * num_values_per_param\n\nv_reset     = -80\nv_threshold = -55\ncurrent_to_voltage_mult_factor = 3\nrefreactory_time_constant = 20\n\nmodel_type = 'F&F'\n#model_type = 'I&F'\n\ntime_limit_ms = 120\n\n# synapse non-learnable parameters\nif model_type == 'F&F':\n    tau_rise_range  = [1,18]\n    tau_decay_range = [7,25]\nelif model_type == 'I&F':\n    tau_rise_range  = [3,3]\n    tau_decay_range = [25,25]\n\ntau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]\ntau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)\n\ntau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]\ntau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))\n\nnormlized_syn_filter_small = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)\n\noffset_size = 0.15\n\nplt.close('all')\nplt.figure(figsize=(25,20));\nplt.subplot(1,2,1); plt.imshow(normlized_syn_filter_small);\nplt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);\nplt.subplot(1,2,2);\n\nuse_colors = False\nif use_colors:\n    colors = 'rgbymcrgbymc'\n\n    end_ind = 0\n    for k in range(num_values_per_param):\n        start_ind = end_ind\n        end_ind = start_ind + num_values_per_param\n        print(start_ind, end_ind, colors[k])\n        plt.plot(offset_size * k * num_values_per_param + add_offset_for_plotting(normlized_syn_filter_small[start_ind:end_ind], offset_size=offset_size).T, c=colors[k]);\nelse:\n    plt.plot(add_offset_for_plotting(normlized_syn_filter_small, offset_size=offset_size).T, c='k');\n\nplt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(-1,time_limit_ms);","d8a00afa":"#%% Create all possible combinations\n\n# input parameters\nnum_values_per_param = 20\n\ntau_rise_vec = np.linspace(tau_rise_range[0], tau_rise_range[1] , num_values_per_param)[:,np.newaxis]\ntau_rise_vec = np.kron(np.ones((num_values_per_param,1)), tau_rise_vec)\n\ntau_decay_vec = np.linspace(tau_decay_range[0], tau_decay_range[1] , num_values_per_param)[:,np.newaxis]\ntau_decay_vec = np.kron(tau_decay_vec, np.ones((num_values_per_param,1)))\n\nnormlized_syn_filter_large = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)\n\nplt.close('all')\nplt.figure(figsize=(25,20));\nplt.subplot(1,2,1); plt.imshow(normlized_syn_filter_large);\nplt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22)\nplt.xlim(0,time_limit_ms);\nplt.subplot(1,2,2); plt.plot(normlized_syn_filter_large.T, alpha=0.15);\nplt.title('normlized synaptic filters as PSPs', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22);\nplt.xlim(0,time_limit_ms);","9f8dfa74":"#%% apply SVD and display\n\nX = normlized_syn_filter_large\nPSP_SVD_model = TruncatedSVD(n_components=100)\nPSP_SVD_model.fit(X)\n\nSVD_cutoff_ind = 3\nmax_SVD_basis_to_present = 18\n\nplt.close('all')\nplt.figure(figsize=(25,20));\nplt.subplot(3,1,1); plt.imshow(PSP_SVD_model.components_[:max_SVD_basis_to_present]);\nplt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);\nplt.subplot(3,1,2); plt.plot(PSP_SVD_model.components_[:SVD_cutoff_ind].T);\nplt.title('first 3 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);\nplt.subplot(3,1,3); plt.plot(PSP_SVD_model.components_[SVD_cutoff_ind:max_SVD_basis_to_present].T);\nplt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);","99fd7dfd":"#%% show variance explained\n\nnum_basis_functions = PSP_SVD_model.explained_variance_ratio_.shape[0]\nexplained_var_percent = 100 * PSP_SVD_model.explained_variance_ratio_\ncumsum_explained_var_percent = np.concatenate((np.array([0]), np.cumsum(explained_var_percent)))\ndot_selected_ind = 3\n\nplt.close('all')\nplt.figure(figsize=(10,7));\nplt.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')\nplt.scatter(dot_selected_ind, cumsum_explained_var_percent[dot_selected_ind+1], c='r', s=200)\nplt.xlabel('num basis functions', fontsize=16); plt.ylabel('explained %s' %('%'), fontsize=16);\nplt.title('SVD cumulative explained percent \\ntotal variance explained = %.2f%s' %(cumsum_explained_var_percent[dot_selected_ind+1],'%'), fontsize=18);\nplt.ylim(-1,105); plt.xlim(-1,num_basis_functions+1);\nplt.xlim(-0.3,12);","d53638fa":"#%% Apply NMF\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\n# to avoid numberic instability, replicate the data and add some noise\nnoisy_data_for_NMF = np.tile(X,[3,1])\nnoisy_data_for_NMF = noisy_data_for_NMF + 0.0 * np.random.rand(noisy_data_for_NMF.shape[0], noisy_data_for_NMF.shape[1])\n\nPSP_NMF_model = NMF(n_components=20)\nPSP_NMF_model.fit(noisy_data_for_NMF)\n\nNMF_cutoff_ind = 3\nmax_basis_to_present = 10\n\n# normalize each basis vector to it's maximum (for presentation)\nNMF_basis = PSP_NMF_model.components_\nNMF_basis_norm = NMF_basis \/ np.tile(NMF_basis.max(axis=1, keepdims=True), [1, NMF_basis.shape[1]])\n\nplt.close('all')\nplt.figure(figsize=(25,20));\nplt.subplot(3,1,1); plt.imshow(NMF_basis_norm[:max_basis_to_present]);\nplt.title('normlized synaptic filters as heatmaps', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('synaptic filter index', fontsize=22); plt.xlim(0,time_limit_ms);\nplt.subplot(3,1,2); plt.plot(NMF_basis_norm[:NMF_cutoff_ind].T);\nplt.title('first 4 basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);\nplt.subplot(3,1,3); plt.plot(NMF_basis_norm[NMF_cutoff_ind:max_basis_to_present].T);\nplt.title('rest of the basis functions', fontsize=22); plt.xlabel('time [ms]', fontsize=22); plt.ylabel('normalized PSP (A.U)', fontsize=22); plt.xlim(0,time_limit_ms);\n","5b85cdaf":"#%% load file for capacity plot\n\nresults_filename = data_folder + 'FF_vs_IF_capacity_comparision__num_axons_200__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'\n\nloaded_script_results_dict = pickle.load(open(results_filename, \"rb\" ))\n\nprocessed_res_curves = loaded_script_results_dict['processed_res_curves']\nall_results_curves   = loaded_script_results_dict['all_results_curves']\n\nnum_axons = loaded_script_results_dict['script_main_params']['num_axons']\nstimulus_duration_sec = loaded_script_results_dict['script_main_params']['stimulus_duration_sec']\n\nfilename_str = 'FF_vs_IF_capacity_comparision__num_axons_%d__sim_duration_sec_120__num_mult_conn_6__rand_rep_18.pickle'\nnum_axons_list = sorted([100, 112, 125, 137, 150, 162, 175, 187, 200, 212, 225, 237])\nall_filenames_str = [filename_str %(x) for x in num_axons_list]\n\nmodel_keys = list(loaded_script_results_dict['processed_res_curves'].keys())\nconnections_per_axon_2C = loaded_script_results_dict['processed_res_curves'][model_keys[0]]['connections_per_axon']\n\nprecisely_timed_spikes_per_axon_2C = {}\nprecisely_timed_spikes_per_axon_error_2C = {}\nfor key in model_keys:\n    precisely_timed_spikes_per_axon_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))\n    precisely_timed_spikes_per_axon_error_2C[key] = np.zeros((len(all_filenames_str), len(connections_per_axon_2C)))\n\nfor k, (curr_num_axons, curr_filename) in enumerate(zip(num_axons_list, all_filenames_str)):\n    curr_results_filename = data_folder + curr_filename\n    curr_loaded_results_dict = pickle.load(open(curr_results_filename, \"rb\" ))\n\n    for key in model_keys:\n        precisely_timed_spikes_per_axon_2C[key][k,:] = curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'] \/ curr_num_axons\n\n        for j, (num_M_conn, num_spikes) in enumerate(zip(curr_loaded_results_dict['processed_res_curves'][key]['connections_per_axon'],\n                                                         curr_loaded_results_dict['processed_res_curves'][key]['num_almost_perfectly_placed_spikes'])):\n\n            model_connections_str = '%s, %d connections' %(key, num_M_conn)\n            error_index = list(curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes']).index(num_spikes)\n            error_scale = (curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][error_index + 1] -\n                           curr_loaded_results_dict['all_results_curves'][model_connections_str]['num_spikes'][max(0, error_index - 1)])\n\n            if error_index > 1:\n                error_scale \/= 2\n\n            precisely_timed_spikes_per_axon_error_2C[key][k,j] = error_scale \/ curr_num_axons\n\n\ncolor_map = {}\ncolor_map['I&F'] = '0.05'\ncolor_map['F&F'] = 'orange'","d6b9e9ce":"#%% Calculate Capacity with optimal PSP profiles (m = 3)\n\noptimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]\nprint(optimal_basis_PSPs.shape)\n\nnum_axons = 3\nnormlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))\n\n#%% Define several new helper functions (including two simulation functions)\n\ndef simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes, synaptic_weights, normlized_syn_filter,\n                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc local currents\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)\n    for k in range(normlized_syn_filter.shape[0]):\n        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]\n\n    # multiply by weights to get the somatic current\n    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\n# use local currents as \"features\" and fit a linear model to the data\ndef prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):\n\n    # remove all \"negative\" time points that are too close to spikes\n    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1\n    desired_timepoints = ~desired_output_spikes_LPF\n\n    # massivly subsample the remaining timepoints\n    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0\n    desired_timepoints[desired_output_spikes > 0.1] = 1\n\n    X = local_normlized_currents.T[desired_timepoints,:]\n    y = desired_output_spikes[desired_timepoints]\n\n    return X, y\n\n\n#%% check if desired number of spikes is better than desired AUC score\n\nrequested_number_of_output_spikes = 93\n\noptimal_basis_PSPs = NMF_basis_norm[:NMF_cutoff_ind]\n\n# input parameters\nnum_axons = 200\n\n# neuron model parameters\nconnections_per_axon = NMF_cutoff_ind\nnum_synapses = connections_per_axon * num_axons\n\nv_reset     = -80\nv_threshold = -55\ncurrent_to_voltage_mult_factor = 3\nrefreactory_time_constant = 15\n\nmodel_type = 'F&F optimal'\n\n# synapse learnable parameters\nsynaptic_weights_vec = np.random.normal(size=(num_synapses, 1))\n\n# generate sample input\nstimulus_duration_ms = 90000\ninstantanious_input_spike_probability = 0.004\n\naxons_input_spikes = np.random.rand(num_axons, stimulus_duration_ms) < instantanious_input_spike_probability\npresynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1)), axons_input_spikes)\nnormlized_syn_filter = np.kron(optimal_basis_PSPs, np.ones((num_axons,1)))\n\nassert presynaptic_input_spikes.shape[0] == num_synapses, 'number of synapses doesnt match the number of presynaptic inputs'\n\n# generate desired pattern of output spikes\nmin_time_between_spikes_ms = 90\n\ndesired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms \/ min_time_between_spikes_ms), size=requested_number_of_output_spikes)\ndesired_output_spike_times = np.sort(np.unique(desired_output_spike_times))\n\ndesired_output_spikes = np.zeros((stimulus_duration_ms,))\ndesired_output_spikes[desired_output_spike_times] = 1.0\n\nprint('number of requested output spikes = %d' %(requested_number_of_output_spikes))\n\n# simulate cell with normlized currents\nlocal_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_PSPs(presynaptic_input_spikes,\n                                                                                                               synaptic_weights_vec, normlized_syn_filter,\n                                                                                                               refreactory_time_constant=refreactory_time_constant,\n                                                                                                               v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\noutput_spikes = np.zeros((stimulus_duration_ms,))\ntry:\n    output_spikes[np.array(output_spike_times_in_ms)] = 1.0\nexcept:\n    print('no output spikes created')\n\n#%% fit linear model to local currents\n\nlogistic_reg_model = linear_model.LogisticRegression(C=100000, fit_intercept=True, penalty='l2', max_iter=3000)\n\nspike_safety_range_ms = 5\nnegative_subsampling_fraction = 0.5\n\nX, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nlogistic_reg_model.fit(X,y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\ny_hat = logistic_reg_model.predict_proba(X)[:,1]\n\n# calculate AUC\ntrain_AUC = roc_auc_score(y, y_hat)\n\nfitted_output_spike_prob = logistic_reg_model.predict_proba(local_normlized_currents.T)[:,1]\nfull_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob)\n\n# get desired FP threshold\ndesired_false_positive_rate = 0.004\n\nfpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob)\n\ndesired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))\nif desired_fp_ind == 0:\n    desired_fp_ind = 1\n\nactual_false_positive_rate = fpr[desired_fp_ind]\ntrue_positive_rate         = tpr[desired_fp_ind]\ndesired_fp_threshold       = thresholds[desired_fp_ind]\n\nAUC_score = auc(fpr, tpr)\n\nprint('AUC = %.4f' %(AUC_score))\nprint('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))\n\noutput_spikes_after_learning = fitted_output_spike_prob > desired_fp_threshold\n","c638e5a4":"#%% Build the final figure\n\nxy_label_fontsize = 16\ntitle_fontsize = 21\n\nplt.close('all')\nfig = plt.figure(figsize=(20,18.5))\ngs_figure = gridspec.GridSpec(nrows=8,ncols=5)\ngs_figure.update(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.6, hspace=0.9)\n\nax_PSP_heatmap     = plt.subplot(gs_figure[ :6, :3])\nax_SVD_heatmap     = plt.subplot(gs_figure[6: , :3])\nax_PSP_traces      = plt.subplot(gs_figure[ :2,3: ])\nax_NMF_trance      = plt.subplot(gs_figure[2:4,3: ])\nax_explained_var   = plt.subplot(gs_figure[4:6,3: ])\nax_n_spikes_m_cons = plt.subplot(gs_figure[6: ,3: ])\n\ninterp_method_PSP = 'spline16'\ninterp_method_SVD = 'bilinear'\ncolormap = 'jet'\n\nax_PSP_heatmap.imshow(normlized_syn_filter_small, cmap=colormap, interpolation=interp_method_PSP);\nax_PSP_heatmap.set_xlim(0,time_limit_ms);\nax_PSP_heatmap.set_title('All PSPs as heatmap', fontsize=title_fontsize)\nax_PSP_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)\nax_PSP_heatmap.set_xticks([0,30,60,90,120])\nax_PSP_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)\nax_PSP_heatmap.set_ylabel('PSP index', fontsize=xy_label_fontsize)\nax_PSP_heatmap.set_yticks([0,24,48,72,96,120])\nax_PSP_heatmap.set_yticklabels([1,25,49,73,97,121], fontsize=xy_label_fontsize)\n\nax_SVD_heatmap.imshow(np.kron(PSP_SVD_model.components_[:max_SVD_basis_to_present], np.ones((2,1))), cmap=colormap, interpolation=interp_method_SVD);\nax_SVD_heatmap.set_xlim(0,time_limit_ms);\nax_SVD_heatmap.set_title('SVD basis functions as heatmap', fontsize=title_fontsize)\nax_SVD_heatmap.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)\nax_SVD_heatmap.set_xticks([0,30,60,90,120])\nax_SVD_heatmap.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)\nax_SVD_heatmap.set_ylabel('Basis function index', fontsize=xy_label_fontsize)\nax_SVD_heatmap.set_yticks([0,9,19,29])\nax_SVD_heatmap.set_yticklabels([1,10,20,30], fontsize=xy_label_fontsize)\n\nax_PSP_traces.plot(normlized_syn_filter_large.T, alpha=0.15);\nax_PSP_traces.set_xlim(-1,time_limit_ms);\nax_PSP_traces.set_title('All PSPs as traces', fontsize=title_fontsize)\nax_PSP_traces.set_ylabel('Magnitude (A.U.)', fontsize=xy_label_fontsize);\nax_PSP_traces.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)\nax_PSP_traces.set_yticks([0.0,0.25,0.50,0.75,1.00])\nax_PSP_traces.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)\nax_PSP_traces.set_xticks([0,30,60,90,120])\nax_PSP_traces.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)\n\nax_NMF_trance.plot(NMF_basis_norm[:NMF_cutoff_ind].T);\nax_NMF_trance.set_xlim(-1,time_limit_ms);\nax_NMF_trance.set_title('NMF first %d basis functions' %(NMF_cutoff_ind), fontsize=title_fontsize)\nax_NMF_trance.set_ylabel('Magnitude  (A.U.)', fontsize=xy_label_fontsize);\nax_NMF_trance.set_xlabel('Time (ms)', fontsize=xy_label_fontsize)\nax_NMF_trance.set_yticks([0.0,0.25,0.50,0.75,1.00])\nax_NMF_trance.set_yticklabels([0.0,0.25,0.50,0.75,1.00], fontsize=xy_label_fontsize)\nax_NMF_trance.set_xticks([0,30,60,90,120])\nax_NMF_trance.set_xticklabels([0,30,60,90,120], fontsize=xy_label_fontsize)\n\nax_explained_var.plot(np.arange(num_basis_functions + 1), cumsum_explained_var_percent, c='k')\n\nax_explained_var.scatter(dot_selected_ind, cumsum_explained_var_percent[NMF_cutoff_ind + 1], c='r', s=200)\nax_explained_var.set_title('Variance explained = %.2f%s' %(cumsum_explained_var_percent[NMF_cutoff_ind + 1],'%'), fontsize=title_fontsize);\nax_explained_var.set_xlabel('Num basis functions', fontsize=xy_label_fontsize);\nax_explained_var.set_ylabel('Explained Percent (%s)' %('%'), fontsize=xy_label_fontsize);\nax_explained_var.set_ylim(-1,115);\nax_explained_var.set_yticks([0,25,50,75,100])\nax_explained_var.set_yticklabels([0,25,50,75,100], fontsize=xy_label_fontsize)\nax_explained_var.set_xlim(-0.3,12);\nax_explained_var.set_xticks([0,3,6,9,12])\nax_explained_var.set_xticklabels([0,3,6,9,12], fontsize=xy_label_fontsize)\n\nfor key in processed_res_curves.keys():\n    y_error = precisely_timed_spikes_per_axon_2C[key].std(axis=0)\n    ax_n_spikes_m_cons.errorbar(connections_per_axon_2C, precisely_timed_spikes_per_axon_2C[key].mean(axis=0), yerr=y_error, label=key, lw=4, color=color_map[key])\n\nax_n_spikes_m_cons.legend(loc='upper left', fontsize=22)\nax_n_spikes_m_cons.set_title('Placing Precisely Timed output Spikes', fontsize=title_fontsize)\nax_n_spikes_m_cons.set_xlabel('Number of Multiple Contacts - M', fontsize=xy_label_fontsize)\nax_n_spikes_m_cons.set_ylabel('Precisely Timed Spikes \/ Axon', fontsize=xy_label_fontsize);\nax_n_spikes_m_cons.spines['top'].set_visible(False)\nax_n_spikes_m_cons.spines['right'].set_visible(False)\nax_n_spikes_m_cons.set_yticks([0.15,0.3,0.45])\nax_n_spikes_m_cons.set_yticklabels([0.15,0.3,0.45], fontsize=xy_label_fontsize)\nax_n_spikes_m_cons.set_xticks([1,2,3,5,10,15])\nax_n_spikes_m_cons.set_xticklabels([1,2,3,5,10,15], fontsize=xy_label_fontsize)\n\n# add the asimptote line\nif AUC_score > 0.99:\n    optimal_const_value = np.ones(connections_per_axon_2C.shape) * requested_number_of_output_spikes \/ num_axons\n    ax_n_spikes_m_cons.plot(connections_per_axon_2C, optimal_const_value, label='Optimal 3 PSPs', ls='dashed', lw=2, color='red')\n    ax_n_spikes_m_cons.scatter(3, optimal_const_value[0], label='Optimal 3 PSPs', s=200, color='red')\n    ax_n_spikes_m_cons.set_ylim(0.09,1.06 * optimal_const_value[0])\n    ax_n_spikes_m_cons.legend(loc='center right', fontsize=17)\n\n# save figure\nif save_figures:\n    figure_name = 'F&F_explanatory_Figure_4_%d' %(np.random.randint(200))\n    for file_ending in all_file_endings_to_use:\n        if file_ending == '.png':\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n        else:\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n","9b5e328a":"save_figures = True\n\n# save figure\nif save_figures:\n    figure_name = 'F&F_explanatory_Figure_4_%d' %(np.random.randint(200))\n    for file_ending in all_file_endings_to_use:\n        if file_ending == '.png':\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n        else:\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')","46f6cef2":"## Calculate Capacity with optimal PSP profiles (M = 3)","f4f028b4":"# Helper functions","f13a6ae1":"# Build the final figure","bd26095c":"## Show variance explained","7fa543d3":"# Apply NMF amd display","1158a556":"## Apply SVD and display","7c0411d7":"## Script params","1785ca37":"## Load file for capacity plot","16621967":"## Create all possible combinations"}}