{"cell_type":{"651bd300":"code","9a68f1ca":"code","fe50c3ef":"code","7fb900f9":"code","7242495f":"code","4aed1cbb":"code","20412951":"code","df0d900e":"code","7dca9384":"code","8e14e2fc":"code","4190d83f":"code","c9d72097":"code","3196caec":"code","ad8aafde":"code","ee1e0e0d":"code","c0293e49":"code","bc02dd96":"code","d9218007":"code","50e69afd":"code","c3a51993":"code","72dff73d":"code","3cbedaf6":"code","5d4a80f0":"markdown","e1a6561b":"markdown","3a208761":"markdown","5a01aa98":"markdown","a4809aa6":"markdown","9f235480":"markdown","12635091":"markdown","823f4fbd":"markdown","f15091a3":"markdown","be82ce15":"markdown","bcca9bba":"markdown","8bb21ade":"markdown","1b642639":"markdown","bbb4a08e":"markdown","2ffd4b8a":"markdown"},"source":{"651bd300":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px \nsns.set_style('darkgrid')","9a68f1ca":"data = pd.read_csv('..\/input\/SolarEnergy\/SolarPrediction.csv')\nprint(data.shape)\ndata.head()","fe50c3ef":"data.describe()","7fb900f9":"fig, ax = plt.subplots(figsize=(20, 6))\nsns.heatmap(data.isnull(), cbar=False, yticklabels=False)","7242495f":"data['Date'] = pd.to_datetime(data['Data']).dt.date.astype(str)\ndata['TimeSunRise'] = data['Date'] + ' ' + data['TimeSunRise']\ndata['TimeSunSet'] = data['Date'] + ' ' + data['TimeSunSet']\ndata['Date'] = data['Date'] + ' ' + data['Time']\n\ndata = data.sort_values('Date').reset_index(drop=True)\ndata.set_index('Date', inplace=True)\ndata.drop(['Data', 'Time', 'UNIXTime'], axis=1, inplace=True)\ndata.index = pd.to_datetime(data.index)\ndata.head()","4aed1cbb":"data.rename({\n    'Radiation': 'Radiation(W\/m2)', 'Temperature': 'Temperature(F)', 'Pressure': 'Pressure(mm Hg)', 'Humidity': 'Humidity(%)',\n    'Speed': 'Speed(mph)'\n}, axis=1, inplace=True)\ndata.head()","20412951":"fig, ax = plt.subplots(figsize=(20, 6))\ndata['Radiation(W\/m2)'].plot(ax=ax, style=['--'], color='red')\nax.set_title('Radiation as a Time Series', fontsize=18)\nax.set_ylabel('W\/m2')\nplt.show()","df0d900e":"fig, ax = plt.subplots(figsize=(20, 6))\ndata.groupby(pd.Grouper(freq=\"D\"))['Radiation(W\/m2)'].mean().plot(ax=ax, style=['--'], color='red')\nax.set_title('Radiation as a Time Series (Daily)', fontsize=18)\nax.set_ylabel('W\/m2')\nplt.show()","7dca9384":"for col in ['Radiation(W\/m2)','Temperature(F)', 'Pressure(mm Hg)', 'Humidity(%)', 'WindDirection(Degrees)', 'Speed(mph)']:\n    fig, ax = plt.subplots(figsize=(20, 3))\n    data[col].plot.box(ax=ax, vert=False, color='red')\n    ax.set_title(f'{col} Distrubution', fontsize=18)\n    plt.show()","8e14e2fc":"fig = plt.figure()\nfig.suptitle('Feature Correlation', fontsize=18)\nsns.heatmap(data.corr(), annot=True, cmap='RdBu', center=0)","4190d83f":"def total_seconds(series):\n    return series.hour*60*60 + series.minute*60 + series.second","c9d72097":"data['MonthOfYear'] = data.index.strftime('%m').astype(int)\ndata['DayOfYear'] = data.index.strftime('%j').astype(int)\ndata['WeekOfYear'] = data.index.strftime('%U').astype(int)\ndata['TimeOfDay(h)'] = data.index.hour\ndata['TimeOfDay(m)'] = data.index.hour*60 + data.index.minute\ndata['TimeOfDay(s)'] = total_seconds(data.index)\ndata['TimeSunRise'] = pd.to_datetime(data['TimeSunRise'])\ndata['TimeSunSet'] = pd.to_datetime(data['TimeSunSet'])\ndata['DayLength(s)'] = total_seconds(data['TimeSunSet'].dt) - total_seconds(data['TimeSunRise'].dt)\ndata['TimeAfterSunRise(s)'] = total_seconds(data.index) - total_seconds(data['TimeSunRise'].dt)\ndata['TimeBeforeSunSet(s)'] = total_seconds(data['TimeSunSet'].dt) - total_seconds(data.index)\ndata['RelativeTOD'] = data['TimeAfterSunRise(s)'] \/ data['DayLength(s)']\ndata.drop(['TimeSunRise','TimeSunSet'], inplace=True, axis=1)\ndata.head()","3196caec":"fig, ax = plt.subplots(4, 2, figsize=(20, 20))\nfor j, timeunit in enumerate(['MonthOfYear', 'TimeOfDay(h)']):\n    grouped_data=data.groupby(timeunit).mean().reset_index()\n    palette = sns.color_palette(\"YlOrRd\", len(grouped_data))\n    for i, col in enumerate(['Radiation(W\/m2)', 'Temperature(F)', 'Pressure(mm Hg)', 'Humidity(%)']):\n        sns.barplot(data=grouped_data, x=timeunit, y=col, ax=ax[i][j], palette=palette)\n        ax[i][j].set_title(f'Mean {col} by {timeunit}', fontsize=12)\n        range_values = grouped_data[col].max() - grouped_data[col].min()\n        ax[i][j].set_ylim(max(grouped_data[col].min() - range_values, 0), grouped_data[col].max() + 0.25*range_values)","ad8aafde":"fig = plt.figure(figsize=(20, 12))\nfig.suptitle('Feature Correlation', fontsize=18)\nsns.heatmap(data.corr(), annot=True, cmap='RdBu', center=0)","ee1e0e0d":"feats = [\n    'Temperature(F)', 'Pressure(mm Hg)', 'Humidity(%)', 'WindDirection(Degrees)', 'Speed(mph)', \n    'MonthOfYear','DayOfYear', 'RelativeTOD',\n]\nX = data[feats].values\ny = data['Radiation(W\/m2)'].values\n\nprint(X.shape)","c0293e49":"from sklearn.model_selection import KFold, RandomizedSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nkf = KFold(shuffle=True, random_state=19)","bc02dd96":"scores = []\nrmse = []\nmae = []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    model = DummyRegressor(strategy='mean').fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n    rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n    \nprint('Mean R2 Score:', round(np.mean(scores), 5))\nprint('Mean RMSE:', round(np.mean(rmse), 5))\nprint('Mean MAE:', round(np.mean(mae), 5))","d9218007":"%%time\n\nscores = []\nrmse = []\nmae = []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    dtmodel = DecisionTreeRegressor(random_state=19).fit(X_train, y_train)\n    scores.append(dtmodel.score(X_test, y_test))\n    rmse.append(np.sqrt(mean_squared_error(y_test, dtmodel.predict(X_test))))\n    mae.append(mean_absolute_error(y_test, dtmodel.predict(X_test)))\n    \nprint('Mean R2 Score:', round(np.mean(scores), 5))\nprint('Mean RMSE:', round(np.mean(rmse), 5))\nprint('Mean MAE:', round(np.mean(mae), 5))","50e69afd":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor, XGBRFRegressor\nfrom catboost import CatBoostRegressor\n\ntrees = {\n    'RandomForest': RandomForestRegressor(random_state=19), 'ExtraTrees': ExtraTreesRegressor(random_state=19),\n    'GradientBoosting': GradientBoostingRegressor(random_state=19), 'LightGBM': LGBMRegressor(random_state=19),\n    'XGBoost': XGBRegressor(random_state=19), 'XGBoostRF': XGBRFRegressor(random_state=19), \n    'CatBoost': CatBoostRegressor(random_state=19, silent=True)\n}","c3a51993":"%%time\n\nperformance = {'rmse':[], '100* r2':[], 'mae':[]}\nfor name, model in trees.items():\n    scores = []\n    rmse = []\n    mae = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model = model.fit(X_train, y_train)\n        scores.append(100*model.score(X_test, y_test))\n        rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n        mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n    performance['100* r2'].append(np.mean(scores))\n    performance['rmse'].append(np.mean(rmse))\n    performance['mae'].append(np.mean(mae))","72dff73d":"fig = px.bar(pd.DataFrame(performance, index=trees.keys()), barmode='group', title='Model Comparison')\nfig.show()","3cbedaf6":"feat_imp = {\n    k: trees[k].feature_importances_ for k, v in trees.items()\n}\nfeat_imp['DecisionTree'] = dtmodel.feature_importances_\nfeat_imp = pd.DataFrame(feat_imp)\n\nfeat_imp \/= feat_imp.sum()\nfeat_imp.index = feats\n\nfig, ax= plt.subplots(figsize=(20, 6))\nfig.suptitle('Feature Importance', fontsize=18)\npd.DataFrame(feat_imp).plot.bar(ax=ax, color=sns.color_palette(\"summer\", 8))","5d4a80f0":"**Checking Missing Values**","e1a6561b":"* As derived from visualizations temperature is the most important feature\n* Relative Time of the day, is also a key feature","3a208761":"**Feature Analysis**","5a01aa98":"**Decision Tree**","a4809aa6":"* Solar radiation is positively correlated with temperature\n* Atmospheric Pressure and Humidity are correlated with each other\n* Temperature plots are as expected bell shaped peaked at 12 noon\n* Slight decrease in temperature and solar radiation as winter arrives","9f235480":"**Feature Importance Analysis**","12635091":"* Extra Trees seems to work best for our purpose\n    * Maximum R2 score and Minimum RMSE and MAE\n* Other good models are Random Forest and Cat Boost","823f4fbd":"# Trees That Determine Solar Radiation\n\nIn the 1600s, it was discovered that trees (plants) use solar radiation (sunlight) to make their food. Ever thought about some trees that could determine the amount of solar radiation at any time...\n\nYes I'm talking about Decision Trees, A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification or regression rules.\n\n![img](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/decision-tree-classification-algorithm.png)\n\n\n## Understanding the Problem and Data\n\nSolar irradiance is the power per unit area received from the Sun in the form of electromagnetic radiation as reported in the wavelength range of the measuring instrument. The solar irradiance is measured in watt per square metre (W\/m<sup>2<\/sup>) in SI units. Solar irradiance is often integrated over a given time period in order to report the radiant energy emitted into the surrounding environment (joule per square metre, J\/m<sup>2<\/sup>) during that time period. This integrated solar irradiance is called solar irradiation, solar exposure, solar insolation, or insolation.\n\n![img](https:\/\/www.newport.com\/medias\/sys_master\/images\/images\/hef\/hb0\/8798462345246\/LS-158b-400w.gif)\n\nThe dataset includes observations of:\n\n- Solar Irradiance (W\/m<sup>2<\/sup>)\n- Temperature (&deg;F)\n- Barometric Pressure (Hg)\n- Humidity (%)\n- Wind Direction (&deg;)\n- Wind Speed (mph)\n- Sun Rise\/Set Time\n\nIt contains measurements for the 4 months (2016-09-01 to 2016-12-31) [Pacific\/Honolulu] and you have to predict the level of solar radiation.","f15091a3":"**Radiation as a time series**","be82ce15":"**Tree Ensembles**\n\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\n1. **Random Forest**: A random forest is a meta estimator that fits a number of decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n\n2. **Extra Trees**: In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.\n\n3. **Gradient Boosting**: Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. The gradient boosting algorithm begins by training a decision tree in which each observation is assigned an equal weight. After evaluating the first tree, we increase the weights of those observations that are difficult to fit and lower the weights for those that are easy to fit. The second tree is therefore grown on this weighted data. Here, the idea is to improve upon the predictions of the first tree.\n\n4. **Light GBM**: Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithms grow level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\n5. **XG Boost**: XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n\n6. **XG Boost RF** XGBoost RF is an optimized distributed gradient boosting library combining the features of random forests with gradient boosting.\n\n7. **Cat Boost**: \u201cCatBoost\u201d name comes from two words \u201cCategory\u201d and \u201cBoosting\u201d. For fitting a model on some data generally, we are required to convert categorical data into the numerical format. using several pre-processing methods like \u201clabel encoding\u201d, \u201cone hot encoding\u201d and others. But catboost can use categorical features directly and is scalable in nature.","bcca9bba":"# Exploring the data\n\n**Parsing date time data**","8bb21ade":"**Baseline Model**","1b642639":"# Modelling","bbb4a08e":"**Feature Distribution**","2ffd4b8a":"**Feature Extraction**"}}