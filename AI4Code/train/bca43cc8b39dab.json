{"cell_type":{"a9f25ee3":"code","7d287be3":"code","9d67caa2":"code","afd2f808":"code","cbbdf193":"markdown"},"source":{"a9f25ee3":"from sklearn.datasets import load_breast_cancer\n#from sklearn.datasets import load_iris\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport graphviz \nimport matplotlib.pyplot as plt\n\ndata = load_breast_cancer()\nprint(f\"Feature names: {data.feature_names}\")\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nprint(f\"first data point in the training set: {X_train[0]}\\nlabel: {y_train[0]}\")\n\n#decision tree built using the training data \n#DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, ..., ccp_alpha=0.0)\n#criterion{\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d\n#split made by the best feature within a random selected subset of features \nclf = tree.DecisionTreeClassifier(random_state=0)\nclf = clf.fit(X_train, y_train)\n\n#test the predictor using the first test case\nq = [X_test[1]]\nprint(\"query\", q)\nprint(\"prediction\", clf.predict(q))\n\n#pruning the tree using different alphas, save the pruned trees to clfs\n#clfs[0] -- the original tree without pruning; clfs[-1] -- the last tree with only one single node\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))\n#remove the last tree with only one node\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\n#compute the accuracy over training dataset and the test dataset\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n","7d287be3":"#show the accuracy versus alpha (for pruning)\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","9d67caa2":"#visualize the pruned tree\nmax_test_accuracy = test_scores[0]\nclf_select = 0\nfor idx in range(1, len(clfs)):\n    if test_scores[idx] > max_test_accuracy:\n            max_test_accuracy = test_scores[idx]\n            clf_select = idx\nprint(\"prunned tree, accuracy \", max_test_accuracy)\n#print(\"alpha \", ccp_alpha[clf_select])\n\n#prepare dot file for graphviz\ndot_data = tree.export_graphviz(clfs[clf_select], out_file=None, \n                      feature_names=data.feature_names,  \n                      class_names=data.target_names,  \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph \n","afd2f808":"#the original tree is a lot more complex\n#prepare dot file for graphviz\ndot_data = tree.export_graphviz(clfs[0], out_file=None, \n                      feature_names=data.feature_names,  \n                      class_names=data.target_names,  \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph \n\n","cbbdf193":"# Demonstration of decision tree using breast cancer dataset\n* sklearn tree for building the predictor\n* minimal cost-complexity pruning (complexity parameter alpha)\n\n    the expected misclassification rate (estimated using the resubstitution error rate) of a tree $R(T)=\\sum_{t \\in T} r(t) p(t)$, where p(t) is the fraction of data points coming into node t, and r(t) is the probability of making a wrong classification for points in node t. For a point in a given leaf node t, the estimated probability of misclassification is 1 minus the probability of the majority class in node t based on the training data.\n    \n    weighted misclassification rate for the parent node is guaranteed to be greater or equal to the sum of the weighted misclassification rates of the left and right child nodes\n    \n    if we simply minimize the resubstitution error rate, we would always prefer a bigger tree, which could cause overfitting.\n    \n    let $\\alpha$ be a real number called the complexity parameter and define the cost-complexity measure as: $R_\\alpha(T)=R(T)+\\alpha |T|$, where $|T|$ is the number of terminal or leaf nodes in T\n    \n    if $\\alpha=0$ then the biggest tree will be chosen because the complexity penalty term is essentially dropped. As $\\alpha$ approaches infinity, the tree of size 1, i.e., a single root node, will be selected.\n    \n* graphviz for visualization of the decision tree\n* [Ref 1](https:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_cost_complexity_pruning.html); [Ref 2](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#minimal-cost-complexity-pruning); [Ref 3](https:\/\/online.stat.psu.edu\/stat508\/lesson\/11\/11.8\/11.8.2)"}}