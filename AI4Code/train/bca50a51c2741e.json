{"cell_type":{"9df045db":"code","073811b9":"code","352590a1":"code","ada83c5c":"code","111ac73a":"code","e2c907e9":"code","5d8c1936":"code","f06891d5":"code","abe872e0":"code","ba7ccdde":"code","0082771f":"code","95c1ca22":"code","73a87a7c":"code","1433b1c9":"code","3fb3d988":"code","20094460":"code","b22690ed":"code","ca71e8d7":"code","b036b521":"code","917e270d":"code","dec074d8":"code","8716cb8d":"code","8a833bb6":"code","e718b95f":"code","31f348fe":"code","daa25eee":"code","8227b434":"code","971ef418":"code","1f617604":"code","96dc861e":"code","21c65790":"code","18fef28a":"code","0e660a90":"code","5f489f5a":"code","2b2c36f0":"code","342e88a5":"code","53acca1b":"code","766cdbb9":"code","3fe8989b":"code","573f04dc":"code","219dba63":"code","87b7979e":"code","cd8db551":"code","90f63158":"code","8d922364":"code","d32210ab":"code","aa032573":"code","f8d86bcc":"code","eb9a1969":"code","5a00afe4":"code","4595a84a":"code","5e60c92c":"code","9f713c2b":"code","85609d82":"code","6d1da47c":"code","6afe8901":"code","ce02df4f":"code","cad29f7f":"code","25de5e58":"code","cce9e5ac":"code","30e0a61f":"code","112efe31":"code","f50deb91":"code","5ee55f18":"code","066eae52":"code","c23f132f":"code","e1226ab2":"code","477c8f62":"code","38cdde61":"code","53a59c28":"code","77fb8b3a":"code","d1539104":"code","ebba0632":"code","21648f0d":"code","ac0852fc":"code","704c9195":"code","75e68101":"code","0875702c":"code","3a5389a8":"code","c3c4597b":"code","652d2441":"code","1e6e0af4":"code","9a6451a5":"code","0ddcd50f":"code","b7d778ce":"code","064b9603":"code","7e8c9e85":"code","4499bc99":"code","5c07d743":"code","2c90f694":"code","80e8677c":"code","ddd68fd0":"code","1062e70b":"code","158de9ec":"code","fb66b487":"code","7b0e1edf":"code","49ad1f4e":"code","3c969ce0":"code","bec1c8db":"code","783337de":"code","df6ac4b0":"code","b311af7d":"code","f569fbce":"code","0562a13b":"code","513c746b":"code","25ea0424":"code","8133b811":"code","a762c39b":"code","a1b3f013":"code","f52995b3":"code","af0f9a30":"code","82160ac5":"code","eefea457":"code","2cc01a6d":"code","a599e495":"code","63cdf204":"code","327a6c7e":"code","d01aa1e7":"code","0951af61":"code","dafcc0ea":"code","f96121d2":"code","883ef863":"code","0752d24f":"code","82cc93ee":"code","0611e617":"code","4cf38598":"code","1eca711f":"code","7173aa77":"code","63d1055b":"markdown","e0b8ac2e":"markdown","0e952836":"markdown","4a4673d4":"markdown","6fcc6621":"markdown","64394877":"markdown","e1ef4786":"markdown","ac152999":"markdown","0e7de68a":"markdown","7e20a2b9":"markdown","edf46666":"markdown","a1d4d692":"markdown"},"source":{"9df045db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","073811b9":"import pandas as pd","352590a1":"df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","ada83c5c":"df.shape","111ac73a":"# Read the train.csv file.\n# usecols is used to load specific columns into dataframe.\n# dropna() method allows to remove rows and column with null\/nan values.\ndf=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"])","e2c907e9":"# Print No. of rows and columns.\ndf.shape","5d8c1936":"#sum of all null values in each column\ndf.isnull().sum()","f06891d5":"# count of each value\ndf['LotFrontage'].value_counts(dropna=False)","abe872e0":"# replace the nan values with the mean of the column\ndf['LotFrontage'].fillna((df['LotFrontage'].mean()),inplace=True)","ba7ccdde":"df['LotFrontage'].value_counts(dropna=False)","0082771f":"# Print the rows.\ndf.head(20)\n","95c1ca22":"# Print a concise summary of a DataFrame.\ndf.info()","73a87a7c":"# Print unique values present in each column.\n# str.format() is one of the string formatting methods in Python3, which allows multiple substitutions and value formatting. This method lets us concatenate elements within a string through positional formatting.\n# Syntax used here: { } { } .format(value1, value2)\n# unique() method is used to know all type of unique values in each column.\nfor i in df.columns:\n    print(\"Column name {} and unique values are {}\".format(i,len(df[i].unique())))","1433b1c9":"# Today's date\nimport datetime\ndatetime.datetime.now().year","3fb3d988":"# How old is the building.\ndf['Total Years']=datetime.datetime.now().year- df['YearBuilt']","20094460":"# remove the Yearbuilt column.\n# axis = 1 refers to column.\n# inplace = True , the data is modified in place, which means it will return nothing and the dataframe is now updated. When inplace = False , which is the default, then the operation is performed and it returns a copy of the object. You then need to save it to something.\ndf.drop(\"YearBuilt\", axis= 1, inplace = True)","b22690ed":"df.head()","ca71e8d7":"# Print column names\ndf.columns","b036b521":"# Creating Categorical features.\ncategorical_features=[\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]\nout_feature=\"SalePrice\"","917e270d":"# print the uniques value of each categorical_features.\nprint(df['MSSubClass'].unique(),\ndf['MSZoning'].unique(),\ndf['Street'].unique(),\ndf['LotShape'].unique())","dec074d8":"# For example- Use label encoding for MSSubClass.\nfrom sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\n# label_encoder object knows how to understand word labels. \nlbl_encoders[\"MSSubClass\"]=LabelEncoder()\n# Encode labels in column 'MSSubclass'.\nlbl_encoders[\"MSSubClass\"].fit_transform(df[\"MSSubClass\"])","8716cb8d":"lbl_encoders","8a833bb6":"from sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\n# Apply Label encoding for every faeature in categorical feature list.\nfor feature in categorical_features:\n    lbl_encoders[feature]=LabelEncoder()\n    df[feature]=lbl_encoders[feature].fit_transform(df[feature])","e718b95f":"# Print dataframe\ndf","31f348fe":"# Stacking and Converting Into Tensors.\n# numpy.stack() function is used to join a sequence of same dimension arrays along a new axis.The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.\n# Syntax : numpy.stack(arrays, axis)\ncategorical_features=np.stack([df['MSSubClass'],df['MSZoning'],df['Street'],df['LotShape']],1)\ncategorical_features","daa25eee":"# Convert numpy to Tensors\nimport torch\ncategorical_features=torch.tensor(categorical_features,dtype=torch.int64)\ncategorical_features","8227b434":"categorical_features.shape","971ef418":"# create continuous variable\n# Create a null list.\ncont_features=[]\nfor i in df.columns:\n    if i in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\",\"SalePrice\"]:\n        # SalePrice is also included cause it is a dependent feature.\n        # The pass statement in Python is used when a statement is required syntactically but you do not want any command or code to execute. The pass statement is a null operation; nothing happens when it executes.\n        pass\n    else:\n        cont_features.append(i)","1f617604":"cont_features","96dc861e":"# Stacking continuous variable into a numpy array and coverting it into a tensor.\ncont_values=np.stack([df[i].values for i in cont_features],axis=1)\ncont_values=torch.tensor(cont_values,dtype=torch.float)\ncont_values","21c65790":"cont_values","18fef28a":"cont_values.dtype","0e660a90":"# Dependent Feature.\n# reshape the tensor otherwise it will give 1D tensor.\ny=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\ny","5f489f5a":"df.info()","2b2c36f0":"categorical_features.shape,cont_values.shape,y.shape","342e88a5":"# Categorical dimension - the array contains no. of unique values for each feature.\ncat_dims=[len(df[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]","53acca1b":"# Print the categorical dimension.\ncat_dims","766cdbb9":"#The rule of thumb for determining the embedding size is the cardinality(no. of unique vaues) size divided by 2, but no bigger than 50.\n# For exampke: We have 7 days of the week but on looking at DayOfWeek cardinality is 8 the extra cardinality is added just incase there is an unknown in the test set, divide that by 2, thus the 4 random numbers. Even if there were no missing values in the original data, you should still set aside one for unknown just in case.\n# embedding_dim - size of embedding for categorical variable.\n# By having higher dimensionality vector rather than just a single number, it gives the deep learning network a chance to learn these rich representations.\nembedding_dim= [(x, min(50, (x+1 ) \/\/ 2)) for x in cat_dims]","3fe8989b":"#Print the embedding dimension.\nembedding_dim","573f04dc":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# nn.Embedding - This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\nembed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n# embedding layer \nembed_representation","219dba63":"# print label encoded categorical feature.\ncategorical_features","87b7979e":"# display.max_rows:\n#       Displays the default number of value. Interpreter reads this value and displays the rows with this value as upper limit to display.\npd.set_option('display.max_rows', 500)\n# Create a null list. It will store the indices of a value of columns of tensor. \nembedding_val=[]\n# Here i represent the column of tensor. In categorical_feature each column represent its respective feature.\n# 'e' coverts the input size of embedding layer to the require output size.\n#Enumerate() method adds a counter to an iterable and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method.\nfor i,e in enumerate(embed_representation):\n    embedding_val.append(e(categorical_features[:,i]))","cd8db551":"cat_featuresz=categorical_features[:4]\ncat_featuresz","90f63158":"# Print the embedded values.\nembedding_val","8d922364":"# Arrange the 4 tensors in one tensor in 4 column. Concatenating the 4 tensors.\nz = torch.cat(embedding_val, 1)\nz","d32210ab":"# Implement dropupout\ndroput=nn.Dropout(.4)","aa032573":"# Perform the dropout on concatenated tensor i.e z\nfinal_embed=droput(z)\nfinal_embed","f8d86bcc":"seed = 20\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","eb9a1969":"torch.cuda.is_available()\n","5a00afe4":"torch.cuda.get_device_name()","4595a84a":"torch.cuda.device_count()","5e60c92c":"torch.cuda.current_device()\n","9f713c2b":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","85609d82":"# Create a Feed Forward Neural Network\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# Define a class for model.\nclass FeedForwardNN(nn.Module):\n    \n    # function contains embedding dimension, no. of continuous values, output, layers and dropout value.\n    def __init__(self, embedding_dim, n_cont, out_sz, layers, p=0.5):\n        super().__init__()\n        # Create a module list for embedding layers.\n        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist = []\n        # n_emb = embedding dimension, n_cont = no.of continuous values.\n        n_emb = sum((out for inp,out in embedding_dim))\n        # n_in = input.\n        n_in = n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i)) \n            layerlist.append(nn.ReLU(inplace=True))\n            # Since our input is a 1D array we will use BatchNorm1d class present in the Pytorch nn module.\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in = i\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n            \n        self.layers = nn.Sequential(*layerlist)\n    \n    def forward(self, x_cat, x_cont):\n        embeddings = []\n        # emedding the categorical features.\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings, 1)\n        x = self.emb_drop(x)\n        \n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x, x_cont], 1)\n        x = self.layers(x)\n        return x\n","6d1da47c":"cont_features","6afe8901":"torch.manual_seed(100)\n# h1 = 100 neurons , h2 = 50 neurons\nmodel=FeedForwardNN(embedding_dim,len(cont_features),1,[ 100,50],p=0.1).to(DEVICE)","ce02df4f":"cont_features","cad29f7f":"model","25de5e58":"loss_function=nn.MSELoss()\noptimizer=torch.optim.Adam(model.parameters(),lr=0.01)","cce9e5ac":"df.shape","30e0a61f":"cont_values","112efe31":"cont_values.shape","f50deb91":"# train and test split\nbatch_size=1200\n# take 15 % of of train data as test data.\ntest_size=int(batch_size*0.15)\ntrain_categorical=categorical_features[:batch_size-test_size].to(DEVICE)\ntest_categorical=categorical_features[batch_size-test_size:batch_size].to(DEVICE)\ntrain_cont=cont_values[:batch_size-test_size].to(DEVICE)\ntest_cont=cont_values[batch_size-test_size:batch_size].to(DEVICE)\ny_train=y[:batch_size-test_size].to(DEVICE)\ny_test=y[batch_size-test_size:batch_size].to(DEVICE)","5ee55f18":"len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(y_train),len(y_test)","066eae52":"import torch.nn as nn\nepochs= 5000\nfinal_losses=[]\nfor i in range(epochs):\n    i=i+1\n    y_pred=model(train_categorical,train_cont)\n    loss=torch.sqrt(loss_function(y_pred,y_train))### RMSE\n    print(loss)\n    final_losses.append(loss)\n    if i%10==1:\n        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n    #clear old gradient from the last step.\n    optimizer.zero_grad()\n    # compute the derivative of the loss w.r.t parameter \n    loss.backward()\n    # step() updates the parameters\n    optimizer.step()","c23f132f":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(range(epochs), final_losses)\nplt.ylabel('RMSE Loss')\nplt.xlabel('epoch');","e1226ab2":"test_categorical,test_cont = test_categorical.to(DEVICE),test_cont.to(DEVICE)","477c8f62":"y_pred,y_test = y_pred.to(DEVICE),y_test.to(DEVICE)","38cdde61":"# Validate the Test Data\ny_pred=\"\"\nwith torch.no_grad():\n    y_pred=model(test_categorical,test_cont)\n    loss=torch.sqrt(loss_function(y_pred,y_test))\nprint('RMSE: {}'.format(loss))","53a59c28":"y_pred","77fb8b3a":"data_verify=pd.DataFrame(y_test.tolist(),columns=[\"Test\"])","d1539104":"data_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])","ebba0632":"data_predicted","21648f0d":"final_output=pd.concat([data_verify,data_predicted],axis=1)\nfinal_output[\"Difference\"]= final_output['Test']-final_output['Prediction']\nfinal_output.head()","ac0852fc":"# Save the model\ntorch.save(model,'HousePrice.pt')","704c9195":"torch.save(model.state_dict(),'HouseWeights.pt')","75e68101":"# Loading the saved Model\nembs_size=[(15, 8), (5, 3), (2, 1), (4, 2)]\nmodel1=FeedForwardNN(embs_size,5,1,[100,50],p=0.1).to(DEVICE)","0875702c":"model1.load_state_dict(torch.load('HouseWeights.pt'))","3a5389a8":"model1.eval()","c3c4597b":"df_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',usecols=[ \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"])","652d2441":"df_test.shape","1e6e0af4":"df_test.head()","9a6451a5":"df_test.info()","0ddcd50f":"for i in df_test.columns:\n    print(\"Column name {} and unique values are {}\".format(i,len(df_test[i].unique())))","b7d778ce":"df_test.isnull().sum()","064b9603":"df_test['MSSubClass'].value_counts(dropna=False)","7e8c9e85":"df_test['MSSubClass'] = df_test['MSSubClass'].replace(150,160)","4499bc99":"df_test['MSSubClass'].value_counts(dropna=False)","5c07d743":"df_test['MSZoning'].value_counts(dropna=False)","2c90f694":"df_test['MSZoning']=df_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0])","80e8677c":"df_test['LotFrontage'].value_counts(dropna=False)","ddd68fd0":"df_test['LotFrontage'].fillna((df_test['LotFrontage'].mean()),inplace=True)","1062e70b":"df_test['LotFrontage'].value_counts(dropna=False)","158de9ec":"for i in df_test.columns:\n    print(\"Column name {} and unique values are {}\".format(i,len(df_test[i].unique())))","fb66b487":"\nimport datetime\ndatetime.datetime.now().year","7b0e1edf":"df_test['Total Years']=datetime.datetime.now().year-df_test['YearBuilt']","49ad1f4e":"df_test.drop(\"YearBuilt\",axis=1,inplace=True)","3c969ce0":"df_test.columns","bec1c8db":"\ncat_features=[\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]","783337de":"lbl_encoders","df6ac4b0":"print(df_test['MSSubClass'].unique(),\ndf_test['MSZoning'].unique(),\ndf_test['Street'].unique(),\ndf_test['LotShape'].unique())","b311af7d":"from sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\nfor feature in cat_features:\n    lbl_encoders[feature]=LabelEncoder()\n    df_test[feature]=lbl_encoders[feature].fit_transform(df_test[feature])","f569fbce":"df_test","0562a13b":"# Stacking and Converting Into Tensors\ncat_features=np.stack([df_test['MSSubClass'],df_test['MSZoning'],df_test['Street'],df_test['LotShape']],1)\ncat_features","513c746b":"cat_features","25ea0424":"# Convert numpy to Tensors\nimport torch\ncat_features=torch.tensor(cat_features,dtype=torch.int64)\ncat_features","8133b811":"cat_features.to(DEVICE)","a762c39b":"# create continuous variable\ncontinuous_features=[]\nfor i in df_test.columns:\n    if i in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]:\n        pass\n    else:\n        continuous_features.append(i)","a1b3f013":"continuous_features","f52995b3":"# Stacking continuous variable to a tensor\ncontinuous_values=np.stack([df_test[i].values for i in continuous_features],axis=1)\ncontinuous_values=torch.tensor(continuous_values,dtype=torch.float)\ncontinuous_values","af0f9a30":"continuous_values.to(DEVICE)","82160ac5":"# Embedding Size For Categorical columns\ncategorical_dims=[len(df_test[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]","eefea457":"categorical_dims","2cc01a6d":"embedding_dimensions= [(x, min(50, (x + 1) \/\/ 2)) for x in categorical_dims]\n                                        \n                       ","a599e495":"embedding_dimensions","63cdf204":"cat_features,continuous_values = cat_features.to(DEVICE),continuous_values.to(DEVICE)","327a6c7e":"y_pred1=\"\"\nwith torch.no_grad():\n    y_pred1=model1(cat_features,continuous_values)","d01aa1e7":"y_pred1","0951af61":"model1.eval()","dafcc0ea":"len(cat_features),len(continuous_values)","f96121d2":"y_pred1","883ef863":"result=pd.DataFrame(y_pred1.tolist(),columns=[\"SalePrice\"])","0752d24f":"result","82cc93ee":"result['Id'] = np.array(result.index)\n","0611e617":"result['Id'] = result['Id'] + 1461\n","4cf38598":"result = pd.DataFrame(result, columns=['Id', 'SalePrice'])\n","1eca711f":"result","7173aa77":"result.to_csv('submission.csv', columns=['Id', 'SalePrice'], index=False)","63d1055b":"# **Embeddings**\nAn embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models. To overcome the problem of  sparse matrix (many zeros and 1) when it come to large dataset we use embedding.\n\n\n**Embedding layer** -\n\nOne can imagine the Embedding layer as a simple matrix multiplication that transforms words into their corresponding word embeddings OR turns positive integers (indexes) into dense vectors of fixed size.\n Input dimension has to be equal to the number of unique word.\n![](http:\/\/)![image.png](attachment:image.png)","e0b8ac2e":"MSSubClass has 15 unique values repeated all over the MSSubClass record likewise for other columns also.\nThesecolumns has different data type. We need to convert it into categorical features and for that we will use \nlabel encoding.\n\n\n**Label Encoding** refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.\n","0e952836":"Categorical features are label encoded now.","4a4673d4":"We created a numpy array above.","6fcc6621":"Prefer how old is the house i.e Total Years instead of when the house was built i.e YearBuilt.\n","64394877":"# **Neural Network** - \n \n \n A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. Neural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria\n\nTo create a DL model perform all the above steps inside NN. ","e1ef4786":"# **Dropout** - \n\nThe term \u201cdropout\u201d refers to dropping out units (both hidden and visible) in a neural network. Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. Dropout is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks.\n\n# **Batch Normalization** - \n\nBatch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.","ac152999":"The first vector denotes 5 , second denotes 0 , third denotes 5 again and so on.\n\"5\" is distributed into 8 indices according to the embedding layer for first categorical feature i.e (15,8).\nTherefore creating a tensor for each categorical feature.","0e7de68a":"# **Embedding size** for categorical column","7e20a2b9":"**Define loss and optimizer**","edf46666":"From above it seems that MSSubclass, MSZoning , Street and LotShape can be converted to categorical features. Since there unique value count is less than 50.","a1d4d692":"MSSubClass has 16 unique values replace the least count value with one of its close value. So that MSSubClasswill have 15 unique values"}}