{"cell_type":{"eb322b82":"code","2f324f14":"code","09067d0b":"code","4d1e58d5":"code","b03bec0b":"code","be797b79":"code","e91d35ff":"code","287a1e2c":"code","9872f69c":"code","ab3f29a1":"code","d124de43":"code","0940fc58":"code","7f08efe5":"code","5b09f3b5":"code","32d48285":"code","1bf8fde4":"code","a20df958":"code","259ee4e3":"code","39108484":"code","6b0efafe":"code","e05689c0":"code","279a13ea":"code","0fcd5e75":"code","e2a2dcff":"code","57c27110":"code","896b625f":"code","de7ac253":"code","64a418e1":"code","3a596fd4":"code","b7623a35":"code","d1f14861":"code","00ae3638":"code","f1d59f16":"code","261fb9e5":"code","e26c4198":"code","c15f2e33":"code","a455fa60":"code","afa33a80":"code","1e00a72a":"code","a38c8569":"code","05139ff9":"code","28f30073":"code","a38bd03e":"code","05220126":"code","edb5d9d1":"code","9171b385":"code","01072ce6":"markdown","bebe0498":"markdown","416bd8cd":"markdown","024e006f":"markdown","a5f4d9d0":"markdown","612937c2":"markdown","8eae3987":"markdown","1673e57b":"markdown","accf6d35":"markdown","506be908":"markdown","c9cae4ee":"markdown","63a4d85f":"markdown","06bdf79b":"markdown","64523c29":"markdown","a8d72d19":"markdown","255552eb":"markdown","7bf16749":"markdown","98bc8bf5":"markdown","1b0d0cd5":"markdown","40c4b9f6":"markdown","31c95d53":"markdown","6c612015":"markdown","a2f23e57":"markdown","d4a4ecac":"markdown","8578e645":"markdown","45d65a75":"markdown","f244a20e":"markdown","b3676a32":"markdown","8750e135":"markdown"},"source":{"eb322b82":"!pip install pyspark","2f324f14":"import os\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import L2\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09067d0b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4d1e58d5":"!pip install pyspark","b03bec0b":"import os\nfrom collections import OrderedDict\nimport numpy as np7\nimport pandas as pd\nfrom sklearn import preprocessing\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import L2\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window","be797b79":"spark = SparkSession.builder.appName('yelp_dataset').getOrCreate()","e91d35ff":"reviews_sk = spark.read.json('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_review.json')\nbusiness_sk = spark.read.json('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_business.json')\nusers_sk = spark.read.json('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_user.json')","287a1e2c":"print(\"Total number of reviews: {}\".format(reviews_sk.count()))\nprint(\"Total number of business: {}\".format(business_sk.count()))\nprint(\"Total number of users: {}\".format(users_sk.count()))","9872f69c":"city = 'toronto'\nbiz = 'restaurant'","ab3f29a1":"restaurant_sk = business_sk.filter(F.lower((business_sk.city)).contains(city)) \\\n                           .filter(F.lower((business_sk.categories)).contains(biz))\nrestaurant_sk.count()","d124de43":"w = Window().partitionBy().orderBy(\"business_id\")\nrestaurant_sk = restaurant_sk.withColumn('business_cid', F.row_number().over(w) - 1)","0940fc58":"restaurant_sk.select('business_cid', 'business_id', 'name', 'stars').show(5)","7f08efe5":"restaurant_reviews_sk = reviews_sk.join(restaurant_sk.select('business_id', 'business_cid'),\n                                        'business_id',\n                                        'inner')\nrestaurant_reviews_sk.count()","5b09f3b5":"w = Window().partitionBy().orderBy(\"review_id\")\nrestaurant_reviews_sk = restaurant_reviews_sk.withColumn('review_cid', F.row_number().over(w) - 1)","32d48285":"restaurant_reviews_sk.select('review_cid', 'review_id',\n                             'business_cid', 'business_id',\n                              'user_id', 'stars').show(5)","1bf8fde4":"# Unique users extraction\nunique_users = restaurant_reviews_sk.select(['user_id']).drop_duplicates()\nrestaurant_users_sk = users_sk.join(unique_users, 'user_id', 'inner')\nrestaurant_users_sk.count()","a20df958":"# ID generation\nw = Window().partitionBy().orderBy(\"user_id\")\nrestaurant_users_sk = restaurant_users_sk.withColumn('user_cid', F.row_number().over(w) - 1)","259ee4e3":"restaurant_users_sk.select(['user_cid', 'user_id']).show(5)","39108484":"# New column in the review dataset for user_cid\nrestaurant_reviews_sk = restaurant_reviews_sk.join(restaurant_users_sk.select('user_id', 'user_cid'),\n                                        'user_id',\n                                        'inner')\nrestaurant_reviews_sk.count()","6b0efafe":"restaurant_reviews_sk.select('review_cid', 'review_id',\n                             'business_cid', 'business_id',\n                             'user_cid', 'user_id', 'stars').show(5)","e05689c0":"review_counts = restaurant_reviews_sk.groupby('user_cid').count().toPandas()\nreview_counts = review_counts.rename(columns={'count': 'review_count'})","279a13ea":"bins = [0, 1, 2, 3, 4, 5, 10, 20, 50, 100, 200, 500, 1000, 5000]\ngroups = pd.cut(review_counts['review_count'], bins)\nreview_counts = review_counts.groupby(groups)['review_count'].agg(['count'])\nreview_counts = review_counts.rename(columns={'count': 'user_count'})","0fcd5e75":"review_counts","e2a2dcff":"df = restaurant_reviews_sk.select(['user_cid', 'business_cid', 'review_cid', 'stars']).toPandas()\ndf.shape","57c27110":"high_reviews = df[df['stars'] >= 4.].groupby('user_cid')['business_cid'] \\\n                                    .count() \\\n                                    .reset_index() \\\n                                    .rename(columns={'business_cid': 'review_count'})","896b625f":"conditions = (df['user_cid'].isin(high_reviews[high_reviews['review_count'] > 10]['user_cid'])) \\\n             & (df['stars'] >= 4)","de7ac253":"def pick_top_reviews(x):\n    factor = 0.20\n    nitems = int(np.floor(factor * x.shape[0]))\n    return x.head(nitems)\n\ndf_test = df[conditions].groupby('user_cid').apply(lambda x: pick_top_reviews(x)).reset_index(drop=True)\ndf_test.shape","64a418e1":"df_train = df[~df['review_cid'].isin(df_test['review_cid'])]\ndf_train.shape","3a596fd4":"df_train['stars_rescale'] = (df_train['stars'] - 1.) \/ 4.\ndf_test['stars_rescale'] = (df_test['stars'] - 1.) \/ 4.","b7623a35":"df_train.head()","d1f14861":"df_test.head()","00ae3638":"n_users = restaurant_users_sk.count()\nn_rests = restaurant_sk.count()\nlist_unique_users = restaurant_users_sk.select(['user_cid']).rdd.flatMap(lambda x: x).collect()\nlist_unique_rests = restaurant_sk.select(['business_cid']).rdd.flatMap(lambda x: x).collect()","f1d59f16":"# Embedding size\nemb_size = 50\n# L2 regularisation coefficient\nl2 = 0.0005\n# Learning rate\nlr = 0.01","261fb9e5":"user_input_lbl = 'User-Input'\nuser_emb_lbl = 'User-Embedding'\nuser_bias_emb_lbl = 'User-Bias-Embedding'\n\nrest_input_lbl = 'Restaurant-Input'\nrest_emb_lbl = 'Restaurant-Embedding'\nrest_bias_emb_lbl = 'Restaurant-Bias-Embedding'","e26c4198":"avg_rating = np.mean(df_train['stars_rescale'])","c15f2e33":"class MFLayer(tf.keras.layers.Layer):\n    def __init__(self, n_users, n_rests, emb_size):\n        super(MFLayer, self).__init__(name='MF-Layer')\n        self.n_users = n_users\n        self.n_rests = n_rests\n        self.emb_size = emb_size\n        \n    # Code adapted from TF implementation for embedding layer\n    # https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.4.1\/tensorflow\/python\/keras\/layers\/embeddings.py\n    def build(self, input_shape):\n        # p\n        self.user_embedding = self.add_weight(\n            shape=(self.n_users, self.emb_size),\n            initializer='uniform',\n            name=user_emb_lbl,\n            regularizer=L2(l2),\n            experimental_autocast=False)\n        \n        # q\n        self.rest_embedding = self.add_weight(\n            shape=(self.n_rests, self.emb_size),\n            initializer='uniform',\n            name=rest_emb_lbl,\n            regularizer=L2(l2),\n            experimental_autocast=False)\n        \n        # b_u\n        self.user_bias_embedding = self.add_weight(\n            shape=(self.n_users, 1),\n            initializer='uniform',\n            name=user_bias_emb_lbl,\n            regularizer=L2(l2),\n            experimental_autocast=False)\n    \n        # b_i\n        self.rest_bias_embedding = self.add_weight(\n            shape=(self.n_rests, 1),\n            initializer='uniform',\n            name=user_bias_emb_lbl,\n            regularizer=L2(l2),\n            experimental_autocast=False)\n        \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'n_users': self.n_users,\n            'n_rests': self.n_rests,\n            'emb_size': self.emb_size,\n        })\n        return config\n    \n    def call(self, user_input, rest_input):\n        u_emb = tf.nn.embedding_lookup(self.user_embedding, user_input)\n        r_emb = tf.nn.embedding_lookup(self.rest_embedding, rest_input)\n        \n        u_bias_emb = tf.nn.embedding_lookup(self.user_bias_embedding, user_input)\n        r_bias_emb = tf.nn.embedding_lookup(self.rest_bias_embedding, rest_input)\n        \n        return tf.math.reduce_sum(tf.math.multiply(u_emb, r_emb), axis=1) \\\n                + avg_rating + u_bias_emb + r_bias_emb","a455fa60":"user_input = layers.Input(shape=(1, ), name=user_input_lbl, dtype=np.int32)\nrest_input = layers.Input(shape=(1, ), name=rest_input_lbl, dtype=np.int32)\n\nmf_layer = MFLayer(n_users, n_rests, emb_size)(user_input, rest_input)\nmodel = tf.keras.models.Model(inputs=[user_input, rest_input], outputs=mf_layer)\n\nmodel.compile(loss='mse', optimizer=SGD(learning_rate=lr), metrics=['RootMeanSquaredError'])","afa33a80":"model.summary()","1e00a72a":"X = {\n    \"User-Input\": tf.convert_to_tensor(df_train['user_cid']),\n    \"Restaurant-Input\": tf.convert_to_tensor(df_train['business_cid'])\n}\n\ny = tf.convert_to_tensor(df_train['stars_rescale'])","a38c8569":"model_path = '\/kaggle\/temp\/yelp.tf'\nload_model = False\n\nif load_model:\n    model.load_weights(model_path)\nelse:\n    model.fit(X, y,batch_size=256, epochs=1)\n    model.save(model_path, overwrite=True)\n","05139ff9":"def predict(model, user_id, item_ids):\n    user_vector = model.get_layer('MF-Layer').get_weights()[0][user_id]\n    item_mat = model.get_layer('MF-Layer').get_weights()[0][item_ids]\n    pred = np.dot(user_vector, item_mat.T)\n    return zip(item_ids, pred)\n\ndef map_k(model, data, items, k=10):\n    ap = []\n    \n    for user, actual in data:\n        preds = dict(predict(model, user, items))\n        preds = sorted(preds.items(), key=lambda kv: kv[1], reverse=True)[:k]\n        preds = list(OrderedDict(preds).keys())\n        \n        score = 0.0\n        num_hits = 0.0\n        \n        for ii, pp in enumerate(preds):\n            if pp in actual:\n                num_hits += 1.0\n                score += num_hits \/ (ii + 1.)\n                \n        score = score \/ min(len(actual), k)\n        ap.append(score)\n        \n    return np.mean(ap)","28f30073":"test = df_test.groupby('user_cid')['business_cid'].agg(list).reset_index()","a38bd03e":"map_k(model, test.values, list_unique_rests, k=5)","05220126":"len(list_unique_users) * len(list_unique_rests)","edb5d9d1":"df.shape","9171b385":"100 * df.shape[0] \/ (len(list_unique_users) * len(list_unique_rests))","01072ce6":"## Training\n<a id='sect_train' \/>","bebe0498":"In this notebook, we use the following hyperparameters for our model:","416bd8cd":"In this notebook, we will implement the [matrix factorisation (MF) method](https:\/\/datajobs.com\/data-science-repo\/Recommender-Systems-[Netflix].pdf) for collaborative filtering (CF) to recommend restaurants to users on Yelp. The method is simple, in which it only takes into account the list of unique users, items and the rating of users for those items. In this model, the only latent factor is the interaction between users and items through the recorded ratings.","024e006f":"In the original dataset, the business ID is written in some hash form, which is hard for us to read and inconvenient for ML purpose. Thus, we generate a new ID in integer form ```restaurant_cid```, starting from 0, for each restaurant.","a5f4d9d0":"By carrying out the same procedure, we will create a similar subset containing only reviews for those restaurants in Toronto:","612937c2":"Normally, for this task, we would use the function ```traint_test_split``` from ```scikit```. However, due to the distinct need of recommender system problem, in which positive items are ranked in higher priority and returned accordingly from the predictor, it is desirable to have a testing set containing only highly\/positively rated items.\n\nFirstly, we will take a look on the distribution of users' reviews, by counting the number of reviews for each user, then grouping them into bins.\n","8eae3987":"Since the dataset is stored in json format and the size is pretty huge, using pandas (or dask) for data transformation will cause long waiting time for tasks to complete or even problems in memory allocation. To avoid such inconvenience, we will use PySpark to deal with this dataset.","1673e57b":"## Navigation\n[Creating Subset](#sect_subset)\n\n[Preparing Data for Training and Testing](#sect_prep_dat)\n\n[Building Model](#sect_model)\n\n[Training](#sect_train)\n\n[Summary](#sect_summary)","accf6d35":"## Testing\n<a id='sect_testing' \/>\n","506be908":"In the matrix factorisation method for CF, the prediction for rating $\\hat{r}_{ui}$ from a user $u$ given to an item $i$ is:\n\n$$\n\\hat{r}_{ui} = p_u^Tq_i + b_u + b_i + \\mu.\n$$\n\nHere, $p_u$ and $q_i$ are latent factor vectors for user $u$ and item $i$ respectively; the parameters $b_u$ and $b_i$ represents deviations of user $u$ and item $i$ from the average rating $\\mu$ of the dataset. Thus, the objective of the system is to minimise the least square problem:\n\n$$\n\\min_{p, q, b} \\sum_{u, i} (r_{ui} - \\hat{r}_ui)^2 + \\lambda (||p_u||^2 + ||q_i||^2 + b_u^2 + b_i^2).\n$$\nHere, $r_{ui}$ is the rating user $u$ gives to item $i$ and $\\lambda$ is the regularisation coefficient.\n\nTo solve this problem with tensorflow, we write a new custom layer, named ```MFLayer``` which accepts two inputs: one for user IDs and the other for restaurant IDs. Inside this layer, the user and restaurant IDs vectors will be embedded into latent factor matrices $p$ and $q$ and the bias terms $b_u$ and $b_i$, respectively.\n","c9cae4ee":"We are now done in creating a working dataset for our recommender system model:","63a4d85f":"## Creating Subset\n<a id='sect_subset' \/>","06bdf79b":"Then, we rescale the columns ```stars``` in both sets for the training purpose:","64523c29":"## Summary\n<a id='sect_summary' \/>","a8d72d19":"Thus, due to the extreme sparsity condition, the reconstruction of interaction matrix cannot yield optimal solutions.","255552eb":"## Building Model\n<a id='sect_model' \/>","7bf16749":"If you want to start training the model from scratch, set the parameter ```load_model``` to ```False```. After training, the weights of the model will be saved to the file defined in ```model_path```. Otherwise, the weights will be loaded from the file given in ```model_path```.","98bc8bf5":"As we can see, the majority of users only contribute one or two reviews. Only about 10% of users are active enough to write more than ten reviews. Surprisingly, we have one hyper-active user who has 1000 reviews under his\/her arm.\n\nBased on this result, we will split the original dataset into training and testing sets as follows:\n* All reviews from those users, who have lesser than ten reviews, will be added to the training set, but not to the testing set.\n* For each active user, who have written more than ten reviews, 20% of positive reviews will be moved to the testing set, while the rest will be comprehended to the training set.","1b0d0cd5":"In this notebook, we implement the matrix factorisation model for collaborative filtering. Even though it is very simple to implement and to understand, the model doesn't take into account many rich features provided from the dataset (e.g. business' attributes and specialty). Moreover, the method doesn't have any built-in mechanism to deal with priority ranking, which is one of the top key performance indicators for a recommender system. Such limitations and the way it handles sparsity lead to low scoring in MAP metric. In the future, I will explore the knowledge graph methodology to tackle these issues.","40c4b9f6":"We assume that a recommender system will only suggest restaurants that are located in the local area of users. In this experiment, we choose Toronto to be our target since the dataset contains a lot of entries for restaurants in Toronto.","31c95d53":"Finally, we create a list of unique users and restaurants for embedding:","6c612015":"... and some string constants for layer naming:","a2f23e57":"Then, we will filter out those users, who dont have any reviews written for any restaurants in Toronto:\n* Extract unique users from the review dataset\n* Assign each user with a new integer ID\n* Add the new ID back to the review dataset as a new column","d4a4ecac":"The MAP of the model MF-CF for the Yelp dataset is very low (even after 30 epochs training on Google Colab, I couldnt get the MAP score more than 0.001. The result surprised me at first. However, after taking into account the dimensions and the number of records, that would have been a miracle if the model would yield a higher score.\n\nRecall that a complete matrix representing the user-restaurant will have the following number of elements:","8578e645":"In reality, we only have the following number of elements from the dataset:","45d65a75":"... which represents less than 1% of the original matrix.","f244a20e":"To test the performance of our model, we use the [MAP@K](http:\/\/sdsawtelle.github.io\/blog\/output\/mean-average-precision-MAP-for-recommender-systems.html) metric. MAP rewards a recommender system if that system shows relevant items **first** in the list. Thus, to obtain a higher MAP score, a system must not only deliver a list of recommended system but also rank which are the best. In this sense, the recommendation problem becomes [learning-to-rank problem](https:\/\/en.wikipedia.org\/wiki\/Learning_to_rank).","b3676a32":"We train our model only in one epoch to maintain a short running time of this notebook on Kaggle.","8750e135":"## Preparing Data for Training and Testing\n<a id='sect_prep_dat' \/>"}}