{"cell_type":{"dcae915b":"code","81f7be78":"code","0bf572ef":"code","7dc53820":"code","649405d9":"code","1ca7f423":"code","97bc00a9":"markdown","1655310d":"markdown","66dda11c":"markdown","e405460f":"markdown","107a5549":"markdown","535eb631":"markdown","894d5bba":"markdown","9a747ce2":"markdown"},"source":{"dcae915b":"#!pip install trax\nimport trax\nimport trax.fastmath.numpy as np\nimport numpy\nimport random as rnd\nfrom trax import fastmath\nfrom trax import layers as tl\nfrom trax import shapes\n\n# set random seed\n\nrnd.seed(32)","81f7be78":"x = np.array([[1, 1, 2, 3, 4],[1, 2, 3 , 4, 5]])\nprint(f\"Some tokenized data:\\n {x} \\n of shape {x.shape}. This is e.g. a batch of 2 sentences with 5 words each after tokenization.\")","0bf572ef":"shifter = tl.ShiftRight(mode=\"train\")\nshifted = shifter(x)\nprint(f\"Right-shifted data:\\n {shifted} \\n of shape {shifted.shape}.\")","7dc53820":"emb_dim = 4\nembed = tl.Embedding(vocab_size=10, d_feature=emb_dim)\n_, _ = embed.init(None)\nembedded = embed(y)\nprint(f\"After embedding each word with a 4-dimentsional embedding we get:\\n {embedded} \\n of shape {embedded.shape} - 2 baches of 5 words, each represented as a 4-dimensional embedding.\")","649405d9":"grucell = tl.GRUCell(n_units=emb_dim)\nembedded_2 = (embedded, embedded)\n_, _ = grucell.init(shapes.signature(embedded_2))\ngrus = grucell(embedded_2)\nprint(f\"Output of the gru cell is \\n\\n {grus} \\n\\nwhich is of type {type(grus)} of lenght {len(grus)}.\")\nprint(f\"Each element of the tuple is of shape {grus[0].shape}.\")","1ca7f423":"grucell_n_2 = tl.GRUCell(emb_dim-2)\nx_and_h = (z, np.ones_like(z[:,:,:2])) # the first element is the input x_t and the second is h_{t-1} which we have now reduced to a dimension of 2 instead of 4 (the embedding dimension)\n_, _ = grucell_n_2.init(shapes.signature(x_and_h))\ngrus_n_2 = grucell_n_2(x_and_h)\ngrus_n_2\nprint(f\"Output of the gru cell is \\n\\n {grus_n_2} \\n\\nwhich is of type {type(grus_n_2)} of lenght {len(grus_n_2)}.\")\nprint(f\"Each element of the tuple is of shape {grus_n_2[0].shape} - different from the previous one which had a shape of {grus[0].shape}.\")","97bc00a9":"# A step by step build-up of the trax GRU model in C3 W2 assignment (Sequence Models in NLP)","1655310d":"We can see that tokens 0, 1, etc are represented by the same 4-dimensional embedding in both batches as it should be.","66dda11c":"## Input some data as fastmath.numpy array.","e405460f":"## 1. Mimic the `shiftRight` layer","107a5549":"Note that the GRU cell returns two copies of the next cell state as documented here: https:\/\/github.com\/google\/trax\/blob\/1372b903bb66b0daccee19fd0b1fdf44f659330b\/trax\/layers\/rnn.py#L128\n\nIf `n_units` does not equal `emb_dim` this GRU cell fails due to this: https:\/\/github.com\/google\/trax\/blob\/1372b903bb66b0daccee19fd0b1fdf44f659330b\/trax\/layers\/rnn.py#L132\n\nIt seems like the GRU cell in trax is \"forced\" to have the same number of units in the hidden state as the embedding dimension. However, this is here by design (in the assignment it is due to the fact that the data generator returns 2 copies of the batch of embeddedings) and can be changed, for example as follows:","535eb631":"## 2. Convert the shifted batches to embeddings with dimension 4.","894d5bba":"## Conclusion\nThe `GRU` class in trax allows for a number of hidden cells to be different from the input dimension (in this case the embedding dimension) but the data generator would need to be adjusted to allow for that.","9a747ce2":"## 3. Run a GRU cell with 2 copies of embedded (as the data generator returns that)."}}