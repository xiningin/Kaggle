{"cell_type":{"926d657f":"code","d7221d40":"code","cc6826ce":"code","472d20c1":"code","b92406fc":"code","275e51e2":"code","fa99f274":"code","dddcf653":"code","eca2430b":"code","1c14950a":"code","7bf54ee3":"code","5f6cc297":"code","ee868151":"code","8cebe8e1":"code","75b56e39":"code","419ce01b":"code","2804ed75":"code","73c82e7b":"code","1c1344e9":"code","6cca59bd":"code","da0768bb":"code","3afde285":"code","e604025e":"code","28094630":"code","ceefe7ac":"code","e8c83b53":"code","49b0691c":"code","65485808":"code","5a60659c":"markdown","b2470b03":"markdown","8e63939b":"markdown","fa80bea8":"markdown","f00d8bb1":"markdown","7f5baf7c":"markdown","197e02ca":"markdown","06c07570":"markdown","45475cd5":"markdown","c2b25f1e":"markdown","bb009427":"markdown","57b0e2be":"markdown","c4070026":"markdown"},"source":{"926d657f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d7221d40":"# default libraries\nimport numpy as np\nimport pandas as pd\n\n# for data preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# for classifier models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n# matplotlib inline\n\n# for models evaluation\nfrom sklearn.metrics import confusion_matrix, accuracy_score","cc6826ce":"data = pd.read_csv('\/kaggle\/input\/eating-health-module-dataset\/ehresp_2014.csv')\ndata.head()","472d20c1":"data.shape","b92406fc":"def init_check(df):\n    \"\"\"\n    A function to make initial check for the dataset including the name, data type, \n    number of null values and number of unique varialbes for each feature.\n    \n    Parameter: dataset(DataFrame)\n    Output : DataFrame\n    \"\"\"\n    columns = df.columns    \n    lst = []\n    for feature in columns : \n        dtype = df[feature].dtypes\n        num_null = df[feature].isnull().sum()\n        num_unique = df[feature].nunique()\n        lst.append([feature, dtype, num_null, num_unique])\n    \n    check_df = pd.DataFrame(lst)\n    check_df.columns = ['feature','dtype','num_null','num_unique']\n    check_df = check_df.sort_values(by='dtype', axis=0, ascending=True)\n    \n    return check_df","275e51e2":"init_check(df=data)","fa99f274":"def categorical_encoding(df, categorical_cloumns, encoding_method):\n    \"\"\"\n    A function to encode categorical features to a one-hot numeric array (one-hot encoding) or \n    an array with value between 0 and n_classes-1 (label encoding).\n    \n    Parameters:\n        df (pd.DataFrame) : dataset\n        categorical_cloumns  (string) : list of features \n        encoding_method (string) : 'one-hot' or 'label'\n    Output : pd.DataFrame\n    \"\"\"\n    \n    if encoding_method == 'label':\n        print('You choose label encoding for your categorical features')\n        encoder = LabelEncoder()\n        encoded = df[categorical_cloumns].apply(encoder.fit_transform)\n        return encoded\n    \n    elif encoding_method == 'one-hot':\n        print('You choose one-hot encoding for your categorical features') \n        encoded = pd.DataFrame()\n        for feature in categorical_cloumns:\n            dummies = pd.get_dummies(df[feature], prefix=feature)\n            encoded = pd.concat([encoded, dummies], axis=1)\n        return encoded","dddcf653":"categorical_columns = data.select_dtypes(include=['float64']).columns","eca2430b":"encoded=categorical_encoding(df=data,categorical_cloumns=categorical_columns, encoding_method='label')","1c14950a":"data = data.drop(columns=categorical_columns, axis=1)\ndata = pd.concat([data, encoded], axis=1)\ndata.head()","7bf54ee3":"data.hist(bins = 10, figsize=(18, 16), color=\"#2c5af2\")","5f6cc297":"for a in ['eufinlwgt','erbmi','ertpreat','ertseat','euwgt']:\n    ax=plt.subplots(figsize=(6,3))\n    ax=sns.distplot(data[a])\n    title=\"Histogram of \" + a\n    ax.set_title(title, fontsize=12)\n    plt.show()","ee868151":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_scaled=pd.DataFrame(scaler.fit_transform(data))\ndata_scaled.columns=data.columns\ndata_scaled.index=data.index","8cebe8e1":"data_scaled.describe()","75b56e39":"from sklearn.decomposition import PCA\nn_components=37\npca = PCA(n_components=n_components)\npca.fit(data_scaled)","419ce01b":"plt.subplots(figsize=(10,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","2804ed75":"explained_variance_ratio = pca.explained_variance_ratio_ \ncum_explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\nlst = []\nfor i in range (0, n_components):\n  lst.append([i+1, round(explained_variance_ratio[i],6), cum_explained_variance_ratio[i]])\n\npca_predictor = pd.DataFrame(lst)\npca_predictor.columns = ['Component', 'Explained Variance', 'Cumulative Explained Variance']\npca_predictor","73c82e7b":"pca = PCA(n_components=8)\npca.fit(data_scaled)\nexplained_variance_ratio = pca.explained_variance_ratio_\nsingular_values = pca.singular_values_    ","1c1344e9":"data_transformed = pca.fit_transform(data_scaled)\ndata_transformed.shape","6cca59bd":"data_transformed","da0768bb":"plt.subplots(figsize=(10,8))\nplt.scatter(data_transformed[:,[0]], data_transformed[:,[1]])","3afde285":"from sklearn.cluster import KMeans\n\nn_clusters = 10\nkmeans = KMeans(n_clusters=n_clusters, random_state=123)\nkmeans.fit(data_transformed)","e604025e":"cluster_labels = kmeans.labels_\ncluster_labels","28094630":"ax=plt.subplots(figsize=(10,5))\nax=sns.countplot(cluster_labels)\ntitle=\"Histogram of Cluster Counts\"\nax.set_title(title, fontsize=12)\nplt.show()","ceefe7ac":"data['X'] = data_transformed[:,[0]]\ndata['Y'] = data_transformed[:,[1]]\ndata['cluster'] = cluster_labels\nax=plt.subplots(figsize=(10,10))\nax = sns.scatterplot(x='X', y='Y',hue='cluster', legend=\"full\", palette=\"Set1\", data=data)","e8c83b53":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=8).fit(data_transformed)\nlabels = gmm.predict(data_transformed)\nplt.scatter(data_transformed[:, 0],data_transformed[:, 1], c=labels, s=40, cmap='viridis');","49b0691c":"def cluster_stats(columns):\n    output = pd.DataFrame({'cluster':[ i for i in range(n_clusters)]})\n    for column in columns:\n        lst = []\n        for i in range(n_clusters):\n            mean = data[data['cluster'] == i].describe()[column]['mean']\n            lst.append([i, round(mean,2)])\n        df = pd.DataFrame(lst)\n        df.columns = ['cluster', column]\n        output = pd.merge(output, df, on='cluster', how='outer')\n    return output","65485808":"columns =data_scaled.columns\ncluster_stats(columns)","5a60659c":"# GMM","b2470b03":"### Visualizing","8e63939b":"### Clean","fa80bea8":"## Although I have retained 8 dimensions, after categorization, only two dimensions of the scatter plot can still see the points of different classes come together, so I kept the code for my wrong visualization process.","f00d8bb1":"# Modelling","7f5baf7c":"# Conclusion","197e02ca":"###  K-Means","06c07570":"### Comparison of clusters' stats","45475cd5":"1. Using  PCA and K-Means methods can effectively classify data.\n2. By comparison of clusters' stats, I am able to find out the differences between the categories.\n3. Although it is only when the dimension is reduced to 2 or 3 dimensions, the visualization can be well achieved. However, I found that when I can't drop to such a low dimension, I can only see the distribution of the data in these two dimensions.","c2b25f1e":"# Cleaning and exploration\n","bb009427":"# Ingest","57b0e2be":"### Feature engineering","c4070026":"### PCA"}}