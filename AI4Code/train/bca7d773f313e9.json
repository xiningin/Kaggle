{"cell_type":{"3a44b673":"code","2c0ef796":"code","3270abfc":"code","fb4acac7":"code","eec230e4":"code","33283168":"code","f413451d":"code","91c98f67":"code","5bd6b146":"code","9ee6f5f6":"code","04024bc5":"code","25ec3a47":"code","dd884649":"code","1ab1aea7":"code","4a98405e":"code","111b0bc1":"code","8919b9cb":"code","a37e92fb":"code","d6a41e16":"code","e641893c":"code","e5657cde":"code","624307b9":"code","813f1642":"code","3dd89810":"code","897a2041":"code","903b42df":"code","72d071e4":"code","e8b10e06":"code","0f0d034a":"code","1008f4df":"code","983a17c6":"code","a8b4213f":"code","ffffdc92":"code","fcf02638":"code","aea2a582":"code","c84635f1":"code","4a93e3af":"code","ac527110":"code","a0162878":"code","6c04108e":"code","c677c0ca":"code","a286ce68":"code","dce64e32":"code","2ebb7be2":"code","b1ae63f6":"code","b442e51f":"code","a577ffd7":"code","a04f1bb8":"code","0ecf3cab":"code","de81a179":"code","d24412ae":"code","81970a52":"code","8c094bd3":"code","289718e7":"code","030cc137":"code","20fe4c68":"code","aed423a6":"code","eadef954":"code","673db212":"code","18e129d2":"markdown","08ac86ff":"markdown","79b26449":"markdown","6543a7d7":"markdown","4c8cc7de":"markdown","6d18f585":"markdown","467d10e2":"markdown","c93f7909":"markdown","b650a125":"markdown","61c1418c":"markdown","20f49da9":"markdown","f3ef8220":"markdown","5cf72c5b":"markdown","1d0150bf":"markdown","7634f9ea":"markdown","b2fe8e2a":"markdown","e3604f57":"markdown","ba053482":"markdown","24fa1009":"markdown","5cab16ab":"markdown","0a559877":"markdown","22bc4f87":"markdown","d5127599":"markdown","3312565d":"markdown","f76e3568":"markdown","9d7e0752":"markdown","8415ed00":"markdown","18494b52":"markdown","21220cd0":"markdown","20a8201f":"markdown","00511d8a":"markdown","60cba585":"markdown","8312d490":"markdown","ade6f7db":"markdown","51973ffe":"markdown","552266e5":"markdown"},"source":{"3a44b673":"# importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","2c0ef796":"data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf = pd.DataFrame(data)","3270abfc":"df.head()","fb4acac7":"df.describe()\n","eec230e4":"col = df.columns  #getting list of column names","33283168":"# showing column wise %ge of NaN values they contains \n\nfor i in col:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","f413451d":"df = df.drop([\"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\"], axis=1)","91c98f67":"df.head()","5bd6b146":"num_df = df.select_dtypes(exclude=['object'])\ncat_df = df.drop(num_df, axis=1)","9ee6f5f6":"num_df.head()","04024bc5":"num_df.describe()","25ec3a47":"num_df = num_df.drop([\"Id\"], axis = 1)   #Since Id does not has any role in price prediction of houses","dd884649":"cormap = num_df.corr()\nfig, ax = plt.subplots(figsize=(36,36))\nsns.heatmap(cormap, annot = True)","1ab1aea7":"# Simple Function to get the name of top most corelated attributes\n\ndef get_corelated_col(cor_dat, threshold): \n  # Cor_data to be column along which corelation to be measured \n  #Threshold be the value above wich of corelation to considered\n  feature=[]\n  value=[]\n\n  for i ,index in enumerate(cor_dat.index):\n    if abs(cor_dat[index]) > threshold:\n      feature.append(index)\n      value.append(cor_dat[index])\n\n  df = pd.DataFrame(data = value, index = feature, columns=['corr value'])\n  return df\n","4a98405e":"top_corelated_values = get_corelated_col(cormap['SalePrice'], 0.60)\ntop_corelated_values","111b0bc1":"final_num_df = num_df[top_corelated_values.index]\nfinal_num_df.head()","8919b9cb":"cat_df.head()","a37e92fb":"cat_df.describe()","d6a41e16":"from sklearn.preprocessing import LabelEncoder\n\ncat_col = cat_df.columns\nfor i in cat_col:\n  enc = LabelEncoder()\n  cat_df[i] = enc.fit_transform(cat_df[i].astype('str'))","e641893c":"cat_df.head()\n","e5657cde":"cat_df['SalePrice'] = df['SalePrice']  # to get coreltion with target attribute which is Sales Price","624307b9":"cormat = cat_df.corr()\nfig, ax = plt.subplots(figsize=(36,36))\nsns.heatmap(cormat, annot = True)","813f1642":"top_corelated_values_cat = get_corelated_col(cormat['SalePrice'], 0.50)\ntop_corelated_values_cat","3dd89810":"final_cat_df = cat_df[top_corelated_values_cat.index].drop(['SalePrice'], axis=1) #since it was added only to find corelation and will double the attribute whice concation\nfinal_cat_df.head()","897a2041":"# Final selected 10 most favourable features for prediction\n\nfinal_df = pd.concat([ final_cat_df, final_num_df], axis = 1, sort=False)\nfinal_df.head()","903b42df":"sns.pairplot(final_df)\nplt.tight_layout()","72d071e4":"for i in final_df.columns:\n  print(i,\"\\t-\\t\", final_df[i].isna().mean()*100)","e8b10e06":"X = final_df.drop(['SalePrice'], axis=1)\ny = final_df['SalePrice']","0f0d034a":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX.head()","1008f4df":"print(X.shape, y.shape)","983a17c6":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","a8b4213f":"# model training\n\nmodel_1 = LinearRegression()\nmodel_1.fit(X_train, y_train)","ffffdc92":"# prediction \n\ny_pred_1 = model_1.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_1})\npred_df.head()","fcf02638":"#Evaluating the Model\n\nfrom sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_1))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_1))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_1)))","aea2a582":"# model training \n\nfrom sklearn.svm import SVR\n\nmodel_2 = SVR(kernel='rbf')   # Here kernel used is RBF (Radial Basis Function)\nmodel_2.fit(X_train, y_train)","c84635f1":"# Prediction\n\ny_pred_2 = model_2.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_2})\npred_df.head()","4a93e3af":"#Evaluating the Model\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_2))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_2))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_2)))","ac527110":"#model training\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel_3 = RandomForestRegressor(n_estimators = 500) # Here n_estimator which means no of descision trees used is 500\nmodel_3.fit(X_train, y_train)","a0162878":"# Prediction\n\ny_pred_3 = model_3.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_3})\npred_df.head()","6c04108e":"#Evaluating the Model\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_3))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_3))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_3)))","c677c0ca":"# Model Training\n\nfrom sklearn.linear_model import RidgeCV\n\nmodel_4 = RidgeCV()\nmodel_4.fit(X_train, y_train)\n","a286ce68":"# Prediction\n\ny_pred_4 = model_4.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_4})\npred_df.head()","dce64e32":"#Evaluating the Model\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_4))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_4))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_4)))","2ebb7be2":"# Model Creation\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ndef regressor():\n  model = Sequential()\n  model.add(Dense(20, input_dim=10, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))\n  model.compile(loss='mean_squared_error', optimizer='adam')\n  return model","b1ae63f6":"# Model Training\n\nmodel_5 = regressor()\nmodel_5.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)","b442e51f":"# Prediction\n\ny_pred_5 = model_5.predict(X_test)\n","a577ffd7":"pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_5.flatten()})\npred_df.head()","a04f1bb8":"#Evaluating the Model\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_5))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_5))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_5)))","0ecf3cab":"# Here we ready the Test Data\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_df = pd.DataFrame(test_data)\ntest_df[\"SalePrice\"] = df[\"SalePrice\"]\ntest_df.head()","de81a179":"# here we get the numerical attributes on which model will be trained\ntest_num_df = test_df[top_corelated_values.index]\ntest_num_df = test_num_df.drop([\"SalePrice\"], axis=1)\ntest_num_df.head()","d24412ae":"# here we get the categorical attributes on which model will be trained\ntest_cat_df = test_df[top_corelated_values_cat.index].drop(['SalePrice'], axis=1)\ntest_cat_df.head()","81970a52":"# Let's Encode the test categorical dataset also\n\nfor i in test_cat_df.columns:\n  enc = LabelEncoder()\n  test_cat_df[i] = enc.fit_transform(test_cat_df[i].astype('str'))\n\ntest_cat_df.head()\n","8c094bd3":"final_test_df = pd.concat([ test_cat_df, test_num_df], axis = 1, sort=False)\nfinal_test_df.head()","289718e7":"X = pd.DataFrame(scaler.transform(final_test_df), columns=final_test_df.columns)\nX.head()","030cc137":"#NaN values check\nfor i in X.columns:\n  print(i,\"\\t-\\t\", X[i].isna().mean()*100)","20fe4c68":"X = X.fillna(0)\n\n#NaN values check\nfor i in X.columns:\n  print(i,\"\\t-\\t\", X[i].isna().mean()*100)","aed423a6":"# Prediction\n\nY_Pred = model_3.predict(X)","eadef954":"final_test_df['Sale_Price'] = Y_Pred\nfinal_test_df.head()","673db212":"final_test_df.to_csv('.\/predicted.csv', index=False)","18e129d2":"### Data Information","08ac86ff":"\n## House Prices: Advanced Regression Techniques\n\nProblems to be solved:\n\n1. Descriptive Analysis of the dataset (Statistical analysis and visualization with explanation)\n\n2. Choose the 10 most important features for predicting house prices.\n\n3. Apply at least 5 prediction models on the selected features and discuss the proposed model's pros and cons for the underlying dataset.\n\n\n\n","79b26449":"> Since __Random Forest__ appears to be the best model from the 5 models choesen and tested on training dataset, Now lets apply it on test.csv","6543a7d7":"## Prediction Models\n","4c8cc7de":"> Since No columns have NaN values , now we can move towards prediction","6d18f585":"> Since range of data in different columns veries significantly we need to scale the independent variable. For this we will use _Min-Max Scaling_.\n> Since it is a test datset we are going to use same scaler used on training dataset to scale it.\n","467d10e2":"##### ___Pros and Cons___\n\n__Pros:__\n+ Trades variance for bias (i.e. in presence of co-linearity, it is worth to have biased results, in order to lower the variance.)\n+ Prevents over fitting\n\n__Cons:__\n+ Increases bias\n+ Need to select perfect alpha (hyper parameter)\n+ Model interpret-ability is low","c93f7909":"### ___4th Prediction Model: Ridge Regression___","b650a125":"\n### ___1st Prediction Model: Linear Regression (Multiple Linear Regression)___","61c1418c":"> As we can see that the value of root mean squared error is 38543.405300405764, which is slightly greater than 25% of the mean value. This means that our algorithm was not very accurate but can still make reasonably good predictions. But it has given best accuracy in prediction so far as compared to other models. \n","20f49da9":"### ___5th Prediction Model: Neural Network___","f3ef8220":"> As we can see that the value of root mean squared error is 39627.19527057066, which is slightly lesser than 25% of the mean value. Which means this model is not accurate so far.","5cf72c5b":"> As we can see that the value of root mean squared error is 42579.53443770319, which is slightly lesser than 25% of the mean value. Which means this model is not accurate so far but better than some of above.","1d0150bf":"> Since the given dataset contains both categorical and numerical dataset we have to separate them for further analysis. ","7634f9ea":"> As we can see that the value of root mean squared error is 39387.269823099276, which is slightly lesser than 25% of the mean value. This means that our algorithm was not very accurate but can still make reasonably good predictions.","b2fe8e2a":"> Now start analysis with numerical data. Main objective is to determine the columns fit for predictions by checking their skewness.\n","e3604f57":"> 3 columns has NaN values.","ba053482":"> A __Neural Network__ consists of an interconnected group of nodes called neurons. The input feature variables from the data are passed to these neurons as a multi-variable linear combination, where the values multiplied by each feature variable are known as weights. A neural network can have multiple layers where the output of one layer is passed to the next one in the same way. ","24fa1009":"##### __Pros and Cons__\n\n___Pros___\n+ The Linear regression model is the simplest equation using which the relationship between the multiple predictor variables and predicted variable can be expressed.\n+ The modeling speed of Linear regression is fast as it does not require complicated calculations and runs predictions fast when the amount of data is large.\n+ The ability of Linear regression to determine the relative influence of one or more predictor variables to the predicted value when the predictors are independent of each other is one of the key reasons of the popularity of Linear regression.\n\n___Cons___\n+ The Linear regression model is too simplistic to capture real world complexity\n+ Linear regression makes strong assumptions that there is Predictor (independent) and Predicted (dependent) variables are linearly related which may not be the case.\n+ Outliers can have a large effect on the output, as the Best Fit Line tries to minimize the MSE for the outlier points as well, resulting in a model that is not able to capture the information in the data.\n\n\n","5cab16ab":"> Now all the NaN values are fixed , Lets Move further","0a559877":"> Let's first encode the categorical data into numerical for futher analysis","22bc4f87":"### ___2nd Prediction Model: SVR (Support Vector Regressor)___","d5127599":"> So above mentioned Colums are best fit to use in prediction model","3312565d":"> In the __SVR__ algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform regression by finding the hyper-plane that differentiates the two classes very well.","f76e3568":"##### ___Pros and Cons___\n\n__Pros:__\n+ Great at learning complex, highly non-linear relationships. They usually can achieve pretty high performance, better than polynomial regression and often on par with neural networks.\n+ Very easy to interpret and understand. Although the final trained model can learn complex relationships, the decision boundaries that are built during training are easy and practical to understand.\n\n__Cons:__\n+ Because of the nature of training decision trees they can be prone to major overfitting. A completed decision tree model can be overly-complex and contain unnecessary structure. \n+ Using larger random forest ensembles to achieve higher performance comes with the drawbacks of being slower and requiring more memory.","9d7e0752":"#### ___Pros and Cons___\n\n__Pros:__\n+ Since neural networks can have many layers (and thus parameters) with non-linearities, they are very effective at modelling highly complex non-linear relationships.\n+ We generally don\u2019t have to worry about the structure of the data at neural networks are very flexible in learning almost any kind of feature variable relationships.\n+ Research has consistently shown that simply giving the network more training data, whether totally new or from augmenting the original data set, benefits network performance.\n\n__Cons:__\n+ Because of the complexity of these models, they\u2019re not easy to interpret and understand.\n+ They can be quite challenging and computationally intensive to train, requiring careful hyper-parameter tuning and setting of the learning rate schedule.\n+ They require a lot of data to achieve high performance and are generally outperformed by other ML algorithms in \u201csmall data\u201d cases.","8415ed00":"> __Ridge__ regression is a variation of linear regression specifically adapted for data that shows heavy multicollinearity. Ridge regression applies shrinking. Ridge regression is well-suited for datasets that have an abundant amount of features which are not independent (collinearity) from one another.","18494b52":"> Since range of data in different columns veries significantly we need to scale the independent variable i.e. X. For this we will use _Min-Max Scaling_.\n","21220cd0":"### ___3rd Prediction Model: Random Forest___","20a8201f":"__Random Forests__ are simply an ensemble of decision trees. The input vector is run through multiple decision trees. For regression, the output value of all the trees is averaged","00511d8a":"> __Linear Regression__ is a statistical method that allows us to summarize and study relationships between continuous (quantitative) variables. The term \u201clinear\u201d in linear regression refers to the fact that the method models data with linear combination of the explanatory\/predictor variables (attributes)","60cba585":"*****","8312d490":"> Now let's analyse the categorical part of dataset.\n\n","ade6f7db":"> Since  __Alley__ , __FireplaceQu__ , __PoolQC__, __Fence__, __MiscFeature__ has very significant no of Nan values , so we can drop them for better results ","51973ffe":"> __Corelation Matrix__ further clarifyies that columns choesen from distribution curve are more apt for performing predictions as compared to rest ","552266e5":"> As we can see that the value of root mean squared error is 84632.2162542007, which is slightly lesser than 50% of the mean value. This means that our algorithm was not accurate and poorly trained and cannot make reasonably good predictions. Errors in prediction of SVR is far more than as Multiple Linear Regression"}}