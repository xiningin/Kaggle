{"cell_type":{"357f37cb":"code","f7a7cdd5":"code","3e68d987":"code","a968bf04":"code","0d65c210":"code","8ac52121":"code","45b7c677":"code","aa176411":"code","3393ae2f":"code","cd27abf8":"code","3fc2ef7b":"code","a635db41":"code","56ee49b7":"code","1981599b":"code","58dde398":"code","8c964476":"code","180627eb":"code","b800ba34":"code","b6651f11":"code","b99f5016":"code","bd4ba085":"code","3829b50c":"code","5a7872f5":"code","ca85b773":"code","deee751c":"code","b0ce5ad5":"code","2b508811":"code","a7f595f0":"code","03b5db0b":"code","f3426b84":"code","2aa9ebbe":"code","7d33ff0f":"code","28bc31ea":"code","b8cc6daf":"code","2c2ec9cc":"code","fdf43da7":"code","cf5fb397":"code","c18934c8":"code","2cda184a":"code","bcf53d7d":"code","ad0ec5ec":"code","4cad2766":"code","5cf69cf9":"code","4ad8f56b":"code","6bc1c86c":"code","467cf9cd":"code","3ae67699":"code","8132dd37":"code","9a953d21":"code","fbce2e51":"markdown","05bc914b":"markdown","5464c45a":"markdown","b544e30e":"markdown","bf2ac4e3":"markdown","2438aedb":"markdown","cab58dc0":"markdown","dcb217f3":"markdown","5230a51e":"markdown","638dc3d4":"markdown","45b404e9":"markdown","0393c313":"markdown","c71b81ba":"markdown","4e653d84":"markdown","3368b3c0":"markdown","2d9b9422":"markdown","4a0d4ba9":"markdown","57dcd365":"markdown","11603dbd":"markdown","481e526a":"markdown","60bb9050":"markdown","b68e4290":"markdown","d758c9a1":"markdown","1cfcf0b4":"markdown","d120be56":"markdown","d0560fc1":"markdown","d5d381bc":"markdown","b23cf5f0":"markdown","95311b33":"markdown","c5b1eed7":"markdown","8827956e":"markdown","f5effd37":"markdown","af3c1d56":"markdown","5ecaadb3":"markdown","fe740b57":"markdown","349ca181":"markdown","e249ee13":"markdown","b0d2c24a":"markdown"},"source":{"357f37cb":"!pip install pandarallel\n!pip install xgboost\n!pip install catboost","f7a7cdd5":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV, RFE\nfrom sklearn.metrics import mean_squared_error, make_scorer, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport xgboost\nimport lightgbm\nimport catboost","3e68d987":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a968bf04":"train_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","0d65c210":"def get_basic_info(df):\n    \"\"\" Get basic information and statistics on a given dataframe.\n    \n    :param df: dataframe to be analyzed\n    \"\"\"\n    print('Basic info: \\n')\n    print(df.info())\n    print('\\n Basic stats: \\n')\n    print(df.describe(include='all'))\n    \n    num_cols = df.loc[:, df.dtypes != object].columns\n    \n    if len(num_cols) > 0:\n        f, axes = plt.subplots(1, len(num_cols), figsize=(16, 6), sharex=False)\n\n        for i, col in enumerate(num_cols):\n            sns.histplot(df[col], color=\"skyblue\", ax=axes[i])","8ac52121":"get_basic_info(train_df)","45b7c677":"get_basic_info(test_df)","aa176411":"print(\"Min Target:\", train_df[\"target\"].min(), \"\\n\" +\n      \"Text:\", train_df[train_df[\"target\"] == train_df[\"target\"].min()][\"excerpt\"][1705], \"\\n\" +\n      \"\\n\" +\n      \"Max Target:\", train_df[\"target\"].max(), \"\\n\" +\n      \"Text:\", train_df[train_df[\"target\"] == train_df[\"target\"].max()][\"excerpt\"][2829])","3393ae2f":"sns.scatterplot(x='target', y='standard_error', data=train_df)","cd27abf8":"train_df = train_df[train_df['standard_error'] != 0.0]","3fc2ef7b":"train_df['reading_difficulty'] = pd.cut(\n    train_df['target'], bins=[min(train_df['target']),\n                              np.percentile(train_df['target'], 33),\n                              np.percentile(train_df['target'], 67),\n                              max(train_df['target'])], \n    labels=['hard', 'moderate', 'easy'])","a635db41":"dummy_segments = pd.get_dummies(train_df['reading_difficulty'])\n\ntrain_df['reading_difficulty_easy'] = dummy_segments.easy\ntrain_df['reading_difficulty_moderate'] = dummy_segments.moderate\ntrain_df['reading_difficulty_hard'] = dummy_segments.hard","56ee49b7":"train_df['reading_difficulty'].value_counts(normalize = True) * 100\nsns.countplot(x = 'reading_difficulty', data = train_df, color = 'teal')","1981599b":"sns.catplot(x = 'reading_difficulty', y = 'target', kind = 'violin', split = True, data = train_df)","58dde398":"sns.catplot(x = 'reading_difficulty', y = 'standard_error', kind = 'violin', split = True, data = train_df)","8c964476":"def get_cleaned_text(text):\n    \"\"\" Clean the provided text: get rid of special characters, convert to lowercase.\n    \n    :param text: string to be cleaned\n    :returns cleaned text as a string\n    \"\"\"\n    text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', text)\n    text = text.lower()\n    return text","180627eb":"train_df['excerpt_cleaned'] = train_df['excerpt'].apply(lambda text: get_cleaned_text(text))","b800ba34":"train_df['excerpt_tokenized'] = train_df['excerpt_cleaned'].apply(\n    lambda text: word_tokenize(text))","b6651f11":"stop_words = set(stopwords.words('english')) \ntrain_df['excerpt_without_stopwords'] = train_df['excerpt_tokenized'].apply(\n    lambda text: [word for word in text if not word in stop_words])","b99f5016":"lemmatizer = WordNetLemmatizer()\ntrain_df['excerpt_lemmatized'] = train_df['excerpt_without_stopwords'].apply(\n    lambda text: [lemmatizer.lemmatize(word, pos='v') for word in text])","bd4ba085":"train_df['paragraph_length'] = train_df['excerpt_cleaned'].apply(\n    lambda text: len(text))","3829b50c":"train_df['avg_sentence_length'] = train_df['excerpt'].apply(\n    lambda text: round((sum([len(sentence) for sentence in text.split('.')])\/len(text.split('.'))),2))","5a7872f5":"train_df['avg_word_length'] = train_df['excerpt_tokenized'].apply(\n    lambda text: round((sum([len(word) for word in text])\/len(text)),2))","ca85b773":"train_df['sentence_count'] = train_df['excerpt'].apply(\n    lambda text: len(text.split('.')))","deee751c":"train_df['word_count'] = train_df['excerpt_tokenized'].apply(\n    lambda text: len(text))","b0ce5ad5":"train_df['unique_word_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        lambda text: len(set(text))\n    )\n)","2b508811":"def get_parts_of_speech_count(text, list_of_pos):\n    \"\"\" Count the specified parts of speech in a string.\n    \n    :param text: tokenized string the specified pos will be calculated in\n    :param list_of_pos: the list of the parts of speech to determine the occurence of\n    :returns the number of times a pos occured within the text as a integer\n    \"\"\"\n    text_pos_list = nltk.pos_tag(text)\n\n    pos_tag_list = []\n    pos_tag_list = [text_pos_tuple[1] for text_pos_tuple in text_pos_list]\n\n    pos_count = 0\n    pos_count = [1 if pos_tag in list_of_pos else 0 for pos_tag in pos_tag_list].count(1)\n    \n    return pos_count","a7f595f0":"train_df['noun_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['NN', 'NNS'])\n)\n\ntrain_df['proper_noun_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['NNP', 'NNPS'])\n)\n\ntrain_df['verb_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n)\n\ntrain_df['adjective_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['JJ', 'JJR', 'JJS'])\n)\n\ntrain_df['cardinal_digit_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['CD'])\n)\n\ntrain_df['adverb_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['RB', 'RBR', 'RBS'])\n)\n\ntrain_df['preposition_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['IN'])\n)\n\ntrain_df['foreign_word_count'] = (\n    train_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['FW'])\n)","03b5db0b":"word_freq = pd.read_csv('..\/input\/english-word-frequency\/unigram_freq.csv')\n\nword_freq = dict(zip(word_freq['word'], word_freq['count']))\navailable_words = set(word_freq.keys())\n\ntrain_df['word_freq_in_text'] = train_df['excerpt_lemmatized'].parallel_apply(\n    lambda x: [word_freq.get(word, 0) for word in list(x) if word in available_words])","f3426b84":"train_df['mean_word_freq_in_text'] = train_df['word_freq_in_text'].apply(lambda x: np.mean(x))\ntrain_df['std_word_freq_in_text'] = train_df['word_freq_in_text'].apply(lambda x: np.std(x))\ntrain_df['min_word_freq_in_text'] = train_df['word_freq_in_text'].apply(lambda x: np.min(x))\ntrain_df['max_word_freq_in_text'] = train_df['word_freq_in_text'].apply(lambda x: np.max(x))","2aa9ebbe":"plt.figure(figsize=(16, 6))\n\nmask = np.triu(np.ones_like(train_df.corr(), dtype=np.bool))\nheatmap = sns.heatmap(train_df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Variable Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","7d33ff0f":"ax = sns.pairplot(train_df, \n                  vars=['paragraph_length','sentence_count','avg_sentence_length',\n                        'avg_word_length','word_count','unique_word_count'], \n                  hue='reading_difficulty', \n                  palette='Set2', \n                  diag_kind='kde', \n                  height=2.5)","28bc31ea":"ax = sns.pairplot(train_df, \n                  vars=['mean_word_freq_in_text','std_word_freq_in_text','min_word_freq_in_text',\n                        'max_word_freq_in_text','noun_count','verb_count'], \n                  hue='reading_difficulty', \n                  palette='Set2', \n                  diag_kind='kde', \n                  height=2.5)","b8cc6daf":"X = train_df[['paragraph_length', 'avg_sentence_length',\n                       'avg_word_length', 'sentence_count', 'word_count', 'unique_word_count',\n                       'noun_count', 'proper_noun_count', 'verb_count', 'adjective_count',\n                       'cardinal_digit_count', 'adverb_count', 'preposition_count',\n                       'foreign_word_count', 'mean_word_freq_in_text',\n                       'std_word_freq_in_text', 'min_word_freq_in_text',\n                       'max_word_freq_in_text']].copy()\n\nX = X.astype(float)\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\ny = train_df[['target']]\ny = np.array(y)\ny = y.ravel()","2c2ec9cc":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)","fdf43da7":"scoring_func = make_scorer(mean_squared_error)","cf5fb397":"pipe = Pipeline([('scaler', RobustScaler()),\n                 ('dimension_reduction', PCA()),\n                 ('regression', RandomForestRegressor())])","c18934c8":"search_space = [{'scaler' : [None, StandardScaler(), RobustScaler(), MinMaxScaler()],\n                 'dimension_reduction' : [None, PCA(0.97), PCA(0.95), PCA(0.9)],\n                 'regression' : [LinearRegression()]\n                },\n                {'scaler' : [None, MinMaxScaler()],\n                 'dimension_reduction' : [None, PCA(0.97), PCA(0.95), PCA(0.9)],\n                 'regression': [RandomForestRegressor()],\n                 'regression__n_estimators' : [10, 100, 200, 300],\n                 'regression__max_features' : ['sqrt', 'auto'],\n                 'regression__min_samples_split' :[2, 5, 10],\n                 'regression__min_samples_leaf' :[1,2,4],\n                 'regression__max_depth' : [10, 20, 50, 60,90, 100],\n                 'regression__bootstrap' : [True]\n                },\n                {'scaler' : [None, MinMaxScaler()],\n                 'dimension_reduction' : [None, PCA(0.97), PCA(0.95), PCA(0.9)],\n                 'regression': [xgboost.XGBRegressor()],\n                 'regression__n_estimators' : [10, 100, 200, 300],\n                 'regression__learning_rate' : [0.05, 0.10, 0.20, 0.30 ] ,\n                 'regression__max_depth' : [ 3, 8, 12],\n                 'regression__min_child_weight' : [ 1, 3, 7 ],\n                 'regression__gamma' : [ 0.0,  0.2, 0.4 ]\n                },\n                {'scaler' : [None, MinMaxScaler()],\n                 'dimension_reduction' : [None, PCA(0.97), PCA(0.95), PCA(0.9)],\n                 'regression' : [lightgbm.LGBMRegressor()],\n                 'regression__learning_rate' : [0.05, 0.10, 0.20, 0.30],\n                 'regression__max_depth' : [-1, 3, 8, 12],\n                 'regression__n_estimators' : [ 100, 200, 300, 500],\n                 'regression__scale_pos_weight' : [0.5, 0.54, 0.6, 0.8, 1.0],\n                 'regression__boosting_type' : ['gbdt', 'dart', 'goss']\n                }, \n                {'scaler' : [None, MinMaxScaler()],\n                 'dimension_reduction' : [None, PCA(0.97), PCA(0.95), PCA(0.9)],\n                 'regression' : [catboost.CatBoostRegressor()],\n                 'regression__learning_rate' : [0.05, 0.10, 0.20, 0.30],\n                 'regression__iterations' : [100,200,500],\n                 'regression__depth' : [2,3,5,6],\n                 'regression__grow_policy' : ['SymmetricTree', 'Lossguide']\n                }\n               ]","2cda184a":"clf = RandomizedSearchCV(pipe, param_distributions = search_space, n_iter = 50, \n                         cv = 3, scoring=scoring_func, verbose = 2, n_jobs = -1)","bcf53d7d":"clf.fit(X_train, y_train)\nclf_df = pd.DataFrame(clf.cv_results_)","ad0ec5ec":"clf.best_estimator_.get_params()['steps'][2][1]","4cad2766":"y_pred = clf.best_estimator_.predict(X_test)\nprint('RMSE: %1.4f\\n' % (np.sqrt(mean_squared_error(y_test, y_pred))))","5cf69cf9":"X_train = X_scaled.copy()\ny_train = train_df[['target']]\ny_train = np.array(y_train)\ny_train = y_train.ravel()","4ad8f56b":"clf_final = clf.best_estimator_.get_params()['steps'][2][1]","6bc1c86c":"clf_final.fit(X_train, y_train)","467cf9cd":"y_test = test_df[['excerpt']]","3ae67699":"test_df['excerpt_cleaned'] = test_df['excerpt'].apply(lambda text: get_cleaned_text(text))\n\ntest_df['excerpt_tokenized'] = test_df['excerpt_cleaned'].apply(\n    lambda text: word_tokenize(text))\n\ntest_df['excerpt_without_stopwords'] = test_df['excerpt_tokenized'].apply(\n    lambda text: [word for word in text if not word in stop_words])\n\ntest_df['excerpt_lemmatized'] = test_df['excerpt_without_stopwords'].apply(\n    lambda text: [lemmatizer.lemmatize(word, pos='v') for word in text])\n\ntest_df['paragraph_length'] = test_df['excerpt_cleaned'].apply(\n    lambda text: len(text))\n\ntest_df['avg_sentence_length'] = test_df['excerpt'].apply(\n    lambda text: round((sum([len(sentence) for sentence in text.split('.')])\/len(text.split('.'))),2))\n\ntest_df['avg_word_length'] = test_df['excerpt_tokenized'].apply(\n    lambda text: round((sum([len(word) for word in text])\/len(text)),2))\n\ntest_df['sentence_count'] = test_df['excerpt'].apply(\n    lambda text: len(text.split('.')))\n\ntest_df['word_count'] = test_df['excerpt_tokenized'].apply(\n    lambda text: len(text))\n\ntest_df['unique_word_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        lambda text: len(set(text))\n    )\n)\n\ntest_df['noun_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['NN', 'NNS'])\n)\n\ntest_df['proper_noun_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['NNP', 'NNPS'])\n)\n\ntest_df['verb_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n)\n\ntest_df['adjective_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['JJ', 'JJR', 'JJS'])\n)\n\ntest_df['cardinal_digit_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['CD'])\n)\n\ntest_df['adverb_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['RB', 'RBR', 'RBS'])\n)\n\ntest_df['preposition_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['IN'])\n)\n\ntest_df['foreign_word_count'] = (\n    test_df['excerpt_lemmatized'].apply(\n        get_parts_of_speech_count, \n        list_of_pos=['FW'])\n)\n\ntest_df['word_freq_in_text'] = test_df['excerpt_lemmatized'].parallel_apply(\n    lambda x: [word_freq.get(word, 0) for word in list(x) if word in available_words])\n\ntest_df['mean_word_freq_in_text'] = test_df['word_freq_in_text'].apply(lambda x: np.mean(x))\ntest_df['std_word_freq_in_text'] = test_df['word_freq_in_text'].apply(lambda x: np.std(x))\ntest_df['min_word_freq_in_text'] = test_df['word_freq_in_text'].apply(lambda x: np.min(x))\ntest_df['max_word_freq_in_text'] = test_df['word_freq_in_text'].apply(lambda x: np.max(x))","8132dd37":"X_test = test_df[['paragraph_length', 'avg_sentence_length',\n                       'avg_word_length', 'sentence_count', 'word_count', 'unique_word_count',\n                       'noun_count', 'proper_noun_count', 'verb_count', 'adjective_count',\n                       'cardinal_digit_count', 'adverb_count', 'preposition_count',\n                       'foreign_word_count', 'mean_word_freq_in_text',\n                       'std_word_freq_in_text', 'min_word_freq_in_text',\n                       'max_word_freq_in_text']].copy()\n\nX_test = X_test.astype(float)\n\nscaler = StandardScaler()\nscaler.fit(X_test)\nX_test_scaled = scaler.transform(X_test)","9a953d21":"data = [test_df['id'], pd.Series(clf_final.predict(X_test_scaled))]\nheaders = ['id', 'target']\nsubmission = pd.concat(data, axis=1, keys = headers)\n\nprint(submission)\n\nsubmission.to_csv('submission.csv', index = False)","fbce2e51":"As we have seen above on the scatter plot, the standard error in case of medium difficulty is slightly lower, however, as for the average standard error per reading difficulty there are no large differences, we can say that on average there is a standard error of 0.5 per rating.","05bc914b":"Each excerpt was rated by multiple people which makes this variable a measure of the spread of scores among these raters.\n\nWhen it comes to rating the difficulty of texts, the tasks is actually more complex than it might seem at first. There are no well-defined set of factors that raters can use to map how easily a text can be read to values on a scale. This makes these ratings very subjective.\n\nThe latter is reflected in the *standard_error* variable as we can see on the histogram above, the 25th percentile is 0.4685! The mean standard error is 0.4914 which means that raters disagreed on the difficulty of a particular text by almost half a point on average! This is a huge disagreement if we take into account that the range of values in the target is only around 5.387.","5464c45a":"During this first step, I'm going to take a look at basic information about the variables we have in the dataset like the share of missing values, unique values and the distribution of values in case of numeric variables.","b544e30e":"Let's calculate word frequencies within each text as well and create some more features out of it!\n\n*For this part @andradaolteanu's awesome notebook has been a major inspiration.*","bf2ac4e3":"First of all, I'm going to extract some basic characteristics of the excerpts into features.","2438aedb":"Next, using NLTK's parts of speech tags, I'm going to count certain parts of speech in the excerpts.","cab58dc0":"#### **Create space of candidate learning algorithms and their hyperparameters**","dcb217f3":"So, now the distribution of the target varible per segment looks like this:","5230a51e":"**The Target**","638dc3d4":"**Variables**\n- *id* - unique ID for excerpt\n- *url_legal* - URL of source - this is blank in the test set.\n- *license* - license of source material - this is blank in the test set.\n- *excerpt* - text to predict reading ease of\n- *target* - reading ease\n- *standard_error* - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","45b404e9":"#### **Make root mean squared error a scoring function**","0393c313":"### **Feature engineering**","c71b81ba":"The values in the *target* variable that contains scores about reading ease range from -3.676 which is supposed to be the most difficult text to 1.711, the easiest one.\n\nAs we have seen above, values in the target variable follow a distribution very close to normal.\n\nLet's see some example texts from either end of the scale!","4e653d84":"### **Exploratory data analysis**","3368b3c0":"The RMSE is above 1! Having checked the leader board of the competition, I have to say that there is plenty of room for improvement. It's not so surprising since the techniques I've implemented in this notebook are rather basic. For the next attempt I should probably include tf-idf vectorization or definitely experiment with transformer models.","2d9b9422":"We can see that the target variable correlates positively and somewhat significantly with the verb count, and the mean word frequency per text. \n\nIt also correlates negatively and yet again somewhat significantly with the average word length, the paragraph length and the average sentence length. In case of these three varibales there is a high chance that we are dealing with multicollinearity since the longer words are, the longer the sentences and the paragraphs are so these three variables are probably correlated. During modelling I will apply feature selection techniques that ensure multicollinearity is dealth with properly.","4a0d4ba9":"### **Data cleaning and text preprocessing**","57dcd365":"### **Modelling**","11603dbd":"#### **Randomized search**","481e526a":"*In contrast to GridSearchCV, in this case not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.*\n\n**n_iter** trades off runtime vs. quality of the solution. First I set it to 100 and **cv** to 5 which meant 500 iterations but it took me quite a few hours to get it done so after experimenting a bit, I settled with 50 as **n_iter** and 3-fold cv.","60bb9050":"Based on this plot we can say that on average, people tended to disagree more in case of texts which are towards either end of the reading ease scale. What is pretty obvious is that there is no correlation between the target and the standard error. We also have an outlier whose target and standard error is both 0. This is the observation we have seen earlier on the distribution plot, it should be excluded from the dataset.","b68e4290":"As we can see, in 4 out of 6 columns there are no missing values while in the remaining two, *url_legal* and *license*, around 70% of the values is missing. \n\nAll of the ids and excerpts are unique. \n\nAs for the distributions of the numeric features, *target*'s distribution is very close to normal distribution while *standard_error* is a bit trickier. It has some 0 values yet the 25th percentile is at 0.4685 so taking the range of values into account, one could first assume that it's a negatively or left-skewed distribution. However, the median is somewhat smaller than the mean which shouldn't be the case for a negatively skewed distribution, right? The histogram on the right justifies this suspicion as we see a positively skewed distribution. The zeros are therefore presumably outliers.","d758c9a1":"**Distributions and correlations**","1cfcf0b4":"Now let's do some segmenting in terms of difficulty to understand the data a bit more. I'm dividing the dataset into 3 parts based on percentiles: easy, moderate and hard.","d120be56":"#### **Create pipeline**","d0560fc1":"### **Introduction**","d5d381bc":"#### **Load the data**","b23cf5f0":"In this notebook I'm going to do exploratory data analysis, data cleaning, some basic feature engineering utilizing NLP techniques and finally, I'll build an sklearn pipeline to find a reasonably good model.","95311b33":"Since there is nothing we can do about imputing values for the missing ones in the *url_legal* and *license* columns, I'm going to leave them as they are and just clean the texts in the *excerpt* column. I'm also going to tokenize this text, get rid of the stop words and lemmatize the word tokens.","c5b1eed7":"**The Standard Error**","8827956e":"### **Import modules and packages**","f5effd37":"#### **Prediction**","af3c1d56":"Regarding the test set's variables, we can make similar observations as all ids and excerpts are unique and there are some missing values in the *url_legal* and *license* columns.","5ecaadb3":"Before the prediction could be done on the test set, we need to create the same features we have in the training data.","fe740b57":"#### **Re-train the chosen model on all data**","349ca181":"**Segmenting**","e249ee13":"**Let's check out the correlation between the target and the standard error!**","b0d2c24a":"#### **Train-test split**"}}