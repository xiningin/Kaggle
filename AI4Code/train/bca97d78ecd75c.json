{"cell_type":{"a66666c5":"code","3de351bb":"code","d8b17962":"code","a8944cfd":"code","d7188973":"code","9ae6aab0":"code","529490b3":"code","022fc330":"code","722c79db":"code","55084f4f":"code","7b90d45a":"code","fba35118":"code","f0d42590":"code","bfa634bd":"code","365dec66":"code","e38b2dc6":"code","744593ac":"markdown","6629ab3d":"markdown","ab5e65fd":"markdown","c74928b8":"markdown","32801b5c":"markdown"},"source":{"a66666c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3de351bb":"data = pd.read_csv('..\/input\/Automobile_data.csv')\nlist(data)","d8b17962":"data['horsepower'] = pd.to_numeric(data['horsepower'], errors = 'coerce')\ndata['price'] = pd.to_numeric(data['price'], errors = 'coerce')\n# data.any().isna()\ndata.dropna(subset=['price', 'horsepower'], inplace=True)\n# type(data['horsepower'][1])","a8944cfd":"from scipy.stats.stats import pearsonr\npearsonr(data['horsepower'], data['price'])\ndata['horsepower'].head()","d7188973":"from bokeh.io import output_notebook\nfrom bokeh.plotting import ColumnDataSource, figure, show\n\n# enable notebook output\noutput_notebook()\n\nsource = ColumnDataSource(data=dict(\n    x=data['horsepower'],\n    y=data['price'],\n    make=data['make'],\n))\n\ntooltips = [\n    ('make', '@make'),\n    ('horsepower', '$x'),\n    ('price', '$y{$0}')\n]\n\np = figure(plot_width=600, plot_height=400, tooltips=tooltips)\np.xaxis.axis_label = 'Horsepower'\np.yaxis.axis_label = 'Price'\n\n# add a square renderer with a size, color, and alpha\np.circle('x', 'y', source=source, size=8, color='blue', alpha=0.5)\n\n# show the results\nshow(p)","9ae6aab0":"from sklearn.model_selection import train_test_split\ntrain, test=  train_test_split(data, test_size = 0.25)","529490b3":"from sklearn import linear_model\nmodel = linear_model.LinearRegression()\ntraining_x = np.array(train['horsepower']).reshape(-1,1)\ntraining_y = np.array(train['price'])\nmodel.fit(training_x, training_y)\nslope = np.asscalar(np.squeeze(model.coef_))\nintercept = model.intercept_\nprint('slope:', slope, 'intercept:', intercept)\n","022fc330":"# Now let's add the line to our graph\nfrom bokeh.models import Slope\nbest_line = Slope(gradient=slope, y_intercept=intercept, line_color='red', line_width=3)\np.add_layout(best_line)\nshow(p)","722c79db":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# function to predict the mean_absolute_error, mean_squared_error and r-squared\ndef predict_metrics(lr, x, y):\n    pred = lr.predict(x)\n    mae = mean_absolute_error(y, pred)\n    mse = mean_squared_error(y, pred)\n    r2 = r2_score(y, pred)\n    return mae, mse, r2\n\ntraining_mae, training_mse, training_r2 = predict_metrics(model, training_x, training_y)\n\ntest_x = np.array(test['horsepower']).reshape(-1,1)\ntest_y = np.array(test['price'])\n\ntest_mae, test_mse, test_r2 = predict_metrics(model, test_x, test_y)\n\nprint('training mean error:', training_mae, 'training mse:', training_mse, 'training r2:', training_r2)\nprint('test mean error:', test_mae, 'test mse:', test_mse, 'test r2:', test_r2)","55084f4f":"#Getting the correlation between other variables\/columns\n\ncols = ['horsepower', 'engine-size', 'peak-rpm', 'length', 'width', 'height']\nfor col in cols:\n    data[col] = pd.to_numeric(data[col], errors = 'coerce')\ndata.dropna(subset = ['price', 'horsepower'], inplace = True)\n\n# Let's see how strongly each column is correlated to price\nfor col in cols:\n    print(col, pearsonr(data[col], data['price']))","7b90d45a":"# split train and test data as before\nmodel_cols = ['horsepower', 'engine-size', 'length', 'width']\nmulti_x = np.column_stack(tuple(data[col] for col in model_cols))\ny = data['price']\n\nmulti_train_x, multi_test_x, multi_train_y, multi_test_y = train_test_split(multi_x, y, test_size = 0.25)\n\n","fba35118":"# fit the model as before\nmulti_model = linear_model.LinearRegression()\nmulti_model.fit(multi_train_x, multi_train_y)\nmulti_model_intercept = multi_model.intercept_\nmulti_coefficient = dict(zip(model_cols,multi_model.coef_))\nprint('intercept:', multi_model_intercept)\nprint('Co-efficients:', multi_coefficient)\n","f0d42590":"# calculate error metrics\nm_train_mae, m_train_mse, m_train_r2 = predict_metrics(multi_model, multi_train_x, multi_train_y)\nm_test_mae, m_test_mse, m_test_r2 = predict_metrics(multi_model, multi_test_x, multi_test_y)\n\nprint('m_train_mean_error:', m_train_mae, 'm_train_mae:', m_train_mse, 'm_train_r2', m_train_r2 )\nprint('m_test_mean_error:', m_test_mae, 'm_test_mae:', m_test_mse, 'm_test_r2', m_test_r2 )","bfa634bd":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha = 0.05, normalize = True)\nridge.fit(multi_train_x, multi_train_y)\n\nr_train_mae, r_train_mse, r_train_r2 = predict_metrics(ridge, multi_train_x, multi_train_y)\nr_test_mae, r_test_mse, r_test_r2 = predict_metrics(ridge, multi_test_x, multi_test_y)\n\nprint('r_train_mean_error:', r_train_mae, 'r_train_mae:', r_train_mse, 'r_train_r2', r_train_r2 )\nprint('r_test_mean_error:', r_test_mae, 'r_test_mae:', r_test_mse, 'r_test_r2', r_test_r2 )\n","365dec66":"from sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha = 0.05, normalize = True)\nlasso_model.fit(multi_train_x, multi_train_y)\n\nl_train_mae, l_train_mse, l_train_r2 = predict_metrics(lasso_model, multi_train_x, multi_train_y)\nl_test_mae, l_test_mse, l_test_r2 = predict_metrics(lasso_model, multi_test_x, multi_test_y)\n\nprint('train_mean_error:', l_train_mae, 'train_mae:', l_train_mse, 'train_r2', l_train_r2 )\nprint('test_mean_error:', l_test_mae, 'test_mae:', l_test_mse, 'test_r2', l_test_r2 )","e38b2dc6":"from sklearn.linear_model import ElasticNet\nenet_model = ElasticNet(alpha=0.01, l1_ratio=0.5, normalize=False)\nenet_model.fit(multi_train_x, multi_train_y)\n\nel_train_mae, el_train_mse, el_train_r2 = predict_metrics(enet_model, multi_train_x, multi_train_y)\nel_test_mae, el_test_mse, el_test_r2 = predict_metrics(enet_model, multi_test_x, multi_test_y)\n\nprint('train_mean_error:', el_train_mae, 'train_mae:', el_train_mse, 'train_r2', el_train_r2 )\nprint('test_mean_error:', el_test_mae, 'test_mae:', el_test_mse, 'test_r2', el_test_r2 )","744593ac":"> ## **Introduction**\n\nThis kernel will go through the below regression techniques:\n* Linear Regression\n* Ridge Regression\n* Lasso Regression\n* Elastic Net Regression\n \n Go through the link to understand all the Regression techniques:\n   [Beginner Guide to Regression techniques](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/)","6629ab3d":"> ## **ElasticNet Regression**\n  In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.","ab5e65fd":"> ## **Lasso Regression**","c74928b8":"> ## **Ridge Regression**","32801b5c":"> ## **Linear Regression**"}}