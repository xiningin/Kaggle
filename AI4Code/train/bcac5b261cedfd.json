{"cell_type":{"978c1238":"code","10f72023":"code","eb95a648":"code","e182e2b0":"code","2524ae79":"code","21480deb":"code","5f432af8":"code","96d7bcb2":"code","f4c26bdb":"code","398c6ecb":"code","fc2df16d":"code","268188d8":"code","6944854b":"code","2d891fb4":"code","3dbdccf0":"code","2a1d955e":"code","d5bd8353":"code","37a06984":"code","cbbd22ae":"code","f509a980":"code","5c115f3c":"code","0a454a13":"code","eaef48dc":"code","01e79e9a":"code","9d38c22a":"code","92de0445":"code","e61096c8":"code","c824f4b3":"code","8536ead7":"code","4540e1be":"code","9876eccd":"code","3946f3d4":"code","25f12ee7":"code","c93d47ef":"code","7059461d":"code","977b2a4c":"code","1e5e5347":"code","19d89509":"code","19199672":"code","fbec0fbe":"code","06868b2a":"code","e36082da":"code","8cb4c83a":"code","a3e2ad72":"code","2a288dcb":"code","54f9ddfc":"code","d7db8326":"code","169155d5":"code","3a333890":"code","760a99ec":"code","b4d28e75":"code","a0a12366":"code","7c5d3a91":"code","9c2747cd":"code","ffa28c26":"code","4bda30dd":"code","02ff4f3c":"code","f9560862":"code","1c4cc919":"code","aa483b29":"code","d38c4a6f":"code","9e62a46f":"code","42d777eb":"code","02a60cb9":"code","2f514214":"code","e36ee813":"code","23986cb0":"code","1624613d":"code","2b0e6f6b":"code","b0e6f73b":"code","805edf07":"code","0c454a44":"code","a8a99103":"code","0cdd18f4":"code","0b81ba1e":"code","00c67221":"code","575cdca1":"code","8110a226":"code","a4c6ce5e":"code","60bf9959":"code","796d0f9c":"code","4c53dd5e":"code","d7a36010":"code","0242c47c":"code","61208025":"code","5492fdb9":"code","7e42f0f5":"code","b1c8ae37":"code","c1124ea1":"code","4ee2b316":"code","ec65d690":"code","c6a6328a":"code","7f94d1c6":"code","13a733d4":"code","c5f9ae80":"code","c8be08b7":"markdown","d31cf010":"markdown","defaa2e0":"markdown","8b8fe805":"markdown","99286039":"markdown","ee5a0f04":"markdown","f128499d":"markdown","2913fda1":"markdown","09f552b1":"markdown","e47c4951":"markdown","3ac0c506":"markdown","2f6a359f":"markdown","b0c9ef1b":"markdown","c507ba38":"markdown","126f2ad8":"markdown","0d9ae044":"markdown","03cf1dbf":"markdown","7198f29a":"markdown","d98ab03f":"markdown","29c97807":"markdown","edde76e6":"markdown","ddc859d9":"markdown","24be198e":"markdown","fee3c2ad":"markdown","150133d4":"markdown","6236327c":"markdown","c92b1084":"markdown","16950d83":"markdown"},"source":{"978c1238":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nimport missingno as msno\n\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nfrom matplotlib_venn import venn3\n\n#text cleaning\nimport re\nimport string\n\n## nlp\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\n","10f72023":"train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntest = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\")","eb95a648":"train.shape","e182e2b0":"#missing value computation\ndef cal_missing_val(df):\n    data_dict = {}\n    for col in df.columns:\n        data_dict[col] = (df[col].isnull().sum()\/df.shape[0])*100\n    return pd.DataFrame.from_dict(data_dict, orient='index', columns=['MissingValueInPercentage'])","2524ae79":"cal_missing_val(train)","21480deb":"cal_missing_val(test)","5f432af8":"rowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)*1","96d7bcb2":"xxx=train.iloc[:,2:9].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(xxx.index, xxx.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = xxx.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\nplt.show()","f4c26bdb":"df1 = pd.DataFrame(train.loc[:,\"toxic\"].value_counts())\ndf2 = pd.DataFrame(train.loc[:,\"severe_toxic\"].value_counts())\ndf3 = pd.DataFrame(train.loc[:,\"obscene\"].value_counts())\ndf4 = pd.DataFrame(train.loc[:,\"threat\"].value_counts())\ndf5 = pd.DataFrame(train.loc[:,\"insult\"].value_counts())\ndf6 = pd.DataFrame(train.loc[:,\"identity_hate\"].value_counts())\ndf_train_distribution = pd.concat([df1,df2,df3,df4,df5,df6], axis = 1)\ndf_train_distribution","398c6ecb":"percentage_of_1 = []\nfor i in range(len(df_train_distribution.columns)):\n    a = round(df_train_distribution.iloc[1,i]\/(df_train_distribution.iloc[0,i]+df_train_distribution.iloc[1,i])*100,2)\n    percentage_of_1.append(a)\n\npercentage_of_1","fc2df16d":"train_msno = train.replace(1,float(\"NaN\"))\n# checking overlaps among 6 columns\nmsno.matrix(train_msno)","268188d8":"#the larger percentage, larger proportion explained by other factors \n\n# severe_toxic can be 100% explained by toxic \nprint(1-(train[train.severe_toxic == 1].severe_toxic - train[train.severe_toxic == 1].toxic).sum()) ","6944854b":"# identity_hate\nnumber_identity_hate = len(train[train.identity_hate == 1].identity_hate)\nprint(1-(train[train.identity_hate == 1].identity_hate - train[train.identity_hate == 1].insult).sum()\/number_identity_hate) #83% explained by insult\nprint(1-(train[train.identity_hate == 1].identity_hate - train[train.identity_hate == 1].toxic).sum()\/number_identity_hate) #93% expalined by toxic\nprint(1-(train[train.identity_hate == 1].identity_hate - train[train.identity_hate == 1].severe_toxic).sum()\/number_identity_hate) #22% expalined by severe_toxic\nprint(1-(train[train.identity_hate == 1].identity_hate - train[train.identity_hate == 1].obscene).sum()\/number_identity_hate) #73% expalined by obscene","2d891fb4":"# threat\nnumber_threat = len(train[train.threat == 1].threat)\nprint(1-(train[train.threat == 1].threat - train[train.threat == 1].insult).sum()\/number_threat) #64% explained by insult\nprint(1-(train[train.threat == 1].threat - train[train.threat == 1].toxic).sum()\/number_threat) #94% expalined by toxic\nprint(1-(train[train.threat == 1].threat - train[train.threat == 1].severe_toxic).sum()\/number_threat) #24% expalined by severe_toxic\nprint(1-(train[train.threat == 1].threat - train[train.threat == 1].obscene).sum()\/number_threat) #63% expalined by obscene","3dbdccf0":"train[\"number_tags\"] = train.toxic+train.severe_toxic+train.obscene+train.threat+train.insult+train.identity_hate\nx = train.number_tags.value_counts().index\ny = train.number_tags.value_counts().values\n\nplt.figure(figsize=(8,4))\nax= sns.barplot(x, y, alpha=0.8)\nplt.title(\"# number of 1s\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\nax.legend(x)\n#adding the text labels\nrects = ax.patches\nlabels = y\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\nplt.show()","2a1d955e":"train_col_name = train.columns.drop([\"id\",\"comment_text\",\"clean\"]).tolist()","d5bd8353":"f, ax = plt.subplots(figsize=(9, 6))\nf.suptitle('Correlation matrix for categories')\nsns.heatmap(train[train_col_name].corr(), annot=True, linewidths=.5, ax=ax)","37a06984":"t = train[(train['toxic'] == 1) & (train['insult'] == 0) & (train['obscene'] == 0)].shape[0]\ni = train[(train['toxic'] == 0) & (train['insult'] == 1) & (train['obscene'] == 0)].shape[0]\no = train[(train['toxic'] == 0) & (train['insult'] == 0) & (train['obscene'] == 1)].shape[0]\n\nt_i = train[(train['toxic'] == 1) & (train['insult'] == 1) & (train['obscene'] == 0)].shape[0]\nt_o = train[(train['toxic'] == 1) & (train['insult'] == 0) & (train['obscene'] == 1)].shape[0]\ni_o = train[(train['toxic'] == 0) & (train['insult'] == 1) & (train['obscene'] == 1)].shape[0]\n\nt_i_o = train[(train['toxic'] == 1) & (train['insult'] == 1) & (train['obscene'] == 1)].shape[0]\n\n\n# Make the diagram\nplt.figure(figsize=(8, 8))\nplt.title(\"Venn diagram for 'toxic', 'insult' and 'obscene'\")\nvenn3(subsets = (t, i, t_i, o, t_o, i_o, t_i_o), \n      set_labels=('toxic', 'insult', 'obscene'))\nplt.show()","cbbd22ae":"from wordcloud import WordCloud ,STOPWORDS\nstopword=set(STOPWORDS)\n\ndef get_word_cloud(column):\n    subset=train[train[column]==1]\n    text=subset.comment_text.values\n    wc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\n    wc.generate(\" \".join(text))\n    plt.figure(figsize=(20,10))\n    plt.axis(\"off\")\n    plt.title(\"Words frequented in Clean Comments\", fontsize=20)\n    plt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n    plt.show()","f509a980":"get_word_cloud('clean')   #wordcloud for clean comments","5c115f3c":"get_word_cloud('toxic')","0a454a13":"get_word_cloud('obscene')","eaef48dc":"get_word_cloud('severe_toxic')","01e79e9a":"train['ip']  = [re.findall('[0-9]+(?:\\.[0-9]+){3}', i)for i in train.comment_text.tolist()]\ntrain['ip'] = train['ip'].astype(str).str.replace('[','').str.replace(']','').str.replace(\"'\",'')\ntest['ip']  = [re.findall('[0-9]+(?:\\.[0-9]+){3}', i)for i in test.comment_text.tolist()]\ntest['ip'] = test['ip'].astype(str).str.replace('[','').str.replace(']','').str.replace(\"'\",'')","9d38c22a":"train.head()","92de0445":"train[train['ip'] != ''].count()\n10083\/train.shape[0]*100","e61096c8":"test[test['ip'] != ''].count()\n759\/test.shape[0]*100","c824f4b3":"# create a columns 'is_ip'\ntrain['is_ip'] = 0\ntrain.loc[train['ip'] != '', 'is_ip'] =1","8536ead7":"sub_ip_df = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate','clean','is_ip','ip']]\nsame_ip = sub_ip_df.groupby('ip').sum()","4540e1be":"same_ip","9876eccd":"# take a look at the guy with ip 199.101.61.190\nip_199 = pd.DataFrame(train.loc[train.ip == '199.101.61.190', ['toxic','severe_toxic','obscene','threat','insult','identity_hate','clean','comment_text']])\nip_199.head(10)","3946f3d4":"# toxic\nip_199.iloc[5,7]","25f12ee7":"def ip_by_category(df):\n    lables = train.columns[2:9]\n    lst = []\n    for i in lables:\n        ips = df.loc[(df[i]==1) & (df['is_ip'] ==1), 'is_ip'].sum()\n        totals = df[lables].sum()\n        lst.append(ips)\n        combined = list(zip(lables,totals,lst))\n        res = pd.DataFrame(combined, columns=['category','totals','total_ips'])\n        res['pctage'] = round(res.total_ips \/ res.totals*100,2)\n    return res\n","c93d47ef":"ip_by_category(train)","7059461d":"#get raw comments for each category, can be used for more feature engineering\ndef get_raw_comment(category, df):\n    text = df.loc[df[category] ==1,'comment_text']\n    return text\n\n#get cleaned comments for each category, can be used for count_vectorizer\ndef get_cleaned_comment(category, df):\n    text = df.loc[df[category] ==1,'clean_corpus']\n    return text","977b2a4c":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n\n    return comment","1e5e5347":"train['clean_corpus']=train['comment_text'].apply(lambda x :clean(x))\ntest['clean_corpus']=test['comment_text'].apply(lambda x :clean(x))","19d89509":"def word_freq(category, df):\n    corpus = get_cleaned_comment(category, df)\n    \n    '''in the countvectorizer, get rid of english stop words, use lower case (default), use binary so \n    when count >1 it will be 1 ''' \n    vectorizer = CountVectorizer(stop_words = 'english', binary = True)\n    x = vectorizer.fit_transform(corpus)\n    word_freq = pd.DataFrame(x.toarray(), columns = vectorizer.get_feature_names())\n    return word_freq","19199672":"id_word_freq = word_freq('identity_hate',train)\nid_word_freq.head()","fbec0fbe":"# run the total of words in each category\nid_word_freq.sum().sum()","06868b2a":"to_word_freq = word_freq('toxic',train)\nto_word_freq.head()","e36082da":"to_word_freq.sum().sum()","8cb4c83a":"se_to_word_freq = word_freq('severe_toxic',train)\nse_to_word_freq.head()","a3e2ad72":"se_to_word_freq.sum().sum()","2a288dcb":"in_word_freq = word_freq('insult',train)\nin_word_freq.head()","54f9ddfc":"in_word_freq.sum().sum()","d7db8326":"ob_word_freq = word_freq('obscene',train)\nob_word_freq.head()","169155d5":"ob_word_freq.sum().sum()","3a333890":"th_word_freq = word_freq('threat',train)\nth_word_freq","760a99ec":"th_word_freq.sum().sum()","b4d28e75":"def top_freq_word(category,df):\n    freq = word_freq(category, df)\n    top_freq = pd.DataFrame(freq.sum(), columns = ['Freq']).sort_values(by = 'Freq', ascending = False)\n \n    return top_freq.head(50)","a0a12366":"id_f = top_freq_word('identity_hate',train)","7c5d3a91":"id_f.head()","9c2747cd":"to_f = top_freq_word('toxic',train)","ffa28c26":"to_f.head()","4bda30dd":"st_f = top_freq_word('severe_toxic',train)","02ff4f3c":"st_f.head()","f9560862":"o_f = top_freq_word('obscene',train)","1c4cc919":"o_f.head()","aa483b29":"th_f = top_freq_word('threat',train)","d38c4a6f":"th_f.head()","9e62a46f":"in_f = top_freq_word('insult',train)","42d777eb":"in_f.head()","02a60cb9":"from itertools import combinations \ndef top_combinations(category, df):\n    freq_df = word_freq(category, df)\n    comb = combinations(top_freq_word(category, df).index,2)\n    c = list(comb)\n    #print(c)\n    for a,b in c: \n        freq_df[str(a)+'_'+str(b)] = 0  \n        freq_df[str(a)+'_'+str(b)] = [1 if i >= 1 else 0 for i in (freq_df[a]*freq_df[b])]\n    return freq_df","2f514214":"top_combined_id = top_combinations('identity_hate',train)\ntop_combined_id.head()","e36ee813":"top_combined_toxic = top_combinations('toxic',train)\ntop_combined_toxic.head()","23986cb0":"top_combined_severe_toxic = top_combinations('severe_toxic',train)\ntop_combined_severe_toxic.head()","1624613d":"top_combined_obscene = top_combinations('obscene',train)\ntop_combined_obscene.head()","2b0e6f6b":"top_combined_threat = top_combinations('threat',train)\ntop_combined_threat.head()","b0e6f73b":"top_combined_insult = top_combinations('insult',train)\ntop_combined_insult.head()","805edf07":"# see the difference of toxic ~ obscene\nset(to_f.index) - (set(to_f.index)&set(o_f.index))","0c454a44":"# see the difference of obscene ~ toxic\nset(o_f.index) - (set(to_f.index)&set(o_f.index))","a8a99103":"# see the difference of insult ~ toxic\nset(in_f.index) - (set(to_f.index)&set(in_f.index))","0cdd18f4":"# see the difference of toxic ~ insult\nset(to_f.index) - (set(to_f.index)&set(in_f.index))","0b81ba1e":"# see the difference of obscene ~ insult\nset(o_f.index) - (set(o_f.index)&set(in_f.index))","00c67221":"# see the difference of insult ~ obscene\nset(in_f.index) - (set(o_f.index)&set(in_f.index))","575cdca1":"# see the difference of identity ~ threate\nset(id_f.index) - (set(id_f.index)&set(th_f.index))","8110a226":"# see the difference of threate ~ identity\nset(th_f.index) - (set(id_f.index)&set(th_f.index))","a4c6ce5e":"### a lot of racist word, consider to use the racist list from \n#https:\/\/en.wikipedia.org\/wiki\/List_of_ethnic_slurs_by_ethnicity to generate more engineered features\nid_text = get_raw_comment('identity_hate',train)\nid_text","60bf9959":"# toxic comments are more used by regular sware words\ntox_text = get_raw_comment('toxic',train)\ntox_text","796d0f9c":"# server toxic is long and has lot of duplicated copy paste. Consider to calculate duplicated words count\nst_text = get_raw_comment('severe_toxic',train)\nst_text","4c53dd5e":"# genitals related words\nob_text = get_raw_comment('obscene',train)\nob_text","d7a36010":"tr_text = get_raw_comment('threat',train)\ntr_text","0242c47c":"#http:\/\/www.insult.wiki\/wiki\/Insult_List\nins_text =get_raw_comment('insult',train)\nins_text","61208025":"clean_text = get_raw_comment('clean',train)\nclean_text","5492fdb9":"from nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))","7e42f0f5":"sample = train[0:100]","b1c8ae37":"#lemmatization\nfrom nltk.corpus import wordnet\n\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# sentence_words = nltk.word_tokenize(sample.comment_text)\n# # [j for j in sample['comment_text']]\n# [wordnet_lemmatizer.lemmatize(word) for word in sentence_words]\n\n\ndef lemmaSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    lemma_sentence=[]\n    for word in token_words:\n        lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n        lemma_sentence.append(\" \")\n    return \"\".join(lemma_sentence)\n\nx=[lemmaSentence(i) for i in sample['comment_text']]\nprint(x)","c1124ea1":"# porterstemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\nx=[stemSentence(i) for i in sample['comment_text']]\nprint(x)","4ee2b316":"import nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(language=\"english\")\nstems = [stemmer.stem(t) for t in sample['comment_text']]  \nstems","ec65d690":"import emoji","c6a6328a":"corpus = pd.DataFrame([\n     'This is the first document \ud83e\udd70\ud83d\ude07\ud83d\ude06.',\n     'This document is the \ud83d\ude07second document.',\n     'And this is the third one. \ud83d\udc9d',\n     'Is this the first document, \u262f\ufe0f you\\'ll ?',\n],columns = ['a'])","7f94d1c6":"corpus","13a733d4":"corpus['decoded'] = corpus['a'].apply(lambda x: emoji.demojize(x))","c5f9ae80":"corpus","c8be08b7":"# 7. Other thoughts\nIn this part, you will notice that problems start to show, such as there are multi-languages. For non-english languages countervectorizer might not be able to seperate them well, so we can expect a lot of mistakes here and should start to think of how to deal with multi-language issue.\n\nAnother issue is since we only did basic cleaning, there are still a lot of noises in the text, so more detailed cleaning strategy is in demand.","d31cf010":"### 5.1 Find same ip by each category","defaa2e0":"# 1. Check missing values for Train and Test:\n\nThere are no missing values for train and test data","8b8fe805":"in train set there are 10083 (6.3%) comments has ip address, in test set there are 759 (49%) comments has ip address","99286039":"### 7.3 Get rid of emojis","ee5a0f04":"### 3.2 Distribution of number of 1s each row get (coocurrence within rows)","f128499d":"### 6.5 To see the difference of each categories' top frequent words","2913fda1":"### 6.1 Basic cleaning for praoeration of countvectorization","09f552b1":"# 3. Relationship across different categories\nSince this is multilable problem, we can expect high correlations between each cateogries. In the MSNO plot down below, white line is the occurence of the tag.","e47c4951":"### 3.4 Venn diagram of overlapping among categories\nVenn diagram is another good visual tool to show the overlaps, here we just showed toxic, insult and obscene besed on result from correlation matrix above","3ac0c506":"#### In this notebook, we will explore:\n**1.\tCheck missing values for Train and Test**\n\n**2.\tComments distribution by category<br\/>**\n   > 2.1 Get a distribution plot by catergory<br\/>\n   > 2.2 check the percentage of 1 in each category\n\n**3.\tRelationship across different categories<br\/>**\n   >   3.1 MSNO plot to show coocurrence<br\/>\n   >   3.2 Distribution of number of 1s each row get (coocurrence within rows)<br\/>\n   >   3.3 Correlation matrix by category<br\/>\n   >   3.4 Venn diagram of overlapping among categories<br\/>\n\n**4. Wordclouds - Frequent words**\n\n**5. If IPs reveal anything interesting?<br\/>**\n   >    5.1 Find same ip by each category<br\/>\n    5.2 Which category has more ip address\n\n**6. Finding most frequent used words<br\/>**\n   >    6.1 Basic cleaning for praoeration of countvectorization<br\/>\n    6.2 Get count vectorizer to find word frequency<br\/>\n    6.3 Get top freq words for each categories<br\/>\n    6.4 Combinations for the top words<br\/>\n    6.5 To see the difference of each categories' top frequent words\n\n**7. Other thoughts<br\/>**\n   >   7.1 Patterns in each category<br\/>\n    7.2 Stemming and Lemmatization<br\/>\n    7.3 Get rid of emojis\n","2f6a359f":"### 2.1 Get a distribution plot by catergory","b0c9ef1b":"### Get a comment column as a Clean\n\n- whenever there is zero toxicity, that column is a clean column","c507ba38":"### 7.1 Patterns in each category","126f2ad8":"### 6.3 Get top freq words for each categories\nNOTE: when calculate frequency, only use train's so far, and apply the count to test, no matter whether test has that combination or not","0d9ae044":"Server_toxic, threat and identity_hate are three small cetagories, we wanted to see if they will be able to be explained by other categories, here by saying 'explain', we are acutally meaning coorcurring.","03cf1dbf":"### 6.2 Get count vectorizer to find word frequency","7198f29a":"# 5. If IPs reveal anything interesting?\nWe found in some comments there is an ip pattern, so just wanted to explore to see if ip could be used as an engineered feature","d98ab03f":"### 3.1 MSNO plot to show coocurrence","29c97807":"### 5.2 Which category has more ip address\ntoxic, obscence, insult and identity_hate have higher ip percentages, and servere_toxic, thread have less ips, might be people are afraid to be tracked for more severer comments?","edde76e6":"### 7.2 Stemming and Lemmatization\nstemming might not always returns acutal words, it is used to strip suffix, prefix (return the root of the words)\nbut lemmatization returns actual words.","ddc859d9":"#### identity_hates:","24be198e":"# 6. Finding most frequent used words\nbefore doing the countvectorization we did some basic cleaning including convert everything to lower case, remove ips and usernames","fee3c2ad":"# 2. Comments distribution by category\nThe data is highly imbalanced","150133d4":"### 6.4 Combinations for the top words\nwe are interested to see the combinations because this might be our future feature engineering work, here we are just showing the exploration process, which might be very interesting to you. :)","6236327c":"### 3.3 Correlation matrix by category\n#### The correlation matrix shows interesting things :\n\n'toxic' is clearly correlated with 'obscene' and 'insult' (0.68 and 0.65)\n'toxic' and 'severe_toxic' are only got a 0.31 correlation factor\n'insult' and 'obscene' have a correlation factor of 0.74\nFrom my point of view, there are several combinations that are worth digging into :\n\n'toxic' <-> 'severe_toxic'. The semantic of these two categories seems to show some kind of graduation between them\n'toxic' <-> 'insult' and 'toxic' <-> 'obscene'\n'insult' <-> 'obscene'","c92b1084":"# 4. Wordclouds - Frequent words:\nNow, let's take a look at words that are associated with these classes. The visuals here are word clouds (ie) more frequent words appear bigger. A cool way to create word clouds with funky pics is given here. It involves the following steps.\n\n* Search for an image and its base 64 encoding\n* Paste encoding in a cell and convert it using codecs package to image\n* Create word cloud with the new image as a mask\nA simpler way would be to create a new kaggle dataset and import images from there.","16950d83":"### 2.2 check the percentage of 1 in each category"}}