{"cell_type":{"1f3b4953":"code","f1ce40d1":"code","dec97fe9":"code","a91f7e0d":"code","6ac31c2f":"code","90103e3f":"code","922d009e":"code","b890161e":"code","7b604c15":"code","0d6af7ae":"markdown","ecbb4bc8":"markdown","74430228":"markdown","c15c4f73":"markdown","2cee850e":"markdown","0ea83ee7":"markdown","1cfec9b2":"markdown","b3d3cacd":"markdown","538d4f74":"markdown","72552b53":"markdown","b25e4b07":"markdown","31b670eb":"markdown","4e89910e":"markdown","d2b27bbf":"markdown","5f5e8e57":"markdown","76d20581":"markdown","d67bf355":"markdown","065064f4":"markdown","2b5cf484":"markdown","00695e34":"markdown","f737760c":"markdown","e3147206":"markdown","d290e18a":"markdown","4e526cc0":"markdown","4185d15f":"markdown","2963c92f":"markdown","a27cffd8":"markdown","178b7f36":"markdown","e77e1745":"markdown","d465400a":"markdown","b510c677":"markdown","dc8e69d5":"markdown","23e6a332":"markdown","78657c5e":"markdown","991593ad":"markdown","9179254f":"markdown","7cd53c87":"markdown","5ff78a22":"markdown","ac65a6ed":"markdown","57f000db":"markdown","e4e1925b":"markdown","7ef4d7d2":"markdown","fc1bcc1f":"markdown","bed88879":"markdown","875005c4":"markdown","3c519115":"markdown","bd4ede12":"markdown","f30fd0fe":"markdown","0cfadfd0":"markdown","77ec07a0":"markdown","bdafbe9b":"markdown","2a3cd642":"markdown","e33228ce":"markdown","cecc6c22":"markdown","12887ef4":"markdown"},"source":{"1f3b4953":"import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom matplotlib import rcParams\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nrcParams[\"font.size\"] = 15\n\niris = sns.load_dataset(\"iris\").dropna()\npenguins = sns.load_dataset(\"penguins\").dropna()\n\ni_input, i_target = iris.drop(\"species\", axis=1), iris[[\"species\"]]\np_input, p_target = penguins.drop(\"body_mass_g\", axis=1), penguins[[\"body_mass_g\"]]\n\np_input = pd.get_dummies(p_input)\n\nle = LabelEncoder()\ni_target = le.fit_transform(i_target.values.ravel())\n\nX_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n    i_input, i_target, test_size=0.2, random_state=1121218\n)\n\n\nX_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n    p_input, p_target, test_size=0.2, random_state=1121218\n)","f1ce40d1":"import xgboost as xgb\n\n# Regression\nreg = xgb.XGBRegressor()\n# Classification\nclf = xgb.XGBClassifier()","dec97fe9":"reg1 = xgb.XGBRegressor(random_state=1).fit(X_train_p, y_train_p)\nreg2 = xgb.XGBRegressor(random_state=2).fit(X_train_p, y_train_p)\n\nreg1.score(X_test_p, y_test_p) == reg2.score(X_test_p, y_test_p)","a91f7e0d":"reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=1000)\n\nreg = reg.fit(\n    X_train_p,\n    y_train_p,\n    eval_set=[(X_test_p, y_test_p)],\n    early_stopping_rounds=5,\n)","6ac31c2f":"clf = xgb.XGBClassifier(\n    objective=\"multi:softprob\", n_estimators=200, use_label_encoder=False\n)\n\neval_set = [(X_test_i, y_test_i)]\n\n_ = clf.fit(\n    X_train_i,\n    y_train_i,\n    eval_set=eval_set,\n    eval_metric=[\"auc\", \"mlogloss\"],\n    early_stopping_rounds=5,\n)","90103e3f":"reg = xgb.XGBRegressor(\n    objective=\"reg:squaredlogerror\", n_estimators=1000, learning_rate=0.01\n)\n\neval_set = [(X_test_p, y_test_p)]\n_ = reg.fit(\n    X_train_p,\n    y_train_p,\n    eval_set=eval_set,\n    early_stopping_rounds=10,\n    eval_metric=\"rmsle\",\n    verbose=False,\n)","922d009e":"from sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import KFold\n\ncv = KFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=1121218,\n)\n\nfold = 0\nscores = np.empty(5)\nfor train_idx, test_idx in cv.split(p_input, p_target):\n    print(f\"Started fold {fold}...\")\n    # Create the training sets from training indices\n    X_cv_train, y_cv_train = p_input.iloc[train_idx], p_target.iloc[train_idx]\n    # Create the test sets from test indices\n    X_cv_test, y_cv_test = p_input.iloc[test_idx], p_target.iloc[test_idx]\n    # Init\/fit XGB\n    model = xgb.XGBRegressor(\n        objective=\"reg:squarederror\", n_estimators=10000, learning_rate=0.05\n    )\n    model.fit(\n        X_cv_train,\n        y_cv_train,\n        eval_set=[(X_cv_test, y_cv_test)],\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    # Generate preds, evaluate\n    preds = model.predict(X_cv_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_cv_test, preds))\n    print(\"RMSLE of fold {}: {:.4f}\\n\".format(fold, rmsle))\n    scores[fold] = rmsle\n    fold += 1\n\nprint(\"Overall RMSLE: {:.4f}\".format(np.mean(scores)))","b890161e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Make a simple pipeline\nxgb_pipe = Pipeline(\n    steps=[\n        (\"scale\", StandardScaler()),\n        (\"clf\", xgb.XGBClassifier(objective=\"multi:softmax\", use_label_encoder=False)),\n    ]\n)","7b604c15":"_ = xgb_pipe.fit(\n    X_train_i.values,\n    y_train_i,  # Make sure to pass the rest after the data\n    clf__eval_set=[(X_test_i.values, y_test_i)],\n    clf__eval_metric=\"mlogloss\",\n    clf__verbose=False,\n    clf__early_stopping_rounds=10,\n)","0d6af7ae":"> The rest of the references to XGBoost algorithms mainly imply the Sklearn-compatible XGBRegressor and XGBClassifier (or similar) estimators.\n\nThe estimators have the `random_state` parameter (the alternative seed has been deprecated but still works). However, running XGBoost with default parameters will yield identical results even with different seeds.","ecbb4bc8":"## 15. What are the most important hyperparameters in XGBoost?","74430228":"## 17. How to tune min_child_weight in XGBoost?","c15c4f73":"You can specify various evaluation metrics using the `eval_metric` of the fit method. Passed metrics only affect internally - for example, they are used to assess the quality of the predictions during early stopping.\n\nYou should change the metric according to the objective you choose. You can find the full list of objectives and their supported metrics on [this page](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#learning-task-parameters) of the documentation. \n\nBelow is an example of an XGBoost classifier with multi-class log loss and ROC AUC as metrics:","2cee850e":"Each time XGBoost adds a new tree to the ensemble, it is used to correct the residual errors of the last group of trees.\n\nThe problem is that this approach is fast and powerful, making the algorithm quickly learn and overfit the training data. So, XGBoost or any other gradient boosting algorithm has a `learning_rate` parameter that controls the speed of fitting and combats overfitting.\n\nTypical values for `learning_rate` range from 0.1 to 0.3, but it is possible to go beyond these, especially towards 0.\n\nWhatever value passed to `learning_rate`, it plays as a weighting factor for the corrections made by new trees. So, a lower learning rate means we place less importance on the corrections of the new trees, hence avoiding overfitting.\n\nA good practice is to set a low number for `learning_rate` and use early stopping with a larger number of estimators (`n_estimators`):","0ea83ee7":"Doing CV inside a `for` loop enables you to use evaluation sets and early stopping, while simple functions like `cross_val_score` does not.","1cfec9b2":"This API enables you to integrate XGBoost estimators into your familiar workflow. The benefits are (and are not limited to):\n\n- the ability to pass core XGB algorithms into [Sklearn pipelines](https:\/\/towardsdatascience.com\/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------)\n- using a more efficient cross-validation workflow\n- avoiding the hassles that come with learning a new API, etc.","b3d3cacd":"> XGBoost has 3 types of gradient boosted learners - these are gradient boosted (GB) linear functions, GB trees and DART trees. You can switch the learners using the `booster` parameter.\n\nIf you ask Kagglers, they will choose boosted trees over linear functions on any day (as do I). The reason is that trees can capture non-linear, complex relationships that linear functions cannot.\n\nSo, the only question is which tree booster should you pass to the `booster` parameter - `gbtree` or `dart`?\n\nI won\u2019t bother you with the full differences here. The thing you should know is that XGBoost uses an ensemble of decision tree-based models when used with gbtree booster.\n\nDART trees are an improvement (to be yet validated) where they introduce random dropping of the subset of the decision trees to prevent overfitting.\n\nIn the few small experiments I did with default parameters for `gbtree` and `dart`, I got slightly better scores with dart when I set the `rate_drop` between 0.1 and 0.3.\n\nFor more details, I refer you to [this page](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/dart.html) of the XGB documentation to learn about the nuances and additional hyperparameters.","538d4f74":"> Set `verbose` to False to get rid of the log messages.","72552b53":"## Setup","b25e4b07":"Both regression and classification tasks have different types. They change depending on the objective function, the distributions they can work with, and their loss function.\n\nYou can switch between these implementations with the `objective` parameter. It accepts special code strings provided by XGBoost.\n\nRegression objectives have the `reg:` prefix while classification starts either with `binary:` or `multi:`.\nI will leave it to you to explore the full list of objectives from [this documentation page](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#learning-task-parameters) as there are quite a few.\n\nAlso, specifying the correct objective and metric gets rid of that unbelievably annoying warning you get when fitting XGB classifiers.","31b670eb":"Since we named the XGBoost step as `clf` in the pipeline, every fit parameter should be prefixed with `clf__` for the pipeline to work properly.\n\n> Also, since `StandardScaler` removes the column names of the Pandas DataFrames, XGBoost kept throwing errors because of the mismatch between `eval_set`s and the training data. So, I used `.values` on both sets to avoid that.","4e89910e":"## Summary","d2b27bbf":"If you want to use `fit` parameters of XGBoost within pipelines, you can easily pass them to the pipeline's `fit` method. The only difference is that you should use the `stepname__parameter` syntax:","5f5e8e57":"## 7. What is `early_stopping_rounds` in XGBoost?","76d20581":"## 12. What is the best way of doing cross-validation with XGBoost?","d67bf355":"There are 5 types of algorithms that control tree construction. You should pass `hist` to `tree_method` if you are doing distributed training.\n\nFor other scenarios, the default (and recommended) is `auto` which changes from `exact` for small-to-medium datasets to `approx.` for large datasets.","065064f4":"Finally, this painfully long but hopefully useful article has come to an end.\n\nWe have covered a lot\u200a-\u200ahow to choose the right API, correct objective and metrics for the task, most important parameters of the fit and some valuable XGBoost best practices collected from constantly updated sources such as Kaggle.\n\nIf you have further questions on XGBoost, post them in the comments. I will try to answer faster than dudes on StackExchange sites (a favor not received while writing this article\ud83d\ude09).","2b5cf484":"Early stopping is only enabled when you pass a set of evaluation data to the `fit` method. These evaluation sets are used to keep track of the ensemble's performance from one round to the next.\n\nA tree is trained on the passed training sets at each round, and to see if the score has been improving, it makes predictions on the passed evaluation sets. Here is what it looks like in code:","00695e34":"## 9. When do evaluation metrics have effect in XGBoost?","f737760c":"A more challenging parameter is `gamma`, and for laypeople like me, you can think of it as the complexity control of the model. The higher the gamma, the more regularization is applied.\n\nIt can range from 0 to infinity\u200a-\u200aso, tuning it can be tough. Also, it is highly dependent on the dataset and other hyperparameters. This means there can be multiple optimal gammas for a single model.\n\nMost often, you can find the best gamma within 0\u201320.","e3147206":"# 20 Burning XGBoost FAQs Answered to Use the Library Like a Pro\n## Gradient-boost your XGBoost knowledge by learning these crucial lessons\n![](https:\/\/miro.medium.com\/max\/2000\/1*n6aa_ZbeL5c4O5vvKKJMVg.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/unsplash.com\/@haithemfrd_off?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Haithem Ferdi<\/a>\n        on \n        <a href='https:\/\/unsplash.com\/s\/photos\/boost?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText'>Unsplash.<\/a> All images are by the author unless specified otherwise.\n    <\/strong>\n<\/figcaption>","d290e18a":"## 3. What are objectives in XGBoost and how to specify them for different tasks?","4e526cc0":"From one boosting round to the next, XGBoost builds upon the predictions of the last tree.\n\nIf the predictions do not improve after a sequence of rounds, it is sensible to stop training even if we are not at a hard stop for `num_boost_round` or `n_estimators`.\n\nTo achieve this, XGBoost provides `early_stopping_rounds` parameter. For example, setting it to 50 means we stop the training if the predictions have not improved for the last 50 rounds.\n\nIt is a good practice to set a higher number for `n_estimators` and change early stopping accordingly to achieve better results.\n\nBefore I show an example of how it is done in code, there are two other XGBoost parameters to discuss.","4185d15f":"## 10. What is learning rate (eta) in XGBoost?","2963c92f":"## 18. How to tune gamma in XGBoost?","a27cffd8":"After the 14th iteration, the score starts decreasing. So the training stops at the 19th iteration because 5 rounds of early stopping is applied.\n\nIt is also possible to pass multiple evaluation sets to `eval_set` as a tuple, but only the last pair will be used when used alongside early stopping.\n\n> Check out [this post](https:\/\/machinelearningmastery.com\/avoid-overfitting-by-early-stopping-with-xgboost-in-python\/) to learn more about early stopping and evaluation sets. ","178b7f36":"You will immediately see the effect of slow `learning_rate` because early stopping will be applied much later during training (in the above case, after the 430th iteration).\n\nHowever, each dataset is different, so you need to tune this parameter with hyperparameter optimization.\n\n> Check out [this post](https:\/\/machinelearningmastery.com\/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python\/) on how to tune learning rate. ","e77e1745":"## 13. How to use XGBoost in [Sklearn Pipelines](https:\/\/towardsdatascience.com\/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d)?","d465400a":"## 1. Which API should I choose - Scikit-learn or the core learning API?","b510c677":"After the above 6 parameters, it is highly recommended to tune those that control random sampling. Indeed, random sampling is another method applied in algorithms to further prevent overfitting.\n\n- `subsample`: recommended values [0.5 - 0.9]. The proportion of all training samples to be randomly sampled (without replacement) for each boosting round.\n- `colsample_by*`: parameters that start with this prefix refers to the proportion of columns to be randomly selected for\n  - `colsample_bytree`: each boosting round\n  - `colsample_bylevel`: each depth level achieved in a tree\n  - `colsample_bynode`: each node created or in each split\n\nLike `subsample`, the recommended range is [0.5 - 0.9].","dc8e69d5":"```python\nreg.fit(X_train, y_train)\n\nclf.fit(X_train, y_train)\n```","23e6a332":"As we said, XGBoost is an ensemble of gradient boosted decision trees. Each tree in the ensemble is called a base or weak learner. A weak learner is any algorithm that performs slightly better than random guessing.\n\nBy combining the predictions of multiples of weak learners, XGBoost yields a final, robust prediction (skipping a lot of details now).\n\nEach time we fit a tree to the data, it is called a single boosting round.\n\nSo, to specify the number of trees to be built, pass an integer to `num_boost_round` of the Learning API or to `n_estimators` of the Sklearn API.\n\nTypically, too few trees lead to underfitting, and a too large number of trees lead to overfitting. You will normally tune this parameter with hyperparameter optimization.","78657c5e":"For this, I will give the advice I've got from two different Kaggle Competition Grandmasters.\n\n1. If you give `np.nan` to tree-based models, then, at each node split, the missing values are either send to the left child or the right child of the node, depending on what's best. So, at each split, missing values get special treatment, which may lead to overfitting. A simple solution that works pretty well with trees is to fill in nulls with a value different than the rest of the samples, like -999.\n\n2. Even though packages like XGBoost and LightGBM can treat nulls without preprocessing, it is always a good idea to develop your own imputation strategy.\n\nFor real-world datasets, you should always investigate the type of missingness (MCAR, MAR, MNAR) and choose an imputation strategy (value-based [mean, median, mode] or model-based [KNN imputers or tree-based imputers]).\n\nIf you are not familiar with these terms, I got you covered [here](https:\/\/towardsdatascience.com\/going-beyond-the-simpleimputer-for-missing-data-imputation-dd8ba168d505?source=your_stories_page-------------------------------------).","991593ad":"XGBoost is a real beast.\n\nIt is a tree-based power horse that is behind the winning solutions of many tabular competitions and datathons. Currently, it is the \u201chottest\u201d ML framework of the \u201csexiest\u201d job in the world.\n\nWhile basic modeling with XGBoost can be straightforward, you need to master the nitty-gritty to achieve maximum performance.\n\nWith that said, I present you this article, which is the result of\n- hours of reading the documentation (it wasn't fun)\n- crying through some awful but useful Kaggle kernels\n- hundreds of Google keyword searches\n- completely exhausting my Medium membership by reading a lotta articles\n\nThe post answers 20 most burning questions on XGBoost and its API. These should be enough to make you look like you have been using the library forever.","9179254f":"## 16. How to tune max_depth in XGBoost?","7cd53c87":"No matter what metric you pass to `eval_metric`, it only affects the fit function. So, when you call `score()` on the classifier, it will still yield accuracy, which is the default in Sklearn:","5ff78a22":"## 14. How to improve the default score significantly?","ac65a6ed":"These parameters refer to regularization strength on feature weights. In other words, increasing them will make the algorithm more conservative by placing less importance on features with low coefficients (or weights).\n\n`reg_alpha` refers to L1 regularization of Lasso regression and `reg_lambda` for Ridge regression.\nTuning them can be a real challenge since their values can also range from 0 to infinity.\n\nFirst, choose a wide interval such as [1e5, 1e2, 0.01, 10, 100]. Then, depending on the optimum value returned from this range, choose a few other nearby values.","57f000db":"`max_depth` is the longest length between the root node of the tree and the leaf node. It is one of the most important parameters to control overfitting.\n\nThe typical values range is 3\u201310, but it rarely needs to be higher than 5\u20137. Also, using deeper trees make XGBoost extremely memory-consuming.","e4e1925b":"## 8. What are `eval_set`s in XGBoost?","7ef4d7d2":"## 2. How do I completely control the randomness in XGBoost?","fc1bcc1f":"## 20. How to tune random sampling hyperparameters in XGBoost?","bed88879":"`min_child_weight` controls the sum of weights of all samples in the data when creating a new node. When this value is small, each node will group a smaller and smaller number of samples in each node.\n\nIf it is small enough, the trees will be highly likely to overfit the peculiarities in the training data. So, set a high value for this parameter to avoid overfitting.\n\nThe default value is 1, and its value is only limited to the number of rows in the training data. However, a good range to try for tuning is 2\u201310 or up to 20.","875005c4":"## 4. Which booster should I use in XGBoost - gblinear, gbtree, dart?","3c519115":"## 6. What is a boosting round in XGBoost?","bd4ede12":"## 19. How to tune reg_alpha and reg_lambda in XGBoost?","f30fd0fe":"## 5. Which tree method should I use in XGBoost?","0cfadfd0":"This behavior is because XGBoost induces randomness only when `subsample` or any other parameter that starts with `colsample_by*` prefix is used. As the names suggest, these parameters have a lot to do with [random sampling](https:\/\/towardsdatascience.com\/why-bootstrap-sampling-is-the-badass-tool-of-probabilistic-thinking-5d8c7343fb67?source=your_stories_page-------------------------------------).","77ec07a0":"XGBoost in Python has two APIs \u2014 Scikit-learn compatible (estimators have the familiar `fit\/predict` pattern) and the core XGBoost-native API (a global `train` function for both classification and regression).\n\nThe majority of the Python community, including Kagglers and myself, use the Scikit-learn API.","bdafbe9b":"Hyperparameter tuning should always be the last step in your project workflow.\n\nIf you are short on time, you should prioritize to tune XGBoost's hyperparameters that control overfitting. These are:\n- `n_estimators`: the number of trees to train\n- `learning_rate`: step shrinkage or `eta`, used to prevent overfitting\n- `max_depth`: the depth of each tree\n- `gamma`: complexity control - pseudo-regularization parameter\n- `min_child_weight`: another parameter to control tree depth\n- `reg_alpha`: L1 regularization term (as in LASSO regression)\n- `reg_lambda`: L2 regularization term (as in Ridge regression)","2a3cd642":"Even though XGBoost comes with built-in CV support, always go for the Sklearn CV splitters.\n\nWhen I say Sklearn, I don't mean the basic utility functions like `cross_val_score` or `cross_validate`.\nNo one cross-validates that way in 2021 (well, at least not on Kaggle).\n\nThe method that gives more flexibility and control over the CV process is to use the\u00a0`.split` function of Sklearn CV splitters and implement your own CV logic inside a `for` loop.\n\nHere is what a 5-fold CV looks like in code:","e33228ce":"If you use the Sklearn API, you can include XGBoost estimators as the last step to the pipeline (just like other Sklearn classes):","cecc6c22":"After establishing a base performance with default XGBoost settings, what can you do to boost the score significantly?\n\nMany hastily move on to hyperparameter tuning, but it does not always give that huge jump in the score you want. Often, the improvements from parameter optimization can be marginal.\n\nIn practice, any substantial score increase mostly comes from proper feature engineering and using techniques like model blending or stacking.\n\nYou should spend most of your time feature engineering- effective FE comes from doing proper EDA and having a deep understanding of the dataset. Especially, creating domain-specific features might do wonders to the performance.\n\nThen, try combining multiple models as part of an ensemble. What tends to work reasonably well on Kaggle is to stack the big three\u200a-\u200aXGBoost, CatBoost, and LightGBM.","12887ef4":"## 11. Should you let XGBoost deal with missing values?"}}