{"cell_type":{"79040a48":"code","2216728e":"code","47a6669b":"code","95427e8d":"code","89e78625":"code","851b7dd7":"code","6f194166":"code","fe08d881":"code","052e5f23":"code","cccc19f1":"code","980207ae":"code","e43b7afb":"code","a3dc471f":"code","672fb99d":"code","4955b233":"code","312b6039":"code","8ff58e19":"code","a8ae3795":"code","2123a680":"code","5a3bb4b2":"code","43be6b1f":"code","19184b13":"code","764d7454":"code","03536e95":"code","669e7b53":"code","b0b98107":"code","ccd2041e":"code","88aee16b":"code","1db27624":"code","92286dc6":"code","8f08752a":"code","911b73b5":"code","45ea9380":"code","030135d3":"code","8e448580":"code","e8037a26":"markdown","5b088372":"markdown","d43179b5":"markdown","b99c39ec":"markdown","d18e9b15":"markdown","c8db5c80":"markdown","f06264a9":"markdown","db94997b":"markdown","6575a304":"markdown","d6f99fdf":"markdown","eb2cf01c":"markdown","3c75642e":"markdown","6dddfcb9":"markdown","c464f871":"markdown","cb1d7f4c":"markdown","236b4ea0":"markdown","8e1925ed":"markdown","55461394":"markdown","dca49ce7":"markdown","81f8093b":"markdown","d1ffe197":"markdown","6413b044":"markdown"},"source":{"79040a48":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tqdm\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom keras import Input, Model, Sequential\nfrom tensorflow.keras.regularizers import l2\nfrom keras.layers import Dense, Flatten, InputLayer, Reshape, BatchNormalization, Dropout, Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import plot_model\n\n%matplotlib inline","2216728e":"# Dict of labels\ncategories = {'buildings': 0,\n            'forest': 1,\n            'glacier': 2,\n            'mountain': 3,\n            'sea': 4,\n            'street': 5 }","47a6669b":"def load_images(images_folder, img_size = (128,128), scale=True, pred_set=False):\n\n    # Store paths to images\n    image_path = []\n    for dirname, _, filenames in os.walk(images_folder):\n        for filename in filenames:\n            image_path.append(os.path.join(dirname, filename))\n\n    print(\"There are {} images in {}\".format(len(image_path), images_folder))\n    \n    # Load images and associated labels\n    images = []\n    labels = []\n\n    for path in tqdm.tqdm(image_path):\n\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)    \n        img = cv2.resize(img, img_size) # Resize the images\n\n        img = np.array(img)\n\n        images.append(img)\n        if not pred_set: # pred doesn't have label\n            labels.append(categories[path.split('\/')[-2]]) # last folder before the image name is the category\n\n    images = np.array(images)  \n    images = images.astype(np.int64)\n    \n    if scale:\n        images = images\/255 # scale\n        \n    return image_path, images, np.asarray(labels)","95427e8d":"img_size = (128,128)\nimages_train_folder = os.path.join('\/', 'kaggle', 'input', 'intel-image-classification', 'seg_train')\nimage_path, intel_train_images, y_train = load_images(images_train_folder, img_size=img_size)\n\n# Resize\nintel_train_images = np.array(intel_train_images).reshape(-1,128,128,1)\nintel_train_images.shape","89e78625":"images_test_folder = os.path.join('\/', 'kaggle', 'input', 'intel-image-classification', 'seg_test')\n_, X_test, y_test = load_images(images_test_folder, img_size=img_size)\n\nX_test = np.array(X_test).reshape(-1,128,128,1)\nX_test.shape","851b7dd7":"set(y_train)","6f194166":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(len(image_path),36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(intel_train_images[image_ind]), cmap=plt.cm.binary)","fe08d881":"y_train_df = pd.DataFrame(y_train)\ny_train_df.head()","052e5f23":"y_train_df.value_counts()","cccc19f1":"len(categories)","980207ae":"from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n\ndef build_cnn_model():\n    cnn_model=tf.keras.Sequential([\n      Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=intel_train_images.shape[1:]),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D((2,2)),\n      BatchNormalization(),\n      Dropout(0.4),\n\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n      Conv2D(filters=256,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=256,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=128,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D(2,2),\n      BatchNormalization(),\n      Dropout(0.4),\n        \n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      Conv2D(filters=64,kernel_size=(3,3),activation='relu', padding='same'),\n      MaxPooling2D((2,2)),\n      BatchNormalization(),\n      Dropout(0.4),\n      Flatten(),\n\n      Dense(units=len(categories),activation='softmax')\n    ])\n\n    return cnn_model\n  \nmodel = build_cnn_model()\n# Initialize the model by passing some data through\nmodel.predict(intel_train_images[[0]])\n# Print the summary of the layers in the model.\nprint(model.summary())","e43b7afb":"tf.keras.utils.plot_model(model, show_shapes=True)","a3dc471f":"model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])","672fb99d":"checkpoint_filepath = '\/kaggle\/working\/checkpoint.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    save_freq=56300)","4955b233":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=10, \n    min_delta=0.001, \n    mode='max',\n    restore_best_weights=True\n)","312b6039":"# intel_train_images","8ff58e19":"intel_train_images[0][0]","a8ae3795":"print(intel_train_images.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","2123a680":"history = model.fit(intel_train_images, y_train, \n                    batch_size = 128, \n                    epochs = 1000, \n                    verbose = 1, \n                    validation_data = (X_test, y_test),\n                    callbacks=[model_checkpoint_callback, early_stopping])","5a3bb4b2":"plt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","43be6b1f":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","19184b13":"def predict_class(img):\n    # Resize\n    img = img.reshape(1,128,128,1)\n    # Predict\n    predictions = model.predict(img)\n    true_prediction = [tf.argmax(pred) for pred in predictions]\n    true_prediction = np.array(true_prediction)\n    \n    # Return label corresponding to predicted index\n    return list(categories.keys())[list(categories.values()).index(true_prediction)]\n    ","764d7454":"images_pred_folder = os.path.join('\/', 'kaggle', 'input', 'intel-image-classification', 'seg_pred', 'seg_pred')\n_, X_pred, y_pred = load_images(images_pred_folder, img_size=img_size, scale=True, pred_set=True)\n\nX_pred = np.array(X_pred).reshape(-1,128,128,1)\nX_pred.shape","03536e95":"y_pred.shape","669e7b53":"y_pred","b0b98107":"X_pred[0].shape","ccd2041e":"predict_class(X_pred[0])","88aee16b":"X_pred.shape","1db27624":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(X_pred.shape[0],36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(X_pred[image_ind]), cmap=plt.cm.binary)\n    \n    # Predict and get label\n    label = predict_class(X_pred[image_ind])\n    plt.xlabel(label)","92286dc6":"model.save(\"intel_image_classifier.h5\")","8f08752a":"model_preds = model.predict(X_test)\nmodel_preds=np.argmax(model_preds,axis=1)\nmodel_preds.shape","911b73b5":"y_test.shape","45ea9380":"y_test[0]","030135d3":"model_preds[0]","8e448580":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, model_preds)","e8037a26":"Let's take a look at the categories (target values) :","5b088372":"![sharon-mccutcheon-tn57JI3CewI-unsplash.jpg](attachment:670102c8-6be1-4579-9caa-87a604d9b299.jpg)","d43179b5":"# Intel Image Classification","b99c39ec":"### Some callbacks","d18e9b15":"## Test set","c8db5c80":"## Compile model and proceed to training","f06264a9":"## Training set","db94997b":"Plot some of the images from the train set","6575a304":"## Plot the model","d6f99fdf":"We add some callbacks to save the weights of the model and for early stopping.","eb2cf01c":"How are the categories distributed?","3c75642e":"### Plot some images and predicted labels","6dddfcb9":"Credit photo: @sharonmccutcheon from Unsplash.","c464f871":"# Visualizing","cb1d7f4c":"# Model building","236b4ea0":"**Edit: I made a Flask application for deploying this model. You can find the project [here](https:\/\/github.com\/charbelkindji\/Intel-Image-Classification).**  \n\n**Demo app is available [here](https:\/\/whatsthisimageabout.herokuapp.com).**","8e1925ed":"So we have classes from 0 to five. ","55461394":"# Predict on new data","dca49ce7":"# Accuracy score","81f8093b":"### Plot the performances","d1ffe197":"### Save the model","6413b044":"# Load images"}}