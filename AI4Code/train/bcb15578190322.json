{"cell_type":{"9d9422dd":"code","1a620d22":"code","663c38e5":"code","3c3f61d5":"code","0ed3609d":"code","48588bc3":"code","b9e89c30":"code","d409012f":"code","bf9160da":"code","f27eed9b":"code","5b3980a0":"code","e6067331":"code","27d2e2c8":"code","81f9e32b":"code","20a61051":"code","5173fea5":"code","2f524de5":"code","dda4b80e":"code","1202d82c":"code","f0d4db4d":"code","71a02800":"code","6c84df72":"code","e121a9f4":"code","4c1843a3":"code","40c2fc15":"code","3aa4ae0a":"code","24018791":"code","54cc78c1":"code","e8e972f9":"code","e604d28f":"code","b62a993a":"code","81433b00":"code","c10b7465":"code","7275ff3d":"code","4b5e4fe1":"code","8c74b103":"code","a5754f5b":"code","858c9a0d":"code","0c867666":"code","6eff504b":"code","a717e5b6":"code","11e108a9":"code","79c682aa":"code","b0df06f0":"code","ad41ba53":"code","b71e0493":"code","34368f2b":"code","87808c2c":"markdown","a74dcd09":"markdown","0993cbad":"markdown","ba9681f0":"markdown","b5ccbb01":"markdown","30b2bc83":"markdown","f2119228":"markdown","1b5da7d1":"markdown","3910cc02":"markdown","bc5ceb81":"markdown","437113a7":"markdown","6f6ab94b":"markdown","c1f90882":"markdown","1ca516bf":"markdown","ee5308fe":"markdown"},"source":{"9d9422dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1a620d22":"plt.style.use('seaborn')","663c38e5":"df = pd.read_csv('..\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv')","3c3f61d5":"df.head()","0ed3609d":"df.info()","48588bc3":"df.isna().sum()","b9e89c30":"df.describe()","d409012f":"print(f'Number of duplicate entries = {df[df.duplicated()].size}')","bf9160da":"#The Duplicate Entries are\ndf[df.duplicated()]","f27eed9b":"#We Drop the duplicate values\ndf = df.drop_duplicates(keep='first')\ndf.shape","5b3980a0":"print(f'Number of data samples: {df.shape[0]}')\nprint(f'Number of features: {df.shape[1] - 1}')","e6067331":"print(f'Number of samples of Class 0: {df[\"class\"].value_counts()[0]}')\nprint(f'Number of features of Class 1: {df[\"class\"].value_counts()[1]}')","27d2e2c8":"plt.bar(df['class'].unique(),df['class'].value_counts(), width=0.5)\nplt.title('Target Value Distribution')\nplt.xlabel('Target Class')\nplt.ylabel('Counts for each class')\nplt.xticks([0,1])\nplt.show()","81f9e32b":"df.describe()","20a61051":"x = sns.pairplot(df, hue='class')","5173fea5":"df.hist(bins=20,figsize=(11,9),layout=(2,3))","2f524de5":"X = df.drop(labels=['class'],axis=1)","dda4b80e":"y = df['class']\ny","1202d82c":"from sklearn.model_selection import train_test_split","f0d4db4d":"X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.20, random_state=1)","71a02800":"print(f'X_train shape is {X_train.shape}')\nprint(f'y_train shape is {y_train.shape}')\n\nprint(f'X_test shape is {X_test.shape}')\nprint(f'X_test shape is {y_test.shape}')","6c84df72":"fig,ax = plt.subplots(1,2,sharey=True,sharex=True,figsize=(10,5))\n\nax[0].bar(y_train.unique(),y_train.value_counts(), edgecolor='black')\nax[0].set_title('Training Class Labels Distribution')\nax[0].set_xticks(ticks=[0,1])\nax[0].set_xticklabels(labels=[\"Class 0\",\"Class 1\"])\nax[0].set_ylabel('Counts')\n\nax[1].bar(y_test.unique(),y_test.value_counts(), edgecolor='black',color='yellow')\nax[1].set_title('Test Class Labels Distribution')\nax[1].set_xticks(ticks=[0,1])\nax[1].set_xticklabels(labels=[\"Class 0\",\"Class 1\"])","e121a9f4":"from sklearn.preprocessing import StandardScaler","4c1843a3":"scaler = StandardScaler()\n#! Remember to scale on X_train that is based on mean and std of training data\nscaler.fit(X_train)","40c2fc15":"X_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","3aa4ae0a":"from sklearn.neighbors import KNeighborsClassifier","24018791":"def knn_get_training_testing_scores(neighbour_counts):\n    \n    training_scores = []\n    test_scores = []\n\n    for k_neighbours in neighbour_counts:\n        knn_clf = KNeighborsClassifier(n_neighbors=k_neighbours)\n        knn_clf.fit(X_train_scaled,y_train)\n\n        training_scores.append(knn_clf.score(X_train_scaled, y_train))\n        test_scores.append(knn_clf.score(X_test_scaled, y_test))\n\n    return training_scores, test_scores","54cc78c1":"neighbour_count_parameter = [i for i in range(1,11,1)]\ntraining_scores, test_scores = knn_get_training_testing_scores(neighbour_count_parameter)","e8e972f9":"plt.plot(neighbour_count_parameter,test_scores,label='Training Score')\nplt.plot(neighbour_count_parameter,training_scores, label='Test Score')\n\nplt.xticks(neighbour_count_parameter)\nplt.title('Number of Neighbours vs Train-Test Scores')\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy Scores')\nplt.legend()\nplt.show()","e604d28f":"knn_clf = KNeighborsClassifier(n_neighbors=2)\nknn_clf.fit(X_train_scaled,y_train)","b62a993a":"knn_y_hat = knn_clf.predict(X_test_scaled)\n#knn_y_hat","81433b00":"print(f'Training Set accuracy score : {knn_clf.score(X_train_scaled, y_train)}')\nprint(f'Test Set accuracy score : {knn_clf.score(X_test_scaled, y_test)}')","c10b7465":"from sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import classification_report","7275ff3d":"knn_confusion_matrix = confusion_matrix(y_test,knn_y_hat)","4b5e4fe1":"fig,ax = plt.subplots()\nsns.heatmap(knn_confusion_matrix, annot=True, square=True)\n\nax.set_xlabel('Predicted Class')\nax.set_ylabel('Actual Class')\n\nlabels = ['Class 0', 'Class 1']\nax.set_xticklabels(labels,ha='center', minor=False)\nax.set_yticklabels(labels,ha='center', minor=False)\n\ntitle_string = f'Accuracy Score: {round(knn_clf.score(X_test_scaled, y_test),3)}'\nax.set_title(title_string, size = 13)","8c74b103":"score_report = classification_report(y_test, knn_y_hat, target_names=[\"Class 0\",\"Class 1\"])\nprint(score_report)","a5754f5b":"from sklearn.linear_model import LogisticRegression","858c9a0d":"def get_logistic_regression_train_test_scores(c_parameters):\n    \n    training_scores = []\n    testing_scores = []\n    \n    for c in c_parameters:\n        model = LogisticRegression(C=c)\n        model.fit(X_train_scaled, y_train)\n        training_scores.append(model.score(X_train_scaled,y_train))\n        testing_scores.append(model.score(X_test_scaled,y_test))\n\n    return training_scores,testing_scores","0c867666":"c_parameters = [0.01, 0.1, 1, 10, 100, 1000]\nlr_train_scores, lr_test_scores = get_logistic_regression_train_test_scores(c_parameters)\n\nprint(f'Training Score, Test Scores')\nfor train_score,test_score in zip(lr_train_scores, lr_test_scores):\n    print((train_score,test_score))","6eff504b":"plt.figure(figsize=(8,6))\n\nplt.plot(lr_train_scores, label='Training Score',c='b',marker='o')\nplt.plot(lr_test_scores, label='Test Score',c='g',marker='o')\n\nplt.xticks(ticks=range(len(c_parameters)),labels=['0.01', '0.1', '1', '10', '100', '1000'])\n\nplt.xlabel('Coefficient of Regularization (C)')\nplt.ylabel('Accuracy Score')\n\nplt.legend(loc='lower right')\nplt.show()","a717e5b6":"logit_reg = LogisticRegression(C=10)\nlogit_reg.fit(X_train_scaled, y_train)","11e108a9":"logit_reg_y_hat = logit_reg.predict(X_test_scaled)\n#print(logit_reg_y_hat)","79c682aa":"print(f'Training Set accuracy score : {logit_reg.score(X_train_scaled, y_train)}')\nprint(f'Test Set accuracy score     : {logit_reg.score(X_test_scaled, y_test)}')","b0df06f0":"print(f\"Features  : {list(df.columns[:-1])}\" )\nprint(f\"Weights   : {list(np.round(logit_reg.coef_[0],3))}\")\nprint(f\"Intercept : {np.round(logit_reg.intercept_,2)}\")","ad41ba53":"lr_confusion_matrix = confusion_matrix(y_test,logit_reg_y_hat)","b71e0493":"fig,ax = plt.subplots()\nsns.heatmap(lr_confusion_matrix, annot=True, square=True)\n\nax.set_xlabel('Predicted Class')\nax.set_ylabel('Actual Class')\n\nlabels = ['Class 0', 'Class 1']\nax.set_xticklabels(labels,ha='center', minor=False)\nax.set_yticklabels(labels,ha='center', minor=False)\n\ntitle_string = f'Accuracy Score: {round(logit_reg.score(X_test_scaled, y_test),3)}'\nax.set_title(title_string, size = 13)","34368f2b":"score_report = classification_report(y_test, logit_reg_y_hat, target_names=[\"Class 0\",\"Class 1\"])\nprint(score_report)","87808c2c":"- Smaller values of C implies lots of regularization, making the model simpler and reducing chance of overfitting.\n- As C increases the effect of regularization decreases, thereby making the model more complex.\n- Very high values of C results in little to no regularization, which results to the model overfitting.","a74dcd09":"# 2. Exploring the Data","0993cbad":"## 5.1.2 Perfomace Measures","ba9681f0":"- No missing values seen in the data","b5ccbb01":"- Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n\n- Dataset can be used for Binary Classification sample problems\n\n- Identify if bank note is authentic or not","30b2bc83":"##### We find that for C = 1, 10 the model has good scores","f2119228":"# Bank Note Authentication UCI data","1b5da7d1":"# 5. Machine Learning Models","3910cc02":"# 1. Import the libraries","bc5ceb81":"# 3. Train-Test Splitting","437113a7":"###### We find that k=2 has a good generalization","6f6ab94b":"## 5.2.1 Logistic Regression","c1f90882":"## 5.2.2 Performace Measure","1ca516bf":"# 4. Scaling the Data","ee5308fe":"## 5.1.1 K Nearest Neighbors"}}