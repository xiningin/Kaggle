{"cell_type":{"a563ad10":"code","10fe39bc":"code","22c755ca":"code","f537d3cf":"code","7ee14b88":"code","6eefb49d":"code","4091523e":"code","4159dd5b":"code","16406b58":"markdown","ebb142c2":"markdown","198be97c":"markdown","c71ad0e3":"markdown","6da50ea9":"markdown"},"source":{"a563ad10":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom tqdm import tqdm\nfrom numba import njit, jit\n\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef weighted_average_price(df, N=1):\n    if N==1:\n        return (df.bid_price1 * df.ask_size1 + df.bid_size1 * df.ask_price1) \/ (df.bid_size1 + df.ask_size1)\n    elif N==2:\n        return (df.bid_price2 * df.ask_size2 + df.bid_size2 * df.ask_price2) \/ (df.bid_size2 + df.ask_size2)\n    else:\n        assert False, 'Super duper sumo!'\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","10fe39bc":"base_agg = [np.mean, np.std]\n\nfeature_dictionary = {\n    'bid_price1': base_agg,\n    'bid_price2': base_agg,\n    'ask_price1': base_agg,\n    'ask_price2': base_agg,\n    'bid_size1': base_agg,\n    'bid_size2': base_agg,\n    'ask_size1': base_agg,\n    'ask_size2': base_agg,\n    'wap1': base_agg,\n    'wap2': base_agg,\n    'log_return1': base_agg + [realized_volatility],\n    'log_return2': base_agg + [realized_volatility]\n}\n\n\ndef calculate_features(file_list):\n    lst = []\n    # read the training data file for book\n    for g in tqdm(file_list):\n        df = pd.read_parquet(g)\n        # calculate wap and add as a column\n        df['wap1'] = weighted_average_price(df)\n        df['wap2'] = weighted_average_price(df, N=2)\n        # calculate log-return and add as a column\n        df['log_return1'] = log_return(df.wap1)\n        df['log_return2'] = log_return(df.wap2)\n        # remove null values created from 'diff' and log\n        df = df[~df.log_return1.isnull()]\n\n        # calculate aggregation; rv=realized_volatility\n        rv_per_stock_timeid = df.groupby('time_id').agg(feature_dictionary)\n        # groupby with multiple aggregations create multi-index columns\n        # flatten them\n        rv_per_stock_timeid.columns = ['-'.join(x) for x in rv_per_stock_timeid.columns]\n        # add row-id\n        rv_per_stock_timeid['row_id'] = [f'{g.split(\"=\")[-1]}-{time_id}' for time_id in rv_per_stock_timeid.index]\n        # drop the index which is time-id\n        rv_per_stock_timeid.reset_index(drop=True, inplace=True)\n\n        lst.append(rv_per_stock_timeid)\n\n    return pd.concat(lst)","22c755ca":"rv_per_stock_timeid = calculate_features(\n    glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*'))","f537d3cf":"%%time\n# get the target values; cast type since we don't need the ids to be floats\ntarget = pd.read_csv(\n    '..\/input\/optiver-realized-volatility-prediction\/train.csv',\n    dtype={'stock_id':str, 'time_id':str, 'target':float}\n)\n# create row_id\ntarget['row_id'] = [f'{r.stock_id}-{r.time_id}' for _,r in target.iterrows()]\n# keep only the necessary columns\ntarget = target.loc[:,['row_id','target']]","7ee14b88":"# xy dataframe is the merged result of the features dataframe and target dataframe\nxy = rv_per_stock_timeid.merge(\n    right = target,\n    how = 'left',\n    on = 'row_id'\n)\nprint(xy.shape)","6eefb49d":"xy.head(2)","4091523e":"xtrain, xtest, ytrain, ytest = train_test_split(\n    xy.loc[:,[c for c in xy.columns if c not in ['row_id','target']]],\n    xy.target,\n    test_size=0.1,\n    random_state=42\n)\nprint(len(xtrain), len(xtest))\n\nmdl = LGBMRegressor(random_state=42)\nmdl.fit(xtrain, ytrain)\nprint('rmspe:', rmspe(ytest, mdl.predict(xtest)))","4159dd5b":"list_order_book_file_test = glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\nprint(len(list_order_book_file_test))\nsub = calculate_features(list_order_book_file_test)\nfeats = sub.loc[:,[c for c in sub.columns if c not in ['row_id','target']]]\npred = mdl.predict(feats)\nresult = pd.DataFrame({\n    'row_id': sub.row_id,\n    'target': pred\n})\nresult.to_csv('submission.csv',index = False)","16406b58":"# Submission","ebb142c2":"# Aggregation by per stock per time-id Explanation\n\n- The book table must be aggregated to calculate the `realized_volatility`, also to have the same aggregation level as the submission file.\n- When aggregating, we lose the row level data for all columns.\n- To reduce loss of information, we add a few statistical functions for each column.\n- The feature dictionary below defines which statistical aggregation we will use for each column from book table.","198be97c":"# Model Building, Evaluation and Prediction","c71ad0e3":"## Evaluation","6da50ea9":"# Get Training Target Values"}}