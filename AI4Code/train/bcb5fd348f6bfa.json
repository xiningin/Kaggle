{"cell_type":{"86403454":"code","711c9278":"code","0cf9b7f4":"code","788ae020":"code","76c9a742":"code","82b5f6df":"code","8203f532":"code","a7080a57":"code","8d5e5a8f":"code","f61fd95d":"code","163b4277":"code","7b06d08f":"code","734230a4":"code","4af6493c":"code","ed37aef8":"code","e0730ca7":"code","193d1e06":"code","d1ce0124":"code","106ed4b1":"code","9f1aaf4b":"code","122379b1":"code","0844e378":"code","3ab2c519":"code","147e5ee1":"code","dd6e38e6":"code","cf582bf1":"code","84ca0039":"code","9011bb10":"code","cfbfe056":"code","77313777":"code","a966ab00":"code","90cf2d4f":"code","1979775e":"code","be4af843":"code","2af49b4c":"code","cc0624d8":"code","d9d61657":"code","0f2d9997":"code","d67347da":"code","42378b07":"code","189689ab":"code","81474ba9":"code","673583f6":"code","c83c64fe":"code","c8b20f16":"code","01ae4667":"code","e71e7746":"code","3aa1a8fb":"code","a1e1b6c1":"markdown","0e57e1ca":"markdown","8ac62e57":"markdown","69676890":"markdown","0a26c97b":"markdown","c5a4a8ed":"markdown","223bc4a3":"markdown","ea8ebe70":"markdown","04fd3728":"markdown","f39abcb3":"markdown","cc06ed86":"markdown","bfefe1c3":"markdown","f240ce90":"markdown","52f6bb4f":"markdown","6c6c9cc2":"markdown","93ec5a1a":"markdown","2eac5ce7":"markdown","4ed191c5":"markdown","d4ef16dd":"markdown","7eb54f03":"markdown","79b07858":"markdown","6a121795":"markdown","b9f73a77":"markdown","d1e0038c":"markdown","dca27c04":"markdown","65634eaf":"markdown","d0980bf4":"markdown","440a0615":"markdown","36507ff8":"markdown","6dc5faa0":"markdown","44994982":"markdown","2ded231d":"markdown","c4d37d69":"markdown","9cdb6ba0":"markdown","326b9a83":"markdown","d57b11c4":"markdown","339b7bcb":"markdown","3f75c0bd":"markdown","9cc67c08":"markdown"},"source":{"86403454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#!pip install spacy\n#!python3 -m spacy download en_core_web_sm\n#!pip install wordcloud\n#!pip install cufflinks\nimport os\nimport re\nimport string\nfrom collections import Counter, defaultdict\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nnltk.download('stopwords')#Error loading\nnltk.download('punkt')#Error loading\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import word_tokenize, pos_tag\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport gensim\nfrom gensim.models import phrases, word2vec\n\nimport spacy\nfrom spacy import displacy\nimport en_core_web_sm\nfrom PIL import Image\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\nimport plotly.express as px\n\n#!pip install textblob  #Error installing\n","711c9278":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/dreams\/'): #'\/home\/C00219805\/Learning\/dreams\/'):#\n    for filename in filenames:\n        print(filename)\n        df = pd.read_csv(os.path.join(dirname, filename), header = 0)\n        df.columns = ['id', 'text']\n        print(df.head(3))\n        print(\"Total records in dataset: {}\".format(len(df)))\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cf9b7f4":"print(\"Nulls in Datasets: \")\nprint(df.isnull().sum())\ndf = df[df['text'].notna()]","788ae020":"# word_count\ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\nstop_words = set(stopwords.words('english'))\ndf['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n\n# mean_word_length\ndf['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf['char_count'] = df['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\nprint(df.head(3))\ndf = df[df['mean_word_length'].notna()]","76c9a742":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count',  'mean_word_length', 'char_count', 'punctuation_count']\nfig, axes = plt.subplots(ncols=1, nrows=len(METAFEATURES), figsize=(6, 10), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(df[feature],ax = axes[i], color='green')    \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=6)\n    axes[i].tick_params(axis='y', labelsize=6)\n    axes[i].legend()    \n    axes[i].set_title(f'{feature}', fontsize=8)\n\nplt.show()","82b5f6df":"# Utility Functions for Text Cleaning\n# Import spaCy's language model\nen_model = en_core_web_sm.load()\n#en_model = spacy.load('en', disable=['parser', 'ner'])\n\n# function to lemmatize text\ndef lemmatization(texts):\n    '''Get lemmatized tokens.'''\n    output = []\n    for i in texts:\n        s = [token.lemma_ for token in en_model(i)]\n        output.append(' '.join(s))\n    return output\ndef clean_text(text):\n    '''Text cleaning including punctation and numbers removal.'''\n    if not is_nan(text):\n        text = text.lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text\ndef is_nan(x):\n    '''Checks if an entity is a null value.'''\n    return (x != x)\n\n# Applying the cleaning function to text and remove records with nulls in text\ndf['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\nprint(df['cleaned_text'].head(3))","8203f532":"def get_word_cloud_with_image(data,is_freq, title= None, image = None ):\n    stopwords = set(stop_words)\n    stopwords.add(\".It\")\n    if is_freq:\n        wc = WordCloud( max_words=2000, mask=image, width = 800, height = 800,\n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate_from_frequencies(data)\n    else:\n        wc = WordCloud( max_words=2000, mask=image,\n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(data)\n\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.title('Word Cloud of {}'.format(title), size =20)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","a7080a57":"#Contacenate all text rows into single string\ntext = df['text'].str.cat(sep=' ')\ntokens = word_tokenize(text)\nresult = [i for i in tokens if not i.lower() in stop_words]\n#lemmatized_result = lemmatization(result)\nall_dreams = \" \".join(result)\n\nget_word_cloud_with_image(all_dreams, False , title = 'Dreams data')","8d5e5a8f":"#Utility functions for n-gram generation\ndef get_top_n_grams(corpus, n=None, ngram = 1):\n    vec = CountVectorizer(ngram_range=(ngram, ngram), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef plot_n_grams(n, ngram= 1):\n    common_words = get_top_n_grams(df['cleaned_text'], n=n, ngram = ngram)\n    df4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n    df4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n        kind='bar', yTitle='Count', linecolor='black', title='Top {} {}-grams in dreams'.format(n, ngram))","f61fd95d":"N = 20\nfrom sklearn.feature_extraction.text import CountVectorizer\nplot_n_grams(N, ngram =2)\nplot_n_grams(N, ngram =3)\nplot_n_grams(N, ngram =4)","163b4277":"def prep_text(in_text):\n    return in_text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split()\ndf['clean_token_text'] = df.apply(lambda row: prep_text(row['cleaned_text']), axis=1)\nsentences = df.clean_token_text.values","7b06d08f":"from gensim.models.phrases import Phrases, Phraser\n\nphrases = Phrases(sentences, min_count=1, threshold=1)\nbigram = Phraser(phrases)","734230a4":"print(bigram[df['cleaned_text'][9].split()])\n\ndf['cleaned_text'][9]","4af6493c":"bigram_counter = Counter()\nfor key in list(itertools.chain.from_iterable(bigram[sentences])):\n    if key not in stopwords.words(\"english\") :\n        if len(key.split(\"_\")) > 1 \\\n        and (key.split(\"_\")[0] not in stopwords.words(\"english\") \\\n             and key.split(\"_\")[1] not in stopwords.words(\"english\")):\n            bigram_counter[key] += 1","ed37aef8":"for key, counts in bigram_counter.most_common(20):\n    print('{0: <20} {1}'.format(key, counts))\n#print(bigram_counter['in_cambodia'])","e0730ca7":"model = word2vec.Word2Vec(bigram[sentences], size=50, min_count=3, iter=20)","193d1e06":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=11)\nvocab = list(model.wv.vocab)\n#vocab = bigram_counter.keys()\nclf = tsne.fit_transform(model[vocab])\n\ntmp = pd.DataFrame(clf, index=vocab, columns=['x', 'y'])\n\ntmp.head(3)","d1ce0124":"tmp = tmp.reset_index()\nprint(len(tmp))","106ed4b1":"print(tmp.columns)\ntmp.columns = ['words', 'x', 'y']\ntmp['count'] = tmp['words'].map(bigram_counter)\ntmp= tmp.fillna(0)\nprint(len(tmp[tmp['count'] == 0]))\n#This step will eliminate unigrams, If you want to\ntmp = tmp[tmp['count'] != 0]\nprint(len(tmp[tmp['count'] == 0]))\nprint(len(tmp))","9f1aaf4b":"tmp1 = tmp.sample(150)\nfig = px.scatter(tmp1, x='x', y='y', hover_name='words', text='words', color='words', size = 'count', size_max=45\n                 , template='plotly_white', title='Bigram similarity and frequency'\n                 , color_continuous_scale=px.colors.sequential.Sunsetdark)\nfig.update_traces(marker=dict(line=dict(width=1, color='Gray')))\nfig.update_xaxes(visible=False)\nfig.update_yaxes(visible=False)\nfig.show()","122379b1":"#Pos Tags per dream\npos_tags = df['cleaned_text'].apply(lambda x: pos_tag(word_tokenize(x)))\ndf['pos_tags'] = pos_tags\nprint(pos_tags)","0844e378":"#Collecting all POS tags together in a list\nall_tags = []\nfor sent in pos_tags:\n    for word, tag in enumerate(sent):\n        all_tags.append((word, tag))\npos_counts= Counter([ j[1] for i,j in all_tags])\nprint(pos_counts)\n    ","3ab2c519":"pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\npos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(14,4))\nax.bar(range(len(pos_counts)), pos_sorted_counts);\nax.set_xticks(range(len(pos_counts)));\nax.set_xticklabels(pos_sorted_types);\nax.set_title('Part-of-Speech Tags of Dreams');\nax.set_xlabel('POS Type');","147e5ee1":"noun_counts= Counter([ j[0] for i,j in all_tags if (j[1] == 'NN') or (j[1] == 'NNP')])\nprint(len(noun_counts))","dd6e38e6":"#Deleting 'dream' and 'i' which are highest frequency in nouns of the dreams dataset\nnoun_counts.pop('i', None)\nnoun_counts.pop('dream', None)\n\nget_word_cloud_with_image(noun_counts,True, title= 'Nouns', image = None )","cf582bf1":"adj_counts= Counter([ j[0] for i,j in all_tags if (j[1] == 'JJ') or (j[1] == 'JJR') or (j[1] == 'JJS')])\n#print(adj_counts)","84ca0039":"#Deleting 'dream' and 'i' which are highest frequency in nouns of the dreams dataset\nadj_counts.pop('i', None)\nadj_counts.pop('dream', None)\nadj_counts.pop('other', None)\n\nget_word_cloud_with_image(adj_counts,True, title= 'Adjectives', image = None )","9011bb10":"#Loading the required package from Spacy\nnlp = en_core_web_sm.load()","cfbfe056":"displacy.render(nlp(str(df['text'].values[122])), jupyter=True, style='ent')","77313777":"named_entities = []\nentities = []\nfor sent in df['text'].values:\n    article = nlp(sent)\n    named_entities.append([(X.text, X.label_) for X in article.ents])\n    entities.append(article.ents)\nprint(named_entities)","a966ab00":"named_entities2 = list(itertools.chain.from_iterable(named_entities))\nlabels = [y for x,y in named_entities2]\nnamed_entities_counter = Counter(labels)\nprint(Counter(labels))","90cf2d4f":"#Plot \npos_sorted_types = sorted(named_entities_counter, key=named_entities_counter.__getitem__, reverse=True)\npos_sorted_counts = sorted(named_entities_counter.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(14,4))\nax.bar(range(len(named_entities_counter)), pos_sorted_counts);\nax.set_xticks(range(len(named_entities_counter)));\nax.set_xticklabels(pos_sorted_types, rotation = 'vertical', size = 10);\nax.set_title('Named Entities in Dreams');\nax.set_xlabel('Entity Type');","1979775e":"all_entities_dict = {}\nfor entity_type in named_entities_counter.keys():\n    words_entity_dict = {}\n    for word, entity in named_entities2:\n        if entity == entity_type:\n            if word in words_entity_dict:\n                words_entity_dict[word] += 1\n            else:\n                words_entity_dict[word] = 1\n    all_entities_dict[entity_type] = words_entity_dict\n    \n#print(all_entities_dict)","be4af843":"person_mask = np.array(Image.open(\"\/kaggle\/input\/images\/trump.png\"))\nget_word_cloud_with_image(all_entities_dict['PERSON'], True, 'PERSON', image = person_mask)\n\ngpe_mask = np.array(Image.open(\"\/kaggle\/input\/images3\/world.png\"))\nget_word_cloud_with_image(all_entities_dict['GPE'], True, 'GPE', image = gpe_mask)\n\norg_mask = np.array(Image.open(\"\/kaggle\/input\/images2\/tree.png\"))\nget_word_cloud_with_image(all_entities_dict['ORG'], True, 'ORGANIZATION', image = org_mask)\n","2af49b4c":"# initialize afinn sentiment analyzer\n!pip install afinn\nfrom afinn import Afinn\naf = Afinn()\n\n# compute sentiment scores (polarity) and labels\nsentiment_scores = [af.score(dream) for dream in df['cleaned_text'].values]\nsentiment_category = ['positive' if score > 0 \n                          else 'negative' if score < 0 \n                              else 'neutral' \n                                  for score in sentiment_scores]\n    \nprint(df.columns)    \n# sentiment statistics per news category\ndf_senti = pd.DataFrame([list(df['text']), list(df['cleaned_text']), sentiment_scores, sentiment_category]).T\ndf_senti.columns = ['text', 'cleaned_text', 'sentiment_score', 'sentiment_category']\ndf_senti['sentiment_score'] = df_senti.sentiment_score.astype('float')\ndf_senti.head(5)","cc0624d8":"print(df_senti[['sentiment_score','sentiment_category']].iloc[227])\nprint(df_senti['text'].values[227])\nprint('\\n')\nprint(df_senti[['sentiment_score','sentiment_category']].iloc[4])\nprint(df_senti['text'].values[4])\nprint('\\n')\nprint(df_senti[['sentiment_score','sentiment_category']].iloc[18])\nprint(df_senti['text'].values[18])","d9d61657":"grouped = df_senti.groupby([ 'sentiment_category']).describe().reset_index()\ngrouped","0f2d9997":"#!pip install plotly\ndf_g =pd.DataFrame({'sentiment_category':grouped['sentiment_category'], \n                    'counts': grouped['sentiment_score']['count']})\nfig = px.treemap(df_g,  path=['sentiment_category'], values = 'counts',\n                   title = 'Sentiment Polarity Distribution', \n                color_discrete_sequence = px.colors.qualitative.Dark2)\nfig.data[0].textinfo = 'label+text+value'\nfig.show()","d67347da":"fig = px.histogram(df, x=\"sentiment_score\", title = 'Sentiment Score Distribution',  nbins=100)\nfig.show()","42378b07":"print(df[df['sentiment_score'] == -62]['text'].values[0])\nprint(df[df['sentiment_score'] == 95]['text'].values[0])","189689ab":"def generate_bar_plots_sentiment(sentiment):\n    #print(df.columns)\n    blob = list(itertools.chain.from_iterable(df[df['sentiment_category'] == sentiment]['pos_tags'].values))\n    pos_df = pd.DataFrame(blob, columns = ['word' , 'pos'])\n    common_words = get_top_n_grams(pos_df[(pos_df['pos'] == 'JJ') | \n                                          (pos_df['pos'] == 'JJR') | \n                                          (pos_df['pos'] == 'JJS')]['word'], n = 20, ngram = 1)\n    df1 = pd.DataFrame(common_words, columns = ['words' , 'count'])\n    fig = df1.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n        kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in {} sentiment'.format(sentiment))\n    #fig.update_layout(width=1200, height=500)\n    #fig.show()\n","81474ba9":"#from textblob import TextBlob\ngenerate_bar_plots_sentiment('positive')\ngenerate_bar_plots_sentiment('negative')\ngenerate_bar_plots_sentiment('neutral')","673583f6":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\n#from bokeh.io import output_notebook\n#count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\ncount_vectorizer = TfidfVectorizer(stop_words='english', max_features=40000)\ntext_sample = df['cleaned_text'].values\n\nprint('Dreams before vectorization: {}'.format(text_sample[1]))\n\ndocument_term_matrix = count_vectorizer.fit_transform(text_sample)\n\nprint('Dreams after vectorization: \\n{}'.format(document_term_matrix[1]))\nprint(document_term_matrix.shape[1])","c83c64fe":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)\n\n# Define helper functions\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        #print(t, np.mean(articles_in_that_topic, axis = 0))\n        #articles_in_that_topic = np.vstack(articles_in_that_topic)\n        \n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","c8b20f16":"n_topics = 20\n\ncolormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:n_topics]\n\n#LDA Model Generation\nlda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(document_term_matrix)\nlda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","01ae4667":"terms = count_vectorizer.get_feature_names()\ntopic_names = []\n\nfor i, comp in enumerate(lda_model.components_):\n    terms_comp = zip(terms, comp)\n    #print(comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:4]#Change this number to indicate the #words you want to see per topic\n    print(\"Topic \"+str(i)+\": \")\n    topic = []\n    for t in sorted_terms:\n        topic.append(t[0])\n    print(\" \".join(topic))\n    topic_names .append(\" \".join(topic))\nprint(topic_names)\n","e71e7746":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)\n\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=800, plot_height=800)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=topic_names[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","3aa1a8fb":"print(len(df))\npage = requests.get(\"http:\/\/sleepanddreamdatabase.org\/dream\/search?searchconstraint={}\")\nsoup = BeautifulSoup(page.content, 'html.parser')\ndreams = soup.find_all('div', class_='searchhittext')\nall_dreams = [dream.get_text().replace('\\n\\t\\t\\t\\t\\t\\t','') for dream in dreams]\nall_dreams = [dream.replace('\\n\\t\\t\\t\\t','') for dream in all_dreams]\nall_dreams.extend(df['dream'].values)\nall_dreams = list(dict.fromkeys(all_dreams))\ndf = pd.DataFrame({'dreams_text' :all_dreams})\ndf.to_csv('dreams.csv')\n","a1e1b6c1":"**Detailed Entity Distribution**\n\nNow that we see there are lot of named entities in our data. Let us examine the top named entities of PERSON, ORGANIZATION and GPE, which indicate the entities of people, organisations and places that are in our dreams dataset. I am excluding Cardinal and dates as they do not carry much information for our analysis.\n\nWe will plot word clouds with masked images to see understand the frequencies of the entities.","0e57e1ca":"There are numericals in the text for large number of dreams. These would be cleaned in the later sections.\n\n**Basic Exploratory Data Analysis**\n\nIdentify nulls in the dataset and create any new features if required.","8ac62e57":"Testing our generated bigram model on an example","69676890":"Now that we have our input matrix ready we can start working with our topic modelling algorithms Latent Dirichelet Analysis(LDA).\n\n**Latent Dirichilet Analysis**\n\nLDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n\nFirstly define some utilty functions for generating topic names for the LDA output.","0a26c97b":"We can hover over to see the values in the plot. \n\nThe highest positive value is 95 and highest neagtive value is -62. We can manually examine those dreams. We can get this information from the describe() in the previous code blocks.","c5a4a8ed":"We can see the highlighted tokens and their respective Named Entities. More details about what each named entity represents can be found at https:\/\/spacy.io\/api\/annotation#named-entities\n\nNow let us generate the named entities for all the dreams in the dataset and find out what entities people are dreaming about. \n\n**Overall Entity Distribution**\n\nWe will first see the overall entity distribution in our dreams and later see detailed distribution within each named entity.","223bc4a3":"# **Install and Import Libraries**\n\nInstalling all the required libraries for data handling, plotting, web scarping among others.","ea8ebe70":"# Human Dreams Text Analysis\n\nHi Kagglers! This is my first Notebook on Kaggle. \n\nThis notebook has the analysis of human dreams collected from online websites. \n\nLet me explain my motivation!\n\nI happened to read Freud's theories during this lockdown and got interested in his dream analysis. \n\nI searched for dream datasets and was not able to find good structured ones. So I web scrapped and collected ~30000 dreams. More details about web scrapping is mentioned in the last section Dataset Preparation through Web Scraping.\n\nSince there are no labelled datasets I performed unsupervised analysis of Named Entity Recognition, Sentiment Analysis and Topic Modelling.\n\nPlease share if you have know any labelled dreams datasets. Also, share your feedback in the comments section. \n\nThe notebook is organised in the following order:\n* Install and Import libraries\n* Data Loading and Basic Exploratoty Analysis\n* Dream Analysis using POS Tagging\n* Named Entity Recognition\n    * Overall Distribution\n    * Detailed Distribution\n* Sentiment Analysis\n* Topic Modelling\n* Data Preparation using Web Scraping\n\nEDIT: I updated few plots using Plotly","04fd3728":"The positive dreams are more compared to negative and neutral ones.\n\nLet us plot the sentiment score distrution for further analysis.","f39abcb3":"Using the full bigrams will take a lot of time and also make the plot clumsy. So let us plot it forr a sample of 150 bigrams for analysis.","cc06ed86":"Another popular dreams database is http:\/\/www.dreambank.net\/\n\nThere are some useful links for collecting more data and also for better understanding of dreams database.\n\nhttps:\/\/dreams.ucsc.edu\/Library\/domhoff_2008c.html\nhttps:\/\/www.atlasobscura.com\/articles\/the-stanford-doctor-s-attempt-to-build-a-bank-for-dreams\n\nThanks.","bfefe1c3":"We see that certain sentiments are not rightly assigned. We can do a better job if we have labelled data and use algorithms to understand the latent information of the text. ","f240ce90":"Test for Bi-grams and Tri-grams to see if we can get any better understanding.","52f6bb4f":"Not bad for an almost raw data. We can see that words like dream, people, house etc. \nBut we cannot draw any conclusions from the above wordcloud clearly. \n\nEDIT: I am unable to download nltk stop words and Afinn package. The word cloud has lot of stop words in this run.\n\n# **Generate n-Grams for Analysis**\n\nSo until now we have seen single tokens distribution in the dreams data. \n\nN-grams analysis might give more insights into the data. Lets perform N-gram analysis.\n\nLet us first define a function for N-gram generation.","6c6c9cc2":"Text is our main component in this analysis.\n\n\nLet us first create some meta features like word counts, unique words, stop word count, punctuation count. It might help a for more detailed analysis of dreams. \n\n**Creating Meta Features**","93ec5a1a":"Let us generate at max 20 topics from our dreams using LDA. There are ways to calculate the optimum number of LDA topics. But it is not the scope of this notebook.","2eac5ce7":"We can see names of people in the PERSON word clouds and all the places people are dreaming about in the GPE word cloud. These word clouds are pretty good. But we can see lot of misclassifications in the ORG word cloud.\n\nLet us do Sentiment Analysis on the dreams and clearly find out what each dream is actually about or what is the emotion the dreamer has experienced after a dream by giving it a score.\n\n# **Sentiment Analysis**\n\nSince we do not have any labelled data of dreams and their sentiments, let us do an unsupervised sentiment analysis.\n\nUnsupervised Sentiment Analysis can be done by using knowledgebases, ontologies, databases, and lexicons that have detailed information, specially curated and prepared just for sentiment analysis. We will perform an unsupervised lexicon-based approach.\n\nThere are many available lexicons which are dictionaries created for sentiments.\n\n* AFINN lexicon\n* Bing Liu\u2019s lexicon\n* MPQA subjectivity lexicon\n* SentiWordNet\n* VADER lexicon\n* TextBlob lexicon\n\nEDIT: I initially used AFFINN lexicon. In the latest run I am getting error installing it. ","4ed191c5":"Now I will train a word2vec model to generate bigrams. Let us plot bigrams in a 2-D plot by using their embeddings from word2Vec and later apply it to TNSE for reduction to 2D space and plotting using Ployly.\n\nPrepare the text to use gensim phrases package.","d4ef16dd":"The above generated counts can be used to represent the size element in the plot for the respective bigrams.","7eb54f03":"Here we will pass all the dreams text together.","79b07858":"Okay! We can see that there are issues of anxiety, fear, good, bad, uncomfortable, happy among many others.\n\nDo you think just analysing nouns and adjectives is giving us a full picture?\n\nNow-a-days, NLP is so progressed in understanding the latent information in the given text. Why not exploit that? We can perform Named-Entity Recognition, Sentiment analysis, Topic Modelling on our dreams to get deeper understanding of whats going on.\n\nSince we do not have labelled daa, all the above mentioned tasks are perormed in an unsupervised manner.\n\n# **Named Entity Recognition**\n(Unsupervised)\n\nNamed Entity Recognition(NER) is the task of classifing named entities in the text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. It is the basic step of any Information Extraction task.\n\nIt helps us in answering the questions like who are the dreamers dreaming about, what popular companies are they dreaming about, what places etc. This totally depends on the personal experiences of the dreamers and nothing to do with the general popularity of the persons or organizations or places that show up.\n","6a121795":"**Nouns Analysis**\n\nNow we will extract only nouns from the dreams POS tags and analyse.","b9f73a77":"We can do very eloborate analysis of these meta features in association the demographics and sleep patterns of the dreamer are available. But our focus in this notebook is to study the data in dreams. Hence we will skip it for now.\n\n**Text Cleaning and Preprocessing**\n\nText processing includes removal of puntuation, numbers and converting to lower case. In certain cases lemmatized text is used for some visualizations.","d1e0038c":"Let us plot the distributions of these meta features.","dca27c04":"Now that we have trained our LDA model, let us visualize the topics for interpretability. We should first condense the multi-dimentional LDA topic matrix to 2-dimensions for visualization using TSNE.","65634eaf":"Here, high-dimensional bigrams are represented as two-dimensional representations using a dimensionality reduction technique called t-SNE. Hovering over the above plot gives you a clear picture. One can zoom in\/out the plotly plots.\n\n# Dream Analysis based on POS Tagging\n\nIn dreams we see animals, people, things, places etc. All these come under the category of nouns. \n\nNow let us start with analysing the Part-of-Speech tags by extracting the nouns that our dreamers are dreaming about. ","d0980bf4":"Wow! People are dreaming about places, people in their lives, in general about their life. They dream about 'house', 'room' which may indicate that they are worried about their security.They dream about school, time, girlfriend\/boyfriend\/friend, but definetely about 'something' or 'someone' in their 'life'.\n\n**Adjective Analysis**\n\nOk now that we have seen what different objects\/nouns that people are dreaming about let us see their emotions or feelings. They are nothing but the adjectives in the dreams data.","440a0615":"Let us plot the POS tags distribution in our dataset.\n\nEDIT: Because the stopwords are not removed in the previous step, there is a lot of unnecessary data in this run.","36507ff8":"Edit: I am unable to load nltk and Afinn packages in this run on Kaggle. I might be facing some proxy issues. You can run the full notebook at your end without errors. I redid it on my Jupyter notebook.\n\n\n# Data Loading and Basic Exploratory Analysis\n\nI collected the dataset by web scaraping sleepanddreamsdatabase.org. Since it took a while I saved it to a CSV file and loaded in the input folder here. But the details of web scraping are mentioned in the last section.\n\nThe Sleep and Dream Database(SDDb) contains thousands of dream reports from a wide variety of people, along with information about their sleep patterns and demographic factors like gender, age, and marital status.  \n\nBut in this analysis I used only the dreams text data. In future I plan to collect the sleep patterns and demographics as well for further analysis.","6dc5faa0":"Let us first make a Word Cloud to see what words are present in our dreams data. One can uncomment the lemmatized code line to see only lemmatized words which help in removing the redundancy in the text.\n\nWe should first prepare the text data for wordcloud generation. We can pass all the text together or we can pass words with their frequencies. Also, word clouds can be generated using image masks.\n\nWe will first define a function to create word cloud with all these alternatives.","44994982":"Let us see the raw markup for a specific dream using the displacy package after applying the Spacy's NER model.","2ded231d":"We can infact try with Topic Modelling algorithms like Latent Semantic Analysis(LSA) and Latent Dirichlet Analysis(LDA) to understand the latent topics in our dreams data.\n\nLet's start!\n\n# **Topic Modelling**\n\nAny topic modelling algorithms requires the words in the corpus to be represented as numerical values. The data is represented as a matrix of sentence by word matrix or aka document-term matrix format. Let's preprocess our data using the word counts as the values for each word.","c4d37d69":"The above step takes quite sometime.","9cdb6ba0":"We can make better decisions by calculating the optimal number of LDA topics by plotting log-likehood scores against each num_of_topics. It is not scope of this notebook.\n\n# **Web Scrapping and Dataset Preparation**\n\nThe dataset I prepared is screpped from [sleepanddreamdatabase.org](http:\/\/sleepanddreamdatabase.org)\n\nBelow is the code I used for data scraping.\n\nI intially had a downloaded CSV of 230 dreams from their dataset. That was initially stored in df variable and used as a sample for testing all the above tasks. Later I merged it with the scraped data. You need not worry about that step at all. You can simply use the CSV file of dreams.csv.","326b9a83":"That's cool!\n\nEvery dream has an assigned sentiment.\n\nLet's examine few dreams manually and see if the sentiment is rightly assigned.","d57b11c4":"So we can see that every token is assigned a POS tag. \nOne can find out what each POS tag stands for from https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html","339b7bcb":"Generating topic names","3f75c0bd":"They are pretty long descriptions!\n\nWe can infact plot the frfrequent words of positive, negative and neutral dreams.","9cc67c08":"Since there are a huge number of nouns, a bar plot will not be readable. We will again plot a Word Cloud to analyse. Here we will use the frequencies to plot the word cloud."}}