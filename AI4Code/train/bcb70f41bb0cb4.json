{"cell_type":{"7db2ab24":"code","ac7c9f00":"code","92caacfa":"code","0b714e12":"code","1374640e":"code","ee2cfa29":"code","73bef191":"code","6ac1d84b":"code","76fadc24":"code","573bc06b":"code","9947b5a4":"code","deb0c36b":"code","5d36d5eb":"code","e0e2b2a8":"code","e3bf1330":"code","e0f00f6b":"code","dd6c398c":"code","ba61e3d6":"code","922eb2af":"code","15a3ae9b":"code","c0d51881":"code","270e138f":"code","73536559":"code","a98a6cd6":"code","556ebc3b":"code","fa1f66fb":"code","b01cccd5":"markdown","19ea35f2":"markdown"},"source":{"7db2ab24":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","ac7c9f00":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image","92caacfa":"PATH = '..\/input\/'","0b714e12":"def return_data_label(fileName):\n    df = pd.read_csv(filepath_or_buffer=PATH+fileName)\n    if(fileName != 'test.csv'):\n        label = np.array(df['label'])\n        data = np.array(df[df.columns[1:]],dtype=np.float)\n        new_data = np.reshape(a=data,newshape=(data.shape[0],28,28))\n        return new_data, label\n    else:\n        data = np.array(df,dtype=np.float)\n        new_data = np.reshape(a=data,newshape=(data.shape[0],28,28))\n        return new_data\n        ","1374640e":"trainData, trainLabel = return_data_label('train.csv')\ntestData = return_data_label('test.csv')","ee2cfa29":"#preprocessing the dataset\ntrainData = trainData \/ 255\ntrainData = (trainData - 0.5)\/0.5\n\ntestData = testData \/ 255\ntestData = (testData - 0.5)\/0.5\n\ntrainData = torch.from_numpy(trainData)\ntestData = torch.from_numpy(testData)\ntrainData, testData = trainData.type(torch.FloatTensor), testData.type(torch.FloatTensor)","73bef191":"trainData = trainData.unsqueeze_(dim=1)\ntestData = testData.unsqueeze_(dim=1)","6ac1d84b":"trainDataset = torch.utils.data.TensorDataset(trainData,torch.from_numpy(trainLabel))\ntrainDataLoader = torch.utils.data.DataLoader(trainDataset,batch_size=100,shuffle=False, num_workers=4)\n\n# testDataset = torch.utils.data.TensorDataset(testData)\ntestDataLoader = torch.utils.data.DataLoader(testData,batch_size=100,shuffle=False, num_workers = 4)","76fadc24":"print(\"Training batches == \\n\",len(trainData))\nprint(\"Testing batches == \\n\",len(testData))\n","573bc06b":"#visualizing no. of examples of each type\ndef total_count(loader):\n    totalClassCount = [0,0,0,0,0,0,0,0,0,0]\n\n    for batch_id,(images,labels) in enumerate(loader):\n        for label in labels:\n            totalClassCount[int(label)] += 1\n    return totalClassCount","9947b5a4":"classes = [0,1,2,3,4,5,6,7,8,9]\nprint(\"Digit class = \",classes)\ntotalCount = total_count(trainDataLoader)\n\nfig0, ax0 = plt.subplots()\nax0.barh(y=classes,width=totalCount)\nax0.set_xlabel('# Examples')\nax0.set_ylabel('# Digit Classes')\nax0.set_title('Train Set')\n","deb0c36b":"#Visualizing single digit:\ntemp = trainDataLoader.dataset[0][0].numpy()\ntemp = np.reshape(a=temp,newshape=(temp.shape[1],temp.shape[2]))\nplt.imshow(temp)","5d36d5eb":"#Creating LeNet5 nn class module\n# conv2d => relu => maxpooling => conv2d => relu => maxpooling => fully connected layer(fc)1 \n#=> fc2 => softmax output\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1,6,5)\n        self.conv2 = nn.Conv2d(6,16,5)\n        \n        self.fc1 = nn.Linear(1024,120)\n        self.fc2 = nn.Linear(120,84)\n        self.fc3 = nn.Linear(84,10)\n        self.fc1Size = 0\n        self.toKnowMaxPoolSize= False\n        \n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n        x = F.max_pool2d(F.relu(self.conv2(x)),1)\n        \n        if(self.toKnowMaxPoolSize == True):\n            self.fc1Size = x.size()\n            print(x.size())\n            return\n        #now lets reshape the matrix i.e. unrolling the matrix\n        x = x.view(x.size()[0],-1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","e0e2b2a8":"n1 = Net()\nn1 = n1.cuda()","e3bf1330":"print(n1)\n","e0f00f6b":"#Creating function for training our NNs\ndef train_model(model,mode,decay,criterion,dataloader,optimizer,dictionary,num_epochs=30):\n    #mode = True means model.train() and False is model.eval()\n    #decay = True means decrease LR with no. of epochs \n    \n    totalLoss = []\n    totalLRs = []\n    correct = 0\n    total = 0\n    LR = 0\n    \n    for epoch in range(num_epochs):\n        if(decay == True):\n            for param in optimizer.param_groups:\n                LR = param['lr'] * (0.1**(epoch\/\/7))\n                param['lr'] = LR\n            totalLRs.append(LR)\n            \n        print(\"Epoch = {}\/{} \".format(epoch,num_epochs),end=\" \")\n        for batch_id,(image, label) in enumerate(dataloader):\n            if(mode == True):\n                optimizer.zero_grad()\n                image = torch.autograd.Variable(image)\n                label = torch.autograd.Variable(label)\n                image = image.cuda()\n                label = label.cuda()\n            else:\n                image = torch.autograd.Variable(image)\n                image = image.cuda()\n\n            output = model.forward(image)\n            \n            if(mode == True):\n                loss = criterion(output,label)\n\n            _, predictated = torch.max(output.data,1)\n            \n            if(mode == True):\n                correct += (predictated == label.data).sum()\n                total += label.size(0)\n\n                loss.backward()\n                optimizer.step()\n\n            del image,label\n            \n        torch.cuda.empty_cache()\n        print(\"Loss = {:.5f}\".format(loss.data[0]))\n        totalLoss.append(loss.data[0])\n        \n    dictionary['totalLoss'] = totalLoss\n    dictionary['correct'] = correct\n    dictionary['totalSize'] = total\n    dictionary['totalLRs'] = totalLRs\n    \n    return model,dictionary","dd6c398c":"# forward => loss => backward => upadte weights\nn1.toKnowMaxPoolSize = False   # To print the size of the last maxpool layer.\noptimizer = torch.optim.SGD(n1.parameters(),lr=0.1)\ncriterion = nn.CrossEntropyLoss().cuda()","ba61e3d6":"#Let's first find correct Learning Rate using Learning decay.\n\ndictModel = {}\nn1, dictModel = train_model(model=n1,mode=True,decay=True,criterion=criterion,dataloader=trainDataLoader,optimizer=optimizer,dictionary=dictModel,num_epochs=50)","922eb2af":"# LOSS vs EPOCHS\nplt.plot(dictModel['totalLoss'])","15a3ae9b":"# Loss vs LR \nplt.plot(dictModel['totalLRs'],dictModel['totalLoss'])","c0d51881":"print(\"Accuracy == \",100*(dictModel['correct']\/dictModel['totalSize']))","270e138f":"# forward => loss => backward => upadte weights\nn1.toKnowMaxPoolSize = False     # To print the size of the last maxpool layer.\noptimizer = torch.optim.SGD(n1.parameters(),lr=0.01)\ncriterion = nn.CrossEntropyLoss().cuda()\n\ndictModel = {}\nn1, dictModel = train_model(model=n1,mode=True,decay=False,criterion=criterion,dataloader=trainDataLoader,optimizer=optimizer,dictionary=dictModel,num_epochs=20)","73536559":"plt.plot(dictModel['totalLoss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Train Dataset')","a98a6cd6":"#Accuracy\nprint(\"Accuracy == \",100*(dictModel['correct']\/dictModel['totalSize']))","556ebc3b":"#evaluating test set\n# forward => loss => backward => upadte weights\nn1.toKnowMaxPoolSize = False     # To print the size of the last maxpool layer.\navgLossTest = []\ntotalPrediction = []\n\nfor batch_id,image in enumerate(testDataLoader):\n        image = torch.autograd.Variable(image)\n        image = image.cuda()\n\n        output = n1.forward(image)\n        \n        _, predictated = torch.max(output.data,1)\n        \n        totalPrediction.append(predictated)","fa1f66fb":"temp = [list(x.cpu().numpy()) for x in totalPrediction]\nLabel = []\n\nfor x in temp:\n    for y in x:\n        Label.append(y)\n# with open('res.txt','r+') as fp:\n#     fp.write(str(temp))\nImageId = [t for t in range(1,28001)]\nlen(ImageId)\nsubData = {\n    'ImageId':ImageId,\n    'Label':Label\n}\ndf = pd.DataFrame(data=subData)\ndf.to_csv(path_or_buf='submission.csv',index=False)","b01cccd5":"That's 1","19ea35f2":"It can be seen from the above graph of Loss vs Learning Rate, Loss remains low for the LRs = 0.001,0.002,..0.008 till 0.01. So Keeping LR = 0.005 will still yeild us good results and would help us to converge fast. This method helps us to find Better Learning rate by decreasing LR every epoch. Plotting this graph gives us insights about when the loss is least at which LR."}}