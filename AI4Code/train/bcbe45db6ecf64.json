{"cell_type":{"8ff9a0af":"code","1f454931":"code","c9df763b":"code","e8374d63":"code","8c32a86c":"code","6c96200e":"code","31bda63b":"code","6dfce3b8":"code","d0e7b3db":"code","817f91a6":"code","c7b3853a":"code","9eafaa5a":"code","076a2a26":"code","bb26097a":"code","959662ff":"code","7297d9c3":"code","8099340d":"code","3d619ce6":"code","c8c2173e":"code","839bb0e4":"code","d4cbe52d":"markdown","393ee469":"markdown","cec42b8b":"markdown","aa0b2132":"markdown","ad8655a0":"markdown","3b18ba54":"markdown"},"source":{"8ff9a0af":"import numpy as np \nimport pandas as pd \nimport gensim\nimport datetime\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","1f454931":"df = pd.read_csv('..\/input\/retokenize.csv', index_col=0)\nos.system('read in data')","c9df763b":"NYT_id_list = df[df['publication'] == \"New York Times\"][\"id\"].tolist()\n\ndf_copy_NYT = df.copy(deep=True)\n\ndf_copy_NYT = df_copy_NYT[df_copy_NYT['id'].isin(NYT_id_list)]\n\nprint('New York Times data')\nprint(df_copy_NYT.tail(15))\n\n# New York post\nNYP_id_list = df[df['publication'] == \"New York Post\"][\"id\"].tolist()\n\ndf_copy_NYP = df.copy(deep=True)\n\ndf_copy_NYP = df_copy_NYP[df_copy_NYP['id'].isin(NYP_id_list)]\n\n\n# Breitbart\nBreitbart_id_list = df[df['publication'] == \"Breitbart\"][\"id\"].tolist()\n\ndf_copy_Breitbart = df.copy(deep=True)\n\ndf_copy_Breitbart = df_copy_Breitbart[df_copy_Breitbart['id'].isin(Breitbart_id_list)]\n\ndf_copy_Breitbart.tail(15)\n\n\n#CNN\nCNN_id_list = df[df['publication'] == \"CNN\"][\"id\"].tolist()\n\ndf_copy_CNN = df.copy(deep=True)\n\ndf_copy_CNN = df_copy_CNN[df_copy_CNN['id'].isin(CNN_id_list)]\n\ndf_copy_CNN.tail(15)\n\n\n\n#Guardian\nGuardian_id_list = df[df['publication'] == \"Guardian\"][\"id\"].tolist()\n\ndf_copy_Guardian = df.copy(deep=True)\n\ndf_copy_Guardian = df_copy_Guardian[df_copy_Guardian['id'].isin(Guardian_id_list)]\n\n\n#Vox\nVox_id_list = df[df['publication'] == \"Vox\"][\"id\"].tolist()\n\ndf_copy_Vox = df.copy(deep=True)\n\ndf_copy_Vox = df_copy_Vox[df_copy_Vox['id'].isin(Vox_id_list)]\n","e8374d63":"#BuzzFeed\nBuzz_id_list = df[df['publication'] == \"Buzzfeed News\"][\"id\"].tolist()\n\ndf_copy_Buzz = df.copy(deep=True)\n\ndf_copy_Buzz = df_copy_Buzz[df_copy_Buzz['id'].isin(Buzz_id_list)]\n\n\n#NationalReview\nNR_id_list = df[df['publication'] == \"National Review\"][\"id\"].tolist()\n\ndf_copy_NR = df.copy(deep=True)\n\ndf_copy_NR = df_copy_NR[df_copy_NR['id'].isin(NR_id_list)]\n\n\n#Fox News \nFox_id_list = df[df['publication'] == \"Fox News\"][\"id\"].tolist()\n\ndf_copy_Fox = df.copy(deep=True)\n\ndf_copy_Fox = df_copy_Fox[df_copy_Fox['id'].isin(Fox_id_list)]\n\n#Rueters \nReuters_id_list = df[df['publication'] == \"Reuters\"][\"id\"].tolist()\n\ndf_copy_Reuters = df.copy(deep=True)\n\ndf_copy_Reuters = df_copy_Reuters[df_copy_Reuters['id'].isin(Reuters_id_list)]","8c32a86c":"#Atlantic\nAtlantic_id_list = df[df['publication'] == \"Atlantic\"][\"id\"].tolist()\n\ndf_copy_Atlantic = df.copy(deep=True)\n\ndf_copy_Atlantic = df_copy_Atlantic[df_copy_Atlantic['id'].isin(Atlantic_id_list)]\n\n\n#Atlantic\nBI_id_list = df[df['publication'] == \"Business Insider\"][\"id\"].tolist()\n\ndf_copy_BI = df.copy(deep=True)\n\ndf_copy_BI = df_copy_BI[df_copy_BI['id'].isin(BI_id_list)]\n\n\n#NPR\nNPR_id_list = df[df['publication'] == \"NPR\"][\"id\"].tolist()\n\ndf_copy_NPR = df.copy(deep=True)\n\ndf_copy_NPR = df_copy_NPR[df_copy_NPR['id'].isin(NPR_id_list)]\n\n\n#Talking Points Memo\nTPM_id_list = df[df['publication'] == \"Talking Points Memo\"][\"id\"].tolist()\n\ndf_copy_TPM = df.copy(deep=True)\n\ndf_copy_TPM = df_copy_TPM[df_copy_TPM['id'].isin(TPM_id_list)]\n\n\n#Talking Points Memo\nWP_id_list = df[df['publication'] == \"Washington Post\"][\"id\"].tolist()\n\ndf_copy_WP = df.copy(deep=True)\n\ndf_copy_WP= df_copy_WP[df_copy_WP['id'].isin(WP_id_list)]\n","6c96200e":"split_date = datetime.date(2016,11,8)","31bda63b":"#make pre-election values\npre_c1_text = df_copy_NR[(pd.to_datetime(df_copy_NR['date']) <split_date)][\"content\"].append(pre_NYP).append(df_copy_Breitbart[(pd.to_datetime(df_copy_Breitbart['date']) <split_date)][\"content\"])\n\n\npre_d1_text = df_copy_CNN[(pd.to_datetime(df_copy_CNN['date']) <split_date)][\"content\"].append(df_copy_Atlantic[(pd.to_datetime(df_copy_Atlantic['date']) <split_date)][\"content\"]).append(df_copy_NYT[(pd.to_datetime(df_copy_NYT['date']) <split_date)][\"content\"])\n\n#NYP, Fox, NR\npre_c4_text =  df_copy_NYP[(pd.to_datetime(df_copy_NYP['date']) <split_date)][\"content\"].append(df_copy_Fox[(pd.to_datetime(df_copy_Fox['date']) <split_date)][\"content\"]).append(df_copy_NR[(pd.to_datetime(df_copy_NR['date']) <split_date)][\"content\"])\n#TPM, CNN, NYT\npre_d4_text = df_copy_TPM[(pd.to_datetime(df_copy_TPM['date']) <split_date)][\"content\"].append(df_copy_CNN[(pd.to_datetime(df_copy_CNN['date']) <split_date)][\"content\"]).append(df_copy_NYT[(pd.to_datetime(df_copy_NYT['date']) <split_date)][\"content\"])\n\n#TPM, NYT, Buzz\npre_d5_text = df_copy_TPM[(pd.to_datetime(df_copy_TPM['date']) <split_date)][\"content\"].append(df_copy_NYT[(pd.to_datetime(df_copy_NYT['date']) <split_date)][\"content\"]).append(df_copy_Buzz[(pd.to_datetime(df_copy_Buzz['date']) <split_date)][\"content\"])\n","6dfce3b8":"#make post-election values\npost_c1_text = df_copy_NR[(pd.to_datetime(df_copy_NR['date']) >split_date)][\"content\"].append(pre_NYP).append(df_copy_Breitbart[(pd.to_datetime(df_copy_Breitbart['date']) >split_date)][\"content\"])\n\n\npost_d1_text = df_copy_CNN[(pd.to_datetime(df_copy_CNN['date']) >split_date)][\"content\"].append(df_copy_Atlantic[(pd.to_datetime(df_copy_Atlantic['date']) >split_date)][\"content\"]).append(df_copy_NYT[(pd.to_datetime(df_copy_NYT['date']) >split_date)][\"content\"])\n\n#NYP, Fox, NR\npost_c4_text =  df_copy_NYP[(pd.to_datetime(df_copy_NYP['date']) >split_date)][\"content\"].append(df_copy_Fox[(pd.to_datetime(df_copy_Fox['date']) >split_date)][\"content\"]).append(df_copy_NR[(pd.to_datetime(df_copy_NR['date']) >split_date)][\"content\"])\n\n#TPM, NYT, Buzz\npost_d5_text = df_copy_TPM[(pd.to_datetime(df_copy_TPM['date']) >split_date)][\"content\"].append(df_copy_NYT[(pd.to_datetime(df_copy_NYT['date']) >split_date)][\"content\"]).append(df_copy_Buzz[(pd.to_datetime(df_copy_Buzz['date']) >split_date)][\"content\"])\n","d0e7b3db":"pre_cons = pre_c1_text.append(pre_c4_text)\npre_lib = pre_d1_text.append(pre_d5_text)","817f91a6":"post_cons = post_c1_text.append(post_c4_text)\npost_lib = post_d1_text.append(post_d5_text)","c7b3853a":"vector = TfidfVectorizer(stop_words = 'english')\ntfidf_post_cons = vector.fit_transform(post_cons)\nterms_post_cons = vector.get_feature_names()","9eafaa5a":"vector = TfidfVectorizer(stop_words = 'english')\ntfidf_post_lib = vector.fit_transform(post_lib)\nterms_post_lib = vector.get_feature_names()","076a2a26":"vector = TfidfVectorizer(stop_words = 'english')\ntfidf_pre_lib = vector.fit_transform(pre_lib)\nterms_pre_lib = vector.get_feature_names()","bb26097a":"vector = TfidfVectorizer(stop_words = 'english')\ntfidf_pre_cons = vector.fit_transform(pre_cons)\nterms_pre_cons = vector.get_feature_names()","959662ff":"def get_nmf_topics(vectorizor, feat_names, n_top_words, num_topics):\n    nmf  = NMF(n_components = num_topics)\n    nmf.fit(vectorizor)\n    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n    \n    word_dict = {};\n    for i in range(num_topics):\n        \n        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n        words_ids = nmf.components_[i].argsort()[:-20 - 1:-1]\n        words = [feat_names[key] for key in words_ids]\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n    \n    return pd.DataFrame(word_dict);","7297d9c3":"df = get_nmf_topics(tfidf_pre_cons, terms_pre_cons, 10, 15)\ndf.to_csv('pre_cons.csv')","8099340d":"df = get_nmf_topics(tfidf_post_cons, terms_post_cons, 10, 15)\ndf.to_csv('post_cons.csv')","3d619ce6":"df = get_nmf_topics(tfidf_pre_lib, terms_pre_lib, 10, 15)\ndf.to_csv('pre_lib.csv')","c8c2173e":"df = get_nmf_topics(tfidf_post_lib, terms_post_lib, 10, 15)\ndf.to_csv('post_lib.csv')","839bb0e4":"# K = range(30,40)\n# SSE = []\n# for k in K:\n#     nmf =  NMF(n_components = k)\n#     nmf.fit(tfidf_pre_cons)\n#     SSE.append(nmf.reconstruction_err_)\n    \n# import matplotlib.pyplot as plt\n# plt.plot(K,SSE,'bx-')\n# plt.title('Elbow Method')\n# plt.xlabel('cluster numbers')\n# plt.show()\n# os.system('finished elbow')","d4cbe52d":"**Split Content Based on 2016 Election Date **","393ee469":"**NMF Analysis**","cec42b8b":"**Isolate Articles for Each Publication**","aa0b2132":"**Code for Elbow Visualizations**","ad8655a0":"# NMF Analysis on News Articles \n* contact info: madeleinecheyette@gmail.com\n* input: files of news article data that have been retokenized to include 'hillary_clinton' and 'donald_trump,' data originally from https:\/\/www.kaggle.com\/snapcrack\/all-the-news\n* output: files pre_lib.csv, post_lib.csv, pre_cons.csv, post_cons.csv, which contain 15 NMF topics for liberal & conservative articles before and after the 2016 election\n","3b18ba54":"**Use TFIDF Vecotrizer as Input to NMF**"}}