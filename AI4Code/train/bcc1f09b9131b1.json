{"cell_type":{"256a2d54":"code","11b9f726":"code","8261dd38":"code","1e76fd63":"code","d9c9364e":"code","c8bdad17":"code","f3b1fa87":"code","9c74aaff":"code","c5b86415":"code","c634d6c5":"code","9d37220c":"code","d9272896":"code","720c1722":"code","14089b74":"code","c6adfc0f":"code","7fa70909":"code","63f8f097":"code","b80e2fe9":"code","af4600b7":"code","3dd34c3a":"code","32d031a2":"code","b3007e3b":"code","ae319469":"code","f2b12afe":"code","e512dd0a":"code","2bfa8d42":"code","8203bd59":"markdown","6e3084d2":"markdown","298e3eb4":"markdown","7586aa31":"markdown","141b650a":"markdown","89e9e7e6":"markdown","25620cbe":"markdown","3e8a269f":"markdown","af152505":"markdown","4d9a28e1":"markdown","0f691373":"markdown","f0878984":"markdown","0bca22a1":"markdown","02dd706b":"markdown","0cd6a515":"markdown","4d9c5b6d":"markdown","f7dcec2c":"markdown"},"source":{"256a2d54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11b9f726":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8261dd38":"df=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","1e76fd63":"df.info()","d9c9364e":"df.describe()","c8bdad17":"df.isnull().sum()","f3b1fa87":"dict={}\nfor i in df.columns:\n    dict[i]=df[i].value_counts().shape[0]\npd.DataFrame(dict,index=['unique']).transpose()","9c74aaff":"df.dtypes","c5b86415":"df.hist(figsize=(20,10))\nplt.show","c634d6c5":"fig=plt.figure(figsize=(20,12))\ngs=fig.add_gridspec(2,4)\nax0=fig.add_subplot(gs[0,0])\nax1=fig.add_subplot(gs[0,1])\nax2=fig.add_subplot(gs[0,2])\nax3=fig.add_subplot(gs[0,3])\nax4=fig.add_subplot(gs[1,0])\nax5=fig.add_subplot(gs[1,1])\nax6=fig.add_subplot(gs[1,2])\nax7=fig.add_subplot(gs[1,3])\n\nsns.boxplot(df['Pregnancies'],data=df,ax=ax0)\nsns.boxplot(df['Glucose'],data=df,ax=ax1)\nsns.boxplot(df['BloodPressure'],data=df,ax=ax2)\nsns.boxplot(df['SkinThickness'],data=df,ax=ax3)\nsns.boxplot(df['Insulin'],data=df,ax=ax4)\nsns.boxplot(df['BMI'],data=df,ax=ax5)\nsns.boxplot(df['DiabetesPedigreeFunction'],data=df,ax=ax6)\nsns.boxplot(df['Age'],data=df,ax=ax7)","9d37220c":"!pip install autoviz","d9272896":"!pip install xlrd","720c1722":"from autoviz.AutoViz_Class import AutoViz_Class\nav=AutoViz_Class()","14089b74":"dft = av.AutoViz('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv', \n                 dfte=df,\n                 header=0, \n                 verbose=2, \n                 lowess=False,\n                 chart_format=\"svg\", \n                 max_rows_analyzed=1000, \n                 max_cols_analyzed=10)","c6adfc0f":"X=df.drop('Outcome',axis='columns')\ny=df['Outcome']","7fa70909":"print(X.shape)\nprint(y.shape)","63f8f097":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","b80e2fe9":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","af4600b7":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","3dd34c3a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix,mean_absolute_error,mean_squared_error,r2_score\nmodel=LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",model.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))\nprint(model.get_params())","32d031a2":"from sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier()\nmodel.fit(X_train,y_train)\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,confusion_matrix,r2_score,accuracy_score\ny_pred=model.predict(X_test)\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",model.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))\nprint(model.get_params())\nprint(accuracy_score(y_test,y_pred)*100)","b3007e3b":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nparams={\n    'n_neighbors':[1,3,5,7,9],\n    'weights':['uniform', 'distance'],\n    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']\n}\nmodel2=GridSearchCV(model,param_grid=params,cv=10,n_jobs=None,verbose=True)\nmodel2.fit(X_train,y_train)\ny_pred=model2.predict(X_test)\nprint('best estimator: ',model2.best_estimator_)\nprint('best params: ',model2.best_params_)\nprint('classification report: ',classification_report(y_test,y_pred))\nprint('mae: ',mean_absolute_error(y_test,y_pred))\nprint('mse: ',mean_squared_error(y_test,y_pred))\nprint('r2 score: ',r2_score(y_test,y_pred))\nprint('model2-score: ',model2.score(X_test,y_test)*100)\nprint('accuracy score: ',accuracy_score(y_test,y_pred)*100)","ae319469":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123,criterion='entropy')\n\ndtree.fit(X_train,y_train)\ny_pred=dtree.predict(X_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",dtree.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))","f2b12afe":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=0)\nmodel.fit(X_train,y_train)\ny_pred=dtree.predict(X_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",model.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))\nprint(model.get_params())\nprint('accuracy score',accuracy_score(y_test,y_pred)*100)","e512dd0a":"from sklearn.ensemble import GradientBoostingClassifier\nmodel=GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",model.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))\nprint(model.get_params())\nprint('accuracy score',accuracy_score(y_test,y_pred)*100)","2bfa8d42":"from xgboost import XGBClassifier\nmodel1=XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\nmodel1.fit(X_train,y_train)\ny_pred=model1.predict(X_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Testing Score:\\n\",model1.score(X_test,y_test)*100)\nprint(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\nprint(\"R2 score is:\\n\",r2_score(y_test,y_pred))\nprint(model1.get_params())\nprint('accuracy score',accuracy_score(y_test,y_pred)*100)","8203bd59":"> **XGBoost Classifier**","6e3084d2":"**If You find this notebook useful, *PLEASE UPVOTE ***\n\n# **PIMA Indians Diabetes**\nBackground\nDiabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n# Objective\nWe will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n# Data\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n* Pregnancies: Number of times pregnant\n\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n \n* BloodPressure: Diastolic blood pressure (mm Hg)\n\n* SkinThickness: Triceps skin fold thickness (mm)\n \n* Insulin: 2-Hour serum insulin (mu U\/ml)\n \n* BMI: Body mass index (weight in kg\/(height in m)^2)\n \n* DiabetesPedigreeFunction: Diabetes pedigree function\n \n* Age: Age (years)\n\n* Outcome: Class variable (0 or 1)","298e3eb4":"> **Scaling down the columns**","7586aa31":"# To find null values in the data set","141b650a":"**There are no null values in this DataSet**","89e9e7e6":"> **Decision Tree & Hypertuning**","25620cbe":"> **LogisticRegression & Hypertuning**","3e8a269f":"> **KNN & Hypertuning**","af152505":"**columns and number of Unique values in the columns**","4d9a28e1":"> **checking the datatypes of all the columns in the dataset**\n**all the columns have numeric values so no need to encode the columns**","0f691373":"> **Random Forest Regressor**","f0878984":"# Models","0bca22a1":"> **Preprocessing**","02dd706b":"> **Gradient Boosting Algorithm**","0cd6a515":"> to find the outliers of all the columns ","4d9c5b6d":"# Logistic Regression is performing best for the Diabetes dataset","f7dcec2c":"> Pregnancies maximum range is varing from 0.0-5.0\n> \n> Glucose maximum range is ranging from 75-140\n> \n> BloodPressure range is ranging from 60-75\n> \n> Skin Thickness is widely distributed from 0-40\n> \n> Insulin maximum amount of people having in range 0-200\n> \n> BMI is 20-60\n> \n> DiabetesPedigreeFunction is maximum between 0.0-0.6\n> \n> Age of maximum people is between 20-40 \n> \n> Outcome is eithe 0 or 1\n"}}