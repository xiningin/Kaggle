{"cell_type":{"1f3c93e9":"code","b3f06224":"code","7090062f":"code","753254d6":"code","53a01613":"code","dcbddb11":"code","7d7e229d":"code","3f0722e3":"code","0019467d":"code","7912c47c":"code","037f641d":"code","34360ddc":"code","f7467d8c":"code","5f9d2b96":"markdown","f7c518be":"markdown","30d0434b":"markdown","8211c5ce":"markdown","0e58a7c6":"markdown","eb381824":"markdown","06f289e2":"markdown","35cb7108":"markdown","df969f9a":"markdown","3e8dd66d":"markdown","512b4722":"markdown"},"source":{"1f3c93e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3f06224":"import pandas\n\ndf = pandas.read_csv('\/kaggle\/input\/cusersmarildownloadswinecsv\/wine.csv',  delimiter=';')\ndf.head()","7090062f":"# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nwarnings.filterwarnings('ignore')","753254d6":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","53a01613":"df.select_dtypes([\"object\"]).nunique()","dcbddb11":"df[\"alcohol\"].value_counts()","7d7e229d":"# Encoding split\nX_encode = df.sample(frac=0.20, random_state=0)\ny_encode = X_encode.pop(\"quality\")\n\n# Training split\nX_pretrain = df.drop(X_encode.index)\ny_train = X_pretrain.pop(\"quality\")","3f0722e3":"# Choose a set of features to encode and a value for m\nencoder = MEstimateEncoder(\n    cols=[\"alcohol\"],\n    m=1.0,\n)\n\n\n# Fit the encoder on the encoding split\nencoder.fit(X_encode, y_encode)\n\n# Encode the training split\nX_train = encoder.transform(X_pretrain, y_train)","0019467d":"#See how the encoded feature compares to the target\n\nfeature = encoder.cols\n\nplt.figure(dpi=90)\nax = sns.distplot(y_train, kde=True, hist=False)\nax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"quality\");","7912c47c":"X = df.copy()\ny = X.pop(\"quality\")\nscore_base = score_dataset(X, y)\nscore_new = score_dataset(X_train, y_train)\n\nprint(f\"Baseline Score: {score_base:.4f} RMSLE\")\nprint(f\"Score with Encoding: {score_new:.4f} RMSLE\")","037f641d":"# Try experimenting with the smoothing parameter m\n# Try 0, 1, 5, 50\nm = 0\n\nX = df.copy()\ny = X.pop('quality')\n\n# Create an uninformative feature\nX[\"Count\"] = range(len(X))\nX[\"Count\"][1] = 0  # actually need one duplicate value to circumvent error-checking in MEstimateEncoder\n\n# fit and transform on the same dataset\nencoder = MEstimateEncoder(cols=\"Count\", m=m)\nX = encoder.fit_transform(X, y)\n\n# Results\nscore =  score_dataset(X, y)\nprint(f\"Score: {score:.4f} RMSLE\")","34360ddc":"plt.figure(dpi=90)\nax = sns.distplot(y, kde=True, hist=False)\nax = sns.distplot(X[\"Count\"], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"quality\");","f7467d8c":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke Was here' )","5f9d2b96":"#Apply M-Estimate Encoding\n\nApply a target encoding to your choice of categorical features. Also choose a value for the smoothing parameter m (any value is okay for a correct answer).","f7c518be":"#Really? 0.0000 RMSLE. Why did I get zero?","30d0434b":"Apply a target encoding to your choice of feature. To avoid overfitting, we need to fit the encoder on data heldout from the training set.","8211c5ce":"#This notebook is an exercise in the Feature Engineering course by Ryan Holbrook.\n\nhttps:\/\/www.kaggle.com\/ryanholbrook\/exercise-target-encoding","0e58a7c6":"To see how many times a category occurs in the dataset, you can use the value_counts method. This cell shows the counts for alcohol, but you might want to consider others as well.","eb381824":"Depending on which feature or features you chose, you may have ended up with a score significantly worse than the baseline. In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding.","06f289e2":"#Encoding Categorical Features\n\nFirst you'll need to choose which features you want to apply a target encoding to. Categorical features with a large number of categories are often good candidates. Run this cell to see how many categories each categorical feature in the Portugal Wine dataset has.","35cb7108":"![](https:\/\/miro.medium.com\/max\/2968\/1*dFSpzDQJK50ur8ilFN59mQ.png)towardsdatascience.com","df969f9a":"#Choose Features for Encoding\n\nAlmost any of the nominal features would be worth trying because of the prevalence of rare categories.","3e8dd66d":"Since Count never has any duplicate values, the mean-encoded Count is essentially an exact copy of the target. In other words, mean-encoding turned a completely meaningless feature into a perfect feature.\n\nNow, the only reason this worked is because we trained XGBoost on the same set we used to train the encoder. If we had used a hold-out set instead, none of this \"fake\" encoding would have transferred to the training data.\n\nThe lesson is that when using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!\n\nAs I got 0.0000 RMSLE (input 19)\n\nhttps:\/\/www.kaggle.com\/ryanholbrook\/exercise-target-encoding","512b4722":"#This cell will show you the score of the encoded set compared to the original set."}}