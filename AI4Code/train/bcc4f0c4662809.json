{"cell_type":{"d1c1db7a":"code","dfa7ed4a":"code","526354d8":"code","55c6a791":"code","21fef8c5":"code","bd920943":"code","ea3011b4":"code","ed1bbf4e":"code","6793f8e9":"code","c18b863d":"code","de04ed91":"code","93d2403c":"code","f3b1ae0a":"code","adde77db":"code","0b85c0a7":"code","4ce7af0f":"code","f8104f14":"code","954cc571":"code","c8fab140":"code","9c0bec6f":"code","20c32992":"code","38a2db28":"code","57accb5f":"code","c742f047":"code","7a3a7392":"code","75c70b78":"code","edfcf349":"code","7b50f705":"code","bc054762":"code","1f200ffc":"code","ba367045":"code","40462e63":"code","74011445":"code","715da2a7":"code","02c33c53":"code","a98d3ee0":"code","76aa143c":"code","c1880963":"code","2c98dbd6":"code","3c2327d7":"code","59448e3a":"code","fa7de4ec":"code","5e0cc631":"code","ed74073b":"code","e9d306c5":"code","137df917":"markdown","8d369bdd":"markdown","e2ea192e":"markdown","63ed5a71":"markdown","7bd5323e":"markdown","1efcb79c":"markdown","118bf480":"markdown","03be3a3e":"markdown","744126e1":"markdown","06c53432":"markdown","ddc259f8":"markdown","060a5992":"markdown","413ad71b":"markdown","79f0c82d":"markdown","50f96273":"markdown","e6fe1a64":"markdown","793cad67":"markdown","2b524776":"markdown","55fd28c7":"markdown","18b4a89c":"markdown","0f693f1a":"markdown","aad97d35":"markdown","b4ab6a22":"markdown","05a70419":"markdown","27afec1b":"markdown","4faf3c49":"markdown"},"source":{"d1c1db7a":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/AfK9LPNj-Zo\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","dfa7ed4a":"import numpy as np\nimport random\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nfrom timeit import timeit\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n\n#color\nfrom colorama import Fore, Back, Style\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","526354d8":"def seed_everything(seed): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(2000)","55c6a791":"ROOT = '..\/input\/osic-pulmonary-fibrosis-progression'\n\ntrain_df = pd.read_csv(f'{ROOT}\/train.csv')\nprint(f'Train data has {train_df.shape[0]} rows and {train_df.shape[1]} columnns and looks like this:')\n","21fef8c5":"train_df.sample(5)","bd920943":"train_unique_df = train_df.drop_duplicates(subset = ['Patient'], keep = 'first')\n#train_unique_df.head()","ea3011b4":"# CHECK FOR DUPLICATES & DEAL WITH THEM\n# keep = False: All duplicates will be shown\ndupRows_df = train_df[train_df.duplicated(subset = ['Patient', 'Weeks'], keep = False )]\n# dupRows_df.head()","ed1bbf4e":"train_df.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","6793f8e9":"print(f'So there are {dupRows_df.shape[0]} (= {dupRows_df.shape[0] \/ train_df.shape[0] * 100:.2f}%) duplicates.')","c18b863d":"test_df = pd.read_csv(f'{ROOT}\/test.csv')\nprint(f'Test data has {test_df.shape[0]} rows and {test_df.shape[1]} columnns, has no duplicates and looks like this:')\ntest_df.head()","de04ed91":"## CHECK SUBMISSION FORMAT\nsub_df = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\nprint(f\"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns.\")","93d2403c":"# sub_df.head()","f3b1ae0a":"# split Patient_Week Column and re-arrage columns\nsub_df[['Patient','Weeks']] = sub_df.Patient_Week.str.split(\"_\",expand = True)\nsub_df =  sub_df[['Patient','Weeks','Confidence', 'Patient_Week']]","adde77db":"sub_df = sub_df.merge(test_df.drop('Weeks', axis = 1), on = \"Patient\")","0b85c0a7":"# introduce a column to indicate the source (train\/test) for the data\ntrain_df['Source'] = 'train'\nsub_df['Source'] = 'test'\n\ndata_df = train_df.append([sub_df])\ndata_df.reset_index(inplace = True)\n#data_df.head()","4ce7af0f":"def get_baseline_week(df):\n    # make a copy to not change original df    \n    _df = df.copy()\n    # ensure all Weeks values are INT and not accidentaly saved as string\n    _df['Weeks'] = _df['Weeks'].astype(int)\n    # as test data is containing all weeks, \n    _df.loc[_df.Source == 'test','min_week'] = np.nan\n    _df[\"min_week\"] = _df.groupby('Patient')['Weeks'].transform('min')\n    _df['baselined_week'] = _df['Weeks'] - _df['min_week']\n    \n    return _df   ","f8104f14":"data_df = get_baseline_week(data_df)\n#data_df.head()","954cc571":"def get_baseline_FVC_old(df):\n    # copy the DF to not in-place change the original one\n    _df = df.copy()\n    # get only the rows containing the baseline (= min_weeks) and therefore the baseline FVC\n    baseline = _df.loc[_df.Weeks == _df.min_week]\n    baseline = baseline[['Patient','FVC']].copy()\n    baseline.columns = ['Patient','base_FVC']      \n    \n    # fill the df with the baseline FVC values\n    for idx in _df.index:\n        patient_id = _df.at[idx,'Patient']\n        _df.at[idx,'base_FVC'] = baseline.loc[baseline.Patient == patient_id, 'base_FVC'].iloc[0]\n    _df.drop(['min_week'], axis = 1)\n    \n    return _df","c8fab140":"def get_baseline_FVC(df):\n    # same as above\n    _df = df.copy()\n    base = _df.loc[_df.Weeks == _df.min_week]\n    base = base[['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    # add a row which contains the cumulated sum of rows for each patient\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    # drop all except the first row for each patient (= unique rows!), containing the min_week\n    base = base[base.nb == 1]\n    base.drop('nb', axis = 1, inplace = True)\n    \n    # merge the rows containing the base_FVC on the original _df\n    _df = _df.merge(base, on = 'Patient', how = 'left')    \n    _df.drop(['min_week'], axis = 1)\n    \n    return _df","9c0bec6f":"def old_baseline_FVC():\n    return get_baseline_FVC_old(data_df)\n    pass\n\ndef new_baseline_FVC():\n    return get_baseline_FVC(data_df)\n    \n\nduration_old = timeit(old_baseline_FVC, number = 3)\nduration_new = timeit(new_baseline_FVC, number = 3)\n\n#print(f\"Taking the old, non-vectorized version took {duration_old \/ 3:.2f} sec, while the vectorized version only took {duration_new \/ 3:.3f} sec. That's {duration_old\/duration_new:.0f} times faster!\" )","20c32992":"data_df = get_baseline_FVC(data_df)\ndata_df.head()","38a2db28":"def own_MinMaxColumnScaler(df, columns):\n    \"\"\"Adds columns with scaled numeric values to range [0, 1]\n    using the formula X_scld = (X - X.min) \/ (X.max - X.min)\"\"\"\n    for col in columns:\n        new_col_name = col + '_scld'\n        col_min = df[col].min()\n        col_max = df[col].max()        \n        df[new_col_name] = (df[col] - col_min) \/ ( col_max - col_min )","57accb5f":"def own_OneHotColumnCreator(df, columns):\n    \"\"\"OneHot Encodes categorical features. Adds a column for each unique value per column\"\"\"\n    for col in cat_attribs:\n        for value in df[col].unique():\n            df[value] = (df[col] == value).astype(int)","c742f047":"## APPLY DEFINED TRANSFORMATIONS\n# define which attributes shall not be transformed, are numeric or categorical\nno_transform_attribs = ['Patient', 'Weeks', 'min_week']\nnum_attribs = ['FVC', 'Percent', 'Age', 'baselined_week', 'base_FVC']\ncat_attribs = ['Sex', 'SmokingStatus']\n\nown_MinMaxColumnScaler(data_df, num_attribs)\nown_OneHotColumnCreator(data_df, cat_attribs)\n\ndata_df[data_df.Source != \"train\"].head()","7a3a7392":"# get back original data split\ntrain_df = data_df.loc[data_df.Source == 'train']\nsub = data_df.loc[data_df.Source == 'test']","75c70b78":"######## BASIC CONFIG --- THIS IS GOING TO BE OVERWRITTEN BY OUR PARAMETER SEARCH BELOW ########\n#the purpose of this section is to set a base-model and initialize all values. The (random)-Grid search is going to overwrite many of those values.\n# be careful, the resulsts are VERY SEED-DEPENDEND!\nseed_everything(1949)\n\n\n### Features: choose which features you want to use\nfeatures_list = ['baselined_week_scld', 'Percent_scld', 'Age_scld', 'base_FVC_scld', 'Male', 'Female', 'Ex-smoker', 'Never smoked', 'Currently smokes']\n\n### Basics for training:\nNFOLDS = 5\nEPOCHS = 1000\nBATCH_SIZE = 96\n\n\n### LOSS; set tradeoff btw. Pinball-loss and adding score\n_lambda = 0.8 # 0.8 default\n\n\n### Optimizers\n# choose ADAM or SGD\noptimizer = 'ADAM'\n\n### Learning Rate Scheduler\ndef get_lr_callback(BATCH_SIZE = 64,\n                    lr_start   = 0.000001,\n                    lr_max     = 0.00001,\n                    lr_min     = 0.000001,\n                    lr_ramp_percent =  0.3,\n                    plot = False):\n    \"\"\"Returns a lr_scheduler callback which is used for training.\n    Feel free to change the values below!\n    \"\"\"\n    lr_start   = lr_start\n    lr_max     = lr_max * BATCH_SIZE # higher batch size --> higher lr\n    lr_min     = lr_min\n    # lr_ramp_percent defines which percentage of all epochs are used for ramping up the LR to the max\n    lr_ramp_ep = EPOCHS * lr_ramp_percent\n    lr_sus_ep  = 0\n    lr_decay   = 0.991\n\n    def lr_scheduler(epoch):\n            if epoch < lr_ramp_ep:\n                lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n\n            elif epoch < lr_ramp_ep + lr_sus_ep:\n                lr = lr_max\n\n            else:\n                lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n            return lr\n    \n    if plot == False:\n        # get the Keras-required callback with our LR for training\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose = False)\n        return lr_callback \n    \n    else: \n        return lr_scheduler\n    \n# plot & check the LR-Scheulder for sanity-check\nlr_scheduler_plot = get_lr_callback(BATCH_SIZE = 64, plot = True)\nrng = [i for i in range(EPOCHS)]\ny = [lr_scheduler_plot(x) for x in rng]\nplt.plot(rng, y)\nprint(f\"Default Learning rate scheduler: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}\")\n\n\n# logging & saving\nLOGGING = True\n\n# defining custom callbacks\nclass LogPrintingCallback(tf.keras.callbacks.Callback):\n    \n    # defining a class variable to ensure LogPrinting, evaluation & saving is consitent\n    # choose between 'val_score' and 'val_loss'\n    optimization_variable = 'val_loss'\n    \n    def on_train_begin(self, logs = None):\n        #print(\"Training started for this fold\")\n        self.val_loss = []\n        self.val_score = []        \n        \n    def on_epoch_end(self, epoch, logs = None):\n        self.val_loss.append(logs['val_loss']) \n        self.val_score.append(logs['val_score'])\n        \"\"\"if epoch % 1000 == 0 or epoch == (EPOCHS -1 ):\n            print(f\"The average val-loss for epoch {epoch} is {logs['val_loss']:.2f}\"\n                  f\" and the score is {logs['val_score']}\")\"\"\"\n            \n    def on_train_end(self, logs = None):\n        # get index of best epoch\n        monitored_variable = self.val_loss if LogPrintingCallback.optimization_variable == 'val_loss' else self.val_score\n        best_epoch = np.argmin(monitored_variable)        \n        # get score in best epoch\n        best_result_loss = self.val_loss[best_epoch]\n        best_result_score = self.val_score[best_epoch]\n        print(f\"Best model was found and saved in epoch {best_epoch + 1} with val_loss: {best_result_loss} and val_score: {best_result_score} \") \n        \n        \ndef get_checkpoint_saver_callback(iteration, fold):\n    checkpt_saver = tf.keras.callbacks.ModelCheckpoint(\n        f'iteration-{i}_fold-{fold}.h5',\n        monitor = LogPrintingCallback.optimization_variable,  # can be 'val_score' or 'val_loss'\n        verbose = 0,\n        save_best_only = True,\n        save_weights_only = True,\n        mode = 'min',    \n        save_freq = 'epoch')\n    \n    return checkpt_saver","edfcf349":"def swap (a,b):\n     return b, a","7b50f705":"######## (RANDOM) GRID SEARCH ########\n# this value determins how many random combinations out of our param_grid are used for training\nITERATIONS = 60\n\nparam_grid = {\n    'NFOLDS'    : [5],\n    'EPOCHS'    : [800],\n    'batch_size': [32, 64, 128, 256],\n    'optimizer' : ['SGD', 'ADAM'],\n    'dropout1' :  np.linspace(0.1, 0.5, 5),\n    'dropout2' :  np.linspace(0.1, 0.4, 5),\n    # _labmda is used in the loss function as trade-off for qloss and score\n    '_lambda'   : np.linspace(0.5, 0.9, 5),\n    'lr_start'  : np.linspace(0.00001, 0.0001, 5),\n    # remember that lr_max is multiplied with BATCH_SIZE!\n    'lr_max'    : np.linspace(0.00001, 0.0001, 5),\n    'lr_min'    : np.linspace(0.00001, 0.0001, 5),\n    # below value defines which percentage of all epochs are used for ramping up the LR to the max\n    # and then declining starts\n    'lr_ramp_percent': [0.3],\n}\n\n# iterate over the param_grid: set the values & print them out\nfor key, value in param_grid.items():\n    exec(key + '= value[random.randint(0, len(value) - 1)]')\n    print(f'{key} = {eval(key)}')\n\n# squish illogical results\nif (lr_min > lr_max * BATCH_SIZE):\n    swap(lr_min,lr_max)\n    print(f'lr_min: {lr_min}, lrmax: {lr_max}')\nif (lr_start > lr_max * BATCH_SIZE):\n    print(f'lr_start: {lr_start}, lrmax: {lr_max}' )","bc054762":"# count possible combinations \ncombinations = 1\nfor x in param_grid.values():\n    combinations *= len(x)\nprint(f'There are {combinations} possible combinations to be calculated for {NFOLDS} folds!')\n\n## calc. runtime in seconds, measured by %timeit\n# average runtime is 415 sec\/iteration with 1k epochs and 5 folds\navg_runtime_1k_epochs = 415\ntotal_runtime = avg_runtime_1k_epochs * EPOCHS \/ 1000 * NFOLDS \/ 5 * combinations\ntotal_runtime_iterations = avg_runtime_1k_epochs * EPOCHS \/ 1000 * NFOLDS \/ 5 * ITERATIONS\n# comapring runtime to max-runtime (6h)\nexeeds_limit = \"EXEEDING\" if (total_runtime_iterations \/ 3600) > 6 else \"NOT EXEEDING\"\n\nprint(f'To cover all combinations & folds it takes {total_runtime \/ 3600:.1f} hours to finish. \\n'\n      f'The defined number of {ITERATIONS} random combinations takes {total_runtime_iterations \/ 3600:.1f} hours and is {exeeds_limit} our Kaggle maximum runtime!')\n      ","1f200ffc":"# create constants for the loss function\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\n# define competition metric\ndef score(y_true, y_pred):\n    \"\"\"Calculate the competition metric\"\"\"\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype = tf.float32) )\n    metric = (delta \/ sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\n\n# define pinball loss\ndef qloss(y_true, y_pred):\n    \"\"\"Calculate Pinball loss\"\"\"\n    # IMPORTANT: define quartiles, feel free to change here!\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype = tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q-1) * e)\n    return K.mean(v)\n\n# combine competition metric and pinball loss to a joint loss function\ndef mloss(_lambda):\n    \"\"\"Combine Score and qloss\"\"\"\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)\n    return loss","ba367045":"import tensorflow_addons as tfa\n\ndef get_model(optimizer = 'ADAM', dropout1 = 0.3, dropout2 = 0.2):\n    \"Creates and returns a model\"\n    # instantiate optimizer\n    optimizer = tf.keras.optimizers.Adam(lr = 0.1) if optimizer == 'ADAM' else tf.keras.optimizers.SGD()\n\n    # create model    \n    inp = Layers.Input((len(features_list),), name = \"Patient\")\n    x = Layers.BatchNormalization()(inp)\n    x = tfa.layers.WeightNormalization(Layers.Dense(196, activation = \"elu\", name = \"d1\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(dropout1)(x)\n    x = tfa.layers.WeightNormalization(Layers.Dense(128, activation = \"elu\", name = \"d2\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(dropout2)(x)\n    # predicting the quantiles\n    p1 = Layers.Dense(3, activation = \"relu\", name = \"p1\")(x)\n    # quantile adjusting p1 predictions\n    p2 = Layers.Dense(3, activation = \"relu\", name = \"p2\")(x)\n    preds = Layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis = 1), \n                     name = \"preds\")([p1, p2])\n    \n    model = tf.keras.Model(inputs = inp, outputs = preds, name = \"NeuralNet\")\n    model.compile(loss = mloss(_lambda), optimizer = optimizer, metrics = [score])\n    \n    return model","40462e63":"## GET TRAINING DATA AND TARGET VALUE\n\n# get target value\ny = train_df['FVC'].values.astype(float)\n\n# get training & test data\nX_train = train_df[features_list].values\nX_test = sub[features_list].values","74011445":"# Create a dict for saving training results\nresults_df = pd.DataFrame(columns = ['iteration', 'val_score', 'params', 'test_predictions'],\n                              index = list(range(ITERATIONS)))","715da2a7":"# measure execution time of training process\nimport time\nstart_time = time.time()","02c33c53":"## Non-Stratified GroupKFold-split (can be further enhanced with stratification!)\n\"\"\"K-fold variant with non-overlapping groups.\nThe same group will not appear in two different folds: in this case we dont want to have overlapping patientIDs in TRAIN and VAL-Data!\nThe folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.\"\"\"\n\ngkf = GroupKFold(n_splits = NFOLDS)\n# extract Patient IDs for ensuring uniqueness of patients in each fold\ngroups = train_df['Patient'].values\n\nfor i in range(ITERATIONS):\n    # create empty dict for logging\n    params_iteration = {}\n    val_scores = []\n    \n    # instantiate target arrays\n    train_preds = np.zeros((X_train.shape[0], 3))\n    test_preds = np.zeros((X_test.shape[0], 3))\n\n    # iterate over the param_grid: set the new random values for this iteration and log them\n    for key, value in param_grid.items():\n        exec(key + '= value[random.randint(0, len(value) - 1)]')\n        params_iteration[key] = {eval(key)}\n          \n    \n    # correct illogical results\n    if (lr_min > lr_max * BATCH_SIZE):\n        swap(lr_min,lr_max)\n        print(f'lr_min: {lr_min}, lrmax: {lr_max}')\n    if (lr_start > lr_max * BATCH_SIZE):\n        print(f'lr_start: {lr_start}, lrmax: {lr_max}' )\n\n    # training and evaluation\n    fold = 0\n    for train_idx, val_idx in gkf.split(X_train, y, groups = groups):\n        fold += 1\n        print(f\"ITERATION: {i}, FOLD {fold}:\")\n\n        # callbacks: logging & model saving with checkpoints each fold\n        callbacks = [get_lr_callback(BATCH_SIZE = BATCH_SIZE,\n                                     lr_start   = lr_start,\n                                     lr_max     = lr_max * BATCH_SIZE,\n                                     lr_min     = lr_min,\n                                     lr_ramp_percent =  lr_ramp_percent,\n                                     plot = False)]        \n\n        if LOGGING == True:\n            callbacks +=  [get_checkpoint_saver_callback(i,fold),                     \n                         LogPrintingCallback()]\n\n        # build and train model\n        model = get_model(optimizer, dropout1, dropout2)\n        model.fit(X_train[train_idx], y[train_idx], \n                  batch_size = BATCH_SIZE, \n                  epochs = EPOCHS, \n                  validation_data = (X_train[val_idx], y[val_idx]), \n                  callbacks = callbacks,\n                  verbose = 0) \n\n        # load best model to make preds\n        model.load_weights(f'iteration-{i}_fold-{fold}.h5')\n        train_preds[val_idx] = model.predict(X_train[val_idx],\n                                             batch_size = BATCH_SIZE,\n                                             verbose = 0)\n        \n        val_scores.append(model.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE)[1])\n        \n        # predict on test set and average the predictions over all folds\n        test_preds += model.predict(X_test, batch_size = BATCH_SIZE, verbose = 0) \/ NFOLDS\n    \n    print(f\"{val_scores} #\\n\")\n    val_score_iteration = np.mean(val_scores)\n    # log results    \n    results_df.at[i, 'iteration'] = i\n    results_df.at[i, 'val_score'] = val_score_iteration\n    results_df.at[i, 'params'] = str(params_iteration)\n    results_df.at[i, 'test_predictions'] = test_preds\n    # print(results)","a98d3ee0":"end_time = time.time()\nprint (f\"It took {(end_time - start_time)\/60\/60:.1f} hours. Meaning {(end_time - start_time)\/(ITERATIONS * EPOCHS):.2f} seconds per iteration @ {NFOLDS} folds and {EPOCHS} epochs.\")","76aa143c":"# Sort the results based OOF-Score\nresults_df.sort_values('val_score', ascending = True, inplace = True)\nresults_df.reset_index(inplace = True)\n# saving dataframe\nresults_df.to_csv('results_df_with_params')\nresults_df.head()","c1880963":"## Choose how many of the best models shall be used for ensembling\/averaging\nN_best_models = 3\n\n# initialize\/reset test_preds array\ntest_preds = np.zeros((X_test.shape[0], 3))\n\n# add up best predictions and average afterwards\nfor model in range(N_best_models):\n    test_preds += results_df.loc[model, 'test_predictions']\n\ntest_preds \/= N_best_models","2c98dbd6":"## FIND OPTIMIZED STANDARD-DEVIATION\nsigma_opt = mean_absolute_error(y, train_preds[:,1])\nsigma_uncertain = train_preds[:,2] - train_preds[:,0]\nsigma_mean = np.mean(sigma_uncertain)\nprint(sigma_opt, sigma_mean)","3c2327d7":"## PREPARE SUBMISSION FILE WITH OUR PREDICTIONS\nsub['FVC1'] = test_preds[:, 1]\nsub['Confidence1'] = test_preds[:,2] - test_preds[:,0]\n\n# get rid of unused data and show some non-empty data\nsubmission = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission.loc[~submission.FVC1.isnull()].head(10)","59448e3a":"submission.loc[~submission.FVC1.isnull(),'FVC'] = submission.loc[~submission.FVC1.isnull(),'FVC1']\n\nif sigma_mean < 70:\n    submission['Confidence'] = sigma_opt\nelse:\n    submission.loc[~submission.FVC1.isnull(),'Confidence'] = submission.loc[~submission.FVC1.isnull(),'Confidence1']","fa7de4ec":"submission.head()","5e0cc631":"submission.describe().T","ed74073b":"org_test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\n\nfor i in range(len(org_test)):\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70","e9d306c5":"submission[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index = False)","137df917":"# Training","8d369bdd":"In the following we want to create leak-free folds to get a robust **cross-validation strategy** in order to evaluate all our models & our training. The idea is to avoid having the same patient (= PatientID) in training- and in validation-Data, as this might lead to evaluate a higher CV-score for a model which is luckily learning\/memorizing the data for a particular patientID which is also frequently occuring in the validation-data.  \n\n\nThe idea on how to do that is coming from @PAB97 [Pierre's great notebook (CHECK IT OUT!)](https:\/\/www.kaggle.com\/rftexas\/osic-eda-leak-free-kfold-cv-lgb-baseline#kln-440)\nPlease note, that we still don't use proper stratification based on 'Age', 'Sex', 'SmokingStatus'.\n\nIn the following code-blocks we are trying the specified amount (ITERATIONS) of random chosen hyper-parameter values, taken from our parameter-grid and train & evaluate the model. In the end, we make our prediction based on the best model. Additionally, we log all our results to have a better overview.","e2ea192e":"## Load all dependencies you need\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","63ed5a71":"In the following last step we overwrite our predictions with the known data from the orginal submission file to not waste known data.","7bd5323e":"Now the data has the format we need to work with.\nIn the next section, we go trough two possibilities on how to normalize, standardize and prepare the data for the neural Network.","1efcb79c":"### Average the best models","118bf480":"# Data Wrangling","03be3a3e":"## Domain knowledge\n\n### Some domain knowledge can be gained from watching the following video and from reading [here.](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/165727)","744126e1":"## Random Grid Search config","06c53432":"# <font color='blue'>CONFIG Section <\/font>","ddc259f8":"In this section you can configure the following:\n* Features used for training\n* Basic training setup: BATCH_SIZE and EPOCHS,\n* Configuration for the loss function\n* Optimizers, Learning-Rate-Schedulers incl. Learning Rate start- & endpoint\n* Custom Logging Callback\n* Checkpoint-Saving Callback\n\nThe Learning-Rate scheduler below is inspired by Chris great [Melanoma-detection notebook](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords).  \nFeel free to experiment with the scheduler and it's max\/min and decay values.\n\n**Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches?** The reason for this is the following: the larger the BATCH_SIZE, the more averaged & smoothened a step of gradient decent is and the bigger our confidence in the *direction* of the step is. As there is less \"randomness\" in a huge averaged batch (compared with for example Stochastic Gradient Decent (=SGD) with batch size = 1) and our confidence in the direction is higher, the learning rate can be bigger to advance fast to the optimum.","060a5992":"## Getting the format right","413ad71b":"### Prepare submission file\nIn the next section we are going to use the ```train_preds``` to calculate the optimized sigma, which is a measure for certainty or rather uncertainty. We can do that, as we have both: the model's estimate and the real data. We subtract the lower quartile from the upper quartile (defined in the loss function) and average it.","79f0c82d":"## Update history:\nV30: Final version  \nV25: Improvements in logging & ensembling  \nV20: Enhanced logging, ENSEMBLING!  \nV16: Several improvements, getting better results now!\nV10: Introduced usage of tensorflow.addons (tfa): WeightNormalization  \nV7&8: Added support for maximizing cv-score or minimizing val_loss; added TODOS  \nV6: Fixed an important bug: Hyperparameter search should MAXIMIZE score (was set to MINIMIZE due to copy paste)  \nV5: Formatting, correcting typos.  \nV2,3,4: Fast consequitive runs for fixing minor bugs, formatting, correcting typos.  \nV1: Initial commit\n","50f96273":"### Loss Function","e6fe1a64":"# Model\nWe are going to use weight-normalization (link to the paper, click [here](https:\/\/arxiv.org\/abs\/1602.07868)) from ```tensorflow_addons``` to support faster convergence. \nThe authors describe the method like this:\n> Weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.\n\nAdditionally we gain more robustness for the choosing of the hyperparameter learning rate. Cited from page 3:\n> Empirically, we find that the ability to grow the norm ||v|| makes optimization of neural networks\n> with weight normalization very robust to the value of the learning rate: If the learning rate is too\n> large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate\n> is reached.\n\nAs the given task without the usage of images is not very compute-intensive (you don't need a GPU, CPU will do), we will change the activation-function from 'relu' to 'elu'.\nFor more info you can read [here](https:\/\/mlfromscratch.com\/activation-functions-explained\/#elu), below you can find a short summary:\n\n**Pros**\n* Avoids the \"dead ReLu\" problem: ReLus provides activation-values & gradients of 0 for negative input values\n* Produces activations for negative inputs instead of letting them be zero when calculating the gradient.\n* Produces negative outputs, which helps the network nudge weights and biases in the right directions for negative inputs, too.\n\n**Cons**\n* Introduces longer computation time, because of the exponential operation included.\n* Does not avoid the exploding gradient problem.","793cad67":"Okay, we made it! Let's finally check our stats and submit it!","2b524776":"# Quick intro\n\n## This notebook is extending (and changing a lot) of my [notebook-walkthrough for quantile regression with LR-Schedulers and model-checkpoints](https:\/\/www.kaggle.com\/chrisden\/6-82-quantile-reg-lr-schedulers-checkpoints).  \n## The goal of this Notebook is to introduce several methods for hyperparameter-optimization backed with cross-validation. For this goal, a lot of code the orginial code will be hidden (of course you can open it) and shortcutted to enhance readbility.\n\n","55fd28c7":"# First glimpse at the data","18b4a89c":"In this section we are going to do all the Data-Wrangling and pre-processing. For this we are going to define some functions and transformations, which then are applied to the data.\nIt's good practice to concatinate all tabular data (train, test, submission), to ensure all data get's the same & correct treatment.\nIf you don't do that, you need to be careful with some steps, e.g.: \n* Standardization or Normalization (e.g. MinMax Scaling) in ```test_df``` will not have the same range of values (e.g. min\/max values) and therefore scaling than in ```train_df```.\n* The categorical features might have different categories in ```test_df``` than in ```train_df``` (e.g. ```test_df``` only contains male, Ex-smokers).\n\nSo let's concatinate all our data first and then start with the transformations.\n### The functions for this are hidden, as the focus for this notebook is the RandomCV hyperparameter optimization.","0f693f1a":"### Let's do the Data-Prep and all related legwork ourselfs, w\/o sklearn. How to use sklearn, you can read here: \nhttps:\/\/www.kaggle.com\/chrisden\/6-82-quantile-reg-lr-schedulers-checkpoints  \n\nThe good thing is: we dont need a NoTransformer here, as we simply can work in the DataFrame itself and not change any data which we want to preserve.\nDownside is, we need to implement the MinMaxScaler by hand. Make sure to not call it MinMaxScaler and shadow the already important MinMaxScaler from Sklearn!\n","aad97d35":"## Preparing the data for the Neural Network","b4ab6a22":"# (RANDOM) Grid Search","05a70419":"# Model & Loss\nIn this section we are going to define the loss & a first model.\nFirst we are taking care of the loss. We are trying to minimize the following:\n\n![image.png](attachment:image.png)\n\nThe global minimum of this function is achieved for delta = 0 and sigma = 70, which [results in a loss of roughly -4.59.](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/168469).\n\nGetting our model to predict the Confidence and FVC values (which is what we need!) is not working fine so far, as you can read [here](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/167764).\nCurrently the way to go seems to be pinball loss. \n","27afec1b":"# Evaluation & Ensemble\n### WELL DONE! Grid Search & Training COMPLETED!\n\nWe are currently using an *average of our predictions over all folds* in each ITERATION.\nWe can additionally average those results\/predicstion using several of the best iterations \n\nLet's use the best results to make our predictions for submission. In addition, we are going to save the results_df to be able to check for the best parameters after the commit is completed.\n\nAnother Notebook explaning how to do ensembling in this competition can be found [here.](https:\/\/www.kaggle.com\/ChristianDenich\/multi-model-ensembling-evaluation)\n\n","4faf3c49":"In this section we define our search grid. We also want to clarify how long does it take to search through ALL combinations? How long does it take to go through a defined number of random-combinations (called ITERATIONS)? Can it be complete within the 6 hours runtime-cap on Kaggle.com, or do we need to switch to Google Colab?\n\nLet me start with a cool story I found about [random Grid search](https:\/\/www.dattocon.com\/#atlanta):\n\n> This is the storyline of \u201cRandom search for hyperparameter optimization\u201d by Bergstra and Bengio. [...] Random search wasn\u2019t taken very seriously before. This is because it doesn\u2019t search over all the grid points, so it cannot possibly beat the optimum found by grid search. But then came along Bergstra and Bengio. They showed that, in surprisingly many instances, random search performs about as well as grid search. All in all, trying 60 random points sampled from the grid seems to be good enough.\n> \n> In hindsight, there is a simple probabilistic explanation for the result: for any distribution over a sample space with a finite maximum, the maximum of 60 random observations lies within the top 5% of the true maximum, with 95% probability. That may sound complicated, but it\u2019s not. Imagine the 5% interval around the true maximum. Now imagine that we sample points from his space and see if any of it lands within that maximum. Each random draw has a 5% chance of landing in that interval, if we draw n points independently, then the probability that all of them miss the desired interval is (1\u22120.05)n. So the probability that at least one of them succeeds in hitting the interval is 1 minus that quantity. We want at least a .95 probability of success. To figure out the number of draws we need, just solve for n in the equation:\n\n$$1 \u2212 (1\u22120.05) ^n > 0.95$$\n\nWe get $n \u2a7e 60$. Ta-da!\n\n> The moral of the story is: if the close-to-optimal region of hyperparameters occupies at least 5% of the grid surface, then random search with 60 trials will find that region with high probability!\n\n\nThe below example is just an nice start: you can also add modifications to the Neural Network which is built below, such as: number of layers, number of neurons, dropout-chance."}}