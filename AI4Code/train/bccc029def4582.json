{"cell_type":{"489d020b":"code","190776de":"code","4a9c87a4":"code","c3bd23ce":"code","6cd9e1b6":"code","bfb1dccc":"code","82543e02":"code","af908900":"code","06c8ebbd":"code","170e1e20":"code","418428cb":"code","51338867":"code","4b8d53d3":"code","eb13c16c":"code","263c2b79":"code","a328f52e":"code","5b585291":"code","67c9f12a":"code","a37d1fa7":"code","3f121342":"code","c20c58f7":"code","8195a783":"code","9ad2ad39":"code","34902afa":"code","07ef9442":"code","bf5fb39c":"code","448e71e7":"code","20d84ebb":"code","8a4e47c7":"code","5bb37acc":"code","6acebe0e":"code","75b20df0":"code","52e4357b":"code","f4953231":"code","7ef9aff9":"code","b507a40e":"markdown","03be668f":"markdown","0297614f":"markdown","df92e299":"markdown","c75ebd57":"markdown","e8ca18f7":"markdown","1898fc82":"markdown","f16ee56d":"markdown","e0a6fc48":"markdown","1c91ba9a":"markdown","d421268c":"markdown","9c106979":"markdown","ec0c2afd":"markdown","e40db794":"markdown","5a2d3ba2":"markdown","17c87d89":"markdown","679f3a8f":"markdown","f8da4a2d":"markdown","d39593cb":"markdown","4863b3ed":"markdown","289b3a2a":"markdown","dcfa6ab3":"markdown","84374e20":"markdown","eb2655bf":"markdown","695cedbd":"markdown","af1342c4":"markdown","23783b3c":"markdown"},"source":{"489d020b":"#Imports\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline ","190776de":"#Read in the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nfull = pd.concat([train,test])","4a9c87a4":"#Check head\nfull.head()","c3bd23ce":"#Check info\nfull.info()","6cd9e1b6":"#Drop 'Cabin'\nfull.drop('Cabin',axis=1,inplace=True)","bfb1dccc":"#Check which port most of the passengers came from\nfull['Embarked'].value_counts()","82543e02":"#Fill na values with 'S'\nfull['Embarked'].fillna('S',inplace = True)\n\n#Convert 'Embarked' into dummy variables and drop 'Embarked'\nfull = pd.concat([full,pd.get_dummies(full['Embarked'],drop_first=True,prefix='Port')],axis=1)\nfull.drop('Embarked',axis=1,inplace=True)","af908900":"#Fill na values with average Fare\nfull.loc[full['Fare'].isnull(),'Fare'] = full['Fare'].mean()","06c8ebbd":"#Split names into a list\nfull_title = full['Name'].apply(lambda x: x.split()[1])\n\n#Function: Takes in a title and returns the title if it's among those specified; Else returns 'No Title'\ndef impute_title(title):\n    if title not in ['Mr.','Miss.','Mrs.','Master.']:\n        return 'No title'\n    else:\n        return title\n\n#Assign titles, convert into dummy variables, and drop 'Name'\nfull_title = full_title.apply(impute_title)\nfull = pd.concat([full,pd.get_dummies(full_title,drop_first=True)],axis=1)\nfull.drop('Name',axis=1,inplace=True)","170e1e20":"#Store test ids for later use and drop 'PassengerId'\ntest_id = test['PassengerId']\nfull.drop('PassengerId',axis=1,inplace=True)","418428cb":"#Convert 'Pclass' into dummy variables and drop 'Pclass'\nfull = pd.concat([full,pd.get_dummies(full['Pclass'],drop_first=True,prefix='Pclass')],axis=1)\nfull.drop('Pclass',axis=1,inplace=True)","51338867":"#Convert 'Sex' into dummy variable and drop 'Sex'\nfull = pd.concat([full,pd.get_dummies(full['Sex'],drop_first=True)],axis=1)\nfull.drop('Sex',axis=1,inplace=True)","4b8d53d3":"#Drop 'Ticket'\nfull.drop('Ticket',axis=1,inplace=True)","eb13c16c":"#Take a look at the data frame now\nfull.head()","263c2b79":"#Import Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\n\n#Fit model on data that does have an Age entry\nimpute_age = LinearRegression()\nimpute_age.fit(full[full['Age'].isnull()==False].drop(['Survived','Age'],axis=1),\n               full[full['Age'].isnull()==False].drop('Survived',axis=1)['Age'])\n\n#Impute ages for those that were missing\nages = impute_age.predict(full[full['Age'].isnull()].drop(['Survived','Age'],axis=1))","a328f52e":"#Compare Age Distributions with and without imputed ages\nplt.figure(figsize=(13.5,6))\nplt.subplot(1,2,1)\nplt.hist(full[full['Age'].isnull()==False].drop('Survived',axis=1)['Age'],\n         bins=range(0,80,5),edgecolor='white')\nplt.title('Without Age Imputations')\nplt.xlabel('Age')\n\nplt.subplot(1,2,2)\nplt.hist(list(full[full['Age'].isnull()==False].drop('Survived',axis=1)['Age']) + list(ages),\n         bins=range(0,80,5),edgecolor='white',alpha=.5)\nplt.title('With Age Imputations')\nplt.xlabel('Age')","5b585291":"#Fill dataframe in with imputed ages\nfull.loc[full['Age'].isnull(),'Age'] = ages","67c9f12a":"#Produce heatmap of correlations\nplt.figure(figsize=(16,8))\nsns.heatmap(full.corr(),annot=True,cmap='viridis')\nplt.tight_layout","a37d1fa7":"#Illustrate relationship between family size and survival rate\nplt.figure(figsize=(13.5,6))\nsns.countplot((full['SibSp'] + full['Parch'] + 1),hue=full['Survived'],palette='viridis')\nplt.xlabel('Family Size')","3f121342":"#Create new column for family size\nfull['Family'] = (full['SibSp'] + full['Parch'] + 1)\n\n#Function: Takes in family size and returns corresponding description\ndef impute_alone(x):\n    if x == 1:\n        return 'Alone'\n    elif x > 4:\n        return 'Large Family'\n    else:\n        return 'Small Family'\n\n#Label each passenger's family size\nfull['Family'] = full['Family'].apply(impute_alone)","c20c58f7":"#Again, illustrate relationship between family size and survival rate\nplt.figure(figsize=(13.5,6))\nsns.countplot(full['Family'],hue=full['Survived'],palette='viridis')","8195a783":"#Convert into dummy variable and drop 'SibSp', 'Parch', and 'Family'\nfull = pd.concat([full,pd.get_dummies(full['Family'],drop_first=True)],axis=1)\nfull.drop(['SibSp','Parch','Family'],inplace=True,axis=1)","9ad2ad39":"#Take a look\nfull.head(10)","34902afa":"#Standardize Age and Fare\nfull['Age'] = (full['Age'] - full['Age'].mean()) \/ full['Age'].std()\nfull['Fare'] = (full['Fare'] - full['Fare'].mean()) \/ full['Fare'].std()","07ef9442":"#Split the data back into train and test sets\ntrain = full.iloc[0:len(train)]\ntest = full.iloc[len(train):len(full)]","bf5fb39c":"#Scikit imports\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","448e71e7":"#Split the data 'train' data into train and test sets\nX = train.drop('Survived',axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)","20d84ebb":"def error(clf, X, y, ntrials=100, test_size=0.2) :\n    train_error = 0\n    test_error = 0\n    \n    for i in range(ntrials):\n        X_, X_t, y_, y_t = train_test_split(X, y, test_size=test_size, random_state=i)\n        model = clf\n        model.fit(X_,y_)\n        train_pred = model.predict(X_)\n        test_pred = model.predict(X_t)\n        train_error += 1 - accuracy_score(y_, train_pred, normalize=True)\n        test_error += 1 - accuracy_score(y_t, test_pred, normalize=True)\n    \n    train_error \/= ntrials\n    test_error \/= ntrials\n    \n    return train_error, test_error","8a4e47c7":"depth = list(range(1,21,1))\ndepth_train_error = []\ndepth_test_error = []\n    \nfor i in depth:\n    results = error(DecisionTreeClassifier(criterion='entropy',max_depth=i),X,y,ntrials=100,test_size=.2)\n    depth_train_error.append(results[0])\n    depth_test_error.append(results[1])\n\nplt.plot(depth,depth_train_error,label='Train Error')\nplt.plot(depth,depth_test_error,label='Test Error')\nplt.title('Error vs. Depth')\nplt.xlabel('Depth')\nplt.ylabel('Error')\nplt.legend()\n#plt.savefig('Depth.png')\nplt.show()","5bb37acc":"#Fit and predict with Decision Tree\ndtree = DecisionTreeClassifier(criterion='entropy',max_depth=3)\ndtree.fit(X_train,y_train)\npred_dtree = dtree.predict(X_test)\n\nscores = cross_val_score(dtree, X_train, y_train, cv=10)\nprint(scores.mean())\nprint('\\n')\nprint(confusion_matrix(y_test,pred_dtree))\nprint('\\n')\nprint(classification_report(y_test,pred_dtree))","6acebe0e":"from IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydot \n\nfeatures = list(X_train.columns)\n\ndot_data = StringIO()  \nexport_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())  \nImage(graph[0].create_png()) ","75b20df0":"pred_final = dtree.predict(test.drop('Survived',axis=1))\npred_final = [int(x) for x in pred_final]","52e4357b":"#Create Submission\nsubmission = pd.DataFrame(\n    {'PassengerId' : test_id,\n     'Survived' : pred_final}\n)","f4953231":"#Make sure everything looks good\nsubmission.head()","7ef9aff9":"#Store it\nsubmission.to_csv('Submission',index=False)","b507a40e":"### 10.SibSp: # of siblings or spouses a passenger had.","03be668f":"It's worth taking note of the number of missing values:\n- Age: 263\n- Cabin: 1014!!!\n- Embarked: 2\n- Fare: 1\n- Survived: 418 (But this corresponds to our test set, so no worries here)","0297614f":"### Decision Tree","df92e299":"### Any strong correlations?","c75ebd57":"# Model Fitting","e8ca18f7":"# Thank you!\nLand, ho! (hopefully...). Thanks for making it with me this far. Again, please feel free to leave comments. I'm continually trying to improve my coding skill, analysis, and presentation, so all advice or criticism is welcomed and well received. Cheers!","1898fc82":"### 7.PassengerId: Passenger Index. \n    - Not a predictor.\n    - Store Test IDs before dropping.","f16ee56d":"If you were alone, you had a much greater chance of dying. If you were in a large family (greater than 4 members), you had a greater chance of dying. But if you were in a small family (2-4 members), you actually had a better chance of surviving. Let's go ahead then and create features for this in our dataframe.","e0a6fc48":"### 3.Embarked: Indicates port that the passenger embarked from.\n    - Fill in the na values with whichever port was most prevalent.\n    - Convert into a dummy variable.","1c91ba9a":"# Conclusions & Extensions\nFor now, this model achieved a very popular result of 77.99% accuracy. Often times it can be imperative to consider the precision and recall of the predictions, but for this project our primary concern was simply accuracy. Here are some potential extensions to further enhance the analysis:\n    - Feature engineer 'Cabin' and 'Ticket': Perhaps information on passenger's location on the boat can be found.\n    - Dive deeper into the family dynamics, looking at whether Mothers and Daughters perhaps had higher survival rates.\n    - There are always more models to try!\n        - Does giving greater weight to the passengers that were incorrectly classified improve the predictions?","d421268c":"### 6.Parch: # of parents or children a passenger had. ","9c106979":"### 11.Survived: Whether or not the passenger survived. \n    - This is our target.","ec0c2afd":"Visually, the distribution appears unchanged. We've added roughly 100 passeners in their late 20s, which appears to be the most significant change. This looks to be about where the average age of the distribution without age imputations lies, though, so I'm actually pleased with this result. I feel confident that these results won't skew the outcome, so we can proceed with filling in the missing age values with the imputed ones.","e40db794":"### 2.Cabin: Cabin the passenger stayed in.\n    - Nearly 80% missing, so I'm just going to drop it.","5a2d3ba2":"### Family Size Exploration","17c87d89":"### 4.Fare: Price of the ticket the passenger paid for.\n    - Fill missing Fare value in with average Fare.","679f3a8f":"# Visualizations\nWith our data cleaned up and in a workable format, it's time to take a look at it!","f8da4a2d":"### 9.Sex: Passenger sex. \n    - Assign a binary variable.","d39593cb":"### 12.Ticket: Ticket number of passenger.\n    - Has potential for feature engineering, but I'm going to ignore for this analysis.","4863b3ed":"### 5.Name: Passenger name.\n    - Title may signify importance.\n    - Reduce Name to title.","289b3a2a":"### One last look at the data","dcfa6ab3":"Wow! Who would've thought 'Mr.' and 'male' would be so strongly correlated?? Jokes aside, there is some useful insight to be had here. First and foremost, I'm curious what's most strongly correlated with 'Survived'. It appears the largest contributors are 'male' (or 'Mr.'...) and 'Pclass_3'. 'Miss.' and 'Mrs.' are prevalent as well, but it's fair to assume that the sex is really what's contributing to this relationship as opposed to the title. Another couple standouts are the relationships between 'Pclass_3' and 'Age' and 'Fare'. It's easy to see how the 3rd class is going to be cheaper, but it's also interesting that they tend to be younger as well. For passengers buying their own tickets, this makes sense, but let's think about those who aren't, the children. If class 3 tended to be younger, it means that they either had more children or the higher classes had fewer. The question worth asking then is whether or not family size impacted survival rates.","84374e20":"### 1.Age: Age of Passenger\n    - We'll have to deal with some missing values, but I'm going to back to this last.","eb2655bf":"### 8.Pclass: Numeric representation of class. \n    - Effectively a categorical variable, so let's represent it as such.\n","695cedbd":"# Data Cleaning\n\nNow that we have a sense of our data, I'm going to go through column by column, thinking about each variable and organizing it into a form we can use for analysis.","af1342c4":"# Introduction\n\nAs many others before me, this is my first \"dive\" into a kaggle project. With its popularity and relative ease of manipulation, I felt the \"Titanic: ML from Disaster\" was a great place to start. Join me on this voyage through cleaning, visualizing, and modeling the Titanic survivals and please feel free to leave comments below. All criticisms are well received.\n\nBon Voyage!","23783b3c":"# Returning to Age.\n\nWe're going to fill in Age based on a linear regression performed on the rest of the variables. It would likely be fine for the analysis to fill in with simply average age or even average age of Pclass or Sex, but we have the tools to make a more detailed prediction, so might as well!"}}