{"cell_type":{"fbf609e5":"code","1181b135":"code","9c6dbd20":"code","d242376a":"code","43b97d53":"code","0d6bb756":"code","38dac701":"code","9dc595ab":"code","7cee311c":"code","b44f0bd2":"markdown","f5d6bdad":"markdown","b5934334":"markdown","c0c4f50a":"markdown","ca400621":"markdown","89296451":"markdown","066fcf83":"markdown","96065ac6":"markdown","30fab843":"markdown","e6565dc7":"markdown","868ac331":"markdown","1d6c06a8":"markdown"},"source":{"fbf609e5":"from statsmodels.stats.weightstats import ttest_ind\nimport statsmodels.formula.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris_dat = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ncar_dat = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')","1181b135":"# Subset the data into only two groups\niris_dat_sub = iris_dat[iris_dat['Species'].isin(['Iris-versicolor','Iris-virginica'])].reset_index()\n\n# Permutation test - diff of means ----------------------\n\n# Set the number of iterations\nx = 1000\n\n# Create an empty datasets\nreplicates = np.empty(x)\n\n# Find the difference in means for the original data set\nog_grouped = iris_dat_sub.loc[:,['SepalLengthCm','Species']].groupby('Species').mean()\nog_mean_diff = og_grouped.iloc[1] - og_grouped.iloc[0]\nprint('Here are the original means for the Sepal Lengths of the two species \\n')\nprint(og_grouped)\n\n# Run a loop for the number of permutations we want to do\nfor i in range(x):\n    permuted_dat_base = np.random.permutation(iris_dat_sub['SepalLengthCm']) # permute (shuffle) the data\n    permuted_dat = pd.concat([pd.Series(permuted_dat_base),iris_dat_sub['Species']], axis=1) # re-assign data points to random species\n    grouped_dat = permuted_dat.groupby('Species').mean() # Calculate the permuted means for each species\n    diff_of_means = grouped_dat.iloc[1] - grouped_dat.iloc[0] # Find the permuted mean difference for comparison to the original mean difference\n    replicates[i] = diff_of_means # Add to our empty dataset\n    \n# Plot the replicates and compare to the original mean\nplt.hist(replicates)\nplt.axvline(og_mean_diff['SepalLengthCm']\n, color='red')\nplt.show()\n\n# Compute the p-value\np_val = np.sum(replicates >= og_mean_diff['SepalLengthCm'])\/len(replicates)\nprint('Here is the p-value: {}'.format(p_val))\n\n# --------------------------------------------------------","9c6dbd20":"# Credit for this function: DataCamp course: 'Statistical Thinking in Python (Part 1)'\ndef ecdf(data): \n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\" \n    \n    # Number of data points: n \n    n = len(data) \n\n    # x-data for the ECDF: x \n    x = np.sort(data) \n\n    # y-data for the ECDF: y \n    y = np.arange(1, n + 1) \/ n \n\n    return x, y \n\n# Credit for this idea: DataCamp course: 'Statistical Thinking in Python (Part 2)'\nfor _ in range(50):\n    \n    # Generate permutation samples\n    permuted_dat_base = np.random.permutation(iris_dat_sub['SepalLengthCm']) # permute (shuffle) the data\n    permuted_dat = pd.concat([pd.Series(permuted_dat_base),iris_dat_sub['Species']], axis=1) # re-assign data points to random species\n    perm_versicolor = permuted_dat[permuted_dat['Species'] == 'Iris-versicolor'].iloc[:,0]\n    perm_virginica  = permuted_dat[permuted_dat['Species'] == 'Iris-virginica'].iloc[:,0]\n    \n    # Compute ECDFs\n    x_1, y_1 = ecdf(perm_versicolor)\n    x_2, y_2 = ecdf(perm_virginica)\n\n    # Plot ECDFs of permutation sample\n    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',\n                 color='red', alpha=0.02)\n    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',\n                 color='blue', alpha=0.02)\n\n# Create and plot ECDFs from original data\nx_1, y_1 = ecdf(iris_dat[iris_dat['Species'] == 'Iris-versicolor']['SepalLengthCm'])\nx_2, y_2 = ecdf(iris_dat[iris_dat['Species'] == 'Iris-virginica']['SepalLengthCm'])\n_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red', label='Versicolor')\n_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue', label='Virginica')\nplt.xlabel('SepalLengthCm')\nplt.ylabel('Cumulative Pct Total')\nplt.title('Sepal Lengths Between Versicolor & Virginica')\nplt.legend()\nplt.show()","d242376a":"versicolor = iris_dat[iris_dat['Species'] == 'Iris-versicolor']['SepalLengthCm']\nvirginica = iris_dat[iris_dat['Species'] == 'Iris-virginica']['SepalLengthCm']\n\nplt.hist(versicolor, color='red', alpha=.5, label='Versicolor')\nplt.title('Checking for a Normal Distribution')\nplt.hist(virginica, alpha=.5, label='Virginica')\nplt.legend()\nplt.show()\n\nprint('Here is the p-value for a two-sample test: {}'.format(ttest_ind(virginica, versicolor, alternative='larger')[1])) # two-sample","43b97d53":"car_dat.head(10)","0d6bb756":"plt.scatter(car_dat['weight'], car_dat['mpg'])\nplt.ylabel('MPG')\nplt.xlabel('Weight')\nplt.title('MPG as a Function of Weight')","38dac701":"# Set the number of iterations to run our permutations\nx = 1000\n\n# Our test statistic is correlation, so we will find our value to test our null hypothesis\nog_coeff = np.corrcoef(car_dat['mpg'], car_dat['weight'])[0][1]\n\n# Create empty dataframe to put correlation coefficients in\nreplicates = np.empty(x)\n\n# Run a loop for the number of permutations we want to do\nfor i in range(x):\n    permuted_dat_base = np.random.permutation(car_dat['mpg']) # permute (shuffle) the data\n    permuted_dat = pd.concat([pd.Series(permuted_dat_base),car_dat['weight']], axis=1) # re-assign data points to random car weights\n    replicates[i] = np.corrcoef(permuted_dat.iloc[:,0], permuted_dat['weight'])[0][1] # Add to our empty dataset\n    \n# Plot the replicates and compare to the original correlation\nplt.hist(replicates)\nplt.axvline(og_coeff\n, color='red')\nplt.show()\n\n# Compute the p-value\np_val = np.sum(replicates <= og_coeff)\/len(replicates)\nprint('Here is the p-value: {}'.format(p_val))","9dc595ab":"x = sm.ols('mpg ~ weight', data = car_dat).fit()\nplt.scatter(car_dat['weight'], car_dat['mpg'])\nplt.plot(car_dat['weight'],x.predict(), color='red')\n# plt.xlabel('weight')\nplt.ylabel('MPG')\nplt.xlabel('Weight')\nplt.title('MPG as a Function of Weight')\nplt.show()\ndisplay(x.summary())\n","7cee311c":"# Test Datasets\ndata_norm1 = np.random.normal(55,.2,1000)\ndata_norm2  = np.random.normal(55,.2,1000)\n\n# Set the number of iterations\nx = 1000\n\n# Create an empty datasets\nreplicates = np.empty(x)\np_val = []\n\n# Run a loop for the number of p_values we want\nfor z in range(100):\n    \n    # Find the difference in means for the original data set\n    og_mean_diff = np.mean(data_norm1) - np.mean(data_norm2)\n    \n    # Run a loop for the number of permutations we want to do\n    for i in range(x):\n\n        # Generate permutation samples\n        permuted_dat_base = np.random.permutation(data_norm1) # permute (shuffle) the data\n        permuted_dat = pd.concat([pd.Series(permuted_dat_base),pd.Series(data_norm2)], axis=1) # re-assign data points to random species\n        replicates[i] = np.mean(permuted_dat.iloc[:,0]) - np.mean(permuted_dat.iloc[:,1])\n    \n    # Need to \n    if og_mean_diff > 0:\n        p = np.sum(og_mean_diff>=replicates)\/len(replicates)\n    else:\n        p = np.sum(og_mean_diff<replicates)\/len(replicates)\n    \n    # Append p-values to my list\n    p_val.append(p)\n\nplt.hist(p_val)\nplt.show()","b44f0bd2":"# Importing all required packages and data\n\n\n\n","f5d6bdad":"You can see that there does seem to be a negative correlation between weight and mpg. Let's test this using a permutation test. ","b5934334":"# Example 2  - Results Interpretation\n\nThe p values for both the permutation test was 0 and the linear regression test supports that finding. This is another example of how you can use a permutation test on a metric besides difference of means.","c0c4f50a":"# Warning\n\nThe permutation test doesn't give the same p-value all the time due to the nature of the random shuffling. The histogram below shows a distribution of p-values that were given by a permutation test for data that was built from the same normal distribution. This variability in the p-value will decrease with the size of your data. So be careful with small datasets and  and p-values that are really close to your level of significance. If you wanted to be really strict with deciding whether something is significant or not, you could plot the distribution of the p-values and only use the mean p-value as your final p-value. Variation might also be reduced by using a t-statistic as your test statistic instead of a mean difference.","ca400621":"# Conclusion\n\nIn these examples, I used common datasets and compared the results to typical statistical tests. In both situations, the p-values matched the statistical tests you would normally use. In this notebook, I demonstrated the permutation method compared to a typical statistical test in order to increase your confidence to use this test on non-normally distributed datasets. The permutation test is a great tool due to its flexibility to be used on a variety of datasets. Its flexibility is due to that fact that it makes no assumptions about the underlying distribution of your data. I hope you found this useful and can use it in your work, please upvote if you liked the content. Also, any feedback is appreciated! Thanks!","89296451":"You can see that the two-sample t-test also has a p-value very close to 0","066fcf83":"# Introduction\n\nA permutation test is a statistical test that does not make assumptions about the normality of the distribution of the data (aka a 'non-parametric' test). This can be very useful for testing data that is not normally distributed. The permutation test simply runs mixes up the data (without replacement) and then computes a test statistic an n number of times. You then can plot the resulting distribution and compute the p-value.\n\nI will show two examples of how you could use a permutation test to test for significance. Both datasets come from the UCI Machine Learning repository, which are typical beginner datasets for stats and machine learning.","96065ac6":"# Example 1 - Iris Dataset\n\nFor this first example, we will focus on comparing the sepal lengths for two different iris species and test for significance using a permutation test. The null hypothesis is that the average sepal lengths are the same. The alternate hypothesis is that they're not the same.","30fab843":"# Example 1  - Results Interpretation\n\nThe p-value shows that there are no data points that we as or more extreme than our data point. This means that the Virginica Iris sepal lengths were statistically signifantly longer than the Versicolor Iris sepal lengths. If our mean difference would have been negative, we would have calculated our p-value by seeing how many replicates were below the original mean difference. This is because the definition of a p-value is finding values as or more extreme than what was observed. In this case, this translates to the area of the distribution greater than the mean differences we observed. \n\nOne thing to note is we could have had our test statistic be based on the medians instead of the mean as well. That's part of the flexibility of the permutation test, you can choose your test statistic. In the next example, I will use a permutation test on the cars dataset using correlation.\n\nOne more thing that would be good to do is plot the cumulative distribution function to check the distribution of your data. This can also help you get a sense if the difference is significant or not.","e6565dc7":"# Example 2 - Cars Dataset\n\nWe will use the cars dataset from the UCI Machine Learning Repository to help demonstrate a permutation on a correlation between two variables. We want to know if the correlation between the mpg and weight of a car is significant. We will then use a linear regression to verify our results. The null hypothesis is the correlation is 0 and the alternate hypothesis is that there is a correlation.","868ac331":"# Outline\n\n* Importing all required packages and data\n* Introduction\n* Example #1 - Iris Dataset\n* Example #1 - Results Interpretation\n* Example #2 - Cars Dataset\n* Example #2 - Results Interpretation\n* Warning\n* Conclusion","1d6c06a8":"You can see that the permutations are clearly inbetween the versicolor and virginica petal lengths, which also visually indicates that they are significantly different. The last thing to really drive this home is comparing this to the result from a two-sample t-test, which you could also do for this data assuming it is normally distributed."}}