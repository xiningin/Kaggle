{"cell_type":{"5cb54d13":"code","f2e55678":"code","83858afa":"code","cfed5ccb":"code","3b1febf9":"code","82efe3c8":"code","f917017b":"code","04acaba3":"code","24264dde":"code","f9c5c9a9":"code","efd9089c":"code","7a444f96":"code","873020a9":"code","b32a8f02":"code","82fabc09":"code","78ff7b28":"code","664f71f0":"code","4bbcdeb9":"code","bee45845":"code","28e9da59":"code","a183d9aa":"code","8bdfa86f":"code","59a466d4":"code","32d2aec1":"code","67061253":"code","2d931ab1":"code","303d8bd7":"code","52083180":"code","4929c42d":"code","22ec1498":"code","30e776c8":"code","b92a536d":"code","e615bf44":"code","41c3a44f":"code","97f23eba":"markdown","67cdf43c":"markdown","0e610a0e":"markdown","769a1ac8":"markdown","1e778100":"markdown","9234b968":"markdown","1ffc12a0":"markdown","fe2307c7":"markdown"},"source":{"5cb54d13":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\nfrom tensorflow.keras.utils import plot_model","f2e55678":"data=pd.read_csv('..\/input\/deep-learning-az-ann\/Churn_Modelling.csv')","83858afa":"data.head()","cfed5ccb":"y=data.Exited","3b1febf9":"X=data.drop(['Exited','RowNumber','CustomerId','Surname'],axis=1)","82efe3c8":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nX.Gender=le.fit_transform(X.Gender)","f917017b":"obj=pd.get_dummies(X.Geography,drop_first=True)","04acaba3":"X=pd.concat([obj,X],axis=1)","24264dde":"X=X.drop('Geography', axis=1)","f9c5c9a9":"X.head()","efd9089c":"X.info()","7a444f96":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX=ss.fit_transform(X)","873020a9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","b32a8f02":"X_train,X_test, y_train,y_test=train_test_split(X,y,random_state=42, stratify=y)","82fabc09":"print('Shapes')\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape)\nprint('y_test: ', y_test.shape)","78ff7b28":"lr=LogisticRegression()\nlog_model=lr.fit(X_train,y_train)","664f71f0":"preds=log_model.predict(X_test)","4bbcdeb9":"print('The error of the logistic regression model: ',np.sqrt(mean_squared_error(y_test,preds)))","bee45845":"from lightgbm import LGBMClassifier\nlc_model=LGBMClassifier().fit(X_train,y_train)\npreds=lc_model.predict(X_test)\nprint('The error of the lightgbm model: ',np.sqrt(mean_squared_error(y_test,preds)))","28e9da59":"Input_tensor=Input(shape=(11,))\nh_layer=Dense(32, activation='relu')(Input_tensor)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(16, activation='relu')(h_layer)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=BatchNormalization()(h_layer)\nOutput_tensor=Dense(1, activation='sigmoid')(h_layer)\n\nmodel=Model(inputs=Input_tensor,\n           outputs=Output_tensor)\nplot_model(model,show_shapes=True,show_dtype=True)","a183d9aa":"X_train2,X_val,y_train2,y_val=train_test_split(X_train,y_train,stratify=y_train, random_state=42)","8bdfa86f":"model.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\n\n\n\n\nhist=model.fit(X_train2,y_train2,epochs=30,batch_size=8,\n          validation_data=(X_val,y_val)\n         )","59a466d4":"model.evaluate(X_test,y_test)","32d2aec1":"pre=model.predict(X_test)","67061253":"print('Error of the ANN1: ',np.sqrt(mean_squared_error(pre,y_test)))","2d931ab1":"Input_tensor=Input(shape=(11,))\nh_layer=Dense(32, activation='relu')(Input_tensor)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(16, activation='relu')(h_layer)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(8, activation='relu')(h_layer)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=BatchNormalization()(h_layer)\nOutput_tensor=Dense(1, activation='sigmoid')(h_layer)\n\nmodel=Model(inputs=Input_tensor,\n           outputs=Output_tensor)\nplot_model(model,show_shapes=True,show_dtype=True)","303d8bd7":"model.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\n\n\n\n\nhist=model.fit(X_train2,y_train2,epochs=30,batch_size=8,\n          validation_data=(X_val,y_val)\n         )","52083180":"model.evaluate(X_test,y_test)","4929c42d":"pre=model.predict(X_test)","22ec1498":"print('Error of the ANN2: ',np.sqrt(mean_squared_error(pre,y_test)))","30e776c8":"Input_tensor=Input(shape=(11,))\nh_layer=Dense(32, activation='relu')(Input_tensor)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(16, activation='relu')(h_layer)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(8, activation='relu')(h_layer)\nh_layer=Dropout(0.5)(h_layer)\nh_layer=Dense(4, activation='relu')(h_layer)\nh_layer=BatchNormalization()(h_layer)\nOutput_tensor=Dense(1, activation='sigmoid')(h_layer)\n\nmodel=Model(inputs=Input_tensor,\n           outputs=Output_tensor)\nplot_model(model,show_shapes=True,show_dtype=True)","b92a536d":"model.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\n\n\n\n\nhist=model.fit(X_train2,y_train2,epochs=30,batch_size=8,\n          validation_data=(X_val,y_val)\n         )","e615bf44":"print('Error of the ANN3: ',np.sqrt(mean_squared_error(pre,y_test)))","41c3a44f":"plt.figure(figsize=(10,6))\nsns.barplot(x=['ANN1','ANN2','ANN3','Logistic Regression', 'LightGBM'],y=[0.3322950346213218,0.35373324579924387,0.35373324579924387,0.4368065933568311,0.3622154055254967])\nplt.title('Error Rates for Different Predictor Models')\nplt.xlabel('$Models$')\nplt.ylabel('$Error Rates$')\nplt.show()","97f23eba":"# 2. Creating Models","67cdf43c":"# 2.3.3. ANN Model 3 (4 Hidden Dense Layers)","0e610a0e":"# 1.Importing, Analyzing and Preprocessing the Data","769a1ac8":"# Introduction\n\nThis notebook compares LogisticRegression, LightGBM and ANN performance. Another aim of this project is investigating whether building a deeper neural networks is always a good idea. In this regard 3 ANN were constructed and compared the error between them.  ","1e778100":"# Conclusion\n\nAs seen from graph, making deeper neural networks is not always the best approach. Except for the more complex datasets, a machine learning model (LightGBM in this case) can work really close to ANN.","9234b968":"# 2.3. ANN Models","1ffc12a0":"# 2.3.2. ANN Model 1 (3 Hidden Dense Layers)","fe2307c7":"# 2.3.1. ANN Model 1 (2 Hidden Dense Layers)"}}