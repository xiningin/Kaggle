{"cell_type":{"b8080802":"code","05ffeb1c":"code","d1083d9f":"code","fd65807f":"code","29dfae27":"code","fa52f545":"code","bcac7772":"code","4993c528":"code","5f78e5d7":"code","d6be7495":"code","2ae4c015":"code","ad8120f2":"code","df4eda80":"code","27ce02e5":"code","f3b0f64f":"code","eb878a07":"code","22d51651":"code","d72e52ce":"code","ec5fc221":"code","ea213a86":"code","b1305b58":"code","d3d9a805":"code","2a17632f":"code","04ba3170":"code","dbcb6c66":"code","1bde44e9":"code","673eb475":"code","695441b0":"code","52259917":"code","c8faf6a8":"code","d2162a2d":"code","703f1011":"code","1ad9287a":"code","28629c84":"code","875e73c1":"code","48a05c71":"code","e485f2b5":"code","e0d78176":"code","7ecd1cce":"code","cab50588":"code","68e910c6":"code","cf9d2c31":"code","fb31aa6d":"code","32ed50f7":"code","64c105aa":"markdown","33bef0fb":"markdown","27bf88be":"markdown","be39187e":"markdown","fa91bfd8":"markdown","4d852721":"markdown","3b2a5b3b":"markdown","842d8ae5":"markdown","a0ea6c5f":"markdown","fcb57c4a":"markdown","3cd50cfa":"markdown","bbacab3a":"markdown","fb122adc":"markdown","62346131":"markdown","192c5826":"markdown","f826e05c":"markdown","8cdff21b":"markdown"},"source":{"b8080802":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05ffeb1c":"import torch\nfrom torchtext import data\n\nimport pandas as pd\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchtext\n\nimport spacy\n\nfrom torch.autograd import Variable\n\nimport time\nimport copy\nfrom torch.optim import lr_scheduler\n\nfrom sklearn.model_selection import train_test_split\nfrom torchtext.vocab import Vectors, GloVe\nfrom matplotlib.pyplot import plot, hist, xlabel, legend","d1083d9f":"train_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntrain_data.head()","fd65807f":"import re\nurl = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\ntrain = train_data['text'].apply(lambda tweet: url.sub(r'',tweet))\ntest = test_data['text'].apply(lambda tweet: url.sub(r'',tweet))","29dfae27":"train=train.str.lower().str.replace(\"[^a-z]\", \" \")\ntest=test.str.lower().str.replace(\"[^a-z]\", \" \")","fa52f545":"tokenized_train = train.apply(lambda tweet: tweet.split())\ntokenized_test = test.apply(lambda tweet: tweet.split())","bcac7772":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \nstop_words = stopwords.words('english')\n\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\ntokenized_train = tokenized_train.apply(lambda tweet: [lem.lemmatize(word) for word in tweet if word not in stop_words])\ntokenized_test = tokenized_test.apply(lambda tweet: [lem.lemmatize(word) for word in tweet if word not in stop_words])","4993c528":"detokenized_train = [] \nfor i in range(len(train)): \n    t = ' '.join(tokenized_train[i]) \n    detokenized_train.append(t) \n\ndetokenized_test = [] \nfor i in range(len(test)): \n    t = ' '.join(tokenized_test[i]) \n    detokenized_test.append(t) \n\ntrain_data['text'] = detokenized_train\ntest_data['text'] = detokenized_test","5f78e5d7":"train_data.head()","d6be7495":"train = train_data[['text', 'target']]\ntest = test_data[['text']]","2ae4c015":"X = train['text']\ny = train['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)","ad8120f2":"train_data = pd.concat([X_train, y_train], axis=1)\nval_data = pd.concat([X_val, y_val], axis=1)","df4eda80":"!mkdir torchtext_data","27ce02e5":"train_data.to_csv(\"torchtext_data\/train.csv\", index=False)\nval_data.to_csv(\"torchtext_data\/val.csv\", index=False)","f3b0f64f":"is_cuda = torch.cuda.is_available()\nprint(\"Cuda Status on system is {}\".format(is_cuda))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","eb878a07":"tweet_len=train['text'].str.split().map(lambda x: len(x))\nhist(tweet_len,color='blue')","22d51651":"fix_length = 17\nTEXT = data.Field(sequential=True, tokenize=\"spacy\", fix_length=fix_length)\nLABEL = data.LabelField(dtype=torch.long, sequential=False)","d72e52ce":"train_data, val_data = data.TabularDataset.splits(\n    path=\"torchtext_data\/\", train=\"train.csv\", \n    test=\"val.csv\",format=\"csv\", skip_header=True, \n    fields=[('Text', TEXT), ('Label', LABEL)]\n)","ec5fc221":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of testing examples: {len(val_data)}')","ea213a86":"TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\nLABEL.build_vocab(train_data)\n\nprint(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\nprint(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")","b1305b58":"batch_size = 16\n \ntrain_iterator, val_iterator = data.BucketIterator.splits(\n    (train_data, val_data), sort_key=lambda x: len(x.Text),\n    batch_size=batch_size,\n    device=device)","d3d9a805":"class LSTM2DMaxPoolClassifier(nn.Module):\n\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, num_layers, weights):\n\t\tsuper(LSTM2DMaxPoolClassifier, self).__init__()\n\t\t\n\t\t\"\"\"\n\t\tArguments\n\t\t---------\n\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n\t\toutput_size : 2 = (pos, neg)\n\t\thidden_sie : Size of the hidden_state of the LSTM\n\t\tvocab_size : Size of the vocabulary containing unique words\n\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\t\t\n\t\t\"\"\"\n\t\tself.hidden_size = hidden_size\n\t\tself.batch_size = batch_size\n\t\tself.num_layers = num_layers\n\t\t\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n\t\tself.lstm = nn.LSTM(embedding_length, hidden_size, num_layers, batch_first = True)\n\t\tself.maxpool = nn.MaxPool1d(4) # Where 4 is kernal size\n\t\tself.label = nn.Linear(hidden_size\/\/4, output_size)  #\/\/4 for maxpool\n\t\t\n\tdef forward(self, input_sentence, batch_size=None):\n\t\n\t\t\"\"\" \n\t\tParameters\n\t\t----------\n\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\t\t\n\t\tReturns\n\t\t-------\n\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n\t\tfinal_output.shape = (batch_size, output_size)\n\t\t\n\t\t\"\"\"\n\t\t\n\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n\t\tif batch_size is None:\n\t\t  h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM, num_layers*2 for biderection\n\t\t  c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM, num_layers*2 for biderection\n\t\telse:\n\t\t\th_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n\t\t\tc_0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n\n\t\tpooled = self.maxpool(output)\n\n\t\tfinal_output = self.label(pooled[:, -1, :]) #the same if we would use final_hidden_state\n\n\t\treturn final_output","2a17632f":"word_embeddings = TEXT.vocab.vectors\noutput_size = 2\nnum_layers = 1\nhidden_size = 32\nembedding_length = 300\nvocab_size = len(TEXT.vocab)","04ba3170":"model = LSTM2DMaxPoolClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, num_layers, word_embeddings)","dbcb6c66":"optimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)","1bde44e9":"model = model.to(device)\ncriterion = criterion.to(device)","673eb475":"dataiter_dict = {'train': train_iterator, 'val': val_iterator}\ndataset_sizes = {'train':len(train_data), 'val':len(val_data)}","695441b0":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n    print('starting')\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 200\n\n    val_loss = []\n    train_loss = []\n    val_acc = []\n    train_acc = []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            sentiment_corrects = 0\n            tp = 0.0\n            tn = 0.0\n            fp = 0.0\n            fn = 0.0\n                      \n            # Iterate over data.\n            for batch in dataiter_dict[phase]:\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    text = batch.Text\n                    label = batch.Label\n                    label = torch.autograd.Variable(label).long()\n                    if torch.cuda.is_available():\n                      text = text.cuda()\n                      label = label.cuda()\n                    if (batch.Text.size()[1] is not batch_size):\n                      continue\n                    \n                    outputs = model(text)\n\n                    outputs = F.softmax(outputs,dim=-1)\n                    \n                    loss = criterion(outputs, label)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        \n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * text.size(0)\n                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == label)\n\n                tp += torch.sum(torch.max(outputs, 1)[1] & label)\n                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-label)\n                fp += torch.sum(torch.max(outputs, 1)[1] & 1-label)\n                fn += torch.sum(1-torch.max(outputs, 1)[1] & label)\n\n                \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n \n            sentiment_acc = sentiment_corrects.double() \/ dataset_sizes[phase]\n\n            if phase == 'train':\n                train_acc.append(sentiment_acc)\n                train_loss.append(epoch_loss)\n            elif phase == 'val':\n                val_acc.append(sentiment_acc)\n                val_loss.append(epoch_loss)\n\n            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n            print('{} sentiment_acc: {:.4f}'.format(\n                phase, sentiment_acc))\n\n            if phase == 'val' and epoch_loss < best_loss:\n                print('saving with loss of {}'.format(epoch_loss),\n                      'improved over previous {}'.format(best_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), 'lstm_model_test.pth')\n\n        print()\n\n    confusion_matrix = [[tp, fp],[fn, tn]]\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val loss: {:4f}'.format(float(best_loss)))\n    results = {'time': time_elapsed, 'conf_matr': confusion_matrix,\n               'val_loss': val_loss, 'train_loss': train_loss, 'val_acc': val_acc, 'train_acc': train_acc}\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, results","52259917":"model_fit, res = train_model(model, criterion, optimizer, exp_lr_scheduler,\n                       num_epochs=8)","c8faf6a8":"plot(res['train_acc'], label = 'Train Accuracy')\nplot(res['val_acc'], label = 'Test Accuracy')\nxlabel('Epoch')\nlegend()","d2162a2d":"test.head()","703f1011":"test.to_csv(\"torchtext_data\/test.csv\", index=False)","1ad9287a":"test_data = data.TabularDataset(\n    path=\"torchtext_data\/test.csv\", format=\"csv\", skip_header=True, \n    fields=[('Text', TEXT)]\n)","28629c84":"print(f'Number of testing examples: {len(test_data)}')","875e73c1":"batch_size = 16\n\n# keep in mind the sort_key option \ntest_iterator = data.BucketIterator(\n    test_data,\n    train=False,\n    sort = False,\n    sort_within_batch=False,\n    repeat=False,\n    batch_size=batch_size,\n    device=device)","48a05c71":"model.eval()\npred = []\n\nfor batch in test_iterator:\n  text = batch.Text\n  if torch.cuda.is_available():\n    text = text.cuda()\n  if (batch.Text.size()[1] is not batch_size):\n    continue\n  outputs = model(text)\n  outputs = F.softmax(outputs,dim=-1)\n  outputs = torch.max(outputs, 1)[1]\n  pred.append(outputs.cpu().numpy())","e485f2b5":"pred = np.array(pred)\npred = pred.reshape(-1)","e0d78176":"test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.concat([test['id'], pd.Series(pred, name='target')], axis=1)","7ecd1cce":"sub.head()","cab50588":"sub = sub.fillna(0)","68e910c6":"sub = sub.astype(int)","cf9d2c31":"sub","fb31aa6d":"sub.to_csv('sub.csv', index=False)","32ed50f7":"!head sub.csv","64c105aa":"Next step will be **lowercasing** and noise removal","33bef0fb":"Let's make a prediction:","27bf88be":"Define train and validation datasets:","be39187e":"Splitting to **train** and **validation** sets","fa91bfd8":"And saving this sets:","4d852721":"Fit the model:","3b2a5b3b":"Leave only the necessary columns","842d8ae5":"Also, we should know length of tweets:","a0ea6c5f":"Inspired by model from [paper](https:\/\/www.aclweb.org\/anthology\/C16-1329.pdf)","fcb57c4a":"Now we will clean it. Firstly, we will **remove urls**:","3cd50cfa":"Finaly, our model:","bbacab3a":"Building vocabulary using GloVe with dim = 300\nIt can take some time for downloading","fb122adc":"Splitting text for **lemmatization ** and** stop-word removal**","62346131":"Defining iterators:","192c5826":"So, now it looks like that:","f826e05c":"So, I'd like to choose fix_length = 17. It length of sequence, which we will input in our LSTM, shorter will be padded by zeros, when longer will be truncated","8cdff21b":"Let's have a look at our data"}}