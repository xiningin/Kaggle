{"cell_type":{"c9d646c0":"code","20a2ed8d":"code","3ce69105":"code","37e51c82":"code","0eeb17fd":"code","cfec2136":"code","dbb07aa5":"code","aea75b90":"code","d1330f3c":"code","216d167a":"code","bd341d32":"code","98242b85":"code","1f13a69b":"code","fe7cadb8":"code","e1b01263":"code","e0c4b822":"code","ce5a34b2":"code","be6621ed":"code","a9618c92":"code","4bdfeabf":"code","0274c8d0":"code","72bfefbd":"code","ce646ba2":"code","c123aed6":"code","347e1248":"code","278d5d18":"code","0e7eeab1":"code","f73b2ffc":"code","8292e33f":"code","a0136c3d":"code","03be70a6":"code","17425fa4":"code","770b3bfb":"code","20f6d42e":"code","2d16ef18":"code","2551756a":"code","1e19656a":"code","1237074d":"code","b692365d":"code","14fdaf9e":"markdown","45c32b34":"markdown","2c6b3979":"markdown","8f435165":"markdown","79612b1f":"markdown","31cb7a88":"markdown","9bbf8e9f":"markdown","59058e06":"markdown","2192a912":"markdown","6f800cce":"markdown","82062af4":"markdown","c51a0c85":"markdown","652dfb78":"markdown","77b39887":"markdown","51f069bc":"markdown","2c493775":"markdown","757f0304":"markdown","af89ff4e":"markdown","0830ab9f":"markdown","d2ccfff1":"markdown","228b0dc7":"markdown","7a6a6741":"markdown","1a194878":"markdown","2e2375af":"markdown","bb13f813":"markdown","0991b070":"markdown","4bb759b7":"markdown","3e9087b4":"markdown","003ed415":"markdown","8ff19c21":"markdown","48367bb6":"markdown","b53d619f":"markdown","cf5d13a5":"markdown","c80f9684":"markdown","f55cb108":"markdown","1414e0ed":"markdown","d5a57bbb":"markdown","0b349b73":"markdown","726a42dd":"markdown","46a34356":"markdown","6bc216e6":"markdown","ce51d1f6":"markdown","0fd87f53":"markdown","2b095c94":"markdown","98234967":"markdown"},"source":{"c9d646c0":"import warnings\nwarnings.filterwarnings(\"ignore\")","20a2ed8d":"import nltk\nnltk.download('all')","3ce69105":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport string\nfrom nltk.tokenize import WordPunctTokenizer \nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, auc, confusion_matrix, roc_curve\n!pip install treeinterpreter\nfrom treeinterpreter import treeinterpreter as ti\nimport pickle","37e51c82":"original_dataset = pd.read_csv(\"\/kaggle\/input\/paatal-lok-imdb-reviews-dataset\/Paatal_Lok_IMDB_Reviews.csv\",\n                              index_col = '#')\ndataset = original_dataset.copy()\ndataset.head()","0eeb17fd":"dataset = dataset.drop(['username', 'review-date', 'actions-helpful', 'rating'], axis = 1)\ndataset.head()","cfec2136":"dataset[['title', 'review-text']] = dataset[['title', 'review-text']].astype(str)\ndataset['Type'].value_counts().plot(kind = 'barh')\nplt.show()","dbb07aa5":"pos, neg = dataset['Type'].value_counts().values\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total))","aea75b90":"random.seed(123)\nmsk = np.random.rand(len(dataset)) < 0.75\ntrain_dataset = dataset[msk]\ntest_dataset = dataset[~msk]\nprint(len(train_dataset))\nprint(len(test_dataset))","d1330f3c":"train_dataset.head()","216d167a":"test_dataset.head()","bd341d32":"train_dataset['Type'].value_counts().plot(kind ='barh')\nplt.show()","98242b85":"test_dataset['Type'].value_counts().plot(kind ='barh')\nplt.show()","1f13a69b":"y_train = train_dataset.pop('Type')\ny_test = test_dataset.pop('Type')","fe7cadb8":"labelencoder_y = LabelEncoder()\ny_train = labelencoder_y.fit_transform(y_train)\ny_test = labelencoder_y.transform(y_test)","e1b01263":"def preprocess(text):\n    text = \"\".join([character for character in text if character not in string.punctuation + '0123456789'])\n    text = text.lower()\n    tk = WordPunctTokenizer()\n    text = tk.tokenize(text)\n    stopword = stopwords.words('english')\n    text = [word for word in text if word not in stopword]\n    wn = WordNetLemmatizer()\n    text = [wn.lemmatize(word) for word in text]\n    text = \" \".join(text)\n    text = text.replace('propoganda', 'propaganda')\n    return text","e0c4b822":"train_corpus = []\nfor i in range(len(train_dataset)):\n    review = train_dataset.iloc[i, 0] + ' ' + train_dataset.iloc[i, 1]\n    train_corpus.append(review)\n    \ntest_corpus = []\nfor i in range(len(test_dataset)):\n    review = test_dataset.iloc[i, 0] + ' ' + test_dataset.iloc[i, 1]\n    test_corpus.append(review)","ce5a34b2":"weight_for_neg = (1 \/ neg)*(total)\nweight_for_pos = (1 \/ pos)*(total)\n\nclass_weight = {0: weight_for_neg, 1: weight_for_pos}\n\nprint('Weight for Negative Reviews: {:.2f}'.format(weight_for_neg))\nprint('Weight for Positive Reviews: {:.2f}'.format(weight_for_pos))","be6621ed":"text_clf_gs = Pipeline([\n    ('tfidf', TfidfVectorizer(preprocessor = preprocess)),\n    ('clf', RandomForestClassifier(n_estimators = 500, random_state = 0, class_weight = class_weight))\n])\n\nparameters = {\n    'tfidf__ngram_range' : [(1, 1), (1, 2), (1, 3)],\n    'clf__criterion' : ['gini', 'entropy'],\n    'clf__min_samples_leaf' : [2, 3, 4],\n    'clf__max_features' : ['None', 'sqrt', 'log2']\n}","a9618c92":"text_clf_gs = GridSearchCV(text_clf_gs, param_grid = parameters,\n                  cv = 5,\n                  scoring = 'roc_auc')\ntext_clf_gs = text_clf_gs.fit(train_corpus, y_train)\nprint(\"Best CV ROC_AUC Score:\", text_clf_gs.best_score_)\nbest_parameters = text_clf_gs.best_params_\nprint(\"Best Parameters:\", best_parameters)","4bdfeabf":"text_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(preprocessor = preprocess, ngram_range = best_parameters['tfidf__ngram_range'])),\n    ('clf', RandomForestClassifier(n_estimators = 500, random_state = 0, class_weight = class_weight,\n                                  criterion = best_parameters['clf__criterion'], \n                                   max_features = best_parameters['clf__max_features'], \n                                   min_samples_leaf = best_parameters['clf__min_samples_leaf']))\n])\n\ntext_clf = text_clf.fit(train_corpus, y_train)","0274c8d0":"scores = text_clf.predict_proba(test_corpus)[: , 1]\nfpr, tpr, thresholds = roc_curve(y_test, scores)\nroc_auc = auc(fpr, tpr) \nprint(\"ROC-AUC:\", roc_auc)\n\nplt.subplots(figsize=(10, 10))\nplt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\nplt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\nfor x, y, txt in zip(fpr[::2], tpr[::2], thresholds[::2]):\n    plt.annotate(np.round(txt,2), (x, y-0.04))\nrnd_idx = 12\nplt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)), \n             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")","72bfefbd":"i = np.arange(len(tpr))\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),\n                    'tpr' : pd.Series(tpr, index = i), \n                    '1-fpr' : pd.Series(1-fpr, index = i), \n                    'tf' : pd.Series(tpr - (1-fpr), index = i), \n                    'thresholds' : pd.Series(thresholds, index = i)})\nroc.iloc[(roc.tf-0).abs().argsort()[:1], :]","ce646ba2":"thres = roc.iloc[(roc.tf-0).abs().argsort()[:1], 4].values","c123aed6":"def plot_cm(labels, predictions, p = 0.5):\n  cm = confusion_matrix(labels, predictions > p)\n  plt.figure(figsize = (5, 5))\n  sns.heatmap(cm, annot = True, fmt = \"d\")\n  plt.title('Confusion matrix for threshold = {}'.format(p))\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Negative Reviews Detected Negative (True Negatives): ', cm[0][0])\n  print('Negative Reviews Detected Positive (False Positives): ', cm[0][1])\n  print('Positive Reviews Detected Negative (False Negatives): ', cm[1][0])\n  print('Positive Reviews Detected Positive (True Positives): ', cm[1][1])\n  print('Total Negative Reviews: ', np.sum(cm[0]))","347e1248":"plot_cm(y_test, scores, p = thres)\n# Predict test set result\ny_pred = (scores > thres).astype('int64')\nprint(\"Test Accuracy:\", accuracy_score(y_test, y_pred))","278d5d18":"prediction, bias, contributions = ti.predict(text_clf['clf'], text_clf['tfidf'].transform(test_corpus))","0e7eeab1":"contribution_frame = pd.DataFrame(contributions[:, :, 1], columns = text_clf['tfidf'].get_feature_names())\ncontribution_frame.describe().T.iloc[np.argsort(-(contribution_frame.describe().T['mean'].abs()))]","f73b2ffc":"sns_colors = sns.color_palette('colorblind')","8292e33f":"# Boilerplate code for plotting :)\ndef _get_color(value):\n    \"\"\"To make positive DFCs plot green, negative DFCs plot red.\"\"\"\n    green, red = sns.color_palette()[2:4]\n    if value >= 0: return green\n    return red\n\ndef _add_feature_values(feature_values, ax):\n    \"\"\"Display feature's values on left of plot.\"\"\"\n    x_coord = ax.get_xlim()[0]\n    OFFSET = 0.15\n    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):\n        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(round(feat_val, 2)), size=12)\n        t.set_bbox(dict(facecolor='white', alpha=0.5))\n    from matplotlib.font_manager import FontProperties\n    font = FontProperties()\n    font.set_weight('bold')\n    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\\nvalue',\n    fontproperties=font, size=12)\n\ndef plot_example(example):\n  TOP_N = 8 # View top 8 features.\n  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.\n  example = example[sorted_ix]\n  colors = example.map(_get_color).tolist()\n  ax = example.to_frame().plot(kind='barh',\n                          color=[colors],\n                          legend=None,\n                          alpha=0.75,\n                          figsize=(10,6))\n  ax.grid(False, axis='y')\n  ax.set_yticklabels(ax.get_yticklabels(), size=14)\n\n  # Add feature values.\n  _add_feature_values(pd.DataFrame.sparse.from_spmatrix(text_clf['tfidf'].transform(test_corpus),\n                                 columns = text_clf['tfidf'].get_feature_names()).iloc[ID][sorted_ix], ax)\n  return ax","a0136c3d":"probs = prediction[: , 1]\nlabels = y_test","03be70a6":"ID = np.where(y_test == 0)[0][0]\nexample = contribution_frame.iloc[ID] \nTOP_N = 8  \nsorted_ix = example.abs().sort_values()[-TOP_N:].index\nax = plot_example(example)\nax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\nax.set_xlabel('Contribution to predicted probability', size = 14)\nplt.show()","17425fa4":"ID = np.where(y_test == 1)[0][0]\nexample = contribution_frame.iloc[ID]  \nTOP_N = 8 \nsorted_ix = example.abs().sort_values()[-TOP_N:].index\nax = plot_example(example)\nax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\nax.set_xlabel('Contribution to predicted probability', size = 14)\nplt.show()","770b3bfb":"# Boilerplate plotting code.\ndef dist_violin_plot(df_dfc, ID):\n  # Initialize plot.\n  fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n  # Create example dataframe.\n  TOP_N = 8  # View top 8 features.\n  example = df_dfc.iloc[ID]\n  ix = example.abs().sort_values()[-TOP_N:].index\n  example = example[ix]\n  example_df = example.to_frame(name='dfc')\n\n  # Add contributions of entire distribution.\n  parts=ax.violinplot([df_dfc[w] for w in ix],\n                 vert=False,\n                 showextrema=False,\n                 widths=0.7,\n                 positions=np.arange(len(ix)))\n  face_color = sns_colors[0]\n  alpha = 0.15\n  for pc in parts['bodies']:\n      pc.set_facecolor(face_color)\n      pc.set_alpha(alpha)\n\n  # Add feature values.\n  _add_feature_values(pd.DataFrame.sparse.from_spmatrix(text_clf['tfidf'].transform(test_corpus),\n                                 columns = text_clf['tfidf'].get_feature_names()).iloc[ID][sorted_ix], ax)\n\n  # Add local contributions.\n  ax.scatter(example,\n              np.arange(example.shape[0]),\n              color=sns.color_palette()[2],\n              s=100,\n              marker=\".\",\n              label='contributions for example')\n\n  # Legend\n  # Proxy plot, to show violinplot dist on legend.\n  ax.plot([0,0], [1,1], label='test set contributions\\ndistributions',\n          color=face_color, alpha=alpha, linewidth=10)\n  legend = ax.legend(loc='lower center', shadow=True, fontsize='x-large',\n                     frameon=True)\n  legend.get_frame().set_facecolor('white')\n\n  # Format plot.\n  ax.set_yticks(np.arange(example.shape[0]))\n  ax.set_yticklabels(example.index)\n  ax.grid(False, axis='y')\n  ax.set_xlabel('Contribution to predicted probability', size=14)","20f6d42e":"ID = np.where(y_test == 1)[0][0]\ndist_violin_plot(contribution_frame, ID)\nplt.title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\nplt.show()","2d16ef18":"# Gain Based\nimportances = text_clf['clf'].feature_importances_\ndf_imp = pd.Series(importances, index = text_clf['tfidf'].get_feature_names())\ndf_imp = df_imp.sort_values(ascending = False)\nN = 8\nax = (df_imp.iloc[0:N][::-1]\n    .plot(kind='barh',\n          color=sns_colors[2],\n          title='Gain feature importances',\n          figsize=(10, 6)))\nax.grid(False, axis='y')","2551756a":"# Aggregated Feature Importances\ncontribution_mean = contribution_frame.abs().mean()\nN = 8\nsorted_ix = contribution_mean.abs().sort_values()[-N:].index  \nax = contribution_mean[sorted_ix].plot(kind='barh',\n                       color=sns_colors[1],\n                       title='Mean directional feature contributions',\n                       figsize=(10, 6))\nax.grid(False, axis='y')","1e19656a":"filename = 'Paatal_Lok_Review_Classifier.pkl'\npickle.dump(text_clf, open(filename, 'wb'))","1237074d":"model = pickle.load(open(filename, 'rb'))","b692365d":"# Put the review in a list\n# Put in this format [review title : review text]\na_positive_and_a_negative_review = [\"Don't go by the overall ratings of the show : I don't write reviews but the ratings break my heart. The show shows exactly what is wrong with the Indian society. The bad ratings and the backlash proves how ignorant and overtly sensitive we are to the truth. Stellar performances, incredibly layered characters, amazing character building, brilliant writing & direction. This tops Sacred Games for me as an overall show. Believe a stranger, give it a watch if you haven't. Watch it with an open mind. Set aside your ego and privileges before you do, though.\",\n                                   \"Hindu religion is being shown in Bad light. So much hatred based on lies : Showing a Hindu Pandit cooking meat and serving to a guy sitting in front of Hindu Goddess photo and eating Non-veg. Is this artistry ? And the dialogue used there is so puke worthy. Can't even say it here. It's regarding a mother. Sickest minds. Calling a Nepalese woman as prostitute. This is Racism. Sickness. And showing our Hindu symbols and Pooja as violence. All exaggerations.\"]\nmodel.predict_proba(a_positive_and_a_negative_review)","14fdaf9e":"Let's find out the best threshold.","45c32b34":"# Extracting The Labels","2c6b3979":"## Global Interpretation","8f435165":"# Vectorization & Building The Classifier","79612b1f":"Let's now have a look at the testing ROC-AUC curve.","31cb7a88":"Let's now extract the labels of the training set and the testing set.","9bbf8e9f":"Now visualize the top 8 features resulting the positive review for the first individual of the testing set giving positive review.","59058e06":"In this data set we actually have two columns of text - one is review title, and another one is review description. We shall join these two strings and apply the preprocessing afterwards.","2192a912":"# Importing The Libraries","6f800cce":"## Confusion Matrix","82062af4":"Let's now see the frequency distribution of number of positive and negative reviews in the data set.","c51a0c85":"Now we shall interprete our model both locally and globally. Local interpretability refers to an understanding of a model\u2019s predictions at the individual example level, while global interpretability refers to an understanding of the model as a whole. Such techniques can help machine learning (ML) practitioners detect bias and bugs during the model development stage.\n\nAs pointed out in [here](https:\/\/blog.datadive.net\/interpreting-random-forests\/), there is a very straightforward way to make random forest predictions more interpretable, leading to a similar level of interpretability as linear models - not in the static but dynamic sense. Every prediction can be trivially presented as a sum of feature contributions, showing how the features lead to a particular prediction. This opens up a lot of opportunities in practical machine learning and data science tasks:\n* Explain to an analyst why a particular prediction is made.\n* Debug models when results are unexpected.\n* Explain the differences of two datasets (for example, behavior before and after treatment), by comparing their average predictions and corresponding average feature contributions.\n\nThis way, any prediction can be decomposed into contributions from features, such that prediction = bias + contribution_from_feature_1 + contribution_from_feature_2 + ... + contribution_from_feature_K.\n\nFor global interpretability we shall retrieve and visualize gain-based feature importances (based on training data), and also show feature importances aggregated over all testing data.\n\nThe codes for this portion is taken from this [tutorial](http:\/\/www.tensorflow.org\/tutorials\/estimator\/boosted_trees_model_understanding#how_to_interpret_boosted_trees_models_both_locally_and_globally).","652dfb78":"# Text Preprocessing","77b39887":"# Train-Test Splitting","51f069bc":"# Interpretation of The Classifier","2c493775":"# How to Save & Load The Classifier","757f0304":"# Importing The Data","af89ff4e":"## Local Interpretation","0830ab9f":"Now I shall discuss little bit about TF-IDF vectorizer. After preprocessing the raw text, the next step is to vectorize the texts to find out meaningful features from them. We shall use TF-IDF vectorizer for that. The raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. The most common ways to extract numerical features from text content, namely:\n* Tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n* Counting the occurrences of tokens in each document.\n* Normalizing and weighting with diminishing importance tokens that occur in the majority of samples \/ documents.\n\nIn this scheme, features and samples are defined as follows:\n* Each individual token occurrence frequency (normalized or not) is treated as a feature.\n* The vector of all the token frequencies for a given document is considered a multivariate sample.\n\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n\nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\nIn a large text corpus, some words will be very present hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms. In order to reweight the count features into floating point values suitable for usage by a classifier it is very common to use the TF-IDF transform. Tf means term-frequency while tf\u2013idf means term-frequency times inverse document-frequency: tf-idf(t,d) = tf(t,d) * idf(t). Term Frequency (TF) summarizes how often a given word appears within a document, whereas Inverse Document Frequency (IDF) downscales words that appear a lot across documents. TF-IDFs are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The tf-idf vectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow us to encode new documents.\n\nTo understand this, lets say our corpus has documents on football. The TF will capture how many times a particular word appears in a single document. To understand it better lets take a word: \u201cthe\u201d. This word is quite common and would appear with high frequency in all our documents. But if we think about it, \u201cthe\u201d does not give any extra information about my document. But if we talk about the word \u201cmessi\u201d, it gives us a context. We can say with some certainty that the document may be about football. So we need to reduce the weightage of \u201cthe\u201d somehow. Now one may say we can take a reciprocal of their respective frequencies to give more weight to \u201cmessi\u201d. There may exist very rare words in our document. For example : \u201cpneumonoultramicroscopicsilicovolcanoconiosis\u201d. It is an invented long word said to mean a lung disease caused by inhaling very fine ash and sand dust. Now if we use reciprocal for this word, it would certainly be close to 1 but again does not tell us about the context. Let's say, tf('the') = 100, tf('messi') = 5, and tf('pneumonoultramicroscopicsilicovolcanoconiosis') = 1. IDF is calculated by taking the log of {number of docs in your corpus divided by the number of docs in which this term appears}. So for \u201cthe\u201d, the ratio will be close to 1 since it is present in most (all) docs. Therefore, the log will take this ratio to 0 - idf('the') = 0, idf('messi') = 0.52 (since corpus is about football), and idf('pneumonoultramicroscopicsilicovolcanoconiosis') = 1. Now we want that the weight given to \u201cmessi\u201d should be higher than the other two words. For this we multiply the two terms:  tfidf = tf * idf. So, tfidf('the') = 0, tfidf('messi') = 2.6, tfidf('pneumonoultramicroscopicsilicovolcanoconiosis') = 1. This weighting technique helps ML model during classification as we explicitly tell which word weighs more\/less.","d2ccfff1":"Now we shall randomly divide the data set into training and testing data set. Training data set will be used to fit the classifier, and the later one will be used to test its performance. If you train and test your classifier on the same data set, you'll introduce bias in your analysis. ","228b0dc7":"# Personal Opinion\n\nProviding with more data will make the classifier stronger. The data size is very small for this kind of task.","7a6a6741":"Let's open the model and do some predictions.","1a194878":"# References:\n* https:\/\/towardsdatascience.com\/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n* https:\/\/www.geeksforgeeks.org\/tokenize-text-using-nltk-python\/\n* https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction\n* https:\/\/www.tensorflow.org\/tutorials\/estimator\/boosted_trees_model_understanding\n* https:\/\/github.com\/andosa\/treeinterpreter\n* https:\/\/www.youtube.com\/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL","2e2375af":"Let's drop the unnecessary columns. Also since we are going to classify a review as postive or negative, let's just drop the 'rating' column also.","bb13f813":"Let's peek through the testing data set.","0991b070":"Let's now save the model.","4bb759b7":"Additionally, one might want to understand the model as a whole, rather than studying individual predictions. Below, we will see:\n* Gain-based feature importances.\n* Aggregrate feature contributions over the test examples.\n\nGain-based feature importances measure the loss change when splitting on a particular feature. This method can be unreliable in situations where potential predictor variables vary in their scale of measurement or their number of categories and when features are correlated.","3e9087b4":"Contributions of all the features (in descending order of their average) on the testing examples can be summarized as the following: ","003ed415":"Let us import the necessary libraries:","8ff19c21":"## ROC-AUC","48367bb6":"Let's see the confusion matrix of the testing data.","b53d619f":"Hi, everyone. This is my first post in kaggle. Recently I am spending my lockdown time learning various concepts of Natural Language Processing (NLP) tasks. Firstly I want to confess that I am new to Python programming. For the past 5 years I had been using R as my tool for Statistical data analysis. I have taken the help of various websites to learn how to write necessary codes for sentiment analysis in Python. I have also written some of the codes on my own. So you may find some codes to be unnecessarily big. I appologize for that.\n\nThe field of NLP deals with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amount of natural language data. Sentiment analysis is one of the most common NLP tasks that systematically identifies, extracts, quantifies, and studies affective states and subjective information. In this project I am going to apply sentiment analysis techniques to analyze the reviews of a very popular Indian web series 'Paatal Lok'. Here our task will be to make the computer understand a raw text review to tell us whether it is a positive or a negative review.","cf5d13a5":"# Assessing The Quality of The Classifier","c80f9684":"Let us visualize the top 8 features resulting the negative review for the first individual of the testing set giving negative review.","f55cb108":"So our classifier is doing a very good job in predicting the minority class, i.e. the negative reviews.","1414e0ed":"Here after applying TF-IDF vectorizer we shall train a random forest classifier to predict the sentiments of the reviews. First let us tune some parameters by grid search.","d5a57bbb":"Since, here we are dealing with imbalance classification, we need to give more weight to the minority class in order to make the classifier able to detect it. Let's do that now.","0b349b73":"Let's import the data set into a pandas dataframe and quickly glance over its first few rows.","726a42dd":"Let's peek through the training data set.","46a34356":"So we shall classify a review as positive only when its probability of being postive exceeds this threshold.","6bc216e6":"As we see here that the number of positive reviews is 73.07% of the total. So this data set will bring to us the problem of imbalance classification. For more information about the problems of imbalance classification see this [link](http:\/\/https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/#:~:text=Imbalanced%20classification%20refers%20to%20a,is%20instead%20biased%20or%20skewed.).","ce51d1f6":"The larger magnitude contributions have a larger impact on the model's prediction. Negative contributions indicate the feature value for this given example reduced the model's prediction (towards positive review), while positive values contribute an increase in the prediction (towards positive review).","0fd87f53":"# Introduction","2b095c94":"Now our task will be to preprocess the raw text. First we shall remove any kind of punctuations and numbers from the raw text. Then we shall set all the characters of the text into lowercase. After that we shall tokenize the text to extract the tokens (here the individual words) from the text. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. These tokens are often loosely referred to as terms or words. To know more about tokenization visit [this](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html) and [this](https:\/\/www.geeksforgeeks.org\/nlp-how-tokenizing-text-sentence-words-works\/). Now we shall remove the stop words from the list of words. Stop words are basically a set of commonly used words in any language. The reason why stop words are critical to many applications is that, if we remove the words that are very commonly used in a given language, we can focus on the important words instead. For example to find the published Snowball stop word list go to [this link](http:\/\/snowball.tartarus.org\/algorithms\/english\/stop.txt). Then we shall lemmatize the words. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. To learn more about lemmatization visit [this link](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html#:~:text=Lemmatization%20usually%20refers%20to%20doing,is%20known%20as%20the%20lemma%20.). The step of preprocessing the raw text actually gets done here, but on going through the reviews I found that some people misspelled 'propaganda' as 'propoganda'. So, I just add one more step to correct this.","98234967":"One can also plot the example's feature contributions and compare with the entire distribution using a voilin plot."}}