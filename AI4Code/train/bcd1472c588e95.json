{"cell_type":{"8186ec04":"code","74ff469a":"code","bcc11e4f":"code","6d40052c":"code","6200e6a3":"code","7153a190":"code","10df0032":"code","82023daa":"code","8b87c35c":"code","ce15d9c3":"code","68b3d551":"code","b42e7ea8":"code","609cd759":"markdown","5b3857dd":"markdown","feb9ab08":"markdown","5549f9b2":"markdown","dcf61dfa":"markdown","3729053f":"markdown","50015598":"markdown","afca884e":"markdown","49b6f34e":"markdown","02adb053":"markdown"},"source":{"8186ec04":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport pathlib\nimport os\nfrom IPython import display","74ff469a":"label_dict = {'right': 0, 'down': 1, 'no': 2, 'left': 3, 'up': 4, 'go': 5, 'yes': 6, 'stop': 7}\ncommands = np.array(list(label_dict.keys()))\n\ndata_dir = pathlib.Path('data\/mini_speech_commands')\nif not data_dir.exists():\n    tf.keras.utils.get_file(\n        'mini_speech_commands.zip',\n        origin=\"http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/mini_speech_commands.zip\",\n        extract=True,\n        cache_dir='.', cache_subdir='data'\n    )\nfilenames = tf.io.gfile.glob(str(data_dir) + '\/*\/*')\n\n## Preprocessing \ndef decode_audio(file_path):\n    audio_binary = tf.io.read_file(file_path)\n    audio, _ = tf.audio.decode_wav(audio_binary)\n    audio = tf.squeeze(audio, axis=-1)\n    zero_padding = tf.zeros([16000] - tf.shape(audio), dtype=tf.float32)\n    audio = tf.cast(audio, tf.float32)\n    equal_length = tf.concat([audio, zero_padding], 0)\n    return equal_length\n\ndef get_label(file_path):\n    parts = tf.strings.split(file_path, os.path.sep)\n    label_id = tf.argmax(parts[-2] == commands)\n    return label_id\n\ndef get_waveform_and_label(file_path):\n    label = get_label(file_path)\n    waveform = decode_audio(file_path)\n    return waveform, label\n\ndef get_spectrogram(waveform):\n    # Padding for files with less than 16000 samples\n    spectrogram = tf.signal.stft(\n        waveform, frame_length=255, frame_step=128)\n    spectrogram = tf.abs(spectrogram)\n    return spectrogram\n\n\n### Datasets\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nfiles_ds = tf.data.Dataset.from_tensor_slices(filenames)\nwaveform_ds = files_ds.map(decode_audio, num_parallel_calls=AUTOTUNE)\nspectrogram_ds = waveform_ds.map(get_spectrogram, num_parallel_calls=AUTOTUNE)\nclassification_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\ntrain_ds = classification_ds.shuffle(buffer_size=int(1e5), seed=42).take(6400).batch(64).cache().prefetch(AUTOTUNE)\nval_ds =  classification_ds.shuffle(buffer_size=int(1e5), seed=42).skip(6400).batch(64).cache().prefetch(AUTOTUNE)\ntrain_autoencoder_ds = train_ds.map(lambda x, y: (x, x))\nval_autoencoder_ds = val_ds.map(lambda x, y: (x, x))\n\n## Plots\nn_plots = 3\nfig, axs = plt.subplots(n_plots, 1, sharex=True, figsize=(25, 8))\nfor i, (waveform) in enumerate(waveform_ds.take(n_plots)):\n    axs[i].plot(waveform.numpy())\nplt.xlabel('Time')\nfig.show()\nfor spectrogram in spectrogram_ds.take(3): \n    pass","bcc11e4f":"class FTHelper: \n    \n    @staticmethod\n    def build_conv_matrix(n_frames): \n        conv_matrix = tf.eye(n_frames)\n        conv_matrix = tf.expand_dims(conv_matrix, 0)\n        conv_matrix = tf.expand_dims(conv_matrix, -1)\n        return conv_matrix\n\n    @staticmethod\n    def build_window(frame_length, window_type='hann'): \n        if window_type == 'hann': \n            weights = tf.signal.hann_window(frame_length)\n            weights = tf.expand_dims(weights, -1)\n            weights = tf.expand_dims(weights, -1)\n            weights = tf.expand_dims(weights, 0)\n            return tf.sqrt(weights)\n        elif window_type == 'ones': \n            return tf.ones([1, frame_length, 1, 1])\n        elif window_type == 'uniform': \n            return tf.random.uniform([1, frame_length, 1, 1], minval=0, maxval=1)\n        elif window_type == 'random': \n            return tf.random.normal([1, frame_length, 1, 1])\n        else: \n            # TODO: Raise error\n            return \n    \n    @staticmethod\n    def build_frequencies(n_freq, freq_type='linspace'): \n        if freq_type == 'linspace': \n            return tf.linspace(0., 3.14, n_freq)\n        elif freq_type == 'uniform': \n            return tf.random.uniform([n_freq], minval=0, maxval=3.14)\n        else: \n            # TODO: Raise Error\n            return \n\n\nclass STFT(tf.keras.layers.Layer):\n\n    def __init__(\n        self, \n        frame_length, \n        frame_step, \n        n_freqs, \n        frequency_trainable=False, \n        window_trainable=False, \n        freq_type='linspace', \n        window_type='hann',\n        out_module=True\n    ):\n        super(STFT, self).__init__()\n        self.frame_length = frame_length\n        self.frame_step = frame_step\n        self.n_freqs = n_freqs\n        self.frequency_trainable = frequency_trainable\n        self.window_trainable = window_trainable\n        self.freq_type = freq_type\n        self.window_type = window_type\n        self.out_module = out_module\n    \n    def _call_frame_matrix(self):\n        # Convolution2d Transpose\n        frame_matrix = tf.squeeze(\n            tf.nn.conv2d_transpose(\n                input=self.conv_matrix, \n                filters=tf.square(self.window), \n                output_shape=(1, self.n_frames, self.n_y, 1),\n                strides=[1, 1, self.frame_step, 1], \n                padding=\"VALID\"\n            )\n        )\n        # Padding\n        to_pad = self.sequence_length - self.n_y\n        frame_matrix = tf.pad(frame_matrix, tf.constant([[0, 0], [0, to_pad]]), 'CONSTANT')\n        return frame_matrix\n  \n    def build(self, input_shape):\n        self.sequence_length = input_shape[-1]\n        self.n_frames = (self.sequence_length - self.frame_length) \/\/ self.frame_step\n        self.n_y = (self.n_frames-1) * self.frame_step + self.frame_length\n        self.conv_matrix = FTHelper.build_conv_matrix(self.n_frames)\n        self.times = tf.range(self.sequence_length, dtype=tf.float32)\n        self.window = tf.Variable(\n            name='window', \n            initial_value=FTHelper.build_window(self.frame_length, self.window_type),\n            trainable=self.window_trainable\n        )\n        self.frequencies = tf.Variable(\n            name='frequencies',\n            initial_value=FTHelper.build_frequencies(self.n_freqs, self.freq_type), \n            trainable=self.frequency_trainable\n        )\n\n    def call(self, inputs):\n        frame_matrix = self._call_frame_matrix()\n        frames = tf.multiply(\n            tf.expand_dims(tf.transpose(frame_matrix), 0), \n            tf.expand_dims(inputs, -1), \n        )\n        frames = tf.transpose(frames, perm=[0, 2, 1])\n        # DFT Computations\n        operands = tf.tensordot(self.times, self.frequencies, axes=0)\n        w_real = tf.math.cos(-operands)\n        w_im = tf.math.sin(-operands)\n        y_real = tf.tensordot(frames, w_real, axes=1)\n        y_im = tf.tensordot(frames, w_im, axes=1)\n        if self.out_module: \n            y_mod = tf.add(tf.square(y_real), tf.square(y_im))\n            return tf.expand_dims(tf.math.log(y_mod + 1) \/ 2, -1)\n        else: \n            return tf.stack([y_real, y_im], axis=-1) \/ self.sequence_length\n\n        \nclass STFTInverseShared(tf.keras.layers.Layer): \n    \n    def __init__(self, stft_layer): \n        super(STFTInverseShared, self).__init__()\n        self.stft_layer = stft_layer\n        self.epsilon = 1e-10  # Regularizing parameter\n        \n    def build(self, input_shape):  \n        assert self.stft_layer.built\n    \n    def call(self, inputs): \n        frame_matrix = self.stft_layer._call_frame_matrix()\n        frame_matrix_t = tf.transpose(frame_matrix)\n        y_real = inputs[:, :, :, 0]\n        y_im = inputs[:, :, :, 1]\n        operands = tf.tensordot(self.stft_layer.times, self.stft_layer.frequencies, axes=0)\n        v_real = tf.math.cos(operands)\n        v_im = tf.math.sin(operands)\n        y_real_t = tf.transpose(y_real, [2, 0, 1])\n        y_im_t = tf.transpose(y_im, [2, 0, 1])\n        x_real = (tf.tensordot(v_real, y_real_t, axes=1) - tf.tensordot(v_im, y_im_t, axes=1))\n        x_im = (tf.tensordot(v_im, y_real_t, axes=1) + tf.tensordot(v_real, y_im_t, axes=1))\n        x_real_t = tf.transpose(x_real, [1, 0, 2])\n        x_real_ts = tf.reduce_sum(x_real_t * frame_matrix_t, axis=2) \/ (tf.reduce_mean(frame_matrix_t, axis=1) + self.epsilon)\n        return x_real_ts \n        \n\nclass STFTInverse(tf.keras.layers.Layer): \n    \n    def __init__(\n        self, \n        frame_length, \n        frame_step, \n        frequency_trainable=False, \n        window_trainable=False,\n        freq_type='linspace', \n        window_type='hann'\n    ): \n        super(STFTInverse, self).__init__()\n        self.frame_length = frame_length\n        self.frame_step = frame_step\n        self.frequency_trainable = frequency_trainable\n        self.window_trainable = window_trainable\n        self.freq_type = freq_type\n        self.window_type = window_type\n    \n    def _call_frame_matrix(self):\n        # Convolution2d Transpose\n        frame_matrix = tf.squeeze(\n            tf.nn.conv2d_transpose(\n                input=self.conv_matrix, \n                filters=tf.square(self.window), \n                output_shape=(1, self.n_frames, self.n_y, 1),\n                strides=[1, 1, self.frame_step, 1], \n                padding=\"VALID\"\n            )\n        )\n        # Padding\n        to_pad = self.sequence_length - self.n_y\n        frame_matrix = tf.pad(frame_matrix, tf.constant([[0, 0], [0, to_pad]]), 'CONSTANT')\n        return frame_matrix\n    \n    def build(self, input_shape): \n        # Shapes\n        self.n_frames = input_shape[-3]\n        self.n_freqs = input_shape[-2]\n        self.sequence_length = self.n_frames * self.frame_step + self.frame_length + 1\n        self.n_y = (self.n_frames-1) * self.frame_step + self.frame_length\n        assert input_shape[-1] == 2, 'Last dim must be two (Real \/ Imaginary)'\n        # Not Trainable\n        self.times = tf.range(self.sequence_length, dtype=tf.float32)\n        self.conv_matrix = FTHelper.build_conv_matrix(self.n_frames)\n        # Trainable\n        self.window = tf.Variable(\n            name='window', \n            initial_value=FTHelper.build_window(self.frame_length, self.window_type), \n            trainable=self.window_trainable)\n        self.frequencies = tf.Variable(\n            name='frequencies',\n            initial_value=FTHelper.build_frequencies(self.n_freqs, self.freq_type), \n            trainable=self.frequency_trainable)\n    \n    def call(self, inputs): \n        frame_matrix = self._call_frame_matrix()\n        frame_matrix_t = tf.transpose(frame_matrix)\n        y_real = inputs[:, :, :, 0]\n        y_im = inputs[:, :, :, 1]\n        operands = tf.tensordot(self.times, self.frequencies, axes=0)\n        v_real = tf.math.cos(operands)\n        v_im = tf.math.sin(operands)\n        y_real_t = tf.transpose(y_real, [2, 0, 1])\n        y_im_t = tf.transpose(y_im, [2, 0, 1])\n        x_real = (tf.tensordot(v_real, y_real_t, axes=1) - tf.tensordot(v_im, y_im_t, axes=1))\n        x_im = (tf.tensordot(v_im, y_real_t, axes=1) + tf.tensordot(v_real, y_im_t, axes=1))\n        epsilon = 1e-10  # Regularizing parameter\n        x_real_t = tf.transpose(x_real, [1, 0, 2])\n        x_real_ts = tf.reduce_sum(x_real_t * frame_matrix_t, axis=2) \/ (tf.reduce_mean(frame_matrix_t, axis=1) + epsilon)\n        return x_real_ts \n        # x_im_t = tf.transpose(x_im, [1, 0, 2])\n        # x_im_ts = tf.reduce_sum(x_im_t * frame_matrix_t, axis=1) \/ (tf.reduce_sum(frame_matrix_t, axis=1) + epsilon)","6d40052c":"# Shape parameters\nsequence_length = 16000\nframe_length = 255\nframe_step = 128\nn_freqs = 130\nn_frames = (sequence_length - frame_length) \/\/ frame_step\n\nstft_layer = STFT(\n    frame_length=frame_length, \n    frame_step=frame_step, \n    n_freqs=n_freqs, \n    frequency_trainable=True, \n    window_trainable=True, \n    out_module=False\n)\nstft_inv_layer = STFTInverseShared(\n    stft_layer=stft_layer\n)\nspectrogram_layer = STFT(\n    frame_length=frame_length, \n    frame_step=frame_step, \n    n_freqs=n_freqs, \n    frequency_trainable=True, \n    window_trainable=True, \n    out_module=True\n)\n\nfor waveform in waveform_ds.skip(15).batch(1): \n    break\n    \n# Spectrograms Computations\nspectrogram_layer_output = tf.squeeze(spectrogram_layer(waveform))\nspectrogram_function_output = tf.squeeze(tf.math.log(1 + tf.abs(\n    tf.signal.stft(waveform, frame_length=255, frame_step=128)\n)))\n# AutoEncoder Computations\nstft_ouput = stft_layer(waveform)\nstft_inverse_output = stft_inv_layer(stft_ouput)\n\n# Plots Spectrograms\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\naxs[0].imshow(spectrogram_function_output.numpy().T)\naxs[0].set_title('STFT Spectrogram')\naxs[1].imshow(spectrogram_layer_output.numpy().T)\naxs[1].set_title('Network STFT Spectrogram')\nplt.show()\n\n# Plots Autoencoder\nfig, axs = plt.subplots(2, 1, sharex=True, figsize=(15, 5))\naxs[0].plot(waveform.numpy()[0, :])\naxs[1].plot(stft_inverse_output[0, :])\naxs[0].set_title('Original Waveform')\naxs[1].set_xlabel('Time')\naxs[1].set_title('AutoEncoded Waveform (STFT + STFT Inverse)')\nfig.show()\n\nprint('Original:')\ndisplay.display(display.Audio(waveform.numpy()[0, :], rate=16000))\nprint('Autoencoded:')\ndisplay.display(display.Audio(stft_inverse_output[0, :], rate=16000))","6200e6a3":"sequence_length = 16000\nframe_length = 255       # Same as before, but can be a hyperparam\nframe_step = 128         # Same as before, but can be a hyperparam\nn_freqs = 60            # Quite Low, can be a hyperparam\n\n\nstft = STFT(\n    frame_length=frame_length, \n    frame_step=frame_step, \n    n_freqs=n_freqs, \n    frequency_trainable=True, \n    window_trainable=True, \n    freq_type='uniform',       # Random Init \n    window_type='uniform',     # Random Init\n    out_module=False\n)\n\nstft_inv = STFTInverseShared(\n    stft_layer=stft\n)\n\ninputs = tf.keras.Input((sequence_length))\nstft_output = stft(inputs)\nautoencoder_output = stft_inv(stft_output)\nautoencoder = tf.keras.models.Model(inputs, autoencoder_output)\n\n\nautoencoder.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-2), \n    loss=tf.keras.losses.MeanSquaredError()\n)\ntf.keras.utils.plot_model(autoencoder, show_shapes=True)","7153a190":"fig, axs = plt.subplots(1, 2, figsize=(20, 5))\naxs[0].scatter(np.arange(0, frame_length), stft.get_weights()[0][0, :, 0, 0], marker='+')\naxs[0].set_title('Window Function')\n\nsns.distplot(stft.get_weights()[1], ax=axs[1], rug=True, kde=False, bins=int(np.sqrt(n_freqs)))\naxs[1].set_title('Distribution of Frequencies')\nplt.show()","10df0032":"# Training\nEPOCHS = 15\nhistory = autoencoder.fit(\n    train_autoencoder_ds, \n    validation_data=val_autoencoder_ds,   \n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)","82023daa":"plt.figure(figsize=(5, 3))\nplt.plot(history.history['loss'], label='TrainingLoss')\nplt.plot(history.history['val_loss'], label='ValLoss')\nplt.legend()\nplt.xlabel('# epoch')\nplt.title('Training')\nplt.show()","8b87c35c":"fig, axs = plt.subplots(1, 2, figsize=(20, 5))\naxs[0].scatter(np.arange(0, frame_length), stft.get_weights()[0][0, :, 0, 0] ** 2,  marker='+')\naxs[0].set_title('Window Function')\n\nsns.distplot(stft.get_weights()[1], ax=axs[1], rug=True, kde=False, bins=int(np.sqrt(n_freqs)))\naxs[1].set_title('Distribution of Frequencies')\nfig.show()","ce15d9c3":"for val_batch_X, _ in val_autoencoder_ds: \n    break\nval_batch_pred = autoencoder.predict(val_batch_X)\n\n\nidx = np.random.randint(64)\n\nfig, axs = plt.subplots(2, 1, sharex=True, figsize=(10, 5))\naxs[0].plot(val_batch_pred[idx, :], label='Autoencoded')\naxs[0].set_title('Autoencoded')\naxs[1].plot(val_batch_X.numpy()[idx, :], label='Original')\naxs[1].set_title('Original')\naxs[1].set_xlabel('Time')\nplt.show()\n\n\nprint('Original')\ndisplay.display(display.Audio(val_batch_X.numpy()[idx, :], rate=16000))\n\nprint('Autoencoded')\ndisplay.display(display.Audio(val_batch_pred[idx, :], rate=16000))\n","68b3d551":"# Plot\nimport matplotlib.image as mpimg  \nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(15,15))\nimg_files = [\n    '\/kaggle\/input\/stft-windows-learnt\/stft_window_1.png', \n    '\/kaggle\/input\/stft-windows-learnt\/stft_window_2.png', \n    '\/kaggle\/input\/stft-windows-learnt\/stft_window_3.png', \n    '\/kaggle\/input\/stft-windows-learnt\/stft_window_4.png'\n]\nimgs = [mpimg.imread(name) for name in img_files]\nfor imgs, ax in zip(imgs, axes.flat):\n    ax.imshow(imgs)\n    ax.axis('off')\n    plt.xticks([], [])\n    plt.yticks([], [])\nfig.show()","b42e7ea8":"n_labels = 8\nmodel = models.Sequential([\n    layers.Input((16000)),\n    STFT(\n        frame_length=255, \n        frame_step=128, \n        n_freqs=130, \n        frequency_trainable=False, \n        window_trainable=False, \n        freq_type='linspace', \n        window_type='hann',     \n        out_module=False\n    ),\n    preprocessing.Resizing(32, 32),\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(n_labels),\n])\n\nmodel.summary()\n\n# model.compile(\n#     optimizer=tf.keras.optimizers.Adam(),\n#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#     metrics=['accuracy'],\n# )\n# EPOCHS = 10\n# history = model.fit(\n#     train_ds, \n#     validation_data=val_ds,   \n#     epochs=EPOCHS,\n#     callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n# )","609cd759":"# TensorFlow STFT and STFT Inverse Layers\n\n\nThis notebook is based on [TensorFlow tutorial](https:\/\/www.tensorflow.org\/tutorials\/audio\/simple_audio#build_and_train_the_model). The tutorial is about dealing with Audio data, with the mini-speech command dataset. \n\nIn the tutorial, the model consists of a CNN with the spectrogram of the audio as an input. Therefore, a preprocessing step is required to transform the audio waveform into a spectrogram via STFT. TensorFlow has the function `tf.signal.stft` for this. \n\n### Intro on STFT ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Short-time_Fourier_transform))\n\n\n###### Continous Time\nCut the time intervals into smaller windows, on each of these windows, you perform a FT on the signal. \n\nYou end up with a series of FT along time. Meaning that the STFT function has two variables, one $\\omega$ which represents a frequency, just like in normal FT and another, called $\\tau$, which gives information about the window. \n\nIn the continuous case, if for singal $x$, we call $\\hat X$ the STFT of $x$:\n\n$$ \\hat X (\\tau, \\omega) = \\int x(t) w(t - \\tau) e^{-i \\omega t} dt $$\n\n##### Discrete time\nIn discrete settings, we will discretize time with variables $m, n$ and frequencies with variables $k$. \n\n$$\\hat X (m, k) = \\sum_n x_n w_{n-m} e^{-i k n}$$\n\n##### Parameters\n\n$w$ can simply be an indicator function. For example, with $s$ would be the \"window size\" of the STF: $ w(t) = \\mathbb{1}_{t < s}$. \\\nHowever, there are many different window functions available ([and a dedicated wiki page](https:\/\/en.wikipedia.org\/wiki\/Window_function)). You basically can change the weights however you want. \\\nIn the TF tutorial, the Hann window is used. \n\nYou can also chose the frequencies to sample on. Usually, a `linspace` of $[0, \\pi]$ is used. \n\n\n##### Spectrogram\n\nThe spectrogram of a time series $x$ is simply a module of the STFT. Hence, it is a function of two parameters and has non-negative values. We usually plot it as an image. \\\nThe TF tutorial uses that image as input of a CNN to perform the classification. \n$$\\text{spectrogram}(x)(m, k) = |X(m, k)|^2$$\n\n\n##### STFT Inverse and Autoencoding\nLike FT, the STFT has an inverse operation. The inverse operation can also be coded within the TensorFlow framework and backpropagated through. \\\nIt allows us to have an easy test case for those two layers: autoencoded audio data into a spectrogram. This is done below","5b3857dd":"### Backproagating and learning: an autoencoder\n\nWe define a model (via Functional API) using the newly created. \\\nNow the STFT Layers weights (window and frequencies) will be initialized randomly. We will learn the best window and frequencies from this starting point to encode the data. ","feb9ab08":"##### Looking at some results","5549f9b2":"### Appendix: TF Tutorial with STFT Layer","dcf61dfa":"### The TF Layers Code\n\nFour classes in the code\n- FTHelper: Namespace for common operations\n- STFT: STFT Layer\n- STFTInverse: STFT Inverse Layer standalone, not used here\n- STFTInverseShared: STFT Inverse Layer linked to a STFT Layer operation. I found that sharing the same window and frequencies when performing autoencoding is important. Apparently the only way to share weigths accross layer is to use this type of coding: [TF GitHub Issue](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37278)","3729053f":"# Conclusions\n\nThe STFT and STFT Inverse functions can be coded with the TensorFlow framework. Hence the gradient backprop can go through these operations. This notebook is a proof that the gradients are stable when going those kind of frameworks. \n\nMore experiments could help us create beter window for some specific tasks. \n\nFinally, these layers can be used inside a TF model. Below, I reproduce the model used in the TF tutorial on Audio speech recognition, including the preprocessing STFT step in the model (non trainable). ","50015598":"##### Learnt Frequencies\nThe frequencies, even initialized at random seem to always evolve through learning into a configuration where they are evenly spaced. They always maximize their entropy, which seems to be the best way for the autoencoder to preserve as much information as necessary if the input is \"random\" enough. \n\nThese observations seem to be universal on the set of hyperparameters used.\n\n##### Learnt Window\nThe most interesting part is about the window. Even if the window function is random at the begining, hence not smooth, irregular, the window always converges toward a good window function. According to wikipedia, a \"good window function\" is:  \n> In typical applications, the window functions used are non-negative, smooth, \"bell-shaped\" curves.[2] Rectangle, triangle, and other functions can also be used. \n\nMost interestingly, the window function the autoencoder converges to depends on the learning: hyperparameters and the random initialization. Despite converging towards different window functions the different instances of the autoencoder seem to have the same loss at the end of the training. That would mean that performing best autoencoding can be achieve with several different window functions. \n\n###### Some window functions found in experiments\n","afca884e":"### Loading the data","49b6f34e":"### Encoding STFT with a TF Layer\n\n\n##### Idea\nSTFT operations are mainly matrix multiplication and summation. It can be implemented inside a NN. Also, nothing prevents us from backpropagating the gradients through the STFT operation. \n\n##### Possible Perks\n- Learn which frequency are the most important in a datasets. Why bother discretizing $[0, \\pi]$ with 1000 points if the results are good enough with 10 points. Important frequencies range depends on the task \/ dataset, eg. \"outlier detection in engine noise\" vs. \"speech-to-text\" \n- Learn a window function. The choice of the function is dependent on the application of the signal processing and usually a human made decision. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Window_function#Choice_of_window_function)\n- Backprop the gradients to signal preprocessing operations. Before the STFT, one could imagine implementing convolutions and applying backprop on those convolution weights. \n- (I didn't look into this yet but it might be possible to:) Accomodate the layer to make use of other signal processing mechanisms, eg. wavelet where you learn the wavelet funciton \n\n##### Drawbacks\nThe main drawback is that a TF STFT layer cannot be implemented using FFT. Therefore, the time of performing the operation can be much larger. If the number of frequencies used can be reduced, that could offset the lose of performance. Also, an STFT layer can be parallelized on the batch, mitigating the increase in complexity. And GPU is necessary to run this notebook in a reasonable time.  ","02adb053":"### Testing the layers\n\nTo test the layer, we take as input a few waveforms, we perform the STFT operation both with the newly created layers and the `tf.signal.stft` function and compare the results. \n\nWe also show that having those two layers are inverse of each other. \n\nIn this setting, we don't train the STFT Layers and we carefully chose the parameters to match the `tf.signal.stft` params. "}}