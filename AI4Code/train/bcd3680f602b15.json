{"cell_type":{"d86571a0":"code","c1120ef0":"code","58f069a1":"code","24fd24a5":"code","b99f3a60":"code","d88eaabf":"code","7dd00243":"code","68485e21":"code","29cb7efc":"code","9d2e1a6b":"code","dcb8fc5e":"code","967fa480":"code","58685289":"code","be4d2098":"code","65ff0641":"code","bf45ab5d":"code","c68a7d57":"code","da1c8684":"code","98c17970":"code","128dee64":"code","77a01692":"code","f27bc2d3":"code","a56ed0d7":"code","abe91d09":"code","08ea6ae1":"code","04df93ef":"code","d30f265b":"code","c23f896f":"markdown","edbac496":"markdown","3c58a4a2":"markdown","a5c07b99":"markdown","df072679":"markdown","831d7e7a":"markdown","e688b84d":"markdown","95e92287":"markdown","3711b50b":"markdown","1e1fe98c":"markdown","7f0f0c1c":"markdown","e90fadba":"markdown","4f8f87f5":"markdown","dbe95891":"markdown","b0cdf3a6":"markdown","3afe9075":"markdown","b52154d1":"markdown","59eb1d13":"markdown","44d8f573":"markdown","dd4ad2f3":"markdown","5e5d8cc9":"markdown","7ccc8d62":"markdown","470e07e6":"markdown","67d0a9e3":"markdown","d2ef0f1c":"markdown"},"source":{"d86571a0":"import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nfrom keras.utils import to_categorical","c1120ef0":"token_path = \"..\/input\/flickr-8k\/Flickr8k_text\/Flickr8k.token.txt\"\ntrain_images_path = '..\/input\/flickr-8k\/Flickr8k_text\/Flickr_8k.trainImages.txt'\ntest_images_path = '..\/input\/flickr-8k\/Flickr8k_text\/Flickr_8k.testImages.txt'\nimages_path = '..\/input\/flickr-8k\/Flickr8k_Dataset\/Flicker8k_Dataset'\n\n\ndoc = open(token_path,'r').read()\nprint(doc[:410])","58f069a1":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","24fd24a5":"# table = str.maketrans('', '', string.punctuation)\n# for key, desc_list in descriptions.items():\n#     for i in range(len(desc_list)):\n#         desc = desc_list[i]\n#         desc = desc.split()\n#         desc = [word.lower() for word in desc]\n#         desc = [w.translate(table) for w in desc]\n#         desc_list[i] =  ' '.join(desc)","b99f3a60":"pic = '1000268201_693b08cb0e.jpg'\nx=plt.imread(images_path+'\/'+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['1000268201_693b08cb0e']","d88eaabf":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\nprint('Original Vocabulary Size: %d' % len(vocabulary))","7dd00243":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)","68485e21":"doc = open(train_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntrain = set(dataset)","29cb7efc":"img = glob.glob(images_path + '\/*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i.split('\/')[5] in train_images:\n        train_img.append(i)\n\ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i.split('\/')[5] in test_images: \n        test_img.append(i)","9d2e1a6b":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)\n","dcb8fc5e":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)","967fa480":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n        \nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint('Vocabulary = %d' % (len(vocab)))","58685289":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1\n","be4d2098":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","65ff0641":"glove_path = '..\/input\/glove6b\/'\n\nembeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","bf45ab5d":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","c68a7d57":"model = InceptionV3(weights='imagenet')\nmodel_new = Model(model.input, model.layers[-2].output)","da1c8684":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","98c17970":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    return fea_vec\n\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img] = encode(img)\ntrain_features = encoding_train\n\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img] = encode(img)\n","128dee64":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","77a01692":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","f27bc2d3":"model.compile(loss='categorical_crossentropy', optimizer='adam')","a56ed0d7":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[images_path + '\/' + key + '.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","abe91d09":"epochs = 15\nbatch_size = 3\nsteps = len(train_descriptions)\/\/batch_size\n\ngenerator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","08ea6ae1":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","04df93ef":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","d30f265b":"pic = list(encoding_test.keys())[7]\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(pic)\nplt.imshow(x)\nplt.show()\n\nprint(\"Greedy:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))","c23f896f":"Now, we load the descriptions of the training images into a dictionary. However, we will add two tokens in every caption, which are \u2018startseq\u2019 and \u2018endseq\u2019:-","edbac496":"# **Data loading and Preprocessing**","3c58a4a2":"Now let\u2019s save the image id\u2019s and their new cleaned captions in the same format as the token.txt file:-","a5c07b99":"**Glove Embeddings**\n\nWord vectors map words to a **vector space, where similar words are clustered together and different words are separated**. The advantage of using Glove over Word2Vec is that GloVe does not just rely on the local context of words but it incorporates global word co-occurrence to obtain word vectors.\n\nThe basic premise behind Glove is **that we can derive semantic relationships between words from the co-occurrence matrix.**","df072679":"# **Model Building and Training**","831d7e7a":"We can train the data in batches.","e688b84d":"Next, we load all training image id\u2019s in a variable train from the \u2018Flickr_8k.trainImages.txt\u2019 file:-","95e92287":"Next, we create a vocabulary of all the unique words present across all the 8000*5 (i.e. 40000) image captions in the data set. We have 9630 unique words across all the 40000 image captions.","3711b50b":"To make our model more robust we will reduce our vocabulary to only those words which occur at least 10 times in the entire corpus.\n\nIf you have a dictionary, **get()** is a method where w is a variable holding the word you're looking up and 0 is the default value. If w is not present in the dictionary, get returns 0.","1e1fe98c":"Now we save all the training and testing images in train_img and test_img lists respectively:-","7f0f0c1c":"Since we are using InceptionV3 we need to pre-process our input before feeding it into the model. Hence we define a preprocess function to reshape the images to (299 x 299) and feed to the preprocess_input() function of Keras.","e90fadba":"We must remember that we do not need to classify the images here, we only need to extract an image vector for our images. Hence we remove the softmax layer from the inceptionV3 model.","4f8f87f5":"We also need to find out what the max length of a caption can be since we cannot have captions of arbitrary length.","dbe95891":"# **Greedy and Beam Search**\n\nAs the model generates a long vector with a probability distribution across all the words in the vocabulary we greedily pick the word with the highest probability to get the next word prediction. This method is called **Greedy Search.**","b0cdf3a6":"Before training the model we need to keep in mind that we do not want to retrain the weights in our embedding layer (pre-trained Glove vectors).","3afe9075":"Now we create two dictionaries to map words to an index and vice versa. Also, we append 1 to our vocabulary since we append 0\u2019s to make all captions of equal length.","b52154d1":"Create a list of all the training captions:-","59eb1d13":"Next, compile the model using Categorical_Crossentropy as the Loss function and Adam as the optimizer.","44d8f573":"**Now let\u2019s define our model.**\nWe are creating a Merge model where we combine the image vector and the partial caption. Therefore our model will have 3 major steps:\n\n1. Processing the sequence from the text \n2. Extracting the feature vector from the image\n3. Decoding the output using softmax by concatenating the above two layers","dd4ad2f3":"# **Model Training**","5e5d8cc9":"# **Introduction**\nImage caption Generator is a popular research area of Artificial Intelligence that deals with image understanding and a language description for that image. Generating well-formed sentences requires both syntactic and semantic understanding of the language. Being able to describe the content of an image using accurately formed sentences is a very challenging task, but it could also have a great impact, by helping visually impaired people better understand the content of images. \n\nThis task is significantly harder in comparison to the image classification or object recognition tasks that have been well researched. \n\nThe biggest challenge is most definitely being able to create a description that must capture not only the objects contained in an image, but also express how these objects relate to each other.","7ccc8d62":"# **Approach to the problem statement**\nWe will tackle this problem using an Encoder-Decoder model. Here our encoder model will combine both the encoded form of the image and the encoded form of the text caption and feed to the decoder.\n\nOur model will treat CNN as the \u2018image model\u2019 and the RNN\/LSTM as the \u2018language model\u2019 to encode the text sequences of varying length. The vectors resulting from both the encodings are then merged and processed by a Dense layer to make a final prediction.\n\nWe will create a merge architecture in order to keep the image out of the RNN\/LSTM and thus be able to train the part of the neural network that handles images and the part that handles language separately, using images and sentences from separate training sets. \n\nIn our merge model, a different representation of the image can be combined with the final RNN state before each prediction.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/10\/Screenshot-2020-10-20-at-7.36.38-PM-768x289.png)\n\nThe above diagram is a visual representation of our approach.\n\nThe merging of image features with text encodings to a later stage in the architecture is advantageous and can generate better quality captions with smaller layers than the traditional inject architecture (CNN as encoder and RNN as a decoder).\n\nTo encode our image features we will make use of transfer learning. There are a lot of models that we can use like VGG-16, InceptionV3, ResNet, etc.\nWe will make use of the inceptionV3 model which has the least number of training parameters in comparison to the others and also outperforms them.\n\nTo encode our text sequence we will map every word to a 200-dimensional vector. For this will use a pre-trained Glove model. This mapping will be done in a separate layer after the input layer called the embedding layer.\n\nTo generate the caption we will be using two popular methods which are Greedy Search and Beam Search. These methods will help us in picking the best words to accurately define the image.\n\nI hope this gives you an idea of how we are approaching this problem statement.\n\n\nLet\u2019s dive into the implementation and creation of an image caption generator!","470e07e6":"**Beam Search** is where we take top k predictions, feed them again in the model and then sort them using the probabilities returned by the model. So, the list will always contain the top k predictions and we take the one with the highest probability and go through it till we encounter \u2018endseq\u2019 or reach the maximum caption length.","67d0a9e3":"# **Evaluation**","d2ef0f1c":"Now we can go ahead and encode our training and testing images"}}