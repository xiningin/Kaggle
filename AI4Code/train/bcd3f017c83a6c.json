{"cell_type":{"86d8c7bb":"code","02fd623e":"code","f692dc57":"code","25e2c3fe":"code","853d0b33":"code","7cf18781":"code","b2f7e9c8":"code","5a662046":"markdown","b9726a35":"markdown","aabc464b":"markdown","89c7d012":"markdown","2783d97e":"markdown"},"source":{"86d8c7bb":"import numpy as np\nfrom random import random\nfrom random import randint\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Flatten, TimeDistributed","02fd623e":"# generate the next frame in the sequence\ndef next_frame(last_step, last_frame, column):\n    # define the scope of the next step\n    lower = max(0, last_step-1)\n    upper = min(last_frame.shape[0]-1, last_step+1)\n    # choose the row index for the next step\n    step = randint(lower, upper)\n    # copy the prior frame\n    frame = last_frame.copy()\n    # add the new step\n    frame[step, column] = 1\n    return frame, step\n\n# generate a sequence of frames of a dot moving across an image\ndef build_frames(size):\n    frames = list()\n    # create the first frame\n    frame = np.zeros((size,size))\n    step = randint(0, size-1)\n    # decide if we are heading left or right\n    right = 1 if random() < 0.5 else 0\n    col = 0 if right else size-1\n    frame[step, col] = 1\n    frames.append(frame)\n    # create all remaining frames\n    for i in range(1, size):\n        col = i if right else size-1-i\n        frame, step = next_frame(step, frame, col)\n        frames.append(frame)\n    return frames, right\n\n# generate sequence of frames\nsize = 5\nframes, right = build_frames(size)\n# plot all frames\nplt.figure()\nfor i in range(size):\n    # create a gray scale subplot for each frame\n    plt.subplot(1, size, i+1) \n    plt.imshow(frames[i], cmap='Greys') \n    # turn of the scale to make it clearer \n    ax = plt.gca() \n    ax.get_xaxis().set_visible(False) \n    ax.get_yaxis().set_visible(False)\n    # show the plot\n    plt.show()","f692dc57":"# generate multiple sequences of frames and reshape for network input\ndef generate_examples(size, n_patterns):\n    X, y = list(), list()\n    for _ in range(n_patterns):\n        frames, right = build_frames(size)\n        X.append(frames)\n        y.append(right)\n  # resize as [samples, timesteps, width, height, channels]\n    X = np.array(X).reshape(n_patterns, size, size, size, 1)\n    y = np.array(y).reshape(n_patterns, 1)\n    return X, y","25e2c3fe":"size = 50\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv2D(2, (2,2), activation='relu'), \n                          input_shape=(None,size,size,1)))\nmodel.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(50))\nmodel.add(Dense(1, activation='sigmoid')) \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \nmodel.summary()","853d0b33":"# fitting model in loop inorder to avoid kernel breakdown due to memory \nfor i in range(10):\n    print(\"Loop : \",i)\n    X, y = generate_examples(size, 500)\n    model.fit(X, y, batch_size=32, epochs=1)","7cf18781":"# evaluate model\nX, y = generate_examples(size, 100)\nloss, acc = model.evaluate(X, y, verbose=0) \nprint('loss: %f, acc: %f' % (loss, acc*100))","b2f7e9c8":"# prediction on new data\nfor i in range(10):\n    print('Example : ',i+1)\n    X, y = generate_examples(size, 1)\n    yhat = model.predict_classes(X, verbose=0)\n    expected = \"Right\" if y[0]==1 else \"Left\"\n    predicted = \"Right\" if yhat[0]==1 else \"Left\" \n    print('Expected: %s, Predicted: %s' % (expected, predicted))","5a662046":"<h3>CNN Model<\/h3>\nThe Conv2D will interpret snapshots of the image (e.g. small squares) and the pooling layers will consolidate or abstract the interpretation.<br>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nWe will define a Conv2D as an input layer with 2 filters and a 2 \u00d7 2 kernel to pass across the input images. The use of 2 filters was found with some experimentation and it is convention to use small kernel sizes. The Conv2D will output 2 49 \u00d7 49 pixel impressions of the input.<br><br>\nConvolutional layers are often immediately followed by a pooling layer. Here we use a MaxPooling2D pooling layer with a pool size of 2 \u00d7 2, which will in effect halve the size of each filter output from the previous layer, in turn outputting two 24 \u00d7 24 maps.<br><br>\nThe pooling layer is followed by a Flatten layer to transform the [24,24,2] 3D output from the MaxPooling2D layer into a one-dimensional 1,152 element vector..<br><br>\n    We want to apply the CNN model to each input image and pass on the output of each input image to the LSTM as a single time step.<br>\nWe can achieve this by wrapping the entire CNN input model (one layer or more) in a TimeDistributed layer.\n    <br><br>\n    Next, we can define the LSTM elements of the model. We will use a single LSTM layer with 50 memory cells, configured after a little trial and error. The use of a TimeDistributed wrapper around the whole CNN model means that the LSTM will see 50 time steps, with each time step presenting a 1,152 element vector as input.\n<\/div>","b9726a35":"<h3>The CNN LSTM<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.<br><br>\nThis architecture has also been used on speech recognition and natural language processing problems where CNNs are used as feature extractors for the LSTMs on audio and textual input data. This architecture is appropriate for problems that:<br><br>\n    <ul>\n        <li>Have spatial structure in their input such as the 2D structure or pixels in an image or the 1D structure of words in a sentence, paragraph, or document.\n        <li>Have a temporal structure in their input such as the order of images in a video or words in text, or require the generation of output with temporal structure such as words in a textual description.\n    <\/ul>        \n<\/div>\n\n![image.png](attachment:image.png)","aabc464b":"<center><h3>Importing Libraries","89c7d012":"<center><h3>Instance Generator","2783d97e":"<center><h2>Moving Square Video Prediction Problem<\/h2><\/center>\n<br>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    The moving square video prediction problem is contrived to demonstrate the CNN LSTM. The problem involves the generation of a sequence of frames. In each image a line is drawn from left to right or right to left. Each frame shows the extension of the line by one pixel. The task is for the model to classify whether the line moved left or right in the sequence of frames. Technically, the problem is a sequence classification problem framed with a many-to-one prediction model.\n    <\/div>\n\n![image.png](attachment:image.png)"}}