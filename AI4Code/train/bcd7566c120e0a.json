{"cell_type":{"ed984b0a":"code","a148fec4":"code","2dd1e3af":"code","f45130e0":"code","63625787":"code","5883a1a2":"code","aa0ad050":"code","2e17e5ea":"code","0f130134":"code","308e13a4":"code","b719b2d7":"code","bcda8a6f":"code","8f491dd5":"code","f848c82d":"code","b50b5a8b":"code","97f816bd":"code","c607bba6":"code","a9c3f7fa":"code","7cb5426a":"code","c7419d35":"code","a8d65948":"code","98bd0e1f":"code","331d3e76":"code","d29a7fbf":"code","710a26bf":"code","e6b6aeed":"code","ed1d6e40":"code","e6498438":"code","99d665c9":"code","340c0a6a":"code","1f74e3ed":"code","595a58ed":"code","2fce5e95":"code","9a38307a":"code","40fe3dbd":"code","b9c1e58b":"code","e82bc73b":"code","144fd78a":"code","e7bb7def":"code","b5567396":"code","8d44f92e":"code","10fc000e":"code","e0fe3062":"code","9026a05f":"markdown","f87b5668":"markdown","36170cfb":"markdown","ee3d1bde":"markdown","4504e2e3":"markdown","26035261":"markdown","73fb1f7e":"markdown","0987c03a":"markdown","6b506cf6":"markdown","ed864311":"markdown","fdd10cac":"markdown","da6bf472":"markdown","dbb59374":"markdown","dae4ab08":"markdown","ec06f228":"markdown","6f62f22e":"markdown","3aee5c4b":"markdown","1f3b89e2":"markdown","49f33b26":"markdown","8c970d42":"markdown","6180f616":"markdown","4accf9ae":"markdown","ee805534":"markdown"},"source":{"ed984b0a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tabulate import tabulate\nfrom datetime import date, datetime, timedelta\n\nfrom sklearn.linear_model import LinearRegression, HuberRegressor\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import QuantileTransformer,OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nsia = SentimentIntensityAnalyzer()\nplt.style.use('ggplot')\npd.set_option('max_columns', 200)","a148fec4":"# !pip install sklego\n\n# from sklego.meta import GroupedPredictor\n# from sklego.preprocessing import ColumnSelector","2dd1e3af":"train_source = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest_source = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","f45130e0":"category_id_map = {\n    1: \"Film & Animation\",\n    2: \"Autos & Vehicles\",\n    10: \"Music\",\n    15: \"Pets & Animals\",\n    17: \"Sports\",\n    19: \"Travel & Events\",\n    20: \"Gaming\",\n    22: \"People & Blogs\",\n    23: \"Comedy\",\n    24: \"Entertainment\",\n    25: \"News & Politics\",\n    26: \"Howto & Style\",\n    27: \"Education\",\n    28: \"Science & Technology\",\n    29: \"Nonprofits & Activism\",\n}\n\ndef addvariables(df):\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    df['category_name'] = df['categoryId'].map(category_id_map)\n    \n    average_duration = df.groupby('categoryId')['duration_seconds'].mean().to_dict()\n    df['cat_average_duration'] = df['categoryId'].map(average_duration)\n    df['duration_seconds'] = df['duration_seconds'].fillna(df['cat_average_duration'])\n    \n    # Feature 1 - Age of video\n    df['daystotrend'] = abs(df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n\n    df['age'] = (pd.to_datetime(date(2021, 12, 31), utc=True)-df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    df['categoryId'] = df['categoryId'].astype('category')\n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    df['day_of_week'] = df['publishedAt'].dt.day_name()\n    \n    # Try to capture the curvelinear distribution \n    df['duration_seconds2'] = (1\/df['duration_seconds'])**2\n    df['daystotrend2'] = df['daystotrend']**2\n    \n    #Transform\n    df['daystotrend_qt'] = QuantileTransformer(n_quantiles=100).fit_transform(df[['daystotrend']])\n    df['duration_seconds_qt'] = QuantileTransformer(n_quantiles=100).fit_transform(df[['duration_seconds']])\n    \n    return df","63625787":"def addsentiment(df):\n    for i, c in df.iterrows():\n        # Title sentiment scores:\n        df.loc[i, 'sent_neg'] = sia.polarity_scores(c['title'])['neg']\n        df.loc[i, 'sent_neu'] = sia.polarity_scores(c['title'])['neu']\n        df.loc[i, 'sent_pos'] = sia.polarity_scores(c['title'])['pos']\n    return df","5883a1a2":"train_df = addvariables(train_source)\ntest_df = addvariables(test_source)","aa0ad050":"train_df = addsentiment(train_df)\ntest_df = addsentiment(test_df)","2e17e5ea":"train_df.info()","0f130134":"shape = train_df.shape\nprint(f'Dataset has {shape[0]} observations. \\n')\ntrain_df.head()","308e13a4":"train_df.describe()","b719b2d7":"train_df.query('daystotrend<1_000_000')['daystotrend'].plot(kind='hist')\nplt.show()","bcda8a6f":"train_df.query('daystotrend>3_000_000')['daystotrend']","8f491dd5":"train_df.loc[6022]","f848c82d":"# 10 hour+ videos\ntrain_df.query('duration_seconds>36000').describe()","b50b5a8b":"# 100mil + view_count videos\ntrain_df.query('view_count>100000000').describe()","97f816bd":"print('Average Video Length', round(train_df['duration_seconds'].mean()\/60,2), 'mins \\n')\nprint('By Category (in mins):')\nround(train_df.groupby('category_name')['duration_seconds'].mean()\/60,2).sort_values(ascending=False)","c607bba6":"print('Average View Count', round(train_df['view_count'].mean(),0))\nprint('By Category:')\nround(train_df.groupby('category_name')['view_count'].mean(),0).sort_values(ascending=False)","a9c3f7fa":"print('Average Like Count', round(train_df['likes'].mean(),0))\nprint('By Category:')\nround(train_df.groupby('category_name')['likes'].mean(),0).sort_values(ascending=False)","7cb5426a":"print('Average Target', round(train_df['target'].mean(),6))\nprint('By Category:')\nround(train_df.groupby('category_name')['target'].mean(),6).sort_values(ascending=False)","c7419d35":"# Decided to keep the outliers and deal with them another way, didn't want to edit code. \nnewtraindf = train_df\n# .query('daystotrend<1_000_000').copy().reset_index(drop=True)\n# .query('duration_seconds<10000').query('view_count<100000000').copy().reset_index(drop=True)\nnewtraindf.describe()","a8d65948":"test_df.describe()","98bd0e1f":"# We have multiple observations for each video\nday_df = train_df.drop_duplicates(subset=['video_id']).copy().reset_index(drop=True)\n\ndays = day_df['day_of_week'].value_counts().sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(10,6))\nbars = ax.bar(days.index,days, color='#0044cc')\nax.bar_label(bars)\nax.set_ylabel('# of Videos')\nax.set_xlabel('Weekdays', fontdict={'fontsize': 18})\nplt.show()","331d3e76":"fig, ax = plt.subplots(figsize=(15,6))\nbars = ax.bar(day_df['category_name'].value_counts().index,day_df['category_name'].value_counts(), color='g')\nax.bar_label(bars)\nax.set_ylabel('# of Videos')\nax.set_xlabel('Categories', fontdict={'fontsize': 18})\nplt.xticks(rotation = 90)\nplt.show()","d29a7fbf":"fig, ax = plt.subplots(figsize=(10,6))\nax.scatter(train_df.query('duration_seconds<10000')['duration_seconds'], train_df.query('duration_seconds<10000')['target'])\nplt.show()","710a26bf":"fig, ax = plt.subplots(figsize=(10,6))\nax.scatter(train_df.query('duration_seconds<10000')['daystotrend'], train_df.query('duration_seconds<10000')['target'])\nplt.show()","e6b6aeed":"fig, ax = plt.subplots(figsize=(10,6))\nax.scatter(train_df.query('duration_seconds<10000')['age'], train_df.query('duration_seconds<10000')['target'])\nplt.show()","ed1d6e40":"# Looks familiar? \n\nx2s = [-10, -9, -8, -6, -5, -3, 0, 1, 2]\nfig, ax = plt.subplots(figsize=(10,6))\nax.scatter(x2s, np.square(x2s), color='g', linewidth=4)\nax.plot(x2s, np.square(x2s), color='g',linewidth=4)\n\nplt.show()","e6498438":"# Variables\nxs = ['age', 'categoryId', \n#       'daystotrend', \n#       'duration_seconds',\n      'channel_occurance','channel_unique_video_count','video_occurance_count',\n      'daystotrend2',\n      'duration_seconds2',\n      'sent_neg',\n      'sent_pos',\n      'sent_neu',\n      'daystotrend_qt',\n      'duration_seconds_qt'\n     ]\ny1 = newtraindf['target']\ny2 = newtraindf['likes']\ny3 = newtraindf['view_count']\n\n# Model\nregm1 = LinearRegression()\nregm1.fit(newtraindf[xs], y1)\ncoefm1 = regm1.coef_\ninterceptm1 = regm1.intercept_\n\n# Variable Signifance\nf_test1 = f_regression(newtraindf[xs], y1)\n\n# Additional Models with Likes and View_Count as dv\nys = [y2, y3]\ny_names = ['likes_p', 'view_count_p']\n\nfor i in range(2):\n    reg = LinearRegression()\n    reg.fit(newtraindf[xs], ys[i])\n    coef = reg.coef_\n    intercept = reg.intercept_\n    train_df[y_names[i]] = reg.predict(train_df[xs])\n\n    # Variable Signifance\n    f_test1 = f_regression(newtraindf[xs], ys[i])\n    for p in range(len(xs)):\n        print(f'{xs[p]}: b: {round(coef[p],6)} F: {round(f_test1[0][p],4)} p: {round(f_test1[1][p],6)}')\n    print(f'Intercept: {round(intercept,6)}')\n    print(f'R2: {round(reg.score(newtraindf[xs], ys[i]),4)}')","99d665c9":"# Results (b, f, p)\nfor i in range(len(xs)):\n    print(f'{xs[i]}: b: {round(coefm1[i],6)} F: {round(f_test1[0][i],4)} p: {round(f_test1[1][i],6)}')\nprint(f'Intercept: {round(interceptm1,6)}')\n# R-square (explanatory power) is very low\nprint(f'R2: {round(regm1.score(newtraindf[xs], y1),4)}')","340c0a6a":"train_df.describe()","1f74e3ed":"newtraindf.describe()","595a58ed":"# Variables\nxs = ['age', 'categoryId', \n#       'daystotrend', \n#       'duration_seconds',\n      'channel_occurance','channel_unique_video_count','video_occurance_count',\n      'daystotrend2',\n      'duration_seconds2',\n      'sent_neg',\n      'sent_pos',\n      'sent_neu',\n      'daystotrend_qt',\n      'duration_seconds_qt'\n     ]\ny1 = newtraindf['target']\ny2 = newtraindf['likes']\ny3 = newtraindf['view_count']\n\n# Model\nregm1c = HuberRegressor()\nregm1c.fit(newtraindf[xs], y1)\ncoefm1c = regm1c.coef_\ninterceptm1c = regm1c.intercept_\n\n# Variable Signifance\nf_test1c = f_regression(newtraindf[xs], y1)\n\n# Additional Models with Likes and View_Count as dv\nys = [y2, y3]\ny_names = ['likes_p', 'view_count_p']\n\nfor i in range(2):\n    reg = HuberRegressor()\n    reg.fit(newtraindf[xs], ys[i])\n    coef = reg.coef_\n    intercept = reg.intercept_\n    train_df[y_names[i]] = reg.predict(train_df[xs])\n\n    # Variable Signifance\n    f_test1 = f_regression(newtraindf[xs], ys[i])\n    for p in range(len(xs)):\n        print(f'{xs[p]}: b: {round(coefm1c[p],6)} F: {round(f_test1[0][p],4)} p: {round(f_test1[1][p],6)}')\n    print(f'Intercept: {round(intercept,6)}')\n    print(f'R2: {round(reg.score(newtraindf[xs], ys[i]),4)}')","2fce5e95":"# Results (b, f, p)\nfor i in range(len(xs)):\n    print(f'{xs[i]}: b: {round(coefm1c[i],6)} F: {round(f_test1c[0][i],4)} p: {round(f_test1c[1][i],6)}')\nprint(f'Intercept: {round(interceptm1c,6)}')\n# R-square (explanatory power) is very low\nprint(f'R2: {round(regm1c.score(newtraindf[xs], y1),4)}')","9a38307a":"# Tried to copy from somewhere else, better write it myself. \n# def plot_model(df, xs, y, model):\n#     model.fit(df[xs], df[y])\n#     metric_df = df.assign(pred=lambda d: model.predict(d[[xs]]))\n#     metric = mean_absolute_error(metric_df[y], metric_df['pred'])\n#     print(metric)\n#     plt.figure(figsize=(12, 4))\n#     for i in df['categoryId'].unique:\n#         pltr = metric_df[[xs, 'pred']].drop_duplicates().loc[lambda d: d['categoryId'] == i]\n#         plt.plot(pltr['time'], pltr['pred'], color='.rbgy'[i])\n#     plt.title(f\"linear model per group, MAE: {np.round(metric, 2)}\");","40fe3dbd":"# Group Estimations from sklearn, I already implemented my version below, added this just to compare. \n# Kaggle is having difficulty with sklego\n\n# xs = ['age', \n#       'categoryId', \n# #       'daystotrend', \n# #       'duration_seconds',\n#       'channel_occurance',\n#       'channel_unique_video_count',\n#       'video_occurance_count',\n#       'daystotrend2',\n#       'duration_seconds2',\n#       'sent_neg',\n#       'sent_pos',\n#       'sent_neu',\n#       'daystotrend_qt',\n#       'duration_seconds_qt'\n#      ]\n# y = 'target'\n# mod = GroupedPredictor(LinearRegression(), groups=[\"categoryId\"])\n\n# # plot_model(newtraindf, xs, y, mod)\n\n# mod.fit(newtraindf[xs], newtraindf[y])\n\n# metric_df = newtraindf\n# metric_df['pred'] = mod.predict(metric_df[xs])\n# metric = mean_absolute_error(metric_df[y], metric_df['pred'])\n# print(metric)\n\n# metric_df.describe()","b9c1e58b":"results = pd.DataFrame()\nfinaltestdf = pd.DataFrame()\ntrain_df_cats = pd.DataFrame()\n\nfor cat in newtraindf['categoryId'].unique():\n    if cat!=29 and cat!=15: \n        df = newtraindf.query('categoryId=={}'.format(cat)).copy().reset_index(drop=True)\n        xs = ['age', 'categoryId', \n        #       'daystotrend', \n        #       'duration_seconds',\n              'channel_occurance','channel_unique_video_count','video_occurance_count',\n              'daystotrend2',\n              'duration_seconds2',\n              'sent_neg',\n              'sent_pos',\n              'sent_neu',\n              'daystotrend_qt',\n              'duration_seconds_qt'\n             ]\n\n        y1 = df['likes']\n        y2 = df['view_count']\n        y3 = df['target']\n        \n        # Model\n        reg1 = LinearRegression()\n        reg1.fit(df[xs], y1)\n        print(f'{cat} Likes R2: {round(reg1.score(df[xs], y1),4)}')\n        \n        reg2 = LinearRegression()\n        reg2.fit(df[xs], y2)\n        print(f'{cat} View R2: {round(reg2.score(df[xs], y2),4)}')\n        \n        reg3 = LinearRegression()\n        reg3.fit(df[xs], y3)\n        print(f'{cat} Target R2: {round(reg3.score(df[xs], y3),4)}')\n        \n        train_df['target_m2a'] = reg3.predict(train_df[xs])\n        \n        testdf = test_df.query('categoryId=={}'.format(cat)).copy().reset_index(drop=True)\n        testdf['likes'] = reg1.predict(testdf[xs])\n        testdf['view_count'] = reg2.predict(testdf[xs])\n        testdf['target_m2a'] = reg3.predict(testdf[xs])\n        \n        finaltestdf = finaltestdf.append(testdf, ignore_index=True)\n        \n        traindfcats = train_df.query('categoryId=={}'.format(cat)).copy().reset_index(drop=True)\n        traindfcats['likes_p2'] = reg1.predict(traindfcats[xs])\n        traindfcats['view_count_p2'] = reg2.predict(traindfcats[xs])\n        traindfcats['target_m2a'] = reg3.predict(traindfcats[xs])\n        \n        train_df_cats = train_df_cats.append(traindfcats, ignore_index=True)       ","e82bc73b":"train_df_cats.describe()","144fd78a":"finaltestdf['target_m2b'] = finaltestdf['likes'] \/ finaltestdf['view_count']","e7bb7def":"train_df['target_m1a'] = regm1.predict(train_df[xs])\ntrain_df['target_m1c'] = regm1c.predict(train_df[xs])\nfinaltestdf['target_m1a'] = regm1.predict(finaltestdf[xs])\nfinaltestdf['target_m1c'] = regm1c.predict(finaltestdf[xs])\n\ntrain_df['target_m1b'] = train_df['likes_p'] \/ train_df['view_count_p']\ntrain_df_cats['target_m2b'] = train_df_cats['likes_p2'] \/ train_df_cats['view_count_p2']\n\nprint(f\"Model 1a mae: {mean_absolute_error(train_df['target'],train_df['target_m1a'])}\")\nprint(f\"Model 1b mae: {mean_absolute_error(train_df['target'],train_df['target_m1b'])}\")\nprint(f\"Model 1c mae: {mean_absolute_error(train_df['target'],train_df['target_m1c'])}\")\nprint(f\"Model 2a mae: {mean_absolute_error(train_df_cats['target'],train_df_cats['target_m2a'])}\")\nprint(f\"Model 2b mae: {mean_absolute_error(train_df_cats['target'],train_df_cats['target_m2b'])}\")","b5567396":"finaltestdf[['target_m1a','target_m2a']].describe()","8d44f92e":"train_df[['target','target_m1a','target_m1c','target_m2a']].describe()","10fc000e":"finaltestdf = finaltestdf.rename(columns={'target_m1c':'target'})","e0fe3062":"finaltestdf[['id', 'target']].to_csv('submission.csv', index=False)","9026a05f":"- Model 2a public score: 0.02617\n- Model 1a public score: 0.02928","f87b5668":"# Load the dataset","36170cfb":"# Descriptive Statistics","ee3d1bde":"## Average Like Counts","4504e2e3":"- Competition metric is MAE, if we can match the m, sd we should do well in the competition. \n- Our models do well matching the m, however std is still not close enough. ","26035261":"## Results of Model 1:\n- R2 is very low. \n- Possibly categories impacting the results. ","73fb1f7e":"## Average Target","0987c03a":"## Average View Counts","6b506cf6":"# Data Structure","ed864311":"- Most common day to publish a video is on Friday","fdd10cac":"## We have some outliers to deal with. ","da6bf472":"- Most popular category is Entertainment, least popular is Nonprofits & Activism.","dbb59374":"- MAE from GroupedPredictor:\n    0.023735958913543107\n\n- MAE from my group predictor:\n    0.02375604303693317\n","dae4ab08":"# Visualizations","ec06f228":"# Problems:\n- Target is Likes to View ratio. \n- Outliers will cause issues. \n- Can remove outliers from the model.\n- Or replace their score with the closest number. \n","6f62f22e":"# Regression Model 2","3aee5c4b":"## Need to get rid of some of the outliers. Linear models don't like them. ","1f3b89e2":"## Final Evaluation of models","49f33b26":"## Here we set a new DF, removing some of the outliers","8c970d42":"## Average Video Length","6180f616":"# Regression Model 1","4accf9ae":"# Create New variables","ee805534":"# Regression Model 1C\n- Huber Regressor"}}