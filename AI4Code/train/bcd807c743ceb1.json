{"cell_type":{"7611456b":"code","451f6478":"code","1f48f416":"code","1a4314b6":"code","06dab544":"code","6c59cf5c":"markdown","892a135c":"markdown","3387ab74":"markdown","fc7dae7d":"markdown","0acad8cb":"markdown","89ae35b7":"markdown","45b26503":"markdown","eabf27cf":"markdown","1380f2cd":"markdown","202386b8":"markdown","ead77ea7":"markdown"},"source":{"7611456b":"import matplotlib.pyplot as plt\nimport seaborn; seaborn.set()\nfrom numpy import exp, array, append\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Sigmoid\n\ndef Sigmoid(x):\n    return 1 \/ (1 + exp(-x))\n\nfor i in array([range(-10, 10)]):\n    x = append(array([]), i)\n    plt.plot(x, Sigmoid(x))\n    plt.title(\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Sigmoid\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Sigmoid(x)\")\n","451f6478":"# Tan-h \ndef Tanh(x):\n    return 2 \/ (1 + exp(-2*x)) - 1\nfor i in array([range(-10, 10)]):\n    x = append(array([]), i)\n    plt.plot(x, Tanh(x))\n    plt.title(\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Tan-h\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Tanh(x)\")\n    ","1f48f416":"from numpy import sum\n# Softmax\ndef Softmax(x):\n    return exp(x) \/ sum(exp(x))\nx = array([1, 2, 3, 4, 5])\nplt.plot(x, Softmax(x))\nplt.title(\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Softmax\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Softmax(x)\")","1a4314b6":"# ReLU\ndef ReLU(x):\n    return x * (x > 0)\nx = array(range(-10, 10))\nplt.plot(x, ReLU(x))\nplt.title(\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU\")\nplt.xlabel(\"x\")\nplt.ylabel(\"ReLU(x)\")","06dab544":"from numpy import where\ndef LeakyReLU(x):\n    return where(x > 0, x, x * 0.1)\nx = array(range(-10, 10))\nplt.plot(x, LeakyReLU(x))\nplt.title(\"Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Leaky ReLU\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Leaky ReLU(x)\")","6c59cf5c":"## Qual fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a \ud83c\udfc6 \"melhor\"?\n\n\ud83e\udd71 Resposta: N\u00e3o existe a \"melhor\" fun\u00e7\u00e3o de ativa\u00e7\u00e3o.\n\nO uso de uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o depende da tarefa com a qual voc\u00ea est\u00e1 lidando. \n<br \/>\n**A tarefa \u00e9 um problema de regress\u00e3o ou classifica\u00e7\u00e3o?** \n<br \/>\nSe \u00e9 um problema de classifica\u00e7\u00e3o, ent\u00e3o \u00e9 uma classifica\u00e7\u00e3o bin\u00e1ria ou uma tarefa de classifica\u00e7\u00e3o em v\u00e1rias classes? \n> tudo se resume \u00e0 tarefa com a qual voc\u00ea est\u00e1 lidando.\n","892a135c":"## Sigmoid\nUma fun\u00e7\u00e3o sigm\u00f3ide \u00e9 uma fun\u00e7\u00e3o matem\u00e1tica que possui uma curva em forma de \"S\" ou curva sigm\u00f3ide. <br \/>Geralmente usado em casos que geram um conjunto de sa\u00eddas de probabilidades entre 0 e 1. \n<br \/>\n<br \/>\n$$Sigmoid(x) = \\frac{1}{1 + e^{(-x)}} = \\frac{e^x}{1 + e^x}$$\n<center>\n<i>A fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigm\u00f3ide \u00e9 bastante usada em tarefas de classifica\u00e7\u00e3o bin\u00e1ria.<\/i>\n<\/center>    \n<hr \/>\n","3387ab74":"## ReLU\n\nUma ReLU (Unidade Linear Retificada) possui sa\u00edda 0 se a entrada for menor que 0 e sa\u00edda n\u00e3o \u00e9 processada. Ou seja, se a entrada for maior que 0, a sa\u00edda ser\u00e1 igual \u00e0 entrada.\n<br \/>\n$$ReLU(x) = max(0,x)$$\n<hr \/>","fc7dae7d":"# Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o\n\n\ud83d\udd18 Em redes neurais artificiais, a fun\u00e7\u00e3o de ativa\u00e7\u00e3o de um n\u00f3 define a sa\u00edda desse n\u00f3, dada uma entrada ou conjunto de entradas.\n<br \/>\n<img src='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSdzk5y2W_GQToFAcIT9HnnO-4KQyDjKh4zl4ux63YdY3wU9pU&s' \/>","0acad8cb":"*continua...\n*\n* 1. Cross entropy error\n* Back-propagation\n\n","89ae35b7":"## Softmax\n\nUma fun\u00e7\u00e3o softmax \u00e9 usada para a classifica\u00e7\u00e3o de v\u00e1rias classes. Ela calcula a distribui\u00e7\u00e3o de probabilidades do evento em diferentes eventos. As probabilidades calculadas s\u00e3o \u00fateis para determinar a classe de destino para as entradas fornecidas.\n<br \/>\n$$Softmax(x) = \\frac{e^j}{\\sum_i{e^i}}$$\n<hr \/>\n","45b26503":"## Por que usamos fun\u00e7\u00f5es de ativa\u00e7\u00e3o?\nEnt\u00e3o.. \u2615\ud83d\udc4c\ud83d\ude0c, sem uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o, deixaremos de introduzir a **n\u00e3o linearidade** na rede.\n<br \/>\nUma fun\u00e7\u00e3o de ativa\u00e7\u00e3o nos permitir\u00e1 modelar uma vari\u00e1vel de resposta (vari\u00e1vel de destino, r\u00f3tulo de classe ou pontua\u00e7\u00e3o) que varia de maneira n\u00e3o linear com suas vari\u00e1veis explicativas.\n\n> \"n\u00e3o linear significa que a sa\u00edda n\u00e3o pode ser reproduzida a partir de uma combina\u00e7\u00e3o linear.\"\n\nSem uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o n\u00e3o linear, uma rede neural artificial,se comportar\u00e1 como um Perceptron de camada \u00fanica, porque a soma dessas camadas daria a voc\u00ea apenas outra Fun\u00e7\u00e3o linear.\n[](http:\/\/)","eabf27cf":"# Redes Neurais Artificiais\n\ud83e\udde0 Redes neurais artificiais (RNA), s\u00e3o sistemas de computa\u00e7\u00e3o inspirados nas redes neurais biol\u00f3gicas.\n\n*RNA \u00e9 um conjunto de neur\u00f4nios conectados organizados em camadas.*\n\n<strong style='font-size:15px'>Tipos de RNAs<\/strong>\n<hr \/>\n<strong>Perceptron<\/strong>\n<br \/>\n<i>\u00c9 o modelo mais simples e antigo de uma RNA, o perceptron \u00e9 um classificador linear usado para previs\u00f5es bin\u00e1rias.\n<\/i> \n<img src='https:\/\/www.allaboutcircuits.com\/uploads\/articles\/how-to-train-a-basic-perceptron-neural-network_rk_aac_image1.jpg'  style='height:200px'\/>\n<br \/>\n<strong>Multi-layer<\/strong>\n<br \/>\n<i>\u00c9 mais sofisticada do que perceptron, uma RNA com v\u00e1rias camadas  (por exemplo: Rede Neural Convolucional, Rede Neural Recorrente, etc ...) \u00e9 mais eficiente na solu\u00e7\u00e3o de tarefas de classifica\u00e7\u00e3o e regress\u00e3o.\n<\/i> \n<img src='https:\/\/www.researchgate.net\/profile\/Anderson_Castro_Soares_De_Oliveira\/publication\/240772105\/figure\/fig2\/AS:667857415319554@1536241024122\/Figura-1-Rede-Neural-Artificial-Multicamadas.png'  style='height:300px'\/>\n<br \/>\n\n\n","1380f2cd":"## Leaky ReLU\nA Leaky ReLU funciona da mesma forma que a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU, exceto que, em vez de substituir os valores negativos das entradas por 0, os \u00faltimos s\u00e3o multiplicados por um valor alfa, na tentativa de evitar o problema *\"morte de ReLU\"*.\n\n<br \/>\n<br \/>\n\n\\begin{equation}\n  LeakyReLU(x)=\\begin{cases}\n    x, & \\text{if x > 0}.\\\\\n    \\alpha x, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n<hr \/>","202386b8":"## Tan-h\nComo a sigmoide, a fun\u00e7\u00e3o tan-h (tangente hiperb\u00f3lica), tamb\u00e9m \u00e9 *sigmoidal* (em forma de \"S\"), mas em vez disso gera valores que variam de -1 e 1\n<br \/>\n<br \/>\n$$tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$$\n<hr \/>","ead77ea7":"**Refer\u00eancias:**\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Perceptron e Adaline](https:\/\/www.youtube.com\/watch?v=6yYUc6nU3Cw)\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Parametriza\u00e7\u00e3o de redes neurais profundas](https:\/\/www.youtube.com\/watch?v=qDmKwmkc4vs&t=1s)\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Algoritmo da retropropaga\u00e7\u00e3o de erros (Backpropagation) para redes multi-layer perceptron](https:\/\/www.youtube.com\/watch?v=DGNbd2FGw2s)\n* https:\/\/medium.com\/@franckepeixoto\/mlp-multilayer-perceptron-conceito-de-b%C3%A1sico-93afa91dd03e\n* https:\/\/medium.com\/@franckepeixoto\/machine-learning-indaga%C3%A7%C3%B5es-f0fce290451c\n* https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network\n* https:\/\/en.wikipedia.org\/wiki\/Activation_function\n* https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function\n* https:\/\/theclevermachine.wordpress.com\/tag\/tanh-function\/\n* http:\/\/dataaspirant.com\/2017\/03\/07\/difference-between-softmax-function-and-sigmoid-function\/\n* https:\/\/github.com\/Kulbear\/deep-learning-nano-foundation\/wiki\/ReLU-and-Softmax-Activation-Functions\n* http:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html\n* https:\/\/en.wikipedia.org\/wiki\/Backpropagation"}}