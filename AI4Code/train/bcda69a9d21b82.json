{"cell_type":{"4a57270c":"code","a5f1a7fd":"code","3c6f9339":"code","0bc93a98":"code","a68e54e0":"code","27ca9dae":"code","27aac0e8":"code","ae83e0da":"code","9d78f49c":"code","40931278":"code","40f225ca":"code","05d93fd3":"code","7db4dfa3":"code","fae3df06":"code","76387ce5":"code","83e0a5a1":"code","9d2b330c":"code","8d508089":"code","fa4204b2":"code","462fc645":"code","6733737f":"code","43181ea1":"code","e1862d9c":"code","7d33d7d1":"code","eb146145":"code","890d254a":"code","8eaaead1":"code","3649e308":"code","56e8fa15":"code","b9720544":"code","5715f75a":"code","3c751804":"code","8411960e":"markdown","e469127a":"markdown","ccdb2a5c":"markdown","1bf19a7b":"markdown","bfee3bb5":"markdown"},"source":{"4a57270c":"!pip3 install jupyterthemes","a5f1a7fd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom colorama import Fore, Style\nimport os\nimport sys\nfrom jupyterthemes import jtplot\njtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True)","3c6f9339":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndf.head(5)","0bc93a98":"print(Fore.YELLOW, \"Loading data information ...\", Style.RESET_ALL)\ndf.info()","a68e54e0":"# Check class distribution\nsns.countplot(x=\"class\", data=df)","27ca9dae":"for cols in df.columns:\n    unique_values = df[cols].unique()\n    print(Fore.YELLOW, f\"Number of unique values in '{cols}':\", Style.RESET_ALL, len(unique_values))","27aac0e8":"X, Y = df.drop(\"class\", axis=1), df[\"class\"]","ae83e0da":"import torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nlabel_encoder = LabelEncoder()\nfor i in X.columns:\n    X[i] = label_encoder.fit_transform(X[i])\n    \nlabel_encoder = LabelEncoder()\nY = label_encoder.fit_transform(Y)\n","9d78f49c":"X.head()","40931278":"Y","40f225ca":"X = pd.get_dummies(X, columns=X.columns, drop_first=True)\nX.head(5)","05d93fd3":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)","7db4dfa3":"from sklearn.preprocessing import StandardScaler\n\nstsc = StandardScaler()\nXtrain = stsc.fit_transform(Xtrain)\nXtest = stsc.transform(Xtest)","fae3df06":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nXtrain = pca.fit_transform(Xtrain)\nXtest = pca.transform(Xtest)","76387ce5":"D = X.shape[1]\n\nYtrain = Ytrain.reshape(-1, 1)\nYtest = Ytest.reshape(-1, 1)\n\nprint(Fore.YELLOW, \"Shapes for Training Data....\", Style.RESET_ALL)\nprint(f\"Shape of Xtrain : {Xtrain.shape}\")\nprint(f\"Shape of Ytrain : {Ytrain.shape}\")\n\n\nprint(Fore.BLUE, \"Shapes for Testing Data....\", Style.RESET_ALL)\nprint(f\"Shape of Xtest : {Xtest.shape}\")\nprint(f\"Shape of Ytest : {Ytest.shape}\")","83e0a5a1":"print(Fore.YELLOW, \"Creating PyTorch Datasets for computation\")\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.from_numpy(Xtrain.astype(np.float32)), torch.from_numpy(Ytrain.astype(np.float32)))\ntest_dataset = torch.utils.data.TensorDataset(torch.from_numpy(Xtest.astype(np.float32)), torch.from_numpy(Ytest.astype(np.float32)))","9d2b330c":"batch_size = 128\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","8d508089":"class Logistic(nn.Module):\n    \n    def __init__(self, n_units, n_classes):\n        \n        super(Logistic, self).__init__()\n        \n        self.seq = nn.Sequential(nn.Linear(n_units, n_classes), nn.Sigmoid())\n        \n    def forward(self, X):\n        X = self.seq(X)\n        \n        return X","fa4204b2":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","462fc645":"logregmodel = Logistic(Xtrain.shape[1], 1)\nlogregmodel.to(device)","6733737f":"# Define the loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(logregmodel.parameters())","43181ea1":"# Define the training loop \ndef batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs=20):\n    \n    \"\"\"\n    ----------------------------------------------------\n    Description : Function to do batch gradient descent \n                  on the input dataset\n                  \n    Arguments :\n    \n    model -- a pytorch model \n    criterion -- a pytorch module which contains the loss\n    optimizer -- a pytorcch module which contains the optimizers used for batch gradient descent\n    train_loader -- a pytorch dataloader representing the training set\n    test_loader -- a pytorch dataloader representing the testing set\n    epochs -- an integer representing the number of training loops to go through\n    \n    Return :\n    \n    train_losses -- a numpy array containing the loss values encountered during training\n    test_losses -- a numpy array containing the loss values encountered during validation\n    \n    Usage :\n    \n    train_loss, test_val = batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs=10000)\n    \n    -------------------------------------------------------    \n    \n    \"\"\"\n    \n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    \n    for epoch in range(epochs):\n        \n        train_loss = []\n        \n        for inputs, targets in train_loader:\n            \n            # Move the inputs and targets to the device\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero-initiialize the optimizer gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            \n            \n            train_loss.append(loss.item())\n            \n        \n        test_loss = []\n        \n        for inputs, targets in test_loader:\n            \n            # Move the inputs and targets to the device\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            test_loss.append(loss.item())\n            \n        \n        train_loss = np.mean(train_loss)\n        test_loss = np.mean(test_loss)\n        \n        train_losses[epoch] = train_loss\n        test_losses[epoch] = test_loss\n        \n        print(f\"Epoch : {epoch+1}\/{epochs} | Train Loss : {train_loss} | Test Loss : {test_loss}\")\n            \n        \n    return train_losses, test_losses\n            \n            ","e1862d9c":"train_losses, test_losses = batch_gd(logregmodel, criterion, optimizer, train_loader, test_loader, epochs=200)","7d33d7d1":"plt.title(\"Epochs vs Losses\")\nplt.plot(train_losses, label=\"Train losses\")\nplt.plot(test_losses, label=\"Test losses\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Losses\")\nplt.legend()\nplt.show()","eb146145":"# Get the model accuracy\n\ndef get_accuracy(evalmodel, train_loader, test_loader):\n    \n    \"\"\"\n    -----------------------------------------------\n    Description : To calculate the accuracy rate of the model\n    \n    Arguments :\n    \n    model : a pytorch model \n    train_loader : a pytorch data loader representing the training set\n    test_loader : a pytorch data loader representing the testing set\n    \n    Return:\n    \n    train_acc : a float value representing the training accuracy of the model\n    test_acc : a float value representing the testing accuracy of the model\n    \n    \n    Usage :\n    \n    trainAcc, testAcc = get_accuracy(model, train_loader, test_loader)\n    --------------------------------------------------\n    \n    \"\"\"\n    \n    \n    n_correct = 0\n    n_total = 0\n    \n    for inputs, targets in train_loader:\n        \n        # move targets to the device\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Forward pass\n        outputs = evalmodel(inputs).detach().numpy()\n        \n        n_correct += np.mean(targets.detach().numpy() == np.round(outputs))\n        \n        n_total += 1\n        \n    \n    train_acc = n_correct \/ n_total\n    \n    \n    for inputs, targets in test_loader:\n        \n        # move targets to the device\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Forward pass\n        outputs = evalmodel(inputs).detach().numpy()\n        \n        n_correct += np.mean(targets.detach().numpy() == np.round(outputs))\n        n_total += 1\n        \n    \n    test_acc = n_correct \/ n_total\n    \n    \n    return train_acc, test_acc\n        \n        ","890d254a":"train_acc , test_acc = get_accuracy(logregmodel, train_loader, test_loader)\n\nprint(f\"Training Accuracy : {train_acc} || Testing Accuracy : {test_acc}\")","8eaaead1":"class ANN(nn.Module):\n    \n    def __init__(self, n_features, n_classes):\n        \n        super(ANN, self).__init__()\n        \n        self.dense = nn.Sequential(\n                nn.Linear(n_features, 20),\n                nn.ReLU(),\n                nn.Linear(20, 10),\n                nn.ReLU(),\n                nn.Linear(10, n_classes),\n                nn.Sigmoid()\n        )\n        \n    def forward(self, X):\n        \n        X = self.dense(X)\n        \n        return X","3649e308":"annmodel = ANN(Xtrain.shape[1], 1)\nannmodel.to(device)","56e8fa15":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(annmodel.parameters())","b9720544":"train_losses, test_losses = batch_gd(annmodel, criterion, optimizer, train_loader, test_loader, epochs=100)","5715f75a":"plt.title(\"Epochs vs Losses\")\nplt.plot(train_losses, label=\"Training loss\")\nplt.plot(test_losses, label=\"Test loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","3c751804":"train_acc, test_acc = get_accuracy(annmodel, train_loader, test_loader)\n\nprint(f\"Training Acc : {train_acc} | Test Acc : {test_acc}\")","8411960e":"# Artificial Neural Network","e469127a":"## Poisonous = 1 \n## Edible = 0","ccdb2a5c":"# Logistic Model","1bf19a7b":"# Data Processing","bfee3bb5":"The Logistic Model seems to be a pretty good model. But let's see what an ANN can do in comparison to the logistic model."}}