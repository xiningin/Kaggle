{"cell_type":{"ccff1132":"code","07bd94f8":"code","0785adea":"code","843e8496":"code","817011ec":"code","a6ffda06":"code","7f2ad500":"code","92fa7268":"code","6fdf58ba":"code","5471bb8e":"code","ec62c7e2":"code","050b737b":"code","bad10d0b":"code","48c73620":"code","1e011633":"code","fbced023":"code","5a8af193":"code","88ccc5f8":"code","1c6144c1":"code","b3652462":"code","9cd57eef":"code","bec1d836":"code","46ef18b6":"code","e7387b5c":"code","fb3cb257":"code","0d14ff0d":"code","862f70db":"code","ec577a34":"code","7d599747":"code","0c4a64ab":"code","925a58d1":"code","5c9e772f":"code","1b94fa2e":"code","d766fbd7":"code","edf8abd9":"code","59234715":"code","9efa1195":"code","98b3bc6b":"code","2e38d484":"code","ee2561f6":"code","eaf973a6":"code","856e96f8":"code","bee98147":"markdown","834ba1cd":"markdown","92d1dce8":"markdown","8791e189":"markdown","49e43f04":"markdown","040a2d9a":"markdown","5fd87f95":"markdown","9c1c29af":"markdown","38ebcae9":"markdown","bf6060c4":"markdown","cced58a7":"markdown","5fe50e08":"markdown","1c36e50a":"markdown"},"source":{"ccff1132":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet, BayesianRidge,RANSACRegressor,HuberRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline","07bd94f8":"data = pd.read_csv('..\/input\/kc-house-data\/kc_house_data.csv')\ndata.head()","0785adea":"#see the datatype of each column\ndata.info()","843e8496":"#fill all the values with 0\ndata.fillna(0, inplace=True)","817011ec":"#iterate through the columns to see the frequency of different values\nfor i in data.columns:\n    print(data[i].value_counts())","a6ffda06":"#format the date\nd =[]\nfor i in data['date'].values:\n    d.append(i[:4])\n    \ndata['date'] = d\n\n# convert everything to same datatype\nfor i in data.columns:\n    data[i]=data[i].astype(float)\n    \n#make a new column age of the house  \ndata['age'] = data['date'] - data['yr_built']\n\n#calculate the total years of renovation\ndata['renov_age'] = np.abs(data['yr_renovated'] - data['yr_built'])\ndata['renov_age'] = data.renov_age.apply(lambda x: x if len(str(int(x)))==2 else 0.0)\n\n#remove unwanted columns like yr_built, date, id\ndata.drop(['id','date', 'yr_built', 'yr_renovated'], axis=1, inplace=True)\ndata.head()","7f2ad500":"#print highly correlated variables\ncorr_features =[]\n\nfor i , r in data.corr().iterrows():\n    k=0\n    for j in range(len(r)):\n        if i!= r.index[k]:\n            if r.values[k] >=0.5:\n                corr_features.append([i, r.index[k], r.values[k]])\n        k += 1\ncorr_features","92fa7268":"#plot the frequency of the columns\ndata.hist(figsize=(30,20))\nplt.show()","6fdf58ba":"#let us remove highly correlated features that is above 0.8\nfeat =[]\nfor i in corr_features:\n    if i[2] >= 0.8:\n        feat.append(i[0])\n        feat.append(i[1])\n        \ndata.drop(list(set(feat)), axis=1, inplace=True)\ndata.head()\n\n#plot the (dependent) variable to see its distribution\n# plt.title()\nplt.figure(figsize=(20,10))\nsns.distplot(data.price, kde=False).set_title('Distribution of Price variable')\nplt.show()","5471bb8e":"# creating boxplots to see the outliers in the price variable \n\nplt.figure(figsize=(6,4))\nsns.boxplot(y=data['price']).set_title\nplt.show()","ec62c7e2":"#let us numerically draw conclusions\n#creating function that can calculate interquartile range of the data\ndef calc_interquartile(data, column):\n    global lower, upper\n    #calculating the first and third quartile\n    first_quartile, third_quartile = np.percentile(data[column], 25), np.percentile(data[column], 75)\n    #calculate the interquartilerange\n    iqr = third_quartile - first_quartile\n    # outlier cutoff (1.5 is a generally taken as a threshold thats why i am also taking it)\n    cutoff = iqr*1.5\n    #calculate the lower and upper limits\n    lower, upper = first_quartile - cutoff , third_quartile + cutoff\n    #remove the outliers from the columns\n    upper_outliers = data[data[column] > upper]\n    lower_outliers = data[data[column] < lower]\n    print('Lower outliers', lower_outliers.shape[0])\n    print('Upper outliers', upper_outliers.shape[0])\n    return print('total outliers', upper_outliers.shape[0] + lower_outliers.shape[0])","050b737b":"#applying the above function on columns to find the total outliers in every feature\nfor i in data.columns:\n    print('Total outliers in ', i)\n    calc_interquartile(data, i)\n    print()","bad10d0b":"#plotting outliers graph for 'price' feature \ncalc_interquartile(data, 'price')\nplt.figure(figsize = (10,6))\nsns.distplot(data['price'], kde=False)\nprint(upper, lower)\nplt.axvspan(xmin = lower,xmax= data['price'].min(),alpha=0.2, color='blue', label='Lower Outliers')\nplt.axvspan(xmin = upper,xmax= data['price'].max(),alpha=0.2, color='red', label='Upper Outliers')\nplt.legend()\nplt.show()","48c73620":"\"\"\" creating function for calculating zscore which is subtracting the mean from every data point and dividing by the standard deviation and if the zscore value of any data point \nis less than -3 or greater than 3, then that data point is an outlier\"\"\"\n\ndef z_score(data, column):\n    #creating global variables for plotting the graph for better demonstration\n    global zscore, outlier\n    #creating lists to store zscore and outliers \n    zscore = []\n    outlier =[]\n    # for zscore generally taken thresholds are 2.5, 3 or 3.5 hence i took 3\n    threshold = 3\n    # calculating the mean of the passed column\n    mean = np.mean(data[column])\n    # calculating the standard deviation of the passed column\n    std = np.std(data[column])\n    for i in data[column]:\n        z = (i-mean)\/std\n        zscore.append(z)\n        #if the zscore is greater than threshold = 3 that means it is an outlier\n        if np.abs(z) > threshold:\n            outlier.append(i)\n    return print('total outliers', len(outlier))","1e011633":"#plotting outliers graph for 'price' feature \nz_score(data, 'price')\nplt.figure(figsize = (10,6))\nsns.distplot(zscore, kde=False)\nprint(upper, lower)\nplt.axvspan(xmin = -3 ,xmax= min(zscore),alpha=0.2, color='blue', label='Lower Outliers')\nplt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.2, color='red', label='Upper Outliers')\nplt.legend()\nplt.show()","fbced023":"#remove the outliers from price using zscore\ndj=[]\nfor i in data.price:\n    if i in set(outlier):\n        dj.append(0.0)\n    else:\n        dj.append(i)\n        \ndata['P'] = dj\n\nx = data.drop(data[data['P'] == 0.0].index) \nx.shape","5a8af193":"plt.figure(figsize = (10,6))\nsns.distplot(x['price'], kde=False)\nplt.show()","88ccc5f8":"#defining the independent and dependent variable\nX = x.drop(['price','P'], axis=1)\nY = x['price']","1c6144c1":"#removing the outliers using interquartile method\nupper_outliers = data[data['price'] > upper]\nupper_outliers","b3652462":"x_ = data.drop(data[data['price']>upper].index)\nx_","9cd57eef":"y_ = x_['price']\nx_ = x_.drop(['price','P'], axis=1)\ny_","bec1d836":"#isolation forest\nimport warnings\nwarnings.filterwarnings(action='ignore')\nfrom sklearn.ensemble import IsolationForest\niso = IsolationForest(contamination=0.1)\noutlier = iso.fit_predict(data)","46ef18b6":"#mask variable contains all the outliers\nmask = outlier == -1\n#task variable contains all the non-outliers data\ntask = outlier != -1\n#creating dataframe containing outliers\ndf_1 = data[mask]\n#creating dataframe containing non-outliers\ndf_2 = data[task]","e7387b5c":"#plotting graph to show the original data, outliers and non-outliers \nplt.figure(figsize=(18, 8))\nplt.title('Plots of original data vs outliers from IsolationForest')\nplt.hist(data['price'], label= 'original', color='darkorange')\nplt.hist(df_2['price'], label='without outlier')\nplt.hist(df_1['price'], label='outliers', color='red')\nplt.legend()\nplt.show()","fb3cb257":"y2 = df_2['price']\ndf_2.drop(['price','P'], axis=1, inplace=True)","0d14ff0d":"#z_score one \nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\nlr = LinearRegression()\nlr.fit(x_train, y_train)\npred = lr.predict(x_test)\nr2_score(y_test, pred)","862f70db":"#interquartile one \nx_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size = 0.3, random_state=42)\nlr = LinearRegression()\nlr.fit(x_train, y_train)\npred = lr.predict(x_test)\nr2_score(y_test, pred)","ec577a34":"#isolationforest \nx_train, x_test, y_train, y_test = train_test_split(df_2, y2, test_size = 0.3, random_state=42)\nlr = LinearRegression()\nlr.fit(x_train, y_train)\npred = lr.predict(x_test)\nr2_score(y_test, pred)","7d599747":"#Hyperparameter tuning\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=42)\nkfold = KFold(n_splits=5)\nresults = cross_val_score(lr, x_train, y_train, cv=kfold)\nresults.mean()","0c4a64ab":"sc = ('Scaler', StandardScaler())\nest =[]\nest.append(('LinearRegression', Pipeline([sc, ('LinearRegression', LinearRegression())])))\nest.append(('Ridge', Pipeline([sc, ('Ridge', Ridge())])))\nest.append(('Lasso', Pipeline([sc, ('Lasso', Lasso())])))\nest.append(('BayesianRidge', Pipeline([sc, ('BayesianRidge', BayesianRidge())])))\nest.append(('ElasticNet', Pipeline([sc,('Elastic', ElasticNet())])))\nest.append(('SGD', Pipeline([sc,('SGD', SGDRegressor())])))\nest.append(('Huber', Pipeline([sc,('Huber', HuberRegressor())])))\nest.append(('RANSAC', Pipeline([sc,('RANSAC', RANSACRegressor())])))\nest.append(('GradientBoosting', Pipeline([sc,('GradientBoosting',GradientBoostingRegressor())])))\nest.append(('AdaBoost', Pipeline([sc, ('AdaBoost', AdaBoostRegressor())])))\nest.append(('ExtraTree', Pipeline([sc,('ExtraTrees', ExtraTreesRegressor())])))\nest.append(('RandomForest', Pipeline([sc,('RandomForest', RandomForestRegressor())]))) \nest.append(('Bagging', Pipeline([sc,('Bagging', BaggingRegressor())])))\nest.append(('KNeighbors', Pipeline([sc,('KNeighbors', KNeighborsRegressor())])))\nest.append(('DecisionTree', Pipeline([sc,('DecisionTree', DecisionTreeRegressor())])))\nest.append(('XGB', Pipeline([sc,('XGB', XGBRegressor())])))","925a58d1":"import warnings\nwarnings.filterwarnings(action='ignore')\nseed = 4\nsplits = 7\nscore = 'r2'\nmodels_score =[]\nfor i in est:\n    kfold = KFold(n_splits=splits, random_state=seed)\n    results = cross_val_score(i[1], x_train, y_train, cv=kfold, scoring=score)\n    models_score.append({i[0] : '{} +\/- {}'.format(results.mean(), results.std())})","5c9e772f":"models_score","1b94fa2e":"#Tuning only XGB as it has the higher accuracy\nest =[]\nest.append(('XGB', Pipeline([sc,('XGB', XGBRegressor())])))\n\nbest = []\n\nparameters = {\n              \n              'XGB': {'XGB__learning_rate': [0.1,0.2,0.3,0.4],\n                         \n                         'XGB__max_depth': [4,6,8],\n                      'XGB__n_estimators': [100,500,1000,1500]}\n             }\n\nfor i in est:\n    kfold = KFold(n_splits=5, random_state=seed)\n    grid = GridSearchCV(estimator=i[1], param_grid = parameters[i[0]], cv = kfold, n_jobs=-1)\n    grid.fit(x_train, y_train)\n    best.append((i[0], grid.best_score_,  grid.best_params_))","d766fbd7":"#implementing it with best parameters\nxgb = XGBRegressor(learning_rate=0.1, max_depth=4, n_estimators=1000)\nxgb.fit(x_train, y_train)\npred = xgb.predict(x_test)\nxgb.score(x_test,y_test)","edf8abd9":"best","59234715":"#plotting the regression line\nfor i,e in enumerate(x_train.columns):\n    lr.fit(x_train[e].values[:,np.newaxis], y_train.values)\n    plt.title(\"Best fit line\")\n    plt.xlabel(str(e))\n    plt.ylabel('Price')\n    plt.scatter(x_train[e].values[:,np.newaxis], y_train)\n    plt.plot(x_train[e].values[:,np.newaxis], lr.predict(x_train[e].values[:,np.newaxis]),color='r')\n    plt.show()","9efa1195":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor","98b3bc6b":"def base_model():\n    model = Sequential()\n    model.add(Dense(16, input_dim=16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","2e38d484":"estimator = KerasRegressor(build_fn=base_model, epochs=100, batch_size= 5)\nresults = cross_val_score(estimator, x_train, y_train, cv=kfold)\nresults.mean()","ee2561f6":"results.mean()","eaf973a6":"estimator.fit(x_train, y_train)\nr2_score(y_test, estimator.predict(x_test))","856e96f8":"r2_score(y_test, estimator.predict(x_test))","bee98147":"# Creating Pipeline to pass each models in it ","834ba1cd":"I am applying neural network just for practise","92d1dce8":"# Applying Linear Models on the Housing Data","8791e189":"    ExtraTreesRegressor, RandomForestRegressor, BaggingRegressor and XBGRegressor are best","49e43f04":"# Hence XGB is the best model for this problem","040a2d9a":"Definitely not a neural network","5fd87f95":"# Preprocessing","9c1c29af":"# END OF NOTEBOOK","38ebcae9":"# training the data","bf6060c4":"# outlier detection","cced58a7":"# Applying Neural Networks","5fe50e08":"## So its clear that we should go with z_score one","1c36e50a":"### Hyperparameter Tuning"}}