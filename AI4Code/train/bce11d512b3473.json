{"cell_type":{"67be28f9":"code","8c37cc74":"code","2b5e78a3":"code","a5279bfe":"code","61c2a842":"code","978970de":"code","8427b32b":"code","a05b69bb":"code","01ae535d":"code","b79587cc":"code","d627768c":"code","e54ad9ed":"code","c5a0dc03":"code","96d79269":"code","9daeebf2":"code","3da81390":"code","04478aea":"code","26ba5877":"code","5d8d5de1":"code","928307f4":"code","443afc24":"code","6af20894":"code","e5516593":"code","6eb6e6e0":"code","ae21be54":"code","e4e1d7df":"code","5b757d3a":"code","b8bbb732":"code","4fc45467":"code","7255c52e":"code","ec63e3ea":"code","9d09245e":"code","5323b855":"code","e80de81c":"code","d204c863":"code","999b8a27":"code","e42c91cd":"code","5cef37b8":"code","db24b7e3":"code","39b7f182":"code","340455af":"code","6369706c":"code","2127f076":"code","436f8047":"code","ef57c845":"code","63002e8f":"code","d6c9d0e8":"code","0a8837d2":"code","26c72acb":"code","59e5973f":"code","c8a410ff":"code","964ba24f":"markdown","95f3b0f7":"markdown","e7c42547":"markdown","dbd0a2b0":"markdown","3f38b23e":"markdown","42a6d1ce":"markdown","9358146a":"markdown","27ca79d8":"markdown","377ab80b":"markdown","abc72984":"markdown","a6d52926":"markdown","7f804642":"markdown","a9d3096f":"markdown","53441557":"markdown","dc41f534":"markdown","f9db5326":"markdown"},"source":{"67be28f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c37cc74":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')","2b5e78a3":"df = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv')\ndf.head()","a5279bfe":"df.isnull().sum()","61c2a842":"sns.countplot(x=df.Species)","978970de":"# Saving the dependent and idependent data\nfeatures = df.drop(['Species'], axis=1)\nlabels = df['Species']","8427b32b":"st_scaler = StandardScaler()\nX = st_scaler.fit_transform(features)\nX = pd.DataFrame(X, columns=features.columns)\nX","a05b69bb":"# Debugging steps\n# print(\"Mean after applying Standardization: \\n\", X.mean(axis=0),'\\n','-'*50)\n# print(\"Standard Deviation after applying Standardization \\n\", X.std(axis=0))","01ae535d":"l_encoder = LabelEncoder()\ny = l_encoder.fit_transform(labels)\n# y","b79587cc":"# Saving label mappings\nlabel_mappings = {index:label for index, label in enumerate(l_encoder.classes_)}\nlabel_mappings","d627768c":"sns.pairplot(df, hue='Species')","e54ad9ed":"X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.3, \n                                                    random_state=100)","c5a0dc03":"model_logistic_regression = LogisticRegression()\nmodel_logistic_regression.fit(X_train, y_train)\nprint(\"Accuracy: \", round(model_logistic_regression.score(X_test, y_test),4))","96d79269":"# Plotting confusion matrix for default model\nprint(label_mappings)\nplot_confusion_matrix(model_logistic_regression, X_train, y_train);","9daeebf2":"model_logistic_regression_2 = LogisticRegression(penalty='none')\nmodel_logistic_regression_2.fit(X_train, y_train)\nprint(\"Accuracy: \", round(model_logistic_regression_2.score(X_test, y_test),4))","3da81390":"# Plotting confusion matrix for no penalty model\nprint(label_mappings)\nplot_confusion_matrix(model_logistic_regression_2, X_train, y_train);","04478aea":"X_train, y_train","26ba5877":"model_logistic_regression_3 = LogisticRegression(C=0.0,penalty='none', class_weight='balanced')\nmodel_logistic_regression_3.fit(X_train, y_train)\nmodel_logistic_regression_3.score(X_test, y_test)","5d8d5de1":"# Plotting confusion matrix with penalty=l2 and balanced class weights\nprint(label_mappings)\nplot_confusion_matrix(model_logistic_regression_3, X_train, y_train);","928307f4":"C = np.concatenate((np.arange(0,1,0.1), np.arange(1,11,1)),axis=None)","443afc24":"from sklearn.model_selection import GridSearchCV\nparameters = {'penalty':['l2', 'none'], 'C':C}","6af20894":"lr = LogisticRegression(class_weight='balanced')\nclf = GridSearchCV(lr, parameters)\nclf.fit(X_train, y_train)","e5516593":"cv_result = pd.DataFrame(clf.cv_results_)\ncv_result[cv_result.rank_test_score==1]","6eb6e6e0":"print('Best_estimator: ',clf.best_estimator_)\nprint('Best Score: ', clf.best_score_)","ae21be54":"sns.heatmap(features.corr(), annot=True, cmap='Blues')","e4e1d7df":"from sklearn.decomposition import PCA","5b757d3a":"num_components = X.shape[1]\nPC_column_names = ['PC' + str(i) for i in range(1,num_components+1)]\npca = PCA(n_components= num_components)\npca.fit(X)\n# pca.explained_variance_\nexplained_variance_ratio = pca.explained_variance_ratio_","b8bbb732":"principal_component_variance_plot = plt.figure(figsize=(15,8))\nprincipal_component_variance_plot = sns.barplot(x=explained_variance_ratio, \n                                                y=PC_column_names, \n                                                orient='h', palette='husl')\nprincipal_component_variance_plot.set_title(\"Variance Explained by Principle components\")\nprincipal_component_variance_plot.set_xlabel(\"Exaplined variance\");\nprincipal_component_variance_plot.set_ylabel(\"Principal component\");","4fc45467":"components_to_use = 4\nPC_X_all = pd.DataFrame(pca.transform(X), columns=PC_column_names)\nPC_X = PC_X_all.iloc[:,:components_to_use]","7255c52e":"PC_X_all.dtypes","ec63e3ea":"PC_Xtrain, PC_Xtest, PC_y_train, PC_y_test = train_test_split(PC_X, y,\n                                                    test_size=0.3,\n                                                    random_state=42)","9d09245e":"model_logistic_regression_PCA = LogisticRegression(C=0.0, class_weight='balanced', penalty='none', solver='sag')\nmodel_logistic_regression_PCA.fit(PC_Xtrain, PC_y_train)\nmodel_logistic_regression_PCA.score(PC_Xtest, PC_y_test)","5323b855":"fig, ax = plt.subplots(2,2,figsize=(15,8))\nfig1 = plot_confusion_matrix(model_logistic_regression, X_test, y_test, ax=ax[0,0])\nfig2 = plot_confusion_matrix(model_logistic_regression_2, X_test, y_test, ax=ax[0,1])\nfig3 = plot_confusion_matrix(model_logistic_regression_3, X_test, y_test, ax=ax[1,0])\nfig4 = plot_confusion_matrix(model_logistic_regression_PCA, PC_Xtest, PC_y_test, ax=ax[1,1])\nax[0,0].set_title(\"Default model\")\nax[0,1].set_title(\"No Penalty\")\nax[1,0].set_title(\"No penalty balanced\")\nax[1,1].set_title(\"PCA model\")\nfig.tight_layout()\nfig.suptitle('Testing data confusion matrix');","e80de81c":"df_FE = pd.DataFrame()","d204c863":"df_FE['Weight'] = df['Weight']\ndf_FE['Height'] = df['Height']\ndf_FE['Width'] = df['Width']\ndf_FE['Total_Length'] = df['Length1']+df['Length2']+df['Length3']\n\ndf_FE","999b8a27":"st_scaler = StandardScaler()\nX_FE = st_scaler.fit_transform(df_FE)\nX_FE = pd.DataFrame(X_FE, columns=df_FE.columns)\nX_FE","e42c91cd":"XFE_train, XFE_test, yFE_train, yFE_test = train_test_split(X_FE, y,\n                                                    test_size=0.3,\n                                                    random_state=42)","5cef37b8":"model_logistic_regression_FE = LogisticRegression(C=0.0, class_weight='balanced', penalty='none', solver='sag')\nmodel_logistic_regression_FE.fit(XFE_train, yFE_train)\nmodel_logistic_regression_FE.score(XFE_test, yFE_test)","db24b7e3":"random_stats = list(range(1,400,5))","39b7f182":"def model_accuracy_random_stat(model, random_stats, X_fun, y_fun):\n    model_eval = dict.fromkeys(random_stats)\n    for random_state in random_stats:\n        X_train_fun, X_test_fun, y_train_fun, y_test_fun = train_test_split(X_fun, y_fun,\n                                                        test_size=0.3,\n                                                        random_state=random_state)\n        model.fit(X_train_fun, y_train_fun)\n        accuracy = model.score(X_test_fun, y_test_fun)\n        model_eval[random_state] = accuracy\n#         print(f'Random state= {random_state}, Accuracy = {round(accuracy,2)}')\n    return model_eval","340455af":"Default_model_eval = model_accuracy_random_stat(model_logistic_regression, random_stats, X, y)\nno_penalty_model_eval = model_accuracy_random_stat(model_logistic_regression_2, random_stats, X, y)\nbalanced_no_penalty_model_eval = model_accuracy_random_stat(model_logistic_regression_3, random_stats, X, y)\nPCA_model_eval = model_accuracy_random_stat(model_logistic_regression_PCA, random_stats, PC_X, y)\nFE_model_eval = model_accuracy_random_stat(model_logistic_regression_FE, random_stats,X_FE, y)","6369706c":"model_evaluation = pd.DataFrame([Default_model_eval,no_penalty_model_eval,\n                                 balanced_no_penalty_model_eval,PCA_model_eval,FE_model_eval], \n                                index=['Default_model_eval','no_penalty_model_eval',\n                                       'balanced_no_penalty_model_eval','PCA_model_eval','FE_model_eval']).T","2127f076":"model_evaluation.plot(figsize = (15,8))\nplt.axhline(model_evaluation['Default_model_eval'].mean(), color='blue')\nplt.axhline(model_evaluation['no_penalty_model_eval'].mean(), color='orange')\nplt.axhline(model_evaluation['balanced_no_penalty_model_eval'].mean(), color='green')\nplt.axhline(model_evaluation['PCA_model_eval'].mean(), color='red')\nplt.axhline(model_evaluation['FE_model_eval'].mean(), color='violet')","436f8047":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","ef57c845":"sgd_model = SGDClassifier()\nSVC_model = SVC()\nKNN_model = KNeighborsClassifier()\nDT_model = DecisionTreeClassifier()\nRF_model = RandomForestClassifier()","63002e8f":"models_list = [sgd_model, SVC_model, KNN_model, DT_model, RF_model]","d6c9d0e8":"for model in models_list:\n    model.fit(X_train, y_train)\n    acc = model.score(X_test, y_test)\n    print(\"Model: \", model)\n    print(\"Params: \", model.get_params(deep=False))\n    print(\"Accuracy: \",acc)\n    print(\"-\"*100)","0a8837d2":"try:\n    import lazypredict\nexcept:\n    !pip install lazypredict;\nfinally:\n    from lazypredict.Supervised import LazyClassifier","26c72acb":"clf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)","59e5973f":"models","c8a410ff":"predictions","964ba24f":"# Other Algorithms\n**let's explore other classification techniques on original X_train, Y_train dataset.**","95f3b0f7":"## Observation\nWe are able to get good accuracy by using class weight. It is less than accuracy without class weights, but it will not generalised well on future data as it is biased towards majority class.","e7c42547":"## PreProcessing\n### Scaling the data and encoding the labels","dbd0a2b0":"## observations\n1. Everytime we split the data randomly we get huge differences in accuracy score. Reason is small and imbalanced dataset.\n2. Without penalty and class weights it is overfitting training data and working well on small test data but it may not generalised well in future.","3f38b23e":"# Feature Engineering\nAs we show in heatmap and paitplot length1, length2 and length3 are highly correlated.  \nLet's use this linearity to reduce the features. We can take average or sum of these features.","42a6d1ce":"### We can do feature engineering or feature selection to choose the best features. But PCA is kind of hack to reduce the dimensionality. But we loose the interpretability here.","9358146a":"# Observations\n## Data is clean\nData is really clean with no missing value.\n## Scale of different features\nHere the scale of different features are very different. So we need to scale it before training the model.\n## Handling imbalanced dataset\nHere we can observe that some species have very high counts than other. For example Perch and Bream have very high counts than Whitefish. In Some models classweight helps to improve the performance.","27ca79d8":"### Let's draw a pariplot to see the correlations","377ab80b":"## Observations\n1. Length1, Length2 and Length3 are highly correlated\n\nWe will use this observation while doing feature engineering","abc72984":"# Bonus\n## Let's try out all models\nbe Lazy be smart","a6d52926":"## Training","7f804642":"# Better accuracy?\nyes!! But everytime we change the random_state we get the different accuracy.  \nWe have tried to reduce the features to 4 by PCA and got the good accuracy.  \nAlso we tried to remove the correlated length columns by simply taking sum, and again it worked with 4 features.  \n\n## How to analyse further?\nWe can split the data randomly for different random stat. If we get average good accuracy with balanced class weight and penalty, we can choose it over biased model.\n\n","a9d3096f":"## Conlusion\n### We got high accuracy with no penalty models. Accuracy difference between balanced and imbalanced class weight models are almost equal (orange and green line).\n### We have tried to split the data using random stats, so this average accuracy is more trustable.\n### We should use balanced model with no penalty.","53441557":"## let's do grid search\nWe will try to get the best result for value of C for balanced classweight.","dc41f534":"# INDEX  \n\n## GOAL 1\n* Data clearning, Preprocessing\n* Train Logistic regression model with\n    - With\/Without class weights on imbalanced data\n    - With\/Without Penalty (L2 Regularisaion)\n* Perform Principal Component Analysis (PCA)\n    - Train Logistic Regression using Principal Components\n* Perform Feature Engineering\n    - Train Model with less number of features\n* Conclusion  \n\n## GOAL 2\n* Predict using Support vector classifier\n* Predict using K-Nearest Neighbors\n* Predict using Decision Tree\n* Predict using Random Forest","f9db5326":"## PCA\n**So many features are correlated to each other**  \nLet's use PCA to reduce the dimensionality"}}