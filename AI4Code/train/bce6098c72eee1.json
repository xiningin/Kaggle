{"cell_type":{"45c5319b":"code","c0754c26":"code","fcdae98a":"code","0a64aff2":"code","6eadd7da":"code","461feb54":"code","88a2845d":"code","d2d78cda":"code","002bc1d2":"code","cbfae4a4":"code","962bc339":"code","2034c967":"code","99f7f85f":"code","459ca37b":"code","045afc7c":"code","73a517b7":"code","33fe9cd9":"code","e14f0e23":"code","81871b28":"code","bf03ee1c":"code","179fe5d3":"code","db82fab2":"code","be4c5225":"code","abe8e6e2":"markdown","8d1fc580":"markdown","55d231f1":"markdown","4afc817f":"markdown","c413a7cf":"markdown","a395ce3e":"markdown","5ae86b79":"markdown","dd3220ac":"markdown","1870f0e5":"markdown","290ae522":"markdown","acdcaa5f":"markdown","7abc5b9b":"markdown","cf5a0aab":"markdown","d6d5e7b0":"markdown","00941c07":"markdown","4d181e0e":"markdown","e8b821fc":"markdown","d9ced6dc":"markdown","2eb3684c":"markdown","fbfe6676":"markdown"},"source":{"45c5319b":"import pandas as pd\nimport numpy as np","c0754c26":"add = \"..\/input\/mushrooms.csv\"\ndata = pd.read_csv(add)\n","fcdae98a":"# seperating X vaules from y values\nX= data.iloc[:,1:]\ny = data.iloc[:,0]","0a64aff2":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nd = defaultdict (LabelEncoder)\nXfit = X.apply(lambda x: d[x.name].fit_transform(x))","6eadd7da":"le_y = LabelEncoder()\nyfit = le_y.fit_transform(y)\n# for x in Xfit.columns:\n#     print(x)\n#     print(Xfit[x].value_counts())","461feb54":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import OneHotEncoder\nohc = defaultdict (OneHotEncoder)\n# Xfit_ohc = Xfit.apply(lambda x: ohc[x.name].fit_transform(x))\nfinal = pd.DataFrame()\n\nfor i in range(22):\n    # transforming the columns using One hot encoder\n    Xtemp_i = pd.DataFrame(ohc[Xfit.columns[i]].fit_transform(Xfit.iloc[:,i:i+1]).toarray())\n   \n    #Naming the columns as per label encoder\n    ohc_obj  = ohc[Xfit.columns[i]]\n    labelEncoder_i= d[Xfit.columns[i]]\n    Xtemp_i.columns= Xfit.columns[i]+\"_\"+labelEncoder_i.inverse_transform(ohc_obj.active_features_)\n    \n    # taking care of dummy variable trap\n    X_ohc_i = Xtemp_i.iloc[:,1:]\n    \n    #appending the columns to final dataframe\n    final = pd.concat([final,X_ohc_i],axis=1)","88a2845d":"final.shape","d2d78cda":"final.head(20)","002bc1d2":"final[1:4]","cbfae4a4":"data[1:4]","962bc339":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(final, yfit, test_size = 0.1, random_state = 0)","2034c967":"from sklearn.neighbors import KNeighborsClassifier\nclassifier =  KNeighborsClassifier(n_neighbors=30,p=2, metric='minkowski')\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)","99f7f85f":"from sklearn.metrics import confusion_matrix\ncm= confusion_matrix(y_test,y_pred)\ncm","459ca37b":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","045afc7c":"classif =  KNeighborsClassifier(n_neighbors=200,p=2, metric='minkowski')\nclassif.fit(X_train,y_train)\ny_pred = classif.predict(X_test)\naccuracy_score(y_test,y_pred)","73a517b7":"from sklearn.model_selection import cross_val_score\n\n# creating odd list of K for KNN\nmyList = list(range(1,200))\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in myList[::2]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n","33fe9cd9":"from matplotlib import pyplot as plt","e14f0e23":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\n# determining best k\noptimal_k = myList[::2][MSE.index(min(MSE))]\nprint (\"The optimal number of neighbors is %d\" % optimal_k)\n\n# plot misclassification error vs k\nplt.plot(myList[::2], MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","81871b28":"n_features = final.shape[1]\nclf = KNeighborsClassifier()\nfeature_score = []\n\nfor i in range(n_features):\n    X_feature= np.reshape(final.iloc[:,i:i+1],-1,1)\n    scores = cross_val_score(clf, X_feature, yfit)\n    feature_score.append(scores.mean())\n    print('%40s        %g' % (final.columns[i], scores.mean()))\n","bf03ee1c":"feat_imp = pd.Series(data = feature_score, index = final.columns)\nfeat_imp.sort_values(ascending=False, inplace=True)\nfeat_imp[feat_imp>0.7]\n\n","179fe5d3":"columns_imp = feat_imp[feat_imp>0.7].index.values\nfinal_Xy= pd.concat([final,pd.DataFrame(yfit,columns=['class'])], axis=1)\ngrouped = final_Xy.groupby('class')","db82fab2":"# Edible group of mushrooms\ngrouped.get_group(0)[columns_imp].sum()","be4c5225":"# Poisonous group of mushrooms\ngrouped.get_group(1)[columns_imp].sum()","abe8e6e2":"## Approach","8d1fc580":"First column is a classifier\n0. Class : edible e, poisonous p\n\nRest of the columns are \n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s \n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \n4. bruises?: bruises=t,no=f \n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \n....","55d231f1":"## 3. Design Multi-column One Hot encoder ","4afc817f":"###  Compare final vs data ","c413a7cf":"## 4. Predict results","a395ce3e":"   # <center>For more clarity on parts of a mushroom<\/center>\n \n![Parts of mushroom](https:\/\/infovisual.info\/storage\/app\/media\/01\/img_en\/024%20Mushroom.jpg)\n\nNow, it is pretty clear that all these factors indicate a poisonous mushroom.\n\n <b> DO NOT EAT A MUSHROOM if : <\/b>\n1. <b> Odor is foul <\/b>\n2. <b> Stalk surface above ring is  silky <\/b>\n3. <b> Stalk surface below ring is  silky <\/b>\n4. <b> Gill size is narrow <\/b>\n5. <b> Spore prints are chocolatey in color <\/b>","5ae86b79":"- Need to avoid dummy variable trap\n- Using the \"d\" the defaultdictionary to rename columns after one hot encoder\n- appending new columns after encoding into \"final\" variable","dd3220ac":"## 6.  Finding traits of poisonous mushrooms\n( Feature importance in K-NN )","1870f0e5":"   # <center>Classification of poisonous mushroom<\/center>\n![](https:\/\/kennettmushrooms.com\/wp-content\/uploads\/2017\/05\/fungi-funnys-570x285.jpg)","290ae522":"KNN provides us a power classifier but finding optimal value of K is very critical. We can take additional test data sets and measure performance with current K = 30 to calibrate and avoid overfit\n","acdcaa5f":"## Final Conclusion","7abc5b9b":"##  If I am in the Jungle : How do I survive on mushroom without K-NN\n\nTo answer the above question. We need to find the most significant feature ","cf5a0aab":"## 2.Use Label encoder to replace text data","d6d5e7b0":" Question from the Jungle : <b>Should I eat or not ? considering factors<\/b>\n\nAnswer : Need to deep dive to figure out the positive or negative correlation !\n\n","00941c07":"Notes : \n- There are several columns of categorical variables. We need to avoid dummy variable Trap\n- Find a way to rename the columns after one hot encoder operation is done","4d181e0e":"## 5. Review optimal K ","e8b821fc":"## The 5 most important factors : to determine poisonous or not","d9ced6dc":"1. Separate X and y variables \n2. Use Label encoder to replace text data\n3. Design multicolumn one hot encoder\n4. Predict results\n5. Review optimal K \n6. Finding traits of poisonous Mushroom","2eb3684c":"## About  Mushroom dataset ","fbfe6676":"## 1. Separate X and y  variables"}}