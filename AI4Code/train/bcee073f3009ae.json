{"cell_type":{"b9527d55":"code","729d6708":"code","77b66b2e":"code","7aea3035":"code","330a0b21":"code","def7cb1e":"code","8a2d5ce6":"code","a57008e3":"code","f9dfe509":"code","8faa77f9":"code","0cceea2a":"code","49b97fd6":"code","70e3a72d":"code","66485939":"code","4b39fb78":"code","f7848d48":"code","3d6e0d74":"code","0812044f":"code","cc062227":"code","87901e2e":"code","540c29d9":"code","eb6023df":"code","4d1e93f9":"code","fe40519c":"code","8f58226d":"code","fd411f6c":"code","877edc14":"code","b0b5f317":"code","81f84d3b":"code","f9b352cf":"code","cdd9343d":"code","66dcc9da":"code","b9e5eb96":"code","7ef87077":"code","99924838":"code","6ea16104":"code","fd114770":"code","a795457e":"code","a74fe997":"code","b33abbdb":"code","c8a0f004":"code","9f077647":"code","96766d8f":"code","99022ff4":"code","dc533a65":"code","e0de5fba":"code","d17ed301":"markdown","a695d1ff":"markdown","57271acd":"markdown","c6874796":"markdown","df212da4":"markdown","4129fce5":"markdown","d7d10072":"markdown","a020110f":"markdown"},"source":{"b9527d55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","729d6708":"# Import libraries\nimport torch\nimport os\nimport torch.nn as nn\n\n# AutoTokenizer and AutoModelForSequenceClassification will allow to try different model architerctures with minimal changes in code\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler","77b66b2e":"import random\n\nseed = 142\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)","7aea3035":"DATA_DIR = os.path.realpath('\/kaggle\/input\/nlp-getting-started')","330a0b21":"# Load dataset into dataframe\ndf = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='utf-8')","def7cb1e":"df.head()","8a2d5ce6":"# Distribution of target\n\ndf['target'].value_counts() \/ df.shape[0]","a57008e3":"train_indices, val_indices = train_test_split(df.index, stratify=df['target'], test_size=0.15,random_state=42)","f9dfe509":"import re\nimport string\n\nprintable_chars = set(string.printable)\n\ndef preprocess_text(str_txt: str) -> str:\n    \"\"\"Preprocessing for raw text data\"\"\"\n    # Remove urls from tweet\n    str_txt = re.sub(r'(https?:\\\/\\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\\/\\w\\.-]*)', '', str_txt, flags=re.MULTILINE)\n    # Remove mentions\n    str_txt = re.sub(r'@([A-z0-9_]+)', '', str_txt, flags=re.MULTILINE)\n    # Remove # from hashtags\n    str_txt = re.sub(r'#([A-z0-9_]+)', '\\g<1>', str_txt, flags=re.MULTILINE)\n    # Convert numbers int NUMBER\n    str_txt = re.sub(r'\\d+[,.]?(?:\\d+)?', 'NUMBER', str_txt, flags=re.MULTILINE)\n    # Remove non printable characters\n    str_txt = ''.join([ch for ch in str_txt if ch in printable_chars])\n    \n    return str_txt","8faa77f9":"# DataFrame before preprocessing\ndf.head()","0cceea2a":"def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Preprocessing for data frame\"\"\"\n    df['text'] = df['text'].apply(preprocess_text)\n    return df","49b97fd6":"df = preprocess_df(df)","70e3a72d":"df.head()","66485939":"from typing import Union\n\nclass DisasterTweetsDataset(Dataset):\n    def __init__(self, tweets_df: pd.DataFrame, text_column: str, label_column: str = None) -> None:\n        super().__init__()\n        self.tweets_df = tweets_df\n        self.text_column = text_column\n        self.label_column = label_column\n\n\n    def __len__(self) -> int:\n        return self.tweets_df.shape[0]\n\n    def __getitem__(self, idx: int):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        if self.label_column:\n            df_row = self.tweets_df.loc[idx, [self.text_column, self.label_column]]\n            sample = {'text': df_row[self.text_column], 'label': df_row[self.label_column]}\n        else:\n            df_row = self.tweets_df.loc[idx, [self.text_column]]\n            sample = {'text': df_row[self.text_column]}\n        return sample","4b39fb78":"# SubsetRandomSampler samples elements randomly from a given list of indices, without replacement.\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)","f7848d48":"tweet_dataset = DisasterTweetsDataset(df, 'text', 'target')\nprint(\"Dataset Size:\", len(tweet_dataset))","3d6e0d74":"# Example of positive class\nprint(\"Text:\", tweet_dataset[2]['text'])\nprint(\"Label:\", tweet_dataset[2]['label'])","0812044f":"# Example of negative class\nprint(\"Text:\", tweet_dataset[18]['text'])\nprint(\"Label:\", tweet_dataset[18]['label'])","cc062227":"hf_weights_name = 'roberta-large'\n# Create tokenizer from pretrained weights\nhf_tokenizer = AutoTokenizer.from_pretrained(hf_weights_name)","87901e2e":"# For our case collate_fn is called with a list of data samples at each time. \n# It is expected to collate the input samples into a batch for yielding from the data loader iterator. \n\ndef collate_fn(batch):\n    if 'label' in batch[0]:\n        texts, labels = zip(*[(batch[i]['text'], batch[i]['label']) for i in range(len(batch))])\n        result = dict(labels=labels)\n    else:\n        texts = [batch[i]['text'] for i in range(len(batch))]\n        result = {}\n    hf_example_ids = hf_tokenizer.batch_encode_plus(list(texts),\n        add_special_tokens=True,\n        return_attention_mask=True,\n        padding='longest')\n    return dict(**result, **hf_example_ids)","540c29d9":"import multiprocessing\n\n\nnum_workers = multiprocessing.cpu_count()\nbatch_size = 8","eb6023df":"# Create data loaders for train and validation sets\ntrain_loader = DataLoader(tweet_dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, sampler=train_sampler)\nval_loader = DataLoader(tweet_dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, sampler=val_sampler)","4d1e93f9":"print(len(train_loader))\nprint(len(val_loader))","fe40519c":"data_loaders = {'train': train_loader, 'val': val_loader}\nprogress_bars = {}\nepoch_stats = {}","8f58226d":"# Check if cuda is available\ngpu_available = torch.cuda.is_available()\nprint(\"GPU is available:\", gpu_available)","fd411f6c":"# Check \ndevice = torch.device('cuda' if gpu_available else 'cpu')\nprint(device)","877edc14":"if gpu_available:\n    \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(\"Cuda Device Name:\",torch.cuda.get_device_name())","b0b5f317":"# Create model from pretrained weights\nmodel = AutoModelForSequenceClassification.from_pretrained(hf_weights_name, num_labels=2)\nmodel.to(device);","81f84d3b":"num_epochs = 4\nverbose = True","f9b352cf":"from transformers import AdamW, get_linear_schedule_with_warmup\n\n\noptimizer = AdamW(model.parameters(), lr=1.5e-6, eps=1e-8)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(train_loader)*num_epochs)","cdd9343d":"best_acc = 0.0\nbest_loss = float('inf')","66dcc9da":"import copy\n\n# Weights of best model so far\nbest_model_weights = copy.deepcopy(model.state_dict())\nepoch_bar = tqdm(desc='training routine', total=num_epochs,\n                  initial=0, position=0, disable=(verbose is not True))\nfor split, data_loader in data_loaders.items():\n    progress_bars[split] = tqdm(desc=f'split={split}',\n                                total=len(data_loader),\n                                position=1,\n                                disable=(verbose is not True),\n                                leave=True)\n    epoch_stats[split] = {'loss': [], 'accuracy': []}\n\ntraining_data = []\ntry:\n    for epoch in range(1, num_epochs + 1):\n        \n        for split, data_loader in data_loaders.items():\n            epoch_loss = torch.FloatTensor([0.0]).to(device)\n            num_correct = torch.LongTensor([0]).to(device)\n            total_samples = 0\n            is_training = (split == 'train')\n            model.train(is_training)\n            for batch in data_loader:\n                with torch.set_grad_enabled(is_training):\n                    input_ids = torch.LongTensor(batch['input_ids']).to(device)\n                    labels = torch.LongTensor(batch['labels']).to(device)\n                    masks = torch.LongTensor(batch['attention_mask']).to(device)\n                    \n                    optimizer.zero_grad()\n\n                    outputs = model(input_ids, masks, labels=labels)\n                    loss = outputs.loss\n\n                    if is_training:\n                        loss.backward()\n                    epoch_loss += loss\n                    _, predictions = torch.max(outputs.logits, 1)\n                    num_correct += torch.eq(predictions, labels).sum()\n                    total_samples += labels.size(0)\n                    \n                    if is_training:\n                        optimizer.step()\n                        scheduler.step()\n                    progress_bars[split].update()\n            epoch_loss \/= len(data_loader)\n            epoch_accuracy = num_correct \/ total_samples\n            epoch_bar.set_postfix({f\"{split}_loss\": epoch_loss.item(), f\"{split}_acc\": round(epoch_accuracy.item(), 3)})\n            if not is_training:\n                training_data.append((epoch_loss.item(), round(epoch_accuracy.item(), 3)))\n                if epoch_accuracy.item() > best_acc:\n                    best_model_weights = copy.deepcopy(model.state_dict())\n                    best_acc = epoch_accuracy.item()\n\n        for bar in progress_bars.values():\n            bar.n = 0\n            bar.reset()\n        epoch_bar.update()\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print(training_data)","b9e5eb96":"# [(0.46483272314071655, 0.792), (0.42263394594192505, 0.813), (0.40105751156806946, 0.83), (0.40114283561706543, 0.83)]","7ef87077":"import matplotlib.pyplot as plt","99924838":"training_data","6ea16104":"val_losses, val_acc = zip(*training_data)","fd114770":"epochs = list(range(1, num_epochs + 1))\nfig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\naxes[0].plot(epochs, val_losses, 'o-')\naxes[0].set_ylabel('Val. loss')\naxes[1].plot(epochs, val_acc, 'o-', color=\"orange\")\naxes[1].set_ylabel('Val. accuracy')\n\nplt.xlabel(\"epochs\")\nplt.show()","a795457e":"# Load best model weights\nmodel.load_state_dict(best_model_weights)","a74fe997":"# Check best model performance on validation set\nmodel.eval()\nnum_correct = torch.LongTensor([0]).to(device)\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n        masks = torch.LongTensor(batch['attention_mask']).to(device)\n        labels = torch.LongTensor(batch['labels']).to(device)\n        \n        outputs = model(input_ids, masks)\n        _, predictions = torch.max(outputs.logits, 1)\n        num_correct += torch.eq(predictions, labels).sum()\nprint(\"Val. accuracy of best model:\", round(num_correct.item()\/len(val_indices), 3))","b33abbdb":"test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), encoding='utf-8')\ntest_df = preprocess_df(test_df)","c8a0f004":"# Dataset for test data\ntweet_dataset_test = DisasterTweetsDataset(test_df, 'text', None)","9f077647":"test_loader = DataLoader(tweet_dataset_test, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, shuffle=False)\npredictions_all = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n        masks = torch.LongTensor(batch['attention_mask']).to(device)\n        \n        outputs = model(input_ids, masks)\n        _, predictions = torch.max(outputs.logits, 1)\n\n        predictions_all.append(predictions)","96766d8f":"predictions_all_tensor = torch.cat(predictions_all)","99022ff4":"# Predictions for first 10 samples\npredictions_all_tensor[:10]","dc533a65":"import csv\nimport uuid\n\n\ndef prepare_submission(test_data: pd.DataFrame, predicted: np.ndarray):\n    f_name = f\"submissions_{uuid.uuid4()}.csv\"\n    print(f_name)\n    with open(f_name, mode=\"w\") as f:\n        csv_writer = csv.DictWriter(f, fieldnames=['id', \"target\"])\n        csv_writer.writeheader()\n        for idx, df_row in test_data.iterrows():\n            csv_writer.writerow({\"id\": df_row['id'], \"target\": predicted[idx]})","e0de5fba":"prepare_submission(test_df, predictions_all_tensor.cpu().numpy())","d17ed301":"## Text preprocessing","a695d1ff":"### Training results visualization","57271acd":"### Dataset and Dataloader","c6874796":"## Split data into train, validation sets\n","df212da4":"# Distaster tweet classification with HF transformers\n\n## Goal\n\nPredict which Tweets are about real disasters and which ones are not","4129fce5":"I am going to use RoBERTa base model with pretrained weights from HuggingFace transformers. More details can be seen [here](https:\/\/huggingface.co\/roberta-large) ","d7d10072":"## Make predictions on test data","a020110f":"### FineTuning procedure"}}