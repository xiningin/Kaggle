{"cell_type":{"0f0becce":"code","630df97e":"code","b2958371":"code","073620d2":"code","6b86299e":"code","3085a2b8":"code","ebbc8aeb":"code","b222a7f6":"code","10f6569e":"code","4dd8acc7":"code","13840e47":"code","6887c4d9":"code","20e11d9f":"code","e0cdca89":"code","3cfcf3c4":"code","f5e9600c":"code","c684de37":"code","af8312b9":"code","bd093fa9":"code","368aa9e2":"code","31a9d584":"code","1dcb6c48":"code","4c5cb86c":"code","265a1579":"code","55a71277":"code","cd2d0999":"code","641dc59d":"code","9d5963f8":"code","a7f7d8bf":"code","6049d86f":"code","6820f653":"code","79a5491e":"code","fd3f9856":"markdown","f9bfe305":"markdown","ffe81289":"markdown","0ce50028":"markdown","11930835":"markdown","c3de5a32":"markdown","0671076b":"markdown","6a11991a":"markdown","5008a351":"markdown","4af623e7":"markdown","02e33258":"markdown","b3168172":"markdown","92523a97":"markdown","0dc3c262":"markdown","2977e6e9":"markdown","f0fc8f21":"markdown","1d3bdc6c":"markdown","e2917caa":"markdown","48f92a6c":"markdown","eaafacb0":"markdown","4f221987":"markdown"},"source":{"0f0becce":"import os\nimport json\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom altair.vega import v3\nfrom IPython.display import HTML\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nalt.data_transformers.enable('default', max_rows=None)\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(0)\nseed(0)","630df97e":"# Preparing altair. I use code from this great kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\",\n)))","b2958371":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","073620d2":"total_comments = train.shape[0]\nn_unique_comments = train['comment_text'].nunique()\nn_comments_both = len(set(train['comment_text'].unique()) & set(test['comment_text'].unique()))\nprint('Train set: %d rows and %d columns.' % (total_comments, train.shape[1]))\ndisplay(train.head())\ndisplay(train.describe())\nprint('Test set: %d rows and %d columns.' % (test.shape[0], test.shape[1]))\ndisplay(test.head())\nprint('Number of unique comments: %d or %.2f%%'% (n_unique_comments, (n_unique_comments \/ total_comments * 100)))\nprint('Number of comments that are both in train and test sets: %d'% n_comments_both)","6b86299e":"train['comment_length'] = train['comment_text'].apply(lambda x : len(x))\ntest['comment_length'] = test['comment_text'].apply(lambda x : len(x))\ntrain['word_count'] = train['comment_text'].apply(lambda x : len(x.split(' ')))\ntest['word_count'] = test['comment_text'].apply(lambda x : len(x.split(' ')))\nbin_size = max(train['comment_length'].max(), test['comment_length'].max())\/\/10\n\nplt.figure(figsize=(20, 6))\nsns.distplot(train['comment_length'], bins=bin_size)\nsns.distplot(test['comment_length'], bins=bin_size)\nplt.show()","3085a2b8":"bin_size = max(train['word_count'].max(), test['word_count'].max())\/\/10\nplt.figure(figsize=(20, 6))\nsns.distplot(train['word_count'], bins=bin_size)\nsns.distplot(test['word_count'], bins=bin_size)\nplt.show()","ebbc8aeb":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\nsns.distplot(train['target'], bins=20, ax=ax1).set_title(\"Complete set\")\nsns.distplot(train[train['target'] > 0]['target'].values, bins=20, ax=ax2).set_title(\"Only toxic comments\")\nplt.show()","b222a7f6":"train['is_toxic'] = train['target'].apply(lambda x : 1 if (x > 0.5) else 0)\nplt.figure(figsize=(8, 6))\nsns.countplot(train['is_toxic'])\nplt.show()","10f6569e":"HEIGHT = 300\nWIDTH = 600\ntrain['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\ntarget_df = train.sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count'], 'target':['mean']})\ntarget_df.columns = ['Date', 'Count', 'Toxicity rate']\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['created_date'], empty='none')\n\nrate_line = alt.Chart().mark_line(interpolate='basis', color='red').encode(\n    x='Date:T',\n    y='Toxicity rate:Q'\n).interactive()\n\n# The basic line\ncount_line = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q'\n).properties(title=\"Counts and rate of toxicity comments\").interactive()\n\nrender(alt.layer(rate_line, count_line, data=target_df, width=WIDTH, height=HEIGHT).resolve_scale(\n    y='independent'\n))","4dd8acc7":"disability_gp = ['intellectual_or_learning_disability', 'other_disability', 'physical_disability', 'psychiatric_or_mental_illness']\ndisability_df = train[train[disability_gp[0]] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\ndisability_df.columns = ['Date', disability_gp[0]]\n\nfor x in disability_gp[1:]:\n    df = train[train[x] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\n    df.columns = ['Date', x]\n    disability_df = pd.merge(disability_df, df)\n\ndisability_df = disability_df.melt('Date', var_name='category', value_name='Count').sort_values('Date')\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['Date'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q',\n    color='category:N'\n).properties(title=\"Counts of disability related toxic comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='Date:T',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'Count:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='Date:T',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=disability_df, width=WIDTH, height=HEIGHT))","13840e47":"race_gp = ['asian', 'black', 'latino', 'other_race_or_ethnicity', 'white']\nrace_df = train[train[race_gp[0]] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\nrace_df.columns = ['Date', race_gp[0]]\n\nfor x in race_gp[1:]:\n    df = train[train[x] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\n    df.columns = ['Date', x]\n    race_df = pd.merge(race_df, df)\n\nrace_df = race_df.melt('Date', var_name='category', value_name='Count').sort_values('Date')\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['Date'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q',\n    color='category:N'\n).properties(title=\"Counts of race related toxic comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='Date:T',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'Count:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='Date:T',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=race_df, width=WIDTH, height=HEIGHT))","6887c4d9":"religion_gp = ['atheist', 'buddhist', 'christian', 'hindu', 'jewish', 'muslim', 'other_religion']\nreligion_df = train[train[religion_gp[0]] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\nreligion_df.columns = ['Date', religion_gp[0]]\n\nfor x in religion_gp[1:]:\n    df = train[train[x] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\n    df.columns = ['Date', x]\n    religion_df = pd.merge(religion_df, df)\n\nreligion_df = religion_df.melt('Date', var_name='category', value_name='Count').sort_values('Date')\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['Date'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q',\n    color='category:N'\n).properties(title=\"Counts of religion related toxic comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='Date:T',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'Count:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='Date:T',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=religion_df, width=WIDTH, height=HEIGHT))","20e11d9f":"gender_gp = ['bisexual', 'female', 'heterosexual',  'homosexual_gay_or_lesbian', 'male', 'other_gender', 'other_sexual_orientation', 'transgender']\ngender_df = train[train[gender_gp[0]] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\ngender_df.columns = ['Date', gender_gp[0]]\n\nfor x in gender_gp[1:]:\n    df = train[train[x] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\n    df.columns = ['Date', x]\n    gender_df = pd.merge(gender_df, df)\n\ngender_df = gender_df.melt('Date', var_name='category', value_name='Count').sort_values('Date')\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['Date'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q',\n    color='category:N'\n).properties(title=\"Counts of gender & orientation related toxic comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='Date:T',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'Count:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='Date:T',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=gender_df, width=WIDTH, height=HEIGHT))","e0cdca89":"toxicity_gp = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat' , 'sexual_explicit']\ntoxicity_df = train[train[toxicity_gp[0]] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\ntoxicity_df.columns = ['Date', toxicity_gp[0]]\n\nfor x in toxicity_gp[1:]:\n    df = train[train[x] > 0].sort_values('created_date').groupby('created_date', as_index=False).agg({'id':['count']})\n    df.columns = ['Date', x]\n    toxicity_df = pd.merge(toxicity_df, df)\n\ntoxicity_df = toxicity_df.melt('Date', var_name='category', value_name='Count').sort_values('Date')\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['Date'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line(interpolate='basis').encode(\n    x='Date:T',\n    y='Count:Q',\n    color='category:N'\n).properties(title=\"Counts of toxic type comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='Date:T',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'Count:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='Date:T',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=toxicity_df, width=WIDTH, height=HEIGHT))","3cfcf3c4":"# Got this from here https:\/\/www.kaggle.com\/artgor\/toxicity-eda-model-interpretation-and-more\/data\ntrain['disability'] = train['intellectual_or_learning_disability'] + train['other_disability'] + train['physical_disability'] + train['psychiatric_or_mental_illness']\ntrain['race'] = train['asian'] + train['black'] + train['latino'] + train['other_race_or_ethnicity'] + train['white']\ntrain['religion'] = train['atheist'] + train['buddhist'] + train['christian'] + train['hindu'] + train['jewish'] + train['muslim'] + train['other_religion']\ntrain['gender'] = train['bisexual'] + train['female'] + train['heterosexual'] + train['homosexual_gay_or_lesbian'] + train['male'] + train['other_gender'] + train['other_sexual_orientation'] + train['transgender']\ntrain['type'] = train['severe_toxicity'] + train['obscene'] + train['identity_attack'] + train['insult'] + train['threat'] + train['sexual_explicit']\nsuper_groups_gp = ['disability', 'race', 'religion', 'gender', 'type']\nplot_dict = {}\nbins = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.40, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.80, 0.85, 0.9, 0.95, 1]\n\nfor col in super_groups_gp:\n    df_ = train.loc[train[col] > 0]\n    hist_df = pd.cut(df_[col], bins).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n    hist_df['bins'] = hist_df['bins'].astype(str)\n    plot_dict[col] = alt.Chart(hist_df).mark_bar().encode(\n        x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n        y=alt.Y(f'{col}:Q', axis=alt.Axis(title='Count')),\n        tooltip=[col, 'bins']\n    ).properties(title=f\"Distribution of {col}\", width=300, height=200).interactive()\n    \nrender((plot_dict['disability'] | plot_dict['race']) & (plot_dict['religion'] | plot_dict['gender']) & (plot_dict['type']))","f5e9600c":"eng_stopwords = stopwords.words('english')\ntrain['comment_text'] = train['comment_text'].apply(lambda x: x.lower())\n# train['comment_text'] = train['comment_text'].str.replace('[^\\w\\s]','')\ntrain['comment_text'] = train['comment_text'].str.replace('[^a-z ]','')\ntrain['comment_text'] = train['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in eng_stopwords))\n\nfreq_dist = FreqDist([word for comment in train[train['target'] > 0.8]['comment_text'] for word in comment.split()])\n\nplt.figure(figsize=(20, 6))\nplt.title('Word frequency on toxic comments').set_fontsize(20)\nfreq_dist.plot(60, marker='.', markersize=10)\nplt.show()","c684de37":"wc_stopwords = set(STOPWORDS)\ndef plot_WordCloud(group, mask_path):\n    mask = np.array(Image.open(mask_path))[:,:,1]\n    text = train[train[group] > 0.8 ]['comment_text'].values\n    wc = WordCloud(background_color=\"white\", max_words=2000, mask=mask,\n                   stopwords=wc_stopwords, contour_width=3, contour_color='steelblue')\n\n    wc.generate(\" \".join(text))\n\n    plt.figure(figsize=(6, 6))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n\nplot_WordCloud(group='type', mask_path=\"..\/input\/toxicity-comments-images\/toxic1.png\")","af8312b9":"plot_WordCloud(group='race', mask_path=\"..\/input\/toxicity-comments-images\/race.png\")","bd093fa9":"plot_WordCloud(group='disability', mask_path=\"..\/input\/toxicity-comments-images\/disability3.png\")","368aa9e2":"plot_WordCloud(group='gender', mask_path=\"..\/input\/toxicity-comments-images\/gender2.png\")","31a9d584":"plot_WordCloud(group='religion', mask_path=\"..\/input\/toxicity-comments-images\/religion1.png\")","1dcb6c48":"annotators_len_df = train[train['identity_annotator_count'] > 0].groupby('word_count', as_index=False).agg({'identity_annotator_count': 'mean'})\nannotators_len_df.columns = ['word_count', 'mean_annotators']\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['word_count'], empty='none')\n\n# The basic line\nline = alt.Chart().mark_line().encode(\n    y='mean_annotators:Q',\n    x='word_count:Q'\n).properties(title=\"Counts of toxic type comments\")\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = alt.Chart().mark_point().encode(\n    x='word_count:Q',\n    opacity=alt.value(0),\n).add_selection(\n    nearest\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align='left', dx=5, dy=-5).encode(\n    text=alt.condition(nearest, 'mean_annotators:Q', alt.value(' '))\n)\n\n# Draw a rule at the location of the selection\nrules = alt.Chart().mark_rule(color='gray').encode(\n    x='word_count:Q',\n).transform_filter(\n    nearest\n)\n\n# Put the five layers into a chart and bind the data\nrender(alt.layer(line, selectors, points, rules, text, data=annotators_len_df, width=WIDTH, height=HEIGHT))","4c5cb86c":"X_train = train['comment_text'].astype(str)\nX_test = test['comment_text'].astype(str)\ny = np.where(train['target'] >= 0.5, True, False) * 1","265a1579":"num_words = 20000\nmax_len = 150\nemb_size = 128","55a71277":"tok = Tokenizer(num_words = num_words)\ntok.fit_on_texts(list(X_train))","cd2d0999":"X = tok.texts_to_sequences(X_train)\ntest = tok.texts_to_sequences(X_test)","641dc59d":"X = sequence.pad_sequences(X, maxlen=max_len)\nX_test = sequence.pad_sequences(test, maxlen=max_len)","9d5963f8":"# Got this model here: https:\/\/www.kaggle.com\/lsjsj92\/toxic-simple-eda-and-modeling-with-lstm\ninp = Input(shape = (max_len, ))\nlayer = Embedding(num_words, emb_size)(inp)\nlayer = Bidirectional(LSTM(50, return_sequences=True, recurrent_dropout=0.15))(layer)\nlayer = GlobalMaxPool1D()(layer)\nlayer = Dropout(0.5)(layer)\nlayer = Dense(100, activation='relu')(layer)\nlayer = Dropout(0.5)(layer)\nlayer = Dense(1, activation='sigmoid')(layer)\n\nmodel = Model(inputs=inp, outputs=layer)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","a7f7d8bf":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory = model.fit(X, y, batch_size=1024, epochs=20, validation_split=0.2, callbacks=[es])","6049d86f":"sns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 8))\n\nax1.plot(history.history['acc'], label='Train Accuracy')\nax1.plot(history.history['val_acc'], label='Validation accuracy')\nax1.legend(loc='best')\nax1.set_title('Accuracy')\n\nax2.plot(history.history['loss'], label='Train loss')\nax2.plot(history.history['val_loss'], label='Validation loss')\nax2.legend(loc='best')\nax2.set_title('Loss')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","6820f653":"Y_test = model.predict(X_test)","79a5491e":"submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\nsubmission['prediction'] = Y_test\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","fd3f9856":"<h1><center>Jigsaw Unintended Bias in Toxicity Classification<\/center><\/h1>\n<h2><center>Detect toxicity across a diverse range of conversations<\/center><\/h2>\n\n#### In this competition we need to detect toxic comments and minimize unintended model bias, for that we to optimize a metric designed to measure unintended bias, and we are being provided a ~2 million dataset labeled for identity mentions\n\n#### Problem background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where, unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n\n##### Content:\n- [Data overview](#First-let's-take-a-quick-look-at-the-data)\n- [EDA](#EDA)\n- [Target distribution](#Target-distribution-[toxicity])\n- [Toxicity overtime](#Let's-take-a-look-at-how-the-comment-toxicity-behaves-overtime)\n- [Toxicity by group](#Toxicity-rate-dsitribution-for-each-super-group-[race,-religion,-type,-etc...])\n- [Top 60 Most frequent word on toxic comments](#Top-60-Most-frequent-word-on-toxic-comments)\n- [Toxic comments word clouds](#Toxic-comments-frequency)\n- [Hypotesis validation](#Hypotesis)\n\n\n\n\n","f9bfe305":"Most of the sub-groups comment count increase overtime then drop close to the end, also its good to know which specific kind has higher or lower count.\n\n### Toxicity rate dsitribution for each super group [race, religion, type, etc...]","ffe81289":"### Disability toxic comments frequency","0ce50028":"### Load data","11930835":"### Racial toxic comments frequency","c3de5a32":"So it seems that the word count doens't really matter, most comments that have annotators have a mean around 4, with some spikes.\n\n## Model\n\n- [From this article](https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/)\n> Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n> \n> In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.","0671076b":"As one would expect the data is heavily imbalanced, this makes sense since it mirror the real world.\n\n### Let's take a look at how the comment toxicity behaves overtime","6a11991a":"### Gender toxic comments frequency","5008a351":"### Toxic comments frequency","4af623e7":"### Distribution of comment length","02e33258":"### Number of toxic comments of each type","b3168172":"Comments related to disability seem to be less toxic than the other groups.\n\n### Would be nice to see the distribution of the actual words used on the comments, this may give us some insights.\n\n### Top 60 Most frequent word on toxic comments","92523a97":"### Dependencies","0dc3c262":"- Most of the comments are small (less than 200 characters)\n- Also in regard of comment length and word count, train and test sets have a similar distribution.\n\n### Target distribution [toxicity]","2977e6e9":"- Most of the comments are not toxic\n- On the toxic comments most of them are not \"highly toxic\" (target around 0.2)\n\n### Target distribution (toxic and non-toxic (toxic >= 0.5))","f0fc8f21":"## Hypotesis\n- When we are dealing with a database it's always good to validate any hypotesis that we might have and could impact the problem.\n\n### Does smaller comments have more raters?\n- At first I thought that smaller comments may have more raters as they are simpler and faster to rate, this may increase their rating confidence, lets see.","1d3bdc6c":"### Religion toxic comments frequency","e2917caa":"## EDA","48f92a6c":"We can see a peak at 1000 length, maybe this is because of where data was sampled from (Twitter have max size of characters), this happen both on train and test set.\n\n### Distribution of comment word count","eaafacb0":"> - Red = toxicity rate\n> - Blue = Comment count\n\n- One interesting point is that on November 2015 the comments were mostly non-toxic, but after that the toxicity just increased and remained stable after July 2016.\n- As we can see the comment count increased as time increase but droped after October 2017, maybe some of those comments are on the test set.\n\n### Now let's see on a more detailed level, looking at each group.\n\n### Number of toxic comments for each group","4f221987":"### First let's take a quick look at the data"}}