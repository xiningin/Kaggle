{"cell_type":{"e592d489":"code","4e539841":"code","027a9be4":"code","81385c1e":"code","fe4163c8":"code","5404ee13":"code","df86d5ac":"code","375e3c90":"code","8c66626b":"code","0941c625":"code","6d083af6":"code","7b19efbe":"code","0b6517b4":"code","05019f28":"code","9ceed83b":"code","e0e7da3a":"code","9bb8b0fe":"code","b1f2e99e":"code","477d0f07":"code","bfceb442":"markdown","9c153ba8":"markdown","8e56641c":"markdown","89e606f8":"markdown","d5b0d962":"markdown","c0f70d52":"markdown"},"source":{"e592d489":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2\nfrom sklearn.neighbors import KNeighborsClassifier\n","4e539841":"df=pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')","027a9be4":"df.head()\n","81385c1e":"df.columns","fe4163c8":"df.isnull().values.any()","5404ee13":"X = df.iloc[:,1:]\ny = df.iloc[:,0]\nprint(y.shape)\nbf=SelectKBest(score_func=chi2, k=10)\nfit=bf.fit(X,y)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","df86d5ac":"u=df[['Bankrupt?',' Fixed Assets to Assets',' Cash\/Current Liability',' Net Value Growth Rate',' Fixed Assets Turnover Frequency',' Revenue per person',' Total assets to GNP price',' Quick Ratio',' Quick Asset Turnover Rate',' Total Asset Growth Rate',' Research and development expense rate']]\ndff=pd.DataFrame(u)\ndff","375e3c90":"#Analysis","8c66626b":"sns.countplot(data=dff,x='Bankrupt?')","0941c625":"x = dff.iloc[:,0].values.reshape(-1,1)\ny=dff.iloc[:,1:].values","6d083af6":"data_x=dff.iloc[:,0]","7b19efbe":"from collections import Counter \nprint(Counter(data_x))","0b6517b4":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n\nover_sample=RandomOverSampler()\nX_ros, y_ros=over_sample.fit_resample(y,x)\nprint(Counter(y_ros))\n","05019f28":"print(X_ros.shape)\nprint(y_ros.shape)","9ceed83b":"print(y_ros.reshape(-1,1).shape)","e0e7da3a":"dff.hist(figsize=(20,20),edgecolor='white')\nplt.show()","9bb8b0fe":"X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, random_state=76)","b1f2e99e":"print(X_train.shape)\nprint(y_train.shape)","477d0f07":"best_n = 0\nbest_training = 0\nbest_test = 0\n\nfor i in range(1,10):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    \n    training = knn.score(X_train, y_train)\n    test = knn.score(X_test, y_test)\n    \n    if test > best_test:\n        best_n = i\n        best_training = training\n        best_test = test\n\nprint(\"best number of neighbors: {}\".format(best_n))\nprint(\"best training set score : {:.3f}\".format(best_training))\nprint(\"best test set score: {:.3f}\".format(best_test))","bfceb442":"# Random OverSampling\n\nRandom Oversampling includes selecting random examples from the minority class with replacement and supplementing the training data with multiple copies of this instance, hence it is possible that a single instance may be selected multiple times.","9c153ba8":"# Feature Selection \n\nUnivariate Selection\n\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\n\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nThe example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Mobile Price Range Prediction Dataset.","8e56641c":"# Target Variable ","89e606f8":"he Imbalanced classification problem is what we face when there is a severe skew in the class distribution of our training data. Okay, the skew may not be extremely severe (it can vary), but the reason we identify imbalanced classification as a problem is because it can influence the performance on our Machine Learning algorithms.\n\nOne way the imbalance may affect our Machine Learning algorithm is when our algorithm completely ignores the minority class. The reason this is an issue is because the minority class is often the class that we are most interested in. For instance, when building a classifier to classify fraudulent and non-fraudulent transactions from various observations, the data is likely to have more non-fraudulent transactions than that of fraud \u2014 I mean think about it, it would be very worrying if we had an equal amount of fraudulent transactions as non-fraud.","d5b0d962":"# Training KNN Model ","c0f70d52":"I hope you understand this notebook . \nPlease don't forget to upvote . It's boost the self confident and also motivate to more work on the kaggle platform . \nFeel free to give any kinds of suggestion . \n\n\nHAVE A NICE DAY ! \n"}}