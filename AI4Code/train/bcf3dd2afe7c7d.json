{"cell_type":{"6b5a11fb":"code","5dac3851":"code","360ebaa3":"code","e82db760":"code","e659b6aa":"code","5311e88e":"code","dcac0827":"code","27dd6722":"code","e159e709":"code","467ea048":"code","876bc69e":"code","1b4afa84":"code","da6eed3d":"code","19889a8e":"code","90fc02c7":"code","75f7ecc2":"code","bb22b86d":"code","e73ef5ee":"code","357a3828":"code","7dadeed4":"code","3a6bcb83":"code","4da5f9c4":"code","1a1640da":"code","33adf046":"code","fc42f97c":"code","4b92cfa2":"code","695d4709":"code","b8990afa":"code","d57653da":"code","d2aea028":"code","e3e14153":"code","10e92ee2":"code","ba129f4d":"code","a5841c83":"code","b2288813":"code","61cd9268":"code","68b28cc0":"code","8883247f":"code","0ff21864":"code","c48ce4c3":"code","24e28c2a":"code","d6df3286":"code","d64ff79a":"code","ed527090":"code","c4f68dda":"code","e3e6d3c8":"code","a9a2c22c":"code","ebbc36d2":"code","71980278":"code","9d92ab7d":"code","ccb09290":"code","2aca4e7c":"code","202fa1a4":"code","9e770720":"code","775d7444":"code","510645c6":"code","dbb52962":"code","0c1ec8e6":"code","e2032172":"code","d5c2d05b":"code","91759ceb":"code","e16c8ce0":"code","2984f4e0":"code","cfb69a3f":"code","c569b0fd":"code","ef9c8907":"code","29972eef":"code","48d477e3":"code","253d3e14":"code","d281f314":"code","99990e0d":"code","e1c648d9":"code","aaa22cb6":"code","c6695a4a":"code","e8c2ddbf":"code","7e9544cd":"code","f185aabf":"code","0d3f3c1c":"code","8f96eca9":"code","6e1e862c":"code","6f34acbf":"code","8a61dbe6":"code","12ff1a76":"code","e3a4f717":"code","9e3abb49":"code","b496a7ac":"code","c048ac14":"code","f47246ce":"code","fabe8590":"code","b767ce23":"code","6f219d18":"code","9887ff97":"code","6d62a9c6":"code","a30e6afd":"code","7ffdd835":"code","f8d20073":"code","2c56b9c7":"code","391d9a6b":"code","175c4251":"code","ad63a4df":"code","1f1366be":"code","f4337f78":"code","eb9f7955":"code","174c5f44":"markdown","596c927c":"markdown","8ae078d6":"markdown","8600c3c5":"markdown","6ab3d6c1":"markdown","815241a5":"markdown","ea77512f":"markdown","3797df4e":"markdown","08f699ab":"markdown","f1354b7d":"markdown","1aab8efb":"markdown","82cd8cce":"markdown","93bed819":"markdown","632b3137":"markdown"},"source":{"6b5a11fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sea #for visualization\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 15\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Any results you write to the current directory are saved as output.p","5dac3851":"#let's look at all available files:\nimport os\nprint(os.listdir(\"..\/input\"))","360ebaa3":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint (\"Train Dataset: Rows, Columns: \", train_df.shape)\nprint (\"Test Dataset: Rows, Columns: \", test_df.shape)","e82db760":"#the prediction will be based on the head of the household\nsubmit = test_df[['Id','idhogar']]\n#https:\/\/www.geeksforgeeks.org\/different-ways-to-create-pandas-dataframe\/","e659b6aa":"# a glimpse at train_df\ntrain_df.head(5)","5311e88e":"train_df.info()","dcac0827":"#First, let's deal with non-numeric columns\ntrain_df.select_dtypes(['object']).head(15)","27dd6722":"#Id and idhogar won't be used for training so we'll take care of them later\n#1. 'dependency'\ntrain_df['dependency'].value_counts(ascending=False)","e159e709":"#Notice there is a column containing squared values for dependency, 'SQBdependency'. \n#see what are its analogs to 'yes' and 'no' of 'dependency':\nprint (train_df.loc[train_df['dependency']=='no',['SQBdependency']]['SQBdependency'].value_counts())\nprint (train_df.loc[train_df['dependency']=='yes',['SQBdependency']]['SQBdependency'].value_counts())\n","467ea048":"#Convert 'yes' to 1 and 'no' to 0\ntrain_df['dependency'] = train_df['dependency'].replace(('yes', 'no'), (1, 0))\ntest_df['dependency'] = test_df['dependency'].replace(('yes', 'no'), (1, 0))\ntrain_df['dependency']=train_df['dependency'].astype(float)\ntest_df['dependency']=test_df['dependency'].astype(float)","876bc69e":"#2 and #3 'edjefe'\/'edjefa'\ntrain_df['edjefe'].value_counts()\n#edjefe, years of education of male head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","1b4afa84":"train_df['edjefa'].value_counts()\n#edjefa, years of education of female head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","da6eed3d":"#Again, correlate 'edjefe' with 'SQBedjefe'(squared value)\nprint (train_df.loc[train_df['edjefe']=='no',['SQBedjefe']]['SQBedjefe'].value_counts())\nprint (train_df.loc[train_df['edjefe']=='yes',['SQBedjefe']]['SQBedjefe'].value_counts())","19889a8e":"#Based on 'SQBedjefe' column, convert 'no' to 0 and 'yes' to 1 to make the rows of 'edjefa'\/'edjefe' numeric\ntrain_df['edjefa'] = train_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntrain_df['edjefe'] = train_df['edjefe'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefa'] = test_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefe'] = test_df['edjefe'].replace(('yes', 'no'), (1, 0))","90fc02c7":"#converting these object type columns to floats\ntrain_df['edjefa']=train_df['edjefa'].astype(float)\ntrain_df['edjefe']=train_df['edjefe'].astype(float)\ntest_df['edjefa']=test_df['edjefa'].astype(float)\ntest_df['edjefe']=test_df['edjefe'].astype(float)\n","75f7ecc2":"#double checking that all columns are now numeric - except for Id and idhogar\nprint (train_df.select_dtypes(['object']).describe(), '\\n')\nprint (test_df.select_dtypes(['object']).describe())","bb22b86d":"#Now let's take care of the missing columns\nprint (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()\n\n","e73ef5ee":"print (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()","357a3828":"#1 'v18q1' - number of tablets household owns\ntrain_df.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","7dadeed4":"#Every family that has nan for v18q1 does not own a tablet. \n#Therefore, we can fill in this missing value with zero.\ntrain_df['v18q1'] = train_df['v18q1'].fillna(0)\ntest_df['v18q1'] = test_df['v18q1'].fillna(0)","3a6bcb83":"#2 'rez_esc' - Years behind in school \n#let's see if high percentage of missing values in 'rez_esc' accounts for minors and people without education\nprint (train_df.loc[train_df['rez_esc'].isnull()]['age'].value_counts().head(6))\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel1'].value_counts())\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel2'].value_counts())","4da5f9c4":"#another theory is that those 'na' are for individuals outside of school age\nprint (train_df.loc[train_df['rez_esc'].notnull()]['age'].describe())","1a1640da":"#which is actually true: min age - 7, max age - 17. Assigning '0' to those people\ntrain_df['rez_esc'] = train_df['rez_esc'].fillna(0)\ntest_df['rez_esc'] = test_df['rez_esc'].fillna(0)\ntrain_df.loc[train_df['rez_esc'] > 5, 'rez_esc'] = 5\ntest_df.loc[test_df['rez_esc'] > 5, 'rez_esc'] = 5 #5 is a maximum value per competition's discussion, so here we're accounting for the outliers","33adf046":"#3 v2a1, Monthly rent payment\nprint(train_df['v2a1'].isnull().sum())\n","fc42f97c":"#Let's correlate it with tipovivi1, =1 own and fully paid house\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())\nprint(train_df['tipovivi1'].value_counts())\n","4b92cfa2":"#Replacing with '0' na for fully paid house \ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi1'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi1'] == 1), 'v2a1'] = 0","695d4709":"print (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())","b8990afa":"#tipovivi2, \"=1 own,  paying in installments\"\n#tipovivi3, =1 rented\n#tipovivi4, =1 precarious\n#tipovivi5, \"=1 other(assigned,  borrowed)\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi2'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi3'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi4'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi5'].value_counts())","d57653da":"#Let's replace na for precarious with '0' as well\ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi4'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi4'] == 1), 'v2a1'] = 0","d2aea028":"print (train_df.loc[train_df['v2a1'].isnull()]['Target'].value_counts())","e3e14153":"#see if we can find a feature to correlate with those remaining missing values\nv2a1_na_corr = train_df\nv2a1_na_corr.v2a1.where(v2a1_na_corr.v2a1.isnull(), 1, inplace=True)\nv2a1_na_corr['v2a1'].fillna(0, inplace = True)\nprint (v2a1_na_corr.corr()['v2a1'].sort_values())","10e92ee2":"#No luck. But since the property is 'assigned, borrowed', let's assume there's no monthly rent associated with it\ntrain_df['v2a1'].fillna(train_df['v2a1'].mean(), inplace = True)\ntest_df['v2a1'].fillna(test_df['v2a1'].mean(), inplace = True)","ba129f4d":"print (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())\nprint (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())","a5841c83":"#the rest of the missing values can be replaced with mean as their percentage towards total number of entries is insignificant\ntrain_df.fillna (train_df.mean(), inplace = True)\ntest_df.fillna(test_df.mean(), inplace = True)","b2288813":"print ('Columns having missing values:')\nprint (train_df.columns[train_df.isnull().any()])\nprint (test_df.columns[test_df.isnull().any()])","61cd9268":"#top 30 features with best correlation to 'Target'\nbest_correlations = train_df.corr()['Target'].abs().sort_values().tail(30)\ntype(best_correlations)\nbest_correlations","68b28cc0":"best_correlation = best_correlations.index\nbest_correlation","8883247f":"d = {'dependency':'dependency, Dependency rate', 'v18q1':'v18q1, number of tablets household owns', 'epared1':'epared1, if walls are bad', 'qmobilephone':'qmobilephone, # of mobile phones', \n     'pisocemento':'pisocemento, =1 if predominant material on the floor is cement',\n       'eviv1':'eviv1, =1 if floor are bad', 'instlevel8':'instlevel8, =1 undergraduate and higher education', 'rooms':'rooms,  number of all rooms in the house', 'r4h1':'r4h1, Males younger than 12 years of age', \n        'v18q': 'v18q, owns a tablet:', 'edjefe':'edjefe, years of education of male head of household', 'SQBedjefe':'SQBedjefe, years of education of male head of household squared',\n       'etecho3':'etecho3, =1 if roof are good', 'r4m1':'r4m1, Females younger than 12 years of age', 'SQBovercrowding':'SQBovercrowding, overcrowding squared', \n       'paredblolad':'paredblolad, =1 if predominant material on the outside wall is block or brick', 'SQBmeaned':'SQBmeaned, square of the mean years of education of adults (>=18) in the household',\n       'pisomoscer':'pisomoscer, \"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"', 'overcrowding':'overcrowding, # persons per room', 'epared3':'epared3, =1 if walls are good',\n        'eviv3':'eviv3, =1 if floor are good', 'SQBescolari' :'SQBescolari, years of schooling squared',\n       'escolari':'escolari, years of schooling', 'cielorazo':'cielorazo, =1 if the house has ceiling', 'SQBhogar_nin':'SQBhogar_nin, Number of children 0 to 19 in household, squared',\n        'r4t1':'r4t1, persons younger than 12 years of age', 'hogar_nin':'hogar_nin, Number of children 0 to 19 in household',\n       'meaneduc':'meaneduc,average years of education for adults (18+)', 'Target':'Target', 'elimbasu5':'elimbasu5, \"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"'}","0ff21864":"for i in best_correlation:\n    if len(train_df[i].unique())>2:\n        sea.boxplot(train_df[i])\n        plt.xlabel(d.get(i))\n        plt.show()\n        sea.distplot(train_df[i])\n        plt.xlabel(d.get(i))\n        plt.show()","c48ce4c3":"#we'll drop only the ones with less than 100 outliers\nprint(len(train_df.loc[(train_df['SQBmeaned']>900)]))\nprint(train_df['SQBmeaned'].value_counts(sort = True))","24e28c2a":"to_drop = train_df.loc[(train_df['rooms']>9)|(train_df['r4m1']>3)|\n                       (train_df['r4t1']>5)|(train_df['hogar_nin']>6)|\n                       (train_df['meaneduc']>25)|\n                       (train_df['qmobilephone']>8)|(train_df['r4h1']>2)|\n                       (train_df['SQBedjefe']>300)|(train_df['SQBescolari']>300)|\n                       (train_df['SQBhogar_nin']>70)|(train_df['SQBovercrowding']>25)|\n                       (train_df['SQBmeaned']>900)].index","d6df3286":"len(to_drop)","d64ff79a":"train_df.drop(to_drop, inplace=True)","ed527090":"train_df.groupby('Target').mean()","c4f68dda":"train_df['Target'].hist()","e3e6d3c8":"#features with <5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())<5:\n        sea.countplot(x=j, hue='Target', data=train_df)\n        plt.xlabel(d.get(j))\n        plt.ylabel(\"Count\")\n        #plt.title(str(j),' vs Target') \n        plt.figure()\n        plt.show()","a9a2c22c":"#if target distribution in each feature cathegory is similar to overall target distribution, \n#then the chances are that the feature will have a better correlation to Target\n#I tried to combine a couple of features to produce better distributions\/correlations\ntrain_df['v18q+etecho3'] = train_df['v18q']+train_df['etecho3']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['etecho3'])\nprint (train_df.corr()['Target']['v18q+etecho3'])\ntest_df['v18q+etecho3'] = test_df['v18q']+test_df['etecho3']","ebbc36d2":"train_df['v18q+paredblolad'] = train_df['v18q']+train_df['paredblolad']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['paredblolad'])\nprint (train_df.corr()['Target']['v18q+paredblolad'])\ntest_df['v18q+paredblolad'] = test_df['v18q']+test_df['paredblolad']","71980278":"train_df['v18q+pisomoscer'] = train_df['v18q']+train_df['pisomoscer']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['v18q+pisomoscer'])\ntest_df['v18q+pisomoscer'] = test_df['v18q']+test_df['pisomoscer']","9d92ab7d":"train_df['pisomoscer+instlevel8'] = train_df['pisomoscer']+train_df['instlevel8']\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['instlevel8'])\nprint (train_df.corr()['Target']['pisomoscer+instlevel8'])\ntest_df['pisomoscer+instlevel8'] = test_df['pisomoscer']+test_df['instlevel8']","ccb09290":"def plot_distribution(df, var, target, **kwargs):\n    row = kwargs.get('row', None)\n    col = kwargs.get('col', None)\n    facet = sea.FacetGrid(df, hue = target, size=4.0, aspect=1.3, sharex=False, sharey=False)\n    facet.map(sea.kdeplot, var)\n    facet.set(xlim = (0, df[var].max()))\n    facet.add_legend()\n    plt.xlabel(d.get(j))\n    plt.show()","2aca4e7c":"#features with >=5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())>5:\n        plot_distribution(train_df, j, 'Target')\n\n#In the first graph instead of 0's should be nulls(we changed these before). So there is no info about monthly rate payment for non vulnerable households ","202fa1a4":"#following the same logic, let's try to combine features to get a better distribution\ntrain_df['edjefe+escolari'] = train_df['edjefe']+train_df['escolari']\nprint (train_df.corr()['Target']['edjefe'])\nprint (train_df.corr()['Target']['escolari'])\nprint (train_df.corr()['Target']['edjefe+escolari'])\ntest_df['edjefe+escolari'] = test_df['edjefe']+test_df['escolari']","9e770720":"#we can also do some pairwise feature comparison for various target cathegories with jointplots:\ndict={4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"}\nfor i in range(1, 5):\n    sea.set(font_scale=1, style=\"white\")\n    sea_jointplot = sea.jointplot('hogar_nin', 'age', data=train_df[train_df['Target'] == i], size=6,color = sea.color_palette(\"deep\")[i], kind='kde', stat_func=None)\n    plt.title(dict.get(i))\nplt.show()\n","775d7444":"#finally, let's have some interactive plots as well - using plotly\n# Standard plotly imports\npip install plotly chart-studio\nimport chart_studio.plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)","510645c6":"train_df['Target1'] = train_df.Target\ntrain_df.Target1 = train_df.Target1.apply(str)\ntrain_df.iplot(\n    x='hogar_nin',\n    y='meaneduc',\n    # Specify the category\n    categories=\"Target1\",\n    xTitle = d.get('hogar_nin'),\n    yTitle = d.get('meaneduc'),\n    title=\"Number of children vs. Average education by Poverty level\")\ntrain_df = train_df.drop(['Target1'], axis =1)","dbb52962":"train_df.pivot(columns='Target', values='meaneduc').iplot(\n        kind='box',\n        xTitle = d.get('meaneduc'),\n        yTitle= 'Target',\n        title='Education level per different poverty groups')","0c1ec8e6":"trace1 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['meaneduc'],\n    name=d.get('meaneduc')\n)\ntrace2 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['hogar_nin'],\n    name=d.get('hogar_nin')\n)\ntrace3 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['rooms'],\n    name=d.get('rooms')\n)\ndata = [trace2, trace3,trace1]\n\n\nlayout = go.Layout(\n    barmode=\"group\",\n    hovermode= 'closest',\n    showlegend= True,\n    xaxis ={\"title\":\"Target\"},\n    yaxis ={\"title\":\"Count\"}\n    \n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","e2032172":"best_correlation_df = train_df[['Target']]\nfor i in best_correlation:\n    if len(train_df[i].unique())>2:\n        best_correlation_df[i] = train_df[i]\n#Correlation Heatmap\ncorrs = best_correlation_df.corr()\ncorrs.style.background_gradient(cmap='coolwarm').set_precision(2)","d5c2d05b":"sea.clustermap(corrs)","91759ceb":"c1 = corrs.abs().unstack().drop_duplicates()\nc1.sort_values(ascending = True)","e16c8ce0":"train_df_sample = train_df.sample(500) #sampling for better graph\nsea.set(rc={'figure.figsize':(12,8)})\nsea.swarmplot(x='rooms', y = 'hogar_nin', hue='Target', dodge = True, data=train_df_sample, size = 4)\n#sea.violinplot(x='r4t1', y = 'hogar_nin', hue='Target', dodge = True, data=train_df_sample, size = 4)\nplt.xlabel(d.get('rooms'))\nplt.ylabel(d.get('hogar_nin'))\nplt.figure()\nplt.show()\n","2984f4e0":"#some of it has already been done in visualization part above\n#poor materials used\ntrain_df[\"Poor_materials\"]=train_df['pareddes']+train_df['paredfibras']+train_df['pisonatur']+train_df['pisonotiene']+train_df['techocane']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']\ntest_df[\"Poor_materials\"]=test_df['pareddes']+test_df['paredfibras']+test_df['pisonatur']+test_df['pisonotiene']+test_df['techocane']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']\nprint ('Pearson correlation coefficients:')\nprint ('Poor Materials (training set): ',train_df['Poor_materials'].corr( train_df['Target']))","cfb69a3f":"#rich materials used\ntrain_df[\"Rich_Materials\"]=train_df['paredblolad']+train_df['pisomoscer']+train_df['techoentrepiso']+train_df['techootro']+train_df['cielorazo']+train_df['epared3']+train_df['etecho3']+train_df['eviv3']\ntest_df[\"Rich_Materials\"]=test_df['paredblolad']+test_df['pisomoscer']+test_df['techoentrepiso']+test_df['techootro']+test_df['cielorazo']+test_df['epared3']+test_df['etecho3']+test_df['eviv3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Rich_Materials'].corr( train_df['Target']))","c569b0fd":"train_df[\"Poor_Infrastructure\"]=train_df['abastaguano']+train_df['noelec']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']+train_df['lugar3']+train_df['sanitario1']+train_df['energcocinar1']+train_df['elimbasu3']\ntest_df[\"Poor_Infrastructure\"]=test_df['abastaguano']+test_df['noelec']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']+test_df['lugar3']+test_df['sanitario1']+test_df['energcocinar1']+test_df['elimbasu3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Poor_Infrastructure'].corr( train_df['Target']))","ef9c8907":"train_df[\"Good_Infrastructure\"]=train_df['sanitario2']+train_df['energcocinar2']+train_df['elimbasu1']+train_df['abastaguadentro']+train_df['planpri']+train_df['epared3']+train_df['etecho3']*(3)+train_df['eviv3']+train_df['lugar1']+train_df['lugar2']+train_df['lugar6']\ntest_df[\"Good_Infrastructure\"]=test_df['sanitario2']+test_df['energcocinar2']+test_df['elimbasu1']+test_df['abastaguadentro']+test_df['planpri']+test_df['epared3']+test_df['etecho3']*(3)+test_df['eviv3']+test_df['lugar1']+test_df['lugar2']+test_df['lugar6']\nprint ('Pearson correlation coefficients:')\nprint ('Infrastructure (training set): ',train_df['Good_Infrastructure'].corr( train_df['Target']))","29972eef":"#overcrowding + total of persons younger than 12 years of age + no level of education + zona rural\ntrain_df[\"overcrowding_total\"] = train_df[\"hacdor\"]+train_df[\"r4t1\"] +train_df[\"instlevel1\"] + train_df[\"area2\"]\ntest_df[\"overcrowding_total\"] = test_df[\"hacdor\"]+ test_df[\"r4t1\"] + test_df[\"instlevel1\"] + test_df[\"area2\"]\nprint ('overcrowding_total: ',train_df['overcrowding_total'].corr( train_df['Target']))","48d477e3":"#years of schooling + overcdrowding\ntrain_df[\"escolari+hacapo\"] = train_df[\"escolari\"]+train_df[\"hacapo\"]\ntest_df[\"escolari+hacapo\"] = test_df[\"escolari\"]+test_df[\"hacapo\"]\nprint (train_df['escolari+hacapo'].corr( train_df['Target']))","253d3e14":"print(train_df.columns[-30:])","d281f314":"pip install --upgrade https:\/\/github.com\/featuretools\/featuretools\/zipball\/master","99990e0d":"#credits to Will Koehrsen for his excellent kernel: https:\/\/www.kaggle.com\/willkoehrsen\/featuretools-for-good#Deep-Feature-Synthesis\n#first we need to define variable types in the dataframe (boolean vs. ordered vs. continuous) \n#and group them into individual vs. household categories\nind_bool = list()\nind_ordered = list()\nind_cont = list()\nhh_bool = list()\nhh_ordered = list()\nhh_cont = list()\nprint(train_df.drop(['age', 'SQBescolari','SQBage', 'agesq'], axis = 1).columns.get_loc('Target'))","e1c648d9":"train_list_hh = list(train_df.drop(['escolari', 'rez_esc'], axis = 1).loc[:,'v2a1':'eviv3'].columns)+list(train_df.loc[:,'idhogar':'meaneduc'].columns)+list(train_df.drop(['age', 'SQBescolari','SQBage', 'agesq','Target'], axis = 1).loc[:,'bedrooms':].columns)\ntrain_list_ind = [column for column in list(train_df.columns) if column not in set (train_list_hh)]\nprint (len(train_list_hh)+len(train_list_ind)-len(list(train_df.columns)))\nfor i in train_list_hh:\n    if len(train_df[i].unique())<=2:\n        hh_bool.append(i)\n    elif train_df[i].dtypes == 'int':\n        hh_ordered.append(i)\n    elif train_df[i].dtypes == 'float':\n        hh_cont.append(i)\n\nfor i in train_list_ind:\n    if len(train_df[i].unique())==2:\n        ind_bool.append(i)\n    elif train_df[i].dtypes == 'int':\n        ind_ordered.append(i)\n    elif train_df[i].dtypes == 'float':\n        ind_cont.append(i) \n","aaa22cb6":"test_df['Target'] = np.nan\n\ndata = train_df.append(test_df, sort = True)\nfor variable in (hh_bool + ind_bool):\n    data[variable] = data[variable].astype('bool')\nfor variable in (hh_cont + ind_cont):\n    data[variable] = data[variable].astype(float)\nfor variable in (hh_ordered + ind_ordered):\n    try:\n        data[variable] = data[variable].astype(int)\n    except Exception as e:\n        print(f'Could not convert {variable} because of missing values.')","c6695a4a":"import featuretools as ft\nes = ft.EntitySet(id = 'households') #creating the entity set\nes.entity_from_dataframe(entity_id = 'data',    #adding first entity (table) to the entity set\n                         dataframe = data, \n                         index = 'Id')\n\n'''hh = hh_bool+hh_ordered+hh_cont+[\"Target\"]+[\"idhogar\"]\nhousehold = data.loc[data['parentesco1']==1, hh]\nes.entity_from_dataframe(entity_id = 'household',    #adding second entity (table) to the entity set\n                         dataframe = household, \n                         index = \"idhogar\")\nhousehold_rl = ft.Relationship(es[\"household\"][\"idhogar\"],\n                              es[\"data\"][\"idhogar\"])\nes = es.add_relationship(household_rl)'''\n#we'll make a new entity by normalization of the original table \nes.normalize_entity(base_entity_id='data', \n                    new_entity_id='household', #adding household table to the entity set \n                    index = 'idhogar',\n                   additional_variables = hh_bool + hh_ordered + hh_cont+[\"Target\"])","e8c2ddbf":"feature_matrix, feature_names = ft.dfs(entityset=es, \n                                       target_entity = 'household', \n                                       max_depth = 2, \n                                       verbose = 1, \n                                       n_jobs = -1,\n                                       chunk_size = 100)","7e9544cd":"all_features = [str(x.get_name()) for x in feature_names]\nfeature_matrix.head()","f185aabf":"feature_matrix.shape","0d3f3c1c":"drop_cols = []\nfor col in feature_matrix:\n    if col == 'Target':\n        pass\n    else:\n        if 'Target' in col:\n            drop_cols.append(col)\n            \nprint(drop_cols)            \nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]         \nfeature_matrix.head()","8f96eca9":"train_df = feature_matrix[feature_matrix['Target'].notnull()].reset_index()\ntest_df = feature_matrix[feature_matrix['Target'].isnull()].reset_index()\ntest_df.head(5)","6e1e862c":"print(train_df.shape, test_df.shape)","6f34acbf":"idhogar = test_df['idhogar']\ntrain_df = train_df.select_dtypes(exclude=['object'])\ntest_df = test_df.select_dtypes(exclude=['object'])\ntrain_df = train_df.dropna(axis='columns')\ntest_df = test_df.dropna(axis='columns')","8a61dbe6":"print(train_df.shape, test_df.shape)","12ff1a76":"#Removing columns with greater than 99% correlation as redundant\n# Create correlation matrix\ncorr_matrix = train_df.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.99\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.99)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)\n\ntrain_df = train_df.drop(columns = to_drop)","e3a4f717":"print(train_df.shape, test_df.shape)","9e3abb49":"#let's compare all the correlation coefficients now\nprint (train_df.corr()['Target'].abs().sort_values().tail(30))","b496a7ac":"#realligning two datasets based on the features selected in training\ntrain_df_H20 = train_df # for use with autoML\ny_df = train_df['Target']\ntrain_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\nprint(f\"Training set shape:{train_df.shape}, testing set shape:{test_df.shape}\")","c048ac14":"#converting to numpy array\nX = train_df.values\ny = y_df.values\ny = y.reshape(-1, 1)\ntest_np = test_df.values\nX.shape","f47246ce":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split (X, y,test_size = 0.1, random_state = 123)\nX_train.shape","fabe8590":"#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX = sc.transform(X)\nprint (X)\ntest_np = sc.transform (test_np)\nprint (test_np)","b767ce23":"'''#First, we'll try some autoML tool to generate a model\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\n\nhtrain = h2o.H2OFrame(train_df_H20)\nhtest = h2o.H2OFrame(test_df)\nx = htrain.columns\ny =\"Target\"\nx.remove(y)\n# This line is added in the case of classification\nhtrain[y] = htrain[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs = 400)\naml.train(x=x, y =y, training_frame=htrain)\nlb = aml.leaderboard\nprint (lb)'''","6f219d18":"'''print(\"Generate predictions\u2026\")\ntest_y = aml.leader.predict(htest)\ntest_y = test_y.as_data_frame()'''","9887ff97":"#AutoML can make a decent prediction, but not as good as the manually tuned model yet. For now we'll use LGBoost with early stopping for our final prediction\n#credits to https:\/\/www.kaggle.com\/mlisovyi\/lighgbm-hyperoptimisation-with-f1-macro for the parameters values\nimport lightgbm as lgb\nclassifier = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","6d62a9c6":"eval_set = [(X_train, y_train), (X_test, y_test)]\nclassifier.fit(X_train, y_train, eval_metric=\"multiclass\", eval_set=eval_set, verbose=True, early_stopping_rounds=400) #LGBoost model model\ny_pred = classifier.predict(X_test) \ny_pred = y_pred.reshape(-1, 1)","a30e6afd":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred)\nprint (cm1)","7ffdd835":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred, average ='macro')\nprint ('f1 score for LGBoost model:',f1)","f8d20073":"y_pred = classifier.predict(test_np)\ny_pred = y_pred.reshape(-1, 1)\ny_pred = y_pred.astype(int)\nprint(plt.hist(y_pred))","2c56b9c7":"# Visualise with a barplot\nimport seaborn as sns\nindices = np.argsort(classifier.feature_importances_)[::-1]\nindices = indices[:30]\n\n\nplt.subplots(figsize=(40, 40))\ng = sea.barplot(y=train_df.columns[indices], x = classifier.feature_importances_[indices], orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=40)\ng.set_ylabel(\"Features\",fontsize=40)\ng.tick_params(labelsize=40)\ng.set_title(\"Feature importance\", fontsize=40)","391d9a6b":"#Submitting the prediction\ntest_df['Target'] = y_pred.astype(int)\ntest_df['idhogar'] = idhogar\nsubmit = submit.merge(test_df[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n#submit['TARGET'] = test_y['predict'].values - for autoML  ","175c4251":"submit['Target'] = submit['Target'].fillna(4) #there is no head of the household, assigning '4' to those\nsubmit['Target'] = submit['Target'].astype(int)","ad63a4df":"submit['Target'].hist()","1f1366be":"submit","f4337f78":"# Save the submission to a csv file\nsubmit.to_csv('LGBClassification.csv', index = False)","eb9f7955":"print ('The prediction was based on LGBoost model with early stopping, trained on ', train_df.shape[1],' features. F1 score for the the training dataset was ',f1,'.')","174c5f44":"## 10. Making prediction ","596c927c":"## 4. Removing outliers with box-plots.","8ae078d6":"## 12.Submitting to the competition","8600c3c5":"## **6. Feature Engineering**","6ab3d6c1":"## #7. Dimension reduction.","815241a5":"## 11. Visualizing\/explaining the model","ea77512f":"## **9. Building the model**","3797df4e":"### Automated Feature Engineering with Featuretools","08f699ab":"## **2. Importing\/exploring the train\/test datasets and converting to numeric form**","f1354b7d":"\n## 3. Taking care of the missing values","1aab8efb":"### Manual feature engineering","82cd8cce":"## **8. Converting to numpy arrays and scaling**","93bed819":"## 1. **Importing the libraries**","632b3137":"## **5. Data visuzalisation**"}}