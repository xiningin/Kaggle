{"cell_type":{"ff7b290a":"code","2eeacdf0":"code","d8b860cc":"code","96f87442":"code","d41f600b":"code","a4c1bf26":"code","22dc7bec":"code","6a615a2b":"code","ea12d9cc":"code","1c7f89dc":"code","fa34bf04":"code","3ee1b00c":"code","63a6daf9":"code","fca54d31":"code","2a5e9861":"code","eb30b541":"code","d7768cf6":"code","8a68b305":"code","ff424a45":"code","8dbbf6f1":"code","5eee4189":"code","56d8fa73":"code","1d527cf6":"code","c7b39d57":"code","ea70e15f":"code","9d6c5f77":"code","380f9457":"code","70507d8a":"code","5067a03a":"code","ebdd92f6":"code","dd6656f9":"code","1ecaf88f":"code","e3f691f7":"code","fa4e3556":"code","0208ca83":"code","d750c0e1":"code","1d96da00":"code","950da1b9":"code","2f45b008":"code","e464d50a":"code","6d72f416":"code","d247f0d0":"code","93307d29":"code","b63f18dc":"code","4da69c3f":"code","c07ab9d5":"markdown","23b2b6c0":"markdown","358143fe":"markdown","e85585af":"markdown","88a85715":"markdown","2112b702":"markdown","2507de8c":"markdown","2ecac358":"markdown","b0de3bbf":"markdown","d7e78c21":"markdown","a29c7439":"markdown","f864efc8":"markdown","da909378":"markdown","658012de":"markdown","4a194dc3":"markdown","4f9b3fb0":"markdown","739c1bac":"markdown","51d1698b":"markdown","588cb3d6":"markdown","6152652c":"markdown","b4fb448a":"markdown","d410d7de":"markdown","a4e81f79":"markdown"},"source":{"ff7b290a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import division\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.ensemble import RandomForestClassifier\n#from xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2eeacdf0":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.shape","d8b860cc":"train_df.head()","96f87442":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.shape","d41f600b":"test_df.head()","a4c1bf26":"#process name column to extract passenger titles\n#training data\nname_col = train_df['Name']\ntitles_ls = []\nfor name in name_col:\n    title = name.split(', ')[1].split('. ')[0]\n    titles_ls.append(title)\nlist(set(titles_ls)) #get the unique titles","22dc7bec":"title_mapper = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",    \n    \"Jonkheer\":   \"Upper_Class\",\n    \"Don\":        \"Upper_Class\",\n    \"Sir\" :       \"Upper_Class\",\n    \"Lady\" :      \"Upper_Class\",    \n    \"the Countess\":\"Upper_Class\",\n    \"Dona\":       \"Upper_Class\",    \n    \"Mlle\":       \"Miss\",\n    \"Miss\" :      \"Miss\",\n    \"Mme\":        \"Mrs\",    \n    \"Ms\":         \"Mrs\",\n    \"Mrs\" :       \"Mrs\",    \n    \"Master\" :    \"Master\",\n    \"Mr\" :        \"Mr\"    \n}","6a615a2b":"title_col = []\nfor title in titles_ls:\n    title_col.append(title_mapper[title])\ntrain_df['Title'] = title_col  \ntrain_df.head()","ea12d9cc":"#test data\nname_col = test_df['Name']\ntitles_ls = []\nfor name in name_col:\n    title = name.split(', ')[1].split('. ')[0]\n    titles_ls.append(title)\nlist(set(titles_ls)) #get the unique titles","1c7f89dc":"title_col = []\nfor title in titles_ls:\n    title_col.append(title_mapper[title])\ntest_df['Title'] = title_col  \ntest_df.head()","fa34bf04":"train_df.drop('Name', axis=1, inplace=True)\ntrain_df.head()","3ee1b00c":"test_df.drop('Name', axis=1, inplace=True)\ntest_df.head()","63a6daf9":"passengerId = test_df['PassengerId']","fca54d31":"train_df.drop('PassengerId', axis=1, inplace=True)\ntrain_df.head()","2a5e9861":"test_df.drop('PassengerId', axis=1, inplace=True)\ntest_df.head()","eb30b541":"#print number of missing entries in each column of the training data\ntrain_df.isnull().sum()","d7768cf6":"#detect which columns include NaN's in the training data\nnan_col_train = train_df.columns[train_df.isna().any()].tolist() #train data\nnan_col_train","8a68b305":"#print number of missing entries in each column of the test data\ntest_df.isnull().sum()","ff424a45":"#detect which columns include NaN's in the test dataset\nnan_col_test = test_df.columns[test_df.isna().any()].tolist() #test data\nnan_col_test","8dbbf6f1":"#train data\nfor col in nan_col_train:\n    col_data = train_df[col]\n    null_entry_cnt = col_data.isnull().sum()\n    #print(null_entry_cnt)    \n    total_entry_cnt = len(col_data)\n    nul_ratio = float(null_entry_cnt)\/float(total_entry_cnt)\n    print(\"For the training set, %.2f percent of data in the column %s is missing entries\" \\\n          % (100*nul_ratio, col))\n# test data\nfor col in nan_col_test:\n    col_data = test_df[col]\n    null_entry_cnt = col_data.isnull().sum()\n    #print(null_entry_cnt)\n    total_entry_cnt = len(col_data)\n    nul_ratio = float(null_entry_cnt)\/float(total_entry_cnt)\n    print(\"For the test set, %.2f percent of data in the column %s is missing entries\" \\\n          % (100*nul_ratio, col))","5eee4189":"imp_const = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=\"unk\")\nimp_const.fit(train_df[['Cabin']])","56d8fa73":"train_df['Cabin'].head(10)","1d527cf6":"train_df['Cabin'] = imp_const.transform(train_df[['Cabin']]).ravel()","c7b39d57":"train_df['Cabin'].head(10)","ea70e15f":"test_df['Cabin'] = imp_const.transform(test_df[['Cabin']]).ravel()\ntest_df['Cabin'].head(10)","9d6c5f77":"imp_median_age = SimpleImputer(missing_values=np.nan, strategy='median')\nimp_median_age.fit(train_df[['Age']])","380f9457":"train_df['Age'] = imp_median_age.transform(train_df[['Age']]).ravel()","70507d8a":"test_df['Age'] = imp_median_age.transform(test_df[['Age']]).ravel()","5067a03a":"imp_median_fare = SimpleImputer(missing_values=np.nan, strategy='median')\nimp_median_fare.fit(train_df[['Fare']])","ebdd92f6":"test_df['Fare'] = imp_median_fare.transform(test_df[['Fare']]).ravel()","dd6656f9":"imp_frequent = SimpleImputer(strategy='most_frequent')\nimp_frequent.fit(train_df[['Embarked']])","1ecaf88f":"train_df['Embarked'] = imp_frequent.transform(train_df[['Embarked']]).ravel()","e3f691f7":"# categorical variables in training data\nall_col = train_df.columns\nnum_cols = train_df._get_numeric_data().columns\ncat_cols_train = list(set(all_col) - set(num_cols))\n# categorical variables in test data\nall_col = test_df.columns\nnum_cols = test_df._get_numeric_data().columns\ncat_cols_test = list(set(all_col) - set(num_cols))","fa4e3556":"cat_cols = list(set(cat_cols_train + cat_cols_test))\ncat_cols","0208ca83":"#train = pd.get_dummies(train_df, columns=cat_cols)\n#test = pd.get_dummies(test_df, columns=cat_cols)\nlen_train = train_df.shape[0]\ntrain_test_df = pd.concat([train_df, test_df])\nX_train_test_ohe = pd.get_dummies(train_test_df, columns=cat_cols)\n\n# Separate them again into train and test\ntrain_df, test_df = X_train_test_ohe.iloc[:len_train, :], X_train_test_ohe.iloc[len_train:, :]","d750c0e1":"y_train_col = train_df.Survived \ny_train = y_train_col.values\nx_train_df = train_df.drop(['Survived'], axis=1)\nx_train = x_train_df.values","1d96da00":"x_train.shape","950da1b9":"y_train.shape","2f45b008":"# drop the label column in test data\nx_test_df = test_df.drop(['Survived'], axis=1)\nx_test = x_test_df.values\nx_test.shape","e464d50a":"label_map = {}\nlabel_map = {1: \"survived\", 0: \"deceased\"} #map label to survival status for visualization purposes\nlabel_map","6d72f416":"passngr_cnt  = y_train_col.value_counts()\npassngr_idx = passngr_cnt.index\nplt_idx = [label_map[idx] for idx in passngr_idx]    \nplt.figure()\nsns.barplot(plt_idx, passngr_cnt.values, alpha=0.8)\nplt.title('Passenger survival according to training data')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('passenger survival', fontsize=12)\nplt.show()","d247f0d0":"#perform feature standardization\nscaler = preprocessing.StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train) \nx_test = scaler.transform(x_test)","93307d29":"# Random Forest\nclf = RandomForestClassifier(random_state=0)\nparam_grid = {'n_estimators': [5, 10, 15, 20, 25, 50],\n              'max_depth': [2, 5, 7, 10],\n              'max_features': [250, 300, 400, 500],\n              'min_samples_leaf': [5, 10, 50]\n             }\n#'min_samples_leaf': [2, 3, 4, 5]\ngrid_search = GridSearchCV(estimator=clf, param_grid=param_grid, \n                        cv=5, scoring ='accuracy')\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_estimator_","b63f18dc":"preds = [int(pred) for pred in grid_search.predict(x_test)]\nresult = pd.DataFrame({'PassengerId': passengerId, 'Survived': preds})\n# save to csv\nresult.head(10)","4da69c3f":"fname = 'Titanic_randomforest_7.csv'\nresult.to_csv(fname,index=False)","c07ab9d5":"Let's standardize the features before fitting our model:","23b2b6c0":"For this part, I have borrowed Peter Begle's creative idea for feature engineering from the following Medium post: https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9\nEach row of data already has a unique identifier (PassengerId). I have preprocessed the Name column to extract the passenger title from the text data in the Name column. The extracted titles can act as another feature in the model","358143fe":"I have used imputation to handle the missing values:","e85585af":"For the column \"Cabin\", since the percentage of missing values is very big, I used the constant imputation method to replace all the NaN's with string \"unk\". Let's use the fitted imputer to transform the test set as well:","88a85715":"It would help the generalizability of the model to have broader categories as far as the different title categories go. So, next, I manually narrowed down the titles to the following:","2112b702":"For Age and Fare, use median of the existing values for imputation:","2507de8c":"Add Title column to data:","2ecac358":"# **Getting Started**\nIn this project, the goal is to predict which passengers survived the tragedy of the sinking of Titanic.\nLet's start by importing the necessary modules and importing data:","b0de3bbf":"As the quick scanning of the dataset hints, there are columns in both train and test data that contain missing values. Let's look closer to the data for possible missing values and go about handling these missing entries in the data:","d7e78c21":"As indicated above, each of the training and test dataset has three columns that include at least one NaN value. We can also take a look at the percentage of the missing entries for each of these features:","a29c7439":"Now let's further process train data to extract the labels:","f864efc8":"Okay there are no new titles in the test set. With that being confirmed, let's add the feature to the test set as well","da909378":"## **Handling Missing Values**","658012de":"For my classification model, I have used Random Forest classifier from Scikit-learn. I performed hyperparameter tuning via cross validation as below:","4a194dc3":"To get more insight into the data, let's take a look at the training labels to get a sense of the relative population of the two classes (\"survived\" vs. \"not-survived\")","4f9b3fb0":"Now, let's perform one-hot encoding on categorical features. For that, we would first need to detect which column(s) represent categorical variables:","739c1bac":"## **Classification**","51d1698b":"# **Pre-processing**","588cb3d6":"## **Handling Categorical Features**","6152652c":"## **Feature Scaling**","b4fb448a":"Now that the feature Title has been successfully created, we can go ahead and remove the Name column:","d410d7de":"## **Title Extraction from Passenger Names**","a4e81f79":"Before we add the Title column to test data, we need to make sure that we will not face any unseen titles (i.e. absent keys in the mapper):"}}