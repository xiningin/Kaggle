{"cell_type":{"1534430a":"code","0760aa63":"code","6222272a":"code","5a9979cd":"code","98d5a3bb":"code","12079ca2":"code","17f604db":"code","fa960927":"code","b9401daa":"code","0b5fbfe6":"code","b577740b":"code","0fdc1d67":"code","32487da7":"code","d0b53f37":"code","f7f6f335":"code","8cbfe618":"code","ba241f54":"code","2a73b787":"code","2d4f1007":"markdown","bfa2a1da":"markdown","5faf1ff7":"markdown","588c90e2":"markdown","132c5ba8":"markdown","729a9576":"markdown","76dc51d7":"markdown","04dbfa4c":"markdown","664c126f":"markdown","a2e3648d":"markdown","38ccd25a":"markdown","73dce1ca":"markdown","8dd2e734":"markdown","305b0119":"markdown"},"source":{"1534430a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0760aa63":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","6222272a":"heart = pd.read_csv('..\/input\/heart-disease-dataset\/heart.csv')","5a9979cd":"heart.head()","98d5a3bb":"heart.shape","12079ca2":"heart.dtypes","17f604db":"heart.isnull().sum()","fa960927":"heart.describe()","b9401daa":"plt.rcParams['figure.figsize'] = [10, 5]\nax = sns.histplot(x='age',data=heart,hue='target',kde=True)\nax.set_title('Heart Disease Presence by Age')","0b5fbfe6":"ax = sns.countplot(x='sex',hue='target',data=heart)\nax.set_title('Heart Disease Presence by Gender')\nax.set_xticklabels(['Female','Male'])","b577740b":"plt.rcParams['figure.figsize'] = [16, 4]\nplt.subplot(1,3,1)\nax = sns.histplot(x='trestbps',data=heart,hue='target',kde=True)\nax.set_title('1.) Heart Disease Presence by Resting Blood Pressure')\nax.set_xlabel('Resting Blood Pressure (bpm)')\n\nplt.subplot(1,3,2)\nax = sns.histplot(x='chol',data=heart,hue='target',kde=True)\nax.set_title('2.) Heart Disease Presence by Cholestrol Level')\nax.set_xlabel('Cholestrol Level')\n\nplt.subplot(1,3,3)\nax = sns.histplot(x='thalach',data=heart,hue='target',kde=True)\nax.set_title('3.) Heart Disease Presence by Max Heart Rate')\nax.set_xlabel('Max Heart Rate')","0fdc1d67":"plt.rcParams['figure.figsize'] = [16, 8]\n\nplt.subplot(2,3,1)\nax = sns.countplot(x='cp',hue='target',data=heart)\nax.set_title('1.) Heart Disease Presence by Chest Pain Type')\nax.set_xlabel('Chest Pain Type')\n\nplt.subplot(2,3,2)\nax = sns.countplot(x='fbs',hue='target',data=heart)\nax.set_title('2.) Heart Disease Presence by Fasting Blood Sugar')\nax.set_xlabel('Fasting Blood Sugar')\nax.set_xticklabels(['< 120mg\/dl','> 120mg\/dl'])\n\nplt.subplot(2,3,3)\nax = sns.countplot(x='restecg',hue='target',data=heart)\nax.set_title('3.) Heart Disease Presence by EKG Heartbeat')\nax.set_xlabel('EKG Heartbeat')\n\nplt.subplot(2,3,4)\nax = sns.countplot(x='exang',hue='target',data=heart)\nax.set_title('4.) Heart Disease Presence by Exercise Heart Pain')\nax.set_xlabel('Exercise Heart Pain')\n\nplt.subplot(2,3,5)\nax = sns.countplot(x='ca',hue='target',data=heart)\nax.set_title('5.) Heart Disease Presence by Fluoroscopy')\nax.set_xlabel('# of Vessels Colored by Fluoroscopy')\n\nplt.subplot(2,3,6)\nax = sns.countplot(x='thal',hue='target',data=heart)\nax.set_title('6.) Heart Disease Presence by Thal???')\nax.set_xlabel('Thal???')","32487da7":"plt.rcParams['figure.figsize'] = [16, 4]\nplt.subplot(1,2,1)\nax = sns.histplot(x='oldpeak',data=heart,hue='target',kde=True)\nax.set_title('1.) Heart Disease Presence by EKG Peak')\nax.set_xlabel('EKG Peak')\n\nplt.subplot(1,2,2)\nax = sns.countplot(x='slope',data=heart,hue='target')\nax.set_title('2.) Heart Disease Presence by EKG Slope')\nax.set_xlabel('EKG Slope')","d0b53f37":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score","f7f6f335":"X_data = heart.drop(['age','sex','target'],axis=1)\ny_data = heart['target']","8cbfe618":"scores = pd.DataFrame(columns = ['Test Type','Accuracy','F1'])","ba241f54":"for rand in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2,random_state=rand)\n    #Perform normalization\n    scaler = StandardScaler()\n    X_train_std = scaler.fit_transform(X_train)\n    X_test_std = scaler.transform(X_test)\n    #Logistic Regression\n    model = LogisticRegression()\n    model.fit(X_train_std, y_train)\n    y_predict = model.predict(X_test_std)\n    scores.loc[len(scores)] = ['Logistic Regression',accuracy_score(y_test,y_predict),f1_score(y_test,y_predict)]\n    #Decision Tree\n    model = DecisionTreeClassifier()\n    model.fit(X_train_std, y_train)\n    y_predict = model.predict(X_test_std)\n    scores.loc[len(scores)] = ['Decision Tree',accuracy_score(y_test,y_predict),f1_score(y_test,y_predict)]\n    #Random Forest\n    model = RandomForestClassifier()\n    model.fit(X_train_std, y_train)\n    y_predict = model.predict(X_test_std)\n    scores.loc[len(scores)] = ['Random Forest',accuracy_score(y_test,y_predict),f1_score(y_test,y_predict)]\n    #KNN\n    model = KNeighborsClassifier()\n    model.fit(X_train_std, y_train)\n    y_predict = model.predict(X_test_std)\n    scores.loc[len(scores)] = ['KNN',accuracy_score(y_test,y_predict),f1_score(y_test,y_predict)]","2a73b787":"scores.groupby('Test Type').mean()","2d4f1007":"#### Section 1: Data Exploration \/ Cleaning\n\nGoals of this section are to:\n- Ensure that there is no missing data\n- Ensure there is no weird or misentered data\n- Get a sense of the values in this dataset","bfa2a1da":"### Data Exploration\n\nLet's check if older people in this dataset are more likely to have heart disease","5faf1ff7":"## Introduction\n\nHeart disease is a general term for a range of cardiovascular diseases, with the most common one being Coronary Artery Disease (CAD). According to the CDC, heart disease was the leading cause of death in 2018 and 2019 killing around 650k Americans per year. \n\n![display_image](https:\/\/www.cdc.gov\/nchs\/images\/databriefs\/351-400\/db395-fig4.gif)\n\nhttps:\/\/www.cdc.gov\/nchs\/products\/databriefs\/db395.htm\n\nMost forms of heart diseases are preventable, with the risk being greatly lowered through personal lifestyle changes. If characteristics that represent high risk of heart disease are caught early enough, a candidate can undergo treatment that includes dietary and exercise changes, smoking, and sometimes medicine. In this project, we will be creating a machine learning model with data from 1,025 patients, to predict whether or not a patient has heart disease.\n\n#### The Data\n\nData used for this analysis is displayed with these 14 columns:\n1. age\n2. sex\n3. cp (chest pain type - 4 values)\n4. trestbps (resting blood pressure)\n5. chol (serum cholestoral in mg\/dl)\n6. fbs (fasting blood sugar > 120 mg\/dl)\n7. restecg (resting electrocardiographic EKG results - values 0,1,2)\n8. thalach (maximum heart rate achieved)\n9. exang (exercise induced angina chest pain)\n10. oldpeak (ST depression induced by exercise relative to rest)\n11. slope (the slope of the peak exercise ST segment)\n12. ca (number of major vessels (0-3) colored by flourosopy)\n13. thal ( 0 = normal; 1 = fixed defect; 2 = reversable defect\n14. target (0 - no heart disease, 1 - heart disease)\n\nAll data pulled from Kaggle user johnsmith88\nhttps:\/\/www.kaggle.com\/johnsmith88\/heart-disease-dataset","588c90e2":"The Random Forest gives an Accuracy and F1 score of >99.7%\n\n### Conclusion\n\nInvestigation of the data shows trends that suggest a strong sampling bias. Even though our model gives a high Accuracy and F1 Score, it is likely that it is strongly overfitting to this data. If the intention is to predict \"heart disease\" in patients, I'd expect that any change to the future demographic of patients will cause the model to perform poorly\n\nTo raise confidence in this model, I would request these few improvements to the dataset:\n\n1. \"Heart disease\" may be too broad of a designation for an accurate model. This dataset provides risk factors that are unique to different types of heart disease. Although I am not an expert on this subject and therefore I don't know how different types of heart characteristics relate to one another, it seems to me that certain types of heart disease can be very independent from one another. It would be better to be able to narrow down this designation into multiple categories (abnormal heartbeat, atherosclerosis etc.)\n2. There should be some sort of random sampling from the general population if possible. This dataset seems very focused on people with pre-existing conditions, and people already in higher risk demographics (age\/gender). Someone that is unlike the typical person evaluated in this dataset can give different results\n3. Investigation into the sampling methods that produce this data, is it repeatable? For example, do subjects go through the same amount of exercise to produce the data that has to do with exercise?  \n\nThank you for reading :)","132c5ba8":"#### Takeaways from these graphs\n\nI am not confident that building a model off of the data presented would be trustworthy enough to be applied to the real world. From looking at the correlations independently, there are some questionable results. **It seems that the data produced in this dataset was taken from patients that already had a reason to have their heart evaluated**. This could accurately capture heart disease for patients that experience noticeable symptoms like abnormal heart behavior. On the other hand, it could miss unnoticeable symptoms that quietly develop over time, like high blood pressure or coronary artery disease. Additionally, we do not know what type of heart disease each of the patients with heart disease actually have. **This dataset can skew towards a specific type of heart disease that is not representative of the actual percentages of heart disease in the general population. Therefore it model may overfit and end up not being a model to predict \"heart disease\" as a whole, but rather the specific type of heart disease that is overrepresented with this data**.  \n\nHowever for now, I will continue on to produce a model based off of the current data from this data set. Although it will not be representative of the general population, I believe it could still be useful for future patients that fall into the same demographics as the ones represented in this test, and the qualifications and reasons to be evaluated in the first place stay the exact same as it was when this data was produced.\n","729a9576":"### Building a Model\n\nThere are a few different options that we can use for a classification algorithm:\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest\n4. k-Nearest Neighbors\n\nWe will run each of these algorithms and see which gives the best predictive score","76dc51d7":"In some train_test splits, we are able to get an accuracy for 100% for some of these model types. Therefore, I have decided to perform repeated trainings of the algorithm by resampling, and average out the scores to determine which algorithm performs the best.\n\nData will also be normalized before model training. Shoutout to Renato Bosnjak for the helpful suggestions on using StandardScaler()","04dbfa4c":"#### Initial thoughts on data\n**Age**: Our dataset for this group is on the old side a mean age of 54 years. This makes sense as it is a dataset of patients who have been evaluated for heart disease, which the risk increases with age. However, this means our dataset is not representative of the general population  \n**Sex**: Our dataset is 69.5% male. Males are known to have more problems than women when it comes to heart disease.  \n**CP**: Four values of either [0, 1, 2, 3] (not sure what each value means though, but we will check later if this is significant)  \n**Trestbps**: Resting blood pressure, with the range of numbers provided, I assume systolic blood pressure is being reported in mmHg. Above 120 is considered having high blood pressure, which a majority of patients have in this dataset  \n**Chol**: Assuming the dataset is reporting Total Cholestrol in mg\/dl. Over 200 is considered to be High Cholestrol  \n**Fbs**: Fasting blood sugar, reports if blood sugar levels are >120 mg\/dl, which is categorized as being pre-diabetic or diabetic. About 15% of patients here fall into this category  \n**Restecg**: EKG checks for abnormal heartbeat. Split into [0, 1, 2]. However, no legend is given to us as to what these mean.  \n**Thalach**: Maximum heart rate achieved. Assuming the patients went through an exercise procedure to get their maximum heart rate evaluated  \n**Exang**: Exercise induced chest pain, about 34% of our patients encounter this  \n**Oldpeak**: Measure of abnormal heartbeat  \n**Slope** Another measure of abnormal heartbeat  \n**Ca**: Heart imaging  \n**Thal**: Not sure what this is  \n**Target**: Our indicator of whether a person has heart disease. 51.3% of the patients in the dataset are reported to have heart disease","664c126f":"First we will need to decide which parameters we will be modeling on. I stated previously that there are a few parameters whose correlations do not make sense when related to heart disease. The two I will omit for now are Age and Gender. However, I will be keeping other parameters in, since the goal is now not to create a model for the general public for heart disease, but rather people of similar demographic of the ones sampled to predict their risk of whatever disease is disproportionately captured in this data.","a2e3648d":"Will have to ask a doctor (or Google) as to whether these make sense ","38ccd25a":"We see that a large majority of females in this dataset end up being diagnosed with heart disease, while less than half of males are diagnosed. Like the age column, this effect is not representative of the general population as a whole, and may be due to data sampling biasness. Perhaps less females are evaluated for heart disease out of precauation, just like how younger people are less likely to be evaluated. More investigation will need to be done in the future as to how this dataset was actually curated. However, my takeaway is that if this data was made from precautionary evaluations, **females are very underevaluated for heart disease and should be more frequently evaluated in the future**.  \n\nJust like the Age column, I will be omitting the Sex column because of hypothesized data curation biasness.  \n\n**Because it is now established that this dataset, is not representative of the general population, we must be careful to choose what pieces of information to add into our model to avoid overfitting. I will now continue to check each characteristic to see if the trend for each makes sense**","73dce1ca":"1. It seems safe to assume that a value of 0 indicates no chest pain. If this is true, the resulting data makes sense\n2. Lower blood sugar slightly correlates with no heart disease. Does not really make sense\n3. Assuming 0 means no abnormal heartbeat. Makes sense.\n4. Initially would assume 0 means no heart pain. 0 strongly correlates with presence of heart disease. We would expect presence of heart pain to correlate with presence of heart disease. Perhaps 0 means heart pain, but data is unlabelled so I don't know for sure\n5. The more vessels that are colored by fluoroscopy, the less chance of heart disease. Will have to ask a doctor whether this makes sense\n6. Not sure what this data is, but there is a very strong correlation here","8dd2e734":"The older the person, the less likely they are to have heart disease. This in itself is false and a large bias of the dataset. I hypothesize that this is because the elderly will get regular check for heart disease, while younger people will not get checked unless they actually experience heart issues. Because of this bias, I will omit using Age in the model of this dataset, since the goal is to model strictly on the data of the heart. **If this model is ever to be used on the general public, it would be incorrect to fit to the trend from the Age data in this dataset**.   \n\nHow do the stats look for gender?  ","305b0119":"1. Resting Blood Pressure does not seem to have an effect on whether someone has heart disease. Does not really make sense  \n2. Lower cholestrol seems to indicate higher chance of heart disease.  Does not make sense\n3. Higher max heart rate raises chance of heart disease. Makes sense"}}