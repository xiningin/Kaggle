{"cell_type":{"0508db50":"code","a2d8b1ba":"code","4ce96c40":"code","f8d754ff":"code","47472f46":"code","236469d1":"code","860e5757":"markdown","c00de922":"markdown","f96c4d7f":"markdown","d4cb63e4":"markdown","e3203343":"markdown","b9ffe059":"markdown"},"source":{"0508db50":"# To do linear algebra\nimport numpy as np\n\n# To store data\nimport pandas as pd\n\n# To open zipped files\nimport bz2\n\n# To use regular expressions\nimport re\n\n# To shuffle training- & testing-dataset\nfrom sklearn.utils import shuffle\n\n# To build models\nfrom keras.layers import Dense, Input, Dropout, Flatten, Conv1D, BatchNormalization, MaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\n\n# To get progression bars\nfrom tqdm import tqdm\n\n# To remove rare words\nfrom gensim.corpora import Dictionary\n\n# To search directories\nimport os\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# To aggressively suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a2d8b1ba":"# Open file and read lines\ntrain_file = bz2.BZ2File('..\/input\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/test.ft.txt.bz2')\n\n# Decode bytes\ntrain_lines = [x.decode('utf-8') for x in train_file.readlines()]\ntest_lines = [x.decode('utf-8') for x in test_file.readlines()]\ndel train_file, test_file\n\n\ndef createDataset(lines):\n    # Create mapping for positive-negative labels\n    label_mapping = {'__label__1':[1,0], '__label__2':[0,1]}\n    \n    y = []\n    X = []\n    # Iterate over all lines\n    for line in lines:\n        # Split label and review\n        y_tmp, X_tmp = line.split(' ', 1)\n    \n        # Store labels and texts\n        y.append(label_mapping[y_tmp])\n        X.append(X_tmp)\n    return X, y\n\n# Get train- & testset\nX_train_data, y_train_data = createDataset(train_lines)\nX_test_data, y_test_data = createDataset(test_lines)\ndel train_lines, test_lines\n\nprint('Length Train:\\t{}\\nLength Test:\\t{}\\n'.format(len(y_train_data), len(y_test_data)))\n# Display random sample\nprint('Random Review:\\n', X_train_data[np.random.choice(range(len(X_train_data)))])\n\n\n# All signs in the dataset (can be substituted by iterating over the entire dataset)\nsigns = '\\x02\\x03\\x04\\x05\\x07\\x08\\n\\x0f\\x10\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1b !\"#$%&\\'()*+,-.\/0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x7f\u00a1\u00b4\u00bf\u00c0\u00c1\u00c3\u00c4\u00c8\u00c9\u00cc\u00cd\u00d1\u00d2\u00d3\u00d6\u00d9\u00da\u00dc\u00df\u00e0\u00e1\u00e4\u00e8\u00e9\u00ec\u00ed\u00f1\u00f2\u00f3\u00f6\u00f9\u00fa\u00fc\u0153\u0192\u03a9\u2013\u2014\u2018\u2019\u201c\u201d\u201e\u2020\u2022\u2026\u2032\u2033\u20ac\u2122\u2193\u2202\u2205\u2260\u2282\u2295\u2660\u2663\u2665\u2666\u27e8'\n\n# Create dictionary mapping from sign to id\ndictionary = Dictionary([list(signs)])\nprint('Different Signs In The Dataset:\\t{}'.format(len(dictionary)))\n\n\ndef generatorDictionary(data):\n    # Iterate over all reviews\n    for review in tqdm(data):\n        # Yield tokenized review\n        yield list(review)\n\n# Create dictionary by iterating over the entire dataset\n#dictionary = Dictionary(generatorDictionary(X_train_data))","4ce96c40":"# Length of the sequence for the model\nmaxlen = 800\n\ndef generator(X, y, batchsize, length):\n    # Iterate over all samples and yield batches\n    while True:\n        # Shuffle the dataset\n        X, y = shuffle(X, y)\n    \n        # Variables to save the batch\n        X_data = []\n        y_data = []\n        # Iterate over the dataset\n        for review, label in zip(X, y):\n            # Transform review to sequence of indices\n            review_idx = np.array(dictionary.doc2idx(list(review)))\n            # Filter unknown indices\n            filtered_review_idx = review_idx[review_idx!=-1]\n            # Limit sequence to maxlen entries\n            pad_review_idx = pad_sequences([filtered_review_idx], maxlen=length, padding='post', value=-1)[0]\n            # Append sequence and label to batch\n            X_data.append(pad_review_idx)\n            y_data.append(label)\n        \n            # Yield batch if batchsize is reached\n            if len(y_data)==batchsize:\n                yield (np.array(X_data), np.array(y_data))\n                X_data = []\n                y_data = []","f8d754ff":"def createModel():\n    # Input layer\n    input = Input(shape=[maxlen])\n\n    # Embedding to reduce featurespace\n    net = Embedding(len(dictionary), 30)(input)\n\n    # 1D Convolution\n    net = BatchNormalization()(net)\n    net = Conv1D(16, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(24, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(32, 7, padding='same', activation='relu')(net)\n    net = MaxPool1D(pool_size=5)(net)\n\n    # 1D Convolution\n    net = BatchNormalization()(net)\n    net = Conv1D(64, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(96, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(128, 7, padding='same', activation='relu')(net)\n    net = MaxPool1D(pool_size=5)(net)\n\n    # 1D Convolution\n    net = BatchNormalization()(net)\n    net = Conv1D(256, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(384, 7, padding='same', activation='relu')(net)\n    net = BatchNormalization()(net)\n    net = Conv1D(512, 7, padding='same', activation='relu')(net)\n    net = MaxPool1D(pool_size=5)(net)\n\n    # Flatten\n    net = Flatten()(net)\n\n    # Dense layer for combination\n    net = BatchNormalization()(net)\n    net = Dropout(0.2)(net)\n    net = Dense(1024, activation='relu')(net)\n\n    # Dense layer for combination\n    net = BatchNormalization()(net)\n    net = Dropout(0.2)(net)\n    net = Dense(1024, activation='relu')(net)\n\n    # Dense layer for combination\n    net = BatchNormalization()(net)\n    net = Dropout(0.2)(net)\n    output = Dense(2, activation='softmax')(net)\n\n\n    # Create and compile model\n    model = Model(inputs = input, outputs = output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n    # Display model\n    model.summary()\n    \n    return model","47472f46":"# Create model\nmodel = createModel()\n\n# Setup batch generators\ntrain_generator = generator(X_train_data, y_train_data, 1000, maxlen)\nvalid_generator = generator(X_test_data[:300000], y_test_data[:300000], 1000, maxlen)\ntest_generator = generator(X_test_data[300000:], y_test_data[300000:], 1000, maxlen)\n    \n# Fit model\nmodel.fit_generator(generator=train_generator, steps_per_epoch=500, epochs=45, validation_data=valid_generator, validation_steps=300)\n\n# Test model\ntest_loss, test_accuracy = model.evaluate_generator(generator=test_generator, steps=100)","236469d1":"# Get training history\nhistory = model.history.history\n\n# Create subplots\nfig, axarr = plt.subplots(2, 1, figsize=(12,8))\n# Plot accuracy\naxarr[0].plot(history['acc'], label='Train')\naxarr[0].plot(history['val_acc'], label='Val')\naxarr[0].set_title('{:.4f}: Training Accuracy'.format(test_accuracy))\naxarr[0].set_xlabel('Epoch')\naxarr[0].set_ylabel('Accuracy')\naxarr[0].legend()\n# Plot loss\naxarr[1].plot(history['loss'], label='Train')\naxarr[1].plot(history['val_loss'], label='Val')\naxarr[1].set_title('{:.4f}: Training Losses'.format(test_loss))\naxarr[1].set_xlabel('Epoch')\naxarr[1].set_ylabel('Loss')\naxarr[1].legend()\n\nplt.tight_layout()\nplt.show()","860e5757":"***\n## <a id=3>3. Create Batch-Generator<\/a>\n\nSince there are 3.6 million training reviews in the dataset, I am creating a **generator to yield randomized batches for the training, validating and testing process.**","c00de922":"***\n## <a id=4>4. Setup Convolution Network<\/a>\n\nThe model contains several **convolution layers to find patterns** in the sequence and has some **dense layers at the end to combine the created features.**","f96c4d7f":"# Sentiment Analysis On Letter-Basis\n\nThis notebook illustrates how to **predict the sentiment of a text by using a sequence of letters** as input. This approach bypasses the need of textual preprocessing and enables language independent predictions.<br>\nThe dataset consists of [Amazon Reviews](https:\/\/www.kaggle.com\/bittlingmayer\/amazonreviews\/home) and their corresponding positive or negative assessment.\n\n+ [1. Import Libraries](#1)<br>\n+ [2. Load And Preprocess Data](#2)<br>\n+ [3. Create Batch-Generator](#3)<br>\n+ [4. Setup Convolution Network](#4)<br>\n+ [5. Train Model](#5)<br>\n+ [6. Conclusion](#6)<br>\n\n***\n## <a id=1>1. Import Libraries<\/a>\n\nI am using **tensorflow and keras for the model** and basic numpy- and pandas-functions for the data storage.","d4cb63e4":"***\n## <a id=5>5. Train Model<\/a>\n\nThe model **trains on 3,6 million reviews.** After each 500 batches of 1.000 reviews a validation step with 300.000 reviews is added. Finally the model is tested on 100.000 unseen reviews.<br>\nTo get a better understanding of the training process no early-stopping has been implemented.","e3203343":"***\n## <a id=6>6. Conclusion<\/a>\n\nWith more than **90% accuracy** this approach seems worthwhile.<br>\nEspecially the **absence of a preprocessing step for the reviews is interesting.** A deep understanding of how language works or which words to use for classification is not needed in this approach.<br>\nThe model itself separates useful and unimportant combinations of letters. Hence the **model should be valuable in different languages.**\n\nHave a good day!","b9ffe059":"***\n## <a id=2>2. Load And Preprocess Data<\/a>\n\nAll reviews, which are presplit into train- and testset,  are stored in zipped files. I am separating the label from the reviews and these **raw reviews will be used as input for the network.**<br>\nTo **map each letter into a distinct index** I am using a gensim-dictionary. The model will use a sequence of indices as datasource for the prediction afterwards."}}