{"cell_type":{"f42922b1":"code","100c7297":"code","3fd2677a":"code","3143dfe7":"code","7e3a524d":"code","88a1fb95":"code","00ba25ad":"code","ce5f6e0a":"code","2ae0025e":"code","8a3129f9":"code","6890361a":"code","5a991103":"code","5e88e232":"code","73c9aaaf":"code","06e8b034":"code","0be1690a":"code","fe13c6b1":"code","c87821ad":"code","9b3009ad":"code","3a43d6f1":"code","fb750158":"markdown","90cfc350":"markdown","8fc06643":"markdown","e0a363d8":"markdown","cd40d119":"markdown","c30acf9e":"markdown","d3072055":"markdown","806349c8":"markdown","45b02463":"markdown"},"source":{"f42922b1":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport os\nimport re\nimport random\nimport collections, json","100c7297":"COCO_PATH = \"..\/input\/coco-2017-dataset\/coco2017\/\" \n\nwith open(COCO_PATH + \"annotations\/captions_train2017.json\", \"r\") as f:\n    annotations = json.load(f)","3fd2677a":"image_path_to_caption = collections.defaultdict(list)\nfor val in annotations[\"annotations\"]:\n    image_path = COCO_PATH + \"train2017\/\" + \"%012d.jpg\" % (val[\"image_id\"])\n    image_path_to_caption[image_path].append(val[\"caption\"])","3143dfe7":"image_paths = list(image_path_to_caption.keys())\nrandom.shuffle(image_paths)\n\n# Select the first 6000 image_paths from the shuffled set.\n# Approximately each image id has 5 captions associated with it, so that will\n# lead to 30,000 examples.\ntrain_image_paths = image_paths[:6000]\nprint(len(train_image_paths))","7e3a524d":"train_captions = []\nimg_name_vector = []\n\nfor image_path in train_image_paths:\n    caption_list = image_path_to_caption[image_path]\n    train_captions.extend(caption_list)\n    img_name_vector.extend([image_path] * len(caption_list))","88a1fb95":"rid = random.randint(0, len(train_captions))\nprint(train_captions[rid])\nImage.open(img_name_vector[rid])","00ba25ad":"# @title Load objects detections model names\nALL_MODELS = {\n'CenterNet HourGlass104 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512\/1',\n'CenterNet HourGlass104 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512_kpts\/1',\n'CenterNet HourGlass104 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024\/1',\n'CenterNet HourGlass104 Keypoints 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024_kpts\/1',\n'CenterNet Resnet50 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512\/1',\n'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512_kpts\/1',\n'CenterNet Resnet101 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet101v1_fpn_512x512\/1',\n'CenterNet Resnet50 V2 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512\/1',\n'CenterNet Resnet50 V2 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512_kpts\/1',\n'EfficientDet D0 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d0\/1',\n'EfficientDet D1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d1\/1',\n'EfficientDet D2 768x768' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d2\/1',\n'EfficientDet D3 896x896' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d3\/1',\n'EfficientDet D4 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d4\/1',\n'EfficientDet D5 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d5\/1',\n'EfficientDet D6 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d6\/1',\n'EfficientDet D7 1536x1536' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d7\/1',\n'SSD MobileNet v2 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/2',\n'SSD MobileNet V1 FPN 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v1\/fpn_640x640\/1',\n'SSD MobileNet V2 FPNLite 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_320x320\/1',\n'SSD MobileNet V2 FPNLite 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_640x640\/1',\n'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_640x640\/1',\n'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_1024x1024\/1',\n'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_640x640\/1',\n'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_1024x1024\/1',\n'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_640x640\/1',\n'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_640x640\/1',\n'Faster R-CNN ResNet50 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_800x1333\/1',\n'Faster R-CNN ResNet101 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_640x640\/1',\n'Faster R-CNN ResNet101 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_1024x1024\/1',\n'Faster R-CNN ResNet101 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_800x1333\/1',\n'Faster R-CNN ResNet152 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_640x640\/1',\n'Faster R-CNN ResNet152 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_1024x1024\/1',\n'Faster R-CNN ResNet152 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_800x1333\/1',\n'Faster R-CNN Inception ResNet V2 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_640x640\/1',\n'Faster R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_1024x1024\/1',\n'Mask R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/mask_rcnn\/inception_resnet_v2_1024x1024\/1'\n}","ce5f6e0a":"#@title Model Selection { display-mode: \"form\", run: \"auto\" }\nmodel_display_name = 'SSD MobileNet V2 FPNLite 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\nmodel_handle = ALL_MODELS[model_display_name]\n\nprint('Selected model:'+ model_display_name)\nprint('Model Handle at TensorFlow Hub: {}'.format(model_handle))","2ae0025e":"print('loading model...')\nhub_model = hub.load(model_handle)\nprint('model loaded!')","8a3129f9":"sample_im = plt.imread(img_name_vector[rid])\nsample_im = tf.expand_dims(sample_im, axis=0)","6890361a":"%%time\n\nresult = hub_model(sample_im)","5a991103":"def get_image_detections(result, max_detections=20):\n    detections = result[\"detection_boxes\"]\n    best_ind = tf.image.non_max_suppression(boxes=result[\"detection_boxes\"][0],\n                                          scores=result[\"detection_scores\"][0],\n                                          max_output_size=max_detections)\n\n    detections = tf.gather(detections, best_ind, axis=1)\n    return detections  # (1, num_detections, bbox)\n\ndetections = get_image_detections(result) ","5e88e232":"def show_image(image):\n    image = tf.cast(image, tf.float32)\n    plt.figure(figsize=(8, 8))\n    plt.imshow( (image \/ 255.)[0] )\n    plt.axis(\"off\")\n    plt.show()","73c9aaaf":"colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\ntest_im = tf.image.draw_bounding_boxes(images=sample_im.numpy(), \n                                       boxes=detections, \n                                       colors=colors)\n\nshow_image(test_im)","06e8b034":"def extract_image_regions(image, detections):\n  \n    h, w = image.shape[1:3]\n    patches = detections * [h, w, h, w]\n    regions = []\n    coors = tf.cast(patches, tf.int32) # (1, num_detections, 4)\n\n    # Remove the first axis before iterate\n    for coor in tf.squeeze(coors):\n        y_min, x_min, y_max, x_max = coor\n        regions.append(image[:, y_min:y_max, x_min:x_max, : ])\n\n    return regions\n\nim_regions = extract_image_regions(sample_im, detections)\nshow_image(im_regions[0])","0be1690a":"image_features_extractor = tf.keras.applications.ResNet101(include_top=False, \n                                                           weights=\"imagenet\", \n                                                           pooling=\"avg\")\nimage_features_extractor.trainable = False\nimage_features_extractor(tf.random.uniform((1, 200, 100, 3)))","fe13c6b1":"def detect_and_encode_image(image, max_detections=20):\n    \"\"\"\n    Perform an object detection model from a specific image to produce bbox on \n    detected objects. The bbox result then become the coordinates for image\n    regions that'll be extracted from the images. The results of the image regions \n    before will be encoded with pretrained-CNN model.\n\n    args:\n    image: A 4D tensor with shape [1, height, width, channel].\n    max_detections: Total detections needed from object detection model, cap to 300.\n\n    return:\n    image_regions_features \n    \"\"\"\n    assert max_detections <= 300, \"max detections limits is 300\"\n\n    # Feed image into object detection model\n    object_detection_result = hub_model(image)\n\n    # Get 'max_detections' bounding boxes\n    detections = get_image_detections(object_detection_result, max_detections=max_detections)\n\n    # Extract the region of image based on the bounding boxes values\n    regions = extract_image_regions(image, detections)\n\n    features = np.zeros( (len(regions), 2048) )\n\n    for index, r in enumerate(regions):\n        feature = image_features_extractor(r)\n        features[index] = feature\n\n    return features # (max_detections, 2048)","c87821ad":"%%time\n\nsample_feature = detect_and_encode_image(sample_im, max_detections=20)\nsample_feature.shape","9b3009ad":"# Get unique image path\nunique_img_name_vector = list(set(img_name_vector))\nlen(unique_img_name_vector)","3a43d6f1":"def read_image_file(path):\n    image = tf.io.read_file(path)\n    image = tf.io.decode_jpeg(image, channels=3)\n\n    return path, image\n\nAUTO = tf.data.AUTOTUNE\nfeature_file = \"features\/\"\nif not os.path.exists(feature_file):\n    os.mkdir(feature_file)\n\nimg_dataset = tf.data.Dataset.from_tensor_slices(unique_img_name_vector)\nimg_dataset = img_dataset.map(read_image_file, num_parallel_calls=AUTO).cache().prefetch(AUTO)\n\nfor path, im in tqdm(img_dataset):\n    path_to_img = path.numpy().decode(\"utf-8\")\n    image_id = re.search(r\"\\d{12}\", path_to_img).group()\n    features = detect_and_encode_image(im[np.newaxis, ...])\n    np.save(feature_file + image_id, features)\n","fb750158":"## Libraries","90cfc350":"## Visualize the Bounding Box","8fc06643":"# Prepare MS-COCO Datasets","e0a363d8":"## Cache Image Features","cd40d119":"## Extract image regions","c30acf9e":"# Image Preprocessing","d3072055":"## Loading Selected Model from Tensorflow Hub","806349c8":"## Image features from ResNet101","45b02463":"# Setup"}}