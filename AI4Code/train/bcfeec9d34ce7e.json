{"cell_type":{"9dfe07f1":"code","f5993669":"code","396d5156":"code","b98e04f8":"code","b2c31138":"code","17640390":"code","727f5301":"code","66eaac94":"code","108cf8d1":"code","e4f5303b":"code","272fa4e4":"code","cad4a7f4":"code","eaa718fb":"code","c5a8241d":"code","21e2997c":"code","f9a8ccec":"code","40563602":"code","0040d447":"code","ec7318ed":"code","b9fb8135":"code","498c9176":"code","a173ca16":"code","ca4f73ca":"code","42ab6d5d":"markdown","9fa24e67":"markdown","4949a32c":"markdown","c00e1961":"markdown","1b9ef6ef":"markdown","d0d4e94d":"markdown","ab1da113":"markdown","d24f7c80":"markdown","a3808e92":"markdown","fe77c9ad":"markdown","44e947d5":"markdown","ace726f8":"markdown","aa3877e4":"markdown"},"source":{"9dfe07f1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\nimport tensorflow as tf\nimport math\n\ntrain_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv')","f5993669":"train_df[train_df['Country\/Region'] == 'Korea, South'].head(200)\n#test_df.head()","396d5156":"train_df.groupby(['Country\/Region', 'Province\/State'])['Id'].agg(['count']).reset_index()","b98e04f8":"train_df = train_df.fillna({'Province\/State': 'Unknown'})\ntest_df = test_df.fillna({'Province\/State': 'Unknown'})\ntrain_df.isna().sum()","b2c31138":"def to_datetime(dt):\n    return datetime.datetime.strptime(dt, '%Y-%m-%d')\n\ndef count_days(dt):\n    return (dt - datetime.datetime.strptime('2020-01-22', \"%Y-%m-%d\")).days\n\nplot_df = train_df[train_df['Country\/Region'] == 'Iran']\nplot_df['Date'] = plot_df['Date'].map(to_datetime)\nplot_df['Day'] = plot_df['Date'].map(count_days)\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","17640390":"train_df['ConfirmedCases_log'] = train_df['ConfirmedCases'].map(math.log1p)\ntrain_df['ConfirmedCases_log'].hist(bins=50)","727f5301":"train_df['Country\/Region'].unique()","66eaac94":"train_df['Date_dt'] = train_df['Date'].map(to_datetime)\ntrain_df['Day'] = train_df['Date_dt'].map(count_days)\ntest_df['Date_dt'] = test_df['Date'].map(to_datetime)\ntest_df['Day'] = test_df['Date_dt'].map(count_days)","108cf8d1":"historical_steps = 30\n\n# todo: better split validate data\nval_df = train_df[train_df['Day'] > (train_df['Day'].max() - historical_steps)]\n#val_df = train_df[train_df['Country\/Region'].isin(['China'])]\n#train_drop_df = train_df.drop(val_df.index)\nprint('# Train DF \\n {} \\n# Val DF \\n {} \\n# Test DF \\n {}'.format(train_df.head(), val_df.head(), test_df.head()))","e4f5303b":"# historical_steps = 30\n# output_steps = 10\n\n# def make_sequential_input(df):\n    \n#     inputs, targets = [], []\n    \n#     for i in range(len(df) - historical_steps - 1):\n        \n#         if df.iloc[i]['Lat'] == df.iloc[i + historical_steps]['Lat'] and \\\n#             df.iloc[i]['Long'] == df.iloc[i + historical_steps]['Long']:\n            \n#             # iloc[a:b] startnig from index 'a' and ending before b\n#             inputs.append(np.array(df.iloc[i:i + historical_steps][['Day', 'Lat', 'Long', 'ConfirmedCases_log', 'Fatalities']]).tolist())\n#             targets.append(np.array(df.iloc[i + historical_steps][['ConfirmedCases_log']]).tolist())\n        \n#     return inputs, targets\n\n\n# # Make sequential input for training and validation\n# train_inputs, train_targets = make_sequential_input(train_df)\n# val_inputs, val_targets = make_sequential_input(val_df)\n# np.shape(train_inputs)","272fa4e4":"# historical_steps = 30\n\n# input_feature_count = 5\n# output_feature_count = 1\n# hidden_node_count = 32\n\n# batch_size = 32\n# epochs = 20\n# lr = 0.001\n\n# model = tf.keras.Sequential()\n# model.add(tf.keras.layers.LSTM(hidden_node_count, batch_input_shape=[None, historical_steps, input_feature_count], return_sequences=True))\n# model.add(tf.keras.layers.LSTM(hidden_node_count))\n# model.add(tf.keras.layers.Dense(output_feature_count, activation='sigmoid'))\n\n# optimizer = tf.keras.optimizers.Adam(lr=lr)\n# model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n\n# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=5)\n# history = model.fit(train_inputs, train_targets, \\\n#                     epochs=epochs, \\\n#                     batch_size=batch_size, \\\n#                     validation_data=(val_inputs, val_targets), \\\n#                     callbacks=[early_stopping])","cad4a7f4":"# def _inverse_log(val):\n#     return math.expm1(val)\n\n# _inverse_log_v = np.vectorize(_inverse_log)\n\n# # For certain country,\n# df = train_df[(train_df['Country\/Region'] == 'Korea, South') & (train_df['Province\/State'] == 'Unknown') ]\n\n# # Get latest tail & Conver to array\n# inputs = np.array(df[['Day', 'Lat', 'Long', 'ConfirmedCases_log', 'Fatalities']][-tail_len:])\n\n# # Get next ConfirmedCases & Fatalities\n# _inverse_log_v(model.predict(np.array(inputs).reshape(1, historical_steps, input_feature_count)).reshape(-1))","eaa718fb":"import statsmodels.api as sm\n\nhistorical_steps = 7\ntrain_df['Geo'] = train_df['Province\/State'].astype(str) + ',' +train_df['Country\/Region'].astype(str)\ntrain_df['Fatalities_log'] = train_df['Fatalities'].map(math.log1p)\n\nx_arr, y_arr, y_inverse_arr, grp_arr, day_arr = [], [], [], [], []\nxf_arr, yf_arr, yf_inverse_arr = [], [], []\n\nfor i in range(len(train_df) - historical_steps -1):\n    if train_df.iloc[i]['Lat'] == train_df.iloc[i+historical_steps]['Lat'] and \\\n        train_df.iloc[i]['Long'] == train_df.iloc[i+historical_steps]['Long']: \n        \n        x_arr.append(np.array(train_df.iloc[i:i+historical_steps][['ConfirmedCases_log']]).reshape(-1).tolist())\n        y_arr.append(np.array(train_df.iloc[i+historical_steps][['ConfirmedCases_log']]).tolist())\n        y_inverse_arr.append(np.array(train_df.iloc[i+historical_steps][['ConfirmedCases']]).tolist())\n        \n        xf_arr.append(np.array(train_df.iloc[i:i+historical_steps][['Fatalities_log']]).reshape(-1).tolist())\n        yf_arr.append(np.array(train_df.iloc[i+historical_steps][['Fatalities_log']]).tolist())\n        yf_inverse_arr.append(np.array(train_df.iloc[i+historical_steps][['Fatalities']]).tolist())\n        \n        grp_arr.append(np.array(train_df.iloc[i+historical_steps][['Geo']]).tolist())\n        day_arr.append(np.array(train_df.iloc[i+historical_steps][['Day']]).tolist())\n\n\nlmm_df = pd.DataFrame(np.hstack((x_arr, xf_arr)), columns=['L1','L2','L3','L4','L5','L6','L7', 'F1','F2','F3','F4','F5','F6','F7'])\nlmm_df['ConfirmedCases_log'] = sum(y_arr,[])\nlmm_df['ConfirmedCases'] = sum(y_inverse_arr,[])\n\nlmm_df['Fatalities_log'] = sum(yf_arr,[])\nlmm_df['Fatalities'] = sum(yf_inverse_arr,[])\n\nlmm_df['Geo'] = sum(grp_arr,[])\nlmm_df['Day'] = sum(day_arr,[])\nlmm_df.tail()","c5a8241d":"model = sm.MixedLM(endog=lmm_df['ConfirmedCases_log'], exog=lmm_df[['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7']], exog_re=lmm_df[['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7']], groups=lmm_df['Geo'])\nmodel_fat = sm.MixedLM(endog=lmm_df['Fatalities_log'], exog=lmm_df[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7']], exog_re=lmm_df[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7']],groups=lmm_df['Geo'])\nfitted = model.fit()\nfitted_fat = model_fat.fit()","21e2997c":"test_df['Geo'] = test_df['Province\/State'] + ',' + test_df['Country\/Region']\ntest_lmm_df = test_df\n\n# For each row\nfor i in range(len(test_lmm_df)):\n    \n    day = test_lmm_df.iloc[i].Day\n    geo = test_lmm_df.iloc[i]['Geo']\n    confirmedCases = 0\n    fatalities = 0\n    \n    # Find previous day\n    prev_df = lmm_df[(lmm_df['Day'] == day-1) & (lmm_df['Geo'] == geo)]\n    \n    # Confirmed Cases \n    if len(prev_df) != 0:\n        \n        # Generate new time lags \n        temp_l_df = prev_df[['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7']].iloc[:,1:]        \n        temp_l_df.columns = ['L1', 'L2', 'L3', 'L4', 'L5', 'L6']\n        temp_l_df['L7'] = prev_df['ConfirmedCases_log']        \n        \n        # Compute new exog array & Predict\n        confirmedCases_log = fitted.predict(exog=np.array(temp_l_df).reshape(-1).tolist())[0]        \n        confirmedCases = math.expm1(confirmedCases_log)\n        \n    # Fatalities \n    if len(prev_df) != 0:\n        \n        # Generate new time lags \n        temp_f_df = prev_df[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7']].iloc[:,1:]        \n        temp_f_df.columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6']\n        temp_f_df['F7'] = prev_df['Fatalities_log']        \n        \n        # Compute new exog array & Predict\n        fatalities_log = fitted_fat.predict(exog=np.array(temp_f_df).reshape(-1).tolist())[0]        \n        fatalities = math.expm1(fatalities_log)\n    \n    current_df = lmm_df[(lmm_df['Day'] == day) & (lmm_df['Geo'] == geo)]\n    \n    if len(current_df) != 0:\n        lmm_df = lmm_df.drop(current_df.index.tolist())\n    lmm_df = lmm_df.append(pd.Series([temp_l_df['L1'].values[0], temp_l_df['L2'].values[0], temp_l_df['L3'].values[0], temp_l_df['L4'].values[0], temp_l_df['L5'].values[0], temp_l_df['L6'].values[0], temp_l_df['L7'].values[0], \\\n                                      temp_f_df['F1'].values[0], temp_f_df['F2'].values[0], temp_f_df['F3'].values[0], temp_f_df['F4'].values[0], temp_f_df['F5'].values[0], temp_f_df['F6'].values[0], temp_f_df['F7'].values[0], \\\n                                      confirmedCases_log, confirmedCases, fatalities_log, fatalities, geo, day], index=lmm_df.columns ), ignore_index=True)\n                                        \nlmm_df[(lmm_df['Geo'] == 'Unknown,Korea, South')].tail()","f9a8ccec":"plot_df = lmm_df[(lmm_df['Geo'] == 'New South Wales,Australia')]\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","40563602":"plot_df = lmm_df[(lmm_df['Geo'] == 'Hubei,China')]\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","0040d447":"plot_df = lmm_df[(lmm_df['Geo'] == 'Unknown,Korea, South')]\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","ec7318ed":"plot_df = lmm_df[(lmm_df['Geo'] == 'Unknown,Italy')]\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","b9fb8135":"plot_df = lmm_df[(lmm_df['Geo'] == 'Unknown,Iran')]\nplt.plot(plot_df['Day'], plot_df['ConfirmedCases'].cumsum())","498c9176":"confirmedCases = []\nfatalities = [] \nfor i in range(len(test_df)):\n    \n    day = test_lmm_df.iloc[i].Day\n    geo = test_lmm_df.iloc[i]['Geo']    \n       \n    current_df = lmm_df[(lmm_df['Day'] == day) & (lmm_df['Geo'] == geo)]    \n    \n    if len(current_df) != 0:\n        confirmedCases.append(current_df['ConfirmedCases'].values[0])\n        fatalities.append(current_df['Fatalities'].values[0])        \n    else:\n        confirmedCases.append(0)\n        fatalities.append(0)    \n\ntest_df['ConfirmedCases'] = confirmedCases\ntest_df['Fatalities'] = fatalities\ntest_df.head()","a173ca16":"test_df[['ForecastId', 'ConfirmedCases', 'Fatalities']].to_csv('submission.csv', index=False)","ca4f73ca":"train_df[(train_df['Country\/Region'] == 'Korea, South') & (train_df['Province\/State'] == 'Unknown')].tail(20) \ntest_df[(test_df['Country\/Region'] == 'Korea, South') & (test_df['Province\/State'] == 'Unknown') ].head(25)\nlmm_df[(lmm_df['Geo'] == 'Unknown,Korea, South')].tail(35)\ntrain_df[(train_df['Country\/Region'] == 'Korea, South') & (train_df['Province\/State'] == 'Unknown') & (train_df['Day'] == 88)].head()\ntest_df[(test_df['Country\/Region'] == 'Korea, South') & (test_df['Province\/State'] == 'Unknown') & (test_df['Day'] == 88)].head()\n\n\n","42ab6d5d":"# Explore dataset","9fa24e67":"* Fill NaN values\n* Convert Date to Day count from the start date of major pandemic","4949a32c":"**Predict Next Day**","c00e1961":"* Logtransform ConfirmedCases because the distribution is left-skewed","1b9ef6ef":"# Model Selection","d0d4e94d":"# 1. LSTM Model (Initially Chosen)\n\n**Reshape dataset to sequences**\n\nInput data includes a series of historical responses from historical_steps to the given time wherareas targets only include last snapshot","ab1da113":"# Preprocess dataset","d24f7c80":"# 2. Linear Mixed Model (Finally Chosen)\n\nLinear Mixed Effects models are used for regression analyses involving dependent data. Such data arise when working with longitudinal and other study designs in which multiple observations are made on each subject. Some specific linear mixed effects models are\n\n* Random intercepts models, where all responses in a group are additively shifted by a value that is specific to the group.\n* Random slopes models, where the responses in a group follow a (conditional) mean trajectory that is linear in the observed covariates, with the slopes (and possibly intercepts) varying by group.\n* Variance components models, where the levels of one or more categorical covariates are associated with draws from distributions. These random terms additively determine the conditional mean of each observation based on its covariate values.","a3808e92":"**Prediction by Geography**","fe77c9ad":"**Train the model**","44e947d5":"Found that the predicted results are way inaccurate, and one of main reasons is due to that:\n* Depending on geograhy, ConfirmedCases & Fatalities data can show different degree in performance\n\nTo train all data in a model, random effects should beb computed for multiple countries and applied to the model.\n","ace726f8":"# Read dataset","aa3877e4":"**Create LSTM layers**"}}