{"cell_type":{"06041d68":"code","ce68d6b4":"code","d3994506":"code","8dca32f1":"code","7a4cd8a5":"code","ea20c435":"code","eb38fb46":"code","08cf043b":"code","d3e41519":"code","bc4c08f7":"code","5389f573":"code","74c0d16e":"code","1b06d6d1":"code","6d2d1c2c":"code","47fb2684":"code","3e76f0a4":"code","aa70e16b":"code","cb1d3689":"code","9411841d":"code","2ce36c3d":"code","37ba64cf":"code","f1760a51":"code","70b9961a":"code","364c89b2":"code","7dc2cc0f":"code","587875af":"code","90078e92":"code","8764098c":"code","e1d672ac":"code","195f5f0b":"code","71fbab6d":"code","eb83856b":"code","79166546":"code","655cc5ff":"code","acf509d7":"code","6cedecff":"code","129d239e":"code","f1bf4077":"code","7073972d":"code","dbec4c18":"code","78b66ff4":"code","10178fb4":"code","a12e03e2":"code","4f398c37":"code","31d515d3":"code","2f7dafcf":"code","31452c82":"code","922e844f":"code","0d3c47c2":"code","25708d3d":"code","594b60d3":"code","6ffd53aa":"code","2a7d4e10":"code","c3762cbf":"code","b91026ee":"code","e87f113d":"code","d7d32ed6":"code","05d17dbf":"code","361e2226":"code","cd42ff72":"code","d5158d8e":"code","0f987aa6":"code","b7ccc617":"code","6e923ea1":"code","53703678":"code","6fc16c45":"code","fe0e0c91":"code","16ab6a68":"markdown","e9623784":"markdown","1a6c9c73":"markdown","3e92abbb":"markdown","235c29cb":"markdown","040a2f41":"markdown","aa295a90":"markdown","aa96f57e":"markdown","8edf95a4":"markdown","82067096":"markdown","7b5085f3":"markdown","352387cb":"markdown","5c587e77":"markdown","c78bbdd0":"markdown","d6d07316":"markdown","e48a9df9":"markdown","9a1b5f80":"markdown","36fa4b58":"markdown","faa31934":"markdown","c703d962":"markdown","cc5df3b9":"markdown","8d7823a1":"markdown","f14482a4":"markdown","f3d1ddab":"markdown","c71469fe":"markdown","8f8f6e9a":"markdown","bf04abec":"markdown","56aa3441":"markdown","230f519e":"markdown","9e6901a4":"markdown","52d27ad7":"markdown","5946300c":"markdown","ab5b3bce":"markdown","4fe8a771":"markdown","5dfde3a1":"markdown","834b68e8":"markdown","a4293363":"markdown","2e653940":"markdown","73cce123":"markdown","ab7c52ee":"markdown","80919a8c":"markdown","51c79d94":"markdown","ae503389":"markdown","e062f161":"markdown","9454bf9d":"markdown","06462cd3":"markdown","8ddfbbec":"markdown","5d26c788":"markdown","71fc343a":"markdown","91d51dd7":"markdown","0b24403d":"markdown"},"source":{"06041d68":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ce68d6b4":"dataset = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\nprint('Dataset: ',dataset.head(5))\nprint('*******************************************************')\nprint('Dataset Shape: ',dataset.shape)\nprint('*******************************************************')\nprint(dataset.columns)\nprint('*******************************************************')\n","d3994506":"dataset.drop(['id','Unnamed: 32'],axis=1,inplace=True)\nprint(dataset.head(5))","8dca32f1":"Positive=dataset[dataset['diagnosis'].isin(['M'])]\nNegative=dataset[dataset['diagnosis'].isin(['B'])]\nprint('Positive',Positive)\nprint('*******************************************************')\nprint('Negative',Negative)","7a4cd8a5":"y=dataset.diagnosis\nB ,M = y.value_counts()\nprint('Number of Benign',B)\nprint('Number of Malignant',M)\nax=sns.countplot(y,data = dataset,label='Count')","ea20c435":"dataset.std()","eb38fb46":"dataset.corr()","08cf043b":"plt.figure(figsize=(30,15))\nsns.heatmap(dataset.corr(),annot=True)\nplt.show()","d3e41519":"dataset.corr()['radius_mean'].plot(kind='bar')","bc4c08f7":"dataset.isnull().sum()","5389f573":"import missingno as msno\nmsno.bar(dataset)","74c0d16e":"num = dataset.select_dtypes('number').columns.to_list()\ncat = dataset.select_dtypes('object').columns.to_list()\ndiagnosis_num = dataset[num]\ndiagnosis_cat = dataset[cat]\n\nprint('Number :')\nfor i in diagnosis_num.columns:\n  print(i)\nprint('----------------------------------')\nprint('Categorical :')\nfor v in diagnosis_cat.columns:\n  print(v)\n","1b06d6d1":"dataset['diagnosis'] = dataset['diagnosis'].map({'B':0,'M':1})","6d2d1c2c":"y = dataset['diagnosis'].values\ndataset.drop(['diagnosis'],axis=1,inplace=False)\nX = dataset.values","47fb2684":"from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2\nFeature_Selection  = SelectPercentile(score_func=chi2,percentile=70)\nprint('X shape :',X.shape)\nX = Feature_Selection.fit_transform(X,y)\nprint('X New shape :',X.shape)\nprint('Feature Selected : \\n',Feature_Selection.get_support())","3e76f0a4":"one_column = np.ones((X.shape[0],1))\nX = np.concatenate((one_column, X), axis = 1)","aa70e16b":"y = np.reshape(y,(y.shape[0],1))\ntheta2 = np.zeros(X.shape[1])","cb1d3689":"print('X: ',X[:3,:])\nprint('*******************************************************')\nprint('X Shape: ',X.shape)\nprint('*******************************************************')\nprint('Y: ',y[:3,:])\nprint('*******************************************************')\nprint('Y Shape: ',y.shape)\nprint('*******************************************************')\nprint('Theta ',theta2)\nprint('Theta Shape ',theta2.shape)\n","9411841d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX[:,1:] = sc.fit_transform(X[:,1:])\nprint(X[:2,:])","2ce36c3d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=22)\nprint('X_Train')\nprint(X_train[:3,:])\nprint('****************************')\nprint('X_test')\nprint(X_test[:3,:])\nprint(X_test.shape)\nprint('****************************')\nprint('Y_train')\nprint(y_train[:3,:])\nprint('****************************')\nprint('Y_test')\nprint(y_test[:3,:])","37ba64cf":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))\nn = np.arange(-20,20,step=1)\nplt.figure(figsize=(8,5))\nplt.plot(n,sigmoid(n),'r')","f1760a51":"def cost_function_reg(theta2,X2,y2,lr):\n    theta2 = np.matrix(theta2)\n    X2 = np.matrix(X2)\n    y2 = np.matrix(y2)\n    first = np.multiply(-y2, np.log(sigmoid(X2 * theta2.T)))\n    second = np.multiply((1 - y2), np.log(1 - sigmoid(X2 * theta2.T)))\n    total_equation=np.sum(first - second) \/ (len(X2))\n    reg = (lr \/ 2 * len(X2)) * np.sum(np.power(theta2[:,1:theta2.shape[1]], 2))\n    final = total_equation + reg\n    return final ","70b9961a":"def gradientReg(theta, X, y, learningRate):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    \n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n    \n    error = sigmoid(X * theta.T) - y\n    \n    for i in range(parameters):\n        term = np.multiply(error, X[:,i])\n        \n        if (i == 0):\n            grad[i] = np.sum(term) \/ len(X)\n\n        else:\n            grad[i] =(np.sum(term)\/len(X))+((learningRate\/len(X))*theta[:,i])\n\n    return grad\n\nlearningRate = 0.0001\n\n\nrcost = cost_function_reg(theta2, X_train, y_train, learningRate)\nprint()\nprint('regularized cost = ' , rcost)\nprint()\n\n","364c89b2":"import scipy.optimize  as opt\nresult = opt.fmin_tnc(func=cost_function_reg, x0=theta2, fprime=gradientReg,args=(X_train, y_train, learningRate))\nprint( 'result = ' , result )\nprint()\nCost_After_Optimize = cost_function_reg(result[0], X_train, y_train,learningRate)\nprint()\nprint('Cost_Function After Optimize = ' , Cost_After_Optimize)\nprint()","7dc2cc0f":"print('coefficient :',result[0])\nprint('No. Iteration :',result[1])","587875af":"def predict(theta, X):\n    probability = sigmoid(X * theta.T)\n    return [1 if x >= 0.5 else 0 for x in probability]\n\ntheta_min = np.matrix(result[0])\npredictions = predict(theta_min, X_test)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y_test)]\naccuracy_LR = np.sum(correct)\/ len(correct) *100\nprint('Accuracy',accuracy_LR)","90078e92":"from sklearn.metrics import confusion_matrix\ncm_LR  = confusion_matrix(y_test,predictions)\nprint(cm_LR)","8764098c":"plt.figure(figsize=(8,5))\nsns.heatmap(cm_LR,annot=True)\nplt.show()","e1d672ac":"from sklearn.metrics import classification_report\ncr_LR = classification_report(y_test,predictions)\nprint(cr_LR)","195f5f0b":"from sklearn.metrics import roc_curve,auc\ntpr_LR,fpr_LR,threshold_LR = roc_curve(y_test,predictions)\nprint('tpr_LR: ',tpr_LR)\nprint('fpr_LR: ',fpr_LR)\nprint('threshold_LR:',threshold_LR)\n\nauc_LR = auc(tpr_LR,fpr_LR)\nprint('Area Under The Curve :',auc_LR)\n\n#Draw ROC Curve && AUC [Area Under The Curve]\nplt.figure(figsize=(10,8))\nplt.plot(tpr_LR,fpr_LR,marker='o',color='blue',label='Logistic Regression (auc = %0.3f)'% auc_LR)\nplt.ylabel('True Positive Rate -->')\nplt.xlabel('False Positive Rate -->')\n\nplt.legend()\n\nplt.show()\n","71fbab6d":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nS_V_C = SVC(C =1,kernel='rbf', random_state=22)\nCross_Validation_S_V_C = cross_validate(S_V_C, X_train, y_train, cv=6, return_train_score=True)\n\nprint('Train score value :',Cross_Validation_S_V_C['train_score'])\nprint('Test score value :',Cross_Validation_S_V_C['test_score'])\n\nscore_value_S_V_C = cross_val_score(estimator=S_V_C, X = X_train, y = y_train, cv=6)\nprint('Cross_Validation Score is :',np.mean(score_value_S_V_C)*100)","eb83856b":"S_V_C.fit(X_train,y_train)","79166546":"y_pred_S_V_C = S_V_C.predict(X_test)\nprint('Y Test: \\n',y_test[:3,:])\nprint('Y Predict: ',y_pred_S_V_C[:3,])","655cc5ff":"from sklearn.metrics import accuracy_score\naccuracy_S_V_C=accuracy_score(y_test,y_pred_S_V_C)\nprint(accuracy_S_V_C)","acf509d7":"from sklearn.metrics import confusion_matrix\ncm_S_V_C=confusion_matrix(y_test,y_pred_S_V_C)\nprint(cm_S_V_C)\nplt.figure(figsize=(9,5))\nsns.heatmap(cm_S_V_C,center=True,annot=True)\nplt.show()","6cedecff":"from sklearn.metrics import classification_report\ncr_S_V_C=classification_report(y_test,y_pred_S_V_C)\nprint(cr_S_V_C)","129d239e":"from sklearn.metrics import roc_curve,auc\ntpr_S_V_C,fpr_S_V_C,threshold_S_V_C=roc_curve(y_test,y_pred_S_V_C)\nauc_S_V_C=auc(tpr_S_V_C, fpr_S_V_C)\nprint('tpr_S_V_C',tpr_S_V_C)\nprint('fpr_S_V_C',fpr_S_V_C)\nprint('threshold_S_V_C',threshold_S_V_C)\n\n\n#Draw ROC Curve && AUC [Area Under The Curve]\nplt.figure(figsize=(9, 5))\nplt.plot(tpr_S_V_C, fpr_S_V_C, linestyle=':', label='SVM (auc = %0.3f)' % auc_S_V_C)\n\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\n\nplt.legend()\n\nplt.show()","f1bf4077":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nknn = KNeighborsClassifier(n_neighbors=25, metric = 'minkowski', p = 2)\nCross_Validation_knn = cross_validate(knn, X_train,y_train,cv=6,return_train_score=True)\n\nprint('Train score value',Cross_Validation_knn['train_score'])\nprint('Test score value',Cross_Validation_knn['test_score'])\n\nscore_value_knn = cross_val_score(estimator=knn, X=X_train,y= y_train,cv=6)\nprint('Cross_Validation Score is :',np.mean(score_value_knn)*100)","7073972d":"knn.fit(X_train,y_train)","dbec4c18":"y_pred_knn=knn.predict(X_test)\nprint('Y Test:',y_test[:3,:])\nprint('Y Predict:',y_pred_knn[:3,])","78b66ff4":"from sklearn.metrics import accuracy_score\naccuracy_knn=accuracy_score(y_test,y_pred_knn)\nprint('Accuracy Score:',accuracy_knn)","10178fb4":"from sklearn.metrics import confusion_matrix\ncm_knn=confusion_matrix(y_test,y_pred_knn)\nprint(cm_knn)\nplt.figure(figsize=(9,5))\nsns.heatmap(cm_knn,center=True,annot=True)\nplt.show()","a12e03e2":"from sklearn.metrics import classification_report\ncr_knn=classification_report(y_test,y_pred_knn)\nprint(cr_knn)","4f398c37":"from sklearn.metrics import roc_curve,auc\nknn_tpr,knn_fpr,threshold=roc_curve(y_test,y_pred_knn)\nknn_auc=auc(knn_tpr,knn_fpr)\nprint('knn_tpr',knn_tpr)\nprint('knn_fpr',knn_fpr)\nprint('threhold',threshold)\n\nplt.figure(figsize=(9,5))\nplt.plot(knn_tpr,knn_fpr,linestyle='-', label='KNN (auc = %0.3f)' % knn_auc)\n\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\n\nplt.legend()\n\nplt.show()","31d515d3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\ndt=DecisionTreeClassifier(criterion = 'entropy',random_state=22,max_depth=10)\nCross_Validation_dt = cross_validate(dt, X_train,y_train,cv=6,return_train_score=True)\n\nprint('Train score value',Cross_Validation_dt['train_score'])\nprint('Test score value',Cross_Validation_dt['test_score'])\n\nscore_value_dt = cross_val_score(estimator=dt, X=X_train,y= y_train,cv=6)\nprint('Cross_Validation Score is :',np.mean(score_value_dt)*100)","2f7dafcf":"dt.fit(X_train,y_train)","31452c82":"y_pred_dt=dt.predict(X_test)\nprint('Y Test: ',y_test[:3,:])\nprint('Y Pred',y_pred_dt[:3,])\n","922e844f":"from sklearn.metrics import accuracy_score\naccuracy_dt=accuracy_score(y_test,y_pred_dt)\nprint('Accuracy Score',accuracy_dt)","0d3c47c2":"from sklearn.metrics import confusion_matrix\ncm_dt=confusion_matrix(y_test,y_pred_dt)\nprint(cm_dt)\nplt.figure(figsize=(9,5))\nsns.heatmap(cm_dt,center=True,annot=True)\nplt.show()","25708d3d":"from sklearn.metrics import classification_report\ncr_dt=classification_report(y_test,y_pred_dt)\nprint(cr_dt)","594b60d3":"from sklearn.metrics import roc_curve,auc\ndt_tpr,dt_fpr,threshold=roc_curve(y_test,y_pred_dt)\ndt_auc=auc(dt_tpr,dt_fpr)\nprint('dt_tpr Value  : ', dt_tpr)\nprint('dt_fpr Value  : ', dt_fpr)\nprint('thresholds Value  : ', threshold)\n\n#Draw ROC Curve && AUC [Area Under The Curve]\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot(dt_tpr, dt_fpr, linestyle='--', label='DecisionTree (auc = %0.3f)' % dt_auc)\n\nplt.xlabel('True Positive Rate -->')\nplt.ylabel('False Positive Rate -->')\n\nplt.legend()\nplt.show()","6ffd53aa":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nrf=RandomForestClassifier(criterion = 'entropy',random_state=22,max_depth=10,n_estimators=50)\nCross_Validation_rf = cross_validate(rf, X_train,y_train,cv=6,return_train_score=True)\n\nprint('Train score value',Cross_Validation_rf['train_score'])\nprint('Test score value',Cross_Validation_rf['test_score'])\n\nscore_value_rf = cross_val_score(estimator=rf, X=X_train,y= y_train,cv=6)\nprint('Cross_Validation Score is :',np.mean(score_value_rf)*100)","2a7d4e10":"rf.fit(X_train,y_train)","c3762cbf":"y_pred_rf=rf.predict(X_test)\nprint('Y Test: ',y_test[:3,:])\nprint('Y Pred',y_pred_rf[:3,])\n","b91026ee":"from sklearn.metrics import accuracy_score\naccuracy_rf=accuracy_score(y_test,y_pred_rf)\nprint('Accuracy Score',accuracy_rf)","e87f113d":"from sklearn.metrics import confusion_matrix\ncm_rf=confusion_matrix(y_test,y_pred_rf)\nprint(cm_rf)\nplt.figure(figsize=(9,5))\nsns.heatmap(cm_rf,center=True,annot=True)\nplt.show()","d7d32ed6":"from sklearn.metrics import classification_report\ncr=classification_report(y_test,y_pred_rf)\nprint(cr)","05d17dbf":"from sklearn.metrics import roc_curve,auc\nrf_tpr,rf_fpr,threshold=roc_curve(y_test,y_pred_rf)\nrf_auc=auc(rf_tpr,rf_fpr)\nprint('rf_tpr Value  : ', rf_tpr)\nprint('rf_fpr Value  : ', rf_fpr)\nprint('thresholds Value  : ', threshold)\n\n#Draw ROC Curve && AUC [Area Under The Curve]\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot(rf_tpr, rf_fpr, linestyle='-', label='Random Force (auc = %0.3f)' % rf_auc)\n\nplt.xlabel('True Positive Rate -->')\nplt.ylabel('False Positive Rate -->')\n\nplt.legend()\nplt.show()","361e2226":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nNB=BernoulliNB()\nCross_Validation_NB = cross_validate(NB, X_train,y_train,cv=6,return_train_score=True)\n\nprint('Train score value',Cross_Validation_NB['train_score'])\nprint('Test score value',Cross_Validation_NB['test_score'])\n\nscore_value_NB = cross_val_score(estimator=NB, X=X_train,y= y_train,cv=6)\nprint('Cross_Validation Score is :',np.mean(score_value_NB)*100)","cd42ff72":"NB.fit(X_test,y_test)\nprint('Naive Bayse Test Score',NB.score(X_test,y_test))","d5158d8e":"#Calculating Prediction\ny_pred_NB = NB.predict(X_test)\ny_pred_prob = NB.predict_proba(X_test)\ny_pred_prob=y_pred_prob.astype(int)\nprint('Y Test \\n' ,y_test[:10,:])\nprint('Predicted Value for BernoulliNBModel is :\\n ' , y_pred_NB[:10,])\nprint('Prediction Probabilities Value for BernoulliNBModel is : \\n' , y_pred_prob[:10,:])","0f987aa6":"from sklearn.metrics import accuracy_score\naccuracy_NB=accuracy_score(y_test,y_pred_NB)\nprint(accuracy_NB)","b7ccc617":"from sklearn.metrics import confusion_matrix\ncm_NB=confusion_matrix(y_test,y_pred_NB)\nprint(cm_NB)\nplt.figure(figsize=(9,5))\nsns.heatmap(cm_NB,center=True,annot=True)\nplt.show()","6e923ea1":"from sklearn.metrics import classification_report\ncr_NB=classification_report(y_test,y_pred_NB)\nprint(cr_NB)","53703678":"from sklearn.metrics import roc_curve,auc\nnb_tpr,nb_fpr,threshold=roc_curve(y_test,y_pred_NB)\nnb_auc=auc(nb_tpr,nb_fpr)\nprint('nb_tpr Value  : ', nb_tpr)\nprint('nb_fpr Value  : ', nb_fpr)\nprint('thresholds Value  : ', threshold)\n\n#Draw ROC Curve && AUC [Area Under The Curve]\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot(rf_tpr, rf_fpr, linestyle='-', label='Naive Basye (auc = %0.3f)' % nb_auc)\n\nplt.xlabel('True Positive Rate -->')\nplt.ylabel('False Positive Rate -->')\n\nplt.legend()\nplt.show()","6fc16c45":"from sklearn.metrics import roc_curve, auc\n\ntpr_LR,fpr_LR,threshold_LR=roc_curve(y_test,predictions)\nauc_LR = auc(tpr_LR, fpr_LR)\n\ntpr_S_V_C,fpr_S_V_C , threshold_S_V_C = roc_curve(y_test, y_pred_S_V_C)\nsvm_auc = auc(tpr_S_V_C, fpr_S_V_C)\n\nknn_fpr, knn_tpr, threshold = roc_curve(y_test, y_pred_knn)\nknn_auc = auc(knn_fpr, knn_tpr)\n\ndt_fpr, dt_tpr, threshold = roc_curve(y_test, y_pred_dt)\ndt_auc = auc(dt_fpr, dt_tpr)\n\nrf_fpr, rf_tpr, threshold = roc_curve(y_test, y_pred_rf)\nrf_auc = auc(rf_fpr, rf_tpr)\n\n\nnb_fpr, nb_tpr, threshold = roc_curve(y_test, y_pred_NB)\nnb_auc = auc(nb_fpr, nb_tpr)\n\nplt.figure(figsize=(9, 5))\nplt.plot(tpr_LR, fpr_LR, marker='o', label='Logistic Regression (auc = %0.3f)' % auc_LR)\nplt.plot(tpr_S_V_C, fpr_S_V_C, linestyle='--', label='SVM (auc = %0.3f)' % auc_S_V_C)\nplt.plot(knn_fpr, knn_tpr, linestyle=':', label='KNN (auc = %0.3f)' % knn_auc)\nplt.plot(dt_fpr, dt_tpr, linestyle='-', label='DT (auc = %0.3f)' % dt_auc)\nplt.plot(rf_fpr, rf_tpr, linestyle='-', label='RF (auc = %0.3f)' % rf_auc)\nplt.plot(nb_fpr, nb_tpr, linestyle='--', label='NB (auc = %0.3f)' % nb_auc)\n\n\n\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\n\nplt.legend()","fe0e0c91":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','Support Vector Machines', 'KNN','Decision Tree', \n              'Random Forest', 'Naive Bayes'],\n    'Score': [accuracy_LR, accuracy_S_V_C, accuracy_knn, \n              accuracy_dt, accuracy_rf, accuracy_NB]})\nmodels.sort_values(by='Score', ascending=False)","16ab6a68":"> Naive Bayes : -","e9623784":"> Dataset information","1a6c9c73":"> Missing Dataset","3e92abbb":"> Logistic Regression","235c29cb":"> Variance In Data","040a2f41":"> In the beginning I will talk about the Introduction to breast cancer\n\n> Second, we will explain what it is Data Preprocessing and why you use it ? and what are the Data Preprocessing steps?\n\n> Finally, we will talk about the algorithms used in the project","aa295a90":"> The Algorithms used in the Project : -\n1. Logistic Regression\n2. Support Vector Machine [ SVM ]\n3. K Nearest Neighbors [ KNN ] \n4. Decision Tree \n5. Random Force\n6. Naive Bayes\n\n> Evaluation Classification Methods Performance : - [ Metrics Model ]\n1. Confusion Matrix\n2. Classification Report\n3. ROC Curve && AUC [ Area Under The Curve ]\n\n\n","aa96f57e":"> Roc Curve And AUC","8edf95a4":"> Compare all the algorithm of the AUC","82067096":"> Separated dataset into Independent variables && dependent variables\n","7b5085f3":"> Splitting the dataset into the Training set and Test set\n","352387cb":"> Random Force : -","5c587e77":"> Accurance Score : -","c78bbdd0":"> Roc Curve And AUC","d6d07316":"> Accurac Score","e48a9df9":"> Support Vector Machine [ SVM ]","9a1b5f80":"> Roc Curve And AUC [ Area Under The Curve ]","36fa4b58":"\n\n> Importing The Dataset\n\n","faa31934":"> Introduction to breast cancer\n\nBreast cancer is a malignant cell growth in the breast. If left untreated, the cancer spreads to other areas of the body., breast cancer is the most common type of cancer in women in the United States, accounting for one of every three cancer diagnoses.\n\n  The incidence of breast cancer rises after age 40. The highest incidence (approximately 80% of invasive cases) occurs in women over age 50.\n\n  Cancer begins when healthy cells in the breast change and grow out of control, forming a mass or sheet of cells called a tumor. A tumor can be cancerous or benign. A cancerous tumor is malignant, meaning it can grow and spread to other parts of the body. A benign tumor means the tumor can grow but will not spread.","c703d962":"> Accuracy Score ","cc5df3b9":"> Accuracy Score : -","8d7823a1":"> Feature Selection","f14482a4":"> Classification Report : -","f3d1ddab":"Classification Report","c71469fe":"> Comfusion Matrix","8f8f6e9a":"> Compare all the algorithm of the AUC","bf04abec":"> Correlation In Data","56aa3441":"> Classification Report ","230f519e":"> Confusion Matrix : -","9e6901a4":"> Roc Curve And AUC [ Area Under the Graph]","52d27ad7":"> In the end, we will compare all the algorithms to each other and see which one is the best ","5946300c":"> Roc Curve And AUC [ Area Under Tge Curve ]","ab5b3bce":"\n> Importing The Libraries\n\n\n","4fe8a771":"> Logistic Regression without Library ","5dfde3a1":"> Classification Report : -","834b68e8":"> K Nearest Neighbors [ KNN ]","a4293363":"# Project Name: Predicting breast cancer detection\n","2e653940":"> Confusion Matrix","73cce123":"> What is mean Data Pre-processing ? and why you use it?\n\nData Pre-processing it is one of the most important steps data mining.The way you collect data is often controlled incorrectly, result out of range for example: income= -100 , or impossible data for example: gender:male -> prednant:yes , or missing data.\nAnalyzing data that has not been examined carefully can lead to misleading or illogical results.\nIf a lot of data is irrelevant or unorganized. discovered during training take large amount of processing time so we uesd data preprocessing\n\n> What are the Data Pre-processing steps?\n\n\n1-Importing The Libraries\n\n2-Importing The Dataset\n\n3-Missing Data\n\n4-Encoding categorical data\n\n5-Feature Selection\n\n6-Feature Scaling\n\n7-Splitting the dataset into the Training set and Test set\n","ab7c52ee":"> Confusion Matrix","80919a8c":"> Decision Tree : -","51c79d94":"> There are other things that you should know :-\n1. Variance in data\n2. Correlation in data \n3. Cross Validation","ae503389":"> Roc Curve And AUC [ Area Under The Roc ]\n","e062f161":"> Encoding categorical data","9454bf9d":"> Classification Report : -","06462cd3":"> Classification Report","8ddfbbec":"> Feature Scaling","5d26c788":"> Confusion Matrix","71fc343a":"> Confusion Matrix :-\n","91d51dd7":"> Drop Unless Columns","0b24403d":"> Accuracy Score : -"}}