{"cell_type":{"02ff08de":"code","583587c5":"code","d03fd962":"code","3bc2c9d8":"code","8d7980f5":"code","73f340f1":"code","9ebb050f":"code","f9e72258":"code","bf514d49":"code","f29f4aab":"code","7f7b4016":"code","96fd0a49":"markdown","d7e5da05":"markdown","f50f0905":"markdown","5389d692":"markdown","0228c03e":"markdown","33bc5f95":"markdown","721f9521":"markdown","8c16dbb7":"markdown"},"source":{"02ff08de":"import torch as th\nimport numpy as np\nfrom tqdm import tqdm, trange\nfrom typing import Tuple\n\ndevice = \"cuda:0\" if th.cuda.is_available() else \"cpu\"","583587c5":"class FastTensorDataLoader:\n    \"\"\"\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https:\/\/discuss.pytorch.org\/t\/dataloader-much-slower-than-manual-batching\/27014\/6\n    \"\"\"\n\n    def __init__(self, *tensors, batch_size):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        \"\"\"Reset i and retrun self\"\"\"\n        self.i = 0\n        return self\n\n    def __next__(self):\n        \"\"\"Return the next batch.\"\"\"\n        if self.i >= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches","d03fd962":"def _check_parity(x:np.ndarray)-> int:\n    \"\"\"Return even parity check of x\"\"\"\n    return np.sum(x[x==1])%2\n\ndef get_data(N:int, input_size:int,batch_size=128,seq_range:list=[0,0])->np.ndarray:\n    \"\"\"Generate a dataset for parity.\n    \n    Args:\n        N: The number of samples\n        input_size: The length of parity vector\n    \n    Retruns:\n        X,y: Tuple of X,y dataset for parity problem\n    \"\"\"\n    min_len,max_len = seq_range\n    if max_len <= 1:\n        max_len = input_size\n    if min_len < 1:\n        min_len = 1\n        \n    X = th.zeros((N,input_size))\n    for i in range(N):\n        size = np.random.randint(min_len, max_len+1)\n        x = 2*np.random.randint(2, size=size) - 1\n        X[i,:x.shape[0]] = th.Tensor(x)\n    y = th.Tensor(np.apply_along_axis(_check_parity, 1, X))\n    return FastTensorDataLoader(X,y, batch_size=batch_size)\n\nnext(iter(get_data(5,7,2,[-1,-1])))","3bc2c9d8":"next(iter(get_data(5,7,2,[-1,3])))","8d7980f5":"class StepModule(th.nn.Module):\n    def __init__(self,\n                 input_size:int,\n                 max_steps:int,\n                 output_size:int=1,\n                 hidden_size:int=128,\n                ):\n        \n        super(StepModule, self).__init__()\n        self.rnn = th.nn.RNNCell(input_size, hidden_size, nonlinearity=\"tanh\")\n        # layers for lambda\n        self.fc_lambda = th.nn.Sequential(\n                                th.nn.Linear(hidden_size,1),\n                                th.nn.Sigmoid())\n        # output layer\n        self.fc_output = th.nn.Linear(hidden_size,output_size)\n                                \n        \n        self.is_inference = False\n        self.hidden_size = hidden_size\n        self.max_steps = max_steps\n    \n    def forward(self,x:th.Tensor)->Tuple[th.Tensor, th.Tensor, th.Tensor, th.Tensor]:\n        # y_n, h_{n+1}, lambda_n = s(x, h_n)\n        batch_size = x.shape[0]\n\n        hx = self.rnn(x)\n        # values of all ps and ys\n        \n        p = []\n        y = []\n        # probability it is unhalted so far\n        # (1- lambda_n-1)*(1- lambda_n-2)*...*(1- lambda_1)\n        \n        # values after halt\n        p_m = th.zeros(batch_size).to(device)\n        y_m = th.zeros(batch_size).to(device)\n        \n        # vector representing if halted\n        halted = th.zeros(batch_size).to(device)\n        \n        prob_unhalted = th.ones(batch_size).to(device) # base case (= 1)\n        for n in range(self.max_steps):\n            if n == self.max_steps: \n            # if max seq len lambda_n = 1\n                lambda_n = th.zeros(batch_size)\n            else:\n                lambda_n = self.fc_lambda(hx)[:, 0] # take the first values (or last) of hidden\n            \n            y_n = self.fc_output(hx)[:, 0]\n            \n            # pn for current step (n)\n            p_n = lambda_n*prob_unhalted\n            \n            y.append(y_n)\n            p.append(p_n)\n            # update prob_unhalted\n            prob_unhalted = (1 - lambda_n)*prob_unhalted\n            \n            halt = th.bernoulli(lambda_n) * (1 - halted) # (1-halted) checks if halted\n            \n            p_m = p_m*(1 - halt) + p_n*(halt)\n            y_m = y_m*(1 - halt) + y_n*(halt)\n            \n            halted += halt\n            \n            \n            # next hidden state\n            hx = self.rnn(x, hx)\n            \n            if self.is_inference and halted.sum() == batch_size:\n                break # break if inference and halted for all samples in batch_size\n            \n        return th.stack(p), th.stack(y), p_m, (y_m>0).float()","73f340f1":"class PonderLoss(th.nn.Module):\n    def __init__(self,\n                 beta:float,\n                 lambda_p:float,\n                 max_steps:int,\n                 loss:th.nn.Module = th.nn.BCEWithLogitsLoss(reduction=\"none\")):\n        super(PonderLoss, self).__init__()\n        # loss function used in reconstruction loss\n        self.loss_fn = loss\n        # prior for regularization loss\n        prior = th.zeros(max_steps).to(device)\n        not_halted = 1.\n        for k in range(max_steps):\n            prior[k] = lambda_p*not_halted\n            not_halted = (1 - lambda_p)*not_halted\n        \n        self.prior = th.nn.Parameter(prior, requires_grad=False)\n        # klloss calculated KL Divergence\n        self.klloss = th.nn.KLDivLoss(reduction=\"batchmean\")\n        # hyper parameter\n        self.beta = beta\n    \n    def forward(self, probs, pred, target):\n        batch_size = probs.shape[0]\n        l_rec = th.tensor(0.).to(device)\n        \n        for n in range(batch_size):\n            loss = (probs[n]*self.loss_fn(pred[n], target)).mean()\n            l_rec += loss \n        \n        probs = probs.transpose(0,1)\n        l_reg = self.beta*self.klloss(probs.log(), self.prior.expand_as(probs))\n        return th.sum(l_rec + l_reg)","9ebb050f":"def train(model:th.nn.Module,\n          input_size:int,\n          max_steps:int,\n          beta:float=0.01,\n          lambda_p:float=0.2,\n          lr:float=0.0003,\n          max_epochs:int=10,\n          batch_size:int=128,\n          no_of_batches:int=100,\n          seq_range:tuple=[1,0],\n          **kwargs\n         ):\n    model.to(device)\n    \n    criterion = PonderLoss(beta=beta,lambda_p=lambda_p,\n                           max_steps=max_steps)\n    optim = th.optim.Adam(model.parameters(),lr=lr)\n    \n    data_loader = get_data(batch_size*no_of_batches,\n                           input_size=input_size,\n                           batch_size=batch_size,\n                           seq_range=seq_range # the length of -1,1 seq\n                          )\n    with trange(max_epochs, ncols=100) as pbar:\n        for epoch in pbar:\n            t_loss = 0.\n            for i,(data,target) in enumerate(data_loader):\n                data = data.to(device)\n                target = target.to(device)\n            \n                optim.zero_grad()\n                p,y,p_m,y_m = model(data)\n                loss = criterion(p,y,target)\n                loss.backward()\n                optim.step()\n                t_loss += loss.item()\n            pbar.set_postfix({\"loss\":t_loss\/i})","f9e72258":"config = {\n    # model\n    \"input_size\":8,\n    \"output_size\":1,\n    \"hidden_size\":128,\n    \"max_steps\":10,\n    # loss fn\n    \"beta\":0.01,\n    \"lambda_p\":0.2,\n    # optim\n    \"lr\":0.0003,\n    \"max_epochs\":100,\n    # data \n    \"batch_size\":256,\n    \"no_of_batches\":200\n}","bf514d49":"model = StepModule(input_size=config[\"input_size\"],\n                   max_steps=config[\"max_steps\"],\n                   output_size=config[\"output_size\"],\n                   hidden_size=config[\"hidden_size\"])\n    \ntrain(model, **config)","f29f4aab":"total = 0\ncorrect = 0\nwith th.no_grad():\n    model.is_inference = True\n    for data,target in get_data(500,8,batch_size=1):\n        data = data.to(device)\n        target = target.to(device)\n        \n        _,_,_,ym = model(data)\n        total += ym.size(0)\n        correct += (ym == target).sum().item()\n\nprint(f\"Accuracy: {100*correct\/total}\")","7f7b4016":"# The last empty cell","96fd0a49":"## PonderNet","d7e5da05":"# Eval \/ Inference","f50f0905":"The PonderNet architecture requires a step function $s$ of the form\n\n$$\n\\hat{y_n}, h_{n+1}, \\lambda_n = s(x,h_n)\n$$\n\nas well as initial state $h_0$\n\n- $\\hat{y_n}$ the network's prediction\n- $\\lambda_n$ scalar probability of halting at step $n$\n- $s$ The step function can be any neural network -> MLPs, LSTMs ...\n\nWe apply the step function recurrently up to $N$ times, i.e, $n \\in \\{1,...,N\\}$\n\n$$\nP(\\Lambda_n = 1|\\Lambda_{n\u22121} = 0) = \\lambda_n  \\forall 1 \\leq n \\leq N \n$$\n\nDerieved probability distribution $p_n$\n\n$$\np_n = \\lambda_n \\prod_{j=1}^{n-1}(1 - \\lambda_j)\n$$\n\n![](https:\/\/i.ibb.co\/s3GV9PL\/diagram.jpg)","5389d692":"### Step function","0228c03e":"### Training","33bc5f95":"## Data for Parity Problem","721f9521":"### Loss function\n\n$$\nL = \\sum_{n=1}^{N}p_n \\mathcal{L}(y,\\hat{y_n}) + \\beta KL(p_n|| p_G(\\lambda_p))\n$$\n\n$p_G(\\lambda_p)$ geometric prior distribution \n","8c16dbb7":"# PonderNet: Learning to Ponder"}}