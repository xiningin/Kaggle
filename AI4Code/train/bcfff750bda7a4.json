{"cell_type":{"8e793c1f":"code","87232e12":"code","d171a734":"code","7eb61221":"code","7cf55ffd":"code","9ad5345b":"code","8ff42fc8":"code","e4773fa8":"code","a1ef6d28":"code","e73d96ee":"code","dc55e6f8":"code","dd510184":"code","966c8422":"code","7add0551":"code","1b68c127":"code","0e059de3":"code","512a3ba0":"code","6b9aa5f2":"code","66127db9":"code","64043395":"code","b5e90e0d":"code","c4640d62":"code","70a1c2f6":"code","74929aeb":"code","eb189b74":"code","cd893b7a":"code","60752a24":"code","f2ca5d52":"code","fd0d6c2a":"code","0b27f562":"code","2bab92c1":"code","557b20bd":"code","a55042fc":"code","d73b2b15":"code","a0f340af":"markdown","ba747833":"markdown","24b2ecd2":"markdown","d0bd2cc4":"markdown","f4ee8e64":"markdown"},"source":{"8e793c1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","87232e12":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport math\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nmatplotlib.rcParams['figure.figsize'] = ((15,5))","d171a734":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","7eb61221":"# lets look into top 5 rows\nprint(\"Top 5 Rows\\n\",)\ntrain.head()","7cf55ffd":"# lets look into random 5 rows\nprint(\"Random 5 Rows\\n\",)\ntrain.sample(5)","9ad5345b":"# Info of training data\ntrain.info()","8ff42fc8":"# as we can see there are missing values in training data, lets look using seaborn\nsns.heatmap(train.isna(), annot=False, )","e4773fa8":"print(\"Average age of entire data:\" , math.ceil(train.Age.mean()))\nsns.distplot(train.Age, kde=False, )","a1ef6d28":"sns.boxplot(y=train.Age, x= train.Pclass, palette=\"magma\")","e73d96ee":"# There are multiple approch to fill missing values, we trying one simple manual approach here!\n\n# train.Age=train.Age.fillna(train.Age.mean())\nfor i in range(len(train.Age)):\n      if pd.isnull(train.Age[i]):\n        if train.Pclass[i]==1:\n            train.Age[i]=37\n        elif train.Pclass[i]==2:\n            train.Age[i]=30  \n        elif train.Pclass[i]==3:\n            train.Age[i]=25\n            \nfor i in range(len(test.Age)):\n      if pd.isnull(test.Age[i]):\n        if test.Pclass[i]==1:\n            test.Age[i]=37\n        elif test.Pclass[i]==2:\n            test.Age[i]=30  \n        elif test.Pclass[i]==3:\n            test.Age[i]=25            \n            \ntest.Fare.fillna(test.Fare.mean(), inplace=True)            ","dc55e6f8":"# Lets see if there is any correlation between any numeric column\nsns.heatmap(train.drop([\"PassengerId\"], axis=1).corr(), annot=True, )\nplt.tight_layout()","dd510184":"sns.countplot(train.Sex, palette=\"magma\")","966c8422":"print(train.Survived.value_counts())\nsns.countplot(train.Survived)","7add0551":"sns.countplot(x=train.Survived, hue=train.Sex,)","1b68c127":"sns.countplot(x=train.Survived, hue=train.SibSp,)","0e059de3":"sns.countplot(x=train.Survived, hue=train.Pclass)","512a3ba0":"plt.figure(figsize=(12,5))\nsns.distplot(train.Fare, kde=False, bins=10, )","6b9aa5f2":"train.head(2)","66127db9":"# So here, we are applying OHE on Sex and Embarked column\nSex_dummies=pd.get_dummies(train.Sex)\nEmbarked_dummies=pd.get_dummies(train.Embarked)\ntrain=pd.concat([train,Sex_dummies,Embarked_dummies], axis=1)\ntrain=train.drop(['Sex', 'Embarked'],axis=1)\n\nSex_dummies=pd.get_dummies(test.Sex)\nEmbarked_dummies=pd.get_dummies(test.Embarked)\ntest=pd.concat([test,Sex_dummies,Embarked_dummies], axis=1)\ntest=test.drop(['Sex', 'Embarked'],axis=1)\n\ntrain.head()","64043395":"print(\"Percentage of NaN value in Canbin:\", (train.Cabin.isna().sum()\/train.Cabin.isna().count())*100)","b5e90e0d":"PassengerId=test['PassengerId']","c4640d62":"# Now lets remove few irrelevant columns, columns that  do not play any crucial role and has too many NaN value.\ntrain.drop(['Name', 'Cabin', 'PassengerId', 'Ticket'], axis=1,inplace=True)\ntest.drop(['Name', 'Cabin', 'PassengerId', 'Ticket'], axis=1,inplace=True)\ntrain.head(2)","70a1c2f6":"sns.pairplot(train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']],)","74929aeb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), train.Survived,\n                                                    test_size=0.25)","eb189b74":"from sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\ndef kfold_and_confusion_matrix(model):\n    kfold = KFold(n_splits=5)\n    model_kfold = model\n    results_kfold = model_selection.cross_val_score(model_kfold, X_train, y_train,  cv=kfold)\n    print(\"K Fold Accuracy: %.2f%%\" % (results_kfold.mean()*100.0)) \n    print(\"Confusion Matrix from Testing data:\")\n    y_pred=model.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')","cd893b7a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\nkfold_and_confusion_matrix( model)","60752a24":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nlogregpipe = Pipeline([('scale', StandardScaler()),\n                   ('logreg',LogisticRegression())])\n\n# Gridsearch to determine the value of C\nparam_grid = {'logreg__C':np.arange(0.01,1,30)}\nlogreg_cv = GridSearchCV(logregpipe,param_grid,cv=5,return_train_score=True)\nlogreg_cv.fit(X_train,y_train)\nprint(logreg_cv.best_params_)\n\n\nbestlogreg = logreg_cv.best_estimator_\nbestlogreg.fit(X_train,y_train)\nbestlogreg.coef_ = bestlogreg.named_steps['logreg'].coef_\nprint(\"Normal Accuracy: %.2f%%\" % (bestlogreg.score(X_train,y_train)*100))\n\nkfold_and_confusion_matrix( bestlogreg)","f2ca5d52":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix( model)","fd0d6c2a":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(model)","0b27f562":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(class_weight=\"balanced\", n_estimators=250)\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(model)","2bab92c1":"from sklearn.ensemble import  GradientBoostingClassifier\ngb_model = GradientBoostingClassifier()\ngb_model.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(gb_model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix( gb_model)","557b20bd":"import lightgbm as lgb\nfrom sklearn import metrics\n\n\nlg = lgb.LGBMClassifier(silent=False)\nparam_dist = {\"max_depth\": [25,50, 75],\n              \"learning_rate\" : [0.01,0.05,0.1],\n              \"num_leaves\": [300,900,1200],\n              \"n_estimators\": [100,200,250,300]\n             }\ngrid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3,)\ngrid_search.fit(X_train,y_train)\n# grid_search.best_estimator_\nmodel= grid_search.best_estimator_\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix( model)\n","a55042fc":"import xgboost as xgb\nfrom sklearn import metrics\n# Parameter Tuning\nmodel = xgb.XGBClassifier()\nparam_dist = {\"max_depth\": [10,30,50],\n              \"min_child_weight\" : [1,3,6],\n              \"n_estimators\": [200],\n              \"learning_rate\": [0.05, 0.1,0.16],}\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nmodel=grid_search.best_estimator_\n# model = xgb.XGBClassifier(max_depth=50, min_child_weight=1,  n_estimators=200,\\\n#                           n_jobs=-1 , verbose=1,learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(model)","d73b2b15":"result=pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\nresult[\"PassengerId\"]=PassengerId\nresult[\"Survived\"]= gb_model.predict(test)\nresult.to_csv(\"Submission.csv\", index=False)","a0f340af":"**Now, as I am beginner, I was confused between One Hot Encoding(OHE) and Label Encoding(LE) for non-numerical columns. After going through few good blogs, I found OHE is generally used for nominal data(where order is not important) and LE is generally used in case of ordinal data(where order pays crucial role eg., High school< Secondary School<UG)**","ba747833":"### This shows, we missing values in Age and Cabin column","24b2ecd2":"### **After seeing above heatmap, for now, we can say Fare has highest correlation with survival**","d0bd2cc4":"### Creating a function for KFold Cross-Validation and Confusion Matrix(using heatmap)","f4ee8e64":"Survived is our target class!"}}