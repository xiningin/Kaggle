{"cell_type":{"80ed78b2":"code","3c1c7d42":"code","dea4fcdf":"code","b089feff":"code","b503b131":"code","a7f3149d":"code","e4e0df6e":"code","74180ae6":"code","3b2fa409":"code","aed45179":"code","61cfa64f":"code","9c1abc29":"code","ed68d76d":"code","a33e10bf":"code","72899b16":"code","961de366":"code","7feec6ba":"code","dffcfbbf":"code","2cc680fb":"code","37e85c09":"code","95b2cddc":"code","cb098d92":"code","589875a7":"code","1e8fabcf":"code","216b2f62":"code","d0b270ab":"code","9d8dcfee":"code","db9f12ca":"code","25bcefc9":"code","bd191c4c":"code","0abdf565":"code","fcb2948a":"code","31836ce1":"code","f142419e":"code","1218f2fb":"code","039aaf65":"code","7a2654a3":"code","1b16e720":"code","a2cf0825":"code","f59ef5d6":"code","97f8faad":"code","d24a29ba":"code","60a975b8":"code","18a10e1a":"code","2ac9ca88":"code","6d5cc652":"code","42d25541":"code","5c15cd29":"code","3d546e5d":"code","b0d15a3b":"code","072871e8":"code","8b07586e":"code","c13e4dde":"code","58f012c7":"code","0f624e24":"code","9d91deff":"code","f2e7e5b3":"code","dac70788":"code","b1482395":"code","7fa4aeaf":"code","ccf59cc1":"code","74f0ba84":"code","ec29163c":"code","e63e79d0":"code","6bc6ca3f":"code","0a932642":"code","499390bc":"code","809c0eff":"code","f864fc9f":"code","a9ff342b":"code","9f2da2b9":"code","7da8f793":"code","003c8eff":"code","c8e764dd":"code","9a8971b2":"code","e4de9bb5":"code","1bbc8e09":"code","a0f9f2bd":"markdown","e5e0f8fd":"markdown","94053a4b":"markdown","b3c9e17f":"markdown","900c608a":"markdown","a55104ea":"markdown","f2da38b3":"markdown","59eab12d":"markdown","1435ef6c":"markdown","e7844991":"markdown","af56b694":"markdown","14531bf2":"markdown","a5b47425":"markdown","f6c236ff":"markdown","312cab14":"markdown","faaf0f1f":"markdown","2065bebe":"markdown","44e0fc22":"markdown","1ac979f9":"markdown","029eb6f3":"markdown","8644d899":"markdown","26d5b3f8":"markdown","467ca61a":"markdown","0200b11c":"markdown","04973185":"markdown","a73ba3c3":"markdown","de494763":"markdown","2a32ef38":"markdown","66697d7e":"markdown","0d53977b":"markdown","65437944":"markdown"},"source":{"80ed78b2":"#Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n%matplotlib inline","3c1c7d42":"# # Loading the datasets. Since Passenger ID is a personal information it is not relevant as an attribute to build the predictive\n#model and will be used as an index.\ndataTrain = pd.read_csv('..\/input\/titanic\/train.csv', index_col = 'PassengerId')\ndataTest = pd.read_csv('..\/input\/titanic\/test.csv', index_col = 'PassengerId')","dea4fcdf":"dataTrain.head()","b089feff":"dataTest.head()","b503b131":"dataTrain.dtypes","a7f3149d":"dataTest.dtypes","e4e0df6e":"# These attributes are not important to predict the result\ndataTrain.drop(columns=['Name',\"Ticket\"], inplace =True)\ndataTest.drop(columns=['Name',\"Ticket\"], inplace =True)","74180ae6":"dataTrain.head()","3b2fa409":"dataTest.head()","aed45179":"dataTrain.shape","61cfa64f":"dataTest.shape","9c1abc29":"dataTrain.isnull().sum()","ed68d76d":"dataTest.isnull().sum()","a33e10bf":"dataTrain.isnull().sum().plot.bar(rot=0);","72899b16":"dataTrain.drop(columns=['Cabin'], inplace =True)\ndataTest.drop(columns=['Cabin'], inplace =True)","961de366":"dataTrain2 = dataTrain.copy()\ndataTest2 = dataTest.copy()\ndataTrain2['Age'] = dataTrain2['Age'].fillna(dataTrain2.groupby('Pclass')['Age'].transform('mean'))\ndataTest2['Age'] = dataTest2['Age'].fillna(dataTest2.groupby('Pclass')['Age'].transform('mean'))","7feec6ba":"dataTrain2['Embarked'].fillna(np.array([dataTrain2['Embarked'].mode().values]).item(),inplace = True)\ndataTest2['Embarked'].fillna(np.array([dataTrain2['Embarked'].mode().values]).item(),inplace = True)\ndataTest2['Fare'].fillna(np.array([dataTrain2['Fare'].mode().values]).item(),inplace = True)","dffcfbbf":"dataTrain2.isnull().sum()","2cc680fb":"dataTest2.isnull().sum()","37e85c09":"sns.countplot(x=\"Survived\",data=dataTrain2);","95b2cddc":"sns.countplot(x=\"Survived\",hue=\"Sex\",data=dataTrain2);","cb098d92":"sns.countplot(x=\"Survived\",hue=\"Pclass\",data=dataTrain2);","589875a7":"sns.countplot(x=\"Survived\",hue=\"Embarked\",data=dataTrain2);","1e8fabcf":"sns.countplot(x=\"Survived\",hue=\"SibSp\",data=dataTrain2);","216b2f62":"sns.countplot(x=\"Survived\",hue=\"Parch\",data=dataTrain2);","d0b270ab":"plt.hist(dataTrain2[\"Fare\"])\nplt.show;","9d8dcfee":"plt.hist(dataTrain2[\"Age\"])\nplt.show;","db9f12ca":"# Basic statistics from the dataset\ndataTrain2.describe()","25bcefc9":"# Making copies from the datasets\ndataTrain3 = dataTrain2.copy()\ndataTest3 = dataTest2.copy()","bd191c4c":"# Converting numerical values from Age to ranges values\nbins = [0, 20, 40, 60, np.inf]\nnames = ['0-20', '20-40', '40-60','60+']\ndataTrain3['Age'] = pd.cut(dataTrain3['Age'], bins, labels=names)\ndataTest3['Age'] = pd.cut(dataTest3['Age'], bins, labels=names)","0abdf565":"# Converting numerical ranges from Fare to ranges values\nbins = [-1, 200, 400, np.inf]\nnames = ['0-200', '200-400', '400+']\ndataTrain3['Fare'] = pd.cut(dataTrain3['Fare'], bins, labels=names)\ndataTest3['Fare'] = pd.cut(dataTest3['Fare'], bins, labels=names)","fcb2948a":"dataTest3.isnull().sum()","31836ce1":"# Converting attributes to Encoded numbers\nlabel_encode = LabelEncoder()\ndataTrain3['Sex'] = label_encode.fit_transform(dataTrain3['Sex'])\ndataTrain3['Embarked'] = label_encode.fit_transform(dataTrain3['Embarked'])\ndataTrain3['Age'] = label_encode.fit_transform(dataTrain3['Age'])\ndataTrain3['Fare'] = label_encode.fit_transform(dataTrain3['Fare'])\ndataTest3['Sex'] = label_encode.fit_transform(dataTest3['Sex'])\ndataTest3['Embarked'] = label_encode.fit_transform(dataTest3['Embarked'])\ndataTest3['Age'] = label_encode.fit_transform(dataTest3['Age'])\ndataTest3['Fare'] = label_encode.fit_transform(dataTest3['Fare'])","f142419e":"#  Converting numerical to categorical values\ndataTrain3.iloc[:,:]= dataTrain3.iloc[:,:].astype(\"category\")\ndataTest3.iloc[:,:]= dataTest3.iloc[:,:].astype(\"category\")","1218f2fb":"dataTrain3.dtypes","039aaf65":"dataTest3.head()","7a2654a3":"dataTest3.isnull().sum()","1b16e720":"dataTest3.dtypes","a2cf0825":"dataTrain3.head()","f59ef5d6":"dataTest3.head()","97f8faad":"sns.countplot(x=\"Survived\",hue=\"Age\",data=dataTrain3);","d24a29ba":"sns.countplot(x=\"Survived\",hue=\"Fare\",data=dataTrain3);","60a975b8":"# Spliting train dataset in data and labels:\ndata = dataTrain3.iloc[:,1:]\nlabels = dataTrain3.iloc[:,0]","18a10e1a":"data.head()","2ac9ca88":"labels.head()","6d5cc652":"labels.shape","42d25541":"data.shape","5c15cd29":"# Instance of the classifier\nclf = RandomForestClassifier(n_estimators = 100)","3d546e5d":"# Fitting the classifier\nclf = clf.fit(data,labels)","b0d15a3b":"dataTest3.head()","072871e8":"# Predicting the values\npredictions = clf.predict(dataTest3.iloc[:])","8b07586e":"#Including the predictions to the test dataset\ndataTest3[\"Survived\"] = predictions","c13e4dde":"dataTest3.head()","58f012c7":"#Saving the file to be submitted\ndataTest3[\"Survived\"].to_csv('submission2.csv', index = True)","0f624e24":"# Instance of the second classifier\nclf2 = RandomForestClassifier(n_estimators = 100, max_depth = 3)","9d91deff":"# Fitting the second classifier\nclf2 = clf2.fit(data,labels)","f2e7e5b3":"dataTest3.head()","dac70788":"# Removing the prior predictions from the train dataset\ndataTest = dataTest3.drop(columns=[\"Survived\"])\ndataTest.head()","b1482395":"# Predicting the values\npredictions = clf2.predict(dataTest.iloc[:])","7fa4aeaf":"#Including the predictions to the test dataset\ndataTest[\"Survived\"] = predictions","ccf59cc1":"#Saving the file to be submitted\ndataTest[\"Survived\"].to_csv('submission3.csv', index = True)","74f0ba84":"#### After adding the hyperparameter max_depth = 3 the accuracy improved from 76,55% to 78,23%.","ec29163c":"# Instance of the third classifier\nclf3 = ExtraTreesClassifier(n_estimators = 1000, max_depth = 3)","e63e79d0":"# Fitting the second classifier\nclf3 = clf3.fit(data,labels)","6bc6ca3f":"dataTest3.head()","0a932642":"# Removing the prior predictions from the train dataset\ndataTest = dataTest3.drop(columns=[\"Survived\"])\ndataTest.head()","499390bc":"# Predicting the values\npredictions = clf3.predict(dataTest.iloc[:])","809c0eff":"#Including the predictions to the test dataset\ndataTest[\"Survived\"] = predictions","f864fc9f":"#Saving the file to be submitted\ndataTest[\"Survived\"].to_csv('submission4.csv', index = True)","a9ff342b":"# Instance of a basic classifier\nbasic_clf = DecisionTreeClassifier(max_depth = 3)\n# Instance of Ada Classifier\nada_clf = AdaBoostClassifier(base_estimator = basic_clf, \n                             learning_rate = 1, \n                             n_estimators = 100, \n                             algorithm='SAMME.R')","9f2da2b9":"dataTest3.head()","7da8f793":"# Removing the prior predictions from the train dataset\ndataTest = dataTest3.drop(columns=[\"Survived\"])\ndataTest.head()","003c8eff":"# Fitting the classifier\nada_clf = ada_clf.fit(data,labels)","c8e764dd":"ada_clf","9a8971b2":"# Predicting the values\npredictions = ada_clf.predict(dataTest.iloc[:])","e4de9bb5":"#Including the predictions to the test dataset\ndataTest[\"Survived\"] = predictions","1bbc8e09":"#Saving the file to be submitted\ndataTest[\"Survived\"].to_csv('submission5.csv', index = True)","a0f9f2bd":"### Analyzing the variables","e5e0f8fd":"#### The accuracy with AdaBoost Classifier was 75,598%. The score has not been improved.","94053a4b":"#### As a result of Exploratory Data Analyzis and Pre-processing, we removed some attributes, imputated values for missing data and converted numerical values to categorical values for all attributes.","b3c9e17f":"#### Comparing Survived by ranges of Age","900c608a":"#### No missing data","a55104ea":"##### Predicting the values","f2da38b3":"##### The dataTrain dataset has 10 attributes while the dataTest has 9. The difference is because the attribute \"Survived\" will be  predicted by the model as the target value. The original datasets have 11 and 10 attributes, repectively, since the passager ID was used as the index of the Pandas Dataframe and the attributes Name and Ticket were removed from the data set. These 2 attributes have no meaning for developing the predictive model.","59eab12d":"### Building the Classifier Model","1435ef6c":"## EAD - Exploratory Data Analysis and Pre-Processing","e7844991":"# <font color='blue'>Titanic Disaster<\/font>","af56b694":"#### Let\u00b4s continue finding a better accuracy: Adaboost Model","14531bf2":"#### The accuracy with Extremely Randomized Trees was 77,751%. The score has not been improved.","a5b47425":"#### Comparing Survived by Sex","f6c236ff":"#### Comparing Survived by Embarked","312cab14":"#### Distribution by Fare","faaf0f1f":"#### According to the graphics above we can see that most of the passengers hat died are men, traveled in Pclass 3, embarked at  Southampton portand and had no siblings, spouses, children and parentes aboard the Titanic. The attributes Age and Fare have many different values and will be converted to categorical values. The convertions must be done to both train and test datasets. The attributes Pclass, Sex and Survived will also be converted from numerical to categorical.","2065bebe":"#### There are only 2 missig values for the Embarked attribute. Let\u00b4s imputate the mode of this attribute to the missing values. Applying the same transformations to train and test datasets. There are missing values for the Fare attibute only to he test dataset.","44e0fc22":"#### Comparing Survived by Parch","1ac979f9":"#### Let's try a new model: Extremely Randomized Trees","029eb6f3":"#### Distribution by Age","8644d899":"#### Comparing Survived and Non-Survived","26d5b3f8":"#### Comparing Survived by ranges of Fare","467ca61a":"#### There are 177 null values for the attribute age. Let's take the mean age for each Pclass value and imputate it to the missing values. Applying the same transformations to train and test datasets.","0200b11c":"#### Analyzing the Accuracy and testing new models and hyperparameters","04973185":"### Working on missing data","a73ba3c3":"### Analyzing the shape of the modified datasets\n","de494763":"##### Most of the values of attribute Cabin are missing values, so this variable will de removed.","2a32ef38":"#### Comparing Survived by Class","66697d7e":"### Removing  attributes \"Name\" and \"Ticket\"","0d53977b":"The first submission had a accuracy received from Kaggle of 76,31%. It considered categorical values for the Age attribute in ranges of tens years (0-10, 10-20,...). After changing the ranges of 20 years (0-20,20-40,40-60, 60+) the seconding submission had an accuracy a little bit better: 76,55%. Now let's change the values for the hyperparameters and try to improve the results.","65437944":"#### Comparing Survived by SibSp"}}