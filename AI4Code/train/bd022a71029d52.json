{"cell_type":{"ebf726a1":"code","638c1638":"code","7ed8d888":"code","055d57e0":"code","4cd7380d":"code","2f5ac24b":"code","857da807":"code","63590a4d":"code","8b35b290":"code","b6f79872":"code","fdb169f0":"code","167f99c8":"code","d00c66da":"code","710df94f":"code","ad05d86c":"code","8d54c5a5":"code","ccb748e2":"code","af847eed":"code","8bd0f568":"code","13d07937":"code","2ed0c134":"code","983719ba":"code","027fa77a":"code","c0060c01":"code","2501510b":"code","787ea7b8":"code","4eb316ea":"code","e9ea0a99":"code","e7d2b51e":"code","c05698bf":"code","2357fb2a":"code","21be6822":"code","0fd6fb71":"code","f6164e2b":"code","3f7e7223":"code","878d2ef4":"code","84b9196e":"code","768aecfc":"code","c44ad17c":"code","48430f1c":"markdown","f632482b":"markdown"},"source":{"ebf726a1":"import numpy as np \nimport pandas as pd \nfrom IPython.display import display, HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport matplotlib.dates as md\nfrom multiprocessing import  Pool\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, KFold, cross_val_score, ShuffleSplit\nimport xgboost as xgb\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport math","638c1638":"#load data:\n#items_df = pd.read_csv(\"items.csv\")\n#item_categories_df = pd.read_csv(\"item_categories.csv\")\n#shops_df = pd.read_csv(\"shops.csv\")\n#train_df = pd.read_csv(\"sales_train.csv\")\n#test_df = pd.read_csv(\"test.csv\")","7ed8d888":"# loading data:\nPATH = \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/\"\nitems_df = pd.read_csv(PATH + \"items.csv\")\nitem_categories_df = pd.read_csv(PATH + \"item_categories.csv\")\nshops_df = pd.read_csv(PATH + \"shops.csv\")\ntrain_df = pd.read_csv(PATH + \"sales_train.csv\")\ntest_df = pd.read_csv(PATH + \"test.csv\")","055d57e0":"train_df","4cd7380d":"#change the date format and sort ascendingly\neda_df = train_df.copy()\neda_df[\"date\"]=  pd.to_datetime(eda_df[\"date\"], format='%d.%m.%Y')\neda_df.sort_values(by=\"date\", ascending=True, inplace=True)","2f5ac24b":"#obtain the top 5% for item_price & item_cnt_day and sales_per_item by item_price times item_cnt_day\nfor col in [\"item_price\", \"item_cnt_day\"]:\n    upper_quantile = eda_df[col].quantile(0.95)\n    eda_df[col] = np.where(eda_df[col]>upper_quantile, upper_quantile,eda_df[col])\n\neda_df[\"sales_per_item\"] = eda_df[\"item_price\"] * eda_df[\"item_cnt_day\"]","857da807":"eda_df","63590a4d":"#merge data with other dataframe (items_df, item_categories_df)\neda_df = pd.merge(eda_df, items_df, on='item_id', how='inner')\neda_df = pd.merge(eda_df, item_categories_df, on='item_category_id', how='inner')\neda_df.head()","8b35b290":"#create item_category_info_df which is item category information\nitem_category_info_df = pd.DataFrame(\n    columns=[\"name\", \"num_products\", \"first_sold\", \"last_sold\", \"min_price\", \"max_price\", \"mean_price\", \"median_price\",\n              \"mean_item_cnt_month\", \"median_item_cnt_month\", \"mean_cnt_jan\", \"mean_cnt_feb\", \"mean_cnt_mar\", \"mean_cnt_apr\",\n              \"mean_cnt_may\", \"mean_cnt_jun\", \"mean_cnt_jul\", \"mean_cnt_aug\", \"mean_cnt_sep\", \"mean_cnt_oct\", \"mean_cnt_nov\", \"mean_cnt_dez\"],\n    index = item_categories_df[\"item_category_id\"].unique())\n\nfor cid in item_category_info_df.index:\n    item_category_info_df.at[cid, \"name\"] = item_categories_df[item_categories_df[\"item_category_id\"]==cid][\"item_category_name\"].values\n    item_category_info_df.at[cid, \"num_products\"] = items_df[items_df[\"item_category_id\"]==cid][\"item_id\"].nunique()\n    cdf= eda_df[eda_df[\"item_category_id\"]==cid].copy()\n    item_category_info_df.at[cid, \"first_sold\"] = cdf[\"date\"].min()\n    item_category_info_df.at[cid, \"last_sold\"] = cdf[\"date\"].max()\n    item_category_info_df.at[cid, \"min_price\"] = cdf[\"item_price\"].min()\n    item_category_info_df.at[cid, \"max_price\"] = cdf[\"item_price\"].max()\n    item_category_info_df.at[cid, \"mean_price\"] = cdf[\"item_price\"].mean()\n    item_category_info_df.at[cid, \"median_price\"] = cdf[\"item_price\"].median()\n    \n    \n    \"\"\"\n    change the date feature to the first of each month so that each date in a month is mapped to the first day of the month.\n    Then we group by this new date. We then get a dataframe with the values sum_sales and sum_item_cnt for each month in each year.\n    As a final step, we group by month to get the mean value for each month.\n    \"\"\"\n    \n    cdf[\"month\"] = cdf[\"date\"].dt.month\n    cdf[\"year\"] = cdf[\"date\"].dt.year\n    cdf[\"date\"] = pd.to_datetime(cdf[[\"year\", \"month\"]].assign(DAY=1))\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(cdf[\"date\"].dt.month).mean().reset_index()\n    cdf.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\n   \n    item_category_info_df.at[cid, \"mean_item_cnt_month\"] = cdf[\"item_cnt_month\"].mean()\n    item_category_info_df.at[cid, \"median_item_cnt_month\"] = cdf[\"item_cnt_month\"].median()\n    \n    #average number of items sold per month\n    month_mapping = {1: \"mean_cnt_jan\", 2: \"mean_cnt_feb\", 3: \"mean_cnt_mar\", 4: \"mean_cnt_apr\", 5: \"mean_cnt_may\", 6: \"mean_cnt_jun\",\n              7: \"mean_cnt_jul\", 8: \"mean_cnt_aug\", 9: \"mean_cnt_sep\", 10: \"mean_cnt_oct\", 11: \"mean_cnt_nov\", 12: \"mean_cnt_dez\"}\n    for m in cdf[\"date\"].unique():\n        item_category_info_df.at[cid, month_mapping[m]] = cdf[cdf[\"date\"]==m][\"item_cnt_month\"].values[0]","b6f79872":"item_category_info_df","fdb169f0":"item_cat_sales_dev_df = pd.DataFrame(columns=[\"item_cat_id\",\"month\", \"mean_cnt\"])\ni = 0\nfor index, row in item_category_info_df.iterrows():\n    for m in month_mapping.keys():\n        item_cat_sales_dev_df.at[i, \"item_cat_id\"] = index\n        item_cat_sales_dev_df.at[i, \"month\"] = m\n        item_cat_sales_dev_df.at[i, \"mean_cnt\"] = row[month_mapping[m]]\n        i += 1;\nitem_cat_sales_dev_df = item_cat_sales_dev_df.astype({\"item_cat_id\": \"int8\", \"month\": \"int32\", \"mean_cnt\": \"float32\"});","167f99c8":"#cast date to datetime\nm_df = eda_df.copy()\n\nm_df['date']= pd.to_datetime(m_df['date'])\nm_df[\"month\"] = m_df[\"date\"].dt.month\nm_df[\"year\"] = m_df[\"date\"].dt.year\nm_df['date'] = pd.to_datetime(m_df[['year', 'month']].assign(DAY=28))\nm_df = m_df[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\nm_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_df.head()","d00c66da":"#add the shop_id \nm_shop_df = eda_df.copy()\n\nm_shop_df['date']= pd.to_datetime(m_shop_df['date'])\nm_shop_df[\"month\"] = m_shop_df[\"date\"].dt.month\nm_shop_df[\"year\"] = m_shop_df[\"date\"].dt.year\nm_shop_df['date'] = pd.to_datetime(m_shop_df[['year', 'month']].assign(DAY=28))\nm_shop_df = m_shop_df[[\"date\", \"item_cnt_day\", \"sales_per_item\", \"shop_id\"]].groupby([\"date\", \"shop_id\"]).sum().reset_index()\nm_shop_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_shop_df.head()","710df94f":"#create shop_info_df which is shop informationn\nshop_info_df = pd.DataFrame(columns=[\"shop_name\", \"num_products\", \"fist_business_m\", \"last_business_m\",\n                                     \"min_price\", \"max_price\", \"mean_price\", \"median_price\", \"mean_sales_pm\", \"median_sales_pm\"],\n                            index = m_shop_df[\"shop_id\"].unique())\n\nfor sid in m_shop_df[\"shop_id\"].unique():\n    shop_info_df.at[sid, \"shop_name\"] = shops_df[shops_df[\"shop_id\"]==sid][\"shop_name\"].values[0]\n    shop_info_df.at[sid, \"num_products\"] = train_df[train_df[\"shop_id\"]==sid][\"item_id\"].nunique()\n    sdf= m_shop_df[m_shop_df[\"shop_id\"]==sid]\n    shop_info_df.at[sid, \"fist_business_m\"] = sdf[\"date\"].min()\n    shop_info_df.at[sid, \"last_business_m\"] = sdf[\"date\"].max()\n    shop_info_df.at[sid, \"min_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].min()\n    shop_info_df.at[sid, \"max_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].max()\n    shop_info_df.at[sid, \"mean_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].mean()\n    shop_info_df.at[sid, \"median_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].median()\n    shop_info_df.at[sid, \"mean_sales_pm\"] = sdf[\"sales_per_month\"].mean()\n    shop_info_df.at[sid, \"median_sales_pm\"] = sdf[\"sales_per_month\"].median()\n    \"\"\"\n    We will do it a little bit different for this shop dataframe. Here, the average turnover per month would\n    not be so interesting.Instead, we can use our m_shop_df for the sales figures per month.\n    \"\"\"\n# change datatypes (datetime float and int)\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df = shop_info_df.astype({'num_products': 'int32',\n                                    \"min_price\": 'float32',\n                                    \"max_price\": 'float32',\n                                    \"mean_price\": 'float32',\n                                    \"median_price\": 'float32',\n                                    \"mean_sales_pm\": 'float32',\n                                    \"median_sales_pm\": 'float32'})","ad05d86c":"def preprocessing(data, item_data=items_df, shop_data=shops_df, category_data=item_categories_df):\n    \"\"\"\n    Some basic stuff.\n    \"\"\" \n    print(50*'-')\n    print(\"preprocessing...\")\n    # 1). Create a copy of the Dataframe.\n    df = data.copy()\n    # 2). Remove all values with item_cnt_day < 1.\n    df = df[df[\"item_cnt_day\"]>0]\n    #3). Add Month feature\n    df[\"date\"]= pd.to_datetime(df[\"date\"], format='%d.%m.%Y')\n    df[\"month\"] = df[\"date\"].dt.month\n    #4). Group by date_block_nu$m.\n    df = df[[\"month\", \"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\"]].groupby(\n        [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        {\"item_price\": \"mean\",\"item_cnt_day\": \"sum\", \"month\": \"min\"}).reset_index()\n    df.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\n    #5). Add category_id and item_name.\n    df = pd.merge(df, item_data, on=\"item_id\", how=\"inner\")\n    #6). Add shop_name \n    df = pd.merge(df, shop_data, on=\"shop_id\", how=\"inner\")\n    #7). Add category_name\n    df = pd.merge(df, category_data, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(50*'-')\n    return df","8d54c5a5":"piped_df = preprocessing(data=train_df)\npiped_df.head()","ccb748e2":"def add_city_feature(data):\n    data.loc[data[\"shop_name\"] == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', \"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n    data[\"city\"] = data[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    data.loc[data[\"city\"] == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n    data[\"city_code\"] = data[\"city\"].factorize()[0]\n    return data","af847eed":"piped_df=add_city_feature(piped_df)\npiped_df.head()","8bd0f568":"shop_info_df.head()","13d07937":"# add city\nshop_info_df=add_city_feature(shop_info_df)","2ed0c134":"shop_info_df.head()","983719ba":"#Convert data to int\nfist_date = datetime.strptime(\"2013-01-01\", \"%Y-%m-%d\").date()\n\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\n\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df.head()","027fa77a":"# do pca and clustering\ndef get_cluster_feature(data, columns, index_name, cluster_name, num_cluster=5, linkage=\"average\", n_pca_components=3, svd_solver=\"auto\", visualize=True):\n    df = data[columns].copy()\n    \n    # PCA\n    pca = PCA(n_components=n_pca_components)\n    components = pca.fit_transform(df)\n    components = pd.DataFrame(components)\n    \n    # Clustering\n    clusterer = AgglomerativeClustering(n_clusters=num_cluster, linkage=linkage)\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    \n    # Evaluation:\n    scorelist = []\n    nrange = range(2, 6)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        l = clusterer.fit_predict(components)\n        silscore = silhouette_score(df, l)\n        scorelist.append(silscore)\n        \n    for i in df.index:\n        df.at[i, cluster_name] = labels[i]\n        df = df[cluster_name].reset_index()\n        df.rename(columns={\"index\": index_name}, inplace = True)\n    \n    # plotting:    \n    if visualize:\n        fig = plt.figure(figsize=(25,15))\n        gs = fig.add_gridspec(2, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax02 = fig.add_subplot(gs[1,:])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax02.tick_params(axis='both', labelsize=15)\n        ax00.set_title('PCA Components', fontsize=20)\n        ax01.set_title('Cluster Score by Number of Clusters', fontsize=20)\n        ax02.set_title('Clustering', fontsize=20)\n\n        ax00.set(xlabel='component number', ylabel='covered variance')\n        ax01.set(xlabel='number of clusters', ylabel='silhouette score')\n        ax02.set(xlabel='component 1 score', ylabel='component 2 score')\n\n        sns.barplot(x=list(range(pca.n_components_)), y=pca.explained_variance_ratio_, ax=ax00, palette=\"Set2\")\n        sns.lineplot(x=nrange, y=scorelist, ax=ax01, color=\"darkblue\")\n        sns.scatterplot(x=x, y=y, hue=labels, ax=ax02, palette=\"dark\")\n\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(f\"PCA and Clustering Output (num_cluster = {num_cluster})\", fontsize=\"28\")\n    \n    return df","c0060c01":"shop_info_df.info()","2501510b":"#find numeric columns\nnumeric_shop_columns = shop_info_df.select_dtypes(include=[\"int32\", \"int64\", \"float32\"]).columns\nshop_cluster_df = get_cluster_feature(data=shop_info_df, columns=numeric_shop_columns, n_pca_components=2, index_name=\"shop_id\", cluster_name=\"shop_cluster\");","787ea7b8":"#add shop_cluster_df\npiped_df = pd.merge(piped_df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\npiped_df.head()","4eb316ea":"item_category_info_df.head()","e9ea0a99":"#convert data columns to int\nitem_category_info_df['first_sold']= pd.to_datetime(item_category_info_df['first_sold'])\nitem_category_info_df['last_sold']= pd.to_datetime(item_category_info_df['last_sold'])\n\nitem_category_info_df[\"first_sold\"] = item_category_info_df[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df[\"last_sold\"] = item_category_info_df[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df.head()","e7d2b51e":"#obtain numeric columns\n#fill na\n\nitem_category_info_df = item_category_info_df.astype({\"name\": \"string\"})\nnumeric_item_columns = item_category_info_df.select_dtypes(include=[\"float64\", \"int64\", \"object\"]).columns\nitem_category_info_df.fillna(0, inplace=True);","c05698bf":"item_cat_claster = get_cluster_feature(data=item_category_info_df, columns=numeric_item_columns, index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\")","2357fb2a":"#add item categorie cluster\npiped_df = pd.merge(piped_df, item_cat_claster, on=\"item_category_id\", how=\"inner\")","21be6822":"piped_df.head()","0fd6fb71":"#adding lag feature and label\ndef add_lag_feature_and_label(args):\n    \n    def add_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_value = item_df[item_df[\"date_block_num\"]==date_block-lag-1][\"item_cnt_month\"].values[0]\n                df.at[index, f\"lag_{lag+1}\"] = lag_value\n        return df \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"lag_{lag+1}\"] = 0\n    for shop in df[\"shop_id\"].unique():\n        shop_df = df[df[\"shop_id\"]==shop].copy()\n        for item in shop_df[\"item_id\"].unique():\n            item_df = shop_df[shop_df[\"item_id\"]==item].copy()\n            last_index = 0\n            for index, row in item_df.iterrows():\n                date_block = row[\"date_block_num\"]\n                if target_date_block and date_block == target_date_block:\n                    df = add_lags(df, date_block)\n                if target_date_block is None:\n                    df = add_lags(df, date_block)\n    if target_date_block:\n        df = df[df[\"date_block_num\"]==target_date_block].copy()\n    df.rename(columns={\"item_cnt_month\":\"label\"}, inplace=True)\n    return df\n\ndef parallelize_lag_and_target_processing(df, num_lags, target_date_block_num=None, func=add_lag_feature_and_label, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"shop_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","f6164e2b":"\n#evaluate model\ndef evaluate_xgboost(model):\n        results = model.evals_result()\n        fig = plt.figure(figsize=(25,10))\n        gs = fig.add_gridspec(1, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax00.set_title('Feature Importance', fontsize=20)\n        ax01.set_title('loss vs validation loss', fontsize=20)\n        ax00.set(xlabel='Importance', ylabel='Feature')\n        ax01.set(xlabel='n_estimators', ylabel='rmse')\n        sns.barplot(y=model.get_booster().feature_names, x=model.feature_importances_, ax=ax00, palette=\"Set2\", orient=\"h\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_0\"][\"rmse\"], ax=ax01, color=\"darkblue\", label=\"loss\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_1\"][\"rmse\"], ax=ax01, color=\"orange\", label=\"validation loss\")\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(\"Model Evaluation\", fontsize=\"28\")","3f7e7223":"#adding lag feature and label\n#avg_lags per item feature\ndef add_avg_lag_feature(args):\n    \n    def add_avg_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_df = item_df[item_df[\"date_block_num\"]==date_block-lag-1]\n                lag_value = lag_df[\"item_cnt_month\"].mean()\n                df.at[index, f\"avg_lag_{lag+1}\"] = lag_value\n        return df \n    \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"avg_lag_{lag+1}\"] = 0\n\n    for item in df[\"item_id\"].unique():\n        item_df = df[df[\"item_id\"]==item].copy()\n        last_index = 0\n        for index, row in item_df.iterrows():\n            date_block = row[\"date_block_num\"]\n            if target_date_block and date_block == target_date_block:\n                df = add_avg_lags(df, date_block)\n            if target_date_block is None:\n                df = add_avg_lags(df, date_block)\n    return df\n\ndef parallelize_avg_lag_processing(df, num_lags, target_date_block_num=None, func=add_avg_lag_feature, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"item_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","878d2ef4":"#combines previous steps without the preprocessing function\ndef pipeline(data, num_lags, num_avg_lags, target_date_block_num=None, first_data_str=\"2013-01-01\",\n             shop_info_data=shop_info_df, categorie_info_data=item_category_info_df,\n             shops=None, items=None):\n    \n    print(100*\"#\")\n    print(f\"running pipeline for target_date_block_num {target_date_block_num}  with {num_lags} lags...\")\n    fist_date = datetime.strptime(first_data_str, \"%Y-%m-%d\").date()\n    print(\"adding city feature...\")\n    df = add_city_feature(data)\n    print(\"done.\")\n    print(\"adding shop_cluster feature...\")\n    # preprocessing shop_info_df\n    shop_info_data['fist_business_m']= pd.to_datetime(shop_info_data['fist_business_m'])\n    shop_info_data['last_business_m']= pd.to_datetime(shop_info_data['last_business_m'])\n    shop_info_data[\"fist_business_m\"] = shop_info_data[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    shop_info_data[\"last_business_m\"] = shop_info_data[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    numeric_shop_columns = shop_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns\n    shop_cluster_df = get_cluster_feature(data=shop_info_data, columns=numeric_shop_columns, n_pca_components=2,\n                                          index_name=\"shop_id\", cluster_name=\"shop_cluster\", visualize=False)\n    # add shop_cluster\n    df = pd.merge(df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding item_cat_cluster feature...\")\n    # preprocessing categorie_info_data\n    categorie_info_data['first_sold']= pd.to_datetime(categorie_info_data['first_sold'])\n    categorie_info_data['last_sold']= pd.to_datetime(categorie_info_data['last_sold'])\n    categorie_info_data[\"first_sold\"] = categorie_info_data[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data[\"last_sold\"] = categorie_info_data[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data = categorie_info_data.astype({\"name\": \"string\"})\n    numeric_item_columns = categorie_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\", \"object\"]).columns\n    categorie_info_data.fillna(0, inplace=True)\n    item_cluster_df = get_cluster_feature(data=categorie_info_data, columns=numeric_item_columns, n_pca_components=3,\n                                          index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\", visualize=False)\n    # add item_cat_cluster\n    df = pd.merge(df, item_cluster_df, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding avg lag features...\")\n    df = parallelize_avg_lag_processing(df=df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    #df = add_avg_lag_feature(df, num_avg_lags, target_date_block_num)\n    print(\"done.\")\n    print(\"adding lag features...\")\n    df = parallelize_lag_and_target_processing(df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    print(\"done.\")\n    \n    print(\"dropping item_name, shop_name, item_category_name and city...\")\n    df.drop([\"item_name\", \"shop_name\", \"item_category_name\", \"city\"], axis=1, inplace=True)\n    print(\"done.\")\n    print(100*\"#\")\n    return df\n\ndef get_training_data(num_lags, num_avg_lags,  target_data_block_number=None, train_data=train_df):\n    df = pipeline(data=preprocessing(data=train_data), num_lags=num_lags, num_avg_lags=num_avg_lags,\n                  target_date_block_num=target_data_block_number)\n    #clap the label and all lag features:\n    columns = [col for col in df.columns if col[:3]==\"lag\"]\n    columns = [\"label\"] + columns\n    for col in columns:\n        df[col] = np.where(df[col]>20, 20,df[col])\n    #delete the the f\u00fcrst \"num_lags\" date_block_nums\n    valid_data_block_num = range(num_lags,34)\n    df = df[df[\"date_block_num\"].isin(valid_data_block_num)]\n\n    y = df[\"label\"]\n    X = df.copy()\n    X.drop([\"label\"], axis=1, inplace=True)\n    return df","84b9196e":"#obtain training data\n#split into test set which is the final month and training set which is first 33 months\nDF = get_training_data(num_lags=3, num_avg_lags=3)\n\ntrain = DF[DF.date_block_num != 33]\ntest = DF[DF.date_block_num == 33]\nX_train = train.copy()\nX_train.drop([\"label\"], axis=1, inplace=True)\nX_test = test.copy()\nX_test.drop([\"label\"], axis=1, inplace=True)\n\ny_train = train[\"label\"]\ny_test = test[\"label\"]","768aecfc":"#train model\nevalset = [(X_train, y_train), (X_test,y_test)]\nxgb_model2 = xgb.XGBRegressor(max_depth=5, n_estimators=100, subsample=0.6, eval_metric='rmse', learning_rate=0.1)\nxgb_model2.fit(X_train, y_train, eval_set=evalset)","c44ad17c":"# predict and calculat the rmse for test & train data\ntest_prediction = xgb_model2.predict(X_test)\ntrain_prediction = xgb_model2.predict(X_train)\nrmse_test = mean_squared_error(y_true = y_test, y_pred = test_prediction)**(0.5)\nrmse_train = mean_squared_error(y_true = y_train, y_pred = train_prediction)**(0.5)\nprint(50*\"*\")\nprint(f\"RMSE test: {rmse_test}\")\nprint(f\"RMSE train: {rmse_train}\")\nprint(50*\"*\")\n#evaluate model\nevaluate_xgboost(xgb_model2)","48430f1c":"**Model**","f632482b":"**Data cleaning & Grouping by month**"}}