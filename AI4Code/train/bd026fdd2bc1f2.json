{"cell_type":{"0ee9465d":"code","60800595":"code","1273a3b3":"code","2af80ac4":"code","75928628":"code","b107f626":"code","03e57661":"code","0be9b68e":"code","d5beaadd":"code","31ae07fc":"code","75356949":"code","4f2f89ce":"code","72d0ddc7":"code","3951f400":"code","1f497c1e":"code","36e3dbf2":"code","2c5c1112":"code","3ebd3b9f":"code","83593f8b":"code","51322716":"markdown","f68de05d":"markdown","495bc914":"markdown","5bed1854":"markdown","7e71933d":"markdown","709c4992":"markdown","69452f81":"markdown","b180be38":"markdown","f1ec9d4e":"markdown","f84440ee":"markdown","4bb1a7aa":"markdown","adc2d00e":"markdown","0fd4358e":"markdown"},"source":{"0ee9465d":"#Importing all required libraries\nimport pandas as pd\nimport numpy as np\nnp.random.seed(42)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import colors\nfrom matplotlib.colors import ListedColormap\ncmap = colors.ListedColormap([\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"])\nimport seaborn as sns\nplt.style.use('seaborn')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\n\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Model","60800595":"#Reading the dataset\ndata = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep='\\t',parse_dates=['Dt_Customer'])\ndata.head()","1273a3b3":"#Information on features\ndata.info()","2af80ac4":"print(data.Education.value_counts())\n#Segmenting Education levels in three groups\ndata.Education = data.Education.replace({'PhD':'PostGraduation','Master':'PostGraduation','2n Cycle':'UnderGraduation','Basic':'UnderGraduation'})\nprint('-'*30)\nprint(data.Education.value_counts())","75928628":"print(data.Marital_Status.value_counts())\n#Segmenting Marital Status in two groups\ndata.Marital_Status = data.Marital_Status.replace({'Married':'Together','Single':'Alone','Divorced':'Alone','Widow':'Alone','Absurd':'Alone','YOLO':'Alone'})\nprint('-'*40)\nprint(data.Marital_Status.value_counts())","b107f626":"#Drop NA values\ndata.dropna(inplace=True)\n#Dropping the outlier by setting a cap on income\ndata = data[data.Income<600000]\n#Dropping some of the features\ndata.drop(['ID','Year_Birth','Dt_Customer','Z_CostContact','Z_Revenue'],axis=1,inplace=True)\n#New feature for total spendings on various items\ndata[\"Spent\"] = data[\"MntWines\"] + data[\"MntFruits\"] + data[\"MntMeatProducts\"] + data[\"MntFishProducts\"] + data[\"MntSweetProducts\"] + data[\"MntGoldProds\"]\n#New feature for total members in the householde\ndata['Family_Size'] = data.Kidhome + data.Teenhome + data.Marital_Status.replace({'Alone':1,'Together':2})\n#Feature pertaining parenthood\ndata['Is_Parent'] = np.where((data.Kidhome + data.Teenhome)>0,1,0)\ndata.head()","03e57661":"#Converting the labels into a numeric form so as to convert them into the machine-readable form\nlabel_encoder = LabelEncoder()\nfor item in ['Education','Marital_Status']:\n    data[item] = data[[item]].apply(label_encoder.fit_transform)\ndata.head()","0be9b68e":"#Standardizes a feature by subtracting the mean and then scaling to unit variance\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\nscaled_data","d5beaadd":"#Transformation of data\npca = PCA(n_components=3)\npca_data = pd.DataFrame(pca.fit_transform(scaled_data),columns=['pca1', 'pca2', 'pca3'])\npca_data.head()","31ae07fc":"#Plotting the reduced data\nax = plt.axes(projection='3d')\nfor color in ['black', 'blue', '#B9C0C9']:\n    n = 2340\n    x, y = np.random.rand(2, n)\n    scale = 200.0 * np.random.rand(n)\n    ax.scatter3D(pca_data['pca1'],pca_data['pca2'],pca_data['pca3'],c=color,marker='o');","75356949":"#Input Lyer\ninput_df = Input(shape =(data.shape[1],))\n#Encoding\nx = Dense(500, activation = 'relu')(input_df)\nx = Dense(500, activation = 'relu')(x)\nx = Dense(2000, activation = 'relu')(x)\n#Decoding\nencoded = Dense(2000, activation = 'relu')(x)\nx = Dense(500, activation = 'relu')(encoded)\nx = Dense(500, activation = 'relu')(x)\n#Output Layer\ndecoded = Dense(data.shape[1])(x)\n\n#autoencoder\nautoencoder = Model(input_df, decoded)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')","4f2f89ce":"autoencoder.fit(data,data,batch_size=128,epochs=100,verbose=3)","72d0ddc7":"encoded_data = autoencoder.predict(scaled_data)","3951f400":"#Elbow method to find numbers of clusters to make\nelbow = KElbowVisualizer(KMeans(), k=10)\nelbow.fit(pca_data)\nelbow.show();","1f497c1e":"kmeans = KMeans(4)\nkmeans.fit(encoded_data)\nlabels = kmeans.labels_\ny = kmeans.fit_predict(scaled_data)\npca_data['Cluster'] = y\n#Adding the cluster to original dataset\ndata['Cluster'] = y","36e3dbf2":"#3-D distribution of the clusters\nplt.figure(figsize=(10,8))\nax = plt.axes(projection='3d')\nax.scatter3D(pca_data['pca1'],pca_data['pca2'],pca_data['pca3'],c=pca_data['Cluster'],cmap=cmap,marker='o',s=40);","2c5c1112":"f,ax=plt.subplots(2,2,figsize=(15,8))\nsns.countplot('Cluster',data=data,ax=ax[0,0],palette=[\"#682F2F\",\"#B9C0C9\", \"blue\",\"#F3AB60\"]);\nsns.countplot('Cluster',data=data,ax=ax[0,1], hue='Education');\nsns.countplot('Cluster',data=data,ax=ax[1,0], hue='Marital_Status');\nsns.countplot('Cluster',data=data,ax=ax[1,1], hue='Is_Parent');","3ebd3b9f":"sns.scatterplot(data=data,x='Spent',y='Income',hue='Cluster',palette=[\"#682F2F\",\"#B9C0C9\", \"blue\",\"#F3AB60\"]);","83593f8b":"features = [\"Kidhome\",\"Teenhome\", \"Family_Size\", \"Is_Parent\", \"Income\"]\n\nfor item in features:\n    sns.jointplot(x=data[item], y=data[\"Spent\"], hue =data[\"Cluster\"], kind=\"kde\", palette=[\"#682F2F\",\"#B9C0C9\", \"blue\",\"#F3AB60\"]);","51322716":"**Autoencoder Model and Clustering Technique for Customer Segmentation**\n\nCustomer segmentation is the problem of uncovering information about a firm's customer base, based on their interactions with the business. In most cases this interaction is in terms of their purchase behavior and patterns. Customer segmentation is the practice of separating customers into groups that reflect similarities among customers in each cluster. We explore some of the ways in which this can be used.","f68de05d":"# Autoencoder","495bc914":"* **Cluster 0** \n1. Low Spent and Low Income \n1. Majority of these are parent \n1. At the max 5 memebrs in the family\n\n* **Cluster 1** \n1. Average Spent & Average Income \n1. Definitely a parent \n1. At the max 4 memebrs in the family\n\n* **Cluster 2**\n1. High Spent and High Income \n1. Definitely not a parent \n1. At the max 2 memebrs in the family\n\n* **Cluster 3** \n1. High Spent and Average Income \n1. Definitely not a parent \n1. At the max 3 memebrs in the family","5bed1854":"# Data Preprocessing\nData preprocessing is a data mining technique which is used to transform the raw data in a useful and efficient format.\n\n**Steps Involved in Data Preprocessing:**\n1. Data Cleaning\n1. Data Transformation\n1. Data Reduction","7e71933d":"An autoencoder is a regression task where the network is asked to predict its input. These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input.","709c4992":"In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.\n\nThe K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.","69452f81":"# Libraries & Data Loading","b180be38":"# PCA\n\nThe higher the number of features, the harder it is to work with it. Many of these features are correlated, and hence redundant. This is why will be performing dimensionality reduction on the selected features before putting them through a classifier.\n\nDimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables.","f1ec9d4e":"We do not have a tagged feature to evaluate or score our model. We will be having a look at the data in light of clusters via exploratory data analysis and drawing conclusions.","f84440ee":"# Profiling","4bb1a7aa":"* There are missing values in income\n* There are some categorical features in our dataset","adc2d00e":"# Clustering\n\nSteps involved in the Clustering\n1. Elbow Method to determine the number of clusters to be formed\n1. Clustering\n1. Examining the clusters formed via scatter plot","0fd4358e":"We have formed the clusters and looked at their purchasing habits. Let's see who all are there in these clusters. For that, we will be profiling the clusters formed and come to a conclusion about who is our star customer and who needs more attention from the store."}}