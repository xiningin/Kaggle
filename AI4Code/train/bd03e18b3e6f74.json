{"cell_type":{"73fb8202":"code","2820afe9":"code","aab5d5b0":"code","8c82a0db":"code","f3cff19e":"code","fd667817":"code","4a0b7de8":"code","895d0691":"code","d951e4a2":"code","1f4e7f31":"code","b4c7ee52":"code","63712421":"code","21f2956e":"code","48662a71":"code","1ef90b2a":"code","b1dc5645":"code","d9888867":"code","a3f1a192":"code","72337f90":"code","0c7dfe3b":"code","172641a9":"markdown","a8ddc4cf":"markdown","d514a559":"markdown","7e9cf548":"markdown","0f9d090f":"markdown","070b57bc":"markdown","01642b59":"markdown","a53e99c2":"markdown","657fdc0e":"markdown","a4864b37":"markdown","ad812569":"markdown","4b98c24a":"markdown"},"source":{"73fb8202":"#importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2820afe9":"df= pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")   #importing dataset and making dataframe \ndf.head()                                        #showing top 5 data entry ","aab5d5b0":"df.describe() #describes are data ","8c82a0db":"df.info() #gives information about the columns","f3cff19e":"df.shape #tells us about no. of rows and column [rows , columns]","fd667817":"df.drop(\"Id\",axis=1,inplace=True)    #droping  id becuase it is no use to us , Inplace = True means changes will take effect in original dataframe\ndf.head()","4a0b7de8":"print(df[\"Species\"].value_counts())\nsns.countplot(df[\"Species\"])","895d0691":"plt.figure(figsize=(8,4)) \nsns.heatmap(df.corr(),annot=True,fmt=\".0%\") #draws  heatmap with input as the correlation matrix calculted by(df.corr())\nplt.show()\n","d951e4a2":"# We'll use seaborn's FacetGrid to color the scatterplot by species\nsns.FacetGrid(df, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()","1f4e7f31":"#let Create a pair plot of some columns \nsns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns ","b4c7ee52":"# We can quickly make a boxplot with Pandas on each feature split out by species\ndf.boxplot(by=\"Species\", figsize=(15,15))","63712421":"# importing alll the necessary packages to Logistic Regression \nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn import metrics #for checking the model accuracy","21f2956e":"X=df.iloc[:,0:4]\nY=df[\"Species\"]\nX.head()","48662a71":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(\"Train Shape\",X_train.shape)\nprint(\"Test Shape\",X_test.shape)","1ef90b2a":"log = LogisticRegression()\nlog.fit(X_train,Y_train)\nprediction=log.predict(X_test)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,Y_test))","b1dc5645":"from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn.svm import SVC   #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm","d9888867":"tree=DecisionTreeClassifier()\ntree.fit(X_train,Y_train)\nprediction=tree.predict(X_test)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,Y_test))","a3f1a192":"knn=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nknn.fit(X_train,Y_train)\nprediction=knn.predict(X_test)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,Y_test))","72337f90":"svc=SVC()\nsvc.fit(X_train,Y_train) \nprediction=svc.predict(X_test)\nprint('The accuracy of the SVC is',metrics.accuracy_score(prediction,Y_test))","0c7dfe3b":"from sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=0)\nforest.fit(X_train,Y_train)\nprint('The accuracy of the SVC is',metrics.accuracy_score(prediction,Y_test))","172641a9":"### Decison Tree","a8ddc4cf":"# **Bonus Models**","d514a559":"# That is it guys!!!!! \n### I will Update it and more content to it\n\n#### Hope this will help you **Upvote it if you Like it**\n#### Follow me for more.","7e9cf548":"### K Nearest Neighbor Classification ","0f9d090f":"# Data Visualization","070b57bc":"### Steps To Be followed When Applying an Algorithm\n* Split the dataset into training and testing dataset. The testing dataset is generally smaller than training one as it will help in training the model   better.\n* Select any algorithm based on the problem (classification or regression) whatever you feel may be good.\n* Then pass the training dataset to the algorithm to train it. We use the .fit() method\n* Then pass the testing data to the trained algorithm to predict the outcome. We use the .predict() method.\n* We then check the accuracy by passing the predicted outcome and the actual output to the model.","01642b59":"### Support Vector Machine","a53e99c2":"#### Observation--->\n\nThe Sepal Width and Length are not correlated The Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n\nThen we will use 1 Petal Feature and 1 Sepal Feature to check the accuracy of the algorithm as we are using only 2 features that are not correlated. Thus we can have a variance in the dataset which may help in better accuracy. We will check it later.****","657fdc0e":"# About\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies","a4864b37":"### Random forest Classfier","ad812569":"### Splitting The Data into Training And Testing Dataset","4b98c24a":"## Logistic Regression"}}