{"cell_type":{"7d3a8b74":"code","3db1c5ab":"code","5be33620":"code","c003f1bf":"code","3341223c":"code","93b03855":"code","16f5814f":"code","4097d912":"code","a935ac47":"code","47de7ccf":"code","e1132251":"code","df04f1f2":"code","a6e2d99d":"code","1011a0bd":"code","eb5cd33b":"code","da7c94e5":"code","f7f812fc":"code","aca68017":"code","52989770":"code","3fd6d511":"code","9c5dd385":"code","c52f7fa7":"code","33f7e72c":"code","348a8833":"code","6d5d7824":"code","91de0787":"code","e976f34b":"code","dc029b14":"code","93ac346d":"code","c0c36c35":"code","31f97413":"code","3f0acbcd":"code","0980f20d":"code","72fd90f9":"code","fdedba9f":"code","28bfef0d":"code","592f6691":"code","bc6b9d5e":"code","0a9e7f3f":"code","7c1c7985":"code","5b169050":"code","b4f2d86c":"code","c8e93eef":"code","6a4fae7d":"code","254e88de":"code","42c81d74":"code","630dfc23":"code","e6183d72":"code","42cdcc12":"code","22e1f16e":"code","34ca7f82":"code","13e4ab8d":"code","0d5ba02b":"code","31613945":"code","c0b12cda":"code","a5658929":"code","335e6f4f":"code","dde2a63b":"code","13da5ba1":"code","79d523fb":"code","417fce7b":"code","caeddb29":"code","92d4bf2c":"code","4d4cf499":"code","6360b10c":"code","edf90d27":"code","dee4f791":"code","3e490107":"code","a3dd0611":"code","78b0b9df":"code","baa51b7b":"code","8041f331":"code","1996620d":"code","df3c0d52":"code","3f5c2ffd":"code","eff3ca90":"code","587eac45":"code","7addf397":"code","55c840ac":"code","8e331aed":"code","3617ef1b":"code","9f403a0e":"code","d076d906":"code","e89403f6":"code","04e06a55":"code","35300136":"code","92211832":"code","a308b26c":"markdown","1d530f85":"markdown","ff125f4c":"markdown","eb0cae92":"markdown","ced15591":"markdown","d668db9b":"markdown","89cd9420":"markdown","4a933707":"markdown","9f3e663b":"markdown","a4ccf78c":"markdown","3d7d0a01":"markdown","2dacfdc6":"markdown","a8dbd7f8":"markdown","85eb11bd":"markdown","924b7005":"markdown","18c66091":"markdown","45f9d104":"markdown","38e30a7f":"markdown","a18c8d19":"markdown","f6c857e3":"markdown"},"source":{"7d3a8b74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3db1c5ab":"from itertools import combinations\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import shapiro, t\nimport os\n\nfrom statsmodels.stats.proportion import proportion_confint\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\n\n%matplotlib inline\nplt.style.use(\"seaborn-darkgrid\")","5be33620":"os.chdir(\"\/kaggle\/input\")","c003f1bf":"df_placement = pd.read_csv(\"factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")","3341223c":"df_placement.head()","93b03855":"df_placement.info()","16f5814f":"df_1 = df_placement.drop(columns = [\"salary\", 'hsc_s', 'degree_t'])\ndf_1['gender'].replace(['M', 'F'], [0, 1], inplace = True)\ndf_1['status'].replace(['Not Placed', 'Placed'], [0, 1], inplace = True)\ndf_1['ssc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_1['hsc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_1['workex'].replace(['No', 'Yes'], [0, 1], inplace = True)\ndf_1['specialisation'].replace(['Mkt&Fin', 'Mkt&HR'], [0, 1], inplace = True)\ndf_1 = df_1.join([pd.get_dummies(df_placement.hsc_s, prefix = df_placement.hsc_s.name),\n                  pd.get_dummies(df_placement.degree_t, prefix = df_placement.degree_t.name)\n                 ])\n#a loop with if condition can also convert to one hot vectors\n#can be packed into preprocessing data function","4097d912":"df_1.head()","a935ac47":"df_1.info()","47de7ccf":"plt.figure(figsize = (15,15))\nsns.heatmap(df_1.corr(), annot = True)","e1132251":"sns.pairplot(df_1)","df04f1f2":"X = df_1.drop(columns = [\"status\"])\nY = df_1['status']","a6e2d99d":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)","1011a0bd":"data = {k : [] for k in range(1, 11)}\n#One could choose to express in the form of 4 lists or 4 dictinaries\n\nfor i in range(1, 11):\n    knncls = KNeighborsClassifier(n_neighbors = i)\n    knncls.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, knncls.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, knncls.predict(X_test)))","eb5cd33b":"plt.figure(figsize = (15,15))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\nplt.legend()\n\nplt.show()","da7c94e5":"max([i[-1] for i in data.values()])","f7f812fc":"list(data.values())","aca68017":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train, Y_train)\n\ncm = pd.DataFrame(confusion_matrix(Y_test, knn.predict(X_test)).T, index=['No', 'Yes'], columns=['No', 'Yes'])\ncm.index.name = 'Predicted'\ncm.columns.name = 'True'\ncm","52989770":"correct = confusion_matrix(Y_test, knn.predict(X_test))[0][0] + confusion_matrix(Y_test, knn.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","3fd6d511":"lr = LogisticRegression(max_iter = 200, n_jobs = -1)","9c5dd385":"lr.fit(X_train, Y_train)","c52f7fa7":"train_accuracy = accuracy_score(Y_train, lr.predict(X_train))\ntest_accuracy = accuracy_score(Y_test, lr.predict(X_test))","33f7e72c":"print(\n     \"train_accuracy: \", train_accuracy,\n     \"\\ntest_accuracy: \", test_accuracy,)","348a8833":"correct = confusion_matrix(Y_test, lr.predict(X_test))[0][0] + confusion_matrix(Y_test, lr.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","6d5d7824":"data = {k : [] for k in np.geomspace(1e-3, 1e2, 10)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n\nfor i in data.keys():\n    ridge = RidgeClassifier(alpha = i, normalize = True, random_state = 1)\n    ridge.fit(X_train, Y_train)\n    \n    data[i].append(accuracy_score(Y_train, ridge.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, ridge.predict(X_test)))","91de0787":"plt.figure(figsize = (10,10))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\n\nplt.show()","e976f34b":"max([i[-1] for i in data.values()])","dc029b14":"[i[-1] for i in data.values()]","93ac346d":"test_alpha = list(data.keys())[4]","c0c36c35":"ridge = RidgeClassifier(alpha = test_alpha, normalize = True)\nridge.fit(X_train, Y_train)","31f97413":"print(classification_report(Y_test, ridge.predict(X_test)))","3f0acbcd":"cm = pd.DataFrame(confusion_matrix(Y_test, ridge.predict(X_test)).T, index=['No', 'Yes'], columns=['No', 'Yes'])\ncm.index.name = 'Predicted'\ncm.columns.name = 'True'\ncm","0980f20d":"mean_squared_error(Y_test, ridge.predict(X_test))","72fd90f9":"correct = confusion_matrix(Y_test, ridge.predict(X_test))[0][0] + confusion_matrix(Y_test, ridge.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","fdedba9f":"train_ac, test_ac = [], []\n\nfor i in range(1,1001):\n    X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X, Y)\n    \n    ridge_ = RidgeClassifier(alpha = test_alpha, normalize = True)\n    ridge_.fit(X_train_, Y_train_)\n    \n    train_ac.append(accuracy_score(Y_train_, ridge_.predict(X_train_)))\n    test_ac.append(accuracy_score(Y_test_, ridge_.predict(X_test_)))\n    \nplt.figure(figsize = (10,6))\n#plt.plot([i for i in range(1,1001)], train_ac, marker='o', label=\"Train_accuracy\",\n#        drawstyle=\"steps-post\")\nplt.plot([i for i in range(1,1001)], test_ac, marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.legend()\n\n\nplt.show()","28bfef0d":"sns.distplot(test_ac, bins= 20)","592f6691":"shapiro(test_ac)","bc6b9d5e":"#default tree\ntree = DecisionTreeClassifier(random_state = 0)","0a9e7f3f":"tree.fit(X_train, Y_train)","7c1c7985":"print(classification_report(Y_test, tree.predict(X_test)))\nmean_squared_error(Y_test, tree.predict(X_test)), accuracy_score(Y_test, tree.predict(X_test))","5b169050":"fig, ax = plt.subplots(figsize=(100, 100))\nfeatures = X_train.columns\n\nplot_tree(tree, filled = True, ax = ax, feature_names = features, proportion = True, rounded = True);","b4f2d86c":"Importance = pd.DataFrame({'Importance':tree.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='b', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","c8e93eef":"path = tree.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","6a4fae7d":"fig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","254e88de":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, Y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","42c81d74":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n#we remove last pruned tree with only one terminal node\n\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize = (10,10))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","630dfc23":"train_scores = [clf.score(X_train, Y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, Y_test) for clf in clfs]\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","e6183d72":"dic = dict(zip(ccp_alphas, test_scores))\nalpha = max(dic, key = dic.get)","42cdcc12":"tree = DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\ntree.fit(X_train, Y_train)","22e1f16e":"print(classification_report(Y_test, tree.predict(X_test)))","34ca7f82":"fig, ax = plt.subplots(figsize=(100, 100))\nfeatures = X_train.columns\n\nplot_tree(tree, filled = True, ax = ax, feature_names = features, proportion = True, rounded = True);","13e4ab8d":"rf = RandomForestClassifier(random_state = 1, n_jobs = -1)","0d5ba02b":"rf.fit(X_train, np.ravel(Y_train));","31613945":"Importance = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh' )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","c0b12cda":"data = {k : [] for k in np.geomspace(1e-3, 1e2, 20)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n\nfor i in list(data.keys()):\n    rf = RandomForestClassifier(random_state = 1, n_jobs = -1, ccp_alpha = i)\n    rf.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, rf.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, rf.predict(X_test)))","a5658929":"plt.figure(figsize = (15,15))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\nplt.show()","335e6f4f":"regr = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.01, random_state = 1)\nregr.fit(X_train, np.ravel(Y_train))","dde2a63b":"Importance = pd.DataFrame({'Importance':regr.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","13da5ba1":"grid_params = {'learning_rate': np.geomspace(1e-4, 100, num = 7),\n               'n_estimators': np.arange(100, 1000, 100),\n               'max_depth': [i for i in range(1,11)]}\n\ngs = GridSearchCV(regr, grid_params, cv = 10, n_jobs = -1, scoring = 'accuracy')\ngs.fit(X_train, Y_train)","79d523fb":"gs.best_estimator_.get_params()","417fce7b":"gs.best_estimator_.fit(X_train, Y_train)","caeddb29":"accuracy_score(Y_test, gs.best_estimator_.predict(X_test))","92d4bf2c":"data = {k : [] for k in np.geomspace(1e-4, 100, num = 7)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n#Different learning rate\n\nfor i in list(data.keys()):\n    regr = GradientBoostingClassifier(n_estimators = 1000, learning_rate = i, random_state = 1)\n    regr.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, regr.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, regr.predict(X_test)))","4d4cf499":"plt.figure(figsize = (10,6))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\nplt.show()","6360b10c":"depth = [i for i in range(1,11)]\n\n\nfor i in depth:\n    \n    regr = GradientBoostingClassifier(n_estimators = 500, learning_rate = 10, random_state = 1, max_depth = i)\n    #Using prior best test accuracy learning rate\n    regr.fit(X_train, Y_train)\n\n    train_acc.append(accuracy_score(Y_train, regr.predict(X_train)))\n    test_acc.append(accuracy_score(Y_test, regr.predict(X_test)))","edf90d27":"plt.figure(figsize = (10,6))\nplt.plot(depth, train_acc, marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(depth, test_acc, marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xlabel(\"depth\")\n\nplt.legend()\n\nplt.show()","dee4f791":"df_placement.info()","3e490107":"df_2 = df_placement.drop(columns = [\"status\", 'hsc_s', 'degree_t'])\ndf_2 = df_2.dropna()\ndf_2['gender'].replace(['M', 'F'], [0, 1], inplace = True)\ndf_2['ssc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_2['hsc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_2['workex'].replace(['No', 'Yes'], [0, 1], inplace = True)\ndf_2['specialisation'].replace(['Mkt&Fin', 'Mkt&HR'], [0, 1], inplace = True)\ndf_2 = df_2.join([pd.get_dummies(df_placement.hsc_s, prefix = df_placement.hsc_s.name),\n                  pd.get_dummies(df_placement.degree_t, prefix = df_placement.degree_t.name)\n                 ])\n#a loop with if condition can also convert to one hot vectors\n#can be packed into preprocessing data function","a3dd0611":"df_2.head()","78b0b9df":"plt.figure(figsize = (15,15))\nsns.heatmap(df_2.corr(method = \"spearman\"), annot = True)","baa51b7b":"X = df_2.drop(columns = [\"salary\"])\nY = df_2[\"salary\"]","8041f331":"reg_X_train, reg_X_test, reg_Y_train, reg_Y_test = train_test_split(X, Y, random_state = 1) ","1996620d":"lin_r = LinearRegression(normalize = True, n_jobs = -1)","df3c0d52":"lin_r.fit(reg_X_train, reg_Y_train)","3f5c2ffd":"def scoring(model, Y_test = reg_Y_test, X_test = reg_X_test, Y_train = reg_Y_train, X_train = reg_X_train):\n    R_squared = model.score(X_test, Y_test)\n    R2_adj =1 - (((1 - R_squared) * (len(X_test) - 1)) \/ (len(X_test) - (len(X_test.columns) - 1)))\n    test_mse = mean_squared_error(Y_test, model.predict(X_test))\n    train_mse = mean_squared_error(Y_train, lin_r.predict(X_train))\n    return [R_squared, R2_adj, train_mse, test_mse] \n    ","eff3ca90":"scoring(lin_r)","587eac45":"lasso_scores, ridge_scores = [], []\n\nfor i in np.geomspace(1e-2, 1e2, 5):\n    lasso = Lasso(alpha = i, normalize = True, random_state = 1, max_iter = 50000)\n    ridge = Ridge(alpha = i, normalize = True, max_iter = 50000)\n    \n    lasso.fit(reg_X_train, reg_Y_train)\n    ridge.fit(reg_X_train, reg_Y_train)\n    \n    lasso_scores.append(scoring(lasso))\n    ridge_scores.append(scoring(ridge))","7addf397":"plt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[0] for i in lasso_scores], label = \"Lasso R2\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[0] for i in ridge_scores], label = \"Ridge R2\", drawstyle=\"steps-post\")\nplt.xscale(\"log\")\nplt.legend()\n\nplt.show()\n\nplt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[1] for i in lasso_scores], label = \"Lasso Adjusted R2\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[1] for i in ridge_scores], label = \"Ridge Adjusted R2\", drawstyle=\"steps-post\")\nplt.xscale(\"log\")\nplt.legend()","55c840ac":"plt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[3] for i in ridge_scores],\n         label = \"Ridge test mse\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[3] for i in lasso_scores],\n         label = \"Lasso test mse\", drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\nplt.legend()\n\nplt.show()","8e331aed":"reg_tree = DecisionTreeRegressor(random_state = 1)","3617ef1b":"reg_tree.fit(reg_X_train, reg_Y_train)","9f403a0e":"scoring(reg_tree)\n#Very poor values of R2","d076d906":"path = reg_tree.cost_complexity_pruning_path(reg_X_train, reg_Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","e89403f6":"fig, ax = plt.subplots(figsize = [10,6])\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","04e06a55":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeRegressor(random_state=1, ccp_alpha=ccp_alpha)\n    clf.fit(reg_X_train, reg_Y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","35300136":"node_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize = (10,10))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","92211832":"scores = [scoring(clf) for clf in clfs]\n\nfig, ax = plt.subplots(2, 1, figsize=(10,6))\nax[0].set_xlabel(\"alpha\")\nax[1].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"mse\")\nax[1].set_ylabel(\"mse\")\nax[0].set_title(\"MSE vs alpha for training and testing sets\")\nax[0].plot(ccp_alphas, [i[2] for i in scores], marker='o', label=\"train mse\",\n        drawstyle=\"steps-post\")\nax[1].plot(ccp_alphas, [i[3] for i in scores], marker='o', label=\"test mse\",\n        drawstyle=\"steps-post\")\nax[0].legend()\nax[1].legend()\nplt.show()","a308b26c":"# Linear Regression","1d530f85":"# Logistic Regression","ff125f4c":"# Pruning Trees","eb0cae92":"# Trees also provide poor regression statistics","ced15591":"# Lasso and Ridge","d668db9b":"# Salary Regression","89cd9420":"# Classification task for placement","4a933707":"# Overall, Linear Regression is weak to predict the data, even with distance regularization","9f3e663b":"# Gradient Boosting","a4ccf78c":"# DecisionTreeClassifier, tune $\\alpha$","3d7d0a01":"# Random Forests","2dacfdc6":"# Prepare Train Test Sets","a8dbd7f8":"# Test on whether tuned parameter can be robust to random initialization of train_test_split","85eb11bd":"# Tree Regression","924b7005":"Suprisingly, Simple Logistic Regression yields higher test accuracy than KNN even with n_neighbors tuned.","18c66091":"# Trees","45f9d104":"# Ridge with tuned hyperparameter has very high test accuracy","38e30a7f":"# Ridge","a18c8d19":"Accuracy of KNN Classifier $\\approx$ 0.8 with n = 8","f6c857e3":"# KNN Classifier"}}