{"cell_type":{"a581cb4d":"code","5538901a":"code","6043fa2d":"code","d1cd0193":"code","da0bc22f":"code","92efd893":"code","9b4e4837":"code","a92a7b3b":"code","c1f84378":"code","5d0e6ac8":"code","01393b25":"code","85a934b5":"code","a118304e":"code","acb825f4":"code","4a915f23":"code","12354664":"code","c7229218":"code","2baa03ce":"code","e7b0356e":"code","7ad9f1ed":"code","1af39a99":"code","0829132d":"code","b2e99f55":"code","b851f3cf":"code","a6c5b0dd":"code","7ab723f7":"code","a13175be":"code","6f2ed9a1":"code","57f7eca0":"code","e8491cdd":"code","d473996b":"code","fe7c6ab3":"code","7596c3d0":"code","8f78a0b5":"code","4ecb7cc4":"code","ecab43ca":"code","c3d127b3":"code","3a2a9374":"code","5c75cb3f":"code","77653c75":"code","5e5a6af7":"code","e1f78ada":"code","4a2b7ebe":"code","4a77f317":"code","76e9a93b":"code","9f5d2cd3":"code","14f42915":"code","3c20079e":"code","be41f7a2":"code","6c611dcc":"code","e253203f":"code","ae4252b0":"code","40a5d171":"code","1964731e":"code","5c8d8e3b":"code","7c3d64da":"code","9bf23e6a":"code","1f62ee42":"code","aa4cea13":"code","372fec67":"code","9ac04042":"code","b549f4f1":"code","e7367251":"code","90931785":"code","12c7ce9b":"code","2815dcf4":"code","e48411bf":"code","5bf005a1":"code","81b32d06":"code","68d03369":"code","c81bd621":"code","332bd91f":"code","165eea02":"code","d4503743":"code","d4130b2f":"markdown","8b7ab76e":"markdown","232822b7":"markdown","c3f94868":"markdown","14a51a1c":"markdown","98e56a19":"markdown","53179e3d":"markdown","ed0dcd37":"markdown","e1aea73c":"markdown","2f30e1e3":"markdown","47740293":"markdown","1c3a2ddf":"markdown","892beb5d":"markdown","025a5ee7":"markdown","82142803":"markdown","ded30a04":"markdown","654030aa":"markdown","5eaff8d4":"markdown","f7060aad":"markdown","fff81f07":"markdown","fce3646c":"markdown","f928b728":"markdown","4687e6ec":"markdown","ced3bf60":"markdown","7e64f1c9":"markdown","e56f7e9c":"markdown","e3720b29":"markdown","65f202e9":"markdown","901a347e":"markdown","1e2089de":"markdown","1e0ccf3d":"markdown","b1918b0e":"markdown","749beec7":"markdown","344c0bbe":"markdown","a95e3a84":"markdown","b61af2d3":"markdown","266978eb":"markdown","13f52c3b":"markdown","0d434e2e":"markdown","56f63158":"markdown","ebd885fb":"markdown","23dfdba5":"markdown","62432b60":"markdown","ba38128e":"markdown","d6ca705a":"markdown","63cdf615":"markdown","11d76e69":"markdown","87449156":"markdown","5581700f":"markdown","3f5044a2":"markdown","5daafc61":"markdown","16fa2364":"markdown","3e080bf6":"markdown","cc9f401b":"markdown","9721185e":"markdown","48f8599d":"markdown","8b3436eb":"markdown","2bdc0aad":"markdown","8d75c495":"markdown","6709a6cc":"markdown","2130612f":"markdown","a513d92d":"markdown","3b110753":"markdown","32c91a12":"markdown","3e7f1d52":"markdown","e1c870a1":"markdown","53ca08b1":"markdown","e7728c97":"markdown","2f469e2c":"markdown","2c852b43":"markdown","9173ca76":"markdown","3a3f1228":"markdown","a28e4d8f":"markdown","692f027a":"markdown","545f9ad1":"markdown","4c43ea60":"markdown","a9f8603b":"markdown","b7800635":"markdown","444484d1":"markdown","0dbe5a60":"markdown","e409d7dc":"markdown","1ed2f433":"markdown","953fffb7":"markdown","0bd5d938":"markdown","8e2678af":"markdown","805fdb97":"markdown","98aeb3f8":"markdown","1a22ab13":"markdown","fabcf788":"markdown","a305639a":"markdown","7efdc3d9":"markdown","07e0b4e0":"markdown","5e9bfe75":"markdown"},"source":{"a581cb4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5538901a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sys\nimport os\nimport warnings\nwarnings.filterwarnings('ignore') \n#plt.style.use('seaborn')\nplt.style.use('fivethirtyeight')","6043fa2d":"admt=pd.read_csv('..\/input\/Admission_Predict.csv')","d1cd0193":"admt.head()","da0bc22f":"admt.isnull().sum()","92efd893":"print('Rows     :',admt.shape[0])\nprint('Columns  :',admt.shape[1])\nprint('\\nFeatures :\\n     :',admt.columns.tolist())\nprint('\\nMissing values    :',admt.isnull().values.sum())\nprint('\\nUnique values :  \\n',admt.nunique())","9b4e4837":"admt.columns.to_frame().T","a92a7b3b":"admt.count().to_frame().T","c1f84378":"print(\"There are\",len(admt.columns),\"columns:\")\nfor x in admt.columns:\n    sys.stdout.write(str(x)+\", \")                                                      #admt.columns also works ","5d0e6ac8":"admt.rename(columns={'Serial No.':'Srno','GRE Score':'GRE','TOEFL Score':'TOEFL','University Rating':'UnivRating','Chance of Admit ':'Chance'},inplace=True)","01393b25":"admt.head()","85a934b5":"admt.columns","a118304e":"admt.drop('Srno', axis=1, inplace=True)\nadmt.head()","acb825f4":"admt.describe().plot(kind = \"area\",fontsize=27, figsize = (20,8), table = True,colormap=\"rainbow\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Admissions\")\nplt.show()","4a915f23":"plt.figure(1, figsize=(10,6))\nplt.subplot(1,4, 1)\nplt.boxplot(admt['GRE'])\nplt.title('GRE Score')\n\nplt.subplot(1,4,2)\nplt.boxplot(admt['TOEFL'])\nplt.title('TOEFL Score')\n\nplt.subplot(1,4,3)\nplt.boxplot(admt['UnivRating'])\nplt.title('University Rating')\n\nplt.subplot(1,4,4)\nplt.boxplot(admt['CGPA'])\nplt.title('CGPA')\n\nplt.show()\n","12354664":"fig=plt.gcf()\nfig.set_size_inches(10,10)\nfig=sns.heatmap(admt.corr(),annot=True,cmap='inferno',linewidths=1,linecolor='k',square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)","c7229218":"#correlations_data = admt.corr()['Chance'].sort_values(ascending=False)\ncor=admt.corr()['Chance']\n# Print the correlations\nprint(cor)","2baa03ce":"admt[['GRE','TOEFL','UnivRating','CGPA']].hist(figsize=(10,8),bins=10,color='#ffd700',linewidth='1',edgecolor='k')\nplt.tight_layout()\nplt.show()","e7b0356e":"category = ['GRE','TOEFL','UnivRating','SOP','LOR ','CGPA','Research','Chance']\ncolor = ['yellowgreen','gold','lightskyblue','pink','red','purple','orange','gray']\nstart = True\nfor i in np.arange(4):\n    \n    if start == True:\n        fig = plt.figure(figsize=(14,8))\n        start = False\n        \n    plt.subplot2grid((4,2),(i,0))\n    admt[category[2*i]].hist(color=color[2*i],bins=10)\n    plt.title(category[2*i])\n    plt.subplot2grid((4,2),(i,1))\n    admt[category[2*i+1]].hist(color=color[2*i+1],bins=10)\n    plt.title(category[2*i+1])\n    \nplt.subplots_adjust(hspace = 0.7, wspace = 0.2)    \nplt.show()","7ad9f1ed":"print('Mean CGPA Score is :',int(admt[admt['CGPA']<=500].CGPA.mean()))\nprint('Mean GRE Score is :',int(admt[admt['GRE']<=500].GRE.mean()))\nprint('Mean TOEFL Score is :',int(admt[admt['TOEFL']<=500].TOEFL.mean()))\nprint('Mean University rating is :',int(admt[admt['UnivRating']<=500].UnivRating.mean()))","1af39a99":"a=len(admt[admt.Research==1])\nb=len(admt[admt.Research==0])\nprint('Total number of students',a+b)\nprint('Students having Research:',len(admt[admt.Research==1]))\nprint('Students not having Research:',len(admt[admt.Research==0]))","0829132d":"y=np.array([len(admt[admt.Research==1]),len(admt[admt.Research==0])])\nx=['Having Research','Not having Research']\nax=plt.bar(x,y,width=0.5,color='red',edgecolor='k',align='center',linewidth=2)\n#plt.xlabel('',fontsize=20)\nplt.ylabel('Student Count',fontsize=20)\n#ax.tick_params(labelsize=20)\nplt.title('Student Research',fontsize=25)\nplt.grid()\nplt.ioff()","b2e99f55":"f,ax=plt.subplots(1,2,figsize=(18,8))\nadmt['Research'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Students Research')\nax[0].set_ylabel('Student Count')\nsns.countplot('Research',data=admt,ax=ax[1])\nax[1].set_title('Students Research')\nplt.show()","b851f3cf":"sns.scatterplot(data=admt,x='GRE',y='TOEFL',hue='Research')\nplt.show()","a6c5b0dd":"def modiffy(row):\n    if row['Chance'] >0.7 :\n        return 1\n    else :\n        return 0\nadmt['Admit'] = admt.apply(modiffy,axis=1)\nadmttemp = admt.drop(['Chance'], axis=1)\n#sns.pairplot(admttemp,hue='Admit')\nsns.scatterplot(data=admttemp,x='GRE',y='TOEFL',hue='Admit')\ndel admttemp","7ab723f7":"sns.factorplot('Research','Admit',data=admt)\nplt.show()","a13175be":"admt_sort=admt.sort_values(by=admt.columns[-1],ascending=False)\nadmt_sort.head()\n#admt.head()\n#admttemp.head()","6f2ed9a1":"admt_sort[(admt_sort['Chance']>0.90)].mean().reset_index()","57f7eca0":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(\"Research\",\"GRE\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,2)\nsns.violinplot(\"Research\",\"TOEFL\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,3)\nsns.violinplot(\"Research\",\"CGPA\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,4)\nsns.violinplot(\"Research\",\"UnivRating\",hue=\"Admit\", data=admt,split=True)\n#ax[0].set_title('Pclass and Age vs Survived')\n#ax[0].set_yticks(range(0,110,10))\n#sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\n#ax[1].set_title('Sex and Age vs Survived')\n#ax[1].set_yticks(range(0,110,10))\nplt.ioff()\nplt.show()\n","e8491cdd":"f,ax=plt.subplots(1,2,figsize=(18,8))\nadmt['Admit'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Admitted to University')\nax[0].set_ylabel('')\nsns.countplot('Admit',data=admt,ax=ax[1])\nax[1].set_title('Admitted to University')\nplt.show()","d473996b":"from sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\n","fe7c6ab3":"#admt.head()","7596c3d0":"X=admt.iloc[:,:-2].values\n#X[0]\ny=admt.iloc[:,-2].values # or we can use y=data.iloc[:,3].values\n#y[0]","8f78a0b5":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05,random_state=0)","4ecb7cc4":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nreg=linear_model.LinearRegression()\nreg.fit(X_train,y_train)\n","ecab43ca":"pred1=reg.predict(X_test)\nprint(\"Mean Squared Error: \",mean_squared_error(y_test,pred1))\n#('Accuracy for Linear Regression is ',metrics.accuracy_score(y_pred,y_test))","c3d127b3":"y_test","3a2a9374":"pred1","5c75cb3f":"Score=['337','118','4','4.5','4.5','9.65','1']\nScore=pd.DataFrame(Score).T\nchance=reg.predict(Score)\nchance","77653c75":"plt.figure(figsize=(12,8))\ny=pred1\ny1=y_test\nx=np.arange(1, 21, 1)\nx1=np.arange(0,21,2)\nplt.plot(x,y,color='r',marker='o',label='Predicted')\nplt.plot(x,y1,color='g',label='Actual')\nplt.xticks(x1)\nplt.gca().legend(('Predicted','Test'))\nplt.xlabel('Cases',fontsize=20)\nplt.ylabel('Chance of Admission',fontsize=20)\nplt.title('Chance Predicted Vs Actual Values',fontsize=25)\nplt.grid()\nplt.ioff()","5e5a6af7":"#admt.head()","e1f78ada":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 1000,random_state = 123)\ncolumns = ['Admit']\nadmt.drop(columns, inplace=True, axis=1)\nX = admt.drop('Chance',axis = 1)\ny = admt['Chance']\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = .25,random_state = 123)\nrf_model = RandomForestRegressor(n_estimators = 1000,random_state = 123)\nrf_model.fit(X_train,y_train)\nfeature_importance = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.xlabel('Value',fontsize=20)\nplt.ylabel('Feature',fontsize=20)\nplt.title('Random Forest Feature Importance',fontsize=25)\nplt.grid()\nplt.ioff()\nplt.tight_layout()","4a2b7ebe":"#admt.head()","4a77f317":"X=admt_sort.iloc[:,[0,5]].values    # O represents GRE Score and 5 represnts CGPA \ny=admt_sort.iloc[:,8].values        # 8 tells us if the Candidate got Admission or not \n","76e9a93b":"from sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\nY = lab_enc.fit_transform(y)","9f5d2cd3":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0) \n#y_train","14f42915":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","3c20079e":"from sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","be41f7a2":"y_pred=classifier.predict(X_test)","6c611dcc":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","e253203f":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('yellow','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Predicting University Admission')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","ae4252b0":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('yellow','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Predicting University Admission')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","40a5d171":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_4=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier_4.fit(X_train,y_train)","1964731e":"y_pred_4=classifier_4.predict(X_test)","5c8d8e3b":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred_4)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","7c3d64da":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier_4.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Training set)')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","9bf23e6a":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier_4.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Test set)')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","1f62ee42":"X=admt_sort.iloc[:,[0,5]].values \n#X","aa4cea13":"from sklearn.cluster import KMeans\nwcss=[]\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","372fec67":"kmeans=KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans=kmeans.fit_predict(X)","9ac04042":"plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Must Improve') \nplt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='greenyellow',label='Excellent')  \nplt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='yellow',label='Good')   \nplt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='green',label='Outstanding')  #cyan\n#plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='burlywood',label='Sensible')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='magenta',label='Centroids')\nplt.title('Cluster of Students')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","b549f4f1":"admt_sort.head()","e7367251":"X=admt_sort.iloc[:,0:8].values\n\ny=admt_sort.iloc[:,8].values\n#X\n#y","90931785":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","12c7ce9b":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","2815dcf4":"import keras \nfrom keras.models import Sequential \nfrom keras.layers import Dense ","e48411bf":"classifier_6=Sequential()","5bf005a1":"classifier_6.add(Dense(output_dim=6,init='uniform',activation='relu',input_dim=8))","81b32d06":"classifier_6.add(Dense(output_dim=5,init='uniform',activation='relu'))","68d03369":"classifier_6.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))","c81bd621":"classifier_6.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","332bd91f":"classifier_6.fit(X_train, y_train,batch_size=10,nb_epoch=100)","165eea02":"y_pred=classifier_6.predict(X_test)\ny_pred=(y_pred>0.7)\n#y_pred","d4503743":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","d4130b2f":"## 5. K Means Clustering:\nK means is an unsupervised clustering algorithm.We use it here to see how the students will be getting clustered based on their GRE and CGPA Scores.","8b7ab76e":"## 6. Artificial Neural Network (ANN)","232822b7":"### Pie and Count Plot","c3f94868":"### Predicting the test set results","14a51a1c":"# 3.Machine Learning Model Build ","98e56a19":"Yes your chance of Admission increases if you do Research.","53179e3d":"# TO BE CONTINUED ","ed0dcd37":"### Feature Scaling","e1aea73c":"Heat map gives a good pictorial representation of the correlation of features to our target value chance of admit to university.Looking at the heat map to get the correlation can sometimes be condusing.One way out would be the to get the correlation values against target(Chance) as shown below.","2f30e1e3":"### 4.3 Making the confusion matrix","47740293":"### Making the confusion matrix","1c3a2ddf":"### Accuracy Score","892beb5d":"From the Elbow plot we can see that four could be the optiminal number of cluster for this analysis.","025a5ee7":"We can see that the name of the columns have be changed as per our convience.We can see that first  column is serial number it will not have any effect on the chance of admission to the University.We better drop the column of serial number from the data set.","82142803":"### Importing the Keras Library","ded30a04":"### Scatter Plot","654030aa":"## 2.Decision Tree","5eaff8d4":"We can clearly see that students with higher GRE and TOEFL scores have very high chance of getting an university admission.","f7060aad":"Based on the cluster I have catogerised the students into four catogeries.\n\n1.Outstanding - GRE> 327 + and CGPA > 8.5\n\n2.Ecxcellent -GRE> 317 + and CGPA > 7.7\n\n3.Good -GRE> 306 + and CGPA > 7.3\n\n4.Must Improve -GRE> 290 + and CGPA > 6.7","fff81f07":"We can see that 55% Students have done Research.It possible only the better student could get a chance for doing research.Doing research does add practical knowledge and increases the student skill of working with groups or teams.","fce3646c":"# 2.Exploratory Data Analysis","f928b728":"For having a 90% Chance to get admission one should have GRE=333.61,TOEFL=116.28,CGPA=9.53 .If you get scores more than this then your chances of admission are very good.","4687e6ec":"### Feature Scaling","ced3bf60":"### Splitting the dataset to Train and Test Set","7e64f1c9":"**Chance of admission**","e56f7e9c":"### Visualizing the Test Set Results","e3720b29":"Correct predictions =24+41=65\n\nWrong predictions =6+9=15\n\nAccuracy = (65\/180)*100 =81. 25 %","65f202e9":"### Getting Correlation Values","901a347e":"### Distribution Plot","1e2089de":"We are assuming here that students with 0.7 chance of admission have secured admission.We create another column in oour dataset named Admit.The value of Admit=1 if Chance>0.7 and Admit=0 if Chance<0.7.","1e0ccf3d":"## 4. K Means Classification ","b1918b0e":"### Renaming the columns to make our lives easy","749beec7":"The Values predicted by Linear regression are :","344c0bbe":"Above method can be used to find out the rows of values in the data set.","a95e3a84":"Looking at the column names we can see that we can make the names of the colums shorter.","b61af2d3":"## 1.Linear Regression","266978eb":"## 3.Logistic Regression:\nIt is used to predict binary results.In this case we have crerated the column Admit which tells us detail of Whether the candidate has got admission(1) or not (0).We have seen from the decision tree algorithm that CGPA and the GRE Score has the highest influence on the chance of admission.So while making a Logistic Regression we will use the values of CGPA and GRE score to predict the Admission to the University ","13f52c3b":"### Generating the Array of Features and Target Values","0d434e2e":"We can see that the column for serial number is droped or removed from the dataset.","56f63158":"We can display describe dunction in pictorial way.In most cases the describle table is sufficient for us the get valuable information about the data.","ebd885fb":"### Visualizing the Training Set Results","23dfdba5":"### 6.10 Fitting the ANN to training set","62432b60":"**Summary Of Dataset**","ba38128e":"**6.9 Compliling the ANN**","d6ca705a":"### Visualizing the Training Set Results","63cdf615":"### How important is Research to get an Admission?","11d76e69":"Correct predictions =28+39=64 \n\nWrong predictions =6+7=13\n\nAccuracy =(64\/77)*100 =83.11 %","87449156":"We can see CGPA,GRE,TOEFL and SOP arte most important features in the data set.","5581700f":"### Violin Plots","3f5044a2":"### Splitting the Data ","5daafc61":"### Factor Plot","16fa2364":"We can see that the maximum Chance of admission is 0.97.Lets find out the scores needed for 90 % chance of admission.","3e080bf6":"### Making the confusion matrix","cc9f401b":"### Predicted Vs Actual ","9721185e":"We can see that CGPA,GRE,TOEFL,University Ranking has the highest correlation with the chance of admission to the university.The other parameters like SOP,LOR and Research has less impact on the chance of admission.We can dropt he column Srno from our dataframe as it doesnt have any impact on the chance of admission.","48f8599d":"Correct prediction=30+42=72\n\nWrong predictions =8+0=8\n\nAccuracy of Prediction =(72\/80)*100 =90 %","8b3436eb":"**Importing the data into kernel**","2bdc0aad":"We can see that stutents who have done research do have good TOEFL and GRE Score.","8d75c495":"### Box Plot","6709a6cc":"### 6.6 Adding the input layer and the first hidden layer","2130612f":"**Importing the modules needed for the analysis **","a513d92d":"### Matrix of Features","3b110753":"# 6.Conclusion:\n\n1.We have students data like GRE,TOEFL score,University Rating,SOP,LOR,Research.We have renamed the columns,dropped unwanted features from our dataset.\n\n2.From EDA we can see that CGPA,GRE,TOEFL,University Ranking has the highest correlation with the chance of admission to the university.The other parameters like SOP,LOR and Research has less impact on the chance of admission.We can dropt he column Srno from our dataframe as it doesnt have any impact on the chance of admission.\n\n3.We can see that 55% Students have done Research.It possible only the better student could get a chance for doing research.Doing research does add practical knowledge and increases the student skill of working with groups or teams.We can clearly see that students with higher GRE and TOEFL scores have very high chance of getting an university admission.\n\n4.For having a 90% Chance to get admission one should have GRE=333.61,TOEFL=116.28,CGPA=9.53 .If you get scores more than this then your chances of admission are very good.We can clearly see that the student with research have higher chance of admission and their overall all GRE,TOEFL and CPGA scores are also high.We can see that 59% of the student have high chance of Admission.From the decision tree algorithm CGPA and GRE Score are most important feature for predicting the chance of admission.\n\n5.We have used Linear,Decision Tree,Logistic Regression,Kmeans classification,K Means Clustering and ANN to draws inference from the dataset.","32c91a12":"### Fitting Logistic Regression into Training set","3e7f1d52":"Above box plot shows us the min,median and max values for GRE,TOEFL,University rating and CGPA for the dataset.","e1c870a1":"### 5.2 Using Elbow method to find the optiminal cluster number","53ca08b1":"### Displaying columns in dataset ","e7728c97":"### Predicting the test set results","2f469e2c":"### Fitting K Nearest  Neighbor to Training set","2c852b43":"The Yellow region is the area of people who failed to get admission.Red dots represent the students who failed to get admission.\nGreen Dotd and Green Area represent the people who Managed to get admission.\n\n0-Not Admitted \n\n1-Admitted \n","9173ca76":"We can see that 59% of the student have high chance of Admission.","3a3f1228":"Target of an aspirant would be get more than the mean scores displayed above.","a28e4d8f":"This data set has the information on the GRE,TOEFL,CGPA and other details of students seeking Post graduation admission at Universities.We will try to exprole the data and see what we can understand from it.The deeper question would be are college degrees revalent in the era of Nano degrees? In this kernel we will be covering following things.\n\n1.Data Import and Pre Processing \n\n2.Exploratory Data Analysis \n\n3.Machine Learning Model Build \n\n4.Conclusion ","692f027a":"### 5.3. Applying K means to the Dataset","545f9ad1":"**6.7 Adding the second hidden Layer**","4c43ea60":"### 6.8 Adding the output layer","a9f8603b":"### Dropping Unwanted Columns","b7800635":"### Splitting the dataset to Train and Test Set","444484d1":"Predicting the chance for a use case.We give the input to the algorithm in the form of a list as shown below.","0dbe5a60":"So the algrothim predicts the value as 0.95 against the actual value 0.92","e409d7dc":"### Generating Array of Features and Target Values","1ed2f433":"### Visualizing the Test Set Results","953fffb7":"We can clearly see that the student with research have higher chance of admission and their overall all GRE,TOEFL and CPGA scores are also high.","0bd5d938":"The test values from the dataset are :","8e2678af":"We can see from the above plot that we have fairly good correlation.","805fdb97":"### Describing the Dataset","98aeb3f8":"### 5.1 Generating the Array of Features","1a22ab13":"### Visualizing the clusters","fabcf788":"### 6.5 Initialising the ANN","a305639a":"### Correlation Plot","7efdc3d9":"### Predicting the test set results","07e0b4e0":"We can see that there are no missing values in the data set.","5e9bfe75":"### What should be your Scores for fore than 90 % Chance of Admission?"}}