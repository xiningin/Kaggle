{"cell_type":{"d570c8fb":"code","4f7f1ea2":"code","e855979c":"code","4e74356a":"code","1116eeb7":"code","3b2008e2":"code","f557edf8":"code","678923c5":"code","aa6164d4":"code","1b516232":"code","7bbd1398":"code","dfd3cdc6":"code","70f97c39":"code","ae285ad5":"code","f8131629":"code","715a4ecc":"code","78fa7420":"code","495bbb76":"code","fd44cde9":"code","f1c691a9":"code","fea9af34":"code","dc0cc285":"code","db45cc81":"code","11b3951f":"code","78a512b1":"code","fbd53245":"code","90417984":"code","5c4c8856":"code","426d6836":"code","dcf1c330":"code","b968296a":"code","5b13593c":"code","032f210f":"code","4cf1c3cd":"code","5281767f":"code","33e3947f":"code","32663f94":"code","647de8bc":"code","c9f8c12a":"code","371d8aca":"code","41e7423b":"code","c936d7d2":"code","ffae6c11":"code","3d38adc4":"code","7fe65ee7":"code","9e82e84b":"code","5a8a3cc5":"code","71ffa38e":"code","5ba78337":"code","e1a04c41":"code","81adb9a9":"code","bcc0eba7":"code","ad29342d":"code","52c7c6c9":"markdown","3799248e":"markdown","704c4d20":"markdown","72112065":"markdown","252f3a10":"markdown","553f5581":"markdown","2cb3d31c":"markdown","926f0451":"markdown","f4b8e626":"markdown","ca4b83a0":"markdown","2047f487":"markdown","a0652173":"markdown","5d63348c":"markdown","b3f5b162":"markdown","676da865":"markdown","9d026d6e":"markdown","bfca00b7":"markdown","2416dcbc":"markdown","716261af":"markdown","c54a0f1d":"markdown","34b5c723":"markdown"},"source":{"d570c8fb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","4f7f1ea2":"df=pd.read_csv('..\/input\/Test 1.csv')\ndf.head()","e855979c":"df.shape","4e74356a":"df.isnull().sum()","1116eeb7":"df.info()","3b2008e2":"cust_id=df.customer_id\ntype(cust_id)","f557edf8":"df.drop('customer_id',axis=1,inplace=True)\ndf.head()","678923c5":"df['country_reg'].value_counts()","aa6164d4":"df2=pd.get_dummies(df,columns=['demographic_slice','ad_exp','country_reg'])\ndf2.head()\n","1b516232":"outcome=np.where(df['card_offer']==True,1,0)\ndf2['card_offer']=outcome\ndf2.head()","7bbd1398":"X=df2.drop('card_offer',axis=1)\nY=df2['card_offer']\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)\n","dfd3cdc6":"log=LogisticRegression(C=0.0007)\nlog.fit(x_train,y_train)\nprint('Train score:',log.score(x_train,y_train))\nprint('Test score:',log.score(x_test,y_test))\n#print(log.C_)","70f97c39":"from sklearn.metrics import confusion_matrix,f1_score,roc_curve,roc_auc_score","ae285ad5":"fpr,tpr,threholds=roc_curve(y_test,log.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(log.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","f8131629":"y_pred=np.where(log.predict_proba(x_test)[:,1]>0.15,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","715a4ecc":"f1_score(y_test,y_pred)","78fa7420":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","495bbb76":"dt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint('Train score:',dt.score(x_train,y_train))\nprint('Test score:',dt.score(x_test,y_test))","fd44cde9":"fpr,tpr,threholds=roc_curve(y_test,dt.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(dt.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","f1c691a9":"y_pred=np.where(dt.predict_proba(x_test)[:,1]>0,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","fea9af34":"f1_score(y_test,y_pred)","dc0cc285":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","db45cc81":"rf=RandomForestClassifier()\nrf.fit(x_train,y_train)\nprint('Train score:',rf.score(x_train,y_train))\nprint('Test score:',rf.score(x_test,y_test))","11b3951f":"fpr,tpr,threholds=roc_curve(y_test,rf.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(rf.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","78a512b1":"y_pred=np.where(rf.predict_proba(x_test)[:,1]>0.3,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","fbd53245":"f1_score(y_test,y_pred)","90417984":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","5c4c8856":"gbr=GradientBoostingClassifier()\ngbr.fit(x_train,y_train)\nprint('Train score:',gbr.score(x_train,y_train))\nprint('Test score:',gbr.score(x_test,y_test))","426d6836":"fpr,tpr,threholds=roc_curve(y_test,gbr.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(gbr.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","dcf1c330":"y_pred=np.where(gbr.predict_proba(x_test)[:,1]>optTh,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","b968296a":"f1_score(y_test,y_pred)","5b13593c":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","032f210f":"from sklearn.naive_bayes import GaussianNB","4cf1c3cd":"nb=GaussianNB()\nnb.fit(x_train,y_train)\nprint('Train score:',nb.score(x_train,y_train))\nprint('Test score:',nb.score(x_test,y_test))","5281767f":"fpr,tpr,threholds=roc_curve(y_test,nb.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(nb.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","33e3947f":"y_pred=np.where(nb.predict_proba(x_test)[:,1]>optTh,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","32663f94":"f1_score(y_test,y_pred)","647de8bc":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","c9f8c12a":"neighbors = np.arange(1, 20)\ntrain_accuracy_plot = np.empty(len(neighbors))\ntest_accuracy_plot = np.empty(len(neighbors))\n# Loop over different values of k\nsc=StandardScaler()\nscaledX_train = sc.fit_transform(x_train)\nscaledX_test = sc.transform(x_test)\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(scaledX_train,y_train)\n    train_accuracy_plot[i] = knn.score(scaledX_train,y_train)\n    test_accuracy_plot[i] = knn.score(scaledX_test,y_test)\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy_plot, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy_plot, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","371d8aca":"knn=KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train,y_train)\nprint('Train score:',knn.score(x_train,y_train))\nprint('Test score:',knn.score(x_test,y_test))","41e7423b":"fpr,tpr,threholds=roc_curve(y_test,knn.predict_proba(x_test)[:,1])\noptF1=0\noptTh=0\nfor th in threholds:\n    preds=np.where(knn.predict_proba(x_test)[:,1]>th,1,0)\n    f1=f1_score(y_test,preds)\n    if(optF1<f1):\n        optF1=f1\n        optTh=th\nprint('Optimum F1:',optF1)\nprint('Optimum Threshold:',optTh)","c936d7d2":"y_pred=np.where(knn.predict_proba(x_test)[:,1]>optTh,1,0)\ncn=confusion_matrix(y_test,y_pred)\ncn=pd.DataFrame(cn,columns=['Predicted 0','Predicted 1'],index=['Actual 0','Actual 1'])\ncn","ffae6c11":"f1_score(y_test,y_pred)","3d38adc4":"roc_auc=roc_auc_score(y_test,y_pred)\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","7fe65ee7":"test=pd.read_csv('..\/input\/Test 2.csv')\ntest.head()","9e82e84b":"test.shape","5a8a3cc5":"test.isnull().sum()","71ffa38e":"test=pd.get_dummies(test,columns=['demographic_slice','ad_exp','country_reg'])\ntest.head()","5ba78337":"X=test.drop(['customer_id','card_offer'],axis=1)\noptTh=0.3618855170800892","e1a04c41":"Y_pred=np.where(gbr.predict_proba(X)[:,1]>optTh,1,0)","81adb9a9":"Y_pred.shape","bcc0eba7":"test['card_offer']=Y_pred\ntest['card_offer']=np.where(test['card_offer']==1,True,False)\ntest.head()","ad29342d":"test.to_csv('Prediction.csv')","52c7c6c9":"* Training accuracy is 85%\n* Testing accuracy is 83%\n* F1 score with optimum threshold is 33%","3799248e":"### Boosting","704c4d20":"#### Splitting data into train and test","72112065":"Converting all categorical features into encoded features using one hot encoding","252f3a10":"* There are no null values in the data set. It is a clean data set\n* The dataset consists of 10000 observations and 12 variables","553f5581":"There are :\n* 3 categorical variables\n* 8 continuous variables\n* 1 boolean output variable","2cb3d31c":"### Naive Bayes","926f0451":"### Random Forest","f4b8e626":"* Training accuracy is 99%\n* Testing accuracy is 97%\n* F1 score with optimum threshold is 92%","ca4b83a0":"### KNN","2047f487":"### Decision Tree","a0652173":"### Conclusion","5d63348c":"* Training accuracy is 84%\n* Testing accuracy is 84%\n* F1 score with optimum threshold is 37%","b3f5b162":"* Training accuracy is 99%\n* Testing accuracy is 96%\n* F1 score with optimum threshold is 89%","676da865":"* Training accuracy is 100%\n* Testing accuracy is 96%\n* F1 score with optimum threshold is 87%","9d026d6e":"* Taking customer id in a series so that we can refer it later\n* cutomer_id is of no use to us in model building. So it can be safely removed","bfca00b7":"* Training accuracy is 84%\n* Testing accuracy is 84%\n* F1 score with optimum threshold is 50%","2416dcbc":"### Predicting using Gradient Boosting","716261af":"### Logistic Regression","c54a0f1d":"* KNN gives Training accuracy as 85%, Testing accuracy as 83% and F1 score with optimum threshold is 33%\n* Naive Bayes gives Training accuracy as 84%, Testing accuracy as 84% and F1 score with optimum threshold is 50%\n* Logistic gives Training accuracy as 84%, Testing accuracy as 84% and F1 score with optimum threshold is 37%\n* Decision Tree gives Training accuracy as 100%, Testing accuracy as 96% and F1 score with optimum threshold is 87%\n* Random Forest gives Training accuracy as 99%, Testing accuracy as 96% and F1 score with optimum threshold is 89%\n* Gradient Boosting gives Training accuracy as 99%, Testing accuracy as 97% and F1 score with optimum threshold is 92%\n* By comparing all models we found that GradientBoosting gives best accuracy and F1 score for our data set. Thus GradientBoosting will be our final model","34b5c723":"### -------------------------------------------------------------------END-------------------------------------------------------------------"}}