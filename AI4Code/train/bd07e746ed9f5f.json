{"cell_type":{"c811eaa4":"code","c678d9bf":"code","e68a2cca":"code","c5b85591":"code","70c7fd4b":"code","d28be8a9":"code","80965133":"code","3abfc24e":"code","032b4dce":"code","705e02cd":"code","9fd5279e":"code","7541d81f":"code","b52adab5":"code","db789256":"code","93cd3620":"code","41d55d84":"code","204c7e84":"code","64785c65":"code","14c537cc":"code","a2203dcb":"code","0c15465d":"code","c2c94a88":"code","67555a99":"code","87f0e3ac":"code","52277f2d":"code","1102c978":"code","75820341":"code","d25ee306":"code","89fc3a28":"code","56b4eb63":"code","ca33a639":"code","7394f9e7":"code","ac10de6c":"code","52946bfc":"code","e7a5e6f8":"code","ae444123":"code","2a2e13e4":"code","51646ed7":"code","1f4cbaaa":"code","3f55d686":"code","0fd4df46":"code","3dc868cd":"code","eea228e3":"code","e488c6a7":"code","2921d8a6":"code","44d0b00d":"code","d7ce35fc":"code","b8ebcf88":"code","2a540810":"code","a09bd5d3":"code","795ae57b":"code","4ea5ab22":"code","b0cd7fb5":"code","2de0a7b4":"code","aa74aa21":"code","36f273c9":"code","efa1fa3c":"code","7b460cb1":"code","6c27393d":"code","123ea38e":"code","5f95631f":"code","a60d5a7e":"code","37bba0db":"code","67796ea5":"code","600bfdeb":"code","e88613a8":"code","5e72671c":"code","b34d0f50":"code","c3248704":"code","3a8c1828":"code","d703afbd":"code","12d03047":"code","6f7028dc":"code","d2430f92":"code","a811e077":"code","1666d3c5":"code","6ed3ad59":"code","44a29375":"code","0a84437a":"code","649889d9":"code","2f501a2b":"code","deec2435":"code","dd3e2d28":"code","cf54c1e8":"code","4138c128":"code","4ddb094a":"code","5c5836ce":"code","4a94f1b8":"code","03179586":"code","1f2cfc98":"code","ed85ea6c":"code","d3ef9a12":"code","acb8a966":"code","5508a740":"code","1823a9b9":"code","3d25ba0c":"code","b986bb21":"code","aceab73a":"code","45e96116":"code","80ebca92":"code","53044627":"code","fb82e247":"code","45953431":"code","11d016d2":"code","89f0fccd":"code","7d30983d":"code","3d9b44e3":"code","5e7852da":"code","7992df9a":"code","2b944fb0":"code","2fecdf6f":"code","970b9822":"code","3ec56d7c":"code","e6f7b633":"markdown","14872bad":"markdown","9bb944f6":"markdown","3395be20":"markdown","d2f76834":"markdown","f904122b":"markdown","a08cc258":"markdown","1f242e35":"markdown","64cdb5a0":"markdown","5b305cbe":"markdown","de3e1554":"markdown","487482fb":"markdown","abe57d93":"markdown","cdff1de8":"markdown","1d396d77":"markdown","0c8f56eb":"markdown"},"source":{"c811eaa4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c678d9bf":"train = pd.read_csv('..\/input\/titanic\/train.csv')","e68a2cca":"test = pd.read_csv('..\/input\/titanic\/test.csv')","c5b85591":"train.head()","70c7fd4b":"test.head()","d28be8a9":"titanic_data = [train, test]","80965133":"train.count()","3abfc24e":"test.count()","032b4dce":"train.shape,test.shape","705e02cd":"pd.set_option('display.max_row',None) \npd.set_option('display.max.column', None)","9fd5279e":"train.info()","7541d81f":"test.info()","b52adab5":"# i will explore the survived information with the sex,age and pclass from the train data set\n","db789256":"train.groupby('Sex')[['Sex','Survived']].mean()","93cd3620":"pd.crosstab(train.Sex, train.Survived)","41d55d84":"Sex=(pd.crosstab(train.Sex, train.Survived) \/ train.shape[0]*100)\nSex.div(Sex.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(15,4))\n","204c7e84":"train.Sex.value_counts() \/ train.shape[0] * 100","64785c65":"#print(train.groupby(('Pclass')[['Pclass','Survived']], as_index=False).mean())\nprint (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","14c537cc":"pd.crosstab(train.Pclass, train.Survived)","a2203dcb":"Pclass=pd.crosstab(train.Pclass, train.Survived)\nPclass.div(Pclass.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(15,4))\n\n","0c15465d":"train.groupby('Age')[['Age','Survived']].mean()","c2c94a88":"pd.crosstab(train.Age, train.Survived)","67555a99":"train.Age.value_counts() \/ train.shape[0] * 100","87f0e3ac":"Age=pd.crosstab(train.Age, train.Survived)\nAge.div(Age.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(15,4))\n\n","52277f2d":"test.head()","1102c978":"train.shape, test.shape","75820341":"titanic_data = [train, test]","d25ee306":"for dataset in titanic_data:\n    dataset['Familysize'] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\nprint(train[['Familysize','Survived']].groupby(['Familysize'],as_index=False).mean())","89fc3a28":"pd.crosstab(train.Familysize, train.Survived)","56b4eb63":"Familysize=pd.crosstab(train.Familysize, train.Survived)\nFamilysize.div(Familysize.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(15,4))","ca33a639":"train.head()\ntrain.drop(labels= ['SibSp','Parch'], axis=1, inplace = True)","7394f9e7":"train.head()","ac10de6c":"test.head()\ntest.drop(labels= ['SibSp','Parch'], axis=1, inplace = True)","52946bfc":"test.head()","e7a5e6f8":"train.isna().sum()\n#train.isnull().sum()[train.isnull().any()]\n#train.isnull().sum()[train.isnull().any()] \/ train.shape[0] * 100","ae444123":"train.isnull().sum()[train.isnull().any()]","2a2e13e4":"train.isnull().sum()[train.isnull().any()] \/ train.shape[0] * 100","51646ed7":"test.head()\ntest.drop(labels= ['Cabin'], axis=1, inplace = True)","1f4cbaaa":"test.head()","3f55d686":"train.head()\ntrain.drop(labels= ['Cabin'], axis=1, inplace = True)","0fd4df46":"train.head()","3dc868cd":"train['Age'] = train['Age'].fillna(train['Age'].mean())\ntrain['Embarked'] = train['Embarked'].fillna('S')","eea228e3":"test['Age'] = test['Age'].fillna(test['Age'].mean())\ntest['Embarked'] = test['Embarked'].fillna('S')\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())\n","e488c6a7":"test.info()","2921d8a6":"train.info()","44d0b00d":"train['Ticket'].nunique() \/ train.shape[0] * 100","d7ce35fc":"train.head()\ntrain.drop(labels= ['Ticket'], axis=1, inplace = True)","b8ebcf88":"test.head()\ntest.drop(labels= ['Ticket'], axis=1, inplace = True)","2a540810":"titanic_data = [train,  test]","a09bd5d3":"for dataset in titanic_data:\n    dataset['title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)","795ae57b":"pd.crosstab(train.title, train.Sex)","4ea5ab22":"for dataset in titanic_data:\n    dataset['title'] = dataset['title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['title'] = dataset['title'].replace('Mlle', 'Miss')\n    dataset['title'] = dataset['title'].replace('Ms', 'Miss')\n    dataset['title'] = dataset['title'].replace('Mme', 'Mrs')","b0cd7fb5":"print(train[['title','Survived']].groupby(['title'], as_index=False).mean())","2de0a7b4":"pd.crosstab(train.title, train.Survived)","aa74aa21":"title=pd.crosstab(train.title, train.Survived)\ntitle.div(title.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,figsize=(15,4))","36f273c9":"train.drop(labels='Name', axis=1, inplace=True)\ntest.drop(labels='Name', axis=1, inplace=True)","efa1fa3c":"test.info()","7b460cb1":"train.info()","6c27393d":"train.drop(labels='PassengerId', axis=1, inplace=True)","123ea38e":"for dataset in titanic_data:\n    dataset['Sex'] = dataset['Sex'].map({'female':0,'male':1}).astype('int64')","5f95631f":"train.Sex.value_counts()","a60d5a7e":"train.info()","37bba0db":"train.Embarked.value_counts()","67796ea5":" titanic_data = [train, test]","600bfdeb":"for dataset in titanic_data:\n    dataset['Embarked'] = dataset['Embarked'].map({'S':0,'C':1,'Q':2}).astype('int64')","e88613a8":"train.Embarked.value_counts()","5e72671c":"test.Embarked.value_counts()","b34d0f50":"train.title.value_counts()","c3248704":"for dataset in titanic_data:\n    dataset['title'] = dataset['title'].map({'Mr':0,'Miss':1,'Mrs':2,'Master':3,'Rare':4}).astype('int64')","3a8c1828":"train.title.value_counts()","d703afbd":"test.title.value_counts()","12d03047":"test.info()\n","6f7028dc":"train.info()","d2430f92":"train[\"AgeState\"] = pd.cut(train.Age, bins = [0, 2, 5, 18,60,100], labels=[\"Infant\", \"Toddler\", \"Child\", \"Adult\", 'Senior_Citizen'],include_lowest=True)","a811e077":"test[\"AgeState\"] = pd.cut(test.Age, bins = [0, 2, 5, 18,60,100], labels=[\"Infant\", \"Toddler\", \"Child\", \"Adult\", 'Senior_Citizen'],include_lowest=True)","1666d3c5":"train.AgeState.value_counts()","6ed3ad59":"train.drop(labels='Age', axis = 1, inplace = True)\ntest.drop(labels='Age', axis = 1, inplace = True)\n","44a29375":"titanic_data = [train, test]","0a84437a":"for dataset in titanic_data:\n    dataset['AgeState'] = dataset['AgeState'].map({'Adult':0,'Child':1,'Infant':2,\n                                                  'Senior_Citizen':3,'Toddler':4}).astype('int64')","649889d9":"train.AgeState.value_counts()","2f501a2b":"train.info()","deec2435":"test.info()","dd3e2d28":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","cf54c1e8":"from xgboost import XGBRFClassifier\nfrom sklearn.model_selection import  train_test_split","4138c128":"X = train.drop('Survived', axis = 1)\ny = train['Survived']\nX_test_data = test.drop('PassengerId', axis = 1).copy()","4ddb094a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state = 10)","5c5836ce":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","4a94f1b8":"from sklearn.model_selection import GridSearchCV","03179586":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","1f2cfc98":"y_pred = model.predict(X_test)\ny_pred","ed85ea6c":"print(accuracy_score(y_pred, y_test) *100)","d3ef9a12":"y_pred_train = model.predict(X_train)\nprint(accuracy_score(y_pred_train, y_train) *100)","acb8a966":"param_grid = {\n    'n_estimators':[200,150,100,50],\n    'criterion':['gini','entropy'],\n    'max_depth':[2,4,6]\n}","5508a740":"rf = RandomForestClassifier()","1823a9b9":"grid = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)","3d25ba0c":"grid.fit(X_train, y_train)","b986bb21":"rf = grid.best_estimator_","aceab73a":"y_pred = rf.predict(X_test)\nprint(accuracy_score(y_pred, y_test))","45e96116":"y_pred_train = rf.predict(X_train)\nprint(accuracy_score(y_pred_train, y_train))","80ebca92":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(criterion='entropy', max_depth=6)","53044627":"rf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(accuracy_score(y_pred, y_test))\ny_train_pre = rf.predict(X_train)\nprint(accuracy_score(y_train_pre, y_train))","fb82e247":"scores = cross_val_score(rf, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","45953431":"print(\"Accuracy_score\", accuracy_score(y_train_pre, y_train))\nprint(\"Mean:\", scores.mean())\nprint(\"Variation:\", scores.var())","11d016d2":"#X_test_data.drop(labels='Ticket', axis=1,inplace = True)","89f0fccd":"X_test_data.info()","7d30983d":"y_preds_test = rf.predict(X_test_data)","3d9b44e3":"y_preds_test.shape","5e7852da":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(rf.feature_importances_,3)})","7992df9a":"importances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","2b944fb0":"importances.plot(kind='bar',figsize=(15,6))","2fecdf6f":"my_submission_titanic = pd.DataFrame({'Passengerid':test['PassengerId'],\n                                     'Survived':y_preds_test})","970b9822":"my_submission_titanic.head()","3ec56d7c":"my_submission_titanic.to_csv(\"nkechiijeoma_submission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","e6f7b633":"Using Randomforestclassifier","14872bad":"Cleaning the Data, Checking for missing information","9bb944f6":"SEX","3395be20":"the cabin would result in a lot of outliners so it is best to drop it for both the test and train data","d2f76834":"MODEL BUILDING\n","f904122b":"Using LogisticRegression to build","a08cc258":"Create a new variable called Title from Name","1f242e35":"More of the survival where Females ","64cdb5a0":"Exploring SibSp and Parch they are  quite similar so i would be combining them","5b305cbe":"Tickets","de3e1554":"AGE","487482fb":"Data Exploration","abe57d93":"We have created family sixe to represent SibSp and Parch so we have have to drop the data from both the train and test data set","cdff1de8":"PCLASS","1d396d77":"A large percentage of the tickets are unqiue and would not add any value to the data set , i wuld drop the ticket for both the test and train data\n","0c8f56eb":"The survival rate was higher in Pclass 1 compared to the remaining class\nThe Death rate was hiher in Pclass 3."}}