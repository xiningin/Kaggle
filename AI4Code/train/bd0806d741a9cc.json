{"cell_type":{"84a80322":"code","245f11f7":"code","b4de2cbd":"code","51eb1cb5":"code","52ca36f5":"code","94d6e56d":"code","d58163ee":"code","c73c14cf":"code","b094052e":"code","135e6e12":"code","ac858fdc":"code","47f604ad":"code","5919f404":"code","94922ecc":"code","d4c29f57":"code","55c87fe3":"code","d21a4790":"code","2e9561e2":"code","84eb542f":"code","94893dcd":"code","c415be29":"code","a4a37b48":"code","b10515a7":"code","96857855":"markdown","ffe23055":"markdown","ea9eea53":"markdown","9e232ed3":"markdown","4c33df66":"markdown","b79c6bfe":"markdown","1ee7f5b2":"markdown","300e7e38":"markdown","ed2d8771":"markdown","a9336982":"markdown","a598fb22":"markdown","5d52c1b8":"markdown","07febee8":"markdown","0c40a8a3":"markdown","4d6829e4":"markdown","ba4cfca9":"markdown","bb576016":"markdown","45df5440":"markdown","862c266f":"markdown","1f2aee82":"markdown","04d6958a":"markdown","e723f838":"markdown","ee63d4a7":"markdown","61bf4f4b":"markdown"},"source":{"84a80322":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport tensorflow as tf\n\npd.options.mode.chained_assignment = None\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ndf = pd.read_csv('\/kaggle\/input\/netflix-signups\/netflix-signups.csv')","245f11f7":"from sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport tensorflow as tf\n\npd.options.mode.chained_assignment = None","b4de2cbd":"df.timestamp = pd.to_datetime(df.timestamp)\ndf['date'] = df.timestamp.dt.date #We create the column date that contains the date of the signup\ndf = df.assign(timestamp_H=df.timestamp.dt.round('H')) #We create the column timestamp_H that contains the date and the hour\ndf['COUNT'] = 1 #We will use this count variable in many groupby functions below.","51eb1cb5":"df.groupby('http_platform_name').count()['COUNT'].plot.pie(); plt.show()","52ca36f5":"df = df.loc[df.http_platform_name.isin(['Linux', 'Mac OS', 'Windows'])]","94d6e56d":"df.pivot_table(index = 'date', columns = 'ip_continent', aggfunc = 'count').COUNT.plot(figsize=(12,4))\nplt.gcf().autofmt_xdate(); plt.ylabel('Number of signups per day'); plt.show()","d58163ee":"df1 = df.groupby(['ip_country', 'date']).count().COUNT\ndispersion_index = (df1.groupby(['ip_country']).var()\/df1.groupby(['ip_country']).mean()).sort_values(ascending = False)\ndispersion_index[:20].plot.barh(); plt.xlabel('Dispersion Index'); plt.show()","c73c14cf":"fig=plt.figure(figsize=(12,4))\nplt.plot(df.groupby([df.timestamp_H]).count().COUNT)\nplt.xlabel('date & hour'); plt.ylabel('number of signups'); plt.gcf().autofmt_xdate(); plt.show()","b094052e":"print('___________________________________CONTINENTS___________________________________')\nfor continent in ['Europe', 'Asia', 'North-America', 'Oceania', 'South-America','Africa']:\n    df_continent = df.loc[df.ip_continent == continent]\n    df_continent_per_date_hour = df_continent.groupby([df_continent.timestamp_H]).count()[['COUNT']].reset_index()\n    groupby_hour = df_continent_per_date_hour.groupby(df_continent_per_date_hour.timestamp_H.dt.hour).COUNT\n    mean_per_hour = groupby_hour.mean()\n    std_per_hour  = groupby_hour.std()   \n    fig, ax = plt.subplots()\n    ax.bar(mean_per_hour.index, height = mean_per_hour.values, yerr = std_per_hour.values)\n    fig.suptitle(continent); ax.set_xlabel('hour'); ax.set_ylabel('average number of signups')\nplt.show()\n\nprint('___________________________________TIMEZONES___________________________________')\n\nfor timezone in df.js_timezone.value_counts().index[:5]:\n    df_timezone = df.loc[df.js_timezone == timezone]\n    df_timezone_per_date_hour = df_timezone.groupby([df_timezone.timestamp_H]).count()[['COUNT']].reset_index()\n    groupby_hour = df_timezone_per_date_hour.groupby(df_timezone_per_date_hour.timestamp_H.dt.hour).COUNT\n    mean_per_hour = groupby_hour.mean()\n    std_per_hour  = groupby_hour.std()\n    fig, ax = plt.subplots()\n    ax.bar(mean_per_hour.index, height = mean_per_hour.values, yerr = std_per_hour.values)\n    fig.suptitle(timezone)\n    plt.show()","135e6e12":"coeff_IQR = 1.5\ns = 0\n\ndf_errors_inf = pd.DataFrame() #Contains the list of abnormally low values \ndf_errors_sup = pd.DataFrame() #Contains the list of abnormally high values\n\nfor timezone in df.js_timezone.unique(): #We work with timezones\n    df_timezone = df.loc[df.js_timezone == timezone]\n    count_per_timestamp = df_timezone.groupby(\"timestamp\").count()[['COUNT']].reset_index()\n    \n    \n    df_timezone['hour'] = df_timezone.timestamp.dt.hour\n    \n    count_per_timestamp['hour'] = count_per_timestamp.timestamp.dt.hour\n    \n    groupby_hour = count_per_timestamp.groupby('hour').COUNT\n    Q1 = groupby_hour.quantile(0.25)\n    Q3  = groupby_hour.quantile(0.75)\n    mean_per_hour = groupby_hour.mean()\n    IQR = Q3-Q1\n    count_per_timestamp.set_index('hour', inplace = True)\n    count_per_timestamp['LB'] = np.min(Q1 - coeff_IQR*IQR, 0)\n    count_per_timestamp['UB'] = Q3 + coeff_IQR*IQR\n    count_per_timestamp['mean'] = mean_per_hour\n    \n    \n    df_errors_inf_timezone = count_per_timestamp.loc[(count_per_timestamp.COUNT < count_per_timestamp['LB'])]\n    df_errors_sup_timezone = count_per_timestamp.loc[(count_per_timestamp.COUNT > count_per_timestamp['UB'])]\n    \n    \n    df_errors_inf_timezone['timezone'] = timezone\n    df_errors_sup_timezone['timezone'] = timezone\n\n    \n    df_errors_inf = pd.concat([df_errors_inf, df_errors_inf_timezone])\n    df_errors_sup = pd.concat([df_errors_sup, df_errors_sup_timezone])","ac858fdc":"df_errors_inf.head()","47f604ad":"df_errors_sup.head()","5919f404":"df = pd.read_csv('\/kaggle\/input\/netflix-signups\/netflix-signups.csv')\ndf.timestamp = pd.to_datetime(df.timestamp)\ndf['date'] = df.timestamp.dt.date\ndf = df.assign(timestamp_H=df.timestamp.dt.round('H'))\ndf['COUNT'] = 1\ndf = df.loc[df.http_platform_name.isin(['Linux', 'Mac OS', 'Windows', 'iOS', 'Android'])]\ndf = df.loc[df.ip_country == 'United-States']\ndf.head()","94922ecc":"timezone_USAs = df.js_timezone.value_counts()\ntimezones_to_use = timezone_USAs.loc[timezone_USAs > 100].index.values #We only select timezones that appear more than 100 times\n\ndf.http_lang = df.http_lang.str.lower()\ndf.http_lang = df.http_lang.str.replace('-', '_') #We preprocess a little the language data\nlang_USAs = df.http_lang.value_counts()\nlanguages_to_use = lang_USAs.loc[lang_USAs > 100].index.values #We only select languages that appear more than 100 times\n\ndf = df.loc[df.js_timezone.isin(timezones_to_use) & df.http_lang.isin(languages_to_use)] ","d4c29f57":"def sum_per_t(df, t):\n    \"\"\"\n    Add 4 columns in the dataset that contains the count of signups in the time interval (that can be hour, day, ...)\n    And for each granularity (country, country and same language, country and same timezone, country and same platforme)\n    \"\"\"\n    df = df.set_index(df.timestamp.dt.round(t))\n    df['count_' + t] = df.groupby([df.timestamp.dt.round(t)]).COUNT.count()\n    \n    for granularity in ['http_lang', 'js_timezone', 'http_platform_name']:\n        df = df.set_index([df.timestamp.dt.round(t), granularity])\n        df['count_' + t + '_' + granularity] = df.groupby([df.timestamp.dt.round(t), granularity]).COUNT.count()\n        df = df.reset_index(level = 1)\n    return df\n  \ndf2 = sum_per_t(df, 'D') #Add the number of signups in the day\ndf2 = sum_per_t(df2, 'H') #Add the number of signups in the hour\ndf2 = sum_per_t(df2, 'min') #Add the number of signups in the minute\ndf2 = sum_per_t(df2, 's') #Add the number of signups in the second\n\ndf2['hour'] = df2.timestamp.dt.hour #We add a column that contains the hour in the day\ndf2['weekday'] = df2.timestamp.dt.weekday #We add a column that contains the week day","55c87fe3":"df_study = df2[['http_platform_name', 'js_timezone', 'http_lang', 'hour',\n       'weekday', 'count_D', 'count_D_http_lang', 'count_D_js_timezone',\n       'count_D_http_platform_name', 'count_H', 'count_H_http_lang',\n       'count_H_js_timezone', 'count_H_http_platform_name', 'count_min',\n       'count_min_http_lang', 'count_min_js_timezone',\n       'count_min_http_platform_name', 'count_s', 'count_s_http_lang',\n       'count_s_js_timezone', 'count_s_http_platform_name']]\n\ndf_dum = pd.get_dummies(df_study)","d21a4790":"clf = IsolationForest(random_state = 42)\npreds = clf.fit_predict(df_dum)\nprint('Signups detected as abnormal by the isolation forest:')\ndf.loc[preds == -1]","2e9561e2":"pipeline = Pipeline([('normalizer', Normalizer()), ('scaler', MinMaxScaler())]) #Because we use an autoencoder, \n#which is a neural network based model, we have to standardize data.\n\ndf_dum_transformed = pipeline.fit_transform(df_dum)","84eb542f":"input_dim = df_dum_transformed.shape[1]\nlatent_dim = 10\nBATCH_SIZE = 256\nEPOCHS = 30\n\nautoencoder = tf.keras.models.Sequential([\n    \n    tf.keras.layers.Dense(input_dim, activation='relu', input_shape=(input_dim, )), \n        \n    tf.keras.layers.Dense(12, activation='relu'),\n    tf.keras.layers.Dense(12, activation='relu'),\n    \n    tf.keras.layers.Dense(input_dim, activation='relu')\n    \n])\n\nautoencoder.compile(optimizer='adam', \n                    loss='mse',\n                    metrics=['acc'])\n\n# print an overview of our model\nautoencoder.summary();","94893dcd":"EPOCHS = 50\nBATCH_SIZE = 32\n\nhistory = autoencoder.fit(\n    df_dum_transformed, df_dum_transformed,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n);","c415be29":"reconstructions = autoencoder.predict(df_dum_transformed)\nmse = np.mean(np.square(df_dum_transformed - reconstructions), axis = 1)\nsns.distplot(mse); plt.yscale('log'); plt.suptitle('MSE on the test set for the autoencoder'); plt.xlabel('MSE'); plt.ylabel('Distribution (log scale)'); plt.show()","a4a37b48":"print(np.sum(mse > 0.05), 'observations have a reconstruction error greater than 0.05')\ndf_bigger_anomalies = df_dum_transformed[mse > 0.05]","b10515a7":"import shap\n\nexplainer = shap.KernelExplainer(autoencoder, data = df_bigger_anomalies)","96857855":"It seems that countries in Asia are subject to the most variations. The use of the dispersion index by day and country confirms this; Asian countries have the largest index dispersion, as we can see below:","ffe23055":"In order to reduce the dimensionality of the data, which is mainly due to categorical data, we remove from our data the categorical data that is not very present (i.e. with less than 100 observations)","ea9eea53":"# 1. Data analysis and statistical models","9e232ed3":"SHAP is very slow to execute, so we have to limit the execution of the algorithm to a small amount of data. For this, we select the 27 observations which have the greatest reconstruction error:","4c33df66":"An anomaly is not linked to a single signup, but to several close signups, either temporally or spatially. To take this into account, we need to add columns containing the number of signups in the same time window, and by similarity of country, language, timezone, and platform. We add these columns with the code below:","b79c6bfe":"We will start with two statistical methods:\n1. The dispersion index, which allows us to classify countries from the most problematic to the least problematic in terms of variability in the number of signups.\n2. An outlier detection method by quartile range, which allows us to detect anomalies by timezone and by hour.\n\nThen, we use two machine learning methods:\n1. An isolation forest, which detects, for the USA, 1338 anomalies, i.e. 6% of the USA signups.\n2. An autoencoder, which makes it possible to detect a desired number of anomalies, given that it depends on a threshold to be set.\n\nFinally, we present a method, based on the autoencoder, which allows to explain anomalies.","1ee7f5b2":"However, looking at the number of connections per day is not granular enough. Below, we look at the number of registrations for each hour, and for the entire world.\nWhat should be noted is that a **dependence on the hour in the day** appears, although we have here all the time zones mixed together.","300e7e38":"First, with this method, we are not able to detect low signups, df_errors_inf being empty.\n\nThen, we observe that we detect anomalies when, for example, the count is 2 whereas it is most of the time 1, which is however probably not an anomaly.\n\nIt seems that using only statistical methods is not enough to correctly detect anomalies. For this reason, we are now going to use **machine learning techniques**.","ed2d8771":"## 2.3. Autoencoder\n\nTo use this method, we must first build an autoencoder.\n\nWe start by building an autoencoder, which from the reconstruction error, allows us to find the anomalies (the anomalies are the observations for which the reconstruction error is beyond a threshold to be defined).\nThen, we use SHAP to explain which are the features which contribute to increase and decrease the reconstruction error.\n\nThanks to this, we will know which features are unexpected, and explain that the observation is an anomaly.","a9336982":"To confirm this dependence, we look at the number of signups per hour on average, for each continent, then for a few timezones:","a598fb22":"## 1.2 Study of the number of signups per hour","5d52c1b8":"# 2. Focus on the USA\n\nWith the USA being the country where the majority of signups come from, we are now focusing on that country only. Indeed, I am assuming here that the creation of fake accounts or the problems of creating accounts are independent between the countries.\n\nWe no longer restrict ourselves to desktop OS, but keep the most used platforms. We can afford it since we have limited the dimensions of the problem by keeping only one country.\n\nWe will use two machine learning algorithms to detect unexpectedly high or low signups: An Isolation forest and an autoencoder.\nBecause we use here two unsupervised machine learning methods, there is no need to split the data set into a train part and a test part.","07febee8":"## 2.1. Preprocessing","0c40a8a3":"To use machine learning algorithms on this data, we need to use one-hot-encoding on categorical columns:","4d6829e4":"A first overview of the data is made by looking at the number of signups per day and per continent.","ba4cfca9":"#### 1.2.1 Quartiles method","bb576016":"Due to a library problem, probably because of my Tensorflow version, SHAP is not able to build the explainer in the cell above. Otherwise, SHAP must be able to construct this kind of result, allowing to explain, for each output, the reason for which it is obtained:\n![title](https:\/\/raw.githubusercontent.com\/slundberg\/shap\/master\/docs\/artwork\/iris_instance.png)","45df5440":"Seeing this, a first method of detecting \"anomalies\" is to treat as outlier any number of signup that falls outside the interval [Q1 - coef * IQR; Q1 + coef * IQR], where Q1 is the 25% quartile, Q3 the 75% quartile, and IQR is the interquartile range. Q1, Q3 and IQR are defined by hour and timezone or country.","862c266f":"#### 1.1.1 Index dispersion","1f2aee82":"## 1.1. Study of the number of signups per day","04d6958a":"## 2.2. Isolation forest\nAn isolation forest is an unsupervised learning algorithm for anomaly detection that works on the principle of isolating anomalies. The advantage of this method is that it directly gives the anomalies it detects, and labels them -1.","e723f838":"Using the isolation forest, with the default hyperparameters, we find **1338 anomalies**, i.e. **6% of the data**. \n\n\nHowever, although we have detected anomalies, we do not know if they are credible, as we do not have an explanation of the model. \nA method explained in the article \"Explaining Anomalies Detected by Autoencoders Using SHAP\" (https:\/\/arxiv.org\/abs\/1903.02407) by Liat Antwarg, Bracha Shapira and Lior Rokach proposes a method allowing to explain why an anomaly is detected as such.\n","ee63d4a7":"The vast majority of signups are done with Windows.\nWe can easily imagine that the fake accounts are created by bots, which are likely used on desktop OS: Windows, Linux and Mac OS.  \nNote that this assumption works only if the user agents match the machine (it is actually possible to change the user agent in the web browser). For this reason, we will first work with these 3 platforms:","61bf4f4b":"A possible future work would be to:\n\n    - Work using SHAP to understand the reasons for the anomalies.  \n    - Look directly the results of the isolation forest, and, with a business knowledge, see if too many or not enough anomalies are detected. Then modify the hyperparameters of the isolation forest to match the number of desired anomalies.   "}}