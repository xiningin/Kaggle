{"cell_type":{"957fd3a4":"code","3a58ef49":"code","9a8c5a57":"code","f78567ce":"code","d54b6411":"code","fbbbd7b5":"code","0d35c88a":"code","91baff09":"code","a84e26cf":"code","f123267c":"code","c1cc595d":"code","3e79cd8c":"code","447b9c44":"code","2cca8fe9":"code","dd3e6358":"code","dfb1468e":"code","d2873191":"code","d99fa121":"markdown","1d201208":"markdown","94d3247b":"markdown","117a5556":"markdown","c1f98e33":"markdown","19d72a6f":"markdown","9606132c":"markdown","d9795112":"markdown","2ba59afb":"markdown","c716c7a3":"markdown","efb6b8b5":"markdown","dfd2f93b":"markdown","ba14085c":"markdown","67306a1f":"markdown","ad9afca0":"markdown","a188e52d":"markdown","20fa34a8":"markdown","c431840d":"markdown","fea66f27":"markdown","30c4afdf":"markdown"},"source":{"957fd3a4":"import os\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error\n\n# Global settings\n\nwarnings.filterwarnings(\"ignore\") # To ignore warnings\nn_jobs = -1 # This parameter conrols the parallel processing. -1 means using all processors.\nrandom_state = 42 # This parameter controls the randomness of the data. Using some int value to get same results everytime this code is run.","3a58ef49":"X = pd.read_csv('\/kaggle\/input\/modelling-ready-data\/X.csv')\nprint(f'Shape of X= {X.shape}')\nX.head()","9a8c5a57":"y = pd.read_csv('\/kaggle\/input\/modelling-ready-data\/y.csv')\nprint(f'Shape of y= {y.shape}')\ny.head()","f78567ce":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state = random_state)\n\nprint(f'Training set--> X_train shape= {X_train.shape}, y_train shape= {y_train.shape}')\nprint(f'Holdout set--> X_test shape= {X_test.shape}, y_test shape= {y_test.shape}')","d54b6411":"models_scores = [] # To store model scores\n\ndef rmse(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    return mean_squared_error(y_test, y_pred, squared= False) # squared= False > returns Root Mean Square Error                  ","fbbbd7b5":"linear_regression = make_pipeline(LinearRegression())\nscore = rmse(linear_regression)\n\nmodels_scores.append(['LinearRegression', score])\nprint(f'LinearRegression Score= {score}')","0d35c88a":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state= random_state))\n\nscore = rmse(lasso)\nmodels_scores.append(['Lasso', score])\nprint(f'Lasso Score= {score}')","91baff09":"elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio= .9, random_state= random_state))\n\nscore = rmse(elastic_net)\nmodels_scores.append(['ElasticNet', score])\nprint(f'ElasticNet Score= {score}')","a84e26cf":"kernel_ridge= KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmse(kernel_ridge)\nmodels_scores.append(['KernelRidge', score])\nprint(f'KernelRidge Score= {score}')","f123267c":"# Ranking the scores of each model\npd.DataFrame(models_scores).sort_values(by=[1], ascending=True)","c1cc595d":"def bagging_predictions(estimator):\n    \"\"\"\n    I\/P\n    estimator: The base estimator from which the ensemble is grown.\n    O\/P\n    br_y_pred: Predictions on test data for the base estimator.\n    \n    \"\"\"\n    regr = BaggingRegressor(base_estimator=estimator,\n                            n_estimators=10,\n                            max_samples=1.0,\n                            bootstrap=True, # Samples are drawn with replacement\n                            n_jobs= n_jobs,\n                            random_state=random_state).fit(X_train, y_train)\n\n    br_y_pred = regr.predict(X_test)\n\n    rmse_val = mean_squared_error(y_test, br_y_pred, squared= False) # squared= False > returns Root Mean Square Error   \n\n    print(f'RMSE for base estimator {regr.base_estimator_} = {rmse_val}\\n')\n    return br_y_pred\n\n\npredictions = np.column_stack((bagging_predictions(linear_regression),\n                              bagging_predictions(lasso),\n                              bagging_predictions(elastic_net),\n                              bagging_predictions(kernel_ridge)))\nprint(f\"Bagged predictions shape: {predictions.shape}\")\n       \ny_pred = np.mean(predictions, axis=1)\nprint(\"Aggregated predictions (y_pred) shape\", y_pred.shape)\n\nrmse_val = mean_squared_error(y_test, y_pred, squared= False) # squared= False > returns Root Mean Square Error   \nmodels_scores.append(['Bagging', rmse_val])\n\nprint(f'\\nBagging RMSE= {rmse_val}')","3e79cd8c":"# Ranking the scores of each model\npd.DataFrame(models_scores).sort_values(by=[1], ascending=True)","447b9c44":"gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state = random_state)\n\nscore = rmse(gradient_boosting_regressor)\nmodels_scores.append(['GradientBoostingRegressor', score])\nprint(f'GradientBoostingRegressor Score= {score}')","2cca8fe9":"xgb_regressor= xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,verbosity=0, nthread = -1, random_state = random_state)\nscore = rmse(xgb_regressor)\nmodels_scores.append(['XGBRegressor', score])\nprint(f'XGBRegressor Score= {score}')","dd3e6358":"lgbm_regressor= lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,random_state = random_state)\n\nscore = rmse(lgbm_regressor)\nmodels_scores.append(['LGBMRegressor', score])\nprint(f'LGBMRegressor Score= {score}')","dfb1468e":"estimators = [ ('elastic_net', elastic_net), ('kernel_ridge', kernel_ridge),('xgb_regressor', xgb_regressor) ]\n\nstack = StackingRegressor(estimators=estimators, final_estimator= lasso, cv= 5, n_jobs= n_jobs, passthrough = True)\n\nstack.fit(X_train, y_train)\n\npred = stack.predict(X_test)\n\nrmse_val = mean_squared_error(y_test, pred, squared= False) # squared= False > returns Root Mean Square Error    \nmodels_scores.append(['Stacking', rmse_val])\nprint(f'rmse= {rmse_val}')","d2873191":"# Ranking the scores of each model\npd.DataFrame(models_scores).sort_values(by=[1], ascending=True)","d99fa121":"### KernelRidge Regression <a id= \"13\"><\/a>\n* Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients.","1d201208":"### Linear Regression <a id= \"10\"><\/a>\n\n* Lest test using Ordinary least squares Linear Regression.\n* LinearRegression fits a linear model with coefficients w = (w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.","94d3247b":"# Bagging <a id= \"2\"><\/a>\n\n![EnsembleI_Learning_Bagging](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Ensemble_Learning_Bagging.png)\n\n* In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions.\n* Bagging is a short form of **B*ootstrap *Agg*regat*ing*. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms.\n* Since multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. \n* Bagging is a special case of the model averaging approach, in case of regression problem we take mean of the output and in case of classification we take the majority vote. \n* Bagging is more helpfull if we have over fitting (high variance) base models.\n* We can also build independent estimators of same type on each subset. These independent estimators also enable us to parallelly process and increase the speed.\n* Most popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest'\n\n**Bootstrapping**\n* It is a resampling technique, where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.\n* So this technique will enable us to produce as many subsample as we required from the original training data.\n* So the defination is simple to understand, but \"replacement\" word may be confusing sometimes. Here 'replacement' word signifies that the same obervation may repeat more than once in a given sample, and hence this technique is also known as **sampleing with replacement**\n\n![Bootstrap_Sampling_ML](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Bootstrap_Sampling_ML.png)\n\n* As you can see in above image we have training data with observations from X1 to X10. In first bootstrap training sample X6, X10 and X2 are repeated where as in second training sample X3, X4, X7 and X9 are repeated.\n* Bootstrap sampling helps us to generate random sample from given training data for each model in order to genralise the final estimation.\n\nSo in case of Bagging we create multiple number of bootstrap samples from given data to train our base models. Each sample will contain training and test data sets which are different from each other and remember that training sample may contain duplicate observations.","117a5556":"### LGBMRegressor <a id= \"19\"><\/a>\nLight GBM is a gradient boosting framework that uses tree based learning algorithm.","c1f98e33":"As you can see from above list that 'Stacking' resulted in the best possible score. We can even try multiple permutation of each model till we get the best possible results. Apart from Bagging, Boosting and Stacking ensembling methods we can also try model blending approach where we can assign weightage to each model and combining the results till we get the best possible score.","19d72a6f":"## Modeling <a id= \"9\"><\/a>\n* I am also skipping the hyperparameter tuning and used already tuned hyperparameters here\n* We will use multiple models individually as well as in ensemble mode to test the final predictions.\n* We are going to use **Root Mean Square Error(RMSE)** metric to compare the scores. Since this metrics is not available out of the box we will create a function for it.\n\nNote: RMSE metric is used to express the loss in the same unit of measurement as label value, in this case house price in dollars. For example if RMSE for house price is 2, then we can loosely interpret it as 'on average incorrect predictions are wrong by around 2 house prices'","9606132c":"#### XGBRegressor <a id= \"18\"><\/a>\n* XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n* It implements machine learning algorithms under the Gradient Boosting framework.\n* XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.","d9795112":"## Load Data <a id= \"7\"><\/a>\nSince the objective of this article is to test the different ensemble techniques, I have excluded the data preprocessing and EDA steps. I am going to use model ready dataset, so that we can straight away start modeling and ensembling. **X.csv** contains all the training data and **y.csv** contains the label values. In this case \"SalePrice\" is the label\/target variable which represent the property's sale price in dollars that we are trying to predict.\n\nBelow is the list of features:\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","2ba59afb":"### ElasticNet Regression <a id= \"12\"><\/a>\n* Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n* A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.\n* This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","c716c7a3":"![Ensemble_Learning](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Ensemble_Learning.png)\n\n# Index\n* [Introduction](#1)\n* [Bagging](#2)\n* [Boosting](#3)\n* [Stacking](#4)\n* [Python Example](#5)\n  - [Import Libraries](#6)\n  - [Load Data](#7)\n  - [Train and Test Data](#8)\n  - [Modeling](#9)\n    - [Linear Regression](#10)\n\t- [Lasso Regression](#11)\n\t- [ElasticNet Regression](#12)\n\t- [KernelRidge Regression](#13)\n  - [Ensemble Modeling](#14)\n    - [Bagging](#15)\n\t- [Boosting](#16)\n\t  - [GradientBoostingRegressor](#17)\n\t  - [XGBRegressor](#18)\n\t  - [LGBMRegressor](#19)\n\t- [Stacking](#20)\n    \n# Introduction <a id= \"1\"><\/a>\n\nWhenever we make any important decision we first discuss it with friends, family or an expert. Nowadays we check the reviews on social media or check a YouTube video. Considering other people's opinion just make final decision more informed and make sure to avoid any kind of surprises as we are combining multiple opinions about the same thing together. \n\nEnsemble modeling in machine learning operates on the same principle, where we combine the predictions from multiple models to generate the final model which provide better overall performance. Ensemble modeling helps to generalize the learning based on training data, so that it will be able to do predictions accurately on unknown data. \n\nModeling is one of the most important step in machine learning pipeline. The main motivation behind ensemble learning is to correctly combine weak models to get a more accurate and robust model with bias-variance trade off. For example Random Forest algorithm is ensemble of Decision Tree and since it combine multiple decision  tree models it always perform better than single decision tree model.\n\nDepending on how we combine the base models, ensemble learning can be classified in three different types Bagging, Boosting and  Stacking.\n\n* **Bagging**: The working principle is to build several base models independently and then to average them for final predictions. \n* **Boosting**: Boosting models are built sequentially and tries to reduce the bias on final predictions. \n* **Stacking**: The predictions of each individual model are stacked together and used as input to a final estimator to compute the prediction. \n \nEnsemble learning approach makes the model more robust and helps to achieve the better performance.","efb6b8b5":"# Python Example <a id= \"5\"><\/a>\n\nWe are going to use [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition data. Our objective is to predict the final price of each house based on the 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. We will try all the ensemble learning approaches and compare their results.\n\n![House_Prices_Advanced_Regression_Techniques](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/House_Prices_Advanced_Regression_Techniques.png)\n\n## Import Libraries <a id= \"6\"><\/a>","dfd2f93b":"# Stacking <a id= \"4\"><\/a>\nModel stacking is a method for combining models to reduce their biases. The predictions of each individual model are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.\n\nNote that in case of stacking we use heterogeneous weak learners (different learning algorithms) but in case bagging and boosting we mainly use homogeneous weak learners. \n\n![Ensemble_Learning_Stacking](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Ensemble_Learning_Stacking.png)","ba14085c":"# When to use Ensemble Learning?\nSince Ensemble learning results in better accuracy, high consistency and also helps to avoid bias variance tradeoff should'nt we use it everywhere? The short answer is it depends on the problem in hand. If our model with available training data is not performing well and showing the signs of overfitting\/unterfitting and additinal compute power is not an issue then going for Ensemble Learning is best option. However one shouldnt skip the first steps of improving the input data and trying different hyperparmeters before going for ensemple approach. ","67306a1f":"As you can see from above results, because of the high score of \"KernelRidge\" estimator total bagging RMSE is less than that of \"Lasso\" and \"ElasticNet\". \n\n### Boosting <a id= \"16\"><\/a>\n\nWe are going to use GradientBoostingRegressor, XGBRegressor, LGBMRegressor algorithms.","ad9afca0":"#### GradientBoostingRegressor <a id= \"17\"><\/a>\n\n* Gradient Boosting for regression.\n* GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.","a188e52d":"##  Train and Test Data <a id= \"8\"><\/a>\nWe will use train_test_split() method to create training and test sets.","20fa34a8":"## Ensemble Modeling <a id= \"14\"><\/a>\n\n### Bagging <a id= \"15\"><\/a>\n* We are going to use sklearns \"**BaggingRegressor**\" to fit the base regressors (LinearRegression, Lasso, ElasticNet, KernelRidge)\n* A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n* In particular, **max_samples** and **max_features** control the size of the subsets (in terms of samples and features), while **bootstrap** and **bootstrap_features** control whether samples and features are drawn with or without replacement.\n* We are using 10 base estimators in ensemble. \n* Method bagging_predictions() calculate the score of each base estimator against the test data and also returns the test prediction values.\n* Using **column_stack** we will store the predictions for each base estimator in a separate column and then take the average of all the predictions for final RMSE calculations.","c431840d":"### Lasso Regression <a id= \"11\"><\/a>\n* The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. \n* This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","fea66f27":"### Stacking <a id= \"20\"><\/a>\n\n* We can use sklearns **StackingClassifier** and **StackingRegressor** to for classification and regression problem respectively.\n* Since \"lasso\" is our best performing model we will use it as our meta learner and rest models as base estimators.","30c4afdf":"# Boosting <a id= \"3\"><\/a>\n* In case of boosting, machine learning models are used one after the other and the predictions made by first layer models are used as input to next layer models. The last layer of models will use the predictions from all previous layers to get the final predictions. \n* So boosting enables each subsequent model to boost the performance of the previous one by overcomming or reducing the error of the previous model.\n* Unlike bagging, in case of boosting the base learners are trained in sequence on a weighted version of the data. Boosting is more helpful if we have biased base models.\n* Boosting can be used to solve regression and classification problems.\n\n![Ensemble_Learning_Boosting](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Ensemble_Learning_Boosting.png)\n\nDifferent types of Boosting algorithms\n* Gradient Boosting Machine (GBM)\n* Extreme Gradient Boosting Machine (XGBM)\n* LightGBM\n* CatBoost"}}