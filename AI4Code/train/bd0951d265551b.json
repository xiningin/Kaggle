{"cell_type":{"14c93082":"code","2fa6d759":"code","79f279d6":"code","e395b3f4":"code","e36542cd":"code","39f8e43b":"code","bdc91a87":"code","e3d5ee5b":"code","1f70f42f":"code","85b2b264":"code","e161fbd7":"code","221cc332":"code","8a8296f1":"code","c4a8297e":"code","54376054":"code","f3109634":"code","79b80342":"code","c1fb7986":"code","1f308338":"code","1be6f893":"code","ae2ab828":"code","a2a0d9e0":"code","2cadb53e":"code","ed530256":"code","ac465029":"code","c3e29173":"code","cfe4dfc1":"code","eb40f0d9":"code","875280ce":"code","7d996e7f":"code","826970d1":"code","e1a9e46a":"code","cf6d8051":"code","d9155fea":"code","49f5309b":"code","692b4198":"code","f84cd649":"code","3e7f4e36":"code","4895cd42":"markdown","b5892fb6":"markdown","c662aab6":"markdown","3977e1cb":"markdown","e04d18ac":"markdown","1d8bece3":"markdown","44a337cc":"markdown","262aa41e":"markdown","bff06dfe":"markdown","df698598":"markdown","a6628a15":"markdown","8638932b":"markdown","d9d6713b":"markdown","ea7aa0a3":"markdown","a88a032f":"markdown","ae70551f":"markdown","afe67df8":"markdown","dd47e968":"markdown","5fc11e11":"markdown","ec3ac9ed":"markdown","709d5e1d":"markdown","d1bbab5a":"markdown","ecebbc31":"markdown","31e6e261":"markdown","f9574fd8":"markdown","e6ba58be":"markdown","f484cf3c":"markdown","3a37aeb2":"markdown","cf24e46d":"markdown","f1e2c11c":"markdown"},"source":{"14c93082":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,plot_confusion_matrix\nfrom sklearn import tree\n\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.model_selection import KFold\nimport time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score","2fa6d759":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\n\nX = data.drop('target',axis=1)\ny = data['target']\n\n#Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train.head(5)","79f279d6":"print(\"Samples:\",len(y))\nprint(\"No disease:\",list(y).count(0))\nprint(\"Disease:\",list(y).count(1))","e395b3f4":"def r2(a,b):\n    this_correlation = np.corrcoef(a, b)[0,1]\n    this_r2 = this_correlation**2\n    return this_r2","e36542cd":"def plot_confusion(title, model, X_train, y_train, X_test, y_test):\n    svm_confusion_matrix = plot_confusion_matrix(model, X_train, y_train,\n                      display_labels=['No Heart Disease','Heart Disease'],\n                      cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Training Set)\")\n    plt.show()\n\n    svm_confusion_matrix = plot_confusion_matrix(model, X_test, y_test,\n                          display_labels=['No Heart Disease','Heart Disease'],\n                          cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Testing Set)\")\n    plt.show()","39f8e43b":"#https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Predictive Accuracy\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True,verbose=2)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    print(\"CV Scores:\",test_scores_mean)\n\n    return plt","bdc91a87":"decision_tree = tree.DecisionTreeClassifier(max_depth=6, \n                                  criterion='gini',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\n\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Initial Decision Tree Learning Rate\", \n                    X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","e3d5ee5b":"dt_params = {\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(3,15),\n    \"min_samples_leaf\":range(1,15),\n    \"min_samples_split\":range(1,12),\n    \n}\ndecision_tree = tree.DecisionTreeClassifier()\n\n\ngrid = GridSearchCV(decision_tree,\n                    param_grid = dt_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 11, 'min_samples_split': 2}","1f70f42f":"start_time = time.time();\ndecision_tree = tree.DecisionTreeClassifier(max_depth=4, \n                                  criterion='entropy',\n                                  min_samples_leaf=11,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 3},\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\nprint(\"Training time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Decision Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","85b2b264":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Final Decision Tree Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","e161fbd7":"plot_confusion(\"Decision Tree\", decision_tree, X_train, y_train, X_test, y_test)","221cc332":"nn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(13,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=500,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Initial Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","8a8296f1":"nn_params = {\n    \"hidden_layer_sizes\":[(13,),(),(13,6),(6,)],\n    \"activation\":['identity', 'logistic', 'tanh', 'relu'],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"verbose\":[True],\n    \"max_iter\":range(20,800,50)\n}\n\nnn = MLPClassifier()\ngrid = GridSearchCV(nn,\n                    param_grid = nn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'activation': 'identity', 'hidden_layer_sizes': (6,), 'max_iter': 120, 'solver': 'lbfgs', 'verbose': True}","c4a8297e":"start_time = time.time();\nnn = MLPClassifier(activation='identity',\n                   hidden_layer_sizes=(6,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=54,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Train r2 score:',r2(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Neural Network Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","54376054":"nn_scores_train = [];\nnn_scores = [];\nfor i in range(20):\n    nn = MLPClassifier(activation='identity',\n                   hidden_layer_sizes=(),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=i*6+1,\n                   random_state=1,\n                   early_stopping=True)\n    \n    nn.fit(X_train,y_train);\n    y_test_pred = nn.predict(X_test)\n    nn_scores_train.append(accuracy_score(y_train, y_train_pred))\n    nn_scores.append(accuracy_score(y_test, y_test_pred))\n    print(\"I\",i)\nprint(\"Training Scores:\",nn_scores_train)\nprint(\"Testing Scores:\",nn_scores)","f3109634":"plt.plot(range(1,201,10),nn_scores_train,marker='o',label=\"Training Set\")\nplt.plot(range(1,201,10),nn_scores,marker='o',label=\"Testing Set\")\nplt.xlabel('Network Iterations')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Neural Network Accuracy by Iteration Number\")\nplt.legend()\nplt.show()","79b80342":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Final Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","c1fb7986":"plot_confusion(\"Neural Network\", nn, X_train, y_train, X_test, y_test)","1f308338":"booster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=3, \n                                  criterion='gini',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 1},\n                                  random_state=0),\n    n_estimators=100,\n    learning_rate=0.1,\n    random_state=0) \nbooster.fit(X_train, y_train)\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"Initial ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 1.05),\n                    cv=cv, n_jobs=4)","1be6f893":"start_time = time.time();\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=5, \n                                  criterion='entropy',\n                                  min_samples_leaf=11,\n                                  min_samples_split=2,\n                                  class_weight={0: 1, 1: 3},\n                                  random_state=0),\n    n_estimators=500,\n    learning_rate=0.75,\n    random_state=0) \nbooster.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Boosted Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Boosted Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","ae2ab828":"start_time = time.time();\ntrain_accuracies = [];\ntest_accuracies = [];\nfor i in range(1,1001,50):\n    booster = AdaBoostClassifier(\n        tree.DecisionTreeClassifier(max_depth=5, \n                                      criterion='entropy',\n                                      min_samples_leaf=11,\n                                      min_samples_split=2,\n                                      class_weight={0: 1, 1: 3},\n                                      random_state=0),\n        n_estimators=i,\n        learning_rate=0.75,\n        random_state=0) \n    booster.fit(X_train, y_train)\n    y_test_pred = booster.predict(X_test)\n    y_train_pred = booster.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n    print(i,end=\" \")\nprint(\"Training Scores:\",train_accuracies)\nprint(\"Testing Scores:\",test_accuracies)","a2a0d9e0":"plt.plot(range(1,1001,50),train_accuracies,marker='o',label=\"Training Set\")\nplt.plot(range(1,1001,50),test_accuracies,marker='o',label=\"Testing Set\")\nplt.xlabel('Estimators')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Adaptive Boosting Accuracy by Estimator Count\")\nplt.legend()\nplt.show()","2cadb53e":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 1.05),\n                    cv=cv, n_jobs=4)","ed530256":"plot_confusion(\"Boosted Decision Tree\", booster, X_train, y_train, X_test, y_test)","ac465029":"svm_params1 = {\n    \"kernel\":['poly'],\n    \"degree\":range(1,9),\n    \"C\":[4000,6000,8000, 12000,16000]\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\nprint(grid.best_score_)\n#{'C': 8000, 'degree': 2, 'kernel': 'poly'}","c3e29173":"svm_params1 = {\n    \"kernel\":['rbf'],\n    \"gamma\":['scale','auto']\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'gamma': 'scale', 'kernel': 'rbf'}","cfe4dfc1":"start_time = time.time();\nsvm = SVC(kernel='rbf',\n          gamma='scale',\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"\\nUsing RBF Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","eb40f0d9":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"RBF Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.4, 0.9),\n                    cv=cv, n_jobs=4)","875280ce":"plot_confusion(\"SVM, Using RBF Kernel\", svm, X_train, y_train, X_test, y_test)","7d996e7f":"svm = SVC(kernel='poly',\n          C=1,\n          degree=3,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Initial Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.8),\n                    cv=cv, n_jobs=4)","826970d1":"start_time = time.time();\nsvm = SVC(kernel='poly',\n          C=8000,\n          degree=2,\n          class_weight={0: 1, 1: 5},\n          random_state=0)\nsvm.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","e1a9e46a":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","cf6d8051":"plot_confusion(\"SVM, Using Poly Kernel\", svm, X_train, y_train, X_test, y_test)","d9155fea":"knn_params = {\n    \"n_neighbors\":range(1,10),\n    \"leaf_size\":range(1,15),\n    \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn,\n                    param_grid = knn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 3}","49f5309b":"knn = KNeighborsClassifier(n_neighbors=10,\n                           algorithm='auto',\n                           leaf_size=10)\nknn.fit(X_train, y_train)\n\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint('KNN Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"Initial KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.85),\n                    cv=cv, n_jobs=4)","692b4198":"start_time = time.time();\nknn = KNeighborsClassifier(n_neighbors=3,\n                           algorithm='auto',\n                           leaf_size=1)\nknn.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\n\nstart_time = time.time();\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"K = 4\")\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"KNN Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","f84cd649":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.95),\n                    cv=cv, n_jobs=4)","3e7f4e36":"plot_confusion(\"K Nearest Neighbors\", knn, X_train, y_train, X_test, y_test)","4895cd42":"# Boosting","b5892fb6":"Run Adaptive Boosting model","c662aab6":"function to draw learning curves","3977e1cb":"Show the Decision Tree confusion matrix","e04d18ac":"Show the Decision Tree learning rate","1d8bece3":"Calculate model accuracy by iteration","44a337cc":"Run the Decision tree algorithm.","262aa41e":"# Decision Tree","bff06dfe":"# Neural Network","df698598":"Use GridSearch to tune Poly kernel SVM hyperparameters.","a6628a15":"Gridsearch decision tree hyperparameters","8638932b":"Create estimated model","d9d6713b":"import libraries","ea7aa0a3":"Estimate poly kernel:","a88a032f":"Run the Neural Network","ae70551f":"# KNN Model","afe67df8":"# Heart Disease Identification","dd47e968":"# SVM","5fc11e11":"Draw chart for Ada Booster learning rate","ec3ac9ed":"Read dataset and split into training and testing sets.","709d5e1d":"Draw Boosting Accuracy by estimator count","d1bbab5a":"Tune the Neural Network hyperparameters with GridSearch","ecebbc31":"Calculate the Neural Network accuracy by iterations.","31e6e261":"Create and display a network with estimated parameters","f9574fd8":"Plot Neural Network learning rate","e6ba58be":"Estimate KNN model:","f484cf3c":"Plot Neural Network confusion matrix","3a37aeb2":"Draw decision tree for Adaptive Boosting model","cf24e46d":"function to plot confusion matrices","f1e2c11c":"First, Create and display the untuned model:"}}