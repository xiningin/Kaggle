{"cell_type":{"4815dd71":"code","d77576e8":"markdown","897f73b1":"markdown"},"source":{"4815dd71":"import numpy as np\nimport pandas as pd\nimport copy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport optuna\n\ndef convert(df_input):\n    df = copy.deepcopy(df_input)\n    df.loc[:, 'bin_3':'bin_4'] = df.loc[:, 'bin_3':'bin_4'].applymap(lambda x: 'FTNY'.find(x) % 2)\n    df = pd.get_dummies(df, columns=['nom_{}'.format(i) for i in range(5)])\n    df.loc[:, 'nom_5':'nom_9'] = df.loc[:, 'nom_5':'nom_9'].applymap(lambda x: int(x, 16))\n    df['ord_1'] = df['ord_1'].map(lambda x: ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'].index(x))\n    df['ord_2'] = df['ord_2'].map(lambda x: ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'].index(x)) \n    df['ord_3'] = df['ord_3'].map(lambda x: (ord(x) - ord('a')))\n    df['ord_4'] = df['ord_4'].map(lambda x: (ord(x) - ord('A')))\n    df['ord_6'] = df['ord_5'].map(lambda x: (ord(x[1]) - ord('A')))\n    df['ord_5'] = df['ord_5'].map(lambda x: (ord(x[0]) - ord('A')))\n    return df\n\ndf_train, df_test = (convert(pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')), convert(pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')))\nX_train, X_valid, T_train, T_valid = train_test_split(df_train.drop(['id', 'target'], axis=1), df_train['target'], random_state=0)\nX_test = df_test.drop('id', axis=1)\nparams_fixed = {'objective':'binary', 'metric':'binary_logloss', 'boosting_type':'gbdt', 'num_iterations':10000, 'early_stopping_round':10,\\\n                'max_depth':7, 'max_bin':255, 'reg_alpha':0., 'min_split_gain':0., 'learning_rate':0.01, 'random_state':0}\nmodels = []\n\ndef objective(trial):\n    global params_fixed, models\n    params_tuning = {'num_leaves' : trial.suggest_int('num_leaves', 2, 100), \\\n                     'subsample' : trial.suggest_uniform('subsample', 0.5, 1.0), \\\n                     'subsample_freq' : trial.suggest_int('subsample_freq', 1, 20), \\\n                     'colsample_bytree' : trial.suggest_uniform('colsample_bytree', 0.01, 1.0), \\\n                     'min_child_samples' : trial.suggest_int('min_child_samples', 1, 50), \\\n                     'min_child_weight' : trial.suggest_loguniform('min_child_weight', 1e-3, 1e+1), \\\n                     'reg_lambda' : trial.suggest_loguniform('reg_lambda', 1e-2, 1e+3)}\n    model = lgb.train({**params_fixed, **params_tuning}, lgb.Dataset(X_train, T_train), valid_sets=lgb.Dataset(X_valid, T_valid), verbose_eval=100)\n    models.append(model)\n    score = roc_auc_score(T_valid, model.predict(X_valid))\n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\npd.concat([df_test['id'], pd.Series(models[study.best_trial.number].predict(X_test), name='target')], axis=1).to_csv('submission.csv', index=False)","d77576e8":"# Code","897f73b1":"# Preface\nThis notebook is a continuation of [25-line-model of LightGBM](https:\/\/www.kaggle.com\/yutanakamura\/25-line-model-of-lightgbm).\n\n- With the spirit of ZEN ......\n- The aim is to make LightGBM & Optuna beginners (like me) familiar with them, and to undergo a submission quickly."}}