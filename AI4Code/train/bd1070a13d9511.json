{"cell_type":{"fa5b165b":"code","bed25a31":"code","1894ea94":"code","b9810a34":"code","aa3c4972":"code","8460ad45":"code","2455f272":"code","2fee5653":"code","d7441af4":"code","42f3bc44":"code","982019ab":"code","152ecc5e":"code","a5f85115":"code","5523eacc":"code","d51c8a85":"code","addf06a2":"code","3aa2a269":"code","2921898c":"code","c6e49d11":"code","22cbc7cd":"code","5e54d62c":"code","2263e950":"code","01bef076":"code","27277bfa":"code","64bd7919":"code","e174e9b2":"code","13d34c43":"code","71c0be65":"code","2c6db6ce":"code","f006b59e":"code","07893ed9":"code","3618568a":"code","785ec7be":"code","aa490684":"code","d0640e38":"code","53b80855":"code","73b18837":"code","bcffa0a5":"code","a35fd1df":"code","95346cd4":"code","ba30ba5a":"code","deede458":"code","973e8549":"code","9f15cf8d":"code","3254dc3e":"code","a439ddc3":"code","4ca69ffd":"code","85737f41":"code","d93356b6":"code","859e6f00":"code","27c484b2":"code","398b0459":"code","3d17e5f6":"code","001f8470":"code","a9fee831":"code","4c083009":"code","c6f758cc":"code","194d9e94":"code","d3a7ad29":"code","bb82ee5e":"code","d18227d4":"code","525bcc1e":"code","f0837084":"code","ee266c81":"code","59c3d5ef":"code","9ee35a27":"code","93f84d06":"code","bdf7f493":"code","9eb76137":"markdown","f3cbe6ee":"markdown","975057f1":"markdown","4c1f1eb4":"markdown","76732610":"markdown","c3d35d85":"markdown","91ff9bfe":"markdown"},"source":{"fa5b165b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bed25a31":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1894ea94":"data = pd.read_csv('\/kaggle\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv')\ndata.head()","b9810a34":"data.shape","aa3c4972":"data.info()","8460ad45":"data.describe()","2455f272":"plt.figure(figsize=(8,7))\nsns.heatmap(data.corr(),annot=True)\nplt.show()","2fee5653":"data.info()","d7441af4":"data.isnull().sum()","42f3bc44":"# Checking the pairplot.\nsns.pairplot(data)\nplt.show()","982019ab":"# Age columns seems as it is in the form of categories","152ecc5e":"data['age'].nunique()\n# We'll proceed by using it as s numeric column only.","a5f85115":"# checking the distribution of the data\nfor i in data.iloc[:,:-1].columns:\n    sns.kdeplot(data[i])\n    plt.show()","5523eacc":"data.columns","d51c8a85":"for i in data.iloc[:,:-1].columns:\n    print(i,end=' :')\n    print(data[i].nunique())","addf06a2":"# Checking for outliers in the data\nfor i in data.iloc[:,:-1].columns:\n    sns.boxplot(data[i])\n    plt.show()","3aa2a269":"data1 = data.copy(deep=True)","2921898c":"data1.isnull().sum()","c6e49d11":"# Checking skewness of data now\ndata1.skew()\n\n## Still skewness can be corrected using the Power Transformer","22cbc7cd":"from sklearn.preprocessing import PowerTransformer","5e54d62c":"pt = PowerTransformer()\nX = pt.fit_transform(data1.iloc[:,:-1])\nX = pd.DataFrame(X,columns=data1.iloc[:,:-1].columns)\nX.head()","2263e950":"X.skew()","01bef076":"Y = data[['csMPa']]","27277bfa":"# Final Data prepared for models is\n# Input Variables X and Target column Y.","64bd7919":"import statsmodels.api as sm","e174e9b2":"inp = X\nout = Y\nc = sm.add_constant(inp)\nmodel = sm.OLS(out,c).fit()\nmodel.summary()","13d34c43":"import scipy.stats as stats","71c0be65":"stats.probplot(model.resid,plot=plt)","2c6db6ce":"# Checking Normality of the residue\nsns.distplot(model.resid)\nplt.show()\n\n# Data can be seen somewhat normal.","f006b59e":"model.resid.skew()   # Skewness of residue is also under control.","07893ed9":"# Checking Multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","3618568a":"vif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(inp.values,i) for i in range(inp.shape[1])]\nvif['Features'] = inp.columns\nvif.sort_values('VIF',ascending=False)","785ec7be":"# Multicollinearity is also under check.","aa490684":"from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error","d0640e38":"# Splitting the dataset\nxtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.25,random_state=0)","53b80855":"LR = LinearRegression()\nLR.fit(xtrain,ytrain)","73b18837":"LR.score(xtrain,ytrain)","bcffa0a5":"LR.score(xtest,ytest)","a35fd1df":"ytrain_pred = LR.predict(xtrain)\nytest_pred = LR.predict(xtest)\n\nr2_train = r2_score(ytrain,ytrain_pred)\nr2_test = r2_score(ytest,ytest_pred)\n\nrmse_train = np.sqrt(mean_squared_error(ytrain,ytrain_pred))\nrmse_test = np.sqrt(mean_squared_error(ytest,ytest_pred))\n\nprint('R2 Train: ',r2_train,end='    ')\nprint('R2 Test: ', r2_test)\nprint('RMSE Train: ',rmse_train,end='    ')\nprint('RMSE Test: ',rmse_test)","95346cd4":"kf = KFold(shuffle=True,n_splits=5,random_state=0)\nscore1 = cross_val_score(LR,xtrain,ytrain,cv=kf,scoring='r2')\nLR_be = np.mean(1-score1)\nLR_ve = np.std(score1,ddof=1)\nprint('Bias Error: ',LR_be)\nprint('Variance Error: ',LR_ve)","ba30ba5a":"Tab = pd.DataFrame()","deede458":"Tab['LR'] = [LR_be,LR_ve]","973e8549":"Tab","9f15cf8d":"from sklearn.neighbors import KNeighborsRegressor","3254dc3e":"KNR = KNeighborsRegressor()\nKNR.fit(xtrain,ytrain)","a439ddc3":"KNR.score(xtrain,ytrain)","4ca69ffd":"KNR.score(xtest,ytest)","85737f41":"ytrain_pred = KNR.predict(xtrain)\nytest_pred = KNR.predict(xtest)\n\nr2_train = r2_score(ytrain,ytrain_pred)\nr2_test = r2_score(ytest,ytest_pred)\n\nrmse_train = np.sqrt(mean_squared_error(ytrain,ytrain_pred))\nrmse_test = np.sqrt(mean_squared_error(ytest,ytest_pred))\n\nprint('R2 Train: ',r2_train,end='    ')\nprint('R2 Test: ', r2_test)\nprint('RMSE Train: ',rmse_train,end='    ')\nprint('RMSE Test: ',rmse_test)","d93356b6":"# The model is highly Over fitting","859e6f00":"kf = KFold(shuffle=True,n_splits=5,random_state=0)\nscore2 = cross_val_score(KNR,xtrain,ytrain,cv=kf,scoring='r2')\nKNR_be = np.mean(1-score2)\nKNR_ve = np.std(score2,ddof=1)\nprint('Bias Error: ',KNR_be)\nprint('Variance Error: ',KNR_ve)","27c484b2":"from sklearn.ensemble import RandomForestRegressor","398b0459":"RF = RandomForestRegressor()\nRF.fit(xtrain,ytrain)","3d17e5f6":"RF.score(xtrain,ytrain)","001f8470":"RF.score(xtest,ytest)","a9fee831":"ytrain_pred = RF.predict(xtrain)\nytest_pred = RF.predict(xtest)\n\nr2_train = r2_score(ytrain,ytrain_pred)\nr2_test = r2_score(ytest,ytest_pred)\n\nrmse_train = np.sqrt(mean_squared_error(ytrain,ytrain_pred))\nrmse_test = np.sqrt(mean_squared_error(ytest,ytest_pred))\n\nprint('R2 Train: ',r2_train,end='    ')\nprint('R2 Test: ', r2_test)\nprint('RMSE Train: ',rmse_train,end='    ')\nprint('RMSE Test: ',rmse_test)","4c083009":"kf = KFold(shuffle=True,n_splits=5,random_state=0)\nscore3 = cross_val_score(RF,xtrain,ytrain,cv=kf,scoring='r2')\nRF_be = np.mean(1-score3)\nRF_ve = np.std(score3,ddof=1)\nprint('Bias Error: ',RF_be)\nprint('Variance Error: ',RF_ve)","c6f758cc":"from xgboost import XGBRegressor","194d9e94":"XGB = XGBRegressor(random_state=0)\nXGB.fit(xtrain,ytrain)","d3a7ad29":"XGB.score(xtrain,ytrain)","bb82ee5e":"XGB.score(xtest,ytest)","d18227d4":"ytrain_pred = XGB.predict(xtrain)\nytest_pred = XGB.predict(xtest)\n\nr2_train = r2_score(ytrain,ytrain_pred)\nr2_test = r2_score(ytest,ytest_pred)\n\nrmse_train = np.sqrt(mean_squared_error(ytrain,ytrain_pred))\nrmse_test = np.sqrt(mean_squared_error(ytest,ytest_pred))\n\nprint('R2 Train: ',r2_train,end='    ')\nprint('R2 Test: ', r2_test)\nprint('RMSE Train: ',rmse_train,end='    ')\nprint('RMSE Test: ',rmse_test)","525bcc1e":"# Applyting Regularization","f0837084":"from sklearn.linear_model import Ridge, Lasso","ee266c81":"# Searching for the best parameter.\nrid = Ridge()\nparam = {'alpha':[0.0001,0.00,0.01,0.1,0.5,1,2,5,10]}\nGS = GridSearchCV(rid,param,cv=5, scoring='neg_mean_squared_error')\nmodel1 = GS.fit(xtrain,ytrain)","59c3d5ef":"model1.best_params_","9ee35a27":"rid = Ridge(alpha=5)\nrid.fit(xtrain,ytrain)","93f84d06":"ytrain_pred = rid.predict(xtrain)\nytest_pred = rid.predict(xtest)\n\nr2_train = r2_score(ytrain,ytrain_pred)\nr2_test = r2_score(ytest,ytest_pred)\n\nrmse_train = np.sqrt(mean_squared_error(ytrain,ytrain_pred))\nrmse_test = np.sqrt(mean_squared_error(ytest,ytest_pred))\n\nprint('R2 Train: ',r2_train,end='    ')\nprint('R2 Test: ', r2_test)\nprint('RMSE Train: ',rmse_train,end='    ')\nprint('RMSE Test: ',rmse_test)","bdf7f493":"# No imporvement at all.","9eb76137":"for i in data1.columns:\n    data1[i].fillna(method='ffill',inplace=True)","f3cbe6ee":"for i in data1.iloc[:,:-1].columns:\n    sns.boxplot(data1[i])\n    plt.show()","975057f1":"#### Data has no null values","4c1f1eb4":"for i in data1.iloc[:,:-1].columns:\n    sns.kdeplot(data1[i])\n    plt.show()","76732610":"data1.isnull().sum()","c3d35d85":"data1.isnull().sum()","91ff9bfe":"# Outlier Treatment\nfor i in data1.iloc[:,:-1].columns:\n    q1 = data1[i].quantile(0.25)\n    q3 = data1[i].quantile(0.75)\n    iqr = q3-q1\n    ub = q3+(1.5*iqr)\n    lb = q1-(1.5*iqr)\n    data1[i]=data1[~((data1[i] < lb) | (data1[i] > ub))]"}}