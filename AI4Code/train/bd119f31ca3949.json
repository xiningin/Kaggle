{"cell_type":{"24920236":"code","bbc0cc40":"code","6f8b9e82":"code","ec5a8216":"code","ee58922a":"code","794bd8ec":"code","94a8aee4":"code","a7d32ac5":"code","35e5a7ba":"code","2dd047fb":"code","1ffe4783":"code","3e0ea5ba":"code","1b59f8c0":"code","a3847d6b":"code","75652976":"code","199f669b":"code","e695333b":"code","11655905":"code","68e1ca32":"code","4dc697ec":"code","61ee0f59":"code","f02e3617":"code","d6c68d7b":"code","fdc89ce6":"code","9de88fed":"code","9f2ca7c0":"code","e960c01c":"code","9babd4a7":"code","9d1d7312":"code","ad526a56":"code","6aaabd81":"code","993993d8":"code","fa9912fd":"code","ac43ab13":"code","23d74ba4":"code","867cc3d1":"code","df82c66d":"code","3df7724b":"code","cec69c5a":"code","afb82755":"code","e7a0226b":"code","81ebb1f3":"code","72f2cf25":"code","8741a8a7":"code","734728db":"code","31999a30":"code","af2ad83c":"code","b737dcfd":"code","c56eabf2":"code","1ba6a9af":"code","b8c50a04":"code","baa9a926":"code","f441d473":"code","24b33c45":"code","9538c479":"code","d8dd6816":"code","c03155bc":"code","ae76fac2":"code","05f1cb11":"code","297ee723":"code","d8f790c8":"code","b491ff0f":"code","8b063360":"code","16ca96e2":"code","93c895a9":"code","3e54df91":"code","f3deac7f":"code","cc1999b8":"code","c67ee562":"code","93549cd2":"code","a2970ca2":"code","b36be3da":"code","987e0953":"code","1c5aac99":"code","02a596b2":"code","3089ad85":"code","e6ca5b11":"code","efe90cd9":"code","af161536":"code","3548a658":"code","727e8760":"code","021795e8":"markdown","2acf1825":"markdown","b72226a4":"markdown","afea2648":"markdown","db772c50":"markdown","8184bbcf":"markdown","109dd893":"markdown","9afb16fe":"markdown","a1045fac":"markdown","41e06be9":"markdown","e7069b22":"markdown","9c7ed958":"markdown","70e417b1":"markdown","7f6d7e4b":"markdown","85b328ce":"markdown","100fceaf":"markdown","01b6eaaf":"markdown","e31d2ed3":"markdown","c911a5d8":"markdown","8bc23612":"markdown","7aca915e":"markdown","4b7cafc9":"markdown","9916d8ef":"markdown","17390004":"markdown","9950c13e":"markdown","23835e90":"markdown","79ff0710":"markdown","fc3bb7df":"markdown","a2812465":"markdown","d8262655":"markdown","1b8535fb":"markdown","268013aa":"markdown","2d31e07a":"markdown","2d172f1b":"markdown","d680e4d1":"markdown","be1def04":"markdown","2ded0732":"markdown","0c4a02c8":"markdown","534403ce":"markdown","b643cc81":"markdown","b80aceab":"markdown","d89d9438":"markdown","5220cecc":"markdown","86c4a486":"markdown","04753033":"markdown","33ae8acf":"markdown","ed8db294":"markdown","75fc1a0a":"markdown","96cd1536":"markdown","154f5724":"markdown","a7b89395":"markdown","6972eb75":"markdown","a7053459":"markdown","306e4980":"markdown","15e4df79":"markdown","63688f9d":"markdown","0ea24609":"markdown","12ddbde4":"markdown","95fadfe8":"markdown","92fb1f10":"markdown","d6f9f043":"markdown","68d7b232":"markdown","56f34979":"markdown"},"source":{"24920236":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.metrics import r2_score\n#from pmdarima.arima import auto_arima\n#from pmdarima.arima import ADFTest\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom datetime import datetime, date\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nimport numpy as np\nimport math\nfrom math import sqrt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom numpy import asarray\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\nimport pickle\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bbc0cc40":"Aquifer_Doganella = pd.read_csv(\"\/kaggle\/input\/acea-water-prediction\/Aquifer_Doganella.csv\")\nAquifer_Auser = pd.read_csv(\"\/kaggle\/input\/acea-water-prediction\/Aquifer_Auser.csv\")\nAquifer_Luco = pd.read_csv(\"\/kaggle\/input\/acea-water-prediction\/Aquifer_Luco.csv\")\nAquifer_Petrignano = pd.read_csv(\"\/kaggle\/input\/acea-water-prediction\/Aquifer_Petrignano.csv\")\naquifers_lst = [Aquifer_Petrignano,Aquifer_Doganella,Aquifer_Auser,Aquifer_Luco]","6f8b9e82":"class MyPreprocessor:\n    '''If downsample is set to true then the the PreProcessor downsamples data to the granualarity parameter\n    granularity should be either \"7D\" for weekly, or \"M\" for Monthly.\n    \n    The processor will take the average of each feature. if \"get_averages\" is set to True'''\n    \n    def __init__(self,data,features,downsample=False,granularity=None,get_averages=False):\n        self.df = data.copy()\n        self.features = features\n        self.downsample = downsample\n        self.granularity = granularity\n        self.get_averages = get_averages\n        self.PCA = PCA\n\n        \n    def filter_year(self):\n        self.df['Date'] = self.df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\n        self.df = self.df.loc[(self.df['Date']>=self.start_year)]\n        return df\n\n    def fill_zero_volume(self,df):\n        '''Since zero volume is highly unlikely we will replace 0s with null\n        We will also replace the 0 values in Temperature with nan since many observations have this as the default where\n        a real observation is missing.  We we then do an interpolation, it will fill these with the temperatures of the days\n        following or before as proxys'''\n        for col in df.columns:\n            if col.startswith(\"Volume\"):\n                df[col] = np.where((df[col] == 0),np.nan, df[col])\n            else:\n                df[col]\n        return df\n    \n    \n    def fill_nas(self,df):    \n        for col in df.columns:\n            df[col] = df[col].interpolate().fillna(value=None, method='backfill', axis=None, limit=None, downcast=None)\n        \n        return df\n    \n    def feature_engineer(self,df):\n        \n        '''Create date columns for year, month, day. Average the values of the diffent sampled locations'''\n        #df['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\n        #df = df.loc[(df['Date']>=self.start_year)]\n        #y = df.iloc[0]['Date'].year\n        #df['year-month'] = df['Date'].apply(lambda x: int((x.year-y)*12+x.month))\n        df['year'] = df['Date'].apply(lambda x: int(x.year))\n        df['month'] = df['Date'].apply(lambda x: int(x.month))\n        df['day'] = df['Date'].apply(lambda x: int(x.day))\n        return df\n    \n    def engineer_averages(self,df):\n        df['rainfall-avg'] = df[[ f for f in df.columns if f.startswith('Rain')]].apply(lambda x: x.mean(), axis=1)\n        #df['rainfall-std'] = df[[ f for f in df.columns if f.startswith('Rain')]].apply(lambda x: np.std(x, axis=0), axis=1)\n        df['temperature-avg'] = df[[ f for f in df.columns if f.startswith('Temperature')]].apply(lambda x: x.mean(), axis=1)\n        #df['temperature-std'] = df[[ f for f in df.columns if f.startswith('Temperature')]].apply(lambda x: x.mean(), axis=1)\n        df['hydrometry-avg'] = df[[ f for f in df.columns if f.startswith('Hydrometry')]].apply(lambda x: x.mean(), axis=1)\n        df['volume-avg'] = df[[ f for f in df.columns if f.startswith('Volume')]].apply(lambda x: x.mean(), axis=1)\n        df['depth_to_groundwater-avg'] = df[[ f for f in df.columns if f.startswith('Depth')]].apply(lambda x: x.mean(), axis=1)\n        df = df.set_index('Date')\n        df = df.iloc[:,-9:]\n        data = df.drop([f for f in df.columns if (f.startswith('Rainfall') or f.startswith('Temperature') \n                            or f.startswith('Hydrometry') or f.startswith('Volume') or f.startswith('Depth'))],axis=1)\n        return data\n    \n    \n    def main(self):\n        data = self.df\n        #data = self.filter_year()\n        data = self.fill_zero_volume(data)\n        data = self.fill_nas(data)\n        data = self.feature_engineer(data)\n        \n        if self.get_averages == True:\n            data = self.engineer_averages(data)\n        else:\n            data\n        \n        if self.downsample == True:\n            df = self.data.resample(f'{granularity}', on='Date').sum().reset_index(drop=False)\n            return df\n        else:\n            return data\n        \n            \n","ec5a8216":"class Data_Exploration:\n    \n    def __init__(self,df,features,aquifer=None,PCA=False):\n        self.df = df\n        self.features = features\n        self.nrows = len(features)\n        self.aquifer = aquifer\n        self.PCA = PCA\n        \n    def scaler(self):\n        data = self.df.copy()\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n        return scaled_data\n    \n\n        \n    def line_plots(self):\n        depths = [f for f in self.df.columns if f.startswith('Depth')]\n        for f in depths:\n            depth_array = self.df[f].values\n            _index_array = np.array(self.df.index)\n            normalized_depth = preprocessing.normalize([depth_array])\n            normalized_depth = pd.Series(normalized_depth[0])\n\n            for feature in self.features:\n                feat_values = self.df['{}'.format(feature)].values\n                normalized_feature = preprocessing.normalize([feat_values])\n                normalized_feature = normalized_feature[0]\n                normalized_feature = pd.Series(normalized_feature)\n                #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                normalized_df = pd.DataFrame(data= {'Date':self.df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                fig= plt.figure(figsize=(10,3))\n                plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                plt.legend()\n                plt.title(f'{feature} vs. {f} (Normalized)')\n                plt.show()\n        \n    def hist_plot(self):\n        print(\"DISTRIBUTION CHARTS=====================================\")    \n        for feat in self.df.columns:\n            fig= plt.figure(figsize=(10,3))\n            sns.distplot(self.df[feat].fillna(np.inf), color='indianred')\n            plt.title(f'{str.capitalize(feat)}: ', fontsize=14)\n            plt.tight_layout()\n            plt.show()\n            \n    def hist_plots(self):\n            \n        f, ax = plt.subplots(nrows=self.nrows, ncols=1, figsize=(10, 35))\n        for i,feat in enumerate(self.features):\n            sns.distplot(self.df[feat].fillna(np.inf), ax=ax[i], color='indianred')\n            ax[i].set_title(f'{str.capitalize(feat)}: ', fontsize=14)\n            #ax[i].set_ylabel(ylabel=f'{str.capitalize(feat)}', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n    def stationarity(self):\n        # Dickey-Fuller Test\n        def interpret_dftest(dftest):\n            dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n            return dfoutput\n\n        for feature in self.features:\n            split = str(feature).split(\"_\")\n            feat = \" \".join(split)\n            print(f\"{str.capitalize(feat)}:\")\n            print(interpret_dftest(adfuller(self.df[feature])))\n            print(\"--------------------------------------------------------\")\n            \n        \n    def plot_corr_matrix(self):\n        df = self.df.copy()\n        df.set_index('Date')\n        #for col in df.columns:\n        #    df[col] = df[col].abs()\n        # Change values of columns to absolute values\n        fig= plt.figure(figsize=(15,15))\n        corrMatrix = df.corr()\n        sns.heatmap(corrMatrix, annot=True)\n        plt.show()\n                                                     \n    def plot_auto_correlation(self):\n        plot_acf(self.df['depth_to_groundwater-avg'])\n        plot_pacf(self.df['depth_to_groundwater-avg'])\n        \n    \n    def _PCA_(self):\n        aquifer_df = self.df.copy()\n        df= aquifer_df.drop(\"Date\",axis=1)\n        X_reduced = PCA(n_components=2).fit_transform(df)\n        pf = pd.DataFrame(X_reduced, columns=['PCA1','PCA2'])\n        df['PCA1'] = pf['PCA1']\n        df['PCA2'] = pf['PCA2']\n        for col in df.columns: \n            if col.startswith('Depth'):\n                xval = preprocessing.normalize([np.array(df['PCA1'])])\n                yval = preprocessing.normalize([np.array(df['PCA2'])])\n                ax.set_zlabel(f'{col}')\n                zval = preprocessing.normalize([np.array(df[f'{col}'])])\n                #zval = preprocessing.normalize([np.array(df['PCA12'])])\n                ax.scatter(xval,yval,zval,c=df[f'{col}'])\n        \n\n    def main(self):\n        self.line_plots()\n        self.hist_plots()\n        self.stationarity()\n        if self.PCA == True:\n            self._PCA_()\n        else: \n            pass\n        self.plot_corr_matrix()\n        ","ee58922a":"class Data_Exploration2:\n    \n    def __init__(self,df,features,aquifer=None):\n        self.df = df\n        self.features = features\n        self.nrows = len(features)\n        self.aquifer = aquifer\n        \n    def scaler(self):\n        data = self.df.copy()\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n        return scaled_data\n    \n    def lines_plot(self,df):\n        depths = [f for f in df.columns if f.startswith('Depth')]\n        for f in depths:\n            depth_array = df[f].values\n            _index_array = np.array(df.index)\n            normalized_depth = depth_array\n            normalized_depth = pd.Series(normalized_depth[0])\n\n            for feature in self.features:\n                feat_values = df['{}'.format(feature)].values\n                normalized_feature = feat_values\n                normalized_feature = normalized_feature[0]\n                normalized_feature = pd.Series(normalized_feature)\n                #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                normalized_df = pd.DataFrame(data= {'Date':df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                fig= plt.figure(figsize=(8,4))\n                plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                plt.legend()\n                plt.title(f'{feature} vs. {f} (Normalized)')\n                plt.show()\n        \n    def line_plots(self):\n        depths = [f for f in self.df.columns if f.startswith('Depth')]\n        for f in depths:\n            depth_array = self.df[f].values\n            _index_array = np.array(self.df.index)\n            normalized_depth = preprocessing.normalize([depth_array])\n            normalized_depth = pd.Series(normalized_depth[0])\n\n            for feature in self.features:\n                feat_values = self.df['{}'.format(feature)].values\n                normalized_feature = preprocessing.normalize([feat_values])\n                normalized_feature = normalized_feature[0]\n                normalized_feature = pd.Series(normalized_feature)\n                #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                normalized_df = pd.DataFrame(data= {'Date':self.df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                fig= plt.figure(figsize=(8,4))\n                plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                plt.legend()\n                plt.title(f'{feature} vs. {f} (Normalized)')\n                plt.show()\n        \n    def hist_plots(self):\n            \n        f, ax = plt.subplots(nrows=self.nrows, ncols=1, figsize=(10, 12))\n        for i,feat in enumerate(self.features):\n            sns.distplot(self.df[feat].fillna(np.inf), ax=ax[i], color='indianred')\n            ax[i].set_title(f'{str.capitalize(feat)}: ', fontsize=14)\n            ax[i].set_ylabel(ylabel=f'{str.capitalize(feat)}', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n    def stationarity(self):\n        # Dickey-Fuller Test\n        def interpret_dftest(dftest):\n            dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n            return dfoutput\n\n        for feature in self.features:\n            split = str(feature).split(\"_\")\n            feat = \" \".join(split)\n            print(f\"{str.capitalize(feat)}:\")\n            print(interpret_dftest(adfuller(self.df[feature])))\n            print(\"--------------------------------------------------------\")\n            \n    def resampling(self, year):\n        fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,12))\n\n        ax[0, 0].bar(self.df.Date, self.df['rainfall-avg'], width=5, color='dodgerblue')\n        ax[0, 0].set_title('Daily Rainfall (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','rainfall-avg']].resample('7D', on='Date').sum().reset_index(drop=False)\n        ax[1, 0].bar(resampled_df.Date, resampled_df['rainfall-avg'], width=10, color='dodgerblue')\n        ax[1, 0].set_title('Weekly Rainfall (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','rainfall-avg']].resample('M', on='Date').sum().reset_index(drop=False)\n        ax[2, 0].bar(resampled_df.Date, resampled_df['rainfall-avg'], width=15, color='dodgerblue')\n        ax[2, 0].set_title('Monthly Rainfall (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','rainfall-avg']].resample('12M', on='Date').sum().reset_index(drop=False)\n        ax[3, 0].bar(resampled_df.Date, resampled_df['rainfall-avg'], width=20, color='dodgerblue')\n        ax[3, 0].set_title('Annual Rainfall (Acc.)', fontsize=14)\n\n        for i in range(4):\n            ax[i, 0].set_xlim([date(year, 1, 1), date(2020, 6, 30)])\n\n        sns.lineplot(self.df.Date, self.df['temperature-avg'], color='dodgerblue', ax=ax[0, 1])\n        ax[0, 1].set_title('Daily Temperature (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','temperature-avg']].resample('7D', on='Date').mean().reset_index(drop=False)\n        sns.lineplot(resampled_df.Date, resampled_df['temperature-avg'], color='dodgerblue', ax=ax[1, 1])\n        ax[1, 1].set_title('Weekly Temperature (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','temperature-avg']].resample('M', on='Date').mean().reset_index(drop=False)\n        sns.lineplot(resampled_df.Date, resampled_df['temperature-avg'], color='dodgerblue', ax=ax[2, 1])\n        ax[2, 1].set_title('Monthly Temperature (Acc.)', fontsize=14)\n\n        resampled_df = self.df[['Date','temperature-avg']].resample('365D', on='Date').mean().reset_index(drop=False)\n        sns.lineplot(resampled_df.Date, resampled_df['temperature-avg'], color='dodgerblue', ax=ax[3, 1])\n        ax[3, 1].set_title('Annual Temperature (Acc.)', fontsize=14)\n\n        for i in range(4):\n            ax[i, 1].set_xlim([date(year, 1, 1), date(2020, 6, 30)])\n            ax[i, 1].set_ylim([-5, 35])\n        plt.show()\n        \n    def plot_corr_matrix(self,df):\n        df.set_index('Date')\n        #for col in df.columns:\n        #    df[col] = df[col].abs()\n        # Change values of columns to absolute values\n        fig= plt.figure(figsize=(10,10))\n        corrMatrix = df.corr()\n        sns.heatmap(corrMatrix, annot=True)\n        plt.show()\n                                                     \n    def plot_auto_correlation(self):\n        plot_acf(self.df['depth_to_groundwater-avg'])\n        plot_pacf(self.df['depth_to_groundwater-avg'])\n        \n\n    def main(self):\n        scaled_data = self.scaler()\n        self.lines_plot(scaled_data)\n        #self.line_plots()\n        self.hist_plots()\n        self.stationarity()\n        self.plot_corr_matrix(scaled_data)","794bd8ec":"date_time = pd.to_datetime(Aquifer_Auser.Date, format='%d\/%m\/%Y')\nplot_cols = Aquifer_Auser.iloc[:,1:-1].columns\nplot_features = Aquifer_Auser[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","94a8aee4":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(Aquifer_Auser.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","a7d32ac5":"df = Aquifer_Auser.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01\/01\/2011')]","35e5a7ba":"# Check time intervals\ndf['Time_Interval'] = df.Date - df.Date.shift(1)\n\nprint(df[['Date', 'Time_Interval']].head())\n\nprint(f\"{df['Time_Interval'].value_counts()}\")\ndf = df.drop('Time_Interval', axis=1)","2dd047fb":"# create list of features\nfeatures = [f for f in df.columns if (f.startswith('Depth')==False and f !='Date')]\nprocessor = MyPreprocessor(df,features=features,get_averages=False)\nAuser_df = processor.main()","1ffe4783":"Auser_df.info()","3e0ea5ba":"explorer = Data_Exploration(Auser_df,features,'Aquifer Auser',PCA=False)\nexplorer.main()","1b59f8c0":"date_time = pd.to_datetime(Aquifer_Doganella.Date, format='%d\/%m\/%Y')\nplot_cols = Aquifer_Doganella.iloc[:,1:-1].columns\nplot_features = Aquifer_Doganella[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","a3847d6b":"df = Aquifer_Doganella.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01-01-2019')]\n#df = df.set_index('Date')","75652976":"df.info()","199f669b":"processor = MyPreprocessor(df,features=features,get_averages=False)\nDoganella_df = processor.main()","e695333b":"# Get new features for Doganella \nfeatures = [f for f in Doganella_df.columns if (f.startswith('Depth')==False and f !='Date')]\n\nexplorer = Data_Exploration(Doganella_df,features,'Aquifer Doganella')\nexplorer.main()","11655905":"date_time = pd.to_datetime(Aquifer_Petrignano.Date, format='%d\/%m\/%Y')\nplot_cols = Aquifer_Petrignano.iloc[:,1:-1].columns\nplot_features = Aquifer_Petrignano[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","68e1ca32":"df = Aquifer_Petrignano.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01-01-2009')]","4dc697ec":"features = [f for f in df.columns if (f.startswith('Depth')==False and f !='Date')]\nprocessor = MyPreprocessor(df,features=features,get_averages=False)\nPetrignano_df = processor.main()","61ee0f59":"Petrignano_df.head()","f02e3617":"# Call our data explorer object for \nexplorer = Data_Exploration(Petrignano_df,features,'Aquifer Petrignano')\nexplorer.main()","d6c68d7b":"date_time = pd.to_datetime(Aquifer_Luco.Date, format='%d\/%m\/%Y')\nplot_cols = Aquifer_Luco.iloc[:,1:-1].columns\nplot_features = Aquifer_Luco[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","fdc89ce6":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols = list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n    # put it all together\n    agg = concat(cols, axis=1)\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg.values\n","9de88fed":"def walk_forward_validation(data, n_test):\n    predictions = list()\n    # split dataset\n    train, test = train_test_split(data, n_test)\n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # split test row into input and output columns\n        testX, testy = test[i, :-1], test[i, -1]\n        # fit model on history and make a prediction\n        yhat = random_forest_forecast(history, testX)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n        # summarize progress\n        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n    # estimate prediction error\n    error = mean_absolute_error(test[:, -1], predictions)\n    return error, test[:, -1], predictions","9f2ca7c0":"# fit an random forest model and make a one step prediction\ndef random_forest_forecast(train, testX):\n    # transform list into array\n    train = asarray(train)\n    # split into input and output columns\n    trainX, trainy = train[:, :-1], train[:, -1]\n    # fit model\n    model = RandomForestRegressor(n_estimators=100)\n    model.fit(trainX, trainy)\n    # make a one-step prediction\n    yhat = model.predict([testX])\n    return yhat[0]","e960c01c":"def forecast(df):\n    values = df.values\n    train = series_to_supervised(values, n_in=1)\n    # split into input and output columns\n    trainX, trainy = train.iloc[:, :-1], train.iloc[:, -1]\n    # fit model\n    model = RandomForestRegressor(n_estimators=100)\n    model.fit(trainX, trainy)\n    values = trainX.values\n    # construct an input for a new prediction\n    row = values[-1:].flatten()\n    yhat = model.predict(asarray([row]))\n    print('Input: %s\\n, Predicted: %.3f' % (row, yhat[0]))\n    return yhat","9babd4a7":"# transform a time series dataset into a supervised learning dataset\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols = list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n    # put it all together\n    agg = concat(cols, axis=1)\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg.values\n \n# split a univariate dataset into train\/test sets\ndef train_test_split(data, n_test):\n    return data[:-n_test, :], data[-n_test:, :]\n \n# fit an random forest model and make a one step prediction\ndef random_forest_forecast(train, testX):\n    # transform list into array\n    train = asarray(train)\n    # split into input and output columns\n    trainX, trainy = train[:, :-1], train[:, -1]\n    # fit model\n    model = RandomForestRegressor(n_estimators=100)\n    model.fit(trainX, trainy)\n    # make a one-step prediction\n    yhat = model.predict([testX])\n    return yhat[0]\n \n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test):\n    predictions = list()\n    # split dataset\n    train, test = train_test_split(data, n_test)\n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # split test row into input and output columns\n        testX, testy = test[i, :-1], test[i, -1]\n        # fit model on history and make a prediction\n        yhat = random_forest_forecast(history, testX)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n        # summarize progress\n        print('Day %.0f  expected=%.1f  predicted=%.1f' % (i,testy, yhat))\n    # estimate prediction error\n    error = mean_absolute_error(test[:, -1], predictions)\n    return error, test[:, -1], predictions\n\n# load the dataset\nPetrignano_df = Petrignano_df[['year','month','day','Rainfall_Bastia_Umbra','Temperature_Bastia_Umbra',\n                               'Temperature_Petrignano','Volume_C10_Petrignano','Hydrometry_Fiume_Chiascio_Petrignano',\n                               'Depth_to_Groundwater_P24','Depth_to_Groundwater_P25']]\n\nvalues = Petrignano_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 100)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at P25 to Aquifer Petrigrano')\npyplot.show()","9d1d7312":"# Reorder Columns to have the 'Depth' columns on the end, and the exact \"Pozo\" depth we want to predict on\nPetrignano_df = Petrignano_df[['year','month','day','Rainfall_Bastia_Umbra','Temperature_Bastia_Umbra',\n                               'Temperature_Petrignano','Volume_C10_Petrignano','Hydrometry_Fiume_Chiascio_Petrignano',\n                               'Depth_to_Groundwater_P24','Depth_to_Groundwater_P25']]\n\ndef forecast(df):\n    values = df.values\n    train = series_to_supervised(values, n_in=1)\n    # split into input and output columns\n    trainX, trainy = train[:, :-1], train[:, -1]\n    # fit model\n    model = RandomForestRegressor(n_estimators=100)\n    model.fit(trainX, trainy)\n    values = trainX\n    # construct an input for a new prediction\n    row = values[-1:].flatten()\n    yhat = model.predict(asarray([row]))\n    print('Input: %s\\n, Predicted: %.3f' % (row, yhat[0]))\n    return yhat\n\nDepth_to_Groundwater_P25_forecast = forecast(Petrignano_df)","ad526a56":"Depth_to_Groundwater_P25_forecast","6aaabd81":"class RF_Regression_Model:\n    def __init__(self,df,columns,predictor):\n        self.df = df\n        self.columns = columns\n        self.predictor = predictor\n        df = self.df[self.columns]\n    \n    # transform a time series dataset into a supervised learning dataset\n    def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n        n_vars = 1 if type(data) is list else self.data.shape[1]\n        df = DataFrame(data)\n        cols = list()\n        # input sequence (t-n, ... t-1)\n        for i in range(n_in, 0, -1):\n            cols.append(df.shift(i))\n        # forecast sequence (t, t+1, ... t+n)\n        for i in range(0, n_out):\n            cols.append(df.shift(-i))\n        # put it all together\n        agg = concat(cols, axis=1)\n        # drop rows with NaN values\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg.values\n\n    # split a univariate dataset into train\/test sets\n    def train_test_split(self,data, n_test):\n        return data[:-n_test, :], data[-n_test:, :]\n\n    # fit an random forest model and make a one step prediction\n    def random_forest_forecast(self,train, testX):\n        # transform list into array\n        train = asarray(train)\n        # split into input and output columns\n        trainX, trainy = train[:, :-1], train[:, -1]\n        # fit model\n        model = RandomForestRegressor(n_estimators=100)\n        model.fit(trainX, trainy)\n        # make a one-step prediction\n        yhat = model.predict([testX])\n        return yhat[0]\n\n    # walk-forward validation for univariate data\n    def walk_forward_validation(self,data, n_test):\n        predictions = list()\n        # split dataset\n        train, test = self.train_test_split(data, n_test)\n        # seed history with training dataset\n        history = [x for x in train]\n        # step over each time-step in the test set\n        for i in range(len(test)):\n            # split test row into input and output columns\n            testX, testy = test[i, :-1], test[i, -1]\n            # fit model on history and make a prediction\n            yhat = random_forest_forecast(history, testX)\n            # store forecast in list of predictions\n            predictions.append(yhat)\n            # add actual observation to history for the next loop\n            history.append(test[i])\n            # summarize progress\n            print('Day %.0f  expected=%.1f  predicted=%.1f' % (i,testy, yhat))\n        # estimate prediction error\n        error = mean_absolute_error(test[:, -1], predictions)\n        return error, test[:, -1], predictions\n\n    def test_model(self):\n        values = self.df.values\n        # transform the time series data into supervised learning\n        data = self.series_to_supervised(values)\n        # evaluate\n        mae, y, yhat = walk_forward_validation(data, 30)\n        print('MAE: %.3f' % mae)\n        # plot expected vs predicted\n        fig= plt.figure(figsize=(15,8))\n        pyplot.plot(y, label='Expected')\n        pyplot.plot(yhat, label='Predicted')\n        pyplot.legend()\n        plt.title(f'Predicted {self.predictor}')\n        pyplot.show()\n        \n    def forecast(self):\n        values = self.df.values\n        train = series_to_supervised(values, n_in=1)\n        # split into input and output columns\n        trainX, trainy = train[:, :-1], train[:, -1]\n        # fit model\n        model = RandomForestRegressor(n_estimators=100)\n        model.fit(trainX, trainy)\n        values = trainX\n        # construct an input for a new prediction\n        row = values[-1:].flatten()\n        depth_feature = 'Depth_to_Groundwater_P24'\n        yhat = model.predict(asarray([row]))\n        print('Input: %s\\n, Predicted: %.3f' % (row, yhat[0]))\n        return yhat\n    \n    # load the dataset\n    def main(self):\n        self.test_model()\n        forecast = self.forecast()\n        return forecast\n","993993d8":"# Reorder Columns to have the 'Depth' columns on the end\nPetrignano_df = Petrignano_df[['year','month','day','Rainfall_Bastia_Umbra','Temperature_Bastia_Umbra',\n                               'Temperature_Petrignano','Volume_C10_Petrignano','Hydrometry_Fiume_Chiascio_Petrignano',\n                               'Depth_to_Groundwater_P25','Depth_to_Groundwater_P24']]\n    \nvalues = Petrignano_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# evaluate\nmae, y, yhat = walk_forward_validation(data, 100)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at P24 Aquifer Petrigrano')\npyplot.show()\n    \n    \nDepth_to_Groundwater_P24_forecast = forecast(Petrignano_df)","fa9912fd":"class VAR_MODEL:\n    \n    def __init__(self,df):\n        self.data = df\n        #self.features = features\n        #self.data = self.df.drop(['Date'], axis=1)\n        #self.data.index = self.df.Date\n        #self.data = self.data[features]\n        \n    def _train_fit(self):\n        scaler = MinMaxScaler()\n        scaled = scaler.fit_transform(self.data)\n        #creating the train and validation set\n        self.train = scaled[:int(0.8*(len(scaled)))]\n        self.valid = scaled[int(0.8*(len(scaled))):]\n        #self.train = self.data[:int(0.8*(len(self.data)))]\n        #self.valid = self.data[int(0.8*(len(self.data))):]\n        \n        #fit the model\n        self.model = VAR(endog=self.train)\n        self.model_fit = self.model.fit()\n        \n    \n    def _predict_(self):\n        \n        # make prediction on validation\n        prediction = self.model_fit.forecast(self.model_fit.y, steps=len(self.valid))\n        \n        cols = self.data.columns\n\n        #converting predictions to dataframe\n        pred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])\n        for j in range(0,len(cols)):\n            for i in range(0, len(prediction)):\n                pred.iloc[i][j] = prediction[i][j]\n        \n\n        print(\"Predicted values of test set\")\n        print(pred.head())\n        print(\"----------------------------------\")\n        \n        valid = pd.DataFrame(data=self.valid,columns=cols)\n        print(\"Valid (true) values of test set\")\n        print(self.valid.head())\n        print(\"----------------------------------\")\n        \n        valid = self.valid.reset_index()\n        for c in pred.columns:\n            c = c[0]\n            if c.startswith('Depth'):\n                fig= plt.figure(figsize=(10,3))\n                plt.plot(pred[f'{c}'], label=f'{c}_Prediction')\n                plt.plot(valid[f'{c}'], label=f'{c}_True')\n                plt.legend()\n                plt.title(f'Predicted {c} vs. True {c}')\n                plt.show()\n                \n        #check rmse\n        for i in cols:\n            if i.startswith('Depth'):\n                print('rmse value for', i, 'is : ', sqrt(mean_squared_error(pred[[i]], self.valid[[i]])))\n            \n        #make final forecast predictions for next day\n        model = VAR(endog=self.data)\n        model_fit = model.fit()\n        yhat = model_fit.forecast(model_fit.y, steps=1)\n        print(\"----------------------------------\")\n        print(\"Forecasted feature predictions for next time interval\")\n        print(yhat)\n        return yhat\n    \n    def main(self):\n        self._train_fit()\n        yhat = self._predict_()\n        return yhat\n    \n","ac43ab13":"# load the dataset\nDoganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_4',\n                             'Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_7']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 100)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater to Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_7_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_7_forecast)","23d74ba4":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_4',\n                             'Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater to Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_6_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_6_forecast)","867cc3d1":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_4',\n                             'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_5']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater to Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_5_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_5_forecast)","df82c66d":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3',\n                             'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_4']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater to Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_4_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_4_forecast)","3df7724b":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2',\n                            'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_3']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater to Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_3_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_3_forecast)","cec69c5a":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_5',\n                             'Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_2']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at Pozzo 2, Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_2_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_2_forecast)","afb82755":"Doganella_df = Doganella_df[['year','month','day','Rainfall_Monteporzio','Rainfall_Velletri','Volume_Pozzo_4',\n                             'Volume_Pozzo_5+6','Volume_Pozzo_7','Volume_Pozzo_8','Volume_Pozzo_9','Temperature_Monteporzio','Temperature_Velletri',\n                            'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_5',\n                             'Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_1']]\n\nvalues = Doganella_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at Pozzo 1, Aquifer Doganella')\npyplot.show()\n\nDepth_to_Groundwater_Pozzo_1_forecast = forecast(Doganella_df)\nprint(Depth_to_Groundwater_Pozzo_1_forecast)","e7a0226b":"Auser_df.columns","81ebb1f3":" Auser_df = Auser_df[['year', 'month','day','Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli','Rainfall_Fabbriche_di_Vallico',\n        'Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico',\n       'Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL',\n       'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione',\n        'Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_PAG',\n       'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_DIEC']]\n    \nvalues = Auser_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at DIEC, Aquifer Auser')\npyplot.show()\n\nDepth_to_Groundwater_DIEC_forecast = forecast(Auser_df)\nprint(Depth_to_Groundwater_DIEC_forecast)","72f2cf25":" Auser_df = Auser_df[['year', 'month','day','Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli','Rainfall_Fabbriche_di_Vallico',\n        'Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico',\n       'Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL',\n       'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione',\n        'Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_PAG',\n       'Depth_to_Groundwater_DIEC','Depth_to_Groundwater_CoS']]\n    \nvalues = Auser_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at CoS, Aquifer Auser')\npyplot.show()\n\nDepth_to_Groundwater_CoS_forecast = forecast(Auser_df)\nprint(Depth_to_Groundwater_CoS_forecast)","8741a8a7":" Auser_df = Auser_df[['year', 'month','day','Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli','Rainfall_Fabbriche_di_Vallico',\n        'Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico',\n       'Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL',\n       'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione',\n        'Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_SAL',\n       'Depth_to_Groundwater_DIEC','Depth_to_Groundwater_CoS','Depth_to_Groundwater_PAG']]\n    \nvalues = Auser_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at PAG, Aquifer Auser')\npyplot.show()\n\nDepth_to_Groundwater_PAG_forecast = forecast(Auser_df)\nprint(Depth_to_Groundwater_PAG_forecast)","734728db":" Auser_df = Auser_df[['year', 'month','day','Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli','Rainfall_Fabbriche_di_Vallico',\n        'Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico',\n       'Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL',\n       'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione',\n        'Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_DIEC','Depth_to_Groundwater_CoS','Depth_to_Groundwater_PAG',\n        'Depth_to_Groundwater_SAL']]\n    \nvalues = Auser_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at SAL, Aquifer Auser')\npyplot.show()\n\nDepth_to_Groundwater_SAL_forecast = forecast(Auser_df)\nprint(Depth_to_Groundwater_SAL_forecast)","31999a30":" Auser_df = Auser_df[['year', 'month','day','Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli','Rainfall_Fabbriche_di_Vallico',\n        'Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico',\n       'Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL',\n       'Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione',\n       'Depth_to_Groundwater_DIEC','Depth_to_Groundwater_CoS','Depth_to_Groundwater_PAG',\n        'Depth_to_Groundwater_SAL','Depth_to_Groundwater_LT2']]\n    \nvalues = Auser_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Depth to Groundwater at LT2, Aquifer Auser')\npyplot.show()\n\nDepth_to_Groundwater_LT2_forecast = forecast(Auser_df)\nprint(Depth_to_Groundwater_LT2_forecast)","af2ad83c":"River_Arno = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/River_Arno.csv')\nRiver_Arno.info()","b737dcfd":"date_time = pd.to_datetime(River_Arno.Date, format='%d\/%m\/%Y')\nplot_cols = River_Arno.iloc[:,1:-1].columns\nplot_features = River_Arno[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","c56eabf2":"class Data_Exploration_Rivers:\n    \n    def __init__(self,df,features,aquifer=None,PCA=False):\n        self.df = df\n        self.features = features\n        self.nrows = len(features)\n        self.aquifer = aquifer\n        self.PCA = PCA\n        \n    def scaler(self):\n        data = self.df.copy()\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n        return scaled_data\n    \n\n        \n    def line_plots(self):\n        depths = [f for f in self.df.columns if f.startswith('Hydrometry')]\n        for f in depths:\n            depth_array = self.df[f].values\n            _index_array = np.array(self.df.index)\n            normalized_depth = preprocessing.normalize([depth_array])\n            normalized_depth = pd.Series(normalized_depth[0])\n\n            for feature in self.features:\n                feat_values = self.df['{}'.format(feature)].values\n                normalized_feature = preprocessing.normalize([feat_values])\n                normalized_feature = normalized_feature[0]\n                normalized_feature = pd.Series(normalized_feature)\n                #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                normalized_df = pd.DataFrame(data= {'Date':self.df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                fig= plt.figure(figsize=(10,3))\n                plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                plt.legend()\n                plt.title(f'{feature} vs. {f} (Normalized)')\n                plt.show()\n        \n    def hist_plot(self):\n        print(\"DISTRIBUTION CHARTS=====================================\")    \n        for feat in self.df.columns:\n            fig= plt.figure(figsize=(10,3))\n            sns.distplot(self.df[feat].fillna(np.inf), color='indianred')\n            plt.title(f'{str.capitalize(feat)}: ', fontsize=14)\n            plt.tight_layout()\n            plt.show()\n            \n    def hist_plots(self):\n            \n        f, ax = plt.subplots(nrows=self.nrows, ncols=1, figsize=(10, 35))\n        for i,feat in enumerate(self.features):\n            sns.distplot(self.df[feat].fillna(np.inf), ax=ax[i], color='indianred')\n            ax[i].set_title(f'{str.capitalize(feat)}: ', fontsize=14)\n            #ax[i].set_ylabel(ylabel=f'{str.capitalize(feat)}', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n    def stationarity(self):\n        # Dickey-Fuller Test\n        def interpret_dftest(dftest):\n            dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n            return dfoutput\n\n        for feature in self.features:\n            split = str(feature).split(\"_\")\n            feat = \" \".join(split)\n            print(f\"{str.capitalize(feat)}:\")\n            print(interpret_dftest(adfuller(self.df[feature])))\n            print(\"--------------------------------------------------------\")\n            \n        \n    def plot_corr_matrix(self):\n        df = self.df.copy()\n        df.set_index('Date')\n        #for col in df.columns:\n        #    df[col] = df[col].abs()\n        # Change values of columns to absolute values\n        fig= plt.figure(figsize=(15,15))\n        corrMatrix = df.corr()\n        sns.heatmap(corrMatrix, annot=True)\n        plt.show()\n                                                     \n    def plot_auto_correlation(self):\n        for col in self.df.columns:\n            if col.startswith('Hydrometry'):\n                plot_acf(self.df[f'{col}'])\n                plot_pacf(self.df[f'{col}'])\n        \n    \n    def _PCA_(self):\n        aquifer_df = self.df.copy()\n        df= aquifer_df.drop(\"Date\",axis=1)\n        X_reduced = PCA(n_components=2).fit_transform(df)\n        pf = pd.DataFrame(X_reduced, columns=['PCA1','PCA2'])\n        df['PCA1'] = pf['PCA1']\n        df['PCA2'] = pf['PCA2']\n        for col in df.columns: \n            if col.startswith('Hydrometry'):\n                xval = preprocessing.normalize([np.array(df['PCA1'])])\n                yval = preprocessing.normalize([np.array(df['PCA2'])])\n                ax.set_zlabel(f'{col}')\n                zval = preprocessing.normalize([np.array(df[f'{col}'])])\n                #zval = preprocessing.normalize([np.array(df['PCA12'])])\n                ax.scatter(xval,yval,zval,c=df[f'{col}'])\n        \n\n    def main(self):\n        self.line_plots()\n        self.hist_plots()\n        self.stationarity()\n        self.plot_auto_correlation()\n        if self.PCA == True:\n            self._PCA_()\n        else: \n            pass\n        self.plot_corr_matrix()","1ba6a9af":"df = River_Arno.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01-01-2004') & (df['Date']<'01-01-2007')]","b8c50a04":"features = ['Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero', 'Rainfall_Vernio',\n       'Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa',\n       'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina',\n       'Rainfall_Bibbiena', 'Rainfall_Camaldoli', 'Temperature_Firenze',\n       'Hydrometry_Nave_di_Rosano']","baa9a926":"processor = MyPreprocessor(df,features)\nArno_df = processor.main()\n\nexplorer = Data_Exploration_Rivers(Arno_df,features,'Aquifer Auser',PCA=False)\nexplorer.main()","f441d473":"Arno_df = Arno_df[features]","24b33c45":"values = Arno_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Hydrometry of Arno River at Nave di Rosano')\npyplot.show()\n\nArno_River_Hydrometry_forcast = forecast(Arno_df)\nprint(Arno_River_Hydrometry_forcast)","9538c479":"print(\"01\/01\/2017\")\nprint(River_Arno[River_Arno['Date']=='01\/01\/2017']['Hydrometry_Nave_di_Rosano'].values)\nprint(f\"Prediction: {Arno_River_Hydrometry_forcast}\")","d8dd6816":"Lake_Bilancino = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Lake_Bilancino.csv')\n\ndate_time = pd.to_datetime(Lake_Bilancino.Date, format='%d\/%m\/%Y')\nplot_cols = Lake_Bilancino.iloc[:,1:-1].columns\nplot_features = Lake_Bilancino[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","c03155bc":"class Data_Exploration:\n    \n    def __init__(self,df=None,features=None,targets=None,aquifer=None,PCA=False):\n        self.df = df\n        self.features = features\n        self.nrows = len(features)\n        self.aquifer = aquifer\n        self.PCA = PCA\n        self.targets=targets\n        \n    def scaler(self):\n        data = self.df.copy()\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n        return scaled_data\n    \n\n        \n    def line_plots(self):\n        for target in self.targets:\n            levels = [f for f in self.df.columns if f.startswith(f'{target}')]\n            for f in levels:\n                depth_array = self.df[f].values\n                _index_array = np.array(self.df.index)\n                normalized_depth = preprocessing.normalize([depth_array])\n                normalized_depth = pd.Series(normalized_depth[0])\n\n                for feature in self.features:\n                    feat_values = self.df['{}'.format(feature)].values\n                    normalized_feature = preprocessing.normalize([feat_values])\n                    normalized_feature = normalized_feature[0]\n                    normalized_feature = pd.Series(normalized_feature)\n                    #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                    normalized_df = pd.DataFrame(data= {'Date':self.df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                    fig= plt.figure(figsize=(10,3))\n                    plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                    plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                    plt.legend()\n                    plt.title(f'{feature} vs. {f} (Normalized)')\n                    plt.show()\n        \n    def hist_plot(self):\n        print(\"DISTRIBUTION CHARTS=====================================\")    \n        for feat in self.df.columns:\n            fig= plt.figure(figsize=(10,3))\n            sns.distplot(self.df[feat].fillna(np.inf), color='indianred')\n            plt.title(f'{str.capitalize(feat)}: ', fontsize=14)\n            plt.tight_layout()\n            plt.show()\n            \n    def hist_plots(self):\n            \n        f, ax = plt.subplots(nrows=self.nrows, ncols=1, figsize=(10, 35))\n        for i,feat in enumerate(self.features):\n            sns.distplot(self.df[feat].fillna(np.inf), ax=ax[i], color='indianred')\n            ax[i].set_title(f'{str.capitalize(feat)}: ', fontsize=14)\n            #ax[i].set_ylabel(ylabel=f'{str.capitalize(feat)}', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n    def stationarity(self):\n        # Dickey-Fuller Test\n        def interpret_dftest(dftest):\n            dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n            return dfoutput\n\n        for feature in self.features:\n            split = str(feature).split(\"_\")\n            feat = \" \".join(split)\n            print(f\"{str.capitalize(feat)}:\")\n            print(interpret_dftest(adfuller(self.df[feature])))\n            print(\"--------------------------------------------------------\")\n            \n        \n    def plot_corr_matrix(self):\n        df = self.df.copy()\n        df.set_index('Date')\n        #for col in df.columns:\n        #    df[col] = df[col].abs()\n        # Change values of columns to absolute values\n        fig= plt.figure(figsize=(15,15))\n        corrMatrix = df.corr()\n        sns.heatmap(corrMatrix, annot=True)\n        plt.show()\n                                                     \n    def plot_auto_correlation(self):\n        for col in self.df.columns:\n            if col.startswith('Hydrometry'):\n                plot_acf(self.df[f'{col}'])\n                plot_pacf(self.df[f'{col}'])\n        \n    \n    def _PCA_(self):\n        aquifer_df = self.df.copy()\n        df= aquifer_df.drop(\"Date\",axis=1)\n        X_reduced = PCA(n_components=2).fit_transform(df)\n        pf = pd.DataFrame(X_reduced, columns=['PCA1','PCA2'])\n        df['PCA1'] = pf['PCA1']\n        df['PCA2'] = pf['PCA2']\n        for col in df.columns: \n            if col.startswith('Hydrometry'):\n                xval = preprocessing.normalize([np.array(df['PCA1'])])\n                yval = preprocessing.normalize([np.array(df['PCA2'])])\n                ax.set_zlabel(f'{col}')\n                zval = preprocessing.normalize([np.array(df[f'{col}'])])\n                #zval = preprocessing.normalize([np.array(df['PCA12'])])\n                ax.scatter(xval,yval,zval,c=df[f'{col}'])\n        \n\n    def main(self):\n        self.line_plots()\n        self.hist_plots()\n        self.stationarity()\n        self.plot_auto_correlation()\n        if self.PCA == True:\n            self._PCA_()\n        else: \n            pass\n        self.plot_corr_matrix()","ae76fac2":"# Filter for 2004 and later\ndf = Lake_Bilancino.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01-01-2004')]","05f1cb11":"features = [f for f in df.columns if (f.startswith('Depth')==False and f !='Date')]\nprocessor = MyPreprocessor(df,features=features,get_averages=False)\nBilancino_df = processor.main()","297ee723":"features = ['Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata',\n       'Rainfall_Cavallina', 'Rainfall_Le_Croci', 'Temperature_Le_Croci',\n       'Lake_Level', 'Flow_Rate']\n\n# Call our data explorer object for \nexplorer = Data_Exploration(df=Bilancino_df,features=features,targets=['Flow_Rate','Lake_Level'])\nexplorer.main()","d8f790c8":"Bilancino_df = Bilancino_df[['year', 'month', 'day','Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata',\n       'Rainfall_Cavallina', 'Rainfall_Le_Croci', 'Temperature_Le_Croci',\n       'Lake_Level', 'Flow_Rate']]\n\n\nvalues = Bilancino_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Hydrometry (Flow Rate) of Lake Bilancino')\npyplot.show()\n\nLake_Bilancino_Flow_Rate_forcast = forecast(Bilancino_df)\nprint(Lake_Bilancino_Flow_Rate_forcast)","b491ff0f":"Bilancino_df = Bilancino_df[['year', 'month', 'day','Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata',\n       'Rainfall_Cavallina', 'Rainfall_Le_Croci', 'Temperature_Le_Croci',\n       'Flow_Rate','Lake_Level']]\n\n\nvalues = Bilancino_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Hydrometry (Lake Level) of Lake Bilancino')\npyplot.show()\n\nLake_Bilancino_Lake_Level_forcast = forecast(Bilancino_df)\nprint(Lake_Bilancino_Lake_Level_forcast)","8b063360":"Spring_Amiata = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Water_Spring_Amiata.csv')\nSpring_Madonna_di_Canneto = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Water_Spring_Madonna_di_Canneto.csv')\nSpring_Lupa = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Water_Spring_Lupa.csv')","16ca96e2":"date_time = pd.to_datetime(Spring_Amiata.Date, format='%d\/%m\/%Y')\nplot_cols = Spring_Amiata.iloc[:,1:-1].columns\nplot_features = Spring_Amiata[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","93c895a9":"# Filter for 2016 and later\ndf = Spring_Amiata.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\ndf = df.loc[(df['Date']>='01-01-2016')]\n\nfeatures = [f for f in df.columns if (f.startswith('Depth')==False and f !='Date')]\nprocessor = MyPreprocessor(df,features=features,get_averages=False)\nSpring_Amiata_df = processor.main()","3e54df91":"Spring_Amiata_df.info()","f3deac7f":"class Data_Exploration:\n    \n    def __init__(self,df=None,features=None,targets=None,aquifer=None,PCA=False):\n        self.df = df\n        self.features = features\n        self.nrows = len(features)\n        self.aquifer = aquifer\n        self.PCA = PCA\n        self.targets=targets\n        \n    def scaler(self):\n        data = self.df.copy()\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(data)\n        return scaled_data\n    \n\n        \n    def line_plots(self):\n        for target in self.targets:\n            levels = [f for f in self.df.columns if f.startswith(f'{target}')]\n            for f in levels:\n                depth_array = self.df[f].values\n                _index_array = np.array(self.df.index)\n                normalized_depth = preprocessing.normalize([depth_array])\n                normalized_depth = pd.Series(normalized_depth[0])\n\n                for feature in self.features:\n                    feat_values = self.df['{}'.format(feature)].values\n                    normalized_feature = preprocessing.normalize([feat_values])\n                    normalized_feature = normalized_feature[0]\n                    normalized_feature = pd.Series(normalized_feature)\n                    #the_array = np.hstack((_index_array, normalized_depth,normalized_feature))\n                    normalized_df = pd.DataFrame(data= {'Date':self.df.index,f'{f}':normalized_depth,f'{feature}':normalized_feature})\n                    fig= plt.figure(figsize=(10,3))\n                    plt.plot(normalized_df[f'{f}'], label=f'{f}')\n                    plt.plot(normalized_df[f'{feature}'], label=str.capitalize(feature))\n                    plt.legend()\n                    plt.title(f'{feature} vs. {f} (Normalized)')\n                    plt.show()\n        \n    def hist_plot(self):\n        print(\"DISTRIBUTION CHARTS=====================================\")    \n        for feat in self.df.columns:\n            fig= plt.figure(figsize=(10,3))\n            sns.distplot(self.df[feat].fillna(np.inf), color='indianred')\n            plt.title(f'{str.capitalize(feat)}: ', fontsize=14)\n            plt.tight_layout()\n            plt.show()\n            \n    def hist_plots(self):\n            \n        f, ax = plt.subplots(nrows=self.nrows, ncols=1, figsize=(10, 35))\n        for i,feat in enumerate(self.features):\n            sns.distplot(self.df[feat].fillna(np.inf), ax=ax[i], color='indianred')\n            ax[i].set_title(f'{str.capitalize(feat)}: ', fontsize=14)\n            #ax[i].set_ylabel(ylabel=f'{str.capitalize(feat)}', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n    def stationarity(self):\n        # Dickey-Fuller Test\n        def interpret_dftest(dftest):\n            dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n            return dfoutput\n\n        for feature in self.features:\n            split = str(feature).split(\"_\")\n            feat = \" \".join(split)\n            print(f\"{str.capitalize(feat)}:\")\n            print(interpret_dftest(adfuller(self.df[feature])))\n            print(\"--------------------------------------------------------\")\n            \n        \n    def plot_corr_matrix(self):\n        df = self.df.copy()\n        df.set_index('Date')\n        #for col in df.columns:\n        #    df[col] = df[col].abs()\n        # Change values of columns to absolute values\n        fig= plt.figure(figsize=(15,15))\n        corrMatrix = df.corr()\n        sns.heatmap(corrMatrix, annot=True)\n        plt.show()\n                                                     \n    def plot_auto_correlation(self):\n        for col in self.df.columns:\n            if col.startswith('Hydrometry'):\n                plot_acf(self.df[f'{col}'])\n                plot_pacf(self.df[f'{col}'])\n        \n    \n    def _PCA_(self):\n        aquifer_df = self.df.copy()\n        df= aquifer_df.drop(\"Date\",axis=1)\n        X_reduced = PCA(n_components=2).fit_transform(df)\n        pf = pd.DataFrame(X_reduced, columns=['PCA1','PCA2'])\n        df['PCA1'] = pf['PCA1']\n        df['PCA2'] = pf['PCA2']\n        for col in df.columns: \n            if col.startswith('Hydrometry'):\n                xval = preprocessing.normalize([np.array(df['PCA1'])])\n                yval = preprocessing.normalize([np.array(df['PCA2'])])\n                ax.set_zlabel(f'{col}')\n                zval = preprocessing.normalize([np.array(df[f'{col}'])])\n                #zval = preprocessing.normalize([np.array(df['PCA12'])])\n                ax.scatter(xval,yval,zval,c=df[f'{col}'])\n        \n\n    def main(self):\n        self.line_plots()\n        self.hist_plots()\n        self.stationarity()\n        self.plot_auto_correlation()\n        if self.PCA == True:\n            self._PCA_()\n        else: \n            pass\n        self.plot_corr_matrix()","cc1999b8":" features    =   ['year', 'month',\n       'day','Rainfall_Castel_del_Piano', 'Rainfall_Abbadia_S_Salvatore',\n       'Rainfall_S_Fiora', 'Rainfall_Laghetto_Verde', 'Rainfall_Vetta_Amiata',\n       'Depth_to_Groundwater_S_Fiora_8', 'Depth_to_Groundwater_S_Fiora_11bis',\n       'Depth_to_Groundwater_David_Lazzaretti',\n       'Temperature_Abbadia_S_Salvatore', 'Temperature_S_Fiora',\n       'Temperature_Laghetto_Verde', 'Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n       'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']\n    \ntargets = ['Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n       'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']\n\n# Call our data explorer object for \nexplorer = Data_Exploration(df=Spring_Amiata_df,features=features,targets=targets)\nexplorer.main()","c67ee562":"Spring_Amiata_df = Spring_Amiata_df[features]\n\nvalues = Spring_Amiata_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Flow Rate Galleria Alta, Spring Amiata')\npyplot.show()\n\nFlow_Rate_Galleria_Alta_forcast = forecast(Spring_Amiata_df)\nprint(Flow_Rate_Galleria_Alta_forcast)","93549cd2":"Spring_Amiata_df = Spring_Amiata_df[['year', 'month',\n       'day','Rainfall_Castel_del_Piano', 'Rainfall_Abbadia_S_Salvatore',\n       'Rainfall_S_Fiora', 'Rainfall_Laghetto_Verde', 'Rainfall_Vetta_Amiata',\n       'Depth_to_Groundwater_S_Fiora_8', 'Depth_to_Groundwater_S_Fiora_11bis',\n       'Depth_to_Groundwater_David_Lazzaretti',\n       'Temperature_Abbadia_S_Salvatore', 'Temperature_S_Fiora',\n       'Temperature_Laghetto_Verde', 'Flow_Rate_Bugnano', 'Flow_Rate_Arbure',\n       'Flow_Rate_Galleria_Alta','Flow_Rate_Ermicciolo']]\n\nvalues = Spring_Amiata_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Flow Rate Ermicciolo, Spring Amiata')\npyplot.show()\n\nFlow_Rate_Ermicciolo_forcast = forecast(Spring_Amiata_df)\nprint(Flow_Rate_Ermicciolo_forcast)","a2970ca2":"Spring_Amiata_df = Spring_Amiata_df[['year', 'month',\n       'day','Rainfall_Castel_del_Piano', 'Rainfall_Abbadia_S_Salvatore',\n       'Rainfall_S_Fiora', 'Rainfall_Laghetto_Verde', 'Rainfall_Vetta_Amiata',\n       'Depth_to_Groundwater_S_Fiora_8', 'Depth_to_Groundwater_S_Fiora_11bis',\n       'Depth_to_Groundwater_David_Lazzaretti',\n       'Temperature_Abbadia_S_Salvatore', 'Temperature_S_Fiora',\n       'Temperature_Laghetto_Verde', 'Flow_Rate_Bugnano',\n       'Flow_Rate_Galleria_Alta','Flow_Rate_Ermicciolo', 'Flow_Rate_Arbure']]\n\nvalues = Spring_Amiata_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Flow Rate Ermicciolo, Spring Amiata')\npyplot.show()\n\nFlow_Rate_Arbure_forcast = forecast(Spring_Amiata_df)\nprint(Flow_Rate_Arbure_forcast)","b36be3da":"Spring_Amiata_df = Spring_Amiata_df[['year', 'month',\n       'day','Rainfall_Castel_del_Piano', 'Rainfall_Abbadia_S_Salvatore',\n       'Rainfall_S_Fiora', 'Rainfall_Laghetto_Verde', 'Rainfall_Vetta_Amiata',\n       'Depth_to_Groundwater_S_Fiora_8', 'Depth_to_Groundwater_S_Fiora_11bis',\n       'Depth_to_Groundwater_David_Lazzaretti',\n       'Temperature_Abbadia_S_Salvatore', 'Temperature_S_Fiora',\n       'Temperature_Laghetto_Verde',\n       'Flow_Rate_Galleria_Alta','Flow_Rate_Ermicciolo', 'Flow_Rate_Arbure','Flow_Rate_Bugnano']]\n\nvalues = Spring_Amiata_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Flow Rate Bugnano, Spring Amiata')\npyplot.show()\n\nFlow_Rate_Bugnano_forcast = forecast(Spring_Amiata_df)\nprint(Flow_Rate_Bugnano_forcast)","987e0953":"Spring_Lupa","1c5aac99":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(Spring_Lupa.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","02a596b2":"Spring_Lupa = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Water_Spring_Lupa.csv')\n\ndate_time = pd.to_datetime(Spring_Lupa.Date, format='%d\/%m\/%Y')\nplot_cols = Spring_Lupa.iloc[:,1:-1].columns\nplot_features = Spring_Lupa[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True,figsize=(15,20))","3089ad85":"# Filter for 2016 and later\ndf = Spring_Lupa.copy()\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, \"%d\/%m\/%Y\"))\n#df = df.loc[(df['Date']>='01-01-2016')]\n\nfeatures = [f for f in df.columns if (f !='Date')]\nprocessor = MyPreprocessor(df,features=features,get_averages=False)\nSpring_Lupa_df = processor.main()","e6ca5b11":"Spring_Lupa_df.info()","efe90cd9":" features    =   ['year', 'month',\n       'day','Rainfall_Terni','Flow_Rate_Lupa']\n    \ntargets = ['Flow_Rate_Lupa']\n\n# Call our data explorer object for \nexplorer = Data_Exploration(df=Spring_Lupa_df,features=features,targets=targets)\nexplorer.main()","af161536":"Spring_Lupa_df = Spring_Lupa_df[features]\n\nvalues = Spring_Lupa_df.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=1)\n# Evaluate\nmae, y, yhat = walk_forward_validation(data, 50)\nprint('MAE: %.3f' % mae)\n# plot expected vs predicted\nfig= plt.figure(figsize=(15,8))\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.legend()\nplt.title('Predicted Flow Rate Lupa Sping')\npyplot.show()\n\nFlow_Rate_Lupa_forcast = forecast(Spring_Lupa_df)\nprint(Flow_Rate_Lupa_forcast)","3548a658":"depths = {\"Water Body\": ['Aquifer Auser','Aquifer Auser','Aquifer Auser','Aquifer Auser',\n                         'Aquifer Doganella','Aquifer Doganella','Aquifer Doganella','Aquifer Doganella','Aquifer Doganella','Aquifer Doganella','Aquifer Doganella',\n                      'Aquifer Petrignano','Aquifer Petrignano',\n                        'River Arno','Lake Bilancino','Lake Bilancino','Spring Amiata','Spring Amiata','Spring Amiata','Spring Amiata','Sping Lupa'],\n          \"Output\" : ['Depth_to_Groundwater_PAG','Depth_to_Groundwater_CoS','Depth_to_Groundwater_SAL','Depth_to_Groundwater_LT2',\n                      'Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_4',\n                       'Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_7',\n                      'Depth_to_Groundwater_P24','Depth_to_Groundwater_P25','Hydrometry (River Level)','Flow Rate','Lake Level',\n                     'Flow_Rate_Galleria_Alta','Flow_Rate_Ermicciolo_Alta','Flow Rate Arbure','Flow Rate Bugnano','Flow Rate Lupa'],\n          'Forecast': [Depth_to_Groundwater_PAG_forecast,Depth_to_Groundwater_CoS_forecast,Depth_to_Groundwater_SAL_forecast,Depth_to_Groundwater_LT2_forecast,\n                        Depth_to_Groundwater_Pozzo_1_forecast,Depth_to_Groundwater_Pozzo_2_forecast,Depth_to_Groundwater_Pozzo_3_forecast,Depth_to_Groundwater_Pozzo_4_forecast,\n                        Depth_to_Groundwater_Pozzo_5_forecast,Depth_to_Groundwater_Pozzo_6_forecast,Depth_to_Groundwater_Pozzo_7_forecast,\n                        Depth_to_Groundwater_P24_forecast,Depth_to_Groundwater_P25_forecast,Arno_River_Hydrometry_forcast,Lake_Bilancino_Flow_Rate_forcast,Lake_Bilancino_Lake_Level_forcast,\n                      Flow_Rate_Galleria_Alta_forcast,Flow_Rate_Ermicciolo_forcast,Flow_Rate_Arbure_forcast,Flow_Rate_Bugnano_forcast,Flow_Rate_Lupa_forcast]\n                        }\ndf = pd.DataFrame(depths,columns=[\"Water Body\",\"Output\",'Forecast'])\ndf1=df.set_index([\"Water Body\", 'Output'])","727e8760":"df1","021795e8":"### Forecasting Pozzo 4 Doganella","2acf1825":"### Flow Rate Galeria Alta Forecast","b72226a4":"[](https:\/\/storage.cloud.google.com\/kaggle-media\/competitions\/Acea\/Screen%20Shot%202020-12-02%20at%2012.40.17%20PM.png)","afea2648":"**From the charts its very clear that Hydrometry is strongly correlated with Rainfall and Temperature**","db772c50":"The data for this Aquifer is missing alot of data, there are many holes to fill.  We will only use the last year of data for this one.","8184bbcf":"We can see that month, temperature, and volume had a fairly significant correlation with Depth to Groundwater, **while rainfall had a poor correlation**, suprisingly. This may be due to that Rainfall is on average over the whole time period is very low, and that effects of rainfall are not immediate, there is a delay.  The histogram shows that the majority of days had 0-2 cm of precipitation and this leads to the correlation being weak.\n\nInterestingly, there is a strong correlation betweeen year and Hydrometry especially at POL.  There is also strong correlations between Hydrometry and Depth, depending on if they are at the North or South sections.  ","109dd893":"### Observations\n\n* There is greater flow rates in the Winter months as seen where there are spikes in flow rates when temperatures are low","9afb16fe":"We will filter the data from Aquifer Auser to only after 2011","a1045fac":"### AQUIFER LUCO","41e06be9":"### Forecasting Depth as DIEC , Auser","e7069b22":"### Forecasting Pozzo 2 - Doganella","9c7ed958":"# Random Forest Model","70e417b1":"## Time series as supervised learning problem","7f6d7e4b":"We can use the RandomForestRegressor class to make a one-step forecast.\n\nThe random_forest_forecast() function below implements this, taking the training dataset and test input row as input, fitting a model and making a one-step prediction.","85b328ce":"### Forecasting Pozzo 6 - Doganella","100fceaf":"The model doesnt do such a good job of predicting flow Rate.  We may have to Scale our data ","01b6eaaf":"### Aquifer Auser","e31d2ed3":"In *walk-forward validation*, the dataset is first split into train and test sets by selecting a cut point, e.g. all data except the last 12 months is used for training and the last 12 months is used for testing.\n\nIf we are interested in making a one-step forecast, e.g. one month, then we can evaluate the model by training on the training dataset and predicting the first step in the test dataset. We can then add the real observation from the test set to the training dataset, refit the model, then have the model predict the second step in the test dataset.\n\nThe function below performs walk-forward validation.\n\nIt takes the entire supervised learning version of the time series dataset and the number of rows to use as the test set as arguments.\n\nIt then steps through the test set, calling the random_forest_forecast() function to make a one-step forecast. An error measure is calculated and the details are returned for analysis.","c911a5d8":"## Arno River","8bc23612":"### Model for Depth_at_P24 Petrigrano\n\nNow we use the same model to predict the depth at station Pozo P24.","7aca915e":"### Forecasting Depth to PAG Station","4b7cafc9":"### AQUIFER DOGANELLA","9916d8ef":"We will only use data later than 2009 for Aquifer Petrignano.  Since zero volume is highly unlikely we will replace 0s with null\nWe will also replace the 0 values in Temperature with nan since many observations have this as the default where\na real observation is missing.  We we then do an interpolation, it will fill these with the temperatures of the days\nfollowing or before as proxys.  This is done on the other datasets as well","17390004":"## Training models and forecasting next time step at each station of each aquifer","9950c13e":"### Forecasting Flow Rate Bugnano","23835e90":"Time series data can be phrased as supervised learning.\n\nGiven a sequence of numbers for a time series dataset, we can restructure the data to look like a supervised learning problem. We can do this by using previous time steps as input variables and use the next time step as the output variable.\n\nWe can restructure this time series dataset as a supervised learning problem by using the value at the previous time step to predict the value at the next time-step.\n\n* **time  measure**\n* 1,      100\n* 2,      110\n* 3,      108\n* 4,      115\n* 5,      120\n\nReorganizing the time series dataset this way, the data would look as follows:\n\n\n* **X,   y**\n* ?,     100\n* 100,   110\n* 110,   108\n* 108,   115\n* 115,   120\n* 120,   ?\n\nNote that the time column is dropped and some rows of data are unusable for training a model, such as the first and the last.\n\nThis representation is called a sliding window, as the window of inputs and expected outputs is shifted forward through time to create new \u201csamples\u201d for a supervised learning model.\n\nWe can use the **shift()** function in Pandas to automatically create new framings of time series problems given the desired length of input and output sequences.\n\nThis would be a useful tool as it would allow us to explore different framings of a time series problem with machine learning algorithms to see which might result in better-performing models.\n\nThe function below will take a time series as a NumPy array time series with one or more columns and transform it into a supervised learning problem with the specified number of inputs and outputs. We can use this function to prepare a time series dataset for Random Forest.\n\nThe function takes four arguments:\n\n* data: Sequence of observations as a list or 2D NumPy array. Required.\n* n_in: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.\n* n_out: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.\n* dropnan: Boolean whether or not to drop rows with NaN values. Optional. Defaults to True.","79ff0710":"# LAKES","fc3bb7df":"The model on Pozo 2 performs relatively poorly due to the sudden drop in the groundwater level.  The model isnt particularly strong at anticipating these drastic changes","a2812465":"The final forecast function will use the RandomForestRegressor on the entire dataset to forecast the next time step","d8262655":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F6195295%2Fcca952eecc1e49c54317daf97ca2cca7%2FAcea-Input.png?generation=1606932492951317&alt=media)","1b8535fb":"## SPRING LUPA","268013aa":"### Flow Rate Ermicciolo Forecast","2d31e07a":"### Fitting the final model to all the data for new predictions\n\nOnce a final Random Forest model configuration is chosen, a model can be finalized and used to make a prediction on new data.\n\nThis is called an **out-of-sample forecast**, e.g. predicting beyond the training dataset. This is identical to making a prediction during the evaluation of the model, as we always want to evaluate a model using the same procedure that we expect to use when the model is used to make predictions on new data.\n\nThe code below demonstrates fitting a final Random Forest model on all available data and making a one-step prediction beyond the end of the dataset.","2d172f1b":"### Forecasting Depth at CoS","d680e4d1":"## Lake Bilancino","be1def04":"# SPRINGS","2ded0732":"### Forecasting Pozzo 5 Doganella","0c4a02c8":"It is of the utmost importance to notice that some features like rainfall and temperature, which are present in each dataset, don\u2019t go alongside the date. Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. This means, for instance, that rain fell on 1st January doesn\u2019t affect the mentioned features right the same day but some time later. As we don\u2019t know how many days\/weeks\/months later rainfall affects these features, this is another aspect to keep into consideration when analyzing the dataset.","534403ce":"### Depth forecast at SAL, Aquifer Auser","b643cc81":"We'll take a quick look at all the data from each column and then at how each feature correlates with each of the depth (target) features","b80aceab":"## Forecast Lake LEvel - Bilancino","d89d9438":"### Pozzo 3 Doganella","5220cecc":"From the correlations chart we can see that the two depth readings from the two pozos perfectly correlate with each other.  The is also a strong correlation between Volume_C10_Petrignano and the two depth readings as also seen in the line chart. \n\nInterestingly, the year has a significant correlation with Volume_C10_Petrignano.  Year also has a relatively strong correlation with the Depths.  This could signify a particularly dry or wet year.","86c4a486":"We can use the model to predict data when we have sufficient data to train on.  For now for example purposes this is a prediction of the 1st of Jan 2017","04753033":"This competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody. As each waterbody is different from the other, the related features as well are different from each other. So, if for instance we consider a water spring we notice that its features are different from the lake\u2019s one. This is correct and reflects the behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided).\n\nLet\u2019s see how these nine waterbodies differ from each other.\n\nWaterbody: Auser\nType: Aquifer\n\nDescription: This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.\n\nThe levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.\n\nWaterbody: Petrignano\nType: Aquifer\n\nDescription: The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n\nWaterbody: Doganella\nType: Aquifer\n\nDescription: The wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, ..., Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes.\n\nWaterbody: Luco\nType: Aquifer\n\nDescription: The Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes.\n\nWaterbody: Amiata\nType: Water spring\n\nDescription: The Amiata waterbody is composed of a volcanic aquifer not fed by rivers or lakes but fed by meteoric infiltration. This aquifer is accessed through Ermicciolo, Arbure, Bugnano and Galleria Alta water springs. The levels and volumes of the four sources are influenced by the parameters: rainfall, depth to groundwater, hydrometry, temperatures and drainage volumes.\n\nWaterbody: Madonna di Canneto\nType: Water spring\n\nDescription: The Madonna di Canneto spring is situated at an altitude of 1010m above sea level in the Canneto valley. It does not consist of an aquifer and its source is supplied by the water catchment area of the river Melfa.\n\nWaterbody: Lupa\nType: Water spring\n\nDescription: this water spring is located in the Rosciano Valley, on the left side of the Nera river. The waters emerge at an altitude of about 375 meters above sea level through a long draining tunnel that crosses, in its final section, lithotypes and essentially calcareous rocks. It provides drinking water to the city of Terni and the towns around it.\n\nWaterbody: Arno\nType: River\n\nDescription: Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Arno results to be the main source of water supply of the metropolitan area of Florence-Prato-Pistoia. The availability of water for this waterbody is evaluated by checking the hydrometric level of the river at the section of Nave di Rosano.\n\nWaterbody: Bilancino\nType: Lake\n\nDescription: Bilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river.\n\nEach waterbody has its own different features to be predicted. The table below shows the expected feature to forecast for each waterbody.","33ae8acf":"### AQUIFER PETRIGRANO","ed8db294":"### Forecast for Arbure, Spring Amiata","75fc1a0a":"## Aquifer Auser","96cd1536":"We can use the RandomForestRegressor class to make a one-step forecast.\n\nThe random_forest_forecast() function below implements this, taking the training dataset and test input row as input, fitting a model and making a one-step prediction.","154f5724":"## DATA PROCESSING & EXPLORATORY ANALYSIS","a7b89395":" **We can fill in the missing Temperatures from Monteporzio with forcasted values of a pmdarima","6972eb75":"# AQUIFERS","a7053459":"![](https:\/\/storage.cloud.google.com\/kaggle-media\/competitions\/Acea\/Screen%20Shot%202020-12-02%20at%2012.40.17%20PM.png)","306e4980":"# Final Results","15e4df79":"### Forecasting Depth at LT2, Aquifer Auser","63688f9d":"We can see that the model after training on just the last 100 days using just one days lag as input does rather well at predicting the true value of that day, with a MAE of 0.035 meters.  So our model can predict the next day with error expected of about 3.5 centimeters!  If we want we can change the n_in variable input to be 7 if we want use a lag time of 7 days (since each observations is daily, and set the n_out to 7 to see predict the data 7 days ahead.  However, our model will probably not perform as well so for our purposes here we will only predict one day ahead","0ea24609":"Install pmdarima module which we will use to fill in missing stationary values for temperature","12ddbde4":"# RIVERS","95fadfe8":"### AQUIFER PETRIGNANO","92fb1f10":"### DOGANELLA","d6f9f043":"### Forecasting Pozzo 7 - Doganella","68d7b232":"We will train a new model on the previous 100 days to observe how to model performs at each station.  Then we will use the entire dataset for the aquifer to predict the next (future) time step.  In this instance that will be one day, but we could also have used the next week by setting that downsampleing parameter in the PreProcessing module.","56f34979":"Since this dataset is missing the key feature of Temperature for the previous years, we will pick a time period when there were measurements for all the stations for each variable, and use that to train a model and predict the first day of 2007 and compare with the actual value for Hydrometry.  The model can be reused for a later day when there is better data"}}