{"cell_type":{"c8f9a963":"code","089e16f6":"code","52186f3a":"code","101ecdfe":"code","e19569b5":"code","0e658e9b":"code","7c289500":"code","c887cce8":"code","94b03599":"code","2665dcab":"code","9ee3c0f9":"code","7f05e73c":"code","acce5cf2":"code","44b3893a":"code","8166419e":"code","f8d5c8dc":"code","9292bfba":"code","34d41b81":"code","9c4326a1":"markdown","e3002cf0":"markdown","e600b929":"markdown","ce22d853":"markdown","4480603c":"markdown"},"source":{"c8f9a963":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Import all the necessary files!\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom os import getcwd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","089e16f6":"path_inception = \"..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n\n# Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = path_inception\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\n# Make all the layers in the pre-trained model non-trainable\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \n# Print the model summary\npre_trained_model.summary()","52186f3a":"# we freeze the inception model by selecting mixed7 conv layer\nlast_layer = pre_trained_model.get_layer('mixed7') # Modelin Bu layer\u0131na kadar ki k\u0131sm\u0131n\u0131 al demek.\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\n# Expected Output:\n# ('last layer output shape: ', (None, 7, 7, 768))","101ecdfe":"# The callback helps us to choose on which accuracy level we want to stop model   \nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.97):\n            print(\"\\nReached 97% accuracy so cancelling training!\")\n            self.model.stop_training = True\n","e19569b5":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)             \n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a final sigmoid layer for classification\nx = layers.Dense  (6, activation='softmax')(x)          \n\nmodel = Model( pre_trained_model.input, x)  \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])\n\nmodel.summary()","0e658e9b":"# Dot representation of the model\nfrom IPython.display import SVG\nimport tensorflow.keras.utils as Utils\nfrom keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\nUtils.plot_model(model,to_file='model.png',show_shapes=True)","7c289500":"# We assign each image directory to different variables\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport zipfile\nimport shutil\n\n# Define our example directories and files\ntrain_dir = '\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/'\nvalidation_dir = '\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/'\npred_dir = '\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred\/'\n\n\n\ntrain_buildings_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/buildings')\ntrain_forest_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/forest')\ntrain_glacier_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/glacier')\ntrain_mountain_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/mountain')\ntrain_sea_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/sea')\ntrain_street_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/street')\n\n\nvalidation_buildings_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/buildings')\nvalidation_forest_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/forest')\nvalidation_glacier_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/glacier')\nvalidation_mountain_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/mountain')\nvalidation_sea_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/sea')\nvalidation_street_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/street')\n\n\npred_images_dir = os.path.join('\/kaggle\/input\/intel-image-classification\/seg_pred\/seg_pred')\npred_images_fnames = os.listdir(pred_images_dir)\n\ntrain_buildings_fnames = os.listdir(train_buildings_dir)\ntrain_forest_fnames = os.listdir(train_forest_dir)\ntrain_glacier_fnames = os.listdir(train_glacier_dir)\ntrain_mountain_fnames = os.listdir(train_mountain_dir)\ntrain_sea_fnames = os.listdir(train_sea_dir)\ntrain_street_fnames = os.listdir(train_street_dir)\n\n\nvalidation_buildings_fnames = os.listdir(validation_buildings_dir)\nvalidation_forest_fnames = os.listdir(validation_forest_dir)\nvalidation_glacier_fnames = os.listdir(validation_glacier_dir)\nvalidation_mountain_fnames = os.listdir(validation_mountain_dir)\nvalidation_sea_fnames = os.listdir(validation_sea_dir)\nvalidation_street_fnames = os.listdir(validation_street_dir)\n\n\n\nprint(\"TRA\u0130N\u0130NG SET\")\nprint(\"train_buildings_fnames\",len(train_buildings_fnames))\nprint(\"train_forest_fnames\", len(train_forest_fnames))\nprint(\"train_glacier_fnames\", len(train_glacier_fnames))\nprint(\"train_mountain_fnames\", len(train_mountain_fnames))\nprint(\"train_sea_fnames\", len(train_sea_fnames))\nprint(\"train_street_fnames\", len(train_street_fnames))\n\n\nprint(\"*\" * 20)\nprint(\"VALIDATION SET\")\nprint(\"validation_buildings_fnames\",len(validation_buildings_fnames))\nprint(\"validation_forest_fnames\", len(validation_forest_fnames))\nprint(\"validation_glacier_fnames\", len(validation_glacier_fnames))\nprint(\"validation_mountain_fnames\", len(validation_mountain_fnames))\nprint(\"validation_sea_fnames\", len(validation_sea_fnames))\nprint(\"validation_street_fnames\", len(validation_street_fnames))\n\n\nprint(\"*\" * 20)\nprint(\"pred_images_fnames\", len(pred_images_fnames))\n","c887cce8":"# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n      rescale = 1.\/255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator( rescale = 1.0\/255.)\n\ndesired_batch_size = 32\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size =desired_batch_size,\n                                                    class_mode = 'categorical',\n                                                    target_size = (150, 150))  \n\n\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator =  test_datagen.flow_from_directory( validation_dir,\n                                                          batch_size  = desired_batch_size,\n                                                          class_mode  = 'categorical',\n                                                          shuffle=False,\n                                                          target_size = (150, 150))\n\n","94b03599":"# It shows  classes in the train set\ntrain_generator.class_indices","2665dcab":"# It shows  classes in the test set\nvalidation_generator.class_indices","9ee3c0f9":"callbacks = myCallback()","7f05e73c":"# Run this and see how many epochs it should take before the callback\n# fires, and stops training at 97% accuracy\n\n\nhistory = model.fit(train_generator,\n                              validation_data = validation_generator,\n                              steps_per_epoch = len(train_generator)\/desired_batch_size,\n                              epochs = 20,\n                              validation_steps = len(validation_generator)\/desired_batch_size,\n                              verbose = 1,\n                              callbacks = [callbacks]) ","acce5cf2":"# Plotting accuracy and loss values of the train and validation sets\n%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","44b3893a":"# Predicting the Test images\npred=model.evaluate_generator(validation_generator,verbose=1)","8166419e":"test_labels= validation_generator.labels\ntest_labels","f8d5c8dc":"# Plot confusion matrix \nimport matplotlib.pyplot as plt\n# Note: This code snippet for confusion-matrix is taken directly from the SKLEARN website.\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=30)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual class')\n    plt.xlabel('Predicted class')","9292bfba":"import sklearn.metrics as metrics\nfrom collections import Counter\nimport itertools\n\nvalidation_generator.reset()\nY_pred = model.predict_generator(validation_generator)\ny_pred = np.argmax(Y_pred, axis=-1)\nclasses = validation_generator.classes\n\nprint(sum(y_pred==classes)\/validation_generator.samples)\n\nconfusion_matrix = metrics.confusion_matrix(y_true=classes, y_pred=y_pred) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_matrix, classes = range(6))","34d41b81":"\"\"\"\n#\u00a0UPLOAD YOUR OWN PHOTO AND RUN IT IF YOU WISH\n\n\nimport numpy as np\n\nfrom keras.preprocessing import image\n\n\n \n# predicting images\npath = \"..\/input\/photos\/a1.png\"\nimg = image.load_img(path, target_size=(150, 150))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\nclasses = model.predict(images, batch_size=10)\n\nprint(classes)\n\n\n\"\"\"\n","9c4326a1":"# Prediction of Test set","e3002cf0":"# TRANSFER LEARNING","e600b929":"#  Model building\n","ce22d853":"# Importing Incept\u0131on Model","4480603c":"## Data augmentation"}}