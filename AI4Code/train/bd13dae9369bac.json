{"cell_type":{"c941ab4c":"code","9e8e35bf":"code","dc4cbd29":"code","09675bca":"code","037ce699":"code","5db51722":"code","68831c52":"code","47b45f38":"code","79055e01":"code","48aacb07":"code","9933669c":"code","edf15d7d":"code","4fd778a2":"code","6103e1c7":"code","3b02631e":"code","c20577e8":"code","81d8edb2":"code","2f779333":"code","107da9a5":"code","17cd7f23":"code","12af3566":"code","1bd8cbc9":"code","82a04ab4":"code","97aa85df":"code","d2a031a2":"code","26e59563":"code","c347801f":"code","c74919ed":"code","7a1b7741":"code","f18fd3f4":"code","db59f09d":"code","d084840b":"code","d43c254e":"code","9b15bab8":"code","0e92dace":"code","3c5df390":"code","418ea4be":"code","97a73b5d":"code","1099452e":"code","5d762f5e":"code","9dac31a8":"code","d80c34cc":"code","26ea9db8":"code","9552bb5a":"code","beff7f98":"markdown","abf072f4":"markdown","bc0fa616":"markdown","02a29526":"markdown","22dfa1d4":"markdown","a79b9258":"markdown","cb5390e5":"markdown","5e0ebbe7":"markdown","0835032f":"markdown","8b2fe073":"markdown","61f873a5":"markdown","ef75f5ba":"markdown","10e8445e":"markdown","01a626fb":"markdown","c6af2d04":"markdown","db941170":"markdown","89745850":"markdown","7a8cd574":"markdown","5233aa60":"markdown","61c6cf67":"markdown","cf8d7db3":"markdown","b79c2e05":"markdown","446b8e1a":"markdown","a343911a":"markdown"},"source":{"c941ab4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9e8e35bf":"import pandas as pd\nimport numpy as np","dc4cbd29":"rating_df = pd.read_csv( \"..\/input\/u.data\", delimiter = \"\\t\", header = None )","09675bca":"rating_df.head( 10 )","037ce699":"rating_df.columns = [\"userid\", \"movieid\", \"rating\", \"timestamp\"]","5db51722":"rating_df.head( 10 )","68831c52":"len( rating_df.userid.unique() )","47b45f38":"len( rating_df.movieid.unique() )","79055e01":"rating_df.drop( \"timestamp\", inplace = True, axis = 1 )","48aacb07":"rating_df.head( 10 )","9933669c":"movies_df = pd.read_csv( \"..\/input\/u.item\", delimiter = '\\|', header = None )","edf15d7d":"movies_df = movies_df.iloc[:,:2]\nmovies_df.columns = ['movieid', 'title']","4fd778a2":"movies_df.head( 10 )","6103e1c7":"from sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine, correlation","3b02631e":"user_movies_df = rating_df.pivot( index='userid', columns='movieid', values = \"rating\" ).reset_index(drop=True)","c20577e8":"user_movies_df.fillna(0, inplace = True)","81d8edb2":"user_movies_df.shape","2f779333":"user_movies_df.iloc[10:20, 20:30]","107da9a5":"from IPython.display import Image\nImage(\"..\/input\/recomend-1.png\")","17cd7f23":"from IPython.display import Image\nImage(\"..\/input\/recomend-2.png\")","12af3566":"user_sim = 1 - pairwise_distances( user_movies_df.as_matrix(), metric=\"cosine\" )","1bd8cbc9":"user_sim_df = pd.DataFrame( user_sim )","82a04ab4":"user_sim_df[0:5]","97aa85df":"user_sim_df.idxmax(axis=1)[0:5]","d2a031a2":"np.fill_diagonal( user_sim, 0 )","26e59563":"user_sim_df = pd.DataFrame( user_sim )","c347801f":"user_sim_df[0:5]","c74919ed":"user_sim_df.idxmax(axis=1).sample( 10, random_state = 10 )","7a1b7741":"def get_user_similar_movies( user1, user2 ):\n  common_movies = rating_df[rating_df.userid == user1].merge(rating_df[rating_df.userid == user2], on = \"movieid\", how = \"inner\" )\n\n  return common_movies.merge( movies_df, on = 'movieid' )","f18fd3f4":"get_user_similar_movies( 310, 247 )","db59f09d":"rating_mat = rating_df.pivot( index='movieid', columns='userid', values = \"rating\" ).reset_index(drop=True)","d084840b":"rating_mat.fillna( 0, inplace = True )","d43c254e":"rating_mat.shape","9b15bab8":"rating_mat.head( 10 )","0e92dace":"movie_sim = 1 - pairwise_distances( rating_mat.as_matrix(), metric=\"correlation\" )","3c5df390":"movie_sim.shape","418ea4be":"movie_sim_df = pd.DataFrame( movie_sim )","97a73b5d":"movie_sim_df.head( 10 )","1099452e":"movies_df['similarity'] = movie_sim_df.iloc[0]\nmovies_df.columns = ['movieid', 'title', 'similarity']","5d762f5e":"movies_df.head( 10 )","9dac31a8":"movies_df.sort_values(by='similarity', ascending=False)[1:10]","d80c34cc":"def get_similar_movies( movieid, topN = 5 ):\n  movies_df['similarity'] = movie_sim_df.iloc[movieid -1]\n  top_n = movies_df.sort_values( [\"similarity\"], ascending = False )[0:topN]\n  print( \"Similar Movies to: \", )\n  return top_n","26ea9db8":"get_similar_movies( 118 )","9552bb5a":"get_similar_movies( 127, 10 )","beff7f98":"**Calculating the item distances and similarities**","abf072f4":"**Finding user similarities**","bc0fa616":"**User 310 Vs. User 247**","02a29526":"**Loading Ratings dataset**","22dfa1d4":"**Challenges with User similarity**\nThe challenge with calculating user similarity is the user need to have some prior purchases and should have rated them. This recommendation technique does not work for new users. The system need to wait until the user make some purchases and rates them. Only then similar users can be found and recommendations can be made. This is called cold start problem. This can be avoided by calculating item similarities based how how users are buying these items and rates them together. Here the items are entities and users are dimensions.\n\n**Finding Item Similarity**\nLet's create a pivot table of Movies to Users The rows are movies and columns are users. And the values in the matrix are the rating for a specific movie by a specific user.","a79b9258":"For calculating distances, many similarity coefficients can be calculated. Most widely used similarity coefficients are Euclidean, Cosine, Pearson Correlation etc.\nWe will use cosine distance here. Here we are insterested in similarity. That means higher the value more similar they are. But as the function gives us the distance, we will deduct it from 1.","cb5390e5":"* So a total of 1682 movies and 943 users data is available in the dataset. Let's drop the timestamp columns. We do not need it.","5e0ebbe7":"**Similar movies to Twister**","0835032f":"**Finding User Similarities**","8b2fe073":"That means anyone who buys Toy Story and likes it, the top 3 movies that can be recommender to him or her are Star Wars (1977), Independence Day (ID4) (1996) and Rock, The (1996)\n\n**Utility function to find similar movies**","61f873a5":"The above results show that user are most similar to themselves. But this is not what we want. So, we will fill the diagonal of the matrix (which represent the relationship with self) with 0.\n\n**Setting correlation with self to 0**","ef75f5ba":"**Loading Movies Data**","10e8445e":"**Number of unique users**","01a626fb":"**Number of unique movies**","c6af2d04":"**Fill with 0, where users have not rated the movies**","db941170":"This shows which results are similar to each other. The actual user id will be the index number + 1. That means user 545 is similar to user 757 and so on and so forth.\n\n**Movies similar users like or dislike**\n* We can find the actual movie names and check if the similar users have rated them similarity or differently.","89745850":"**Similar movies to The Godfather**","7a8cd574":"## Calculate the distances\nBased on what users have given ratings to different items, we can calculate the distances between them. Less the distance more similar they are.\n\nFor example, following users have given different ratings to differnt books.\n\nNow, we can find similar users based the distance between user depending on how they have rated the movies. The dimensions are the books and scale is the ratings users have provided.","5233aa60":"## Recommendation Systems\n\n- Knowing \"What customers are most likely to buy in future\" is key to personalized marketing for most of the businesses. Understanding customers past purchase behavior or customer demographics could be key to make future buy predictions. But how to use the customer behavior data, depends on many different algorithms or techniques. Some alogorithms may use demographic information to make this predictions. But most of the times, the orgranizations may not have these kind of information about customers at all. All that organization will have are what customers bought in past or if the liked it or not.\n\n- Recommendation systems use techniques to leverage these information and make recommendation, which has been proved to be very successful. For examples, Amazon.com's most popular feature of \"Customers who bought this also buys this?\"\n\n- Some of the key techiques that recommendation systems use are\n\n        - Association Rules mining\n        - Collaborative Filtering\n        - Matrix Factorization\n        - Page Rank Algorithm\n- We will discuss Collaborative filtering techinque in this article.\n- Two most widely used Collaborative filtering techniques are\n\n        - User Similarity\n        - Item Similarity\n        \n- Here is a nice blog explanation of collaborative filtering.\n- For the purpose of demonstration, we will use the data provided by movilens. It is available here.\n- The dataset contains information about which user watched which movie and what ratings (on a scale of 1 - 5 ) he have given to the movie.","61c6cf67":"**Name the columns**","cf8d7db3":"**Create the pivot table**","b79c2e05":"**Fill '0' for ratings not given by users**","446b8e1a":"**Who is similar to who?**\nUsers with highest similarity values can be treated as similar users.","a343911a":"**Finding similar movies to \"Toy Story\"**"}}