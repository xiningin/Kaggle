{"cell_type":{"eb1c7a74":"code","680c839d":"code","3186fcfc":"code","6e4d4a79":"code","8a615f65":"code","c84531f5":"code","32c6a59d":"code","014f3f9a":"code","c5875317":"code","f260a604":"code","d0f647d5":"code","99b0175f":"code","3fdc035f":"code","c2533169":"code","bd6c2fdd":"code","11b4dfc6":"code","c366b378":"code","2c86b6ae":"code","001dce3e":"code","ce9797c4":"code","e94cdd30":"code","27661b54":"code","f13cfb71":"code","62c58385":"code","4c8d1de3":"code","607c66c7":"code","69c526ec":"code","d5dddc7f":"code","4f1c5120":"code","e30d6b6e":"code","514021a0":"code","d5a90185":"code","14f8a258":"code","8b72bb67":"code","7d08ba50":"code","a59715f3":"code","eed421f0":"code","d8c84184":"code","21e094f4":"code","dd56d46d":"code","d6633360":"code","b9e0627c":"code","46f8346c":"code","0a120873":"code","815cffaa":"code","253abfa4":"code","cb8fe630":"code","555d36bf":"code","9e73242d":"code","7fefdf78":"code","c5b71a5f":"code","8bf95a3b":"code","8b87bcf6":"code","14d28688":"code","c41bf551":"code","74f1b959":"code","a17bcbe0":"code","867fbbd1":"code","9c2402a0":"code","fde15217":"code","0ac0ab7a":"code","00a53043":"code","7edae0f5":"code","036941fa":"code","ff0d6475":"code","0cbdb1eb":"code","643b7155":"code","c5b14c0f":"code","a8c31131":"code","a791e64c":"code","a371409e":"code","5d93dc99":"code","58a5303a":"code","5d7a07bf":"code","38f6f510":"code","0789db19":"code","7b0ad19a":"code","f7d7e158":"code","d550f6bd":"code","48bce56d":"code","d5c870d0":"code","6edf10d6":"code","3020f6ec":"code","56651d99":"code","955573d9":"code","cf68989f":"code","8fc8c2da":"code","36606c48":"code","59d9d247":"code","eddcbc33":"code","226b5e02":"code","ba65de75":"code","8c7492e3":"code","2c34c64c":"code","bfad4a7b":"code","30bed673":"code","d2e72798":"code","be80b0d6":"code","ac9bb8fc":"markdown","6689d2d9":"markdown","a345a487":"markdown","a674523d":"markdown","c86f2aaf":"markdown","58de7ced":"markdown","7cc26a05":"markdown","a6ad843f":"markdown","1ac196b7":"markdown","d7fb8c23":"markdown","495e8989":"markdown","83a58df8":"markdown","f0e007d6":"markdown","c8ce58da":"markdown","9133a453":"markdown","847b6a3e":"markdown","07eeb5cd":"markdown","a3c6b70a":"markdown","93f90a22":"markdown","30941a73":"markdown","1ab0a00e":"markdown","bba7d527":"markdown"},"source":{"eb1c7a74":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","680c839d":"df=pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\n","3186fcfc":"df.head()","6e4d4a79":"df.info()","8a615f65":"df.isnull()","c84531f5":"df.isnull().sum()","32c6a59d":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","014f3f9a":"df.shape","c5875317":"df.region.unique()","f260a604":"#df.describe()\ndf.describe().transpose() #for more organization","d0f647d5":"print(df['age'].median())\nprint(df['bmi'].median())","99b0175f":"df.describe(include=['O'])","3fdc035f":"c=df.corr()\nc","c2533169":"if len(df[df.duplicated()]) > 0:\n    print(\"No. of duplicated entries: \", len(df[df.duplicated()]))\n    print(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())\nelse:\n    print(\"No duplicated entries found\")","bd6c2fdd":"df.drop(df.index[581], inplace=True)","11b4dfc6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(data=df)","c366b378":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()","2c86b6ae":"df.shape[0]","001dce3e":"#convert sex to 1 for female and 0 male:\n\ndf.sex.loc[df['sex']=='female']=1\ndf.sex.loc[df['sex']=='male']=0\n\n#convert smoker to 1 for yes and 0 for no :\ndf.smoker.loc[df['smoker']=='yes']=1\ndf.smoker.loc[df['smoker']=='no']=0\n\n#convert region to numeric :\nregions={'region':{'southwest':1, 'southeast':2, 'northwest':3, 'northeast':4}}\ndf.replace(regions,inplace=True)\ndf.head()\n    ","ce9797c4":"df.to_csv('CleanedInsurance.csv', index = False)","e94cdd30":"#Correlation with output variable\ncor_target = abs(c[\"charges\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.2]\nrelevant_features","27661b54":"plt.figure(figsize=(10,5))\nsns.heatmap(c, annot=True)","f13cfb71":"labels = ['Female', 'Male']\nsize = df['sex'].value_counts()\ncolors = ['red', 'blue']\nexplode = [0, 0.1]\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')\nplt.title('sex', fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","62c58385":"plt.rcParams['figure.figsize'] = (15, 8)\nsns.countplot(df['age'], palette = 'hsv')\nplt.title('Distribution of Age', fontsize = 20)\nplt.show()","4c8d1de3":"sns.catplot(x=\"smoker\", kind=\"count\",hue = 'sex', palette=\"cool\", data=df)","607c66c7":"sns.catplot(x=\"children\", kind=\"count\", palette=\"pink\", data=df, size = 6)","69c526ec":"sns.pairplot(df)\nplt.title('Pairplot for the Data', fontsize = 20)\nplt.show()","d5dddc7f":"sns.scatterplot(x=\"bmi\", y=\"charges\", data=df, palette='Set2', hue='sex')","4f1c5120":"sns.distplot(df['charges'])","e30d6b6e":"sns.distplot(df['age'])","514021a0":"sns.distplot(df['bmi'])","d5a90185":"feature_cols = ['age', 'bmi','children']\n# multiple scatter plots, note that we're not including 'sex' and 'smoker', why? because it is catogorical\u0629\nsns.pairplot(df, x_vars=feature_cols, y_vars='charges', kind='reg')","14f8a258":"#Scatter plot to seeif there is a dependency between attributes smoker and charges accross different ages\nplt.figure(figsize=(8,6))\nsns.scatterplot(df.age, df.charges,hue=df.smoker,palette= ['red','green'] ,alpha=0.6)\nplt.show()","8b72bb67":"#Scatter plot to see if there is a dependency between attributes sex and charges accross different ages\nplt.figure(figsize=(8,6))\nplt.figure(figsize=(8,6))\nsns.scatterplot(df.age, df.charges,hue=df.sex,palette= ['blue','red'] )\nplt.show()","7d08ba50":"#Scatter plot to see if there is a dependency between attributes sex and charges accross different ages\nplt.figure(figsize=(8,6))\nplt.figure(figsize=(8,6))\nsns.scatterplot(df.bmi, df.charges,hue=df.sex,palette= 'Set2' )\nplt.show()","a59715f3":"plt.figure(figsize=(14,6))\nplt.title('Relation between children and Charges')\n#sns.regplot(x=df['children'],y=df['charges'])\nsns.barplot(x=df['children'], y=df['charges'])","eed421f0":"charges = df['charges'].groupby(df.region).sum().sort_values(ascending = True)\nf, ax = plt.subplots(1, 1, figsize=(8, 6))\nax = sns.barplot(charges.head(), charges.head().index, palette='Purples')","d8c84184":"f, ax = plt.subplots(1, 1, figsize=(12, 8))\nax = sns.barplot(x='region', y='charges', hue='sex', data=df, palette='cool')","21e094f4":"ax = sns.lmplot(x = 'age', y = 'charges', data=df, hue='smoker', palette='Set1')\nax = sns.lmplot(x = 'bmi', y = 'charges', data=df, hue='smoker', palette='Set2')\nax = sns.lmplot(x = 'children', y = 'charges', data=df, hue='smoker', palette='Set3')","dd56d46d":"print(df['charges'].min())\nprint(df['charges'].max())\n","d6633360":"df['charges_bins'] = pd.cut(df['charges'], bins=[0, 1300, 26000, 39000, 52000, 65000])\ndf.head()\n","b9e0627c":"#Creating a countplot based on the amount of charges\nplt.figure(figsize=(12,4))\nsns.countplot(x='charges_bins', data=df, palette='husl') \nplt.title('Number of people paying x amount\\n for each charges category', size='23')\nplt.xticks(rotation='25')\nplt.ylabel('Count',size=18)\nplt.xlabel('Charges',size=18)\nplt.show()","46f8346c":"df.head()","0a120873":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","815cffaa":"df.head()","253abfa4":"feature_cols = ['age','sex','bmi','children','smoker','region'] # a lsit of the predictors\nX1 = df[feature_cols] # subsetting our data to only the predictors\ny1 = df['charges'] # our response variable\n#X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1)\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1,train_size=0.75)","cb8fe630":"\n# Pick a new random training and test set\n\nlinreg = LinearRegression()\nlinreg.fit(X_train1, y_train1)\ny_pred = linreg.predict(X_test1)","555d36bf":"# pair the feature names with the coefficients\nzip(feature_cols, linreg.coef_)\nprint(feature_cols)\nprint(linreg.intercept_)\nprint(linreg.coef_)","9e73242d":"# score it on our test set to get a better sense of out of sample performance\nlinreg.score(X_test1, y_test1)","7fefdf78":"linreg.score(X_train1, y_train1)","c5b71a5f":"\nprint('MAE:', metrics.mean_absolute_error(y_test1, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test1, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test1, y_pred)))","8bf95a3b":"diff = y_test1 - y_pred\ndiff.hist(bins = 40)\nplt.title('Histogram of prediction errors')\nplt.xlabel('cost prediction error')\nplt.ylabel('Frequency')","8b87bcf6":"                fig = px.scatter(x=y_test1, y=y_pred, labels={'x': 'ground truth', 'y': 'prediction'})\n                fig.add_shape(\n                    type=\"line\", line=dict(dash='dash'),\n                    x0=y1.min(), y0=y1.min(),\n                    x1=y1.max(), y1=y1.max()\n                )\n                fig.show()","14d28688":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import learning_curve","c41bf551":"### Bundling our previous work into a function ###\ndef learning_curves(estimator, data, features, target, train_sizes, cv):\n    train_sizes, train_scores, validation_scores = learning_curve(\n    estimator, data[features], data[target], train_sizes =\n    train_sizes,\n    cv = cv, scoring = 'neg_mean_squared_error')\n    train_scores_mean = -train_scores.mean(axis = 1)\n    validation_scores_mean = -validation_scores.mean(axis = 1)\n\n    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n\n    plt.ylabel('MSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.legend()\n\n### Plotting the two learning curves ###\n\nfrom sklearn.ensemble import RandomForestRegressor\ntrain_sizes = [1, 100, 500, 800, 900, 1000]\nfeatures = ['age','sex','bmi','children','smoker','region']\ntarget = ['charges']\nplt.figure(figsize = (16,5))\n\nfor model, i in [(DecisionTreeRegressor(), 1), (LinearRegression(),2)]:\n    plt.subplot(1,2,i)\n    learning_curves(model, df, features, target, train_sizes, 5)","74f1b959":"average_charges = df['charges'].mean()\naverage_charges","a17bcbe0":"num_rows = df.shape[0]\nnum_rows","867fbbd1":"null_model_predictions = [average_charges]*num_rows\nnull_model_predictions","9c2402a0":"print('MAE:', metrics.mean_absolute_error(y1, null_model_predictions))\nprint('MSE:', metrics.mean_squared_error(y1, null_model_predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y1, null_model_predictions)))","fde15217":"from sklearn.ensemble import BaggingRegressor\n#from sklearn import tree\n#model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n#model.fit(X_train1, y_train1)\n#model.score(X_test1,y_test1)","0ac0ab7a":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","00a53043":"# from sklearn import model_selection\n# from sklearn.ensemble import BaggingClassifier\n# from sklearn.tree import DecisionTreeClassifier\n\n# seed = 7\n# kfold = model_selection.KFold(n_splits=10, random_state=seed)\n# cart = DecisionTreeClassifier()\n# num_trees = 100\n# model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n# results = model_selection.cross_val_score(model, X1, y1, cv=kfold)\n# print(results.mean())","7edae0f5":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","036941fa":"from sklearn.tree import DecisionTreeRegressor\nestimators = list(range(1, 20))\naccuracyTest = []\naacuracyTrain=[]\nfor n_estimators in estimators:\n    clf = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n                            max_samples=0.2,\n                            n_estimators=n_estimators).fit(X_train1, y_train1)\n    acc = clf.score(X_test1, y_test1)\n    accuracyTest.append(acc)\n    acc1 = clf.score(X_train1, y_train1)\n    aacuracyTrain.append(acc1)\n    fig = px.scatter(x=y_test1, y=clf.predict(X_test1), labels={'x': 'ground truth', 'y': 'prediction'})\n    fig.add_shape(\n    type=\"line\", line=dict(dash='dash'),\n    x0=y1.min(), y0=y1.min(),\n    x1=y1.max(), y1=y1.max()\n    )\n    fig.show()\n\nplt.plot(estimators, accuracyTest)\nplt.xlabel(\"Number of estimators\")\nplt.ylabel(\"Accuracy\")\nplt.show()","ff0d6475":"accuracyTest","0cbdb1eb":"np.mean(accuracyTest)","643b7155":"np.max(accuracyTest)","c5b14c0f":"np.max(aacuracyTrain)","a8c31131":"# clf1 = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n#                             n_estimators=10,\n#                             bootstrap=False,\n#                             bootstrap_features=False,\n#                             random_state=5).fit(X_train1, y_train1)","a791e64c":"#clf1.estimators_","a371409e":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()","5d93dc99":"q1 = df['charges'].quantile(0.25)\nq3=df['charges'].quantile(0.75)\niqr = q3 - q1","58a5303a":"charges = df[df['charges']< (q1 - 1.5 * iqr)]\ncharges = df[df['charges']> (q3 + 1.5 * iqr)]\ncharges.describe()","5d7a07bf":"dfWithoutOutlier = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndfWithoutOutlier.shape","38f6f510":"dfWithoutOutlier.head()","0789db19":"dfWithoutOutlier.describe()","7b0ad19a":"feature_cols = ['age','sex','bmi','children','smoker','region'] # a lsit of the predictors\nX2 = dfWithoutOutlier[feature_cols] # subsetting our data to only the predictors\ny2 = dfWithoutOutlier['charges'] # our response variable\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1)\n# Pick a new random training and test set\n\nlinreg = LinearRegression()\nlinreg.fit(X_train2, y_train2)\ny_pred = linreg.predict(X_test2)\n# score it on our test set to get a better sense of out of sample performance\nlinreg.score(X_test2, y_test2)","f7d7e158":"linreg.score(X_train2, y_train2)","d550f6bd":"# mean=df['charges'].mean()\n# median=df['charges'].median()\n# print(median)\n# #we will consider any value higher than mean as expensive (1) otherwise normal price(0).\n\ndf2=df\ndf2.head()","48bce56d":"df2.loc[df['charges'] <= 9596, 'charges'] = 0 #on or under avarage\ndf2.loc[df['charges'] > 9596, 'charges'] = 1 # above avarage\ndf2","d5c870d0":"df3=dfWithoutOutlier\ndf3.loc[df3['charges'] <= 9596, 'charges'] = 0 #on or under avarage\ndf3.loc[df3['charges'] > 9596, 'charges'] = 1 # above avarage\ndf3","6edf10d6":"df2.groupby(['charges']).count()","3020f6ec":"sns.catplot(x=\"charges\", kind=\"count\", palette=\"cool\", data=df3)","56651d99":"sns.catplot(x=\"charges\", kind=\"count\", palette=\"cool\", data=df2)","955573d9":"feature_cols = ['age','sex','bmi','children','smoker','region'] # a lsit of the predictors\nX = df2[feature_cols] # subsetting our data to only the predictors\ny = df2['charges'] # our response variable","cf68989f":"y","8fc8c2da":"#split data to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nprint (X_train.shape,X_test.shape,y_train.shape, y_test.shape)","36606c48":"from sklearn.linear_model import LogisticRegression\n# instantate our model\nlogreg = LogisticRegression()\n# fit our model to our training set\nlogreg.fit(X_train, y_train)","59d9d247":"# score it on our test set to get a better sense of out of sample performance\nlogreg.score(X_test, y_test)\n","eddcbc33":"logreg.score(X_train, y_train)","226b5e02":"# pair the feature names with the coefficients\nzip(feature_cols, logreg.coef_)\nprint(feature_cols)\nprint(logreg.intercept_)\nprint(logreg.coef_)","ba65de75":"feature_cols = ['age','sex','bmi','children','smoker','region'] # a lsit of the predictors\nX4 = df3[feature_cols] # subsetting our data to only the predictors\ny4 = df3['charges'] # our response variable\n#split data to train and test\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4)\n# instantate our model\nlogreg = LogisticRegression()\n# fit our model to our training set\nlogreg.fit(X_train4, y_train4)\nlogreg.score(X_test4, y_test4)","8c7492e3":"from sklearn.metrics import confusion_matrix\nLR_prediction = logreg.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test.tolist(), LR_prediction.tolist())\nprint(confusion_matrix)","2c34c64c":"class_names=[1,0] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\n\nsns.heatmap(pd.DataFrame(confusion_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\n\nplt.tight_layout()\nplt.figure(figsize=(5,2))\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","bfad4a7b":"error = pd.DataFrame({'Actual': y_test, 'Predicted': LR_prediction})\nerror.head(20)","30bed673":"from sklearn import metrics\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","d2e72798":"from sklearn.metrics import mean_squared_error\n# Compute error between our test predictions and the actual values.\nmean_squared_error(LR_prediction, y_test.tolist())","be80b0d6":"\n# generate class probabilities\nprobs = logreg.predict_proba(X_test)\nprint (probs)","ac9bb8fc":"### * 1- linear regression \n### * 2-linear regression -after removing the outliers- \n### * 3-Ensemble - bagging-\n### * 4-Lofistic regression \n### * 5-logistic regression -after removing the outliers- ","6689d2d9":"### Start Regression","a345a487":"### Frome graph we can notice thet there is some outliers in charges attributes ! , However we will see accuracy with and without removing outliers to if see our models are roubust to them and give a good accuracy? or not ?","a674523d":"### 1- build model without removing outliers!","c86f2aaf":"# Health insurance cost prediction","58de7ced":"# Build different 5 models on insurance data :\n","7cc26a05":"![Insurance cost](https:\/\/www.getinsuredonline.com\/wp-content\/uploads\/2020\/08\/the-importance-of-having-health-insurance.jpg)","a6ad843f":"### Train data using ensemble (bagging) and see its performance !\n","1ac196b7":"### Check duplicates rows then drop them !","d7fb8c23":"### No null values found in the dataset!","495e8989":"#### From above and correaltopn tabe we can see that there is no strong corelations on the data , However there are many cases where  variables might not show a strong bivariate correlation but may show a strong association in regression , so lets build our model and see the accuracy!","83a58df8":"### Plot to discover outliers!","f0e007d6":"### Train data with linear regression again BUT after removing outliers to see their affect in the model !","c8ce58da":"## visualisation","9133a453":"### Extract unique values of region attribute so later we can conert to numeric and use it in the model!","847b6a3e":"So, we are beating the null model ","07eeb5cd":"# Logistic Regrssion :","a3c6b70a":"### From graph we can make sure there are no null values !","93f90a22":"#### The accuracy very bad as a random model !! which indicates that the outliers must not remove and they are significant ","30941a73":"### I chose the threshold based on some articles and resourcese that reference the avg cost of the insurance in US is 9596","1ab0a00e":"## Witout outliers !","bba7d527":"### using null model "}}