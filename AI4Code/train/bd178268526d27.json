{"cell_type":{"a0f07317":"code","bde8ed8c":"code","36859322":"code","99bdc210":"code","57e8485e":"code","540ac3e0":"code","14ef81ba":"code","58934bde":"code","be8a6314":"code","fd869da8":"code","fefbbf1c":"code","13ae9b18":"code","c6fe7f1d":"code","a8fe9bbc":"code","ba878c07":"code","1688a474":"code","d4ee4f4f":"code","56e7c17f":"code","7b3bb0ed":"code","ffdfd014":"code","dc8c2a35":"code","104a074a":"code","fe83e938":"code","9e4aedcb":"code","cbd8444a":"code","918bc40c":"code","39da1ceb":"code","7e98a9eb":"code","9d899b32":"code","87a159cd":"code","8f7a4e5a":"code","d0799441":"code","b8ff62f3":"code","e7ac043d":"code","8b4f3e35":"code","ad79e646":"code","dfac8100":"code","a58fc04d":"code","795f5c66":"code","4075b3d4":"markdown","1ba91e11":"markdown","5e3bba76":"markdown","abb41f7f":"markdown"},"source":{"a0f07317":"import os\nimport numpy as np\nnp.random.seed(69)\nimport pandas as pd\nimport random\nimport pickle as pkl\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport seaborn as sns\nsns.set()\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,concatenate, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, ZeroPadding2D, LeakyReLU, ReLU, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import load_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n# import kerastuner as kt\n# from kerastuner import HyperModel\nimport time","bde8ed8c":"df = pd.read_csv(\"..\/input\/paper-dataset\/paperDF.csv\",index_col=0)","36859322":"# # Already have the colours'u_g', 'g_r', 'r_i', 'i_z'\n# # MORE COLOURS CAN BE OBTAINED FROM BELOW CODE\n# def get_color(df2,c1,c2):\n#     df = df2.copy()\n#     df[f\"{c1}_{c2}\"] = (df[f\"dered_{c1}\"]+df[f\"extinction_{c1}\"]) - (df[f\"dered_{c2}\"]+df[f\"extinction_{c2}\"])\n#     return df\n\n    \n# bands = [\"u\",\"g\",\"r\",\"i\",\"z\"]\n# for c1 in bands:\n#     for c2 in bands:\n#         if c1==c2:\n#             continue\n#         df = get_color(df,c1,c2)","99bdc210":"photodf = df.loc[:,['dered_u', 'deVRad_u', 'psffwhm_u', 'extinction_u',\n       'dered_g', 'deVRad_g', 'psffwhm_g', 'extinction_g', 'dered_r',\n       'deVRad_r', 'psffwhm_r', 'extinction_r', 'dered_i', 'deVRad_i',\n       'psffwhm_i', 'extinction_i', 'dered_z', 'deVRad_z', 'psffwhm_z',\n       'extinction_z', 'u_g', 'g_r', 'r_i', 'i_z']]\n\nobjlist = np.load(\"..\/input\/paperobjlist\/paperobjlist.npy\")\n\ndnnx=[]\ndnny=[]\nfor i,objnum in tqdm(enumerate(objlist),total=len(objlist)):\n    dnny.append(df.loc[objnum,\"class\"])\n    dnnx.append(photodf.loc[objnum].values)\ndnny=np.array(dnny)\ndnnx=np.array(dnnx)","57e8485e":"del(df,photodf)","540ac3e0":"X = np.load(\"..\/input\/paper-dataset\/paperX.npy\")\ny = dnny.copy()","14ef81ba":"idx_drop = np.where(dnny==\"QSO\")[0]","58934bde":"X = np.delete(X,idx_drop,axis=0)\ndnnx = np.delete(dnnx,idx_drop,axis=0)\ny = np.delete(y,idx_drop,axis=0)\nobjlist = np.delete(objlist,idx_drop,axis=0)\n\ndel(dnny)","be8a6314":"y, label_strings = pd.factorize(y,sort=True)\ny = to_categorical(y)\nprint(label_strings)","fd869da8":"zipX = list(zip(X, dnnx))\nzipy = list(zip(y, objlist))\n\nzipX_train, zipX_test, zipy_train, zipy_test = train_test_split(zipX, zipy, test_size = 0.125,random_state=42)\nzipX_train, zipX_val, zipy_train, zipy_val = train_test_split(zipX_train, zipy_train, test_size = 0.1428, random_state=42)\n\nX_train, dnnx_train = zip(*zipX_train)\nX_val, dnnx_val = zip(*zipX_val)\nX_test, dnnx_test = zip(*zipX_test)\n\ny_train, objlist_train = zip(*zipy_train)\ny_val, objlist_val = zip(*zipy_val)\ny_test, objlist_test = zip(*zipy_test)\n\nX_train = np.array(X_train)\nX_val = np.array(X_val)\nX_test = np.array(X_test)\n\ndnnx_train = np.array(dnnx_train)\ndnnx_val = np.array(dnnx_val)\ndnnx_test = np.array(dnnx_test)\n\ny_train = np.array(y_train)\nobjlist_train = np.array(objlist_train)\ny_val = np.array(y_val)\nobjlist_val = np.array(objlist_val)\ny_test = np.array(y_test)\nobjlist_test = np.array(objlist_test)\n\n\ndel(zipX,zipX_test,zipX_train,zipX_val, X, zipy, zipy_test, zipy_train, zipy_val, objlist)","fefbbf1c":"print(dnnx_train.shape)\nprint(dnnx_val.shape)\nprint(dnnx_test.shape)","13ae9b18":"def get_metrics(y_pred, y_test, labels, to_print=True):\n    correct_labels = np.where(y_pred==y_test)[0]\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred,average='macro')\n    recall = metrics.recall_score(y_test, y_pred,average='macro')\n    f1score = metrics.f1_score(y_test, y_pred,average='macro')\n    # rocscore = metrics.roc_auc_score(y_test, y_pred,average='micro',multi_class=\"ovo\")\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)  \n    classification_report = metrics.classification_report(y_test, y_pred)\n\n    if to_print:\n        print(\"Identified {} correct labels out of {} labels\".format(len(correct_labels), y_test.shape[0]))\n        print(\"Accuracy:\",accuracy)\n        print(\"Precision:\",precision)\n        print(\"Recall:\",recall)\n        print(\"F1 Score:\",f1score)\n        # print(\"ROC AUC Score:\",rocscore)\n        print(f\"Labels are: {labels}\")\n        print(\"Confusion Matrix:\\n\", confusion_matrix)\n        print(\"Classification_Report:\\n\", classification_report)\n\n    return (correct_labels, accuracy, precision, recall, confusion_matrix, classification_report)\n\ndef plot_model_change(history,fname=\"time.pdf\"):\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'],label=\"Training Acc\")\n    plt.plot(history.history['val_accuracy'],label=\"Val Acc\")\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend()\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'],label=\"Training Loss\")\n    plt.plot(history.history['val_loss'],label=\"Val Loss\")\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend()\n    plt.savefig(fname)\n    plt.show()","c6fe7f1d":"model = Sequential()\n\nmodel.add(Dense(1024, activation=\"sigmoid\", input_dim=dnnx_train.shape[1]))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation=\"sigmoid\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation=\"sigmoid\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64, activation=\"sigmoid\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(32, activation=\"sigmoid\"))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\nes = EarlyStopping(monitor='val_loss', verbose=0, patience=100, restore_best_weights=True)\ncb = [es]\nhistory = model.fit(dnnx_train, y_train,\n                    batch_size=2048,\n                    epochs = 4000,\n                    validation_data = (dnnx_val,y_val),\n                    callbacks = cb,\n                    verbose = 2)","a8fe9bbc":"model.save(\"DNNClassifier.h5\")","ba878c07":"plot_model(model,\"DNNMod.pdf\",show_shapes=True)","1688a474":"plot_model_change(history,fname=\"DNNTraining.pdf\")","d4ee4f4f":"preds_test = model.predict(dnnx_test,batch_size=2048, verbose = 0)\nprint(get_metrics(preds_test.argmax(axis=1), y_test.argmax(axis=1),label_strings))","56e7c17f":"cm = metrics.confusion_matrix(preds_test.argmax(axis=1), y_test.argmax(axis=1),normalize='true')\ndf_cm = pd.DataFrame(cm, index = label_strings,columns = label_strings)\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True,cmap=\"Blues\",square=True,fmt='.2%')\nplt.savefig(\"dnn_cm.pdf\")","7b3bb0ed":"del(dnnx_train)","ffdfd014":"inp_layer = tf.keras.Input(shape=X_train.shape[1:])\n\nmod = Conv2D(filters=64, kernel_size=(5,5), padding='same')(inp_layer)\nmod = ReLU()(mod)\n\n\n# mod = AveragePooling2D(pool_size=(2, 2), strides=2)(mod)\n\nc1 = Conv2D(filters=48, kernel_size=(1,1), padding='same')(mod)\nc1 = ReLU()(c1)\nc2 = Conv2D(filters=48, kernel_size=(1,1), padding='same')(mod)\nc2 = ReLU()(c2)\nc3 = Conv2D(filters=48, kernel_size=(1,1), padding='same')(mod)\nc3 = ReLU()(c3)\nc4 = Conv2D(filters=64, kernel_size=(1,1), padding='same')(c1)\nc4 = ReLU()(c4)\nc5 = Conv2D(filters=64, kernel_size=(3,3), padding='same')(c1)\nc5 = ReLU()(c5)\nc6 = Conv2D(filters=64, kernel_size=(5,5), padding='same')(c2)\nc6 = ReLU()(c6)\np1 = AveragePooling2D(pool_size=(1, 1))(c3)\nmod = concatenate([c4,c5,c6,p1])\n\nc7 = Conv2D(filters=64, kernel_size=(1,1), padding='same')(mod)\nc7 = ReLU()(c7)\nc8 = Conv2D(filters=64, kernel_size=(1,1), padding='same')(mod)\nc8 = ReLU()(c8)\nc9 = Conv2D(filters=64, kernel_size=(1,1), padding='same')(mod)\nc9 = ReLU()(c9)\nc10 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(c7)\nc10 = ReLU()(c10)\nc11 = Conv2D(filters=92, kernel_size=(3,3), padding='same')(c7)\nc11 = ReLU()(c11)\nc12 = Conv2D(filters=92, kernel_size=(5,5), padding='same')(c8)\nc12 = ReLU()(c12)\np2 = AveragePooling2D(pool_size=(1, 1))(c9)\nmod = concatenate([c10,c11,c12,p2])\nmod = AveragePooling2D(pool_size=(2, 2))(mod)\n\nc13 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc13 = ReLU()(c13)\nc14 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc14 = ReLU()(c14)\nc15 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc15 = ReLU()(c15)\nc16 = Conv2D(filters=128, kernel_size=(1,1), padding='same')(c13)\nc16 = ReLU()(c16)\nc17 = Conv2D(filters=128, kernel_size=(3,3), padding='same')(c13)\nc17 = ReLU()(c17)\nc18 = Conv2D(filters=128, kernel_size=(5,5), padding='same')(c14)\nc18 = ReLU()(c18)\np3 = AveragePooling2D(pool_size=(1, 1))(c15)\nmod = concatenate([c16,c17,c18,p3])\n\nc19 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc19 = ReLU()(c19)\nc20 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc20 = ReLU()(c20)\nc21 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc21 = ReLU()(c21)\nc22 = Conv2D(filters=128, kernel_size=(1,1), padding='same')(c19)\nc22 = ReLU()(c22)\nc23 = Conv2D(filters=128, kernel_size=(3,3), padding='same')(c19)\nc23 = ReLU()(c23)\nc24 = Conv2D(filters=128, kernel_size=(5,5), padding='same')(c20)\nc24 = ReLU()(c24)\np4 = AveragePooling2D(pool_size=(1, 1))(c21)\nmod = concatenate([c22,c23,c24,p4])\nmod = AveragePooling2D(pool_size=(2, 2))(mod)\n\nc25 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc25 = ReLU()(c25)\nc26 = Conv2D(filters=92, kernel_size=(1,1), padding='same')(mod)\nc26 = ReLU()(c26)\nc27 = Conv2D(filters=128, kernel_size=(1,1), padding='same')(mod)\nc27 = ReLU()(c27)\nc28 = Conv2D(filters=128, kernel_size=(3,3), padding='same')(c25)\nc28 = ReLU()(c28)\np5 = AveragePooling2D(pool_size=(1, 1))(c26)\nmod = concatenate([c27,c28,p5])\nmod = Flatten()(mod)    #Check\nmod = Dense(1024)(mod)\nmod = Dense(1024)(mod)\nout_layer = Dense(2, activation=\"softmax\") (mod)\nmodel = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n\nmodel.compile(optimizer = 'adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","dc8c2a35":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)\ndatagen.fit(X_train)\n\n\nes = EarlyStopping(monitor='val_loss', verbose=1, patience=30, restore_best_weights=True)\n\ncb = [es]\n","104a074a":"history = model.fit(datagen.flow(X_train,y_train, batch_size=512),\n                              epochs = 300, validation_data = (X_val,y_val),\n                              callbacks = cb,\n                              verbose = 1)","fe83e938":"model.save(\"CNNClassifier.h5\")","9e4aedcb":"plot_model(model,\"CNNMod.pdf\",show_shapes=True)","cbd8444a":"plot_model_change(history,fname=\"CNNTraining.pdf\")","918bc40c":"preds_test = model.predict(X_test,batch_size=1024, verbose = 0)\nprint(get_metrics(preds_test.argmax(axis=1), y_test.argmax(axis=1),label_strings))","39da1ceb":"cm = metrics.confusion_matrix(preds_test.argmax(axis=1), y_test.argmax(axis=1),normalize='true')\ndf_cm = pd.DataFrame(cm, index = label_strings,columns = label_strings)\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True,cmap=\"Blues\",square=True,fmt='.2%')\nplt.savefig(\"cnn_cm.pdf\")","7e98a9eb":"del(X_train)","9d899b32":"cnnclassifier = load_model(\".\/CNNClassifier.h5\")\ndnnclassifier = load_model(\".\/DNNClassifier.h5\")","87a159cd":"def define_stacked_model(members):\n    # update all layers in all models to not be trainable\n    for i in range(len(members)):\n        model = members[i]\n        for layer in model.layers:\n            # make not trainable\n            layer.trainable = False\n            # rename to avoid 'unique layer name' issue\n            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n    # define multi-headed input\n    ensemble_visible = [model.input for model in members]\n    # concatenate merge output from each model\n    ensemble_outputs = [model.output for model in members]\n    merge = tf.keras.layers.concatenate(ensemble_outputs)\n    hidden = Dense(10, activation='relu')(merge)\n    output = Dense(2, activation='softmax')(hidden)\n    model = tf.keras.Model(inputs=ensemble_visible, outputs=output)\n    # plot graph of ensemble\n    plot_model(model, show_shapes=True, to_file='model_graph.png')\n    # compile\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","8f7a4e5a":"# define ensemble model\nmembers = [cnnclassifier,dnnclassifier]\nmodel = define_stacked_model(members)","d0799441":"filepath=\"EnsembleClassifier.h5\"\n\ncheckpointcb = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,monitor='loss',mode='min',save_best_only=True,verbose=1,save_weights_only=False)\ncb = [checkpointcb]\n","b8ff62f3":"history = model.fit([X_val, dnnx_val],\n                            y_val, epochs=100,\n                            batch_size=512,\n                            callbacks=cb,\n                            verbose=1)","e7ac043d":"del(X_val, dnnx_val)","8b4f3e35":"model = load_model(\".\/EnsembleClassifier.h5\")","ad79e646":"model.evaluate([X_test, dnnx_test],y_test)","dfac8100":"plot_model(model,\"EnsembleMod.pdf\",show_shapes=True)","a58fc04d":"preds_test = model.predict([X_test, dnnx_test],batch_size=512, verbose = 0)\nprint(get_metrics(preds_test.argmax(axis=1), y_test.argmax(axis=1),label_strings))","795f5c66":"cm = metrics.confusion_matrix(preds_test.argmax(axis=1), y_test.argmax(axis=1),normalize='true')\ndf_cm = pd.DataFrame(cm, index = label_strings,columns = label_strings)\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True,cmap=\"Blues\",square=True,fmt='.2%')\nplt.savefig(\"ensemble_cm.pdf\")","4075b3d4":"# Ensemble","1ba91e11":"# DNN Classifier","5e3bba76":"# Create Sets","abb41f7f":"# CNN Classifier"}}