{"cell_type":{"6745218a":"code","7445e92c":"code","52c78356":"code","1e2e0b06":"code","eea78390":"code","de18061d":"code","13835fd6":"code","ceadca99":"code","f2965bbd":"markdown","493ece7f":"markdown","94c4d65e":"markdown","63228d19":"markdown","dbaa284d":"markdown","444bf29a":"markdown"},"source":{"6745218a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm.notebook import tqdm","7445e92c":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","52c78356":"def housing_clean(train_data, test_data, eda=False):\n    \"\"\"\n    Provides basic cleaning and processing of the House Prices data.\n    param train_data: pandas.DataFrame, input training data\n    param test_data: pandas.DataFrame, input test data\n    param eda: bool, set True to return combined dataframe before scaling for EDA purposes.\n    \"\"\"\n    \n    # First, let's copy our data so that we don't change the original dataframes\n    \n    train = train_data.copy()\n    test = test_data.copy()\n    \n    # Save column names by type\n    \n    categorical = ['MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', \n                   'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n                  'Exterior2nd', 'MasVnrType', 'Foundation', 'HeatingQC', 'CentralAir', 'Electrical', 'GarageType',\n                  'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\n    numerical = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', \n                 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF',\n                '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n                'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', \n                'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n                'ScreenPorch', 'PoolArea', 'MiscVal']\n    \n    years = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n    \n    # Columns for ordinal encoding: order of values in lists\n    # will correspond to the order we will encode them into.\n    # I.e., values of 'IR3' for column 'LotShape' will instead be equal to 0, \n    # 'IR2' - to 1, 'IR1' - to 2 and 'Reg' - to 3.\n    # These are all the columns where it is possible to assign ordinality.\n    # In other words, for these columns we can always say when one value\n    # is better than another. I.e., we know that 'Excellent' is better than\n    # 'Good', which in turn is better than 'Fair' and so on.\n    ordinal = {'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n               'LandSlope': ['Sev', 'Mod', 'Gtl'],\n               'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],\n               'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n               'BsmtFinType2': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n               'Heating': ['Floor', 'Grav', 'Wall', 'OthW', 'GasW', 'GasA'],\n               'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'KitchenQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'Functional': ['None', 'Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n               'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'GarageFinish': ['None', 'Unf', 'RFn', 'Fin'],\n               'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n               'PavedDrive': ['N', 'P', 'Y'],\n               'PoolQC': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']}\n        \n    # Store the names of the columns which we will be dropping.\n    to_drop = ['MSSubClass', 'TotalBsmtSF', 'Utilities']\n    \n    # There are two columns which describe the type of Exterior:\n    # Exterior1st and Exterior2nd. They can be combined together in \n    # one-hot fashion, but if the value exist in both columns,\n    # the corresponding one-hot column will have '2' instead of '1' in it.\n    # We'll save the unique value from both columns here\n    exterior_cols = train['Exterior1st'].unique()[~np.isin(train['Exterior1st'].unique(), ['Other', 'None'])]\n    \n    # Select the target column into a separate variable which the function will return\n    train_target = pd.DataFrame(np.log1p(train['SalePrice']))\n    if not eda:\n        train.drop('SalePrice', axis=1, inplace=True)\n    \n    # We will apply transformations to both dataframes\n    dfs = {'train': train, 'test': test}\n    \n    for df, df_obj in dfs.items(): # needs a dict here; otherwise pd.concat won't work\n\n        categorical_copy = categorical.copy()\n        numerical_copy = numerical.copy()\n        years_copy = years.copy()\n        to_drop_copy = to_drop.copy()\n        \n        # Change years to years from 2010; i.e. age to 2010\n        for col in years_copy:\n            dfs[df][col + 'Rel'] = 2010 - dfs[df][col]\n            to_drop_copy.append(col)\n            numerical_copy.append(col + 'Rel')\n\n        # Correct erroneous Exterior2nd values\n        for incorr, corr in zip(['CmentBd', 'Wd Shng', 'Brk Cmn'], ['CemntBd', 'WdShing', 'BrkComm']):\n            dfs[df].loc[dfs[df]['Exterior2nd'] == incorr, 'Exterior2nd'] = corr\n            \n        # Month and year sold - to months from 12.2010\n        dfs[df]['MoSoldRel'] = dfs[df]['YrSoldRel'] * 12 + (12 - dfs[df]['MoSold'])\n        numerical_copy.remove('YrSoldRel')\n        to_drop_copy.extend(['YrSoldRel', 'MoSold'])\n        numerical_copy.append('MoSoldRel')\n\n        # Replace NaNs: by column type\n        na_replace = {**{col: 'None' for col in categorical_copy}, **{col: 0 for col in numerical_copy}, \n                      **{col: 'None' for col in ordinal.keys()}, **{col: 2010 for col in years_copy}}\n        dfs[df].fillna(na_replace, inplace=True)\n\n        # Convert ordinal columns to categorical\n        for col, order in ordinal.items():\n            dfs[df][col] = pd.Categorical(dfs[df][col], categories=order)\n\n        # Merge Condition1 and Condition2 (same logic as Exterior)\n        condition_cols = dfs[df]['Condition1'].unique()[~np.isin(dfs[df]['Condition1'].unique(), ['Norm'])]\n        condition_df = pd.DataFrame(np.zeros((dfs[df].shape[0], len(condition_cols)), dtype='int'), columns=condition_cols)\n        for col in condition_cols:\n            cond1_tf = np.where(dfs[df]['Condition1'] == col, 1, 0)\n            cond2_tf = np.where(dfs[df]['Condition2'] == col, 1, 0)\n            condition_df[col] = condition_df[col] + cond1_tf + cond2_tf\n        dfs[df] = pd.concat([df_obj, condition_df], axis=1)\n        to_drop_copy.extend(['Condition1', 'Condition2'])\n        categorical_copy.remove('Condition1')\n        categorical_copy.remove('Condition2')\n\n        # Merge Exterior1st and Exterior2nd\n        exterior_df = pd.DataFrame(np.zeros((dfs[df].shape[0], len(exterior_cols)), dtype='int'), columns=exterior_cols)\n        for col in exterior_cols:\n            ext1_tf = np.where(dfs[df]['Exterior1st'] == col, 1, 0)\n            ext2_tf = np.where(dfs[df]['Exterior2nd'] == col, 1, 0)\n            exterior_df[col] = exterior_df[col] + ext1_tf + ext2_tf\n        dfs[df] = pd.concat([df_obj, exterior_df], axis=1)\n        to_drop_copy.extend(['Exterior1st', 'Exterior2nd'])\n        categorical_copy.remove('Exterior1st')\n        categorical_copy.remove('Exterior2nd')\n\n        # Drop columns which needed to be dropped\n        dfs[df].drop(columns=to_drop_copy, axis=1, inplace=True)\n\n        # Map ordinal to numbers; we'll use the method DataFrameMapper\n        # from sklearn_pandas to map OrdinalEncoding to each categorical column.\n        mapper_df = DataFrameMapper([\n            ([col], OrdinalEncoder(categories = [cat], dtype='uint8')) for col, cat in ordinal.items()\n        ], df_out=True)\n        ord_df = mapper_df.fit_transform(dfs[df].copy())\n        for col in ordinal.keys():\n            dfs[df][col] = ord_df[col]\n    \n    categorical = categorical_copy\n    \n    if not eda:\n        \n        train_len = dfs['train'].shape[0]\n        combine = dfs['train'].append(dfs['test'])\n        combine = pd.get_dummies(combine, columns=categorical, drop_first=True)\n        \n        # Add new feature: TotalSF\n        # Sum of all living area in square feet.\n        combine['TotalSF'] = combine['GrLivArea'] + combine['BsmtFinSF1'] + combine['BsmtFinSF2'] + \\\n                             combine['BsmtUnfSF'] + combine['1stFlrSF'] + combine['2ndFlrSF'] + \\\n                             combine['LowQualFinSF']\n        \n        # Because all our numeric features model real life - prices, area etc.,\n        # they are very close to lognorm distribution. So let's take logarithm\n        # to normalize them and remove skew.\n        to_log = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n                  'EnclosedPorch', 'ScreenPorch', 'TotalSF']\n        for col in to_log:\n            combine[col] = np.log1p(combine[col])\n\n        # Apply StandardScaler to standardize features\n        ss = StandardScaler().fit_transform(combine.iloc[:, 1:])\n        \n        combine = pd.DataFrame(ss, columns=combine.columns[1:])\n        \n        train, test = combine.iloc[:train_len, :], combine.iloc[train_len:, :]\n    \n    else:\n        train, test = dfs['train'], dfs['test']\n\n    test_id = dfs['test']['Id'].astype('int')\n    \n    return train, test, train_target, test_id.values\n\ntrain, test, train_target, test_id = housing_clean(train_data, test_data)","1e2e0b06":"train.shape","eea78390":"train_cp, _, train_target_cp, _ = housing_clean(train_data, test_data)\n# Store mean scores and mean standard deviations\nmean_scores, mean_stds = [], []\n# We will use Ridge regression, because pure LinearRegression produces unstable outputs\nlinreg = Ridge(random_state=42, alpha=100)\n# Pre-fit data\nlinreg.fit(train_cp, train_target_cp)\n# Get the list of coefficients\ncoefs = {k: v for k, v in zip(train_cp.columns, linreg.coef_[0])}\n# Sort them by module\nsorted_coefs = sorted(coefs.items(), key=lambda x: abs(x[1]))\ncols = [item[0] for item in sorted_coefs]\n# Iterate through all the coefficients\nfor col in tqdm(cols):\n    # Fit a new model\n    linreg = Ridge(random_state=42)\n    # Calculate cross validated score by measuring RMSE\n    scores = cross_val_score(linreg, train_cp, train_target_cp, cv=10, scoring='neg_root_mean_squared_error')\n    mean_scores.append(scores.mean())\n    mean_stds.append(scores.std())\n    # Drop one column\n    train_cp.drop(col, axis=1, inplace=True)","de18061d":"# Plot our results\nmean_scores, mean_stds = np.array(mean_scores), np.array(mean_stds)\nax = sns.lineplot(x=list(range(len(mean_scores))), y=mean_scores)\nax.fill_between(x=list(range(len(mean_scores))), y1=mean_scores - mean_stds, y2=mean_scores + mean_stds, alpha=0.5)\nax.axhline(max(mean_scores), color='red')\nprint(mean_scores.max(), np.argmax(mean_scores))","13835fd6":"linreg = Ridge(random_state=42, alpha=100)\nlinreg.fit(train, train_target)\ncoefs = {k: v for k, v in zip(train.columns, linreg.coef_[0])}\nsorted_coefs = sorted(coefs.items(), key=lambda x: abs(x[1]))\nto_drop = [item[0] for item in sorted_coefs[:113]]\ntrain.drop(to_drop, axis=1, inplace=True)\ntest.drop(to_drop, axis=1, inplace=True)","ceadca99":"linreg = Ridge(random_state=42, alpha=100)\nlinreg.fit(train, train_target)\npred = linreg.predict(test).reshape(-1,)\npred = np.exp(pred) - 1 # take the exponent of the predictions\nsubm = np.vstack((test_id, pred)).T\npd.DataFrame(subm, columns=['Id', 'SalePrice']).astype('int').to_csv('subm_final.csv', index=False)","f2965bbd":"Now let's do the following. We have 175 features after applying all our transformations. It is highly likely, that our Linear Regression will have difficulties fitting all the features and generating a reasonable model. So let's find out exactly how many features we need to drop. We will be dropping features, starting from the least important ones. We will decide on the importance of the features by analyzing the coefficients by pre-applying linear regression to the data, dropping the columns one by one and calculating RMSE.","493ece7f":"All the basic data cleaning and processing will take place inside one function for convenience.\nFor explanation, see comments inside the function.","94c4d65e":"In this notebook we will try to predict house prices for the ongoing competition [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/).<br>\nWe will be doing some extensive data cleaning, basic data analysis and data engineering and will apply the basic linear model to our data, to get a total score of ~0.12, which at the moment of writing this (April 2021) guarantees you a top-1000 position.","63228d19":"We need to drop 113 least important columns. Let's do it and fit the remaining data to the model.","dbaa284d":"Now let's generate predictions and submit.","444bf29a":"Let's begin by loading necessary modules and our data."}}