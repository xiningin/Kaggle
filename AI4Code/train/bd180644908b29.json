{"cell_type":{"fac88f67":"code","0a7d3dec":"code","49ce4ee8":"code","ee4029ec":"code","016d8e50":"code","19ae029c":"code","e52f8ce1":"code","d03002ae":"code","fa7167fe":"code","cbce0b24":"code","b7bb3b1e":"code","e370ec47":"code","98394daf":"code","1506755e":"code","39e56db4":"code","deb683fb":"code","498a123d":"code","6b0f313a":"code","347f5d96":"code","c2fea2b4":"code","5bda8c19":"code","86ffc18c":"code","c0b54072":"code","e27156c1":"code","c2332111":"code","82984739":"code","d2e28005":"code","26916c8f":"code","c80e1c82":"code","37d8bcc0":"code","9ea52271":"code","a85ec1aa":"code","4b9eb20b":"code","3b0a498f":"code","61cd6b93":"code","8f554128":"code","d5d59640":"code","3adf5828":"markdown","76ae4621":"markdown","32085818":"markdown","cff51ac1":"markdown","73bb0284":"markdown","6e20b761":"markdown","1ce30625":"markdown","f185b2d1":"markdown","38b70ca1":"markdown","31a7fe57":"markdown","67e51d56":"markdown"},"source":{"fac88f67":"import pandas as pd\nimport numpy as np \nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n# from show_confusion_matrix import show_confusion_matrix \n# the above is from http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html\n\nimport matplotlib.gridspec as gridspec\n\n#from sklearn.cross_validation import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix, f1_score,cohen_kappa_score, roc_auc_score\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import binarize","0a7d3dec":"df = pd.read_csv(\"..\/input\/creditcard.csv\")","49ce4ee8":"df.describe()","ee4029ec":"df.isnull().sum()","016d8e50":"print (\"Fraud\")\nprint (df.Time[df.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (df.Time[df.Class == 0].describe())","19ae029c":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 50\n\nax1.hist(df.Time[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Time[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()","e52f8ce1":"print (\"Fraud\")\nprint (df.Amount[df.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (df.Amount[df.Class == 0].describe())","d03002ae":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 30\n\nax1.hist(df.Amount[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Amount[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","fa7167fe":"df['Amount_max_fraud'] = 1\ndf.loc[df.Amount <= 2125.87, 'Amount_max_fraud'] = 0","cbce0b24":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n\nax1.scatter(df.Time[df.Class == 1], df.Amount[df.Class == 1])\nax1.set_title('Fraud')\n\nax2.scatter(df.Time[df.Class == 0], df.Amount[df.Class == 0])\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","b7bb3b1e":"#Select only the anonymized features.\nv_features = df.ix[:,1:29].columns","e370ec47":"plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","98394daf":"#Drop all of the features that have very similar distributions between the two types of transactions.\ndf = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)","1506755e":"#Based on the plots above, these features are created to identify values where fraudulent transaction are more common.\ndf['V1_'] = df.V1.map(lambda x: 1 if x < -3 else 0)\ndf['V2_'] = df.V2.map(lambda x: 1 if x > 2.5 else 0)\ndf['V3_'] = df.V3.map(lambda x: 1 if x < -4 else 0)\ndf['V4_'] = df.V4.map(lambda x: 1 if x > 2.5 else 0)\ndf['V5_'] = df.V5.map(lambda x: 1 if x < -4.5 else 0)\ndf['V6_'] = df.V6.map(lambda x: 1 if x < -2.5 else 0)\ndf['V7_'] = df.V7.map(lambda x: 1 if x < -3 else 0)\ndf['V9_'] = df.V9.map(lambda x: 1 if x < -2 else 0)\ndf['V10_'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\ndf['V11_'] = df.V11.map(lambda x: 1 if x > 2 else 0)\ndf['V12_'] = df.V12.map(lambda x: 1 if x < -2 else 0)\ndf['V14_'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\ndf['V16_'] = df.V16.map(lambda x: 1 if x < -2 else 0)\ndf['V17_'] = df.V17.map(lambda x: 1 if x < -2 else 0)\ndf['V18_'] = df.V18.map(lambda x: 1 if x < -2 else 0)\ndf['V19_'] = df.V19.map(lambda x: 1 if x > 1.5 else 0)\ndf['V21_'] = df.V21.map(lambda x: 1 if x > 0.6 else 0)","39e56db4":"#Create a new feature for normal (non-fraudulent) transactions.\n#df.loc[df.Class == 0, 'Normal'] = 1\n#df.loc[df.Class == 1, 'Normal'] = 0","deb683fb":"#Rename 'Class' to 'Fraud'.\ndf = df.rename(columns={'Class': 'Fraud'})","498a123d":"pd.set_option(\"display.max_columns\",101)\ndf.head()","6b0f313a":"#Create dataframes of only Fraud and Normal transactions.\nFraud = df[df.Fraud == 1]\nNormal = df[df.Fraud == 0]","347f5d96":"# Set X_train equal to 80% of the fraudulent transactions.\nX_train = Fraud.sample(frac=0.8)\ncount_Frauds = len(X_train)\n\n# Add 80% of the normal transactions to X_train.\nX_train = pd.concat([X_train, Normal.sample(frac = 0.8)], axis = 0)\n\n# X_test contains all the transaction not in X_train.\nX_test = df.loc[~df.index.isin(X_train.index)]","c2fea2b4":"#Shuffle the dataframes so that the training is done in a random order.\nX_train = shuffle(X_train)\nX_test = shuffle(X_test)","5bda8c19":"#Add our target features to y_train and y_test.\ny_train = pd.DataFrame()\ny_test = pd.DataFrame()\n#y_train = X_train.Fraud\ny_train = pd.concat([y_train, X_train.Fraud], axis=1)\n\n#y_test = X_test.Fraud\ny_test = pd.concat([y_test, X_test.Fraud], axis=1)","86ffc18c":"#Drop target features from X_train and X_test.\nX_train = X_train.drop(['Fraud'], axis = 1)\nX_test = X_test.drop(['Fraud'], axis = 1)","c0b54072":"#Check to ensure all of the training\/testing dataframes are of the correct length\nprint(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","e27156c1":"#Names of all of the features in X_train.\nfeatures = X_train.columns.values\n\n#Transform each feature in features so that it has a mean of 0 and standard deviation of 1; \n#this helps with training the neural network.\nfor feature in features:\n    mean, std = df[feature].mean(), df[feature].std()\n    X_train.loc[:, feature] = (X_train[feature] - mean) \/ std\n    X_test.loc[:, feature] = (X_test[feature] - mean) \/ std","c2332111":"import keras.backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","82984739":"model = Sequential()\n\n\nmodel.add(Dense(40, input_shape=(37,)))\nmodel.add(Activation('relu'))  # An \"activation\" is just a non-linear function applied to the output\n# of the layer above. Here, with a \"rectified linear unit\",\n# we clamp all values below 0 to 0.\n\nmodel.add(Dropout(0.2))  # Dropout helps protect the model from memorizing or \"overfitting\" the training data\nmodel.add(Dense(40))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid')) ","d2e28005":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n## Now its time to fit the model\n\nmodel.fit(X_train, y_train,\n          batch_size=128, epochs=10,\n           verbose=2,\n           validation_data=(X_test, y_test))\n","26916c8f":"#Confution Matrix and Classification Report\n# predict probabilities for test set\nyhat_probs = model.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test, verbose=0)\n\n# reduce to 1d array\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]","c80e1c82":"# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test[\"Fraud\"], yhat_classes)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test[\"Fraud\"], yhat_classes)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test[\"Fraud\"], yhat_classes)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test[\"Fraud\"], yhat_classes)\nprint('F1 score: %f' % f1)","37d8bcc0":"# kappa\nkappa = cohen_kappa_score(y_test, yhat_classes)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat_probs)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat_classes)\nprint(matrix)","9ea52271":"## Printing confusion matrix\n\n\n\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    #plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n","a85ec1aa":"cnf_matrix = confusion_matrix(y_test['Fraud'], yhat_classes)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n    # Plot non-normalized confusion matrix\nclass_names = [\"Not Fraud\",\"Fraud\"]\nplot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i)","4b9eb20b":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \"\"\"\n    Modified from:\n    Hands-On Machine learning with Scikit-Learn\n    and TensorFlow; p.89\n    \"\"\"\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')","3b0a498f":"precision, recall, thresholds = precision_recall_curve(y_test['Fraud'], yhat_probs)\nplot_precision_recall_vs_threshold(precision, recall, thresholds)","61cd6b93":"plt.hist(yhat_probs, bins=20000)\n\n# x-axis limit from 0 to 1\nplt.xlim(0,0.0005)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of Frauds')\nplt.ylabel('Frequency')","8f554128":"## finding the best threshold, using histogram, most the probs are below 0.0001\n\nfor i in [float(j) \/ 10000 for j in range(0, 10, 1)]:\n    y_pred_class = binarize(yhat_probs.reshape(-1,1), i)\n    cnf_matrix = confusion_matrix(y_test, y_pred_class)\n    class_names = [\"Not Fraud\",\"Fraud\"]\n    print(\"Senstivity metric in the testing dataset: \", cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n    print(\"Specificity metric in the testing dataset: \", cnf_matrix[0,0]\/(cnf_matrix[0,0]+cnf_matrix[0,1]))\n    plot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i) ","d5d59640":"y_pred_class = binarize(yhat_probs.reshape(-1,1), 0.0002)\n\ncnf_matrix = confusion_matrix(y_test, y_pred_class)\n\nprint(\"Recall\/Senstivity metric in the testing dataset: \", cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\nprint(\"Specificity metric in the testing dataset: \", cnf_matrix[0,0]\/(cnf_matrix[0,0]+cnf_matrix[0,1]))\n\n    # Plot non-normalized confusion matrix\nclass_names = [\"Not Fraud\",\"Fraud\"]\nplot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i)","3adf5828":"# Predicting Credit Card Fraud","76ae4621":"## Train the Neural Net","32085818":"## Exploring the Data","cff51ac1":"The goal for this analysis is to predict credit card fraud in the transactional data. I have used Keras to build the predictive model. I have also tried to see the threshold distribution and then arrive at an optimal threshold where one can maximize senstivity and specificity. If you would like to learn more about the data, visit: https:\/\/www.kaggle.com\/dalpozz\/creditcardfraud.\n\nPlease share your feedback and comment\/upvote if you liked it.\n\nThe sections of this analysis include: \n\n - Exploring the Data\n - Building the Neural Network \n","73bb0284":"No missing values, that makes things a little easier.\n\nLet's see how time compares across fraudulent and normal transactions.","6e20b761":"Most transactions are small amounts, less than $100. Fraudulent transactions have a maximum value far less than normal transactions, $2,125.87 vs $25,691.16.\n\nLet's compare Time with Amount and see if we can learn anything new.","1ce30625":"The 'Time' feature looks pretty similar across both types of transactions. You could argue that fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. This could make it easier to detect a fraudulent transaction during at an 'off-peak' time.\n\nNow let's see if the transaction amount differs between the two types.","f185b2d1":"Nothing too useful here.\n\nNext, let's take a look at the anonymized features.","38b70ca1":"The data is mostly transformed from its original form, for confidentiality reasons.","31a7fe57":"I will try to use a custom metric instead of accuracy for model training and evaluation. Custom metric being used is F1 score. More information F1 score at this https:\/\/en.wikipedia.org\/wiki\/F1_score","67e51d56":"Lets visualize the histogram of probabilities and see where most of our probabilities are"}}