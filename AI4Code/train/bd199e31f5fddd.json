{"cell_type":{"736bfe2c":"code","67640012":"code","9b3b6a1a":"code","5fff3949":"code","78e1cee3":"code","b671551a":"code","c2848bae":"code","863dd479":"code","8e2265ca":"code","725fbfc9":"markdown","ea3c892d":"markdown","6a9f8d3d":"markdown","6ad38569":"markdown","6e575415":"markdown","ab1db3cd":"markdown"},"source":{"736bfe2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67640012":"data=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","9b3b6a1a":"data.info()","5fff3949":"data.head()","78e1cee3":"data.tail()","b671551a":"#Normalization of our data\nData=(data-np.min(data)\/np.max(data)-np.min(data))","c2848bae":"from sklearn.cluster import KMeans\n\nwcss=[] #Within Cluster Sum of Square\nfor i in range(1,15):\n    km=KMeans(n_clusters = i)# n_cluster is number of clusters\n    km.fit(Data)\n    wcss.append(km.inertia_)\n    #inertia: how spread out the clusters are distance from each sample\n    #lower inertia means more clusters\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"clusters\")\nplt.ylabel(\"wcss\")\nplt.show()","863dd479":"#Kmeans to 3 clusters\nfrom sklearn.cluster import KMeans\nkm=KMeans(n_clusters=3)\nkm.fit(Data)\nlabels=km.predict(Data)\n\nplt.scatter(Data['trestbps'],Data['chol'],c = labels)\nplt.xlabel('trestbps')\nplt.xlabel('chol')\nplt.show()","8e2265ca":"#h, dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg=linkage(Data,method=\"ward\")\ndendrogram(merg,leaf_rotation=90)\n\nplt.xlabel(\"Data Points\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","725fbfc9":"We use \"euclidean distance\" technique in this Clustering. We can calculate euclidean distance according to four thing.\n1. According to closest two points\n2. According to furthest two points\n3. According to mean\n4. According to centroid","ea3c892d":"We have four step to follow in this unsupervised algorithm.\n1. Should be one cluster for each data point\n2. Take the two data points which are closest and make them cluster\n3. Take the two clusters which are closest and make them one cluster\n4. Repat 3\n\n","6a9f8d3d":"Elbow point gives us the best number of clusters. So the best number of cluster is 3.\n","6ad38569":"Hierarchical Clustering","6e575415":"What is WCSS? : \n\nTo calculate WCSS, you first find the Euclidean distance (see figure below) between a given point and the centroid to which it is assigned. You then iterate this process for all points in the cluster, and then sum the values for the cluster and divide by the number of points.","ab1db3cd":"K-Means Clustering\n\nWe have four step to follow in this unsupervised algorithm.\n1. Choose k value\n2. Assign random cenroid\n3. Find data points distance according to euclidean distance and do clustering.\n4. Find out new centroid until the place of cendroid do not change\n\n"}}