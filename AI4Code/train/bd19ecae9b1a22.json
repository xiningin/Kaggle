{"cell_type":{"9f6af6bf":"code","bf06c460":"code","c3b2b4b2":"code","ba20cf01":"code","c9d5f81f":"code","1ab3f59c":"code","25960bf8":"code","1b7d4c1d":"code","e598f379":"code","0d226612":"code","7e1a03c6":"code","fbc5c8ef":"code","a933446b":"code","21f595c7":"code","f78f59c7":"code","18a7c054":"code","647152d4":"code","dd90a91c":"code","a2309655":"code","967502b0":"code","e188819c":"code","17d5ef6f":"code","ec5d8000":"code","8fe5481a":"code","5aa15a0a":"code","2cc46099":"code","de8caa67":"code","ac68e331":"code","6bb4d9bb":"code","c5bf352e":"code","9e720aaf":"code","d8c9e2fe":"code","66caff28":"code","4bdd4e67":"code","bbf71ecd":"code","45068521":"code","2fcd16fb":"code","107c33e7":"code","daf39906":"code","d2366013":"code","28045338":"code","468a8936":"code","7b8662e0":"code","d065db94":"code","55e75b59":"code","27b21432":"code","77d6bbad":"code","955af584":"code","ad55a00f":"code","198eace9":"code","f8bbc114":"code","cb58e741":"code","ad83fa62":"code","4a8db551":"code","39498c80":"code","c1e7885f":"code","4120ce23":"code","93663f60":"code","f3d3cdfa":"code","7d01cb82":"code","597f7080":"code","6e7c1798":"code","30f65fd1":"code","afa27576":"code","5b875616":"code","bd196c0f":"code","79853420":"code","50c7d7cc":"code","bddcfc36":"code","6593f469":"code","caabbe65":"code","33c7d11b":"code","f8dcc1d0":"code","ce3eaa83":"code","6aee7c57":"code","7bda7e3d":"code","75260276":"code","a5c07c40":"code","02fbec74":"code","3dbab1ed":"code","55a4929b":"code","8d93b817":"code","126caabe":"code","2562dd4c":"code","1f947105":"code","6227327d":"code","dc0dc8c1":"code","f634ff0a":"code","0c428d5d":"code","470ec286":"code","0c9e4a5b":"code","4a5ad686":"code","26000618":"code","d55c7f75":"code","76e71b3b":"code","980f2dbf":"code","5ce86e9f":"code","e267635d":"code","12e4a33a":"code","91a9b81b":"code","7f5ad243":"code","91eda4fb":"code","43c975c5":"code","eb3eb75d":"code","25797aab":"code","05b61538":"code","7394545a":"code","d738213c":"code","7fb9d611":"code","7bb62822":"code","00263fd1":"code","d3198e21":"code","67cf6930":"code","c78d48f9":"code","03bb0861":"code","5be2a7bd":"code","f6b20a3e":"code","ef4ae106":"code","2c6d88db":"code","f5e6f236":"code","c0854ff8":"code","c5a72e70":"code","8a85619a":"code","46fe8a01":"code","8cec2316":"code","3027385e":"code","ad59daf3":"code","859eb2c2":"code","f69b3ca2":"code","6c1ca279":"code","80310c37":"code","e05b9ce2":"code","b08b0e98":"code","f6064bb9":"code","b7683c65":"code","f4ef2fe6":"code","ecc291c9":"code","a02c6a89":"code","eb842189":"code","d1f1927e":"code","d4eddf35":"code","f36dbffb":"code","ed89711b":"code","11e860c7":"code","bb98b102":"code","82650495":"code","c2e0989e":"code","71e26c42":"code","509a2d23":"code","d40ac92b":"code","630d7473":"code","caaab05f":"code","e3c6d391":"code","46a3f9be":"code","88d36ce1":"code","a9035155":"code","1fe7c13f":"code","5b8db63c":"code","0f1c8e1a":"code","b8ffd977":"code","004e5680":"code","1f142b46":"code","51882fd5":"code","b6244b80":"code","fe38206d":"code","15777ba3":"code","2278fe3e":"code","7e804ce8":"code","5a131f1e":"code","ea01f21f":"code","6577caf2":"code","68cf90a5":"code","4056a6b2":"code","67cc8042":"code","0dcf89c6":"code","494e8081":"code","85e9483f":"code","6c408871":"code","c0fbd910":"code","019ef119":"code","47953a6b":"code","0f34a973":"code","4a912c46":"code","b3c6966f":"code","80a78833":"code","e5b504b6":"code","d0498b93":"markdown","5c5e24b4":"markdown","96192c95":"markdown","6c3a4627":"markdown","547dbdc0":"markdown","2aa61f50":"markdown","3be460b9":"markdown","1651b744":"markdown","245243aa":"markdown","5c149365":"markdown","5616ef56":"markdown","b1f27e26":"markdown","34f6406c":"markdown","76de841b":"markdown","c7bd3d01":"markdown","4e10c6ec":"markdown","7cf9fb7e":"markdown","d2a99018":"markdown","df064f9b":"markdown","601759bd":"markdown","757f6875":"markdown","5a4613eb":"markdown","57e0934a":"markdown","6303346a":"markdown","6424e145":"markdown","9e1c2286":"markdown","f1cf56d5":"markdown","360b2e09":"markdown","f431d8f2":"markdown","cc6c57c6":"markdown","c2df4476":"markdown","feca3621":"markdown","aa17be86":"markdown","2d31d0f7":"markdown","4504a413":"markdown","b3620c13":"markdown","7a77780f":"markdown","08797d6d":"markdown","6112441a":"markdown","8e29f6d7":"markdown","7eb0a303":"markdown","591a66c4":"markdown","938be98b":"markdown","46ea923d":"markdown","8a4aca00":"markdown","8429e366":"markdown","f9020c5f":"markdown","c8e02b6b":"markdown","6537bb6e":"markdown","fb564ab8":"markdown","42e2a22b":"markdown","e4a4e29f":"markdown","e9ceb316":"markdown","7ee16b72":"markdown","d9d4c4ea":"markdown","53f4c686":"markdown","f2423696":"markdown","8cf50a42":"markdown","ab1573bb":"markdown","cb442a58":"markdown","3516f619":"markdown","8b9d578f":"markdown","a13fbd2b":"markdown","0d3b4e67":"markdown","405bf6db":"markdown","5e956a2c":"markdown","5a9ab5aa":"markdown","793ea234":"markdown","9ecb4f8b":"markdown","4b03fed3":"markdown","537c8327":"markdown","5344382e":"markdown","a463af4b":"markdown","0225e890":"markdown","1aa5e57f":"markdown","e999b6f8":"markdown","2c301ee1":"markdown","5d401582":"markdown","5b2d1cb3":"markdown","b47b528a":"markdown","b95d712d":"markdown","3f4f5da3":"markdown","8b301a87":"markdown","97001b88":"markdown","f3dbcb0c":"markdown","f8f1cede":"markdown","f3985468":"markdown","0f80038d":"markdown","84ac8d8d":"markdown","e9758a55":"markdown","51279c06":"markdown","436f6679":"markdown","7b9ae669":"markdown","308a0028":"markdown","10547e5e":"markdown","eec1a073":"markdown","77dd22c8":"markdown","ecdbe790":"markdown","e80fd545":"markdown","82a160b0":"markdown","c9ca58a5":"markdown","7d2fb515":"markdown","701702de":"markdown","02e55079":"markdown","44da03c6":"markdown","d26eb5e2":"markdown","318f5ba6":"markdown","968c1f03":"markdown","9a004b02":"markdown","1d137e67":"markdown","d32d1d38":"markdown","69a532c3":"markdown","cd6ea68e":"markdown","859b5dad":"markdown","3fd09065":"markdown","5402abcc":"markdown","6c324fac":"markdown","d4cb4732":"markdown","fe728114":"markdown","515a49fc":"markdown","d8e5af50":"markdown","31194802":"markdown","bce98579":"markdown","12acc637":"markdown","569c38b7":"markdown","63c73cda":"markdown","fa4ad659":"markdown","eddf97ea":"markdown","66c74304":"markdown","f5efeeb2":"markdown","3ab3282f":"markdown","9769b0a7":"markdown","75f2a0fc":"markdown","04402cf0":"markdown","d20396be":"markdown"},"source":{"9f6af6bf":"# data processing tools\nimport pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom math import sqrt\nimport itertools\nfrom collections import Counter\n\n# model tools\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, LassoLarsCV, LassoLarsIC\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn import neighbors\nimport xgboost as xgb\n\n# NLP tools\nimport spacy\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Visualization tools\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","bf06c460":"# load and look at our austin housing data\ndf = pd.read_csv('..\/input\/austinhousingprices\/austinHousingData.csv')\ndf.head()","c3b2b4b2":"df.dtypes","ba20cf01":"df.shape","c9d5f81f":"df.rename(columns={'latestPrice':'price'}, inplace=True)","1ab3f59c":"df['latest_saleyear'].unique()\n# our data spans all the way from 2019 to 2021.","25960bf8":"df['latest_saledate'].min()\n# earliest sale in january 2018","1b7d4c1d":"df['latest_saledate'].max()\n# last sale date january 2021","e598f379":"jan2018_to_2019 = .024\/12\njan2019_to_2020 = .037\/12\njan2020_to_2021 = .193\/12\n\n# create reverse time series number for month\nfor i in range(1,13):\n    df['time_series'] = df['latest_salemonth'].apply(lambda x: abs(x-13))\n\n# adjust 2018 to 2019\ndf.loc[df['latest_saleyear']==2018, 'adj_price'] = df.loc[df['latest_saleyear']==2018].apply(lambda x: int( (x['price'])*(1+(jan2018_to_2019*x['time_series']))*(1+(jan2019_to_2020*x['time_series']))*(1+(jan2020_to_2021*x['time_series'])) ) , axis=1 )\n\n# adjust 2019 to 2020                                                            \ndf.loc[df['latest_saleyear']==2019, 'adj_price'] = df.loc[df['latest_saleyear']==2019].apply(lambda x: int( (x['price'])*(1+(jan2019_to_2020*x['time_series']))*(1+(jan2020_to_2021*x['time_series'])) ) , axis=1)\n                                                                                  \n# adjust 2020 to 2021\ndf.loc[df['latest_saleyear']==2020, 'adj_price'] = df.loc[df['latest_saleyear']==2020].apply(lambda x: int( (x['price'])*(1+(jan2020_to_2021*x['time_series'])) ), axis=1)\n\n#copy 2021 to self\ndf.loc[df['latest_saleyear']==2021, 'adj_price'] = df.loc[df['latest_saleyear']==2021].apply(lambda x: int(x['price']), axis=1)\n\n# rename original price column and make adj_price our price column\ndf.rename(columns={'price' : 'orig_price', 'adj_price' : 'price'}, inplace=True)\n\n# put price at the front of the data frame\ndf.set_index('price', inplace=True)\ndf.reset_index(inplace=True)","0d226612":"# what are the homeTypes?\n\ndf['homeType'].value_counts(normalize=True)","7e1a03c6":"# Ultimately, with Single Family, Condo and Townhouse making up most of the data, we are going to remove all multi-family type listings\ndf = df.loc[((df['homeType'] == 'Single Family') | (df['homeType'] == 'Condo')) | (df['homeType'] == 'Townhouse')]","fbc5c8ef":"# check for duplicate data\n\ndf[df.duplicated(subset=['latitude','longitude'], keep=False)].sort_values('latitude')\n\n# no duplicate data","a933446b":"# plotting latitude and longitude as a visual scatter plot to look for location-based outliers\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"longitude\", y=\"latitude\", hue=\"price\", palette=\"magma_r\");","21f595c7":"# drop latitidue below 30.12 to remove the few outliers in the SE\ndf.drop(df[df['latitude']<30.12].index , inplace=True)","f78f59c7":"# looking for outliers in the percentiles\n\ndf.describe()","18a7c054":"# check how our histograms are looking\ndf.hist(figsize=(18,15), bins=100);\n","647152d4":"#check what is going on with the lotSizeSqFt outliers by sorting descending\ndf.sort_values('lotSizeSqFt', ascending=False).head(5)\n\n# This top listing is legitimate. But we have a problem here where condo and townhouse listings are using the \n# size of the overall lot for their lot, and that isn't really accurate\/representative\n# We'll fix this in a little bit","dd90a91c":"#check what is going on with the livingAreaSqFt outliers by sorting ascending\ndf.sort_values('livingAreaSqFt', ascending=True).head(5)\n\n# just tiny houses I guess ?","a2309655":"#check what is going on with the livingAreaSqFt outliers by sorting descending\ndf.sort_values('livingAreaSqFt', ascending=False).head(5)","967502b0":"# we're dropping the top two listings here. One is a lot, and the other is clearly mistaken.\ndf.drop(index=[705, 2557], inplace=True)","e188819c":"#check what is going on with the numOfBathrooms outliers by sorting descending\ndf.sort_values('numOfBathrooms', ascending=False).head(5)","17d5ef6f":"# I'm going to say this top listing has 2.5 bathrooms not 27. That is clearly a typo.\ndf.loc[df.index==2838, 'numOfBathrooms'] = 2.5","ec5d8000":"#check what is going on with the numOfBathrooms outliers by sorting ascending\ndf.sort_values('numOfBathrooms', ascending=True).head(5)","8fe5481a":"# most listings with 0 bathrooms also have 0 bedrooms. This is clearly wrong, but I'm not going to guess if there are no bedrooms.\n# I will impute typical bathroom count per bedroom count based on year built\n# then drop any remaining listings with 0 bathrooms and 0 bedrooms\n\ndf.loc[(df['numOfBathrooms']==0) & (df['numOfBedrooms']>0) & (df['yearBuilt'] > 1989), 'numOfBathrooms'] = 2\ndf.loc[(df['numOfBathrooms']==0) & (df['numOfBedrooms']>0) & (df['yearBuilt'] <= 1989), 'numOfBathrooms'] = 1\ndf.loc[(df['numOfBathrooms']==0) & (df['numOfBedrooms']>=3) & (df['yearBuilt'] > 1989), 'numOfBathrooms'] = 2.5\ndf.loc[(df['numOfBathrooms']==0) & (df['numOfBedrooms']>=3) & (df['yearBuilt'] <= 1989), 'numOfBathrooms'] = 2\n\ndf.drop(df[df['numOfBathrooms']==0].index, inplace=True)\ndf.drop(df[df['numOfBedrooms']==0].index, inplace=True)","5aa15a0a":"#check what is going on with the numOfBedrooms outliers by sorting descending\ndf.sort_values('numOfBedrooms', ascending=False).head(5)","2cc46099":"# That condo is supposed to have 2 bathrooms, not 20.\ndf.loc[df.index==8597, 'numOfBedrooms'] = 2","de8caa67":"#check what is going on with the garageSpaces outliers by sorting descending\ndf.sort_values('garageSpaces', ascending=False).head(10)","ac68e331":"# a bunch of these garage spaces are definitely just bogus numbers. I'm going to force change a lot of them to numbers that make sense\ndf.loc[(df['garageSpaces'] > 3) & (df['price'] < 1000000) & (df['homeType'] == 'Single Family'), 'garageSpaces'] = 3\ndf.loc[(df['garageSpaces'] > 5) & (df['price'] > 1000000)& (df['homeType'] == 'Single Family'), 'garageSpaces'] = 4\ndf.loc[df.index==6885, 'garageSpaces'] = 2","6bb4d9bb":"#check what is going on with the parkingSpaces outliers by sorting descending\ndf.sort_values('parkingSpaces', ascending=False).head(5)","c5bf352e":"# We are going to do the same forced conversions on parking spaces\ndf.loc[(df['parkingSpaces'] > 3) & (df['price'] < 1000000 & (df['homeType'] == 'Single Family')), 'parkingSpaces'] = 3\ndf.loc[(df['parkingSpaces'] > 5) & (df['price'] > 1000000& (df['homeType'] == 'Single Family')), 'parkingSpaces'] = 5\ndf.loc[df.index==6885, 'parkingSpaces'] = 2\n\ndf.sort_values('parkingSpaces', ascending=False).head(5)","9e720aaf":"df['city'].value_counts()","d8c9e2fe":"# check how our histograms are looking for our columns that seem to have outliers\n\ndf.hist(figsize=(18,15), bins=100);","66caff28":"# A lot of our variables are not normally shaped, so we can't reliably remove outliers via standard deviation.\n# We will use IQR to remove our outliers with the following function\n\ndef iqr_outliers(column):\n    \"\"\"return the lower range and upper range for the data based on IQR*1.6\"\"\"\n    Q1,Q3 = np.percentile(column , [25,75])\n    iqr = Q3 - Q1\n    lower_range = Q1 - (1.6 * iqr)\n    upper_range = Q3 + (1.6 * iqr)\n    return lower_range,upper_range  ","4bdd4e67":"# determing our IQR for price, lot size, sq footage and longitude\nlotlower,lotupper = iqr_outliers(df.lotSizeSqFt)\nsqftlower, sqftupper = iqr_outliers(df.livingAreaSqFt)\n\n# dropping the things outside of our lower and upper range\ndf.drop(df[ (df.lotSizeSqFt > lotupper) | (df.lotSizeSqFt < lotlower) ].index , inplace=True)\ndf.drop(df[ (df.livingAreaSqFt > sqftupper) | (df.livingAreaSqFt < sqftlower) ].index , inplace=True)","bbf71ecd":"# We'll imputing the median lot size into condo and townhouse listings that are over-listed for lot size square feet\n\ndf.loc[(df['homeType']=='Condo') | (df['homeType']=='Townhouse') & (df['livingAreaSqFt']<1200) & (df['lotSizeSqFt']>8000), 'lotSizeSqFt'] = df['lotSizeSqFt'].median()","45068521":"# check how our histograms are looking\n\ndf.hist(figsize=(18,20), bins=100);\n\n# much better","2fcd16fb":"# we're using the median house value for a zip code to determine the zip code's sort, so we can visualize the zip code\n\n# group our dataframe by zipcode on median home price, sorted ascending. \nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# divide our dataframe into groups with entries per group as specified above,\n# and assign this number to a new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# function that looks up a segment that a data entry belongs to\ndef make_group(x, frame, column):\n    y = frame.loc[(frame.index == x)][column]\n    z = np.array(y)\n    z[0]\n    return z[0]\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))\n\n# apply the median home price per zip code to the data frame\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))","107c33e7":"# visualize zip code as a color function\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Median Home Price per Zip', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Rank', fontsize=20)\n;","daf39906":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['livingAreaSqFt'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Feet, by Zip Code Median Rank', fontsize=20)\n;","d2366013":"# we're dropping the values above 3 million, and the 3 entries from zipcode 78734\ndf.drop(df[df['price']>3000000].index, inplace=True)\ndf.drop(df[df['zipcode']==78734].index, inplace=True)\n\n# check price stats by zip code and displaying top 30 zip codes by mean\nfind_zip_outliers = df.groupby('zipcode')['price'].describe()\nfind_zip_outliers.sort_values('mean', ascending=False).head(35)\n# very suspicious values in many zip codes for min","28045338":"# anything under 75k is no way a legitimate market value sale. \n# anything in this range is certainly a inter-family sale,\n# non-commercial, some weird sale type.\n# We are dropping all of those. \ndf.drop(df.loc[(df['price'] <= 75000)].index, axis=0, inplace=True)","468a8936":"# Eliminating outliers on a per-zipcode basis using our IQR 1.6\n\nzipcodes = df['zipcode'].unique()\n\nfor i in zipcodes:\n    lower, upper = iqr_outliers(df[df['zipcode'] == i]['price'])\n    df.drop(df[ ( (df.price > upper) & (df['zipcode'] == i) ) | ( (df.price < lower)  & (df['zipcode'] == i) ) ].index , inplace=True)\n","7b8662e0":"#We can check our price per zip code histograms.\n\ndf['price'].hist(by=df['zipcode'], figsize=(30,30));    \n\n# some of our zip codes don't have enough sales to give us information","d065db94":"# We're going to drop our few zip codes where we have only a couple of data points\n\ndf.drop( df.loc[(df['zipcode']==78653) | (df['zipcode']==78738) | (df['zipcode']==78719)| (df['zipcode']==78652)| (df['zipcode']==78742)].index, axis=0, inplace=True)","55e75b59":"# redo our zip code medians and rankings after outlier removal\n\n# apply the median home price per zip code to the data frame again after outlier removal\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))\n\n# group our dataframe by zipcode on median home price, sorted ascending. We want to bin like-medians together.\nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# divide our dataframe into groups with entries per group as specified above,\n# and assign this number to a new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))","27b21432":"# re-visualize zip code as a color function, using the median zip after outlier removal. \n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Zip Code by Median Rank', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Home Value', fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/zip_prices.png')","77d6bbad":"# apply the median price per square foot per zip code to the data frame\ndf['pr_sqft'] = df.apply(lambda x: round( (x['price'] \/ x['livingAreaSqFt'] ), 0), axis=1 )","955af584":"fig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['livingAreaSqFt'], df['pr_sqft'], c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Total Square Footage', fontsize=12)\nax.set_ylabel('Price Per Square Foot', fontsize=12)\nax.set_title('Price Per Square Foot to Total Square Footage, by Zip Code Median Rank', fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/zip_prices.png')","ad55a00f":"#dropping irrationally high pr\/sqft\ndf.drop(df[df['pr_sqft']>1000].index, inplace=True)","198eace9":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['livingAreaSqFt'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Footage, by Zip Code Median Rank', fontsize=20)\n;","f8bbc114":"low_zips = df.loc[df['median_zip']<df['price'].mean()]\nhigh_zips = df.loc[df['median_zip']>=df['price'].mean()]","cb58e741":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(low_zips['livingAreaSqFt'], low_zips['price'] \/100000, c=low_zips['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Footage, by Zip Code Median Rank \\nFor Zip Medians under Mean', fontsize=20)\n;","ad83fa62":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(high_zips['livingAreaSqFt'], high_zips['price'] \/100000, c=high_zips['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Footage, by Zip Code Median Rank\\nFor Zip Medians over Mean', fontsize=20)\n;","4a8db551":"# plotting latitude and longitude as a visual scatter plot. The improved color map actually visually demonstrates\n# the removal of extreme price outliers.\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"longitude\", y=\"latitude\", hue=\"price\", palette=\"magma_r\");","39498c80":"# we can also map our zip codes in this way.\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"longitude\", y=\"latitude\", hue=\"zip_rank\", palette=\"magma_r\");","c1e7885f":"# look for nulls\n\ndf.isna().sum()\n\n# no missing data. impressive!","4120ce23":"# check data types\n\ndf.dtypes\n\n# data types all look correct","93663f60":"# we're going to convert some of our ordinal features to binary 0\/1\nconvert_to_bool = ['numOfAccessibilityFeatures', 'numOfAppliances', 'numOfParkingFeatures', 'numOfPatioAndPorchFeatures', 'numOfSecurityFeatures', 'numOfWaterfrontFeatures', 'numOfWindowFeatures', 'numOfCommunityFeatures']\n\ndf_convert_to_bool = df[convert_to_bool]\ndf_convert_to_bool.describe()","f3d3cdfa":"# Any element that has no features by the 50th percentile is getting converted to a binary \n\n# change all non-null values > 0 in those columns to 1\ndf.loc[df['numOfAccessibilityFeatures'] > 0, 'numOfAccessibilityFeatures'] = 1\ndf.loc[df['numOfPatioAndPorchFeatures'] > 0, 'numOfPatioAndPorchFeatures'] = 1\ndf.loc[df['numOfSecurityFeatures'] > 0, 'numOfSecurityFeatures'] = 1\ndf.loc[df['numOfWaterfrontFeatures'] > 0, 'numOfWaterfrontFeatures'] = 1\ndf.loc[df['numOfWindowFeatures'] > 0, 'numOfWindowFeatures'] = 1\ndf.loc[df['numOfCommunityFeatures'] > 0, 'numOfCommunityFeatures'] = 1\n\n# now anything that is not a 1 becomes a 0\ndf.loc[df['numOfAccessibilityFeatures']!= 1, 'numOfAccessibilityFeatures'] = 0\ndf.loc[df['numOfPatioAndPorchFeatures'] != 1, 'numOfPatioAndPorchFeatures'] = 0\ndf.loc[df['numOfSecurityFeatures'] != 1, 'numOfSecurityFeatures'] = 0\ndf.loc[df['numOfWaterfrontFeatures'] != 1, 'numOfWaterfrontFeatures'] = 0\ndf.loc[df['numOfWindowFeatures'] != 1, 'numOfWindowFeatures'] = 0\ndf.loc[df['numOfCommunityFeatures'] != 1, 'numOfCommunityFeatures'] = 0\n\n# rename to reflect binary\ndf.rename(columns={'numOfAccessibilityFeatures' : 'accessibility', 'numOfPatioAndPorchFeatures' : 'patioporch', 'numOfSecurityFeatures': 'security', \n                  'numOfWaterfrontFeatures': 'waterfront', 'numOfWindowFeatures' : 'windowfeatures', 'numOfCommunityFeatures' : 'community'}, inplace=True)\n\n# convert original boolean columns to binary 0\/1\nboolean = ['hasAssociation', 'hasCooling', 'hasGarage', 'hasHeating', 'hasSpa', 'hasView']\n\nfor item in boolean:\n    df[boolean] = df[boolean].astype(int)","7d01cb82":"#histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\n\nres = stats.probplot(df['price'], plot=plt)\n\n# our sales price histogram is positively skewed and has a high peak\n# Our QQ-plot shows that we have heavy tails with right skew","597f7080":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df['price'].skew())\nprint(\"Kurtosis: %f\" % df['price'].kurt())\n\n# price is highly right skewed\n# very positive kurtosis, indicating lots in the tails. We can see those tails in the right skew.","6e7c1798":"# log transform our target price to improve normality of distribution\ndf_target_log = np.log(df['price'])\n\n#histogram and normal probability plot\nsns.distplot(df_target_log, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_target_log, plot=plt)\n\n# Our target price is more normally distributed when log transformed, so we'll be doing that when we make our model\n","30f65fd1":"# Load spaCy with English language processor\nnlp = spacy.load(\"en_core_web_sm\")\n\n# add real estate related stop words to default stop word list\nnlp.Defaults.stop_words |= {\"bedroom\", \"bathroom\",\"bath\",\"home\", \"austin\", \"tx\", \"pron\", \"sq\", \"ft\", \"rent\", \"mo\",\n                            \"w\", \"bed\", 'single', 'family', 'contain', 'st', 'dr', 'square', 'foot', 'room', 'square', 'feet',\n                            '-pron-', 'garage', 'pflugerville', 'story', '1st', '1story', '2car', '2nd',\n                            '2story', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th', 'street', 'avenue', 'ave', \n                            'sac', \n                            \n                           }\n\nnlp.Defaults.stop_words.remove('is')\nnlp.Defaults.stop_words.remove('as')","afa27576":"# text processing functions for NLP\n\ndef preprocessor(word):\n    '''processes an individual word to remove punctuation, numbers, special characters etc\n    Returns processed word, or blank string if character removal resulted in no word\n    ARGUMENT:\n    word from line of text'''\n    if type(word) == str:\n        word = re.sub(r'[^\\w\\s]', '', word)\n        word = re.sub('<[^>]*>', '', word)\n        word = re.sub('<[0-9]*>', '', word)\n        word = re.sub('[\\W]+', '', word.lower())\n        try:\n            word = int(word)\n            word = ''\n            return word\n        except:\n            return word\n\ndef word_processor(line):\n    '''Takes a line of text. Tokenizes each word of sentence. \n    If token is stop word, goes to next token. If not stop word,\n    calls preprocessor on word\n    Returns processed words from line\n    ARGUMENT: \n    line of text'''\n    \n    tokens = nlp(line) # nlp reads line and creates tokens from each word  \n    words = [] # empty list of words for this line\n    \n    for token in tokens:\n        if token.is_stop == False: # only continues if token is not stop word\n            token_preprocessed = preprocessor(token.lemma_) # calls preprocessor on word\n            if token_preprocessed != '': # only continues if returned word is not empty\n                words.append(token_preprocessed) # appends word to list of words\n    return(words) # return list of words for this line\n\ndef text_block_processor(text):\n    '''Takes a block of text. Divides block into sentences with words lemmatized.\n    Sends each sentence to word processor. Concatenates all words into one string\n    If the string contains \"zestimate\", returns a DEFAULT listing note\n    Otherwise returns string of cleaned and processed words from text block\n    ARGUMENTS:\n    block of text\n    '''\n    \n    make_sentences = nlp(text)\n    \n    sentences_lemmata_list = [sentence.lemma_.lower() for sentence in make_sentences.sents]\n    \n    these_processed_sentences = ''\n\n    \n    for item in sentences_lemmata_list:\n        words = word_processor(item)\n        line = ' '.join(words)\n        these_processed_sentences += (' ' + line)\n        \n    if 'zestimate' in these_processed_sentences:\n        return 'DEFAULT'\n    else:\n        return these_processed_sentences","5b875616":"# reset indices on original data frame before making a copy\ndf.reset_index(inplace=True)\ndf.drop('index', axis=1, inplace=True)","bd196c0f":"# copy the description column to a new data frame for text processing\nlisting_text = pd.DataFrame(df['description'])\n\nlisting_text['sentences'] = None\n\nlisting_text['sentences'] = listing_text['description'].apply(lambda x: text_block_processor(x))\n    \n# drop the description field and save our listing_text to file so we don't have to run it again\nlisting_text.drop('description', axis=1, inplace=True)\n","79853420":"# append our listing text to our original data frame\ndf = pd.concat([df, listing_text], axis=1)\n\ndf","50c7d7cc":"# set our random seed for the notebook. We could randomize this each time the notebook is run,\n# but ultimately we want all of our train\/test splits to use the same data\nrandomstate = 2\n\ny = pd.DataFrame(df['price'])\nx = df.drop('price', axis=1,)\n\n# creating our train\/validation sets and our test sets\ntrain_data, holdout, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=randomstate)\n\n# reset indices to prevent any index mismatches\ntrain_data.reset_index(inplace=True)\ntrain_data.drop('index', axis=1, inplace=True)\n\nholdout.reset_index(inplace=True)\nholdout.drop('index', axis=1, inplace=True)\n\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)","bddcfc36":"# Adding target encoding, which we will opt to try instead of one-hot with a few models\n\n# smooth mean function by MAx Halford at https:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\ndef calc_smooth_mean(df, by, on, m, target_df):\n    '''input a pandas.DataFrame, a categorical column name, the name of the target column, and a weight .'''\n    # Compute the global mean\n    mean = df[on].mean() \n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])  \n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) \/ (counts + m)\n\n    # Replace each value by the according smoothed mean\n    return round(target_df[by].map(smooth), 0) \n\nnum_of_samples = train_data.shape[0]\nzip_samples = num_of_samples\/train_data['zipcode'].unique().shape[0]\nmonth_samples = num_of_samples\/train_data['latest_salemonth'].unique().shape[0]\n\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\ntrain_data['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, train_data)\ntrain_data['year_smooth'] = calc_smooth_mean(train_data, 'yearBuilt', 'price', 300, train_data)\ntrain_data['month_smooth'] = calc_smooth_mean(train_data, 'latest_salemonth', 'price', month_samples, train_data)\n\n# Create a wider lat and long zone to calculate an area mean\ntrain_data['lat_zone'] = round(train_data['latitude'], 2)\ntrain_data['long_zone'] = round(train_data['longitude'], 2)\n\nlat_samples = num_of_samples\/train_data['lat_zone'].unique().shape[0]\nlong_samples = num_of_samples\/train_data['long_zone'].unique().shape[0]\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\ntrain_data['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, train_data)\ntrain_data['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, train_data)\ntrain_data['lat_long'] = round(np.sqrt(train_data['lat_smooth']) + np.sqrt(train_data['long_smooth']), 0)","6593f469":"# look for multicollinearity of features\nfig, ax = plt.subplots(figsize=(20, 20))\n\nsns.heatmap(train_data.corr(), center=0,  \n           vmin=-1, vmax=1,  square=True)\n\n# title\nplt.title('PEARSON CORRELATION MATRIX', fontsize=18)\n\nplt.show()","caabbe65":"train_data.corr()","33c7d11b":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 75% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.95)]\n","f8dcc1d0":"# Check out our variables correlationg with price\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\ndf_correlations.loc[df_correlations['level_0'] == 'price'].sort_values(0, ascending=False)","ce3eaa83":"# combine all school counts into a single field\ntrain_data['numOfSchools'] = train_data['numOfPrimarySchools'] + train_data['numOfElementarySchools'] + train_data['numOfMiddleSchools'] + train_data['numOfHighSchools']\nholdout['numOfSchools'] = holdout['numOfPrimarySchools']  + holdout['numOfElementarySchools'] + holdout['numOfMiddleSchools'] + holdout['numOfHighSchools']","6aee7c57":"categories = ['zipcode', 'yearBuilt', 'hasAssociation', 'hasCooling', 'hasHeating', 'hasSpa', 'hasView', 'accessibility', 'patioporch', 'security',\n          'waterfront', 'windowfeatures', 'community', 'latest_salemonth', 'numOfSchools', 'garageSpaces', 'propertyTaxRate', ]\n\ndf_categoricals = train_data[categories]","7bda7e3d":"# adding price to our dataframe so that we can do some visualizations    \n\ndf_categoricals['price'] = train_data['price']\n\n# plot our categoricals as box plots vs price\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \nf = pd.melt(df_categoricals, id_vars=['price'], value_vars=categories)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")\n\ndf_categoricals.drop('price', axis=1, inplace=True)","75260276":"# there is only ONE listing with 5 schools, so we will change that one to 4\ndf_categoricals.loc[df_categoricals['numOfSchools']==5, 'numOfSchools'] = 4\n\n# binning our year built bins\n\nnum_bins = 30\nlabels = np.array(range(1,num_bins+1))\ndf_categoricals[\"year_block\"] = pd.qcut(df_categoricals['yearBuilt'], q=num_bins, labels=labels)\n\ndf_categoricals.drop('yearBuilt', axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in df_categoricals.columns:\n    df_categoricals[item] = df_categoricals[item].astype('category')\n    \n# make a processed bins file for use with linear regression\n# We're making TWO categorical sets. One is high one hot encoding. One is low one hot encoding, and the \n# categoricals in that one will be target encoded as continuous instead\n\nhigh_one_hot_cat =  ['zipcode', 'year_block', 'hasAssociation', \n                 'hasCooling', 'hasHeating', 'hasSpa', 'hasView', \n                 'accessibility', 'patioporch', 'security', 'numOfSchools',\n              'waterfront', 'windowfeatures', 'community', 'latest_salemonth',\n                    'garageSpaces', 'propertyTaxRate', ]\nlow_one_hot_cat =  ['hasAssociation', \n                'hasCooling', 'hasHeating', 'hasSpa', \n                'hasView', 'accessibility', 'patioporch', 'numOfSchools',\n                'security', 'waterfront', 'windowfeatures', 'community', 'garageSpaces', 'propertyTaxRate', ]\n\ndf_cats_high_one_hot = pd.get_dummies(df_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=True)\ndf_cats_low_one_hot = pd.get_dummies(df_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=True)","a5c07c40":"continuous = ['numPriceChanges', \n              'lotSizeSqFt', 'livingAreaSqFt', 'avgSchoolDistance', \n              'avgSchoolRating', 'avgSchoolSize', 'numOfBedrooms', \n              'numOfStories', 'numOfPhotos', \n              'numOfAppliances', 'latest_salemonth',\n             'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long'] \n\nx_continuous = train_data[continuous]\nx_continuous['price'] = train_data['price']","02fbec74":"small_cont = ['numPriceChanges', \n              'avgSchoolRating', 'numOfBedrooms', \n              'numOfStories', 'numOfAppliances', \n              'latest_salemonth']\n# plot our continuous as box plots vs price\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\nf = pd.melt(x_continuous, id_vars=['price'], value_vars=small_cont)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")\n","3dbab1ed":"large_cont = ['lotSizeSqFt', 'livingAreaSqFt', 'avgSchoolDistance', \n              'avgSchoolRating', 'avgSchoolSize', 'numOfPhotos',\n             'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long']\n\n# check linearity of continuous predictors\n\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15,25), sharey=True)\n\nfor ax, column in zip(axes.flatten(), large_cont):\n    ax.scatter(x_continuous[column], x_continuous['price']\/100000, label=column, alpha=.1)\n    ax.set_title(f'Sale Price vs {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Sale Price in $100,000')\n\nfig.tight_layout()","55a4929b":"# Checking out our mean sales price for year built scattered versus price shows a polynomial relationship\n\nyearly_prices = train_data.groupby('latest_salemonth')['price'].mean()\n\nplt.scatter(yearly_prices.index, yearly_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('year built')\nplt.ylabel('sales price')\nplt.show()","8d93b817":"# Checking out our mean sales price for latitude  scattered versus price shows a polynomial relationship\nlat_prices = train_data.groupby('livingAreaSqFt')['price'].mean()\n\nplt.scatter(lat_prices.index, lat_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('lat')\nplt.ylabel('sales price')\nplt.show()","126caabe":"# Checking out our mean sales price for lot size scattered versus price shows a polynomial relationship\nlat_prices = train_data.groupby('lotSizeSqFt')['price'].mean()\n\nplt.scatter(lat_prices.index, lat_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('lat')\nplt.ylabel('sales price')\nplt.show()","2562dd4c":"# Checking out our mean sales price for average school size, scattered versus price shows a polynomial relationship\nlat_prices = train_data.groupby('avgSchoolSize')['price'].mean()\n\nplt.scatter(lat_prices.index, lat_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('lat')\nplt.ylabel('sales price')\nplt.show()","1f947105":"def test_feature_combinations(price, variables):\n    \n    \"\"\"Function takes in target price and a dataframe of independent variables, and \n    tests model improvement for each combination of variables\n    ARGUMENTS:\n    Y of target values\n    X-dataframe of continuous features\n    Returns dataframe of score improvements over base score for each interaction combination\"\"\"\n    \n    # select our estimator and our cross validation plan\n    regression = LinearRegression()\n    cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n    \n    # prepare our scoring dataframe\n    scoring_df = pd.DataFrame()\n    \n    # prepare our lists to store our features and scores as we iterate\n    scores = []\n    feature1 = []\n    feature2 = []\n    \n    # Get a list of all of our features, and remove our target variable 'price' from the list\n    features = list(variables.columns)\n\n    # make a list of all of our possible feature combinations\n    feature_combos = itertools.combinations(features, 2)\n    feature_combos = list(feature_combos)\n    \n    # set our y-value as our target variable\n    y = price\n    \n    # prepare our x-value with our independent variables. We do an initial split here in order to run a \n    # linear regression to get a base r^2 on our basic model without interactions\n    X = variables\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n    base_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)   \n    print(\"Model base score is \",base_score)\n    \n    # now we run the regression on each feature combo\n    for feature in feature_combos:\n        feat1, feat2 = feature[0], feature[1]\n        \n        # create the test interaction on our data set\n        variables['test_interaction'] = variables[feat1] * variables[feat2]\n        # create a new X which includes the test interaction and drops our target value\n        X = variables\n        # make a new split so that our x-splits include the test interaction\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n        \n        # Run a linear regression with cross-val just like our base model, and append the score to our scores list\n        new_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)\n        scores.append(new_score)\n        # put feature 1 on a list\n        feature1.append(feat1)\n        # put feature 2 on a list\n        feature2.append(feat2)\n        print(feat1, feat2, new_score)\n        \n        \n    \n    # load all of our lists into the scoring dataframe\n    scoring_df['feature1'] = feature1\n    scoring_df['feature2'] = feature2\n    scoring_df['scores'] = scores\n    scoring_df['improvement'] = scoring_df['scores'] - base_score\n    variables.drop('test_interaction', axis=1, inplace=True)\n    \n    # return our scoring dataframe to the function\n    return scoring_df","6227327d":"x_continuous.drop('price', axis=1, inplace=True)","dc0dc8c1":"# running our function on our continuous variables to look for improvement\n# our R2 is much lower for model base score because we aren't including our categorical variables in this improvement assessment\n\nscoring_df = test_feature_combinations(y_train, x_continuous)","f634ff0a":"# showing our improvement scores for our interactions\n\nscoring_df.sort_values('improvement', ascending=False)","0c428d5d":"x_continuous['sqft-zip'] = np.sqrt(x_continuous['livingAreaSqFt']) + np.sqrt(x_continuous['zip_smooth'])","470ec286":"# check out our histograms\n\nx_continuous.hist(figsize=(18,15), bins='auto');","0c9e4a5b":"x_continuous.describe()","4a5ad686":"# We're going to log transform our continuous variables, so we need to add a slight number to our zero values\nx_continuous.loc[x_continuous['numOfAppliances']==0, 'numOfAppliances'] = .1\n\n# log transform\nlog_continuous = np.log(x_continuous)\n\n# standardize all of our values with scikit-learn StandardScaler\nscaler = StandardScaler()\n\n#transformed_scaled_continuous = pd.DataFrame(scaler.fit_transform(x_train_cont_log),columns = x_train_cont_log.columns)\nscaled_continuous = pd.DataFrame(scaler.fit_transform(log_continuous),columns = log_continuous.columns)\nscaled_continuous.head(5)\n\n# make a processed bins file for use with linear regressiona\n# We're making TWO continuous sets. One is high one hot encoding. One is low one hot encoding, and includes the \n# categoricals that are target encoded as continuous instead\n\nhigh_one_hot_cont =  ['numPriceChanges', \n              'lotSizeSqFt', 'livingAreaSqFt', 'avgSchoolDistance', \n              'avgSchoolRating', 'avgSchoolSize', 'numOfBedrooms', \n              'numOfStories', 'numOfPhotos', \n              'numOfAppliances', 'sqft-zip']\nlow_one_hot_cont =  ['numPriceChanges', \n              'lotSizeSqFt', 'livingAreaSqFt', 'avgSchoolDistance', \n              'avgSchoolRating', 'avgSchoolSize', 'numOfBedrooms', \n              'numOfStories', 'numOfPhotos', \n              'numOfAppliances',  \n             'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long', 'sqft-zip']\n\ndf_cont_high_one_hot = scaled_continuous[high_one_hot_cont]\ndf_cont_low_one_hot = scaled_continuous[low_one_hot_cont]","26000618":"def plot_polys(y, xlabel, title):\n    '''Takes in a y-axis, x-axis label, and title and plots with various polynomial levels\n    ARGUMENTS:\n    y axis variable values\n    x-axis label\n    visualization title'''\n    x = y.index\n    \n    # express numbers as arrays and reshape\n    y = np.array(y)\n    x = np.array(x)\n    x = x.reshape(-1, 1)\n    \n    # make sure indices match up\n    y = y[x[:,0].argsort()]\n    x = x[x[:, 0].argsort()]\n\n    # plot figure\n    plt.figure(figsize=(16, 8))\n\n    # standard linear regression\n    linreg = LinearRegression()\n    linreg.fit(x, y)\n\n    # 2nd degree polynomial regression\n    poly2 = PolynomialFeatures(degree=2)\n    x_poly2 = poly2.fit_transform(x)\n    poly_reg2 = LinearRegression()\n    poly_reg2.fit(x_poly2, y)\n\n    # third degree polynomial regression \n    poly3 = PolynomialFeatures(degree=3)\n    x_poly3 = poly3.fit_transform(x)\n    poly_reg3 = LinearRegression()\n    poly_reg3.fit(x_poly3, y)\n\n    # predict on x values\n    pred = linreg.predict(x)\n    pred2 = poly_reg2.predict(x_poly2)\n    pred3 = poly_reg3.predict(x_poly3)\n\n    # plot regression lines\n    plt.scatter(x, y)\n    plt.yscale('log')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Average')\n    plt.plot(x, pred, c='red', label='Linear regression line')\n    plt.plot(x, pred2, c='yellow', label='Polynomial regression line 2')\n    plt.plot(x, pred3, c='#a3cfa3', label='Polynomial regression line 3');","d55c7f75":"# group by average month sold mean to see relationship\ny = train_data.groupby('latest_salemonth')['price'].mean()\nplot_polys(y, \"Month\", \"Month Sold Mean\")","76e71b3b":"# adding our chosen polynomial features\n\ndef create_polynomial_array(data, column, num_features):\n    values = data[column]\n    poly_array = np.array(values)\n    poly_array = poly_array.reshape(-1,1)\n    poly_fit = PolynomialFeatures(degree=num_features, include_bias=False)\n    fit_features = poly_fit.fit_transform(poly_array)\n    poly_df = pd.DataFrame(fit_features)\n    return poly_df\n\nmonth_poly = create_polynomial_array(df_cont_low_one_hot, 'month_smooth',2)\n\ndf_cont_low_one_hot['month1'] = month_poly[1]\n","980f2dbf":"# make a new dataframe with just the descriptions from our train data\nnlp = ['sentences']\nnlp_train = train_data[nlp]\nnlp_train.head(10)","5ce86e9f":"# prepare the vectorizer with the chosen parameters\nprocessor = TfidfVectorizer(sublinear_tf=True, use_idf=True, max_df=0.9, min_df=.005, ngram_range=(1,4), max_features=1000)\n# fit the vectorizer to the data\nx = processor.fit_transform(nlp_train['sentences'])\n\n# cast the vector array to a data frame with columns named by the features selected by the vectorizer\ntrain_word_vectors = pd.DataFrame(x.toarray(), columns=processor.get_feature_names())","e267635d":"# run a test model using the word vectors\npredictors_train = sm.add_constant(train_word_vectors)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","12e4a33a":"model = LinearRegression()\nmodel.fit(train_word_vectors, y_train)\n\nfrom sklearn.inspection import permutation_importance\nr = permutation_importance(model, train_word_vectors, y_train,\n                           n_repeats=10,\n                            random_state=0,\n                          n_jobs=-1)\n\nimportances = {}\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        importances[train_word_vectors.columns[i]] = r.importances_mean[i]\n    else: continue\n        \nimportances\n\nimportant_ngrams = list(importances.keys())\nprint(important_ngrams)","91a9b81b":"len(important_ngrams)","7f5ad243":"train_word_vectors_refined = train_word_vectors[important_ngrams]\n\n# run a test model using the word vectors\npredictors_train = sm.add_constant(train_word_vectors_refined)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","91eda4fb":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = train_word_vectors_refined.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 75% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.95)]\n","43c975c5":"important_ngrams","eb3eb75d":"remove = ['dryer', 'conveniently locate', 'tub separate shower', 'steel appliance', 'stainless steel', 'garden tub separate shower',\n                'mid century', 'corner lot', 'natural light', 'recess lighting', 'hike bike', 'sport court', 'walnut creek',\n                'hill country', 'south congress', 'sys yes', 'quiet cul de', 'garden tub separate']\n\nfor item in remove:\n    important_ngrams.remove(item)\n\ntrain_word_vectors_refined = train_word_vectors[important_ngrams]","25797aab":"holdout_categoricals = holdout[categories]\n\n# binning our year built bins\nholdout_categoricals[\"year_block\"], year_bins = pd.qcut(holdout_categoricals['yearBuilt'], q=num_bins, retbins=True, labels=labels)\n\nholdout_categoricals.drop('yearBuilt', axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in holdout_categoricals.columns:\n    holdout_categoricals[item] = holdout_categoricals[item].astype('category')\n\n# make a processed bins file for use with linear regression\ndf_cats_high_one_hot_holdout = pd.get_dummies(holdout_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=True)\ndf_cats_low_one_hot_holdout = pd.get_dummies(holdout_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=True)","05b61538":"# apply target encoding to test data, using train data to map\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\nholdout['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, holdout)\nholdout['year_smooth'] = calc_smooth_mean(train_data, 'yearBuilt', 'price', 300, holdout)\nholdout['month_smooth'] = calc_smooth_mean(train_data, 'latest_salemonth', 'price', month_samples, holdout)\n\n# Create a wider lat and long zone to calculate an area mean\nholdout['lat_zone'] = round(holdout['latitude'], 2)\nholdout['long_zone'] = round(holdout['longitude'], 2)\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\nholdout['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, holdout)\nholdout['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, holdout)\nholdout['lat_long'] = round(np.sqrt(holdout['lat_smooth']) + np.sqrt(holdout['long_smooth']), 0)\n\n","7394545a":"holdout['year_smooth'].isna().sum()\n# a few listings weren't able to create year smooth information, so we just impute it","d738213c":"# a few listings weren't able to create year smooth information, so we just impute it\nholdout.loc[holdout['year_smooth'].isna(), 'year_smooth'] = train_data['year_smooth'].mean()\n\nholdout_continuous = holdout[continuous]\n\nholdout_continuous['sqft-zip'] = np.sqrt(holdout_continuous['livingAreaSqFt']) + np.sqrt(holdout_continuous['zip_smooth'])\n\n# We're going to log transform our continuous variables, so we need to add a slight number to our zero values\nholdout_continuous.loc[holdout_continuous['numOfAppliances']==0, 'numOfAppliances'] = .1\n\n# log transform\nlog_continuous = np.log(holdout_continuous)\n\nscaled_holdout_continuous = pd.DataFrame(scaler.transform(log_continuous),columns = log_continuous.columns)\n\n# making our two continuous sets\ndf_cont_high_one_hot_holdout = scaled_holdout_continuous[high_one_hot_cont]\ndf_cont_low_one_hot_holdout = scaled_holdout_continuous[low_one_hot_cont]\n\n# adding polynomial features\nmonth_poly = create_polynomial_array(df_cont_low_one_hot_holdout, 'month_smooth',2)\ndf_cont_low_one_hot_holdout['month1'] = month_poly[1]","7fb9d611":"holdout_continuous","7bb62822":"# create our data frame of descriptions for the holdout set\nnlp_holdout = holdout[nlp]\n\n# transform our holdout set using the same vectorizer as our train set (it will make the same set of ngrams)\nx = processor.transform(nlp_holdout['sentences'])\n\n# cast our word vectors to data frame\nholdout_word_vectors = pd.DataFrame(x.toarray(), columns=processor.get_feature_names())\n\n# refine the holdout words to the same as the train set\nholdout_word_vectors_refined = holdout_word_vectors[important_ngrams]","00263fd1":"# make our train sets for one-hot encoded and target-encoded categoricals\nX_train_onehot = pd.concat([df_cont_high_one_hot, df_cats_high_one_hot, train_word_vectors_refined], axis=1)\nX_train_encoded = pd.concat([df_cont_low_one_hot, df_cats_low_one_hot, train_word_vectors_refined], axis=1)\n\n# make our test sets for one-hot encoded and target-encoded categoricals\nX_test_onehot = pd.concat([df_cont_high_one_hot_holdout, df_cats_high_one_hot_holdout, holdout_word_vectors_refined], axis=1)\nX_test_encoded = pd.concat([df_cont_low_one_hot_holdout, df_cats_low_one_hot_holdout, holdout_word_vectors_refined], axis=1)\n\n# make our target variable train and test sets, after log transforming our target variable\ntarget = 'price' # target variable\ny = np.log(df[target]) # our log-transformed target variable\n\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate) #\ntest_actual = np.exp(y_test)\n\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)","d3198e21":"# prepare dictionary to store results\nmodels = {}\nmodels['Models'] = []\nmodels['r2'] = []\nmodels['mae'] = []\nmodels['rmse'] = []","67cf6930":"def make_model_log(model, train, test, y_train, y_test, title):\n    \n    model.fit(train, y_train)\n    cv_5 = cross_val_score(model, train, y_train, cv=5)\n    r2 = round(cv_5.mean()*100,2)\n    \n    test_predictions = model.predict(test)\n    \n    # reverse log transform our predicted values\n    test_predictions_unscaled = np.exp(test_predictions).astype(int)\n    test_predictions_unscaled = test_predictions_unscaled.flatten()\n    \n    test_actual = np.exp(y_test)\n    \n    # get residuals\n    residuals = test_actual - test_predictions_unscaled\n\n    fig = plt.figure(figsize=(20,15))\n    plt.scatter(test_predictions_unscaled, residuals)\n    plt.show()\n    \n    # Calculate our errors\n    mae = round(mean_absolute_error(test_actual, test_predictions_unscaled), 2)\n    rmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions_unscaled)), 2)\n\n    # append our results to our lists\n    models['Models'].append(title)\n    models['r2'].append(r2)\n    models['mae'].append(mae)\n    models['rmse'].append(rmse)\n\n    print(\"R2: \", r2, \"\\nMAE: \", mae, \"\\nRMSE: \", rmse, \"\\n{} predictors used for this model\".format(train.shape[1]))","c78d48f9":"def make_model_nolog(model, train, test, y_train, y_test, title):\n    \n    model.fit(train, y_train)\n    cv_5 = cross_val_score(model, train, y_train, cv=5)\n    r2 = round(cv_5.mean()*100,2)\n    \n    test_predictions = model.predict(test)\n\n    residuals = y_test - test_predictions\n\n    fig = plt.figure(figsize=(20,15))\n    plt.scatter(test_predictions, residuals)\n    plt.show()\n    \n    # Calculate our errors\n    mae = round(mean_absolute_error(test_actual, test_predictions), 2)\n    rmse = round(np.sqrt(mean_squared_error(test_actual, test_predictions)), 2)\n\n    # append our results to our lists\n    models['Models'].append(title)\n    models['r2'].append(r2)\n    models['mae'].append(mae)\n    models['rmse'].append(rmse)\n\n    print(\"R2: \", r2, \"\\nMAE: \", mae, \"\\nRMSE: \", rmse, \"\\n{} predictors used for this model\".format(train.shape[1]))","03bb0861":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['livingAreaSqFt']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Total Square Footage')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Square Footage - Response is Median_Zip')\n\n\nplt.scatter(z, residuals, label=\"sample\");","5be2a7bd":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['avgSchoolRating']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Average School Rating')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Average School Rating - Response is Median Zip Code')\n\n\nplt.scatter(z, residuals, label=\"sample\");","f6b20a3e":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['lotSizeSqFt']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Lot Size Square Feet')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Lot Size Square Feet - Response is Median_Zip')\n\nplt.scatter(z, residuals, label=\"sample\");","ef4ae106":"x, y = np.array(df['median_zip']).reshape(-1,1), df['price']\nz = np.array(df['numOfBedrooms']).reshape(-1,1)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ntest_predictions = model.predict(x)\n\nresiduals = y - test_predictions\n\nfig = plt.figure(figsize=(15,10))\n\n# Add labels for x and y axes\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('Residuals')\n\n# Add a title for the plot\nplt.title('Residuals vs Number of Bedrooms - Response is Median_Zip')\n\nplt.scatter(z, residuals, label=\"sample\");","2c6d88db":"# put together our basic feature set and preprocess\n\n# one-hot encode categorical\nbase_cat = pd.DataFrame()\nbase_cat['zipcode'] = df['zipcode']\nbase_cat['zipcode'] = base_cat['zipcode'].astype('category')\nbase_cat_processed = pd.get_dummies(base_cat['zipcode'], prefix='zipcode', drop_first=True)\nbase_cat_processed.reset_index(inplace=True)\nbase_cat_processed.drop('index', axis=1, inplace=True)\n\n# log transform and standard scale our continuous\nbase_cont = df[['avgSchoolRating', 'livingAreaSqFt', 'numOfBathrooms', 'lotSizeSqFt']]\nbase_cont = np.log(base_cont)\nscaler = StandardScaler()\nbase_cont_processed = pd.DataFrame(scaler.fit_transform(base_cont),columns = base_cont.columns)\n\n#join cat and cont into predictor data frame\nx_base_set = base_cont_processed.join([base_cat_processed], how='inner') \n\n# train\/test split\nx_base_train, x_base_test = train_test_split(x_base_set, test_size=0.2, random_state=randomstate)","f5e6f236":"model = LinearRegression()\nmake_model_log(model, x_base_train, x_base_test, y_train, y_test, 'Basic LR - Top Features Only, One-Hot')","c0854ff8":"# put together our basic feature set and preprocess\n\n# log transform and standard scale our continuous\nbase_cont = train_data[['avgSchoolRating', 'livingAreaSqFt', 'numOfBathrooms', 'lotSizeSqFt', 'zip_smooth']]\nbase_cont = np.log(base_cont)\nscaler = StandardScaler()\nx_base_train = pd.DataFrame(scaler.fit_transform(base_cont),columns = base_cont.columns)\n\n\ntest_cont = holdout[['avgSchoolRating', 'livingAreaSqFt', 'numOfBathrooms', 'zip_smooth', 'zip_smooth']]\ntest_cont = np.log(test_cont)\nscaler = StandardScaler()\nx_base_test = pd.DataFrame(scaler.fit_transform(test_cont),columns = test_cont.columns)","c5a72e70":"model = LinearRegression()\nmake_model_log(model, x_base_train, x_base_test, y_train, y_test, 'Basic LR - Top Features Only, Target Encoded')","8a85619a":"predictors_train = sm.add_constant(X_train_onehot)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","46fe8a01":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = X_train_onehot.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 75% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.95)]\n","8cec2316":"model = LinearRegression()\nmake_model_log(model, X_train_onehot, X_test_onehot, y_train, y_test, 'LR All - One Hot')","3027385e":"# We need our statsmodels model again to plot residuals\npredictors_train = sm.add_constant(X_train_onehot)\nmodel = sm.OLS(y_train, predictors_train).fit()","ad59daf3":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"livingAreaSqFt\", fig=fig)\nplt.show()","859eb2c2":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"avgSchoolRating\", fig=fig)\nplt.show()","f69b3ca2":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"lotSizeSqFt\", fig=fig)\nplt.show()","6c1ca279":"fig = plt.figure(figsize=(15,8))\nfig = sm.graphics.plot_regress_exog(model, \"numOfBedrooms\", fig=fig)\nplt.show()","80310c37":"predictors_train = sm.add_constant(X_train_encoded)\nmodel = sm.OLS(y_train, predictors_train).fit()\nmodel.summary()","e05b9ce2":"model = LinearRegression()\nmake_model_log(model, X_train_encoded, X_test_encoded, y_train, y_test, 'LR All - Encoded')","b08b0e98":"model = LinearRegression()\nmodel.fit(X_train_onehot, y_train)\n\nfrom sklearn.inspection import permutation_importance\nr = permutation_importance(model, X_train_onehot, y_train,\n                           n_repeats=15,\n                            random_state=0,\n                          n_jobs=-1)\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        print(f\"{X_train_onehot.columns[i]:<8} \"\n            f\"\\t\\tImportance: {r.importances_mean[i]:.3f} \")\n        \nimportances = {}\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        importances[X_train_onehot.columns[i]] = r.importances_mean[i]\n    else: continue\n        \nimportances\n\nimportant_features_again = list(importances.keys())\nprint(important_features_again)","f6064bb9":"permutation_x_train = X_train_onehot[important_features_again]\npermutation_x_test = X_test_onehot[important_features_again]\n\nmodel = LinearRegression()\nmake_model_log(model, permutation_x_train, permutation_x_test, y_train, y_test, 'LR w\/Permutation Importance')","b7683c65":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = included[pvalues.argmax()]\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n","f4ef2fe6":"result = stepwise_selection(X_train_onehot, y_train, verbose=True)\n\nprint('resulting features:', result)","ecc291c9":"# Run our linear regression again, using only the features recommended by our feature selector\n\nX_train_refined = X_train_onehot[result]\nX_test_refined = X_test_onehot[result]\n\npredictors_int = sm.add_constant(X_train_refined)\nmodel = sm.OLS(y_train, predictors_int).fit()\nmodel.summary()","a02c6a89":"model = LinearRegression()\nmake_model_log(model, X_train_refined, X_test_refined, y_train, y_test, 'LR w\/Forward-Backward Selector')","eb842189":"# Using sklearn RFECV to perform integrated CV while picking the number of features\n# picks the number of features itself\n\nmodel = LinearRegression(n_jobs=-4)\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n\nrfecv = RFECV(estimator=model, step=1, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-4)\n\n# fit model to train set\nrfecv.fit(X_train_onehot, y_train)\n\n# print optimal number of features\nprint('Optimal number of features: {}'.format(rfecv.n_features_))","d1f1927e":"dset = pd.DataFrame()\ndset['attr'] = X_train_onehot.columns\ndset['used'] = rfecv.support_\n\n# make a list of the features used in the rfecv\nrfecv_result = list(dset[(dset['used'] == True)]['attr'])\n\n# Show the features that RFECV did not use\ndset[dset['used']==False]","d4eddf35":"# Run our linear regression again in statsmodels, using the features recommended by our feature selector\n\nX_train_rfecv = X_train_onehot[rfecv_result]\nX_test_rfecv = X_test_onehot[rfecv_result]\n\npredictors_int = sm.add_constant(X_train_rfecv)\nmodel = sm.OLS(y_train, predictors_int).fit()\nmodel.summary()","f36dbffb":"model = LinearRegression()\nmodel.fit(X_train_rfecv, y_train)\n\nmake_model_log(model, X_train_rfecv, X_test_rfecv, y_train, y_test, 'LR w\/RFECV')","ed89711b":"plt.figure(figsize=(16, 9))\nplt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Number of features selected', fontsize=14, labelpad=20)\nplt.ylabel('R2', fontsize=14, labelpad=20)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n\nplt.show()","11e860c7":"from sklearn.linear_model import Lasso, LassoCV, LassoLarsCV, LassoLarsIC\n\nmodel_bic = LassoLarsIC(criterion='bic')\nmodel_bic.fit(X_train_onehot, y_train)\nalpha_bic_ = model_bic.alpha_\nprint(\"BIC alpha:\", round(model_bic.alpha_,4))\n\nmodel_aic = LassoLarsIC(criterion='aic')\nmodel_aic.fit(X_train_onehot, y_train)\nalpha_aic_ = model_aic.alpha_\nprint(\"AIC alpha:\", round(model_aic.alpha_, 4))\n\n\ndef plot_ic_criterion(model, name, color):\n    alpha_ = model.alpha_\n    alphas_ = model.alphas_\n    criterion_ = model.criterion_\n    plt.plot((alphas_), criterion_, '--', color=color, linewidth=2, label= name)\n    plt.axvline((alpha_), color=color, linewidth=2,\n                label='alpha for %s ' % name)\n    plt.xlabel('-log(alpha)')\n    plt.ylabel('criterion')\n\nplt.figure()\nplot_ic_criterion(model_aic, 'AIC', 'green')\nplot_ic_criterion(model_bic, 'BIC', 'blue')\nplt.legend()\nplt.title('Information-criterion for model selection');","bb98b102":"train_mse = []\ntest_mse = []\nalphas = []\n\nfor alpha in np.linspace(-10, 10, num=50):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_train_onehot, y_train)\n    \n    train_preds = lasso.predict(X_train_onehot)\n    train_mse.append(mean_squared_error(y_train, train_preds))\n    \n    test_preds = lasso.predict(X_test_onehot)\n    test_mse.append(mean_squared_error(y_test, test_preds))\n    \n    alphas.append(alpha)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots()\nax.plot(alphas, train_mse, label='Train')\nax.plot(alphas, test_mse, label='Test')\nax.set_xlabel('Alpha')\nax.set_ylabel('MSE')\n\n# np.argmin() returns the index of the minimum value in a list\noptimal_alpha = alphas[np.argmin(test_mse)]\n\n# Add a vertical line where the test MSE is minimized\nax.axvline(optimal_alpha, color='black', linestyle='--')\nax.legend();\n\nprint(f'Optimal Alpha Value: {int(optimal_alpha)}')","82650495":"lasso = LassoCV(max_iter=100000, cv=5)\nlasso.fit(X_train_onehot, y_train)\n\nmake_model_log(lasso, X_train_onehot, X_test_onehot, y_train, y_test, 'Lasso')\n\nprint('The optimal alpha for the Lasso Regression is: ', lasso.alpha_)","c2e0989e":"ridge = RidgeCV(cv=5, alphas=(.01, .05,.1, .5, 1, 10))\nridge.fit(X_train_onehot, y_train)\n\nmake_model_log(ridge, X_train_onehot, X_test_onehot, y_train, y_test, 'Ridge')\n\nprint('The optimal alpha for the Ridge Regression is: ', ridge.alpha_)","71e26c42":"mae_val = [] #to store mae values for different k\n\n# checks mean absolute error scores on k from 1 to 20\nfor K in range(20):\n    K = K+1\n    \n    # set up the KNN regressor\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train_encoded, y_train)  #fit the model\n    pred=model.predict(X_test_encoded) #make prediction on test set\n    error = mean_absolute_error(y_test,pred) #calculate rmse\n    mae_val.append(error) #store mae values\n    print('MAE value for k= ' , K , 'is:', error)\n    \n# gets optimal k-value based on score minimum\nindex_min = np.argmin(mae_val) + 1\n\n# makes model and fits using optimal k\nmodel = neighbors.KNeighborsRegressor(n_neighbors = index_min)\nmake_model_log(model, X_train_encoded, X_test_encoded, y_train, y_test, 'KNN')","509a2d23":"'''# Parameter Tuning\n\nparam_grid = {'kernel' : ['linear', 'rbf'],\n              'gamma' : ['scale', 'auto'],\n              'C':[3,6,10],\n              'epsilon':[.05, .1]\n              }\n\nsvr = SVR(tol=.005, verbose=True)\ngrid_search = GridSearchCV(svr, param_grid, verbose=10, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n\ngrid_search.fit(X_train_onehot, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))'''","d40ac92b":"# setting up estimator with our optimal parameters\nparams = {'kernel' : 'rbf', 'C' : 10, 'epsilon' : .05, 'gamma':'auto'}\nsvr = SVR(**params, verbose=True, tol=.005)\n\nmake_model_log(svr, X_train_onehot, X_test_onehot, y_train, y_test, 'SVR')","630d7473":"# categoricals with first not dropped for tree\nboost_train_cats = pd.get_dummies(df_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=False)\nboost_test_cats= pd.get_dummies(holdout_categoricals[high_one_hot_cat], prefix=high_one_hot_cat, drop_first=False)\n\n# continuous not transformed or standardized\nboost_train_continuous = x_continuous[high_one_hot_cont]\nboost_holdout_continuous = holdout_continuous[high_one_hot_cont]\n\n# decision tree regression train and test sets\nx_train_boost = pd.concat([boost_train_continuous, boost_train_cats, train_word_vectors_refined], axis=1)\nx_test_boost = pd.concat([boost_holdout_continuous, boost_test_cats, holdout_word_vectors_refined], axis=1)\n\n# redoing our y_train and y_test as non-log transformed\ny = df[target] # our target variable\n\n# creating our train\/validation sets and our test sets\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate)\n","caaab05f":"best_xgb_model = xgb.XGBRegressor(\n                n_estimators=500,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                colsample_bytree = .4,\n                alpha=75,\n                reg_lambda = 1,\n                seed=42,\n                missing=0,\n                eval_metric='mae') \n\nmake_model_nolog(best_xgb_model, x_train_boost, x_test_boost, y_train, y_test, 'XGB - One-hot')","e3c6d391":"# categoricals with first not dropped for tree\nboost_train_cats = pd.get_dummies(df_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=False)\nboost_test_cats= pd.get_dummies(holdout_categoricals[low_one_hot_cat], prefix=low_one_hot_cat, drop_first=False)\n\n# continuous not transformed or standardized\nboost_train_continuous = x_continuous[low_one_hot_cont]\nboost_holdout_continuous = holdout_continuous[low_one_hot_cont]\n\n# decision tree regression train and test sets\nx_train_boost_encoded = pd.concat([boost_train_continuous, boost_train_cats, train_word_vectors_refined], axis=1)\nx_test_boost_encoded = pd.concat([boost_holdout_continuous, boost_test_cats, holdout_word_vectors_refined], axis=1)\n\n# redoing our y_train and y_test as non-log transformed\ny = df[target] # our target variable\n\n# creating our train\/validation sets and our test sets\ny_train, y_test = train_test_split(y, test_size=0.2, random_state=randomstate)","46a3f9be":"best_xgb_model = xgb.XGBRegressor(\n                n_estimators=500,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                colsample_bytree = .8,\n                alpha=1,\n                reg_lambda = 1,\n                seed=42,\n                missing=0,\n                eval_metric='mae')\n\nmake_model_nolog(best_xgb_model, x_train_boost_encoded, x_test_boost_encoded, y_train, y_test, 'XGB - Encoded')","88d36ce1":"# make data frame from our models dictionary\nmodel_types = pd.DataFrame(models)\n\n# sort data frame by mae and reset index\nmodel_types = model_types.sort_values('mae', ascending=True).reset_index()\nmodel_types.drop('index',axis=1, inplace=True)\nmodel_types.set_index('Models', inplace=True)\n\nmodel_types","a9035155":"# plot model mae\n\nplt.figure(figsize=(15,10))\nplt.plot(model_types['mae'])\nplt.title(\"Mean Average Error\")\nplt.xticks(rotation=90)\nplt.xlabel('Model')\nplt.ylabel(\"MAE\");","1fe7c13f":"# refresh on our original data frame\ndf","5b8db63c":"# get the columns we are going to make visualizations from\nviz_df = df[['price', 'livingAreaSqFt', 'median_zip', 'zip_rank', 'avgSchoolRating']]\nviz_df['pr_sf'] = round(viz_df['price']\/viz_df['livingAreaSqFt'], 2)\nviz_df","0f1c8e1a":"# make simpler variables for our visualiation variables\nviz_target = viz_df['price']\/100000\nviz_sqft = viz_df['livingAreaSqFt']\nviz_grade = viz_df['avgSchoolRating']\nviz_zip = viz_df['zip_rank'] \nviz_zip2 = viz_df['median_zip']\nviz_pr_sf = viz_df['pr_sf']","b8ffd977":"# prepare figure\nfig, ax = plt.subplots(figsize=(20, 15))\n\n#scatter our data\nscatter3 = sns.scatterplot(x=viz_sqft, y=\"price\", data=viz_df, hue='zip_rank', palette='magma_r')\n#ax.scatter(viz_sqft, viz_target, c=viz_zip, cmap='magma_r')\n\n# label our axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price in $100,000', fontsize=16)\nax.set_title(\"Price per Total Square Footage\\nby Zip Code Median Value Rank\", fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/sqft.png');","004e5680":"# prepare figure\nfig, ax = plt.subplots(figsize=(20, 15))\n\n#scatter our data\nscatter3 = sns.scatterplot(x=viz_sqft, y=\"pr_sf\", data=viz_df, hue='zip_rank', palette='magma_r')\n#ax.scatter(viz_sqft, viz_target, c=viz_zip, cmap='magma_r')\n\n# label our axes and title\nax.set_xlabel('Total Square Footage', fontsize=16)\nax.set_ylabel('Price per Square Foot', fontsize=16)\nax.set_title(\"Price per per Square Foot to Total Square Footage\\nby Zip Code Median Value Rank\", fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/pr_sf_zip.png');","1f142b46":"viz_y = viz_df['price']\nviz_x = viz_df.drop('price', axis=1)\n\nfig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(viz_sqft, viz_grade, viz_target, c=viz_zip, cmap='magma_r')\n#ax.scatter(viz_sqft, viz_grade, viz_target, c='red', label=\"Predictions\")\n#ax.scatter(viz_sqft, viz_grade, end_z\/100000, c='green', label=\"Actuals\")\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('School Rating', fontsize=12)\nax.set_zlabel('Price', fontsize=12)\n\nax.set_title(\"Price per Square Footage and School Rating, by Zip Median Rank\", fontsize=20)\n\n# first num is tilt angle, second num is turn angle\n# default is about 30,305\n# 0, 270 creates side view of pr\/sqft\n# 0, 360 creates side view of pr\/grade\nax.view_init(30, 305)\n\n\n# save visualization to png\n#plt.savefig('images\/3d_feats.png');","51882fd5":"break","b6244b80":"# visualize changes to model score as it is tried on different max depths from 10 to 80, to get a starting point for max depth\n\nfrom sklearn.model_selection import validation_curve\ndef ValidationCurve(estimator, predictors, target, param_name, hyperparam):\n    \n    train_score, test_score = validation_curve(estimator, predictors, target, \n                                               param_name, np.arange(1,20,2), \n                                               cv=5, scoring='r2', n_jobs=-4)\n    Rsqaured_train = train_score.mean(axis=1)\n    Rsquared_test= test_score.mean(axis=1)\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(np.arange(1,20,2), Rsqaured_train, color='r', linestyle='-', marker='o', label='Training Set')\n    plt.plot(np.arange(1,20,2), Rsquared_test, color='b', linestyle='-', marker='x', label='Testing Set')\n    plt.legend(labels=['Training Set', 'Testing Set'])\n    plt.xlabel(hyperparam)\n    plt.ylabel('R_squared')\n    plt.title(\"R^squared for Max Depth on Train\/Test\")\n    \nValidationCurve(xgb.XGBRegressor(), x_train_boost, y_train, 'max_depth', 'Maximum Depth')","fe38206d":"# Parameter Tuning max_depth\n\nparam_grid = {\"max_depth\": [4,5,6,]\n                          \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,                                                                    \n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","15777ba3":"# Parameter Tuning learning_rate\n\nparam_grid = {\"learning_rate\" : [.005, .01, .05, .1, .2, .3, .5]  \n                          \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                 max_depth=4,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","2278fe3e":"# Parameter Tuning max_depth and min_child_weight\n\nparam_grid = {\"min_child_weight\" : [6, 8, 10]            \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                learning_rate = .2,\n                 max_depth=4,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","7e804ce8":"# Parameter Tuning max_depth and min_child_weight\n\nparam_grid = {\"min_child_weight\" : [3, 4, 5, 6, 7]            \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                learning_rate = .2,\n                 max_depth=4,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","5a131f1e":"# Parameter Tuning gamma\n\nparam_grid = {'gamma':[.005, .01, .05, .1, .3, .5, 1]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","ea01f21f":"# Parameter Tuning subsample\n\nparam_grid = {\n 'subsample':[.2, .4, .6, .8, 1],\n \n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","6577caf2":"# Parameter Tuning colsample_by_tree\n\nparam_grid = {\n 'colsample_bytree':[.2, .4, .6, .8, 1]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","68cf90a5":"# Parameter Tuning alpha\n\nparam_grid = {\n    'reg_alpha':[0.1, 1, 10, 100, 250, 500]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                colsample_bytree = .4,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","4056a6b2":"# Parameter Tuning alpha\n\nparam_grid = {\n    'reg_alpha':[50, 75, 100, 125, 150]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                colsample_bytree = .4,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","67cc8042":"# Parameter Tuning lambda\n\nparam_grid = {'lambda':[0.1, 1, 10, 100, 500, 1000, 2500]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                colsample_bytree = .4,\n                alpha=75,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","0dcf89c6":"# Parameter Tuning num_estimators\n\nparam_grid = {'n_estimators':[250, 500, 1000, 5000]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .2,\n                min_child_weight=6,\n                gamma=.005,\n                colsample_bytree = .4,\n                alpha=75,\n                reg_lambda = 1,\n                seed=42,\n                missing=0,\n                eval_metric='mae') \n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","494e8081":"# visualize changes to model score as it is tried on different max depths from 10 to 80, to get a starting point for max depth\n\nfrom sklearn.model_selection import validation_curve\ndef ValidationCurve(estimator, predictors, target, param_name, hyperparam):\n    \n    train_score, test_score = validation_curve(estimator, predictors, target, \n                                               param_name, np.arange(1,20,2), \n                                               cv=5, scoring='r2', n_jobs=-4)\n    Rsqaured_train = train_score.mean(axis=1)\n    Rsquared_test= test_score.mean(axis=1)\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(np.arange(1,20,2), Rsqaured_train, color='r', linestyle='-', marker='o', label='Training Set')\n    plt.plot(np.arange(1,20,2), Rsquared_test, color='b', linestyle='-', marker='x', label='Testing Set')\n    plt.legend(labels=['Training Set', 'Testing Set'])\n    plt.xlabel(hyperparam)\n    plt.ylabel('R_squared')\n    plt.title(\"R^squared for Max Depth on Train\/Test\")\n    \nValidationCurve(xgb.XGBRegressor(), x_train_boost_encoded, y_train, 'max_depth', 'Maximum Depth')","85e9483f":"# Parameter Tuning max_depth\n\nparam_grid = {\"max_depth\": [3,4,5,6,]\n                          \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,                                                                    \n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-4)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","6c408871":"# Parameter Tuning learning_rate\n\nparam_grid = {\"learning_rate\" : [.005, .01, .02, .03, .1, .2, .3]            \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                 max_depth=4,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","c0fbd910":"# Parameter Tuning min_child_weight\n\nparam_grid = {\"min_child_weight\" : [4, 8, 12, 16, 20, 24, 28]            \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                 max_depth=4,\n                learning_rate = .1,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","019ef119":"# Parameter Tuning max_depth and min_child_weight\n\nparam_grid = {\"min_child_weight\" : [28, 32, 36]            \n              }\n\nmodel = xgb.XGBRegressor(\n                 n_estimators=250,\n                 max_depth=4,\n                learning_rate = .1,\n                 seed=42,\n                 missing=0,\n                 eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","47953a6b":"# Parameter Tuning gamma\n\nparam_grid = {'gamma':[.01, .05, .1, .3, .5, .7, .9]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","0f34a973":"# Parameter Tuning subsample\n\nparam_grid = {\n 'subsample':[.2, .4, .6, .8, 1],\n \n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","4a912c46":"# Parameter Tuning colsample_by_tree\n\nparam_grid = {\n 'colsample_bytree':[.2, .4, .6, .8, 1]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","b3c6966f":"# Parameter Tuning alpha\n\nparam_grid = {\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 100]\n    }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                colsample_bytree = .8,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","80a78833":"# Parameter Tuning lambda\n\nparam_grid = {'lambda':[0.1, 1, 10, 100, 250, 500, 1000, 2500]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                colsample_bytree = .8,\n                alpha=1,\n                seed=42,\n                missing=0,\n                eval_metric='mae' )\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","e5b504b6":"# Parameter Tuning num_estimators\n\nparam_grid = {'n_estimators':[250, 500, 1000, 5000]            \n              }\n\nmodel = xgb.XGBRegressor(\n                n_estimators=250,\n                max_depth=4,\n                learning_rate = .1,\n                min_child_weight=32,\n                gamma=.01,\n                colsample_bytree = .8,\n                alpha=1,\n                reg_lambda = 1,\n                seed=42,\n                missing=0,\n                eval_metric='mae') \n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv = 5, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(x_train_boost_encoded, y_train)\n\ngrid_search.best_estimator_\n\nprint(\"Best parameters set found on train set: \\n\")\nprint(grid_search.best_params_)\nprint(\"\\nGrid scores on train set:\\n\")\nmeans = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))","d0498b93":"We're now going to use natural language processing on our listing descriptions, which we cleaned earlier in the notebook using SpaCy.","5c5e24b4":"Our word vectors have some multicollinearity, which is not unexpected. Let's take a look and see if we can refine our terms list more.","96192c95":"Best parameters set found on train set: \n\n    {'gamma': 0.01}\n\n    Grid scores on train set:\n    \n    0.811 (+\/-0.029) for {'gamma': 0.01}\n    0.811 (+\/-0.029) for {'gamma': 0.05}\n    0.811 (+\/-0.029) for {'gamma': 0.1}\n    0.811 (+\/-0.029) for {'gamma': 0.3}\n    0.811 (+\/-0.029) for {'gamma': 0.5}\n    0.811 (+\/-0.029) for {'gamma': 0.7}\n    0.811 (+\/-0.029) for {'gamma': 0.9}","6c3a4627":"We're now going to work with some different model types that are entirely different from linear regression.\n\nThere's conflicting information on whether we should use one-hot encoding, or target encoding. We'll solve this by trying both and figuring out what works best for our data set.","547dbdc0":"We can use residuals plots to determine if features are important enough to add to our model. If we regress our target on a predictor, and then plot those residuals against a DIFFERENT predictor, our plot will tell us if the new feature might add to our model.\n\nWe're planning to one-hot encode our zips, but we'll use our zip_median continuous variable for now to start on.\n\nWe're going to add features in order of their correlation with price on our correlation heat map, so our base feature is zip code because it has the strongest correlation.\n\nWe regress our target on zip code, then we plot our residuals against total square footage.","2aa61f50":"Feature selectors are different methods to help us pick which features we want to use in our model. In our example above where we used ALL predictors in our linear regression, several of our features had a p-value over .05, which indicates that there is more than a 5% chance that the changes attributed to that feature were actually by random chance. We want features where our p-value is below a threshold that we specify where we are reasonably confident that the feature is contributing to the model and not by random chance.","3be460b9":"## Analysis\n\n> Our final model utilizes a combination of continuous variables and one-hot-encoded categoricals to produce a support vector machine regression with R^2 of 79.2%, a mean absolute error of 64k, and a root mean squared error of 105k. I tried several different zip code transformations including polynomial features, mean target encoding, lower-granularity binning, and median rank as a continuous, and ALL of these efforts resulted in a lower R^2 and higher mean absolute error, leading to a final decision to one-hot encode all zip codes individually. Similar efforts on other categoricals such as age and month sold also did not improve the model over one-hot encoding. This resulted in the greatest accuracy despite a model that is more \"messy\" with a large number of features.\n\n### What are the primary factors influencing housing prices in the Austin metro area?\n\n> Square footage is, unsurprisingly, a key player in house pricing. And as they say, location is everything, and it is the primary influencing factor for a home price in the Austin metro area. Number of bathrooms, school rating, and lot size all contributed a large amount as well.\n\n> These five features alone explain 71% of the price variance.\n\n### Can we effectively use a regression model based system for realtors to determine a proper list price?\n> Our model, while explaining about 80% of the price variance with our features, was nonetheless far from accurate in absolute terms. A mean average error of 64k in either direction is a huge variance to a home price - one that is so large that it renders the prediction much less meaningful and useful. Other models need to be explored, better data needs to be sourced, or easy-to-use features that an average realtor is capable of evaluating\/acquiring should be added to the model to improve its predictive quality. The model is providing at best a baseline starting point.\n\n### What are the features missing?\n> I believe this data set to be missing some key features that have high influence in other housing sets, foremost among them reasonable metrics of home condition, home quality, and neighborhood quality. We attempted to pick up some of the quality and condition metrics via NLP. While the NLP did contribute a significant amount of improvement to the model, it was insufficient to explain all of these missing metrics.\n","1651b744":"Our parameter tuning for XGBoost is in the APPENDIX","245243aa":"Best parameters set found on train set: \n\n    {'min_child_weight': 6}\n\n    Grid scores on train set:\n\n    0.805 (+\/-0.026) for {'min_child_weight': 6}\n    0.804 (+\/-0.025) for {'min_child_weight': 8}\n    0.805 (+\/-0.027) for {'min_child_weight': 10}","5c149365":"### Outlier Detection","5616ef56":"Best parameters set found on train set: \n\n    {'reg_alpha': 1}\n\n    Grid scores on train set:\n\n    0.811 (+\/-0.028) for {'reg_alpha': 1e-05}\n    0.811 (+\/-0.028) for {'reg_alpha': 0.01}\n    0.811 (+\/-0.028) for {'reg_alpha': 0.1}\n    0.811 (+\/-0.028) for {'reg_alpha': 1}\n    0.811 (+\/-0.028) for {'reg_alpha': 10}\n    0.810 (+\/-0.028) for {'reg_alpha': 100}","b1f27e26":"If we can see a pattern when we plot residuals vs a different predictor, it can tell us if a feature might add value to our model.","34f6406c":"Positive relationship observed with:\n* lot size\n* square footage\n* school rating\n* number of bedrooms\n* lat\/long\n\nNegative relationship observed with:\n* number of price changes\n\nOthers seem neutral\/uncertain","76de841b":"# Visualizations","c7bd3d01":"RFECV still includes features with a p-value over .05. Overall though, accuracy is higher than other feature selection methods.\n","4e10c6ec":"## K-Nearest Neighbors Model","7cf9fb7e":"This looks somewhat linear","d2a99018":"Best parameters set found on train set: \n\n    {'reg_alpha': 100}\n\n    Grid scores on train set:\n\n    0.806 (+\/-0.021) for {'reg_alpha': 0.1}\n    0.806 (+\/-0.021) for {'reg_alpha': 1}\n    0.806 (+\/-0.021) for {'reg_alpha': 10}\n    0.806 (+\/-0.022) for {'reg_alpha': 100}\n    0.804 (+\/-0.020) for {'reg_alpha': 250}\n    0.803 (+\/-0.019) for {'reg_alpha': 500}","df064f9b":"### Target Encoding","601759bd":"#### Target Encoded Categoricals","757f6875":"The correlations it picked up are not circumstancial. However they are all NLP terms, and this is a problem with NLP.","5a4613eb":"Our baseline model has an R^2 of 71.3% on only a few features. Our MAE is pretty high. We will see if we can improve on that with some other feature selection methods, and even some other model types.","57e0934a":"## Create Train\/Test Final Set","6303346a":"#### Add polynomial features","6424e145":"Now we will use permutation importance to determine which of these words is actually relevant to our model. This is a great model-agnostic method that you can use with any model type, and the way it works is very easy to understand. After fitting the model, it calculates a baseline R^2 score. Then for each feature, it scrambles the inputs of that feature, turnings its contribution into noise. The model is evaluated again with the feature scrambled, and the change in overall R^2 is logged as the importance for that feature. After scrambling all features, each feature has been assigned an importance based on the R^2 reduction. You can then select the features that had an effect on R^2 based on your own threshold (I kept anything >= .001) and throw out the remaining features.\n\nYou can learn more about this underrated feature selection method here: https:\/\/explained.ai\/rf-importance\/ The article focuses on Random Forest, but discusses permutation importance as an excellent feature selection method for any model type.","9e1c2286":"Support vector regression is a form of regression that allows us to define the acceptable error in our model and then finds the line that best fits the data, according to our specifications. This is really useful with something like housing price predictions, where we are ok with our prediction being within a certain dollar amount. SVR will attempt to get all of the predictions within that dollar amount when possible. This will result in a fit line that is different than a linear regression would have produced, but should result in a lower absolute error, which is a reasonable scoring metric for housing price predictions.","f1cf56d5":"# Preprocessing","360b2e09":"For the square footage variables, I ultimately concluded that extremely large houses and lots are so seriously under-represented in the dataset that we won't be able to reliably predict on them anyway and they are better left off.\n\nUltimately I opt to remove via IQR on these items. \n\nIn order to prevent a lot of data loss in this way, I kept IQR range of 1.6 instead of the standard 1.5","f431d8f2":"#### Finding Interactions","cc6c57c6":"### Missing Data","c2df4476":"K-Nearest Neighbors is more commonly used for classification. Its basic premise is to determine \"what is this like\" in making a prediction, by looking at other things that are close in value\/type. We can pick how many neighbors it assesses to make a classification. As we will see, it doesn't work very well for this type of application (or, I've not tuned the hyperparameters properly and\/or don't know how to use it well).\n\nWe're using our target encoded data set on this.","feca3621":"### One Hot","aa17be86":"Best parameters set found on train set: \n\n    {'subsample': 1}\n\n    Grid scores on train set:\n\n    0.763 (+\/-0.037) for {'subsample': 0.2}\n    0.778 (+\/-0.034) for {'subsample': 0.4}\n    0.790 (+\/-0.039) for {'subsample': 0.6}\n    0.798 (+\/-0.024) for {'subsample': 0.8}\n    0.805 (+\/-0.026) for {'subsample': 1}","2d31d0f7":"### Categoricals","4504a413":"## Support Vector Regression","b3620c13":"### Recursive Feature Elimination with Cross Validation - Linear Regression","7a77780f":"# Model Explorations","08797d6d":"We can see that a few of our zip codes are very high value. There are also some clear outliers in this data set. We'll take care of removing those, and then come back to this visual again later after we've done some cleanup.","6112441a":"Best parameters set found on train set: \n\n    {'max_depth': 4}\n\n    Grid scores on train set:\n\n    0.786 (+\/-0.019) for {'max_depth': 4}\n    0.785 (+\/-0.020) for {'max_depth': 5}\n    0.785 (+\/-0.021) for {'max_depth': 6}","8e29f6d7":"Run a base model with no cross-validation or specific feature selection with ALL possible features. We're using our target categorical encoded set which performed worse in our first test.","7eb0a303":"Our R-squared of 62% is much lower than when we used our zip code as categoricals.","591a66c4":"# Objective","938be98b":"# Model Selection","46ea923d":"#### One Hot Set","8a4aca00":"####  Transform and Standardize","8429e366":"### NLP ","f9020c5f":"## EDA and Cleaning","c8e02b6b":"Best parameters set found on train set:\n\n    {'n_estimators': 500}\n\n    Grid scores on train set:\n\n    0.808 (+\/-0.020) for {'n_estimators': 250}\n    0.811 (+\/-0.022) for {'n_estimators': 500}\n    0.810 (+\/-0.025) for {'n_estimators': 1000}\n    0.803 (+\/-0.027) for {'n_estimators': 5000}","6537bb6e":"We need to bring all of our home sale prices to the same time scale. It's easy to ignore that these homes were sold over the space of many years, but a year is a long time for real estate. We'll need to appreciate all sale prices into our most recent time series, using months.","fb564ab8":"Best parameters set found on train set: \n\n    {'lambda': 1}\n\n    Grid scores on train set:\n\n    0.801 (+\/-0.022) for {'lambda': 0.1}\n    0.806 (+\/-0.022) for {'lambda': 1}\n    0.803 (+\/-0.022) for {'lambda': 10}\n    0.801 (+\/-0.025) for {'lambda': 100}\n    0.791 (+\/-0.024) for {'lambda': 500}\n    0.787 (+\/-0.025) for {'lambda': 1000}\n    0.769 (+\/-0.023) for {'lambda': 2500}","42e2a22b":"We ran several different types of models, and logged the r^squared and mean absolute error for each model type. Which model performed the best for us?","e4a4e29f":"Best parameters set found on train set: \n\n    {'colsample_bytree': 0.8}\n\n    Grid scores on train set:\n\n    0.807 (+\/-0.026) for {'colsample_bytree': 0.2}\n    0.809 (+\/-0.030) for {'colsample_bytree': 0.4}\n    0.810 (+\/-0.029) for {'colsample_bytree': 0.6}\n    0.811 (+\/-0.028) for {'colsample_bytree': 0.8}\n    0.811 (+\/-0.029) for {'colsample_bytree': 1}","e9ceb316":"Best parameters set found on train set: \n\n    {'min_child_weight': 32}\n\n    Grid scores on train set:\n\n    0.810 (+\/-0.032) for {'min_child_weight': 28}\n    0.811 (+\/-0.029) for {'min_child_weight': 32}\n    0.811 (+\/-0.030) for {'min_child_weight': 36}","7ee16b72":"Best parameters set found on train set: \n\n    {'n_estimators': 500}\n\n    Grid scores on train set:\n\n    0.806 (+\/-0.022) for {'n_estimators': 250}\n    0.807 (+\/-0.022) for {'n_estimators': 500}\n    0.804 (+\/-0.022) for {'n_estimators': 1000}\n    0.799 (+\/-0.018) for {'n_estimators': 5000}","d9d4c4ea":"## XGBoost - Target Encoded Tuning","53f4c686":"Our average per month looks polynomial.","f2423696":"### NLP","8cf50a42":"## Linear Regressions","ab1573bb":"Best parameters set found on train set: \n\n    {'subsample': 1}\n\n    Grid scores on train set:\n\n    0.792 (+\/-0.032) for {'subsample': 0.2}\n    0.802 (+\/-0.032) for {'subsample': 0.4}\n    0.804 (+\/-0.033) for {'subsample': 0.6}\n    0.808 (+\/-0.033) for {'subsample': 0.8}\n    0.811 (+\/-0.029) for {'subsample': 1}","cb442a58":"### Categoricals","3516f619":"There are a good number of features included in this model with a p-value over .05, meaning there is a greater than 5% chance that the results are due to randomness of the sample rather than the feature. A lot of our features have a very low p-value which indicates a very low chance that these results are not affected by the feature. ","8b9d578f":"## Linear Regression - Feature Selectors","a13fbd2b":"### Binary Data","0d3b4e67":"## Feature Visualizations","405bf6db":"Best parameters set found on train set: \n\n    {'gamma': 0.005}\n\n    Grid scores on train set:\n\n    0.805 (+\/-0.026) for {'gamma': 0.005}\n    0.805 (+\/-0.026) for {'gamma': 0.01}\n    0.805 (+\/-0.026) for {'gamma': 0.05}\n    0.805 (+\/-0.026) for {'gamma': 0.1}\n    0.805 (+\/-0.026) for {'gamma': 0.3}\n    0.805 (+\/-0.026) for {'gamma': 0.5}\n    0.805 (+\/-0.026) for {'gamma': 1}","5e956a2c":"We're going to pull relevant information from the listing descriptions with the following parameters:\n   * sublinear_tf = True: A weight scaler that reduces the bias of assuming that an ngram that appears x times more frequently is x times more important\n   * use_idf = True: use Inverse Document Frequency, so that ngrams that appear very frequently to the extent they appear in most document (i.e., a bad indicator) get weighted less compared to ngrams that appear less frequently but they appear in specific documents only (i.e., a good indicator)(citation: https:\/\/stackoverflow.com\/questions\/34435484\/tfidfvectorizer-normalisation-bias) \n   * max_df: the maximum % of documents where an ngram can appear to be utilized\n   * min_df: the minimum % of documents where an ngram must appear to be utilized\n   * ngram_range=(x,y): the types of ngrams we will create, from x to y\n   * max_features: the maximum number of features that will be created\n   \nTfidfVectorizer then creates a binary array for all of the features selected, and assigns the word a weight based on its frequency and importance.","5a9ab5aa":"## EDA and Processing Train Set","793ea234":"We see there might be a relationship between square footage and our zip code. This makes sense because the size of the house is differently priced depending on location, as we saw in our visualizations earlier.","9ecb4f8b":"We need to look to some external sources to get house price appreciation info. I went to the Austin Board of Realtors site to get this info from Texas A&M for the Austin metro area that this data covers  \nhttps:\/\/www.recenter.tamu.edu\/data\/housing-activity\/#!\/activity\/MSA\/Austin-Round_Rock\n\n* Jan 2018 median: 287000\n* Jan 2019 median: 294000 +2.4%\n* Jan 2020 median: 305000 +3.7%\n* Jan 2021 median: 363830 +19.3% (!!!)","4b03fed3":"### Permutation Importance","537c8327":"Our final data sets include:\n\n* X_train_onehot, X_test_onehot - train\/test split predictors for one-hot sets\n* X_train_encoded, X_test_encoded - train\/test split predictors for encoded sets\n* y_train, y_test - target values for all sets\n* y - log transformed price\n* test_actual - exponentiated y_test prices","5344382e":"### Study Target Variable","a463af4b":"Best parameters set found on train set: \n\n    {'min_child_weight': 28}\n\n    Grid scores on train set:\n\n    0.807 (+\/-0.037) for {'min_child_weight': 4}\n    0.807 (+\/-0.030) for {'min_child_weight': 8}\n    0.808 (+\/-0.031) for {'min_child_weight': 12}\n    0.808 (+\/-0.029) for {'min_child_weight': 16}\n    0.809 (+\/-0.028) for {'min_child_weight': 20}\n    0.808 (+\/-0.028) for {'min_child_weight': 24}\n    0.810 (+\/-0.032) for {'min_child_weight': 28}","0225e890":"Best parameters set found on train set: \n\n    {'colsample_bytree': 0.4}\n\n    Grid scores on train set:\n\n    0.794 (+\/-0.015) for {'colsample_bytree': 0.2}\n    0.806 (+\/-0.021) for {'colsample_bytree': 0.4}\n    0.800 (+\/-0.026) for {'colsample_bytree': 0.6}\n    0.798 (+\/-0.032) for {'colsample_bytree': 0.8}\n    0.805 (+\/-0.026) for {'colsample_bytree': 1}","1aa5e57f":"## Create Holdout Set","e999b6f8":"### Checking homeType","2c301ee1":"We need to create our holdout data before any further processing.\n\nThe reasons for this are:\n   * We will standardize our continuous variables, and you should standardize only on your train set and apply that to your test set.\n   * We will be feature engineering on our train set, and applying that later to our test set. We cannot have our test set data leak into our engineered features.\n   * We'll be doing some natural language processing, fitting on our train set and applying to our test set. We don't want data leakage for the same reasons as above.\n    ","5d401582":"Best parameters set found on train set: \n\n    {'max_depth': 4}\n\n    Grid scores on train set:\n\n    0.798 (+\/-0.037) for {'max_depth': 3}\n    0.800 (+\/-0.043) for {'max_depth': 4}\n    0.792 (+\/-0.046) for {'max_depth': 5}\n    0.787 (+\/-0.038) for {'max_depth': 6}","5b2d1cb3":"Using latitude and longitude, we make a visual map of the Austin area that lets us see any map outliers. There don't appear to be any zones that are well outside of the Austin area, except for just a few down in the lower SE area. So we might plan to cut off our latitude just above 30.1.\n\nThis visualization suggests that location is very important to home price. We'll check that out more directly.","b47b528a":"The feature selector designated 191 of the 1000 ngrams that it created as important. The remaining ngrams will be discarded.\n\nThese are copied just below so that the selector code does not need to be run every time.","b95d712d":"This model is entirely different than linear regression. Gradient boosting uses decision trees to learn about outcomes, with trees being added to the model one at a time and existing trees in the model are not changed. Each successive tree tries to improve upon the predictions of the first one, with the weights of the various decision points being updated each time. Gradient boosting uses the residuals to improve its next tree prediction. Overall much more opaque of a process than linear regression.\n\nGradient Boosting performs best with optimal parameter tuning. We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our gradient booster! Here are the parameters we are trying out:\n\n* n_estimators: Number of boosts to perform. Gradient boosting is pretty robust to over-fitting so more is usually better\n* max_depth: This determines how many tree nodes the estimator looks at before making a prediction. We don't know what is best here, so we are trying things from 2-4 to see what works the best\n* min_child_weight: Min sum of instance weight needed in a child\n* gamma: Minimum loss reduction to make another partition on a leaf node. Higher results in more conservative algorithm.\n* subsample: Ratio of training sets. .5 means that it will sample half the training data before making trees. Occurs with each boosting iteration.\n* colsample_by_tree: ratio of columns when making a tree\n* alpha: L1 regularization. Higher will make model more conservative.\n* learning_rate: Tuning this setting alters how much the model corrects after it runs a boost. .1 is a common rate and we will test a lower and higher rate as well.","3f4f5da3":"Our data set includes the listing text for each sale. We're going to use Natural Language Processing methods to extract relevant information from the listing text to boost the effectiveness of our model.\n\nWe're using spaCy and after basic package installation, we also need to download the english language pipeline.","8b301a87":"#### One-Hot Encoded Categoricals","97001b88":"Best parameters set found on train set: \n\n    {'learning_rate': 0.2}\n\n    Grid scores on train set:\n\n    0.337 (+\/-0.030) for {'learning_rate': 0.005}\n    0.714 (+\/-0.029) for {'learning_rate': 0.01}\n    0.793 (+\/-0.032) for {'learning_rate': 0.05}\n    0.800 (+\/-0.029) for {'learning_rate': 0.1}\n    0.800 (+\/-0.034) for {'learning_rate': 0.2}\n    0.792 (+\/-0.030) for {'learning_rate': 0.3}\n    0.779 (+\/-0.037) for {'learning_rate': 0.5}","f3dbcb0c":"##### Study Residuals","f8f1cede":"Now we'll try a simple forward-backward feature selection model based on p-value, using a statsmodel OLS linear regression model. This selector starts with zero features, internally runs the model for each feature individually, and adds the lowest p-value feature to its list to include. It then runs the model again with the original feature included and tries adding each other feature individually. It will either add the next best feature under the threshold or remove an existing feature if it is no longer within the threshold. This process iterates until all features in the model are under the p-value threshold.\n\nThis model takes quite some time to run, so is commented out with the results replicated in markdown following.","f3985468":"# Project Overview\n\n> This is an in-depth notebook which explores the Austin Housing Dataset through several different models. The notebook includes a thorough EDA and cleaning section, natural language processing on text descriptions, exploration of different models using different categorical methods (one-hot encoding vs target encoding) with extensive parameter tuning, an evaluation of the final model, and visualizations.\n\n* **Business Objective**\n\n\n* **Notebook Preparation**\n    * Importing our Modules\n\n\n* **Preprocessing**\n    * EDA and Cleaning\n        * Scaling target value for Time Series\n        * Duplicates\n        * Outlier Detection\n        * Missing Data\n        * Binary Data\n        * Studying our Target Variable\n    * Natural Language Processing\n    * Create Holdout Set\n    * Feature Engineering\n    * Correlations and Multicollinearity\n    * EDA & Process Train Set\n        * Categoricals\n        * Continuous\n            * Standardize Continuous Data\n            * Find Interactions\n            * Adding Polynomial Features\n        * NLP\n    * Process Test Set\n        * Categoricals\n        * Continuous\n        * NLP\n    * Create Train\/Test Final\n\n\n* **Model Explorations**\n    * Picking our Base Features\n    * Linear Regressions\n        * Basic LR with Top Features One-Hot Encoded\n        * Basic LR with Top Features Target Encoded\n        * LR with ALL model features\n        * Linear Regression with various Feature Selection Methods\n            * Permutation Importance\n            * Forward-Backward Selector\n            * RFECV\n    * Regularization Models\n    * K-Nearest Neighbors\n    * Support Vector Regression\n    * XGBoost Models\n        * XGBoost - One Hot Encoded\n        * XGBoost - Target Encoded\n\n\n\n* **Regression Results and Model Selection**\n    * Evaluate results of all attempted models and choose best model\n\n\n* Visualizations\n    * Feature visualizations\n\n* Analysis\n","0f80038d":"### Basic Model Top Features Only","84ac8d8d":"## Correlations\/Multicollinearity","e9758a55":"Best parameters set found on train set: \n\n    {'lambda': 1}\n\n    Grid scores on train set:\n\n    0.809 (+\/-0.028) for {'lambda': 0.1}\n    0.811 (+\/-0.028) for {'lambda': 1}\n    0.808 (+\/-0.028) for {'lambda': 10}\n    0.803 (+\/-0.030) for {'lambda': 100}\n    0.800 (+\/-0.028) for {'lambda': 250}\n    0.793 (+\/-0.028) for {'lambda': 500}\n    0.784 (+\/-0.024) for {'lambda': 1000}\n    0.758 (+\/-0.020) for {'lambda': 2500}","51279c06":"This looks meaningless","436f6679":"Interesting how past a certain lot size, larger rounded numbers are used instead of specific numbers. There does seem to be a relationship here.","7b9ae669":"Our final data sets include:\n\n* x_final_one_hot - Predictors with one-hot encoding for zipcode, month and year\n* x_final_encoded - Predictors with additive smoothed encoding for zipcode, month and year\n* y - log transformed price\n\n\n* X_train_onehot, X_test_onehot - train\/test split predictors for one-hot sets\n* X_train_encoded, X_test_encoded - train\/test split predictors for encoded sets\n* y_train, y_test - target values for all sets\n* test_actual - exponentiated y_test prices","308a0028":"Best parameters set found on train set: \n\n    {'reg_alpha': 75}\n\n    Grid scores on train set:\n\n    0.806 (+\/-0.022) for {'reg_alpha': 50}\n    0.806 (+\/-0.022) for {'reg_alpha': 75}\n    0.806 (+\/-0.022) for {'reg_alpha': 100}\n    0.806 (+\/-0.022) for {'reg_alpha': 125}\n    0.806 (+\/-0.022) for {'reg_alpha': 150}","10547e5e":"### Scale Time Series","eec1a073":"### Linear Regression Model - ALL Features","77dd22c8":"#### Target Encoded Categoricals","ecdbe790":"### Forward-Backward Selector","e80fd545":"We see potential outliers in price, lotSizeSqFt, livingAreaSqFt, and numOfBathrooms, numOfBedrooms, garageSpaces, parkingSpaces.","82a160b0":"Best parameters set found on train set: \n\n    {'min_child_weight': 6}\n\n    Grid scores on train set:\n\n    0.803 (+\/-0.034) for {'min_child_weight': 3}\n    0.801 (+\/-0.030) for {'min_child_weight': 4}\n    0.801 (+\/-0.029) for {'min_child_weight': 5}\n    0.805 (+\/-0.026) for {'min_child_weight': 6}\n    0.803 (+\/-0.025) for {'min_child_weight': 7}","c9ca58a5":"Ultimately we will select the sklearn SVM method for our model. Support Vector Machine's explicit goal to minimize absolute error coordinates nicely with price predictions.","7d2fb515":"#### Manually locating price outliers","701702de":"We're going to evaluate a few different variations of our linear regression model, as well as a few more complex model types. In order to keep track of our results, we'll be making a dictionary to store our model accuracy results.","02e55079":"## Natural Language Processing","44da03c6":"# Notebook Preparation","d26eb5e2":"### Continuous","318f5ba6":"Run a base model with no cross-validation or specific feature selection with ALL possible features. We're going to use our one-hot encoded set which performed better in our first test.","968c1f03":"Regression on median zip, this time residuals plotted against school rating. This may not have the strong relationship we expected.","9a004b02":"We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our SVM! Here are the parameters we are trying out:\n\n* kernel: linear is parametric, and rbf is non-parametric. One of these should perform better. Our data is not totally normal, so it might be rbf.\n* epsilon: This value is how much error we're ok with accepting without assigning a penalty to the model\n* C: The error that we will accept from a point outside our epsilon\n\nOur C and epsilon need to be in scale with our output variable, which is our log-transformed price.\n","1d137e67":"### Duplicate Data","d32d1d38":"### Data Sets Reference","69a532c3":"We need to slightly redo our one-hot encodings to not drop the first entry. We'll also make year_built into total one-hot encodings rather than bins.","cd6ea68e":"We're going to build our most baseline model using only the top three features -\n    \n    * zipcode\n    * avgSchoolRating\n    * livingAreaSqFt\n    * numOfBathrooms\n    * lotSizeSqFt\n\nWe can identify top features from our correlation heat map. Here's a reminder of the top:","859b5dad":"# APPENDIX","3fd09065":"Our parameter tuning for XGBoost is in the APPENDIX","5402abcc":"Best parameters set found on train set: \n\n    {'C': 10, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'rbf'}\n\n    Grid scores on train set:\n\n    -0.141 (+\/-0.004) for {'C': 3, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.144 (+\/-0.004) for {'C': 3, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.141 (+\/-0.004) for {'C': 3, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.139 (+\/-0.003) for {'C': 3, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 3, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.147 (+\/-0.004) for {'C': 3, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 3, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.140 (+\/-0.002) for {'C': 3, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n    -0.141 (+\/-0.004) for {'C': 6, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.149 (+\/-0.004) for {'C': 6, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.141 (+\/-0.004) for {'C': 6, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.136 (+\/-0.003) for {'C': 6, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 6, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.150 (+\/-0.004) for {'C': 6, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 6, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.138 (+\/-0.002) for {'C': 6, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n    -0.141 (+\/-0.004) for {'C': 10, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.151 (+\/-0.004) for {'C': 10, 'epsilon': 0.05, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.141 (+\/-0.004) for {'C': 10, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.135 (+\/-0.003) for {'C': 10, 'epsilon': 0.05, 'gamma': 'auto', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 10, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n    -0.152 (+\/-0.004) for {'C': 10, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n    -0.142 (+\/-0.004) for {'C': 10, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'linear'}\n    -0.136 (+\/-0.002) for {'C': 10, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}","6c324fac":"Now we perform cross-validation with our base model over 5 splits and get our mean R^2.","d4cb4732":"We're running permutation importance again, this time using the entire model predictors.","fe728114":"I wrote a function which finds all of the feature combinations possible in our dataset. Then for each combination, the function runs a linear regression with cross validation on 5 folds and gets the r^2 score for the regression including that feature combination. All scores are recorded and r^2 score improvement is assessed, with the resulting table giving the increase in model improvement from a feature combo. ","515a49fc":"## Regularization Models","d8e5af50":"There is a problem here which is a problem with NLP, where we have to be careful what we eliminate because a term might show up in one listing but not another even if they mean the same thing. So we must be very careful with term deletion and certain that if we remove a term it's certainly because the OTHER term is present as well.\n\nIt's easy enough if it is a trigram and a bigram correlated, because we can safely remove the trigram. Same situation for a bigram to a unigram.","31194802":"There is a multicollinearity of features in our feature set somewhere. Let's check.","bce98579":"## Picking our Base Features","12acc637":"This looks linear","569c38b7":"## XGBoost","63c73cda":"### Continuous","fa4ad659":"RFECV is a reverse forward-backward selector. It starts the model with all features in use then removes the weakest one, and iterates until the best feature set is found. It uses integrated cross-validation to determine the optimal set of features in the model with the best cross-validated score. We score on mean absolute error.","eddf97ea":"## Process Test Set","66c74304":"Build a model that accurately predicts house prices in Austin","f5efeeb2":"We'll not use:\n\n* parkingSpaces, hasGarage, numOfParkingFeatures and keep garageSpaces (higher relationship with Price)\n* numOfElementarySchools, and keep numOfPrimarySchools (higher relationship with Price)\n* MedianStudentsPerTeacher, keeping avgSchoolRating\n* numOfBathrooms correlates with square footage, but I'm not dropping either","3ab3282f":"## XGBoost - One Hot Tuning","9769b0a7":"We can get a sense of the most important features to our price from our correlation table. Zipcode as a plain variable does not correlate, which makes sense, because without some sort of transformation it is an arbitrary unordered number. We can see how transformed as median_zip or zip_rank it becomes the MOST important contributor to price. We can see here that big contributors to price include\n    \n    * Lat\/long in a target encoded form\n    * zip code (in some altered form, not as arbitrary number)\n    * livingAreaSqFt\n    * numBathrooms\n    * avgSchoolRating\n    * lotSizeSqFt","75f2a0fc":"## Feature Engineering","04402cf0":"Best parameters set found on train set: \n\n    {'learning_rate': 0.1}\n\n    Grid scores on train set:\n\n    0.342 (+\/-0.026) for {'learning_rate': 0.005}\n    0.724 (+\/-0.027) for {'learning_rate': 0.01}\n    0.783 (+\/-0.035) for {'learning_rate': 0.02}\n    0.792 (+\/-0.036) for {'learning_rate': 0.03}\n    0.807 (+\/-0.035) for {'learning_rate': 0.1}\n    0.806 (+\/-0.042) for {'learning_rate': 0.2}\n    0.800 (+\/-0.043) for {'learning_rate': 0.3}","d20396be":"Here's a fun way to see the improvements to our data quality after we clean outliers! A much deeper color map."}}