{"cell_type":{"295dd673":"code","26a6d398":"code","d103d529":"code","21872208":"code","0146b0af":"code","eb43259f":"code","29ef78a5":"code","c0df0295":"code","2d457f0b":"code","bf16d493":"code","707b4c48":"code","f77afb3c":"code","88eca491":"code","d77e8d87":"code","d8ba8439":"code","90735dc5":"code","d4ad6fc3":"markdown","32d0cf7b":"markdown","e7768d19":"markdown","9bf0639f":"markdown","b05b7a96":"markdown","da63c3c7":"markdown","8513ba2a":"markdown","ece849e5":"markdown","06901d71":"markdown","86b9502c":"markdown","0e2e393b":"markdown","80f20eb8":"markdown"},"source":{"295dd673":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\nimport torch.nn as nn\n\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN","26a6d398":"train_csv_path = '..\/input\/tensorflow-great-barrier-reef\/train.csv'\ntrain_df = pd.read_csv(train_csv_path)\ntrain_df.head()","d103d529":"print(len(train_df))","21872208":"def add_image_path(row):\n    video_id = row['video_id']\n    video_frame = row['video_frame']\n    return \"video_\" + str(video_id) + \"\/\" + str(video_frame) + \".jpg\"\n\ntrain_df['image_path'] = train_df.apply(lambda x: add_image_path(x), axis=1)\ntrain_df.head()","0146b0af":"class CustomDataset(Dataset):\n    def __init__(self, image_dir, train_df, transform):\n        self.train_df = train_df\n        self.image_dir = image_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.train_df)\n\n    def __getitem__(self, index):\n        \n        ##################\n        # Read the image #\n        ##################\n        image_path = os.path.join(self.image_dir, self.train_df.loc[index, \"image_path\"])\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0    # (fasterrcnn model expects input to be in range [0-1])\n        \n        ##########################\n        # Get the bounding boxes #\n        ##########################\n        annots = self.train_df.loc[index, \"annotations\"]\n        boxes = pd.DataFrame(eval(annots), columns=['x','y','width','height']).astype(np.float32).values\n        # Shape of boxes: (num_of_bounding_boxes, 4)\n        # Columns of boxes: (x,y,w,h)\n        \n        ########################\n        # Convert xywh to xyxy #\n        ########################\n        # xyxy is nothing but (x_min, y_min, x_max, y_max) (since fasterrcnn model expects xyxy)\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]  # (x_max = x_min + w)\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]  # (y_max = y_min + h)\n        \n        ##############################\n        # Create a target dictionary #\n        ##############################\n        # (consisting of boxes and labels as its keys)\n        target = {}\n        target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n        target['labels'] = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        # Label 1: COTS-Fish\n        # Label 0: Background (default)\n        \n        # Some extra info\n        target['image_id'] = torch.tensor([index])\n        \n        ############################\n        # Transform the input data #\n        ############################\n\n        # Before transforming check if the bboxes are are valid (i.e., not partially covering the image)\n        # The image size in the given dataset is (h=720, w=1280)\n        is_not_valid = ((boxes[:, 0] < 0).any() or\n                        (boxes[:, 1] < 0).any() or\n                        (boxes[:, 2] > 1280).any() or\n                        (boxes[:, 3] > 720).any())\n        \n        \n        if is_not_valid:\n            image = ToTensorV2()(image=image)['image']\n            return image, target\n        else:\n            # Transform\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n\n            sample = self.transform(**sample)\n            image = sample['image']\n            \n            if len(boxes) > 0:\n                target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n            \n            return image, target","eb43259f":"transform = A.Compose([A.Flip(0.5),\n                       ToTensorV2()\n                      ], bbox_params={'format': 'pascal_voc','label_fields': ['labels']})\n\n\nimage_dir = '..\/input\/tensorflow-great-barrier-reef\/train_images'\n\ndataset = CustomDataset(image_dir=image_dir, train_df=train_df, transform=transform)","29ef78a5":"BATCH_SIZE = 4\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(dataset,\n                          shuffle=True,\n                          batch_size=BATCH_SIZE,\n                          collate_fn=collate_fn)","c0df0295":"num_bboxes = 0\ntries = 10\nwhile (num_bboxes == 0):\n    images, targets = next(iter(train_loader))\n    # images and targets are of list type\n    idx = np.random.randint(0, BATCH_SIZE)\n    img = images[idx]\n    target = targets[idx]\n    num_bboxes = len(target['boxes'])\n    tries -= 1\n    if tries == 0:\n        break\n\nif num_bboxes > 0:        \n    print(img.shape)\n    print(target.keys())\n    print(target['boxes'])\n\n    img = img.permute(1,2,0).numpy()\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    if num_bboxes > 0:\n        boxes = target['boxes'].numpy()\n        for box in boxes:\n            c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n            cv2.rectangle(img, c1, c2,\n                      (220, 0, 0), 3)\n\n    plt.title(print(target['image_id']))\n    plt.imshow(img)\n    plt.show()\nelse:\n    print(':(')","2d457f0b":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel = model.to(device)","bf16d493":"num_classes = 2  # 1 class (cots) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes).to(device)","707b4c48":"# Model needs list of input tensors and list of targets\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]","f77afb3c":"loss_dict = model(images, targets)\nlosses = sum(loss for loss in loss_dict.values())\nloss = losses.item()\nloss","88eca491":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=1e-3)","d77e8d87":"def train_epoch(model, scaler, data_loader, device, optimizer, gradient_accumulations=32):\n    model.train()\n    losses = []\n    for batch_idx, data in enumerate(tqdm(data_loader)):\n        images, targets = data\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]\n        \n        with autocast():        \n            loss_dict = model(images, targets)\n            loss = sum(loss for loss in loss_dict.values())\n\n        scaler.scale(loss \/ gradient_accumulations).backward()\n            \n        if (batch_idx + 1) % gradient_accumulations == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            model.zero_grad()\n            \n        losses.append(loss.item())\n        \n    return np.mean(losses)","d8ba8439":"# Train\nEPOCHS = 5\n\nscaler = GradScaler()\nmodel.zero_grad()\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch: {epoch+1}\/{EPOCHS}')\n    train_loss = train_epoch(model, scaler, train_loader, device, optimizer)\n    print(f'Train Loss: {train_loss}')","90735dc5":"torch.save(model.state_dict(), 'gbr_fasterrcnn_resnet50_fpn.pth.tar')","d4ad6fc3":"## Build CustomDataset\n\nSince we are using FasterRCNN model, <\/br>\nDuring **training**,the model excepts **input tensors** and **targets**(list of dictionary). <\/br>\nDuring **inference**, the model expects only **input tensors**.\n\nMore info on input~output of FasterRCNN model - [here](https:\/\/pytorch.org\/vision\/main\/generated\/torchvision.models.detection.fasterrcnn_resnet50_fpn.html)","32d0cf7b":"## Save the model","e7768d19":"## Let's train the model\n\nWe will be using gradient accumulation, to solve the problem of cuda out of memory (since we have set batch size to be very low).\n\nMore on gradient accumulation - [here](https:\/\/towardsdatascience.com\/i-am-so-done-with-cuda-out-of-memory-c62f42947dca#:~:text=one%20by%20one.-,Gradient%20Accumulation,-This%20solution%20has)","9bf0639f":"This notebook is inspired from -\n[previous_competition on object detection](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train\/notebook)\n\nThis is a training notebook.\nThe notebook to infer the model can be found [here](https:\/\/www.kaggle.com\/palash97\/gbr-fasterrcnn-pytorch-inference\/)","b05b7a96":"## Visualize one sample from our train_loader","da63c3c7":"## Create PyTorch DataLoader","8513ba2a":"## Add image path for each row in the train dataframe\n\nImage path for given video_id and video_frame is like: **video_id\/video_frame.jpg**","ece849e5":"## Import Libraries","06901d71":"## Load the model pretrained on COCO","86b9502c":"## Load train data","0e2e393b":"Awesome!!","80f20eb8":"## Test the model architecture on sample data "}}