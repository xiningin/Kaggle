{"cell_type":{"3db9f17a":"code","e57562e3":"code","d56a0ea3":"code","ca4a57f5":"code","25cf3ae3":"code","ee0c7f2a":"code","a6bd4340":"code","013cbfe2":"code","0f93faa0":"code","f39ad556":"code","0786e7aa":"code","a06bb40b":"code","29e20410":"code","954aca7a":"code","00757fc6":"code","4e959a4a":"code","2188f590":"code","6e18cc9f":"code","75632775":"code","b04dadab":"code","cdfc01df":"markdown","335e57a3":"markdown","670a8886":"markdown","3fb2be36":"markdown","1c642da2":"markdown","07b3a188":"markdown","0a613a93":"markdown"},"source":{"3db9f17a":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","e57562e3":"train_df.head()","d56a0ea3":"train_target = train_df['label']\ntrain_features = train_df.drop(['label'], axis=1)","ca4a57f5":"import matplotlib.pyplot as plt\nimport seaborn as sns","25cf3ae3":"plt.figure(figsize=(15, 7))\nsns.countplot(x=train_target)\nplt.title(\"The number of digit classes\")","ee0c7f2a":"img = train_features.iloc[0].values\nimg = img.reshape((28, 28))\nplt.imshow(img, cmap='gray')\nplt.title(train_target[0])\nplt.axis('off')\nplt.show()","a6bd4340":"train_features = train_features\/255.0","013cbfe2":"# before train_features type: DataFrame\nprint(train_features.shape, type(train_features))\n\n# after train_features type: ndarray\ntrain_features = train_features.values.reshape(-1, 28, 28, 1)\nprint(train_features.shape, type(train_features))","0f93faa0":"from keras.utils.np_utils import to_categorical \ntrain_target = to_categorical(train_target, num_classes=10)","f39ad556":"# split train_df into train data and validation data for the fitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.1,\\\n                                                   random_state=156)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0786e7aa":"from tensorflow import keras\nimport tensorflow as tf","a06bb40b":"# SELU has self-normalization property with lecun_normal in DNN\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=X_train.shape[1:]),\n    keras.layers.Dense(70, activation='selu', kernel_initializer='lecun_normal'),\n    keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal'),\n    keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal'),\n    keras.layers.AlphaDropout(rate=0.5),\n    keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')\n])","29e20410":"K = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = np.math.ceil(len(X) \/ batch_size) * epochs\n    factor = np.exp(np.log(max_rate \/ min_rate) \/ iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) \/ 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")\n    print('proper_rate:',min(rates))","954aca7a":"batch_size=32\nn_epochs=30","00757fc6":"# class is identical so we use categorical_crossentropy\nmodel.compile(optimizer=keras.optimizers.Nadam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])","4e959a4a":"rates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","2188f590":"history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, \n                    epochs=n_epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])","6e18cc9f":"plt.plot(history.history['val_accuracy'], label='validation accuracy')\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.title('Test accuracy')\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","75632775":"test_df = test_df\/255.0\ntest_df = test_df.values.reshape(-1, 28, 28, 1)","b04dadab":"# predict results\nresults = model.predict(test_df)\n\n# select index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\n\nmy_submission = pd.DataFrame({'ImageId': range(1,28001),\n                             'Label': results})\nmy_submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","cdfc01df":"## Loading data","335e57a3":"## schedule learning rate","670a8886":"## submit answer","3fb2be36":"## plot accuracy curve","1c642da2":"## One-hot Encoding","07b3a188":"## Model Fit","0a613a93":"## plot The number of digit classes"}}