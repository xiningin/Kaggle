{"cell_type":{"2833bf52":"code","03d7d566":"code","3bf6300e":"code","9f42b36a":"code","ada619ad":"code","51316896":"code","fc5db210":"code","5be26e68":"code","6ed501ef":"code","a8865c19":"code","681e0561":"code","cc592d6d":"code","c6fc714d":"code","7cdc1774":"code","fdf2ff9f":"code","31c432fb":"code","701dc51f":"code","7d3ee2aa":"code","c63b9f7b":"code","8fde85d5":"code","0945c3a2":"code","0183cde9":"code","abe4e3b5":"code","cca877be":"code","1d94f226":"code","72092302":"code","814734c3":"code","fbadb1d5":"code","b6667dec":"code","384facd0":"code","8a0bc7f8":"code","5b9dd746":"code","ecec6788":"code","b2a7162a":"code","254cf763":"code","5123dd3d":"code","235c4a45":"code","fa215488":"code","0fea12a4":"code","88dceed8":"code","86ee3e66":"code","1bdc2724":"code","b1125358":"code","9ece4636":"code","fe7f58f6":"code","42dcdb37":"code","cc4b6a16":"code","5fdc74ed":"code","c4eebd53":"code","1c04154d":"code","7f867a6c":"code","1578e046":"code","fe5768d5":"code","909a0915":"code","839ee80b":"code","e1db3fa8":"code","ed1c8507":"code","49e27cd8":"code","7ecd80ff":"code","9063fc68":"code","705910ef":"code","a55bf2ea":"code","31d5e543":"code","59c015eb":"code","874e22b8":"code","16c591d4":"code","abbc01d7":"code","c1d1465a":"code","58539df3":"code","8727f55c":"code","fc971f0c":"code","6bc00ca2":"code","3a8a190d":"code","fedd0993":"code","2be03769":"code","8013154a":"markdown","80289efb":"markdown","d9d425d7":"markdown","6ad2f2f4":"markdown","51693402":"markdown","f3f1c99d":"markdown","d4225a1f":"markdown","af2ac254":"markdown","92c36185":"markdown","52de9793":"markdown","73bbb7dd":"markdown","ae54054f":"markdown","d4802e35":"markdown","964fdc98":"markdown"},"source":{"2833bf52":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12,10\n","03d7d566":"train_df=pd.read_csv('..\/input\/revenue-generated-by-advertisements\/Train_Data.csv')\ntest_df=pd.read_csv('..\/input\/revenue-generated-by-advertisements\/Test_Data.csv')","3bf6300e":"train_df.head()","9f42b36a":"test_df.head()","ada619ad":"train_df.shape,test_df.shape","51316896":"# Adding test and traing flag so that we can split train\/test data\ntrain_df['tst']=0\ntest_df['tst']=1","fc5db210":"# concatenating train and test data for data preprocessing\ndata=pd.concat([train_df,test_df],axis=0,copy=True)\ndata.head()\ndf = data.copy()","5be26e68":"df.describe().T","6ed501ef":"df.isna().sum()","a8865c19":"df[(df.conversions==0) & (df.tst==0)].shape\ndf","681e0561":"data['day']=data['date'].apply(lambda x:x[0:2]).astype(int)\n\ndata['month']=data['date'].apply(lambda x:x[3:5]).astype(int)\n\ndata['year']=pd.DatetimeIndex(data['date']).year\ndata","cc592d6d":"data.shape\ndf=data.copy()\ndf","c6fc714d":"#df['date'] = pd.to_datetime(df.date,format='%d-%m-%Y')\n#df.index = df['date']","7cdc1774":"df['CPC']=df.cost.divide(df.clicks).replace(np.inf,0).replace(np.nan,0)\ndf['CTR']=df.clicks.divide(df.impressions).replace(np.inf,0).replace(np.nan,0)\ndf['COA']=df.cost.divide(df.conversions).replace(np.inf,0).replace(np.nan,0)\n","fdf2ff9f":"# don't run this\n# df.drop(columns=['date'],axis=1,inplace=True)\n# df['ROI']=df[df.tst==0].revenue.divide(df[df.tst==0].cost).replace(np.inf,0).replace(np.nan,0)","31c432fb":"df['ad']=df['ad'].str.slice(3).astype(int)\n\ndf['adgroup']=df['adgroup'].str.slice(8).astype(int)\n\ndf['campaign']=df['campaign'].str.slice(-1).astype(int)\n\ndf.reset_index()","701dc51f":"df.info()","7d3ee2aa":"df.month.unique()","c63b9f7b":"#Time series view\nplt.figure(figsize=(20,8))\ndf.groupby(['date'])['revenue'].sum().plot(kind='line')\nplt.show()","8fde85d5":"plt.figure(figsize=(8,6))\n#plt.hist(df.cost,bins=7,density=True)\nsns.distplot(df.cost,bins=7)\nplt.axvline(df.cost.mean(), color='r', linestyle='dashed', linewidth=1)","0945c3a2":"plt.figure(figsize=(8,6))\nsns.distplot(df.clicks,bins=5)\nplt.axvline(df.clicks.mean(), color='r', linestyle='dashed', linewidth=1)","0183cde9":"plt.figure(figsize=(8,6))\nsns.distplot(df.impressions,bins=5)\nplt.axvline(df.impressions.mean(), color='r', linestyle='dashed', linewidth=1)","abe4e3b5":"plt.figure(figsize=(8,6))\nsns.distplot(df.CTR,bins=5)\nplt.axvline(df.CTR.mean(), color='r', linestyle='dashed', linewidth=1)","cca877be":"\nf, ax = plt.subplots(figsize=(12, 9));\nsns.barplot('month','revenue',data=df)","1d94f226":"plt.scatter(df.conversions,df.revenue)\nplt.show()","72092302":"plt.scatter(df.impressions,df.revenue)\nplt.show()","814734c3":"sns.barplot('adgroup','revenue',data=df,hue='year')","fbadb1d5":"df.groupby(['date','adgroup'])['revenue'].sum().sort_values(ascending=False)","b6667dec":"list(df.select_dtypes(exclude = ['object','datetime']).drop(columns=['revenue','tst']).columns)","384facd0":"df_copy=df.copy()\n","8a0bc7f8":"str_col=['ad','adgroup','campaign','year','month','day','date']\nle=LabelEncoder()\nfor col in str_col:\n    df[col]=le.fit_transform(df[col].values)","5b9dd746":"df.head()","ecec6788":"sc=MinMaxScaler()\nnum_col=list(df.select_dtypes(exclude = ['object']).drop(columns=['revenue','tst']).columns)\ndf_scaled=sc.fit_transform(df[num_col].values)","b2a7162a":"df_scaled=pd.DataFrame(df_scaled,columns=num_col)\ndf_scaled.head()","254cf763":"df[num_col]","5123dd3d":"train=df[df.tst==0]\ntest=df[df.tst==1]","235c4a45":"train=train.drop(columns=['tst','campaign'],axis=1)","fa215488":"test=test.drop(columns=['tst','revenue','campaign'],axis=1)\ntest","0fea12a4":"X=train.columns.drop('revenue')\ny=train.revenue\nX","88dceed8":"plt.figure(figsize=(15,15))\ncorrmat =train.corr()\ntop_corr_features = corrmat.index\n\n#plot heat map\ng=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n#sns.heatmap(train.corr(),annot=True)\nplt.show()\n","86ee3e66":"train[X].var().sort_values(ascending=False)","1bdc2724":"#features=['COA','cost','conversions','adgroup','clicks','CPC']---v5.0\n#features=['conversions','adgroup','date','clicks','year']---v11.0\nfeatures=['date', 'adgroup', 'ad', 'impressions', 'clicks', 'cost', 'conversions',\n       'day', 'month', 'year', 'CPC', 'CTR', 'COA']  # replaced impression->cost\nX=train[features]\ny=train.revenue","b1125358":"# Since the given data is time series,we will not split on random basis...splitting data contigously\nrow=3320\nX_train=X[:row]\nX_val=X[row:]\ny_train=y[:row]\ny_val=y[row:]","9ece4636":"X_test=test[features]","fe7f58f6":"X_train.shape,y_val.shape","42dcdb37":"def score(model,test,name='default'):\n    model.fit(X_train,y_train)\n    ypred=model.predict(test)\n    print(name)\n    print('Accuracy of training set: ',round(model.score(X_train,y_train)*100,2))\n    print('Accuracy of test set: ',round(model.score(test,y_val)*100,2))\n    print('mean squared error: ',round(mean_squared_error(y_val,ypred)\/100,2))\n    print('mean absolute error: ',round(mean_absolute_error(y_val,ypred),2))\n    print('r2_score: ',round(r2_score(y_val,ypred)*100,2))","cc4b6a16":"le=LinearRegression()\nscore(le,X_val,name='Linear Regression')","5fdc74ed":"le.fit(X_train,y_train)\nypred=le.predict(test[features])\npreds=le.predict(X_val)","c4eebd53":"ypred_df=pd.DataFrame(ypred,columns=['revenue'])","1c04154d":"ypred_df","7f867a6c":"train[features][:row].shape","1578e046":"# Best hyperparameters\n\nxgb=XGBRegressor(n_estimators=37,learning_rate=0.05,booster='gbtree')\nxgb.fit(X_train,y_train)\n\nscore(xgb,X_val,name='XG Boost')","fe5768d5":"ypred=xgb.predict(X_test)\npreds=xgb.predict(X_val)","909a0915":"ypred_df=pd.DataFrame(ypred,columns=['revenue'])","839ee80b":"ypred_df.index=test.index\nypred_df.head(10)","e1db3fa8":"plt.figure(figsize=(20,10))\nvalid=train[row:]\nvalid['Predictions'] = 0\nvalid['Predictions'] = preds\ntrain1=train[features][:row]\nvalid.index = train[features][row:].index\ntrain1.index = train1[:row].index\n\nplt.plot(train['revenue'][:row]) # train set\nplt.plot(valid[['revenue', 'Predictions']]) # val set\nplt.plot(ypred_df.revenue) # test set\nplt.show()","ed1c8507":"#use this to find optimum value of n\nmse=[]\nmae=[]\nfor i in range(10,60):\n    xgb=XGBRegressor(n_estimators=i,learning_rate=0.05)\n    xgb.fit(X_train,y_train)\n    pr=xgb.predict(X_val)\n    ms=mean_squared_error(y_val,pr)\n    ma=mean_absolute_error(y_val,pr)\n    mse.append(ms)\n    mae.append(ma)","49e27cd8":"10+mae.index(min(mae))","7ecd80ff":"plt.figure(figsize=(20,10))\nplt.plot(np.array(mae),'b--')\nplt.plot(np.array(mse)\/1000)\nplt.show()","9063fc68":" xgb_model = XGBRegressor(\n         objective = 'reg:squarederror',\n         colsample_bytree = 0.7,\n         learning_rate = 0.1,\n         max_depth = 3,\n         min_child_weight = 3,\n         n_estimators = 60,\n         subsample = 0.7)\n\n %time xgb_model.fit(X_train, y_train, early_stopping_rounds=7, eval_set=[(X_val, y_val)], verbose=False)\n\n y_pred_xgb = xgb_model.predict(X_val)\ny_pred_xgb1 = xgb_model.predict(X_test)\n\nmae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n\n print(\"MAE: \", mae_xgb)\nscore(xgb_model,X_val,name='XG Boost')","705910ef":" def hyperParameterTuning(X_train, y_train):\n        param_tuning = {\n         'learning_rate': [0.01, 0.1,0.05],\n        'max_depth': [3, 5, 7, 10],\n         'min_child_weight': [1, 3, 5],\n         'subsample': [0.5, 0.7],\n         'colsample_bytree': [0.5, 0.7],\n         'n_estimators' : [40, 50, 60],\n         'objective': ['reg:squarederror']\n     }\n\n        xgb_model = XGBRegressor()\n\n        gsearch = GridSearchCV(estimator = xgb_model,\n                            param_grid = param_tuning,                        \n                            #scoring = 'neg_mean_absolute_error', #MAE\n                            #scoring = 'neg_mean_squared_error',  #MSE\n                            cv = 5,\n                            n_jobs = -1,\n                            verbose = 1)\n\n        gsearch.fit(X_train,y_train)\n        ypred=gsearch.predict(X_val)\n        ypred_1=gsearch.predict(X_test)\n        \n        print('Accuracy of training set: ',round(gsearch.score(X_train,y_train)*100,2))\n        print('Accuracy of test set: ',round(gsearch.score(X_val,y_val)*100,2))\n        print('mean squared error: ',round(mean_squared_error(y_val,ypred)\/100,2))\n        print('mean absolute error: ',round(mean_absolute_error(y_val,ypred),2))\n        print('r2_score: ',round(r2_score(y_val,ypred)*100,2))\n        return ypred_1\n        ","a55bf2ea":" ypred_1 = hyperParameterTuning(X_train, y_train)","31d5e543":"ypred2_df=pd.DataFrame(ypred_1,columns=['revenue'])\nypred2_df.to_csv('Submission_GridSearch.csv',index=False)","59c015eb":"import optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer, StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\ndtrain = xgb.DMatrix(X_train,y_train)\ndtest  = xgb.DMatrix(X_val,y_val)","874e22b8":"def objective(trial):\n   \n    param = {\n#               \n                'max_depth':trial.suggest_int('max_depth', 0,12 ),\n                'reg_alpha':trial.suggest_uniform('reg_alpha',0.5,5),\n                'reg_lambda':trial.suggest_uniform('reg_lambda',0,5),\n                'min_child_weight':trial.suggest_int('min_child_weight',0,4),\n                'gamma':trial.suggest_uniform('gamma', 1,8 ),\n                'learning_rate':trial.suggest_loguniform('learning_rate',0.15,2),\n                'colsample_bytree':trial.suggest_uniform('colsample_bytree',0.1,1),\n                'subsample':trial.suggest_uniform('subsample',0.05,1),\n\n                'nthread' : -1\n            }\n    return(return_rmse(param)) # this will return the rmse score ","16c591d4":"import re\ndef return_rmse(params):\n    model = xgb.train(params , dtrain, num_boost_round = 60, evals = [(dtest, 'eval')],\n          early_stopping_rounds=20,verbose_eval = 0)\n    result = model.eval(dtest)\n    result = np.float(re.search(r'[\\d.]+$',result).group(0))\n    print(result)\n    return(result)","abbc01d7":"study1 = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy1.optimize(objective, n_trials= 60,show_progress_bar = True)","c1d1465a":"study1.best_params\n#model(study1.best_params)","58539df3":"optuna.visualization.plot_optimization_history(study1)","8727f55c":"optuna.visualization.plot_slice(study1)","fc971f0c":"xgb_model = XGBRegressor(**(study1.best_params))\n\n%time xgb_model.fit(X_train, y_train, early_stopping_rounds=7, eval_set=[(X_val, y_val)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(X_val)\ny_pred_xgb2 = xgb_model.predict(X_test)\n\nmae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n\nprint(\"MAE: \", mae_xgb)\nscore(xgb_model,X_val,name='XG Boost')","6bc00ca2":"ypred2_df=pd.DataFrame(y_pred_xgb2,columns=['revenue'])\nypred2_df.to_csv('Submission_optuna.csv',index=False)","3a8a190d":"rf=RandomForestRegressor(n_estimators=25,max_depth=5)\nrf.fit(X_train,y_train)\nypredrf=rf.predict(test[features])\nscore(rf,X_val,name='Random Forest')","fedd0993":"ypred_df=pd.DataFrame(ypredrf,columns=['revenue'])\nypred_df.head(10)","2be03769":"ypred_df.to_csv('Submission_RF.csv',index=False)","8013154a":"## Introduction\n**This challenge is the capstone project of the Summer Analytics, a primer course on Data Science, conducted by Consulting and Analytics Club of IIT Guwahati in the summers.**\n\n**The dataset is provided by DeltaX is the pioneering cross-channel digital advertising platform. The cloud-based platform leverages big data, user behavior, and machine learning algorithms to improve performance across the business funnel of advertisers.**","80289efb":"## 2. XGBoost Algorithm","d9d425d7":"## Train Test Split","6ad2f2f4":"## Importing modules","51693402":"## Feature Engineering","f3f1c99d":"##  Random Forest Regressor","d4225a1f":"## Feature Trnsformation and Scaling","af2ac254":"## Feature Selection","92c36185":"#  HyperTuning","52de9793":"## Reading and exploring dataset","73bbb7dd":"## 1. Linear Regresion","ae54054f":"# Optuna HyperTuning\n","d4802e35":"## Exploratory Data Analysis","964fdc98":"## Model Building and Evaluation"}}