{"cell_type":{"b7ac7815":"code","052bba13":"code","2e77779c":"code","8da8cb84":"code","66f9cd0b":"code","fcaf5767":"code","b2c50f09":"code","67cb1ad9":"code","5b7acfba":"code","33f0854f":"code","d5543a3d":"code","5dcec1e5":"code","7ff45d63":"code","aae4845f":"code","a5668715":"code","115ff3c6":"code","ad045fc3":"code","b72ee27a":"code","e0424670":"markdown","ac311204":"markdown","9ba026b3":"markdown","6fd93d7d":"markdown","48e292e2":"markdown","b32935cb":"markdown"},"source":{"b7ac7815":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.compose import TransformedTargetRegressor\n\nimport warnings\nwarnings.simplefilter(action='ignore')","052bba13":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf.head()","2e77779c":"df.info()","8da8cb84":"missing = df.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\nmissing.plot.bar()","66f9cd0b":"df.dtypes.value_counts().plot.bar()","fcaf5767":"target = df['SalePrice']\ndf.drop(columns=['Id', 'SalePrice'], inplace=True)","b2c50f09":"num_features = [f for f in df.columns if len(df[f].unique()) > 20 and df[f].dtype != 'object' ]\nquantitatives = df[num_features]\nqualitatives = df[list(set(df.columns.to_list()) - set(num_features))]","67cb1ad9":"missing = quantitatives.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False) \/ len(df)\nmissing.plot.bar()","5b7acfba":"# for i in quantitatives.columns:\n#     f, axis = plt.subplots(1,2, figsize=(20, 5))\n#     sns.histplot(quantitatives[i], ax=axis[0])\n#     sns.scatterplot(quantitatives[i], target, alpha=0.2, ax=axis[1])\n#     plt.show()","33f0854f":"missing = qualitatives.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False) \/ len(df)\nmissing.plot.bar()","d5543a3d":"# for i in qualitatives.columns:\n#     f, axis = plt.subplots(1,2, figsize=(20, 5))\n#     qualitatives[i].value_counts().plot.bar(ax=axis[0])\n#     sns.boxplot(qualitatives[i], target, ax=axis[1])\n#     plt.show()","5dcec1e5":"corr = quantitatives.corrwith(target)\nsns.heatmap(corr.sort_values(ascending=False).to_frame(), annot=True)","7ff45d63":"fig,axes = plt.subplots(2,2, figsize=(20, 10))\nfig.suptitle('Target Distribution', fontsize=16, y=0.95)\nsns.distplot(target,ax=axes[0][0])\nsns.distplot(np.log1p(target),ax=axes[0][1])\nsns.boxplot(target, ax=axes[1][0])\nsns.boxplot(np.log1p(target), ax=axes[1][1])","aae4845f":"numeric_features = df.select_dtypes(exclude='object').columns.to_list()\n\ncategorical_features = df.select_dtypes('object').columns.to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(df[numeric_features + categorical_features],\n                                                    target,\n                                                    test_size=0.25,\n                                                    random_state=42)\n\nimputer = SimpleImputer(strategy='mean')\nX_train[numeric_features] = imputer.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = imputer.transform(X_test[numeric_features])\n\nimputer = SimpleImputer(strategy='constant')\nX_train[categorical_features] = imputer.fit_transform(X_train[categorical_features])\nX_test[categorical_features] = imputer.transform(X_test[categorical_features])\n\nencoder = MEstimateEncoder()\nX_train[categorical_features] = encoder.fit_transform(X_train[categorical_features], y_train)\nX_test[categorical_features] = encoder.transform(X_test[categorical_features])\n\nscale = StandardScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","a5668715":"models = [('Linear Regression', LinearRegression()), ('Ridge', Ridge()), ('Lasso', Lasso())]\nnames = []\ntrain_scores = []\ntest_scores = []\nfor name, model in models:\n    regressor = TransformedTargetRegressor(model , func=np.log, inverse_func=np.exp)\n    regressor.fit(X_train, y_train)\n    names.append(name)\n    train_scores.append(regressor.score(X_train, y_train))\n    score = regressor.score(X_test, y_test)\n    test_scores.append(score)\n    print('%s > r2 score : ' %name, score)\n\nfig, ax = plt.subplots()\n\nax.bar(names, train_scores, label='Train', alpha=0.5)\nax.bar(names, test_scores, label='Test', alpha=0.5)\n\nax.set_ylabel('Scores')\nax.set_title('R^2 by Models')\nplt.ylim(0.8, 1)\nax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n\nplt.show()","115ff3c6":"regressor = TransformedTargetRegressor(Ridge() , func=np.log, inverse_func=np.exp)\nregressor.fit(X_train, y_train)\npred = regressor.predict(X_test)\n\nf, axs = plt.subplots(2,1, figsize=(10, 10))\naxs[0].axvline(color='r')\naxs[1].axvline(color='r')\nsns.distplot(y_test - pred, bins=30, ax=axs[0])\nsns.boxplot(y_test - pred, ax=axs[1])","ad045fc3":"plt.scatter(y_test, pred, alpha=0.2)\nplt.plot(y_test, y_test, color='r')\nplt.show()","b72ee27a":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nX = df[numeric_features + categorical_features]\ndf_test = df_test[numeric_features + categorical_features]\n\nimputer = SimpleImputer(strategy='mean')\nX[numeric_features] = imputer.fit_transform(X[numeric_features])\ndf_test[numeric_features] = imputer.transform(df_test[numeric_features])\n\nimputer = SimpleImputer(strategy='constant')\nX[categorical_features] = imputer.fit_transform(X[categorical_features])\ndf_test[categorical_features] = imputer.transform(df_test[categorical_features])\n\nscale = StandardScaler()\nX[numeric_features] = scale.fit_transform(X[numeric_features])\ndf_test[numeric_features] = scale.transform(df_test[numeric_features])\n\nencoder = MEstimateEncoder(m=0.5)\nX[categorical_features] = encoder.fit_transform(X[categorical_features], target)\ndf_test[categorical_features] = encoder.transform(df_test[categorical_features])\n\n\nmodel = TransformedTargetRegressor(Ridge() , func=np.log, inverse_func=np.exp)\nmodel.fit(X, target)\n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = model.predict(df_test)\nsubmission.to_csv('my_submission.csv', index=False)","e0424670":"# Numerical","ac311204":"# Feature engineering","9ba026b3":"## Target","6fd93d7d":"# Report (r2 score 0.911)\n\n* Select all features.\n* Handle the missing data.\n    * For quantitative features --> mean\n    * For qualitative features --> constant\n* encode qualitative features using **TargetEncoder**\n* Standarize features\n* Log Transform Target\n* Use Ridge","48e292e2":"# Submit","b32935cb":"# Categorical"}}