{"cell_type":{"dd3115e8":"code","fdca72aa":"code","24cc2c9d":"code","dab88f72":"code","f6d1197f":"code","f9137b17":"code","c8763dba":"code","b17c4081":"code","e0d68e0e":"code","a51245c4":"code","0e4de248":"code","2687924f":"code","9aab8d3a":"code","266ff094":"code","465510b4":"code","4091222b":"code","c87f4c95":"code","90750821":"code","c89a406f":"code","7ae302ac":"code","d34e4c3f":"code","19ac2ec2":"code","95f8be98":"code","2d67a44c":"code","09a1fe53":"code","13799222":"code","6c43e6db":"code","5a93f7a6":"code","3f5abaef":"code","c3eb0fa5":"code","af27eb0a":"code","602dcd88":"code","d1a41c0e":"code","8ec0b582":"code","4b7b928d":"code","564950f2":"code","33e0c124":"code","37115bd7":"code","f18eed4d":"code","1bc30d27":"code","40648195":"code","7172a32a":"code","ad6e6150":"code","00e99c48":"markdown","c46aded8":"markdown","96922d47":"markdown","251d9b3c":"markdown","dce80beb":"markdown","9ddf249e":"markdown","c45deb4f":"markdown","33fa6e32":"markdown","a62184e2":"markdown","e9e0959d":"markdown","4d8a40b5":"markdown","640ef46b":"markdown","2677b031":"markdown","6e5d9ca3":"markdown","df6ec4bb":"markdown","15ddc13c":"markdown","b199f783":"markdown"},"source":{"dd3115e8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams['xtick.labelsize']=8\nplt.rcParams['ytick.labelsize']=8\n\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error, accuracy_score,confusion_matrix, roc_curve, auc,classification_report, recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","fdca72aa":"train_data = pd.read_csv(\"\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv\")","24cc2c9d":"train_data.drop(\"Unnamed: 0\", axis=1, inplace=True)\ntrain_data.describe()","dab88f72":"plt.figure(figsize=(8,3))\ntrain_data['SeriousDlqin2yrs'].value_counts().plot(kind='bar')","f6d1197f":"class0 = train_data['SeriousDlqin2yrs'].value_counts()[0]\nclass1 = train_data['SeriousDlqin2yrs'].value_counts()[1]\nprint(\"class 0 : {}\".format(class0))\nprint(\"class 1 : {}\".format(class1))\nprint(\"delinquency rate: {}\".format(class1\/(class0+class1)))","f9137b17":"train_data[train_data['age'] < 18]","c8763dba":"plt.figure(figsize=(8,3))\nsns.boxplot(train_data['age'])","b17c4081":"train_data.loc[train_data['age'] < 18, 'age'] = train_data['age'].median()","e0d68e0e":"cols =[\"NumberOfTime30-59DaysPastDueNotWorse\",\"NumberOfTime60-89DaysPastDueNotWorse\",\"NumberOfTimes90DaysLate\", \"DebtRatio\",\"NumberOfOpenCreditLinesAndLoans\",\"NumberRealEstateLoansOrLines\", \"RevolvingUtilizationOfUnsecuredLines\"]\nfig, axes = plt.subplots(len(cols),1, figsize=(10,10))\ni = 0\nfor c in cols:\n    ax = sns.boxplot(train_data[c], ax = axes[i])\n    ax.set_ylabel(c, rotation=0,labelpad=150)\n    ax.set_xlabel(\"Number of Times\")\n    i +=1\nplt.show()","a51245c4":"debtratio_q = train_data[\"DebtRatio\"].quantile(0.86)\nprint(\"Debt Ratio: {}\".format(debtratio_q))\n\ncolormap = {0:'blue', 1:'red'}\n\nfig, (ax1, ax2) = plt.subplots(1,2,  figsize=(15,4))\nfor delinquency, color in colormap.items():\n    tmp = train_data[(train_data['DebtRatio'] > debtratio_q) & (train_data['SeriousDlqin2yrs']==delinquency)][['DebtRatio','MonthlyIncome']]\n    ax1.scatter((tmp['DebtRatio']), (tmp['MonthlyIncome']), c=color, alpha=0.8, label= str(delinquency) + \":{}\".format(tmp.shape[0]))\nax1.legend()\nax1.set_title(\"Debt Ratio 86% Quantile against Monthly Income\",fontsize=10)\nax1.set_xlabel(\"DebtRatio\")\nax1.set_ylabel(\"Monthly Income\")\n\nfor delinquency, color in colormap.items():\n    tmp = train_data[(train_data['SeriousDlqin2yrs']==delinquency)][['DebtRatio','MonthlyIncome']]\n    ax2.scatter(np.log(tmp['DebtRatio']), np.log(tmp['MonthlyIncome']), c=color, alpha=0.8, label= str(delinquency) + \":{}\".format(tmp.shape[0]))\nax2.legend()\nax2.set_title(\"Log of Debt Ratio against log of Monthly Income\",fontsize=10)\nax2.set_xlabel(\"log(DebtRatio)\")\nax2.set_ylabel(\"log(Monthly Income)\")\nplt.show()","0e4de248":"print(\"Number of records with monthly income equals 1: {}\".format(train_data[(train_data['MonthlyIncome'] == 1) ].shape[0]))","2687924f":"train_data[\"DebtRatio\"] = np.log(train_data[\"DebtRatio\"])\nremovedmonthincome_1 = train_data[train_data[\"MonthlyIncome\"] != 1]\nremovedmonthincome_1[\"DebtRatio\"].replace([np.inf, -np.inf], -10, inplace=True)\n\nremovedmonthincome_1.describe()","9aab8d3a":"u_list = []\nd_list = []\nfor u in range(int(train_data['RevolvingUtilizationOfUnsecuredLines'].max())):\n    default_rate = train_data[train_data['RevolvingUtilizationOfUnsecuredLines'] > u]['SeriousDlqin2yrs'].mean()\n    u_list.append(u)\n    d_list.append(default_rate)","266ff094":"fig, (ax1,ax2)= plt.subplots(1,2, figsize=(15,4))\ndf = pd.DataFrame({\"utilization\":u_list, \"default\":d_list})\ndf.plot(\"utilization\",\"default\",ax=ax1)\nax1.set_ylabel(\"Default Rate\")\nax1.set_xlabel(\"Utilization Ratio\")\n\nutilization_outlier = df[df.default==0]['utilization'].min()\nprint(\"Remove Outliers at point UtilizatoinRation={}\".format(utilization_outlier))\n\ndftmp = df[df < utilization_outlier]\ndftmp.plot(\"utilization\",\"default\",ax=ax2)\nax2.plot([3500 for i in range(dftmp.shape[0])], np.linspace(0,0.35,dftmp.shape[0]), 'r--')\nax2.plot([6200 for i in range(dftmp.shape[0])], np.linspace(0,0.35,dftmp.shape[0]), 'r--')\nax2.set_ylabel(\"Default Rate\")\nax2.set_xlabel(\"Utilization Ratio\")","465510b4":"removedUtilization = train_data[train_data.RevolvingUtilizationOfUnsecuredLines <= utilization_outlier]\n\ndef categorize_utilization(u):\n    if u < 3500:\n        return 0\n    elif (u >= 3500) & (u < 6200):\n        return 1\n    else:\n        return 2\n    \nremovedUtilization[\"UtlizationCategory\"] = removedUtilization[\"RevolvingUtilizationOfUnsecuredLines\"].apply(categorize_utilization)","4091222b":"cols = train_data.columns\nnullcounts = []\nvalue_counts = []\nfor col in cols:\n    nullcounts.append(train_data[col].isnull().sum())\n    value_counts.append(train_data[col].shape[0] - train_data[col].isnull().sum())\n\nfig, ax = plt.subplots(figsize=(10,3))\nax.barh(cols, value_counts, label='not missing')\nax.barh(cols, nullcounts, label='missing', left=value_counts)\nax.set_xlabel('Value Count')\nax.set_ylabel('Labels')\nplt.show()","c87f4c95":"imputedf = train_data[['age','NumberOfDependents','MonthlyIncome']].copy()\n\ndef categorizeAge(age):\n    if (age < 35):\n        return 'junior'\n    elif (age >= 35) & (age < 60):\n        return'senior'\n    else:\n        return 'mature'\n\nimputedf['seniority'] = imputedf['age'].apply(categorizeAge)\nincome_dict = imputedf.groupby('seniority')['MonthlyIncome'].mean().to_dict()\nincome_dict","90750821":"#Impute Monthly Income by median of seniority\nfor k, v in income_dict.items():\n    imputedf[\"MonthlyIncome\"] = np.where((imputedf[\"MonthlyIncome\"].isnull()) & (imputedf['seniority'] == k), int(v), imputedf[\"MonthlyIncome\"])\ntrain_data['MonthlyIncome'] = imputedf[\"MonthlyIncome\"]","c89a406f":"# Impute NumberOfDependents with Mode\nprint(train_data['NumberOfDependents'].mode())\n# Fill na with mode \ntrain_data['NumberOfDependents'].fillna(0, inplace=True)","7ae302ac":"corr = train_data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True, fmt=\".2f\")","d34e4c3f":"print(train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"].sort_values().unique())\nprint(train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"].sort_values().unique())\nprint(train_data[\"NumberOfTimes90DaysLate\"].sort_values().unique())","19ac2ec2":"tmpdf = train_data[(train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"] == 98) & (train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"] == train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"]) & (train_data[\"NumberOfTimes90DaysLate\"] == train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"])][cols]\nprint(\"98 times past due where 3 columns have same value: {}\".format(tmpdf.shape[0]))","95f8be98":"# Impute outliers with max value\ntimes_map = {\"NumberOfTime30-59DaysPastDueNotWorse\": 13, \"NumberOfTime60-89DaysPastDueNotWorse\":11, \"NumberOfTimes90DaysLate\":17}\nfor col, v in times_map.items():\n    train_data.loc[train_data[col] >= 96, col] = times_map[col]","2d67a44c":"train_data[\"CombinedPastDue\"] = train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"] + train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"] + train_data[\"NumberOfTimes90DaysLate\"]","09a1fe53":"train_data[\"CombinedCreditLoans\"] = train_data[\"NumberOfOpenCreditLinesAndLoans\"] + train_data[\"NumberRealEstateLoansOrLines\"]","13799222":"income_dict","6c43e6db":"def categorizeAge(age):\n    if (age < 35):\n        return 'junior'\n    elif (age >= 35) & (age < 60):\n        return'senior'\n    else:\n        return 'mature'\n    \ndef data_preprocess(df, is_submission=False):\n    print(\"Shape before: {}\".format(df.shape))\n    df.loc[df['age'] < 18, 'age'] = df['age'].median()\n    \n    df[\"DebtRatio\"] = np.log(df[\"DebtRatio\"])\n    df[\"DebtRatio\"].replace([np.inf, -np.inf], -10, inplace=True)\n    if not is_submission:\n        df = df[df[\"MonthlyIncome\"] != 1]\n    \n    utilization_outlier = 8328\n    if not is_submission:\n        df = df[df.RevolvingUtilizationOfUnsecuredLines <= utilization_outlier]\n#     df[\"UtlizationCategory\"] = df[\"UtilisationRatio\"].apply(categorize_utilization)\n    \n    imputedf = df[['age','NumberOfDependents','MonthlyIncome']].copy()\n    imputedf['seniority'] = imputedf['age'].apply(categorizeAge)\n    income_dict = imputedf.groupby('seniority')['MonthlyIncome'].mean().to_dict()\n    for k, v in income_dict.items(): \n        imputedf[\"MonthlyIncome\"] = np.where((imputedf[\"MonthlyIncome\"].isnull()) & (imputedf['seniority'] == k), int(v), imputedf[\"MonthlyIncome\"])\n    df['MonthlyIncome'] = imputedf[\"MonthlyIncome\"]\n    \n    df['NumberOfDependents'].fillna(0, inplace=True)\n    \n    times_map = {\"NumberOfTime30-59DaysPastDueNotWorse\": 13, \"NumberOfTime60-89DaysPastDueNotWorse\":11, \"NumberOfTimes90DaysLate\":17}\n    for col, v in times_map.items():\n        df.loc[df[col] >= 96, col] = times_map[col]\n    \n    df[\"CombinedPastDue\"] = df[\"NumberOfTime30-59DaysPastDueNotWorse\"] + df[\"NumberOfTime60-89DaysPastDueNotWorse\"] + df[\"NumberOfTimes90DaysLate\"]\n    \n    df[\"CombinedCreditLoans\"] = df[\"NumberOfOpenCreditLinesAndLoans\"] + df[\"NumberRealEstateLoansOrLines\"]\n    \n    print(\"Shape after: {}\".format(df.shape))\n    return df\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv\")\ntrain_data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n\ntrain_data = data_preprocess(train_data)","5a93f7a6":"corr = train_data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True, fmt=\".2f\")","3f5abaef":"train_data.columns","c3eb0fa5":"cols_drop = [\"NumberOfTimes90DaysLate\",\"NumberOfTime60-89DaysPastDueNotWorse\",\"NumberOfOpenCreditLinesAndLoans\",\"NumberRealEstateLoansOrLines\"]\ntrain_data.drop(cols_drop, axis=1, inplace=True)","af27eb0a":"train_data.sample(5)","602dcd88":"X_train, X_test, y_train, y_test = train_test_split(train_data.iloc[:,1:], train_data.iloc[:,0], random_state=42)","d1a41c0e":"scaler = StandardScaler().fit(X_train)\n\nX_train_scaled = scaler.transform(X_train) \nX_test_scaled = scaler.transform(X_test)","8ec0b582":"logit = LogisticRegression(random_state=42)\nl_model = logit.fit(X_train_scaled, y_train)","4b7b928d":"logit_scores_proba  = l_model.predict_proba(X_test_scaled)\nlogit_scores = logit_scores_proba[:,1]","564950f2":"def plot_roc(y_test, y_predict):\n    fpr, tpr, _ = roc_curve(y_test, y_predict)\n    roc_auc = auc(fpr,tpr)\n    print(roc_auc)\n    plt.figure(figsize=(10,8))\n    plt.title(\"ROC curve\")\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0,1], [0,1],'r--')\n    plt.legend(loc=\"lower right\")\nplot_roc(y_test, logit_scores)","33e0c124":"random_forest = RandomForestClassifier()\nparam_grid={\n    \"n_estimators\":[9,18,27,36,100],\n    \"max_depth\":[5,7,9],\n    \"min_samples_leaf\":[2,4,6,8]\n}\nrf_model = RandomizedSearchCV(random_forest, param_distributions = param_grid, cv=5)\nrf_model.fit(X_train, y_train)","37115bd7":"rf_model.best_params_","f18eed4d":"best_est_rf = rf_model.best_estimator_\nbest_est_rf.fit(X_train, y_train)\ny_pred_rf = best_est_rf.predict_proba(X_test)[:,1]","1bc30d27":"plot_roc(y_test, y_pred_rf)","40648195":"def plot_feature_importances(model):\n    plt.figure(figsize=(10,8))\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\n\nplot_feature_importances(best_est_rf)","7172a32a":"test_data = pd.read_csv(\"\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv\")\ntest_data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n\ntest_data = data_preprocess(test_data, True)\n\ncols_drop = [\"SeriousDlqin2yrs\",\"NumberOfTimes90DaysLate\",\"NumberOfTime60-89DaysPastDueNotWorse\",\"NumberOfOpenCreditLinesAndLoans\",\"NumberRealEstateLoansOrLines\"]\ntest_data.drop(cols_drop, axis=1, inplace=True)\ntest_data.shape","ad6e6150":"submission_score = best_est_rf.predict_proba(test_data)[:,1]\n\nids = np.arange(1,101504)\nsubmission = pd.DataFrame( {'Id': ids, 'Probability': submission_score})\nsubmission.to_csv(\"\/kaggle\/working\/credit_score_submision.csv\", index=False)","00e99c48":"There are only two features with missing values, I would:\n- Impute Monthly Income with the mean income of three groups categorized by age\n- Impute Number of Dependents with its mode ","c46aded8":"It is expected that the higher the utlization ratio, the higher the default rate is. Let me look into that by plotting the utlization ratio to default rate","96922d47":"From the plotting the Default Rate against the utilization ratio, it's observed that the default rate stops increasing and reaches zero when utilization increases til a certain point. Also, there seems to be three groups of utilization ratio that shows positive linear relationship with the default rate.\n\nFrom this observation, I would:\n- Remove outliers at the utilization point when the default rate stops increasing\n- create a new feature which categorize Utilization Ratio into 3 groups","251d9b3c":"### Data Preprocessing","dce80beb":"### Basis Model - Logitistic Regression","9ddf249e":"#### Debt Ratio","c45deb4f":"There are highly correlation between the \"N Days Past Due\" features and also some outliers, I would:\n- Impute the outliers\n- Combined (N Days Past Due) as new feature\n- Combine (Credit Lines and Loans) as new feature","33fa6e32":"## Age","a62184e2":"## Label Distribution","e9e0959d":"Number of records with monthly income equals 1: 605\nIt is oberserved that top 14% of the debt ratio records have a Monthly Income of either 0 or 1. The debt to income ratio of these records are more than 453 times. It is suspected that there are data entry error or incomplete information given when Monthly Income is 1 while Debt Ratio is high.\n\nFrom the observation above, I would:\n\nTake log of the Debt Ratio\nRemove records where monthly income is 1","4d8a40b5":"### Random Forest","640ef46b":"## For Submission","2677b031":"### Features correlation","6e5d9ca3":"### Missing values","df6ec4bb":"From the observation, I would:\n- Impute age with median","15ddc13c":"#### Utlization Ratio","b199f783":"Impute Number of Dependents with mode of number of dependents"}}