{"cell_type":{"c15fdd51":"code","25bb92ab":"code","7ad0caba":"code","b2b9b0cc":"code","c07b42e3":"code","a583bd47":"code","1e865a9d":"code","2c75f73d":"code","650cba9b":"code","7f7eea3e":"code","ff18a199":"code","843a6045":"code","0246cc39":"code","d2ee073b":"code","3b7793e1":"code","2c5bdf31":"code","907a7881":"code","0d3a68e9":"code","a453d680":"code","86d31116":"code","f95a4a7f":"code","5c0dc513":"code","4bd53f6f":"code","389912b3":"code","fdbc163b":"code","a0b54b5c":"code","3810cb64":"code","fb020eb0":"code","0eae5406":"code","c6331bb7":"code","fe96d8fb":"code","92c49baf":"code","573f43ad":"code","90f71dc7":"code","c9185a6a":"code","5ec85a47":"code","bef80f03":"code","27aff11a":"code","ad1ed4dd":"code","7fb4fa9d":"code","7198deb3":"code","13cceb2c":"code","4b88257d":"code","494db309":"code","b77eec73":"code","357cc240":"code","6c18e3f8":"code","e93cbbaf":"code","49dee14f":"code","c39320d8":"code","9682ef08":"code","7c8eb351":"code","342fdc25":"code","927400b4":"code","c1c3b8ed":"code","83ddb5c8":"code","caf48200":"code","daef8376":"code","241f1aa0":"code","684c5e52":"code","e402e89b":"code","c1897a73":"code","44232f59":"code","0d87ecb5":"code","ac030df1":"code","660010a3":"code","5a6dd367":"code","f25402d2":"code","9be8ca40":"code","93f20cc8":"code","e5429bc2":"code","9ba92b53":"code","2a4ea0fb":"code","9121d230":"code","661ac1f3":"code","1f6b56db":"code","884f4fb5":"code","03b8f483":"code","5a72fbe0":"code","63c0e9f3":"code","1f87449f":"code","8a4d564a":"code","9ae5e3ac":"code","30e397c4":"code","046c9a4a":"code","bfceaf6f":"markdown","7e0cf3a1":"markdown","9f952286":"markdown","096da037":"markdown","fcd92f18":"markdown","331c2de4":"markdown","b954b16a":"markdown","40b25d4f":"markdown","001f6d87":"markdown","2f682189":"markdown","4bcd0fe2":"markdown","9f37ba6f":"markdown","e54fedf4":"markdown","31e3da7d":"markdown","a0882309":"markdown","7bf305d2":"markdown","58d73108":"markdown","cf4313da":"markdown","d30edc29":"markdown","e7c037a9":"markdown","c39550d7":"markdown","83fd2293":"markdown","730e9e51":"markdown","040b5546":"markdown","089f4325":"markdown","9315cb9a":"markdown","f2f89d11":"markdown","5cc9c53d":"markdown","24c7249c":"markdown","861ea145":"markdown","b6d810f9":"markdown","c9513647":"markdown","eb0d2742":"markdown","989feb7e":"markdown","3354de3d":"markdown","77308048":"markdown","cf81b603":"markdown","24529976":"markdown","baedc057":"markdown","8a225a75":"markdown","473adb21":"markdown","a0359ffc":"markdown","6d198f18":"markdown","048c7e91":"markdown","1316a7e0":"markdown","b95bf714":"markdown","6188dd50":"markdown","6838e347":"markdown","445583c3":"markdown","c46a4359":"markdown","5cd52f60":"markdown","d8a57dbb":"markdown","419b55c2":"markdown","d409ad3f":"markdown","0d79c322":"markdown","840b35c4":"markdown","6a1a07c8":"markdown","021100e3":"markdown","b5e31317":"markdown"},"source":{"c15fdd51":"# Just setting some configurations to not run all Kernel all time\n# RUN_VERSION_OPTS = [\"full\", \"final\", \"test\"]\n# run_version = \"final\"\n# def if_run(run_version):\n#     assert (run_version in RUN_VERSION_OPTS), \"Error, the {} option was not founded.\".format(run_version)\n#     if(run_version == \"full\")\n#         return True","25bb92ab":"# !pip install networkx","7ad0caba":"# !pip install seaborn","b2b9b0cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport networkx as nx\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.graph_objs import graph_objs\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport time\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, Lasso, LogisticRegression\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n#from sklearn.gaussian_process import GaussianProcessClassifier\n#from sklearn.gaussian_process.kernels import RBF\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import make_scorer\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c07b42e3":"data = pd.read_csv(\"..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\")\nprint(\"O dataset original tem: {} linhas x {} colunas\".format(data.shape[0], data.shape[1]))\n\n# Just making a simple adjustments to column names (put all in camel-case):\ndata.rename(columns={'oldbalanceOrg' : 'oldBalanceOrig',\n                    'newbalanceOrig' : 'newBalanceOrig',\n                    'oldbalanceDest' : 'oldBalanceDest',\n                     'newbalanceDest' : 'newBalanceDest'}, inplace=True)\n\n# Adding a id column to help to sumarize data\ndata['ID'] = list(range(1, data.shape[0]+1)) # Now and on the data will have 12 columns","a583bd47":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","1e865a9d":"COLORS = []","2c75f73d":"# Some constants\nm = data.shape[0]","650cba9b":"data_isFraud = data[data['isFraud'] == 1]\ndata_isNotFraud = data[data['isFraud'] == 0]","7f7eea3e":"# Some basic and usefull functions\ndef rounder(num, base, decimal_places):\n    return round(((num \/ base ) * 100),decimal_places)","ff18a199":"# Sneek-peak at the base:\ndata.head(20)","843a6045":"# Missing values analysis\nprint(\"Total de \\\"dados ausentes\\\": {}\".format(data.isnull().sum().sum()))","0246cc39":"# [2.1] Os valores transacionados podem ser negativos ?\nprint(\"Total de transa\u00e7\u00f5es com montante transacionado negativo: {}\".format(sum(data['amount'] < 0)))","d2ee073b":"# [2.2] Todas as transa\u00e7\u00f5es feitas afetam os saldos das contas (no total)?\nprint(\"O total transacionado foi de {}\\\n      \\nIsso implica dizer que a diferen\u00e7a entre os saldos (antigo e novo) das contas de Origem deveria ser esse,\\\n      \\ncontudo, podemos ver que n\u00e3o \u00e9 igual a: {};\\\n      \\nO mesmo vale para a difere\u00e7a entre os saldos (antigo e novo) das contas de Destino que na realiadade \u00e9: {}\"\n     .format(\n         round(sum(data['amount']), 2),\n         round((sum(data['newBalanceOrig']) - sum(data['oldBalanceOrig'])), 2),\n         #round((sum(data['newBalanceOrig'] - data['oldBalanceOrig'])), 2),\n         round((sum(data['oldBalanceDest']) - sum(data['newBalanceDest'])), 2),\n         #round((sum(data['oldBalanceDest'] - data['newBalanceDest'])), 2)\n     ))","3b7793e1":"data[['amount', 'oldBalanceOrig', 'newBalanceOrig', 'oldBalanceDest', 'newBalanceDest']].describe()","2c5bdf31":"num_columns = ['amount', 'oldBalanceOrig', 'newBalanceOrig', 'oldBalanceDest', 'newBalanceDest']\nfig = go.Figure()\nfor i in range(0, len(num_columns)):\n    fig.add_trace(go.Box(y=data[num_columns[i]], name=num_columns[i]))\nfig.show()","907a7881":"# An\u00e1lise da correla\u00e7\u00e3o dos dados\n# plt.matshow(data.corr())\n# plt.xticks(range(len(data.columns)), data.columns, rotation=90)\n# plt.yticks(range(len(data.columns)), data.columns)\n# plt.colorbar()\n# plt.show()\n\ndata.corr()\n\n# Doesn't working so good \u00ac\u00ac\n# from plotly.graph_objs import *\n# trace1 = {\n#   \"type\": \"heatmap\", \n#   \"x\": data.columns, \n#   \"y\": data.columns, \n#   \"z\": data.corr()\n# }\n# data_ = Data([trace1])\n# layout = {\"title\": \"Features Correlation Matrix\", \"width\" : 800, \"height\" : 400}\n# fig = Figure(data=data_, layout=layout)\n# fig.show()","0d3a68e9":"print(\"Existem {} ({}%) transa\u00e7\u00f5es marcadas como fraude contra {} ({}%) marcadas como n\u00e3o-fraude\"\n      .format(data_isFraud.shape[0],\n              rounder(data_isFraud.shape[0], m, 2),\n              data_isNotFraud.shape[0],\n              rounder(data_isNotFraud.shape[0], m, 2),\n             ))","a453d680":"isFraud_description = pd.DataFrame(data_isFraud[['amount', 'oldBalanceOrig', 'newBalanceOrig', 'oldBalanceDest', 'newBalanceDest']].describe()).T\nisNotFraud_description = pd.DataFrame(data_isNotFraud[['amount', 'oldBalanceOrig', 'newBalanceOrig', 'oldBalanceDest', 'newBalanceDest']].describe()).T\nisFraud_description.rename((lambda x: x+\"_fraud\"), inplace=True)\nisNotFraud_description.rename((lambda x: x+\"_not_fraud\"), inplace=True)\n\nDescription = isFraud_description.append(isNotFraud_description).sort_index().T\n\nDescription.head(20)","86d31116":"# Quantity of each type of transactions:\ntransaction_distribution = pd.DataFrame({\"Qnt_total\" : data.groupby('type')['ID'].nunique()})\ntransaction_distribution['Percentages_total'] = transaction_distribution.apply(lambda x : rounder(x ,m ,2))\n\ntransaction_isFraud_distribution = pd.DataFrame({\"Qnt_isFraud\" : data_isFraud.groupby('type')['ID'].nunique()})\ntransaction_isFraud_distribution['(Abs)Percentages_isFraud'] = transaction_isFraud_distribution.apply(lambda x : rounder(x ,m ,2))\ntransaction_isFraud_distribution['(Rel)Percentages_isFraud'] = transaction_isFraud_distribution['Qnt_isFraud'].apply(lambda x : rounder(x ,data_isFraud.shape[0] ,2))\n\ntransaction_isNotFraud_distribution = pd.DataFrame({\"Qnt_isNotFraud\" : data_isNotFraud.groupby('type')['ID'].nunique()})\ntransaction_isNotFraud_distribution['(Abs)Percentages_isNotFraud'] = transaction_isNotFraud_distribution.apply(lambda x : rounder(x ,m ,2))\ntransaction_isNotFraud_distribution['(Rel)Percentages_isNotFraud'] = transaction_isNotFraud_distribution['Qnt_isNotFraud'].apply(lambda x : rounder(x ,data_isNotFraud.shape[0] ,2))\n\ndfs = [transaction_distribution, transaction_isFraud_distribution, transaction_isNotFraud_distribution]\n\nDistribution = pd.concat(dfs, axis=1, sort=False)\nDistribution.fillna(0, inplace=True)\nDistribution = Distribution.reindex(['Qnt_total','Qnt_isFraud', 'Qnt_isNotFraud', 'Percentages_total', '(Abs)Percentages_isFraud', '(Rel)Percentages_isFraud', '(Abs)Percentages_isNotFraud', '(Rel)Percentages_isNotFraud'], axis=1)\nDistribution.head(5)","f95a4a7f":"data_aux = data.copy()\ntype_isFraud_distribution = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type', 'isFraud'])['ID'].nunique()})\ntype_isFraud_distribution.sort_values(['type', 'isFraud'])\n\ntype_isFraud_distribution_agg = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type'])['ID'].nunique()})\ntype_isFraud_distribution_agg.sort_values('type')\n\ntype_isFraud_distribution_perc = pd.DataFrame({\"Percentages\" : type_isFraud_distribution['Qnt'] \/ type_isFraud_distribution_agg['Qnt']})\ntype_isFraud_distribution_perc['Percentages'] = type_isFraud_distribution_perc['Percentages'].apply(lambda x: str(round((x*100), 4)) + \" %\")\ntype_isFraud_distribution_perc\n\ndfs = [type_isFraud_distribution, type_isFraud_distribution_perc]\n\nType_isFraud_distributions = pd.concat(dfs, axis=1, sort=False)\nType_isFraud_distributions.reset_index(inplace=True)\nType_isFraud_distributions.loc[Type_isFraud_distributions['isFraud'] == 0, 'isFraud'] = \"No\"\nType_isFraud_distributions.loc[Type_isFraud_distributions['isFraud'] == 1, 'isFraud'] = \"Yes\"\n\nfig = px.bar(Type_isFraud_distributions, x=\"type\", y=\"Qnt\", color='isFraud', barmode='stack', text='Percentages', width=800, height=500)\nfig.show()","5c0dc513":"#fig = px.histogram(data, x=\"amount\", color=\"isFraud\", nbins=7)\nfig = px.histogram(data, x=\"amount\", color=\"isFraud\", nbins=7, histnorm='percent')\nfig.show()","4bd53f6f":"#fig = px.histogram(data, x=\"amount\", color=\"isFraud\", nbins=7)\n#fig = px.histogram(data, x=\"amount\", color=\"isFraud\", nbins=7, histnorm='percent', max=5e07)\n#fig.show()","389912b3":"fig = px.histogram(data, x=\"step\", color=\"isFraud\", nbins=70, histnorm='percent')\nfig.show()","fdbc163b":"#C\u00f3digos para:\n# Quantidade de clientes \u00fanicos na origem\nunique_orig = data['nameOrig'].drop_duplicates().shape[0]\nunique_dest = data['nameDest'].drop_duplicates().shape[0]\n\nunique_orig_isFraud = data[data['isFraud'] == 1]['nameOrig'].drop_duplicates().shape[0]\nunique_orig_isNotFraud = data[data['isFraud'] == 0]['nameOrig'].drop_duplicates().shape[0]\n\nunique_dest_isFraud = data[data['isFraud'] == 1]['nameDest'].drop_duplicates().shape[0]\nunique_dest_isNotFraud = data[data['isFraud'] == 0]['nameDest'].drop_duplicates().shape[0]\n\nprint(\"Quantidade de clientes \u00fanicos na Origem: {}, destes:\\\n      \\n {} ({}%) Est\u00e3o diretamente envolvidos em fraude e\\\n      \\n {} ({}%) N\u00e3o.\".format(\n        unique_orig,\n        unique_orig_isFraud,\n        rounder(unique_orig_isFraud, unique_orig, 2),\n        unique_orig_isNotFraud,\n        rounder(unique_orig_isNotFraud, unique_orig, 2)\n     ))\n# Quantidade de clientes \u00fanicos no destino\nprint(\"Quantidade de clientes \u00fanicos no Destino: {}, destes:\\\n      \\n {} ({}%) Est\u00e3o diretamente envolvidos em fraude e\\\n      \\n {} ({}%) N\u00e3o.\".format(\n        unique_dest,\n        unique_dest_isFraud,\n        rounder(unique_dest_isFraud, unique_dest, 2),\n        unique_dest_isNotFraud,\n        rounder(unique_dest_isNotFraud, unique_dest, 2)\n     ))","a0b54b5c":"# Top 10 em qnt de transa\u00e7\u00f5es na origem\ntop_transactions_orig = pd.DataFrame({\"Qnt_Orig\" : data.groupby('nameOrig')['ID'].nunique()})\ntop_transactions_orig_aux = top_transactions_orig.sort_values('Qnt_Orig', ascending=False).head(10)\ntop_transactions_orig_aux.reset_index(inplace=True)\n\n# Top 10 em qnt de transa\u00e7\u00f5es no destino\ntop_transactions_dest = pd.DataFrame({\"Qnt_Dest\" : data.groupby('nameDest')['ID'].nunique()})\ntop_transactions_dest_aux = top_transactions_dest.sort_values('Qnt_Dest', ascending=False).head(10)\ntop_transactions_dest_aux.reset_index(inplace=True)\n\ntop_transactions = pd.concat([top_transactions_orig_aux, top_transactions_dest_aux], axis=1)\ntop_transactions","3810cb64":"descriptions = pd.concat([top_transactions_dest.describe(), top_transactions_orig.describe()], axis=1)\ndescriptions","fb020eb0":"# Free some memory\ndel top_transactions_orig_aux\ndel top_transactions_dest_aux\ndel top_transactions\ndel top_transactions_orig\ndel top_transactions_dest\ndel descriptions","0eae5406":"# sum(data['nameDest'].isin(data['nameOrig'])) (13308) != sum(data['nameOrig'].isin(data['nameDest'])) (1771)\nclients_in_both_orig_and_dest_filter = data['nameOrig'].isin(data['nameDest'])\nno_unique_clients_both_orig_and_dest = data['nameOrig'].drop_duplicates().isin(data['nameDest'].drop_duplicates())\nintersec = sum(no_unique_clients_both_orig_and_dest) # 1769\nprint(\"N\u00famero de contas de clientes que s\u00e3o origem e destino: {}\".format(intersec))","c6331bb7":"# And taking a further looking (see who they are)\ndata[clients_in_both_orig_and_dest_filter].head(30)","fe96d8fb":"from matplotlib_venn import venn2\nvenn2(subsets = (unique_orig, unique_dest, intersec), set_labels = ('# Diferentes contas na Origem', '# Diferentes contas no Destino'))\nplt.show()","92c49baf":"#e = 0.0000000001 # Just to eliminate round problems \ne = 0.1 # Just to eliminate round problems\ncases_unmatch_amount_and_origin = (data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > e)\nprint(\"Existem {} casos em que os valores (antigo e novo) do balan\u00e7o da conta de origem n\u00e3o batem com o valor transacionado\"\n      .format(sum(cases_unmatch_amount_and_origin)))\n\n# Taking a futher look on this particular set\n#data[cases_unmatch_amount_and_origin].head(10)","573f43ad":"rows = [\"(A) balanceOrig_x_amount\", \"(B) balanceDest_amount\", \"Intersec\u00e7\u00e3o A e B\", \"Uni\u00e3o A e B\"]\n_e = 0.0000000001\ne_high_precision = [\n    sum((data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e) & (data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e) | (data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e))\n]\n_e = 0.1\ne_low_precision = [\n    sum((data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e) & (data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e)),\n    sum((data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > _e) | (data['amount'] - (data['oldBalanceOrig'] - data['newBalanceOrig']) > _e))\n]\ne_diff = [(x - y) for x, y in zip(e_high_precision, e_low_precision)]\ndata_e = pd.DataFrame(data=zip(e_high_precision, e_low_precision, e_diff), index=rows)\ndata_e.columns = [\"e=10^-9\", \"e=10^-1\", \"diff\"]\ndata_e","90f71dc7":"# The same logic could be applied to the destination account:\n\ncases_unmatch_amount_and_dest = (data['amount'] - (data['newBalanceDest'] - data['oldBalanceDest']) > e)\nprint(\"Existem {} casos em que os valores (antigo e novo) do balan\u00e7o da conta de destino n\u00e3o batem com o valor transacionado\"\n      .format(sum(cases_unmatch_amount_and_dest)))\n\n# Taking a futher look on this particular set\n#data[cases_unmatch_amount_and_dest].head(10)","c9185a6a":"cases_unmatch_amount_and_origin_and_dest = cases_unmatch_amount_and_origin & cases_unmatch_amount_and_dest\n\n# And analising the cases in which both \"inconsistences\" occurs:\nprint(\"Existem {} casos em que os valores (antigo e novo) de ambos os balan\u00e7os (contas de origem e de destino) n\u00e3o batem com o valor transacionado\"\n      .format(sum(cases_unmatch_amount_and_origin_and_dest)))\n\n# Taking a futher look on this particular set\n#data[cases_unmatch_amount_and_origin_and_dest].head(10)","5ec85a47":"cases_unmatch_amount_or_origin_and_dest = cases_unmatch_amount_and_origin | cases_unmatch_amount_and_dest\n\n# And analising the cases in which both \"inconsistences\" occurs:\nprint(\"Existem {} casos em que os valores (antigo e novo) de um dos balan\u00e7os (contas de origem e de destino) n\u00e3o batem com o valor transacionado\"\n      .format(sum(cases_unmatch_amount_or_origin_and_dest)))\n\n# Taking a futher look on this particular set\n#data[cases_unmatch_amount_or_origin_and_dest].head(10)","bef80f03":"# The initial 'no' stands for \"Number Of\"\n\norig_merchants_filter = (data['nameOrig'].apply(lambda x : x[0]) == 'M')\nno_orig_merchants = sum(orig_merchants_filter)\ndest_merchants_filter = (data['nameDest'].apply(lambda x : x[0]) == 'M')\nno_dest_merchants = sum(dest_merchants_filter)\n\nprint(\"Existem {} transa\u00e7\u00f5es cuja origem \u00e9 em um 'Mercador' e {} transa\u00e7\u00f5es cujo destino seja um 'Mercador'.\"\n      .format(no_orig_merchants, no_dest_merchants))","27aff11a":"isFlaggedFraud_filter = (data['isFlaggedFraud'] == 1)\nno_isFlaggedFraud = sum(isFlaggedFraud_filter)\n\nprint(\"Existem {} transa\u00e7\u00f5es que existe a marca de 'isFlaggedFraud'.\"\n      .format(no_isFlaggedFraud))","ad1ed4dd":"data_aux = data.copy()\ndata_aux['diff_in_balances_orig'] = False\ndata_aux.loc[cases_unmatch_amount_and_origin, 'diff_in_balances_orig'] = True\ndata_aux['diff_in_balances_dest'] = False\ndata_aux.loc[cases_unmatch_amount_and_dest, 'diff_in_balances_dest'] = True\ntype_unmatch_distribution = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type', 'diff_in_balances_orig', 'diff_in_balances_dest'])['ID'].nunique()})\n#type_unmatch_distribution.sort_values('Qnt', ascending=False)\ntype_unmatch_distribution.sort_values(['type', 'diff_in_balances_orig'])\n\ntype_unmatch_distribution_agg = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type'])['ID'].nunique()})\ntype_unmatch_distribution_agg.sort_values('type')\n\ntype_unmatch_distribution_perc = pd.DataFrame({\"Percentages\" : type_unmatch_distribution['Qnt'] \/ type_unmatch_distribution_agg['Qnt']})\ntype_unmatch_distribution_perc['Percentages'] = type_unmatch_distribution_perc['Percentages'].apply(lambda x: str(round((x*100), 4)) + \" %\")\ntype_unmatch_distribution_perc\n\ndfs = [type_unmatch_distribution, type_unmatch_distribution_perc]\n\nType_unmatch_distributions = pd.concat(dfs, axis=1, sort=False)\nType_unmatch_distributions","7fb4fa9d":"data_aux = data.copy()\ndata_aux['diff_in_balances'] = False\ndata_aux.loc[cases_unmatch_amount_and_origin_and_dest, 'diff_in_balances'] = True\ntype_unmatch_distribution = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type', 'diff_in_balances'])['ID'].nunique()})\ntype_unmatch_distribution.sort_values(['type', 'diff_in_balances'])\n\ntype_unmatch_distribution_agg = pd.DataFrame({\"Qnt\" : data_aux.groupby(['type'])['ID'].nunique()})\ntype_unmatch_distribution_agg.sort_values('type')\n\ntype_unmatch_distribution_perc = pd.DataFrame({\"Percentages\" : type_unmatch_distribution['Qnt'] \/ type_unmatch_distribution_agg['Qnt']})\ntype_unmatch_distribution_perc['Percentages'] = type_unmatch_distribution_perc['Percentages'].apply(lambda x: str(round((x*100), 4)) + \" %\")\ntype_unmatch_distribution_perc\n\ndfs = [type_unmatch_distribution, type_unmatch_distribution_perc]\n\nType_unmatch_distributions = pd.concat(dfs, axis=1, sort=False)\nType_unmatch_distributions.reset_index(inplace=True)\nType_unmatch_distributions.loc[Type_unmatch_distributions['diff_in_balances'] == False, 'diff_in_balances'] = \"No\"\nType_unmatch_distributions.loc[Type_unmatch_distributions['diff_in_balances'] == True, 'diff_in_balances'] = \"Yes\"\n\nfig = px.bar(Type_unmatch_distributions, x=\"type\", y=\"Qnt\", color='diff_in_balances', barmode='group', text='Percentages', width=1000, height=500)\nfig.show()","7198deb3":"# cleaning some memory\ndel data_aux\ndel type_unmatch_distribution\ndel type_unmatch_distribution_agg\ndel type_unmatch_distribution_perc\ndel dfs","13cceb2c":"zero_amount_transactions_filter = (data['amount'] == 0)\n\nprint(\"{} transa\u00e7\u00f5es n\u00e3o movimentaram valor algum. S\u00e3o elas:\".format(sum(zero_amount_transactions_filter)))\n\ndata[zero_amount_transactions_filter]","4b88257d":"halted_transactions_filter = data['amount'] > 0 & cases_unmatch_amount_or_origin_and_dest\n\nhalted_transactions_filter_because_isFlaggedFraud = halted_transactions_filter & isFlaggedFraud_filter\nhalted_transactions_filter_because_orig_merchant = halted_transactions_filter & orig_merchants_filter\nhalted_transactions_filter_because_dest_merchant = halted_transactions_filter & dest_merchants_filter\n\nhalted_transactions_filter_because_merchant = halted_transactions_filter & (orig_merchants_filter | dest_merchants_filter)\n\nhalted_transactions_with_a_reason_filter = halted_transactions_filter & (orig_merchants_filter | dest_merchants_filter | isFlaggedFraud_filter)\n\n#no_hauted_transactions = sum(hauted_transactions_filter)\nprint(\"Em resumo:\\n{} Transa\u00e7\u00f5es \\\"N\u00e3o foram concretizadas\\\", porque n\u00e3o alteraram o balan\u00e7o ou da origem ou do destino e tiveram 'montante' transacionado maior que 0.\\\n        \\n\\n{} transa\u00e7\u00f5es n\u00e3o foram conclu\u00eddas porque havia um mercador envolvido ({} mercadores na origem e {} mercadores como destinat\u00e1rios)\\\n        \\n\\nOutras {} transa\u00e7\u00f5es tamb\u00e9m n\u00e3o foram conclu\u00eddas porque foram 'flagadas' como isFlaggedFraud.\\\n        \\n\\nSomando todas as situa\u00e7\u00f5es, temos que {} transa\u00e7\u00f5es n\u00e3o foram conclu\u00eddas seja porque havia um mercador envolvido na transa\u00e7\u00e3o, seja porque a transa\u00e7\u00e3o foi 'flagada' como isFlaggedFraud.\\\n      \"\n      .format(sum(halted_transactions_filter),\n             sum(halted_transactions_filter_because_merchant),\n              sum(halted_transactions_filter_because_orig_merchant),\n              sum(halted_transactions_filter_because_dest_merchant),\n              sum(halted_transactions_filter_because_isFlaggedFraud),\n              sum(halted_transactions_with_a_reason_filter)\n             ))\n\nhalted_transactions_with_no_reason_filter = (\n                                            (orig_merchants_filter == False) &\n                                            (dest_merchants_filter == False) & \n                                            (isFlaggedFraud_filter == False) &\n                                            (zero_amount_transactions_filter == False)\n                                            )\nprint(\"\\n\\n\\n\")\nprint(\"Por\u00e9m como explicar as outras {} transa\u00e7\u00f5es que n\u00e3o havia um mercador envolvido na transa\u00e7\u00e3o, a transa\u00e7\u00e3o n\u00e3o foi flagada como isFlaggedFraud e o montante transacionado foi superior a 0 ?\"\n      .format(sum(halted_transactions_with_no_reason_filter)))","494db309":"# Base com as transa\u00e7\u00f5es que possuem uma diferen\u00e7a com rela\u00e7\u00e3o ao balan\u00e7o, e que n\u00e3o possuem uma explica\u00e7\u00e3o\ndata[halted_transactions_with_no_reason_filter].head(40)","b77eec73":"# Top 10 transactions that are marked as \"Halted With No Reason\"\nnameOrig_distribution = pd.DataFrame({\"Qnt\" : data[halted_transactions_with_no_reason_filter].groupby('nameOrig')['ID'].nunique()})\nnameOrig_distribution.sort_values('Qnt', ascending=False).head(10)","357cc240":"C2098525306_data = data[(data['nameOrig'] == 'C2098525306') | (data['nameDest'] == 'C2098525306')]\nC2098525306_data","6c18e3f8":"print(\"N\u00famero de transa\u00e7\u00f5es envolvendo 'Mercadores', marcadas como fraude: {}\"\n      .format(sum((orig_merchants_filter | dest_merchants_filter) & data['isFraud'] == 1)))\n\n#data[data['amount'] == 0] -> Todas evid\u00eancias s\u00e3o consideradas Fraudes","e93cbbaf":"isFlaggedFraud = data[data['isFlaggedFraud'] == 1]\nisNotFlaggedFraud = data[data['isFlaggedFraud'] == 0]\n\nmax_isFlaggedFraud = max(isFlaggedFraud['amount'])\nmin_isFlaggedFraud = min(isFlaggedFraud['amount'])\nfalsePositive_isFlaggedFraud = isFlaggedFraud[isFlaggedFraud['amount'] < 200000].shape[0]\n\nmax_isNotFlaggedFraud = max(isNotFlaggedFraud['amount'])\nmin_isNotFlaggedFraud = min(isNotFlaggedFraud['amount'])\nfalseNegative_isFlaggedFraud = isNotFlaggedFraud[isNotFlaggedFraud['amount'] > 200000].shape[0]\n\nprint(\"Maior valor no conjunto de FlaggedFraud: {}\\\n       \\nMenor valor no conjunto de FlaggedFraud: {}\\\n       \\nQuantidade de evid\u00eancias marcadas como FlaggedFraud, que n\u00e3o s\u00e3o FlaggedFraud: {}\\\n        \\nMaior valor fora do conjunto de FlaggedFraud:{}\\\n        \\nMenor valor fora do conjunto de FlaggedFraud: {}\\\n        \\nQuantidade de evid\u00eancias n\u00e3o marcadas como FlaggedFraud, que deveriam ser FlaggedFraud: {}\".format(\n    max_isFlaggedFraud,\n    min_isFlaggedFraud,\n    falsePositive_isFlaggedFraud,\n    \n    max_isNotFlaggedFraud,\n    min_isNotFlaggedFraud,\n    falseNegative_isFlaggedFraud\n))","49dee14f":"data['realAmountTransacted'] = data['amount']\namount_gt_oldBalanceOrig_filter = data['amount'] > data['oldBalanceOrig']\ndata.loc[amount_gt_oldBalanceOrig_filter,'realAmountTransacted'] = data['oldBalanceOrig']\n\ndata['newBalanceCorrectedOrig'] = data['oldBalanceOrig'] - data['realAmountTransacted']\ndata['newBalanceCorrectedDest'] = data['oldBalanceDest'] + data['realAmountTransacted']\n\ninitial_sum_amount = sum(data['amount'])\ncorrected_sum_amount = sum(data['realAmountTransacted'])\n\nprint(\"O valor da soma do montante inicial de todas as transa\u00e7\u00f5es foi de: {}\\\n      \\nAp\u00f3s a corre\u00e7\u00e3o de balan\u00e7o considerando a restri\u00e7\u00e3o de \\\"Fundos Insuficientes\\\" o valor da soma do montante foi de: {}.\\\n      \\n(Ou seja, uma diferen\u00e7a de {} ({}%))\"\n      .format(round(initial_sum_amount, 2),\n              round(corrected_sum_amount, 2),\n              round((initial_sum_amount - corrected_sum_amount),2),\n              rounder((initial_sum_amount - corrected_sum_amount), initial_sum_amount,2)\n             ))\n\n\ndata.head(20)","c39320d8":"df_analysis_dest = pd.DataFrame()\ndf_analysis_dest['name'] = data['nameDest']\ndf_analysis_dest['step'] = data['step']\n\ndf_analysis = pd.DataFrame()\ndf_analysis['name'] = data['nameOrig']\ndf_analysis['step'] = data['step']\n\ndf_analysis = df_analysis.append(df_analysis_dest)\n\ndf_groupedby = pd.DataFrame({\"Qnt\" : df_analysis.groupby('name')['step'].nunique()})\ndf_groupedby.sort_values('Qnt', ascending=False).head(10)","9682ef08":"# Free some memory\ndel df_analysis_dest\ndel df_analysis\ndel df_groupedby","7c8eb351":"C1789550256_data = data[(data['nameOrig'] == 'C1789550256') | (data['nameDest'] == 'C1789550256')]\nC1789550256_data.head(50)","342fdc25":"data = data[(orig_merchants_filter == False) & (dest_merchants_filter == False) & (isFlaggedFraud_filter == False)]\n# shape[0] -> 6362620 - 4211109","927400b4":"# Solve skew problem\ntypes_with_both_examples = Distribution[(Distribution['Qnt_isFraud'] != 0) & (Distribution['Qnt_isNotFraud'] != 0)].index.values\n\ndata_type_with_both_examples_filter = data['type'].apply(lambda x: x in types_with_both_examples)\ndata = data[data_type_with_both_examples_filter]\n\ndata.reset_index(inplace=True)","c1c3b8ed":"# encode hot-in-one: type\n# https:\/\/machinelearningmastery.com\/how-to-one-hot-encode-sequence-data-in-python\/\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(data['type'])\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\ndata = pd.concat([data, pd.DataFrame(onehot_encoded)], axis=1)","83ddb5c8":"from sklearn import preprocessing\n\ndata_norm = data[['oldBalanceOrig', 'oldBalanceDest', 'realAmountTransacted', 'newBalanceCorrectedOrig', 'newBalanceCorrectedDest']]\n\nmode = \"std\"\n\n# Normalize Training Data \nstd_scale = preprocessing.StandardScaler(with_mean=True, with_std=True).fit(data_norm)\nmm_scale = preprocessing.MinMaxScaler().fit(data_norm)\n\n# Just to stop commenting one line and uncomment the other\nif(mode == \"std\"):\n    data_norm_aux = std_scale.transform(data_norm)\nelse:\n    data_norm_aux = mm_scale.transform(data_norm)\n\n# Converting numpy array to dataframe\ndata_normalized = pd.DataFrame(data_norm_aux, index=data_norm.index, columns=data_norm.columns)\n\ndata_aux = data.drop(['oldBalanceOrig', 'oldBalanceDest', 'realAmountTransacted', 'newBalanceCorrectedOrig', 'newBalanceCorrectedDest'], axis = 1)\n#data = pd.concat([data_normalized, data_aux], axis=1)\ndata_ = pd.concat([data_normalized, data_aux], axis=1)\n#data_.head(15)","caf48200":"# Test Purposes, delete this in the final version of this notebook:\n#data = data.head(2000)","daef8376":"#assert 1 == 2, \"Stop execution to analise something\"","241f1aa0":"def blend_parameters(params):\n    # Uniform Blending (all-versus-all)\n    grid_values = list(params.values())\n    keys = list(params.keys())\n    grid = []\n    for param_values in itertools.product(*grid_values):\n        params = dict(zip(keys, param_values))\n        grid.append(params)\n    return grid","684c5e52":"def evaluate_model(Model, X, y, model_params, verbose=False, cv=5):\n    import time\n    # All models need to have the following interface:\n    # - A Constructor that accept the params in 'model_params'\n    # - Need to be \"passable\" to the cross_val_score function\n    grid = blend_parameters(model_params)\n    best_params = {}\n    best_score = 0.0\n\n    for p in grid:\n        if(verbose):\n            print('Testing: ', p)\n        start_time = time.perf_counter()\n        model = Model(**p)\n        score = cross_val_score(model, X, y.values.ravel(), cv=cv).mean()\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        if(verbose):\n            print(\"Model tested in {} secs\".format(round(run_time, 4)))\n        if(verbose):\n            print(\"Result Score: {}\".format(round(score,4)))\n\n        if(score > best_score):\n            best_params = p\n            best_score = score\n\n    return (best_score, best_params)","e402e89b":"# Not generic\ndef get_stratified_samples(n_samples, data):\n    n_samples_isFraud = int(round((sum(data['isFraud'] == 1) \/ data.shape[0]) * n_samples, 1))\n    n_samples_isNotFraud = int(round((1 - (sum(data['isFraud'] == 1) \/ data.shape[0])) * n_samples, 1))\n    data_sample_isFraud = data[data['isFraud'] == 1].sample(n_samples_isFraud)\n    data_sample_isNotFraud = data[data['isFraud'] == 0].sample(n_samples_isNotFraud)\n\n    return data_sample_isFraud.append(data_sample_isNotFraud)","c1897a73":"# Not generic\ndef test_models(models, cv, X, y):\n    for i in range(len(models)):\n        clf = GridSearchCV(models[i]['model'], models[i]['params'], cv=cv, scoring=make_scorer(average_precision_score), verbose=2, n_jobs=-1)\n        clf.fit(X, y)\n        models[i]['exec_time'] = (sum(clf.cv_results_['mean_fit_time']) * cv)\n        models[i]['best_params'] = clf.best_params_\n        models[i]['best_model'] = clf.best_estimator_ \n        models[i]['best_score'] = clf.best_score_","44232f59":"def split_data(df):\n    #X = df.drop(['index', 'type', 'amount', 'nameOrig', 'nameDest', 'isFlaggedFraud', 'ID', 'newBalanceOrig', 'newBalanceDest', 'isFraud'], axis = 1)\n    X = df.drop(['index', 'type', 'amount', 'nameOrig', 'nameDest', 'isFlaggedFraud', 'ID', 'isFraud'], axis = 1)\n    y = df[['isFraud']]\n    return X, y","0d87ecb5":"# If you have processing power, just uncomment the following line:\n#data_stratified = data\n# and comment this line:\ndata_stratified = get_stratified_samples(20000, data)\nX, y = split_data(data_stratified)","ac030df1":"# ALL\n\ncv = 5\n# ---------------------------------------------------------------------\nknn_params = {\n    'n_neighbors' : [3, 15, 50, 1000],\n    'weights' : ['uniform', 'distance'],\n    'p' : [1, 2]\n}\n# ---------------------------------------------------------------------\nsvc_params = {\n    'C' : [0.01, 0.1, 1, 10],\n    'gamma' : ['auto', 'scale'],\n    'class_weight' : [None, 'balanced'],\n}\n# ---------------------------------------------------------------------\nrdg_params = {\n    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 10],\n    'class_weight' : [None, 'balanced'],\n}\n# ---------------------------------------------------------------------\nlas_params = {\n    'alpha' : [0.001, 0.01, 0.1, 1, 10],\n    'normalize' : [False, True],\n    'tol' : [1],\n    'max_iter' : [100000]\n}\n# ---------------------------------------------------------------------\nweights = sum(y['isFraud'] == 0) \/ (100.0 * sum(y['isFraud'] == 1))\nxgbc_params = {\n    'max_depth' : [1, 2, 3, 4, 5],\n    'scale_pos_weight' : [weights, 1]\n}\n# ---------------------------------------------------------------------\nrl_params = {\n    'solver' : ['liblinear'], #'lbfgs', 'sag' # because I need to implement something to solve this: \"Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty\"\n    'penalty' : ['l1', 'l2'],\n    'C' : [0.01, 0.1, 1]\n}\n# ---------------------------------------------------------------------\ndt_params = {\n    'max_depth' : [1, 3, 5],\n    'criterion' : ['gini', 'entropy'],\n    'splitter' : ['best', 'random']\n}\n# ---------------------------------------------------------------------\nrf_params = {\n    'max_depth' : [1, 3, 5, 7, 11, 21],\n    'n_estimators' : [3, 10, 20, 50],\n    'max_features' : [1, 3, 5]\n}\n# ---------------------------------------------------------------------\nmlpc_params = {\n    'alpha' : [0.1, 1],\n    'max_iter' : [1000, 10000]\n}\n# ---------------------------------------------------------------------\nab_params = {\n    'n_estimators' : [50, 100, 200, 500, 100]\n}\n# ---------------------------------------------------------------------\n\nModels = [\n    {'name': \"Dummy\", 'model' : DummyClassifier(strategy='most_frequent', random_state=0), 'params' : {}, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"KNN\", 'model' : KNeighborsClassifier(), 'params' : knn_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"SVC\", 'model' : SVC(), 'params' : svc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RidgeClassifier\", 'model' : RidgeClassifier(), 'params' : rdg_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n#    {'name': \"Lasso\", 'model' : Lasso(), 'params' : las_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"XGBClassifier\", 'model' : XGBClassifier(), 'params' : xgbc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"LogisticRegression\", 'model' : LogisticRegression(), 'params' : rl_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"DecisionTreeClassifier\", 'model' : DecisionTreeClassifier(), 'params' : dt_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"MLPClassifier\", 'model' : MLPClassifier(), 'params' : mlpc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"AdaBoostClassifier\", 'model' : AdaBoostClassifier(), 'params' : ab_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\n#test_models(Models, cv, X, y)\n#Models","660010a3":"import pickle\nfilename = 'trained_models'\noutfile = open(filename,'wb')\npickle.dump(Models,outfile)\noutfile.close()","5a6dd367":"filename = 'trained_models'\ninfile = open(filename,'rb')\nnew_dict = pickle.load(infile)\ninfile.close()","f25402d2":"# Segundo teste, com uma base maior e com os par\u00e2metros \u00f3timos\n# ALL - OPTIMAL\n\nX, y = split_data(data)\n\ncv = 5\n# ---------------------------------------------------------------------\nknn_params = {'n_neighbors' : [15], 'weights' : ['distance'], 'p' : [2]}\n# ---------------------------------------------------------------------\nsvc_params = {'C' : [0.01], 'gamma' : ['scale'], 'class_weight' : [None]}\n# ---------------------------------------------------------------------\nrdg_params = {'alpha' : [0.0001],'class_weight' : [None]}\n# ---------------------------------------------------------------------\nweights = sum(y['isFraud'] == 0) \/ (100.0 * sum(y['isFraud'] == 1))\nxgbc_params = {'max_depth' : [3],'scale_pos_weight' : [weights, 1]}\n# ---------------------------------------------------------------------\nrl_params = {'solver' : ['liblinear'], 'penalty' : ['l1'], 'C' : [1]}\n# ---------------------------------------------------------------------\ndt_params = {'max_depth' : [5], 'criterion' : ['entropy'], 'splitter' : ['best']}\n# ---------------------------------------------------------------------\nrf_params = {'max_depth' : [11], 'n_estimators' : [20],'max_features' : [1]}\n# ---------------------------------------------------------------------\nmlpc_params = {'alpha' : [0.1], 'max_iter' : [1000]}\n# ---------------------------------------------------------------------\nab_params = {'n_estimators' : [100]}\n# ---------------------------------------------------------------------\n\nModels = [\n    {'name': \"Dummy\", 'model' : DummyClassifier(strategy='most_frequent', random_state=0), 'params' : {}, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"KNN\", 'model' : KNeighborsClassifier(), 'params' : knn_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"SVC\", 'model' : SVC(), 'params' : svc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RidgeClassifier\", 'model' : RidgeClassifier(), 'params' : rdg_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"XGBClassifier\", 'model' : XGBClassifier(), 'params' : xgbc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"LogisticRegression\", 'model' : LogisticRegression(), 'params' : rl_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"DecisionTreeClassifier\", 'model' : DecisionTreeClassifier(), 'params' : dt_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"RandomForestClassifier\", 'model' : RandomForestClassifier(), 'params' : rf_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"MLPClassifier\", 'model' : MLPClassifier(), 'params' : mlpc_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n    {'name': \"AdaBoostClassifier\", 'model' : AdaBoostClassifier(), 'params' : ab_params, 'best_model' : None,'best_score' : 0, 'best_params' : None, 'exec_time' : 0.0},\n]\n\ntest_models(Models, cv, X, y)\nModels","9be8ca40":"data.shape","93f20cc8":"data_ = get_stratified_samples(2000, data)\nX_, y_ = split_data(data_)\n\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n# Code source inspiration: Ga\u00ebl Varoquaux, Andreas M\u00fcller and Jaques Grobler\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [x['name'] for x in Models]\n\nclassifiers = [x['best_model'] for x in Models]\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X_)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, y_], axis = 1)\ndatasets = [(list(zip(finalDf['principal component 1'], finalDf['principal component 2'])), list(finalDf['isFraud']))]\n\nfigure = plt.figure(figsize=(27, 9)) # 36\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        #score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            probabilities = clf.decision_function(X_test)\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n            probabilities = clf.predict_proba(X_test)[:, 1]\n            \n        score = average_precision_score(y_test, probabilities)\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()","e5429bc2":"assert 1 == 2, \"Stop execution to analise something\"","9ba92b53":"# Algoritmos de Grafos:\n# BFS; DFS; (n\u00e3o necess\u00e1rios, mas interessantes: Floyd-Marshall; Djikistra; Tarjan)","2a4ea0fb":"def show_dependencies(_id):\n    #print(\"Checking {}\".format(_id))\n    vertex = data[data['ID'] == _id].reset_index()\n    this_node = str(vertex.loc[0, 'nameOrig'])\n    next_node = str(vertex.loc[0, 'nameDest'])\n    amnt = vertex.loc[0, 'amount']\n    is_fraud = vertex.loc[0, 'isFraud']\n    data[next_node == data['nameOrig']]\n    print(\"{} -> [({} - {})] -> \".format(this_node, amnt, is_fraud), end = ' ') #_id\n    if(data[next_node == data['nameOrig']].shape[0] != 0):\n        _next_id = data[next_node == data['nameOrig']].reset_index().loc[0, 'ID']\n        show_dependencies(_next_id)\n    else:\n        print(\" {} -|\".format(next_node))","9121d230":"def show_all_dependencies(_id):\n    print(\"Checking {}\".format(_id))\n    vertex = data[data['ID'] == _id].reset_index()\n    this_node = str(vertex.loc[0, 'nameOrig'])\n    next_node = str(vertex.loc[0, 'nameDest'])\n    amnt = vertex.loc[0, 'amount']\n    is_fraud = vertex.loc[0, 'isFraud']\n    data[next_node == data['nameOrig']]\n    print(\"{} -> [({} - {})] -> \".format(this_node, amnt, is_fraud), end = ' ') #_id\n    if(data[next_node == data['nameOrig']].shape[0] != 0):\n        for idx, row in data[next_node == data['nameOrig']].iterrows():\n            _next_id = row['ID']\n            show_dependencies(_next_id)\n    else:\n        print(\" {} -|\".format(next_node))","661ac1f3":"def gen_dfs_tree(G):\n    l = list(G.edges())\n    list_added_nodes = {}\n    dfs_trees = {}\n    for i in range(0, len(l)):\n        possible_orig = l[i][0]\n        if(possible_orig in list_added_nodes):\n            continue\n        else:\n            list_added_nodes[possible_orig] = possible_orig\n        T = nx.dfs_tree(G, l[i][0])\n        list_of_edges = list(T.edges())\n        list_of_nodes = list(T.nodes())\n        dfs_trees[i] = list_of_edges\n        for node in list_of_nodes:\n            list_added_nodes[node] = node\n    return dfs_trees","1f6b56db":"# https:\/\/codereview.stackexchange.com\/questions\/180005\/sort-dictionary-by-increasing-length-of-its-values\ndef sort_dict_by_value_len(dict_, reverse=False):\n    return sorted(dict_.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=reverse)","884f4fb5":"show_dependencies(870142)","03b8f483":"# Outras an\u00e1lises\nshow_dependencies(960911)","5a72fbe0":"show_dependencies(1030444)","63c0e9f3":"transactions_to_add = [1030443, 6149940]\nedge_labels = {}\nG = nx.DiGraph()\nfor idx in transactions_to_add:\n    row = data.loc[idx]\n    G.add_weighted_edges_from([(row['nameOrig'][0:3], row['nameDest'][0:3], row['amount'])])\n    edge_labels[(row['nameOrig'][0:3], row['nameDest'][0:3])] = str(row['amount'])","1f87449f":"# Test, please delete before presenting:\n\ntransactions_to_add = [1030443, 6149940, 1]\nedge_labels = {}\nG = nx.DiGraph()\n\nfor idx in transactions_to_add:\n    row = data.loc[idx]\n    G.add_weighted_edges_from([(row['nameOrig'][0:3], row['nameDest'][0:3], row['amount'])])\n\nT = nx.dfs_tree(G)\nlist(T.edges())","8a4d564a":"# Example:\n#start = time.time()\nG = nx.DiGraph()\nG.add_edge(\"C11\", \"C12\")\nG.add_edge(\"C12\", \"C13\")\nG.add_edge(\"C12\", \"C14\")\nG.add_edge(\"C15\", \"C14\")\nG.add_edge(\"C14\", \"C19\")\n\nl = list(G.edges())\n\nlist_printed_nodes = {}\n\ndfs_trees = {}\n\nfor i in range(0, len(list(G.edges()))):\n    #T = nx.dfs_tree(G, data.loc[i, 'nameOrig'])\n    possible_orig = l[i][0]\n    if(possible_orig in list_printed_nodes):\n        continue\n    else:\n        list_printed_nodes[possible_orig] = possible_orig\n    T = nx.dfs_tree(G, l[i][0])\n    l1 = list(T.edges())\n    l2 = list(T.nodes())\n    print(\">\", l[i][0])\n    print(\">>\", l1)\n    print(\">>>\", l2)\n    dfs_trees[i] = l1\n    for n in l2:\n        list_printed_nodes[n] = n\n#    if(len(l) > 1):\n#        print(\"idx\", i)\n#        print(l)\n#        break\n\n#end = time.time()\n#print(end - start)\n#print(len(list(G.edges())))\ndfs_trees","9ae5e3ac":"import time\nstart = time.time()\n\n# build the graph for all 6MM lines will take approximately 912 secs (~16 min)\nG = nx.DiGraph()\nfor idx, row in data.iterrows():\n    #print(\"Adding.. \", idx)\n    if(idx == 100):\n        break\n    G.add_edge(row['nameOrig'], row['nameDest'])\n    G[row['nameOrig']][row['nameDest']]['amount'] = row['amount']\n\n#G\nend = time.time()\nprint(end - start)","30e397c4":"start = time.time()\nforest = gen_dfs_tree(G)\nend = time.time()\nprint(end - start)\n#G.edges()\nsort_dict_by_value_len(forest, True)[:,10]","046c9a4a":"#G = nx.Graph()\nG = nx.DiGraph()\nid = 1030443\nG.add_weighted_edges_from([(data.loc[id, 'nameOrig'][0:3], data.loc[id, 'nameDest'][0:3], data.loc[id, 'amount'])])\nid = 6149940\nG.add_weighted_edges_from([(data.loc[id, 'nameOrig'][0:3], data.loc[id, 'nameDest'][0:3], data.loc[id, 'amount'])])\n\noptions = {\n    'node_color': 'blue',\n    'node_size': 200,\n    'width': 20,\n}\n#nx.draw(G, with_labels=True, font_weight='bold', **options)\n\npos = nx.spring_layout(G)\n\nnx.draw(G, with_labels=True)\n#nx.draw_networkx_edge_labels(G,pos,edge_labels={('C11','C17'):'PESO1', ('C17','C13'):'PESO2'},font_color='red')\nnx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels,font_color='red')\nplt.show()","bfceaf6f":"Analisando a descri\u00e7\u00e3o do problema podemos observar que a \u00fanica regra de neg\u00f3cio mencionada pelo autor \u00e9 que o crit\u00e9rio para uma transa\u00e7\u00e3o ser marcada como *\"isFlaggedFraud\"* \u00e9 que a transa\u00e7\u00e3o supere o montante de 200 mil.\n\n- \"O modelo de neg\u00f3cio tenta controlar transfer\u00eancias massivas de uma conta para outra 'flagando' essas transfer\u00eancias como *tentativas ilegais*. Uma tentativa ilegal nesse dataset \u00e9 uma tentativa de tranferir uma quantia maior que 200k numa \u00fanica transa\u00e7\u00e3o.","7e0cf3a1":"- Atrav\u00e9s da an\u00e1lise da tabela acima podemos observar que, para transa\u00e7\u00f5es do tipo:\n    - 'CASH_IN': Na sua maioria (97%) as diferen\u00e7as de balan\u00e7o se encontram nas contas de origem e destino;\n    - 'CASH_OUT': Ocorre o quase que o oposto a CASH_IN, ou seja, a maioria (85%) das diferen\u00e7as se encontram somente nas contas de origem;\n    - 'DEBIT': Apesar de ter muitas diferen\u00e7as (~13k), a maioria das transa\u00e7\u00f5es (~70%) n\u00e3o apresentaram diferen\u00e7as\n    - 'PAYMENT': Para esse tipo de transa\u00e7\u00f5es podemos observar que quase todas (~99%) das amostras possuem alguma diferen\u00e7a, com destaque para as diferen\u00e7as nos balan\u00e7os das contas de destino.\n    - 'TRANSFER': A maioria das amostras possuem alguma diferen\u00e7a (~97%), sendo que a maioria das diferen\u00e7as (92%) se encontram nas contas de origem.\n    \nAtrav\u00e9s dessa an\u00e1lise podemos observar o comportamento dos tipos de transa\u00e7\u00f5es em 3 grupos:\n1. Transa\u00e7\u00f5es com baixa (ou menor) incid\u00eancia de diferen\u00e7as nos balan\u00e7os: Representadas pelo conjunto de transa\u00e7\u00f5es do tipo 'DEBIT';\n2. Transa\u00e7\u00f5es com concentra\u00e7\u00e3o da diferen\u00e7a nas contas de origem: Representadas pelo conjunto de transa\u00e7\u00f5es dos tipos 'CASH_OUT' e 'TRANSFER'\n3. Transa\u00e7\u00f5es com grande quantidade de diferen\u00e7as nas contas de origem, por\u00e9m com quantidade maior ainda de diferen\u00e7as nas contas de destino: Representadas pelo conjunto de transa\u00e7\u00f5es dos tipos 'CASH_IN' e 'PAYMENT'","9f952286":"Comparando as medidas de dispers\u00e3o dos conjuntos isFraud *vs* isNotFraud temos podemos observar que:\n\n- Apesar de termos uma diferen\u00e7a exponencial entre a quantidade de amostras de cada conjunto, a m\u00e9dia dos montantes das transa\u00e7\u00f5es do conjunto de Fraude \u00e9 bem superior a do conjunto de N\u00e3o Fraudes (em pelo menos 1 ordem de grandeza). Isso poderia sinalizar um ind\u00edcio de fraude (transa\u00e7\u00f5es com valores muito altos).\n- A afirma\u00e7\u00e3o acima \u00e9 sustentada quando analisamos os quartis de cada um dos grupos com rela\u00e7\u00e3o ao montante.\n- Analisando o balan\u00e7o das contas de origem dos dois conjuntos tamb\u00e9m podemos notar que as contas que s\u00e3o envolvidas em fraudes, possuem na m\u00e9dia um saldo anterior a transa\u00e7\u00e3o superior a das contas n\u00e3o envolvidas em fraudes.\n- Em compensa\u00e7\u00e3o o valor p\u00f3s-transa\u00e7\u00e3o das contas de origem do grupo onde foi identificado fraude \u00e9 na m\u00e9dia menor do que a do grupo onde n\u00e3o foi identificado, isso poderia justificar o fato das contas envolvidas em fraudes serem \"zeradas\" por volume de saques. Isso pode ser confirmado ao analisar os saldos das contas de Destino, que no grupo que foi detectado fraude, possuem na m\u00e9dia valor inferior com rela\u00e7\u00e3o as contas onde n\u00e3o foi detectado fraude.","096da037":"### 3.5 - Codificando a vari\u00e1vel Type (OneHot)\n\n- Dados categoricos como o caso de type, precisam ter um tratamento difernciado. [REF](). O tratamento que escolhemos para essa vari\u00e1vel \u00e9 o OneHot, que transforma todos os valores de numa combina\u00e7\u00e3o linear bin\u00e1ria e can\u00f4nica para o dom\u00ednio daquela vari\u00e1vel. Nesse caso, como *type* s\u00f3 pode assumir 2 valores ('CASH_OUT', 'TRANSFER'), teremos adicionalmente 2 colunas na base de dados.","fcd92f18":"- An\u00e1lise dos casos em que os valores do balan\u00e7o da **conta de origem** n\u00e3o batem com o **montante** transacionado:","331c2de4":"### 3.6 - Normaliza\u00e7\u00e3o\n\n- Dado que temos vari\u00e1veis num\u00e9ricas com um gaps (diferen\u00e7a entre o maior valor e o menor valor) muito diferentes, iremos normaliza-los para obter melhor performance nos modelos.\n\n*Observa\u00e7\u00e3o:* : Ap\u00f3s alguns testes, decidimos utilizar a normaliza\u00e7\u00e3o \"Standard\" (m\u00e9dia = 0 e desvio padr\u00e3o = 1), ao inv\u00e9s da normaliza\u00e7\u00e3o \"Min-Max\" (com min = 0 e max = 1).\n\n*Observa\u00e7\u00e3o:* : Ap\u00f3s alguns testes, decidimos n\u00e3o utilizar nenhuma normaliza\u00e7\u00e3o, nem a \"Standard\" (m\u00e9dia = 0 e desvio padr\u00e3o = 1), nem a \"Min-Max\" (com min = 0 e max = 1).","b954b16a":"An\u00e1lise da movimenta\u00e7\u00e3o da conta C1496808239.\n\nA conta C1496808239 se envolveu numa transa\u00e7\u00e3o fraudulenta.\nAp\u00f3s isso a conta C1496808239 fez a seguinte movimenta\u00e7\u00e3o de recursos:\n\nC1496808239 -> **(CASH_OUT\t1566057.28)** -> C737489499 -> (CASH_IN\t61945.81) -> C1213907472.\n\nSendo que:\n- A conta C737489499 s\u00f3 foi envolvida como origem desta \u00fanica transa\u00e7\u00e3o;\n- A transa\u00e7\u00e3o de Cash Out destacada em negrito foi a \u00fanica desse \"caminho\" que foi fraudulenta.","40b25d4f":"### 3.1 - Ajuste dos Valores do Balan\u00e7o\n#### ! Decis\u00e3o de Projeto\n\nNesse ponto do projeto decidimos que iremos ignorar as colunas que est\u00e3o inconsistentes e iremos corrigi-las segundo a l\u00f3gica de uma transa\u00e7\u00e3o. Ou seja, todas as transa\u00e7\u00f5es ir\u00e3o seguir as seguintes f\u00f3rmulas:\n\n- **Montante Realmente Transacionado** = **Montante** (se o Montate for menor que o Antigo Balan\u00e7o da Origem) **ou**\n**Antigo Balan\u00e7o da Origem** (em caso contr\u00e1rio)\n                  \n(realAmountTransacted = oldBalanceOrig if oldBalanceOrig < amount else amount)\n\n- **Novo Balan\u00e7o Corrigido da Origem** = **Antigo Balan\u00e7o da Origem** - **Montante Realmente Transacionado**\n\n(newBalanceCorrectedOrig = oldBalanceOrig - realAmountTransacted)\n\n- **Novo Balan\u00e7o Corrigido do Destino** = **Antigo Balan\u00e7o do Destino** + **Montante Realmente Transacionado**\n\n(newBalanceCorrectedDest = oldBalanceDest + realAmountTransacted)","001f6d87":"Como podemos ver, todas as transa\u00e7\u00f5es da conta **\"C2098525306\"** foram representadas (Tanto aquelas em que a conta \u00e9 origem, quanto aquelas em que a conta \u00e9 destino). Nesse caso podemos ver claramente que existe uma inconsist\u00eancia nas transa\u00e7\u00f5es (transa\u00e7\u00f5es que tem o montante maior do que o saldo em conta do cliente) e uma inconsist\u00eancia com rela\u00e7\u00e3o ao hist\u00f3rico da movimenta\u00e7\u00e3o da conta. As 3 transa\u00e7\u00f5es (Ids: 48163, 3970493, 5039139) ocorreram em diferentes momentos do tempo (steps: 9, 297 e 354 respectivamente). Contudo, podemos notar que existe uma diferen\u00e7a entre o **novo balan\u00e7o da conta** ap\u00f3s a transa\u00e7\u00e3o **48163** e o **antigo balan\u00e7o da conta** antes da transa\u00e7\u00e3o **3970493**, e o mesmo ocorre entre as transa\u00e7\u00f5es **3970493** e **5039139**.\n\nRespondendo assim a quest\u00e3o: [3] *O balan\u00e7o de um determinado cliente representa fielmente o hist\u00f3rico de transa\u00e7\u00f5es desse cliente?*","2f682189":"- An\u00e1lise dos casos em que os valores do balan\u00e7o da **conta de destino** n\u00e3o batem com o **montante** transacionado:","4bcd0fe2":"Como pudemos observar n\u00e3o s\u00e3o todas as transa\u00e7\u00f5es que parecem possuem montante batendo com o valor transacionado ou os balan\u00e7os de origem e destino. (Respondendo a [2.3] (*Isso vale, granularmente, para as contas de origem ? E as contas de destino?*))","9f37ba6f":"Na pr\u00f3xima an\u00e1lise iremos considerar que uma transa\u00e7\u00e3o \u00e9 dita **N\u00e3o efetuada\/N\u00e3o concretizada** quando essa transa\u00e7\u00e3o atender 2 crit\u00e9rios:\n1. Tiver o valor de montante trnasacionado superior a 0.\n2. N\u00e3o alterar os valores de balan\u00e7os da conta de Origem **ou** da conta de Destino.","e54fedf4":"- Observando as duas \u00faltimas tabelas podemos ver a grande diferen\u00e7a entre os clientes que mais fizeram transa\u00e7\u00f5es (em termos de quantidade) nos \"pap\u00e9is\" de origem e destino e a distribui\u00e7\u00e3o dessas quantidade de transa\u00e7\u00f5es por \"papel\".","31e3da7d":"- Como vimos acima, por conta de casos como o da conta **C1789550256** n\u00e3o poderemos fazer a \"corre\u00e7\u00e3o\" do hist\u00f3rico.","a0882309":"### 2.1 An\u00e1lise inicial da distribui\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas","7bf305d2":"## 1.1 Os Dados:\n\nOs dados que temos dispon\u00edveis s\u00e3o:\n- 'step'\n- 'type'\n- 'amount'\n- 'nameOrig'\n- 'oldbalanceOrg'\n- 'newbalanceOrig'\n- 'nameDest'\n- 'oldbalanceDest'\n- 'newbalanceDest'\n- 'isFraud'\n- 'isFlaggedFraud'\n","58d73108":"- E respondendo \u00e0: [5] *Temos clientes em comum em ambas as pontas? (origem e destino)?*","cf4313da":"- Essas an\u00e1lises j\u00e1 ajudam a entender a resposta da quest\u00e3o [6] *(Quais pessoas est\u00e3o transacionando para quais pessoas?)*","d30edc29":"- Constantes que ser\u00e3o ao longo das pr\u00f3ximas se\u00e7\u00f5es","e7c037a9":"- Algumas fun\u00e7\u00f5es importantes:","c39550d7":"### 2.5 An\u00e1lise do hist\u00f3rico\n\nUma das nossas hip\u00f3teses \u00e9 que as transa\u00e7\u00f5es no dataset, representam na sua totalidade o hist\u00f3rico de transa\u00e7\u00f5es das contas, de modo que nenhuma transa\u00e7\u00e3o que envolva essas contas n\u00e3o foi representada nesse dataset, ou seja, sempre que o valor de uma conta variar, a pr\u00f3xima transa\u00e7\u00e3o ir\u00e1 considerar essa varia\u00e7\u00e3o.\n\nPara fazer uma an\u00e1lise mais detalhada vamos selecionar um caso em particular. Vamos verificar dentro das transa\u00e7\u00f5es que n\u00e3o n\u00e3o tiveram altera\u00e7\u00e3o de balan\u00e7o, quantas vezes cada uma delas fez uma transa\u00e7\u00e3o e selecionar os casos com maior n\u00famero de transa\u00e7\u00f5es para entender o efeito que as transa\u00e7\u00f5es tem no hist\u00f3rico do balan\u00e7o de cada um dos casos.","83fd2293":"***Disclaimer:***\n\nAs an\u00e1lises abaixo consideram que a coluna de 'amount' representa o valor envolvido na transa\u00e7\u00e3o, de tal forma que poder\u00edamos assumir que a diferen\u00e7a entre o **Antigo Valor do Balan\u00e7o na Origem** e o **Novo Valor do Balan\u00e7o na Origem** deveria ser igual ao **montante** envolvido na transa\u00e7\u00e3o e que necessariamente todo esse valor pode ser visto, tamb\u00e9m na diferen\u00e7a entre o **Antigo Valor do Balan\u00e7o no Destino** e o **Novo Valor do Balan\u00e7o no Destino**. Algo importante que tamb\u00e9m estamos assumindo \u00e9 que a apura\u00e7\u00e3o dos balan\u00e7os das contas envolvidas na transa\u00e7\u00e3o ocorreu de tal forma que nenhuma opera\u00e7\u00e3o que pudesse afetar os seus valores foi realizada no intervalo de tempo de uma transa\u00e7\u00e3o. Isso significa dizer que toda apura\u00e7\u00e3o\/transa\u00e7\u00e3o de valores ocorreu necessariamente na seguinte ordem:\n1. Apura\u00e7\u00e3o do *Antigo Valor do Balan\u00e7o da Conta de Origem*;\n2. Apura\u00e7\u00e3o do *Antigo Valor do Balan\u00e7o da Conta de Destino*;\n3. **Transa\u00e7\u00e3o**\n4. Apura\u00e7\u00e3o do *Novo Valor do Balan\u00e7o da Conta de Origem*;\n5. Apura\u00e7\u00e3o do *Novo Valor do Balan\u00e7o da Conta de Destino*;\n\nE nenhuma outra opera\u00e7\u00e3o que pudesse afetar os valores dos balan\u00e7os das contas ocorreu nesse intervalo de opera\u00e7\u00f5es (1-5).","730e9e51":"An\u00e1lise da movimenta\u00e7\u00e3o da conta C1829721095.\n\nA conta C1829721095 se envolveu numa transa\u00e7\u00e3o fraudulenta.\nAp\u00f3s isso a conta C1829721095 fez a seguinte movimenta\u00e7\u00e3o de recursos:\n\nC1829721095 -> **(CASH_OUT\t534255.94)** -> C991247178 -> (CASH_OUT\t60387.69) -> C1344561835.\n\nSendo que:\n- A conta C1829721095 s\u00f3 foi envolvida como origem desta \u00fanica transa\u00e7\u00e3o;\n- A transa\u00e7\u00e3o de Cash Out destacada em negrito foi a \u00fanica desse \"caminho\" que foi fraudulenta.","040b5546":"- Obs.: Note que todas as transa\u00e7\u00f5es que n\u00e3o envolveram montante foram classificadas como Fraudes!","089f4325":"### 2.6 - Outras an\u00e1lises\n\nOutros fatos que pudemos observar segundo a base de dados \u00e9 que:","9315cb9a":"### 3.4 - Diminuindo o impacto do Desbalanceamento das amostras\n- Como mencionado anteriormente, nesse dataset em particular temos um problema de **desbalanceamento** porque somente uma parcela \u00ednfima da nossa base de dados apresenta o fen\u00f4meno que queremos predizer (fraude). Tendo isso em vista e os problemas para os algoritmos de Aprendizado de M\u00e1quina que podemos ter, decidimos cortar nossa base para reduz\u00ed-la sem descaracterizar o problema. Para tanto, iremos nos manter somente os dados cujo tipo de transa\u00e7\u00e3o apresenta ambos os casos (com e sem fraude).","f2f89d11":"Algumas dessas afirma\u00e7\u00f5es ser\u00e3o questionadas mais a frente por outras an\u00e1lises.\n\nPor\u00e9m, respondendo a 1\u00aa pergunta, podemos ver de antem\u00e3o que nenhuma transa\u00e7\u00e3o possui montante negativo, o que nos indica que necessariamente toda transa\u00e7\u00e3o ir\u00e1 diminuir o saldo da conta de origem e aumentar a conta de destino (partindo do pressuposto que todo valor transacionado sempre sai da conta de origem e vai para a conta de destino), podemos ver tamb\u00e9m a distribui\u00e7\u00e3o de cada uma das vari\u00e1veis n\u00famericas e entender um pouco melhor esses dados.","5cc9c53d":"Segundo o autor em [(1)](https:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/33129#latest-187012) duas situa\u00e7\u00f5es fazem com que o valor dos balan\u00e7os das contas n\u00e3o batam com o valor transacionado:\n1. A transa\u00e7\u00e3o foi marcada como isFlaggedFraud, e por conta disso n\u00e3o foi concretizada.\n2. N\u00e3o existe informa\u00e7\u00e3o sobre balan\u00e7o de contas de \"Mercadores\" (segundo o autor, essas contas poderiam ser identificadas pela letra do nome do Destinat\u00e1rio \"M\"). \n\nEsse fato \u00e9 verificado como abaixo:","24c7249c":"### 2.3 An\u00e1lise dos clientes das transa\u00e7\u00f5es\n- Nossas pr\u00f3ximas an\u00e1lises ir\u00e3o focar nos clientes das transa\u00e7\u00f5es","861ea145":"## Perguntas:\n\n- [1] O que efetivamente significam as transa\u00e7\u00f5es ?\n- [2] Os dados s\u00e3o consistentes? Representam os dados de uma transa\u00e7\u00e3o comum (os balan\u00e7os batem com o montante transacionado?)\n    - [2.1] Os valores transacionados podem ser negativos ?\n    - [2.2] Todas as transa\u00e7\u00f5es feitas afetam os saldos das contas (no total)?\n    - [2.3] Isso vale, granularmente, para as contas de origem ? E as contas de destino?\n- [3] O balan\u00e7o de um determinado cliente representa fielmente o hist\u00f3rico de transa\u00e7\u00f5es desse cliente?\n- [4] Existem regras de neg\u00f3cio a serem aplicadas? Quais s\u00e3o e como elas s\u00e3o aplicadas ?\n- [5] Temos clientes em comum em ambas as pontas? (origem e destino)?\n- [6] Quais pessoas est\u00e3o transacionando para quais pessoas?\n- [7] Quais aspectos s\u00e3o mais peculiares de transa\u00e7\u00f5es fraudulentas ?\n\n- Sobre as fraudes:\n\n- [8] Quando ocorre uma fraude: A transa\u00e7\u00e3o \u00e9 cancelada (isso para entedermos os valores de new e old Balance tanto na Origem quanto no Destino)\n- [9] Quem s\u00e3o as pessoas (na origem e no destino) envolvidas numa fraude?\n    - [9.1 ] O que d\u00e1 pra saber sobre essas pessoas?\n    - [9.2] Quatidade de vezes que cada uma delas se envolveu numa fraude;\n    - [9.3] Volume operado;\n- [10] Podemos mapear todos os envolvidos em transa\u00e7\u00f5es fraudulentas de modo a criar uma \"Rede do crime\" (ou seja, um Grafo (Rede) de associa\u00e7\u00e3o) ?\n- [11] Conseguimos ver a transferencia de valor ao longo do tempo? Ex.: Um cliente frauda uma transa\u00e7\u00e3o e desvia as verbas de uma conta A para uma determinada conta B. Depois (e esse depois precisa ser necessariamente um \"depois de um tempo\") ele transfere o valor dessa conta B para uma nova conta C, e assim por diante. Conseguimos observar o \"Tracking do dinheiro\"?\n                ","b6d810f9":"Iremos desconsiderar algumas colunas que entendemos n\u00e3o serem para o uso dos modelos, seja por serem vari\u00e1veis categ\u00f3ricas ('nameOrig', 'nameDest', 'type'), seja por serem vari\u00e1veis auxiliares ('index', 'ID'), ou seja por terem algum problema\/inconsist\u00eancia nessa coluna ('amount', 'isFlaggedFraud').\n\nObs.: N\u00e3o iremos desconsiderar as colunas de newBalanceOrig e newBalanceDest, pois, apesarem de apresentarem erros\/inconsist\u00eancias pudemos observar dois pontos:\n\n- O modelo se comporta melhor quando treina com essas duas vari\u00e1veis; e\n- Isso pode ser expicado pelo fato de que mesmos as incosist\u00eancias nessas vari\u00e1veis possue um comportamento impl\u00edcito que est\u00e1 correlacionado a presen\u00e7a de Fraude.","c9513647":"#### An\u00e1lises Adicionais:","eb0d2742":" ### 2.4 An\u00e1lise do Montante *vs* Balan\u00e7os\n \n\n - Observando inicialmente a base de dados, um ponto que nos chama aten\u00e7\u00e3o \u00e9 o fato de algumas transa\u00e7\u00f5es n\u00e3o terem afetado o saldo das contas seja da origem, seja do destino.","989feb7e":"- Algumas fun\u00e7\u00f5es auxiliares:","3354de3d":"Observando o gr\u00e1fico vemos que o n\u00famero (percentual) de transa\u00e7\u00f5es com aus\u00eancia de fraude diminui consideravelmente no intervalo de x a y.","77308048":"Importantes pondera\u00e7\u00f5es:\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/81427#latest-475939\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/80445#latest-470855\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/60034#latest-350311\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/39505#latest-221432\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/33129#latest-187012\n\nhttps:\/\/www.kaggle.com\/ntnu-testimon\/paysim1\/discussion\/32786#latest-183354","cf81b603":"# Synthetic Financial Datasets For Fraud Detection - Notebook\n\n_In this notebook you will find all the code and report of the project \"Synthetic Financial Datasets For Fraud Detection\"_\n\n### Conte\u00fado:\n## 1. O Desafio\n## 2. AED - An\u00e1lise Explorat\u00f3ria de Dados e Primeiras Impress\u00f5es\n## 3. Manipula\u00e7\u00e3o de Dados\n## 4. Aprendizado de M\u00e1quina\n## 5. Conclus\u00f5es\n## 6. Pr\u00f3ximos Passos: Grafos e PLD","24529976":"### 3.2 - Ajuste dos valores do balan\u00e7o -> seguindo o hist\u00f3rico de transa\u00e7\u00f5es\n\n#### ! Decis\u00e3o de Projeto\n- Outra altera\u00e7\u00e3o que pretendemos fazer \u00e9 \"corrigir o hist\u00f3rico\" das transa\u00e7\u00f5es de modo que o balan\u00e7o de uma mesma conta \"reflita\" o hist\u00f3rico de movimenta\u00e7\u00e3o dessa conta. Por\u00e9m, antes de fazer essa altera\u00e7\u00e3o, precisamos verificar se nenhuma situa\u00e7\u00e3o de \"concorr\u00eancia\" ocorre.\n    *Situa\u00e7\u00e3o de Concorr\u00eancia*: Uma vez que a menor granularidade de tempo da nossa base de dados \u00e9 o *step* precisamos verificar se dentro de um mesmo step 2 ou mais transa\u00e7\u00f5es envolvendo a mesma conta (seja na origem, seja no destino) \u00e9 efetuada. Caso verifiquemos que esse fen\u00f4meno acontence, n\u00e3o poderemos efetuar a \"corre\u00e7\u00e3o do hist\u00f3rico\" uma vez que \u00e9 imposs\u00edvel determinar qual das transa\u00e7\u00f5es acontenceu primeiro, sendo assim, teremos nesses casos uma situa\u00e7\u00e3o em que 2 ou mais transa\u00e7\u00f5es **concorrem** entre si para alterar o valor dos balan\u00e7os das contas envolvidas.","baedc057":"# 6. Pr\u00f3ximos Passos: Grafos e Preven\u00e7\u00e3o a Lavagem de Dinheiro (PLD) - WIP","8a225a75":"Analisando como s\u00e3o feitas as transa\u00e7\u00f5es que dizem respeito a ambos os grupos temos:\n","473adb21":"A an\u00e1lise desse gr\u00e1fico revela uma caracter\u00edstica interessante sobre a distribui\u00e7\u00e3o das fraudes com valor entre x1 e x2","a0359ffc":"- Links e Refer\u00eancias:\n\n(Draft)\n\n- [Paper do autor da base](http:\/\/bth.diva-portal.org\/smash\/get\/diva2:955852\/FULLTEXT06.pdf)\n\n- [Livro sobre IA](https:\/\/bdpi.usp.br\/item\/002208293) - Interessante para referenciarmos alguns conceitos b\u00e1sicos de IA e ML ao longo do texto\n\n- [Framework de Knowledge Discovery in Databases do Ex-Head da Microsoft em Minera\u00e7\u00e3o de Dados](https:\/\/www.aaai.org\/Papers\/KDD\/1996\/KDD96-014.pdf)\n\n\n\n","6d198f18":"### 2.2 An\u00e1lise da vari\u00e1vel-alvo:\n- Como nossa an\u00e1lise deve se basear na vari\u00e1vel-alvo, isFraud, s\u00e3o feitas as seguintes an\u00e1lises:","048c7e91":"#### Results:\n\n```\n[{\"name\": 'Dummy',\n  \"model\": DummyClassifier(constant=None, random_state=0, strategy='most_frequent'),\n  \"params\": {},\n  \"best_model\": DummyClassifier(constant=None, random_state=0, strategy='most_frequent'),\n  \"best_score\": 0.002950147507375369,\n  \"best_params\": {},\n  \"exec_time\": 0.027907133102416992},\n {\"name\": 'KNN',\n  \"model\": KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                       metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                       weights='uniform'),\n  \"params\": {'n_neighbors': [3, 15, 50, 1000],\n   'weights': ['uniform', 'distance'],\n   'p': [1, 2]},\n  \"best_model\": KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                       metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n                       weights='distance'),\n  \"best_score\": 0.5140196403759582,\n  \"best_params\": {'n_neighbors': 15, 'p': 2, 'weights': 'distance'},\n  \"exec_time\": 2.714417219161987},\n {\"name\": 'SVC',\n  \"model\": SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n      kernel='rbf', max_iter=-1, probability=False, random_state=None,\n      shrinking=True, tol=0.001, verbose=False),\n  \"params\": {'C': [0.01, 0.1, 1, 10],\n   'gamma': ['auto', 'scale'],\n   'class_weight': [None, 'balanced']},\n  \"best_model\": SVC(C=0.01, cache_size=200, class_weight='balanced', coef0=0.0,\n      decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n      max_iter=-1, probability=False, random_state=None, shrinking=True,\n      tol=0.001, verbose=False),\n  \"best_score\": 0.24111878534252007,\n  \"best_params\": {'C': 0.01, 'class_weight': 'balanced', 'gamma': 'scale'},\n  \"exec_time\": 1309.2560505867004},\n {\"name\": 'RidgeClassifier',\n  \"model\": RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n                  max_iter=None, normalize=False, random_state=None,\n                  solver='auto', tol=0.001),\n  \"params\": {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n   'class_weight': [None, 'balanced']},\n  \"best_model\": RidgeClassifier(alpha=0.0001, class_weight=None, copy_X=True,\n                  fit_intercept=True, max_iter=None, normalize=False,\n                  random_state=None, solver='auto', tol=0.001),\n  \"best_score\": 0.14302836353938908,\n  \"best_params\": {'alpha': 0.0001, 'class_weight': None},\n  \"exec_time\": 3.4835813045501713},\n {\"name\": 'XGBClassifier',\n  \"model\": XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                colsample_bynode=1, colsample_bytree=1, gamma=0,\n                learning_rate=0.1, max_delta_step=0, max_depth=3,\n                min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n                nthread=None, objective='binary:logistic', random_state=0,\n                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                silent=None, subsample=1, verbosity=1),\n  \"params\": {'max_depth': [1, 2, 3, 4, 5],\n   'scale_pos_weight': [3.3796610169491523, 1]},\n  \"best_model\": XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                colsample_bynode=1, colsample_bytree=1, gamma=0,\n                learning_rate=0.1, max_delta_step=0, max_depth=3,\n                min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n                nthread=None, objective='binary:logistic', random_state=0,\n                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                silent=None, subsample=1, verbosity=1),\n  \"best_score\": 0.6488315757779229,\n  \"best_params\": {'max_depth': 3, 'scale_pos_weight': 1},\n  \"exec_time\": 71.32026696205139},\n {\"name\": 'LogisticRegression',\n  \"model\": LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n                     multi_class='warn', n_jobs=None, penalty='l2',\n                     random_state=None, solver='warn', tol=0.0001, verbose=0,\n                     warm_start=False),\n  \"params\": {'solver': ['liblinear'],\n   'penalty': ['l1', 'l2'],\n   'C': [0.01, 0.1, 1]},\n  \"best_model\": LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n                     multi_class='warn', n_jobs=None, penalty='l1',\n                     random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                     warm_start=False),\n  \"best_score\": 0.5140003843626525,\n  \"best_params\": {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'},\n  \"exec_time\": 20.42476725578308},\n {\"name\": 'DecisionTreeClassifier',\n  \"model\": DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                         max_features=None, max_leaf_nodes=None,\n                         min_impurity_decrease=0.0, min_impurity_split=None,\n                         min_samples_leaf=1, min_samples_split=2,\n                         min_weight_fraction_leaf=0.0, presort=False,\n                         random_state=None, splitter='best'),\n  \"params\": {'max_depth': [1, 3, 5],\n   'criterion': ['gini', 'entropy'],\n   'splitter': ['best', 'random']},\n  \"best_model\": DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n                         max_features=None, max_leaf_nodes=None,\n                         min_impurity_decrease=0.0, min_impurity_split=None,\n                         min_samples_leaf=1, min_samples_split=2,\n                         min_weight_fraction_leaf=0.0, presort=False,\n                         random_state=None, splitter='best'),\n  \"best_score\": 0.528449630754746,\n  \"best_params\": {'criterion': 'entropy', 'max_depth': 5, 'splitter': 'best'},\n  \"exec_time\": 2.3987016677856445},\n {\"name\": 'RandomForestClassifier',\n  \"model\": RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n                         min_impurity_decrease=0.0, min_impurity_split=None,\n                         min_samples_leaf=1, min_samples_split=2,\n                         min_weight_fraction_leaf=0.0, n_estimators='warn',\n                         n_jobs=None, oob_score=False, random_state=None,\n                         verbose=0, warm_start=False),\n  \"params\": {'max_depth': [1, 3, 5, 7, 11, 21],\n   'n_estimators': [3, 10, 20, 50],\n   'max_features': [1, 3, 5]},\n  \"best_model\": RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                         max_depth=11, max_features=1, max_leaf_nodes=None,\n                         min_impurity_decrease=0.0, min_impurity_split=None,\n                         min_samples_leaf=1, min_samples_split=2,\n                         min_weight_fraction_leaf=0.0, n_estimators=20,\n                         n_jobs=None, oob_score=False, random_state=None,\n                         verbose=0, warm_start=False),\n  \"best_score\": 0.6483024487924732,\n  \"best_params\": {'max_depth': 11, 'max_features': 1, 'n_estimators': 20},\n  \"exec_time\": 151.24310636520386},\n {\"name\": 'MLPClassifier',\n  \"model\": MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n                beta_2=0.999, early_stopping=False, epsilon=1e-08,\n                hidden_layer_sizes=(100,), learning_rate='constant',\n                learning_rate_init=0.001, max_iter=200, momentum=0.9,\n                n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n                random_state=None, shuffle=True, solver='adam', tol=0.0001,\n                validation_fraction=0.1, verbose=False, warm_start=False),\n  \"params\": {'alpha': [0.1, 1], 'max_iter': [1000, 10000]},\n  \"best_model\": MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n                beta_2=0.999, early_stopping=False, epsilon=1e-08,\n                hidden_layer_sizes=(100,), learning_rate='constant',\n                learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n                n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n                random_state=None, shuffle=True, solver='adam', tol=0.0001,\n                validation_fraction=0.1, verbose=False, warm_start=False),\n  \"best_score\": 0.3161322568292917,\n  \"best_params\": {'alpha': 0.1, 'max_iter': 1000},\n  \"exec_time\": 3774.4576816558847},\n {\"name\": 'AdaBoostClassifier',\n  \"model\": AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n                     n_estimators=50, random_state=None),\n  \"params\": {'n_estimators': [50, 100, 200, 500, 100]},\n  \"best_model\": AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n                     n_estimators=100, random_state=None),\n  \"best_score\": 0.5236584821064045,\n  \"best_params\": {'n_estimators': 100},\n  \"exec_time\": 116.34933018684387}]\n```","1316a7e0":"Atrav\u00e9s da an\u00e1lise da tabela acima podemos observar alguns fatos:\n- N\u00e3o temos transa\u00e7\u00f5es cujo montante seja negativo, nem nenhuma conta fica \"descoberta\", ou seja, nenhuma conta fica com valor negativo. Isso implica em nenhuma transa\u00e7\u00e3o ter montante (*amount*) maior do que o saldo da conta de origem (*oldBalanceOrig*)\n- A implica\u00e7\u00e3o anterior contrasta com o fato da transa\u00e7\u00e3o que envolve o maior valor de montante (transa\u00e7\u00e3o com valor superior a 9e+07) poder ser realizada contra a conta com o maior balan\u00e7o antes da transa\u00e7\u00e3o (~6e+07).\n- A compara\u00e7\u00e3o dos valores dos quartis do montante (*amount*) contra o Novo Valor de Destino (*newBalanceDest*) tamb\u00e9m indicam algo estranho com rela\u00e7\u00e3o a base, pois essas medidas sugerem que nem todo o valor transacionado necessariamente iria ao destinatario.","b95bf714":"Essa se\u00e7\u00e3o cont\u00e9m algumas ideias para futuras an\u00e1lises.\nPudemos observar atrav\u00e9s das an\u00e1lises anteriores que o dataset em quest\u00e3o possui dados de transa\u00e7\u00f5es econ\u00f4micas com certas particularidades (como os valores dos balan\u00e7os, a presen\u00e7a de fraude e os clientes envolvidos nessas transa\u00e7\u00f5es). Como observamos, cada transa\u00e7\u00e3o envolve necessariamente duas partes: uma *conta origem* e uma *conta destino* e cada transa\u00e7\u00e3o possui atributos pr\u00f3prios, como o valor transacionado, a presen\u00e7a de fraude, o tempo em que aquela transa\u00e7\u00e3o ocorreu e etc. Atrav\u00e9s da leitura dos dados desse dataset, podemos fazer um mapeamento usando [Teoria dos Grafos](ref), modelando cada transa\u00e7\u00e3o entre pares de clientes como uma aresta e os clientes envolvidos nessa transa\u00e7\u00e3o como sendo os n\u00f3s. Atrav\u00e9s dessa modelagem podemos ent\u00e3o usar de diversos algoritmos, ideias e teoremas da Teoria dos Grafos para fazer novas an\u00e1lises. Uma an\u00e1lise particularmente interessante que podemos fazer \u00e9 sobre **Lavagem de Dinheiro**. O [*\"operacional\"* da lavagem de dinheiro](ref) se d\u00e1 atrav\u00e9s da entrada num sistema econ\u00f4mico legal de dinheiro obtido de maneira il\u00edcita. Nesse caso, para analisarmos a a\u00e7\u00e3o da lavagem de dinheiro queremos observar como os valores que s\u00e3o inicialmente transferidos em **transa\u00e7\u00f5es fraudulentas** s\u00e3o movimentados *a posteriori* para outras contas em transa\u00e7\u00f5es **n\u00e3o necessariamente fraudulentas**, ou seja, isso significa analisar valores que possam ter sido fraudados para contas intermedi\u00e1rias e depois sacados.","6188dd50":"# 2. AED - An\u00e1lise Explorat\u00f3ria de Dados e Primeiras Impress\u00f5es\n\nNessa se\u00e7\u00e3o iremos descrever algumas perguntas e as respectivas an\u00e1lises de dados para respond\u00ea-las bem como, quais insights essas an\u00e1lises nos trouxeram.","6838e347":"Outra an\u00e1lise que podemos fazer seguindo essa mesma linha de raciocinio \u00e9 comparar as os \"steps\" de cada transa\u00e7\u00e3o e a presen\u00e7a\/aus\u00eancia de fraude.","445583c3":"# 1. O Desafio:\n\nO Desafio consiste em analisar uma base de dados (com ~ 6 Milh\u00f5es de registros) com dados resumidos de transa\u00e7\u00f5es econ\u00f4micas (transfer\u00eancias banc\u00e1rias entre duas contas) e tentar determinar um modelo que possa classificar o evento de Fraude presente\/ausente nessa transa\u00e7\u00e3o.\n\n(REF) Uma fraude \u00e9 um evento que tem por objetivo o desvio de valores de uma fonte a um destinatario por meio da interven\u00e7\u00e3o (meio que um men-in-the-middle) no processo de transfer\u00eancia.\n\nOs dados desse estudo s\u00e3o provenientes de um gerador de bases de dados falsas. Uma vez que dados de transa\u00e7\u00f5es econ\u00f4micas s\u00e3o sens\u00edveis (REF) e possuem um alto impacto, se faz necess\u00e1rio a utiliza\u00e7\u00e3o de dados falsos.\n\nO problema em quest\u00e3o \u00e9 dito um problema de classifica\u00e7\u00e3o, pois a sa\u00edda (target) \u00e9 de dom\u00ednio discreto, ainda mais especificamente, o problema ter\u00e1 duas sa\u00edda possiveis, (1, sendo constatada a fraude, ou 0, n\u00e3o sendo constatada a fraude).\n\nNesse trabalho iremos apresentar o estudo da base de dados utilizando a An\u00e1lise Explorat\u00f3ria de Dados, a partir desse ponto iremos responder algumas quest\u00f5es pertinentes ao modelo de neg\u00f3cio apresentado (como entender o mundo de transa\u00e7\u00f5es econ\u00f4micas e quais quest\u00f5es s\u00e3o mais relevantes quanto a seguran\u00e7a do sistema de transa\u00e7\u00f5es, a partir da an\u00e1lise das transa\u00e7\u00f5es em si). Uma vez tendo entendido os dados, iremos fazer alguns tratamentos com o intuito de limpar a base e enriquecer os dados, e por fim iremos rodar alguns modelos com o intuito de gerar um modelo de classifica\u00e7\u00e3o (Classificador) para o problema, analisando sua qualidade com algumas m\u00e9tricas como a F-Medida do Modelo.\n\nPor fim apresentamos futuras an\u00e1lises que podemos fazer a partir do entendimento que esse trabalho nos forneceu.\n","c46a4359":"- *Observa\u00e7\u00e3o* sobre a precis\u00e3o das opera\u00e7\u00f5es de diferen\u00e7as:\n\nNote a diferen\u00e7a na tabela abaixo para os valores de 'e':","5cd52f60":"- Important note:\n\nWe have to carry about this dataset because of his unbalanced nature.\n\nAs unbalaced we refer to: [unbalanced dataset in ML](https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28).\n\nThe so called \"Trivial Classifier\" or \"Dummy Classifier\" (The model that always predict the target mean), in this particular show a success rate (percent of overall correct classification) as 99.87 % !","d8a57dbb":"# 3. Manipula\u00e7\u00e3o de Dados","419b55c2":"Atrav\u00e9s da an\u00e1lise da tabela acima podemos observar que:\n\n- As transa\u00e7\u00f5es fraudulentas s\u00f3 ocorrem nas transa\u00e7\u00f5es do tipo '*CASH_OUT*' e '*TRANSFER*', distribu\u00eddas praticamente igualmente.\n- Enquanto nas transa\u00e7\u00f5es n\u00e3o-fraudelentas as transa\u00e7\u00f5es se concentram principalmente (~90%) nas transa\u00e7\u00f5es do tipo (em ordem crescente): '*CASH_IN*', '*PAYMENT*' e '*CASH_OUT*'","d409ad3f":"### 6.1 An\u00e1lise da movimenta\u00e7\u00e3o das contas","0d79c322":"- Vamos fazer essa an\u00e1lise sobre as movimenta\u00e7\u00f5es de valores e os saldos na vis\u00e3o *\"por tipo de transa\u00e7\u00e3o\"*","840b35c4":"Vamos analisar a vari\u00e1vel-alvo contra o montante das transa\u00e7\u00f5es para podermos verificar se existe uam reala\u00e7\u00e3o entre essas vari\u00e1veis:","6a1a07c8":"# 4. Machine Learning","021100e3":"- *Observa\u00e7\u00e3o*: Checando quantas e quais transa\u00e7\u00f5es tiveram uma movimenta\u00e7\u00e3o at\u00edpica (considerando at\u00edpica uma transa\u00e7\u00e3o que movimentou 0 em 'amount'), podemos observar que:","b5e31317":"### 3.3 - Eliminando amostras que n\u00e3o interessam\n- Por conta do fato das transa\u00e7\u00f5es que possuem o envolvimento de mercadores n\u00e3o possuir dos balan\u00e7os das contas, n\u00e3o iremos consider\u00e1-las."}}