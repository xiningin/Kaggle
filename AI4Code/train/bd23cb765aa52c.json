{"cell_type":{"bd5577fd":"code","1c2f31ec":"code","8b619ccc":"code","618979cd":"code","96bfff74":"code","b6a55b1e":"code","05c7e3cd":"code","df4965bd":"code","fbf0906d":"code","c02cd191":"code","1fb7fbf3":"code","414d6196":"code","18aa4d84":"code","64365252":"code","d40c4974":"code","d8bfb394":"code","e3d5cd22":"code","915b34f9":"code","8a11c2ba":"markdown","d527d1a6":"markdown","b2d80e76":"markdown","283b4792":"markdown","0d909a39":"markdown","f0041a37":"markdown","178a4271":"markdown","9b5f5bdb":"markdown","7579b9fc":"markdown","6f0c2ea8":"markdown","3e9ec060":"markdown","b48f84ac":"markdown","9e9f1134":"markdown","34c389e8":"markdown"},"source":{"bd5577fd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","1c2f31ec":"df.describe()","8b619ccc":"#wow, a complete dataset!\ndf.isnull().sum()","618979cd":"df[\"Class\"].skew() #of course our target is heavily skewed in favor of the normal cases","96bfff74":"df[\"Class\"].value_counts()\ndf[\"Class\"].value_counts().plot(kind = \"bar\")","b6a55b1e":"print(\"Percentage of Frauds:\" + str(len(df[df[\"Class\"]==1])\/len(df.index)*100))\nprint(\"Percentage of Regulars:\" + str(len(df[df[\"Class\"]==0])\/len(df.index)*100))\n# fraud cases make up less than half a percent of all samples","05c7e3cd":"# when scaling we should choose a sclaer which is more robust on outliers\nfig, ax = plt.subplots(2,2, figsize = (10,8)) #alternativly fig, (ax1, ax2, ax3, ax4)\n\nax[0,0].boxplot(df[\"Time\"])\nax[0,1].boxplot(df[\"Amount\"])\nax[0,0].set_title(\"Time\")\nax[0,1].set_title(\"Amount\")\n\nax[1,0].hist(df[\"Time\"], bins = 200)\nax[1,1].hist(df[\"Amount\"],bins = 200)\nax[1,0].set_title(\"Time\")\nax[1,1].set_title(\"Amount\")","df4965bd":"# I settled with the robust scaler\n# it is more robust to outliers due to the fact that it uses the median and the IQR rather than the mean\/std (Standardscaler)\n\nfrom sklearn.preprocessing import RobustScaler\nrob = RobustScaler()\n\ndf[\"scaled_time\"] = rob.fit_transform(df[\"Time\"].values.reshape(-1,1))       #values get directly converted into array\ndf[\"scaled_amount\"] = rob.fit_transform(df[\"Amount\"].values.reshape(-1,1))   #reshape it into (-1,1) because I have only one feature \ndf.drop(columns = {\"Time\", \"Amount\"}, inplace = True)","fbf0906d":"df.columns\ndf = df[['scaled_time', 'scaled_amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class', ]]\ndf.head()","c02cd191":"#the ten folds in cv make this whole expeditur computationally very expensive, but for the sake of \"comparability\" with the following resample-methods..\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nX = df.drop(columns = {\"Class\"})\ny = df[\"Class\"]\n\nlr = LogisticRegression(random_state = 0)\nRFC = RandomForestClassifier(random_state = 0)\nDTC = DecisionTreeClassifier(random_state = 0)\nprint(\"Accuracy LR of original:\", cross_val_score(lr, X, y, cv = 10, scoring = \"accuracy\").mean())\nprint(\"Accuracy DTC of original:\", cross_val_score(DTC, X, y, cv = 10, scoring = \"accuracy\").mean())\nprint(\"Accuracy RFC of original:\", cross_val_score(RFC, X, y, cv = 10, scoring = \"accuracy\").mean())","1fb7fbf3":"# same wiht recall as recall as metric\nprint(\"Recall LR of original:\", cross_val_score(lr, X, y, cv = 10, scoring = \"recall\").mean())\nprint(\"Recall DTC of original:\", cross_val_score(DTC, X, y, cv = 10, scoring = \"recall\").mean())\nprint(\"Recall RFC of original:\", cross_val_score(RFC, X, y, cv = 10, scoring = \"recall\").mean())\n","414d6196":"#First, I will apply the simple undersampling method:\n\n# determine the len total number of fraud cases\nlen_fraud = len(df[df[\"Class\"] == 1])\n\n# take the indices of the fraud\/normal cases\nfraud_indices = df[df[\"Class\"]== 1].index\nnormal_indices = df[df[\"Class\"] == 0].index\n\n# take a randomly provided set of x samples (= len_fraud) from the indices of the normal cases\nrand_norm_ind = np.random.choice(normal_indices ,len_fraud, replace = False)\n\n# concetanate the indices from fraud cases with the randomly selected normal cases\nundersample_indices = np.concatenate([rand_norm_ind, fraud_indices])\n\n# create an undersample dataset with the indices\nundersample = df.iloc[undersample_indices, :]","18aa4d84":"y_undersample = undersample[\"Class\"]\nX_undersample = undersample.drop(columns = {\"Class\"})\nlr2 = LogisticRegression(random_state = 0)\nRFC2 = RandomForestClassifier(random_state = 0)\nDTC2 = DecisionTreeClassifier(random_state = 0)\n#since I declared that recall is my objective, I will now only calculate recall for the different models","64365252":"print(\"Recall LG of undersample:\", cross_val_score(lr2, X_undersample, y_undersample, cv = 10, scoring = \"recall\").mean())\nprint(\"Recall DTC of undersample:\", cross_val_score(DTC2, X_undersample, y_undersample, cv = 10, scoring = \"recall\").mean())\nprint(\"Recall RFC of undersample:\", cross_val_score(RFC2, X_undersample, y_undersample, cv = 10, scoring = \"recall\").mean())","d40c4974":"corr_df = pd.DataFrame()\ncorr_df[\"Corr\"] = undersample.corr()[\"Class\"].sort_values(ascending = False)\nconcat = pd.concat([corr_df.head(2), corr_df.tail(7)])\nprint(concat)\n#showing only values with a correlation to \"Class\" greater than 0.5 or lesser than -0.5\n#it is important to use the correlation after balancing the data set","d8bfb394":"suspect = [\"V3\", \"V4\", \"V9\", \"V10\", \"V11\", \"V12\", \"V14\", \"V16\", \"V17\"]\nbest_recall = 0\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits = 10, random_state = 0, shuffle = True)\n\nfor vars in suspect:\n    for e in [1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2, 2.5, 3]:\n        for m in [lr2, DTC2, RFC2]:\n            UQ = np.percentile(undersample[vars], 75)\n            LQ = np.percentile(undersample[vars], 25)\n            IQR = UQ - LQ\n            UW = (UQ + e*IQR)\n            LW = (LQ - e*IQR)\n            mask = (undersample[[vars]] < LW) | (undersample[[vars]] > UW)\n            undersample[mask] = np.nan\n            undersample.dropna(inplace = True)\n            undersample.shape\n            X_undersample_new = undersample.drop(columns = {\"Class\"})\n            y_undersample_new = undersample[\"Class\"]\n            recall = cross_val_score(m, X_undersample_new, y_undersample_new, cv = skf, scoring = \"recall\").mean()\n            if recall > best_recall:\n                best_recall = recall\n                best_factor = {\"e\": e}\n                best_model = {\"m\": m}\nprint(\"Best Recall: {:.3f}\".format(best_recall))\nprint(\"Best Modell:\", best_model)\nprint(\"Best factor used:\", best_factor)","e3d5cd22":"\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2) #tell python to stratify the target in quotes like in the original\n\nsm = SMOTE(random_state = 10)\nX_train_over, y_train_over  = sm.fit_sample(X_train, y_train) #smote creates arrays\n\n# make sure SMOTE worked correctly\nnp.unique(y_train_over, return_counts = True)","915b34f9":"#first with LogisticRegression\nlr3 = LogisticRegression(random_state = 10)\nRFC3 = RandomForestClassifier(random_state = 0)\nDTC3 = DecisionTreeClassifier(random_state = 0)\n\n#first LogisticRegression\nlr3.fit(X_train_over, y_train_over)\nLR_over_prediction = lr3.predict(X_test)\n\n#then with DecisionTree\nDTC3.fit(X_train_over, y_train_over)\nDTC_over_prediction = DTC3.predict(X_test)\n\n#then with RandomForestClassifier\nRFC3.fit(X_train_over, y_train_over)\nRFC_over_prediction = RFC3.predict(X_test)\n\nprint(\"Recall of LG-Model:\", recall_score(y_test, LR_over_prediction))\nprint(\"Recall of DTC-Model:\", recall_score(y_test, DTC_over_prediction))\nprint(\"Recall of RFC-Model:\", recall_score(y_test, RFC_over_prediction))","8a11c2ba":"### 5. Oversampling with SMOTE\nLast but not least I will try SMOTE for oversampling. I want to try it to oversample the data in favour of the minority class. Two popular methods are Duplication and SMOTE. Since one creates duplicates and the other interpolates samples to create new data, both methods have to be handled carefully to prevent Data Leakage during Cross validation. This would result in a model learning and evaluating from the same sample (or near same sample) which would yield a extremly high accuracy. This is known as the problem of \u201coveroptimism\u201d, which is introduced by a bad Cross Validation design. Because of the complex nature of this problem, I will demonstrate SMOTE in a simpler fashion.\n\nIn the near future I might give it a try to combine it with Cross Validation.","d527d1a6":"As expected, the recall value is very bad for our models. So lets get into action!","b2d80e76":"### 3.1 Alternative Metrics for accuracy\nSo why should we mistrust the accuracy in this case, though its score is pretty high? \"Accuracy\" is defined as: (correctly predicted values)\/(total number of examined values), or in lingo of the confusion matrix: (True Positives + True Negatives)\/Total. Now remember the ratio of our fraud cases -> its 0.1727% of our total cases. So, taking the above formula in mind, even if we would label all our fraud-cases wrong, the ratio of fraud to normal would still result in an extrmely high accuracy.\n\nWhat now? -> we settel for another metric.\n\nf1-score is a harmonic mean from both, recall and precision. Not simple average, so when used it is more resistant to outliers: 2(recallprecision\/recall+precision).\n\nPrecision is defined as: True Positives\/(True Positives + False Positives), so: correctly predicted fraud\/(correctly predicted fraud + predicted as fraud but really normal)\n\nRecall is defined as: True Positives\/(True Positives + False Negatives), so: correctly predicted fraud\/(corretly predicted fraud + predicted as normal but really fraud). Since it is crucial not to miss (in best case) a single fraud case, I will naturally go with recall!\n\nFunny side note: when we increase the recall, we decrease the precision\n\n-> This was heavily borrowed from Will Koehrsen https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n","283b4792":"## Credit Card Fraud Detection\nFraud detection is a very classic field where you encounter imbalanced datasets.\nAnother example of heavily imbalanced classification problems is disease detection.\n\n**If you like my work, please leave a like for motivation!** If you dont like it, please provide positive criticism or impulses ;)","0d909a39":"The features V1 .. V28 are Principal Components. Since PCA trys to maximize the variance (or: sum of squared distances\/n-1) it is very likely that the features for the PCs have been scaled first, because not scaling them would result in distorted PCs. The only features not scaled should be Amount and Time.","f0041a37":"### 4.2 Undersampling and excluded Outliers\nIn my opinion this is an interesting case. Outlier-elimination is helpful in generalization and therefor not always in our interest. I obtained important features from the undersample dataset and tried to eliminate outliers from it within a range for the possible upper and lower boundaries with the IQR-method. Comparing the performance of the models after outlier-elimination revealed a small insight..\n\nSauces used to inspire a automatical outlier detection ;)\n![image.png](attachment:image.png)\n#https:\/\/stackoverflow.com\/questions\/39068214\/how-to-count-outliers-for-all-columns-in-python\n#https:\/\/blog.clairvoyantsoft.com\/mlmuse-approaches-to-outlier-detection-part-1-bd4c8faa4f24\n#https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/?unapproved=506369&moderation-hash=35fe4a1811d34fb2a32a5bd94d67fe28#comment-506369\n","178a4271":"### 4. Undersampling\nFor this imbalanced problem, I want to try two methods: Undersampling and Oversampling with SMOTE.\n\nWhy do we care about imbalanced datasets? First of all it is clear that with less information on the minority class, the chosen ML-algorithm will learn to favor the majority class during training -> Bias. Highly imbalanced datasets can lead to a so called \"overoptimism\" (more later). With more or less samples on one target-class the correlation of features towards the target variable will be off. Not only does it effect the overal performance of the ML-algorithm, it also hinders correct possible feature selection\/engineering.\n\n-> my thanks to Will Badr: https:\/\/towardsdatascience.com\/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb\n\nI want to compare the values for both methods. Later classification models will be Logistic Regression, DecisionTree and RandomForest. Maybe just a prediction with imbalanced dataset to show the hazards of missleading high accuracy (then explaning what happened)\n","9b5f5bdb":"I find this result really interesting, because outlier removal and tuning of the IQR-boundaries did not yielded a model that can better generalize. Actually the removal of outliers seems to accompanied with a substantial information loss. It may be that many of the outliers were fraud cases.\nTHIS said, I just want to remind that I did not include parameter tuning for the different models (additional computational effort). I might include it anyways in the future.","7579b9fc":"### 3. Working on imbalanced Data\nJust for the purpose of demonstration I will first do some ML with the imbalanced dataframe. I expect the (misleading!) accuracy to be extremly high and explain why this metric is so high and why it is wrong to use it with an imbalanced dataset. Also lets compare the performance of some different models.","6f0c2ea8":"### 1. Data Exploration","3e9ec060":"### 2. Minor Feature Engineering","b48f84ac":"## Agenda\n\n1. Data Exploration\n2. Minor Feature Engineering\n3. Working on imbalanced Data\n    \n    3.1 Alternative Metrics for accuracy\n4. Undersampling\n    \n    4.1 Undersampling and included Outliers\n    \n    4.2 Undersampling and excluded Outliers\n5. Oversampling with SMOTE\n6. Conclusion","9e9f1134":"### 4.1 Undersampling and included Outliers\nLets add two other models: RandomForest and DecisionTree. We are going to train models on a balanced set with equally distributed fraud and normal cases. We are still going to use recall as the measure metric.","34c389e8":"### 6. Conclusion\nSome interesting notes on working with this marvelous example of imbalanced dataset:\nOn an imbalanced dataset, accuracy is a missleading mesurement. In this case, recall is a more satisfying measurement for our needs.\nAs expected, working with a balanced dataset (undersampling as well as oversampling) did yield better measures for recall than an highly unbalanced dataset.\nOutlier removal (without model parameter tuning) did not work for me. The outliers seemded to contain valuable information on fraud cases.\nA simple implementation of SMOTE did not yield a better performance than undersampling. It might perform better, if I include SMOTE in cross-validation in the future.\nThank you for your attention!\n"}}