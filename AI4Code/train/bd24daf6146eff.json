{"cell_type":{"b1acdea9":"code","13bcb02b":"code","b8dfe833":"code","584d808b":"code","9b5b82b2":"code","972b7eee":"code","4cae0c42":"code","12478349":"code","a8f9eb5e":"code","d5f284ea":"code","71e886f2":"code","7d136869":"code","732bf82e":"code","5c31fed8":"code","2e9cd56a":"code","2b707c1a":"code","500bf8f5":"code","021f8e06":"code","080fb9e7":"code","1358e2e2":"code","a7d188ab":"code","6aadb69b":"code","d604136e":"code","aa1fbc7c":"code","db048e1e":"code","a1ea6044":"code","3a32ffcb":"code","6c8065cd":"code","5b0c3422":"code","8fa4a1a0":"code","94080a78":"code","46cf0330":"code","ab7f86d0":"code","e8ab06b9":"code","7bbb4824":"code","cad360ec":"code","f0fe359f":"code","e16d10bb":"markdown","e28f9cb5":"markdown","4c636735":"markdown","4ce8cab6":"markdown","1de410d4":"markdown","658e5448":"markdown","9ebbd95d":"markdown"},"source":{"b1acdea9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport datetime\ndate_depart=datetime.datetime.now()\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport glob\nimport random\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nimport nltk\nfrom nltk import ngrams\nimport joblib\nfrom dask.distributed import Client\n\nimport spacy\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\nfrom sklearn.decomposition import  PCA,NMF, LatentDirichletAllocation\nfrom sklearn.decomposition import  IncrementalPCA\n!python -m spacy download fr_core_news_md\n!python -m spacy download fr_core_news_sm\nimport fr_core_news_md\nnlp_fr = fr_core_news_md.load()\nimport  cloudpickle\n","13bcb02b":"try:\n    client = Client('127.0.0.1:8786',timeout=5,set_as_default=True)\nexcept OSError as e:\n#     print(e)\n    client =Client(set_as_default=True,n_workers=2,threads_per_worker=4,dashboard_address=None)\nclient","b8dfe833":"import requests\nimport spacy.lang.fr\nfrom stop_words import get_stop_words\nstopwords_fr_set=set(nltk.corpus.stopwords.words('french'))\nstopwords_fr_set.update(get_stop_words('fr'))\nstopwords_fr_set.update(spacy.lang.fr.stop_words.STOP_WORDS)\nstopwords_fr_set.update([\"c'est\",\"j'ai\",\"n'est\",\"n'ait\",\"ca\",\"\u00e7a\",\"sais\",\"jamais\",\"chose\",\"ex\",\"'quelqu'\",'quelqu',\"br\"])\nstopwords_fr_set.update((str(i) for i in range(30)))\nstopwords_fr_set.update([\"faut\", \"arr\u00eater\", \"faisons\", \"faite\", \"faits\",'oui' ,\"www\",\"https\",\"http\",\"ect\"])\nstopwords_fr_set.update(requests.get(\"https:\/\/raw.githubusercontent.com\/stopwords-iso\/stopwords-fr\/master\/stopwords-fr.json\").json())\nstopwords_fr_set.update(str(i) for i in range(100))\nstopwords_fr_set.update(str(i) for i in range(1980,2025))\nstopwords_fr_set=list(stopwords_fr_set)\n","584d808b":"n_samples = 512*1024\nvectorsamples=int(1e6)\nn_features = 3000\nn_components = 300\nn_top_words = 15\nngram_range=(1,3)\nmax_df=0.9\nmin_df=20","9b5b82b2":"tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n                                   max_features=n_features,\n                                   stop_words=stopwords_fr_set,\n                                   ngram_range=ngram_range,\n                                   dtype=np.float32\n                                   )\ntf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n                                max_features=n_features,\n                                stop_words=stopwords_fr_set,\n                                ngram_range=ngram_range,\n                                  dtype=np.uint16\n                               )","972b7eee":"texts=[]\ntexts_byfiles=dict()\nfilelist=glob.glob(\"..\/input\/**\/*.csv*\", recursive=True)\nrandom.shuffle(filelist)\nfor f in filelist:\n    print (f)\n    df=pd.read_csv(f,low_memory=False)\n    dftext=[]\n    texts_byfiles[\"f\"]=dftext\n    for n,s in df.items():\n        for e in s:\n            if isinstance(e,str):\n                if len(e.split())>2 :\n                    dftext.append(e)\n    texts+=dftext\n\ntexts=list(set(texts))\nrandom.shuffle(texts)\ntextes_base=texts","4cae0c42":"len(texts)","12478349":"texts=random.sample(texts,n_samples)","a8f9eb5e":"tfidf_vectorizer.fit(random.sample(textes_base,vectorsamples))","d5f284ea":"\n\ntfidf = tfidf_vectorizer.transform(texts)\ntfidf","71e886f2":"rr = dict(zip(tfidf_vectorizer.get_feature_names(),  tfidf_vectorizer.idf_))\n\ntoken_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ndel rr\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight.reset_index(drop=True,inplace=True) \n\n\nsns.barplot(x='token', y='weight', data=token_weight.iloc[:60], )            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(25,15)\nax=fig.axes[0]\nplt.yscale(\"log\")\nax.tick_params(axis='x',labelrotation=90 )\nplt.show()\n\n","7d136869":"plt.figure(figsize=(12,12))\ntoken_weight.plot()\n# plt.yscale(\"log\")\nplt.show()","732bf82e":"token_weight","5c31fed8":"\n\nnmf = NMF(n_components=n_components,\n          alpha=.1, l1_ratio=.5,\n           tol=0.005,\n          init=\"nndsvd\",\n          max_iter =20,\n          shuffle =True,\n          verbose=True)\nnmf","2e9cd56a":"tfidf_nmf=nmf.fit_transform(tfidf)\ntfidf_nmf","2b707c1a":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\ndef get_top_words_list(model, feature_names, n_top_words):\n    topwords=[]\n    for topic_idx, topic in enumerate(model.components_):\n       \n        topwords.append([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n    return topwords","500bf8f5":"\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\nnmf_top_words_list=get_top_words_list(nmf, tfidf_feature_names, n_top_words)","021f8e06":"ind_text=np.random.choice(n_samples,5)\n\nfor t,topics in zip([texts[i] for i in ind_text ],\n                    \n        tfidf_nmf[ind_text]):\n    print(t)\n    top_topics=np.argsort(topics)[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(nmf_top_words_list[n][:6])}\")\n    print()\n    \n","080fb9e7":"pca_nmf = IncrementalPCA(n_components=3, batch_size=200)\n\n\ntfidf_nmf_pca=pca_nmf.fit_transform(tfidf_nmf)\n\n\nsns.scatterplot(x=\"pca1\",y=\"pca2\",hue=\"pca3\" ,data=pd.DataFrame(tfidf_nmf_pca[np.random.choice(len(tfidf_nmf_pca),9000)]\n                                                                ,columns=[\"pca1\",\"pca2\",\"pca3\"]))\n","1358e2e2":"norms=np.linalg.norm(pca_nmf.components_[0:2, :],axis=0)\npca2_n=norms>np.quantile(norms,0.03)\n","a7d188ab":"pca_nmf.components_[0:2, pca2_n].shape","6aadb69b":"\nnp.array(nmf_top_words_list)[pca2_n][:,:4]\n[str(l[:4]) for l in np.array(nmf_top_words_list)[pca2_n]]","d604136e":"def myplot(score,coeff,labels=None):\n    plt.figure(figsize=(25,50))\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0\/(xs.max() - xs.min())\n    scaley = 1.0\/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = ys)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n    plt.xlim(-1,1)\n    plt.ylim(-1,1)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n","aa1fbc7c":"\n\n#Call the function. Use only the 2 PCs.\nmyplot(tfidf_nmf_pca[:,0:2],np.transpose(pca_nmf.components_[0:2, pca2_n]),labels=[str(l[:4]) for l in np.array(nmf_top_words_list)[pca2_n]])\nplt.show()","db048e1e":"tf_vectorizer.fit(random.sample(textes_base,vectorsamples))","a1ea6044":"\ntf = tf_vectorizer.transform(texts)\ntf","3a32ffcb":"print(tf.max())","6c8065cd":"\nlda = LatentDirichletAllocation(n_components=n_components,\n                                max_iter=20,\n                                learning_method='online',\n                                learning_offset=50.,\n                              \n                                verbose =1,\n                                n_jobs =-1\n                               )\n\nwith joblib.parallel_backend('dask'):\n    lda.fit(tf)\n\n\n","5b0c3422":"partial_batch=32*1024\nwith joblib.parallel_backend('dask'):\n    for i in range(0,len(textes_base),partial_batch):\n\n        batchmat=tf_vectorizer.transform(textes_base[i:i+partial_batch])\n        lda.partial_fit(batchmat)\n        if (datetime.datetime.now()-date_depart)>datetime.timedelta(hours=7,minutes=20):\n            break\n        print(i)","8fa4a1a0":"with joblib.parallel_backend('dask'):\n    tfidf_lda=lda.transform(tf)","94080a78":"\nprint(\"\\nTopics in LDA model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nlda_top_words_list=get_top_words_list(lda, tf_feature_names, n_top_words)\nprint_top_words(lda, tf_feature_names, n_top_words)","46cf0330":"lda_best_topics=np.argsort(tfidf_lda.mean(axis=0))[-10:]\nfor n in range(6):\n    print(f\"topic {lda_best_topics[-n]}: {', '.join(lda_top_words_list[lda_best_topics[-n]][:15])}\")\n    \n","ab7f86d0":"topic_texts=np.argsort(tfidf_lda,axis=0)[:300]\nfor t in range(tfidf_lda.shape[1]):\n    for i in range(6):\n        nmax=topic_texts[-i,t]\n        print(tfidf_lda[nmax,t])\n        print(texts[nmax][:300])    \n    print(f\"topic {t}: {', '.join(lda_top_words_list[t][:15])}\")\n    print(\"***\")","e8ab06b9":"ind_text=np.random.choice(len(textes_base),10)\n\nfor i in ind_text:\n    t=textes_base[i]\n    vect=tf_vectorizer.transform([t])\n#     .toarray().flatten()\n    topics=lda.transform(vect)\n    \n                    \n        \n    \n    tf_vectorizer.transform(textes_base[i:i+partial_batch])\n    print(t+\"\\n\")\n    top_topics=np.argsort(topics.flatten())[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(lda_top_words_list[n][:6])}\")\n    print(\"\\n\\n\")","7bbb4824":"\nipca = IncrementalPCA(n_components=3, batch_size=200)","cad360ec":"\nwith joblib.parallel_backend('dask'):\n    for i in range(0,len(textes_base),partial_batch):\n        batchmat=tf_vectorizer.transform(textes_base[i:i+partial_batch])\n        batchmat_lda=lda.transform(batchmat)\n        ipca.partial_fit(batchmat_lda)\n        print(i,end=\"\\r\")\n    ","f0fe359f":"\ntfidf_lda_ipca=ipca.transform(tfidf_lda)\nsns.scatterplot(x=\"pca1\",y=\"pca2\",hue=\"pca3\" ,data=pd.DataFrame(tfidf_lda_ipca,columns=[\"pca1\",\"pca2\",\"pca3\"]))","e16d10bb":"#pca_nmf","e28f9cb5":"#mmf","4c636735":"code from sklearn \"Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\" example","4ce8cab6":"#lda","1de410d4":"#load texts","658e5448":"#CountVectorizer","9ebbd95d":"#tfidf"}}