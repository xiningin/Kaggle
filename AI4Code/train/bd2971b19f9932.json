{"cell_type":{"e4547b00":"code","0ef400b4":"code","1cc94875":"code","3e35a531":"code","1c15d97f":"code","1bcb7117":"code","9e278d35":"code","d444c88f":"code","dd361054":"code","34252c28":"code","8a789a61":"markdown","2bfe6211":"markdown","b773e7e8":"markdown","37371799":"markdown","d344b138":"markdown","84b41a1a":"markdown","ce5beeae":"markdown","6134a34c":"markdown"},"source":{"e4547b00":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nsns.set_theme(style=\"whitegrid\")","0ef400b4":"train_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_data.info()","1cc94875":"test_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_data.info()","3e35a531":"from urllib.parse import urlparse\ntrain_data['base_url'] = train_data.apply(lambda row: urlparse(row.url_legal).netloc if not pd.isna(row.url_legal) else 'NA', axis=1)","1c15d97f":"fig = plt.figure(figsize=(25,10))\ng = sns.countplot(x=\"base_url\", data=train_data, palette=\"Set3\")\ng.set_xticklabels(pd.unique(train_data.base_url), rotation=30)\nsns.despine(left=True)\nplt.show()","1bcb7117":"fig = plt.figure(figsize=(25,10))\ng = sns.countplot(x=\"base_url\", data=train_data[train_data.base_url != \"NA\"], palette=\"Set3\")\ng.set_xticklabels(pd.unique(train_data[train_data.base_url != \"NA\"].base_url), rotation=30)\nsns.despine(left=True)\nfig.show()","9e278d35":"fig = plt.figure(figsize=(25,10))\ng = sns.violinplot(x=\"base_url\", y=\"target\", data=train_data, palette=\"Set3\", linewidth=1, scale=\"width\")\ng.set_xticklabels(pd.unique(train_data.base_url), rotation=30)\nsns.despine(left=True, bottom=True)\nplt.show()","d444c88f":"fig = plt.figure(figsize=(25,10))\nsns.ecdfplot(x=\"target\", hue=\"base_url\", data=train_data, palette=\"Set3\", linewidth=2)\nsns.despine(left=True, bottom=True)\nfig.show()","dd361054":"!pip install bs4","34252c28":"import requests\nimport os\nfrom bs4 import BeautifulSoup\n\n# Cleaning before run\nfor f in os.listdir(\".\/\"):\n    os.remove(os.path.join(\".\/\", f))\n\ndef extract_data(row):\n    if not pd.isna(row.url_legal):\n        response = requests.get(row.url_legal)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            text = \"\"\n\n            if row.base_url in [\"simple.wikipedia.org\", \"en.wikipedia.org\", \"en.wikibooks.org\"]:\n                contents = soup.find_all(id=\"mw-content-text\")\n                for content in contents:\n                    paragraphs = content.find_all('p')\n                    for paragraph in paragraphs:\n                        text = text + paragraph.get_text()\n            elif row.base_url in [\"kids.frontiersin.org\"]:\n                contents = soup.find_all('div', class_=\"fulltext-content\")\n                for content in contents:\n                    paragraphs = content.find_all('p')\n                    for paragraph in paragraphs:\n                        text = text + paragraph.get_text()\n            #elif row.base_url in [\"www.commonlit.org\"]:\n                # Not allowed ! see : https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/245665\n                #contents = soup.find_all('div', class_=\"cl-text__excerpt-line-container\")\n                #for content in contents:\n                #    paragraphs = content.find_all('p')\n                #    for paragraph in paragraphs:\n                #        text = text + paragraph.get_text()\n                \n            if len(text) > 0:\n                with open(f'train_{row.base_url}_{row.id}.txt', 'w') as file:\n                    file.write(text)\n                    \n        return response.status_code\n    return 404\n\ntrain_data['data_status'] = train_data.apply(extract_data, axis=1)","8a789a61":"# General idea\n\nThe dataset available in this comptetition gives the sources of the text. Annoted texts are extracted from a wider article\/book\/etc. When the datasource is available, general language models could be pre-trained on those (as proposed on different notebooks).\n\nThis notebook explore available information to have an insight of the problem and download the dataset sources.\n\n## Summary\n- general exploration\n- (WIP) data download","2bfe6211":"Unsurprisingly, the datasource seems to give some insight on the readability.\nFor pretraining every data source is then needed.\n\nCould it be possible to train a masked language model on known site with known readability probability and estimate the readability with the error between the masked token proability and the ground truth ?","b773e7e8":"# Data download for pretraining\n\nThis part download the source data in the output folder of this notebook for models pretraining.\n\nSome data cannot be retreived : \n- Africanstorybooks.org sources are not linked here and cannot be retreived.\n- digitallibrary.io returns 404 error.\n\n[Work in progress.]\nSome PDF are present in the database. PDF read will come in the future.","37371799":"Looking at source names, they might be correlated to the target score :","d344b138":"With some distribution comparison :","84b41a1a":"Without not annoted data :","ce5beeae":"Only 30% of training examples are sourced but it might be a good start for pretraining.\n\nLooking at the input data tab in kaggle notebook edit environment, input data seems to come from some limited source list :","6134a34c":"# General exploration\n\nInput data is distributed in two files train.csv and test.csv. Both have the input source in the column \"url_legal\" :"}}