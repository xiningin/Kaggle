{"cell_type":{"360d0842":"code","a7ba4c87":"code","4d0e298b":"code","80a9f288":"code","45892fda":"code","7736becd":"code","4183985c":"code","a81a8741":"code","2f9f2b85":"code","b5a45c91":"code","9785003f":"code","283103da":"code","531f713c":"code","ef56db69":"code","40b68fb4":"code","9b9fd16b":"code","eb719292":"code","63b48a92":"code","de572fac":"code","a63683a4":"code","161080f7":"code","f343c879":"code","a12d8aa0":"code","d5ba2f6b":"code","0ec955ca":"code","18b9611f":"code","f8267135":"markdown","d481d98a":"markdown","200f6512":"markdown","528114f8":"markdown","796ca9d8":"markdown","bb27622b":"markdown","82aed8ac":"markdown","43d8e21c":"markdown"},"source":{"360d0842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a7ba4c87":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","4d0e298b":"X_train = train_df[\"question_text\"].fillna(\"na\").values\nX_test = test_df[\"question_text\"].fillna(\"na\").values\ny = train_df[\"target\"]","80a9f288":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\nfrom keras.layers import Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.optimizers import *\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport gc\nfrom sklearn import metrics\nfrom keras import regularizers","45892fda":"maxlen = 50\nmax_features = 50000\nembed_size = 300\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","7736becd":"def attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    TIME_STEPS = inputs.shape[1].value\n    SINGLE_ATTENTION_VECTOR = False\n    \n    input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1))(a)\n    output_attention_mul = Multiply()([inputs, a_probs])\n    return output_attention_mul","4183985c":"from keras import backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras import initializers\n\nclass AttLayer(Layer):\n    def __init__(self, attention_dim):\n        self.init = initializers.get('normal')\n        self.supports_masking = True\n        self.attention_dim = attention_dim\n        super(AttLayer, self).__init__()\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n        self.b = K.variable(self.init((self.attention_dim, )))\n        self.u = K.variable(self.init((self.attention_dim, 1)))\n        self.trainable_weights = [self.W, self.b, self.u]\n        super(AttLayer, self).build(input_shape)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def call(self, x, mask=None):\n        # size of x :[batch_size, sel_len, attention_dim]\n        # size of u :[batch_size, attention_dim]\n        # uit = tanh(xW+b)\n        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n        ait = K.dot(uit, self.u)\n        ait = K.squeeze(ait, -1)\n\n        ait = K.exp(ait)\n\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            ait *= K.cast(mask, K.floatx())\n        ait \/= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        ait = K.expand_dims(ait)\n        weighted_input = x * ait\n        output = K.sum(weighted_input, axis=1)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])","a81a8741":"def model_cnn(embedding_matrix):\n    filter_sizes = [1, 2, 3, 5]\n    num_filters = 64\n    inp = Input(shape=(maxlen, ))\n    embed = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1)(embed)\n    \n    mpool = []\n    x = Reshape((maxlen, embed_size, 1))(x)\n    for fil in filter_sizes:\n        conv = Conv2D(num_filters, (fil, embed_size), kernel_initializer='he_normal', activation='relu')(x)\n        pool = MaxPool2D(pool_size=(maxlen - fil + 1, 1))(conv)\n        mpool.append(pool)\n        \n    x = Concatenate(axis=1)(mpool)\n    x = Flatten()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    return model\n    \n\ndef model_bgru(embedding_matrix):\n    inp = Input(shape=(maxlen, ))\n    embed = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = embed\n    x = SpatialDropout1D(0.2)(x)\n    \n    x1 = Bidirectional(CuDNNLSTM(64, return_sequences=True ))(x)\n    x2 = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x1)\n    x = Add()([x1, x2])\n    x = AttLayer(maxlen)(x)\n    x = Dropout(0.4)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    return model","2f9f2b85":"from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y, test_size = 0.1, random_state=42)","b5a45c91":"VAL_Y = []\nTEST_Y = []","9785003f":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","283103da":"model = model_cnn(embedding_matrix)\n#model.summary()\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\nbatch_size = 2048\nepochs = 5\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nmodel.save('.\/model_cnn_glove.h5')","531f713c":"val_pred_cnn_glove = model.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (val_pred_cnn_glove > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)\n\ny_pred_cnn_glove = model.predict(x_test, batch_size=1024, verbose=True)\n\nVAL_Y.append(val_pred_cnn_glove)\nTEST_Y.append(y_pred_cnn_glove)","ef56db69":"model = model_bgru(embedding_matrix)\n#model.summary()\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\nbatch_size = 2048\nepochs = 5\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nmodel.save('.\/model_bgru_glove.h5')","40b68fb4":"val_pred_bgru_glove = model.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (val_pred_bgru_glove > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)\n\ny_pred_bgru_glove = model.predict(x_test, batch_size=1024, verbose=True)\n\nVAL_Y.append(val_pred_bgru_glove)\nTEST_Y.append(y_pred_bgru_glove)","9b9fd16b":"del embeddings_index;\ndel embedding_matrix\ngc.collect()  ","eb719292":"EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","63b48a92":"model = model_bgru(embedding_matrix)\n#model.summary()\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\nbatch_size = 2048\nepochs = 5\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nmodel.save('.\/model_bgru_wiki.h5')","de572fac":"val_pred_bgru_wiki = model.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (val_pred_bgru_wiki > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)\n\ny_pred_bgru_wiki = model.predict(x_test, batch_size=1024, verbose=True)\n\nVAL_Y.append(val_pred_bgru_wiki)\nTEST_Y.append(y_pred_bgru_wiki)","a63683a4":"del embeddings_index;\ndel embedding_matrix\ngc.collect()","161080f7":"EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","f343c879":"model = model_bgru(embedding_matrix)\n#model.summary()\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\nbatch_size = 2048\nepochs = 5\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\nmodel.save('.\/model_bgru_paragram.h5')","a12d8aa0":"val_pred_bgru_paragram = model.predict([X_val], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (val_pred_bgru_paragram > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)\n\ny_pred_bgru_paragram = model.predict(x_test, batch_size=1024, verbose=True)\n\nVAL_Y.append(val_pred_bgru_paragram)\nTEST_Y.append(y_pred_bgru_paragram)","d5ba2f6b":"\npred_val_y = (2.5*val_pred_cnn_glove + 2.5*val_pred_bgru_glove + 2.5*val_pred_bgru_wiki + 2.5*val_pred_bgru_paragram)\/10\n\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(y_val, (pred_val_y > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)","0ec955ca":"# y_pred = 0\n# for i in range(len(y_th)):\n#     y_pred += (y_th[i] * TEST_Y[i])\n# y_pred = y_pred \/ 10\n\ny_pred = (2.5*y_pred_cnn_glove + 2.5*y_pred_bgru_glove + 2.5*y_pred_bgru_wiki + 2.5*y_pred_bgru_paragram)\/10\n\ny_te = (y_pred[:,0] > best_thresh).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","18b9611f":"from IPython.display import HTML\nimport base64  \nimport pandas as pd  \n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index =False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submit_df)","f8267135":"# MODELS\n1. CNN\n2. LSTM\/GRU\n\nFor me LSTM work better than GRU","d481d98a":"# Submission","200f6512":"# GLOVE  - CNN","528114f8":"# GLOVE  - BGRU","796ca9d8":"# Concat Result & Best Threshold","bb27622b":"# PARAGRAM - BGRU ","82aed8ac":"# WIKI_NEWS - BGRU","43d8e21c":"# GLOVE EMBEDDING"}}