{"cell_type":{"6785df0d":"code","9c04fbb2":"code","4fca968f":"code","f233806e":"code","6e108af3":"code","001a2abe":"code","0eb37f1c":"code","c166faac":"code","a4a57a10":"code","33795f8f":"code","15fa9b89":"code","4264dad0":"code","57c16c2d":"code","8d8dc67c":"markdown","024a1742":"markdown","d3178f98":"markdown","5dc8cb80":"markdown","6690c897":"markdown","8a94af51":"markdown","df6a94ac":"markdown","dcf4075d":"markdown","3e1848e1":"markdown","40c2810b":"markdown"},"source":{"6785df0d":"import jieba\nimport re","9c04fbb2":"text = [\n    '\u94b1\u5858\u6c5f\u6d69\u6d69\u6c5f\u6c34\uff0c\u65e5\u65e5\u591c\u591c\u65e0\u7a77\u65e0\u4f11\u7684\u4ece\u4e34\u5b89\u725b\u5bb6\u6751\u8fb9\u7ed5\u8fc7\uff0c\u4e1c\u6d41\u5165\u6d77\u3002',\n    '\u6c5f\u7554\u4e00\u6392\u6570\u5341\u682a\u4e4c\u67cf\u6811\uff0c\u53f6\u5b50\u4f3c\u706b\u70e7\u822c\u7ea2\uff0c\u6b63\u662f\u516b\u6708\u5929\u65f6\u3002\u6751\u524d\u6751\u540e\u7684\u91ce\u8349\u521a\u8d77\u59cb\u53d8\u9ec4\uff0c\u4e00\u62b9\u659c\u9633\u6620\u7167\u4e4b\u4e0b\uff0c\u66f4\u589e\u4e86\u51e0\u5206\u8427\u7d22\u3002',\n    '\u4e24\u682a\u5927\u677e\u6811\u4e0b\u56f4\u7740\u4e00\u5806\u6751\u6c11\uff0c\u7537\u7537\u5973\u5973\u548c\u5341\u51e0\u4e2a\u5c0f\u5b69\uff0c\u6b63\u81ea\u805a\u7cbe\u4f1a\u795e\u7684\u542c\u7740\u4e00\u4e2a\u7626\u524a\u7684\u8001\u8005\u8bf4\u8bdd\u3002',\n    '\u90a3\u8bf4\u8bdd\u4eba\u4e94\u5341\u6765\u5c81\u5e74\u7eaa\uff0c\u4e00\u4ef6\u9752\u5e03\u957f\u888d\u65e9\u6d17\u5f97\u892a\u6210\u4e86\u84dd\u7070\u8272\u3002',\n    '\u53ea\u542c\u4ed6\u4e24\u7247\u68a8\u82b1\u6728\u677f\u78b0\u4e86\u51e0\u4e0b\uff0c\u5de6\u624b\u4e2d\u7af9\u68d2\u5728\u4e00\u9762\u5c0f\u7faf\u9f13\u4e0a\u6572\u8d77\u5f97\u5f97\u8fde\u58f0\u3002',\n    '\u5531\u9053\uff1a\u5c0f\u6843\u65e0\u4e3b\u81ea\u5f00\u82b1\uff0c\u70df\u8349\u832b\u832b\u5e26\u665a\u9e26\u3002\u51e0\u5904\u8d25\u57a3\u56f4\u6545\u4e95\uff0c\u5411\u6765\u4e00\u4e00\u662f\u4eba\u5bb6\u3002',\n    '\u90a3\u8bf4\u8bdd\u4eba\u5c06\u6728\u677f\u6572\u4e86\u51e0\u4e0b\uff0c\u8bf4\u9053\uff1a\u201c\u8fd9\u9996\u4e03\u8a00\u8bd7\uff0c\u8bf4\u7684\u662f\u5175\u706b\u8fc7\u540e\uff0c\u539f\u6765\u7684\u5bb6\u5bb6\u6237\u6237\uff0c\u90fd\u53d8\u6210\u4e86\u65ad\u5899\u6b8b\u74e6\u7684\u7834\u8d25\u4e4b\u5730\u3002',\n    '\u5c0f\u4eba\u521a\u624d\u8bf4\u5230\u90a3\u53f6\u8001\u6c49\u4e00\u5bb6\u56db\u53e3\uff0c\u60b2\u6b22\u79bb\u5408\uff0c\u805a\u4e86\u53c8\u6563\uff0c\u6563\u4e86\u53c8\u805a\u3002',\n    '\u4ed6\u56db\u4eba\u7ed9\u91d1\u5175\u51b2\u6563\uff0c\u597d\u5bb9\u6613\u53c8\u518d\u56e2\u805a\uff0c\u6b22\u5929\u559c\u5730\u7684\u56de\u5230\u6545\u4e61\uff0c\u5374\u89c1\u623f\u5c4b\u5df2\u7ed9\u91d1\u5175\u70e7\u5f97\u5e72\u5e72\u51c0\u51c0\uff0c\u65e0\u53ef\u5948\u4f55\uff0c\u53ea\u5f97\u53bb\u5230\u6c74\u6881\uff0c\u60f3\u89c5\u4e2a\u751f\u8ba1\u3002',\n    '\u4e0d\u6599\u60f3\uff1a\u5929\u6709\u4e0d\u6d4b\u98ce\u4e91\uff0c\u4eba\u6709\u65e6\u5915\u7978\u798f\u3002',\n    '\u4ed6\u56db\u4eba\u521a\u8fdb\u6c74\u6881\u57ce\uff0c\u8fce\u9762\u4fbf\u8fc7\u6765\u4e00\u961f\u91d1\u5175\u3002',\n    '\u5e26\u5175\u7684\u5934\u513f\u4e00\u53cc\u4e09\u89d2\u773c\u89d1\u5c06\u8fc7\u53bb\uff0c\u89c1\u90a3\u53f6\u4e09\u59d0\u751f\u5f97\u7f8e\u8c8c\uff0c\u8df3\u4e0b\u9a6c\u6765\uff0c\u5f53\u5373\u4e00\u628a\u62b1\u4f4f\uff0c\u54c8\u54c8\u5927\u7b11\uff0c\u4fbf\u5c06\u5979\u653e\u4e0a\u4e86\u9a6c\u978d\uff0c\u8bf4\u9053\uff1a\u2018\u5c0f\u59d1\u5a18\uff0c\u8ddf\u6211\u56de\u5bb6\uff0c\u670d\u4f8d\u8001\u7237\u3002\u2019',\n    '\u90a3\u53f6\u4e09\u59d0\u5982\u4f55\u80af\u4ece\uff1f\u62fc\u547d\u6323\u624e\u3002',\n    '\u90a3\u91d1\u5175\u957f\u5b98\u559d\u9053\uff1a\u2018\u4f60\u4e0d\u80af\u4ece\u6211\uff0c\u4fbf\u6740\u4e86\u4f60\u7684\u7236\u6bcd\u5144\u5f1f\uff01\u2019',\n    '\u63d0\u8d77\u72fc\u7259\u68d2\uff0c\u4e00\u68d2\u6253\u5728\u90a3\u53f6\u56db\u90ce\u7684\u5934\u4e0a\uff0c\u767b\u65f6\u8111\u6d46\u8ff8\u88c2\uff0c\u4e00\u547d\u545c\u547c\u3002',\n    '\u6b63\u662f\uff1a\u9634\u4e16\u65b0\u6dfb\u6789\u6b7b\u9b3c\uff0c\u9633\u95f4\u4e0d\u89c1\u5c11\u5e74\u4eba\uff01',\n]","4fca968f":"def remove_chinese_punctuation(line):\n  line = re.sub(\"[\uff01\uff1f\u3002\uff61\uff02\uff03\uff04\uff05\uff06\uff07\uff08\uff09\uff0a\uff0b\uff0c\uff0d\uff0f\uff1a\uff1b\uff1c\uff1d\uff1e\uff20\uff3b\uff3c\uff3d\uff3e\uff3f\uff40\uff5b\uff5c\uff5d\uff5e\uff5f\uff60\uff62\uff63\uff64\u3001\u3003\u300a\u300b\u300c\u300d\u300e\u300f\u3010\u3011\u3014\u3015\\\n                \u3016\u3017\u3018\u3019\u301a\u301b\u301c\u301d\u301e\u301f\u3030\u303e\u303f\u2013\u2014\u2018\u2019\u201b\u201c\u201d\u201e\u201f\u2026\u2027\ufe4f.!\\\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]+\", \"\",line) \n  return line","f233806e":"text = [remove_chinese_punctuation(l) for l in (line.strip() for line in text) if l]\nprint(text[:3])","6e108af3":"test = \"\u6211\u559c\u6b22\u51b0\u6fc0\u51cc\" #I like ice-cream\nseg = jieba.cut(test, cut_all=True)\nprint('\/'.join(seg))","001a2abe":"cut_text = ['\/'.join(jieba.cut(str(line), cut_all=False)) for line in text]\nprint(cut_text[-5:-1])","0eb37f1c":"example_dict = ['\u4e09\u89d2\u773c', '\u5317\u4eac']\nfor w in example_dict:\n    jieba.add_word(w)","c166faac":"cut_text = ['\/'.join(jieba.cut(str(line), cut_all=False)) for line in text]\nprint(cut_text[-5:-1])","a4a57a10":"final_text = [cut_line.strip().split('\/') for cut_line in cut_text]\nprint(final_text[:2])\nflat_text = [item for sublist in final_text for item in sublist] #Flatten the text into 1d array","33795f8f":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","15fa9b89":"#TF-IDF\nvectorizer = TfidfVectorizer()\ntf_idf = vectorizer.fit_transform(flat_text)\nidf_list = [(n, idf) for n, idf in zip(vectorizer.get_feature_names(), vectorizer.idf_)]\n#print(\"idf: \", len(idf_list))\n#Show the most similar words, due to the small sample size the result might not be satisfying\ntest = '\u4eba'\ntest_tf_idf = vectorizer.transform([test])\nresult = cosine_similarity(tf_idf, test_tf_idf)\nresult = np.flip(result.ravel().argsort(), 0)[:3]\nprint('Most similar: {}'.format([flat_text[i] for i in result]))","4264dad0":"file = open('..\/input\/english-and-chinese-stopwords\/cn_stopwords.txt')\nstopwords_cn = [line.strip() for line in file if line]\nfile.close()","57c16c2d":"segments = [jieba.cut(str(line), cut_all=False) for line in text]\nremove_stopword = []\nfor segment in segments:\n    curr = ''\n    for word in segment:\n        if word not in stopwords_cn:\n            curr+=word\n    remove_stopword.append(curr) \nfinal = ['\/'.join(jieba.cut(str(line), cut_all=False)) for line in remove_stopword]\nprint(final[:10])","8d8dc67c":"Now we can separate the sentences into pharses using Jieba, firstly with a shorter example:","024a1742":"To process this text, we first need to remove all the Chinese punctuations. Use regular expression and define a function listing all the possible punctuations.","d3178f98":"Now we process the text again:","5dc8cb80":"The sentence successfully separated into three parts, and each part corresponds to \"I like ice-cream\" exactly. Proceed to the previous text:","6690c897":"In the first sentence, stopping words \"\u7684\u201c and \"\u4ece\" has been removed.","8a94af51":"It worked pretty fine. However, looking closely there are some slight errors. \"\u4e09\u89d2\u773c\" should be one word which describe the shape of the eye, but was separated by Jieba. Same thing can happen for names, terminology and special words. We can define a new dictionary for Jieba not to these words:","df6a94ac":"**What to do next?**\n\nSimilar to English, Chinese has a lot of stopping word which we can remove for better preformance. I'm using Kaggle dataset \"english-and-chinese-stopwords\/cn_stopwords.txt\"","dcf4075d":"Processing Chinese Characters is interesting and challengeing. Unlike English where words are separated by space, chinese sentences doesn't contain space to separated the word. Instead, readers manually separate the sentences into small phrases mostly by experience. \nFor example, \"I love apple\" and \u201c\u6211\u559c\u6b22\u82f9\u679c\u201d have the same meaning. We usually call a split function onto the text to separate the sentece into three words in English, while this technique can't be used onto Chinese sentences. \nThis will make NLP in chinese a bit different, but luckily a well-written python library will help. The library is call Jiaba, which means stammer in English.","3e1848e1":"I copied the first few paragraphs from \u300a\u5c04\u96d5\u82f1\u96c4\u4f20\u300b, a famous Wuxiao novel and very entertaining. Full text can be found: https:\/\/www.jiumodiary.com\/, https:\/\/pan.baidu.com\/s\/1mh3lXhe","40c2810b":"\"\u4e09\u89d2\u773c\" no longer gets cut into parts now. With '\/' to separate the words, we can split the sentences and then do NLP processing. Use TF-IDF for example:"}}