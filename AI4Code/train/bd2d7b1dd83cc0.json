{"cell_type":{"e61d37ef":"code","211cec3a":"code","4977c775":"code","a8ad0f54":"code","19669d6b":"code","3d9d86ac":"code","24123028":"code","778b24be":"code","1e455b7a":"code","0b80dc3a":"code","60ef8a12":"code","d4f010cf":"code","548e1bf6":"code","1fbe8a56":"code","692b1090":"code","38435e08":"code","6ffa44ec":"code","fb9561b9":"markdown","222e0f9b":"markdown","214cd489":"markdown","bed811f2":"markdown","0d96bfc8":"markdown","97d7d694":"markdown","8ef1010c":"markdown","dd793090":"markdown"},"source":{"e61d37ef":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np","211cec3a":"image = cv2.imread('..\/input\/gestures-and-actions\/megan.jpg')\nplt.figure(figsize=(15,15))\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));","4977c775":"image.shape, image.shape[0]*image.shape[1]*image.shape[2]","a8ad0f54":"#change order of image.shape to send to NN\nimage_blob = cv2.dnn.blobFromImage(image=image, scalefactor=1.0\/255, size=(image.shape[1], image.shape[0])) #normalise also\nimage_blob.shape\n\n#batchsize, #channels, #dimensions","19669d6b":"#openCv integrated with Caffe\nnetwork = cv2.dnn.readNetFromCaffe('..\/input\/gestures-and-actions\/pose_deploy_linevec_faster_4_stages.prototxt',  #path\n                                   '..\/input\/gestures-and-actions\/pose_iter_160000.caffemodel')  #weights","3d9d86ac":"network.getLayerNames()","24123028":"len(network.getLayerNames())","778b24be":"network.setInput(image_blob)\noutput = network.forward() #image sent to input layer of NN and output at end","1e455b7a":"output.shape #batchsize, confidences, locations of points in image","0b80dc3a":"position_width = output.shape[3]\nposition_height = output.shape[2]","60ef8a12":"num_points = 15 #totally 0-15 points from this NN but here we're not considering point15- background\npoints =[] #locations points in images\nthreshold = 0.1 #only return points where confidence is higher than 10%\n\nfor i in range(num_points):\n    confidence_map = output[0, i, :, :]# 0-first image; i-contains info about detected points\n    #each of the 14 point contains a vector of size 43 which represents confidence levels. So max confidence is considered\n    _, confidence, _, point = cv2.minMaxLoc(confidence_map) #get max confidence value and the point\n\n    #returned coordinates are to be scaled with respect to original image\n    x = int((image.shape[1] * point[0]) \/ position_width)\n    y = int((image.shape[0] * point[1]) \/ position_height)\n\n    if(confidence > threshold):\n        cv2.circle(image, (x, y), 2, (0, 0, 255), thickness = 2)\n        cv2.putText(image, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness=2)\n        points.append((x,y))\n        print(\"Point :\", i, \"\\nConfidence :\", confidence, \"\\nLocation :\", (x,y), \"\\n\")\n    else:\n        points.append(None)","d4f010cf":"plt.figure(figsize=(20, 20))\nplt.axis('off')\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));","548e1bf6":"point_connections =[[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13]]\npoint_connections","1fbe8a56":"for connection in point_connections:\n    pointA = connection[0]\n    pointB = connection[1]\n    if(points[pointA] and points[pointB]):  #if connection exist\n        cv2.line(image, points[pointA], points[pointB], (255,0,0), 1)  ","692b1090":"plt.figure(figsize=(20, 20))\nplt.axis('off')\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));","38435e08":"image2 = cv2.imread('..\/input\/gestures-and-actions\/player.jpg')\nimage_blob2 = cv2.dnn.blobFromImage(image=image2, scalefactor=1.0\/255, size=(image2.shape[1], image2.shape[0])) #normalise also\n\nnetwork.setInput(image_blob2)\noutput2 = network.forward() #image sent to input layer of NN and output at end\n\nposition_width = output2.shape[3]\nposition_height = output2.shape[2]\n\nnum_points = 15 #totally 0-15 points from this NN but here we're not considering point15- background\npoints =[] #locations points in images\nthreshold = 0.1 #only return points where confidence is higher than 10%\n\nfor i in range(num_points):\n    confidence_map = output2[0, i, :, :]# 0-first image; i-contains info about detected points\n    #each of the 14 point contains a vector of size 43 which represents confidence levels. So max confidence is considered\n    _, confidence, _, point = cv2.minMaxLoc(confidence_map) #get max confidence value and the point\n\n    #returned coordinates are to be scaled with respect to original image\n    x = int((image2.shape[1] * point[0]) \/ position_width)\n    y = int((image2.shape[0] * point[1]) \/ position_height)\n\n    if(confidence > threshold):\n        cv2.circle(image2, (x, y), 2, (0, 0, 255), thickness = 2)\n        cv2.putText(image2, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0))\n        cv2.putText(image2, '{}-{}'.format(point[0], point[1]), (x, y+10), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 0, 0))\n        points.append((x,y))\n        #print(\"Point :\", i, \"\\nConfidence :\", confidence, \"\\nLocation :\", (x,y), \"\\n\")\n    else:\n        points.append(None)\n\nplt.figure(figsize=(20, 20))\nplt.axis('off')\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB));","6ffa44ec":"def verify_arms_up(points):\n    head, right_wrist, left_wrist = 0, 0, 0\n    for i, point in enumerate(points):\n        if(i==0):\n            head = point[1]\n        elif (i==4):\n            right_wrist = point[1]\n        elif (i==7):\n            left_wrist = point[1]\n    #print(head, right_wrist, left_wrist)\n\n    if(right_wrist<head and left_wrist<head):\n        return True\n    else:\n        return False\n\nverify_arms_up(points)","fb9561b9":"## Import Libraries","222e0f9b":"# Detect Movements","214cd489":"## Arms Above Head (Image)","bed811f2":"## Predict Body Points","0d96bfc8":"## Load Images","97d7d694":"## Draw Lines Connecting Points","8ef1010c":"# Detect Body Points","dd793090":"## Load pre-trained network\n- Caffe Deep Learning Framework : https:\/\/caffe.berkeleyvision.org\/"}}