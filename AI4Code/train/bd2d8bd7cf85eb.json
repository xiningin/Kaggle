{"cell_type":{"be9d39c9":"code","b3a86b52":"code","f780a4da":"code","de2b9ceb":"code","e825ea67":"code","1b9a053a":"code","4f1f24ba":"code","2788077d":"code","4d06ac86":"code","b0079a7e":"code","114941d1":"code","0823f264":"code","0d00bc11":"code","93567c80":"code","36dc03f3":"markdown"},"source":{"be9d39c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3a86b52":"\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom keras.preprocessing.text import Tokenizer\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.utils.np_utils import to_categorical\nimport copy","f780a4da":"df = pd.read_csv(os.path.join(\"\/kaggle\/input\/sample-github-code\/sample_code.csv\"),lineterminator='\\n')","de2b9ceb":"df.head","e825ea67":"    #Arguments\n    args = {\n    'vocabulary': None,\n    'targets': None,\n    'minexamples': 600,   # Minimum number of samples of a programming language to be included\n    'max_length':300\n}\n    df=copy.deepcopy(df)\n    df['type'] = df['type'].astype('str')\n    df = df.loc[~df['type'].isin(['sublime-snippet', 'xcworkspacedata', 'gitignore', 'plist', 'ts',\n                                                            'project', 'properties', 'conf', 'config', 'cfg', 'po',\n                                                            'meta', 'test', 'gradle', 'patch', 'ebuild', 'ini', 'rst',\n                                                           'csv', 'json', 'txt', 'geojson', 'svg', 'map', 'pgm', 'st',\n                                                            'tpl', 'less', 'cmake', 'mk', 'd', 'llocs', 'am', 'in',\n                                                            'dat','inc','ll','out','pro','Makefile','algn','info',\n                                                            'sln','svn-base','types','xcconfig'])]\n\n    df.loc[df['type'].isin(['cpp', 'cc', 'hpp','hxx']), 'type'] = 'cpp'\n    df.loc[df['type'].isin(['C', 'c', 'h']), 'type'] = 'c'\n    df.loc[df['type'].isin(['erb', 'rb']), 'type'] = 'rb'\n    df.loc[df['type'].isin(['php', 'phpt']), 'type'] = 'php'\n    df.loc[df['type'].isin(['scss', 'css']), 'type'] = 'css'\n    df.loc[df['type'].isin(['html', 'htm']), 'type'] = 'html'\n    df.loc[df['type'].isin(['yaml', 'yml']), 'type'] = 'yml'\n    df.loc[df['type'].isin(['xaml', 'xml']), 'type'] = 'xml'\n    df.loc[df['type'].isin(['R', 'r']), 'type'] = 'r'\n    df.loc[df['type'].isin(['S', 's']), 'type'] = 's'\n    df.loc[df['type'].isin(['pl', 'pm']), 'type'] = 'pl'\n    df.loc[df['type'].isin(['jsx', 'js']), 'type'] = 'js'\n    tmp = df['type'].value_counts().to_frame().reset_index()\n    valid_types = tmp.loc[(tmp['type'] >= args['minexamples'])]\n    # only inlucde samples greater \n    df = df[df['type'].isin(valid_types['index'].values)]","1b9a053a":"df.head","4f1f24ba":"\ndef get_targets():\n    uniqueValues = df.type.unique()\n    args['targets'] = {}\n    for u in uniqueValues:\n        args['targets'][u] = len(args['targets'])\n    print(uniqueValues)\nget_targets()\nprint(args['targets'])\n","2788077d":"def get_data(text, targets):\n  data = []\n  Y = []\n  maxLen = 0\n  for idx,line in enumerate(text):\n    line = list(line.strip())\n    data.append(line)\n    y = args['targets'][targets[idx]]\n\n    y = to_categorical(y, num_classes= len( args['targets']))\n\n    Y.append(y)\n    maxLen = max(maxLen, len(line))\n  return data,Y, maxLen\nwords = 200 # maxium number of chars to keep based on frequency\n# note that tokenize has split function and filter and char_level\ntokenizer = Tokenizer(num_words=words, split=' ', char_level = True, filters = '')\n\ndata, Y, maxLen = get_data(df.content.values, df.type.values)\ntokenizer.fit_on_texts(data)\ntokenizer.word_index\n#input is a list of list of strings\nX = tokenizer.texts_to_sequences(data)","4d06ac86":"# shuffle X and Y\nY = np.array(Y)\nprint(Y.shape)","b0079a7e":"X = pad_sequences(X, maxlen= min(args['max_length'],maxLen))","114941d1":"print((X[0]))\nprint(X.shape)","0823f264":"#create a lstm model\nembed_dim = 128\n\nlstm_out = 196\noutput_dim = len(args['targets'])\n\nmodel = Sequential()\n\nmodel.add(Embedding(words, embed_dim,input_length = X.shape[1]))\nmodel.add(LSTM(lstm_out, dropout=0.2, return_sequences= True))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dense(output_dim,activation='softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","0d00bc11":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\n\nprint(y_train[1])\nprint(X_train[1])\nmodel.fit(X_train, y_train,epochs = 15, batch_size=2000)","93567c80":"model.evaluate(X_test,y_test)","36dc03f3":"\/kaggle\/input\/sample-github-code\/sample_code.csv"}}