{"cell_type":{"0b546d29":"code","23cf72d3":"code","e97ef7b0":"code","902dc581":"code","235575c6":"code","49c446b4":"code","f6a16ce7":"code","d40f50e5":"code","37f924d1":"code","22238cc4":"code","d710f6b0":"code","53ccd044":"code","fd5e9551":"code","16ea0a2d":"code","f94d59b7":"code","1298bcc0":"code","79ee46c4":"code","63aba895":"code","0c0de32c":"code","7ea675ed":"code","31841534":"code","5902b83b":"code","62401208":"code","9f61463e":"code","52327110":"code","ba0e09c8":"code","e22cef82":"code","cb4bf5b0":"code","11f9fca6":"code","c63b5902":"code","3ac502d5":"code","f0b90f7a":"code","a1a58e10":"code","d8a5c844":"code","1ddc70c3":"markdown","06f4a58f":"markdown","55e09c29":"markdown","f6a5cafc":"markdown","2c2efc11":"markdown","2ef5d04d":"markdown","6ae43e1b":"markdown","241a00e1":"markdown","4f53f5ca":"markdown","54f4c3a2":"markdown","34e42803":"markdown","1688e478":"markdown","4af56fa4":"markdown","9ff94eb5":"markdown","f01be809":"markdown","6572349e":"markdown","30b4e34b":"markdown","ab41e64b":"markdown","0d04786c":"markdown","33701ea0":"markdown","d2c0bf28":"markdown","a758f2c1":"markdown","4c88d979":"markdown","ecfee941":"markdown","5b0f10a3":"markdown","088816d6":"markdown","acd90728":"markdown","2e6fa4c2":"markdown","89a97eff":"markdown","22c67039":"markdown","68488a64":"markdown","5c20231d":"markdown","ccbcfb36":"markdown","40c81742":"markdown","173b19be":"markdown","f9a162f1":"markdown","b9c37cb9":"markdown","476b13bc":"markdown","61f7df7c":"markdown","1eced2b2":"markdown"},"source":{"0b546d29":"# Importing Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report, auc\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom collections import Counter\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","23cf72d3":"# Importing the Data\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head(10)","e97ef7b0":"# Name of the Columns of the Dataset\ndf.columns","902dc581":"# Basic Information about the Data\ndf.info()","235575c6":"# Print the Number of Duplicates in the Data\nprint(f'{df.duplicated().sum()} no. of duplicate records are removed among {df.shape[0]} records and the no. of remaining records is {df.drop_duplicates().shape[0]}.')\ndf = df.drop_duplicates()","49c446b4":"# Pie Diagram showing the percentage of Fraudulent and Non-Fraudulent Transactions\nfig, ax = plt.subplots()\nax.pie(df.Class.value_counts(), \n       labels = ['Non-Fraudulent Transaction', 'Fraudulent Transaction'], \n       autopct='%1.1f%%')\n\n# Draw Circle\ncentre_circle = plt.Circle((0,0),0.75,fc = 'white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal Aspect Ratio ensures that Pie is drawn as a Circle\nax.axis('equal')  \nplt.tight_layout()\nplt.show()","f6a16ce7":"print('Summary of the Amount Variable\\n')\nprint(df.Amount.describe())","d40f50e5":"print(f'{df[df.Amount == 0].shape[0]} records found having Transaction Amount 0 have been dropped from the Data out of {df.shape[0]} records.')\ndf = df[df.Amount != 0]","37f924d1":"# Histogram showing the Distribution of Amount\nplt.figure(figsize= [20,10])\nsns.distplot(df.Amount)\nplt.title('Distribution of Amount')\nplt.show()","22238cc4":"# Boxplot of Amount grouped by Class\nplt.figure(figsize = [16,8])\nsns.boxplot(data = df, x = 'Class', y = 'Amount')\nplt.title('Boxplot of Amount grouped by Class')\nplt.show()","d710f6b0":"# Conversion of Time from Second to Hour\ndf['Hour'] = round(df['Time'] \/ (60 * 60)).astype(int)","53ccd044":"# Histogram showing Frequency Distribution of Hour\nplt.figure(figsize = [16, 8])\nsns.distplot(df.Hour)\nplt.title('Frequency Distribution of Hour')\nplt.ylabel('Relative Frequency')\nplt.show()","fd5e9551":"# Line Diagram showing the Relative Frequency Distribution of Time for Fraudulent and Non-Fraudulent Transaction\nplt.figure(figsize = [16, 8])\nsns.distplot(df[df.Class == 0].Hour, hist = False, color = 'blue', label = \"Non-Fraudulent Transaction\")\nsns.distplot(df[df.Class == 1].Hour, hist = False, color = 'red', label = \"Fraudulent Transaction\")\nplt.ylabel('Relative Frequency')\nplt.title('Distribution of Time for Fraudulent and Non-Fraudulent Transaction')\nplt.show()","16ea0a2d":"# Heatmap showing the Correlations between each Pair of Variables\nplt.figure(figsize = [10, 8])\nsns.heatmap(df.corr())\nplt.show()","f94d59b7":"# Few Scatter Plotsto check the Relation between a few Pair of Variables\nfig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = [16,16])\n\nax[0,0].scatter(x = df.V20, y = df.Amount)\nax[0,0].set_xlabel('V20')\nax[0,0].set_ylabel('Amount')\nax[0,0].set_title('Relation between V20 and Amount')\n\nax[0,1].scatter(x = df.V7, y = df.Amount)\nax[0,1].set_xlabel('V7')\nax[0,1].set_ylabel('Amount')\nax[0,1].set_title('Relation between V7 and Amount')\n\nax[1,0].scatter(x = df.V2, y = df.Amount)\nax[1,0].set_xlabel('V2')\nax[1,0].set_ylabel('Amount')\nax[1,0].set_title('Relation between V2 and Amount')\n\nax[1,1].scatter(x = df.V3, y = df.Hour)\nax[1,1].set_xlabel('V3')\nax[1,1].set_ylabel('Hour')\nax[1,1].set_title('Relation between V3 and Hour')\n\nfig.suptitle('Few Scatter Plots')\nplt.show()","1298bcc0":"# Boxplot of Logrithm of Amount grouped by Class\ndf['Log_Amount'] = np.log(df.Amount)\nplt.figure(figsize = [16,8])\nsns.boxplot(data = df, x = 'Class', y = 'Log_Amount')\nplt.title('Boxplot of Logrithm of Amount grouped by Class')\nplt.show()","79ee46c4":"# Function to return Indices of Outliers \ndef indicies_of_outliers(x): \n    Q1, Q3 = x.quantile([0.25, 0.75]) \n    IQR = Q3 - Q1\n    lower_limit = Q1 - (1.5 * IQR)\n    upper_limit = Q3 + (1.5 * IQR)\n    return np.where((x > upper_limit) | (x < lower_limit))[0] ","63aba895":"outliers_indices = list(indicies_of_outliers(df[df['Class'] == 0]['Log_Amount']))\nprint(f'len(outliers_indices) records have been detected as outliers and removed from the data from {df.shape[0]} records.')\ndf = df[[not i for i in df.index.isin(outliers_indices)]]\nplt.figure(figsize = [16,8])\nsns.boxplot(data = df, x = 'Class', y = 'Log_Amount')\nplt.title('Boxplot of Log_Amount grouped by Class')\nplt.show()","0c0de32c":"Y = df.Class\nX = df.drop(['Time', 'Amount', 'Class'], axis = 1)","7ea675ed":"print('For the Original Data \\n'\nf'No. of Record for Fraudulent Transactions : {Counter(Y)[1]} \\n'\nf'No. of Record for Non-Fraudulent Transactions : {Counter(Y)[0]}')","31841534":"Random_Under_Sampling = RandomUnderSampler()\nX_RUS, Y_RUS = Random_Under_Sampling.fit_resample(X, Y)\nprint('After applying Random Under Sampling on the Original Data \\n'\nf'No. of Record for Fraudulent Transactions : {Counter(Y_RUS)[1]} \\n'\nf'No. of Record for Non-Fraudulent Transactions : {Counter(Y_RUS)[0]}')","5902b83b":"Random_Over_Sampling = RandomOverSampler()\nX_ROS, Y_ROS = Random_Over_Sampling.fit_resample(X, Y)\nprint('After applying Random Over Sampling on the Original Data \\n'\nf'No. of Record for Fraudulent Transactions : {Counter(Y_ROS)[1]} \\n'\nf'No. of Record for Non-Fraudulent Transactions : {Counter(Y_ROS)[0]}')","62401208":"SMOTE_ = SMOTE()\nX_SMOTE, Y_SMOTE = SMOTE_.fit_resample(X, Y)\nprint('After applying Synthetic Minority Over-Sampling Technique on the Original Data \\n'\nf'No. of Record for Fraudulent Transactions : {Counter(Y_SMOTE)[1]} \\n'\nf'No. of Record for Non-Fraudulent Transactions : {Counter(Y_SMOTE)[0]}')","9f61463e":"ADASYN_ = ADASYN()\nX_ADASYN, Y_ADASYN = ADASYN_.fit_resample(X, Y)\nprint('After applying Adaptive Synthetic Sampling on the Original Data \\n'\nf'No. of Record for Fraudulent Transactions : {Counter(Y_ADASYN)[1]} \\n'\nf'No. of Record for Non-Fraudulent Transactions : {Counter(Y_ADASYN)[0]}')","52327110":"def Model_Fit(Predictor, Response, Model, Test_Percentage = None,\n              Imbalanced_Class = False, Imbalanced_Classification_Sampling_Technique = None):\n    \n    X, Y = Predictor, Response\n    \n    if Test_Percentage == None:\n        while(True):\n            test_per = input('Please give the percentage of Test Data \\n (Value must be between 0 and 1) : ') \n            if float(test_per) > 0 and float(test_per) < 1:\n                Test_Percentage = test_per\n                break\n            else:\n                continue\n    \n    Models = ['LogisticRegression', 'SVC', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier']\n    Imbalanced_Classification_Sampling_Techniques = ['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'ADASYN']\n    if Model in Models:\n        Model = eval(Model + '()')\n        if Imbalanced_Class:\n            if Imbalanced_Classification_Sampling_Technique in Imbalanced_Classification_Sampling_Techniques:\n                imb_ = eval(Imbalanced_Classification_Sampling_Technique + '()')\n                \n                X_New, Y_New = imb_.fit_resample(X, Y)\n                X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_New, Y_New, test_size = Test_Percentage)\n                \n                model = Model\n                model.fit(X_Train, Y_Train)\n                Y_Pred = model.predict(X_Test)\n                \n                plt.figure(figsize = [8, 6])\n                sns.heatmap(pd.DataFrame(confusion_matrix(Y_Test , Y_Pred)), annot = True, fmt = 'd', cmap = 'Blues')\n                plt.xlabel('Predicted Class')\n                plt.ylabel('Actual Class')\n                plt.title('Confusion Matrix')\n                plt.show()\n                \n                Area_Under_ROC_Curve = roc_auc_score(Y_Test , Y_Pred)\n                \n                fpr, tpr, _ = roc_curve(Y_Test , Y_Pred)\n                plt.figure(figsize = [8, 6])\n                plt.plot(fpr, tpr, 'k-', label = f'AUC : {roc_auc_score(Y_Test , Y_Pred)}')\n                plt.plot([0, 1], [0, 1], 'k--')\n                plt.xlim([-0.05, 1.05])\n                plt.ylim([-0.05, 1.05])\n                plt.xlabel('False Positive Rate')\n                plt.ylabel('True Positive Rate')\n                plt.title('Receiver Operating Characteristic (ROC) Curve')\n                plt.legend(loc = 'lower right')\n                plt.show()\n                \n                return(model, imb_, Area_Under_ROC_Curve)\n        else:\n            X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = Test_Percentage)\n                \n            model = Model\n            model.fit(X_Train, Y_Train)\n            Y_Pred = model.predict(X_Test)\n\n            plt.figure(figsize = [8, 6])\n            sns.heatmap(pd.DataFrame(confusion_matrix(Y_Test , Y_Pred)), annot = True, fmt = 'd', cmap = 'Blues')\n            plt.xlabel('Predicted Class')\n            plt.ylabel('Actual Class')\n            plt.title('Confusion Matrix')\n            plt.show()\n\n            Area_Under_ROC_Curve = roc_auc_score(Y_Test , Y_Pred)\n\n            fpr, tpr, _ = roc_curve(Y_Test , Y_Pred)\n            plt.figure(figsize = [8, 6])\n            plt.plot(fpr, tpr, 'k-', label = f'AUC : {roc_auc_score(Y_Test , Y_Pred)}')\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.xlim([-0.05, 1.05])\n            plt.ylim([-0.05, 1.05])\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title('Receiver Operating Characteristic (ROC) Curve')\n            plt.legend(loc = 'lower right')\n            plt.show()\n\n            return(model, Area_Under_ROC_Curve)\n    else:\n        print('Please Enter a Valid Model Name.')","ba0e09c8":"Models = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier'] # SVC\nIMB_Sampling_Techniques = ['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'ADASYN']\nFinal_Model, Final_IMB, Final_AUC = None, None, 0\nfor Model in Models:\n    for IMB_Sampling_Technique in IMB_Sampling_Techniques:\n        print(f'Model : {Model} \\nImbalanced_Classification_Sampling_Technique : {IMB_Sampling_Technique} \\n\\n')\n        Model_, IMB_, AUC_ = Model_Fit(X, Y, Test_Percentage = 0.25, Model = Model, Imbalanced_Class = True, \n                                       Imbalanced_Classification_Sampling_Technique = IMB_Sampling_Technique)\n        if AUC_ > Final_AUC: Final_Model, Final_IMB, Final_AUC = Model_, IMB_, AUC_","e22cef82":"X = df.drop(['Time', 'Amount', 'Class'], axis = 1)\nY = df.Class","cb4bf5b0":"X_Final, Y_Final = Final_IMB.fit_resample(X, Y)","11f9fca6":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_Final, Y_Final, test_size = 0.25)","c63b5902":"Final_Model.fit(X_Train, Y_Train)","3ac502d5":"Y_Pred = Final_Model.predict(X_Test)","f0b90f7a":"fpr, tpr, _ = roc_curve(Y_Test , Y_Pred)\nplt.figure(figsize = [8, 6])\nplt.plot(fpr, tpr, 'k-', label = f'AUC : {roc_auc_score(Y_Test , Y_Pred)}')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.title('Receiver Operating Characteristic (ROC) Curve', size = 15)\nplt.legend(loc = 'lower right')\nplt.show()","a1a58e10":"plt.figure(figsize = [8, 6])\nsns.heatmap(pd.DataFrame(confusion_matrix(Y_Test , Y_Pred)), annot = True, fmt = 'd', cmap = 'Blues')\nplt.xlabel('Predicted Class', size = 12)\nplt.ylabel('Actual Class', size = 12)\nplt.title('Confusion Matrix', size = 15)\nplt.show()","d8a5c844":"print(\n'For the Final Model, \\n\\n'\nf' Accuracy   :  {accuracy_score(Y_Test , Y_Pred)}\\n'\nf' Precision  :  {precision_score(Y_Test , Y_Pred)}\\n'\nf' Recall     :  {recall_score(Y_Test , Y_Pred)}\\n'\nf' F1 Score   :  {f1_score(Y_Test , Y_Pred)}\\n\\n'\n'and a detail Classification Report is given below: \\n\\n'\nf'{classification_report(Y_Test, Y_Pred, target_names = [\"Non-Fraudulent Transactions (0)\", \"Fraudulent Transactions (1)\"], digits = 8)}'\n)","1ddc70c3":"# Modelling","06f4a58f":"From the above Boxplot, it can be seen that the Highest Amount of Transaction in Non-Fraudulent cases is much higher than the Highest Amount of Transaction in Fraudulent Cases.\n\nHence, the above assumption is not acceptable. That means there is no certain pattern is the Amount variable which may help to mark a certain transaction as Fraudulent.","55e09c29":"### RandomOverSampler","f6a5cafc":"From the above graph, we can see that,\n\n- Within the first $10$ hours, the increment in Non-Fraudulent Transaction is lower compared to Fradulent Transaction.\n- Around the $30^{th}$ hour, there is a fall in the no. of Non-Fraudulent Transaction, but the no. of Fraudulent Transaction is significantly high.","2c2efc11":"# Context\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.","2ef5d04d":"# An Overview of the Data","6ae43e1b":"## Over Sampling\n\n","241a00e1":"# Exploratory Data Analysis","4f53f5ca":"Now, a transaction of amount 0 is meaningless. So, all the records having Transaction Amount 0 should be dropped from the data.","54f4c3a2":"Is there any duplicate value in the data?","34e42803":" There are 2 basic methods to overcome this situation.\n- Under Sampling\n- Over Sampling","1688e478":"First 10 records","4af56fa4":"Now, Let us assume a hypothesis that the Fraudulent Transaction will have a higher amount corresponding to Non-Fraudulent Transactions.\n\n\nNow, Let's see, whether this assumption is right or wrong.","9ff94eb5":"### Adaptive Synthetic Sampling\n\nhttps:\/\/towardsdatascience.com\/adasyn-adaptive-synthetic-sampling-method-for-imbalanced-data-602a3673ba16","f01be809":"So, from the above heatmap, it can be observed that, there may be some significant positive correlation between (Amount, V7) & (Amount, V20) and some significant negative correlation between (Amount, V2) & (Hour, V3).\n\nSo, Let's look at the corresponding scaterplor separately.","6572349e":"Now, as it is already known that the data is highly imbalanced in the context of *Class* variable, so, let's take a look at the percentage of Fraudulent and Non-Fraudulent Transactions in the Class variable.","30b4e34b":"Now, as the *Time* variable is in seconds, so a conversion to hour will be helpful to look at the No. of Transactions over different hours.","ab41e64b":"So, a dedicated function is created below to fit a model to the data, and then this process will be repeated for each pair of Classification Model and Imbalanced Class Handling Technique.","0d04786c":"Now, as we have found that there is outlier in the *Amount* variable so a dedicated function have been created to identify the outliers from the *Log_Amount* variable in terms of Inter-Quartile Range (IQR).","33701ea0":"Now, a new column *Log_Amount* will be introduced by taking the Logarithm of the *Amount* column.\n\n### **Importance of Logarithm**\n\nLogarithm mainly reduces the dynamic range of a variable. Suppose in a Dataset, the value of a variable varies from 0 to 100,000,000. In this situation, the value 10 and the value 1000 will be too close to be considered by a model. The largest value of the feature is so big that it hardly lets the others be seen properly. Logarithm solves this issue as it reduce the dynamic range of the feature while almost preserving the differences.","d2c0bf28":"Now, let's take a look at the correlations between different variables in the dataset.","a758f2c1":"So, from the above scatterplots it can be seen that, there is no significant relationship between those variables.","4c88d979":"# Imbalanced Class Handling","ecfee941":"Basic Informations","5b0f10a3":"### Synthetic Minority Over-Sampling Technique","088816d6":"Name of the Columns","acd90728":"Now, after having a detail insight of the data, it's time to solve the imbalance problem. So, at first, let's look at the no. of records for the different classes in the pre-processed data.","2e6fa4c2":"# Observations from the Data\n\n- The Dataset has 284804 number of records.\n- There is no Null Values (Missing Values) in the Dataset.\n- From the description of the data, it is clear that the columns V1 to V28 contains values obtained from Principal Component Analysis (PCA) applied on the original data. So, the only columns we have to take a closer look are *Amount*, *Time* and *Class*.\n- There is no catagorical variables in the dataset, all the variables are numeric.","89a97eff":"## Final Model","22c67039":"Now, let's see the performance of the best model chosen above.","68488a64":"So, the distribution of the amount variable is highly positive skewed.","5c20231d":"# Description of the Data\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","ccbcfb36":"Now, let's take a look at the Amount Variable.","40c81742":"Finally, the best model have been chosen on the basis of **Area under Curve (AUC)**. As, in this problem a False Positive Prediction will be more harmful then a False Negative Prediction, so, Recall will be a more meaningful metrics in order to compare two models than other metrices. So, instead of Area Under Curve, Recall can also be used here.","173b19be":"Now, let's see the distribution of the Amount Variable.","f9a162f1":"As this is a Classification problem, so there are several of models readily available to apply. Some of them has been chosen as follows:\n- Logistic Regression\n- Support Vector Machine\n- k Nearest Neighbours\n- Decision Tree\n- Random Forest","b9c37cb9":"From the above graph, we can see that, \n\n- The number of transaction is very Low from $3^{rd}$ to $7^{th}$ and $25^{th}$ to $31^{st}$ hour.\n- The number of transaction is very High from $8^{th}$ to $24^{th}$ and $32^{nd}$ to $48^{th}$ hour.","476b13bc":"So, it can be observed that there is no outlier in Fraudulent Transactions though presence of outlies can be noticed in Non-Fraudulent Transactions, in terms of log of *Amount*.","61f7df7c":"### RandomUnderSampler","1eced2b2":"## Under Sampling\n\n"}}