{"cell_type":{"c9ebe1e0":"code","836abe1a":"code","212f150f":"code","a4d868df":"code","1142f7a1":"code","296fe531":"code","52fb11d2":"code","a9a134eb":"code","6b0d686d":"code","572ba232":"code","12f4a828":"code","65523031":"code","dd1b6200":"code","601f0a22":"code","9ea91928":"code","9d68a368":"code","d74b2b4e":"code","4b5627cb":"code","c3225b36":"code","e29acf7b":"markdown","2773f10d":"markdown","dc9db246":"markdown"},"source":{"c9ebe1e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","836abe1a":"bank_note_data=pd.read_csv('\/kaggle\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv')","212f150f":"import pathlib, os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nprint(tf.__version__)","a4d868df":"bank_note_data.head()","1142f7a1":"bank_note_data.info()","296fe531":"bank_note_data.describe()","52fb11d2":"bank_note_data.hist(figsize=(20,10), grid =False, layout = (2,4), bins=30)","a9a134eb":"plt.figure(figsize = (20,7))\nsns.swarmplot(x = 'class', y = 'curtosis', data = bank_note_data, hue = 'class')\nsns.violinplot(x = 'class', y = 'curtosis', data = bank_note_data)","6b0d686d":"#Standardize rows into uniform scale.\n\nX= bank_note_data.drop(['class'], axis=1)\ny= bank_note_data['class']\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\n\n#Scale and centre the data\n\nbank_note_data_normalized = scaler.transform(X)\n\n#Create a pandas dataframe\n\nbank_note_data_normalized = pd.DataFrame(data = X, index= X.index, columns= X.columns)","572ba232":"bank_note_data_normalized.describe()","12f4a828":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0,stratify=bank_note_data['class'])","65523031":"X_train.shape","dd1b6200":"X_test.shape","601f0a22":"y_train.shape","9ea91928":"y_test.shape","9d68a368":"#Create a Sequential model\nfrom keras.layers import Dropout\nmodel = tf.keras.Sequential()\nmodel.add(Dropout(0.2, input_shape=(4,)))\nmodel.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n\n#Output Layer\nmodel.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\n#Create a Keras version Optimiser\noptimizer = tf.keras.optimizers.Adam()\n\n#Compile and print the summary of model\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics =['accuracy'])\n\n# Model summary can be created by calling the summary() function on the model that returns a string that in turn can be printed.\nmodel.summary()","d74b2b4e":"#Plot model summary\n\ntf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","4b5627cb":"fitted_model = model.fit(\n        X_train, y_train,\n        epochs=50  \n        )","c3225b36":"eval = model.evaluate(X_test, y_test, verbose=0) \nprint(\"\\nLoss, accuracy on test data: \")\nprint(\"%0.4f %0.2f%%\" % (eval[0],eval[1]*100))","e29acf7b":"This concludes my code on bank note prediction. As can be seen, we have got an accuracy of 100%.","2773f10d":"The data is not on the same scale as can be concluded from above.","dc9db246":"The above concludes that we have 1372 rows and all are not null."}}