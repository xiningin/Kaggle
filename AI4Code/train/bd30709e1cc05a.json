{"cell_type":{"636d3e5d":"code","52d0f7ae":"code","eebe1c42":"code","1eee7acf":"code","af4e23b7":"code","259a4b87":"code","aea4dd5b":"code","605f0c03":"code","29941269":"code","4bbf6c6c":"code","4d0dd98b":"code","10c53301":"code","f5c9cde8":"code","21b1ce27":"code","ee75fad3":"code","e204a972":"code","880b391c":"markdown","0b455283":"markdown","7f2092c1":"markdown","3e5ccd0d":"markdown","f84b0f0c":"markdown","b59b3a2f":"markdown","12b8558b":"markdown","5893d5c1":"markdown","47aa5668":"markdown","4969d842":"markdown"},"source":{"636d3e5d":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nimport random, math, cv2\nfrom tqdm import tqdm_notebook as tqdm\n# from tqdm import tqdm\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Compose, ToTensor\n\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings(\"ignore\")","52d0f7ae":"MNIST_DATA_DIR = Path('\/kaggle\/working')\nBSDS_DATA_DIR = Path('..\/input\/berkeley-segmentation-dataset-500-bsds500')\nMODEL_FILE = Path('..\/input\/pretrain-source-model-for-domain-adaptation-mnist\/best_source_weights_mnist.pth')\n\nbatch_size = 64\nepochs = 15\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","eebe1c42":"def visualize_digits(dataset, k=80, mnistm=False, cmap=None, title=None):\n    \n    ncols = 20\n    indices = random.choices(range(len(dataset)), k=k)\n    nrows = math.floor(len(indices)\/ncols)\n    \n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols,nrows+0.4), gridspec_kw=dict(wspace=0.1, hspace=0.1), subplot_kw=dict(yticks=[], xticks=[]))\n    axes_flat = axes.reshape(-1)\n    fig.suptitle(title, fontsize=20)\n    \n    for list_idx, image_idx in enumerate(indices[:ncols*nrows]):\n        ax = axes_flat[list_idx]\n        image = dataset[image_idx][0]\n        image = image.numpy().transpose(1, 2, 0)\n        ax.imshow(image, cmap=cmap)\n\ndef set_requires_grad(model, requires_grad=True):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\nclass GrayscaleToRgb:\n    \"\"\"Convert a grayscale image to rgb\"\"\"\n    def __call__(self, image):\n        image = np.array(image)\n        image = np.dstack([image, image, image])\n        return Image.fromarray(image)","1eee7acf":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 10, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(10, 20, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.Dropout2d(),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(320, 50),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(50, 10),\n            nn.LogSoftmax(),\n        )\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        features = features.view(x.shape[0], -1)\n        logits = self.classifier(features)\n        return logits","af4e23b7":"class GradientReversalFunction(Function):\n    \"\"\"\n    Gradient Reversal Layer from:\n    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n    Forward pass is the identity function. In the backward pass,\n    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n\n    @staticmethod\n    def backward(ctx, grads):\n        lambda_ = ctx.lambda_\n        lambda_ = grads.new_tensor(lambda_)\n        dx = -lambda_ * grads\n        return dx, None\n\n\nclass GradientReversal(torch.nn.Module):\n    def __init__(self, lambda_=1):\n        super(GradientReversal, self).__init__()\n        self.lambda_ = lambda_\n\n    def forward(self, x):\n        return GradientReversalFunction.apply(x, self.lambda_)","259a4b87":"model = Net().to(device)\nmodel.load_state_dict(torch.load(MODEL_FILE, map_location=device))\nfeature_extractor = model.feature_extractor\nclf = model.classifier\n\ndiscriminator = nn.Sequential(\n    GradientReversal(),\n    nn.Linear(320, 50),\n    nn.ReLU(),\n    nn.Linear(50, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n).to(device)","aea4dd5b":"class BSDS500(Dataset):\n\n    def __init__(self):\n        image_folder = BSDS_DATA_DIR \/ 'images'\n        self.image_files = list(map(str, image_folder.glob('*\/*.jpg')))\n\n    def __getitem__(self, i):\n        image = cv2.imread(self.image_files[i], cv2.IMREAD_COLOR)\n        tensor = torch.from_numpy(image.transpose(2, 0, 1))\n        return tensor\n\n    def __len__(self):\n        return len(self.image_files)\n\n\nclass MNISTM(Dataset):\n\n    def __init__(self, train=True):\n        super(MNISTM, self).__init__()\n        self.mnist = datasets.MNIST(MNIST_DATA_DIR \/ 'mnist', train=train,\n                                    download=True)\n        self.bsds = BSDS500()\n        # Fix RNG so the same images are used for blending\n        self.rng = np.random.RandomState(42)\n\n    def __getitem__(self, i):\n        digit, label = self.mnist[i]\n        digit = transforms.ToTensor()(digit)\n        bsds_image = self._random_bsds_image()\n        patch = self._random_patch(bsds_image)\n        patch = patch.float() \/ 255\n        blend = torch.abs(patch - digit)\n        return blend, label\n\n    def _random_patch(self, image, size=(28, 28)):\n        _, im_height, im_width = image.shape\n        x = self.rng.randint(0, im_width-size[1])\n        y = self.rng.randint(0, im_height-size[0])\n        return image[:, y:y+size[0], x:x+size[1]]\n\n    def _random_bsds_image(self):\n        i = self.rng.choice(len(self.bsds))\n        return self.bsds[i]\n\n    def __len__(self):\n        return len(self.mnist)","605f0c03":"half_batch = batch_size \/\/ 2\nsource_dataset = MNIST(MNIST_DATA_DIR\/'mnist', train=True, download=True, transform=Compose([GrayscaleToRgb(), ToTensor()]))\nsource_loader = DataLoader(source_dataset, batch_size=half_batch, shuffle=True, num_workers=1, pin_memory=True)\n\ntarget_train_dataset, target_test_dataset = MNISTM(train=True), MNISTM(train=False)\ntarget_train_loader = DataLoader(target_train_dataset, batch_size=half_batch, shuffle=True, num_workers=16, pin_memory=True)\ntarget_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n\noptim = torch.optim.Adam(list(discriminator.parameters()) + list(model.parameters()))","29941269":"visualize_digits(dataset=target_train_dataset, k=200, mnistm=True, title='Sample MNIST-M Images')","4bbf6c6c":"visualize_digits(dataset=source_dataset, k=120, cmap='gray', title='Sample MNIST Images')","4d0dd98b":"domain_losses, domain_accuracies, domain_train_counter = [], [], []\nlabel_losses, label_accuracies = [], []\ntest_losses, test_accuracies = [], []\ntest_counter = [idx*len(target_train_loader.dataset) for idx in range(0, epochs+1)]","10c53301":"# Initial Testing\ntest_loss = test_accuracy = 0\nmodel.feature_extractor = feature_extractor\nmodel.classifier = clf\nmodel.eval()\ntqdm_bar = tqdm(target_test_loader, desc=f'Testing ', total=int(len(target_test_loader)))\nfor batch_idx, (images, labels) in enumerate(tqdm_bar):\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        outputs = model(images)\n        loss = F.cross_entropy(outputs, labels)\n    test_loss += loss.item()\n    test_accuracy = (outputs.max(1)[1] == labels).float().mean().item()\n    tqdm_bar.set_postfix(test_loss=(test_loss\/(batch_idx+1)), test_accuracy=test_accuracy\/(batch_idx+1))\ntest_losses.append(test_loss\/len(target_test_loader))\ntest_accuracies.append(test_accuracy\/len(target_test_loader))\n","f5c9cde8":"for epoch in range(epochs):\n    batches = zip(source_loader, target_train_loader)\n    n_batches = min(len(source_loader), len(target_train_loader))\n\n    total_domain_loss = total_domain_accuracy = 0\n    total_label_loss = total_label_accuracy = 0\n    tqdm_bar = tqdm(batches, desc=f'Training Epoch {epoch} ', total=n_batches)\n    for batch_idx, ((source_x, source_labels), (target_x, _)) in enumerate(tqdm_bar):\n        x = torch.cat([source_x, target_x])\n        x = x.to(device)\n        domain_y = torch.cat([torch.ones(source_x.shape[0]), torch.zeros(target_x.shape[0])])\n        domain_y = domain_y.to(device)\n        label_y = source_labels.to(device)\n\n        features = feature_extractor(x).view(x.shape[0], -1)\n        domain_preds = discriminator(features).squeeze()\n        label_preds = clf(features[:source_x.shape[0]])\n\n        domain_loss = F.binary_cross_entropy_with_logits(domain_preds, domain_y)\n        label_loss = F.cross_entropy(label_preds, label_y)\n        loss = domain_loss + label_loss\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        total_domain_loss += domain_loss.item()\n        domain_losses.append(domain_loss.item())\n        domain_accuracy = ((domain_preds > 0).long() == domain_y.long()).float().mean().item()\n        total_domain_accuracy += domain_accuracy\n        domain_accuracies.append(domain_accuracy)\n        \n        total_label_loss += label_loss.item()\n        label_losses.append(label_loss.item())\n        label_accuracy = (label_preds.max(1)[1] == label_y).float().mean().item()\n        total_label_accuracy += label_accuracy\n        label_accuracies.append(label_accuracy)\n        domain_train_counter.append(batch_idx*batch_size + source_x.size(0) + target_x.size(0) + epoch*min(len(target_train_dataset),len(source_dataset)))\n        tqdm_bar.set_postfix(domain_loss=(total_domain_loss\/(batch_idx+1)), domain_accuracy=total_domain_accuracy\/(batch_idx+1),\n                             label_loss=(total_domain_loss\/(batch_idx+1)), label_accuracy=total_label_accuracy\/(batch_idx+1))\n        \n    # Testing feature_extractor+clf\n    test_loss = test_accuracy = 0\n    model.feature_extractor = feature_extractor\n    model.classifier = clf\n    model.eval()\n    tqdm_bar = tqdm(target_test_loader, desc=f'Testing ', total=int(len(target_test_loader)))\n    for batch_idx, (images, labels) in enumerate(tqdm_bar):\n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = model(images)\n            loss = F.cross_entropy(outputs, labels)\n        test_loss += loss.item()\n        test_accuracy = (outputs.max(1)[1] == labels).float().mean().item()\n        tqdm_bar.set_postfix(test_loss=(test_loss\/(batch_idx+1)), test_accuracy=test_accuracy\/(batch_idx+1))\n    test_losses.append(test_loss\/len(target_test_loader))\n    test_accuracies.append(test_accuracy\/len(target_test_loader))\n    if np.argmax(test_accuracies) == len(test_accuracies)-1:\n        torch.save(model.state_dict(), 'revgrad_target_weights.pth')\n","21b1ce27":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=domain_train_counter, y=domain_losses, mode='lines', name='Domain Loss'), secondary_y=False)\nfig.add_trace(go.Scatter(x=domain_train_counter, y=domain_accuracies, mode='lines', name='Domain Accuracy', line_color='lightseagreen'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Domain Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Domain <b>Loss<\/b> (BCE)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Domain <b>Accuracy<\/b>\", secondary_y=True)\nfig.show()","ee75fad3":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=np.asarray(domain_train_counter)\/\/2, y=label_losses, mode='lines', name='Domain Loss'), secondary_y=False)\nfig.add_trace(go.Scatter(x=np.asarray(domain_train_counter)\/\/2, y=label_accuracies, mode='lines', name='Domain Accuracy', line_color='lightseagreen'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Domain Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Domain <b>Loss<\/b> (BCE)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Domain <b>Accuracy<\/b>\", secondary_y=True)\nfig.show()","e204a972":"fig = plotly.subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses, marker_symbol='star-diamond', \n                         marker_line_color=\"orange\", marker_line_width=1, marker_size=9, mode='lines+markers', \n                         name='Target Accuracy'), secondary_y=False)\nfig.add_trace(go.Scatter(x=test_counter, y=test_accuracies, marker_symbol='star-square', \n                         marker_line_color=\"lightseagreen\", marker_line_width=1, marker_size=9, mode='lines+markers',\n                         name='Target Loss'), secondary_y=True)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Full Target Model Loss vs Accuracy\")\nfig.update_xaxes(title_text=\"Number of training examples seen\")\nfig.update_yaxes(title_text=\"Target <b>Loss<\/b> (NLLLoss)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Target <b>Accuracy<\/b>\", secondary_y=True)\nfig.show()","880b391c":"### Visualize MNIST-M & MNIST Data \ud83d\uddbc\ufe0f","0b455283":"### Define Model","7f2092c1":"## Introduction\n\n### In this notebook, we use [Unsupervised Domain Adaptation by Backpropagation](http:\/\/proceedings.mlr.press\/v37\/ganin15.pdf) technique to perform Domain Adaptation on MNIST-M (target dataset) using pre-trained source model trained on MNIST.\n### [[Pre-training Source Model on MNIST dataset](https:\/\/www.kaggle.com\/balraj98\/pretrain-source-model-for-domain-adaptation-mnis)]\n\n<h3><center>Domain Adaptation Task<\/center><\/h3>\n<img src=\"https:\/\/media.arxiv-vanity.com\/render-output\/3708497\/x1.png\" width=\"500\" height=\"500\"\/>\n<h4><\/h4>\n<h4><center>Image Source: <a href=\"https:\/\/arxiv.org\/abs\/1702.05464\"> Adversarial Discriminative Domain Adaptation [E. Tzeng et al.]<\/a><\/center><\/h4>","3e5ccd0d":"### Visualize Training & Testing Results \ud83d\udcc8","f84b0f0c":"### Get Datasets & Dataloaders","b59b3a2f":"### Define Dataset Classes","12b8558b":"### Work in Progress ...","5893d5c1":"### Unsupervised Domain Adaptation by Backpropagation","47aa5668":"<h3><center>Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/609\/1*piqiA10qR6b7XpCNVi2eRQ.png\" width=\"750\" height=\"750\"\/>\n<h4><\/h4>\n<h4><center>Image Source:  <a href=\"http:\/\/proceedings.mlr.press\/v37\/ganin15.pdf\">Unsupervised Domain Adaptation by Backpropagation [Y. Ganin et al.]<\/a><\/center><\/h4>","4969d842":"### Libraries \ud83d\udcda\u2b07"}}