{"cell_type":{"cf81a652":"code","2deabb4d":"code","348da5b6":"code","f3f12971":"code","a464377d":"code","2c7acef4":"code","b34fad5c":"code","031086e4":"code","3de80b1a":"code","9aa904c8":"code","16ae5f48":"code","835dcd6d":"code","062a0019":"code","e62ae659":"code","51affac5":"code","60708e36":"code","36cf28d9":"code","3dff53f7":"code","4afa2901":"code","77fd63d9":"code","2abca728":"code","d8705632":"code","c3bd75a9":"code","7625fb42":"code","e35720c2":"code","fdc786cd":"code","ba91e385":"code","3b4c77e8":"code","da4ac191":"code","52b56b48":"code","2f56373e":"code","196d493b":"code","88d4a3a9":"code","33a3a2ba":"code","520ffed5":"code","c18b44ad":"code","fe5f1b18":"code","c19d4d2a":"code","21954321":"code","476ebbd0":"markdown","008b81c5":"markdown","7dd4fb5f":"markdown","a7dec9b9":"markdown","037ec942":"markdown","d829a0c7":"markdown","bd07e924":"markdown","aacf9053":"markdown","b19b1e8b":"markdown","c62500f4":"markdown","438c5cb8":"markdown","b1d9caf4":"markdown","862515a6":"markdown","79f0a510":"markdown","4ceb97b3":"markdown"},"source":{"cf81a652":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2deabb4d":"import pandas as pd\nimport seaborn as sns","348da5b6":"train_df = pd.read_csv('..\/input\/detect-the-sentiments\/train_2kmZucJ.csv')\ntest_df = pd.read_csv('..\/input\/detect-the-sentiments\/test_oJQbWVk.csv')","f3f12971":"train_df.head()","a464377d":"test_df.head()","2c7acef4":"train_df['tweet'][0]","b34fad5c":"train_df['tweet'][7919]","031086e4":"text = train_df['tweet']\ntext","3de80b1a":"test_text = test_df['tweet']\ntest_text","9aa904c8":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \n","16ae5f48":"wordnet = WordNetLemmatizer ()\ncorpus = []","835dcd6d":"for i in range(0,len(text)):\n    review = re.sub('[^a-zA-Z]',' ',text[i])  #re.sub(pattern, repl, string) - substituting characters apart from a-z,A-Z with blank space\n    review = review.lower()\n    review = review.split()#splitting the words\n    \n    review = [wordnet.lemmatize(word)  for word in review if not word in stopwords.words('english')] #for words not in stopwrods we are obtaining the root word of the words throguh stemmer(eg. preference - prefer)\n    review = ' '.join(review)\n    corpus.append(review)\n    \n    ","062a0019":"test_corpus = []\nfor i in range(0,len(test_text)):\n    review = re.sub('[^a-zA-Z]',' ',text[i])  #re.sub(pattern, repl, string) - substituting characters apart from a-z,A-Z with blank space\n    review = review.lower()\n    review = review.split()#splitting the words\n    \n    review = [wordnet.lemmatize(word)  for word in review if not word in stopwords.words('english')] #for words not in stopwrods we are obtaining the root word of the words throguh stemmer(eg. preference - prefer)\n    review = ' '.join(review)\n    test_corpus.append(review)\n","e62ae659":"corpus[2]","51affac5":"test_corpus[10]","60708e36":"from sklearn.feature_extraction.text import TfidfVectorizer","36cf28d9":"tfidf_v = TfidfVectorizer(max_features=5000,ngram_range=(1,3))\nX = tfidf_v.fit_transform(corpus).toarray()","3dff53f7":"y = train_df['label']","4afa2901":"test = tfidf_v.fit_transform(test_corpus).toarray()","77fd63d9":"X.shape","2abca728":"from sklearn.model_selection import train_test_split","d8705632":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","c3bd75a9":"tfidf_v.get_feature_names()[:20] #this is for the training text #the top 20 words","7625fb42":"tfidf_v.get_params() #for the training text","e35720c2":"count_df = pd.DataFrame(X_train,columns=tfidf_v.get_feature_names())","fdc786cd":"count_df","ba91e385":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix,f1_score","3b4c77e8":"classifier = MultinomialNB()\nclassifier.fit(X_train,y_train)\n","da4ac191":"pred = classifier.predict(X_test)","52b56b48":"sns.heatmap(confusion_matrix(y_test,pred),annot = True)","2f56373e":"f1_score(y_test,pred)","196d493b":"from sklearn.linear_model import PassiveAggressiveClassifier","88d4a3a9":"lnr_classifier = PassiveAggressiveClassifier(n_iter_no_change=50)\nlnr_classifier.fit(X_train,y_train)\npred2 = lnr_classifier.predict(X_test)","33a3a2ba":"sns.heatmap(confusion_matrix(y_test,pred2),annot=True)","520ffed5":"f1_score(y_test,pred2)","c18b44ad":"pred_final = classifier.predict(test)","fe5f1b18":"submission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['label'] = pred_final","c19d4d2a":"submission","21954321":"submission['label'].value_counts()","476ebbd0":"Lemmatization is more preferrable than stemming since it preserves the semantic part of the word.","008b81c5":"![image.png](attachment:image.png)\n","7dd4fb5f":"Since we had 7920 tweets in total and 5000 max features,  hence forms an array of shape - (7920,5000).","a7dec9b9":"# -Analytics Vidhya Hackathon","037ec942":"Lets do the text preprocessing.","d829a0c7":"![image.png](attachment:image.png)","bd07e924":"Now lets predict for the test data.","aacf9053":"https:\/\/datahack.analyticsvidhya.com\/contest\/linguipedia-codefest-natural-language-processing-1\/#About","b19b1e8b":"# MultiNomial NB","c62500f4":"# Passive Agressive Classifier","438c5cb8":"MultinomialNB is giving a better performance . Lets predict for the testing data using MultiNomialNb.","b1d9caf4":"Lets look at some random tweets.","862515a6":"Given the problem statement the 0 label represent that the tweet doesn't have a negative sentiment towards the product or comapany and Label 1 represents that the tweet has a negative sentiment.","79f0a510":"**Lets split the data into training and testing.**","4ceb97b3":"Looks great! This dataframe represents how the tf-idf vectors are formed."}}