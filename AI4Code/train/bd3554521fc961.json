{"cell_type":{"46b955e4":"code","e0bb5dc0":"code","da25ae56":"code","5111e9e8":"code","ca188f11":"code","cb87cc4c":"code","c59207c2":"code","70e3512d":"code","a3f270ed":"code","acb574c1":"code","543313c9":"code","8d11563a":"code","d1bfb5d2":"code","2574031e":"code","2e4235e1":"code","bc5d22fa":"code","4ba5228d":"code","9584ab00":"code","66d79095":"code","fe133ad1":"code","5d5435b7":"code","9b390b34":"code","efad6778":"code","52646751":"code","fd99cba3":"code","fd46b706":"code","36da1d60":"code","6e3cdea6":"code","2037dc51":"code","9949f36d":"code","6314034b":"code","a6f13ad5":"code","044128f2":"code","e9b6e995":"markdown","1dcbacb3":"markdown","2d7ddf56":"markdown","73e9465d":"markdown","6d79ad18":"markdown","a973ba93":"markdown","afcf8f10":"markdown","3258841f":"markdown","26202ce5":"markdown","fd4d1933":"markdown","d2747a45":"markdown","d27e23fa":"markdown","54c8e31c":"markdown","c8d1bb5f":"markdown","ee5fe65a":"markdown","b321e570":"markdown","e13f755f":"markdown"},"source":{"46b955e4":"import sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","e0bb5dc0":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics as mt\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\n\n\nplt.style.use('seaborn-muted')\npd.options.display.max_columns = None","da25ae56":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go","5111e9e8":"df = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\ndf.head()","ca188f11":"print(df.shape)","cb87cc4c":"df[['PATIENT_VISIT_IDENTIFIER','WINDOW','ICU']] .head(10)","c59207c2":"# Print list of all the columns\nprint(list(df.columns))","70e3512d":"df.groupby('PATIENT_VISIT_IDENTIFIER',as_index=False).agg({'WINDOW':list, 'ICU':list})[10:15]","a3f270ed":"# Grouped by each unique patient\ndfg = df.groupby(['PATIENT_VISIT_IDENTIFIER'], as_index=False).sum()\n# now ICU columns contains sum of ICU by pateint \n# if patient go ICU in first window ICU = 5 (i.e. 1 1 1 1 1 ) \n# if patient go ICU in third window ICU = 2 (i.e. 0 0 1 1 1 ) \n#...\n# if patient never go to  in  window ICU = 0 (i.e. 0 0 0 0 0 ) \n\n# now get complement of ICU by subtract from 5 \ndfg['ICU'] = 5- dfg.ICU \n#ICU  become as following:\n# if patient go ICU in first window ICU = 0\n# if patient go ICU in third window ICU = 3 (i.e. 0 0 1 1 1 ) \n#...\n# if patient never go to  in  window ICU = 5 (i.e. 0 0 0 0 0 ) \n\n\nwhen_patient_ICU = dfg [['PATIENT_VISIT_IDENTIFIER','ICU']]\nwhen_patient_ICU.columns= ['p','pos']\nwhen_patient_ICU.index = when_patient_ICU.p\n\nwhen_patient_ICU.head(15)","acb574c1":"\nwhen_patient_ICU_summary= when_patient_ICU.pos.value_counts()\nplt.figure(figsize=(10,7))\n\nwhen_patient_ICU_summary.sort_index().plot.bar()\n\nplt.xticks(np.arange(6), [ '0-2', '2-4', '4-6', '6-12', 'ABOVE_12', 'Never' ])\n\nplt.ylabel('No of Patiens')\nplt.xlabel('Time (window No.) when patient is taken to ICU')\nplt.show()","543313c9":"#very simple approach for imputing\n#fill forward (ffill) and backword (bfill) per patent \n\ndata = df.groupby('PATIENT_VISIT_IDENTIFIER', as_index=False)\\\n    .fillna(method='ffill')\\\n    .fillna(method='bfill')\n#added as PATIENT_VISIT_IDENTIFIER removed during grouping  \ndata['PATIENT_VISIT_IDENTIFIER'] = df.PATIENT_VISIT_IDENTIFIER\ndata.head(10)","8d11563a":"\nfor p in data.PATIENT_VISIT_IDENTIFIER.unique():\n    #get start of patent index\n    start_p_index= data[data.PATIENT_VISIT_IDENTIFIER==p].index[0]\n    #get last patent ICU status \n    p_last_ICU= data.loc[start_p_index+4].ICU\n    #set last patent ICU status to all rows target \n    data.loc[data.PATIENT_VISIT_IDENTIFIER == p, 'target'] = p_last_ICU\ndata[ 'target']= data[ 'target'].astype(int)\ndata[['PATIENT_VISIT_IDENTIFIER','WINDOW','ICU', 'target']][10:20]\n","d1bfb5d2":"# import pandas library  \nimport pandas as pd \n  \n# This function take a dataframe \n# as a parameter and returning list \n# of column names whose contents are identical ( duplicates) such as : . \ndef get_identical_features(df): \n  \n    # Create an empty set \n    duplicateColumnNames = set() \n      \n    # Iterate through all the columns  \n    # of dataframe \n    for x in range(df.shape[1]): \n          \n        # Take column at xth index. \n        col = df.iloc[:, x] \n          \n        # Iterate through all the columns in \n        # DataFrame from (x + 1)th index to \n        # last index \n        for y in range(x + 1, df.shape[1]): \n              \n            # Take column at yth index. \n            otherCol = df.iloc[:, y] \n              \n            # Check if two columns at x & y \n            # index are equal or not, \n            # if equal then adding  \n            # to the set \n            if col.equals(otherCol) and df.columns.values[y] not in duplicateColumnNames : \n                duplicateColumnNames.add(df.columns.values[y]) \n                #print(df.columns.values[x] ,df.columns.values[y])\n                  \n    # Return list of unique column names  \n    # whose contents are duplicates. \n    return list(duplicateColumnNames) \n    \n#remove zero variance ( contan only one value) features\ndef get_zero_variance_features(exp_data):    \n    remove_features=[]\n    for c in exp_data.columns: \n        if len( data[c].value_counts()) <2:\n            remove_features.append(c) \n    return remove_features\n\ndef remove_high_corrolated_features(df,threshold=0.5 ):\n    \n    #get co variance of data\n    coVar = df.corr() # or df.corr().abs()\n    #threshold = 0.5 # \n    \n    \"\"\"\n    1. .where(coVar != 1.0) set NaN where col and index is 1\n    2. .where(coVar >= threshold) if not greater than threshold set Nan\n    3. .fillna(0) Fill NaN with 0\n    4. .sum() convert data frame to serise with sum() and just where is co var greater than threshold sum it\n    5. > 0 convert all Series to Boolean\n    \"\"\"\n    coVarCols = coVar.where(coVar != 1.0).where(coVar >=threshold).fillna(0).sum() > 0\n    #print(coVar.where(coVar != 1.0).where(coVar >=threshold))\n    # Not Boolean Becuase we need to delete where is co var greater than threshold \n\n    #print('High corrolated features:',list(coVarCols[coVarCols].index))\n    \n    coVarCols = ~coVarCols\n    \n    # get where you want\n    columns_to_keep = list(coVarCols[coVarCols].index)\n    columns_to_keep.extend( list(df.dtypes[df.dtypes == object].index )) \n    return df[columns_to_keep]\n\ndef corrMatrix(df,fileName='corrMatrix',include_target=False ,annot=False ):\n    import seaborn as sn\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(20,20))\n    if include_target:\n        columns = [x for x in  df.columns if x not in ['ICU','PATIENT_VISIT_IDENTIFIER'] ]\n    else:\n        columns = [x for x in  df.columns if x not in ['target','ICU','PATIENT_VISIT_IDENTIFIER'] ]\n\n    \n    corrMatrix = df[columns].corr()\n    sn.heatmap(corrMatrix, annot=annot)\n    plt.savefig(fileName+'.png')\n    return corrMatrix\n","2574031e":"#remove identical features  and save file data & one value columns\n\n\n#remove zero variance column\none_value_columns = get_zero_variance_features(data)\nprint('one_value_columns to remove:',len(one_value_columns))\n#print('one_value_columns:',one_value_columns)\ndata.drop(columns = one_value_columns, inplace = True)\n\n#data includes  'WINDOW', 'ICU','PATIENT_VISIT_IDENTIFIER', 'target'\n#should subtract no. of features from 4: features without idntifiers and status \n\nprint(' after remove one_value_columns (-4)', data.shape) \n\n#remove columns have same value \nduplicateColNames = get_identical_features(data) \nprint('No. of Duplicate Columns are :', len(duplicateColNames)) \n#print('Duplicate Columns are :',duplicateColNames) \n\ndata = data.drop(columns = duplicateColNames) \nprint('Data After drop  duplicateCol are (-4):', data.shape)\n\n#remmove columns with corrolations more than 70%\ncorrMatrix(data) # requres to remove target from this digram\ndata = remove_high_corrolated_features(data ,0.7)\nprint(' after remove remove_coVarCols(-4) :', data.shape)\n#print(' after remove remove_coVarCols', data.columns)\n#data.to_csv('data_remove_duplicate_columns.csv')\n","2e4235e1":"# get the quantitive features from data to apply caculation on it  \nqualititive_features= ['PATIENT_VISIT_IDENTIFIER', 'AGE_ABOVE65', 'GENDER',\n       'DISEASE GROUPING 1', 'DISEASE GROUPING 2', 'DISEASE GROUPING 3',\n       'DISEASE GROUPING 4', 'DISEASE GROUPING 5', 'DISEASE GROUPING 6', 'HTN',\n       'IMMUNOCOMPROMISED', 'OTHER', 'ICU', 'target',\n       'AGE_PERCENTIL', 'WINDOW'] \nquantitive_features = list(set(data.columns) - set(qualititive_features) )\nquantitive_features","bc5d22fa":"def get_mean_std(ICU_stat_data):\n\n    ICU_stat_data = ICU_stat_data[(ICU_stat_data.index=='mean') | (ICU_stat_data.index=='std') ].T\n    ICU_stat_data['mean'] = ICU_stat_data['mean'].round(2).astype(str)\n    ICU_stat_data['std'] = ICU_stat_data['std'].round(2).astype(str)\n    ICU_stat_data ['ICU-Mean(Std)'] = ICU_stat_data['mean'] + ' ('+ ICU_stat_data['std'] +')'\n    return ICU_stat_data ['ICU-Mean(Std)']\n","4ba5228d":"# for ICU paient with ICU=1\nICU_stat_data = data.loc[data.ICU==1, quantitive_features].describe()\nicu = get_mean_std(ICU_stat_data)\ndata_stat = pd.DataFrame()\ndata_stat['ICU-Mean(Std)']=icu \n\nNone_ICU_stat_data = data.loc[data.ICU==0, quantitive_features].describe()\nnone_icu = get_mean_std(None_ICU_stat_data)\ndata_stat['None ICU-Mean(Std)']=none_icu \ndata_stat.to_csv('data_stat.csv')\ndata_stat.head()","9584ab00":"# for none ICU paient with ICU=0\n\nNone_ICU_stat_data = data.loc[data.ICU==0, quantitive_features].describe()\n\nICU_stat_data = ICU_stat_data[(ICU_stat_data.index=='mean') | (ICU_stat_data.index=='std') ].T\n\nICU_stat_data['mean'] = ICU_stat_data['mean'].round(2).astype(str)\nICU_stat_data['std'] = ICU_stat_data['std'].round(2).astype(str)\nICU_stat_data ['None-ICU-Mean(Std)'] = ICU_stat_data['mean'] + ' ('+ ICU_stat_data['std'] +')'\nICU_stat_data.head()\n","66d79095":"import statsmodels.api as sm\n\n#get all quantitive features  and check p-value with target (will need ICU or not )\nxx = data.loc[:, quantitive_features]\nyy=  data.loc[:, 'target']\n\ndef get_p_value(x, Y):\n    regressor_OLS = sm.OLS(Y, x).fit()\n    return regressor_OLS.pvalues\n\n\np_value = get_p_value(xx.values, yy.values)\np_value_pd = pd.Series(p_value, index =quantitive_features) \np_value_pd = np.round(p_value_pd, 4) \np_value_pd","fe133ad1":"print('Caculated  features corrolations with Target', len( quantitive_features))\ncalc_corrMatrix = corrMatrix(data.loc[:,quantitive_features + ['target']],\n           'corrMatrix_ICU_caculated',include_target=True  ,annot=True)\n","5d5435b7":"calc_corrMatrix['target'].sort_values()","9b390b34":"qualititive_features","efad6778":"none_calc_corrMatrix = corrMatrix(data.loc[:,qualititive_features],\n                                  'corrMatrix_ICU_NoN_caculated',include_target=True  ,annot=True)","52646751":"none_calc_corrMatrix['target'].sort_values()","fd99cba3":"le = LabelEncoder()\ndata['AGE_PERCENTIL'] = le.fit_transform(data['AGE_PERCENTIL'])\ndata['AGE_PERCENTIL']","fd46b706":"def get_data_of_window(data, window_no): #window_no [1-5]\n    indices = np.arange(window_no-1,data.shape[0],5) # window index start from 0\n    window_df = data.iloc[indices,:]\n    '''\n    after filter data of specific window, remove cases when row in ICU\n    target - ICU\n    0      - 0 keep \n    1      - 0 keep \n    1      - 1 delete rows already in ICU\n    '''\n    window_df = window_df[~( (window_df.target==1) & (window_df.ICU==1) )]\n    print('shape:',window_df.shape)\n    return window_df","36da1d60":"#get window data for w1 who taken  to ICU in one of the following windows w2 to w5\nwindow_data={}\nwindow_data[1]=get_data_of_window(data, 1)\n\n#get window data for w2  who taken  to ICU in one of the following windows w2 to w5\nwindow_data[2]=get_data_of_window(data, 2)\n\n#get window data for w3  who taken  to ICU in one of the following windows w4 to w5\nwindow_data[3]=get_data_of_window(data, 3)\n\n#get window data for w4  who taken  to ICU in w5\nwindow_data[4]=get_data_of_window(data, 4)\n\nwindow_data[4].head()","6e3cdea6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif\n\nimport sklearn.metrics as metrics\n\ndef draw_ROC_curve(exp_no, model, X_test,Y_test,best_features,optimal_cutoff):\n    predictions= model.predict_proba(X_test[best_features])[:,1]\n\n    #optimal_cutoff = Find_Optimal_Cutoff(Y_test, predictions)\n\n    #accuracy  = accuracy_score(Y_test, predictions >=optimal_cutoff)\n    roc_auc = roc_auc_score(Y_test.values,predictions )\n    #print('accuracy:',accuracy,'roc_auc',roc_auc )\n\n    fpr, tpr, threshold = metrics.roc_curve(Y_test, predictions >=optimal_cutoff)\n    #roc_auc = roc_auc_score(Y_test.values,predictions>=optimal_cutoff )\n    #plot_roc(exp_no,fpr, tpr,roc_auc)\n    return fpr, tpr, threshold\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scale_data(X):\n    min_max_scaler = MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(X)\n    df = pd.DataFrame(x_scaled)\n    return df\n\n\n\ndef get_feature_importances(x,y, mode='chi2' ):\n\n    if mode == 'chi2':\n        filter  = SelectKBest(chi2, \"all\")  # do not select a subset of features, just score them\n        \n        filter.fit(scale_data(x), y)\n        fi = pd.DataFrame(filter.scores_,\n                                           index = x.columns, #fsf_pd.columns[:-1],\n                                            columns=['importance']).sort_values('importance',  \n                                                                                ascending=False)\n        \n        return fi\n    elif mode=='Pearson':\n        feature_importances_=np.corrcoef(x[x.columns[0:]].T)[-1][:-1]\n        feature_importances = pd.DataFrame(feature_importances_,\n                                           index = x.columns[:-1],\n                                            columns=['importance']).sort_values('importance',  \n                                                                                ascending=False)\n        feature_importances['importance'] = np.abs(feature_importances['importance'] )\n        fi = feature_importances.sort_values(by='importance', ascending=False)\n\n        return fi    \n        \n    elif mode == 'RF':\n        # create a base classifier used to evaluate a subset of attributes\n        #model = RandomForestClassifier(n_estimators = 50 ,max_depth  =10 , random_state=0)\n        model = RandomForestClassifier(n_estimators = 300  , random_state=0)\n        \n        model.fit(x,y)\n        # create the RFE model and select 3 attributes\n\n\n\n        ## check model accurecy \n        #print(confusion_matrix(y, model.predict(x)) )\n        \n        fi = pd.DataFrame(model.feature_importances_,\n                                           index = x.columns,\n                                            columns=['importance']).sort_values('importance',  \n                                                                                ascending=False)\n        return fi\n    elif mode == 'f_class':\n        f_value,p_value =f_classif(x,y)\n        fi = pd.DataFrame(p_value,\n                                   index = x.columns,\n                                    columns=['importance']).sort_values('importance',  \n                                                                        ascending=False)\n        #\u2264 0.05\n        fi=fi[fi.importance<= 0.05]\n        return fi\n        \n        \n    else:\n        a=mutual_info_classif(x,y)\n        print(min(a), max(a), len(a))\n\n\n\nfrom sklearn.utils import class_weight\ndef get_class__weight(y):\n    class_weights = list(class_weight.compute_class_weight('balanced',\n                                                 np.unique(y),\n                                                 y))\n\n    w_array = np.ones(y.shape[0], dtype = 'float')\n    for i, val in enumerate(y):\n        w_array[i] = class_weights[val-1]\n    #print(np.unique(y),class_weights)\n    return w_array\n\nfrom sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve\nimport numpy as np\n\ndef Find_Optimal_Cutoff(target, predicted):\n    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n    Parameters\n    ----------\n    target : Matrix with dependent or target data, where rows are observations\n\n    predicted : Matrix with predicted data, where rows are observations\n\n    Returns\n    -------     \n    list type, with optimal cutoff value\n        \n    \"\"\"\n    fpr, tpr, threshold = roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), \n                        'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold']) \n\n    \n\nfrom sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve\nfrom sklearn.metrics  import roc_auc_score, precision_score, recall_score, average_precision_score\nfrom sklearn.metrics  import f1_score\n\n\n#optimal_cutoff = Find_Optimal_Cutoff(Y_test, predictions)\n#print('optimal_cutoff',optimal_cutoff)\ndef test_measurements(exp_no, model,X_test,Y_test,best_features,optimal_cutoff):\n\n    #result=pd.DataFrame(columns = ['accuracy','roc_auc', 'precision',  'recall', \n    #                                     'pr_auc','f1'])\n    predictions= model.predict_proba(X_test[best_features])[:,1]\n    accuracy  = accuracy_score(Y_test, predictions >=optimal_cutoff)\n    #roc_auc   = roc_auc_score(Y_test, y_pred)\n    roc_auc = roc_auc_score(Y_test.values,predictions >=optimal_cutoff )\n\n    precision = precision_score(Y_test, predictions >=optimal_cutoff)\n    recall    = recall_score(Y_test,predictions >=optimal_cutoff)\n    pr_auc    = average_precision_score(Y_test, predictions >=optimal_cutoff)\n    f1    = f1_score(Y_test, predictions >=optimal_cutoff)\n    #accuracy_lst.append(accuracy)\n    #roc_auc_lst.append(roc_auc)\n    #precision_lst.append(precision)\n    #recall_lst.append(recall)\n    #pr_auc_lst.append(pr_auc)\n    #f1_lst.append(f1)\n\n\n    print(f'Accuracy: {round(accuracy,4)}')\n    print(f'ROC AUC: {round(roc_auc,4)}')\n    print(f'Precision: {round(precision,4)}')\n    print(f'Recall: {round(recall,4)}')\n    print(f'PR Score: {round(pr_auc,4)}')\n    print(f'F1 Score: {round(f1,4)}')\n    '''\n    result.loc[len(result)] = {\n                                   'accuracy':accuracy,\n                                   'roc_auc': roc_auc,\n                                   'precision':precision, \n                                   'recall':recall, \n                                  'pr_auc':pr_auc,\n                                  'f1':f1\n                                  }\n    '''\n\n\n    conf_mat = confusion_matrix(Y_test.values,predictions >=optimal_cutoff )\n    classif_report = classification_report(Y_test.values, predictions >=optimal_cutoff ) \n    #print(result )    \n    #print(result.describe()[1:3] )\n\n    #return  np.mean(roc_auc_lst), np.mean(accuracy_lst),np.mean(precision_lst),np.mean(recall_lst), pr_auc,conf_mat,classif_report,np.mean(f1_lst)\n    \n    return  roc_auc, accuracy,precision,recall, pr_auc,conf_mat,classif_report,f1\n\n    \ndef validate_model(X_train,Y_train,X_test,Y_test, best_iteration, best_features):\n    eval_set = [(X_train[best_features], Y_train), (X_test[best_features], Y_test)]\n    eval_metric = [\"auc\",\"error\"]\n    lr = XGBClassifier(\n                 learning_rate =0.1,\n                 n_estimators=best_iteration+1 ,\n                 max_depth=5,\n                 min_child_weight=1,\n                 gamma=0,\n                 subsample=0.8,\n                 colsample_bytree=0.8,\n                 objective= 'binary:logistic',\n                 nthread=4,\n                 scale_pos_weight=1, seed=27) # the result is very stable even when remove seed\n\n    sample_weight = get_class__weight(Y_train)\n    lr.fit(X_train[best_features], Y_train, eval_metric=eval_metric, \n           eval_set=eval_set,sample_weight=sample_weight, verbose=False  )\n    predictions= lr.predict_proba(X_train[best_features])[:,1]\n    optimal_cutoff = Find_Optimal_Cutoff(Y_train, predictions)\n    print('optimal_cutoff',optimal_cutoff)\n    \n    return lr,optimal_cutoff\n","2037dc51":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.naive_bayes import GaussianNB\n\ndef get_in_out_columns(exp_data):\n    special_columns= ['ICU','WINDOW' ,'PATIENT_VISIT_IDENTIFIER' ,'target', 'keep']\n    input_cols = [c for c in exp_data.columns if c not in special_columns]\n    output_cols = 'target'\n    return input_cols,output_cols \n\ndef split_data(exp_data):\n    X_train, X_test, Y_train, Y_test = train_test_split(exp_data.loc[:,input_cols],\n                                                        exp_data.loc[:,output_cols], \n                                                        stratify =exp_data.loc[:,output_cols],\n                                                       test_size=0.2,\n                                                        #random_state=27\n                                                       )\n    X_train.reset_index(drop=True, inplace=True)\n    X_test.reset_index(drop=True, inplace=True)\n    Y_train.reset_index(drop=True, inplace=True)\n    Y_test.reset_index(drop=True, inplace=True)\n    print(\"Splits Train\/Test: \", X_train.shape, Y_train.shape,'\\n', X_test.shape, Y_test.shape)\n    return X_train, X_test, Y_train, Y_test \n\n\n\ndef evalate(exp_no,x,y,no_features= 20, corr_mode='chi2', build_model=False ):\n    #create StratifiedKFold to split data to 5 folds and get first 80 as training and 20 as testing\n    #then break loop Stratified split\n    #this is a way to get \n    if build_model:\n        skf = StratifiedKFold(n_splits=10,  shuffle=True) #, random_state=10)    \n    else:\n        skf = StratifiedKFold(n_splits=5,  shuffle=True) #, random_state=10)\n    #skf.get_n_splits(x, y)\n\n    #common_features = set()\n    common_features = pd.DataFrame() \n    #add all features as common important features with important score = 0 \n    common_features['name'] = x.columns\n    common_features['score'] = 0\n\n    fi_train=0\n    interest_features = set()\n    #this for will run ony once for split data 80\/20\n    for train_index, test_index in skf.split(x, y):\n        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        #get important features from training data \n        #corr_mode= 'chi2'\n        print('---------corr_mode:',corr_mode)\n        fi_train = get_feature_importances(x_train,y_train.values, mode=corr_mode )  \n        #print('---------',fi_train)\n           \n        features=  list(fi_train.head(no_features).index)\n        #common_features = common_features.union( set(features))\n\n        #update score of important features every fold when features appears in feature_importances\n        #common_features = get_common_features(common_features,features )\n        #features=X.columns\n\n        x_train= x_train[features]         \n        x_test = x_test[x_train.columns]       \n        #build classifier using selected parameters \n        clf = XGBClassifier(\n                 learning_rate =0.1,\n                 n_estimators=500,\n                 max_depth=5,\n                 min_child_weight=1,\n                 gamma=0,\n                 subsample=0.8,\n                 colsample_bytree=0.8,\n                 objective= 'binary:logistic',\n                 nthread=4,\n                 scale_pos_weight=1,\n                 #seed=27\n        )\n\n        #eval_set = [ (x_test, y_test)]\n        eval_set = [(x_train, y_train), (x_test, y_test)]\n\n        eval_metric = [ \"error\", \"auc\"] # best score will be depend on second metric : auc\n        early_stopping_rounds= 100\n        #sample_weight=w_array\n        sample_weight = get_class__weight(y_train)\n\n        clf.fit(x_train, y_train, eval_metric=eval_metric, sample_weight=sample_weight,\n                eval_set=eval_set,verbose=False, early_stopping_rounds=early_stopping_rounds)\n\n        #enable this code after know the best features and best corrolation mode to drow AUC and error curve\n        #of the best score \n        '''\n        \n        if build_model:\n            plot_result(exp_no,clf,'auc')\n            plot_result(exp_no,clf, 'error')\n        '''\n        #roc_auc = roc_auc_score(y_test.values, clf.predict_proba(x_test)[:,1])\n        #roc_auc_scores.append(roc_auc)\n        #print('roc_auc:',roc_auc)\n        #print('best_iteration:',clf.best_iteration)\n        best_auc= clf.best_score\n        #roc_auc_scores.append(best_auc)\n\n        #print('best_score:AUC',best_auc)\n        best_error = clf.evals_result()['validation_1']['error'][clf.best_iteration]\n        #print('best_error',best_error)\n\n        #as only one iteration required, we will return after first fold \n        # we used kfold to split data to 80\/20 by apply 5 fold and break after first fold\n        return best_error, clf.best_iteration, features, best_auc,common_features # list(common_features) #,fi_train #,interest_features\n\n\n\ndef get_avarage_AUC(exp_no,x,y,no_features= 20, corr_mode='chi2', repeatation = 10 ):\n    #Repeate this 10 times and return Avarage of  AUC of selected No. of features\n    roc_auc_scores =[]\n    for r in range(repeatation):\n        _,_,_,auc_score,_ = evalate(exp_no,x,y,no_features, corr_mode)\n        roc_auc_scores.append(auc_score)\n    #print(no_features,'-', roc_auc_scores,np.mean(roc_auc_scores),np.std(roc_auc_scores)  ) \n    return  np.mean(roc_auc_scores)\n\n\ndef early_stopping(exp_no, X_train,Y_train,repeate = 10):\n    best_auc=0\n    best_no_features=0\n    best_iteration = 0\n    best_features = 0\n    best_error = 100\n    best_corr=''\n    results_df = pd.DataFrame(columns = ['feature_group','no_features', 'score',  'corr_mode', \n                                         'error','iteration','features'])\n\n\n    group_features = X_train.columns #  get_group_columns_names(X,'lab')\n    #group_features_names = [ 'all', 'comorb', 'vital','lab']\n    group_name = ['all']\n    corrolation_alg = ['Pearson','chi2'  ]#,'f_class']\n\n    for corr_mode in corrolation_alg: # loop over corrolation mode(algorithms)\n        #group_features = get_group_columns_names(X_train,group_name) \n        feature_list,_ = get_in_out_columns(X_train)\n        print( 'get_in_out_columns', len(feature_list))\n\n        print('corr_mode', corr_mode, '-- Max Features:', len(feature_list) )\n\n        # loop  from 2 to max number of features in selected group \n        #loop to get best avarage AUC for each No_features \n        for no_features in range(2,len(feature_list)): \n            \n            auc_mean = get_avarage_AUC(exp_no,X_train[feature_list],\n                                       Y_train,no_features, corr_mode,repeate) \n            \n            results_df.loc[len(results_df)] = {\n                                               'corr_mode':corr_mode,\n                                               'feature_group': group_name,\n                                               'no_features':no_features, \n                                               'score':auc_mean, #roc_auc, \n                                              'error':0,\n                                              'iteration':0,\n                                              'features':0\n                                              }\n            if auc_mean > best_auc:\n                best_auc = auc_mean \n                best_no_features =no_features\n                best_corr = corr_mode\n                print('Best @', no_features,auc_mean)\n        \n        #end for2 of features\n    #end for 1 of corrolation\n    \n    print('->> Early_stopping Best avarage_AUC',best_auc, \n          'best_no_features',best_no_features,'best_corr',best_corr  )\n    \n    results_df.to_csv('results_exp' +str(exp_no) +'.csv', index=False)\n\n    \n    error, iteration,features, roc_auc, common_features= evalate(exp_no,X_train[group_features],\n                                      Y_train,best_no_features, best_corr,build_model=True) \n            \n    #if  error < best_error:\n    best_error = error\n    best_auc = roc_auc \n    best_iteration = iteration\n    best_features = features\n    best_f =best_no_features\n    best_corr = best_corr\n    \n    #print('Best Avarage Roc_auc:',best_auc, ' @', best_f)\n    return results_df,best_auc,  best_error, best_iteration, best_features, best_f, best_corr\n\n\n","9949f36d":"#get window data for w1 and went to ICU in one of the following windows w2 to w5\nwindow_data={}\nwindow_data[1]=get_data_of_window(data, 1)\n\n#get window data for w2 and went to ICU in one of the following windows w2 to w5\nwindow_data[2]=get_data_of_window(data, 2)\n\n#get window data for w3 and went to ICU in one of the following windows w4 to w5\nwindow_data[3]=get_data_of_window(data, 3)\n\n#get window data for w4 and went to ICU in w5\nwindow_data[4]=get_data_of_window(data, 4)\n\nwindow_data[4].head()","6314034b":"expr_data={}\nexpr_data[1] = window_data[1]\nexpr_data[2] = pd.concat([window_data[1],window_data[2] ])\nexpr_data[3] = pd.concat([window_data[1],window_data[2],window_data[3]  ])\nexpr_data[4] = pd.concat([window_data[1],window_data[2],window_data[3] ,window_data[4] ])\n","a6f13ad5":"\n#prepare result dataframe to log results\nresults_ddf = pd.DataFrame(columns = ['iteration','exp','roc_auc', 'accuracy','precision',\n                                     'recall', 'pr_auc', 'f1','best_features',\n                                     'best_no_features','best_iteration'\n                                    ,'fpr','tpr'])\n#repeate exprements 10 times \n#will use only 1 iteration on kaggle due to   limited resources \nfor r in range(1):\n    # Run four experements \n    for exp_no in range(1,5):\n        print( '--------------------------Exp:',exp_no,r,'------------------')\n        print(exp_no, expr_data[exp_no].target.value_counts().values)\n        input_cols,output_cols = get_in_out_columns( expr_data[exp_no])\n        #print(input_cols,output_cols)\n\n        expr_data[exp_no].reset_index(drop=True, inplace=True)\n        X_train, X_test, Y_train, Y_test = split_data(expr_data[exp_no])\n        print()\n\n        results_df,best_auc,  best_error, best_iteration,best_features, best_f, best_corr\\\n                =early_stopping(exp_no,X_train,Y_train, repeate = 5)\n\n        print('Score on Validation data--> ')\n        '''\n        \n        print( 'best_auc:',best_auc,'best_error:',best_error, \n              'best_iteration:',best_iteration,\n              'best_features:',len(best_features), \n              'best_corr:', best_corr)\n        '''\n        print()\n        #results_df.to_csv('results_exp' +str(i) +'.csv', index=False)\n\n        model,optimal_cutoff = validate_model(X_train,Y_train,X_test,Y_test, best_iteration, best_features)\n        print('Score on Test data--> ')\n\n        roc_auc, accuracy,precision,recall, pr_auc,conf_mat,classif_report,f1=\\\n                                        test_measurements(exp_no,model,X_test,Y_test,\n                                                         best_features,\n                                                         optimal_cutoff)\n\n        \n        fpr, tpr, threshold = draw_ROC_curve(exp_no,model,X_test,Y_test,best_features,optimal_cutoff)\n        print()\n        #print('Exp:',exp, 'conf_mat:\\n', conf_mat  )\n        #print('Exp:',exp, 'classif_report:\\n', classif_report  )\n        results_ddf.loc[len(results_ddf)] = { 'iteration':r,\n                                            'exp':exp_no,\n                                            'roc_auc':roc_auc,\n                                            'accuracy': accuracy,\n                                            'precision':precision, \n                                            'recall':recall, \n                                            'pr_auc':pr_auc,\n                                            'f1':f1,\n                                            'best_features':best_features,\n                                            'best_no_features':len(best_features),\n                                            'best_iteration':best_iteration,\n                                            'fpr':fpr,\n                                            'tpr':tpr\n                                           \n                                            }\n        ","044128f2":"results_ddf","e9b6e995":"## create data for each window : 1-4","1dcbacb3":"# The next sections Apply Data Cleaninig process","2d7ddf56":"### view diffrent patients; windows and when taken to ICU","73e9465d":"### Analysis for None ICU paient( ICU=0)","6d79ad18":"# Predicting the need for ICU admission in COVID-19 ","a973ba93":"## Will use the following expermenets to predict patient needs to be taken to ICU or not:\n1.  expermenet1 : use first data winow  for predict patient needs to be taken to ICU or not\n1.  expermenet2 : use first & Second data  winows for predict patient needs to be taken to ICU or not\n1.  expermenet3 : use first, Second and third  data  winows for predict patient needs to be taken to ICU or not\n1.  expermenet4 : use first, Second,third, and fourth  winows for predict patient needs to be taken to ICU or not","afcf8f10":"# Handling Missing value   \n- next rows filled from previous row per patient","3258841f":"### qualititive_features Corrolations","26202ce5":"# Statistical Data Analysis ","fd4d1933":"### Get p-value of features","d2747a45":"### Get features corrolation ","d27e23fa":"# Set Target of classification from the ICU status of last patent's window\n- patient  taken to ICU at any of time window -->  Target =1 \n- patient don't taken to ICU at any time window --> Target =0","54c8e31c":"## Experiements Setup","c8d1bb5f":"## Convert AGE_PERCENTIL into numerical dummy data\n","ee5fe65a":"# Data Preperation ","b321e570":"### Understanding the features of the data steps of data prepration","e13f755f":"### Analysis for ICU paient( ICU=1)"}}