{"cell_type":{"adc3af97":"code","c5835e05":"code","8e544e5f":"code","adc33dcb":"code","271b91df":"code","7055de46":"code","34a207b2":"code","b65329a3":"markdown","1df1e608":"markdown","2ac0b94e":"markdown","826f81f3":"markdown","9d9c126f":"markdown","d8c6cb87":"markdown"},"source":{"adc3af97":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","c5835e05":"cols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\nskf = StratifiedKFold(n_splits=5, random_state=42)\n   \nfor train_index, test_index in skf.split(train.iloc[:,1:-1], train['target']):\n    clf = LogisticRegression(solver='liblinear',penalty='l2',C=1.0)\n    clf.fit(train.loc[train_index][cols],train.loc[train_index]['target'])\n    oof[test_index] = clf.predict_proba(train.loc[test_index][cols])[:,1]\n    \nauc = roc_auc_score(train['target'],oof)\nprint('LR without interactions scores CV =',round(auc,5))","8e544e5f":"# INITIALIZE VARIABLES\ncols.remove('wheezy-copper-turtle-magic')\ninteractions = np.zeros((512,255))\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n    \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        # LOGISTIC REGRESSION MODEL\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ 25.0\n        # RECORD INTERACTIONS\n        for j in range(255):\n            if clf.coef_[0][j]>0: interactions[i,j] = 1\n            elif clf.coef_[0][j]<0: interactions[i,j] = -1\n    #if i%25==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('LR with interactions scores CV =',round(auc,5))","adc33dcb":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","271b91df":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.figure(figsize=(15,5))\n\n# PLOT ALL ZIPPY\nplt.subplot(1,2,1)\nsns.distplot(train[ (train['target']==0) ]['zippy-harlequin-otter-grandmaster'], label = 't=0')\nsns.distplot(train[ (train['target']==1) ]['zippy-harlequin-otter-grandmaster'], label = 't=1')\nplt.title(\"Without interaction, zippy has no correlation \\n (showing all rows)\")\nplt.xlim((-5,5))\nplt.legend()\n\n# PLOT ZIPPY WHERE WHEEZY-MAGIC=0\nplt.subplot(1,2,2)\nsns.distplot(train[ (train['wheezy-copper-turtle-magic']==0) & (train['target']==0) ]\n             ['zippy-harlequin-otter-grandmaster'], label = 't=0')\nsns.distplot(train[ (train['wheezy-copper-turtle-magic']==0) & (train['target']==1) ]\n             ['zippy-harlequin-otter-grandmaster'], label = 't=1')\nplt.title(\"With interaction, zippy has postive correlation \\n (only showing rows where wheezy-copper-turtle-magic=0)\")\nplt.legend()\n\nplt.show()","7055de46":"plt.figure(figsize=(15,5))\n\n# PLOT ALL HASTY\nplt.subplot(1,2,1)\nsns.distplot(train[ (train['target']==0) ]['hasty-puce-fowl-fepid'], label = 't=0')\nsns.distplot(train[ (train['target']==1) ]['hasty-puce-fowl-fepid'], label = 't=1')\nplt.title(\"Without interaction, hasty has no correlation \\n (showing all rows)\")\nplt.xlim((-5,5))\nplt.legend()\n\n# PLOT HASTY WHERE WHEEZY-MAGIC=0\nplt.subplot(1,2,2)\nsns.distplot(train[ (train['wheezy-copper-turtle-magic']==0) & (train['target']==0) ]\n             ['hasty-puce-fowl-fepid'], label = 't=0')\nsns.distplot(train[ (train['wheezy-copper-turtle-magic']==0) & (train['target']==1) ]\n             ['hasty-puce-fowl-fepid'], label = 't=1')\nplt.title(\"With interaction, hasty has negative correlation \\n (only showing rows where wheezy-copper-turtle-magic=0)\")\nplt.legend()\n\nplt.show()","34a207b2":"# PLOT INTERACTIONS WITH WHEEZY-MAGIC\nplt.figure(figsize=(15,8))\nplt.matshow(interactions.transpose(),fignum=1)\nplt.title(\"Variable Interactions with wheezy-copper-turtle-magic \\n \\\n    Yellow = combines to create positive correlation with target. Blue = negative correlation\",fontsize=16)\nplt.xlabel('values of wheezy-copper-turtle-magic')\nplt.ylabel('columns of train')\nplt.show()","b65329a3":"# Conclusion\nIn conclusion we see that the variable `wheezy-copper-turtle-magic` interacts with other variables to predict target. Also we see that a simple model can score a high CV and LB score.\n\nThis is similar to the classic XOR problem. Suppose we have data with two variables and one target: `(x1,x2,y)` with the following 4 rows: `(0,0,0), (1,0,1), (0,1,1), (1,1,0)`. Notice that neither `x1` nor `x2` correlate with target `y`. Also `x1` and `x2` do not correlate with each other. However, `x1` and `x2` interact. Whenever `x1` is not equal to `x2` then `y=1` and when `x1=x2` then `y=0`. So together they predict `y` but separately they cannot predict `y`.\n\n# Interactions\nBelow shows the interactions between `wheezy-copper-turtle-magic` and the other variables. Each variable by itself cannot predict target well, but when `wheezy-copper-turtle-magic` equals a specific value then other variables can predict target well. For example, when `wheezy-copper-turtle-magic = 0` then `zippy-harlequin-otter-grandmaster` is positively correlated with target. And when `wheezy-copper-turtle-magic = 0` then `hasty-puce-fowl-fepid` is negatively correlated with target.","1df1e608":"# Logistic Regression with interactions\nUsing LR, we can build a model that includes interactions by building 512 separate models. We will build one LR (logistic regression) model for each value of `wheezy-copper-turtle-magic` and use the appropriate model to predict `test.csv`. This scores CV 0.805 and LB 0.808","2ac0b94e":"# Logistic Regression without interactions\nNone of the 256 variables have correlation with the target greater than absolute value 0.04. Therefore if you use LR to model target you score a low CV 0.530 because LR treats the variables as independent and doesn't utilize interactions.","826f81f3":"# Submit Predictions","9d9c126f":"# Logistic Regression scores LB 0.800\nIn this kernel, we present a simple logistic regression model for Kaggle's \"Instant Gratification\" competition. This kernel demonstrates that interactions between the variable `wheezy-copper-turtle-magic` and the other variables exist. And it demonstrates that a simple model can perform well. Without interactions LR (logistic regression) scores CV 0.530, and with interactions LR scores CV 0.805 and LB 0.808\n\n# Load Data","d8c6cb87":"Our submission scores LB 0.808, hooray. This is a good score and it matches our CV 0.805"}}