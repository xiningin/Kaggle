{"cell_type":{"6d46dc97":"code","1eda9998":"code","b1c6f982":"code","22b5c194":"code","11d2a390":"code","d89a96af":"code","feb0116c":"code","a5aa0cd3":"code","1b2bd4d1":"code","ade9e748":"code","26900814":"code","3474b9e5":"code","43769709":"code","9bd0fbcf":"code","9d78db60":"code","ada70650":"code","f5454bb3":"code","0330c0a6":"code","7a230a91":"code","8f2d84c6":"code","d22a1507":"code","a4143b16":"code","14fe185e":"code","23b2b63b":"code","417f4d96":"code","248731b6":"code","99f3f839":"code","ff612d29":"code","b50fb3ac":"code","47c36d0b":"code","e58f69d3":"code","aba0cf75":"code","4c519521":"code","8e5803c9":"code","be930c7a":"code","f39a154a":"code","aa22e254":"code","9da7bf17":"code","c94562bf":"code","8ff35c66":"code","70fb579a":"code","632ac16d":"code","4bb167c8":"code","0638106e":"code","2f1ad0a1":"code","b02df404":"code","3d77d847":"code","1b466606":"code","a594581e":"code","a196244b":"code","2512399b":"code","4b3da42c":"code","94fbee4a":"code","6dcd7d09":"code","c71f5f6c":"code","fa6278bb":"code","b5022a09":"code","da441a60":"code","407d84bd":"code","fddb2ce7":"code","70d29f7c":"code","971c7acd":"code","0aec89b0":"markdown","232e8d0f":"markdown","6caac1b9":"markdown","3f2e7a8e":"markdown","fa577019":"markdown","09fbb56f":"markdown","f9e33bcb":"markdown","ab07bba9":"markdown","0839f657":"markdown","05de991e":"markdown","96a7af5d":"markdown","8119c9da":"markdown","1b78b2ef":"markdown","9b6c23d3":"markdown","3d299084":"markdown","bd063aca":"markdown","85a2b715":"markdown","832f6bf1":"markdown","a65d66c1":"markdown","4f0803cb":"markdown","c4d84d02":"markdown","94b7308a":"markdown","631a72a1":"markdown","562d63a6":"markdown","ca060fca":"markdown","7da4ebc6":"markdown","a42ed168":"markdown","bb707e3b":"markdown","afe7dbc7":"markdown","29588e25":"markdown","028f0dce":"markdown","22617222":"markdown"},"source":{"6d46dc97":"# manipulation data\nimport pandas as pd #\u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445\nimport numpy as np #\u043c\u0430\u0442. \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0441  \u043c\u0430\u0441\u0441\u0438\u0432\u0430\u043c\u0438\n\n#visualiation data\nimport matplotlib.pyplot as plt #pyplot \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 matplotlib \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043a\u0430\u043a \u043c\u0430\u0442\u043b\u0430\u0431. \u0442.\u0435. \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0438 \u0442.\u0434.\nimport seaborn as sns #\u0434\u043b\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nimport matplotlib #\u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445(\u0433\u0440\u0430\u0444\u0438\u043a\u0438)\nimport plotly.graph_objects as go #\u0413\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043e\u0431\u042a\u0435\u043a\u0442\u044b(\u0444\u0438\u0433\u0443\u0440\u044b)\nimport plotly.express as px #\u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f(\u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0444\u0438\u0433\u0443\u0440)\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\n#\u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0444\u043e\u043d\u0430\nmatplotlib.rcParams['figure.figsize'] =[8,8]#\u0440\u0430\u0437\u043c\u0435\u0440 \u0444\u0438\u0433\u0443\u0440\u044b\nmatplotlib.rcParams.update({'font.size': 15})#\u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430\nmatplotlib.rcParams['font.family'] = 'sans-serif' #\u0441\u0442\u0438\u043b\u044c \u0448\u0440\u0438\u0444\u0442\u0430","1eda9998":"train = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv') #\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\ntrain.head()#\u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043f\u0435\u0440\u0432\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438","b1c6f982":"train.info()#\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 \u0442\u0430\u0431\u043b\u0438\u0446\u044b","22b5c194":"train.shape #\u0440\u0430\u0437\u043c\u0435\u0440","11d2a390":"train.describe(include='all') #","d89a96af":"missing_values=train.isnull().sum()#\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\npercent_missing = train.isnull().sum()\/train.shape[0]*100 #\u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u043e\u0433\u043e\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing\n}\nframe=pd.DataFrame(value)\nframe","feb0116c":"train=train.drop('Unnamed: 32',axis=1)#\u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\ntrain.head()","a5aa0cd3":"# drop the id columns\ntrain=train.drop('id',axis=1) ","1b2bd4d1":"#\u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0438\u043f\u0430 \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain.diagnosis = le.fit_transform(train.diagnosis) #\u0442\u043e \u0435\u0441\u0442\u044c, B=0(\u0434\u043e\u0431\u0440\u043e\u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f),M=1(\u0437\u043b\u043e\u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f)\ntrain.diagnosis\n","ade9e748":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(2) \n#\u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0443 \u043f\u043e \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0443 \"\u0442\u0435\u043f\u043b\u043e-\u0445\u043e\u043b\u043e\u0434\u043d\u043e\"\n#\u0447\u0438\u0441\u043b\u0430 \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0435 -- \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0441\u0442\u0440\u043e\u043a \u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432.","26900814":"# \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043a\u043e\u0440\u0435\u043b\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u043c\u0435\u043d\u044c\u0448\u0435 0.07\ntrain=train.drop(['fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1)","3474b9e5":"# \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c 15 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \n\nplt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = train.drop('diagnosis',axis=1)\ny = train.diagnosis\n\nfrom sklearn.ensemble import ExtraTreesClassifier #\u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u0434\u043b\u044f \"\u0434\u0435\u0440\u0435\u0432\u0430\" \u0440\u0435\u0448\u0435\u043d\u0438\u0439\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y) #\u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0438\u0436\u0435\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns) \n#index=x.columns \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u043c\u044b \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432\n#\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u043c\u0435\u0442\u043e\u0434\u0430\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u0441\u0432\u0430\u0438\u0432\u0430\u044e\u0442 \u043e\u0446\u0435\u043d\u043a\u0443 \u0432\u0445\u043e\u0434\u043d\u044b\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044f\u043c\n#\u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0442\u043e\u0433\u043e, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u043f\u043e\u043b\u0435\u0437\u043d\u044b \u043f\u0440\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439.\nfeat_importances.nlargest(15).plot(kind='barh') \n#\u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c 15 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0438\u0436\u0435\nplt.title('the most 15 important feature are')\nplt.show()","43769709":"train.columns #\u041c\u0435\u0442\u043a\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 train \u0434\u0430\u0442\u044b.","9bd0fbcf":"train=train.drop(['texture_mean','smoothness_mean','compactness_mean','symmetry_mean','perimeter_se','compactness_se','concavity_se','concave points_se','smoothness_worst','symmetry_worst','fractal_dimension_worst'],axis=1)","9d78db60":"g = sns.PairGrid(x)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True) \n#\u041f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0434\u0432\u0443\u043c\u0435\u0440\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0438 \u044f\u0434\u0440\u0430.\ng.map_diag(sns.histplot, kde=True)\n#\u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043e\u0441\u0435\u0439 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c \u0441\u043e \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0438 \u044f\u0434\u0440\u0430","ada70650":"plt.rcParams['figure.figsize']=25,7 \nsns.set_style(\"darkgrid\")\nax = sns.countplot(x=train.diagnosis , palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"diagnosis malignant = 1 \/ benign = 0 \", fontsize = 10 )\nplt.ylabel(\"count\", fontsize = 10)\nplt.title('Number of diagnosis ')","f5454bb3":"sns.boxplot(x=train['concave points_worst']) \n#\u043c\u0435\u0440\u0430 \u0442\u043e\u0433\u043e, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u0434\u0430\u043d\u043d\u044b\u0435","0330c0a6":"sns.displot(train, x='concave points_worst') \n#\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435(\u0432 \u0434\u0440\u0443\u0433\u043e\u043c \u0432\u0438\u0434\u0435)","7a230a91":"sns.displot(train, x=\"concave points_worst\", kde=True) \n#\u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","8f2d84c6":"sns.displot(train, x=\"concave points_worst\", col=\"diagnosis\", multiple=\"dodge\")","d22a1507":"sns.boxplot(x=train['concavity_mean'])","a4143b16":"outlier=train[train['concavity_mean']>=0.25]\noutlier","14fe185e":"train = train[train['concavity_mean']<0.25]\ntrain","23b2b63b":"sns.boxplot(x=train['concavity_mean']) #\u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0443 \u043d\u0430 \u0432\u043e\u0433\u043d\u0443\u0442\u043e\u0441\u0442\u044c","417f4d96":"sns.catplot(x=\"concavity_mean\",\n                col=\"diagnosis\",\n                data=train, kind=\"box\",\n                height=4, aspect=.7);","248731b6":"sns.boxplot(x=train['perimeter_worst'])","99f3f839":"outlier=train[train['perimeter_worst']>=165]\noutlier","ff612d29":"train = train[train['perimeter_worst']<165]\ntrain","b50fb3ac":"sns.boxplot(x=train['perimeter_worst'])","47c36d0b":"from sklearn.model_selection import train_test_split #\u0442\u0435\u0441\u0442\nfrom sklearn import metrics \nfrom sklearn.metrics import accuracy_score #\u043e\u0446\u0435\u043d\u043a\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438","e58f69d3":"x=train.drop('diagnosis',axis=1)\ny=train.diagnosis","aba0cf75":"print(x.shape)\nprint(y.shape)","4c519521":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","8e5803c9":"from sklearn.preprocessing import StandardScaler #\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","be930c7a":"# \u0421\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u0440\u0430\u0441\u0447\u0435\u0442 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#\u041f\u043e\u0434\u0431\u043e\u0440 \u043c\u043e\u0434\u0435\u043b\u0438\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nmylist = []\n#\u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438\ncm = confusion_matrix(y_test, y_pred)\n#\u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg,'%')","f39a154a":"# Finding the optimum number of neighbors \n\nfrom sklearn.neighbors import KNeighborsClassifier #\u0434\u043b\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(1,5):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(1,5)), list1)\nplt.show()","aa22e254":"# \u043f\u0440\u043e\u0431\u0443\u0435\u043c KNN \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n\nclassifier = KNeighborsClassifier(n_neighbors=3)\nclassifier.fit(x_train, y_train)\n\n#\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)\n\n","9da7bf17":"# \u0421\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u0440\u0430\u0441\u0447\u0435\u0442 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_knn = accuracy_score(y_test, y_pred)\nmylist.append(acc_knn)\nprint(cm)\nprint(acc_knn)","c94562bf":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    classifier = SVC(C = c, random_state=0, kernel = 'rbf')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","8ff35c66":"#\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435\n\nfrom sklearn.svm import SVC #\u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0449\u0438\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043c\u0430\u0448\u0438\u043d\u044b\nclassifier = SVC(C = 0.9, random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)\n\n","70fb579a":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","632ac16d":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_svc = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_svc,'%')\nmylist.append(acc_svc)","4bb167c8":"# Finding the optimum number of max_leaf_nodes\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,15):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,15)), list1)\nplt.show()","0638106e":"# Training the Decision Tree Classifier on the Training set\n\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","2f1ad0a1":"#Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","b02df404":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_decisiontree = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_decisiontree)\nmylist.append(acc_decisiontree)","3d77d847":"#Finding the optimum number of n_estimators\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30)), list1)\nplt.show()","1b466606":"# Training the RandomForest Classifier on the Training set\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)","a594581e":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","a196244b":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","2512399b":"np.random.seed(0)\nimport tensorflow as tf\n\n# Initialising the ANN\n\nann = tf.keras.models.Sequential()\n\n# Adding the input layer and the first hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the second hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the third hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the fourth hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the output layer\n\nann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n\n# Compiling the ANN\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n\n# Training the ANN on the training set\n\nann.fit(x_train, y_train, batch_size = 16, epochs = 100)","4b3da42c":"# Predicting the test set results\n\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.9)\nnp.set_printoptions()","94fbee4a":"# Making the confusion matrix, calculating accuracy_score \n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint()\n\n# accuracy\nac_ann = accuracy_score(y_test,y_pred)\nprint(\"Accuracy\")\nprint(ac_ann)\nmylist.append(ac_ann)\n\nimport pickle\nwith open (\"model.bin\", \"wb\") as cm:\n    pickle.dump(ac_ann, cm)","6dcd7d09":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","c71f5f6c":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 15, max_depth=12, subsample=0.7)\nclassifier.fit(x_train,y_train)","fa6278bb":"y_pred = classifier.predict(x_test)\nprint(y_pred)","b5022a09":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)","da441a60":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train)","407d84bd":"y_pred = classifier.predict(x_test)\nprint(y_pred)","fddb2ce7":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_catboost)\nprint(cm)\nprint(ac_catboost)","70d29f7c":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'ANN',   \n              'Decision Tree','xgboost','catboost'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, ac_ann, acc_decisiontree,ac_xgboost,ac_catboost\n              ]})\nmodels.sort_values(by='Score', ascending=False)","971c7acd":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","0aec89b0":"### a) Logistic Regression","232e8d0f":"#### this feautres had a corralation valus < 0.07 with the target columns \nfractal_dimension_mean \/ texture_se \/ smoothness_se \/ symmetry_se \/ fractal_dimension_se","6caac1b9":"##### like we see all our feautres are numirical values exept the target value ***diagnosis*** (M = malignant, B = benign)","3f2e7a8e":"#### Diagnosis \n\n1. M = malignant ==> 1\n2. B = benign    ==> 0 ","fa577019":"# 1. import library","09fbb56f":"### f) ANN (\u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c)","f9e33bcb":"## A) \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","ab07bba9":"### b) KNN (k \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439)","0839f657":"# 5) machine learning application","05de991e":"## B) concavity_mean (\u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u043e\u0433\u043d\u0443\u0442\u043e\u0441\u0442\u0438)","96a7af5d":"# 2. load and analys data","8119c9da":"##### like we c our data is clean exept the last columns that is empty so we gonna drop it ","1b78b2ef":"# 3. \u0412\u044b\u0431\u043e\u0440 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a (0 \u0438\u043b\u0438 1)","9b6c23d3":"## 3) perimeter_worst","3d299084":"# Data Set Information:\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at [Web Link]\n\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\n\n## Attribute Information:\n\n1. ID number\n2. Diagnosis (M = malignant, B = benign) \n3. 3-32 Ten real-valued features are computed for each cell nucleus:\n\n    * radius (mean of distances from center to points on the perimeter)\n    * texture (standard deviation of gray-scale values)\n    * perimeter\n    * area\n    * smoothness (local variation in radius lengths)\n    * compactness (perimeter^2 \/ area - 1.0)\n    * concavity (severity of concave portions of the contour)\n    * concave points (number of concave portions of the contour)\n    * symmetry\n    * fractal dimension (\"coastline approximation\" - 1)\n\n![](https:\/\/static.packt-cdn.com\/products\/9781783980284\/graphics\/3a298fcc-54fb-42c2-a212-52823e709e30.png)","bd063aca":"### c)\u041e\u043f\u043e\u0440\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043c\u0430\u0448\u0438\u043d\u044b","85a2b715":"\u0432\u044b\u043a\u0438\u0434\u044b\u0432\u0430\u0435\u043c \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0435\u0441\u044f \u0441\u0442\u043e\u043b\u0431\u0446\u044b (\u043d\u0435 \u0432\u043e\u0448\u0435\u0434\u0448\u0438\u0435 \u0432 15)","832f6bf1":"# 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","a65d66c1":"#### \u041c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u044b (outlier). \u0423\u0434\u0430\u043b\u0438\u043c \u0438\u0445 (0,3-0,5)","4f0803cb":"## A) \u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f","c4d84d02":"##### we had 569 Rows and 33 columns (small data )","94b7308a":"### g) xgboost (\u0443\u0441\u0438\u043b\u0435\u043d\u043d\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e)","631a72a1":"## B) \u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","562d63a6":"## B) concave points_worst (\u0412\u043e\u0433\u043d\u0443\u0442\u043e\u0441\u0442\u044c \u043a\u043b\u0435\u0442\u043e\u043a)","ca060fca":"### e) \u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e \u0440\u0430\u0441\u0441\u0435\u0447\u0435\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430","7da4ebc6":"## Plotting many distributions","a42ed168":"### d) \u0414\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0439","bb707e3b":"### h) catboost","afe7dbc7":"#### from the first look in our data description we can see that :\n\n1. B = \u0434\u043e\u0431\u0440\u043e\u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432 \u043d\u0430\u0448\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445\n2. Unnamed: 32 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u043f\u0443\u0441\u0442\u044b\u0435 ","29588e25":"\u041c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0435\u0441\u0442\u044c 2 \u0432\u044b\u0431\u0440\u043e\u0441\u0430. \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0443\u0434\u0430\u043b\u0438\u043c \u0438\u0445>165","028f0dce":"## A) \u041a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0430","22617222":"## A) \u0414\u0438\u0430\u0433\u043d\u043e\u0437\u044b"}}