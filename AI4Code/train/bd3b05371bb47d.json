{"cell_type":{"75ec283a":"code","9f76ebba":"code","bd644e0d":"code","2e89cfb3":"code","c0fa0cec":"code","c04d9aba":"code","19ca422f":"code","d5671723":"code","9254bfee":"code","f04ea8ec":"code","3058967b":"code","71ab375e":"code","9b2036c2":"code","78a4b4e2":"code","1d462b77":"code","0928f023":"code","98e04b27":"code","31915ee3":"code","91b1d100":"code","bfe71a54":"code","437aabf7":"code","ec17e404":"code","dd927781":"code","d4f1f0a6":"code","c0263272":"code","3eac74ad":"code","3a565009":"code","505742bf":"code","8ad829bb":"code","33cbd265":"code","7560125b":"code","1b2fde61":"code","5cebe244":"code","bdc9b9a6":"code","3064c58c":"code","006b1c87":"code","d9cb79cf":"code","3d52d5f1":"code","eb239310":"code","c575254b":"code","9137c7e0":"code","005a7016":"code","02f93471":"code","49faff38":"markdown","96613d63":"markdown","dc534a96":"markdown","c8ad4c62":"markdown","5bb13555":"markdown","be1e56f7":"markdown","79f567b3":"markdown","c34dab9b":"markdown","f094fa4c":"markdown"},"source":{"75ec283a":"# As usual let us start by importing the commonly used libraries \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nimport warnings \n\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm, skew","9f76ebba":"# Let us read the training and testing  datasets \nD_train = pd.read_csv('..\/input\/train.csv')\nD_test = pd.read_csv('..\/input\/test.csv')","bd644e0d":"print(\"The shape of the training data is {}\".format(D_train.shape))\nprint(\"The shape of the testing data is {}\".format(D_test.shape))","2e89cfb3":"#Let us print some of the rows in the training and test data data \nD_train[0:6]","c0fa0cec":"# test data\nD_test[0:6]","c04d9aba":"fig = plt.figure(figsize=(8,6))\nplt.scatter(D_train['SalePrice'], D_train['GrLivArea'])\nplt.ylabel('GrLivArea')\nplt.xlabel('SalePrice')\nplt.title('Kaggle: Houseing Sale Price')\nplt.show()","19ca422f":"D_train = D_train.drop(D_train[(D_train['GrLivArea']>4000) & (D_train['SalePrice']<800000)].index)","d5671723":"print(\"The Shape of the training data after removing this outlier is {}\".format(D_train.shape))","9254bfee":"# plot GrLivArea vs 'SalePrice' after removing outliers \n\nfig = plt.figure(figsize=(8,6))\nplt.scatter(D_train['SalePrice'], D_train['GrLivArea'])\nplt.ylabel('GrLivArea')\nplt.xlabel('SalePrice')\nplt.title('Sale Price: After removing outliers')\nplt.show()\n","f04ea8ec":"#The target (house price) distribution plot \nfig = plt.figure(figsize=(8,6))\nsns.distplot(D_train['SalePrice'] , fit=norm);\nplt.ylabel('GrLivArea')\nplt.xlabel('SalePrice')\nplt.show()\n\n\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(D_train['SalePrice'], plot=plt)\n(mu, sigma) = norm.fit(D_train['SalePrice'])\nplt.show()\nprint ( \"The mean and width of the house price distribution {:.2f} and  {:.2f} are \".format(mu, sigma))","3058967b":"#The targe (house price)  distribution plot after log transform   \nfig = plt.figure(figsize=(8,6))\nlogSalePrice=np.log1p(D_train['SalePrice'])\nsns.distplot(logSalePrice , fit=norm);\nplt.ylabel('GrLivArea')\nplt.xlabel('SalePrice')\nplt.show()\n\n\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(logSalePrice, plot=plt)\n(mu, sigma) = norm.fit(logSalePrice)\nplt.show()\n\n\nprint ( \"The mean and width of the house price distribution {:.2f} and  {:.2f} are \".format(mu, sigma))","71ab375e":"(mu, sigma) = norm.fit(logSalePrice)\nprint ( \"The mean and width of the house price distribution \\n after log transform  are:  {:.2f} and  {:.2f},  respectively \".format(mu, sigma))","9b2036c2":"# calculate and map the correlation matrix \nfig = plt.figure(figsize=(10,10))\ncorrmatrix = D_train.corr()\nsns.heatmap(corrmatrix, square=True, cmap=\"YlGnBu\")\nplt.show()","78a4b4e2":"#Keep the target first \nvar_target=D_train['SalePrice']\nvarIdtrain=D_train['Id']\nvarIdtest=D_test['Id']\n\n#check SalePrice \nplt.plot(var_target)\nplt.show\n\n","1d462b77":"# Drop the target and ID variables \n\nD_train=D_train.drop('SalePrice',axis=1)\nD_train=D_train.drop('Id', axis=1)\nD_test=D_test.drop('Id', axis=1)\n\n\n#Merge the D_train and D_test\n\nDataAll=pd.concat((D_train, D_test)).reset_index(drop=True)\nprint(\"Shape of the combined data is: {}\".format(DataAll.shape))","0928f023":"# let us see the amount of missing data for each variable \ntotal = DataAll.isnull().sum().sort_values(ascending=False)\n# percent = (DataAll.isnull().sum()\/DataAll.isnull().count()).sort_values(ascending=False)*100\n# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# missing_data.head(30)","98e04b27":"print(\" The number of missing values for each predictor are: \\n {}\".format(total.head(40)))","31915ee3":"#Check if the target itself has a missing value \nprint(\"The number of missing values for the target: {}\".format(var_target.isnull().sum()))","91b1d100":"#PoolQC, MiscFeature, Alley, Fence, FireplaceQu, NA values mean, that paramer is not available \n#so we replace by None \nDataAll[\"PoolQC\"] = DataAll[\"PoolQC\"].fillna(\"None\")\nDataAll[\"MiscFeature\"] = DataAll[\"MiscFeature\"].fillna(\"None\")\nDataAll[\"Alley\"] = DataAll[\"Alley\"].fillna(\"None\")\nDataAll[\"Fence\"] = DataAll[\"Fence\"].fillna(\"None\")\nDataAll[\"FireplaceQu\"] = DataAll[\"FireplaceQu\"].fillna(\"None\")\n\n\n# For LotFrontage,  use the neghbourhood value\n\nDataAll[\"LotFrontage\"] = DataAll.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n#Replace the following varaibles with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    DataAll[col] = DataAll[col].fillna('None')\n    \n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    DataAll[col] = DataAll[col].fillna(0)\n  #reolace missing values with zero   \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    DataAll[col] = DataAll[col].fillna(0)    \n  #NaN means no basement   \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    DataAll[col] = DataAll[col].fillna('None')\n    \nDataAll[\"MasVnrType\"] = DataAll[\"MasVnrType\"].fillna(\"None\")\nDataAll[\"MasVnrArea\"] = DataAll[\"MasVnrArea\"].fillna(0)   \n\nDataAll['MSZoning'] = DataAll['MSZoning'].fillna(DataAll['MSZoning'].mode()[0])\n\nDataAll = DataAll.drop(['Utilities'], axis=1)\n\nDataAll[\"Functional\"] = DataAll[\"Functional\"].fillna(\"Typ\")\n\nDataAll['Electrical'] = DataAll['Electrical'].fillna(DataAll['Electrical'].mode()[0])\n\nDataAll['KitchenQual'] = DataAll['KitchenQual'].fillna(DataAll['KitchenQual'].mode()[0])\n\n\nDataAll['Exterior1st'] = DataAll['Exterior1st'].fillna(DataAll['Exterior1st'].mode()[0])\n\nDataAll['Exterior2nd'] = DataAll['Exterior2nd'].fillna(DataAll['Exterior2nd'].mode()[0])\n\n\nDataAll['SaleType'] = DataAll['SaleType'].fillna(DataAll['SaleType'].mode()[0])\n\nDataAll['MSSubClass'] = DataAll['MSSubClass'].fillna(\"None\")\n","bfe71a54":"# Let us check if there is any missing data \n#print(DataAll.isnull().sum().sort_values(ascending=False))\nprint(\"The number of miising values now is{}\".format(DataAll.isnull().sum().max()))","437aabf7":"#Let us now convert categorical variables into dummy variables\nDataAll = pd.get_dummies(DataAll)\n\nprint(\"The number of dimesion of the all data after dummy conversion : {}\".format(DataAll.shape))","ec17e404":"#Now let us partition DataAll into the training and testing \n\nD_train=DataAll[:len(varIdtrain)]\nD_test=DataAll[len(varIdtrain):]","dd927781":"# It is time to import Our machine learning method \nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(D_train.values)\n    rmse= np.sqrt(-cross_val_score(model, D_train.values, logSalePrice, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n\n","d4f1f0a6":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n#KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=4, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n\n\n\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n","c0263272":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3eac74ad":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3a565009":"#score = rmsle_cv(KRR)\n#print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","505742bf":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8ad829bb":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","33cbd265":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","7560125b":"# Stacking \nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) \n    ","1b2fde61":"averaged_models = AveragingModels(models = (ENet, GBoost,model_xgb, model_lgb, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5cebe244":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bdc9b9a6":"ave_models=averaged_models.fit(D_train, logSalePrice)\nave_train_pred=np.expm1(ave_models.predict(D_train))\nave_pred = np.expm1(ave_models.predict(D_test))\nprint(rmsle(logSalePrice, ave_train_pred))\n\n#ave_pred_train=np.expm1(xgb_train_pred)\nave_targ_train=np.expm1(logSalePrice)\n\ncc_ave=np.corrcoef(ave_train_pred,ave_targ_train)\nprint(cc_ave)","3064c58c":"plt.plot(ave_pred)\nplt.show()","006b1c87":"mdlxg=model_xgb.fit(D_train, logSalePrice)\nxgb_train_pred = mdlxg.predict(D_train)\nxgb_pred = np.expm1(mdlxg.predict(D_test))\nprint(rmsle(logSalePrice, xgb_train_pred))\n\npred_train=np.expm1(xgb_train_pred)\ntarg_train=np.expm1(logSalePrice)\n","d9cb79cf":"cc_ta=np.corrcoef(pred_train,targ_train)","3d52d5f1":"cc_ta","eb239310":"mdlRF=GBoost.fit(D_train, logSalePrice)\nRF_train_pred=mdlRF.predict(D_train)","c575254b":"#plt.figure()\ncc_RF=np.corrcoef(np.expm1(RF_train_pred),np.expm1(logSalePrice))\nprint(\"The Random Forest correlation coefficient is {}\".format(cc_RF))","9137c7e0":"resultRF=np.expm1(mdlRF.predict(D_test))\nresultXGB=xgb_pred","005a7016":"plt.figure(figsize=(14,8))\nplt.plot(resultRF,label='RF')\nplt.plot(xgb_pred,'r', label='XBoost')\nplt.title('RF and XGB')\nplt.show()","02f93471":"# Variable Importance \nvar_importance=model_xgb.feature_importances_\nimportant_idx = np.where(var_importance > 0)\nimportant_var = D_train.columns[important_idx]\nsorted_idx = np.argsort(var_importance[important_idx])[::1]\nvar_importance=var_importance[important_idx]\nsorted_var=important_var[sorted_idx]\nsorted_var_importance=var_importance[sorted_idx]\npos = np.arange(sorted_idx.shape[0]) + -.5\nplt.figure(figsize=(18, 15))\nplt.barh(pos[100:], sorted_var_importance[100:], color='red',align='center')\nplt.yticks(pos[100:], sorted_var[100:],fontsize=20)\nplt.xlabel('Relative Importance',fontsize=20)\nplt.title('Variable Importance',fontsize=20)\nplt.margins(y=0)\nplt.tight_layout()\n#plt.savefig('VariableImportance.eps', facecolor='w', format='eps', dpi=1000)\nplt.show()","49faff38":"# Predicting House Price \n\nThis is my first Notebook in Kaggle. \n\nThis python notebook tries to solve predicting the Housing Price, a Kaggle competitions!","96613d63":"Awesome! we observe that the log transform has improved the distribution of the housing price \n","dc534a96":"Great! let us now deal with each predictor that has missing value \n","c8ad4c62":" ## Now let us plot and see the traget variable","5bb13555":"We observe that the target variable is not quite normaly distributed and is some what  right skewed. We must log transofrom the target variable and see if it improves the distribution \n","be1e56f7":"# Cleaning Missing Data\n\nWe need to identify and clean missing data.  If a large portion of a variable is missing we will remove the varibales totally. If a small porttion of the data is missing we should imploy certain interpolation or estimation technique to fill the gap.\n\nNote the ID parameter is not important and must be removed before predicting.\n\nBefore that let us megre teh training and test data. \n\n","79f567b3":"We observe that there are two low  SalePrices whose GrLivArea is large,  these are outliers and must be removed!\n\n\n\n","c34dab9b":"# Data Transormation and Cleaning \n\nWe need to transform the data (for example use log transform  for linear models) detect and clean (correct) unnessary data.  \nBelow are the main processes (** 4 C's**)  I am going to  do!\n\n1. Correcting (unusual values and outliers) \n2. Completing (missing values\/info.)\n3. Creating (new features for training) and\n4. Converting (fields to the correct format). \n\nThese four steps are the **4 C's**\n\nFirst we start with the target variable, **SalePrice**\n","f094fa4c":"This correlation matrix plot is very useful to see the correlation relationship between the target variable and all  predictors as well the correlation betwwen any pair of predictors among themseveles. For example loooking at the right edge of the map above we clearly see that target varib le (house price) is highly correlated with predictors **OverallQual, GrLivArea, TotalBsmtSF, GarageCars, GarageArea ** and so on. Constant or nearly constant  rows(columns) represent missing value or useless data and must be cleaned. We,  also,  need to be careful about two two highly correlated predictors.   "}}