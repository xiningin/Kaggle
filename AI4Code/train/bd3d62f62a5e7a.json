{"cell_type":{"e54dbda5":"code","6bb22808":"code","6ac37e7f":"code","13750820":"code","bc100e09":"code","62b4412d":"code","0c25fdc6":"code","c41254c3":"code","de88b42e":"code","977d015c":"code","dc99cf07":"code","d9b9d22d":"code","a26d6e64":"code","c5d20127":"code","18fca16d":"code","1817e2ac":"code","818f25f0":"code","9287fc6e":"code","ad47f3dd":"code","d4c8533d":"code","612eb0e8":"code","01b6f2d4":"code","2fca76ba":"code","093476a5":"code","b9b346ef":"code","59469248":"code","f12bd5c9":"code","5f8a0d18":"code","4f66e111":"code","280ce82a":"code","e0b3f058":"code","82798f0f":"code","f0807214":"code","f07f3041":"code","ccc4f086":"code","0b9cadfe":"code","9694a8a1":"code","cf6cf124":"code","ccb9ef6d":"code","3a04a70c":"code","6fd77985":"code","f47cc539":"code","e44cf7a0":"code","bcf9305e":"code","86512178":"code","7830021d":"code","1b736196":"code","18427bb9":"markdown","d78cfa1c":"markdown","1c53d85d":"markdown","92a5281e":"markdown","fbb531a3":"markdown","2badc8cb":"markdown","5b8c5cfe":"markdown","1c965f5c":"markdown","ae667448":"markdown","ccf3f846":"markdown","891968fb":"markdown","c7ebe979":"markdown","1f794091":"markdown","6b6a9863":"markdown","db5f537e":"markdown","d020d1aa":"markdown","5b1ece06":"markdown","e27486cb":"markdown","a599911d":"markdown","2f0fa991":"markdown","cc329003":"markdown","566bfb21":"markdown","a518e9a9":"markdown","d0b0021b":"markdown","bb77863e":"markdown","333e9b1d":"markdown","806d4df5":"markdown","9d1d534d":"markdown","6b31779c":"markdown","2f9b26f8":"markdown","1d770f32":"markdown","afdb076b":"markdown","f6c58430":"markdown","da9898f8":"markdown","8dd7c17c":"markdown","27493db6":"markdown","3df543d7":"markdown","7fc9fe98":"markdown","7bcdb0cb":"markdown","351aaf40":"markdown","73a67ec0":"markdown","630a19c0":"markdown","fdbac0f9":"markdown","77dd5be4":"markdown","d25bf792":"markdown","8f4f7029":"markdown","27cb6f50":"markdown","b31a2c87":"markdown","238aa541":"markdown","31d0381f":"markdown","1907ab0c":"markdown","7296d7f9":"markdown"},"source":{"e54dbda5":"ai_html = \"\"\"\n<html>\n  <head>\n   <title>\n     Web Scraping 101 - by aiadventures\n   <\/title>\n  <\/head>\n  <body>\n    <div id=\"course\">\n      <h3> Courses at \n        <a href=\"www.aiadventures.in\">aiadventures<\/a>\n      <\/h3>\n      <ul>\n        <li>Python<\/li>\n        <li>Data Science<\/li>\n        <li>Machine Learning<\/li>\n        <li>Deep Learning<\/li>\n        <li>Computer Vision<\/li>\n      <\/ul>\n    <\/div>\n    <div class=\"follow_us\">\n      <h3> Follow Us <\/h3>\n      <ul>\n        <li><a href=\"https:\/\/www.instagram.com\/aiadventures.pune\">Instagram<\/a><\/li>\n        <li><a href=\"https:\/\/www.linkedin.com\/company\/aiadventures\">LinkedIn<\/a><\/li>\n        <li><a href=\"https:\/\/medium.com\/aiadventures\">Medium<\/a><\/li>\n        <li><a href=\"https:\/\/www.youtube.com\/channel\/UCPZqWUIXZAs926TBRclhUGw\">Youtube<\/a><\/li>\n      <\/ul>\n    <\/div>\n  <\/body>\n<\/html>\n\"\"\"","6bb22808":"from IPython.core.display import display, HTML\ndisplay(HTML(ai_html))","6ac37e7f":"# import\nfrom bs4 import BeautifulSoup as bs","13750820":"soup = bs(ai_html)\nsoup","bc100e09":"soup.title","62b4412d":"soup.find('title')","0c25fdc6":"soup.find_all('div')","c41254c3":"## Select both h3 and ul tags\nsoup.find_all(['h3', 'ul'])","de88b42e":"soup.find('div', id='course')","977d015c":"soup.find('div', class_='follow_us')","dc99cf07":"## Selects all the 'div' which has 'id' attribute\nsoup.find('div', id=True)","d9b9d22d":"import re\nsoup.find(re.compile('di'), class_= re.compile('follow_us'))","a26d6e64":"title_tag = soup.find('title')\ntitle_tag","c5d20127":"title_tag.name","18fca16d":"title_tag.text","1817e2ac":"a_tag = soup.find('a')\na_tag","818f25f0":"a_tag['href']","9287fc6e":"[a_tag['href'] for a_tag in soup.find_all('a')]","ad47f3dd":"import requests","d4c8533d":"url = 'http:\/\/books.toscrape.com\/'\nresponse = requests.get(url)\nresponse","612eb0e8":"response.status_code","01b6f2d4":"print(response.text[:1000])","2fca76ba":"type(response.text)","093476a5":"soup = bs(response.text)\ntype(soup)","b9b346ef":"soup.find('title').text","59469248":"soup.find('title').text.strip()","f12bd5c9":"books_tag = soup.find_all('article', class_='product_pod')","5f8a0d18":"len(books_tag)","4f66e111":"book_tag = books_tag[0]\nbook_tag","280ce82a":"title_tag = book_tag.find('a', title=True)\ntitle_tag","e0b3f058":"title_tag.text","82798f0f":"title_tag['title']","f0807214":"## Title\ntitle = book_tag.find('a', title=True)['title']\ntitle","f07f3041":"## Rating\nrating = book_tag.find('p')['class'][1]\nrating","ccc4f086":"## Price\nprice = book_tag.find('p', class_='price_color').text[1:]\nprice","0b9cadfe":"## Book link\nlink = 'http:\/\/books.toscrape.com\/' + book_tag.find('a')['href']\nlink","9694a8a1":"def get_details(book_tag):\n    title = book_tag.find('a', title=True)['title']\n    rating = book_tag.find('p')['class'][1]\n    price = book_tag.find('p', class_='price_color').text[1:]\n    link = 'http:\/\/books.toscrape.com\/' + book_tag.find('a')['href']\n    return title, rating, price, link","cf6cf124":"def get_soup(url):\n    \"\"\"Takes URL and returns a soup object\"\"\"\n    resp = requests.get(url)\n    if resp.status_code == 200:\n        return bs(resp.text)\n    else: return None\n\n\ndef get_books(url):\n    \"\"\"Extact details from all the book tags\"\"\"\n    soup = get_soup(url)\n    book_tags = soup.find_all('article', class_='product_pod')\n\n    books = []\n    for book_tag in book_tags:\n        books.append(get_details(book_tag))\n\n    return books","ccb9ef6d":"url = 'http:\/\/books.toscrape.com\/'\nbooks = get_books(url)\nlen(books)","3a04a70c":"books[:3]","6fd77985":"import pandas as pd\n\ndef get_all_books(page = 3):\n    books = []\n    for i in range(1, page+1):\n        ## This is how the url changes with every page\n        url = f'http:\/\/books.toscrape.com\/catalogue\/page-{i}.html'\n        soup = get_soup(url)\n        if soup:    \n            book_tags = soup.find_all('article', class_='product_pod')\n\n            for book_tag in book_tags:\n                books.append(get_details(book_tag))\n            \n    books = pd.DataFrame(books, columns=['title', 'rating', 'price', 'link'])\n    return books","f47cc539":"df = get_all_books(3)\ndf.head()","e44cf7a0":"df.shape","bcf9305e":"import time\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup as bs\n\ndef get_soup(url):\n    \"\"\"Takes URL and returns a soup object\"\"\"\n    try:\n        resp = requests.get(url)\n    except:\n        return None\n    \n    if resp.status_code == 200:\n        return bs(resp.text)\n    else: \n        return None\n\n\ndef get_details(book_tag):\n    ## title\n    try:\n        title = book_tag.find('a', title=True)['title']\n    except:\n        title = None\n        \n    ## rating\n    try: \n        rating = book_tag.find('p')['class'][1]\n    except: \n        rating = None \n        \n    ## Price\n    try: \n        price = book_tag.find('p', class_='price_color').text[1:]\n    except:\n        price = None\n    \n    ## Link\n    try:\n        link = 'http:\/\/books.toscrape.com\/' + book_tag.find('a')['href']\n    except:\n        price = None\n        \n    return title, rating, price, link\n\n\n\ndef get_all_books(page = 3):\n    books = []\n    for i in range(1, page+1):\n        url = f'http:\/\/books.toscrape.com\/catalogue\/page-{i}.html'\n        soup = get_soup(url)\n        if soup:    \n            try:\n                book_tags = soup.find_all('article', class_='product_pod')\n\n                for book_tag in book_tags:\n                    books.append(get_details(book_tag))\n            except:\n                print(f'Error reading page {i} . . .')\n\n            time.sleep(1) # sleep before making the next request\n\n    books = pd.DataFrame(books, columns=['title', 'rating', 'price', 'link'])\n    return books","86512178":"df = get_all_books(50) # 20 books x 50 pages = 1000 books\ndf.head()","7830021d":"df.shape","1b736196":"df.to_csv('books.csv', index=False)","18427bb9":"Wonderful! we have our title tag. To get the title we can simply use `title_tag.text`","d78cfa1c":"Lets have a look at books . . .","1c53d85d":"you can also check the response status as follows","92a5281e":"DataFrame looks much better. We scraped 3 pages so we should have 60 (3 x 20) records. ","fbb531a3":"Since, `response.text` is simply a python string, we can directly pass it to `BeautifulSoup` to get the *soup* object","2badc8cb":"**Book title**\n\nThe title is present inside 'a' tag. We cannot select all the 'a' tags. We only want the tags with title attribute. So, lets select it.","5b8c5cfe":"The above code will select the first tag whose name matches the regular expression `di`, and where the value of class attribute matches the regular expression `follow_us`.","1c965f5c":"We will only scrape first 3 pages to test our code","ae667448":"# Scraping a Real web page\n\n**Web Scraping is not always the solution!**\n\nSome websites provide data that can be downloaded in CSV format, or can be accessible via Application Programming Interfaces (APIs). Use web scraping only when both these options are not available.\n\n**Legalities**\n\nGenerally, if you are going to use the scraped data for personal or educational purpose, then there may not be any problem. But if you are going to use it for commercial purpose then I will highly recommend you to do some background research about website's scraping policies as well about the data you are going to scrape.\n\nTo understand the terms and conditions of any website, you can start with **robots.txt** file. For any website, simply write *robots.txt* after the website address. For example, www.google.com\/robots.txt\n\n**Finally,**\n\nOnce you make sure that you are not breaking any law\/policy, then you should spend some time **analysis the web page**. Doing things like:\n- View page source\n- Inspect DOM elements\n- Is the page static\/dynamic ?\n- Is it using AJAX calls ?\n\nAfter you know answers to all the above questions. You are good to start web scraping.\n\nIn this notebook, we will scrape all the books from http:\/\/books.toscrape.com\/ website. So, try looking at page source, & also inspect the book element.","ccf3f846":"### Further Reading\n\nSo far, we have just scratched the surface. But I think this is good enough to get you started with `BeautifulSoup` & to scrape most of the static pages on the web. `BeautifulSoup` has much more to offer, like \n- Searching the tree, [read more](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#searching-the-tree)\n- CSS Selector, [read more](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#css-selectors)\n- Navigating DOM tree, [read more](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#navigating-the-tree)\n- Manipulating Elements, [read more](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#modifying-the-tree)\n- and much more . . .\n\nI will highly recommend you to take some time and read more about `BeautifulSoup`. Here are some good YT videos on Web Scraping\n- https:\/\/www.youtube.com\/watch?v=RUQWPJ1T6Zc\n- https:\/\/www.youtube.com\/watch?v=RKsLLG-bzEY\n\nNote: Both the videos are approximately 3hrs long but are worth it. Also, they include projects & some more project ideas for you to practice later.\n\nWe will now shift gears and will try to scrape a real web page.","891968fb":"**Note:** `find()` returns a *tag* object and `find_all()` retuns a *ResultSet* object which is very similar to *python list*. So, it's very important to keep checking the data type. Because, it tells you, what operations are allowed.","c7ebe979":"### Requests library\n\n`requests` is python library used to download the contents of the web page. With the help of `requests`, we can get the raw HTML of web pages which can then be parsed using `BeautifulSoup`. \n\nRemember, BeautifulSoup is a parsing library, it cannot fetch a web page by itself. ","1f794091":"and . . . you have your title. You can easily write every thing in just one line of code","6b6a9863":"There are 20 books in a page. Lets verify it","db5f537e":"We start by creating a `soup` object which will help us to extract details. The input can be a *string*, or a *file object*.","d020d1aa":"Take a minute to think & see if the above code make sense.","5b1ece06":"To access the inner Text, you can simple run `tag_element.text`.","e27486cb":"but as you can see, its not the complete title. For complete title of the book, can be extracted from the title attribute","a599911d":"**Note:** The `class` keyword is already taken python language, hence BeautifulSoup uses `class_` (extra '_' at the end).\n\nYou can also select a tag by checking if an attribute is present or not.","2f0fa991":"**Note:** `find()` returns only the first tag\/element. You can use `find_all()` to get a list of all the tags\/elements.  ","cc329003":"`get_details` function takes a 'book_tag', extracts all the details from it and returns them.\n\nLets write some more functions to ","566bfb21":"### Accessing information\n\nSo far, we have learnt how to select HTML tags. This is important because, once you have selected the elements, you can access all the information present inside it.\n\nEvery tag has 3 major components:\n- Tag name\n- Text between the open & close tags, called **Inner text**.\n- Tag attributes and its values\n\nLets see how we can extract all these information from our `title_tag`.","a518e9a9":"Perfection! Excatly what we expected. \n\nBefore we scrape all the 1000 books, we will have to take care of a few more things. \n\n- Whenever you are scraping a website, try to be responsilble. A normal user generally makes 2-5 requests (clicks) per minute. But you python program can make upto 1000 requests per second. This can use all the resources in the server. Sometimes, it can even crash the server. So, make sure you sleep for a couple of seconds before you make the next request.\n\n- There are multiple things that can go wrong when scraping a website, like network error, slow connection, timeout, element missing, code change, etc. So, its high recommended to use `try \/ except` blocks to handle errors effectively.\n\n\nThis is how your final code will look.","d0b0021b":"and to select *div* tag with `class = follow_us`, we can write . . .","bb77863e":"**`response.text` vs `response.content`**\n\n- `text` is the content of the response in **Unicode**, and `content` is the content of the response in **bytes**.\n\n- `text` would be preferred for textual responses, such as an HTML or XML document, and `content` would be preferred for \"binary\" filetypes, such as an image or PDF file.\n","333e9b1d":"<img width=\"800px\" src=\"https:\/\/i.imgur.com\/Kj8fCpd.png\" alt=\"aiadventures Logo\"\/>\n<br>\n<p text-align=\"center\"><a href=\"https:\/\/www.aiadventures.in\">Website <\/a> | <a href=\"https:\/\/www.instagram.com\/aiadventures.pune\/\">Instagram<\/a> | <a href=\"https:\/\/www.linkedin.com\/company\/aiadventures\">LinkedIn<\/a> | <a href=\"https:\/\/www.youtube.com\/channel\/UCPZqWUIXZAs926TBRclhUGw\">YouTube<\/a><\/p>","806d4df5":"Its time to inspect all the HTML tag and to identify the book tag so that we can extract information about the books.\n\n![](https:\/\/i.imgur.com\/LpmDAgg.jpg)\n\nAs you can see in the above image, our book is placed inside 'article' tag and the class name is 'product_pod'. We can easily select all the 'article' tags where the class is 'product_pod' using the below code . . .","9d1d534d":"Lets have a look at the site title","6b31779c":"Once you know how to extract attribute values, you can easily extract all the links by running the following code","2f9b26f8":"#### Inner text","1d770f32":"Since, its just a string. We can use **string operations** & **regex** to extract meaningful informations from it. Even though, python has great support for working with strings. It still would be a lot of work.\n\nIf you look closely, then you might have already noticed that the above HTML document have a particular structure to it. The most important element in a HTML document is **tag**, which may contain other tags\/strings. For example:\n- The complete document is built using tags, such as `<html>`, `<head>`, `<body>`, `<div>`, `<p>`, `<a>`, etc.\n- Each tag has a complementary closing tag, such as `<\/html>`, `<\/head>`, `<\/body>`, `<\/div>`, `<\/p>`, `<\/a>`, etc.\n- Tags can also have attributes.\n\n![](https:\/\/i.imgur.com\/wf3Ahyg.png)\n\nDon't worry if you looking at HTML for the first time. It might look wierd at first, but with time it will grow on you.\n\nThis structure allowed developers to write very efficient HTML parsers. These parsers make it super easy to extract information from the HTML document. Parsers also provide ways of navigating, searching, and modifying the parse tree.\n\n[Beautiful Soup](http:\/\/www.crummy.com\/software\/BeautifulSoup\/) is a Python library for extracting data out of HTML and XML files. There are many such libraries like [requests-html](https:\/\/requests.readthedocs.io\/projects\/requests-html\/en\/latest\/index.html), [lxml](https:\/\/lxml.de\/), [gazpacho](https:\/\/gazpacho.xyz\/), etc. for parsing HTML docs.","afdb076b":"Once you have completed both the assignments, you can get your solutions reviewed from the mentor.","f6c58430":"You can also select multiple tags by passing a list of tags. ","da9898f8":"Great! Our selection is perfect.\n\nNow, lets try to select a single book and extract all the information we can","8dd7c17c":"### What & Why Web Scraping?\nWeb is the greatest source of information. Web scraping, allows us to extract, parse, download and organize useful information from the web automatically.\n\nThis ability to extract data from the web (programmatically) can be very helpful in a lot of different fields. For example:\n- Machine Learning & Data Science\n- Research\n- Business Intelligence\n- E-commerce Websites\n- Marketing and Sales Campaigns\n- Search Engine Optimization (SEO)\n\nYou can google and read more about web scraping. But for now, we wil get started . . . \n\nAll the web pages are build using HTML(Hyper Text Markup Language). Here is a sample HTML code. ","27493db6":"#### Regular Expressions\n\nEverywhere, you can **use regular expressions** (instead of strings) to select tags & its attribute values.","3df543d7":"not very pretty to look at. We can use `strip()` string method","7fc9fe98":"## Selecting Tags\nTo extract information, first we will have to learn how to select\/search HTML tags. Only after selecting HTML tags, we can extraction meaningful information from the tags. So, lets get started","7bcdb0cb":"Amazing! everything looks perfect. Lets save it","351aaf40":"Great, we have created our own dataset using BeautifulSoup. For more practice, here are some assignments for you.\n\n#### 1. Scraping more details\n\nAs an exercise, try to open the webpage for each book and extract some more details about each book; like `Price (excl. tax)`, `Price (incl. tax)`, `Tax`, `Availability` & `Number of reviews`. \n\nAt the end, you data frame will have the following columns `title`, `rating`, `Price (excl. tax)`, `Price (incl. tax)`, `Tax`, `Availability` & `Number of reviews`\n\nThis would be a perfect exercise to practice everything that you have learnt in this notebook. ","73a67ec0":"The output looks exactly the same. But under the hood, the complete string has being parsed and organised in the form of a tree, for easy access. For example,","630a19c0":"#### Tag name\n\nTo access the tag name, you can simple run `tag_element.name`.","fdbac0f9":"### Searching by Tag attributes\n\nSometimes, we want to select tags based on attributes & its value. Its pretty simple, just pass the attribute (with the value) as an argument.\n\nFor example, to select *div* tag with `id = course`, we can write . . . ","77dd5be4":"Once you have the tag, just think of it as a dictionary. You can easily access any attribute by passing it as a key.","d25bf792":"#### Searching by Tag names\nThe easiest way to search a tag (in BeautifulSoup) is to **search by its name**. You can simply select `title` tag by using `find` method.","8f4f7029":"*200* means the request was successfully served. These are called **HTTP response status codes**. You can read more about them, [here](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Status).\n\nNow, that the request to http:\/\/books.toscrape.com\/ has been successfully served, we can get the all HTML text by calling `response.text`. Lets look at the first 1000 characters.","27cb6f50":"#### Attribute values\n\nYou can also extract the attribute values as follows:","b31a2c87":"Lets put the above code inside a function","238aa541":"This is how the above code looks after rendering.","31d0381f":"Not very pretty, right? Don't worry, we will make a pandas dataframe.\n\nIts time we write a function that would extract all the 1000 books from the website. ","1907ab0c":"Follow the same process (as above) to extract ratings, price & book_link.","7296d7f9":"#### 2. Scraping Indian states & Union Territories data \n\nHere, You are suppose to scrape wikipedia page of each Indian States and Union Territories. Collect data points like `Name`, `Formation Date`, `Capital`, `Area`, `Population`, `Literacy Rate`, `Literacy Rate`, `GDP`, `Language`, and `Human Development Index` for each Indian state\/union territory.\n\nThe final dataframe should be of shape (36, 10) i.e 36 rows and 10 columns."}}