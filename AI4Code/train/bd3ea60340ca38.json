{"cell_type":{"99f2130b":"code","bd14156a":"code","1b990fdc":"code","42855fc1":"code","db9b1432":"code","541fb0ae":"code","0b8386a3":"code","7e806113":"code","8b3fad0a":"code","bc2b1892":"code","12ecd35f":"code","186b8483":"code","fb4ff247":"code","aaee34a6":"code","72df42e7":"code","58511d66":"code","ae941b3c":"code","2da471c6":"code","b3329279":"code","cf014cb3":"code","ce7a890b":"code","19743840":"code","bd1a8f4f":"code","a2126e43":"code","1ae55ef5":"code","4a9d45dc":"code","133d6acf":"code","4f94f454":"code","d15b3600":"code","4b7e2502":"code","64b8253a":"code","a8a10af4":"code","1a0fa73b":"code","a258dbed":"code","a90be009":"code","343adfaf":"code","5a6e66cc":"code","b0757b31":"code","7cdbb961":"code","6a0802cf":"code","dcd77543":"code","207fd738":"code","32d1d2c4":"code","64d29561":"code","d0317555":"code","b2e665af":"code","685a7546":"code","a619848b":"code","379da584":"code","2a12cd3f":"code","5808dad8":"code","83081254":"code","ed7a76ae":"code","7dce533c":"code","74c96e78":"code","7bb674c4":"code","d8f508f3":"code","19a1b6fc":"code","345cb98e":"code","0534e91e":"code","269a2597":"code","6c40bb0b":"code","d046e78d":"code","eaae7647":"code","99c07dca":"code","1cd1ee42":"code","4a370b38":"code","dd33fb16":"code","86628234":"code","3dc7b40b":"code","b1bc298d":"code","a1488e40":"code","7d558e2f":"code","b0983d56":"code","e4a97232":"code","df12cb16":"code","c48e115a":"code","1d132ac2":"code","d0893479":"code","46be0fad":"code","8542d579":"code","4b88d1fa":"code","59c455a7":"code","8a92fc27":"code","9fb000fc":"code","81ab0c14":"code","d363b435":"code","5385e4de":"code","ae4554fe":"code","d8cfb3b3":"code","fe48db5f":"code","286487f3":"code","88d42c7c":"code","62a7e513":"code","358895a9":"code","d53a5024":"code","15f52d70":"code","c24fe035":"code","ea3b1a82":"code","b32965d0":"code","48ca205e":"code","a2f83e50":"code","44211c68":"code","8b95ed10":"markdown","3c8c8670":"markdown","5391bdaf":"markdown","a0b6d8d0":"markdown","7199129f":"markdown","00f07c39":"markdown","975432be":"markdown","093f0695":"markdown"},"source":{"99f2130b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Import visualization libraries \nimport matplotlib.pyplot as plt # Matlab-style plotting\nimport seaborn as sns\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd14156a":"# Reading the train and the test datasets \ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1b990fdc":"# How many rows and columns we have in each dataset \nprint(train.shape)\nprint('*' * 10)\nprint(test.shape)","42855fc1":"# let's try to understand our data\ntrain.head()","db9b1432":"# Get some information about our data\n# As we can see we have null values which we need to deal with \nprint(train.info())\nprint('*' * 30)\nprint(test.info())","541fb0ae":"# Finding null values \n# As our data is large so we better visualize them\ntrain.isnull().sum()","0b8386a3":"# Calculate percentage of our missing values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\n# As our data is large it's better to visualize the missing values\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","7e806113":"# Same for testing data \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","8b3fad0a":"# A common approach is that we drop all columns which their missing values exceeds 60%\ntrain.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1, inplace=True)\ntest.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1, inplace=True)","bc2b1892":"# Now let's deal with the rest of the missing data\ntrain.isnull().sum()","12ecd35f":"# We need to know the values we have to determine which approach to use when filling the data \ntrain.LotFrontage.nunique()","186b8483":"# My option would be to randomly fill these missing values with values close to the mean but within one standard deviation.\n# As our missing data is large so we don't want to fill the data with mean values so now change in the dist happen\nLotFrontage_avg = train['LotFrontage'].mean()\nLotFrontage_std = train['LotFrontage'].std()\nLotFrontage_null_count = train['LotFrontage'].isnull().sum()\nLotFrontage_null_random_list = np.random.randint(LotFrontage_avg - LotFrontage_std, LotFrontage_avg + LotFrontage_std, size=LotFrontage_null_count)\ntrain['LotFrontage'][np.isnan(train['LotFrontage'])] = LotFrontage_null_random_list\ntrain['LotFrontage'] = train['LotFrontage'].astype(int)","fb4ff247":"# Same for Test dataset\nLotFrontage_avg = test['LotFrontage'].mean()\nLotFrontage_std = test['LotFrontage'].std()\nLotFrontage_null_count = test['LotFrontage'].isnull().sum()\nLotFrontage_null_random_list = np.random.randint(LotFrontage_avg - LotFrontage_std, LotFrontage_avg + LotFrontage_std, size=LotFrontage_null_count)\ntest['LotFrontage'][np.isnan(test['LotFrontage'])] = LotFrontage_null_random_list\ntest['LotFrontage'] = test['LotFrontage'].astype(int)","aaee34a6":"# Calculating percentage of missing values \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","72df42e7":"# Calculating percentage of missing values \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","58511d66":"# Before dealing with GarageFinish we need to remember it's data type \n# This will help us determine which way we fill the missing data\ntrain.GarageFinish.dtype","ae941b3c":"# What are the values of GarageFinish\ntrain.GarageFinish.value_counts(dropna=False)","2da471c6":"# Trying to find if their is a relation between GarageCars & GarageFinish\n# We found that whenever a GarageFinish is null Garage cars == 0\ntrain['GarageCars'][train['GarageFinish'].isnull() == True].head(20)","b3329279":"test['GarageCars'][test['GarageFinish'].isnull() == True].head()","cf014cb3":"train['GarageCars'].fillna(value=0, inplace=True)\ntest['GarageCars'].fillna(value=0, inplace=True)","ce7a890b":"# Create a new category we will call it Nfn\ntrain['GarageFinish'].fillna(value='Nfn', inplace=True)\ntest['GarageFinish'].fillna(value='Nfn', inplace=True)","19743840":"# Now we find all values in GarageType\ntrain.GarageType.value_counts(dropna=False)","bd1a8f4f":"test.GarageType.value_counts(dropna=False)","a2126e43":"# Same as above we create a new category called Nogarage\ntrain['GarageType'].fillna(value='Nogarage', inplace=True)\ntest['GarageType'].fillna(value='Nogarage', inplace=True)","1ae55ef5":"train.GarageCond.value_counts(dropna=False)","4a9d45dc":"test.GarageCond.value_counts(dropna=False)","133d6acf":"train['GarageCond'].fillna(value='NG', inplace=True)\ntest['GarageCond'].fillna(value='NG', inplace=True)","4f94f454":"# Now to see the remaining features and their missing value percentage \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","d15b3600":"total = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","4b7e2502":"train.GarageQual.value_counts(dropna=False)","64b8253a":"test.GarageQual.value_counts(dropna=False)","a8a10af4":"# Filling missing data in both train and test with NG\ntrain['GarageQual'].fillna(value='NG', inplace=True)\ntest['GarageQual'].fillna(value='NG', inplace=True)","1a0fa73b":"train.GarageYrBlt.nunique(dropna=False)","a258dbed":"# As we can see in the test GarageYrBlt there are outliers  \nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\ntrain.GarageYrBlt.hist(ax=ax[0])\nax[0].set_title('Train GarageYrBlt', fontsize=15)\ntest.GarageYrBlt.hist(ax=ax[1])\nax[1].set_title('Test GarageYrBlt', fontsize=15)","a90be009":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\ntrain.YearBuilt.hist(ax=ax[0])\ntest.YearBuilt.hist(ax=ax[1])\n\nax[0].set_title('Train YearBuilt', fontsize=15)\nax[1].set_title('Test YearBuilt', fontsize=15)\nplt.tight_layout()","343adfaf":"# As we can see that they are similar \ntrain[['GarageYrBlt', 'YearBuilt']]","5a6e66cc":"# Also in the test dataset\ntest[['GarageYrBlt', 'YearBuilt']]","b0757b31":"# So we can fill the missing values with it's corresponding YearBuilt data\ntrain['YearBuilt'][train.GarageYrBlt.isnull() == True]","7cdbb961":"# Filling the GarageYrBlt missing values with it's corresponding YearBuilt values \ntrain.GarageYrBlt.fillna(value=train['YearBuilt'][train.GarageYrBlt.isnull() == True], inplace=True)\ntest.GarageYrBlt.fillna(value=test['YearBuilt'][test.GarageYrBlt.isnull() == True], inplace=True)","6a0802cf":"# Now let's see the remaining missing values in the train data \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","dcd77543":"# Now let's see the remaining missing values in the test data \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","207fd738":"train.BsmtFinType2.nunique(dropna=False)","32d1d2c4":"train.BsmtFinType2.value_counts(dropna=False)","64d29561":"test.BsmtFinType2.value_counts(dropna=False)","d0317555":"train[['BsmtFinType2', 'BsmtFinSF2']]","b2e665af":"print(train.BsmtFinType2.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.BsmtFinType2.value_counts(dropna=False))","685a7546":"# Filling the BsmrFinType2 with it's mode\ntrain.BsmtFinType2.fillna(train.BsmtFinType2.mode()[0], inplace=True)\ntest.BsmtFinType2.fillna(test.BsmtFinType2.mode()[0], inplace=True)","a619848b":"train['BsmtFinSF1'][train.BsmtFinType1.isnull() == True].head(8)","379da584":"print(train.BsmtFinType1.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.BsmtFinType1.value_counts(dropna=False))","2a12cd3f":"train.BsmtFinType1.mode()","5808dad8":"# Same as before we will fill the missing data with the mode\ntrain.BsmtFinType1.fillna(train.BsmtFinType1.mode()[0], inplace=True)\ntest.BsmtFinType1.fillna(test.BsmtFinType1.mode()[0], inplace=True)","83081254":"# We now understand that all basement features depend on each other \n# So fill the rest with their mode\ntrain.BsmtExposure.fillna(value=train.BsmtExposure.mode()[0], inplace=True) \ntrain.BsmtQual.fillna(value=train.BsmtQual.mode()[0], inplace=True) \ntrain.BsmtCond.fillna(value=train.BsmtCond.mode()[0], inplace=True)\n\n# Same as for testing data\ntest.BsmtExposure.fillna(value=test.BsmtExposure.mode()[0], inplace=True) \ntest.BsmtQual.fillna(value=test.BsmtQual.mode()[0], inplace=True) \ntest.BsmtCond.fillna(value=test.BsmtCond.mode()[0], inplace=True)","ed7a76ae":"# Now let's see what is still missing  \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","7dce533c":"total = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","74c96e78":"train.FireplaceQu.nunique(dropna=False)","7bb674c4":"# What are the values for FireplaceQu feature\nprint(train.FireplaceQu.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.FireplaceQu.value_counts(dropna=False))","d8f508f3":"# As we can see here the FireplaceQu values are missing when Fireplaces value is equal to 0\ntrain[['Fireplaces', 'FireplaceQu']].head(20)","19a1b6fc":"# Filling the missing data with NG which we created \ntrain['FireplaceQu'].fillna(value='NG', inplace=True)\ntest['FireplaceQu'].fillna(value='NG', inplace=True)","345cb98e":"# What are the values of MasVnrType\ntrain.MasVnrType.value_counts(dropna=False)","0534e91e":"# What are the values of MasVnrArea\ntrain.MasVnrArea.value_counts(dropna=False).head(20)","269a2597":"# We need to find the relation between MasVnrType & MasVnrArea\n# As there are null values in both of them we can't get any information like the others before \ntrain['MasVnrType'][train['MasVnrArea'].isnull() == True]","6c40bb0b":"# Drop the remaining missing rows \ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)","d046e78d":"# Now let's see our data if we missed anything by accident   \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","eaae7647":"# As we can see we finished cleaning the training dataset but the testing still needs a little bit more cleaning\n# Now let's see our data if we missed anything by accident   \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()\/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","99c07dca":"(train.shape, test.shape)","1cd1ee42":"# Using correlation heatmap\nplt.figure(figsize=(17, 10))\nsns.heatmap(train.corr(), annot=True, cmap='coolwarm')","4a370b38":"train.corr()['SalePrice'].sort_values(ascending=False)[:11]","dd33fb16":"train.OverallQual.value_counts(dropna=False)","86628234":"plt.figure(figsize=(17, 10))\nsns.countplot(x='OverallQual', data=train)","3dc7b40b":"plt.figure(figsize=(17, 10))\nsns.barplot(x='OverallQual', y='SalePrice', data=train)","b1bc298d":"train.GrLivArea.nunique(dropna=False)","a1488e40":"plt.figure(figsize=(15, 10))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice, edgecolors=\"black\")","7d558e2f":"sns.lmplot(x='GrLivArea', y='SalePrice',data=train, size=10)","b0983d56":"train.GarageCars.nunique(dropna=False)","e4a97232":"train.GarageCars.value_counts(dropna=False)","df12cb16":"plt.figure(figsize=(15, 10))\nsns.countplot(x='GarageCars', data=train)","c48e115a":"plt.figure(figsize=(15, 10))\nsns.boxplot(x='GarageCars', y='SalePrice', data=train)","1d132ac2":"plt.figure(figsize=(15, 10))\nsns.barplot(x='GarageCars', y='SalePrice', data=train)","d0893479":"train.GarageArea.nunique()","46be0fad":"columns = ['MSZoning', 'Street',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']","8542d579":"final_df = pd.concat([train, test], axis=0)","4b88d1fa":"final_df.shape","59c455a7":"def One_hot_encoding(columns):\n    df_final=final_df\n    i=0\n    for fields in columns:\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:           \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final","8a92fc27":"final_df = One_hot_encoding(columns)","9fb000fc":"final_df.shape","81ab0c14":"final_df =final_df.loc[:,~final_df.columns.duplicated()]","d363b435":"final_df.shape","5385e4de":"df_Train=final_df.iloc[:1422,:]\ndf_Test=final_df.iloc[1422:,:]","ae4554fe":"df_Train.shape","d8cfb3b3":"df_Test.shape","fe48db5f":"df_Test.drop(['SalePrice'],axis=1,inplace=True)","286487f3":"X_train=df_Train.drop(['SalePrice'],axis=1)\ny_train=df_Train['SalePrice']","88d42c7c":"X_train.shape","62a7e513":"num_estimators = [500,1000]\nlearn_rates = [0.02, 0.05]\nmax_depths = [1, 2]\nmin_samples_leaf = [5,10]\nmin_samples_split = [5,10]\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split}\n\nrandom_search =RandomizedSearchCV(GradientBoostingRegressor(loss='huber'), param_grid, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n\nrandom_search.fit(X_train, y_train)","358895a9":"# Best params\nrandom_search.best_params_","d53a5024":"# Train the model \nrandom_search.fit(X_train, y_train)","15f52d70":"# Accuracy for training data\ngboost_score=random_search.score(X_train,y_train)\nprint(f'{round(gboost_score * 100, 2)}%')","c24fe035":"# Predictions\npred = random_search.predict(df_Test)","ea3b1a82":"pred","b32965d0":"pred_df=pd.DataFrame(pred)\nsample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndata= pd.concat([sample['Id'],pred_df], axis=1)\ndata.columns=['Id','SalePrice']\ndata.to_csv('sample_submission1.csv',index=False)","48ca205e":"data.tail()","a2f83e50":"data.head()","44211c68":"data.shape","8b95ed10":"Using pearson correlation heatmap won't be useful as the data is large","3c8c8670":"## First we read our data into a pandas dataframe ","5391bdaf":"### First Identify the missing data ","a0b6d8d0":"### So we found out that we can convert  the missing values into a category \n#### We 'll call it Nfn which stands for No Finish ","7199129f":"# Dealing with missing values","00f07c39":"# Now let's start understanding our data","975432be":"## Using GradientBoosting with RandomizedSearch ","093f0695":"# Time for some EDA (Exploratory Data Analysis)"}}