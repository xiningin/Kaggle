{"cell_type":{"1a4312c8":"code","13093dae":"code","79a25e37":"code","abf6a37e":"code","360b7973":"code","6de5f0e1":"code","dd945013":"code","cb2eb528":"code","e34fa7cc":"code","b9b14771":"code","a18697d9":"code","5dab87ec":"code","eb45d201":"code","b247edd8":"code","64d54193":"code","86b8ce1d":"code","03e5e1c0":"code","1020a962":"code","34281771":"code","0b8f4192":"code","e36e9c00":"markdown","5dd3e1d6":"markdown","dfecade4":"markdown","6a884eca":"markdown","f0be193b":"markdown","e5281e69":"markdown","af97351f":"markdown"},"source":{"1a4312c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","13093dae":"df = pd.read_csv('\/kaggle\/input\/consumer-complaint-database\/rows.csv', nrows=99999) # limiting the rows\ndf.head()","79a25e37":"df = df[pd.notnull(df['Consumer complaint narrative'])]","abf6a37e":"df.head()","360b7973":"col = ['Product', 'Consumer complaint narrative']\ndf = df[col]","6de5f0e1":"df.head()","dd945013":"df['category_id'] = df['Product'].factorize()[0]\nfrom io import StringIO\ncategory_id_df = df[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Product']].values)","cb2eb528":"category_to_id","e34fa7cc":"df.head()","b9b14771":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ndf.groupby('Product').count().plot.bar(ylim=0)\nplt.show()","a18697d9":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidfVector = TfidfVectorizer(sublinear_tf=True, min_df=50, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfeatures = tfidfVector.fit_transform(df['Consumer complaint narrative']).toarray()\nlabels = df.category_id\nfeatures.shape","5dab87ec":"# We can use sklearn.feature_selection.chi2 to find the terms that are the most correlated with each of the products:\nfrom sklearn.feature_selection import chi2\nimport numpy as np\nN = 2\nfor Product, category_id in sorted(category_to_id.items()):\n    features_chi2 = chi2(features, labels == category_id)\n    indices = np.argsort(features_chi2[0])\n    feature_names = np.array(tfidfVector.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n    print(\"# '{}':\".format(Product))\n    print(\" Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n    print(\" Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","eb45d201":"X_train = features\nY_train = labels","b247edd8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=CV)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","64d54193":"cv_df.groupby('model_name').accuracy.mean()","86b8ce1d":"# Continue with our best model (LogisticRegression), we are going to look at the confusion matrix, \n# and show the discrepancies between predicted and actual labels.\nfrom sklearn.model_selection import train_test_split\nmodel = LogisticRegression(random_state=0)\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","03e5e1c0":"from sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","1020a962":"# To manually verify why some failed\nfrom IPython.display import display\n\nfor predicted in category_id_df.category_id:\n    for actual in category_id_df.category_id:\n        if predicted != actual and conf_mat[actual, predicted] >= 6:\n            print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', 'Consumer complaint narrative']])\n            print('')","34281771":"# Classification report\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))","0b8f4192":"# Some manual tests for new input data\ntexts = [\"I requested a home loan modification through Bank of America. Bank of America never got back to me.\",\n         \"It has been difficult for me to find my past due balance. I missed a regular monthly payment\",\n         \"I can't get the money out of the country.\",\n         \"I have no money to pay my tuition\",\n         \"Coinbase closed my account for no reason and furthermore refused to give me a reason despite dozens of request\"]\ntext_features = tfidfVector.transform(texts)\npredictions = model.predict(text_features)\nfor text, predicted in zip(texts, predictions):\n    print('\"{}\"'.format(text))\n    print(\"  - Predicted as: '{}'\".format(id_to_category[predicted]))\n    print(\"\")","e36e9c00":"This notebook follows this article:\nhttps:\/\/towardsdatascience.com\/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f","5dd3e1d6":"We will calculate a measure called Term Frequency, Inverse Document Frequency, abbreviated to tf-idf. \n\nWe will use sklearn.feature_extraction.text.TfidfVectorizer to calculate a tf-idf vector for each of consumer complaint narratives:\n1. sublinear_df is set to True to use a logarithmic form for frequency.\n1. min_df is the minimum numbers of documents a word must be present in to be kept.\n1. norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1.\n1. ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.\n1. stop_words is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) to reduce the number of noisy features.","dfecade4":"The vast majority of the predictions end up on the diagonal (predicted label = actual label), where we want them to be. However, there are a number of misclassifications, and it might be interesting to see what those are caused by:","6a884eca":"For this project, we need only two columns \u2014 \u201cProduct\u201d and \u201cConsumer complaint narrative\u201d.\n\nInput: Consumer_complaint_narrative\nExample: \u201c I have outdated information on my credit report that I have previously disputed that has yet to be removed this information is more then seven years old and does not meet credit reporting requirements\u201d\n\nOutput: product\nExample: Credit reporting\n\nWe will remove missing values in \u201cConsumer complaints narrative\u201d column, and add a column encoding the product as an integer because categorical variables are often better represented by integers than strings.\nWe also create a couple of dictionaries for future use.","f0be193b":"However, in our case of learning imbalanced data, the majority classes might be of our great interest. It is desirable to have a classifier that gives high prediction accuracy over the majority class, while maintaining reasonable accuracy for the minority classes. Therefore, we will leave it as it is.","e5281e69":"The data looks imbalanced. \n\nWhen we encounter such problems, we are bound to have difficulties solving them with standard algorithms. Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration. In the worst case, minority classes are treated as outliers and ignored. \n\nFor some cases, such as fraud detection or cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for example by undersampling or oversampling each class.","af97351f":"## Multi-Class Classifier"}}