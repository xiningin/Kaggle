{"cell_type":{"6f2773ee":"code","b7b2eec6":"code","65f93ac9":"code","e2cdc7fd":"code","ef18c2e8":"code","51023825":"code","b967d6b5":"code","ce7e8ce5":"code","ccef4491":"code","9758c309":"code","c59e35c2":"code","0b5d5ce0":"code","eae5cec3":"code","ec9dcbb5":"code","9016c195":"code","3ae5eb68":"code","98cf517f":"code","3e3c9e8e":"code","bccf1d85":"code","bf39a2cf":"code","145fd094":"code","ee418e6e":"code","88963855":"code","f53ec364":"code","d6ea5155":"code","43c0aa46":"code","8bb7c842":"code","596ef63d":"code","e952b853":"code","157fdef9":"code","3dc2ed92":"code","d879d0c9":"code","aeea03e3":"code","7d6de6ed":"code","bff9e980":"code","0aa31cf9":"code","4ccab8fd":"code","7405d2bd":"code","b6ef41e9":"code","dfaf0047":"code","2405b125":"code","e7d74098":"code","fe2eab26":"code","a3ee7248":"markdown","37c6c8dd":"markdown","4791a1aa":"markdown","733bf6c1":"markdown","49420282":"markdown","7aa29e9e":"markdown","31e9bb8c":"markdown","a1dfc98d":"markdown","9f0f3e31":"markdown","38f0fab3":"markdown","3a1682fe":"markdown","8ca068c4":"markdown","7bc4b3ea":"markdown","8572c8a0":"markdown","c6c4fc6e":"markdown","da6b3406":"markdown","a15407d5":"markdown","52c0e4b3":"markdown","81e805da":"markdown","257f1735":"markdown","83f3b7c3":"markdown","672aa5ec":"markdown","949f16f9":"markdown","e92c661f":"markdown","7814c399":"markdown","6322e9fb":"markdown","c5bf45bb":"markdown","201e196e":"markdown"},"source":{"6f2773ee":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","b7b2eec6":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","65f93ac9":"train_size = len(df)","e2cdc7fd":"features = pd.concat([df.drop('SalePrice',axis=1),df_test],sort=False).reset_index(drop=True)\ntarget = df['SalePrice'].copy()","ef18c2e8":"df.head()","51023825":"print('number of numeric columns: ' + str(len(df.select_dtypes(['int64','float64']).columns)))\nprint('number of categorical columns: ' + str(len(df.select_dtypes(['object']).columns)))","b967d6b5":"#correlation to SalePrice\nplt.figure(figsize=(12,6))\ndf.corr()['SalePrice'].drop('SalePrice').sort_values().plot(kind='bar')","ce7e8ce5":"sns.jointplot(x='OverallQual', y='SalePrice', data=df)\n\nsns.jointplot(x='GrLivArea', y='SalePrice', data=df)","ccef4491":"#23 catergorical columns with missing values\nmissing_str = features.select_dtypes('object').isnull().sum().sort_values(ascending=False)\nmissing_str[missing_str>0]","9758c309":"fill_none = ['MiscFeature','Alley','Fence','FireplaceQu','GarageType','GarageCond', 'GarageQual','GarageFinish','BsmtCond','BsmtFinType1', 'BsmtFinType2', 'BsmtExposure', 'BsmtQual','MasVnrType']\nfeatures[fill_none] = features[fill_none].fillna('None',inplace=False)","c59e35c2":"sns.scatterplot(x='PoolArea',y='SalePrice',data=df.fillna('Missing'),hue='PoolQC')","0b5d5ce0":"features['PoolQC'].fillna('N',inplace=True)\nfeatures['PoolQC'].replace({'Ex':'Y','Fa':'Y','Gd':'Y'},inplace=True)\nfeatures.drop('PoolArea',axis=1,inplace=True)","eae5cec3":"otr_cate = ['MSZoning', 'Utilities', 'Functional', 'Electrical', 'SaleType', 'Exterior2nd', 'Exterior1st', 'KitchenQual']\nfor c in otr_cate:\n    features[c] = features[c].fillna(features[c].mode()[0],inplace=False)","ec9dcbb5":"#11 catergorical columns with missing values\nmissing_num = features.select_dtypes(['int64','float64']).isnull().sum().sort_values(ascending=False)\nmissing_num[missing_num>0]","9016c195":"df.plot(x='LotFrontage',y='LotArea',kind='scatter')","3ae5eb68":"features['LotFrontage'].fillna(features['LotFrontage'].median(),inplace=True)","98cf517f":"features[features['GarageCond']=='None']['GarageYrBlt'].isnull().sum()","3e3c9e8e":"features['GarageYrBlt'].fillna(0,inplace=True)","bccf1d85":"fill_zero = ['GarageArea','GarageCars','MasVnrArea','BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']\nfeatures[fill_zero] = features[fill_zero].fillna(0)","bf39a2cf":"# focusing on first row, and found some point does not follow the trend\nsns.pairplot(df[['SalePrice','GrLivArea','1stFlrSF','TotalBsmtSF','LotFrontage','LotArea']].dropna())","145fd094":"drop_index = [] # list to contain all rows should be excluded\ndrop_index.append(df[(df['GrLivArea']>4500) & (df['SalePrice']<400000)].loc[:train_size].index)\ndrop_index.append(df[(df['1stFlrSF']>4000) & (df['SalePrice']<400000)].loc[:train_size].index)\ndrop_index.append(df[(df['TotalBsmtSF']>6000) & (df['SalePrice']<400000)].loc[:train_size].index)\ndrop_index.append(df[df['LotArea']>100000].index)\ndrop_index.append(df[df['LotFrontage']>300].index)\n\ndrop_index = list(map(list,drop_index)) # IndexObject -> python list\ntmp = []\nfor sublist in drop_index:\n    for item in sublist:\n        tmp.append(item)\ndrop_index = list(set(tmp))  # merge into single list and take set() to remove the duplicated\n\nfeatures.drop(drop_index,inplace=True)\ntarget.drop(drop_index,inplace=True)","ee418e6e":"features['MSSubClass'] = features['MSSubClass'].astype('object')","88963855":"features['Utilities'].value_counts()","f53ec364":"features.drop('Utilities',axis=1,inplace=True)","d6ea5155":"features.drop('Id',axis=1,inplace=True)","43c0aa46":"df.plot(x='LowQualFinSF',y='SalePrice',kind='scatter')","8bb7c842":"features['LowQualFinSF'] = features['LowQualFinSF'].apply(lambda x: 'Y' if x>0 else 'N')","596ef63d":"#before take square root\nsns.jointplot(x=df['GrLivArea'],y=df['SalePrice'].apply(np.log1p),data=df,kind='reg')","e952b853":"#after take square root\nsns.jointplot(x=df['GrLivArea']**0.5,y=df['SalePrice'].apply(np.log1p),data=df,kind='reg')","157fdef9":"features['GrLivArea'] = features['GrLivArea']**0.5","3dc2ed92":"#predictor pairwise correlation check\ncorr_matrix = features.corr()\ncolinearity = {}\nfor column in corr_matrix.columns:\n    index = corr_matrix[corr_matrix[column]>0.6].index\n    for indice in index:\n        if not column == indice:\n            if not indice+' '+column in colinearity.keys():\n                colinearity[column+' '+indice]=corr_matrix.loc[indice,column]\ncolinearity","d879d0c9":"high_collinerarity = ['GarageArea','TotRmsAbvGrd','1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2','FullBath']\nfeatures = features.drop(high_collinerarity,axis=1)","aeea03e3":"categorical_cols = list(features.select_dtypes('object').columns)\ndummies = pd.get_dummies(features[categorical_cols],drop_first=True)\nfeatures = pd.concat([features.drop(categorical_cols,axis=1),dummies],axis=1)","7d6de6ed":"X_train = features.loc[0:train_size-1]\ny_train = target\nX_test = features.loc[train_size:]","bff9e980":"X_train.shape","0aa31cf9":"from sklearn.linear_model import RidgeCV\nreg = RidgeCV()\nreg.fit(X_train,np.log1p(y_train))\n#reg.fit(X_train,y_train)","4ccab8fd":"from sklearn.model_selection import cross_val_score\nlog_rms = np.sqrt(-np.mean(cross_val_score(reg, X_train,np.log1p(y_train), cv=5,scoring='neg_mean_squared_error')))\n#log_rms = np.sqrt(-np.mean(cross_val_score(reg, X_train,y_train, cv=5,scoring='neg_mean_squared_log_error')))\nprint(f'RMLS : {log_rms}')","7405d2bd":"pred = np.expm1(reg.predict(X_train))\n#pred = reg.predict(X_train)","b6ef41e9":"comparison = pd.DataFrame({'prediction':pred.reshape(pred.shape[0],),'actual':y_train,'error':pred.reshape(pred.shape[0],)-y_train})\nsns.distplot(comparison['error'])","dfaf0047":"comparison.plot(x='prediction',y='error',kind='scatter')","2405b125":"comparison.plot(x='prediction',y='actual',kind='scatter')","e7d74098":"# output result for submission\npred = np.expm1(reg.predict(X_test))\npred = pd.DataFrame(pred.reshape(1459, ))\noutput = pd.concat([df_test['Id'],pred],axis=1).rename(columns={0:'SalePrice'})\noutput.to_csv('submission.csv',index=False)","fe2eab26":"steps = ['Control','Outliers','Log_y','Modification','Non-linearity','Collinearity']\nrmls = [0.15494899171649665,0.14154336306738766,0.11543537404168344,0.11479554199015254,0.11394995795225088,0.11326714995779572]\nscores = pd.DataFrame({'steps':steps,'rmls':rmls})\n\nscores.plot(x='steps',y='rmls',marker='o',rot=90)\nplt.xlabel('Preprocessing Actions',fontsize=14)\nplt.ylabel('RMLS',fontsize=14)\nplt.title('RMLS Changing wrt to Preprocessing Actions',fontsize=16)","a3ee7248":"<div id=\"33\"> <\/div>\n#### 3-3. Column Modification","37c6c8dd":"Since there are some duplicated outliers, first collect them all and then remove duplicated before drop from dataframe.","4791a1aa":"**'GrLivArea'** is the strongest predictor but the linear relationship with 'SalePrice' could be further imporved, by taking square root of 'GrLivArea'.","733bf6c1":"**Categorical missing filling with 'None':**\n\nFrom description text file, we know NA represents the properties does not have these facilities like basement, garage, etc.","49420282":"<div id=\"3\"> <\/div>\n### 3. Data Preprocessing","7aa29e9e":"**'Id**: remove it, because it contains no useful information for price prediction.","31e9bb8c":"### 1. Setup \n<div id=\"1\"> <\/div>","a1dfc98d":"**Categorical missing filling with 'mode':**\n\nThe remained categorical columns with only few missing value, and does not represent the facilities are not available in the houses. As a result, the strategy is to fill them with it mode (most occurring value).","9f0f3e31":"**Numerical missing filling with meidan:**\n\n'LotFrontage' is correlated with 'LotArea', so it may not suitable to fill missing value with 'None'.","38f0fab3":"'OverallQual' and 'GrLivArea' are the strongest predictors for 'SalePrice'","3a1682fe":"<div id=\"36\"> <\/div>\n#### 3-6. Encoding\nNeed to encode categorical variables to dummy variable before model fitting.","8ca068c4":"# Overview\n1. [Setup links](#1)\n2. [Exploratory Data Analyses](#2)\n3. [Data Preprocessing](#3)\n    * 3-1. [Missing Values](#31)\n    * 3-2. [Outliers](#32)\n    * 3-3. [Column Modification](#33)\n    * 3-4. [Non-linearity](#34)\n    * 3-5. [Collinearity](#35)\n    * 3-6. [Encoding](#36)\n4. [Modeling and Evaluation](#4)\n5. [Effectiveness of Preprocessing](#5)","7bc4b3ea":"<div id=\"31\"> <\/div>\n#### 3-1. Missing Values\n* Categorical column with missing values : 23 \n* Numerical column with missing values : 11","8572c8a0":"<div id=\"34\"> <\/div>\n#### 3-4. Non-linearity\n\nOne of the fundamental assumptions of linear model is that every predictor X is linearly correlated to response Y. Since our response Y will take log transformation to improve the problem that residue variance is related to Y.","c6c4fc6e":"**Data Frames**\n* df : original training dataset for EDA use\n* df-test : original test dataset\n* features : merging traing and test datasets feature columns for pre-processing, and will be split when modeling\n* target :merging traing and test datasets target (SalePrice)","da6b3406":"<div id=\"2\"> <\/div>\n### 2. Exploratory Data Analyse\nOnly cover part of EDA to save some space","a15407d5":"**Categorical missing filling with 'N' and making it binary:**\n\n'PoolQC' stands for those properties with 'PoolArea'>0. There are only seven properties with pool regions, so drop 'PoolArea' and represented by 'PoolQC'. Futhermore, 'PoolQC' quality seems to have no effects on final price, so turn it into binary coding.","52c0e4b3":"**Numerical missing filling with '0':**\n\nMissing 'GarageYrBlt' means garages are not available in the properties, and other missing value counts are relatively small for other columns, so fill them all with 0 for simplicity.","81e805da":"<div id=\"5\"> <\/div>\n### 5. Effectiveness of Preprocessing","257f1735":"For Linear Regression, we assume that response Y and p predictors $X_i$ followed this form,\n$$Y = \\beta_0 + \\beta_1 X_1 + ... + \\beta_1 X_P  + \\epsilon$$\n\nand estimate coefficients by using least square method, i.e. minimizing RSS.\n$$RSS = \\sum_{i=1}^n{(y_i-\\hat y_i)^2} = \\sum_{i=1}^n{(y_i-\\beta_0 - \\sum_{j=1}^p{\\beta_i x_{ij}})}$$\n\nFor Ridge Regression, shrinkage penalty is applied to punish large coefficient by a parameter $\\lambda$, resulting in reducing the variance.\n$$RSS + \\lambda \\sum_{i=1}^p{\\beta_i^2}$$\n\nThis seems to work well, as number of observations n is not significantly large than that of predictors p.\n\nIn this database we have only 1453 observations in training dataset, but there are 261 predictors. This may be one of the reasons why it works.","83f3b7c3":"<div id=\"4\"> <\/div>\n### 4. Modeling and Evaluation","672aa5ec":"* cotrol : steps including only handle missing values and encoding (= comment out 3-2~3-5, and log transformation of y)\n* Outliers : 3-2\n* Log_y : take log transformation of response y\n* Modification : 3-3 column modification\n* Non-linearity : 3-4 non-linearity\n* Collinearity : 3-5 collinearity","949f16f9":"**'MSSubClass'**: Identifies the type of dwelling involved in the sale.\n* 20 : 1-STORY 1946 & NEWER ALL STYLES\n* 30 : 1-STORY 1945 & OLDER\n* ...\n\nThis should be a categorical column, instead of numerical one.","e92c661f":"<div id=\"32\"> <\/div>\n#### 3-2. Outliers\n\nOutliers or high leverage points could significantly alter our linear regression, so we have to identify and remove these points from our training set.","7814c399":"<div id=\"35\"> <\/div>\n#### 3-5. Collinearity\n\nCollinearity means any two of the predictors are correlated with each other. This could lead to lower accuracy of the model, because it will be hard to determine the coefficients of each predictor.","6322e9fb":"**'LowQualFinSF'**: transforming into binary variable, since only few observations with value >0, and not sensitive to 'SalePrice'","c5bf45bb":"# House Prices Advanced Regression\n\nI am a beginner in thie field, and actually struggled at this regression problem for a while. When I finally found a way to reach a quite satisfactory result, I would like to share it.\n\nFor this reason, I only use the simplest model (scikit-leanr RidgeCV), and the least modules (Pandas, Numpy, Seaborn, Matplotlib) to address my understanding.\n\nMany of the ideas are from these notebooks and many other great discussions on the forum.\n* [#1 House Prices Solution [top 1%]](https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1) by Nanashi\n* [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) by Alexandru Papiu \n* [An Introduction to Statistical Learning with Applications in R](http:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani","201e196e":"**'Utilities'**: only one row with different value, so remove this column"}}