{"cell_type":{"25e64245":"code","5dbe641a":"code","81cd5583":"code","c300a703":"code","f3ec7999":"code","b1f722bc":"code","63fa3c80":"markdown"},"source":{"25e64245":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nimport optuna","5dbe641a":"df = pd.read_csv(\"..\/input\/tpssep21-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"claim\", \"kfold\")]\ndf_test = df_test[useful_features]","81cd5583":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_df = pd.DataFrame(my_imputer.fit_transform(df))\nmy_imputer = SimpleImputer()\nimputed_df_test = pd.DataFrame(my_imputer.fit_transform(df_test))\n\n# Imputation removed column names; put them back\nimputed_df.columns = df.columns\nimputed_df_test.columns = df_test.columns\n\ndf = imputed_df\ndf_test = imputed_df_test","c300a703":"def run(trial):\n    fold = 0\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    model = XGBClassifier(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=7000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    auc = roc_auc_score(yvalid, preds_valid)\n    return auc","f3ec7999":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run, n_trials=5)","b1f722bc":"study.best_params","63fa3c80":"Tuning hyperparameters as learnt from Abhishek Thakur's videos.\nKFold created here: https:\/\/www.kaggle.com\/rishirajacharya\/tps-sep-2021-kfold\nKFold dataset here: https:\/\/www.kaggle.com\/rishirajacharya\/tpssep21-folds\n\nPlease show some love if this was helpful."}}