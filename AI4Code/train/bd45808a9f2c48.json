{"cell_type":{"112745a4":"code","d714de54":"code","d584ef5e":"code","6c64cfd9":"code","5b1a9bc3":"code","016023e0":"code","c0e8b774":"code","937fe653":"code","75e5cdfd":"code","b6e23f45":"code","b9d3df07":"code","e9ce1e0b":"code","c4d96805":"code","94f27fe2":"code","5eabb17d":"code","12a80787":"code","78ff5d0d":"code","00dc9244":"code","d08963c4":"code","11e83182":"code","eefde51a":"code","6c6db4c9":"code","bf8e63ba":"code","a362f9f3":"markdown","ddb2fde0":"markdown","ef43e566":"markdown","471de37c":"markdown","e5402d28":"markdown","2dfde078":"markdown","bab8d290":"markdown","cddb82a7":"markdown","f1d0c74c":"markdown","6e01566c":"markdown","325deb76":"markdown","c0cf2607":"markdown","cc153cf2":"markdown","30680710":"markdown","cba8062b":"markdown","d43afdbe":"markdown","2fa9eac3":"markdown","3b31598b":"markdown","104db83b":"markdown","7deb344f":"markdown","ceeb5361":"markdown","020c562e":"markdown","05ad521f":"markdown","271bb9c5":"markdown","d7cdcf41":"markdown","16a7d6ce":"markdown"},"source":{"112745a4":"# Import all the libraries that we shall be using\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping","d714de54":"# Import labels (for the whole dataset, both training and testing)\ny = pd.read_csv(\"..\/input\/actual.csv\")\nprint(y.shape)\ny.head()","d584ef5e":"y['cancer'].value_counts()","6c64cfd9":"# Recode label to numeric\ny = y.replace({'ALL':0,'AML':1})\nlabels = ['ALL', 'AML'] # for plotting convenience later on","5b1a9bc3":"# Import training data\ndf_train = pd.read_csv('..\/input\/data_set_ALL_AML_train.csv')\nprint(df_train.shape)\n\n# Import testing data\ndf_test = pd.read_csv('..\/input\/data_set_ALL_AML_independent.csv')\nprint(df_test.shape)","016023e0":"df_train.head()","c0e8b774":"df_test.head()","937fe653":"\ntrain_to_keep = [col for col in df_train.columns if \"call\" not in col]\ntest_to_keep = [col for col in df_test.columns if \"call\" not in col]\n\nX_train_tr = df_train[train_to_keep]\nX_test_tr = df_test[test_to_keep]","75e5cdfd":"train_columns_titles = ['Gene Description', 'Gene Accession Number', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', \n       '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38']\n\nX_train_tr = X_train_tr.reindex(columns=train_columns_titles)","b6e23f45":"test_columns_titles = ['Gene Description', 'Gene Accession Number','39', '40', '41', '42', '43', '44', '45', '46',\n       '47', '48', '49', '50', '51', '52', '53',  '54', '55', '56', '57', '58', '59',\n       '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72']\n\nX_test_tr = X_test_tr.reindex(columns=test_columns_titles)","b9d3df07":"X_train = X_train_tr.T\nX_test = X_test_tr.T\n\nprint(X_train.shape) \nX_train.head()","e9ce1e0b":"# Clean up the column names for training and testing data\nX_train.columns = X_train.iloc[1]\nX_train = X_train.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\n# Clean up the column names for Testing data\nX_test.columns = X_test.iloc[1]\nX_test = X_test.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\nprint(X_train.shape)\nprint(X_test.shape)\nX_train.head()","c4d96805":"# Split into train and test (we first need to reset the index as the indexes of two dataframes need to be the same before you combine them).\n\n# Subset the first 38 patient's cancer types\nX_train = X_train.reset_index(drop=True)\ny_train = y[y.patient <= 38].reset_index(drop=True)\n\n# Subset the rest for testing\nX_test = X_test.reset_index(drop=True)\ny_test = y[y.patient > 38].reset_index(drop=True)","94f27fe2":"X_train.describe()","5eabb17d":"# Convert from integer to float\nX_train_fl = X_train.astype(float, 64)\nX_test_fl = X_test.astype(float, 64)\n\n# Apply the same scaling to both datasets\nscaler = StandardScaler()\nX_train_scl = scaler.fit_transform(X_train_fl)\nX_test_scl = scaler.transform(X_test_fl) # note that we transform rather than fit_transform","12a80787":"pca = PCA()\npca.fit_transform(X_train)","78ff5d0d":"total = sum(pca.explained_variance_)\nk = 0\ncurrent_variance = 0\nwhile current_variance\/total < 0.90:\n    current_variance += pca.explained_variance_[k]\n    k = k + 1\n    \nprint(k, \" features explain around 90% of the variance. From 7129 features to \", k, \", not too bad.\", sep='')\n\npca = PCA(n_components=k)\nX_train.pca = pca.fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nvar_exp = pca.explained_variance_ratio_.cumsum()\nvar_exp = var_exp*100\nplt.bar(range(k), var_exp);","00dc9244":"pca3 = PCA(n_components=3).fit(X_train)\nX_train_reduced = pca3.transform(X_train)\n\nplt.clf()\nfig = plt.figure(1, figsize=(10,6 ))\nax = Axes3D(fig, elev=-150, azim=110,)\nax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:, 2], c = y_train.iloc[:,1], cmap = plt.cm.Paired, linewidths=10)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])","d08963c4":"fig = plt.figure(1, figsize = (10, 6))\nplt.scatter(X_train_reduced[:, 0],  X_train_reduced[:, 1], c = y_train.iloc[:,1], cmap = plt.cm.Paired, linewidths=10)\nplt.annotate('Note the Brown Cluster', xy = (30000,-2000))\nplt.title(\"2D Transformation of the Above Graph \")","11e83182":"kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train_scl)\nkm_pred = kmeans.predict(X_test_scl)\n\nprint('K-means accuracy:', round(accuracy_score(y_test.iloc[:,1], km_pred), 3))\n\ncm_km = confusion_matrix(y_test.iloc[:,1], km_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_km, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('K-means Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","eefde51a":"# Create a Gaussian classifier\nnb_model = GaussianNB()\n\nnb_model.fit(X_train, y_train.iloc[:,1])\n\nnb_pred = nb_model.predict(X_test)\n\nprint('Naive Bayes accuracy:', round(accuracy_score(y_test.iloc[:,1], nb_pred), 3))\n\ncm_nb =  confusion_matrix(y_test.iloc[:,1], nb_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_nb, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Naive Bayes Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","6c6db4c9":"# Parameter grid\nsvm_param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 0.00001, 10], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"decision_function_shape\" : [\"ovo\", \"ovr\"]} \n\n# Create SVM grid search classifier\nsvm_grid = GridSearchCV(SVC(), svm_param_grid, cv=3)\n\n# Train the classifier\nsvm_grid.fit(X_train_pca, y_train.iloc[:,1])\n\nprint(\"Best Parameters:\\n\", svm_grid.best_params_)\n\n# Select best svc\nbest_svc = svm_grid.best_estimator_\n\n# Make predictions using the optimised parameters\nsvm_pred = best_svc.predict(X_test_pca)\n\nprint('SVM accuracy:', round(accuracy_score(y_test.iloc[:,1], svm_pred), 3))\n\ncm_svm =  confusion_matrix(y_test.iloc[:,1], svm_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_svm, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('SVM Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","bf8e63ba":"xgb2_model = xgb.XGBClassifier()\nxgb2_model.fit(X_train_pca, y_train.iloc[:,1])\n\nxgb2_pred = xgb2_model.predict(X_test_pca)\n\nprint('Accuracy: ', round(accuracy_score(y_test.iloc[:,1], xgb2_pred), 3))\n\ncm_xgb2 = confusion_matrix(y_test.iloc[:,1], xgb2_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_xgb2, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('XGB (PCA without Grid Search) Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","a362f9f3":"The naive bayes model is pretty good, just three incorrect classifications.","ddb2fde0":"First we shall try an unsupervised clustering approach using the scaled data.","ef43e566":"With 7129 features, it's also worth considering whether we might be able to reduce the dimensionality of the dataset. Once very common approach to this is principal components analysis (PCA). Let's start by leaving the number of desired components as an open question: ","471de37c":"### XG Boost","e5402d28":"# 17BCB0018 | Niwedita Kumari | ML J-Component\n---\n# Comparative Analysis of Acute and Chronic Leukemia","2dfde078":"Having prepared the dataset, it's now finally time to try out some models. ","bab8d290":"This is still messy as the first two rows are more or less duplicates of one another and we haven't yet created the column names. Let's simply turn the second row into the column names and delete the first row.","cddb82a7":"Dataset : https:\/\/www.kaggle.com\/niweditakumari1997\/ml-project-datasets","f1d0c74c":"Now we can simply transpose the columns and rows so that genes become features and each patient's observations occupies a single row.","6e01566c":"Neither the training and testing column names are not in numeric order, so it's important that we reorder these at some point, so that the labels will line up with the corresponding data.  ","325deb76":"In the combined training and testing sets there are 72 patients, each of whom are labelled either \"ALL\" or \"AML\" depending on the type of leukemia they have. Here's the breakdown:","c0cf2607":"Without the grid search, this is barely any better. However, it seems that the grid search may possibly be resulting in some overfitting.","cc153cf2":"Let's now take a look at some summary statistics:","30680710":"We actually need our labels to be numeric, so let's just do that now.","cba8062b":"Nowadays, gradient boosting models such as XG Boost(XGB) are extremely popular. Here we shall experiment with three alternative versions, PCA with grid search, PCA without grid search and also the orginal data without either PCA or grid search.","d43afdbe":"# Naive Bayes","2fa9eac3":"This SVM model is making just a couple of classification errors.","3b31598b":"---\n# Conclusion\n\n### As we can see, on our medical dataset, SVM performed the best with 94.1% accuracy. so we can use this model and dataset to detect Acute and Chronic Leukemia","104db83b":"![vitl.PNG](attachment:vitl.PNG)","7deb344f":"Now we move on to the features, which are provided for the training and testing datasets separately.","ceeb5361":"Let's start by taking a look at our target, the ALL\/AML label.","020c562e":"The 7129 gene descriptions are provided as the rows and the values for each patient as the columns. This will clearly require some tidying up.\n\nOur first decision is: What should we do about all the \"call\" columns, one for each patient. No explanation for these is provided, so it's difficult to know whether they might be useful or not. We have taken the decision to simply remove them, but this may possibly not be the best approach. ","05ad521f":"### Support Vector Machine\n\nHere we will try another traditional approach, a support vector machine (SVM) classifier. For the SVM, so we using the PCA version of the dataset. Again we use grid search cross-validation to tune the model.","271bb9c5":"We need to remove some components for making it more efficient","d7cdcf41":"This K-means approach is better than the baseline, but we should be able to do better with some kind of supervised learning model.","16a7d6ce":"# K_MEANS"}}