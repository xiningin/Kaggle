{"cell_type":{"b044c5d3":"code","f14c4646":"code","3fc7b3a7":"code","f0c20a90":"code","75ecf401":"code","db60674d":"code","10bfe119":"code","3822b887":"code","20e16a42":"code","e95118b2":"code","f198e6b5":"code","cb762dbd":"code","1e267f77":"code","0d6f757f":"code","3d1f66dc":"code","32df9111":"code","04cabdf2":"code","51e21a68":"code","50b42a0e":"code","98b44183":"code","cbc46938":"code","c4c8f31c":"code","cc166a8c":"code","62f6309d":"code","3bbec58b":"code","4d085ccb":"code","a031850d":"code","809fa068":"code","a3253293":"code","857aba91":"code","556158c9":"code","923098a3":"code","4fff9370":"code","0ad52c35":"code","a565b3f2":"code","984a086b":"markdown","055a96b6":"markdown","dda5ec16":"markdown","3266736d":"markdown","9483aa85":"markdown","5487fe2a":"markdown","d5fd5230":"markdown","babd0730":"markdown","6e2fdcd1":"markdown","169b5853":"markdown","6d84fa05":"markdown","da7e5b79":"markdown","1d098f04":"markdown","25e41f34":"markdown","9773dead":"markdown","80393fe1":"markdown","a974fd0a":"markdown","b0c8a26d":"markdown","4e405785":"markdown","d4105dcc":"markdown","9550d985":"markdown","094ec5ff":"markdown","ee3dfde0":"markdown","9084b792":"markdown","860fa5e1":"markdown","efce62ed":"markdown"},"source":{"b044c5d3":"# Basic packages\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 20)\nimport pandas_profiling as pp\nfrom pandas import Series as Series\nimport datatable as dt\nimport random as rd \nimport datetime\nimport gc\nimport os\nfrom tqdm import tqdm\nimport time\nimport sys\n\n\n# Visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\npy.offline.init_notebook_mode(connected = True)\n\n# For time series analysis\nfrom datetime import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic, pacf\n! pip install pmdarima\nfrom pmdarima import auto_arima\nfrom pandas.plotting import autocorrelation_plot\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom fbprophet import Prophet\n\n# Model Development Related\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport pickle\n\n# Ignore warning \nimport warnings\nwarnings.filterwarnings(\"ignore\")","f14c4646":"# List Of Files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3fc7b3a7":"# Main Data File Import\ntrain_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitems_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems_cat_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n# Convert to date time data type\ntrain_df['date'] = pd.to_datetime(train_df['date'], dayfirst = True)","f0c20a90":"train_df.info()","75ecf401":"train_df.head()","db60674d":"print(\"Total Number Of row : \", train_df.shape[0])\nprint(\"Total Unique Days : \", train_df['date'].nunique())\nprint(\"Starting Days : \", train_df['date'].min())\nprint(\"Ending Days : \", train_df['date'].max())\nprint('---------------------------')\nmissing_values_count = train_df.isnull().sum()[train_df.isna().sum() > 0].sort_values(ascending=False)\ntotal_cells = np.product(train_df.shape)\ntotal_missing = missing_values_count.sum()\nprint('NAN Valued Columns: %d' % train_df.isna().any().sum())\nprint (\"Missing data = \",str(round((total_missing\/total_cells) * 100, 2))+'%')","10bfe119":"print(\"Total Unique Date Block : \", train_df['date_block_num'].nunique())\nprint(\"Total Unique Shop : \", shops_df['shop_id'].nunique())\nprint(\"Total Unique Item Category : \", items_df['item_category_id'].nunique())\nprint(\"Total Unique Item : \", train_df['item_id'].nunique())\nprint('Total Number of duplicates:', len(train_df[train_df.duplicated()]))\n# Only shops that exist in test set.\nprint('Total Number of Common Unique Shops in Training&Test Set:',train_df[train_df['shop_id'].isin(test_df['shop_id'])]['shop_id'].nunique())\n# Only items that exist in test set.\nprint('Total Number of Common Unique Items in Training&Test Set:',train_df[train_df['item_id'].isin(test_df['item_id'])]['item_id'].nunique())\nprint(\"Number Of Items with Price 0 : \", train_df[train_df['item_price']<=0]['item_id'].count())","3822b887":"monthly_sales = train_df.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"date\",\"item_price\",\"item_cnt_day\"]\\\n                   .agg({\"date\":[\"min\",'max'], \"item_price\":\"mean\", \"item_cnt_day\":\"sum\"})","20e16a42":"monthly_sales.head()","e95118b2":"# Understand item category and items per category\nitems_per_category = items_df.groupby(['item_category_id']).count()\nitems_per_category = items_per_category.sort_values(by='item_id', ascending=False)\nprint(\"Understand distribution Of product in each category :\\n\", items_per_category['item_id'].describe())\nitems_per_category = items_per_category.iloc[0:15].reset_index()\n\n# Visualize top item category(Category with grater number of items)\nplt.figure(figsize=(14,6))\nax= sns.barplot(items_per_category.item_category_id, items_per_category.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('Number of items', fontsize=10)\nplt.xlabel('Item Category', fontsize=10)\nplt.show()\nprint(\"Category with more than 800 items : \", items_per_category[items_per_category['item_id']>800]['item_id'].count())","f198e6b5":"(train_df[train_df['item_cnt_day']<0]['item_cnt_day'].count()\/train_df.shape[0])*100","cb762dbd":"# Convert all neg value of \"item_cnt_day\" to positive\ntrain_df['item_cnt_day'] = train_df['item_cnt_day'].apply(abs)","1e267f77":"# Item Sales Per Month Calculation\nitem_sales_per_month = train_df.groupby(['date_block_num'])['item_cnt_day'].sum()\nplt.figure(figsize=(16, 4))\nplt.title('Total item sales of the company')\nplt.xlabel('Month Number')\nplt.ylabel('Item Sales')\nplt.plot(item_sales_per_month, color = 'blue', linewidth = 2, markersize = 12);\nplt.axvspan(10,12,linestyle=':',linewidth=2,label='First Year Peak',color='darkorange',alpha=.2)\nplt.axvspan(22,24,linestyle=':',linewidth=2,label='Second Year Peak',color='green',alpha=.2)\nplt.legend(fontsize=12, ncol=1, loc='upper right');","0d6f757f":"plt.figure(figsize=(16,4))\nplt.plot(item_sales_per_month.rolling(window=15,center=False).mean(),label='Rolling: Mean Item Sales');\nplt.plot(item_sales_per_month.rolling(window=15,center=False).std(),label='Rolling: Standard Deviation');\nplt.legend();","3d1f66dc":"# Year & Month wise Item Count\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df['month'] = train_df['date'].dt.month\n\nplt.figure(figsize=(16,10))\nax1 = plt.subplot(211)\nax2 = plt.subplot(212)\n\ngrouped_item_count = pd.DataFrame(train_df.groupby(['year','month'])['item_cnt_day'].sum().reset_index())\nsns.pointplot(x='month', y='item_cnt_day', hue='year', data=grouped_item_count, ax = ax1)\n\n#Price\ngrouped_item_price = pd.DataFrame(train_df.groupby(['year','month'])['item_price'].mean().reset_index())\nsns.pointplot(x='month', y='item_price', hue='year', data=grouped_item_price, ax = ax2)","32df9111":"res = sm.tsa.seasonal_decompose(item_sales_per_month.values,freq=12,model=\"multiplicative\")\nres.plot()\nplt.show()","04cabdf2":"res = sm.tsa.seasonal_decompose(item_sales_per_month.values,freq=12,model=\"additive\")\nres.plot()\nplt.show()","51e21a68":"# There are some outdated items : didn't sell those items in last 6 months \nout_dated_items = items_df[items_df['item_id'].isin(train_df[train_df['date_block_num'] > 27]['item_id'])== False]['item_id']\nprint(\"Outdated items in last 6 months (training set): \", out_dated_items.nunique())\nprint(\"Outdated items in last 6 months % (training set): \", (out_dated_items.nunique()\/train_df['item_id'].nunique())*100)\n\nprint(\"Outdated items in last 6 months (test set): \", test_df[test_df['item_id'].isin(out_dated_items)==True]['item_id'].nunique())\nprint(\"Outdated items in last 6 months %(test set): \", (test_df[test_df['item_id'].isin(out_dated_items)==True]['item_id'].nunique()\/test_df['item_id'].nunique())*100)","50b42a0e":"# Understand item sales count outlayers\nplt.figure(figsize = (16,4))\nplt.xlim(-100, 3000)\nsns.violinplot(x = train_df.item_cnt_day)\n# Understand item sales price outlayers\nplt.figure(figsize = (16,4))\nplt.xlim(train_df.item_price.min(), train_df.item_price.max()*1.1)\nsns.violinplot(x=train_df.item_price)","98b44183":"print('Sale item outliers:',train_df['item_id'][train_df['item_cnt_day']>500].unique())\nprint('Item price outliers:',train_df['item_id'][train_df['item_price']>50000].unique())","cbc46938":"plt.figure(figsize = (16,4))\nsns.jointplot(x=\"item_cnt_day\", y=\"item_price\", data=train_df, height=8)\nplt.show()","c4c8f31c":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    print('-'*50)\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(item_sales_per_month)","cc166a8c":"# Create a differenced series\n# To make No-Stationary series to Stationary series we have to calculate difference\n# difference = Y(t)-Y(t-1)\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","62f6309d":"plt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original (p-vale : '+str(round(adfuller(item_sales_per_month, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(item_sales_per_month)\n\n# Seasonality is 1 months interval\nnew_ts=difference(item_sales_per_month)\nplt.subplot(312)\nplt.title('After De-trend (p-vale : '+str(round(adfuller(new_ts, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(new_ts)\n\n# Assuming the seasonality is 12 months long\nnew_ts=difference(item_sales_per_month,12) \nplt.subplot(313)\nplt.title('After De-seasonalization (p-vale : '+str(round(adfuller(new_ts, autolag='AIC')[1], 4))+')')\nplt.xlabel('Time')\nplt.ylabel('Sales')    \nplt.plot(new_ts)\nplt.show()","3bbec58b":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","4d085ccb":"def mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","a031850d":"def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n\n    rolling_mean = series.rolling(window=window).mean()\n    \n    plt.figure(figsize=(16,4))\n    plt.title('Moving average\\n window size = {}'.format(window))\n    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n    \n    #Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bound = rolling_mean - (mae + scale * deviation)\n        upper_bound = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bound, 'r--', label='Upper bound \/ Lower bound')\n        plt.plot(lower_bound, 'r--')\n            \n    plt.plot(series[window:], label='Actual values')\n    plt.legend(loc='best')\n    plt.grid(True)\n    \n#Smooth by the previous 1 month \nplot_moving_average(item_sales_per_month, 1)\n\n#Smooth by the previous 1 quarter\nplot_moving_average(item_sales_per_month, 3)\n\n#Smooth by previous 6 month\nplot_moving_average(item_sales_per_month, 6, plot_intervals=True)\n","809fa068":"def exponential_smoothing(series, alpha):\n\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n  \ndef plot_exponential_smoothing(series, alphas):\n \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n    plt.plot(series.values, \"c\", label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Exponential Smoothing\")\n    plt.grid(True);\n\nplot_exponential_smoothing(item_sales_per_month, [0.05, 0.3])\n","a3253293":"def double_exponential_smoothing(series, alpha, beta):\n\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n        trend = beta * (level - last_level) + (1 - beta) * trend\n        result.append(level + trend)\n    return result\n\ndef plot_double_exponential_smoothing(series, alphas, betas):\n     \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        for beta in betas:\n            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n    plt.plot(series.values, label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Double Exponential Smoothing\")\n    plt.grid(True)\n    \nplot_double_exponential_smoothing(item_sales_per_month, alphas=[0.9, 0.02], betas=[0.9, 0.02])","857aba91":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf (y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n     ","556158c9":"tsplot(item_sales_per_month, lags=3)\n\n# Take the first difference to remove to make the process stationary\ndata_diff = item_sales_per_month - item_sales_per_month.shift(1)\n\ntsplot(data_diff[1:], lags=3)","923098a3":"train_df.head()","4fff9370":"# Finding best ARIMA parameter p,d,q\nstepwise_fit = auto_arima(item_sales_per_month, trace=True, suppress_warnings=True)\nstepwise_fit.summary()","0ad52c35":"# Add Month index \nitem_sales_per_month_with_date = item_sales_per_month.copy()\nitem_sales_per_month_with_date.index = pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nitem_sales_per_month_with_date = item_sales_per_month_with_date.reset_index()\nitem_sales_per_month_with_date.columns=['ds','y']\nitem_sales_per_month_with_date.head()","a565b3f2":"# Prophet Model Development\nmodel = Prophet(yearly_seasonality=True) \n#fit the model with dataframe\nmodel.fit(item_sales_per_month_with_date, algorithm='Newton') ","984a086b":"##### Stationarity Checking","055a96b6":"*Quick Observations:*\n> 1. Removing outlayers may increase model performance.\n> 2. Items sales and prices have some outlayers.\n> 3.  remove items with price > 100000 and item sales > 1000","dda5ec16":"#### Basic Exploratory Data Analysis (EDA)","3266736d":"*Quick Observations:*\n> 1. test-statistics : More  negative means more likely to be stationary\n> 2. p-value (small): reject null-hypothesis, reject non-stationary\n> 3. Here p vaule is high (not bellow 5% )\n> 4. Non-Stationary Series : not suitable for time series model \n> 5. Convert Non Stationary : Stationary\n> 6. To make it Stationary : remove trends and seasonality (p value bellow 5%)","9483aa85":"##### Double exponential smoothing","5487fe2a":"*Quick Observations:*\n> 1. Overall selling items following decreasing treand (after 5 months).\n> 2. Clearly showing \"seasonality\" (peak sales around a time of year, mostly in Q4 of a year)\n> 3. Last two months of the year having more sales.\n> 4. 2015, expecting more sales.","d5fd5230":"##### Exponential Smoothing","babd0730":"##### Dickey-Fuller Test : Stationarity ","6e2fdcd1":"> * Data Series bocome stationary ","169b5853":"> Time series is stationary:- easier to model. <br>\n*Stationarity Checking Methods:*\n\n>1. ADF( Augmented Dicky Fuller Test)\n>2. KPSS\n>3. PP (Phillips-Perron test)\n","6d84fa05":"**Overview About Competition :**\n1. This time-series dataset consisting of daily sales data (largest Russian software firms - 1C Company)\n2. We have to predict total sales for every product and store in the next month","da7e5b79":"##### Outlayers","1d098f04":"*Quick Observations:*\n> 1. Almost 60% items didn't sell in last 6 months\n> 2. But in test set it's only 10%","25e41f34":"*Quick Observations:*\n> 1. Some Category has very large number of items (max - 5k+)\n2. 50% category has less than 50 items \n3. 75% category has less than 300 items\n4. On average most of the category has less than 263 items\n5. Only 4 item category has very large number of items (more than 800)","9773dead":"###### ADF( Augmented Dicky Fuller Test)","80393fe1":"*Quick Observations:*\n> First of all, we have to understand the time series data <br>\n> Then we will apply the following models:\n1. AR (Autoregressive Model)\n2. MA (Moving Average Model)\n3. ARIMA (Autoregressive Integrated Moving Average)\n4. SARIMA (Seasonal Autoregressive Integrated Moving Average)","a974fd0a":"##### ARIMA Model","b0c8a26d":"##### Data Cleaning","4e405785":"#### Import Python Packages\n","d4105dcc":"*Quick Observations:*\n> 1. Total 1034 daye is also divided into 34 date blocks (2 year 10 month = 34 month )\n2. Only 6 duplicates value : <br>\na. We can remove it <br>\nb. But I think only 6 duplicate values will not make any difference","9550d985":"##### Item Count and Price ","094ec5ff":"#### Import All Files","ee3dfde0":"*Quick Observations:*\n> 1. item_cnt_day : number of products sold. You are predicting a monthly amount of this measure\n2. We can see that there are almost .25% negative value. Is is possible !!\n3. Maybe by mistake it happend, so convert those neg values to positive ","9084b792":"*Quick Observations:*\n> 1. Total 1034 unique days means almost 3 (2.8) years data is given\n> 2. Data entry starts from January(2013) to October(2015)","860fa5e1":"##### Moving Average Model(MA) For Smoothing","efce62ed":"##### Time Series Analysis"}}