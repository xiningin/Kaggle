{"cell_type":{"0391fc58":"code","65e3a9b6":"code","25c7cb42":"code","86332370":"code","dea81c41":"code","bca5b4df":"code","3a79c1dc":"code","1d0d763a":"code","d8d427af":"code","0910a95e":"code","a0f2fd5d":"code","3d80b142":"code","6599dab8":"code","d69dd6ea":"code","7633f42e":"code","4514bde9":"code","abf9798f":"code","044fec11":"code","28f85499":"code","75843d72":"code","90d2d8bc":"code","cb064fe1":"code","ee5a145f":"code","61d26252":"code","9c5e914f":"code","608c4681":"code","9adbfa0d":"code","3678e93e":"code","4563ade2":"code","2a3c4cbc":"code","17374de9":"code","237d6a88":"markdown","8e143f9c":"markdown","7df5ab7c":"markdown","da3c05f3":"markdown","ffcceea5":"markdown","e1acd613":"markdown","a2b93e26":"markdown","11d03c41":"markdown","13e483a2":"markdown","2d80311d":"markdown","4c793706":"markdown","1d244109":"markdown","2ee4aff1":"markdown","c65fb4bb":"markdown","4fb2b8ae":"markdown","29ecca71":"markdown","5fe3bb38":"markdown","591755de":"markdown","2571a891":"markdown","90e630aa":"markdown","fbe358c4":"markdown","f6329cff":"markdown","adeab4c4":"markdown","7c7455ad":"markdown","c5024872":"markdown","85901e42":"markdown","4fd8265e":"markdown","e8877bf9":"markdown","a76b6d2d":"markdown","2f8453f9":"markdown","def9a94c":"markdown","113327f2":"markdown","01469043":"markdown","f2fd4b0f":"markdown","5840c701":"markdown","00b9417e":"markdown","2be9b328":"markdown"},"source":{"0391fc58":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","65e3a9b6":"sea.set_style(\"darkgrid\")","25c7cb42":"data = pd.read_excel(\"\/kaggle\/input\/bank-loan-modelling\/\"\n                     \"Bank_Personal_Loan_Modelling.xlsx\",\n                     sheet_name = \"Data\")\n\ndata.head(10).style.set_precision(2). \\\n                    set_properties(**{\"min-width\": \"60px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])","86332370":"data.drop(\"ID\", axis=1, inplace=True)\n\n# disable SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"Personal Loan\"]\ndata_Y = data[[\"Personal Loan\"]]\n\nprint(\"data_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","dea81c41":"data_X[\"ZIP Code\"].nunique()","bca5b4df":"data_X.drop(\"ZIP Code\", axis=1, inplace=True)","3a79c1dc":"data_Y[\"Personal Loan\"].value_counts()","1d0d763a":"train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    stratify=data_Y,\n                                                    random_state=0)\n\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);","d8d427af":"def plots(feature):\n    fig = plt.figure(constrained_layout = True, figsize=(10,3))\n    gs = gridspec.GridSpec(nrows=1, ncols=4, figure=fig)\n\n    ax1 = fig.add_subplot(gs[0,:3])    \n    sea.distplot(train_X.loc[train_Y[\"Personal Loan\"]==0,feature],\n                 kde = False, color = \"#004a4d\", norm_hist=False,\n                 hist_kws = dict(alpha=0.8), bins=40,\n                 label=\"Not Loan Customer\", ax=ax1);\n    sea.distplot(train_X.loc[train_Y[\"Personal Loan\"]==1,feature],\n                 kde = False, color = \"#7d0101\", norm_hist=False,\n                 hist_kws = dict(alpha=0.6), bins=40,\n                 label=\"Loan Customer\", ax=ax1);\n    ax2 = fig.add_subplot(gs[0,3])    \n    sea.boxplot(train_X[feature], orient=\"v\", color = \"#989100\",\n                width = 0.2, ax=ax2);\n    \n    ax1.legend(loc=\"upper right\");","0910a95e":"plots(\"Age\")","a0f2fd5d":"plots(\"Experience\")","3d80b142":"plots(\"Income\")","6599dab8":"Q1 = train_X[\"Income\"].quantile(0.25)\nQ3 = train_X[\"Income\"].quantile(0.75)\nq95th = train_X[\"Income\"].quantile(0.95)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"Income\"] = np.where(train_X[\"Income\"] > UW, q95th, train_X[\"Income\"])","d69dd6ea":"plots(\"Family\")","7633f42e":"plots(\"CCAvg\")","4514bde9":"Q1 = train_X[\"CCAvg\"].quantile(0.25)\nQ3 = train_X[\"CCAvg\"].quantile(0.75)\nq95th = train_X[\"CCAvg\"].quantile(0.95)\nIQR = Q3 - Q1\nUW = Q3 + 1.5*IQR\n\ntrain_X[\"CCAvg\"] = np.where(train_X[\"CCAvg\"] > UW, q95th, train_X[\"CCAvg\"])","abf9798f":"plots(\"Education\")","044fec11":"plots(\"Mortgage\")","28f85499":"train_X[\"Mortgage_Int\"] = pd.cut(train_X[\"Mortgage\"],\n                                bins=[0,100,200,300,400,500,600,700],\n                                labels=[0,1,2,3,4,5,6],\n                                include_lowest=True)\n\ntrain_X.drop(\"Mortgage\", axis=1, inplace=True)\n\ntest_X[\"Mortgage_Int\"] = pd.cut(test_X[\"Mortgage\"],\n                                bins=[0,100,200,300,400,500,600,700],\n                                labels=[0,1,2,3,4,5,6],\n                                include_lowest=True)\n\ntest_X.drop(\"Mortgage\", axis=1, inplace=True)","75843d72":"train_X[\"Securities Account\"].value_counts()","90d2d8bc":"train_X[\"CD Account\"].value_counts()","cb064fe1":"train_X[\"Online\"].value_counts()","ee5a145f":"train_X[\"CreditCard\"].value_counts()","61d26252":"feature_names = train_X.columns\n\nscaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","9c5e914f":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=True, fmt=\".3f\",\n            vmin=-1, vmax=1, linewidth = 1,\n            center=0, mask=mask,cmap=\"RdBu_r\");","608c4681":"train_X.drop(\"Experience\", axis=1, inplace=True)\ntest_X.drop(\"Experience\", axis=1, inplace=True)","9adbfa0d":"# convert dataframes to numpy arrays\nnp_train_X = train_X.values\nnp_train_Y = train_Y.values.ravel()\nnp_test_X = test_X.values\nnp_test_Y = test_Y.values.ravel()\n\n# create stratified 5 fold\nskf_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=0)\n\nvec_X = []\nvec_Y = []\n# iterate over each split\nfor train_ind, test_ind in skf_cv.split(np_train_X, np_train_Y):\n\n    # get train and test set for this split\n    train_folds_X, test_fold_X = np_train_X[train_ind], np_train_X[test_ind]\n    train_folds_Y, test_fold_Y = np_train_Y[train_ind], np_train_Y[test_ind]\n    \n    # temporary base learners\n    cls_1t = LogisticRegression()\n    cls_2t = DecisionTreeClassifier(random_state=0)\n    \n    # fit cls_1t on train_folds_X and predict on test_fold_X \n    cls_1t.fit(train_folds_X, train_folds_Y)\n    pred_proba_1 = cls_1t.predict_proba(test_fold_X)[:,1]\n    \n    # fit cls_2t on train_folds_X and predict on test_fold_X \n    cls_2t.fit(train_folds_X, train_folds_Y)\n    pred_proba_2 = cls_2t.predict_proba(test_fold_X)[:,1]\n\n    vec_X.extend(np.concatenate((pred_proba_1.reshape(-1,1),\n                                 pred_proba_2.reshape(-1,1)), axis=1))\n    vec_Y.extend(test_fold_Y)\n    \nmeta_X = np.array(vec_X)\nmeta_Y = np.array(vec_Y)","3678e93e":"# base Learners\ncls_1 = LogisticRegression()\ncls_2 = DecisionTreeClassifier(random_state=0)\n\n# fit cls_1 on train_X\ncls_1.fit(np_train_X, np_train_Y)\n# fit cls_2 on train_X\ncls_2.fit(np_train_X, np_train_Y);","4563ade2":"pred_1_test_X = cls_1.predict(np_test_X)\nprint(\"Classification report for base learner 1\\n\")\nprint(classification_report(np_test_Y, pred_1_test_X,\n                            digits = 4,\n                            target_names=[\"Not Loan Customer\",\n                                          \"Loan Customer\"]))\n\npred_2_test_X = cls_2.predict(np_test_X)\nprint(\"\\n\\nClassification report for base learner 2\\n\")\nprint(classification_report(np_test_Y, pred_2_test_X,\n                            digits = 4,\n                            target_names=[\"Not Loan Customer\",\n                                          \"Loan Customer\"]))","2a3c4cbc":"meta = xgb.XGBClassifier(base_score=0.5, booster='gbtree',\n                         colsample_bylevel=1, colsample_bynode=1,\n                         colsample_bytree=1, gamma=10, gpu_id=-1,\n                         learning_rate=0.1, max_delta_step=0,\n                         max_depth=4, n_estimators=6, n_jobs=0,\n                         random_state=0, subsample=1)\n\nmeta.fit(meta_X, meta_Y);","17374de9":"prob_1_test_X = cls_1.predict_proba(np_test_X)[:,1]\nprob_2_test_X = cls_2.predict_proba(np_test_X)[:,1]\n\nlevel_1_in_X = np.concatenate((prob_1_test_X.reshape(-1,1),\n                               prob_2_test_X.reshape(-1,1)), axis=1)\n\nstacking_pred = meta.predict(level_1_in_X)\nprint(\"Classification report for Stacking Ensemble\\n\")\nprint(classification_report(np_test_Y, stacking_pred,\n                            digits = 4,\n                            target_names=[\"Not Loan Customer\",\n                                          \"Loan Customer\"]))","237d6a88":"Family takes 4 discrete values. It's an ordinal categorical variable and there aren't any outliers.","8e143f9c":"A lot of customers doesn't have mortgage and the right side of the distribution is almost flat. We can construct an ordinal categorical feature **Mortgage_Int** using Mortgage feature. Then we drop **Mortgage** feature. We apply the same procedure to also test_X.","7df5ab7c":"### Feature 0 - Age\n\nFirst one is **Age** feature.","da3c05f3":"## Train Meta Learner\n\nMeta learner XGBoost Classifier, is itself a gradient boosting ensemble that in our case has 6 sequential estimators. So, we use an ensemble as the meta learner of our stacking ensemble.","ffcceea5":"## Test Stacking Ensemble\n\nWe process np_test_X with our base learners. Then, we feed their outputs to Meta Learner and evaluate its output. Then, classification report of Stacking Ensemble is printed.","e1acd613":"There are 12 features. The aim is to construct a model that can identify potential customers who have higher probability of **purchasing loan**. Output column is **Personal Loan**. Features are detailed below:\n\n**Age** &nbsp;&nbsp; Customer's age\n<br>\n**Experience** &nbsp;&nbsp; Number of years of professional experience\n<br>\n**Income** &nbsp;&nbsp; Annual income of the customer\n<br>\n**ZIPCode** &nbsp;&nbsp; Home Address ZIP code\n<br>\n**Family** &nbsp;&nbsp; Family size of the customer\n<br>\n**CCAvg** &nbsp;&nbsp; Average spending on credit cards per month\n<br>\n**Education** &nbsp;&nbsp; Education Level. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional\n<br>\n**Mortgage** &nbsp;&nbsp; Value of house mortgage if any\n<br>\n**Securities Account** &nbsp;&nbsp; Does the customer have a securities account with the bank?\n<br>\n**CD Account** &nbsp;&nbsp; Does the customer have a certificate of deposit (CD) account with the bank?\n<br>\n**Online** &nbsp;&nbsp; Does the customer use internet banking facilities?\n<br>\n**CreditCard** &nbsp;&nbsp; Does the customer uses a credit card issued by UniversalBank?\n<br>\n**Personal Loan** &nbsp;&nbsp; Did this customer accept the personal loan offered in the last campaign?\n<br>\n\nID column is dropped. Features are assigned to **data_X** and corresponding labels to **data_Y**. **Pandas info** shows column (feature) data types and number of non-null values.  ","a2b93e26":"### Feature 3 - Family","11d03c41":"There are 2 classes and class ratio is approximately 1:9 which means our dataset is imbalanced.","13e483a2":"There aren't any outliers for **Age** feature.","2d80311d":"Performance of base learners, LogisticRegression and DecisionTreeClassifier are already very high. If we look at the F1 scores, we see that even in this case, Meta Learner has positive contribution.\n\nJust like other ensemble methods, performance gain provided by stacking ensemble changes according to dataset under consideration. In some cases, base learners are enough, in some cases, performance gain can be moderate or higher.","4c793706":"## Train Base Learners\n\nAfter generating training data for Meta Learner, base learners (LogisticRegression and DecisionTreeClassifier) are trained on whole training data, np_train_X and np_train_Y.","1d244109":"### Feature 1 - Experience","2ee4aff1":"## Visualization and Outlier Check\n\n**Outliers** degrade the learning performance. Outlier analysis is performed for each feature one-by-one. We use **quartile analysis** for outlier detection. For each feature, there are two plots below. On the left, there are two normalized histograms, one for each value of target variable. On the right, there is a box plot of the same feature. Both of them are analyzed together to get an idea about the outliers. From this point on, lower whisker of the boxplot is denoted as LW and upper whisker is denoted as UW.","c65fb4bb":"## Standardization\n\nInput features are standardized to increase the learning performance. The aim is to transform each feature to have mean of 0 and standard deviation of 1.","4fb2b8ae":"### Feature 4 - CCAvg","29ecca71":"### Feature 7 - Securities Account\n\nA customer may have a security account or not. Securities Account is a binary categorical feature and there is no need to make outlier analysis.","5fe3bb38":"We can deem ZIP Code as uninformative in this case and drop it.","591755de":"Now, we have data to train our meta learner. Temporary base learners are discarded.","2571a891":"Distribution of **Income** feature is highly skewed. The samples higher than UW will be replaced with 95th quantile.","90e630aa":"**Age** and **Experience** features have very high correlation, 0.99. It is also intuitively understandable that experience increases as age increases. Correlated features degrade the learning performance and causes instability on the models. We drop **Experience** feature from both **train_X** and **test_X**.","fbe358c4":"Distribution of CCAvg feature is highly skewed. The samples higher than UW will be replaced with 95th quantile.","f6329cff":"## Prepare Training Data for Meta Learner\n\nThere are 2 base learners, **LogisticRegression** and **DecisionTreeClassifier** of scikit-learn. \n\nWe use 5 fold repeated stratified cross validation and n_repeats is 2. There are 10 splits. In each split, 4 folds are used as training data and 1 fold is used as test data.\n\nTemporary base learners are trained on training set, make predictions on test set and then are discarded. Predictions on 10 test sets are concatenated vertically and becomes the training set of meta learner. Base learners and Meta Learner are not trained on the same data. This is done to prevent overfitting. Below cell is used to generate training data for Meta Learner.","adeab4c4":"Let's see number of classes in the output.","7c7455ad":"### Feature 8 - CD Account\n\nA customer may have a CD account or not. CD Account is a binary categorical feature and there is no need to make outlier analysis.","c5024872":"## Split Data\n\nDataset is divided into train and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets.","85901e42":"There aren't any outliers for **Experience** feature.","4fd8265e":"## Test Base Learners\n\nBase learners are evaluated on np_test_X. Their classification reports are printed.","e8877bf9":"### Feature 6 - Mortgage","a76b6d2d":"### Feature 5 - Education","2f8453f9":"### Feature 2 - Income","def9a94c":"### Feature 9 - Online\n\nA customer may use internet banking or not. Online is a binary categorical feature and there is no need to make outlier analysis.","113327f2":"Dataset has 5000 rows (training samples). All features seem numeric, we will return to this issue as we inspect each feature. And there aren't any null values.\n\nThere is one special case about **ZIP Code** feature. Although it seems to be numeric, it is in fact categorical. Different ZIP codes indicates different addresses. If we look at the number of unique values it takes, we see high cardinality.","01469043":"## Load Data\n\n**Bank Loan Modeling** dataset is used. Dataset is an excel file and it has 2 excel sheets, Data and Description. **Pandas read_excel** function is used to read **Data sheet**.","f2fd4b0f":"### Feature 10 - CreditCard\n\nA customer may have credit card or not. CreditCard is a binary categorical feature and there is no need to make outlier analysis.","5840c701":"## Correlation Matrix\n\nNow, we inspect the linear correlations between features and also between features and output. Pandas corr function is used to compute correlation matrix and Seaborn heatmap is used for plotting. The semicolon at the end of seaborn command is used to suppress the output other than the plot. Correlation is in interval [-1,1], so our colorbar is.","00b9417e":"There aren't any outliers for **Education** feature. In fact, Education is a categorical feature already converted to integer. In this case, integer encoding is suitable beacuse there is an ordinal relationship among education levels.","2be9b328":"In ensemble learning, two or more machine learning methods also named as base learners are used together to get the final decision. Bagging and boosting are ensemble methods. Bagging is mostly used to reduce variance. In boosting, simple base learners are used sequentially. Each base model tries to reduce the error of previous models. Bagging and boosting are generally homogeneous ensemble methods. Same type of base learners are used in ensemble. For the details of a bagging application, you may refer to my notebook [Credit Card Fraud Detection with Bagging Ensemble](https:\/\/www.kaggle.com\/ozdemirh\/credit-card-fraud-detection-with-bagging-ensemble). Also, you may refer to [Heart Disease Prediction, Scikit-Learn Ensembles](https:\/\/www.kaggle.com\/ozdemirh\/heart-disease-prediction-scikit-learn-ensembles) for the details of frequently used scikit-learn ensembles. \n\nTo aggragate the decisions of ensemble members, different weights may be assigned to each member. A better way is using another model to learn how to combine decisions of base learners. In **Stacking**, most of the time, there are two layers. Stacking with more than two layers is also possible. Base learners or level-0 models are in the first layer. And in the second layer, a meta learner or a level-1 learner gets the decisions of base models and produce the final prediction.\n\nOutline of the work is as follows:\n\n* Load Data\n* Split Data\n* Visualization and Outlier Check\n* Standardization\n* Prepare Training Data for Meta Learner\n* Train Base Learners\n* Test Base Learners\n* Train Meta Learner\n* Test Stacking Ensemble"}}