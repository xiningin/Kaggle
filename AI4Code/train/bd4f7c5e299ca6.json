{"cell_type":{"a75ad390":"code","1a655e5e":"code","b260159f":"code","eabfb4e2":"code","2a9d91bb":"code","90d019d9":"code","feaa8254":"code","de654639":"code","0dc0d93e":"code","f54a6a7b":"code","9a696fa9":"code","450207d8":"code","5c5ce275":"code","e8f7e1ab":"code","5b622d66":"code","984390d8":"code","5ffcf525":"code","fe22de59":"code","4a864eb7":"code","051e23e6":"code","4eb3d480":"code","bcc03150":"code","ba64d6e3":"code","74ae5ec6":"code","bff2fbc5":"code","0dbeb781":"code","ca833079":"code","f32f4ba7":"code","bcd4e763":"code","ccf9c71f":"code","3cda45c3":"code","7824f035":"code","e2e285e2":"code","ba74f156":"code","10279f09":"code","b1de101d":"code","864ea664":"code","37d2dcd5":"code","33ccec22":"code","4267e107":"code","4875cc57":"code","846c4605":"code","d067b980":"code","4fd2cb94":"code","7d9a713f":"code","49a6e6bc":"code","1415549a":"code","9af3f481":"markdown","8d68b95e":"markdown","aa8cc8f1":"markdown","6bb65d89":"markdown","f1d7f22d":"markdown","8d54f6bb":"markdown","1ce5d346":"markdown","cd87f8bd":"markdown","415203a7":"markdown","c942ba73":"markdown","a2d63f2e":"markdown","95d9d9c7":"markdown","a5562bf4":"markdown","ceb89464":"markdown","8047f856":"markdown","e81e8d0f":"markdown","86359ab6":"markdown","76a432e9":"markdown","264b8b71":"markdown","20b062b2":"markdown","09ea6007":"markdown","bf46cd79":"markdown","14165e6e":"markdown","3e7bfdac":"markdown","fb1d20ab":"markdown"},"source":{"a75ad390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a655e5e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings('ignore')","b260159f":"df_audi = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')","eabfb4e2":"df_audi.head()","2a9d91bb":"print(df_audi['fuelType'].unique())\nprint(df_audi['transmission'].unique())\nprint(df_audi['model'].unique())","90d019d9":"df_audi.isnull().sum()","feaa8254":"listy = ['model', 'transmission', 'fuelType', 'engineSize', 'year']\nfor i in listy:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(df_audi[i])\n    plt.show()","de654639":"for i in listy:\n    plt.figure(figsize=(12, 5))\n    sns.barplot(x=df_audi[i], y=df_audi['price'])\n    plt.show()","0dc0d93e":"age_of_car = 2020 - df_audi['year']","f54a6a7b":"df_audi.drop('year', axis=1, inplace=True)","9a696fa9":"df_audi['age_of_car'] = age_of_car","450207d8":"sns.countplot(df_audi['age_of_car'])","5c5ce275":"sns.residplot(df_audi['mileage'], df_audi['price'] )","e8f7e1ab":"df_audi.corr()","5b622d66":"df_audi.head()","984390d8":"df = pd.get_dummies(df_audi)","5ffcf525":"df.head()","fe22de59":"df.shape","4a864eb7":"x = df.drop('price', axis=1)\ny = df['price']","051e23e6":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","4eb3d480":"def plot(a, title):\n    plt.figure(figsize=(8, 5))\n    sns.distplot(a, hist=False, color='r', label='predicted')\n    sns.distplot(y_test, hist=False, color='b', label='actual')\n    plt.title(title)\n    plt.show()","bcc03150":"lr = LinearRegression()\nInput1 = [('scale', StandardScaler()), ('model', lr)]\npipe1 = Pipeline(Input1)\npipe1.fit(x_train, y_train)\nyhat1 = pipe1.predict(x_test)","ba64d6e3":"lr_score = r2_score(yhat1, y_test)\nprint('The r^2 score for Linear Regression is {}'.format(lr_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat1-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat1-y_test)**2))    ","74ae5ec6":"plot(yhat1, 'Linear Regression')","bff2fbc5":"model = XGBRegressor(random_state=2)\nmodel.fit(x_train, y_train)\nyhat2 = model.predict(x_test)","0dbeb781":"xgb_score = r2_score(yhat2, y_test)\nprint('The r^2 score for XGBRegressor is {}'.format(xgb_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat2-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat2-y_test)**2))  ","ca833079":"plot(yhat2, 'XGBRegressor')","f32f4ba7":"pipe2 = Pipeline([('preprocessor', SimpleImputer()), ('model', RandomForestRegressor(n_estimators= 1000, random_state=0))])\nscore = -1*cross_val_score(pipe2, x_train, y_train , cv=5, scoring='neg_mean_absolute_error')\nprint('cross_val_score of Random Forest Regressor is {}'.format(score.mean()))\npipe2.fit(x_train, y_train)\nyhat3 = pipe2.predict(x_test)","bcd4e763":"rfr_score = r2_score(yhat3, y_test)\nprint('The r^2 score for Random Forest Regressor is {}'.format(rfr_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat3-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat3-y_test)**2))  ","ccf9c71f":"plot(yhat3, 'Random Forest Regressor')","3cda45c3":"audi_tree = DecisionTreeRegressor(random_state=1)\naudi_tree.fit(x_train, y_train)\nyhat4 = audi_tree.predict(x_test)","7824f035":"dtr_score = r2_score(yhat4, y_test)\nprint('The r^2 score for Decision Tree classifier is {}'.format(dtr_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat4-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat4-y_test)**2))  ","e2e285e2":"plot(yhat4, 'Decision Tree Regressor')","ba74f156":"parameter = [{'alpha': [0.001, 0.1, 0.4, 10, 100, 1000, 10000, 100000]}]\nrr = Ridge()\ngrid1 = GridSearchCV(rr, parameter, cv=4)\ngrid1.fit(x_train, y_train)\ngrid1.best_estimator_\nscores = grid1.cv_results_\nscores['mean_test_score']","10279f09":"ridge_model = Ridge(alpha=0.4)\nridge_model.fit(x_train, y_train)\nyhat5 = ridge_model.predict(x_test)","b1de101d":"ridge_score = r2_score(yhat5, y_test)\nprint('The r^2 score for Ridge Regression is {}'.format(ridge_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat5-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat5-y_test)**2))  ","864ea664":"plot(yhat5, 'Ridge Regression')","37d2dcd5":"lasso_model = Lasso(alpha=0.4)\nlasso_model.fit(x_train, y_train)\nyhat6 = lasso_model.predict(x_test)","33ccec22":"lasso_score = r2_score(yhat6, y_test)\nprint('The r^2 score for Lasso Regression is {}'.format(lasso_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat6-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat6-y_test)**2))  ","4267e107":"plot(yhat6, 'Lasso Regression')","4875cc57":"bay_ridge_model = BayesianRidge()\nbay_ridge_model.fit(x_train, y_train)\nyhat7 = bay_ridge_model.predict(x_test)","846c4605":"bay_ridge_score = r2_score(yhat7, y_test)\nprint('The r^2 score for bayesian Ridge Regression is {}'.format(bay_ridge_score))\nprint('The mean absolute error is {}'.format(np.mean(np.absolute(yhat7-y_test))))\nprint('Residual sum of squares : {}'.format(np.mean(yhat7-y_test)**2))  ","d067b980":"plot(yhat7, 'Bayesian Ridge Regression')","4fd2cb94":"poly = PolynomialFeatures(degree=2)\nx_train_pr = poly.fit_transform(x_train)\nx_test_pr = poly.fit_transform(x_test)","7d9a713f":"listy2 = [LinearRegression(),XGBRegressor(n_estimators=1000, learning_rate=0.06, random_state=3, n_jobs=2),\n         BayesianRidge(),Lasso(alpha=0.4),Ridge(alpha=0.4) , DecisionTreeRegressor(random_state=1),\n          RandomForestRegressor(n_estimators= 1000, random_state=0)]\nlisty_title = ['Linear Regression', 'XGBRegressor', 'bayesian Ridge', 'Lasso', 'Ridge', 'DecisionTreeRegressor',\n              'Random Forest Regressor']","49a6e6bc":"poly_score=[]\nfor regr, title in zip(listy2, listy_title):\n    model = regr\n    model.fit(x_train_pr, y_train)\n    yhat = model.predict(x_test_pr)\n    rs = r2_score(yhat, y_test)\n    poly_score.append(rs)\n    print('The r^2 score of {} is {}'.format(title, rs))","1415549a":"score = pd.DataFrame({\n    'models': listy_title,\n    'normal r^2 score': [lr_score, xgb_score, bay_ridge_score,\n                         lasso_score, ridge_score,  dtr_score, rfr_score],\n    'poly r^2 score': poly_score\n})\nscore","9af3f481":"### Model Developement & evaluation","8d68b95e":"## Lasso Regression","aa8cc8f1":"### applying polynomial degree =2","6bb65d89":"### its good to say that there are no missing values in any of the columns in the datset","f1d7f22d":"### training and testing data","8d54f6bb":"### To see how many unique types are there in each category columns,","1ce5d346":"## Ridge Regression","cd87f8bd":"### lets see how well the model predicts with the fitted model","415203a7":"## EDA and Visualizations","c942ba73":"### hello kagglers, I have used many Regression models in this notebook inorder to find a best regression model out of it.","a2d63f2e":"# Audi Price Predictions R^2 ~ 0.96","95d9d9c7":"## XGBRegressor","a5562bf4":"## Decision Tree Regressor","ceb89464":"### using pd.dummies to transform categorical variables to numeric data, we can even use one hot encoding","8047f856":"## from the r^2 scores XGBRegressor ~0.96 is the best model for prediction, even other models too has r^2 score greater than 0.90","e81e8d0f":"### lets see if the columns in the dataset has some missing values","86359ab6":"## Bayesian Regression","76a432e9":"### using grid search to find out which alpha value has high score for ridge regression","264b8b71":"## Random Forest Regressor","20b062b2":"## The above resid plot of price with respect to mileage is not  distributed all over . so it is a good fit for linear models","09ea6007":"![image.png](attachment:image.png)","bf46cd79":"### reading the dataset using pandas","14165e6e":"## Linear Regression","3e7bfdac":"### using polynomial features to check if the model predicts better","fb1d20ab":"## upvoting is a way of appreciation\n## if you like it, do upvote \ud83d\udc4d"}}