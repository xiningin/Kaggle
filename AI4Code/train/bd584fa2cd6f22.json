{"cell_type":{"d3f93d4e":"code","e28a1ca0":"code","e6ab2997":"code","10a2a3a6":"code","32a9c7b3":"code","bcc0852a":"code","ab8f222d":"code","69339c81":"code","741eddac":"code","4363353b":"code","3e3c0554":"code","0dc91731":"code","12618ff7":"code","47610ea1":"code","e09718b6":"code","aa786ef7":"code","dc921835":"code","3b5e25b9":"code","5f9e93b2":"code","c470ad3f":"code","33a9abd8":"code","bb3f9616":"code","1f578b2f":"code","62890f68":"code","5ad3882c":"code","f16b7963":"code","d439589a":"code","11f89444":"code","9819827f":"code","8c6d8b03":"code","f8f2fe33":"markdown","b758c5ec":"markdown","be335a7f":"markdown","f7df4d8c":"markdown","5103f886":"markdown","fca558f3":"markdown","e83f23e3":"markdown","fe6126c2":"markdown","334ee14a":"markdown","fb35d363":"markdown","61f67ddf":"markdown","504ebe82":"markdown","32bfb551":"markdown","f443f7a4":"markdown","7d43b33c":"markdown","39e66d8b":"markdown","afc63290":"markdown","945bfdfd":"markdown","89f4b02d":"markdown","98013435":"markdown","23138bd6":"markdown","e3c38679":"markdown","d70c8d84":"markdown","941df58d":"markdown","bea3476d":"markdown","69ea43e0":"markdown","1a6bb7cf":"markdown","f01b6fab":"markdown","755dddaf":"markdown","9a1828c4":"markdown"},"source":{"d3f93d4e":"# import numpy, matplotlib, etc.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\n# define plt settings\nplt.rcParams[\"font.size\"] = 20\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"xtick.labelsize\"] = 20\nplt.rcParams[\"ytick.labelsize\"] = 20\nplt.rcParams[\"legend.fontsize\"] = 20\nplt.rcParams[\"figure.figsize\"] = (20,10)\n\n# sklearn imports\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import neural_network\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm","e28a1ca0":"titanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data","e6ab2997":"sns.barplot(x =titanic_data.Pclass.value_counts().index,y= titanic_data.Pclass.value_counts()).set_title('Passenger Class')\nplt.show()","10a2a3a6":"pd.pivot_table(titanic_data, index='Survived', columns='Pclass', values = 'Ticket', aggfunc='count', fill_value=0)","32a9c7b3":"titanic_data['Name'] = titanic_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\npd.pivot_table(titanic_data, index='Survived', columns='Name', values = 'Ticket', aggfunc='count', fill_value=0)","bcc0852a":"pd.pivot_table(titanic_data, index='Survived', columns='Sex', values = 'Ticket', aggfunc='count', fill_value=0)","ab8f222d":"plt.hist(titanic_data['Age'])\nplt.show()","69339c81":"#Clearing Name colum to only include titles.\ntitles = titanic_data.Name.unique()\ntitles_median_age = dict()\nfor title in titles:\n  titles_median_age[title] = titanic_data.loc[titanic_data['Name'] == title].Age.median()\n\n#Filling missing information about age with the median age.\nfor title in titles:\n  titanic_data.loc[(titanic_data.Name == title) & (titanic_data.Age.isna()), 'Age'] = titles_median_age[title]\nplt.hist(titanic_data['Age'])\nplt.show()","741eddac":"pd.pivot_table(titanic_data, index = 'Survived', values = ['SibSp','Parch'])","4363353b":"plt.hist(titanic_data['Fare'])\nplt.show()","3e3c0554":"titanic_data = titanic_data.drop(titanic_data[titanic_data.Fare > 150].index)\nplt.hist(titanic_data['Fare'])\nplt.show()","0dc91731":"pd.pivot_table(titanic_data, index='Survived', columns='Embarked', values = 'Ticket', aggfunc='count', fill_value=0)","12618ff7":"#Reading data again.\ntitanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n#Removing irrelevant data.\ndef remove_irrelevant_data(data):\n    data.drop('PassengerId',axis='columns', inplace=True)\n    data.drop('Cabin',axis='columns', inplace=True)\n    data.drop('Ticket',axis='columns', inplace=True)\n    return data\n\n#Last boarding passangers had time to settle in.\ndef embarked_to_numeric(data):\n    data.loc[data.Embarked == 'S', 'Embarked'] = 1\n    data.loc[data.Embarked == 'C', 'Embarked'] = 2\n    data.loc[data.Embarked == 'Q', 'Embarked'] = 3\n    #Will have little impact due to large sample of S departure.\n    data['Embarked'] = data['Embarked'].fillna(1)\n    return data\n    \n#Giving numeric values to male and female.\ndef sex_to_numeric(data):\n    data.loc[data.Sex == 'male', 'Sex'] = 1\n    data.loc[data.Sex == 'female', 'Sex'] = 2\n    return data\n\n#Clearing Name colum to only include titles.\ndef titles_to_age(data):\n    data['Name'] = data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    titles = data.Name.unique()\n    titles_median_age = dict()\n    for title in titles:\n        titles_median_age[title] = data.loc[data['Name'] == title].Age.median()\n\n    #Filling missing information about age with the median age.\n    for title in titles:\n        data.loc[(data.Name == title) & (data.Age.isna()), 'Age'] = titles_median_age[title]\n    return data\n\n# def fare_to_cat(data):\n#     data[\"Fare\"] = data[\"Fare\"].qcut()\n#     return data\n\n#Calling functions.\ntitanic_data = remove_irrelevant_data(titanic_data)\ntitanic_data = embarked_to_numeric(titanic_data)\ntitanic_data = sex_to_numeric(titanic_data)\ntitanic_data = titles_to_age(titanic_data)\n\n#Assigning numeric values to titles.\ntitles = titanic_data.Name.unique()\ntitanic_data['Name'] = titanic_data.Name.apply(lambda x: list(titles).index(x))\n\n#Converting all numeric data to float.\ntitanic_data = titanic_data.astype(np.float64)\ndisplay(titanic_data)","47610ea1":"def show_corr():\n  plt.figure(figsize=(12,10))\n  cor = titanic_data.corr()\n  sns.heatmap(cor, annot=True, cmap=plt.cm.Purples, vmin=-1, vmax=1)\n  plt.show()\n\nshow_corr()","e09718b6":"# split the data to 80% train and 20% test\nt = titanic_data['Survived']\nX = titanic_data.drop('Survived', axis=1)\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=2)","aa786ef7":"#Create Logistic Regression classifier and predict the probabilities of the train and test data.\ndef logistic_regression():\n    logistic_reg = linear_model.LogisticRegression(max_iter=150)\n    logistic_reg.fit(X_train, t_train)\n    Y_train = logistic_reg.predict(X_train)\n    Y_test = logistic_reg.predict(X_test)\n    Y_train_prob = logistic_reg.predict_proba(X_train)\n    Y_test_prob = logistic_reg.predict_proba(X_test)\n    print('Logistic Regression')\n    print('Accuracy score on train', logistic_reg.score(X_train, t_train) * 100)\n    print('Accuracy score on test', logistic_reg.score(X_test, t_test)* 100)\n    print()\n    print('CE on train', metrics.log_loss(t_train, Y_train_prob)* 100)\n    print('CE on test', metrics.log_loss(t_test, Y_test_prob)* 100)\n    print()\n    \nlogistic_regression()","dc921835":"def mlp():\n    MLP_cls = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=10000).fit(X_train, t_train)\n    y_train_prob = MLP_cls.predict_proba(X_train)\n    y_test_prob = MLP_cls.predict_proba(X_test)\n    print('Accuracy score on train', MLP_cls.score(X_train, t_train) * 100)\n    print('Accuracy score on test', MLP_cls.score(X_test, t_test) * 100)\n    print()\n    print('CE on train', metrics.log_loss(t_train, y_train_prob) * 100)\n    print('CE on test', metrics.log_loss(t_test, y_test_prob) * 100)\n    \nmlp()","3b5e25b9":"#Reading data again.\ntitanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data = remove_irrelevant_data(titanic_data)\ntitanic_data = embarked_to_numeric(titanic_data)\ntitanic_data = sex_to_numeric(titanic_data)\ntitanic_data = titles_to_age(titanic_data)\n\n#Assigning numeric values to titles and combining \"special\" titles to a single title.\ndef combine_titles(data):\n    data['Name'] = data['Name'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Special')\n    titles = data.Name.unique()\n    data['Name'] = data.Name.apply(lambda x: list(titles).index(x))\n    return data\n\ntitanic_data = combine_titles(titanic_data)\n\n#Converting all numeric data to float.\ntitanic_data = titanic_data.astype(np.float64)\nshow_corr()\n\n# split the data to 80% train and 20% test\nt = titanic_data['Survived']\nX = titanic_data.drop('Survived', axis=1)\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=2)\n\n#Create Logistic Regression classifier and predict the probabilities of the train and test data.\nlogistic_regression()\n\n#MLP\nmlp()","5f9e93b2":"#Reading data again.\ntitanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data = remove_irrelevant_data(titanic_data)\ntitanic_data = embarked_to_numeric(titanic_data)\ntitanic_data = sex_to_numeric(titanic_data)\ntitanic_data = titles_to_age(titanic_data)\ntitanic_data = combine_titles(titanic_data)\n\n#Family size\ndef family_size(data):\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    data.drop('SibSp',axis='columns', inplace=True)\n    data.drop('Parch',axis='columns', inplace=True)\n    return data\n\ntitanic_data = family_size(titanic_data)\n\n#Converting all numeric data to float.\ntitanic_data = titanic_data.astype(np.float64)\nshow_corr()\n\n# split the data to 80% train and 20% test\nt = titanic_data['Survived']\nX = titanic_data.drop('Survived', axis=1)\nX_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=2)\n\n#Create Logistic Regression classifier and predict the probabilities of the train and test data.\nlogistic_regression()\n\n#MLP\nmlp()","c470ad3f":"titanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#Cleaning train data.\ntitanic_data = remove_irrelevant_data(titanic_data)\ntitanic_data = embarked_to_numeric(titanic_data)\ntitanic_data = sex_to_numeric(titanic_data)\ntitanic_data = titles_to_age(titanic_data)\ntitanic_data = combine_titles(titanic_data)\ntitanic_data = family_size(titanic_data)\ntitanic_data = titanic_data.astype(np.float64)\n\n#Cleaning test data.\ntitanic_data_test = remove_irrelevant_data(titanic_data_test)\ntitanic_data_test = embarked_to_numeric(titanic_data_test)\ntitanic_data_test = sex_to_numeric(titanic_data_test)\ntitanic_data_test = titles_to_age(titanic_data_test)\ntitanic_data_test = combine_titles(titanic_data_test)\ntitanic_data_test = family_size(titanic_data_test)\ntitanic_data_test = titanic_data_test.fillna(1)\ntitanic_data_test = titanic_data_test.astype(np.float64)\n\nt = titanic_data['Survived']\nX = titanic_data.drop('Survived', axis=1)\n\n# logistic_reg_test = linear_model.LogisticRegression(max_iter=150)\n# logistic_reg_test.fit(X, t)\n# Y_test_1 = logistic_reg_test.predict(titanic_data_test).astype(np.int)\n# submit = pd.DataFrame({'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId, 'Survived': Y_test_1})\n# submit.to_csv('submission.csv',index=False)","33a9abd8":"# find generator length\ndef find_generator_len(generator, use_pbar=True):\n    i = 0\n    \n    if use_pbar:\n        pbar = tqdm(desc='Calculating Length', ncols=1000, bar_format='{desc}{bar:10}{r_bar}')\n\n    for a in generator:\n        i += 1\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    return i\n\n# calculate score and loss from cv (KFold or LPO) and display graphs\ndef get_cv_score_and_loss(X, t, model, k=None, p=None, show_score_loss_graphs=False, use_pbar=True, show_confusion_matrix=False):\n    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n\n    if k is not None:\n        cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    elif p is not None:\n        cv = LeavePOut(p)\n    else:\n        raise ValueError('you need to specify k or p in order for the cv to work')\n\n    if use_pbar:\n        pbar = tqdm(desc='Computing Models', total=find_generator_len(cv.split(X)))\n\n    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n        X_train = X.loc[train_ids]\n        t_train = t.loc[train_ids]\n        X_val = X.loc[val_ids]\n        t_val = t.loc[val_ids]\n\n        model.fit(X_train, t_train)\n\n        y_train = model.predict(X_train)\n        y_val = model.predict(X_val)\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train), mean_squared_error(t_train, y_train,squared=False)]\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val), mean_squared_error(t_val, y_val,squared=False)]\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    val_scores_losses_df = scores_losses_df[scores_losses_df['split']=='val']\n    train_scores_losses_df = scores_losses_df[scores_losses_df['split']=='train']\n\n    mean_val_score = val_scores_losses_df['score'].mean()\n    mean_val_loss = val_scores_losses_df['loss'].mean()\n    mean_train_score = train_scores_losses_df['score'].mean()\n    mean_train_loss = train_scores_losses_df['loss'].mean()\n\n    if show_score_loss_graphs:\n        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n        fig.show()\n        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss: {mean_val_loss:.2f}, Mean Train Loss: {mean_train_loss:.2f}')\n        fig.show()\n        \n    if show_confusion_matrix:\n        y_pred = model.predict(X)\n        plot_confusion_matrix(model, X, t, display_labels = ['Died', 'Survived'])  \n        plt.show()  \n        print(classification_report(t, y_pred, target_names=['Died', 'Survived']))\n\n    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss","bb3f9616":"titanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#Fills na and keeping all the data.\ntitanic_data = remove_irrelevant_data(titanic_data)\ntitanic_data = embarked_to_numeric(titanic_data)\ntitanic_data = sex_to_numeric(titanic_data)\ntitanic_data = titles_to_age(titanic_data)\ntitanic_data = combine_titles(titanic_data)\ntitanic_data = family_size(titanic_data)\ntitanic_data = titanic_data.astype(np.float64)\n\n#Cleaning test data.\ntitanic_data_test = remove_irrelevant_data(titanic_data_test)\ntitanic_data_test = embarked_to_numeric(titanic_data_test)\ntitanic_data_test = sex_to_numeric(titanic_data_test)\ntitanic_data_test = titles_to_age(titanic_data_test)\ntitanic_data_test = combine_titles(titanic_data_test)\ntitanic_data_test = family_size(titanic_data_test)\ntitanic_data_test = titanic_data_test.astype(np.float64)\ntitanic_data_test = titanic_data_test.fillna(1)\n\nt = titanic_data['Survived']\nX = titanic_data.drop('Survived', axis=1)\nX[['Fare', 'Age']] = StandardScaler().fit_transform(X[['Fare', 'Age']])","1f578b2f":"# run KNN on the dataset and find best K by accuracy\nhyper_parameters = {'n_neighbors': list(range(1, 30))}\n\ngs_neigh_model = GridSearchCV(KNeighborsClassifier(n_neighbors=22), hyper_parameters).fit(X, t)\nprint('Accuracy score for classification:')\nprint('gs_neigh_model', gs_neigh_model.best_score_)\nprint('best params', gs_neigh_model.best_params_)\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X, t, gs_neigh_model, k=5, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","62890f68":"# Y_pred = gs_neigh_model.predict(titanic_data_test).astype(np.int)\n# submit = pd.DataFrame({'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId, 'Survived': Y_pred})\n# submit.to_csv('submission.csv',index=False)","5ad3882c":"# get score with nfold bagging\nbag_fold_model = BaggingClassifier(base_estimator=SGDClassifier(), n_estimators=50, random_state=1, bootstrap=False).fit(X, t)\nprint('Accuracy score for classification:')\nprint('bag_fold_model', bag_fold_model.score(X, t).mean())\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X, t, bag_fold_model, k=5, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","f16b7963":"# Y_pred = bag_fold_model.predict(titanic_data_test).astype(np.int)\n# submit = pd.DataFrame({'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId, 'Survived': Y_pred})\n# submit.to_csv('submission.csv',index=False)","d439589a":"# get score with ada boosting\nada_boost_model = AdaBoostClassifier(n_estimators=100, random_state=1).fit(X, t)\nprint('Accuracy score for classification:')\nprint('ada_boost_model', ada_boost_model.score(X, t).mean())\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X, t, ada_boost_model, k=5, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","11f89444":"# Y_pred = ada_boost_model.predict(titanic_data_test).astype(np.int)\n# submit = pd.DataFrame({'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId, 'Survived': Y_pred})\n# submit.to_csv('submission.csv',index=False)","9819827f":"# X[['Fare', 'Age']] = StandardScaler().fit_transform(X[['Fare', 'Age']])\nselector = RFECV(SGDClassifier(random_state=1), cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=1)).fit(X, t)\nX_RFECV = X.loc[:, selector.support_].copy()\ndisplay(X_RFECV.columns)\n\nprint('SGDClassifier:')\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_RFECV, t, SGDClassifier(random_state=1), k=5, use_pbar=False)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')\nprint()\nprint('AdaBoostClassifier:')\nada_boost_model = AdaBoostClassifier(n_estimators=100, random_state=1).fit(X_RFECV, t)\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_RFECV, t, ada_boost_model, k=5, use_pbar=False, show_confusion_matrix=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","8c6d8b03":"Y_pred = ada_boost_model.predict(titanic_data_test.loc[:, selector.support_]).astype(np.int)\nsubmit = pd.DataFrame({'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId, 'Survived': Y_pred})\nsubmit.to_csv('submission.csv',index=False)","f8f2fe33":"# Sources  \nWikipedia:  \nhttps:\/\/en.wikipedia.org\/wiki\/Titanic#:~:text=After%20leaving%20Southampton%20on%2010,11%3A40%20p.m.%20ship%27s%20time.  \n\nKenjee notebook:  \nhttps:\/\/www.kaggle.com\/kenjee\/titanic-project-example","b758c5ec":"Previous Baseline Logistic Regression:  \nAccuracy score on train 0.8132022471910112  \nAccuracy score on test 0.776536312849162  \n  \nCE on train 0.4190486126655971  \nCE on test 0.5043379264289621  \n\nPrevious Baseline MLP:  \nAccuracy score on train 0.797752808988764  \nAccuracy score on test 0.7486033519553073  \n  \nCE on train 0.45269697089731914  \nCE on test 0.5210330179408664  \n  \nWe saw little improvement combining SibSp and Parch using one family size to our scores. We made our CE smaller indicating better loss performance. Sometimes we will see lower accuracy, it is attributed to the random data in train and test.","be335a7f":"# Feature Engineering Testing  \nFirst I'll try and combine the special titles of the passengers.","f7df4d8c":"# Testing Summary  \n\nAcross my testing I consistenly saw better scores on both the accuracy and CE when using Logistic Regression. When considering runtime Logistic Regression performed better too.  \n\n### Exercise 3  \nUsing the new models and methods I learned in class I managed to improve my score by 0.7% (not significant). In both exercises I found that with a small number of features the best approach so far is to find the most significant way to fill missing information, test out many diffirent models and taking the one with the highest score. Dropping the features with a lot of missing information showed an improvement.  \nExamining the confusion matrix shows good scores over 80% overall.  \nIn conclusion, when observing the facts mentioned above. We can see that the better model for the titanic compition is AdaBoostClassifier.","5103f886":"Gender:  \nWe will clearly see that the number of female survivors is greater than male. \"Women and children first!\"  \nThis feature will have a big impact on our model.","fca558f3":"Embarked:  \nSeeing that the amount of people from Southampton that didn't survive is twice the amount of people that did survive, is and indication to some correlation. I have given numerical values to the ports.","e83f23e3":"Author: Tal Balelty  \nID: 312270291  \nKaggle Link: https:\/\/www.kaggle.com\/talbalelty\/titanic-investigating-dying-probability  \nExplanation: In this notebook I'm participating in a Kaggle compition. In this compition I'll will investigate what was the probability of a passenger dying onboard the Titanic. I'll use Logistics regression, SGD, MLP and several more to figure out the most appropiate model.  ","fe6126c2":"Our Bagging model produced better accuaracy and loss scores than KNN.  \nWe managed to improve the scores with n_estimator = 50.  \nUsing bootstrap didn't yield better scores.  ","334ee14a":"![](https:\/\/github.com\/Typhonnn\/ML_EX3\/blob\/main\/%D7%94%D7%92%D7%A9%D7%94%205%20RFECV%20ADABOOST.jpg?raw=true)","fb35d363":"Now let's check combining two features to one family size feature.","61f67ddf":"Previous Baseline Logistic Regression:  \nAccuracy score on train 0.8174157303370787  \nAccuracy score on test 0.770949720670391  \n\nCE on train 0.4198687798416333  \nCE on test 0.512678492024797  \n\nPrevious Baseline MLP:  \nAccuracy score on train 0.7963483146067416  \nAccuracy score on test 0.7374301675977654  \n\nCE on train 0.4530239933691066  \nCE on test 0.5324450349365049  \n\nWe saw small improvement combining the titles.  ","504ebe82":"We can clearly see the 1st class passengers had a better chance of survival compared to 3rd class passengers. 2nd class surviving chance is relativly balanced.  ","32bfb551":"# Data Analysis\n![](https:\/\/i.insider.com\/5a2587683339b01a008b4601?width=800&format=jpeg)\n\nPassengerId:  \nThere is no use for the colum so I dropped it.  \n\nPclass:  \nPassenger bought tickets to classes 1 = 1st, 2 = 2nd, 3 = 3rd. Below we can see the amounts distribution.","f443f7a4":"![image.png](attachment:image.png)","7d43b33c":"# Exercise 3  \nThis part of the notebook will demonstrate the new materials learned inclass.  ","39e66d8b":"Family:  \nWe will examine both SibSp and Parch together.  \nOur colum data is combined to include number of parents to a child, and number of children to a parent in one colum. In the second colum we have number of siblings and spouses combined. This can explain that high number of SibSp not surviving. Many husbands were left on the boat infavor of their wife and children.   \nTrying to Combine SibSp and Parch to a single 'FamilySize' feature generally yields very slightly worse results, but improves running time. Therefore, I have decided to keep it and drop the two colums.  ","afc63290":"Age:\nWe can see our age groups have a normal distribution. Considering the fact that our age data is not complete, I have decided to fill the ages by calculating the median of each title group.\nExamaning the correlation heatmap later on will provide us more information regarding the impact of age on surviving chances.  ","945bfdfd":"We can see an improvement over our previous scores other than the original","89f4b02d":"Names:  \nSeeing the titles table below, we can see that there are not a lot of people with special titles. I have given their titles numeric values. Having so few people with special titles I have decided to combine most of them to one title.","98013435":"![](https:\/\/github.com\/Typhonnn\/ML_EX3\/blob\/main\/%D7%94%D7%92%D7%A9%D7%94%203%20bagging.jpg?raw=true)","23138bd6":"We got a worse score with KNN.","e3c38679":"We got an improvement over KNN! Let's move on the AdaBoost.","d70c8d84":"After using the function above our overall score did not change. Only our CE shows little improvement, leading me to believe filling the NaN age with the \"most accurate\" data didn't impact the model.","941df58d":"Cabin:  \nMany of the passengers do not have a cabin assosiated with them. Therefore I have decided to drop the Cabin colum all together to avoid having it's negative impact on my model.   \n\nTicket:  \nI couldn't figure out a relation between the characters on the tickets to the ship, so I have decided to drop the Ticket colum.  \n\nFare:  \nIn the next two barplots we can see that most passengers paid less than 100$. I tried to remove outliers of large payers, and I found it made no difference to our data distribution and model accuracy.","bea3476d":"After we finished with feature engineering, let's examine our non-categoric data with an heatmap. We can see that the highest correlation is 0.41 between SibSp and Parch. It is not high enough to impact our model.","69ea43e0":"# Feature Baseline Summary  \n  \nLet's display the data as is with only giving numeric values to categories. This will function as a baseline.","1a6bb7cf":"![](https:\/\/github.com\/Typhonnn\/ML_EX3\/blob\/main\/%D7%94%D7%92%D7%A9%D7%94%204%20adaboost.jpg?raw=true)","f01b6fab":"# Confusion Matrix  \nConfusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in a actual class while each column represents the instances in an predicted class (or vice versa).  \nIn our case we built our confusion matrix with the CV data. We can take a look at the first square (490) which represents the number of people the model predicted to die and the actual number of people who died.  \nBelow the confusion matrix we can find the KPI values. KPI helps us figure out how our model performed by our desired use case.  \nIn this competition Kaggle uses the accuracy of the prediction to score and rank the participants.  \nLet's see how accuaracy is calculated:  \nAcc = (TN + TP) \/ Total population = (490 + 270) \/ 891 = 0.85  \n\n\nAfter using RFECV chosen features, we will take the selected features with our best model ,AdaBoostClassifier, which gave us the following score.  ","755dddaf":"![](https:\/\/github.com\/Typhonnn\/ML_EX3\/blob\/main\/%D7%94%D7%92%D7%A9%D7%94%202%20KNN.jpg?raw=true)","9a1828c4":"So far we can see AdaBoostClassifier gave us the best scores."}}