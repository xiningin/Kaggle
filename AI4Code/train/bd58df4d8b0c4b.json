{"cell_type":{"4da3a2b8":"code","90b72595":"code","4e2c05a0":"code","4fb8c195":"code","51522449":"code","6d59ac82":"code","231e71a2":"code","b1b21294":"code","943882e7":"code","4f9be2de":"code","371f4214":"code","7f9b11c2":"code","b1c0d23c":"code","66c74770":"code","00d61bd0":"code","7b18092f":"code","072593e3":"code","fc20bdd8":"code","b91ee23e":"code","4fd861d4":"code","794d135e":"code","d81de630":"code","fead4cdf":"code","8d25c5a4":"code","321f2714":"code","96fe0082":"code","93c60bfa":"code","51268dd4":"code","b2561040":"code","5403b653":"code","84db4098":"code","b3608a8a":"code","c161d39c":"code","a67cbd05":"code","4396f63b":"code","8e8a062a":"code","c5f195ce":"code","5fac3348":"code","45e7a7de":"code","36078663":"code","404af4a6":"code","84058448":"code","4da5e9ae":"code","f2efb2a4":"code","81cce2cf":"code","5c1e1bbf":"code","bf1453eb":"code","81af98c9":"code","cfc76b2e":"code","1a121c27":"code","1c8e275d":"code","24853a28":"code","93acffaf":"code","2ae4fd49":"code","177ea83b":"code","403c114b":"code","18103feb":"code","08852c3e":"code","b8c5e57f":"code","61f08c06":"code","9b1a2ce1":"code","1f170d2b":"code","1e23c849":"code","32f62f81":"code","a3396e2b":"code","be727755":"code","71e8f57c":"code","4f4e1348":"code","494f625b":"markdown","a7c0b12a":"markdown","0eb9b648":"markdown","e2ea44de":"markdown","68c7b50c":"markdown","a863d2ec":"markdown","d50abe8c":"markdown","cd8f16f8":"markdown","77c7a868":"markdown","d604c669":"markdown","eec98401":"markdown","bf59246b":"markdown","1aaf10a8":"markdown","4aafdda4":"markdown","194dfc38":"markdown","5ada1d92":"markdown","f18bea03":"markdown","838b6ef3":"markdown","f148e9ca":"markdown","25eb0d5f":"markdown","e6c973df":"markdown","a409338f":"markdown","dadf807d":"markdown","6a511742":"markdown","ae02c5aa":"markdown","ee3ba865":"markdown","85c62699":"markdown","a9a2a5af":"markdown","9a443cae":"markdown","dbfec5a7":"markdown","eb861087":"markdown","d8109c49":"markdown","894c14c9":"markdown","1fbda9ab":"markdown","7af12a22":"markdown","17bc2cd6":"markdown","5776bbb4":"markdown","fa87c4bc":"markdown","c0a21dd8":"markdown","4359c2d8":"markdown","0bc42f72":"markdown","c0cd2684":"markdown","b7e7ab58":"markdown","19826e55":"markdown","032618be":"markdown","ca6b0b6e":"markdown"},"source":{"4da3a2b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90b72595":"import numpy as np \nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import metrics \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport numpy as np # linear algebra\nimport pandas as pd\n\nmin_val_corr = 0.4 ","4e2c05a0":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","4fb8c195":"test_Id = test_df['Id']\n\ntrain_df.drop('Id', axis=1, inplace=True)\ntest_df.drop('Id', axis=1, inplace=True)\n","51522449":"train_df.shape","6d59ac82":"test_df.shape","231e71a2":"#train_category = train_df.dtypes[train_df.dtypes == \"object\"].index\n#test_numeric = test_df.dtypes[test_df.dtypes != \"object\"].index\n#test_category = test_df.dtypes[test_df.dtypes == \"object\"].index\n","b1b21294":"#train_df[train_numeric].columns\n#test_df[test_numeric].columns","943882e7":"#train_df[train_category].columns\n#test_df[train_category].columns","4f9be2de":"print(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","371f4214":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","7f9b11c2":"# Print out Skewness and Kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","b1c0d23c":"num_features = train_df.select_dtypes(exclude='object').drop(['SalePrice'], axis=1)","66c74770":"fig = plt.figure(figsize=(16,20))\n\nfor i in range(len(num_features.columns)):\n    fig.add_subplot(9, 5, i+1)\n    sns.boxplot(y=num_features.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","00d61bd0":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(num_features.columns)):\n    fig.add_subplot(9, 5, i+1)\n    sns.scatterplot(num_features.iloc[:, i],train_df['SalePrice'])\nplt.tight_layout()\nplt.show()","7b18092f":"\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=train_df['TotalBsmtSF'], y=train_df['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=train_df['1stFlrSF'], y=train_df['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=train_df['MasVnrArea'], y=train_df['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=train_df['GarageArea'], y=train_df['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=train_df['TotRmsAbvGrd'], y=train_df['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()\n","072593e3":"\npos = [1298,523,297]\ntrain_df.drop(train_df.index[pos], inplace=True)\n","fc20bdd8":"train_df.head()","b91ee23e":"train_df.shape","4fd861d4":"# Save target value for later\ny = train_df.SalePrice.values\n\n# In order to make imputing easier, we combine train and test data\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)\ndataset = pd.concat((train_df, test_df)).reset_index(drop=True)","794d135e":"dataset.head()","d81de630":"dataset.shape","fead4cdf":"missing = dataset.isnull().sum().sort_values(ascending=False)\ntotal = dataset.isnull().count().sort_values(ascending=False)\npercent = (dataset.isnull().sum() \/ dataset.isnull().count()).sort_values(ascending=False)\nMissing_values = pd.concat([missing,percent], axis=1, keys=['Missing','Percent'])\nMissing_values.head(20)","8d25c5a4":"sns.set()\nmissing.plot(kind='bar',figsize=(20,8))\n","321f2714":"dataset[missing.index].dtypes","96fe0082":"for col in ('FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC', 'MSSubClass'):\n    dataset[col] = dataset[col].fillna('None')\n","93c60bfa":"for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'Functional', 'MSZoning', 'SaleType', 'Utilities'):\n    dataset[col] = dataset[col].fillna(dataset[col].mode()[0])","51268dd4":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    dataset[col] = dataset[col].fillna(0)\n    \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    dataset[col] = dataset[col].fillna('None')","b2561040":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    dataset[col] = dataset[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    dataset[col] = dataset[col].fillna('None')","5403b653":"dataset.isnull().sum()[dataset.isnull().sum() > 0].sort_values(ascending=False)","84db4098":"dataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\ndataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)","b3608a8a":"# LotFrontage is correlated to the 'Neighborhood' feature because the LotFrontage for nearby houses will be really similar, so we fill in missing values by the median based off of Neighborhood\ndataset[\"LotFrontage\"] = dataset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","c161d39c":"list(dataset.select_dtypes(exclude='object').columns)","a67cbd05":"# Features 'MSSubClass', 'YrSold', 'OverallCond' and 'MoSold' are supposed to be categorical features so change their type\ndataset['MSSubClass'] = dataset['MSSubClass'].apply(str)\ndataset['YrSold'] = dataset['YrSold'].apply(str)\ndataset['MoSold'] = dataset['MoSold'].apply(str)\ndataset['OverallCond'] = dataset['OverallCond'].astype(str)\n\n# Features 'LotArea' and 'MasVnrArea' are supposed to be numerical features so change their type\ndataset['LotArea'] = dataset['LotArea'].astype(np.int64)\ndataset['MasVnrArea'] = dataset['MasVnrArea'].astype(np.int64)\n","4396f63b":"figure, (ax1, ax2, ax3,ax4) = plt.subplots(nrows=1, ncols=4)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(train_df['TotalBsmtSF'], y, ax=ax1)\n_ = sns.regplot(train_df['1stFlrSF'], y, ax=ax2)\n_ = sns.regplot(train_df['2ndFlrSF'], y, ax=ax3)\n_ = sns.regplot(train_df['TotalBsmtSF'] + train_df['2ndFlrSF']+train_df['1stFlrSF'], y, ax=ax4)","8e8a062a":"dataset['TotalSF']=dataset['TotalBsmtSF']  + dataset['1stFlrSF'] + dataset['2ndFlrSF']","c5f195ce":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.barplot(train_df['BsmtFullBath'], y, ax=ax1)\n_ = sns.barplot(train_df['FullBath'], y, ax=ax2)\n_ = sns.barplot(train_df['BsmtHalfBath'], y, ax=ax3)\n_ = sns.barplot(train_df['BsmtFullBath'] + train_df['FullBath'] + train_df['BsmtHalfBath'] + train_df['HalfBath'], y, ax=ax4)","5fac3348":"dataset['TotalBath']=dataset['BsmtFullBath'] + dataset['FullBath'] + (0.5*dataset['BsmtHalfBath']) + (0.5*dataset['HalfBath'])","45e7a7de":"figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfigure.set_size_inches(18,8)\n_ = sns.regplot(train_df['YearBuilt'], y, ax=ax1)\n_ = sns.regplot(train_df['YearRemodAdd'],y, ax=ax2)\n_ = sns.regplot((train_df['YearBuilt']+train_df['YearRemodAdd'])\/2, y, ax=ax3)","36078663":"dataset['YrBltAndRemod']=dataset['YearBuilt']+dataset['YearRemodAdd']","404af4a6":"figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(train_df['OpenPorchSF'], y, ax=ax1)\n_ = sns.regplot(train_df['3SsnPorch'], y, ax=ax2)\n_ = sns.regplot(train_df['EnclosedPorch'], y, ax=ax3)\n_ = sns.regplot(train_df['ScreenPorch'], y, ax=ax4)\n_ = sns.regplot(train_df['WoodDeckSF'], y, ax=ax5)\n_ = sns.regplot((train_df['OpenPorchSF']+train_df['3SsnPorch']+train_df['EnclosedPorch']+train_df['ScreenPorch']+train_df['WoodDeckSF']), y, ax=ax6)","84058448":"dataset['Porch_SF'] = (dataset['OpenPorchSF'] + dataset['3SsnPorch'] + dataset['EnclosedPorch'] + dataset['ScreenPorch'] + dataset['WoodDeckSF'])","4da5e9ae":"dataset['Has2ndfloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasBsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasFirePlace'] =dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndataset['Has2ndFlr']=dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasPool']=dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)","f2efb2a4":"from sklearn.preprocessing import LabelEncoder\ncategorical_col = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor col in categorical_col:\n    label = LabelEncoder() \n    label.fit(list(dataset[col].values)) \n    dataset[col] = label.transform(list(dataset[col].values))\n\nprint('Shape all_data: {}'.format(dataset.shape))","81cce2cf":"num_features = dataset.dtypes[dataset.dtypes != 'object'].index\nskewed_features = dataset[num_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(15)\n","5c1e1bbf":"numerical_dataset = dataset.select_dtypes(exclude='object')\n\nfor i in range(len(numerical_dataset.columns)):\n    f, ax = plt.subplots(figsize=(7, 4))\n    fig = sns.distplot(numerical_dataset.iloc[:,i].dropna(), rug=True, hist=False, label='UW', kde_kws={'bw':0.1})\n    plt.xlabel(numerical_dataset.columns[i])","bf1453eb":"skewness = skewed_features[abs(skewed_features) > 0.75]\nskewed_features = skewness.index\n\nlam = 0.15\nfor i in skewed_features:\n    dataset[i] = boxcox1p(dataset[i], lam)\n","81af98c9":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","cfc76b2e":"# Hot-Encode Categorical features\ndataset = pd.get_dummies(dataset) \n\n# Splitting dataset back into X and test data\nX = dataset[:len(train_df)]\ntest = dataset[len(train_df):]\n\nX.shape","1a121c27":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=0)","1c8e275d":"# Indicate number of folds for cross validation\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Parameters for models\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]","24853a28":"# Lasso Model\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas = alphas2, random_state = 42, cv=kfolds))\n\n# Printing Lasso Score with Cross-Validation\nlasso_score = cross_val_score(lasso, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlasso_rmse = np.sqrt(-lasso_score.mean())\nprint(\"LASSO RMSE: \", lasso_rmse)\nprint(\"LASSO STD: \", lasso_score.std())\n","93acffaf":"# Training Model for later\nlasso.fit(X_train, y_train)","2ae4fd49":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv=kfolds))\nridge_score = cross_val_score(ridge, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nridge_rmse =  np.sqrt(-ridge_score.mean())\n# Printing out Ridge Score and STD\nprint(\"RIDGE RMSE: \", ridge_rmse)\nprint(\"RIDGE STD: \", ridge_score.std())","177ea83b":"# Training Model for later\nridge.fit(X_train, y_train)","403c114b":"elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\nelastic_score = cross_val_score(elasticnet, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nelastic_rmse =  np.sqrt(-elastic_score.mean())\n\n# Printing out ElasticNet Score and STD\nprint(\"ELASTICNET RMSE: \", elastic_rmse)\nprint(\"ELASTICNET STD: \", elastic_score.std())\n","18103feb":"# Training Model for later\nelasticnet.fit(X_train, y_train)","08852c3e":"lightgbm = make_pipeline(RobustScaler(),\n                        LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=720,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.2319,\n                                      feature_fraction_seed=9, bagging_seed=9,\n                                      min_data_in_leaf =6, \n                                      min_sum_hessian_in_leaf = 11))\n\n# Printing out LightGBM Score and STD\nlightgbm_score = cross_val_score(lightgbm, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlightgbm_rmse = np.sqrt(-lightgbm_score.mean())\nprint(\"LIGHTGBM RMSE: \", lightgbm_rmse)\nprint(\"LIGHTGBM STD: \", lightgbm_score.std())","b8c5e57f":"# Training Model for later\nlightgbm.fit(X_train, y_train)","61f08c06":"xgboost = make_pipeline(RobustScaler(),\n                        XGBRegressor(learning_rate =0.01, n_estimators=3460, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))\n\n# Printing out XGBOOST Score and STD\nxgboost_score = cross_val_score(xgboost, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nxgboost_rmse = np.sqrt(-xgboost_score.mean())\nprint(\"XGBOOST RMSE: \", xgboost_rmse)\nprint(\"XGBOOST STD: \", xgboost_score.std())","9b1a2ce1":"# Training Model for later\nxgboost.fit(X_train, y_train)","1f170d2b":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, \n                                            xgboost, lightgbm), \n                               meta_regressor=xgboost,\n                               use_features_in_secondary=True)\n\nstack_score = cross_val_score(stack_gen, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nstack_rmse = np.sqrt(-stack_score.mean())\nprint(\"STACK RMSE: \", stack_rmse)\nprint(\"STACK STD: \", stack_score.std())\n","1e23c849":"#Training Model for later\nstack_gen.fit(X_train, y_train)\n","32f62f81":"results = pd.DataFrame({\n    'Model':['Lasso',\n            'Ridge',\n            'ElasticNet',\n            'LightGBM',\n            'XGBOOST',\n            'STACK'],\n    'Score':[lasso_rmse,\n             ridge_rmse,\n             elastic_rmse,\n             lightgbm_rmse,\n             xgboost_rmse,\n             stack_rmse\n            ]})\n\nsorted_result = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nsorted_result\n","a3396e2b":"f, ax = plt.subplots(figsize=(14,8))\nplt.xticks(rotation='90')\nsns.barplot(x=sorted_result['Model'], y=sorted_result['Score'])\nplt.xlabel('Model', fontsize=15)\nplt.ylabel('Performance', fontsize=15)\nplt.ylim(0.10, 0.12)\nplt.title('RMSE', fontsize=15)","be727755":"# Predict every model\nlasso_pred = lasso.predict(test)\nridge_pred = ridge.predict(test)\nelasticnet_pred = elasticnet.predict(test)\nlightgbm_pred = lightgbm.predict(test)\nxgboost_pred = xgboost.predict(test)\nstacked_pred = stack_gen.predict(test)","71e8f57c":"# Combine predictions into final predictions\nfinal_predictions = np.expm1((0.2*elasticnet_pred) + (0.2*lasso_pred) + (0.2*ridge_pred) + \n               (0.1*xgboost_pred) + (0.1*lightgbm_pred) + (0.2*stacked_pred))","4f4e1348":"submission = pd.DataFrame()\nsubmission['Id'] = test_Id\nsubmission['SalePrice'] = final_predictions\nsubmission.to_csv('submission.csv',index=False)","494f625b":"The sales price is right skewed. The reasons for right skewed predictive variable : Mean greater than mode, median greater than mode, Mean greater than median. This eventually affects the performance. So we Log transform the SalesPrice with np.log1p","a7c0b12a":"Imputing LotFrontage with median based on 'Neighborhood'","0eb9b648":"Find the total and percentage of missing values in dataset","e2ea44de":"**Split the numeric and categorical data**","68c7b50c":"**Ridge Regression**","a863d2ec":"**ElasticNet**","d50abe8c":"**Lasso Regression**","cd8f16f8":"**Parameters**\n\n'kfolds' is used for cross validation, the list type variables are for the models","77c7a868":"Imputing Garage-Related Features which are numerical with 0 and 'None' for categorical\n\nA Garage-Related Feature with a missing value usually indicates there is no Garage","d604c669":"# **Feature Engineering**","eec98401":"**Univariate Analysis**","bf59246b":"Stacking: At this point we basically trained and predicted each model so we can combine its predictions into a 'final_predictions' variable for submission.","1aaf10a8":"Change Datatypes","4aafdda4":"**View Model Performance**","194dfc38":"# Missing values Imputation","5ada1d92":"XGBoost","f18bea03":"**Perform BoxCox Transformation on skewed features**","838b6ef3":"# Outlier Detection","f148e9ca":"**Create 'TotalBath' Feature**\n\nVisualize 'BsmtFullBath', 'FullBath', 'BsmtHalfBath' and them together, Create 'TotalBath' for total number of bathrooms using those features.","25eb0d5f":"**Creating Extra Features**\n\nCreating useful extra features in order to strongly distinct data\n\nEx: 'Has2ndfloor' feature below indicates whether there is a 2ndfloor or not","e6c973df":"**Label Encoding**\n\nOur dataset cannot run with categorical columns so we must Label Encode these columns in order to make them numerical.","a409338f":"**Submission**","dadf807d":"**Missing Values Imputation**","6a511742":"Stacking\n\nPredict every model, then combine every prediction into a final predictions used for submission","ae02c5aa":"**Skew and Kurtosis of SalesPrice. Visualization of Sales Price through Distplot and Probplot**","ee3ba865":"Models Used\n\n1. Lassocv\n2. Ridge\n3. ElasticNet\n4. LightGBM\n5. XGBoost\n6. CATBoost\n7. Stacked","85c62699":"**Split the data into train and validation set**","a9a2a5af":"**Create TotalBath Feature**","9a443cae":"There are two features related to MasVnr that have missing values: 'MasVnrType' and 'MasVnrArea'.\n'MasVnrType' is a categorical feature that we need to impute with 'None'\n'MasVnrArea' is a categorical feature that we need to impute with 'MasVnrType'","dbfec5a7":"**LightGBM**","eb861087":"**Data Preparation**","d8109c49":"The reason we fill in these features with their mode (most common value) is because these features are mandatory in a house, meaning that if these values are missing it has to be because of the data, not because of the fact that the house is missing the feature.\nEx: For the 'Electrical' feature we fill in the most common value because every house has electricity, therefore it wouldn't make sense for there to be any missing values.","894c14c9":"**Create 'YrBuiltAndRemod' Feature**\n\nVisualize 'YearBuilt', 'YearRemodAdd', and them together, Create 'YrBltAndRemod' for Year Built and Year remodel combined using those features","1fbda9ab":"* Changing Types\n* Create 'TotalSF' Feature\n* Create 'TotalBath' Feature\n* Create 'YrBuiltAndRemod' Feature\n* Create 'PorchSF' Feature\n* Creating Extra Features\n* Label Encoding","7af12a22":"**Create 'TotalSF' Feature**\n\nVisualize 'TotalBsmtSF' and '2ndFlrSF' and them together, Create 'TotalSF' for total surface area with those features.","17bc2cd6":"Combining train and test data in order to make imputing missing values easier, locating missing values.","5776bbb4":"**Stacked**","fa87c4bc":"**Bivariate Analysis**","c0a21dd8":"Impute Basement related features","4359c2d8":"dataframe.dtypes.index - dtype. Pandas Index is an immutable ndarray implementing an ordered, sliceable set. It is the basic object which stores the axis labels for all pandas objects.","0bc42f72":"# **High Skewed Features**","c0cd2684":"# Housing Sales Price Prediction using Advanced Regression techniques","b7e7ab58":"**Numeric variables Imputation**","19826e55":"**Removing Outliers**","032618be":"**Create 'PorchSF' Feature**\n\nVisualize 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF' and them together, Create 'Porch_SF' for total porch surface using those features","ca6b0b6e":"**Skew Visualization**"}}