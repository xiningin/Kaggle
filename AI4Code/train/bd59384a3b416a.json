{"cell_type":{"25d1a8b9":"code","f1a06e77":"code","de8b946f":"code","05be53f3":"code","da9b2aea":"code","ad1e1495":"code","1d0bb64d":"code","b088b6a0":"code","240beb85":"code","a4713ed3":"code","c8853030":"code","514c1cc0":"code","78054144":"code","07b44ac1":"code","d858e4bb":"code","9faf2443":"code","b1fa6e03":"code","18675411":"code","c3a66c14":"code","34efbaa8":"code","3c69983c":"code","00ee443a":"code","d74ec7dd":"code","82d8f056":"code","cbd65883":"code","a30d96cb":"code","14dc79e0":"code","4103e321":"code","8bc02dab":"code","02d2dae4":"code","3fbb18c9":"code","34f0d2e3":"code","4e140237":"code","395eb8d3":"code","56eb142c":"code","25f62559":"code","bc3b1745":"code","be26766f":"code","374cb8d6":"code","5a766a73":"code","46de6d0b":"code","5d447524":"code","82031907":"code","3e5443c5":"code","f7a0ec26":"code","22124e38":"markdown","b02b9fbe":"markdown","d35f4813":"markdown","0b7cfa41":"markdown","d4ca7921":"markdown","8c2f0abd":"markdown"},"source":{"25d1a8b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1a06e77":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics, svm\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline","de8b946f":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndata_cleaner=[train,test]","05be53f3":"#data cleaning\nprint('Train columns with null values:\\n', train.isnull().sum())\nprint(\"-\"*10)\nprint(\"Test columns with null values:\\n\",test.isnull().sum())","da9b2aea":"columns_drop=['Ticket']\nfor data in data_cleaner:\n    data.drop(columns_drop,axis=1,inplace=True)","ad1e1495":"normalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}","1d0bb64d":"#feature engineering\nfor data in data_cleaner:\n    data['Family']=data['SibSp']+data['Parch']+1\n    data['Alone']=1\n    data['Alone'].loc[data['Family']>1]=0\n    data['Title'] = data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    data.Title = data.Title.map(normalized_titles)","b088b6a0":"train.Title.value_counts()","240beb85":"for data in data_cleaner:\n    data.Age.fillna(data.Age.median(),inplace=True)","a4713ed3":"for data in data_cleaner:\n    data.Cabin.fillna('U',inplace=True)","c8853030":"#imputer fare and embarked\nfor data in data_cleaner:\n    data.Fare.fillna(data.Fare.median(),inplace=True)\n    data.Embarked.fillna(data.Embarked.mode()[0],inplace=True)","514c1cc0":"#Drop Name, SibSp, Parch\ndrop_col=['Name','SibSp','Parch']\nfor data in data_cleaner:\n    data.drop(drop_col,axis=1,inplace=True)","78054144":"#extract first letter and fill in unknown for nan\nfor data in data_cleaner:\n    data.fillna('U',inplace=True)\n    data.Cabin=data.Cabin.map(lambda x:x[0])","07b44ac1":"#one hot encode Sex,Cabin,Title,Embarked\nOH_cols=['Sex','Cabin','Embarked','Title']\nenc=OneHotEncoder(handle_unknown='ignore',sparse=False)\nOH_cols_train=pd.DataFrame(enc.fit_transform(train[OH_cols]))\nOH_cols_train.columns=enc.get_feature_names(OH_cols)\nOH_cols_test=pd.DataFrame(enc.transform(test[OH_cols]))\nOH_cols_test.columns=enc.get_feature_names(OH_cols)\n\n\nOH_cols_train.index=train.index\nOH_cols_test.index=test.index","d858e4bb":"#joining them back and assign new df\ntrain=pd.concat([train,OH_cols_train],axis=1)\ntest=pd.concat([test,OH_cols_test],axis=1)","9faf2443":"df=[train,test]\nfor ind in df:\n    ind.drop(OH_cols,axis=1,inplace=True)","b1fa6e03":"#bin Fare\nfor ind in df:\n    print(ind.info())","18675411":"def fare_bin(x):\n    if x <= 7.91:\n        return 1\n    elif x > 7.91 and x <= 14.454:\n        return 2\n    elif  x > 14.454 and x <= 31:\n        return 3\n    elif x > 31 and x < 513:\n        return 4","c3a66c14":"train.head()","34efbaa8":"for ind in df:\n    ind.Fare=ind.Fare.map(lambda x: fare_bin(x))","3c69983c":"\ndef age_bin(x):\n    if x <=10:\n        return 0\n    elif x>10 and x<=20:\n        return 1\n    elif x>20 and x<=30:\n        return 2\n    elif x>30 and x<=40:\n        return 3\n    elif x>40 and x<=50:\n        return 4\n    elif x>50 and x<=60:\n        return 5\n    elif x>60 and x<=70:\n        return 6\n    elif x>70 and x<=80:\n        return 7\n    elif x>80 and x<=90:\n        return 8","00ee443a":"for ind in df:\n    ind.Age=ind.Age.map(lambda x: age_bin(x))","d74ec7dd":"test.info()","82d8f056":"train.columns","cbd65883":"len(test.columns)","a30d96cb":"len(train.columns)","14dc79e0":"feature_col=['Pclass', 'Age', 'Fare', 'Family', 'Alone',\n       'Sex_female', 'Sex_male', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n       'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_U', 'Embarked_C',\n       'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss', 'Title_Mr',\n       'Title_Mrs', 'Title_Officer', 'Title_Royalty']","4103e321":"target=train['Survived']\nfeatures_train=train[feature_col]\nX_test=test.drop('PassengerId',axis=1)","8bc02dab":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","02d2dae4":"rf_params=dict(     \n    max_depth = [n for n in range(11, 16)],     \n    min_samples_split = [n for n in range(6, 13)], \n    min_samples_leaf = [n for n in range(4, 7)],     \n    n_estimators = [n for n in range(30, 80, 10)],\n)","3fbb18c9":"rf_model=RandomForestClassifier()\nrf=GridSearchCV(estimator=rf_model,param_grid=rf_params,cv=5,n_jobs=-1)\nrf.fit(features_train,target)","34f0d2e3":"print(\"Best score: {}\".format(rf.best_score_))\nprint(\"Optimal params: {}\".format(rf.best_estimator_))","4e140237":"gbcgs_params = {\n    'loss' : [\"deviance\",\"exponential\"],\n    'n_estimators' : [30,40,50,60],\n    'learning_rate': [0.02,0.03,0.04,0.05,0.06],\n    'max_depth':  [2,3,4],\n    'max_features': [2,3,4],\n    \"min_samples_split\": [2,3,4],\n    'min_samples_leaf': [2,3,4]\n}","395eb8d3":"gb_model=GradientBoostingClassifier()\ngb=GridSearchCV(estimator=gb_model,param_grid=gbcgs_params,cv=5,n_jobs=-1)\ngb.fit(features_train,target)","56eb142c":"print(\"Best score: {}\".format(gb.best_score_))\nprint(\"Optimal params: {}\".format(gb.best_estimator_))","25f62559":"svc_params = {\n    'kernel': ['rbf'],\n    'C':     [10,20,30,40,50,60,70],\n    'gamma': [0.005,0.006,0.007,0.008,0.009,0.01,0.011],\n    'probability': [True]\n}","bc3b1745":"svc_model=svm.SVC()\nsvc=GridSearchCV(svc_model,svc_params,cv=5,n_jobs=-1)\nsvc.fit(features_train,target)","be26766f":"print(\"Best score: {}\".format(svc.best_score_))\nprint(\"Optimal params: {}\".format(svc.best_estimator_))","374cb8d6":"ExtC = ExtraTreesClassifier()\n## Search grid for optimal parameters\nex_params = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_params, cv=5, scoring=\"accuracy\", n_jobs= 10, verbose = 1)\ngsExtC.fit(features_train,target)\nExtC_best = gsExtC.best_estimator_","5a766a73":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_params = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_params, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(features_train,target)\n\nada_best = gsadaDTC.best_estimator_","46de6d0b":"ensemble=VotingClassifier(estimators=[('RF',rf.best_estimator_),('GB',gb.best_estimator_),('SVC',svc.best_estimator_),('extc', ExtC_best),('adac',ada_best)],voting='soft')","5d447524":"ensemble.fit(features_train,target)","82031907":"pred=ensemble.predict(X_test)","3e5443c5":"submission=pd.DataFrame({'PassengerId':test.PassengerId,'Survived':pred})","f7a0ec26":"filename = 'my_submission.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)\n","22124e38":"Modelling","b02b9fbe":"Finished Feature Enginnering and Data Cleaning","d35f4813":"Random Forest","0b7cfa41":"Grad Boosting","d4ca7921":"Random Forest","8c2f0abd":"Reference\n\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#Step-5:-Model-Data\n\nhttps:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9\n\nhttps:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic"}}