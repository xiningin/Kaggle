{"cell_type":{"e7fad78f":"code","14be33d9":"code","c8c46aad":"code","970375b2":"code","63b6fc93":"code","0cc8ce50":"code","f4afef92":"code","004efb98":"code","30104a4c":"code","db19695b":"code","e7e9e0eb":"code","c5feea9e":"code","692c22cc":"code","064bfc22":"code","e115d4c5":"code","d53d3a8f":"code","d11b0f3e":"code","fea3a43f":"code","5f141575":"code","50d8e811":"code","fcd661ac":"code","4bec1674":"code","3491c200":"code","7db00b74":"code","000db4e7":"code","02d69b54":"code","afbb30a6":"code","4aa44c72":"code","657b2377":"code","0872e1f9":"code","99c81246":"code","895775ed":"code","825c1505":"markdown","a4f48aba":"markdown","02cc95b6":"markdown","27fad1a4":"markdown","6c391c25":"markdown","08a06161":"markdown","d070a020":"markdown","6e54d1a6":"markdown","061e980a":"markdown","855c07d4":"markdown"},"source":{"e7fad78f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14be33d9":"from __future__ import print_function, division\nfrom future.utils import iteritems\nfrom builtins import range\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom collections import  Counter\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\nimport math\nimport string\nfrom sklearn.metrics import confusion_matrix\n\nwordnet_lemmatizer = WordNetLemmatizer()","c8c46aad":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission_label = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","970375b2":"print('Trainning Set Shape={}'.format(train.shape))\nprint('Testing Set Shape={}'.format(test.shape))","63b6fc93":"train['target'].value_counts(normalize=True) #proportion of disaster and non disaster tweets","0cc8ce50":"train.isnull().sum() * 100 \/ len(train) #Precentage of null values in train set","f4afef92":"test.isnull().sum() * 100 \/ len(test) #Precentage of null values in test set","004efb98":"x=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","30104a4c":"def create_corpus(target):\n    \"\"\"\n    This function is returning a list of words from the text which belongs to particular target value(0 or 1)\n    \"\"\"\n    corpus=[]\n    \n    for doc in train[train['target']==target]['text'].str.split():\n        for word in doc:\n            word = word.lower()\n            corpus.append(word)\n    return corpus","db19695b":"stop=set(stopwords.words('english')) # storing stopwords of english\n#Getting a corpus for non disaster text & creating a dictionary where key is word and its value is count of that word\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] #Sorting keys according to values","e7e9e0eb":"x,y=zip(*top)\nplt.bar(x,y)\nplt.xlabel('Non Disaster Stop Words')  \nplt.ylabel('Count') \nplt.title(\"Count Vs Stop Words\")","c5feea9e":"#Getting a corpus for disaster text & creating a dictionary where key is word and its value is count of that word\ncorpus=create_corpus(1)\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] #Sorting keys according to values","692c22cc":"x,y=zip(*top)\nplt.bar(x,y)\nplt.bar(x,y)\nplt.xlabel('Disaster Stop Words')  \nplt.ylabel('Count') \nplt.title(\"Count Vs Stop Words\")","064bfc22":"#Top 20 Common words for target = 0 (No disaster)\ncorpus=create_corpus(0)\nstart = 0\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:]:\n    if start == 20:\n        break\n    if (word not in stop) :\n        start += 1\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)\nplt.xlabel('Count')  \nplt.ylabel('words') \nplt.title(\"Words vs Count\")","e115d4c5":"#Top 20 Common words for target = 1 (disaster)\ncorpus=create_corpus(1)\nstart = 0\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:]:\n    if start == 20:\n        break\n    if (word not in stop) :\n        start += 1\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)\nplt.xlabel('Count')  \nplt.ylabel('words') \nplt.title(\"Words vs Count\")","d53d3a8f":"tweet = pd.concat([train, test], sort=False) #Combining train and test set for cleaning purpose","d11b0f3e":"#Removing URL\ndef remove_URL(text):\n    \"\"\"\n       Replacing url with empty string \n    \"\"\"\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n#Remove Html Tags\ndef remove_html(text):\n\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n#Removing Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n#Removing punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#Removing stop words\ndef remove_stopwords(text):\n    text = text.lower()\n    text_tokens = nltk.tokenize.word_tokenize(text)\n    text_tokens = [t for t in text_tokens if len(t.strip()) > 2] # remove short words, they're probably not useful\n    text_tokens = [wordnet_lemmatizer.lemmatize(t) for t in text_tokens] # put words into base form\n    token = [word for word in text_tokens if not word in stop] # remove stop words\n    token = ' '.join(token)\n    return token","fea3a43f":"tweet['text']=tweet['text'].apply(lambda x : remove_URL(x))\ntweet['text']=tweet['text'].apply(lambda x : remove_html(x))\ntweet['text']=tweet['text'].apply(lambda x : remove_emoji(x))\ntweet['text']=tweet['text'].apply(lambda x : remove_punct(x))\ntweet['text']=tweet['text'].apply(lambda x: remove_stopwords(x))","5f141575":"current_index = 0\nword_index_map = {} # dictionary to store each word and its index","50d8e811":"def wordIndexMap(text, current_index):\n    tokens = nltk.tokenize.word_tokenize(text) # convert string into token\n    for token in tokens:\n        if token not in word_index_map:\n            word_index_map[token] = current_index #Assigning unique index to unique word\n            current_index += 1\n    return current_index","fcd661ac":"text_list = tweet['text'].tolist()\nfor each_text in text_list:\n    current_index = wordIndexMap(each_text, current_index)","4bec1674":"len(word_index_map) # There are total 20625 unique words in our data","3491c200":"def to_tokens(text):\n    \"\"\"\n    Converting each sentence to token\n    \"\"\"\n    text_tokens = nltk.tokenize.word_tokenize(text)\n    return text_tokens","7db00b74":"tweet['text']=tweet['text'].apply(lambda x: to_tokens(x))","000db4e7":"def words_to_vector(tokens, i):\n    \"\"\"\n    1> Creating a vector of lenght equals to word_index_map\n    2> calculating term frequency(count of a word in a particular tweet\/total no of word in a particular tweet ) for each of the words present in a tweet\n    3> x is a vector form of a word\n    \"\"\"\n    x = np.zeros(len(word_index_map))\n    for t in tokens:\n        if t not in word_index_map:\n            continue\n        i = word_index_map[t]\n        x[i] += 1\n    if x.sum() == 0:\n        '''If tweet become NA after removing all the unnecessary words'''\n        return x\n    x = x \/ x.sum()\n    return x","02d69b54":"data = np.zeros((len(tweet), len(word_index_map))) # This data matrix will be used a input for our model\nfor i in range(0, len(tweet)):\n    data[i,:] = words_to_vector(tweet.iloc[i]['text'], i) # getting a numeric vector for each tweet","afbb30a6":"Xtrain = data[:7613,] #Train data for tweets\nXtest = data[7613:,] #Test data for tweets\nYtrain = np.array(tweet.iloc[:7613]['target']) #Train data labels(target values)","4aa44c72":"#Creating a object of logistic regression\nmodel = LogisticRegression()\n#Fitting train data into the model\nmodel.fit(Xtrain, Ytrain)\nprint(\"Train accuracy:\", model.score(Xtrain, Ytrain))","657b2377":"print(\"Test accuracy:\", model.score(Xtest, submission_label.iloc[:]['target']))","0872e1f9":"submission_label['target'] = model.predict(Xtest)","99c81246":"\nsubmission_label['target']=submission_label['target'].apply(lambda x : int(x))","895775ed":"submission_label.to_csv('submission.csv',index=False)","825c1505":"## Accuracy check","a4f48aba":"## Word Index Map","02cc95b6":"## Distribution","27fad1a4":"# Converting words to vector","6c391c25":"## Creating corpus ","08a06161":"# Model Building","d070a020":"# Text Cleaning\n#### This dataset requires a lot of cleaning. Have to remove Punctuations, Urls, Special Characters, stoping words etc.","6e54d1a6":"## Most Common words","061e980a":"### Convert Text to tokens","855c07d4":"### Let's look at common stopwords and their frequencies"}}