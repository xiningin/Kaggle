{"cell_type":{"c1ff77cf":"code","04cec072":"code","4a9ca901":"code","7321e445":"code","332a1f60":"code","343f2328":"code","f8dd246c":"code","f22c30ae":"code","ae438f1c":"code","06d8861c":"code","11157c90":"code","8ffe6f69":"code","f7c7ac36":"code","fd878517":"code","8a51b368":"code","bcbf978f":"code","e284671a":"code","ef20b953":"code","57329dd0":"code","88946ae9":"code","fc3f0606":"code","5d17ec7a":"code","721256a3":"code","1bee3572":"code","d653170e":"code","f3785231":"code","24415ff4":"markdown","305a683f":"markdown","2543ccaf":"markdown","1ec1d960":"markdown","b4e40df7":"markdown","b4536265":"markdown","a19d57df":"markdown","9217261c":"markdown","f337e720":"markdown","6010c0c4":"markdown","e145d97d":"markdown","09bed552":"markdown","5b468e1b":"markdown","8444519e":"markdown","cc273a11":"markdown","669002be":"markdown"},"source":{"c1ff77cf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","04cec072":"def Euclidean_dist(pt1,pt2):\n    distance=0.0\n    for i in range(len(pt1)):\n        distance += (pt1[i]-pt2[i])**2\n    return math.sqrt(distance)","4a9ca901":"def Nearest_neighbors(train,test_obs,n):\n    neighbor_distance= []\n    for i in range(len(train)):\n        l1=list(train.iloc[i,:])+[Euclidean_dist(train.iloc[i,:-1],test_obs)]\n        neighbor_distance= neighbor_distance+[l1]\n    neighbor_distance.sort(key=lambda x: x[-1])\n    nearest_neighbors= [neighbor_distance[i] for i in range(0,n)]\n    y_pred= [i[-2] for i in nearest_neighbors]\n    return(int(max(y_pred,key=y_pred.count)))","7321e445":"def Prediction(train,test_obs,n):\n    \n    NN=Nearest_neighbors(train,test_obs,3)\n    M= [i[n-1] for i in NN]\n    \n    return(test_obs+[max(M)])","332a1f60":"def Normalize(data):\n    df1=[]\n    for i in range(len(data.columns)):\n        z=[]\n        z= [(k-np.mean(df.iloc[:,i]))\/np.std(df.iloc[:,i]) for k in df.iloc[:,i]]\n        df1.append(z)\n    df1=pd.DataFrame(df1)\n    df1=df1.T\n    df1.columns=data.columns\n    return(df1)","343f2328":"def F_score(Act,Pred):\n    ConfusionMatrix= confusion_matrix(Act,Pred)\n    \n    return((2*ConfusionMatrix[1,1])\/(2*ConfusionMatrix[1,1]+ConfusionMatrix[1,0]+ConfusionMatrix[0,1]))","f8dd246c":"def Accuracy(Act,Pred):\n    ConfusionMatrix= confusion_matrix(Act,Pred)\n    #return(ConfusionMatrix)\n    return((ConfusionMatrix[0,0]+ConfusionMatrix[1,1])\/(len(Act)))","f22c30ae":"df= pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","ae438f1c":"df.describe()","06d8861c":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]= df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","11157c90":"df.fillna(df.mean(),inplace=True)","8ffe6f69":"df.describe()","f7c7ac36":"#Distribution plots of predictors\n\ndf.hist(bins=10,figsize=(15,10))","fd878517":"plt.figure(figsize=(15,10))\np=sns.heatmap(df.corr(),annot=True)","8a51b368":"sns.pairplot(data=df,hue='Outcome')","bcbf978f":"X=df.drop(columns='Outcome')\nY=df['Outcome']","e284671a":"X= Normalize(X)","ef20b953":"#A sneakpeak of Normalized data\nX.head()","57329dd0":"X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.3,random_state=5)","88946ae9":"X_train=X_train.join(Y_train)","fc3f0606":"print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape,sep='\\n')","5d17ec7a":"Acc=[]\nfor j in range(1,20):\n    pred=[]\n    for i in range(len(X_test)):\n        pred.append([Nearest_neighbors(X_train,X_test.iloc[i,:],j)])\n    Acc= Acc+([Accuracy(Y_test,pred)])","721256a3":"Acc","1bee3572":"pred=[]\nfor i in range(len(X_test)):\n    pred.append(Nearest_neighbors(X_train,X_test.iloc[i,:],Acc.index(max(Acc))+1))\n    \nX_test['Pred']= pred\nX_test['Outcome']= Y_test","d653170e":"from sklearn.metrics import classification_report\n\nprint(classification_report(X_test['Outcome'], X_test['Pred']))","f3785231":"pd.crosstab(X_test['Outcome'], X_test['Pred'], rownames=['True'], colnames=['Predicted'], margins=True)","24415ff4":"### Code to calculate Euclidean Distance between two points","305a683f":"### Summary from Confusion Matrix:\n* True Negatives: 125\n* False Positives: 35\n* False Negatives: 21\n* True Positives: 50","2543ccaf":"# Importing Packages","1ec1d960":"### Finding the best possible value for 'k' i.e. optimal value for number for neighbors\n\n* The model is developed on train dataset and the accuracy is calculated on the test data. \n* The value of k that gives maximum accuracy is considered the best","b4e40df7":"# Data Exploration\n\n#### Ideally Glucose, Blood Pressure, Skin Thickness, Insulin levels and BMI should be greater than 0. These values being 0 indicate bad data. \n\n#### Imputing 0 values with means","b4536265":"### F-Score","a19d57df":"### Code to return the prediction (0 or 1) for a test observation","9217261c":"### Code to calculate the accuracy of the Model","f337e720":"# K Nearest Neighbor Classification using Euclidean Distance","6010c0c4":"### The model has an accuracy of 76%","e145d97d":"### Splitting the available data into 70% Train and 30% Test data","09bed552":"### The accuracy is maximum at k=17. ","5b468e1b":"# Importing the data.\n\n### Data can be downloaded from <a href=\"https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\" target=\"_blank\">here<\/a>.","8444519e":"### Normalizing the predictors","cc273a11":"#### Looking at the Correlation matrix, we can hypothesize that Glucose levels, BMI and Age are vital in determining whether or not a patient has diabetes ","669002be":"### Code to get 'n' Nearest Neighbors"}}