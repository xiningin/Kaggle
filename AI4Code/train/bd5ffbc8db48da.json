{"cell_type":{"ab29e39f":"code","d25f1759":"code","78f2ec34":"code","feabe0db":"code","31fb4b5c":"code","17a2e5aa":"code","d3f0f8b2":"code","1f7515f6":"code","01dd4189":"code","4aa1c2b3":"code","09e9f176":"code","1aefc9e2":"code","ece8a56e":"code","ba9f214f":"code","87b08276":"code","7a21b2bc":"code","0b433fa2":"code","ce409180":"code","0355be75":"code","acbbb721":"code","613082fb":"code","02b258be":"code","a6baa76c":"code","a30baa50":"code","a7b73d76":"markdown","60e0d2ec":"markdown","f812bc18":"markdown","2eb98f93":"markdown","eed73fa2":"markdown","6da329a0":"markdown","1d692e48":"markdown","ae708bdf":"markdown","0787cc27":"markdown","cd5430ef":"markdown","9d68034e":"markdown","582f200f":"markdown"},"source":{"ab29e39f":"#importing common libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nimport numpy as np\n%matplotlib inline","d25f1759":"#importing dataset from sklearn\nfrom sklearn.datasets import fetch_20newsgroups\n#importing train and test dataset\ntrain= fetch_20newsgroups(subset=\"train\" ,categories =[ \"alt.atheism\", \"sci.space\" ]) \ntest= fetch_20newsgroups(subset=\"test\" ,categories =[ \"alt.atheism\", \"sci.space\" ])\nX_train = train[\"data\"]\nX_test=test['data']\ny_train = train[\"target\"] \ny_test=test['target']","78f2ec34":"#making a dataframe\ndf=pd.DataFrame(X_train,columns=['mess'])","feabe0db":"#adding a target column\ndf['target']=y_train","31fb4b5c":"#making length a feature for visualizations\ndf['length']=df['mess'].apply(len)\ndf.head()","17a2e5aa":"#BarPlot\nsns.barplot(x='target',y='length',data=df)","d3f0f8b2":"g=sns.FacetGrid(df,hue='target',height=4,aspect=2)\ng=g.map(sns.distplot,'length')\nplt.legend()\n#length is not a good feature they seems to be same","1f7515f6":"#importing string for punctuations\nimport string\n#now we import most common words i.e. stopwords\nfrom nltk.corpus import stopwords","01dd4189":"#making a function to process our data\ndef text_process(mess):\n    no_punc=[c for c in mess if c not in string.punctuation]\n    no_punc=''.join(no_punc)\n    cleaned_mess=[word for word in no_punc.split() if word.lower() not in stopwords.words('english')]\n    return cleaned_mess","4aa1c2b3":"#applying our text_process function\n#adding processed data to a new column\ndf['message']=df['mess'].apply(text_process)","09e9f176":"#dropping our previous unprocessed column\ndf.drop('mess',axis=1,inplace=True)\ndf.head()","1aefc9e2":"df['message'].head()","ece8a56e":"#Importing CountVectorizer to a collection of text documents to a matrix of token counts.\nfrom sklearn.feature_extraction.text import CountVectorizer","ba9f214f":"bow_transformer = CountVectorizer(analyzer=text_process).fit(df['message'])\n# Print total number of vocab words\nprint(len(bow_transformer.vocabulary_))","87b08276":"messages_bow = bow_transformer.transform(df['message'])\nprint('Shape of Sparse Matrix: ', messages_bow.shape)\nprint('Amount of Non-Zero occurences: ', messages_bow.nnz)","7a21b2bc":"#Importing TfidfVectorizer to a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer().fit(messages_bow)","0b433fa2":"messages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","ce409180":"#methods we will be using to predict\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","0355be75":"from sklearn.pipeline import Pipeline\npipeline=Pipeline([\n    ('bow',CountVectorizer(analyzer=text_process)),\n    ('tfidf',TfidfTransformer()),\n    ('Naive Bayes ',MultinomialNB())\n])","acbbb721":"#pipeline fit our train and test data\npipeline.fit(X_train,y_train)","613082fb":"#make predictions\npredictions=pipeline.predict(X_test)","02b258be":"#Using Naive Bayes\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\nprint('F1-score: ',metrics.f1_score(y_test,predictions, labels=np.unique(predictions)))\nprint('Accuracy: ',metrics.accuracy_score(y_test,predictions))","a6baa76c":"# #Using Logistic Regression\n# from sklearn.metrics import f1_score\n# from sklearn import metrics\n# print('F1-score: ',metrics.f1_score(y_test,predictions, labels=np.unique(predictions)))\n# print('Accuracy: ',metrics.accuracy_score(y_test,predictions))","a30baa50":"# #Using SVC\n# from sklearn.metrics import f1_score\n# from sklearn import metrics\n# print('F1-score: ',metrics.f1_score(y_test,predictions, labels=np.unique(predictions)))\n# print('Accuracy: ',metrics.accuracy_score(y_test,predictions))","a7b73d76":"## Making Pipeline for Predictions","60e0d2ec":"<p style=\"padding: 10px;\n              background-color:black;\n                  color:white;\">\nF1-score:  0.963144963144963\n    <\/p>\n    <p style=\"padding: 10px;\n              background-color:black;\n                  color:white;\">\nAccuracy:  0.9579242636746143\n    <\/p>","f812bc18":"**Naive Bayes seems to perform best!**","2eb98f93":"## Visualizations","eed73fa2":"We just need to pass the function and also an additionl the method of prediction here **I have shown using Naive Bayes but you can choose any classifier**","6da329a0":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    \ud83d\udccc Thanks for Learning! Hope to see you again!<\/div>","1d692e48":"## Normalization & Vecorization","ae708bdf":"![Stay Home](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1740\/3025\/a3b95419dcdc1ad06dbff6f54db18511\/dataset-cover.jpg)","0787cc27":"## Text Pre-processing","cd5430ef":"I have predicted using 3 methods but here I have shown using Naive Bayes just change MultinomialNB() to LogisticRegression() if you wanna predict using Logistic Regression","9d68034e":"### This is the main part sklearn pipeline will automatically do all the stuff we did in Normalization & Vectorization","582f200f":"<p style=\"padding: 10px;\n              background-color:black;\n                  color:white;\">\nF1-score:  0.953883495145631\n    <\/p>\n    <p style=\"padding: 10px;\n              background-color:black;\n                  color:white;\">\nAccuracy:  0.9467040673211781\n    <\/p>"}}