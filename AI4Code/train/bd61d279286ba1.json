{"cell_type":{"23c5a9b0":"code","26e687c3":"code","0d7f6a27":"code","e88aa9cc":"code","941b671b":"code","437cd7a8":"code","1f159c58":"code","f8d5204a":"code","05bcfd44":"code","b6cf815f":"code","a891533b":"code","30799dfe":"code","9a7e4b6d":"code","c20a6eea":"code","e2f5eb59":"code","ca0aaadc":"code","abb79df3":"code","fda56c62":"code","9a2ff391":"code","ccd85f5c":"code","92640612":"code","6977dc2f":"code","3516a0a6":"code","f23b74a6":"code","6bd1977c":"code","7c09c33c":"code","61548048":"code","22dad9b6":"code","0229085b":"code","59dba3b5":"code","b27e6edb":"code","c05f2e1e":"code","f4595a39":"code","8faca381":"code","bee2e0bc":"code","c6b8789f":"code","1e15aae9":"code","b0f24e77":"markdown","5e48d991":"markdown","0084d33c":"markdown","dfe979f6":"markdown","43877684":"markdown","59a86517":"markdown","2681cdc0":"markdown","2ada196d":"markdown","f59184dd":"markdown","4ef92ade":"markdown","878233ef":"markdown","f87fb1e1":"markdown","0dc555ab":"markdown","6e1fb8af":"markdown","18637386":"markdown","fef19e6e":"markdown","31491349":"markdown","87e88c2d":"markdown","78a401d5":"markdown","cd60cf3f":"markdown","774c4efd":"markdown","cc35e1fa":"markdown","1a70ee3b":"markdown","498d572e":"markdown","d0dcf488":"markdown","7535e70f":"markdown","0d92f5bd":"markdown","e5dccd1f":"markdown","3b32278c":"markdown","51dcff3f":"markdown","4ef3208d":"markdown","93252a9d":"markdown","eabf270e":"markdown","818e333b":"markdown","cfef61c6":"markdown"},"source":{"23c5a9b0":"# Import the packages and libraries needed for this project\nimport matplotlib as mpl, matplotlib.pyplot as plt, \\\npandas as pd, seaborn as sns, xgboost as xgb, sklearn as sk\nfrom sklearn.metrics import accuracy_score, \\\nconfusion_matrix, multilabel_confusion_matrix\nfrom sklearn.model_selection import train_test_split","26e687c3":"# Checking for package version\nprint(\"Matplotlib Version\", mpl.__version__)\nprint(\"Pandas Version\", pd.__version__)\nprint(\"Xgboost Version\", xgb.__version__)\nprint(\"Seaborn Version\", sns.__version__)\nprint(\"Sci-kit learn Version\", sk.__version__)","0d7f6a27":"# Import the three datasets\ndf_data_1 = pd.read_csv('\/kaggle\/input\/hr-analytics\/HR_comma_sep.csv')\ndf_data_2 = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf_data_3 = pd.read_csv('\/kaggle\/input\/employee-turnover\/turnover.csv', encoding = 'ISO-8859-1')\ndf_model_1 = df_data_1.copy()\ndf_model_2 = df_data_2.copy()\ndf_model_3 = df_data_3.copy()","e88aa9cc":"# Explore dataset 1\ndf_data_1.head(5)","941b671b":"# Explore dataset 2\ndf_data_2.head(5)","437cd7a8":"# Explore dataset 3\ndf_data_3.head(5)","1f159c58":"# Check for null.\ndisplay(df_data_1.isnull().values.any())\ndisplay(df_data_2.isnull().values.any())\ndisplay(df_data_3.isnull().values.any())","f8d5204a":"# Drop unwanted features (i.e. columns) that will be dropped.\ndrop_2 = ['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18']\ndf_model_2 = df_model_2.drop(drop_2,axis=1)\n\nprint('Deleting unwanted features...')\nprint(drop_2)\nprint('\u2714\ufe0fDeleted')","05bcfd44":"# Rename all target features.\ndf_model_1 = df_model_1.rename(columns={'left': 'churn'})\ndf_model_2 = df_model_2.rename(columns={'Attrition': 'churn'})\ndf_model_3 = df_model_3.rename(columns={'event': 'churn'})\nprint(\"\u2714\ufe0f Rename target column names to 'churn'\")","b6cf815f":"# Rename all treatment features.\ndf_model_1 = df_model_1.rename(columns={'promotion_last_5years': 'treatment'})\ndf_model_2 = df_model_2.rename(columns={'OverTime': 'treatment'})\ndf_model_3 = df_model_3.rename(columns={'coach': 'treatment'})\nprint(\"\u2714\ufe0f Rename treatment column names to 'treatment'\")","a891533b":"print(\"Explore the unique data in the 'selected features' (for the label encoding):\")\nstring1, string2, string3 = ['salary'], ['churn',\n                                         'treatment',\n                                         'BusinessTravel'], ['treatment']\n\nprint('\\nDataset 1:')\nfor col in string1:\n    print(col, df_model_1[col].unique())\n\nprint('\\nDataset 2:')\nfor col in string2:\n    print(col, df_model_2[col].unique())\n    \nprint('\\nDataset 3:')\nfor col in string3:\n    print(col, df_model_3[col].unique())","30799dfe":"# Manually label encode the target (i.e. dependent variable) for dataset 1\ndf_model_1.salary = df_model_1.salary.map({'low': 0, 'medium': 1, 'high':2})\nprint(\"\u2714\ufe0f Label encoding the 'salary' feature in dataset 1\")","9a7e4b6d":"# Manually label encode the target and identified treatment (i.e. employee retention program) for dataset 2\ndf_model_2.churn = df_model_2.churn.map({'Yes': 1, 'No': 0})\ndf_model_2.treatment = df_model_2.treatment.map({'Yes': 0, 'No': 1})\n# Declaration BusinessTravel\ndf_model_2.BusinessTravel = df_model_2.BusinessTravel.map({'Non-Travel': 0,\n                                                           'Travel_Rarely': 1,\n                                                           'Travel_Frequently':2})\nprint(\"\u2714\ufe0f Label encoding the 'churn, treatment, & BusinessTravel' features in dataset 2\")","c20a6eea":"# Manually label encode the target and treatment for dataset 3\ndf_model_3.treatment = df_model_3.treatment.map({'yes': 0, 'no': 1, 'my head':2})\ndf_model_3 = df_model_3.loc[df_model_3.treatment <=1].reset_index(drop=True)\nprint(\"\u2714\ufe0f Label encoding the 'treatment' feature in dataset 3\")","e2f5eb59":"# One-Hot Encoding:\ndf_model_1, df_model_inverse_1 = pd.get_dummies(df_model_1), pd.get_dummies(df_model_1)\ndf_model_2, df_model_inverse_2 = pd.get_dummies(df_model_2), pd.get_dummies(df_model_2)\ndf_model_3, df_model_inverse_3 = pd.get_dummies(df_model_3), pd.get_dummies(df_model_3)\nprint(\"\u2714\ufe0f One-hot encoding\")","ca0aaadc":"def correlation_treatment(df:pd.DataFrame):\n    \"\"\"Function to calculate the treatment's correlation\n    \"\"\"\n    correlation = df[['treatment','churn']].corr(method ='pearson') \n    return(pd.DataFrame(round(correlation.loc['churn'] * 100,2)))","abb79df3":"print(\"\u2714\ufe0fTreatment correlation in dataset 1:\", correlation_treatment(df_model_1).iloc[0,0])\nprint(\"\\n\u2714\ufe0fTreatment correlation in dataset 2:\", correlation_treatment(df_model_2).iloc[0,0])\nprint(\"\\n\u2714\ufe0fTreatment correlation in dataset 3:\", correlation_treatment(df_model_3).iloc[0,0])","fda56c62":"def declare_target_class(df:pd.DataFrame):\n    \"\"\"Function for declare the target class\n    \"\"\"\n    #CN:\n    df['target_class'] = 0 \n    #CR:\n    df.loc[(df.treatment == 0) & (df.churn == 0),'target_class'] = 1 \n    #TN:\n    df.loc[(df.treatment == 1) & (df.churn == 1),'target_class'] = 2 \n    #TR:\n    df.loc[(df.treatment == 1) & (df.churn == 0),'target_class'] = 3 \n    return df","9a2ff391":"# Add the four target classes\ndf_model_1, df_model_2, df_model_3 = declare_target_class(df_model_1), \\\ndeclare_target_class(df_model_2), declare_target_class(df_model_3)","ccd85f5c":"def split_data(df_model:pd.DataFrame):\n    \"\"\"Split data into training data and testing data\n    \"\"\"\n    X = df_model.drop(['churn','target_class'],axis=1)\n    y = df_model.churn\n    z = df_model.target_class\n    X_train, X_test, \\\n    y_train, y_test, \\\n    z_train, z_test = train_test_split(X,\n                                       y,\n                                       z,\n                                       test_size=0.3,\n                                       random_state=42,\n                                       stratify=df_model['treatment'])\n    return X_train,X_test, y_train, y_test, z_train, z_test\n\n\ndef machine_learning(X_train:pd.DataFrame,\n                     X_test:pd.DataFrame,\n                     y_train:pd.DataFrame,\n                     y_test:pd.DataFrame,\n                     z_train:pd.DataFrame,\n                     z_test:pd.DataFrame):\n    \"\"\"Machine learning process consists of \n    data training, and data testing process (i.e. prediction) with XGBoost (XGB) Algorithm\n    \"\"\"\n    # prepare a new DataFrame\n    prediction_results = pd.DataFrame(X_test).copy()\n    \n    \n    # train the ETP model\n    model_tp \\\n    = xgb.XGBClassifier().fit(X_train.drop('treatment', axis=1), y_train)  \n    # prediction Process for ETP model \n    prediction_tp \\\n    = model_tp.predict(X_test.drop('treatment',axis=1))\n    probability__tp \\\n    = model_tp.predict_proba(X_test.drop('treatment', axis=1))\n    prediction_results['prediction_churn'] = prediction_tp\n    prediction_results['proba_churn'] = probability__tp[:,1]\n    \n    \n    # train the ETU model\n    model_etu \\\n    = xgb.XGBClassifier().fit(X_train.drop('treatment', axis=1), z_train)\n    # prediction Process for ETU model \n    prediction_etu \\\n    = model_etu.predict(X_test.drop('treatment', axis=1))\n    probability__etu \\\n    = model_etu.predict_proba(X_test.drop('treatment', axis=1))\n    prediction_results['prediction_target_class'] = prediction_etu\n    prediction_results['proba_CN'] = probability__etu[:,0] \n    prediction_results['proba_CR'] = probability__etu[:,1] \n    prediction_results['proba_TN'] = probability__etu[:,2] \n    prediction_results['proba_TR'] = probability__etu[:,3]\n    prediction_results['score_etu'] = prediction_results.eval('\\\n    proba_CN\/(proba_CN+proba_CR) \\\n    + proba_TR\/(proba_TN+proba_TR) \\\n    - proba_TN\/(proba_TN+proba_TR) \\\n    - proba_CR\/(proba_CN+proba_CR)')  \n    \n    # add the churn and target class into dataframe as validation data\n    prediction_results['churn'] = y_test\n    prediction_results['target_class'] = z_test\n    return prediction_results\n\n\ndef predict(df_model:pd.DataFrame):\n    \"\"\"Combining data split and machine learning process with XGB\n    \"\"\"\n    X_train, X_test, y_train, y_test, z_train, z_test = split_data(df_model)\n    prediction_results = machine_learning(X_train,\n                                          X_test,\n                                          y_train,\n                                          y_test,\n                                          z_train,\n                                          z_test)\n    print(\"\u2714\ufe0fPrediction succeeded\")\n    return prediction_results","92640612":"# Machine Learning Modelling Process\nprint(\"Predicting dataset 1 ...\")\nprediction_results_1 = predict(df_model_1)\nprint(\"\\nPredicting dataset 2 ...\")\nprediction_results_2 = predict(df_model_2)\nprint(\"\\nPredicting dataset 3 ...\")\nprediction_results_3 = predict(df_model_3)","6977dc2f":"# Uncomment this line below to save the prediction result into an Excel file.\n# prediction_results_3.to_excel(\"output_prediction3.xlsx\")  ","3516a0a6":"def cm_evaluation(df:pd.DataFrame):\n    \"\"\"Confusion matrix evaluation\n    \"\"\"  \n    print(\"===================================\")\n    print(\"1. ETP's confusion matrix result:\")\n    confusion_etp = confusion_matrix(df['churn'], df['prediction_churn'])\n    df_confusion_etp = pd.DataFrame(confusion_etp, columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n    print(df_confusion_etp)\n    \n    print(\"-----------------------------------\")\n    \n    print(\"2. ETU's confusion matrix result:\")   \n    confusion_etu = multilabel_confusion_matrix(df['target_class'], df['prediction_target_class'])\n    print(\"a. CN's confusion matrix:\")  \n    df_cn = pd.DataFrame(confusion_etu[0], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n    print(df_cn)\n    print(\"b. CR's confusion matrix:\") \n    df_cr = pd.DataFrame(confusion_etu[1], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n    print(df_cr) \n    print(\"c. TN's confusion matrix:\")\n    df_tn = pd.DataFrame(confusion_etu[2], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n    print(df_tn) \n    print(\"d. TR's confusion matrix:\") \n    df_tr = pd.DataFrame(confusion_etu[3], columns = ['Predicted True','Predicted False'], index = ['Actual True','Actual False'])\n    print(df_tr)\n    \n    print(\"===================================\")","f23b74a6":"# Confusion Matrix Evaluation\nprint(\"\u2714\ufe0fDataset 1\")\ncm_evaluation(prediction_results_1)\nprint(\"\\n\\n\u2714\ufe0fDataset 2\")\ncm_evaluation(prediction_results_2)\nprint(\"\\n\\n\u2714\ufe0fDataset 3\")\ncm_evaluation(prediction_results_3)","6bd1977c":"def accuracy_evaluation(df:pd.DataFrame):\n    \"\"\"Accuracy evaluation\n    \"\"\"\n    akurasi_cp = accuracy_score(df['churn'],\n                                df['prediction_churn'])\n    print('\u2714\ufe0fETP model accuracy: %.2f%%' % (akurasi_cp * 100.0))\n    \n    \n    akurasi_uplift = accuracy_score(df['target_class'],\n                                    df['prediction_target_class'])\n    print('\u2714\ufe0fETU model accuracy: %.2f%%' % (akurasi_uplift * 100.0))","7c09c33c":"# Accuracy Evaluation Process.\nprint(\"Dataset 1\")\naccuracy_evaluation(prediction_results_1)\nprint(\"\\nDataset 2\")\naccuracy_evaluation(prediction_results_2)\nprint(\"\\nDataset 3\")\naccuracy_evaluation(prediction_results_3)","61548048":"def sorting_data(df:pd.DataFrame):\n    \"\"\"Function to sort data\n    \"\"\"\n    # Set up new DataFrames for ETP model and ETU model\n    df_c = pd.DataFrame({'n':[], 'target_class':[]})\n    df_u = df_c.copy()\n    df_c['target_class'] = df['target_class']\n    df_u['target_class'] = df['target_class']\n    \n    \n    # Add quantiles\n    df_c['n'] = df.proba_churn.rank(pct=True, ascending=False)\n    df_u['n'] = df.score_etu.rank(pct=True, ascending=False)\n    df_c['score'] = df['proba_churn']\n    df_u['score'] = df['score_etu']\n    \n    \n    # Ranking the data by deciles\n    df_c = df_c.sort_values(by='n').reset_index(drop=True)\n    df_u = df_u.sort_values(by='n').reset_index(drop=True)\n    df_c['model'], df_u['model'] = 'CP', 'Uplift'\n    return df_c, df_u\n\n\ndef calculating_qini(df:pd.DataFrame):\n    \"\"\"Function to measure the Qini value\n    \"\"\"\n    # Calculate the C, T, CR, and TR\n    C, T = sum(df['target_class'] <= 1), sum(df['target_class'] >= 2)\n    df['cr'] = 0\n    df['tr'] = 0\n    df.loc[df.target_class  == 1,'cr'] = 1\n    df.loc[df.target_class  == 3,'tr'] = 1\n    df['cr\/c'] = df.cr.cumsum() \/ C\n    df['tr\/t'] = df.tr.cumsum() \/ T\n    \n\n    # Calculate & add the qini value into the Dataframe\n    df['uplift'] = df['tr\/t'] - df['cr\/c']\n    df['random'] = df['n'] * df['uplift'].iloc[-1]\n    qini_coef= df['uplift'].sum(skipna = True) - df['random'].sum(skipna = True)\n    \n    # Print the Qini coefficient\n    print('\u2714\ufe0fQini coefficient = {} {}'.format(round(qini_coef, 2), '%'))\n    \n    # Add q0 into the Dataframe\n    q0 = pd.DataFrame({'n':0, 'uplift':0, 'target_class': None}, index =[0])\n    qini = pd.concat([q0, df]).reset_index(drop = True)\n    return qini\n\n\ndef merging_data(df_c:pd.DataFrame, df_u:pd.DataFrame):\n    \"\"\"Function to add the 'Model' column and merge the dataframe into one\n    \"\"\"\n    df_u['model'] = 'ETU'\n    df_c['model'] = 'ETP'\n    df = pd.concat([df_u, df_c]).sort_values(by='n').reset_index(drop = True)\n    return df\n\n\ndef plot_qini(df:pd.DataFrame):\n    \"\"\"Function to plot the qini curve\n    \"\"\"\n    print('\\nPlotting the qini curve...')\n    \n    # Define the data that will be plotted\n    order = ['ETU','ETP']\n    ax = sns.lineplot(x='n', y=df.uplift, hue='model', data=df,\n                      style='model', palette=['red','deepskyblue'],\n                      style_order=order, hue_order = order)\n    \n    \n    # Additional plot display settings\n    handles, labels = ax.get_legend_handles_labels()\n    plt.xlabel('Proportion targeted',fontsize=30)\n    plt.ylabel('Uplift',fontsize=30)\n    plt.subplots_adjust(right=1)\n    plt.subplots_adjust(top=1)\n    plt.legend(fontsize=30)\n    ax.tick_params(labelsize=24)\n    ax.legend(handles=handles[1:], labels=labels[1:])\n    ax.plot([0,1], [0,df.loc[len(df) - 1,'uplift']],'--', color='grey')\n    print('\u2714\ufe0fSuccessfully plot the qini curve')\n    return ax\n\n\ndef evaluation_qini(prediction_results:pd.DataFrame):\n    \"\"\"Function to combine all qini evaluation processes\n    \"\"\"\n    df_c, df_u = sorting_data(prediction_results)\n    print('ETP model (previous model):')\n    qini_c = calculating_qini(df_c)\n    print('\\nETU model (our proposed model):')\n    qini_u = calculating_qini(df_u)\n    qini = merging_data(qini_c, qini_u)\n    ax = plot_qini(qini)\n    return ax, qini","22dad9b6":"# Qini evaluation results for DataSet 1 with negative treatment correlation\nax, qini_1 = evaluation_qini(prediction_results_1)\nplt.title('Qini Curve - Dataset 1',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_1_n.pdf', bbox_inches='tight')","0229085b":"# Qini evaluation results for DataSet 2 with negative treatment correlation\nax, qini_2 = evaluation_qini(prediction_results_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_2_n.pdf', bbox_inches='tight'","59dba3b5":"# Qini evaluation results for DataSet 3 with negative treatment correlation\nax, qini_3 = evaluation_qini(prediction_results_3)\nplt.title('Qini Curve - Dataset 3',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_3_n.pdf', bbox_inches='tight')","b27e6edb":"# The process to inverse treatment's parameter\n# Thus also inverse the treatment's correlation from negative to positive\ndf_model_inverse_1.treatment = df_model_inverse_1.treatment.replace({0: 1, 1: 0})\ndf_model_inverse_2.treatment = df_model_inverse_2.treatment.replace({0: 1, 1: 0})\ndf_model_inverse_3.treatment = df_model_inverse_3.treatment.replace({0: 1, 1: 0})","c05f2e1e":"# Recalculate the treatment correlation\nprint(\"\u2714\ufe0fTreatment correlation in dataset 1 (inverted):\", correlation_treatment(df_model_inverse_1).iloc[0,0])\nprint(\"\\n\u2714\ufe0fTreatment correlation in dataset 2 (inverted):\", correlation_treatment(df_model_inverse_2).iloc[0,0])\nprint(\"\\n\u2714\ufe0fTreatment correlation in dataset 3 (inverted):\", correlation_treatment(df_model_inverse_3).iloc[0,0])","f4595a39":"# Add the target class feature to all three datasets\ndf_model_inverse_1, df_model_inverse_2, df_model_inverse_3 = declare_target_class(df_model_inverse_1), \\\ndeclare_target_class(df_model_inverse_2), declare_target_class(df_model_inverse_3)","8faca381":"# Do the prediction process once more time\nprediction_results_inverse_1 = predict(df_model_inverse_1)\nprediction_results_inverse_2 = predict(df_model_inverse_2)\nprediction_results_inverse_3 = predict(df_model_inverse_3)","bee2e0bc":"# Qini evaluation results for DataSet 1 with positive treatment correlation\nax, qini_inverse_1 = evaluation_qini(prediction_results_inverse_1)\nplt.title('Qini Curve - Dataset 1',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_1_p.pdf', bbox_inches='tight')","c6b8789f":"# qini evaluation results for DataSet 2 with positive treatment correlation\nax, qini_inverse_2 = evaluation_qini(prediction_results_inverse_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_2_p.pdf', bbox_inches='tight')","1e15aae9":"# Qini evaluation results for DataSet 3 with positive treatment correlation\nax, qini_inverse_3 = evaluation_qini(prediction_results_inverse_3)\nplt.title('Qini Curve - Dataset 3',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_3_p.pdf', bbox_inches='tight')","b0f24e77":"The Qini curve above shows that the ETU has a better uplift value than the random model. The random model is a straight line from 0 to 100% proportion targeted, this may  indicate the value of the uplift without using any model. By contrast, the ETP model failed to shows a good result above the random mode, therefore we could say that in this case ETP model failed to target the right employees.\n\nMoreover, the Qini coefficient is just the Gini coefficient version of Qini to compare A Model's Qini curve with the random model (i.e. without model). A positive Qini coefficient indicates that the model has succeeded to give a significant benefit.","5e48d991":"# 6. Evaluating prescriptive performance","0084d33c":"# 2. Data Exploration","dfe979f6":"# II. Employee Turnover Uplift (ETU):\nEmployee Turnover Uplift is simply a model to predict and solve employee turnover by using the [Uplift Modeling](https:\/\/en.wikipedia.org\/wiki\/Uplift_modelling) technique.\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F507628%2F1467607%2FUplift%20framework-15.jpg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1599849266&Signature=mXR4h60oO9jOW7FhrFYT%2FovpRjRJtNRiAGpyJftCX5UA%2Fcw972L4JTxSzqElFliGc5HH8%2BqzRBmKLDKWxUlvLkQVPnhS4OQbI5Yjt6xaO233qdDagKAbao%2BDtlMo%2FmNiIqN1rhBWmlYnOzXQHsB83kLNPA0yy4eYy5baGd0t3jOzLoflNc341XQo3dZWLebkJiYb9IUQAO3qT8Q0nhh%2FIInm6FGDzZoJdw6EJHjRPd2nhglJH3E4KAoNQNBvl4KtYxNhI2P8%2BVIB%2Bey7qX4VDGNPvVFaTVAcTxLodgeS0gYod%2Bi86H3M14GT2uO38NFnHA%2BhfevQ49WeZPQZdo9%2Bmw%3D%3D)","43877684":"Now we've the datasets ready, let's check for null data.","59a86517":"# 5. Evaluating predictive performance","2681cdc0":"Now let's use the prediction results to solve the problem. As explained before, for ETP model employees are ranked by their turnover probability. Employees with the highest turnover probability will be targeted with a retention campaign (the treatment features declared before). On the other side, the ETU models are ranked by its uplift score with LGWUM's formulation.","2ada196d":"The Qini curve above shows that ETP is failed.","f59184dd":"# 4. Machine Learning Modeling","4ef92ade":"The Qini curve above shows that ETP and ETU are succeeded.","878233ef":"Now, let's calculate the accuracy result:","f87fb1e1":"Now let's evaluate the predictive performance:","0dc555ab":"# I. Intro:\n- \ud83d\udca1You can also make your own experiment\/research with this project, simply search for **'XGBClassifier()'**, and modify it with other algorithms available out there, or take a look at the **'future research'** section in the end of this project.\n- \ud83d\udc49This project is an experiment for this research http:\/\/ijair.id\/index.php\/ijair\/article\/view\/169\/pdf\n- \ud83d\udc49Published in the International Journal of Artificial Inteligence.\n- \ud83d\udc49Ranked 2 in Science & Technology Index Indonesia.\n- \ud83d\udc49In a nutshell, this project compares the [predictive & prescriptive](https:\/\/insights.principa.co.za\/4-types-of-data-analytics-descriptive-diagnostic-predictive-prescriptive) capabilities of a mainstream [Employee Turnover Prediction (ETP)](https:\/\/www.researchgate.net\/publication\/328796091_Employee_Turnover_Prediction_with_Machine_Learning_A_Reliable_Approach) VS Employee Uplift Modeling (ETU) with the XGBoost algorithm.\n\n\n## Abstract:\n\n**i. \u2753Background:** Employee turnover is the loss of talent in the workforce that can be costly for a company. Uplift modeling is one of several prescriptive methods available in machine learning models that not only predict an outcome but also prescribe a solution. Recent studies are focusing on the conventional predictive models (binary classification) to predict employee turnover rather than uplift modeling.\n\n**ii. \ud83c\udfafAim:** In this research, we analyze whether the Uplift Modeling has better performance than the conventional predictive model in solving employee turnover.\n\n**iii. \u2699\ufe0fMethodology:** Performance comparison between the two methods was carried out by experimentation using two synthetic datasets and one real dataset. XGBoost is used as the machine learning algorithm for both models in this experiment.\n\n**iv. \u2714\ufe0fResults:** After observing the 6 Qini curves, the results show that despite the ETP model yields an average prediction accuracy of 84%; it only yields a success rate of 50% to target the right employee with a retention program on the three datasets. By contrast, the uplift model only yields an average accuracy of 67% but yields a consistent success rate of 100% in targeting the right employee with a retention program.\n\n## Step by step\n1. \ud83d\udccaThree datasets are used in this project: [Dataset-1](https:\/\/www.kaggle.com\/giripujar\/hr-analytics), [Dataset-2](https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset), and [Dataset-3](https:\/\/www.kaggle.com\/davinwijaya\/employee-turnover)\n2. \ud83c\udfb0[XGBoost](https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/) is used in this project as the machine learning algorithm for both models (ETP & ETU)\n3. \ud83e\udd16ETP Model just predict [employee's turnover](https:\/\/smallbusiness.chron.com\/employee-turnover-definitions-calculations-11611.html#:~:text=Employee%20turnover%20refers%20to%20the,to%2Dhire%20for%20budget%20purposes.) (turnover or stay)\n4. \ud83e\udd16ETU model predicts  employee's [uplift category](https:\/\/www.predictiveanalyticsworld.com\/machinelearningtimes\/uplift-modeling-making-predictive-models-actionable\/8578\/) (Sure things, persuadables, lost causes, or sleeping dogs\/do-not-disturbs)\n5. \ud83d\udd0ePredictive performances of both models are evaluated with the [Accuracy package from Sci-kit Learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html)\n6. \ud83c\udfc3ETP Model gives prescription by ranking the employee based on its risk of quitting (predicted probability)\n7. \ud83d\udcc8ETU Model gives prescription by ranking the employee based on its uplift score (in this case I use [LGWUM](https:\/\/www.worldscientific.com\/doi\/abs\/10.1142\/S0219622019500172)'s formulation)\n8. \ud83d\udcc9Prescriptive performances of both models are evaluated with [Qini Curve](https:\/\/pdfs.semanticscholar.org\/147b\/32f3d56566c8654a9999c5477dded233328e.pdf?_ga=2.266187788.2072205534.1599377562-943609542.1599377562)\n\n## Acknowledgement\n\ud83d\ude4f I also thank Giri Pujar, Pavan Subhash, and Eduard Babushkin for sharing the datasets used in this project.\nMoreover, this project is made by [me](linkedin.com\/in\/wijayadavin\/) under [CC BY-NC-SA 4.0 common license](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/). Feel free to use or modify it for your personal use.\n\n\ud83e\udd13 If you write an article, please cite with the citation file from [this page](https:\/\/ijair.id\/index.php\/ijair\/rt\/captureCite\/169\/0) with your [Mendeley](https:\/\/www.mendeley.com\/guides\/desktop\/02-adding-documents) or as shown below (for IEEE style):\n\n#### [1] D. Wijaya, J. H. DS, S. Barus, B. Pasaribu, L. I. Sirbu, and A. Dharma, \u201cUplift modeling VS conventional predictive model: A reliable machine learning model to solve employee turnover,\u201d Int. J. Artif. Intell. Res. Vol 5, No 1 Artic. Press, 2021, doi: 10.29099\/ijair.v4i2.169.\n","6e1fb8af":"Good, now the treatment features are positively correlated with employee turnover. This means, if we target the employees with this treatment, it's more likely that the employee turnover rate will be increased. So it'll be wise to use this treatment carefully. Okay, now let's repeat the prediction procedure once again:","18637386":"![](https:\/\/i.ibb.co\/XJp4Kwh\/Uplift-framework-15.jpg)","fef19e6e":"![](https:\/\/i.ibb.co\/4PFKrfw\/cover-03.jpg)","31491349":"# III. Experimental Analysis\nThe workflow of this project will follow the machine-learning-pipeline below:","87e88c2d":"In the Qini curve above, the ETP model even gives a worse result than not using any model (random model).\n\nSo far the success result are: ETU = 3 and ETP = 1.\n\nFurthermore, now we can use the df_model_inverse dataframes we prepared before, we will inverse the value (from 0 to 1, and 1 to 0):","78a401d5":"# Conclusion\nEmployees with a high [turnover](https:\/\/en.wikipedia.org\/wiki\/Turnover_employment) risk do not necessarily need to be targeted with a retention program. The ETP model tends to target the wrong type of employees (e.g. Do-not-disturbs employees) that will leave the company if targeted and could be [costly](https:\/\/builtin.com\/recruiting\/cost-of-turnover) for the company\/employeers. On the other side, the ETU model tends to target the right employees (Persuadables) and prevent unnecessary costs. The Qini curves show that the ETU model yields better uplift value than the ETP model in overall. In other words, the ETU model is more effective as a prescriptive analytics model than the ETP model.\n\nIn other words, despite the low prediction accuracy, the Uplift Modeling yields better results than the conventional Churn Prediction Model.\n\n# Future Research\nFuture research is needed to validate this research with:\n- other machine learning algorithms (e.g. Logistic Regression, Neural Network, ADA Boost, etc.),\n- other uplift strategies (e.g. Two-model Uplift, LWUM, Pessimistic Uplift, etc.),\n- other datasets (Student Performance, Customer, Promotion, etc.), and evaluation methods (e.g. Uplift values instead of Qini Curve).","cd60cf3f":"Secondly, let's turn the rest of the string\/object data into integer with the magical get_dummies function (One hot encoding) from Pandas package, so we can feed the data into XGBoost. Moreover, I add another dataframe df_model_inverse that will be useful for later:","774c4efd":"Let's check the treatment's correlation to employee turnover:","cc35e1fa":"Finally we're ready to start the machine learning process:","1a70ee3b":"Prediction results are stored in prediction_results_1, prediction_results_2, and prediction_results_3 for dataset 1, dataset 2, and dataset 3, respectively.","498d572e":"![](https:\/\/i.ibb.co\/mFS1V74\/pipeline-english-Artboard-53-01.jpg)","d0dcf488":"# Which model is successful or failed?\n\n### \ud83d\udd22Quantitative measurement:\nNow let's calculate the Qini coefficient to measure which model is successful and failed.The higher the coefficient is, indicating a better benefit (in this case, the benefit is preventing employee turnover).But in this project we won't find which model is better or not, we just want to know which one is more reliable (i.e. consistently returning a good result). A successful model has a Qini coefficient above 0 (Give a benefit).\n\n### \ud83d\udcc8Qualitative measurement:\nAnother method to measure the performance qualitatively is by measuring the Qini curve (the red or blue line) in comparison with the random curve (the grey line). The higher the uplift value is and the smaller the proportion is, indicating a good performance. But again, in this project we just want to know which model yields a good result consistently.","7535e70f":"Four target classes are generated by fitting the treatment status and employee turnover status as visualized in the left part of the image above:\n1. Control Non\u2013responders (CN): Employees who have not been treated with the retention program and left. We want to find the persuadables in this group.\n2. Control Responders (CR): Employees who have not been treated with the retention program and stay. We want to avoid targeting this group because we do not need to treat them to make them stay, and there is a possibility that some of them are Do-Not-Disturbs.\n3. Treated Non\u2013Responders (TN): Employees who have been treated with the retention program but left. We want to avoid treating this group because they will leave if treated, and there is a possibility that some of them are Do-Not-Disturbs that will stay if left untreated.\n4. Treated Responders (TR): Employees who have been treated with the retention program and stayed. We also want to find the persuadables in this group.\n\nTherefore, by combining CN with CR and TN with TR will result in C, and T, respectively. Thus by fitting the treated and control group from those four target classes as visualized in the right table will yield four possible theoretical uplift classes (also known as the four quadrants):\n\n1. Do-Not-Disturb (CRTN): Sometimes referred to as sleeping dogs, employees who will be driven away if treated.\n2. Lost Causes (CNTN): Employees who will leave whether treated or not.\n3. Sure Things (CRTR): Employees who will stay whether treated or not.\n4. Persuadables (CNTR): Employees who are willing to leave but will stay if treated. We want to target this group to reduce overall employee turnover in the company.\n\nAfter predicting the four target classes which are visualized with XGBoost, four probability results are generated. Where P is the probability result, thus uplift score is calculated with LGWUM (Lai's Generalized Weighted Uplift Method) as: \n> ## Uplift Score = P(CN\/C)+ P(TR\/T) \u2013 P(CR\/C) \u2013 P(TN\/T)","0d92f5bd":"# 1. Setup\nFirst let's set up the environment and datasets","e5dccd1f":"Good, there is no null data.","3b32278c":"The last Qini curve above shows that both Employee Turnover Uplift (ETU) and Employee Turnover Prediction (ETP) are succeeded.","51dcff3f":"In [Confusion Matrix](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62), the True Positive and False Negative are the amount of successful predictions and the True Negative and False Positive are the amount of failed predictions. Therefore, let's generate the confusion matrices:","4ef3208d":"# 3. Data preprocessing","93252a9d":"Good, now all of the treatment features are negatively correlated. For instance, when the treatment in dataset 1 (promotion last 5 years) is true, the employee churn tend to be reduced.\n\nWe will use the positive ones later at the end of this project. For now, let's add the four uplift category for each datasets:","eabf270e":"Well, now the ETP model is catching up and give a good result.","818e333b":"Wow, seems like ETP models are much better than ETU models in terms of prediction accuracy. That makes sense anyway, because ETP models only predict two possible outcomes (The employee is turnover or stay), where ETU models predict four possible outcomes (Persuadables, Sure Things, Lost Causes, and Sleeping Dogs\/Do-not-disturbs). But will ETP will also have a better performance in solving the employee turnover (prescriptive performance)? Let's find out.","cfef61c6":"## Related works\n* A simple version of Uplift Modeling with only 1 database: https:\/\/www.kaggle.com\/davinwijaya\/uplift-modeling-qini-curve-with-python"}}