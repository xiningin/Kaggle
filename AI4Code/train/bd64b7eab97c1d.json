{"cell_type":{"b954b81b":"code","9500feb1":"code","f4a7baf2":"code","2568766a":"code","65581e2c":"code","608046d4":"code","90385784":"code","cd6f36a9":"code","5a86d5c6":"code","e20fe808":"code","760f66ab":"code","23a00fcb":"code","7ae2135e":"code","8dad9c8e":"code","34b8346d":"code","c8233ed3":"code","4d3727bd":"code","911f12a7":"code","29b2302a":"code","10fdb8fb":"code","62daa9d5":"code","0ca391c9":"code","7e5e7cfd":"code","ebddd665":"code","f266bc1f":"code","759e9b2e":"code","e8373e29":"code","e08198af":"code","1a68cacc":"code","cc70a6d7":"code","136fa27f":"code","5ab21337":"code","6b982cf7":"code","3bfb1638":"code","bf5160a4":"code","d649ed86":"code","b2055450":"code","fdbcbbfa":"code","3ecb797d":"code","0015b241":"code","ae8d6547":"code","c9577839":"code","7fce7ca5":"code","93100b42":"code","af035da4":"code","67aafc8a":"code","d80e145e":"code","2fe0bee3":"code","7d37c31c":"code","5e0da84a":"code","4adcce99":"code","1c7de43f":"code","de0b6c8b":"code","50e9fd89":"code","34f112aa":"code","8895ac81":"code","548196c5":"code","34bb18f4":"code","1bc48656":"code","5d599c3d":"code","94fea11c":"code","1849f348":"code","caaba251":"code","aaa3c88a":"code","b8408575":"code","47138622":"code","b733c5ee":"code","f877601c":"code","0cf4f188":"code","5bf8e7a9":"code","66bbb6f7":"code","2639a159":"code","90677a02":"code","437ba02b":"code","6d350ecf":"code","6412fd12":"code","1e6376c6":"code","d3c1d918":"code","9360c2c0":"code","11df324f":"code","9224115f":"code","43497e26":"code","be9f78b8":"code","ebc4076b":"code","ce0f58f3":"code","8a2e9293":"code","dd80bead":"code","8890bb79":"code","5f9b1e23":"code","2cda7a9f":"code","9e48e259":"code","342c1f3c":"code","1fa13aa9":"code","9fce01bb":"code","ee8a0555":"code","f825b959":"code","437b0f1c":"code","a76ec538":"markdown","346560f2":"markdown","0a6e351d":"markdown","bbd7fb40":"markdown","a43827bf":"markdown","6c774ba3":"markdown","4b27267b":"markdown","e56f2ccf":"markdown","00b8a888":"markdown","79cb0851":"markdown","040f6c41":"markdown","99bce1d9":"markdown","0d9b975b":"markdown","6bca2a7b":"markdown","73cbb3b2":"markdown","1ee6b713":"markdown","a8a20869":"markdown","c42181ac":"markdown","95f190c5":"markdown","c3484be6":"markdown","c2e3ed71":"markdown","880b8236":"markdown","3a25ddf1":"markdown","67a796b3":"markdown","2d3e7691":"markdown","54d75b96":"markdown","0114240b":"markdown","bcbdf45a":"markdown","b05e9c81":"markdown","f07c9b3e":"markdown","b9c10d1a":"markdown","e960bedd":"markdown","e7e1fe8d":"markdown","bc9584ed":"markdown","82e22e78":"markdown","d5d715c4":"markdown","6bc2b669":"markdown","fd819596":"markdown","baea713f":"markdown"},"source":{"b954b81b":"# standard\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, cross_validate, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.model_selection import learning_curve\n\n#machine learning\nfrom sklearn.linear_model import Lasso, Ridge\n\nimport warnings\nwarnings.filterwarnings('ignore')  # sorry for that, really","9500feb1":"# ---------- DF IMPORT -------------\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ncombine = [df_train, df_test]\ndf_train.name = 'Train'\ndf_test.name = 'Test'","f4a7baf2":"kfolds = KFold(n_splits=10, shuffle=True, random_state=14)\ntarget = np.log1p(df_train['SalePrice'])\nold_target = df_train['SalePrice']","2568766a":"def get_dollars(estimator, kfolds, data, target, old_target):\n    scores = []\n    train = data.copy()\n    for i,(train_index, test_index) in enumerate(kfolds.split(target)):\n        training = train.iloc[train_index,:]\n        valid = train.iloc[test_index,:]\n        tr_label = target.iloc[train_index]\n        val_label = target.iloc[test_index]\n        estimator.fit(training, tr_label)\n        pred = estimator.predict(valid) \n        or_result = old_target.iloc[test_index]\n        score = mean_absolute_error(y_pred=np.expm1(pred), y_true=or_result)\n        scores.append(score)     \n    return round(np.mean(scores),3)\n\n\ndef get_coef(clsf, ftrs):\n    imp = clsf.steps[1][1].coef_.tolist() #it's a pipeline\n    feats = ftrs\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result","65581e2c":"scores = []\nor_scores = []\n\nfor i,(train_index, test_index) in enumerate(kfolds.split(target)):\n    df_train['prediction'] = np.nan\n    print(\"Fold {} in progress\".format(i))\n    base = target.iloc[train_index].median()\n    result = target.iloc[test_index]\n    or_result = old_target.iloc[test_index]\n    df_train['prediction'].iloc[test_index] = base \n    prediction = [p for p in df_train['prediction'].dropna()]\n    print(\"Predicting with median {}\".format(round(base,3)))\n    score = mean_squared_error(y_pred= prediction, y_true=result)\n    or_score = mean_absolute_error(y_pred=np.expm1(prediction), y_true=or_result)\n    print(\"Scoring {}\".format(score))\n    print(\"MAE {}$\".format(or_score))\n    scores.append(score)\n    or_scores.append(or_score)\n    df_train.drop('prediction', axis=1, inplace=True)\n    print(\"_\"*40)\n    \nprint(\"Baseline: {} +- {}\".format(round(np.mean(scores),3), round(np.std(scores),3)))\nprint('MAE: {} +- {}'.format(round(np.mean(or_scores),3), round(np.std(or_scores),3)))","608046d4":"modelname = ['Baseline']\nlassoscore = [0.16] #not true, but we need it later\nridgescore = [0.16]\nkaggle_lasso = [0.41899]\nkaggle_ridge = [0.41899]\ntimeelapsed = [600]\nmaecv_lasso = [55646.558]\nmaecv_ridge = [55646.558]","90385784":"for df in combine:\n    if df.name == 'Train':\n        mis_train = []\n        cols = df.columns\n        for col in cols:\n            mis = df[col].isnull().sum()\n            if mis > 0:\n                print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df.shape[0] * 100, 3)))\n                mis_train.append(col)\n        print(\"_\"*40)\n        print(\"_\"*40)\n    if df.name == 'Test':\n        mis_test = []\n        cols = df.columns\n        for col in cols:\n            mis = df[col].isnull().sum()\n            if mis > 0:\n                print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df.shape[0] * 100, 3)))\n                mis_test.append(col)\n\nprint(\"\\n\")\nprint(mis_train)\nprint(\"_\"*40)\nprint(mis_test)","cd6f36a9":"butch_train = df_train[[col for col in df_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = df_test[[col for col in df_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\n#some of the dummies are not in both datasets (will be cured later)\nbutch_train = butch_train[[col for col in butch_test.columns]] \n\n(butch_train.columns != butch_test.columns).sum()","5a86d5c6":"scl = ('scl', RobustScaler())","e20fe808":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))]) # to avoid convergence problems\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(\"_\"*40)\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","760f66ab":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","23a00fcb":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","7ae2135e":"coefs.tail(10)","8dad9c8e":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.0075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","34b8346d":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","c8233ed3":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","4d3727bd":"coefs.tail(10)","911f12a7":"modelname.append('Butcher')\nlassoscore.append(0.1499)\nridgescore.append(0.1488)\nkaggle_lasso.append(0.13321)\nkaggle_ridge.append(0.13410)\ntimeelapsed.append(1800)\nmaecv_lasso.append(18544.430)\nmaecv_ridge.append(18327.714)","29b2302a":"target = np.log1p(df_train[df_train.GrLivArea < 4500]['SalePrice'])\nold_target = df_train[df_train.GrLivArea < 4500]['SalePrice']","10fdb8fb":"for df in combine:\n    #LotFrontage\n    df.loc[df.LotFrontage.isnull(), 'LotFrontage'] = 0\n    #Alley\n    df.loc[df.Alley.isnull(), 'Alley'] = \"NoAlley\"\n    #MSSubClass\n    df['MSSubClass'] = df['MSSubClass'].astype(str)\n    #MissingBasement\n    fil = ((df.BsmtQual.isnull()) & (df.BsmtCond.isnull()) & (df.BsmtExposure.isnull()) &\n          (df.BsmtFinType1.isnull()) & (df.BsmtFinType2.isnull()))\n    fil1 = ((df.BsmtQual.notnull()) | (df.BsmtCond.notnull()) | (df.BsmtExposure.notnull()) |\n          (df.BsmtFinType1.notnull()) | (df.BsmtFinType2.notnull()))\n    df.loc[fil1, 'MisBsm'] = 0\n    df.loc[fil, 'MisBsm'] = 1\n    #BsmtQual\n    df.loc[fil, 'BsmtQual'] = \"NoBsmt\" #missing basement\n    #BsmtCond\n    df.loc[fil, 'BsmtCond'] = \"NoBsmt\" #missing basement\n    #BsmtExposure\n    df.loc[fil, 'BsmtExposure'] = \"NoBsmt\" #missing basement\n    #BsmtFinType1\n    df.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" #missing basement\n    #BsmtFinType2\n    df.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" #missing basement\n    #FireplaceQu\n    df.loc[(df.Fireplaces == 0) & (df.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" #missing\n    #MisGarage\n    fil = ((df.GarageYrBlt.isnull()) & (df.GarageType.isnull()) & (df.GarageFinish.isnull()) &\n          (df.GarageQual.isnull()) & (df.GarageCond.isnull()))\n    fil1 = ((df.GarageYrBlt.notnull()) | (df.GarageType.notnull()) | (df.GarageFinish.notnull()) |\n          (df.GarageQual.notnull()) | (df.GarageCond.notnull()))\n    df.loc[fil1, 'MisGarage'] = 0\n    df.loc[fil, 'MisGarage'] = 1\n    #GarageYrBlt\n    df.loc[df.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007 #correct mistake\n    df.loc[fil, 'GarageYrBlt'] = 0\n    #GarageType\n    df.loc[fil, 'GarageType'] = \"NoGrg\" #missing garage\n    #GarageFinish\n    df.loc[fil, 'GarageFinish'] = \"NoGrg\" #missing\n    #GarageQual\n    df.loc[fil, 'GarageQual'] = \"NoGrg\" #missing\n    #GarageCond\n    df.loc[fil, 'GarageCond'] = \"NoGrg\" #missing\n    #Fence\n    df.loc[df.Fence.isnull(), 'Fence'] = \"NoFence\" #missing fence\n    \ndf_test[['BsmtUnfSF', \n         'TotalBsmtSF', \n         'BsmtFinSF1', \n         'BsmtFinSF2']] = df_test[['BsmtUnfSF', \n                                   'TotalBsmtSF', \n                                   'BsmtFinSF1', \n                                   'BsmtFinSF2']].fillna(0) #checked","62daa9d5":"for df in combine:\n    if df.name == 'Train':\n        mis_train = []\n        cols = df.columns\n        for col in cols:\n            mis = df[col].isnull().sum()\n            if mis > 0:\n                print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df.shape[0] * 100, 3)))\n                mis_train.append(col)\n        print(\"_\"*40)\n        print(\"_\"*40)\n    if df.name == 'Test':\n        mis_test = []\n        cols = df.columns\n        for col in cols:\n            mis = df[col].isnull().sum()\n            if mis > 0:\n                print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df.shape[0] * 100, 3)))\n                mis_test.append(col)\n\nprint(\"\\n\")\nprint(mis_train)\nprint(\"_\"*40)\nprint(mis_test)","0ca391c9":"butch_train = df_train[[col for col in df_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = df_test[[col for col in df_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\nbutch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\nbutch_test = butch_test[[col for col in butch_train.columns]]\n\n#because redundant with MisGarage\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis=1)\n#because redundant with MisBsm\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n\nbutch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\ndf_train = df_train[df_train.GrLivArea < 4500] #for consistency\n\n(butch_train.columns != butch_test.columns).sum()","7e5e7cfd":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","ebddd665":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","f266bc1f":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","759e9b2e":"coefs.tail(10)","e8373e29":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.0075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","e08198af":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","1a68cacc":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","cc70a6d7":"coefs.tail(10)","136fa27f":"modelname.append('Doc_Impute')\nlassoscore.append(0.1186)\nridgescore.append(0.1185)\nkaggle_lasso.append(0.11980)\nkaggle_ridge.append(0.11974)\ntimeelapsed.append(9000)\nmaecv_lasso.append(14411.229)\nmaecv_ridge.append(14495.809)","5ab21337":"print(df_train.shape, df_test.shape)\ndf_train.drop(\"PoolQC\", axis=1, inplace=True) #I know these are going to be missing always and I don't care\ndf_train.drop(\"MiscFeature\", axis=1, inplace=True)\ndf_test.drop(\"PoolQC\", axis=1, inplace=True)\ndf_test.drop(\"MiscFeature\", axis=1, inplace=True)\nprint(df_train.shape, df_test.shape)","6b982cf7":"print(\"Size of train set before my butchering: {}\".format(df_train.shape))\nfor f in df_train.columns:\n    df_train = df_train[pd.notnull(df_train[f])]\n\ncols = df_train.columns\nprint(\"Start printing the missing values...\")\nmis_train = []\nfor col in cols:\n    mis = df_train[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df_train.shape[0] * 100, 3)))\n        mis_train.append(col)\nprint(\"...done printing the missing values\")\nprint(mis_train)\nprint(\"Size of train set after my butchering: {}\".format(df_train.shape))","3bfb1638":"#To find the segment of the missing values, can be useful to impute the missing values\ndef find_segment(df, feat): \n    mis = df[feat].isnull().sum()\n    cols = df.columns\n    seg = []\n    for col in cols:\n        vc = df[df[feat].isnull()][col].value_counts(dropna=False).iloc[0]\n        if (vc == mis): #returns the columns for which the missing entries have only 1 possible value\n            seg.append(col)\n    return seg\n\n# to find the mode of the missing feature, by choosing the right segment to compare (uses find_segment)\ndef find_mode(df, feat): #returns the mode to fill in the missing feat\n    md = df[df[feat].isnull()][find_segment(df, feat)].dropna(axis=1).mode()\n    md = pd.merge(df, md, how='inner')[feat].mode().iloc[0]\n    return md\n\n# identical to the previous one, but with the median\ndef find_median(df, feat): #returns the median to fill in the missing feat\n    md = df[df[feat].isnull()][find_segment(df, feat)].dropna(axis=1).mode()\n    md = pd.merge(df, md, how='inner')[feat].median()\n    return md\n\n# find the mode in a segment defined by the user\ndef similar_mode(df, col, feats): #returns the mode in a segment made by similarity in feats\n    sm = df[df[col].isnull()][feats]\n    md = pd.merge(df, sm, how='inner')[col].mode().iloc[0]\n    return md\n\n# Find the median in a segment defined by the user\ndef similar_median(df, col, feats): #returns the median in a segment made by similarity in feats\n    sm = df[df[col].isnull()][feats]\n    md = pd.merge(df, sm, how='inner')[col].median()\n    return md","bf5160a4":"#MSZoning\nmd = find_mode(df_test, 'MSZoning')\nprint(\"MSZoning {}\".format(md))\ndf_test[['MSZoning']] = df_test[['MSZoning']].fillna(md)\n#Utilities\nmd = 'AllPub'\ndf_test[['Utilities']] = df_test[['Utilities']].fillna(md)\n#MasVnrType\nmd = find_mode(df_test, 'MasVnrType')\nprint(\"MasVnrType {}\".format(md))\ndf_test[['MasVnrType']] = df_test[['MasVnrType']].fillna(md)\n#MasVnrArea\nmd = find_mode(df_test, 'MasVnrArea')\nprint(\"MasVnrArea {}\".format(md))\ndf_test[['MasVnrArea']] = df_test[['MasVnrArea']].fillna(md)\n#BsmtQual\nsimi = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtQual', simi)\nprint(\"BsmtQual {}\".format(md))\ndf_test[['BsmtQual']] = df_test[['BsmtQual']].fillna(md)\n#BsmtCond\nsimi = ['BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtCond', simi)\nprint(\"BsmtCond {}\".format(md))\ndf_test[['BsmtCond']] = df_test[['BsmtCond']].fillna(md)\n#BsmtCond\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtExposure', simi)\nprint(\"BsmtExposure {}\".format(md))\ndf_test[['BsmtExposure']] = df_test[['BsmtExposure']].fillna(md)\n#BsmtFullBath\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_median(df_test, 'BsmtFullBath', simi)\nprint(\"BsmtFullBath {}\".format(md))\ndf_test[['BsmtFullBath']] = df_test[['BsmtFullBath']].fillna(md)\n#BsmtHalfBath\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_median(df_test, 'BsmtHalfBath', simi)\nprint(\"BsmtHalfBath {}\".format(md))\ndf_test[['BsmtHalfBath']] = df_test[['BsmtHalfBath']].fillna(md)\n#KitchenQual\nmd = df_test.KitchenQual.mode().iloc[0]\nprint(\"KitchenQual {}\".format(md))\ndf_test[['KitchenQual']] = df_test[['KitchenQual']].fillna(md)\n#Functional\nmd = 'Typ'\ndf_test[['Functional']] = df_test[['Functional']].fillna(md)\n#GarageYrBlt\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageYrBlt', simi)\nprint(\"GarageYrBlt {}\".format(md))\ndf_test[['GarageYrBlt']] = df_test[['GarageYrBlt']].fillna(md)\n#GarageFinish\nmd = 'Unf'\nprint(\"GarageFinish {}\".format(md))\ndf_test[['GarageFinish']] = df_test[['GarageFinish']].fillna(md)\n#GarageArea\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageArea', simi)\nprint(\"GarageArea {}\".format(md))\ndf_test[['GarageArea']] = df_test[['GarageArea']].fillna(md)\n#GarageQual\nsimi = ['GarageType', 'MisGarage', 'GarageFinish']\nmd = similar_mode(df_test, 'GarageQual', simi)\nprint(\"GarageQual {}\".format(md))\ndf_test[['GarageQual']] = df_test[['GarageQual']].fillna(md)\n#GarageCond\nsimi = ['GarageType', 'MisGarage', 'GarageFinish']\nmd = similar_mode(df_test, 'GarageCond', simi)\nprint(\"GarageCond {}\".format(md))\ndf_test[['GarageCond']] = df_test[['GarageCond']].fillna(md)\n#GarageCars\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageCars', simi)\nprint(\"GarageCars {}\".format(md))\ndf_test[['GarageCars']] = df_test[['GarageCars']].fillna(md)\n\ncols = df_test.columns\nmis_test = []\nprint(\"Start printing the missing values...\")\nfor col in cols:\n    mis = df_test[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df_test.shape[0] * 100, 3)))\n        mis_test.append(col)\nprint(\"...done printing the missing values\")","d649ed86":"butch_train = df_train[[col for col in df_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = df_test[[col for col in df_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\nprint(list(set(butch_train.columns) - set(butch_test.columns)))\n\nbutch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\nbutch_test = butch_test[[col for col in butch_train.columns]]\n\n#because redundant with MisGarage\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis=1)\n#because redundant with MisBsm\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n\nbutch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\ntarget = np.log1p(df_train[df_train.GrLivArea < 4500]['SalePrice']) #for consistency\nold_target = df_train[df_train.GrLivArea < 4500]['SalePrice']\n\n(butch_train.columns != butch_test.columns).sum()","b2055450":"butch_train.shape","fdbcbbfa":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","3ecb797d":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","0015b241":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","ae8d6547":"coefs.tail(10)","c9577839":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","7fce7ca5":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","93100b42":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","af035da4":"coefs.tail(10)","67aafc8a":"modelname.append('Impute_full')\nlassoscore.append(0.1115)\nridgescore.append(0.1125)\nkaggle_lasso.append(0.11847)\nkaggle_ridge.append(0.11727)\ntimeelapsed.append(12600)\nmaecv_lasso.append(13250.529)\nmaecv_ridge.append(13383.857)","d80e145e":"# just to have it as output\nprediction = np.expm1(best_ridge.predict(butch_test)) \n\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id']\nsub['SalePrice'] = prediction\nsub.to_csv('best_ridge_full_imputation.csv',index=False)","2fe0bee3":"df_train.drop(\"Exterior1st\", axis=1, inplace=True) #these I know they are not helpful already\ndf_test.drop(\"Exterior1st\", axis=1, inplace=True)\ndf_train.drop(\"Exterior2nd\", axis=1, inplace=True)\ndf_test.drop(\"Exterior2nd\", axis=1, inplace=True)\ndf_train.drop(\"SaleType\", axis=1, inplace=True)\ndf_test.drop(\"SaleType\", axis=1, inplace=True)","7d37c31c":"def tr_ExtQual(df):\n    df.loc[df.ExterQual == 'Fa', 'ExterQual'] = 0\n    df.loc[df.ExterQual == 'TA', 'ExterQual'] = 1\n    df.loc[df.ExterQual == 'Gd', 'ExterQual'] = 2\n    df.loc[df.ExterQual == 'Ex', 'ExterQual'] = 3\n    df.ExterQual = pd.to_numeric(df.ExterQual)\n    return df\ndef tr_ExtQual_plus(df):\n    df.loc[df.ExterQual == 'Fa', 'ExterQual'] = 1\n    df.loc[df.ExterQual == 'TA', 'ExterQual'] = 1\n    df.loc[df.ExterQual == 'Gd', 'ExterQual'] = 2\n    df.loc[df.ExterQual == 'Ex', 'ExterQual'] = 3\n    df.ExterQual = pd.to_numeric(df.ExterQual)\n    return df\n\ndef tr_ExtCond(df):\n    df.loc[df.ExterCond == 'Po', 'ExterCond'] = 0\n    df.loc[df.ExterCond == 'Fa', 'ExterCond'] = 1\n    df.loc[df.ExterCond == 'TA', 'ExterCond'] = 2\n    df.loc[df.ExterCond == 'Gd', 'ExterCond'] = 3\n    df.loc[df.ExterCond == 'Ex', 'ExterCond'] = 4\n    df.ExterCond = pd.to_numeric(df.ExterCond)\n    return df\ndef tr_ExtCond_plus(df):\n    df.loc[df.ExterCond == 'Po', 'ExterCond'] = 1\n    df.loc[df.ExterCond == 'Fa', 'ExterCond'] = 1\n    df.loc[df.ExterCond == 'TA', 'ExterCond'] = 1\n    df.loc[df.ExterCond == 'Gd', 'ExterCond'] = 2\n    df.loc[df.ExterCond == 'Ex', 'ExterCond'] = 2\n    df.ExterCond = pd.to_numeric(df.ExterCond)\n    return df\n\ndef tr_BsmtQu(df):\n    df.loc[df.BsmtQual == 'NoBsmt', 'BsmtQual'] = 0\n    df.loc[df.BsmtQual == 'Fa', 'BsmtQual'] = 1\n    df.loc[df.BsmtQual == 'TA', 'BsmtQual'] = 2\n    df.loc[df.BsmtQual == 'Gd', 'BsmtQual'] = 3\n    df.loc[df.BsmtQual == 'Ex', 'BsmtQual'] = 4\n    df.BsmtQual = pd.to_numeric(df.BsmtQual)\n    return df\ndef tr_BsmtQu_plus(df):\n    df.loc[df.BsmtQual == 'NoBsmt', 'BsmtQual'] = 0\n    df.loc[df.BsmtQual == 'Fa', 'BsmtQual'] = 1\n    df.loc[df.BsmtQual == 'TA', 'BsmtQual'] = 4\n    df.loc[df.BsmtQual == 'Gd', 'BsmtQual'] = 10\n    df.loc[df.BsmtQual == 'Ex', 'BsmtQual'] = 21\n    df.BsmtQual = pd.to_numeric(df.BsmtQual)\n    return df\n\ndef tr_BsmtCo(df):\n    df.loc[df.BsmtCond == 'NoBsmt', 'BsmtCond'] = 0\n    df.loc[df.BsmtCond == 'Po', 'BsmtCond'] = 1\n    df.loc[df.BsmtCond == 'Fa', 'BsmtCond'] = 2\n    df.loc[df.BsmtCond == 'TA', 'BsmtCond'] = 3\n    df.loc[df.BsmtCond == 'Gd', 'BsmtCond'] = 4\n    df.BsmtCond = pd.to_numeric(df.BsmtCond)\n    return df\n\ndef tr_BsmtExp(df):\n    df.loc[df.BsmtExposure == 'NoBsmt', 'BsmtExposure'] = 0\n    df.loc[df.BsmtExposure == 'No', 'BsmtExposure'] = 1\n    df.loc[df.BsmtExposure == 'Mn', 'BsmtExposure'] = 2\n    df.loc[df.BsmtExposure == 'Av', 'BsmtExposure'] = 3\n    df.loc[df.BsmtExposure == 'Gd', 'BsmtExposure'] = 4\n    df.BsmtExposure = pd.to_numeric(df.BsmtExposure)\n    return df\ndef tr_BsmtExp_plus(df):\n    df.loc[df.BsmtExposure == 'NoBsmt', 'BsmtExposure'] = 0\n    df.loc[df.BsmtExposure == 'No', 'BsmtExposure'] = 6\n    df.loc[df.BsmtExposure == 'Mn', 'BsmtExposure'] = 7\n    df.loc[df.BsmtExposure == 'Av', 'BsmtExposure'] = 8\n    df.loc[df.BsmtExposure == 'Gd', 'BsmtExposure'] = 12\n    df.BsmtExposure = pd.to_numeric(df.BsmtExposure)\n    return df\n\ndef tr_HeatQ(df):\n    df.loc[df.HeatingQC == 'Po', 'HeatingQC'] = 0\n    df.loc[df.HeatingQC == 'Fa', 'HeatingQC'] = 1\n    df.loc[df.HeatingQC == 'TA', 'HeatingQC'] = 2\n    df.loc[df.HeatingQC == 'Gd', 'HeatingQC'] = 3\n    df.loc[df.HeatingQC == 'Ex', 'HeatingQC'] = 4\n    df.HeatingQC = pd.to_numeric(df.HeatingQC)\n    return df\ndef tr_HeatQ_plus(df):\n    df.loc[df.HeatingQC == 'Po', 'HeatingQC'] = 1\n    df.loc[df.HeatingQC == 'Fa', 'HeatingQC'] = 1\n    df.loc[df.HeatingQC == 'TA', 'HeatingQC'] = 3\n    df.loc[df.HeatingQC == 'Gd', 'HeatingQC'] = 4\n    df.loc[df.HeatingQC == 'Ex', 'HeatingQC'] = 7\n    df.HeatingQC = pd.to_numeric(df.HeatingQC)\n    return df\n\ndef tr_KitcQu(df):\n    df.loc[df.KitchenQual == 'Fa', 'KitchenQual'] = 1\n    df.loc[df.KitchenQual == 'TA', 'KitchenQual'] = 2\n    df.loc[df.KitchenQual == 'Gd', 'KitchenQual'] = 3\n    df.loc[df.KitchenQual == 'Ex', 'KitchenQual'] = 4\n    df.KitchenQual = pd.to_numeric(df.KitchenQual)\n    return df\ndef tr_KitcQu_plus(df):\n    df.loc[df.KitchenQual == 'Fa', 'KitchenQual'] = 1\n    df.loc[df.KitchenQual == 'TA', 'KitchenQual'] = 4\n    df.loc[df.KitchenQual == 'Gd', 'KitchenQual'] = 10\n    df.loc[df.KitchenQual == 'Ex', 'KitchenQual'] = 21\n    df.KitchenQual = pd.to_numeric(df.KitchenQual)\n    return df\n\ndef tr_FireQu(df):\n    df.loc[df.FireplaceQu == 'NoFire', 'FireplaceQu'] = 0\n    df.loc[df.FireplaceQu == 'Po', 'FireplaceQu'] = 1\n    df.loc[df.FireplaceQu == 'Fa', 'FireplaceQu'] = 2\n    df.loc[df.FireplaceQu == 'TA', 'FireplaceQu'] = 3\n    df.loc[df.FireplaceQu == 'Gd', 'FireplaceQu'] = 4\n    df.loc[df.FireplaceQu == 'Ex', 'FireplaceQu'] = 5\n    df.FireplaceQu = pd.to_numeric(df.FireplaceQu)\n    return df\ndef tr_FireQu_plus(df):\n    df.loc[df.FireplaceQu == 'NoFire', 'FireplaceQu'] = 0\n    df.loc[df.FireplaceQu == 'Po', 'FireplaceQu'] = 0\n    df.loc[df.FireplaceQu == 'Fa', 'FireplaceQu'] = 2\n    df.loc[df.FireplaceQu == 'TA', 'FireplaceQu'] = 3\n    df.loc[df.FireplaceQu == 'Gd', 'FireplaceQu'] = 4\n    df.loc[df.FireplaceQu == 'Ex', 'FireplaceQu'] = 8\n    df.FireplaceQu = pd.to_numeric(df.FireplaceQu)\n    return df\n\ndef tr_GarQu(df):\n    df.loc[df.GarageQual == 'NoGrg', 'GarageQual'] = 0\n    df.loc[df.GarageQual == 'Po', 'GarageQual'] = 1\n    df.loc[df.GarageQual == 'Fa', 'GarageQual'] = 2\n    df.loc[df.GarageQual == 'TA', 'GarageQual'] = 3\n    df.loc[df.GarageQual == 'Gd', 'GarageQual'] = 4\n    if df.name == 'Train':\n        df.loc[df.GarageQual == 'Ex', 'GarageQual'] = 5\n    df.GarageQual = pd.to_numeric(df.GarageQual)\n    return df\n\ndef tr_GarCo(df):\n    df.loc[df.GarageCond == 'NoGrg', 'GarageCond'] = 0\n    df.loc[df.GarageCond == 'Po', 'GarageCond'] = 1\n    df.loc[df.GarageCond == 'Fa', 'GarageCond'] = 2\n    df.loc[df.GarageCond == 'TA', 'GarageCond'] = 3\n    df.loc[df.GarageCond == 'Gd', 'GarageCond'] = 4\n    df.loc[df.GarageCond == 'Ex', 'GarageCond'] = 5\n    df.GarageCond = pd.to_numeric(df.GarageCond)\n    return df\n\ndef tr_MSZo(df):\n    df.loc[(df.MSZoning == 'RH') | (df.MSZoning == 'RM'), 'MSZoning'] = 'ResMedHig'\n    df.loc[(df.MSZoning == 'FV'), 'MSZoning'] = 'Vil'\n    df.loc[(df.MSZoning == 'RL')| (df.MSZoning == 'C (all)'), 'MSZoning'] = 'ResLowCom'\n    return df\n\ndef tr_Alley(df):\n    df.loc[(df.Alley == 'Grvl') | (df.Alley == 'Pave'), 'Alley'] = 'Alley'\n    df.loc[df.Alley == 'NoAlley', 'Alley'] = 'NoAlley'\n    return df\n\ndef tr_LotSh(df):\n    irr = ['IR1', 'IR2', 'IR3']\n    df.loc[(df.LotShape.isin(irr)), 'LotShape'] = 'Irreg'\n    df.loc[df.LotShape == 'Reg', 'LotShape'] = 'Reg'\n    return df\n\ndef tr_Cond1(df):\n    ArtFee = ['Artery', 'Feedr']\n    stat = ['RRAe', 'RRAn', 'RRNe', 'RRNn']\n    pos = ['PosA', 'PosN']\n    df.loc[(df.Condition1.isin(ArtFee)), 'Condition1'] = 'ArtFee'\n    df.loc[(df.Condition1.isin(stat)), 'Condition1'] = 'Station'\n    df.loc[(df.Condition1.isin(pos)), 'Condition1'] = 'Station'\n    df.loc[df.Condition1 == 'Norm', 'Condition1'] = 'Norm'\n    return df\n\ndef tr_BldTy(df):\n    df.loc[(df.BldgType == '2fmCon') | (df.BldgType == 'Duplex'), 'BldgType'] = '2FamDup'\n    df.loc[(df.BldgType == 'Twnhs') | (df.BldgType == 'TwnhsE'), 'BldgType'] = 'Twnhs+E'\n    df.loc[(df.BldgType == '1Fam'), 'BldgType'] = '1Fam'\n    return df\n\ndef tr_HSty(df):\n    onepl = ['1.5Fin', '1.5Unf']\n    twopl = ['2.5Fin', '2.5Unf', '2Story']\n    spl = ['SFoyer', 'SLvl']\n    df.loc[df.HouseStyle.isin(onepl), 'HouseStyle'] = '1.5'\n    df.loc[df.HouseStyle.isin(twopl), 'HouseStyle'] = '2plus'\n    df.loc[df.HouseStyle.isin(spl), 'HouseStyle'] = 'Split'\n    df.loc[df.HouseStyle == '1Story', 'HouseStyle'] = \"1Story\"\n    return df\n\ndef tr_Found(df):\n    fancy = ['BrkTil', 'Stone', 'Wood']\n    cement = ['PConc', 'Slab']\n    df.loc[df.Foundation.isin(fancy), 'Foundation'] = 'Fancy'\n    df.loc[df.Foundation.isin(cement), 'Foundation'] = 'Cement'\n    df.loc[df.Foundation == 'CBlock', 'Foundation'] = 'Cider'\n    return df\n\ndef tr_MasVnrTy(df):\n    df.loc[df.MasVnrType == 'None', 'MasVnrType'] = 'None'\n    df.loc[df.MasVnrType == 'Stone', 'MasVnrType'] = 'Stone'\n    df.loc[(df.MasVnrType == 'BrkCmn') | (df.MasVnrType == 'BrkFace'), 'MasVnrType'] = 'Bricks'\n    return df\n\ndef tr_Elec(df):\n    df.loc[df.Electrical == \"SBrkr\", \"Electrical\"] = \"SBrkr\"\n    fuse = ['FuseA', 'FuseF', 'FuseP', 'Mix']\n    df.loc[df.Electrical.isin(fuse), \"Electrical\"] = \"Fuse\"\n    return df\n    \ndef tr_GrgTy(df):\n    incl = ['Attchd', 'Basment', 'BuiltIn']\n    escl = ['2Types', 'CarPort', 'Detchd']\n    df.loc[df.GarageType.isin(incl), 'GarageType'] = 'Connected'\n    df.loc[df.GarageType.isin(escl), 'GarageType'] = 'NonConnected'\n    df.loc[df.GarageType == 'NoGrg', 'GarageType'] = 'NoGrg'\n    return df\n\ndef tr_PvdDr(df):\n    df.loc[df.PavedDrive == 'N', 'PavedDrive'] = 'N'\n    df.loc[df.PavedDrive == 'P', 'PavedDrive'] = 'N'\n    df.loc[df.PavedDrive == 'Y', 'PavedDrive'] = 'Y'\n    return df\n\ndef tr_Fence(df):\n    df.loc[df.Fence == 'NoFence', 'Fence'] = 'NoFence'\n    df.loc[(df.Fence == 'MnPrv') | (df.Fence == 'GdPrv'), 'Fence'] = 'Prv'\n    df.loc[(df.Fence == 'MnWw') | (df.Fence == 'GdWo'), 'Fence'] = 'Wo'\n    return df","5e0da84a":"tr_train = df_train.copy() #because memory is free... sort of\ntr_test = df_test.copy()\n\ntr_train.name = 'Train' #because some dummies were different, see the transformation above\ntr_test.name = 'Test'\n\ntr_train = tr_GarQu(tr_train)\ntr_test = tr_GarQu(tr_test)\ntr_train = tr_FireQu(tr_train)\ntr_test = tr_FireQu(tr_test)\ntr_train = tr_HeatQ_plus(tr_train)\ntr_test = tr_HeatQ_plus(tr_test)\ntr_train = tr_PvdDr(tr_train)\ntr_test = tr_PvdDr(tr_test)\ntr_train = tr_GrgTy(tr_train)\ntr_test = tr_GrgTy(tr_test)\ntr_train = tr_ExtQual_plus(tr_train)\ntr_test = tr_ExtQual_plus(tr_test)\ntr_train = tr_MasVnrTy(tr_train)\ntr_test = tr_MasVnrTy(tr_test) \ntr_train = tr_GarCo(tr_train)\ntr_test = tr_GarCo(tr_test) \ntr_train = tr_BsmtCo(tr_train)\ntr_test = tr_BsmtCo(tr_test) \ntr_train = tr_Elec(tr_train)\ntr_test = tr_Elec(tr_test) \ntr_train = tr_BsmtQu_plus(tr_train)\ntr_test = tr_BsmtQu_plus(tr_test) \ntr_train = tr_LotSh(tr_train)\ntr_test = tr_LotSh(tr_test)\ntr_train = tr_Alley(tr_train)\ntr_test = tr_Alley(tr_test)","4adcce99":"butch_train = tr_train[[col for col in tr_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = tr_test[[col for col in tr_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\nprint(set(df_train.columns) - set(df_test.columns))\n\nbutch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\nbutch_test = butch_test[[col for col in butch_train.columns]]\n\n#because redundant with MisGarage\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis=1)\n#because redundant with MisBsm\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1)\nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n\nbutch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\ntarget = np.log1p(df_train['SalePrice']) #for consistency\n\n(butch_train.columns != butch_test.columns).sum()","1c7de43f":"butch_train.shape","de0b6c8b":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","50e9fd89":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","34f112aa":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","8895ac81":"coefs.tail(10)","548196c5":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","34bb18f4":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","1bc48656":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","5d599c3d":"coefs.tail(10)","94fea11c":"modelname.append('Transform')\nlassoscore.append(0.1106)\nridgescore.append(0.1114)\nkaggle_lasso.append(0.11910)\nkaggle_ridge.append(0.11838)\ntimeelapsed.append(16200)\nmaecv_lasso.append(13259.404)\nmaecv_ridge.append(13375.657)","1849f348":"candidates = ['Condition1', 'Condition2', 'YearRemodAdd', \"RoofStyle\", \n              'RoofMatl','BsmtFinSF1', 'BsmtFinSF2', 'Heating', 'Electrical', \n              '1stFlrSF', '2ndFlrSF', 'LowQualFinSF','Functional', 'TotRmsAbvGrd',\n             'GarageCars', 'YrSold', 'MoSold']","caaba251":"%%time\n\nwinners = []\n\npipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\nfor feat in candidates:\n    sel_train = tr_train.drop(feat, axis=1) #copies of the dataframe optained with the transformations above\n    sel_test = tr_test.drop(feat, axis=1) # minus one feature.\n    butch_train = sel_train[[col for col in sel_test.columns if col not in mis_test]].dropna(axis=1)\n    butch_test = sel_test[[col for col in sel_test.columns if col not in mis_train]].dropna(axis=1)\n    butch_train.drop(\"Id\", axis=1, inplace=True)\n    butch_test.drop(\"Id\", axis=1, inplace=True)\n    butch_train = pd.get_dummies(butch_train)\n    butch_test = pd.get_dummies(butch_test)\n    #print(set(df_train.columns) - set(df_test.columns))\n    butch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\n    butch_test = butch_test[[col for col in butch_train.columns]]\n    butch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1)\n    butch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis=1)\n    butch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1)\n    butch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n    butch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\n    target = np.log1p(df_train['SalePrice']) #for consistency\n    grid.fit(butch_train, target)\n    score = np.sqrt(-grid.best_score_)\n    if score <= 0.11065: #give some margin\n        print(feat, score-0.11063297306583295) #previous best result\n        print(score)\n        winners.append(feat)\n        print(\"_\"*40)\n    \nprint(winners)","aaa3c88a":"# in the comments the cv result you would get by just stopping there and the reason why I considered them.\n\n#not really relevant\nsel_train = tr_train.drop('LowQualFinSF', axis=1)\nsel_test = tr_test.drop('LowQualFinSF', axis=1) #0.110506961384\n# redundant with GrLivArea\nsel_train.drop('TotRmsAbvGrd', axis=1, inplace=True)\nsel_test.drop('TotRmsAbvGrd', axis=1, inplace=True) #0.110401590715\n# looking at pictures of the 3 categories, I couldn't appreciate any difference\nsel_train.drop('RoofStyle', axis=1, inplace=True)\nsel_test.drop('RoofStyle', axis=1, inplace=True) #0.110335024836\n# very sparse classes\nsel_train.drop('Heating', axis=1, inplace=True)\nsel_test.drop('Heating', axis=1, inplace=True) #0.110301207584\n# couldn't see any pattern or imagine that it could matter in any way\nsel_train.drop('MoSold', axis=1, inplace=True)\nsel_test.drop('MoSold', axis=1, inplace=True) #0.110266414516\n# very sparse classes\nsel_train.drop('Electrical', axis=1, inplace=True)\nsel_test.drop('Electrical', axis=1, inplace=True) #0.110241668859\n# surprisingly, not showing any trend in the dataset\nsel_train.drop('YrSold', axis=1, inplace=True)\nsel_test.drop('YrSold', axis=1, inplace=True) #0.110208857877\n# Like roof style\nsel_train.drop('RoofMatl', axis=1, inplace=True)\nsel_test.drop('RoofMatl', axis=1, inplace=True) #0.110189729503\n# reduntant with the other features\nsel_train.drop('BsmtFinSF1', axis=1, inplace=True)\nsel_test.drop('BsmtFinSF1', axis=1, inplace=True) #0.11017219933\n# redundant with GrLivArea\nsel_train.drop('2ndFlrSF', axis=1, inplace=True)\nsel_test.drop('2ndFlrSF', axis=1, inplace=True) #0.11016945367","b8408575":"sel_train.drop('BsmtFinSF2', axis=1, inplace=True)\nsel_test.drop('BsmtFinSF2', axis=1, inplace=True) #0.110080460877\nsel_train.drop('LandSlope', axis=1, inplace=True)\nsel_test.drop('LandSlope', axis=1, inplace=True) #0.1099013725","47138622":"butch_train = sel_train[[col for col in sel_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = sel_test[[col for col in sel_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\nprint(set(df_train.columns) - set(df_test.columns))\n\nbutch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\nbutch_test = butch_test[[col for col in butch_train.columns]]\n\n#because redundant with MisGarage\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis=1)\n#because redundant with MisBsm\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n\nbutch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\ntarget = np.log1p(df_train['SalePrice']) #for consistency\n\n(butch_train.columns != butch_test.columns).sum()","b733c5ee":"butch_train.shape","f877601c":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","0cf4f188":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","5bf8e7a9":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","66bbb6f7":"coefs.tail(10)","2639a159":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","90677a02":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","437ba02b":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","6d350ecf":"coefs.tail(10)","6412fd12":"modelname.append('Feat_Sel_EDA')\nlassoscore.append(0.1099)\nridgescore.append(0.1107)\nkaggle_lasso.append(0.11953)\nkaggle_ridge.append(0.11861)\ntimeelapsed.append(18000)\nmaecv_lasso.append(13161.658)\nmaecv_ridge.append(13273.673)","1e6376c6":"eng_train = sel_train.copy()\neng_test = sel_test.copy()\neng_test['LotDepth'] = eng_test['LotFrontage'] \/ eng_test['LotArea']\neng_train['LotDepth'] = eng_train['LotFrontage'] \/ eng_train['LotArea']","d3c1d918":"eng_train['TotBath'] = eng_train.FullBath + eng_train.HalfBath\neng_test['TotBath'] = eng_test.FullBath + eng_test.HalfBath","9360c2c0":"eng_train['TotPorch'] = (eng_train['WoodDeckSF'] + eng_train['OpenPorchSF'] + eng_train['EnclosedPorch'] + \n                       eng_train['3SsnPorch'] + eng_train['ScreenPorch'])\neng_test['TotPorch'] = (eng_test['WoodDeckSF'] + eng_test['OpenPorchSF'] + eng_test['EnclosedPorch'] + \n                       eng_test['3SsnPorch'] + eng_test['ScreenPorch'])","11df324f":"butch_train = eng_train[[col for col in eng_test.columns if col not in mis_test]].dropna(axis=1)\nbutch_test = eng_test[[col for col in eng_test.columns if col not in mis_train]].dropna(axis=1)\n\nbutch_train.drop(\"Id\", axis=1, inplace=True)\nbutch_test.drop(\"Id\", axis=1, inplace=True)\n\nbutch_train = pd.get_dummies(butch_train)\nbutch_test = pd.get_dummies(butch_test)\n\nprint(set(df_train.columns) - set(df_test.columns))\n\nbutch_train = butch_train[[col for col in butch_test.columns if col in butch_train.columns]]\nbutch_test = butch_test[[col for col in butch_train.columns]]\n\n#because redundant with MisGarage\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoGrg' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoGrg' in col], axis = 1)\n#because redundant with MisBsm\nbutch_train = butch_train.drop([col for col in butch_train.columns if 'NoBsmt' in col], axis=1) \nbutch_test = butch_test.drop([col for col in butch_test.columns if 'NoBsmt' in col], axis=1)\n\nbutch_train = butch_train[butch_train.GrLivArea < 4500] #according to documentation\ntarget = np.log1p(df_train['SalePrice']) #for consistency\n\n(butch_train.columns != butch_test.columns).sum()","9224115f":"butch_train.shape","43497e26":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","be9f78b8":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","ebc4076b":"coefs = get_coef(best_lasso, butch_train.columns)\ncoefs.head(10)","ce0f58f3":"coefs.tail(10)","8a2e9293":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5,\n                                10, 15, 17.5]}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))\nprint(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])","dd80bead":"dol = get_dollars(best_ridge, kfolds, butch_train, target, old_target)\ndol","8890bb79":"coefs = get_coef(best_ridge, butch_train.columns)\ncoefs.head(10)","5f9b1e23":"coefs.tail(10)","2cda7a9f":"modelname.append('Feat_eng')\nlassoscore.append(0.1089)\nridgescore.append(0.1094)\nkaggle_lasso.append(0.11928)\nkaggle_ridge.append(0.11828)\ntimeelapsed.append(19800)\nmaecv_lasso.append(13027.341)\nmaecv_ridge.append(13060.855)","9e48e259":"def learning_print(estimator, train, label, folds):\n    \"\"\"\n    estimator: an estimator\n    train: set with input data\n    label: target variable\n    folds: cross validation\n    \"\"\"\n    train_sizes = np.arange(0.1, 1, 0.05)\n    train_sizes, train_scores, validation_scores = learning_curve(\n                                                   estimator=estimator, X=train,\n                                                   y=label, train_sizes=train_sizes, cv=folds,\n                                                   scoring='neg_mean_squared_error')\n    train_scores_mean = np.mean(np.sqrt(-train_scores), axis=1)\n    train_scores_std = np.std(np.sqrt(-train_scores), axis=1)\n    validation_scores_mean = np.mean(np.sqrt(-validation_scores), axis=1)\n    validation_scores_std = np.std(np.sqrt(-validation_scores), axis=1)\n    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.plot(train_sizes, train_scores_mean, label='Training error')\n    ax1.plot(train_sizes, validation_scores_mean, label='Validation error')\n    ax2.plot(train_sizes, train_scores_std, label='Training std')\n    ax2.plot(train_sizes, validation_scores_std, label='Validation std')\n    ax1.legend()\n    ax2.legend()","342c1f3c":"learning_print(best_lasso, butch_train, target, kfolds)","1fa13aa9":"learning_print(best_ridge, butch_train, target, kfolds)","9fce01bb":"pipe = Pipeline([scl, ('lasso', Lasso(max_iter=2000))])\n\nparam_grid = [{'lasso__alpha' : np.arange(0.0001,0.001, 0.00001)}] #because it was always 0.0005\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_lasso = grid.best_estimator_\nprint(best_lasso)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))","ee8a0555":"dol = get_dollars(best_lasso, kfolds, butch_train, target, old_target)\ndol","f825b959":"pipe = Pipeline([scl, ('ridge', Ridge())])\n\nparam_grid = [{'ridge__alpha' : np.arange(5,15,0.5)}]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, \n                    cv=kfolds, scoring='neg_mean_squared_error', return_train_score=True, n_jobs=-1)\n\n%time grid.fit(butch_train, target)\n\n#let's see the best estimator\nbest_ridge = grid.best_estimator_\nprint(best_ridge)\nprint(\"_\"*40)\n#with its score\nprint(np.sqrt(-grid.best_score_))","437b0f1c":"scoresummary = pd.DataFrame({'Model Name' : modelname,\n                            'CVScore Lasso': lassoscore,\n                            'CVScore Ridge': ridgescore,\n                            'KaggleScore Lasso': kaggle_lasso,\n                            'KaggleScore Ridge': kaggle_ridge,\n                            'MAE Lasso': maecv_lasso,\n                            'MAE Ridge': maecv_ridge,\n                            'Cumulative Time': timeelapsed})\n\nscoresummary[['Model Name', 'CVScore Lasso', 'CVScore Ridge', 'KaggleScore Lasso', \n             'KaggleScore Ridge', 'MAE Lasso', 'MAE Ridge', 'Cumulative Time']]","a76ec538":"# Fast and Butchery\n\nJust drop everything missing and go with the wind.\n\nThis is what is missing.","346560f2":"This entire process took me 5 minutes, got a score of 0.16 which translates into missing the price of a house by 55000 dollars on average.\n\nAt this stage, my model would predict for every house a price of 163000 dollars, scoring a 0.41899 on Kaggle. Let's keep track of these results.","0a6e351d":"So according to my cv scores, I should decrease the regularization of Lasso a bit for a small improvement (not observable on the Leaderboard) and Ridge was already at its best.\n\n# What I have learned and next steps\n\nA total of 5 hours and a half have passed. I am out of ideas on how to get my models a better version of themselves without using more complicated and fancy algorithms. I feel it is a good moment to stop and think about what I have learned.\n\nI always had the problem of finding something interesting in the data and dig into it with all the passion I have. This is **a lot** of fun (and we are all here for that) but more often than not I find myself spending 10 hours on something that will not help me solve the problem. \n\nThis \"agile\" setting forced me to see a problem, ignore it for the time being, see how it goes, fix it in the next iteration. I felt way more productive and efficient because I was not getting distracted by the next problem that I was finding while fixing the previous one.\n\nMoreover, I feel I had the chance of learning what can most likely help a model rather than not because I could see the effect of a single action in my results and thus it was easier to isolate that effect.\n\nOne thing that I am happy about is that my cross-validated scores are very reliable in predicting how the model will generalize (some fluctuations, but what doesn't fluctuate after all). The main reason for that is that I am careful of not use information I am not supposed to use. In other words, data leakage can help your public score (you can use the answers to help the question) but it won't make your model generalizable as you might think because it is easy to *overfit the test* .\n\nThe model here won't change the real estate industry but it is the product of half a day of work and we are talking about a very conservative industry. My concern with the data is that there are too many subjective features that get a lot of importance (Overall Quality above all) and these are not reliable in the long term unless the criteria are very strict.\n\nThis is a playground competition and this is, by any means, a game and an excuse to receive some feedback on how we can all do better. A few things came to my mind so far:\n\n* I keep all the dummies and this can cause collinearity issues. It is true that we are talking about 2 regularized model and it should not matter too much, but it would be a good thing to check that.\n* I didn't remove the skewness. Looking at other kernels and other experiments I did, a boxcox transformation would help. The one thing that I would do is to put the boxcox inside of the pipeline so that it doesn't use information of the validation set during cross-validation.\n* Use a model like RandomForest to select the features. Again, inside of the pipeline. It would make it much slower but we spent already 5.5 hours in failing fast, we can try to fail slowly for once.\n* Get fancy with ensemble and stacking models because it is a cool thing to do.\n\nWhen I submitted my results, I got around the 550th position on the leaderboard (top 13%, kaggle said). The best score I got is 0.11727 and a top 100 result would be 0.114. One may evaluate in a realistic situation if that 0.003 of improvement in the mean squared error is worth the work that would require.\n\nAt last, here a summary of what happened in this kernel.","bbd7fb40":"Let's see what is getting a larger coefficient.","a43827bf":"With both models I need a scaler","6c774ba3":"With about one hour of work, we can reduce our cv score a little bit more.\n\nThanks to the extensive eda performed before the imputation from the documentation, we have already done 2 more incremental steps at a relatively low time investment.\n\nActually, the insights I received from that eda allow me to do a couple more quick steps.\n\n# Feature Selection from EDA\n\nI have seen already that some features make little to no sense in how they can be related to the final price. I can help the learning and the execution time of my models by simply removing them.\n\nSince I was able to make it work in a few lines of code, this time I can show you the entire process.\n\nFirst, I pick some candidates thanks to my EDA.","4b27267b":"I now create my folds for the cross-validation and define my target variable.","e56f2ccf":"I tried a couple of other things such as \"Has a Second Floor\" but they were not helping after all (which makes sense, we removed 2nfFloorSF that was giving a more accurate information because it was \"distracting\" the model).\n\nLet's roll again.","00b8a888":"Let's go again, run everything to see how better we got","79cb0851":"Only 30 minutes to make decisions, write the code, and run it to see this kind of improvement. \n\nI feel we are getting closer to the max predictive power of these 2 models but I still have something that came to my mind during the EDA and I didn't test it yet.\n\n# Simple feature engineering\n\nNothing fancy really. Again there was some try and error (in cv only, of course) involved but it regards features that didn't help after all and thus you will never see them.\n\nFirst, we have *LotFrontage: Linear feet of street connected to property* and *LotArea: Lot size in square feet** \n\nI thought, well, assuming a rectangular lot, I can have the *depth* of the lot and this can indicate how \"isolated\" the house is from the street. I put some 0's in LotFrontage during the imputation, so I will take the inverse of the depth for convenience.","040f6c41":"At this stage, spoiler alert, the ridge result is the best I was able to achieve with these models on Kaggle. Of course, the difference is at the third decimal and way within the normal fluctuations in result one may expect.\n\nOne more hour to get at this point. Let's start manipulating our features.\n\n# Transformations\n\nHere, unfortunately, I can't show the 3 scripts that led me to this decision but I can describe the process:\n\n* drop the useless features from the previous step\n* Explore the data better, for example as I did in [this kernel](https:\/\/www.kaggle.com\/lucabasa\/house-price-detailed-data-exploration)\n* Propose a transformation\n* Check if the cv result improves significantly\n* Gather all the meaningful transformations and add them one by one by checking that the cv score is going down\n\nUntil the end, I purposely never checked on Kaggle the public score because I know it would influence me.\n\nThe transformations are proposed as follows:\n\n* In an ordinal feature, use a value from 0 to, say, 5 instead of making dummies out of it. This makes sense since Excellent is bigger than Good.\n* Sometimes, the difference between Excellent and Good is not the same as the one between Good and Typical. Account for that.\n* Propose transformation to correct for sparsity.\n\nAt times, it is even better to not transform an ordinal feature and leave it as a dummy. My explanation for that, which explains also why I have 2 strategies to make them numerical, is that these features are subjective, as far as I know, they were gathered by different people and there is no parameter for defining something good or excellent.","99bce1d9":"Now I drop the missing values in the training set\n\n**Note**: if the number of problematic entries was higher, I would test several imputers inside of the pipeline right before the scaler. In this way, I would have a trustable comparison between different models. Imputing outside of the pipeline would use the information of the entire dataset and, thus, would make my cross-validation less trustable.","0d9b975b":"Quite a lot, I will just drop everything, create dummies because these models require that and obtain the following.","6bca2a7b":"Quite a few dummies are not in both datasets, I will deal with that later.","73cbb3b2":"Now we are ready to go, we will see faster models due to the fact it will learn on 185 features instead of 211.","1ee6b713":"To sum up, we spent 25 more minutes, got slightly better in cross-validation (a lot better on the leaderboard, and our mean absolute error dropped of about 35000 dollars)\n\n# Follow the documentation\n\nIf instead of dropping everything we focus on what we know from the documentation, we can use more data and get a more credible model. I talked about the topic already in this other kernel and here is the result.\n\nFirst, the documentation talks about 2 outliers to be removed.","a8a20869":"# Baseline\n\nAs a baseline, I simply use a model that takes the median of the price and uses that to predict. The following code allows me to test this idea with cross-validation.","c42181ac":"In this notebook, I want to test a few techniques to get a simple regression model to an acceptable result.\n\nThe main question I want to find an answer to is: **how much can I improve the quality of my results in a given time?**\n\nIn all fairness, one of my problems is that I usually get too curious when I look at the data and find myself in spending a lot of time for nothing. So I thought: let's at least fail very quickly.\n\nTo this end, I decided to structure the kernel as follows:\n\n* Define a baseline, an extremely simple model to confront my results with.\n* Be a butcher, clean the data in a few minutes and start from there\n* Iterate the process to find what works and what doesn't\n\nIn doing so, I will try to avoid any data leak or other bad practices (the most obvious one is using the information contained in the test set to manipulate the training set). Some of those practices may lead to a better score on the Leaderboard, but in a realistic situation will never be applicable.\n\nTo be fair towards realism, I also have to say that this is not the first time I look at these data and, therefore, my eyes and mind are not fresh as they should be for a faithful experiment. After preparing the material, I am fairly sure that my previous experiments with this dataset are influential only in terms of time saved in deciding what to do with some of the features.\n\nThe key of the entire approach is **embracing the iteration process**. If a problem emerge, get anyway till the end and then come back and solve it during the next iteration. This allows me to be more focused and to more easily keep track of what is helping my model and what isn't.\n\nIn preparing the process, I keep track of the cross-validated scores of each step and use that for decision making. However, since everybody likes to see a score on the leaderboard, I also produced a bunch of output data for each step so that I can trace back the progress on truly unseen data. These public results are not used in any way for decision making because, realistically, I won't have access to those data when I am building the model.\n\n\n# Preparation: libraries, data, functions\n\nNothing fancy, I want to use Lasso and Ridge regression because they are simple enough to tune and this will save me a lot of time.\n\nThey both work well if the data are scaled, so I will make that happen in a pipeline. If I was not doing so, my cross-validation would not be trustable since the scaling would happen by seeing the entire dataset.\n\n*In a pipeline you have fewer chances of leakage*.\n\nTo evaluate the model, I use the metric Kaggle suggests, with is the mean squared error and definitely a very appropriate one. In addition, only because it is easier to interpret, I will also have a look at the mean absolute error.\n\n**NOTE**: I wanted to use ElasticNet as well, but I kept having convergence problems. Any suggestions about why it happens are welcome.","95f190c5":"And for Ridge","c3484be6":"They won't be missed so much after all.\n\nNext, a few functions to help me for the imputation of the test. The idea is to use mean and median taken not from the entire set but rather from the similar entries.","c2e3ed71":"Now all the transformation I came up with","880b8236":"Now, I don't know which one I should drop in order to see an improvement in my cv score. So I will just test them all individually and pick only the one that gives me a better performance for, say, the Lasso regression (it is faster and I am lazy). \n\nThen, I will sequentially remove every good candidate starting from the one with the best impact and keep going until my cv score stops improving.\n\nThe entire loop took 1 minute and a half on my pc.","3a25ddf1":"And Ridge, as usual.","67a796b3":"After this operation, we are left with the following missing entries","2d3e7691":"# Full imputation\n\nWith this I mean the following:\n\n* drop columns that I know are problematic already\n* drop every entry with a missing value in the train dataset, because I don't want to reinforce existing patterns\n* since I can't do the same on the test set or Kaggle will punish me, I impute with mean or medians the missing colums","54d75b96":"Next, I simply do the following (I also flag the missing entries)","0114240b":"Not much left, I don't want to think about it anymore. Let's run this new iteration.","bcbdf45a":"In other words, we have a few more features to feed our models with. Let's do this iteration before doing anything else.","b05e9c81":"Something makes sense, other things I can't explain but let's do the ridge regression and move on","f07c9b3e":"Both the absolute mean error and the mean squared error are dropping.","b9c10d1a":"Now only the ones that work, from the most relevant (in terms of impact on the cv score) to the least one.","e960bedd":"The problem of the dummies mismatch is gone.","e7e1fe8d":"From these plots I can say the following:\n\n* they learn very similarly\n* Both have a bias around 0.1, which is about 4 times better than the baseline and I would consider it as low\n* Both models would benefit if they could learn from more data (they 2 curves have still margin to converge)\n* Ridge has more variance (the gap between the training and validation is bigger)\n\nSince I can't get my hands on more data, I could try to increase the regularization and\/or reduce the number of features (which would definitely increase the bias).\n\nLet's give them a chance with a more accurate grid search. From all the previous steps I can narrow down the parameter space and make it more granular.","bc9584ed":"Now I make the pipeline and do some very quick grid search.","82e22e78":"Then we have half and full bathrooms. I thought that I might be interested in the total number of bathrooms.","d5d715c4":"Moreover, seeing who of the candidates won the race to be dropped, I decided to drop a few more categories out of consistency and intuition","6bc2b669":"Other 30 minutes of coding and quick experiments and we got another small boost in performance.\n\nThe well of the EDA is now dry for me, I will take a moment to have a look at some learning curve to see if it triggers some idea.\n\n# Checking the learning process.\n\nLooking at a number is quick and gives some insights but it is always a good thing (at any stage really) to check how a model is learning the data.\n\nTo do so, I can look at the learning curves.\n\nThese show me how the performance on the train and test sets evolves if we vary the size of the training set. Again, I use cross-validation because it is cooler.\n\nIn terms of time, I am cheating because I made the function a while back, so it is not really fair to consider this step in this experiment.","fd819596":"At the very last, hope you have enjoyed and got some kind of inspiration (at the very least, inspired of *not* doing something) and I hope to receive some feedback from you.\n\nCheers.","baea713f":"At last, we have a lot of features regarding the outside porch in all its types. I thought it could be interesting to have a total of that."}}