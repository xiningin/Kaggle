{"cell_type":{"6b90c609":"code","71684141":"code","67545fff":"code","e6069e0e":"code","af49345c":"code","20bcefdf":"code","5bfe1d46":"code","5d903736":"code","e0cd976f":"code","e44b8165":"code","a4f39af1":"code","7941b7f9":"code","b03090a2":"code","4b0ca26c":"code","4674c3b7":"code","c18216dd":"markdown","8f2e86e4":"markdown","c1f13b0d":"markdown","5ff61b12":"markdown","fe494367":"markdown","2eb9980e":"markdown","7fe73fcb":"markdown","a125752c":"markdown","27a3cc5a":"markdown"},"source":{"6b90c609":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder,MinMaxScaler,StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\n\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","71684141":"train_df = pd.read_csv(\"..\/input\/customerattritionprediction\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/customerattritionprediction\/test.csv\")\n\nremove = True\nremove_features = ['MobileService','HardwareSupport','Married','sex']\n\ncat_features = ['Aged','TotalDependents','4GService','CyberProtection','TechnicalAssistance','FilmSubscription','SettlementProcess']\ncont_features = [\"QuarterlyPayment\", \"GrandPayment\",'ServiceSpan']\nlabel_col = [\"CustomerAttrition\"]\nid_col = [\"ID\"]\n\nif remove:\n    train_df = train_df.drop(remove_features,axis = 1)\n    test_df = test_df.drop(remove_features,axis = 1)\nelse:\n    cat_features.extend(remove_features)\n        \n\nlabel_dict = {\n    \"Yes\" : 1,\n    \"No\" : 0\n}\nN_SPLITS = 9\ntrain_df.head()","67545fff":"#cat_encoder = OneHotEncoder()\ncat_encoders = {}\ncont_encoder = MinMaxScaler()\n#cont_encoder = StandardScaler()\n\ndef preprocess_df(df, cat_features, cont_features, data = \"train\"):\n    for col in cat_features:\n        if data == \"train\":\n            cat_encoder = LabelEncoder()\n            df[col] = cat_encoder.fit_transform(df[col])\n            cat_encoders[col] = cat_encoder\n        else:\n            df[col] = cat_encoders[col].transform(df[col])\n            \n    if data == \"train\":\n        df[cont_features] = cont_encoder.fit_transform(df[cont_features])\n    else:\n        df[cont_features] = cont_encoder.transform(df[cont_features])\n    return df","e6069e0e":"train_df[label_col[0]] = train_df[label_col[0]].map(label_dict)\n\ntrain_df = preprocess_df(train_df, cat_features, cont_features, data = \"train\")\n\nlabels = train_df[label_col[0]]\ntrain_ds = train_df.drop(label_col + id_col, axis = 1)\ntot_features = list(train_ds.columns)\ncat_ids = []\n\nfor i, col in enumerate(tot_features):\n    if col in cat_features:\n        cat_ids.append(i)\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 classes = np.unique(labels.values),\n                                                 y = labels.values)\ntrain_ds.head()","af49345c":"test_df = preprocess_df(test_df, cat_features, cont_features, data = \"test\")\n\ntest_ids = test_df[\"ID\"]\ntest_ds = test_df.drop(id_col, axis = 1)\ntest_df.head()","20bcefdf":"roc_scores = np.zeros((N_SPLITS*4))\ntest_pred = {}\nnum = 0","5bfe1d46":"SEED = 143\nkfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\ncount = 0\ncb_best = 0\nfeature_imp_cb = 0\ncb_score = 0\nroc_cb_scores = np.zeros((N_SPLITS))\ncb_pred = {}\n\nfor train_idx,test_idx in kfold.split(train_ds,labels):\n    print(' Running {} of KFold {}'.format(count+1,kfold.n_splits))\n    xtr,xvl = train_ds.loc[train_idx],train_ds.loc[test_idx]\n    ytr,yvl = labels.loc[train_idx],labels.loc[test_idx]\n    \n    cb_model = CatBoostClassifier(\n                        verbose=100,\n                        eval_metric=\"Accuracy\",\n                        loss_function=\"Logloss\",\n                        random_state=2021,\n                        num_boost_round=20000,\n                        od_type=\"Iter\",\n                        od_wait=200,\n                        cat_features=cat_ids,\n                        bagging_temperature=1.288692494969795,\n                        grow_policy=\"Depthwise\",\n                        l2_leaf_reg=9.847870133539244,\n                        learning_rate=0.01877982653902465,\n                        max_depth=8,\n                        min_data_in_leaf=1,\n                        penalties_coefficient=2.1176668909602734,\n            )\n    cb_model.fit(xtr,ytr,eval_set=(xvl,yvl))\n    cb_best += cb_model.predict_proba(test_ds)\n    cb_pred[count] = cb_model.predict_proba(test_ds)\n    \n    roc_cb_scores[count] = roc_auc_score(yvl,cb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    cb_score += roc_auc_score(yvl,cb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    \n    print(\"CB - ROC AUC Score = {}\".format(roc_auc_score(yvl,cb_model.predict_proba(xvl)[:,-1], average=\"macro\")))\n    feature_imp_cb += cb_model.feature_importances_\n    \n    roc_scores[num*N_SPLITS + count] = roc_auc_score(yvl,cb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    test_pred[num*N_SPLITS + count] = cb_model.predict_proba(test_ds)\n    count+=1\n\ncb_score = cb_score\/N_SPLITS\ncb_best = cb_best\/N_SPLITS\nfeature_imp_cb = list(feature_imp_cb\/N_SPLITS)\nnum +=1","5d903736":"#Feature Importances\n\nplt.figure(figsize = (15,6))\nplt.bar(tot_features,feature_imp_cb)\nplt.title(\"Feature Importance with CatBoost\")\nplt.xlabel(\"Feature\")\nplt.xticks(rotation = 45)\nplt.ylabel(\"Importance\")\nplt.show()","e0cd976f":"count = 0\nlgb_best = 0\nfeature_imp_lgb = 0\nlgb_score = 0\nroc_lgb_scores = np.zeros((N_SPLITS))\nlgb_pred = {}\n\nfor train_idx,test_idx in kfold.split(train_ds,labels):\n    print(' Running {} of KFold {}'.format(count+1,kfold.n_splits))\n    xtr,xvl = train_ds.loc[train_idx],train_ds.loc[test_idx]\n    ytr,yvl = labels.loc[train_idx],labels.loc[test_idx]\n    \n    lgb_model = LGBMClassifier(\n               n_estimators = 1000,\n               cat_feature=cat_ids,\n               metric = \"auc\"\n    )\n    \n    lgb_model.fit(xtr,ytr,eval_set=[(xvl,yvl)], verbose = 500)\n    lgb_best += lgb_model.predict_proba(test_ds)\n    lgb_pred[count] = lgb_model.predict_proba(test_ds)\n    \n    roc_lgb_scores[count] = roc_auc_score(yvl,lgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    lgb_score += roc_auc_score(yvl,lgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    print(\"LGB - ROC AUC Score = {}\".format(roc_auc_score(yvl,lgb_model.predict_proba(xvl)[:,-1], average=\"macro\")))\n    feature_imp_lgb += lgb_model.feature_importances_\n    \n    roc_scores[num*N_SPLITS + count] = roc_auc_score(yvl,lgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    test_pred[num*N_SPLITS + count] = lgb_model.predict_proba(test_ds)\n    count+=1\n\nlgb_score = lgb_score\/N_SPLITS\nlgb_best = lgb_best\/N_SPLITS\nfeature_imp_lgb = list(feature_imp_lgb\/N_SPLITS)\nnum+=1","e44b8165":"#Feature Importances\n\nplt.figure(figsize = (15,6))\nplt.bar(tot_features,feature_imp_lgb)\nplt.title(\"Feature Importance with CatBoost\")\nplt.xlabel(\"Feature\")\nplt.xticks(rotation = 45)\nplt.ylabel(\"Importance\")\nplt.show()","a4f39af1":"count = 0\nxgb_best = 0\nfeature_imp_xgb = 0\nxgb_score = 0\nroc_xgb_scores = np.zeros((N_SPLITS))\nxgb_pred = {}\n\nfor train_idx,test_idx in kfold.split(train_ds,labels):\n    print(' Running {} of KFold {}'.format(count+1,kfold.n_splits))\n    xtr,xvl = train_ds.loc[train_idx],train_ds.loc[test_idx]\n    ytr,yvl = labels.loc[train_idx],labels.loc[test_idx]\n    \n    xgb_model = XGBClassifier(\n               use_label_encoder = False,\n               eval_metric = \"auc\"\n    )\n    xgb_model.fit(xtr,ytr,eval_set=[(xvl,yvl)], verbose = 500)\n    xgb_best += xgb_model.predict_proba(test_ds)\n    xgb_pred[count] = xgb_model.predict_proba(test_ds)\n    \n    roc_xgb_scores[count] = roc_auc_score(yvl,xgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    xgb_score += roc_auc_score(yvl,xgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    print(\"XGB - ROC AUC Score = {}\".format(roc_auc_score(yvl,xgb_model.predict_proba(xvl)[:,-1], average=\"macro\")))\n    feature_imp_xgb += np.array(list(xgb_model.get_booster().get_score(importance_type='weight').values()))\n    \n    roc_scores[num*N_SPLITS + count] = roc_auc_score(yvl,xgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    test_pred[num*N_SPLITS + count] = xgb_model.predict_proba(test_ds)\n    count+=1\n    \nxgb_score = xgb_score\/N_SPLITS\nxgb_best = xgb_best\/N_SPLITS\nfeature_imp_xgb = list(feature_imp_xgb\/N_SPLITS)\nnum+=1","7941b7f9":"#Feature Importances\n\nfeatures = list(xgb_model.get_booster().get_score(importance_type='weight').keys())\nplt.figure(figsize = (15,6))\nplt.bar(features,feature_imp_xgb)\nplt.title(\"Feature Importance with CatBoost\")\nplt.xlabel(\"Feature\")\nplt.xticks(rotation = 45)\nplt.ylabel(\"Importance\")\nplt.show()","b03090a2":"count = 0\nhgb_best = 0\nhgb_score = 0\nroc_hgb_scores = np.zeros((N_SPLITS))\nhgb_pred = {}\n\nfor train_idx,test_idx in kfold.split(train_ds,labels):\n    print(' Running {} of KFold {}'.format(count+1,kfold.n_splits))\n    xtr,xvl = train_ds.loc[train_idx],train_ds.loc[test_idx]\n    ytr,yvl = labels.loc[train_idx],labels.loc[test_idx]\n    \n    hgb_model = HistGradientBoostingClassifier(\n                    l2_regularization=1.766059063693552,\n                    learning_rate=0.10675193678150449,\n                    max_bins=128,\n                    max_depth=31,\n                    max_leaf_nodes=185,\n                    random_state=2021,\n                    verbose = 0\n            )\n    hgb_model.fit(xtr,ytr)\n    \n    roc_hgb_scores[count] = roc_auc_score(yvl,hgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    hgb_score += roc_auc_score(yvl,hgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    print(\"HGB - ROC AUC Score = {}\".format(roc_auc_score(yvl,hgb_model.predict_proba(xvl)[:,-1], average=\"macro\")))\n    hgb_best += hgb_model.predict_proba(test_ds)\n    hgb_pred[count] = hgb_model.predict_proba(test_ds)\n    \n    roc_scores[num*N_SPLITS + count] = roc_auc_score(yvl,hgb_model.predict_proba(xvl)[:,-1], average=\"macro\")\n    test_pred[num*N_SPLITS + count] = hgb_model.predict_proba(test_ds)\n    count+=1\n\nhgb_score = hgb_score\/N_SPLITS\nhgb_best = hgb_best\/N_SPLITS","4b0ca26c":"print(\"Final XGB score : \",xgb_score)\nprint(\"Final LGB score : \",lgb_score)\nprint(\"Final CB score : \",cb_score)\nprint(\"Final HGB score : \",hgb_score)","4674c3b7":"#pred_test_full = (cb_best + xgb_best + hgb_best + lgb_best) \/ 4\n#pred_test_full = cb_best\n\npred_test_full = 0\ntop = 5\nindices = np.argpartition(roc_scores, -1*top)[-1*top:]\n\nfor i in indices:\n    pred_test_full += test_pred[i]\n    print(\"Considering model with ROC: \",roc_scores[i])\npred_test_full = pred_test_full\/top\n\nlabel_dict = {\n    1 : \"Yes\" ,\n    0 : \"No\"\n}\npred = [label_dict[np.argmax(i)] for i in pred_test_full]\n\npredictions = {\n    id_col[0] : test_ids,\n    label_col[0] : pred\n}\npred_df = pd.DataFrame.from_dict(predictions)\npred_df.to_csv(\".\/submission.csv\", index = False)\nprint(pred_df[label_col[0]].value_counts())\npred_df.head()","c18216dd":"# CatBoost","8f2e86e4":"# Predictions","c1f13b0d":"# Importing Packages","5ff61b12":"# Reading Data","fe494367":"# Preprocessing Function","2eb9980e":"# Preprocessing Data","7fe73fcb":"# XGBoost","a125752c":"# LightGBM","27a3cc5a":"# Scikit-Learn"}}