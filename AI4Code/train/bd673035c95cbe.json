{"cell_type":{"44ea865f":"code","54352992":"code","cb89661a":"code","f919550d":"code","a81f5ca7":"code","3142d5c4":"code","0417ce17":"code","6ad1580b":"code","91e11777":"code","0499bbee":"code","3814ee41":"code","cd6a447b":"code","f32a15d9":"code","4323fa79":"code","9cf10e2d":"code","19d2374a":"code","e6d31607":"code","3058a08d":"code","da417d2e":"code","52f590ca":"code","e475f805":"code","09681b25":"code","a979de6e":"code","3362841d":"code","cc75e37b":"code","8273a56f":"code","22c0cdb2":"code","44d75bb8":"code","73a06927":"code","2623230b":"code","bab9e443":"code","3517dbd9":"code","837e215b":"code","b6c1e029":"code","a99063f5":"code","4e2a1870":"code","e2d516ca":"code","9dff8fb7":"code","e3a90be8":"code","83b90c56":"code","6b505981":"code","e9352afe":"code","61d688cd":"code","96b59b3a":"code","a115d56a":"code","0663a49d":"code","ebecdec2":"code","ad28a298":"code","05c2d8fe":"code","d005f07e":"code","a1e2abb9":"code","b2caaa88":"code","f87ff59b":"code","f6b1bdc5":"code","8c5215fc":"code","13d896f9":"code","ebdace99":"code","a13e280c":"markdown","1e7332c3":"markdown","c8293fb3":"markdown","8cf04757":"markdown","20d714e2":"markdown","2063fb26":"markdown","673a3956":"markdown","49e9486a":"markdown","3b6c2082":"markdown","07984106":"markdown","5d2231f5":"markdown","10d6b1f5":"markdown","12924f25":"markdown","d9923a35":"markdown","856eee75":"markdown","a94d9302":"markdown","cf02f3ed":"markdown","a980a782":"markdown","611e632d":"markdown","28810fbe":"markdown","70e20737":"markdown"},"source":{"44ea865f":"import numpy as np\nimport pandas as pd","54352992":"path = '\/kaggle\/input\/janata-data\/'","cb89661a":"path1 = '\/kaggle\/input\/covid-19-tweet-classification\/'","f919550d":"trainD = pd.read_csv(path+'train.csv')\ntestD = pd.read_csv(path+'test.csv')\ngameoverviews = pd.read_csv(path+'game_overview.csv')\ntestD.head()","a81f5ca7":"trainD = pd.read_csv(path1+'updated_train.csv')\ntestD = pd.read_csv(path1+'updated_test.csv')","3142d5c4":"gameoverviews.head()","0417ce17":"trainD.head()","6ad1580b":"bigTrain=trainD.merge(gameoverviews,how='left',on='title')","91e11777":"bigTrain.head()","0499bbee":"bigTest=testD.merge(gameoverviews,how='left',on='title')","3814ee41":"from fastai import *\nfrom fastai.text import *\nimport os","cd6a447b":"%%bash\npip install -q transformers","f32a15d9":"from pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\nfrom transformers import AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig\n","4323fa79":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","9cf10e2d":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig),\n    'albert':(AlbertForSequenceClassification,AlbertTokenizer, AlbertConfig)\n}","19d2374a":"# Parameters\nseed = 10\nuse_fp16 = True\nbs = 4\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-large'","e6d31607":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","3058a08d":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","da417d2e":"seed_all(seed)","52f590ca":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","e475f805":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","09681b25":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","a979de6e":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","3362841d":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","cc75e37b":"data_transclas = (TextList.from_df(bigTrain, cols=['title','user_review','overview','tags','developer','publisher'], processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'user_suggestion').databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","8273a56f":"data_transclas = (TextList.from_df(trainD, cols=['text'], processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'target').databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","22c0cdb2":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndata_transclas.show_batch()","44d75bb8":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = data_transclas.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","73a06927":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","2623230b":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\nprint(config)","bab9e443":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","3517dbd9":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(data_transclas, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","837e215b":"print(learner.model)","b6c1e029":"list_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n              learner.model.transformer.roberta.pooler]","a99063f5":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)","4e2a1870":"learner.freeze_to(-1)","e2d516ca":"learner.summary()","9dff8fb7":"#learner.lr_find()","e3a90be8":"#learner.recorder.plot(skip_end=10,suggestion=True)","83b90c56":"learner.fit_one_cycle(1,max_lr=1e-4,moms=(0.8,0.7))","6b505981":"#learner.save('first_cycle')","e9352afe":"#seed_all(seed)\n#learner.load('first_cycle');","61d688cd":"learner.freeze_to(-2)","96b59b3a":"# learner.lr_find()\n# learner.recorder.plot(skip_end=10,suggestion=True)","a115d56a":"lr = 1e-5","0663a49d":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","ebecdec2":"#learner.save('second_cycle')","ad28a298":"# seed_all(seed)\n# import gc\n# del learner\n# gc.collect()\n# learner = Learner(data_transclas, \n#                   custom_transformer_model, \n#                   opt_func = CustomAdamW, \n#                   metrics=[accuracy, error_rate])\n\n# # Show graph of learner stats and metrics after each epoch.\n# learner.callbacks.append(ShowGraph(learner))\n\n# # Put learn in FP16 precision mode. --> Seems to not working\n# if use_fp16: learner = learner.to_fp16()\n# learner.split(list_layers)\n# learner.load('second_cycle');\nlearner.freeze_to(-3)\nlearner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","05c2d8fe":"#learner.save('third_cycle')","d005f07e":"# seed_all(seed)\n# del learner\n# gc.collect()\n# learner = Learner(data_transclas, \n#                   custom_transformer_model, \n#                   opt_func = CustomAdamW, \n#                   metrics=[accuracy, error_rate])\n\n# # Show graph of learner stats and metrics after each epoch.\n# learner.callbacks.append(ShowGraph(learner))\n\n# # Put learn in FP16 precision mode. --> Seems to not working\n# if use_fp16: learner = learner.to_fp16()\n# learner.split(list_layers)\n# learner.load('third_cycle');","a1e2abb9":"learner.unfreeze()","b2caaa88":"learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","f87ff59b":"learner.export(file = 'transformer_robertalarge_full.pkl');","f6b1bdc5":"path = '\/kaggle\/working'\nexport_learner = load_learner(path, file = 'transformer_robertalarge_full.pkl')","8c5215fc":"from tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\nbigTest['user_suggestion']= bigTest.progress_apply(lambda x: learner.predict(x)[0], axis=1)\nbigTest[['review_id','user_suggestion']].to_csv('submission.csv',index=False)","13d896f9":"from tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\ntestD['target']= testD.progress_apply(lambda x: learner.predict(x)[2][1], axis=1)\ntestD[['ID','target']].to_csv('submission3.csv',index=False)","ebdace99":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission1.csv')","a13e280c":"# LR finder","1e7332c3":"# Repeat the same","c8293fb3":"** Creating Data Bunch **","8cf04757":"# Saving The final Model","20d714e2":"# Layer Splitting","2063fb26":"# Creating Learner Instance","673a3956":"# Setting Model Classes to try out","49e9486a":"**Setting Seed**","3b6c2082":"# JanataHack_AV_NLP Hackathon\n\n**Final Approach**\n\nHi All. I am releasing this kernel as many of us would love to see the appraoch of the winning kernels in the different competitions that adds to the great learning one gets while competing in those. If you like this please upvote the kernel. In case you have some suggestion or some feedback you would like to provide, please feel free to comment!! Enjoy :)","07984106":"** Padding Sequence ** ","5d2231f5":"# Model Fitting","10d6b1f5":"**Installing HuggingFace Transformers**","12924f25":"** Converting Text to Features**","d9923a35":"# Checking the Tokens","856eee75":"# Straight Into Transfer Learning - Roberta + FastAI","a94d9302":"# Defining The Model","cf02f3ed":"# Using Tokenizer Class","a980a782":"# Training Model Starts - Progressive Unfreezing","611e632d":"# Custom Transformer Model with attention mask (Need to understand More)","28810fbe":"# Tweaking the Configs for this use case","70e20737":"# Getting Final PRedictions"}}