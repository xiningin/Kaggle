{"cell_type":{"9e8e2362":"code","e2dce2ba":"code","1aa68257":"code","fb297b25":"code","e3c291bd":"code","542a216d":"code","2da07408":"code","d9d4ae74":"code","8c272034":"code","01bfac14":"code","4bd74b29":"code","b6b4382b":"code","83d54b7d":"code","cb3f2791":"code","725bc897":"code","0852a14a":"code","c0b19eda":"code","00f28f30":"code","47a44079":"code","b7709a22":"code","c77d38b5":"code","ed492b74":"code","4a43c691":"code","9f047e1e":"code","e874a161":"code","7feaef27":"code","57dbcc1a":"markdown","943b0700":"markdown","0249315b":"markdown","52390732":"markdown","3a269072":"markdown","ddc37188":"markdown","353911e7":"markdown","b2c37be8":"markdown","ebf00784":"markdown","5537142a":"markdown","1a745a0a":"markdown","0444d45b":"markdown","aa6bf747":"markdown"},"source":{"9e8e2362":"import pandas as pd\nimport numpy as np\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.linear_model import Lasso\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","e2dce2ba":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","1aa68257":"train_data.head()","fb297b25":"test_data.head()","e3c291bd":"print(\"The shape of the training set is:\",train_data.shape)","542a216d":"print(\"The shape of the test set is:\", test_data.shape)","2da07408":"feature_cols = [c for c in train_data.columns if c not in [\"ID\", \"target\"]]\nflat_values = train_data[feature_cols].values.flatten()\n\nlabels = 'Zero_values','Non-Zero_values'\nvalues = [sum(flat_values==0), sum(flat_values!=0)]\ncolors = ['rgba(55, 12, 233, .6)','rgba(125, 42, 123, .2)']\n\nPlot = go.Pie(labels=labels, values=values,marker=dict(colors=colors,line=dict(color='#fff', width= 3)))\nlayout = go.Layout(title='Value distribution', height=400)\nfig = go.Figure(data=[Plot], layout=layout)\niplot(fig)","d9d4ae74":"train_data.info()","8c272034":"test_data.info()","01bfac14":"train_data.describe()","4bd74b29":"def missing_data(data): #calculates missing values in each column\n    total = data.isnull().sum().reset_index()\n    total.columns  = ['Feature_Name','Missing_value']\n    total_val = total[total['Missing_value']>0]\n    total_val = total.sort_values(by ='Missing_value')\n    return total_val","b6b4382b":"missing_data(train_data).head()","83d54b7d":"missing_data(test_data).head()\n","cb3f2791":"sns.distplot(np.log1p(train_data['target']))","725bc897":"X_train = train_data.drop(['ID','target'],axis=1)\ny_train = np.log1p(train_data[\"target\"])\nX_test = test_data.drop('ID', axis = 1)","0852a14a":"#X = X_train.copy()\n#Xa = X_test.copy()\n#clf = IsolationForest(max_samples=100, random_state= 0)\n#clf.fit(X)","c0b19eda":"#y_pred = clf.predict(X)\n#y_pred_df = pd.DataFrame(data=y_pred,columns = ['Values'])\n#y_pred_df['Values'].value_counts()","00f28f30":"#anomaly_score = clf.decision_function(X_train)\n#anomaly_score","47a44079":"#y_test_pred = clf.predict(Xa)\n#y_test_pred_df = pd.DataFrame(data=y_test_pred,columns = ['Out_Values'])\n#y_test_pred_df['Out_Values'].value_counts()","b7709a22":"#anomaly_score = clf.decision_function(X_test)\n#anomaly_score","c77d38b5":"feat = SelectKBest(mutual_info_regression,k=200)","ed492b74":"X_tr = feat.fit_transform(X_train,y_train)\nX_te = feat.transform(X_test)","4a43c691":"tr_data = scaler.fit_transform(X_tr)\nte_data = scaler.fit_transform(X_te)\nreg = Lasso(alpha=0.0000001, max_iter = 10000)","9f047e1e":"reg.fit(tr_data,y_train)","e874a161":"y_pred = reg.predict(te_data)\ny_pred","7feaef27":"sub = pd.read_csv('..\/input\/sample_submission.csv')\n#y_pred = np.clip(y_pred,y_train.min(),y_train.max())\nsub[\"target\"] = np.expm1(y_pred)\nprint(sub.head())\nsub.to_csv('sub_las.csv', index=False)\n","57dbcc1a":"# Importing modules and getting a glimpse of the data","943b0700":"The trend of anonymized data for online competitions is increasing day by day as companies want their data to be secure and thus maintaining the privacy of their customers. Santander has released an anonymized dataset for predicting the value of transactions for each potential customer.\n\nSo in this notebook I will be focusing on gathering insights from the unknown data and selecting appropriate subset of features.","0249315b":"## Histogram view of the log transformed dependent quantitative variable","52390732":"## Missing Data","3a269072":" ## Modelling with Lasso","ddc37188":"The memory usage of the data is approx 170MB and the datatypes for features are distributed as:\n- **float64** - 1845\n-   **int64** - 3147\n-  **object** - 1","353911e7":"The memory usage for test data is 1.8GB and the datatypes for features are:\n\n- **float64** - 4991\n-  **object** - 1","b2c37be8":"Feature Selection is an essential part of feature engineering. The method I have used here is a univariate feature selection method in which there is a scoring function and a    selection method.\n\n- Mutual_info_regression is used as a scoring function which calculates the mutual information between each feature and the target by estimating the entropy from K-Nearest   Neighbors. Mutual Information is the measure of dependency between two random variables which is a non-negative value. It is equal to zero if two variables are independent and higher value means higher dependency.\n\n- SelectKBest is the univariate selection method which takes scoring function as an input that returns univariate scores and p values. It removes all the features except the K-highest scoring features.","ebf00784":"# Outlier detection using Isolation Forest ","5537142a":"There are no missing values in the train and test dataset. This is reasonably good as it is nearly impossible to fill missing data with certain values. ","1a745a0a":"## Feature selection using Mutual Information & SelectKBest method","0444d45b":"Outlier detection is one of the most important aspects of regression analysis. If not removed it can hamper the performance of the model which we will fit to the data for continuous value prediction. So I have used a method which is highly suitable for high dimensional datasets i.e. Isolation forest algorithm, an ensemble method which returns anomaly scores of each sample in the dataset.\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\n+1 indicates that the sample is an inlier whereas -1 indicates that the sample is an outlier.","aa6bf747":"- It is quiet interesting to see that the number of features in the train dataset is greater than the number of data points i.e. **the curse of dimensionality**.\n- The test set is 10 times bigger than the train set in shape.\n- Thus, feature extraction is very important and will substantially improve the score of the model."}}