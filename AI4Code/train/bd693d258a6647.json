{"cell_type":{"6c8976e2":"code","2af81184":"code","57d022ab":"code","37208cb8":"code","b49df90c":"markdown","c3cf8785":"markdown","2756a387":"markdown","a6d0fc98":"markdown","86f2aaa3":"markdown"},"source":{"6c8976e2":"# Imports\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn import preprocessing\nimport traceback\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n# Data Loading\ndataset = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndataset.columns = [x.strip() for x in dataset.columns]","2af81184":"plot_df = dataset\nplot_df['Color'] = plot_df['SARS-Cov-2 exam result'].apply(lambda x: 2 if x == 'positive' else 1)\nfeat_to_plot = [\n    'Hematocrit',\n    'Hemoglobin',\n    'Platelets',\n    'Mean platelet volume', \n    'Red blood Cells',\n    'Lymphocytes',\n    'Mean corpuscular hemoglobin concentration\u00a0(MCHC)',\n    'Leukocytes',\n    'Basophils',\n    'Mean corpuscular hemoglobin (MCH)',\n    'Eosinophils',\n    'Mean corpuscular volume (MCV)',\n    'Monocytes',\n    'Red blood cell distribution width (RDW)']\n#for i in range(len(feat_to_plot)):\n#    for j in range(i+1, len(feat_to_plot)):\n#        fig, ax = plt.subplots()\n#        plot_df.plot.scatter(x=feat_to_plot[j], y=feat_to_plot[i], c='Color', colormap='coolwarm', ax=ax)\n#        plt.axhline(y=np.mean(plot_df[plot_df['Color']==2][feat_to_plot[i]]), color='r', linewidth=1)\n#        plt.axhline(y=np.mean(plot_df[plot_df['Color']==1][feat_to_plot[i]]), color='b', linewidth=1)\n#        plt.axvline(x=np.mean(plot_df[plot_df['Color']==2][feat_to_plot[j]]), color='r', linewidth=1)\n#        plt.axvline(x=np.mean(plot_df[plot_df['Color']==1][feat_to_plot[j]]), color='b', linewidth=1)\n\n# Reducing the number of plots\ncombs = [\n    ['Eosinophils','Leukocytes'],\n    ['Platelets','Leukocytes'],\n    ['Eosinophils','Platelets'],\n]\nfor c in combs:\n    x, y = c\n    fig, ax = plt.subplots()\n    plot_df.plot.scatter(x=x, y=y, c='Color', colormap='coolwarm', ax=ax)\n    plt.axhline(y=np.mean(plot_df[plot_df['Color']==2][x]), color='r', linewidth=1)\n    plt.axhline(y=np.mean(plot_df[plot_df['Color']==1][x]), color='b', linewidth=1)\n    plt.axvline(x=np.mean(plot_df[plot_df['Color']==2][y]), color='r', linewidth=1)\n    plt.axvline(x=np.mean(plot_df[plot_df['Color']==1][y]), color='b', linewidth=1)","57d022ab":"# Fixing pH column\ndataset['Urine - pH'] = dataset['Urine - pH'].apply(lambda x: x if type(x) == float else np.nan)\n\n# Data Frame with results\nresults = pd.DataFrame({\n    'Group':[],\n    'TotalAcc':[],\n    'Acc.Positive':[],\n    'Acc.Negative':[],\n    'Acc.Weighted':[],\n    'Train.Rows':[],\n    'Test.Rows':[],\n    'Method':[],\n    'Features':[]\n})\n\ncoefficients = pd.DataFrame({\n    'Experiment': [],\n    'Group': [],\n    'Attribute': [],\n    'Coefficient': [],\n    'Coefficient (abs)': []\n})\n\nexperiment_number = 1\n\n# Tests seem to be done in batch (several tests are done together), so here I'm analyzing with respect to a given group\ngroups = ['Parainfluenza 1','Platelets','Influenza B, rapid test','Influenza B']\nfor g in groups:\n    # Filtering per group\n    pre_prep = dataset[~dataset[g].isnull()]\n    \n    # Dropping missing data\n    pre_prep = pre_prep.dropna(axis=1, thresh=len(pre_prep)*0.7) # Require at least 50% data in each column\n    \n    # Generating dummies for categorical variables\n    obj_only = pre_prep.select_dtypes(include=['object']).set_index('Patient ID')\n    num_only = pre_prep.select_dtypes(exclude=['object'])\n    num_only.loc[:,'Patient ID'] = dataset['Patient ID']\n    num_only = num_only.set_index('Patient ID')\n    prepared = pd.get_dummies(obj_only, columns=set(obj_only)-{'SARS-Cov-2 exam result','Patient ID'}, dummy_na=False).join(num_only)\n\n    prepared = prepared.drop(columns=['Color']).reset_index(drop=True)\n    \n    # Encoding Labels\n    le = preprocessing.LabelEncoder()\n    prepared.loc[:,'Label'] = le.fit_transform(prepared['SARS-Cov-2 exam result'])\n\n    # Dropping label column for ease of work afterwards\n    prepared = prepared.drop(columns=['SARS-Cov-2 exam result']).reset_index(drop=True)\n    labels = prepared['Label'].copy().reset_index(drop=True)\n    prepared = prepared.drop(columns=['Label']).reset_index(drop=True)\n\n    # Imputing missing data\n    my_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n    prepared = pd.DataFrame(my_imputer.fit_transform(prepared), columns=prepared.columns)\n    prepared\n\n    features = prepared.columns\n\n    # Model Search\n    best_model = None\n\n    try:\n\n        X = prepared\n        y = labels\n        \n        # Running each experiment 3 times\n        for k in range(3):\n            # Train-test split\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n\n            # Oversampling minority class\n            sm = SMOTE(sampling_strategy='all')\n            X_train, y_train = sm.fit_resample(X_train, y_train)\n\n            # Scaling data\n            scaler = preprocessing.StandardScaler().fit(X_train)\n            X_train = scaler.transform(X_train)\n            X_test = scaler.transform(X_test)\n\n            # Getting indices of each class\n            idx_positive = y_test == (le.transform(['positive'])[0])\n            idx_negative = y_test == (le.transform(['negative'])[0])\n\n            # Decision tree should be the simplest method to evaluate\n            for method in [tree.DecisionTreeClassifier(class_weight='balanced'), \n                           LogisticRegression(class_weight='balanced', max_iter=500), \n                           RandomForestClassifier(class_weight='balanced'),\n                           svm.SVC(class_weight='balanced', kernel='linear',C=0.5),\n                           KNeighborsClassifier(n_neighbors=3)]:\n                model = method.fit(X_train, y_train)\n                \n                # Calculating accuracy in each class\n                acc_pos = accuracy_score(y_test[idx_positive], model.predict(X_test[idx_positive]))\n                acc_neg = accuracy_score(y_test[idx_negative], model.predict(X_test[idx_negative]))\n                total_acc = accuracy_score(y_test, model.predict(X_test))\n                w_acc = (acc_pos+acc_neg)\/2\n\n                if method.__class__.__name__ == 'LogisticRegression':\n                    best_model = model\n                    coefficients = coefficients.append(pd.DataFrame({\n                        'Experiment': experiment_number,\n                        'Group': g,\n                        'Attribute': X.columns,\n                        'Coefficient': model.coef_[0],\n                        'Coefficient (abs)': abs(model.coef_[0])\n                    }), ignore_index=True)\n                    experiment_number += 1\n\n                # Calculating accuracy on each class\n                results = results.append({\n                    'Group': g,\n                    'TotalAcc': total_acc,\n                    'Acc.Positive': acc_pos,\n                    'Acc.Negative': acc_neg,\n                    'Acc.Weighted': w_acc,\n                    'Train.Rows': len(y_train),\n                    'Test.Rows': len(y_test),\n                    'Method': method.__class__.__name__,\n                    'Features': \" | \".join(X.columns)\n                }, ignore_index=True)\n                \n                \n\n    except Exception as e:\n        traceback.print_exc()\n\nresults.groupby(['Group','Method','Features']).mean().reset_index().sort_values(by='Acc.Weighted', ascending=False)","37208cb8":"coefficients.sort_values(by=['Group', 'Experiment', 'Coefficient (abs)'], ascending=False)[0:50]","b49df90c":"# Data Preparation\n**Categorical Data:** Converting non-numerical data to numerical.\n\n**Missing data:** This dataset has a lot of missing data, so I'm imputing with K-NN Imputer. Imputing data is not exactly good when there's too much missing data, so I'm dividing the analysis based on the available tests.\nFrom the data we see several blocks of results, so I'm analyzing groups of tests. For instance: blood tests always report Red Blood Cells, Lymphocytes, Platelets, etc. This grants that we'll have as much actual data as possible, so we only impute the other results.\n\n**Imbalancing:** As the dataset is pretty imbalanced, I'm applying SMOTE and applying weights to the models.\n\n# Model Search\nSince we have groups of tests and a lot of missing data in the other columns, the best is to find a model that fits best for the available tests in each case. Of course I didn't test all the possibilities, but found primarily 4 groups to model:\n- Parainfluenza 1\n- Platelets\n- Influenza B, rapid test\n- Influenza B\n\nThese group names refer to one of the tests taken in each group.","c3cf8785":"# Coefficient analysis\nAs all methods have shown similar performance, I've chosen linear regression coefficient analysis as it's more straightforward.\n\n**Interpretation:** the highest the absolute value (magnitude) the more important to the prediction. Negative values depict inverse correlation with the output value. As our model encodes negative=0 and positive=1, negative coefficients mean that the sick people have **less** of that attribute. As we're looking at different experiments within different groups, the analysis should not mix different groups","2756a387":"# Notes on beforehand\nI've analyzed this data on Excel by simply calculating the averages per attribute on each class. The very first thing I've noticed is that there were differences in those averages, meaning that there we should have 'centers' for each class.\n\n** The other versions of this notebook contained a substantial issue of self-fulfilling data preparation. Results don't compare. **","a6d0fc98":"# Conclusions\nSo far, the most efficient way to detect COVID-19 is by blood tests: they top the ranking of results with around 80% of weighted average. Leukocytes and Eosinophils counts are usually lower than usual in patients diagnosed with COVID-19. This is quite clear at the Leukocyte x Eosinophils plot and gets more evident when we analyze the coefficients of the experiments. It's also noteworthy how many times 'Patient admitted to regular ward' appeared as one of the top attributes. \n\nThe other groups of tests often perform greatly in one class, but poorly in the other, so the weighted accuracy is fine, but one of the classes is heavily penalized. The 'blood tests group' show an interesting classification power for the given sample.","86f2aaa3":"# Preliminary Analysis\nBy plotting correlations, it gets easily clear that Platelets, Leukocytes and Eosinophils are the real deal: they form pretty concise groups of red dots."}}