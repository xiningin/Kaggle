{"cell_type":{"6c5a4197":"code","12b23fa0":"code","0e818a85":"code","f6163776":"code","9507d2f1":"code","fea6ceee":"code","b1b99562":"code","80073d9d":"code","3e015c91":"code","64fae16e":"code","97d70993":"code","f2c7a630":"code","b4cf74a8":"code","83f1a3fd":"code","a3a48938":"code","b8d5c29f":"code","73804a49":"code","db4cc89a":"code","77379fa5":"code","0a25a84e":"code","68b13f27":"code","41a65510":"code","414576b6":"code","ac8b6b71":"code","28214bc5":"code","9130af46":"code","52aeffef":"code","d7462e6d":"code","81636480":"code","cc1fd02c":"code","8ca13c44":"code","45c35ec6":"code","b29413e2":"code","5a3026d9":"code","16b8457c":"code","eabbf86e":"code","65e6aa33":"code","71445dba":"code","5e6d582b":"code","d7f7ddf5":"code","bb929e5b":"code","0afb0aa0":"code","8b6d6710":"code","6041e32c":"code","97d2a952":"code","5d2f9444":"code","92d2904a":"code","9e6e8997":"code","b4a63baf":"code","301c47ae":"code","340642cf":"code","ffba97b4":"code","a37f91bd":"code","27b772d4":"code","1b71ff01":"code","30a1615b":"code","52bc7d0c":"code","c1393b7e":"code","fb98b821":"markdown","b2bd6b72":"markdown","c12b4f99":"markdown","262a7028":"markdown","5f184a2e":"markdown","e9a74830":"markdown","64e3e836":"markdown","f51cf535":"markdown"},"source":{"6c5a4197":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12b23fa0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings; warnings.simplefilter('ignore')\n\npd.set_option('display.max_rows',5000)\npd.set_option('display.max_columns',5000)\n","0e818a85":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","f6163776":"df=train.copy()","9507d2f1":"df.head()","fea6ceee":"# List of variables that contain year information\nyear_feature = [feature for feature in df if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","b1b99562":"continous_feature = [feature for feature in df if len(df[feature].unique()) > 80 and feature not in year_feature + ['Id']]\n\nprint('Number of train continous feature : ',len(continous_feature))  \ncontinous_feature","80073d9d":"train_continous_feature = df[continous_feature]\ntrain_continous_feature.head()","3e015c91":"train_categorical_features = df.select_dtypes(exclude=[np.number])\ntrain_categorical_features.head()","64fae16e":"train_year_feature = df[year_feature]\ntrain_year_feature.head()","97d70993":"df_numerical_features = df.select_dtypes(include=[np.number])\n# Numerical variables are usually of 2 type\n# 1. Continous variable and Discrete Variables\n\ndiscrete_feature = [feature for feature in df_numerical_features if len(df_numerical_features[feature].unique()) < 80 \n                                                                     and feature not in year_feature + ['Id']]\n\nprint('Number of discrete features : ',len(discrete_feature))  \ndiscrete_feature","f2c7a630":"train_discrete_feature = df[discrete_feature]\ntrain_discrete_feature.head()","b4cf74a8":"#step1 divide data based on the types of the  data\nprint('train_categorical_features:',train_categorical_features.shape),\nprint('train_discrete_feature:',train_discrete_feature.shape),\nprint('train_continous_feature:',train_continous_feature.shape),\nprint('train_year_feature:',train_year_feature.shape)","83f1a3fd":"train_continous_feature_nan = train_continous_feature.isnull().sum()\ntrain_continous_feature_nan=train_continous_feature_nan[train_continous_feature_nan>0]\n\ntrain_discrete_feature_nan = train_discrete_feature.isnull().sum()\ntrain_discrete_feature_nan=train_discrete_feature_nan[train_discrete_feature_nan>0]\n\ntrain_categorical_features_nan = train_categorical_features.isnull().sum()\ntrain_categorical_features_nan=train_categorical_features_nan[train_categorical_features_nan>0]\n\ntrain_year_feature_nan = train_year_feature.isnull().sum()\ntrain_year_feature_nan=train_year_feature_nan[train_year_feature_nan>0]\n\nprint('train_continous_feature_nan:',\n      train_continous_feature_nan.sort_values(ascending = False))\n\nprint('train_discrete_feature_nan:',\n      train_discrete_feature_nan.sort_values(ascending = False))\n\nprint('train_categorical_features_nan:',\n      train_categorical_features_nan.sort_values(ascending = False))\n\nprint('train_year_feature_nan:',\n      train_year_feature_nan.sort_values(ascending = False))","a3a48938":"#fill train_continous_feature NAN values with mean\ntrain_continous_feature['LotFrontage']=train_continous_feature['LotFrontage'].fillna(train_continous_feature['LotFrontage'].mean())\ntrain_continous_feature['MasVnrArea']=train_continous_feature['MasVnrArea'].fillna(train_continous_feature['MasVnrArea'].mean())","b8d5c29f":"train_continous_feature.head()","73804a49":"#categorical fetures with more NAN values\ntrain_categorical_features.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis=1,inplace=True)\n","db4cc89a":"train_categorical_features['GarageCond']=train_categorical_features['GarageCond'].fillna(train_categorical_features['GarageCond'].mode()[0])\ntrain_categorical_features['GarageQual']=train_categorical_features['GarageQual'].fillna(train_categorical_features['GarageQual'].mode()[0])\ntrain_categorical_features['GarageFinish']=train_categorical_features['GarageFinish'].fillna(train_categorical_features['GarageFinish'].mode()[0])\ntrain_categorical_features['GarageType']=train_categorical_features['GarageType'].fillna(train_categorical_features['GarageType'].mode()[0])\ntrain_categorical_features['BsmtFinType2']=train_categorical_features['BsmtFinType2'].fillna(train_categorical_features['BsmtFinType2'].mode()[0])\ntrain_categorical_features['BsmtExposure']=train_categorical_features['BsmtExposure'].fillna(train_categorical_features['BsmtExposure'].mode()[0])\ntrain_categorical_features['BsmtFinType1']=train_categorical_features['BsmtFinType1'].fillna(train_categorical_features['BsmtFinType1'].mode()[0])\ntrain_categorical_features['BsmtCond']=train_categorical_features['BsmtCond'].fillna(train_categorical_features['BsmtCond'].mode()[0])\ntrain_categorical_features['BsmtQual']=train_categorical_features['BsmtQual'].fillna(train_categorical_features['BsmtQual'].mode()[0])\ntrain_categorical_features['MasVnrType']=train_categorical_features['MasVnrType'].fillna(train_categorical_features['MasVnrType'].mode()[0])\ntrain_categorical_features['Electrical']=train_categorical_features['Electrical'].fillna(train_categorical_features['Electrical'].mode()[0])","77379fa5":"#by using manual check of year data\ntrain_year_feature['GarageYrBlt']=train_year_feature['GarageYrBlt'].fillna(1980)","0a25a84e":"train_continous_feature_nan = train_continous_feature.isnull().sum()\ntrain_continous_feature_nan=train_continous_feature_nan[train_continous_feature_nan>0]\n\ntrain_discrete_feature_nan = train_discrete_feature.isnull().sum()\ntrain_discrete_feature_nan=train_discrete_feature_nan[train_discrete_feature_nan>0]\n\ntrain_categorical_features_nan = train_categorical_features.isnull().sum()\ntrain_categorical_features_nan=train_categorical_features_nan[train_categorical_features_nan>0]\n\ntrain_year_feature_nan = train_year_feature.isnull().sum()\ntrain_year_feature_nan=train_year_feature_nan[train_year_feature_nan>0]\n\nprint('train_continous_feature_nan:',\n      train_continous_feature_nan.sort_values(ascending = False))\n\nprint('train_discrete_feature_nan:',\n      train_discrete_feature_nan.sort_values(ascending = False))\n\nprint('train_categorical_features_nan:',\n      train_categorical_features_nan.sort_values(ascending = False))\n\nprint('train_year_feature_nan:',\n      train_year_feature_nan.sort_values(ascending = False))","68b13f27":"# Temporal Variables (Date Time Variables)\n# Basically we are capturing the difference of years here\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    train_year_feature[feature] = train_year_feature['YrSold'] - train_year_feature[feature]\ntrain_year_feature.head()","41a65510":"categorical_features = [feature for feature in train_categorical_features.columns if train_categorical_features[feature].dtype == 'O']\ncategorical_features","414576b6":"train_categorical_features1 = pd.concat([train_categorical_features,df[['SalePrice']]], axis=1)\ntrain_categorical_features1.head()","ac8b6b71":"for feature in categorical_features:\n    temp = train_categorical_features1.groupby(feature)['SalePrice'].count()\/len(train_categorical_features1)\n    train_categorical_features2 = temp[temp > 0.01].index\n    train_categorical_features1[feature] = np.where(train_categorical_features1[feature].isin(train_categorical_features2), train_categorical_features1[feature], 'Rare_Var')","28214bc5":"train_categorical_features1.head()","9130af46":"# Let's map the categories to some specific values \nfor feature in categorical_features:\n    labels_ordered = train_categorical_features1.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered = {k:i for i,k in enumerate(labels_ordered)}\n    train_categorical_features1[feature] = train_categorical_features1[feature].map(labels_ordered)","52aeffef":"train_categorical_features1.head()","d7462e6d":"train_categorical_features1.drop(['SalePrice'],axis=1,inplace=True)\n","81636480":"train_categorical_features1.head()","cc1fd02c":"final_df = pd.concat([train_year_feature,train_categorical_features1,train_categorical_features1,train_continous_feature], axis=1)\nfinal_df.head()","8ca13c44":"# Creating X_train and y_train \nX_train = final_df.drop(['SalePrice'], axis = 1)\ny_train = final_df['SalePrice']","45c35ec6":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso","b29413e2":"model_sel_feature = SelectFromModel(Lasso(alpha = 0.005, random_state = 49))\nmodel_sel_feature.fit(X_train,y_train)","5a3026d9":"# get_support() will show an array of boolean values i.e. which features are selected and which are not\nmodel_sel_feature.get_support()","16b8457c":"selected_feat = X_train.columns[model_sel_feature.get_support()]\n\n# Let's print some stats\nprint(f\"Total Features : {len(X_train.columns)}\")\nprint(f\"Features Selected : {len(selected_feat)}\")\nprint(f\"features with coefficients shrank to zero: {np.sum(model_sel_feature.estimator_.coef_ == 0)}\")","eabbf86e":"selected_feat","65e6aa33":"X_train = X_train[selected_feat]","71445dba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, test_size=.33)\n\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()\n\nmodel = lr.fit(X_train, y_train)\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\npredictions = model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","5e6d582b":"y=final_df['SalePrice']\nX=final_df.drop(['SalePrice'],axis=1)","d7f7ddf5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()\n\nmodel = lr.fit(X_train, y_train)\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\npredictions = model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","bb929e5b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\nfrom sklearn import tree\nclf = tree.DecisionTreeRegressor()\nmodel = clf.fit(X_train, y_train)\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\npredictions = model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Decission tree Model')\nplt.show()","0afb0aa0":"y=final_df['SalePrice']\nX=final_df.drop(['SalePrice'],axis=1)","8b6d6710":"### Feature Importance\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","6041e32c":"print(model.feature_importances_)","97d2a952":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","5d2f9444":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor=RandomForestRegressor()\n\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nprint(n_estimators)\n\nfrom sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","92d2904a":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","9e6e8997":"\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()","b4a63baf":"# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","301c47ae":"rf_random.fit(X_train,y_train)","340642cf":"rf_random.best_params_","ffba97b4":"rf_random.best_score_","a37f91bd":"predictions=rf_random.predict(X_test)","27b772d4":"sns.distplot(y_test-predictions)","1b71ff01":"plt.scatter(y_test,predictions)\n","30a1615b":"from sklearn import metrics","52bc7d0c":"\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","c1393b7e":"\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('random forest  Model')\nplt.show()","fb98b821":"2.divide data into categorical, numerical continous, numerical descrete,year","b2bd6b72":"1. Import required Libraries.","c12b4f99":"7.Test model accuracy","262a7028":"Handling Rare Categorical Feature\nWe will remove categorical variables that are present less than 1% of the observations","5f184a2e":"5.split train&test","e9a74830":"3.a.Hadling outliers and  Missing Values","64e3e836":"Objective:\n      Predict house price by usnig given features.   \n\nPipeline:\n 1. Import required Libraries.\n 2. Import data\n 3. Divide data by based on data type.\n 4. Feature Engineering\n     a.Handle Outliers and Missing values.\n     b.Select features or Create new features by using co-relation.\n 5. Combine divided data.\n 6. Split data into Train&Test.\n 7. Choose Model and test accuracy.\n ","f51cf535":"4.combine divided features"}}