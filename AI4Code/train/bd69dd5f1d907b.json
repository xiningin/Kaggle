{"cell_type":{"6a2fcf78":"code","4bfce9f5":"code","a6338451":"code","54925310":"code","c8e524de":"code","9b89f2cc":"code","a741b886":"code","870010d5":"code","a02b4a81":"code","90283c8a":"code","b5a576d7":"code","bf61d783":"code","099129e6":"code","c2670fb3":"code","4222f590":"code","adc82897":"code","8ec3d27f":"code","3d86a556":"code","cada71e8":"code","85d2ed1f":"code","80ca78ae":"code","38bfc0a2":"code","0d778b73":"code","83292fd6":"code","72ecd0ba":"code","151d9ffb":"code","a83880e0":"code","1401d1f1":"code","9929d5d2":"code","d4c61fc3":"code","b43bbde2":"code","25c9cf0e":"code","8b060287":"code","f784fbc1":"code","7af3e80a":"code","8cf9cb94":"code","dbf606a3":"code","c4d97e51":"code","6a130797":"code","91969c09":"code","cab0ff29":"code","712308b3":"code","0b426893":"code","6fff0c08":"code","0cd088e3":"code","84272a73":"code","d06b2450":"code","398cb833":"code","f7a18d1d":"code","7acd52a6":"code","5ec5d7bb":"code","43b23d80":"code","cb1df3aa":"code","ade9e719":"code","d156f49b":"code","2525376d":"code","3fb359a9":"code","49807fa0":"code","f7cb0d1f":"code","9b5dec13":"code","73d18213":"code","27e6fc22":"code","999bb374":"code","7e309ebc":"code","c6b148f0":"code","e6dff350":"code","856a9537":"code","595a8f0c":"markdown","147318b1":"markdown","8e5b2d71":"markdown","f25bc7f8":"markdown","24b20aee":"markdown","aa0c0bab":"markdown","9ac83962":"markdown","cd95a7f2":"markdown","0d37894c":"markdown","481cc015":"markdown","f6b2b9f1":"markdown","c38416bb":"markdown","095fc2dd":"markdown","ce68c696":"markdown","6b29167b":"markdown","d7d57bc9":"markdown","fbc53488":"markdown","f0318404":"markdown","b0de2c66":"markdown","356ac18c":"markdown","5412f135":"markdown","9b46d73a":"markdown","faa16b81":"markdown","bbdf214b":"markdown","0a19b64b":"markdown","8027e824":"markdown","05e254b9":"markdown","05d3aaf5":"markdown","a8215b0a":"markdown","965d619b":"markdown","0968255a":"markdown","3f07ae78":"markdown","3acdc7f9":"markdown","f9285ffe":"markdown","96381cce":"markdown","7551f796":"markdown"},"source":{"6a2fcf78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#format columns\/rows\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.4f}'.format \n# Hide system warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \n# Any results you write to the current directory are saved as output.","4bfce9f5":"test = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\ntest.shape,train.shape","a6338451":"train.info()","54925310":"train.describe(include = 'all')","c8e524de":"train.head()","9b89f2cc":"#training set missing most cabin and some age\nsns.heatmap(train.isnull())\nplt.title('Missing Train Data \\n')","a741b886":"#Also some embarked\ntrain.isnull().sum()","870010d5":"#test looks to be the same\nsns.heatmap(test.isnull(),cmap = 'coolwarm')\nplt.title('Missing Test Data \\n')","a02b4a81":"#No embarked, but 1 fare missing\ntest.isnull().sum()","90283c8a":"#Deeper analysis needed for cabin since it's above 40% missing in both data sets\nprint('Train','\\n',(train.isnull().sum()\/len(train))*100)\nprint('-'*20)\nprint('Test','\\n',(test.isnull().sum()\/len(test))*100)","b5a576d7":"#Looks to be different ages by Pclass\nsns.boxplot(x='Pclass',y='Age',data=train)\nplt.title('Age by Class \\n')","bf61d783":"#confirmed use these to input missing ages\ntrain.groupby(['Pclass'], as_index = False)['Age'].median()","099129e6":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]  \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","c2670fb3":"train['Age'] = train[['Age','Pclass']].apply(lambda x: impute_age(x), axis = 1)\ntest['Age'] = test[['Age','Pclass']].apply(lambda x: impute_age(x), axis = 1)","4222f590":"#Imput S since it is the mode\ntrain['Embarked'].value_counts()","adc82897":"train['Embarked'] = np.where(~train['Embarked'].isnull(),train['Embarked'],'S')","8ec3d27f":"#Use median since Fare is so skewed\nsns.distplot(test['Fare'],bins=50)\nplt.title('Distribution of Fare \\n')\n","3d86a556":"#use median \nround(test['Fare'].mean(),4),test['Fare'].median()","cada71e8":"test['Fare'] = np.where(~test['Fare'].isnull(),test['Fare'],test['Fare'].median())","85d2ed1f":"#Resolved all missing except Cabin\nprint('Train','\\n',(train.isnull().sum()\/len(train))*100)\nprint('-'*20)\nprint('Test','\\n',(test.isnull().sum()\/len(test))*100)","80ca78ae":"#Cabin Analysis - read some mention a cabin known variable was helpful - seems if you're cabin was known you were more likely to survive\ntrain['Cabinknown'] = 0\ntrain['Cabinknown'] = np.where((train['Cabin'].isnull()),train['Cabinknown'],1)\ntest['Cabinknown'] = 0\ntest['Cabinknown'] = np.where((test['Cabin'].isnull()),test['Cabinknown'],1)\n\ntrain[['Cabinknown','Survived']].groupby('Cabinknown').mean()","38bfc0a2":"#Also read that pulling the cabin letter may be helpful as well\n#anything you can pull from Cabin - yes, use the letter\nc = train[~train['Cabin'].isnull()]['Cabin']\nc.head()","0d778b73":"train['Cabinletter'] = train['Cabin'].str[0]\ntest['Cabinletter'] = test['Cabin'].str[0]","83292fd6":"#From my MOOC, I remember grabbing title was a good predictor so let's do that\ntrain['Title'] = train['Name'].str.split(r'\\s*,\\s*|\\s*\\.\\s*').str[1]\ntest['Title'] = test['Name'].str.split(r'\\s*,\\s*|\\s*\\.\\s*').str[1]","72ecd0ba":"#From previous analysis, I identified that SibSp and Parch are weak predictors individually and have similar distributions so I am combining\ntrain['Family'] = train['SibSp'] + train['Parch'] + 1\ntest['Family'] = test['SibSp'] + test['Parch'] + 1","151d9ffb":"#looks like kids and elderly survived at a higher rate than did others\nplt.figure(figsize=(10,6)) \ntrain[train['Survived'] ==0]['Age'].hist(alpha=.7, bins=30, label='Survived =0') \ntrain[train['Survived'] ==1]['Age'].hist(alpha=.7, bins=30, label='Survived =1') \nplt.legend(loc='upper right') \nplt.title('Age Survival \\n')\nplt.show() ","a83880e0":"train['Agegroup'] = np.where(train['Age']<17,0,(np.where((train['Age']>17) & (train['Age']<55),1,2)))\ntest['Agegroup'] = np.where(test['Age']<17,0,(np.where((test['Age']>17) & (test['Age']<55),1,2)))","1401d1f1":"round(train['Fare'].mean(),4),train['Fare'].median()","9929d5d2":"#Looks like those who paid more survived at a higher rate\nplt.figure(figsize=(10,6)) \ntrain[train['Survived'] ==0]['Fare'].hist(alpha=.7, bins=30, label='Survived =0') \ntrain[train['Survived'] ==1]['Fare'].hist(alpha=.7, bins=50, label='Survived =1') \nplt.legend(loc='upper right') \nplt.title('Fare Survival \\n')\nplt.show() ","d4c61fc3":"train['Faregroup'] = np.where(train['Fare']<15,0,1)\ntest['Faregroup'] = np.where(test['Fare']<15,0,1)","b43bbde2":"#Looks like traditional families had a higher survival rate than individuals or large families \nplt.figure(figsize=(10,6)) \ntrain[train['Survived'] ==0]['Family'].hist(alpha=.7, bins=30, label='Survived =0') \ntrain[train['Survived'] ==1]['Family'].hist(alpha=.7, bins=30, label='Survived =1') \nplt.legend(loc='upper right') \nplt.title('Family Survival \\n')\nplt.show() ","25c9cf0e":"train['Famgroup'] = np.where(train['Family']<2,0,(np.where((train['Family']>1) & (train['Family']<5),1,2)))\ntest['Famgroup'] = np.where(test['Age']<2,0,(np.where((test['Family']>1) & (test['Family']<5),1,2)))","8b060287":"#drop unnecessary columns - either wrong type or accounted for elsewhere\ntraina = train.drop(columns=['Name','Ticket','Cabin','Fare','Age','Family','SibSp','Parch'])\ntesta = test.drop(columns=['Name','Ticket','Cabin','Fare','Age','Family','SibSp','Parch'])","f784fbc1":"#doesnt look like any of the variables are too highly correlated\nplt.figure(figsize = (10,10))\nsns.heatmap(traina.corr(), annot = True)\nplt.title('Variable Heatmap \\n')","7af3e80a":"#store test ID for submission purposes\nTestId=testa['PassengerId']\n#align data set shapes and get dummies\ntotal_features=pd.concat((traina.drop(['PassengerId','Survived'], axis=1), testa.drop(['PassengerId'], axis=1)))\ntotal_features=pd.get_dummies(total_features, drop_first=True)\ntrain_features=total_features[0:traina.shape[0]]\n\n#making sure the test set matches the train set\ntest_features=total_features[traina.shape[0]:] ","8cf9cb94":"train_features.shape,test_features.shape","dbf606a3":"X = train_features\ny = traina['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","c4d97e51":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores = []\n\n# Sequentially fit and cross validate all models\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores.append(acc.mean())","6a130797":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Score': scores})\n\nresult_df = results.sort_values(by='Score', ascending=False).reset_index(drop=True)\nresult_df","91969c09":"sns.barplot(x='Score', y='Model',data = result_df, color = 'skyblue')\nplt.title('Model Accuracy \\n')\nplt.xlim(0.65, 0.85)","cab0ff29":"# Getting feature importances for the 5 models where we can\nxgb_imp = pd.DataFrame({'Feature':train_features.columns, 'xgb importance':xgb.feature_importances_})\ngbc_imp = pd.DataFrame({'Feature':train_features.columns, 'gbc importance':gbc.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train_features.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train_features.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train_features.columns, 'ada importance':ada.feature_importances_})\n\n# Merging results into a single dataframe\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# Calculating average importance per feature\nimportances['Average'] = importances.mean(axis=1)\n\n# Ranking top to bottom\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)\n\n# Display\nimportances","712308b3":"plt.figure(figsize = (7,7))\nsns.barplot(y = 'Feature', x = 'Average',data = importances, color = 'skyblue')\nplt.title('Feature Importances \\n')","0b426893":"#dropping unimportant features\ncolumns = importances[importances['Average']<.03]['Feature'].values\ntrain_features.drop(columns = columns, inplace = True)\ntest_features.drop(columns = columns, inplace = True)","6fff0c08":"X = train_features\ny = traina['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","0cd088e3":"# Initiate models\nran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v2 = []\n\n# Fit & cross validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v2.append(acc.mean())","84272a73":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Score': scores,\n    'Score w\/Feature Selection': scores_v2})\n\nresult_df = results.sort_values(by='Score w\/Feature Selection', ascending=False).reset_index(drop=True)\nresult_df","d06b2450":"# Parameter's to search\npenalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\n# Setting up parameter grid\nhyperparams = {'penalty': penalty, 'C': C}\n\n# Run GridSearch CV\nlrgd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nlrgd.fit(X_train, y_train)\nprint(lrgd.best_score_)\nprint(lrgd.best_estimator_)","398cb833":"# Parameter's to search\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]\n\n# Setting up parameter grid\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\n# Run GridSearch CV\nkngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nkngd.fit(X_train, y_train)\nprint(kngd.best_score_)\nprint(kngd.best_estimator_)","f7a18d1d":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\nrfgd=GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nrfgd.fit(X_train, y_train)\nprint(rfgd.best_score_)\nprint(rfgd.best_estimator_)","7acd52a6":"# Parameter's to search\nCs = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 50, 100]\ngammas = [0.001, 0.01, 0.1, 1]\n\n# Setting up parameter grid\nhyperparams = {'C': Cs, 'gamma' : gammas}\n\n# Run GridSearch CV\nsvgd=GridSearchCV(estimator = SVC(probability=True), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nsvgd.fit(X_train, y_train)\nprint(svgd.best_score_)\nprint(svgd.best_estimator_)","5ec5d7bb":"# Parameter's to search\nn_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\n# Setting up parameter grid\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\n# Run GridSearch CV\ngpgd=GridSearchCV(estimator = GaussianProcessClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\ngpgd.fit(X_train, y_train)\nprint(gpgd.best_score_)\nprint(gpgd.best_estimator_)","43b23d80":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100, 125, 150, 200]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\n# Run GridSearch CV\nadgd=GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nadgd.fit(X_train, y_train)\nprint(adgd.best_score_)\nprint(adgd.best_estimator_)","cb1df3aa":"# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [100, 250, 500, 750, 1000, 1250, 1500]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngbgd=GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\ngbgd.fit(X_train, y_train)\nprint(gbgd.best_score_)\nprint(gbgd.best_estimator_)","ade9e719":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\netgd=GridSearchCV(estimator = ExtraTreesClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\netgd.fit(X_train, y_train)\nprint(etgd.best_score_)\nprint(etgd.best_estimator_)","d156f49b":"# Parameter's to search\nn_estimators = [10, 15, 20, 25, 50, 75, 100, 150]\nmax_samples = [1, 2, 3, 5, 7, 10, 15, 20, 25, 30, 50]\nmax_features = [1, 3, 5, 7]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\n# Run GridSearch CV\nbcgd=GridSearchCV(estimator = BaggingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs =-1)\n\n# Fitting model and return results\nbcgd.fit(X_train, y_train)\nprint(bcgd.best_score_)\nprint(bcgd.best_estimator_)","2525376d":"# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = XGBClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","3fb359a9":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd=GridSearchCV(estimator = gd.best_estimator_, param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\",n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","49807fa0":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd=GridSearchCV(estimator = gd.best_estimator_,param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\",n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","f7cb0d1f":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n    \nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd=GridSearchCV(estimator = gd.best_estimator_, param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\",n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","9b5dec13":"reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n    \nhyperparams = {'reg_alpha': reg_alpha}\n\nxggd=GridSearchCV(estimator = gd.best_estimator_,param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\",n_jobs=-1)\n\nxggd.fit(X_train, y_train)\nprint(xggd.best_score_)\nprint(xggd.best_estimator_)","73d18213":"# Initiate tuned models \nran = rfgd.best_estimator_\n\nknn = kngd.best_estimator_\n\nlog = lrgd.best_estimator_\n\nxgb = xggd.best_estimator_\n                    \ngbc = gbgd.best_estimator_\n\nsvc = svgd.best_estimator_\n\next = etgd.best_estimator_\n\nada = adgd.best_estimator_\n\ngpc = gpgd.best_estimator_\n\nbag = bcgd.best_estimator_\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v3 = []\n\n# Fit & cross-validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v3.append(acc.mean())","27e6fc22":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2,\n    'Score with tuned parameters': scores_v3})\n\nresult_df = results.sort_values(by='Score with tuned parameters', ascending=False).reset_index(drop=True)\nresult_df","999bb374":"# Initiate tuned models \nran = rfgd.best_estimator_\n\nknn = kngd.best_estimator_\n\nlog = lrgd.best_estimator_\n\nxgb = xggd.best_estimator_\n                    \ngbc = gbgd.best_estimator_\n\nsvc = svgd.best_estimator_\n\next = etgd.best_estimator_\n\nada = adgd.best_estimator_\n\ngpc = gpgd.best_estimator_\n\nbag = bcgd.best_estimator_\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v4 = []\n\n# Fit & cross-validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    predict = mod.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    scores_v4.append(acc)","7e309ebc":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2,\n    'Score with tuned parameters': scores_v3,\n    'Score with full accuracy': scores_v4})\n\nresult_df = results.sort_values(by='Score with full accuracy', ascending=False).reset_index(drop=True)\nresult_df","c6b148f0":"predictions = ext.predict(test_features)","e6dff350":"#test shape \nTestId.shape,predictions.shape","856a9537":"#Create submission (if competition)\nsubmission=pd.DataFrame()\nsubmission['PassengerId']=TestId\nsubmission['Survived']=predictions\nsubmission.to_csv('submission.csv', index=False)","595a8f0c":"# **Classification Training**\nThis is my attempt at the Titanic Classification Competition. My aim was to finish my initial analysis from my [Python Data Science Course](https:\/\/www.udemy.com\/python-for-data-science-and-machine-learning-bootcamp\/) and learn best practices from the Kaggle community on Classification Machine Learning algorithms. Also, I was interested in training and fitting multiple models at the same time to measure effectiveness across them.\n\nKernels I found especially helpful:\n- [Simple end-to-end ML Workflow: Top 5% score](https:\/\/www.kaggle.com\/josh24990\/simple-end-to-end-ml-workflow-top-5-score)\n- [Titanic: A beginner guide to top 6](https:\/\/www.kaggle.com\/toldo171\/titanic-a-beginner-guide-to-top-6)\n- [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n    \nAs of 3\/22\/19, my best accuracy was 0.81339 (top 7%).","147318b1":"# **Machine Learning**","8e5b2d71":"SVC","f25bc7f8":"Imputing Fare","24b20aee":"# **Data Wrangling**","aa0c0bab":"Gaussian Process","9ac83962":"#### Bagging Classifier","cd95a7f2":"Banding Numerical Variables","0d37894c":"#### Gradient Boosting Classifier\nNote: There are many more parameter's that could, and possibly should be tested here, but in the interest i've limited the tuning to establishing the appropriate learning_rate vs n_estimators trade off. The higher one value, the lower the other.","481cc015":"Cabin","f6b2b9f1":"Imputing Embarked","c38416bb":"Random Forest Classifier","095fc2dd":"##### Step 3","ce68c696":"XGBoost\n##### Step 1","6b29167b":"Logistic Regression","d7d57bc9":"##### Step 4","fbc53488":"Shaping","f0318404":"##### Step 2","b0de2c66":"KNN Classifier","356ac18c":"Hyperparameter Tuning","5412f135":"Family","9b46d73a":"Imputing Age ","faa16b81":"#### Adaboost","bbdf214b":"Fare","0a19b64b":"Family","8027e824":"Predictions","05e254b9":"#### Extra Trees","05d3aaf5":"Title","a8215b0a":"# **Import Data**","965d619b":"Inital Model Array","0968255a":"# **Data Cleaning**","3f07ae78":"Splitting Data","3acdc7f9":"Secodary Split After Feature Selection ","f9285ffe":"Test Against y_test","96381cce":"##### Step 5","7551f796":"Age"}}