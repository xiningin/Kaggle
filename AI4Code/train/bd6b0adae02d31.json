{"cell_type":{"2c23ea27":"code","8853276f":"code","45e27641":"code","35074600":"code","a87768af":"code","fd4a089e":"code","e8dd2a0b":"code","5340ffe3":"code","bb4f787c":"code","15de15f3":"code","6ab6d2ad":"code","9ad933fe":"code","27fcc2af":"code","55398144":"code","f467c5e8":"code","0bc45e8c":"code","2acce85f":"code","fd7ebf08":"code","96bda142":"code","091d2603":"code","f6d0295a":"code","4ebf9801":"code","3578b7f6":"code","77e08988":"code","a0532303":"code","c897bc00":"code","b402246d":"markdown","60f00a52":"markdown","a9f2eed6":"markdown","0371afa9":"markdown","f36670e9":"markdown","e669f169":"markdown","0868ddaf":"markdown","57519cad":"markdown","78eac946":"markdown","80ce78fc":"markdown","c5d3d707":"markdown","80fa0e4c":"markdown","cf886d33":"markdown","95e5ddfa":"markdown","33f08f69":"markdown","2c64b248":"markdown","e4838614":"markdown","d895ae60":"markdown","865043fe":"markdown","9f97908e":"markdown","8df52a12":"markdown","e825a920":"markdown","e5591b4b":"markdown","c50b285a":"markdown","b0b10547":"markdown","a3fb50a9":"markdown","8a058ecb":"markdown","f15c7605":"markdown","10d09e01":"markdown"},"source":{"2c23ea27":"import sys\nimport os\nimport pandas as pd\nimport numpy as np\n\n### Import swat\nimport swat\n###\n\n# Set Graphing Options\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Set column\/row display options to be unlimited\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","8853276f":"#Connect to CAS\ns = swat.CAS(os.environ['CASHOST'], os.environ['CASPORT'], None, os.environ.get(\"SAS_VIYA_TOKEN\"))\ns","45e27641":"s.sessionProp.setsessopt(caslib='KAGGLE')\n\n## We can also enable session metrics to view how long our procedures take to run\ns.session.metrics(on=True)","35074600":"# Load actionsets for analysis (for data prep, modelling, assessing)\nactionsets = ['cardinality', 'sampling', 'decisionTree']\n[s.loadactionset(i) for i in actionsets]","a87768af":"s.help(actionSet='cardinality')","fd4a089e":"table_name = \"PVA_TRAIN\"\n\ncastbl = s.load_path(\n  table_name+'.sas7bdat',\n  casOut=dict(name=table_name, replace=True)\n)\ncastbl.head()","e8dd2a0b":"# Create table of summary statistics in SAS\ncastbl.cardinality.summarize(\n    cardinality=dict(name = 'full_data_card', replace = True)\n)","5340ffe3":"full_data_card = s.CASTable('full_data_card').to_frame() # bring the summary data locally\n\n# Modify SAS output table using Python to present summary statistics\nfull_data_card['_PCTMISS_'] = (full_data_card['_NMISS_']\/full_data_card['_NOBS_'])*100\nprint('\\n', 'Summary Statistics'.center(90, ' '))\nfull_data_card[['_VARNAME_','_TYPE_','_PCTMISS_','_MIN_','_MAX_','_MEAN_','_STDDEV_','_SKEWNESS_','_KURTOSIS_']].round(2)","bb4f787c":"'''Note, you can set the following option to fetch more rows of data out Viya memory - this defaults to 10000 rows.'''\n# swat.options.cas.dataset.max_rows_fetched=60000\nfull_data_card.hist(figsize = (15, 10));","15de15f3":"#Declare input variables\ntarget =  'TARGET_D_with0'\ninput_vars = ['GiftCntAll', 'GiftAvgCard36','DemAge', 'DemGender']\nvariables = [target] + input_vars\n              \nselect_castbl = castbl[variables]\nselect_castbl.head(20)","6ab6d2ad":"# Create table of summary statistics in SAS\nselect_castbl.cardinality.summarize(\n    varList=[\n        {'vars': input_vars}\n    ],\n    cardinality=dict(name = 'data_card', replace = True)\n)\n\ndf_data_card = s.CASTable('data_card').to_frame() # bring the summary data locally\n\n# Modify SAS output table using Python to present summary statistics\ndf_data_card['_PCTMISS_'] = (df_data_card['_NMISS_']\/df_data_card['_NOBS_'])*100\nprint('\\n', 'Summary Statistics'.center(90, ' '))\ndf_data_card[['_VARNAME_','_TYPE_','_PCTMISS_','_MIN_','_MAX_','_MEAN_','_STDDEV_','_SKEWNESS_','_KURTOSIS_']].round(2)","9ad933fe":"## Note, you can set the following option to fetch more rows of data out Viya memory - this defaults to 10000 rows.\nswat.options.cas.dataset.max_rows_fetched=60000\nselect_castbl.hist(figsize = (15, 10));","27fcc2af":"missingInputs = ['DemAge', 'GiftAvgCard36']\n\ns.dataPreprocess.impute(\n    table = select_castbl,\n    outVarsNamePrefix = 'IMP',\n    methodContinuous  = 'MEDIAN',\n    inputs            = missingInputs,\n    copyAllVars       = True,\n    casOut            = dict(caslib= 'KAGGLE', name=table_name, replace=True)\n)\n\n# Print the first five rows with imputations\nimp_input_vars = ['IMP_' + s for s in ['DemAge', 'GiftAvgCard36']]\ntotal_inputs = input_vars + imp_input_vars\n\ntotal_inputs.remove('DemAge')\ntotal_inputs.remove('GiftAvgCard36')\n\nselect_castbl = s.CASTable(table_name)[total_inputs]\nselect_castbl.head(5)","55398144":"# Create a 70\/30 simple random sample split\nselect_castbl.sampling.srs(\n    samppct = 70,\n    partind = True,\n    seed    = 1,\n    output  = dict(\n        casOut = dict(\n            name=f'{table_name}',\n            replace=True), \n        copyVars = 'ALL'\n    ),\n    outputTables=dict(replace=True)\n)","f467c5e8":"# Train Decision Tree model\ns.decisionTree.dtreeTrain(\n    table = dict(name= table_name, where='_partind_ = 1'),\n    target = target,\n    inputs = total_inputs,\n    casOut = dict(name='DT_model', replace=True),\n    code = dict(casout=dict(name='DT_model_code', replace=True))\n)","0bc45e8c":"s.CASTable('DT_model').head()","2acce85f":"score_model = s.decisionTree.dtreeScore(\n    table      = dict(name = table_name, where='_partind_=0'),\n    modelTable = 'DT_model',\n    copyVars   = [target, '_partind_'],\n    casOut     = dict(name = '_scored_DT', replace = True),\n    encodeName = True\n)\nscore_model","fd7ebf08":"s.CASTable('_scored_DT').head()","96bda142":"# Model assessment function\nassess_model = s.percentile.assess(\n    table    = dict(name = '_scored_DT', where = '_partind_ = 0'),\n    inputs   = 'P_' + target,      \n    response = target,\n)\nassess_model","091d2603":"plt.figure(figsize = (7, 6))\nplt.plot(assess_model['LIFTRegInfo']['meanT'], assess_model['LIFTRegInfo']['meanT'], label =  'Actuals')\nplt.plot(assess_model['LIFTRegInfo']['meanT'], assess_model['LIFTRegInfo']['meanP'], label =  'Predicted')\nplt.xlabel('Depth')\nplt.ylabel('Predicted Mean')\nplt.legend(loc='lower left')\nplt.title('Predicted Reports -- Predicted by Actuals');","f6d0295a":"# Load into memory \ncastbls = s.load_path(\n  'PVA_SCORE.sas7bdat',\n  caslib='KAGGLE',\n  casOut=dict(name='testset', replace=True)\n)\n\n# Impute evaluation data\ns.dataPreprocess.impute(\n    table = castbls,\n    outVarsNamePrefix = 'IMP',\n    methodContinuous  = 'MEDIAN',\n    inputs            = missingInputs,\n    copyAllVars       = True,\n    casOut            = dict(caslib= 'KAGGLE', name='testset', replace=True)\n)\n\n# Score\neval_model = s.decisionTree.dtreeScore(\n    table      = 'testset',\n    modelTable = 'DT_model',\n    copyVars   = ['ID'],\n    casOut     = dict(name = '_eval_DT', replace = True),\n    encodeName=True\n)\neval_model","4ebf9801":"s.CASTable('_eval_DT').head()","3578b7f6":"keepcolumns = ['ID','P_TARGET_D_with0']\n\n## Need to rename the column with the name Kaggle expects: \"Predicted\"\ns.table.alterTable(\n    columns=[{\n        \"name\": \"P_TARGET_D_with0\",\n        \"rename\":\"Predicted\"\n    }],\n    name='_eval_DT'\n)\nevaluation = s.CASTable('_eval_DT')\n\n## Re-order column as how Kaggle submission expects\nevaluation = evaluation.ix[:, ['ID', \"Predicted\"]]\nevaluation.head()","77e08988":"## Output column as CSV - make sure you set the data download limit to be greater than 10000 rows since the test set has more than 10k samples.\nswat.options.cas.dataset.max_rows_fetched=60000\nevaluation.to_csv('predictionColumn.csv', index=False, float_format='%.12g')","a0532303":"# Set the option to display the entire text and then save to a physical file\npd.set_option(\"display.max_colwidth\", -1)\nfile=open('DT_score.sas', 'w') \nfile.write(s.CASTable('DT_model_code')['DataStepSrc'].str.strip().to_string(index=False).replace(\"\\\\n\",\"\"))\nfile.close()","c897bc00":"'''To view the score code, run this cell'''\n#s.CASTable('DT_model_code')['DataStepSrc'].str.strip().to_string(index=False).replace(\"\\\\n\",\"\")","b402246d":"# What's Next?\n\n---\n\nIf you have made it to the end, congratulations! You can make a submission to the Kaggle Leaderboard with the CSV you have created - the next steps are, how do you improve and do better?\n\nThroughout the duration of the challenge, we will be releasing further hints and tips, as well as code snippets to try, but what you can do is take this template and improve on it by considering:\n* What input variables should be used? Are all the variables in the dataset good predictors of upsell\/xsell (i.e. should all variables be used?) What happens if you run the summary statistics and visualisation on the entire dataset?\n* Is the decision tree the best model that can be trained? What other ML algorithms could be used instead?\n* Beyond imputation, what other types of techniques could be used to create and engineer better features?","60f00a52":"We can then visualise these summar statistics as a historgram by running the below SWAT function. Since __SWAT__ does not have graphing capabilities by itself, it uses __matplotlib__ functionality:","a9f2eed6":"# SAS Viya for Learners ML Pipeline (Python)\n\n---\n\nWelcome to the SAS Viya for Learners Challenge - where you will learn how to build machine learning models for a simulated telecommunication company. For the challenge, you will build a ML model to predict whether an existing customer will upgrade their existing service (upsell) or purchase another service (xsell).\n\nTo get you started, you can use this existing Python notebook that will walkthrough the steps of the machine learning (ML) process by building a simple model. By showing you the basics, you will learn how to program in Python using machine learning running in SAS Viya. However, since this is a simple exmaple, the challenge is to then add more to this example by adding more variables, engineering new features, and build better models to move up the leaderboard!","0371afa9":"### Download Your Submission Files\n\n---\n\nWhen you have created your submission CSV and your model score code, your final submission must also include the training code used to build your model - in this case, this is the Jupyter notebook. Once you have all files, you may simply right-click on the selected file and click 'Download' which downloads the file to your local computer.\n\nAfter downloading the prediction column as a CSV, you should have also downloaded the \"submissionsId.csv\" from the Data section of the Kaggle competition. Open this file and append your prediction column onto this file and save - this is your submission CSV that can now be submitted via Kaggle onto the Leaderboard. Refer to the \"submissionExample.csv\" to see what your submission should look like.","f36670e9":"### Writing CAS Action Sets\n\n---\n\nNow that we have loaded the action sets, let's review the Python syntax for writing CAS actions. Every CAS action requires the connection object (in this case, 's') in front and then follows a specific order - action set, action, parameter, option:\n\n```python \nconnectionObject.actionSetName.actionSet(\n    parameter1={\n        option1=True,\n        option2=True,\n    },\n    parameter2=5\n)\n```\n\nFor example, the action to train a decision tree model is:\n```python\ns.decisionTree.dtreeTrain(\n    table = dict(name= input_data),\n    target = target,\n    inputs = input_variables,\n    nominals = nominal_variables,\n    casOut = dict(name='DecisionTreeModelOutput', replace=True),\n    code = dict(casout=dict(name='DecisionTreeModelCode', replace=True)\n)\n```\n\nNote, using the _dict()_ function is the same as using _{}_ to define a Python dictionary.","e669f169":"This creates a CAS Table containing the model parameters, which can be displayed:","0868ddaf":"### Declare Input Variables\n\n---\n\nAs seen above, the dataset contains a lot of variables of varying characteristics, type & structure. \n\nWe can explicitly declare our variables by creating a list, and then subsetting our data based off a list, __input_vars__. Our subsetted input table we will use to build a model is __select_castbl__.\n\n**Note:** For this starter example, we are going to start with a simple group of variables to build our model for this toy example. After going through this sample, try for yourself with a bigger set of variables and build the best model (See the below of the notebook for further suggestions).","57519cad":"We can use the Python graphing library, __matplotlib__ once again to plot the predicted and target means for our model. This plot illustrates how close the predicted values were to the target - ideally a perfect model will perfect overlap with the orange line.","78eac946":"### Data Transformation - Imputation\n\n---\n\n\nFrom viewing the summary statistics, we can see that one of our variables has missing values. Missing data introduces bias into our models and creates problems with analysis, so in the context of machine learning, we want to create a more helpful input. This is known as feature engineering, where we create new inputs, or features from the inputs in our data. \n\nOne of the methods we can use to deal with missing values is imputation. That is, instead of deleting entries and reducing our sample size, we can replace these missing values with a substituted value (imputation).\n\nThe choice of substituted value depends on your choice of imputation method but for this simple example we will impute with the median to demonstrate a simple data processing transformation.","80ce78fc":"# Data Processing\n\n### Accessing Data on SAS Viya\n\n---\n\nAll data in SAS Viya resides in-memory to perform analytics more efficiently. What that means is, after connecting, you must load the dataset you want to use into the SAS Viya memory. When data is in-memory in SAS Viya, it is represented as a __CAS table__.\n\nData can be loaded into SAS Viya from the client-side (local computer into SAS Viya) or server-side (server machine into SAS Viya engine). For this challenge, the data exists on the server machine (the same cloud environment SAS Viya is configured on) so we must load the tables into memory using the ```load_path()``` function.\n\nNote, that the client-side operations from SWAT are built on top of the __pandas__ functionality, meaning you can pass the same __pandas__ arguments for the equivalent function.","c5d3d707":"# Build Models\n\n### Train Decision Tree algorithm\n\n---\n\nAfter partitioning, we are ready to start building our model. We are going to build a decision tree, of which we will submit the following parameters:","80fa0e4c":"Now we can output the prediction column only as a CAS Table and print:","cf886d33":"We can perform the same set data exploration procedures as well to view the variables we have selected:","95e5ddfa":"### Help & Documentation\n\n---\n\nJupyter Notebook provides in-line help and documentation via use of the help() function - this can be used to view how to use the __CAS action sets__. The action set must be loaded in the previous step.","33f08f69":"### Data Partitioning\n\n---\n\nNow that we have declared our input variables, we partition our data into a training set and validation set for modelling. The training set (70% of the data) will be used to build the model and evaluate initial performance. The validation set (remaining 30%) will be used to evaluate the model on unseen data that still fit the same distribution as the training set.\n\nPartitioning of the data is done to prevent overfitting the model to the training data. That is, the model will be fitted so tightly to the training data that it will inhibit it's ability to accurately predict whether new customers will upsell or xsell on their services. We want the model to be able to generalise and assessing on a validation set helps does this.\n\nHere, we create a partition using a CAS action:","2c64b248":"### Connect to SAS Viya\n\n---\n\nIn order to execute CAS actions, a connection must be made using SWAT from the OS client to the CAS server. Your login credentials and server details are required, but by default these are already saved as environment variables into the ```swat.CAS()``` statement.\n\nThis creates a 'connection object' represented by the Python variable 's', which represents the connection between Jupyter Notebook and SAS Viya. If the connection was successful, the connection object must be invoked in order to run CAS actions. Additionally, any function with the connection object in front of it indicates that it is running on SAS Viya.\n\nUpon successful connection, a __CAS session__ will be started allowing execution of CAS actions. The CAS session information will be returned below, if you print the connection object variable.","e4838614":"### Caslibs\n\n---\n\nOn SAS Viya, all data are organised into __CAS Libraries__ (caslib). They represent a individual or group workspace where data is stored. For the challenge, the training data is located in the __KAGGLE__ caslib so we will set that to our working Caslib.","d895ae60":"# Setup\n\n### Package Import\n\n---\n\nThe first step is to import packages - packages contain the functions that we want to use for our analysis. The key packages are:\n\n- __SWAT__ -- This package allows our Jupyter Notebook to connect to SAS Viya and use the analytical capabilities that SAS Viya provides. See the introduction for a more detailed description.\n\n- __pandas, matplotlib__ -- These are Python packages for data manipulation and graphing respectively.","865043fe":"The above action generates the summary statistics, and we can now view this summary information by bringing it locally into the Jupyter notebook client:","9f97908e":"#### Note - This notebook is meant to be run on the SAS Viya environment, not Kaggle! Download the notebook and upload to SAS Viya following the instructions on the Discussion forum: https:\/\/www.kaggle.com\/c\/sasviyaforlearners2021\/discussion\/","8df52a12":"### Assess Models \n\n---\n\nUsing the scores just generated, we can run model assessment on the models and output the assessment statistics. We then use this to understand how the model performed, so we can utilise it for our submission:","e825a920":"### Output Model Score Code Files\n\n---\n\nIn addition to the submission csv, being able to output your model score files is required for a valid submission to be able to reproduce results.\n\nTrained SAS Viya models are represented through:\n* SAS score code (Statistical models - e..g. linear regression, logistic regression, decision tree)\n* SAS score code & ASTORE (ML models - e.g. random forest, gradient boosting, neural network)\n\nSAS score code are text files consisting of IF-ELSE-THEN rules written in the SAS programming language (regardless of what other programming language was used to _train_ the model). ASTORE are binary objects containing the compressed model logic of complicated ML models to enable these files to be more portable. In this simple example, we trained a decision tree model so we can output the score code from the __CAS Table__ the model was stored in.","e5591b4b":"### Explore Data through Summary Statistics\n\n---\nWe can explore our data and generate summary statistics on our data set. This information can be used in combination with the Data tab view from the Kaggle competition page to assess how our input data is distributed. ","c50b285a":"### Load CAS Action Sets\n\n---\n\nThe analytical procedures in SAS Viya are known as __CAS actions__ and are organised into __CAS action sets__. These __CAS action sets__ represent modular functionality (e.g. processing data, building models) and must be loaded after connecting to SAS Viya, similar to importing Python packages.\n\nAction sets can be loaded using the following command:\n\n```python\ns.loadactionset(action_set_name)\n```\n\nThe list of CAS Action Sets available can be found in the [documentation](https:\/\/go.documentation.sas.com\/?docsetId=allprodsactions&docsetTarget=actionSetsByName.htm&docsetVersion=3.5&locale=en).\n\nThe cell below has been created to allow loading of multiple CAS action sets by specifying the names of the action sets required in the list variable _actionsets_:","b0b10547":"# Introduction\n\n### What is SAS Viya?\n\n---\n\n__SAS Viya__ is the latest analytics platform from SAS for building and developing machine learning models. At its core, SAS Viya has an in-memory analytics engine which has been designed to run analytics and ML procedures more efficiently, than if they were running on your local laptop. These procedures, known as __CAS actions__ can be called and executed using Python, R & SAS so that data scientists can use their preferred programming language of choice, but be able to run these ML procedures using the SAS Viya engine for more scalable performance.\n\nTo do this, we use the __SAS Scripting Wrapper for Analytics Transfer (SWAT)__, which is a library that allows our Jupyter Lab client to connect to SAS Viya. Using __SWAT__, you can analyse large data sets in-memory using the analytical engine and performance of SAS Viya but not be restricted by your choice of programming language.\n\nThe diagram below demonstrates this process, where you can have syntactically different code from three different programming languages (SAS, Python, R) but they are all calling the equivalent CAS Action on the SAS Viya server to execute. For this notebook specifically, we are using Python code in our JupyterLab notebook to call the CAS actions running on the SAS Viya server. ","a3fb50a9":"## Evaluate & Create Submission\n\n### Evaluate on Test Set\n\n---\n\nNow that the model has been trained, we can evaluate it on the test set and create our submission. The first step is to score our model on the evaluation dataset - which is located in the __KAGGLE__ Caslib. \n\nAs stated in the Data section of the Kaggle competition https:\/\/www.kaggle.com\/c\/sasviyaforlearners2021\/data, creating the submission CSV consists of two parts:\n1. Generating the column of predictions corresponding to the IDs\n2. Formating the predictions and then downloading as a CSV\n\nThis is because Kaggle requires all submissions containing predictions to be sorted by a specific Customer ID order.\n\nThe rest of this notebook will walkthrough how to generate the predictions as CSV.\nTo start the scoring process to generate the prediction column we load the test set into memory and then score the model. We will need to impute our test data, since the decision tree model expects the imputed column and we will use the dtScore action to then score the model","8a058ecb":"Finally, we can output the prediction column as a CSV.","f15c7605":"## Score Model\n\n---\n\nHaving trained the decision tree model, we can now score it on our validation set to do this, we reference the CAS Table containing our model, \"DT_model\" that we created in the training action:","10d09e01":"### Notebook Workflow\n\n---\n\nML is an iterative process involving three main steps:\n1. Data Processing (Exploring your data and pre-processing)\n2. Model Building (Building your machine learning models)\n3. Model Evaluation (Assessing the quality of your models)\n\nThis notebook will walk through this process:\n \n1. Data Processing\n    - Load Data\n    - Explore & Pre-process Data\n    - Partition Data\n2. Model Building\n    - Training a model algorithm on training set\n    - Scoring on validation set\n3. Model Evaluation & Submission\n    - Assess on the test dataset for leaderboard\n    - Save model files required for submission"}}