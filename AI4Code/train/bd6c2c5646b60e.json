{"cell_type":{"37bff641":"code","2f18c565":"code","dc03c5da":"code","b0107ae6":"code","2e3ccb85":"markdown","64e592a3":"markdown","d6d0ef70":"markdown"},"source":{"37bff641":"import os\nimport pandas as pd\nimport numpy as np\n\ndata_root = os.environ.get('KAGGLE_DIR', '..\/input')\ntps_root = f'{data_root}\/tabular-playground-series-dec-2021'\ndf = pd.read_csv(f'{tps_root}\/train.csv')\n\nbools = df.columns[df.nunique() <= 2]\ndtypes = {col: np.bool_ for col in bools}\ndtypes['Id'] = np.int32\ndtypes['Cover_Type'] = np.int8\nfloats = {\n    col: np.float32 for col in set(df.columns) - set(dtypes)\n}\ndtypes.update(floats)\ndf = df.astype(dtypes)\ndf_test = pd.read_csv(f'{tps_root}\/test.csv', dtype={col: dtype for col, dtype in dtypes.items() if col != 'Cover_Type'})\n\ndf.info()","2f18c565":"df.to_parquet('train.pq')\ndf_test.to_parquet('test.pq')","dc03c5da":"both = pd.concat([df, df_test], axis=0)\n\nwilderness_sum = both[both.columns[both.columns.str.startswith('Wilderness')]].sum(axis=1).rename('Wilderness_Sum').astype(np.float32)\nsoiltype_sum = both[both.columns[both.columns.str.startswith('Soil_')]].sum(axis=1).rename('Soil_Type_Sum').astype(np.float32)\n\naspect_rollover = (both.Aspect % 360).rename('Aspect_rolled_over')\nhydrology_elevation = (both.Elevation - both.Vertical_Distance_To_Hydrology).rename('Hydrology_Elevation')\nwater_vert_direction = both.Vertical_Distance_To_Hydrology.apply(np.sign).rename('Water_Vertical_direction')\n\ndef make_positive(series): return series + abs(series.min())\n\nmanhattan_hydrology = (\n        make_positive(both.Horizontal_Distance_To_Hydrology) +\n        make_positive(both.Vertical_Distance_To_Hydrology)).rename('Manhattan_Hydrology').astype(np.float32)\n\neuclidean_hydrology = (\n        make_positive(both.Horizontal_Distance_To_Hydrology) ** 2 +\n        make_positive(both.Vertical_Distance_To_Hydrology) ** 2\n).apply(np.sqrt).rename('Euclidian_Hydrology').astype(np.float32)\n\nhillshape_clipped = both[both.columns[both.columns.str.startswith('Hillshade_')]].clip(lower=0, upper=255).add_suffix('_clipped')\n\nfe = pd.concat([\n    both.Elevation,\n    both.Horizontal_Distance_To_Roadways,\n    both.Horizontal_Distance_To_Fire_Points,\n    both[both.columns[both.columns.str.startswith('Wilderness')]],\n    both[both.columns[both.columns.str.startswith('Soil_')]].drop(columns=['Soil_Type7', 'Soil_Type15']),\n    wilderness_sum,\n    soiltype_sum,\n    aspect_rollover,\n    hydrology_elevation,\n    water_vert_direction,\n    manhattan_hydrology,\n    euclidean_hydrology,\n    hillshape_clipped,\n    both.Cover_Type\n], axis=1)\n\ntrain_fe, test_fe = fe.loc[fe.Cover_Type.notna()], fe.loc[fe.Cover_Type.isna()]\n\nfe.info()","b0107ae6":"train_fe = train_fe.astype({'Cover_Type': np.int8})\ntest_fe = test_fe.drop(columns=['Cover_Type'])\n\ntrain_fe.to_parquet('train_fe.pq')\ntest_fe.to_parquet('test_fe.pq')","2e3ccb85":"Next step\n==\n\nThis time around, I chose to create a dataset from these files. In [the next](https:\/\/www.kaggle.com\/kaaveland\/tps202112-lgbm-feature-importance?scriptVersionId=81261111) notebook, I've simply added that dataset to a competition notebook to load it. Note that there's no need to worry about data types in that notebook.\n\nThere's no particular reason why I turned these output files into a kaggle dataset, I could have just as easily added the output of this notebook as input to the next. This is a very reasonable thing to do if you need to do some sort of expensive preprocessing! Or maybe you can add a high number of features, then in the next notebook, you can work on selecting the best ones?\n\nFeature engineering\n==\n\nA number of good features have been found -- I'm not going to link to everything in the discussions, but there are lots of notebooks and topics for this now.\n\nFor this part, I will concatenate the train and test datasets, all these transformations look only on the same row, so this is perfectly fine.","64e592a3":"Converting data to parquet\n==\n\nI see a lot of people copying around a `reduce_memory_usage` function, which they immediately call on the result of reading from a CSV.\n\nObviously, it's nice to reduce the memory usage, and it's preferrable to use 32 bit floats to 64 bit ints, for fast arithmetic. But there's a way to do that, without having to copy a function around all the time.\n\nApache Parquet\n--\n\nIs an open standard for an efficient binary file format for columnar data. It's supported by most tools that work with \"big\" tabular data, for example pandas, spark, polars, arrow. We use this at work, it scales very nicely up to \"billions of things\".\n\nIn general, if you `df.to_parquet(...)` a `DataFrame`, and later, you `pd.read_parquet(...)` the same one, all the columns will have the same dtype! To some extent, this works across languages an tools too -- if you've turned something into a `pd.CategoricalDtype` with `pandas`, chances are good that `polars`, which also supports such a concept, will read it back that way. ","d6d0ef70":"So, we decided on some datatypes. We can just write these dataframes as parquet files now.\n\nIf we want to use them in another notebook, we have two options:\n\n- After saving the notebook, we can navigate to the \"Data\" pane, and there's a \"Create dataset\" button we can use to make it into a Dataset.\n- But we could also just use the output of this notebook as input to the next one.\n\nFirst things first, let's create the files:"}}