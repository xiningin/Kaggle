{"cell_type":{"533687c9":"code","31836443":"code","08294903":"code","2158bb45":"code","edfabfba":"code","ec848e28":"code","b7269d86":"code","1efe1090":"code","9ce7b262":"code","f350b393":"code","d35c0696":"code","612ade6f":"code","3be0ca49":"code","ea7d4c22":"code","1338fa72":"code","6f837c21":"code","9cb04106":"code","45bdbe2b":"code","9be54968":"code","ad9419c3":"code","40218590":"markdown","a7d6f307":"markdown","ee53bec7":"markdown","becf5495":"markdown","7bf7f9f9":"markdown","48341847":"markdown","bde30e60":"markdown","230b550a":"markdown","c52998e4":"markdown","93803cb9":"markdown","d933e178":"markdown","a4a6d5d4":"markdown","11d61d14":"markdown","363ba3e6":"markdown","184047df":"markdown","c3aa2904":"markdown","c9db5c7c":"markdown","52b260c8":"markdown","95cca945":"markdown","0a1ab35a":"markdown","6c3495e2":"markdown"},"source":{"533687c9":"#Load data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#get training data and testing data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","31836443":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train_data.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test_data.shape))\n\n#Save the 'Id' column\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train_data.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test_data.shape))","08294903":"#handling outlier in the training data\n#We do a pair scatter plot between the reponds and its highly correlated predictors and find possible outliers.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Setting style to 'darkgrid'\nsns.set_style('darkgrid')\ncorrmat = train_data.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.6]\nsns.pairplot(train_data[top_corr_features], diag_kind='kde')\n\n#Based on the pairplot, we remove the outlier that \"GrLivArea\" is larger than 4000\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea']>4000)].index)","2158bb45":"#processing the train and test data simulatously.\nntrain = train_data.shape[0]\nntest = test_data.shape[0]\ny_train = train_data.SalePrice.values\nall_data = pd.concat((train_data, test_data),sort=False).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","edfabfba":"#handleing missing value\n\n# miss_number=all_data.isnull().sum()\n# miss_ratio=all_data.isnull().sum()\/len(all_data)\n# miss_info=pd.DataFrame({'Number of miss':miss_number,'Proportion of miss':miss_ratio},)\n# miss_info=miss_info.loc[miss_info['Number of miss']>0]\n# miss_info=miss_info.sort_values(by='Number of miss',ascending=0)\n# print(miss_info)\n\n#fill missing values\nimport copy\nall_data2=copy.copy(all_data)\n\n#By description, the following missing data are replaced by \"None\"\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu','GarageType', \n            'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', \n            'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"MasVnrType\"):\n    all_data2[col] = all_data2[col].fillna('None')\n\n#By descriotion, the following missing data are replaced by number 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', \n            'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\"MasVnrArea\"):\n    all_data2[col] = all_data2[col].fillna(0)\n\n#For \"LotFrontage\", we fill in missing values by the median LotFrontage of the neighborhood.\nall_data2[\"LotFrontage\"] = all_data2.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n    \n#there is only one missing value for the following variable, just replace it by the mode. \nall_data2['Electrical'] = all_data2['Electrical'].fillna(all_data2['Electrical'].mode()[0])\n\n# miss_number=all_data2.isnull().sum()\n# miss_ratio=all_data2.isnull().sum()\/len(all_data2)\n# miss_info=pd.DataFrame({'Number of miss':miss_number,'Proportion of miss':miss_ratio},)\n# miss_info=miss_info.loc[miss_info['Number of miss']>0]\n# miss_info=miss_info.sort_values(by='Number of miss',ascending=0)\n# miss_info","ec848e28":"#Transforming some numerical variables that are really categorical\n\n#MSSubClass=The building class\nall_data2['MSSubClass'] = all_data2['MSSubClass'].astype(str)\n\n#Changing OverallCond into a categorical variable\nall_data2['OverallCond'] = all_data2['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data2['YrSold'] = all_data2['YrSold'].astype(str)\nall_data2['MoSold'] = all_data2['MoSold'].astype(str)","b7269d86":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data2[c].values)) \n    all_data2[c] = lbl.transform(list(all_data2[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data2.shape))","1efe1090":"# Deep feature engineer\nall_data2['YrBltAndRemod']=all_data2['YearBuilt']+all_data2['YearRemodAdd']\nall_data2['TotalSF']=all_data2['TotalBsmtSF'] + all_data2['1stFlrSF'] + all_data2['2ndFlrSF']\n\nall_data2['Total_sqr_footage'] = (all_data2['BsmtFinSF1'] + all_data2['BsmtFinSF2'] +\n                                 all_data2['1stFlrSF'] + all_data2['2ndFlrSF'])\n\nall_data2['Total_Bathrooms'] = (all_data2['FullBath'] + (0.5 * all_data2['HalfBath']) +\n                               all_data2['BsmtFullBath'] + (0.5 * all_data2['BsmtHalfBath']))\n\nall_data2['Total_porch_sf'] = (all_data2['OpenPorchSF'] + all_data2['3SsnPorch'] +\n                              all_data2['EnclosedPorch'] + all_data2['ScreenPorch'] +\n                              all_data2['WoodDeckSF'])\n\n# simplified features\nall_data2['haspool'] = all_data2['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data2['has2ndfloor'] = all_data2['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data2['hasgarage'] = all_data2['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data2['hasbsmt'] = all_data2['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data2['hasfireplace'] = all_data2['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9ce7b262":"#skew data\nfrom scipy.stats import skew \n\nnumeric_feats = all_data2.dtypes[all_data2.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data2[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head(10))\n\nskewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data2[feat] = boxcox1p(all_data2[feat], lam)","f350b393":"#Get dummies for catigory variables.\nall_data3=pd.get_dummies(all_data2)#train2:after missing value, outlier; train3:get dummies for category variable.","d35c0696":"#check the normality of the responds variable\nimport seaborn as sns\nfrom scipy.stats import norm #for some statistics\nimport matplotlib.pyplot as plt  # Matlab-style plotting\n\n#histogram plot\nsns.distplot(y_train, fit=norm);\n#add title axis\n(mu, sigma) = norm.fit(y_train)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#use QQ-plot to see the normality\nfrom scipy import stats\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)\nplt.show()","612ade6f":"#the respond variable is right skew, we use log transformation to make it more normally.\n\ny_train_log = np.log1p(y_train)#use np.log1p which  applies log(1+x) when the data is close or equal to zero\n\n#Check the new distribution \nsns.distplot(y_train_log , fit=norm);\n#add title axis\n(mu, sigma) = norm.fit(y_train_log)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('log-SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y_train_log, plot=plt)\nplt.show()","3be0ca49":"#separate the training and testing data.\nx_train = all_data3[:ntrain]\ny_train_log= np.log1p(y_train)\nx_test = all_data3[ntrain:]","ea7d4c22":"#load packages\nfrom sklearn.linear_model import  Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","1338fa72":"#lasso\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n#Elastic Net Regression \nmodel_ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","6f837c21":"#xgboost with parameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport scipy.stats as st\nimport datetime\n\nprint('####################################################\\n{}\\start_time'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {\n    'colsample_bytree': [0.4],\n    'gamma': st.uniform(0.0,0.05),\n    'learning_rate': [0.05], \n    'max_depth':[3],\n    'min_child_weight': [2],\n    'n_estimators': st.randint(2000,3000),\n    'subsample': st.uniform(0.4,0.6),\n    'objective':['reg:squarederror'],\n    'reg_alpha':st.uniform(0,0.5),\n    }\n\n\n\n\nxgb_temp = xgb.XGBRegressor()\nmodel_xgb_tuned = RandomizedSearchCV(xgb_temp, params, n_iter=3,n_jobs=-1)\nmodel_xgb_tuned.fit(x_train,y_train_log)\nmodel_xgb = xgb.XGBRegressor(**model_xgb_tuned.best_params_)\n\nprint(model_xgb)\nprint('{}\\tEnd_time\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","9cb04106":"#LightGBM: another implementation of grandient boosting \nimport lightgbm as lgb\n\nprint('####################################################\\n{}\\start_time'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {\n    'objective':['regression'],\n    'num_leaves':[4,5],\n    'learning_rate':[0.05], \n    'n_estimators': [700,5000],\n    'max_bin': [50,200], \n    'bagging_fraction':[0.75],\n    'bagging_freq':[5],\n    'bagging_seed':[7], \n    'feature_fraction':[0.2],\n    'feature_fraction_seed':[7] \n    }\n\n\n\nlight_temp = lgb.LGBMRegressor()\nmodel_lgb_tuned = GridSearchCV(light_temp, params, n_jobs=-1)\nmodel_lgb_tuned.fit(x_train,y_train_log)\nmodel_lgb = lgb.LGBMRegressor(**model_lgb_tuned.best_params_)\n\nprint(model_lgb)\nprint('{}\\tEnd_time\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","45bdbe2b":"#random forest\n\nfrom sklearn.ensemble import RandomForestRegressor\nprint('####################################################\\n{}\\start_time'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {\n    'max_depth': [20,None],\n    'min_samples_leaf': [2],\n    'min_samples_split': [4],\n    'n_estimators': [200,500],\n    }\n\nrf_temp = RandomForestRegressor()\nrf_temp_tuned = GridSearchCV(rf_temp, params, n_jobs=-1)\nrf_temp_tuned.fit(x_train,y_train_log)\nmodel_randomforest = RandomForestRegressor(**rf_temp_tuned.best_params_)\n\nprint(model_randomforest)\nprint('{}\\tEnd_time\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","9be54968":"#Use cross validation to compare the performance\nfrom sklearn.model_selection import KFold\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\n    rmse= np.sqrt(-cross_val_score(model, x_train, y_train_log, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse.mean())\n\nmodels = {\n    'Lightgbm':model_lgb,\n    'XGBoost':model_xgb,\n    'Lasso':model_lasso,\n    'Random forest':model_randomforest,\n    'Elastic Net':model_ENet\n    }\n\nfor model_ind, model_fn in models.items():\n    print('Fitting:\\t{}'.format(model_ind))\n    model_fn.fit(x_train, y_train_log)\n    print('Done! Error:\\t{}\\n'.format(rmsle_cv(model_fn)))\n\n    \n#combine the models   \nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)   \n\n#combine the model together(stacking)\naveraged_models = AveragingModels(models = (model_lgb, model_xgb,model_lasso,model_ENet))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: \\t{}\\n\".format(score))","ad9419c3":"#We use the stacked model for our final predictions.\n\naveraged_models.fit(x_train, y_train_log)\ny_pred=averaged_models.predict(x_test)\n\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = np.expm1(y_pred)\nsub.to_csv('Aaron_submission.csv',index=False)","40218590":"<a id=\"xgboost\"><\/a>\n## 3.2 Xgboost with Hyper-parameter Tuning","a7d6f307":"<a id=\"predictions\"><\/a>\n## 4. Predictions and Submit the Results","ee53bec7":"<a id=\"load\"><\/a>\n## 2.1 Load Data","becf5495":"### Transforming some Numerical Variables that are Really Categorical","7bf7f9f9":"<a id=\"rf\"><\/a>\n## 3.4 Random Forest with Hyper-parameter Tuning","48341847":"<a id=\"models\"><\/a>\n# 3. Models","bde30e60":"### Skewed features","230b550a":"### Missing Values","c52998e4":"### Feature engineering","93803cb9":"<a id=\"data_cleaning\"><\/a>\n## 2.2 Data Cleaning","d933e178":"### Check the Normality of the Respond Variable (SalePrice)","a4a6d5d4":"<a id=\"LightGBM\"><\/a>\n## 3.3 LightGBM with Hyper-parameter Tuning","11d61d14":"<a id=\"stacking\"><\/a>\n## 3.5 Use Cross Validation to Compare the Performance and Stacking the Models","363ba3e6":"### Label Encoding the Categorical Variables","184047df":"<a id=\"lasso\"><\/a>\n## 3.1 Linear regression","c3aa2904":"### Get dummies for Catigory Variables.","c9db5c7c":"<a id=\"Introduction\"><\/a>\n# 1. Introduction\n\nCompetition Description:  \nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nAcknowledgments:  \nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. ","52b260c8":"# House Price Prediction\n\n## Contents\n\n### 1. [Introduction](#Introduction)\n\n### 2. [Data](#data)\n \n  2.1 [Load Data](#load)\n  \n  2.2 [Data Cleaning](#data_cleaning)\n   \n### 3. [Models](#models) \n\n  3.1 [Linear Regression](#lasso)\n  \n  3.2 [Xgboost](#xgboost)\n \n 3.3 [LightGBM](#LightGBM)\n \n 3.4 [Random Forest](#rf)\n \n  3.5 [Stacking Models](#stacking)\n\n  \n### 4. [Predictions](#predictions)\n","95cca945":"<a id=\"data\"><\/a>\n# 2. Data\n\nKaggle provide the script to pull data from given path.","0a1ab35a":"### Outliers","6c3495e2":"The skew seems now corrected and the data appears more normally distributed."}}