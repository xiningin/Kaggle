{"cell_type":{"4c54bef7":"code","80f5cd6d":"code","aa0b76c5":"code","84222087":"code","193c68f2":"code","b416e936":"code","31b46bce":"code","043bfccf":"code","d95924da":"code","b64f11f0":"code","ef3a239f":"code","487c8300":"code","08ab044d":"code","3465412e":"code","6dfedad4":"code","6ea63081":"code","779387ef":"code","d167360e":"code","355c4774":"code","6b9c817a":"code","61b79275":"code","3dc5648f":"code","ebd3b074":"code","76ae27da":"code","dad9a4dd":"code","461e6405":"code","949da54d":"code","38d84f73":"code","23c1efea":"code","f30c957d":"code","905020a7":"code","128bcc02":"code","7456d43f":"code","f44b4d73":"code","48f7157f":"code","5e3a2882":"code","0f1c0d6b":"code","d6609a85":"code","85f97498":"code","3c979c8f":"code","8122abcd":"code","43d16e03":"code","dead259d":"code","897349a3":"code","de7fb72b":"code","f638ea53":"code","70f69ef1":"markdown","10df3fa0":"markdown","c77ec239":"markdown","84f755b4":"markdown","ca4d98bd":"markdown","7eb91aef":"markdown","1b6e5dd3":"markdown","e15d8fe7":"markdown","c9644858":"markdown","20dca861":"markdown","ae9d1c31":"markdown","6615cc01":"markdown","711ee995":"markdown","ab9960ee":"markdown","d3dbbdc6":"markdown","e5c02211":"markdown","25e82c3b":"markdown","3ec20324":"markdown","3c056d76":"markdown","733bb1d4":"markdown","c1c61e73":"markdown","19d3a025":"markdown","3d18bc37":"markdown","7ec43196":"markdown","ca24ccf8":"markdown","97877152":"markdown","35062bfd":"markdown","f6b09f5d":"markdown","ca2472ad":"markdown","8e6d6363":"markdown","ddeafee1":"markdown","5cabec41":"markdown","950822ad":"markdown","c44a35cc":"markdown","d3b66f2c":"markdown","bf77141a":"markdown","4834309c":"markdown","041ca7f4":"markdown"},"source":{"4c54bef7":"!pip install comet_ml","80f5cd6d":"from comet_ml import Experiment\nexperiment = Experiment(api_key=\"gtqYH8ytl0mOgS2epkg2XgE2X\",\n                        project_name=\"general\", workspace=\"bryan981\")\n","aa0b76c5":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport nltk\n%matplotlib inline\n\nfrom nltk import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score\nfrom sklearn.preprocessing import MaxAbsScaler, StandardScaler","84222087":"# Importing The dataset\n# Training Datasets\ndf_path1 = r\"..\/input\/climate-change-belief-analysis\/train.csv\"\ntrain = pd.read_csv(df_path1)\n\n# Testing Datasets\ndf_path2 = r\"..\/input\/climate-change-belief-analysis\/test.csv\"\ntest = pd.read_csv(df_path2)","193c68f2":"variables = pd.DataFrame([\"sentiment\", \"message\", \"tweetid\"], columns = [\"variables\"])\nvariables[\"definition\"] = pd.DataFrame([\"sentiment of twitter messages\", \"twitter messages\", \"twitter unique id\"])\nvariables.head()","b416e936":"train.head()","31b46bce":"test.head()","043bfccf":"# Text Preprocessing\ndef data_cleaner(input_df):\n    train2 = input_df.copy()\n    # Removing Twitter Handles (@user)\n    def remove_pattern(user_input, pattern):\n        \n        '''Twitter handles are masked as @user due to concerns\n        surrounding privacy.Twitter handles do note \n        necessarily give necessary information around the \n        overall tweet.We will remove them'''\n    \n        r = re.findall(pattern, user_input)\n    \n        for element in r:\n            user_input = re.sub(element, \"\", user_input)\n    \n        return user_input\n    train2[\"message\"] = np.vectorize(remove_pattern)(train2[\"message\"], \"@[\\w]*\")\n\n    # Remove Special Characters,Numbers And Punctuations\n    train2[\"message\"] = train2[\"message\"].str.replace(\"[^a-zA-Z#]\", \" \") \n    \n    # Substituting multiple spaces with single space\n    #train2[\"message\"] = train2[\"message\"].str.replace(r'\\s+', ' ', flags = re.I)\n    \n    # Converting to Lowercase\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: x.lower())\n\n    # Removing Short Words\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: ' '.join([word for word in x.split() if len(word)>3]))\n\n    # Tokenization\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: nltk.word_tokenize(x))\n\n    # Remove Stop Words\n    def stop_words(user_input):\n        \n        '''A vast majoity of short words which are less than 3 letters long.\n        Words like 'ohh' and 'lol' do not give us much \n        information, thus important to remove them'''\n        \n        stop_words = set(stopwords.words('english'))\n        wordslist = [word for word in user_input if not word in stop_words]\n    \n        return wordslist\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: stop_words(x))\n\n    # Lemmatization\n    stemmer = SnowballStemmer(\"english\")\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n    \n    # Untokenization\n    train2[\"message\"] = train2[\"message\"].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n    \n    return train2","d95924da":"train_df = data_cleaner(train)\ntrain_df.head()","b64f11f0":"test_df = data_cleaner(test)\ntest_df.head()","ef3a239f":"train.describe().astype(int)","487c8300":"unclean_tweets = train[\"message\"].str.len()\nclean_tweets = train_df[\"message\"].str.len()\n\nplt.hist(unclean_tweets, label = 'Uncleaned_Tweet')\nplt.hist(clean_tweets, label = 'Cleaned_Tweet')\nplt.legend()\nplt.show()","08ab044d":"# Visualizing All Words In Clean Train Dataset\nallWords = ' '.join([text for text in train_df[\"message\"]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(allWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.title(\"Most Common Words\")\nplt.axis(\"off\") \nplt.show()","3465412e":"# Visualizing All The Positive Words In Clean Train Dataset\npositive_words =' '.join([text for text in train_df[\"message\"][train_df[\"sentiment\"] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(positive_words) \nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.title(\"Most Common Positive Words\")\nplt.axis(\"off\")\nplt.show()","6dfedad4":"# Visualizing All The Negative Words In Clean Train Dataset\nnegative_words =' '.join([text for text in train_df[\"message\"][train_df[\"sentiment\"] == -1]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(negative_words) \nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.title(\"Most Common Negative Words\")\nplt.axis(\"off\")\nplt.show()","6ea63081":"# Visualizing All The Neutral Words In Clean Train Dataset\nnormal_words =' '.join([text for text in train_df[\"message\"][train_df[\"sentiment\"] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(normal_words) \nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.title(\"Most Common Neutral Words\")\nplt.axis(\"off\")\nplt.show()","779387ef":"# Visualizing All The News Broadcast Words In Clean Train Dataset\nbroadcast_words =' '.join([text for text in train_df[\"message\"][train_df[\"sentiment\"] == 2]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(broadcast_words) \nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.title(\"Most Common News Broadcast Words\")\nplt.axis(\"off\")\nplt.show()","d167360e":"# Function To Extract hashtags\ndef hashtag_extract(user_input):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in user_input:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags","355c4774":"# Extracting Hashtags From News Broadcast\nHT_news = hashtag_extract(train_df[\"message\"][train_df[\"sentiment\"] == 2])\n# Extracting Hashtags From Positive Sentiments\nHT_positive = hashtag_extract(train_df[\"message\"][train_df[\"sentiment\"] == 1])\n# Extracting Hashtags From Neutral Sentiments\nHT_normal = hashtag_extract(train_df[\"message\"][train_df[\"sentiment\"] == 0])\n# Extracting Hashtags From Negative Sentiments\nHT_negative = hashtag_extract(train_df[\"message\"][train_df[\"sentiment\"] == -1])\n\n# Unnesting List\nht_news = sum(HT_news,[])\nht_positive = sum(HT_positive,[])\nht_normal = sum(HT_normal,[])\nht_negative = sum(HT_negative,[])","6b9c817a":"# News Broadcast\na = nltk.FreqDist(ht_news)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns = \"Count\", n = 20) \nplt.figure(figsize = (16,5))\nax = sns.barplot(data = d, x = \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","61b79275":"# Positive Sentiments\na = nltk.FreqDist(ht_positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns = \"Count\", n = 20) \nplt.figure(figsize = (16,5))\nax = sns.barplot(data = d, x = \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","3dc5648f":"# Neutral Sentiments\na = nltk.FreqDist(ht_normal)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns = \"Count\", n = 20) \nplt.figure(figsize = (16,5))\nax = sns.barplot(data = d, x = \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","ebd3b074":"# Negative Sentiments\na = nltk.FreqDist(ht_negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns = \"Count\", n = 20) \nplt.figure(figsize = (16,5))\nax = sns.barplot(data = d, x = \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","76ae27da":"test_df.describe().astype(int)","dad9a4dd":"unclean_tweets = test[\"message\"].str.len()\nclean_tweets = test_df[\"message\"].str.len()\n\nplt.hist(unclean_tweets, label = 'Uncleaned_Tweet')\nplt.hist(clean_tweets, label = 'Cleaned_Tweet')\nplt.legend()\nplt.show()","461e6405":"font = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\nfig = plt.figure()\ntrain_df['sentiment'].value_counts().plot(kind = 'bar', color = 'darkred')\nplt.xlabel(\"sentiment\", fontdict = font)\nplt.ylabel(\"Number Of Tweets\", fontdict = font)\nplt.title(\"Dataset labels distribution\")\nplt.show()","949da54d":"# Create A Column That Contains The Lengths Of The Messages\ntrain_df[\"message_len\"] = train_df[\"message\"].apply(len)\nsns_g = sns.FacetGrid(train_df, col = \"sentiment\")\nsns_g.map(plt.hist, \"message_len\")","38d84f73":"# Split Into X And Y\ny = train_df[\"sentiment\"]\nX = train_df.drop([\"sentiment\"], axis = 1)","23c1efea":"count_vect = CountVectorizer()\nX_counts = count_vect.fit_transform(X[\"message\"])\nX_counts.shape\ncount_vect.vocabulary_.get(u'algorithm')\n\ntest_count = count_vect.transform(test_df[\"message\"])","f30c957d":"sc = MaxAbsScaler().fit(X_counts)\nX_counts = sc.transform(X_counts)\ntest_count = sc.transform(test_count)","905020a7":"X_train, X_test, y_train, y_test = train_test_split(X_counts, y, test_size = 0.3, random_state = 42)","128bcc02":"lr_clf = LogisticRegression(max_iter = 2000)\nlr = lr_clf.fit(X_train, y_train)","7456d43f":"# LOGISTIC REGRESSION\nlr_pred = lr_clf.predict(X_test)\nprint('Classification Report')\nprint(\"F1-Score:\", classification_report(y_test.astype(int), lr_pred))","f44b4d73":"svc_clf = SVC(kernel = \"linear\")\nsvc = svc_clf.fit(X_train, y_train)","48f7157f":"svc_pred = svc_clf.predict(X_test)\nprint('Classification Report')\nprint(\"F1-Score:\", classification_report(y_test.astype(int), svc_pred))","5e3a2882":"nb_clf = MultinomialNB()\nnb = nb_clf.fit(X_train, y_train)","0f1c0d6b":"nb_pred = nb_clf.predict(X_test)\nprint('Classification Report')\nprint(\"F1-Score:\", classification_report(y_test.astype(int), nb_pred))","d6609a85":"test_pred = lr_clf.predict(test_count)","85f97498":"submission = pd.DataFrame(test_df[\"tweetid\"], columns = [\"tweetid\"])\nsubmission[\"sentiment\"] = test_pred","3c979c8f":"submission.head()","8122abcd":"#create a csv submission fo Kaggle\nsubmission.to_csv('submission.csv', index=False)","43d16e03":"# Saving Each Metrics To Add To A Dictionary For Logging\ncr = classification_report(y_test.astype(int), lr_pred)","dead259d":"# Create A Dictionaries For The Data We Want To Log\nparams = {\"random_state\": 42,\n          \"model_type\": \"Logistic Regression\",\n          \"scaler\": \"MaxAbsScaler\",}\n\nmetrics = {\"classification_report\": cr,}","897349a3":"# Log Our Parameters And Results\nexperiment.log_parameters(params)\nexperiment.log_metrics(metrics)","de7fb72b":"experiment.end()","f638ea53":"experiment.display()","70f69ef1":"<h4> Model Training <\/h4>","10df3fa0":"<p> Companies are building around making environmental impact . They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. <\/p>","c77ec239":"<h4> Model Evaluation <\/h4>","84f755b4":"<h3> Linear Support Vector <\/h3>","ca4d98bd":"<h4> Importing Datasets <\/h4>","7eb91aef":"1. Introduction\n2. Data Preprocessing and Cleaning\n3. Explanatory Data Analysis\n4. Model Building\n5. Predict Raw Data\n6. Conclusion","1b6e5dd3":"<h4> Train Dataset Description <\/h4>","e15d8fe7":"<h1> Exploratory Data Analysis <\/h1>","c9644858":"<h4> Model Training <\/h4>","20dca861":"<h4>Problem Statement<\/h4>","ae9d1c31":"<h4> Word Counts With CountVectorizer <\/h4>","6615cc01":"<h4> Word Cloud For Train Data <\/h4>","711ee995":"<h4> Split Into Train And Test Sets <\/h4>","ab9960ee":"<h4> Display Train Datasets First Five Row <\/h4>","d3dbbdc6":"<h4> Clean Vs Unclean Data <\/h4>","e5c02211":"<h4> Test Dataset Description <\/h4>","25e82c3b":"<h1> Data Preprocessing  <\/h1>","3ec20324":"<h4> Importing Modules <\/h4>","3c056d76":"<h3> Naive Bayes <\/h3>","733bb1d4":"<h3> Classifiers <\/h3>","c1c61e73":"<h3> Logistic Regression <\/h3>","19d3a025":"<h4> Model Evaluation <\/h4>","3d18bc37":"<h4> Understanding Sentiments in Tweets <\/h4>","7ec43196":"<h1> Predict Raw Data <\/h1>","ca24ccf8":"# Contents","97877152":"<h4> Hashtags <\/h4>","35062bfd":"<h4> Display Test Datasets First Five Row <\/h4>","f6b09f5d":"<h4> Feature Scaling <\/h4>","ca2472ad":"<h4> Model Training <\/h4>","8e6d6363":"<h4> Split Into Features And Labels <\/h4>","ddeafee1":"<h4> variables definition <\/h4>","5cabec41":"<p> Main Objective From The Dataset Is To Predict Whether Or\n   Not A Person Believes In Climate Change Based On Tweets <\/p>","950822ad":"<h3> Test Dataset <\/h3>","c44a35cc":"<h1> Introduction <\/h1>","d3b66f2c":"<h3> Train Dataset <\/h3>","bf77141a":"<h4> Model Evaluation <\/h4>","4834309c":"<h4> Clean Vs Unclean Data <\/h4>","041ca7f4":"<h1> Model Building <\/h1>"}}