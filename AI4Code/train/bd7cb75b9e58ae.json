{"cell_type":{"53049ea7":"code","c4439082":"code","23a82036":"code","ca86a251":"code","96006757":"code","c5a7ee57":"code","23bbd532":"code","5da7f012":"markdown","7cbd456f":"markdown","710255cb":"markdown","df42787f":"markdown","1313fa27":"markdown","861dc78e":"markdown","1e67d03a":"markdown"},"source":{"53049ea7":"from typing import Optional, Union\n\nimport holoviews as hv\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.utils.validation import check_array\nfrom sklearn.base import TransformerMixin, BaseEstimator, ClusterMixin\nfrom sklearn.mixture._base import BaseMixture\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import check_is_fitted\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import clone\nfrom sklearn.ensemble._base import BaseEnsemble, _set_random_states\nfrom sklearn.linear_model._base import LinearModel\n\nfrom statsmodels.gam.smooth_basis import _eval_bspline_basis\nfrom statsmodels.gam.tests.test_penalized import df_autos\n\n\nhv.extension('bokeh')\n\nclass FeatureSampler(BaseEstimator, TransformerMixin):\n    def __init__(self, n_features: int = 1, random_state=None):\n        self.n_features = n_features\n        self.random_state = random_state\n\n    def fit(self, X: np.ndarray, y=None):\n        self.n_features_ = min(X.shape[1], self.n_features)\n        self.random_state_ = check_random_state(self.random_state)\n        self.feature_ = np.random.choice(X.shape[1], self.n_features)\n\n        return self\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        check_is_fitted(self)\n        check_array(X)\n\n        if isinstance(X, np.ndarray):\n            return X[:, self.feature_]\n        elif isinstance(X, pd.DataFrame):\n            return X.iloc[:, self.feature_]\n\n\nclass TruncatedPowerBasis(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        degree: int = 3,\n        knots: Optional[Union[int, np.ndarray, ClusterMixin, BaseMixture]] = None,\n        random_state=None,\n    ):\n        self.knots = knots\n        self.degree = degree\n        self.random_state = random_state\n\n    def fit(self, X: np.ndarray, y=None):\n        if not isinstance(self.degree, int) or self.degree < 0:\n            raise ValueError(\"Must be an integer greater or equal to 0\")\n\n        self.random_state_ = check_random_state(self.random_state)\n        check_array(X)\n\n        if X.shape[1] > 1:\n            raise ValueError(\n                \"TruncatedPowerBasis only accepts 2D arrays with 1 feature, this has shape %s\"\n                % X.shape[1]\n            )\n\n        if isinstance(self.knots, ClusterMixin) or isinstance(self.knots, BaseMixture):\n            self.knots.fit(X)\n\n            if hasattr(self.knots, \"cluster_centers_\"):\n                self.knots_ = self.knots.cluster_centers_.reshape(1, -1)\n            elif hasattr(self.knots, \"means_\"):\n                self.knots_ = self.knots.means_.reshape(1, -1)\n            elif hasattr(self.knots, \"medoids_\"):\n                self.knots_ = self.knots.medoids_.reshape(1, -1)\n            else:\n                raise TypeError(\n                    \"Model does not have cluster_centers_, means_ or medoids_ attributes.\"\n                )\n        elif isinstance(self.knots, int):\n            indexes = self.random_state_.choice(X.shape[0], self.knots)\n\n            self.knots_ = X[indexes, :].reshape(1, -1)\n        elif self.knots is None and isinstance(X, np.ndarray):\n            self.knots_ = np.unique(X.reshape(1, -1))\n        elif self.knots is None and isinstance(X, pd.DataFrame):\n            self.knots_ = np.unique(X.to_numpy().reshape(1, -1))\n            name = X.columns.astype(str)[0]\n            self.names_ = [\n                f\"|{name} - ({xi})|^{self.degree}\"\n                for xi in (self.knots_.round(3).astype(str).flatten().tolist())\n            ]\n        else:\n            self.knots_ = np.unique(X.T)\n\n        return self\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        check_is_fitted(self)\n        check_array(X)\n\n        if X.shape[1] > 1:\n            raise ValueError(\n                \"TruncatedPowerBasis only accepts 2D arrays with 1 feature\"\n            )\n\n        if isinstance(X, np.ndarray):\n            return (X - self.knots_) ** self.degree\n        elif isinstance(X, pd.DataFrame):\n            return (\n                pd.DataFrame(X.to_numpy() - np.array(self.knots_), columns=self.names_)\n                .abs()\n                .pow(self.degree)\n            )\n\n\nclass CoxdeBoorBasis(TruncatedPowerBasis):\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        check_is_fitted(self)\n        check_array(X)\n\n        if X.shape[1] > 1:\n            raise ValueError(\n                \"TruncatedPowerBasis only accepts 2D arrays with 1 feature\"\n            )\n\n        if isinstance(X, np.ndarray):\n            x = X.flatten()\n            return _eval_bspline_basis(x, x, self.degree, 0)\n        elif isinstance(X, pd.DataFrame):\n            x = X.iloc[:, 0]\n            return _eval_bspline_basis(x, x, self.degree, 0)\n\n\nclass GeneralizedAdditiveModel(BaseEnsemble):\n    def __init__(\n        self,\n        base_estimator=LinearRegression(fit_intercept=False),\n        estimator_params=tuple(),\n        basis=TruncatedPowerBasis(1),\n        max_iter=10,\n        tol=1e-6,\n    ):\n        self.base_estimator = base_estimator\n        self.basis = basis\n        self.max_iter = max_iter\n        self.tol = tol\n        self.estimator_params = estimator_params\n\n    def _make_estimator(self, feature, append=True, random_state=None):\n        estimator = Pipeline(\n            [\n                (\n                    \"selector\",\n                    ColumnTransformer(\n                        [(str(feature), FunctionTransformer(), [feature])]\n                    ),\n                ),\n                (\"basis\", clone(self.basis_)),\n                (\"estimator\", clone(self.base_estimator_)),\n            ]\n        )\n        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n\n        if random_state is not None:\n            _set_random_states(estimator, random_state)\n\n        if append:\n            self.estimators_.append(estimator)\n\n        return estimator\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        self.n_features_ = X.shape[1]\n        if isinstance(self.base_estimator, LinearModel):\n            self.base_estimator_ = self.base_estimator\n        else:\n            raise TypeError(\"This is not an instance of a linear model\")\n        self.basis_ = self.basis\n\n        self.intercept_ = y.mean()\n        errors = y - self.intercept_\n\n        self.estimators_ = []\n        for model, feature in enumerate(range(self.n_features_)):\n            self._make_estimator(feature)\n            self.estimators_[model].fit(X, errors)\n            self.estimators_[feature].named_steps[\"estimator\"].intercept_ -= (\n                self.estimators_[feature].predict(X).mean()\n            )\n            errors = y - self._predict(X)\n\n        for step in range(self.max_iter):\n            for model, feature in enumerate(range(self.n_features_)):\n                # backfitting step\n                errors = y - self._predict(X, exclude=model)\n                self.estimators_[feature].fit(X, errors)\n\n                # mean centering of estimated function\n                self.estimators_[feature].named_steps[\"estimator\"].intercept_ -= (\n                    self.estimators_[feature].predict(X).mean()\n                )\n                prev_errors = errors\n\n            if np.all(np.abs(prev_errors - errors) < self.tol):\n                break\n\n        return self\n\n    def _predict(self, X: np.ndarray, exclude=None):\n        if exclude is None:\n            return (\n                sum((estimator.predict(X) for estimator in self.estimators_))\n                + self.intercept_\n            )\n        else:\n            return (\n                sum(\n                    (\n                        estimator.predict(X)\n                        for i, estimator in enumerate(self.estimators_)\n                        if i != exclude\n                    )\n                )\n                + self.intercept_\n            )\n\n    def predict(self, X: np.ndarray):\n        return self._predict(X)\n","c4439082":"df_autos.head()","23a82036":"df_autos.city_mpg.plot.kde()","ca86a251":"X, y = (pd.concat([df_autos.loc[:,['weight','hp']], \n                   pd.get_dummies(df_autos.fuel),\n                   pd.get_dummies(df_autos.drive)], axis=1),\n                   df_autos.city_mpg.apply(np.log))\nsns.pairplot(X.loc[:,['weight','hp']].assign(city_mpg = y))","96006757":"gam = GeneralizedAdditiveModel(max_iter=50, base_estimator=LinearRegression(fit_intercept=False), basis=CoxdeBoorBasis())\ngam.fit(X, y)\n(gam.predict(X) - y).plot.kde(title='Distribution of Errors')","c5a7ee57":"smooth_basis = pd.DataFrame({f'{name}_spline': model.predict(X) for name, model in zip(X.columns, gam.estimators_)}).assign(city_mpg = y)\nsmooth_basis.head()","23bbd532":"sns.pairplot(smooth_basis.loc[:,['weight_spline', 'hp_spline', 'city_mpg']])","5da7f012":"If we look at the pairwise plots produced by the this data, we can see the 'non-linear' relationships between weight and horsepower and city_mpg.  horsepower has a marginally decreasing relationship with city_mpg.  ","7cbd456f":"Our model estimates city_mpg by adding together splines for each of our features fit against city_mpg. ","710255cb":" The response variance,  miles per gallon for city driving, is likely gamma or log-normally distributed and for now we will estimate its log normal distribution by taking the log of the data. ","df42787f":"My GeneralizedAdditiveModel implementation inherits from scikit-learn's meta-estimator and is instantiated with our spline bases model and a linear regression model, which are both used in backfitting.  After estimating our model, we can the tightly distributed symmetric errors of our model.  ","1313fa27":"So after a lot of code, which those nerdy among you may explore, I have opted to look at the autos dataset offered in statsmodels. I have chosen this datset not just due to its simplicity, but also as it is provided as the benchmark dataset in statsmodeles own GAM examples. The dataset has 4 features, two of which are categorical.  ","861dc78e":"This post is partly inspired by an office-cooler conversation I had at a Machine Learning Meetup with a local Senior Data Scientist, which went something like this:\n  \n  \n \n_\"What have you been up to?\" **- Senior Data Scientist**_  \n  \n_**Marcus -** \"Been busy with a Masters-level course in Advanced Regression\"_  \n  \n_\"So how advanced does regression get?\" **- Senior Data Scientist**_  \n  \n_**Marcus** - \"haha. Quite advanced. \ud83e\udd26\u200d\u2642\ufe0f\"_  \n  \n  \nWe live in the middle of a Deep Learning revolution, every week, we see an exciting headline in our news feed detailing the exciting Deep Learning-led breakthrough in Natural Language Processing, Reinforcement Learning and Computer Vision. However, there is a reason Boosted Trees still dominate Kaggle Competitions- Neural-networks are terrible at dealing with heterogeneous data and no-surprises: most data is heterogeneous.  \n  \nOne major misconception about linear models is that they only model linear relationships- not true. If we look to the Gauss-Markov assumptions, Linear Model is assumed in be linear in the parameters, which means we can do whatever transformation of the original feature space and still keep linear in the parameters. In fact, economists have been using log-transforms to model multiplicative relationships between variables for years when computing elasticities and many researchers will use domain knowledge and automated feature selection methods like step-wise feature selection to model a wide array of polynomial relationships.   \n\nA major concern in the recent years is how to deal with algorithmic fairness and model explainability for deep models and while I love kernel methods like LIME and SHAP and gradient-based methods in Integrated Gradients- why now engineer for explainability from the get-go.  Explainability can have many advantages and insights into business strategy and can help diagnose models and build client trust and buy-in.  \n  \n'Generalized' Linear Models allow for many different distributional assumptions. These assumptions are not distributional assumptions around the feature space but around the conditional distribution of the response or the distribution of the errors.  While you can take a Maximum Likelihood approach to Deep Learning Models- and I am happy to see tools like Tensorflow Probability making this increasingly popular- it is difficult to rationalize about feature importance, or the confidence one has in their estimates.  \n  \nCurrently, one of my favourite models is the Multivariate Adaptive Regression Spline (MARS) Model which proposes a method for optimally identifying hinge functions (think ReLU) and interactions between variables for use in a linear model. This allows one to easily model and explain the non-linear relationships and interactions between features and the response variable while still remaining linear in the parameters, by effectively and robustly partitioning the feature-space as you would do with tree-based models. MARS models have a fantastic implementation in R using the Earth package and in python using the py-Earth package, which is a scikit-learn contrib project.  \n  \nAnother fantastic 'non-linear' linear-model are Generalized Additive Models (GAMs). These models iteratively learn optimal splines which map from each feature onto the response variable. These methods can be computationally expensive and require lots of memory for large datasets but have amazing interpretations and insights for those doing modelling and they not only tell you how much variance is explained by particular features but also the specific 'non-linear' transform which explains that relationship.  \n  \nOne reason why these flexible linear models are interesting is that they allow us to identify effects which take place beyond a certain threshold. When I think of modelling non-linearities, I often think of the data generating process. One reason people are concerned with modelling non-linearities is that variable may have some 'latently linear' correlation, but this may not be observed in the non-linear random transformation the particular 'latently linear' feature underwent. Think of the relationship between petrol consumption and speed in a car. This is marginally increasing relationship- i.e. we require more fuel for the next 1km\/h that the last 1km\/h increase in speed. While this relationship is non-linear, it can be thought of as 'latently linear' if we find a projection of the data which that allows us to model it linearly.  \n  \nIn this notebook, we are going to look at a Scikit-learn-compatible Implementation of Generalized Additive Models, which I have written. GAM's have two important components if you are familiar with Least Squares Regression: a Spline Basis, which allows us to learn the non-linear projection of our variances, and a back-fitting procedure, which allows us to fit these complicated models on these sparse and often correlation new feature spaces.  \n\nOne of the most popular Spline Basis functions is the Truncated Power Basis which is **$|x - \\xi_j|^p$** in which $\\xi_i$ is a 'knot' which is traditionally placed at every data points, and $p$ is used to create a non-linear polynomial basis.  As these  Truncated Power Basis, are highly correlated with one another, the Cox-de Boor Function is used to create an orthogonal version of this basis to stabilize parameter estimation.  While, in recent years, methods in Boosting and Bagging have gained popularity in fitting GAM models, the original back-fitting methodology proposed by Tsitsiklis and Hastie (yes, the people who wrote the Elements of Statistical Learning Book) cycles through basis, continually fitting the spline basis for each univariate feature on the residuals of the existing model until convergence. ","1e67d03a":"If we look at these estimates we can now see the new linear basis we have learned to project our features onto and their relationships with city_mpg. "}}