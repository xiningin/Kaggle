{"cell_type":{"b8b0fe7d":"code","8caeba14":"code","0e8f879a":"code","5a40c811":"code","a53dc4fe":"code","a5bc8752":"code","c347c7af":"code","1411fd52":"code","97cec4b1":"code","870cb25b":"code","0c4587cf":"code","325dd865":"code","a66800d4":"code","e91f02cb":"code","e5912d5f":"code","3a38965d":"code","53f01a1d":"code","30c02c5b":"code","51286e51":"code","ebcb61cb":"code","ae32e6e5":"code","2db5180b":"code","7728751e":"code","f4ece835":"code","1cf5c6ea":"code","59abbcc5":"code","99c00c97":"code","2c4b2871":"code","d09b4146":"code","dcf86087":"code","50233a1c":"code","d2bf19af":"code","bb8216c2":"code","ddc09492":"code","60176a3f":"code","0af08b81":"code","79015f16":"code","3fc28df1":"code","00363f45":"code","2a07ad50":"code","e7670137":"code","40333644":"code","27635382":"code","cc25627a":"code","2b7828b6":"code","cdb7a472":"code","047ab301":"code","b50b3e35":"markdown","2fecefd5":"markdown","0e88fdbd":"markdown","c25af391":"markdown"},"source":{"b8b0fe7d":"import os\nfrom os.path import isdir, join\nfrom pathlib import Path\nimport pandas as pd","8caeba14":"# Math\nimport numpy as np\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport librosa\nfrom sklearn.decomposition import PCA","0e8f879a":"# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport IPython.display as ipd\nimport librosa.display\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline","5a40c811":"train_audio_path = '\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train\/audio'\nfilename = 'yes\/0a7c2a8d_nohash_0.wav'\nsample_rate, samples = wavfile.read(join(train_audio_path, filename))","a53dc4fe":"ll \/kaggle\/input\/tensorflow-speech-recognition-challenge\/train\/audio\/yes\/0a7c2a8d_nohash_0.wav -h","a5bc8752":"sample_rate, samples","c347c7af":"# define a function that calculates spectrogram\ndef log_specgram(audio, smaple_rate, window_size=20, step_size=10, eps=1e-10):\n    nperseg = int(round(window_size * sample_rate \/ 1e3))\n    noverlap = int(round(step_size * sample_rate \/ 1e3))\n    freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', \\\n        nperseg=nperseg, noverlap=noverlap, detrend=False)\n    return freqs, times, np.log(spec.T.astype(np.float32) + eps)","1411fd52":"freqs, times, spec = log_specgram(samples, sample_rate)","97cec4b1":"freqs.shape, times.shape, spec.shape","870cb25b":"fig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + filename)\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate\/len(samples), sample_rate), samples)\n\nax2 = fig.add_subplot(212)\nax2.imshow(spec.T, aspect='auto', origin='lower', \\\n    extent=[times.min(), times.max(), freqs.min(), freqs.max()])\nax2.set_yticks(freqs[::16])\nax2.set_xticks(times[::16])\nax2.set_title('Spectrogram of ' + filename)\nax2.set_ylabel('Freqs in HZ')\nax2.set_xlabel('Seconds')","0c4587cf":"# input normalization for NN\nmean = np.mean(spec, axis=0)\nstd = np.std(spec, axis=0)\nspectrogram = (spec - mean) \/ std","325dd865":"spectrogram.shape","a66800d4":"sample_rate","e91f02cb":"# MFCC \u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\nS = librosa.feature.melspectrogram(samples.astype(float), sr=sample_rate, n_mels=128)\n# Convert to log scale (dB). we'll use peak power (max) as reference\nlog_S = librosa.power_to_db(S, ref=np.max)\nplt.figure(figsize=(12, 4))\nlibrosa.display.specshow(log_S, sr=sample_rate, x_axis='time', y_axis='mel')\nplt.title('Mel power spectrogram')\nplt.colorbar(format='%+02.0f dB')\nplt.tight_layout()","e5912d5f":"mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\ndelta2_mfcc = librosa.feature.delta(mfcc, order=2)\nmfcc.shape, delta2_mfcc.shape","3a38965d":"plt.figure(figsize=(12, 4))\nlibrosa.display.specshow(delta2_mfcc)\nplt.ylabel('MFCC coeffs')\nplt.xlabel('Time')\nplt.title('MFCC')\nplt.colorbar()\nplt.tight_layout()","53f01a1d":"# Spectrogram in 3d\ndata = [go.Surface(x=times, y=freqs, z=spectrogram.T)]\nlayout = go.Layout(\n    title='Spectrogram of \"yes\" in 3d',\n    scene = dict(\n        yaxis = dict(title='Frequencies', range=[freqs.min(), freqs.max()]),\n        xaxis = dict(title='Time', range=[times.min(), times.max()]),\n        zaxis = dict(title='Log amplitude')\n    )\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","30c02c5b":"# Silence removal\nprint(samples.shape)\nipd.Audio(samples, rate=sample_rate)","51286e51":"sample_cut = samples[4000:13000]\nprint(sample_cut.shape)\nipd.Audio(sample_cut, rate=sample_rate)","ebcb61cb":"# guessed slignment of each letter in 'yes'\nfreqs, times, spectrogram_cut = log_specgram(sample_cut, sample_rate)\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + filename)\nax1.set_ylabel('Amplitude')\nax1.plot(sample_cut)\n\nax2 = fig.add_subplot(212)\nax2.set_title('Spectrogram of ' + filename)\nax2.set_ylabel('Frequencies * 0.1')\nax2.set_xlabel('Samples')\nax2.imshow(spectrogram_cut.T, aspect='auto', origin='lower',\n          extent=[times.min(), times.max(), freqs.min(), freqs.max()])\nax2.set_yticks(freqs[::16])\nax2.set_xticks(times[::16])\nax2.text(0.06, 1000, 'Y', fontsize=18)\nax2.text(0.17, 1000, 'E', fontsize=18)\nax2.text(0.36, 1000, 'S', fontsize=18)\nxcoords = [0.025, 0.11, 0.23, 0.49]\nfor xc in xcoords:\n    ax1.axvline(x=xc*16000, c='r')\n    ax2.axvline(x=xc, c='r')","ae32e6e5":"# Resampling - dimension reduction\nfilename = 'happy\/0b09edd3_nohash_0.wav'\nnew_sample_rate = 8000\nsample_rate, samples = wavfile.read(join(train_audio_path, filename))\nresampled = signal.resample(samples, int(new_sample_rate\/sample_rate*samples.shape[0]))","2db5180b":"int(new_sample_rate\/sample_rate*samples.shape[0])","7728751e":"ipd.Audio(samples, rate=sample_rate)","f4ece835":"ipd.Audio(resampled, rate=new_sample_rate)","1cf5c6ea":"# Fast fourier transform\ndef custom_fft(y, fs):\n    T = 1.0 \/ fs\n    N = y.shape[0]\n    yf = fft(y)\n    xf = np.linspace(0.0, 1.0\/(2.0*T), N\/\/2)\n    vals = 2.0 \/ N * np.abs(yf[0:N\/\/2])\n    return xf, vals","59abbcc5":"xf, vals = custom_fft(samples, sample_rate)\nplt.figure(figsize=(12, 4))\nplt.title('FFT of recording sampled with ' + str(sample_rate) + ' Hz')\nplt.plot(xf, vals)\nplt.xlabel('Frequency')\nplt.grid()\nplt.show()","99c00c97":"xf, vals = custom_fft(resampled, new_sample_rate)\nplt.figure(figsize=(12, 4))\nplt.title('FFT of recording sampled with ' + str(new_sample_rate) + ' Hz')\nplt.plot(xf, vals)\nplt.xlabel('Frequency')\nplt.grid()\nplt.show()","2c4b2871":"# number of records\ndirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\ndirs.sort()\nprint('Number of labels: ' + str(len(dirs)))","d09b4146":"# calculate\nnumber_of_recordings = []\nfor direct in dirs:\n    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n    number_of_recordings.append(len(waves))","dcf86087":"# number of total recordings in train sets\nsum(number_of_recordings)","50233a1c":"# plot\ndata = [go.Histogram(x=dirs, y=number_of_recordings)]\ntrace = go.Bar(\n    x=dirs,\n    y=number_of_recordings,\n    marker=dict(color=number_of_recordings, colorscale='viridis', showscale=True)\n)\nlayout=go.Layout(\n    title='Number of recordings in given label',\n    xaxis=dict(title='Words'),\n    yaxis=dict(title='Number of recordings')\n)\npy.iplot(go.Figure(data=[trace], layout=layout))","d2bf19af":"# Speaker doesn't occur in both train and test data sets\nfilenames = ['on\/004ae714_nohash_0.wav', 'on\/0137b3f4_nohash_0.wav']\nfor filename in filenames:\n    sample_rate, samples = wavfile.read(join(train_audio_path, filename))\n    xf, vals = custom_fft(samples, sample_rate)\n    plt.figure(figsize=(12, 4))\n    plt.title('FFT of speaker ' + filename[4:11])\n    plt.plot(xf, vals)\n    plt.xlabel('Frequency')\n    plt.grid()\n    plt.show()","bb8216c2":"print('Speaker ' + filenames[0][4:11])\nipd.Audio(join(train_audio_path, filenames[0]))","ddc09492":"print('Speaker ' + filenames[1][4:11])\nipd.Audio(join(train_audio_path, filenames[1]))","60176a3f":"# Recordings with some weird silence\nfilename = 'yes\/01bb6a2a_nohash_1.wav'\nsample_rate, samples = wavfile.read(join(train_audio_path, filename))\nfreqs, times, spectrogram = log_specgram(samples, sample_rate)\nplt.figure(figsize=(10, 7))\nplt.title('Spectrogram of ' + filename)\nplt.ylabel('Freqs')\nplt.xlabel('Times')\nplt.imshow(spectrogram.T, aspect='auto', origin='lower', \\\n    extent=[times.min(), times.max(), freqs.min(), freqs.max()])\nplt.yticks(freqs[::16])\nplt.xticks(times[::16])\nplt.show()","0af08b81":"ipd.Audio(join(train_audio_path, filename))","79015f16":"# Calculating number of recordings shorter than 1 second\nnum_of_shorter = 0\nfor d in dirs:\n    waves = [f for f in os.listdir(join(train_audio_path, d)) if f.endswith('.wav')]\n    for wav in waves:\n        sample_rate, samples = wavfile.read(join(train_audio_path, d, wav))\n        if samples.shape[0] < sample_rate:\n            num_of_shorter += 1\nprint('Number of recordings shorter than 1 second: ' + str(num_of_shorter))","3fc28df1":"# Mean spectrograms and FFT for each word\nto_keep = 'yes no up down left right on off stop go'.split()\ndirs = [d for d in dirs if d in to_keep]\nprint(dirs)\nfor d in dirs:\n    vals_all = []\n    spec_all = []\n    waves = [f for f in os.listdir(join(train_audio_path, d)) if f.endswith('.wav')]\n    for wav in waves:\n        sample_rate, samples = wavfile.read(join(train_audio_path, d, wav))\n        if samples.shape[0] != 16000: continue\n        xf, vals = custom_fft(samples, 16000)\n        vals_all.append(vals)\n        freqs, times, spec = log_specgram(samples, 16000)\n        spec_all.append(spec)\n    plt.figure(figsize=(14, 4))\n    plt.subplot(121)\n    plt.title('Mean fft of ' + d)\n    plt.plot(np.mean(np.array(vals_all), axis=0))\n    plt.grid()\n    plt.subplot(122)\n    plt.title('Mean spectrogram of ' + d)\n    plt.imshow(np.mean(np.array(spec_all), axis=0).T, aspect='auto', origin='lower', \\\n        extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n    plt.yticks(freqs[::16])\n    plt.xticks(times[::16])\n    plt.show()","00363f45":"# Frequenciy components across the words\ndef violinplot_frequency(dirs, freq_ind):\n    spec_all = []\n    for idx, d in enumerate(dirs):\n        spec_all.append([])\n        waves = [f for f in os.listdir(join(train_audio_path, d)) if f.endswith('.wav')]\n        for wav in waves[:100]:\n            sample_rate, samples = wavfile.read(join(train_audio_path, d, wav))\n            freqs, times, spec = log_specgram(samples, sample_rate)\n            spec_all[idx].extend(spec[:, freq_ind])\n    minimum = min([len(spec) for spec in spec_all])\n    spec_all = np.array([spec[:minimum] for spec in spec_all])\n    plt.figure(figsize=(13, 7))\n    plt.title('Frequency ' + str(freqs[freq_ind]) + ' Hz')\n    plt.ylabel('Amount of frequency in a word')\n    plt.xlabel('Words')\n    sns.violinplot(data=pd.DataFrame(spec_all.T, columns=dirs))\n    plt.show()      ","2a07ad50":"violinplot_frequency(dirs, 20)","e7670137":"violinplot_frequency(dirs, 50)","40333644":"violinplot_frequency(dirs, 120)","27635382":"# Anomaly detection\nfft_all = []\nnames = []\nfor d in dirs:\n    waves = [f for f in os.listdir(join(train_audio_path, d)) if f.endswith('.wav')]\n    for wav in waves:\n        sample_rate, samples = wavfile.read(join(train_audio_path, d, wav))\n        if samples.shape[0] != sample_rate:\n            samples = np.append(samples, np.zeros((sample_rate - samples.shape[0],)))\n        x, val = custom_fft(samples, sample_rate)\n        fft_all.append(val)\n        names.append(join(d, wav))\nfft_all = np.array(fft_all)\n\n# Normalization\nfft_all = (fft_all - np.mean(fft_all, axis=0)) \/ np.std(fft_all, axis=0)\n\n# Dimension reduction\npca = PCA(n_components=3)\nfft_all = pca.fit_transform(fft_all)\n\ndef interactive_3d_plot(data, names):\n    scatt = go.Scatter3d(x=data[:, 0], y=data[:, 1], z=data[:, 2], mode='markers', text=names)\n    data = go.Data([scatt])\n    layout = go.Layout(title='Anomaly detection')\n    figure = go.Figure(data=data, layout=layout)\n    py.iplot(figure)\ninteractive_3d_plot(fft_all, names)","cc25627a":"len(fft_all)","2b7828b6":"# Anomaly samples\nprint('Recording go\/0487ba9b_nohash_0.wav')\nipd.Audio(join(train_audio_path, 'go\/0487ba9b_nohash_0.wav'))","cdb7a472":"print('Recording yes\/e4b02540_nohash_0.wav')\nipd.Audio(join(train_audio_path, 'yes\/e4b02540_nohash_0.wav'))","047ab301":"print('Recording seven\/b1114e4f_nohash_0.wav')\nipd.Audio(join(train_audio_path, 'seven\/b1114e4f_nohash_0.wav'))","b50b3e35":"1. Encoder-decoder\n2. RNNs with CTC loss\n3. Classic speech recognition approach like Kaldi.\n4. Deep CNN \u2714","2fecefd5":"# 2. Basic data statistics","0e88fdbd":"# 1. Visualization","c25af391":"# 3. possible solutions based on above analysis"}}