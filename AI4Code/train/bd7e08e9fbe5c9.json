{"cell_type":{"a9a39ec6":"code","f4fa5cfe":"code","5733f6f7":"code","12e854ee":"code","c28f64ab":"code","939fbab1":"code","27bafbf0":"code","dae99771":"code","a8bde66f":"code","71ded8b8":"code","da0c7280":"code","049da647":"code","0c621dc0":"code","6b68e8f8":"code","6805191c":"code","34e32988":"code","dc9f99a1":"code","90525193":"code","4640ef49":"code","798b3b40":"code","b76d59ca":"code","77b14597":"code","713cc3d6":"code","605244fc":"code","b87886d5":"code","7dabbee9":"code","dbd35188":"code","862fa7cf":"markdown"},"source":{"a9a39ec6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f4fa5cfe":"import torch\nfrom torch import optim, nn\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport numpy as np\nfrom torch.utils.data.sampler import SubsetRandomSampler","5733f6f7":"train_dir = '..\/input\/car_data\/car_data\/train'\ntest_dir = '..\/input\/car_data\/car_data\/test'","12e854ee":"train_transforms = transforms.Compose([transforms.RandomRotation(30),transforms.RandomResizedCrop(224),transforms.RandomHorizontalFlip(),transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\ntest_transforms = transforms.Compose([transforms.Resize(256),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])","c28f64ab":"train_data = datasets.ImageFolder(train_dir , transform=train_transforms)\ntest_data = datasets.ImageFolder(test_dir, transform=test_transforms)","939fbab1":"print(len(train_data))\nprint(len(test_data))","27bafbf0":"valid_size = 0.2\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]","dae99771":"train_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","a8bde66f":"trainloader = torch.utils.data.DataLoader(train_data, batch_size=64,sampler=train_sampler)\nvalidloader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=valid_sampler)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=64)","71ded8b8":"print(len(trainloader))\nprint(len(validloader))\nprint(len(testloader))","da0c7280":"print(len(train_data)\/64)\nprint(len(test_data)\/64)","049da647":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.autograd import Variable\n\n\ndef test_network(net, trainloader):\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    dataiter = iter(trainloader)\n    images, labels = dataiter.next()\n\n    # Create Variables for the inputs and targets\n    inputs = Variable(images)\n    targets = Variable(images)\n\n    # Clear the gradients from all Variables\n    optimizer.zero_grad()\n\n    # Forward pass, then backward pass, then update weights\n    output = net.forward(inputs)\n    loss = criterion(output, targets)\n    loss.backward()\n    optimizer.step()\n    return True\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\ndef view_recon(img, recon):\n    ''' Function for displaying an image (as a PyTorch Tensor) and its\n        reconstruction also a PyTorch Tensor\n    '''\n\n    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n    axes[0].imshow(img.numpy().squeeze())\n    axes[1].imshow(recon.data.numpy().squeeze())\n    for ax in axes:\n        ax.axis('off')\n        ax.set_adjustable('box-forced')","0c621dc0":"images, labels = next(iter(trainloader))\n","6b68e8f8":"import torchvision\ngrid = torchvision.utils.make_grid(images, nrow = 20, padding = 2)\nplt.figure(figsize = (15, 15))  \nplt.imshow(np.transpose(grid, (1, 2, 0)))   \nprint('labels:', labels)","6805191c":"for i in range(2):\n    imshow(images[i])","34e32988":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","dc9f99a1":"from torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = models.resnet152(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n        \nmodel.fc = nn.Sequential(nn.Linear(2048, 512),nn.ReLU(),nn.Linear(512,196),nn.LogSoftmax(dim=1))    \n\ncriterion = nn.NLLLoss()\n\n\noptimizer = torch.optim.Adam(filter(lambda p:p.requires_grad,model.fc.parameters()) , lr = 0.001)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n\n\nmodel.to(device);","90525193":"def train_and_test(e):\n    epochs = e\n    train_losses , test_losses = [] , []\n    valid_loss_min = np.Inf \n    model.train()\n    for epoch in range(epochs):\n      running_loss = 0\n      batch = 0\n      scheduler.step()\n      for images , labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        batch += 1\n        if batch % 10 == 0:\n            print(f\" epoch {epoch + 1} batch {batch} completed\")\n      test_loss = 0\n      accuracy = 0\n      with torch.no_grad():\n        model.eval() \n        for images , labels in validloader:\n          images, labels = images.to(device), labels.to(device)\n          logps = model(images) \n          test_loss += criterion(logps,labels) \n          ps = torch.exp(logps)\n          top_p , top_class = ps.topk(1,dim=1)\n          equals = top_class == labels.view(*top_class.shape)\n          accuracy += torch.mean(equals.type(torch.FloatTensor))\n      train_losses.append(running_loss\/len(trainloader))\n      test_losses.append(test_loss\/len(validloader))\n      print(\"Epoch: {}\/{}.. \".format(epoch+1, epochs),\"Training Loss: {:.3f}.. \".format(running_loss\/len(trainloader)),\"Valid Loss: {:.3f}.. \".format(test_loss\/len(validloader)),\n        \"Valid Accuracy: {:.3f}\".format(accuracy\/len(validloader)))\n      model.train() \n      if test_loss\/len(validloader) <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,test_loss\/len(validloader))) \n        torch.save(model.state_dict(), path)\n        valid_loss_min = test_loss\/len(validloader)","4640ef49":"model_save_name = 'checkpoint.pth'\npath = F\"\/kaggle\/working\/{model_save_name}\"","798b3b40":"def load_model():\n    torch.load(path)","b76d59ca":"load_model()","77b14597":"os.listdir('\/kaggle\/working')","713cc3d6":"accuracy = 0\nwith torch.no_grad():\n    model.eval()\n    p_labels = []\n    img_ids = []\n    i = 0\n    for inputs, labels in testloader:\n        i += 1\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        temp_acc = torch.sum(preds == labels.data)\n        accuracy += temp_acc\n        p_labels.append(preds)\n        if i % 50 == 0:\n            print(f\"batch {i} completed...\")\n    for dir in os.listdir(test_dir):\n        for file in os.listdir(os.path.join(test_dir, dir)):\n            img_id = os.path.splitext(file)[0]\n            img_ids.append(img_id)\n    print('Accuracy =====>>', accuracy.item()\/len(test_data))","605244fc":"pred_labels_expanded = []\nfor l in p_labels:\n    for l1 in l:\n        pred_labels_expanded.append(l1.item())\n","b87886d5":"submission = pd.DataFrame({'Id': img_ids,'Predicted': pred_labels_expanded })","7dabbee9":"print(submission.head())","dbd35188":"submission.to_csv('submission.csv', index=False)","862fa7cf":"My model summary :\n-> used Resnet152 as it contains large amounts of covnets useful for feature extraction and already proven to be one of the best on imagenet dataset.\n-> Applied various transformations so that model will generalize better on testing set\n-> first run :\nsimply training classifier with 30 epochs with adam optimizer , lr = 0.001\n-> second run:\nunfreezing last two layers 'layer3' and 'layer4' with 50 epochs with same optimizer but with lr = 0.0001 \n->\nthird run :\nunfreezing now two more layers : 'layer1' and 'layer2' with same optimizer and lr as above"}}