{"cell_type":{"89ded1af":"code","5df2c6b7":"code","0c63ab93":"code","e204f56a":"code","0b2cac2c":"code","d1338a3f":"code","dcc94540":"code","b8e4b256":"code","3e679323":"code","22aea758":"code","da42641d":"code","af41bc8b":"markdown","cf242551":"markdown","bbcd9a2b":"markdown","044172c3":"markdown","3eb2664f":"markdown"},"source":{"89ded1af":"import gc\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","5df2c6b7":"%%time\ndf_train = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","0c63ab93":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","e204f56a":"%%time\n\n# compressing the dataframe\ndf_train = reduce_memory_usage(df_train)\ndf_test = reduce_memory_usage(df_test)\n\n# # sampling for prototyping\n# np.random.seed(2003)\n# df_train = df_train.sample(100000)\n# df_test = df_test.sample(100000)","0b2cac2c":"# prepare dataframes for modeling\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()\n\ntest_data = df_test.drop(columns=['id']).copy()\n\ndel df_train\ndel df_test\ngc.collect()","d1338a3f":"# get continous feature columns\ndisc_feat = [col for col in X.columns if X[col].dtype == 'bool']","dcc94540":"# create row based features\ndef get_row_stats(df):\n    df['r_sum'] = df[disc_feat].mean(axis=1)\n    return df\n\nX = get_row_stats(X)\ntest_data = get_row_stats(test_data)","b8e4b256":"from sklearn.metrics import roc_curve, auc\n\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score","3e679323":"lgbm_params = {\n    'objective': 'binary', \n    'device_type': 'gpu', \n    'n_estimators': 20000, \n    'learning_rate':  0.01, \n    'min_child_weight': 256,\n    'min_child_samples': 20, \n    'reg_alpha': 10, \n    'reg_lambda': 0.1, \n    'subsample': 0.6, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.4,\n    'categorical_feature': len(disc_feat)\n#     'num_leaves': 840, \n#     'max_depth': 5,\n#     'min_split_gain': 8.435080902790947, \n}","22aea758":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2003)\n\ntmp_preds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = LGBMClassifier(**lgbm_params)\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='auc',\n        early_stopping_rounds=200,\n        verbose=0\n    )\n    \n    # validation prediction\n    valid_pred = model.predict_proba(X_valid)[:,1]\n    score = get_auc(y_valid, valid_pred)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('::'*20)\n    \n    # test_data prediction\n    y_hat = model.predict_proba(test_data)[:,1]\n    tmp_preds.append(y_hat)\n    \n    gc.collect()\n\nprint(f\"Overall Validation Score: {np.mean(scores)}\")","da42641d":"# # average prediction over all folds\npredictions = np.mean(np.column_stack(tmp_preds),axis=1)\n\n# create submission file\nsample_submission['target'] = predictions\nsample_submission.to_csv('.\/lgbm_baseline.csv', index=False)","af41bc8b":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Import Data<\/div>","cf242551":"**Hi,**<br><br>\njust a quick baseline to get started...\n<br><br>\n**Work in Progress**","bbcd9a2b":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Preprocessing<\/div>","044172c3":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Introduction<\/div>","3eb2664f":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Modeling<\/div>"}}