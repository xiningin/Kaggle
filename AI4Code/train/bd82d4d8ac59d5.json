{"cell_type":{"06239971":"code","02d6c5f3":"code","49030a75":"code","24acf57d":"code","800a5371":"code","52a49465":"code","341ad2d7":"code","b974b858":"code","5d5cd0ea":"code","7c0cde61":"code","521954b3":"code","ba129b3c":"code","264c6fa4":"code","1311bf9d":"code","3c4c5690":"code","9e082f1d":"code","f3c9f9bd":"code","fa3be17e":"code","cd7048d5":"code","ef92d8a6":"code","7ba6baac":"code","8ad5a956":"code","cf61f546":"code","a4c97595":"code","2acb9962":"code","70381251":"code","704d5a38":"code","851db9c6":"code","ef9c4b41":"code","5cda301b":"code","7f648542":"code","f8a34efa":"code","d8516b4d":"code","60c1835d":"markdown","58cc0da1":"markdown","74f199ba":"markdown","52ee6b2b":"markdown","e8beabfa":"markdown","4c63c9e2":"markdown","06cfd160":"markdown","50693c72":"markdown","a7513870":"markdown","6842db94":"markdown","4fb49f64":"markdown","f86f3cde":"markdown","db9f8414":"markdown"},"source":{"06239971":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02d6c5f3":"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","49030a75":"data = pd.read_csv('..\/input\/housedata\/data.csv')\ndata.head()","24acf57d":"data.shape","800a5371":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop(['price'], axis=1), \n                                                    data.price, test_size=0.3, \n                                                    random_state=0)","52a49465":"numerical_x_train = x_train[x_train.select_dtypes([np.number]).columns]","341ad2d7":"from sklearn.feature_selection import VarianceThreshold\nvs_constant = VarianceThreshold(threshold=0)","b974b858":"vs_constant.fit(numerical_x_train)","5d5cd0ea":"len(x_train[x_train.select_dtypes([np.number]).columns].columns[vs_constant.get_support()])","7c0cde61":"constant_columns = [column for column in numerical_x_train.columns\n                    if column not in numerical_x_train.columns[vs_constant.get_support()]]","521954b3":"len(constant_columns)","ba129b3c":"\nconstant_cat_columns = [column for column in x_train.columns \n                        if (x_train[column].dtype == \"O\" and len(x_train[column].unique())  == 1 )]","264c6fa4":"all_constant_columns = constant_cat_columns + constant_columns","1311bf9d":"\nx_train.drop(labels=constant_columns, axis=1, inplace=True)\nx_test.drop(labels=constant_columns, axis=1, inplace=True)","3c4c5690":"threshold = 0.98\n\n# create empty list\nquasi_constant_feature = []\n\n# loop over all the columns\nfor feature in x_train.columns:\n\n    # calculate the ratio.\n    predominant = (x_train[feature].value_counts() \/ np.float(len(x_train))).sort_values(ascending=False).values[0]\n    \n    # append the column name if it is bigger than the threshold\n    if predominant >= threshold:\n        quasi_constant_feature.append(feature)   \n        \nprint(quasi_constant_feature)","9e082f1d":"\n# drop the quasi constant columns\nx_train.drop(labels=quasi_constant_feature, axis=1, inplace=True)\nx_test.drop(labels=quasi_constant_feature, axis=1, inplace=True)","f3c9f9bd":"train_features_T = x_train.T\ntrain_features_T.head()","fa3be17e":"\nprint(train_features_T.duplicated().sum())","cd7048d5":"\nduplicated_columns = train_features_T[train_features_T.duplicated()].index.values","ef92d8a6":"\nx_train.drop(labels=duplicated_columns, axis=1, inplace=True)\nx_test.drop(labels=duplicated_columns, axis=1, inplace=True)","7ba6baac":"correlated_features = set()\ncorrelation_matrix = x_train.corr()","8ad5a956":"plt.figure(figsize=(11,11))\nsns.heatmap(correlation_matrix)","cf61f546":"for i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)","a4c97595":"\ncorrelated_features","2acb9962":"x_train.drop(labels=correlated_features, axis=1, inplace=True)\nx_test.drop(labels=correlated_features, axis=1, inplace=True)","70381251":"from sklearn.feature_selection import mutual_info_classif, chi2\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n","704d5a38":"data.head()","851db9c6":"# select the number of features you want to retain.\nselect_k = 10\n\n# get only the numerical features.\nnumerical_x_train = x_train[x_train.select_dtypes([np.number]).columns]\n\n\n# create the SelectKBest with the mutual info strategy.\nselection = SelectKBest(mutual_info_classif, k=select_k).fit(numerical_x_train, y_train)\n\n# display the retained features.\nfeatures = x_train.columns[selection.get_support()]\nprint(features)","ef9c4b41":"mutual_info(x_train, y_train)","5cda301b":"# change this to how much features you want to keep from the top ones.\nselect_k = 10\n\n# apply the chi2 score on the data and target (target should be binary).  \nselection = SelectKBest(chi2, k=select_k).fit(x_train, y_train)\n\n# display the k selected features.\nfeatures = x_train.columns[selection.get_support()]\nprint(features)","7f648542":"def univariate_rmse():\n    mse_values = []\n    for feature in x_train.columns:\n        clf = DecisionTreeRegressor()\n        clf.fit(x_train[feature].to_frame(), y_train)\n        y_scored = clf.predict(x_test[feature].to_frame())\n        mse_values.append(mean_squared_error(y_test, y_scored))\n    mse_values = pd.Series(mse_values)\n    mse_values.index = x_train.columns\n    print(mse_values.sort_values(ascending=False))\n    print(len(mse_values[mse_values > threshold]),'out of the %s featues are kept'% len(x_train.columns))\n    keep_col = mse_values[mse_values > threshold]\n    return keep_col","f8a34efa":"\nunivariate_rmse()","d8516b4d":"def univariate_roc_auc():\n    roc_values = []\n    for feature in x_train.columns:\n        clf = DecisionTreeClassifier()\n        clf.fit(x_train[feature].to_frame(), y_train)\n        y_scored = clf.predict_proba(x_test[feature].to_frame())\n        roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))\n    roc_values = pd.Series(roc_values)\n    roc_values.index = X_train.columns\n    print(roc_values.sort_values(ascending=False))\n    print(len(roc_values[roc_values > threshold]),'out of the %s featues are kept'% len(X_train.columns))\n    keep_col = roc_values[roc_values > threshold]\n    return keep_col","60c1835d":"# Correlation methods","58cc0da1":"# Univariate ROC-AUC","74f199ba":"# Chi Squared Score","52ee6b2b":"# Filter Methods\n\n+ Filter methods select features from a dataset independently for any machine learning algorithm. These methods rely only on the characteristics of these variables, so features are filtered out of the data before learning begins.\n\n+ Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here\n","e8beabfa":"   # Why do we perform Feature Selection\n   \n   + Machine learning works on a simple rule \u2013 if you put garbage in, you will only get garbage to come out. By garbage here, I mean noise in data.\n\n+ This becomes even more important when the number of features are very large. You need not use every feature at your disposal for creating an algorithm. You can assist your algorithm by feeding in only those features that are really important. I have myself witnessed feature subsets giving better results than complete set of feature for the same algorithm.","4c63c9e2":"# Statistical Measures","06cfd160":"# Quasi Constant features","50693c72":"# Univariate RMSE","a7513870":"# Top reasons to use feature selection are:\n\n+ It enables the machine learning algorithm to train faster.\n\n+ It reduces the complexity of a model and makes it easier to interpret.\n\n+ It improves the accuracy of a model if the right subset is chosen.\n\n+ It reduces overfitting.","6842db94":"# Duplicated Features","4fb49f64":"# Mutual Information","f86f3cde":"# In this notebook\n\n+ Why do we perform Feature Selection\n\n+ Top reasons to use feature selection\n\n+ Filter Methods\n  \n  + Constant Features\n  \n  + Quasi Constant features\n  \n  + Duplicated Features\n  \n  + Correlation methods\n  \n  + Statistical Measures\n  \n  + Mutual Information\n  \n  + Chi Squared Score\n  \n  + Univariate RMSE\n  \n  + Univariate ROC-AUC\n  ","db9f8414":"# Constant Features"}}