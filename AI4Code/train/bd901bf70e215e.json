{"cell_type":{"3f0f7652":"code","68fa6406":"code","41c77f6f":"code","130b4a38":"code","fe2fa1ec":"code","90c36d87":"code","681df9ff":"code","f7c80668":"code","bdaae970":"code","233f5efa":"code","07bac69e":"code","67848657":"code","ed360923":"code","55923f75":"code","7cea1876":"code","b552304d":"code","77c7dfc3":"code","2beb4a28":"code","8a5ce149":"code","b4890097":"code","c4f48af9":"code","afc05cf4":"code","6003230b":"code","1289c888":"code","90960088":"code","ba12973e":"code","15728486":"code","4f6563ac":"code","32168db3":"code","fc6d4324":"code","507ae3c1":"code","ff379315":"code","61e3b9e1":"code","96cdd0d6":"code","8078e5af":"code","c683aed5":"code","cbca3147":"code","29acab58":"code","557cedb2":"code","887ac21c":"code","f3b67b0a":"code","ac6e45bc":"code","3479e72f":"code","6ec1dafd":"code","b8088a6f":"code","83fa10f9":"code","219bb520":"code","b97fe83b":"code","5b2f0cd8":"code","2d1932ea":"code","9ae415cc":"code","adfca906":"code","d73c897a":"code","f109e7c6":"code","c974c6b1":"code","252895b4":"code","53f62596":"code","48268e58":"code","b705f829":"code","70d6c161":"code","9f627a8c":"code","4ba8495d":"code","5775e441":"code","eb1e694c":"code","5aed5153":"code","78b1f632":"code","68a14edc":"code","a86a8162":"code","39b1932e":"code","7e4e5d5e":"code","7506d53b":"code","753a1609":"code","b6aa3567":"code","84b597ee":"code","8c2252e1":"code","d54243b3":"code","e5a0cd25":"code","4ef02671":"code","16978dc8":"code","2b92e637":"code","06965871":"code","ccca936c":"code","a1a4b301":"code","b58ff13b":"code","4c04bd98":"code","fe03c85c":"code","1403125e":"markdown","93ecb3fa":"markdown","e49287fc":"markdown","6beb004a":"markdown","4000894e":"markdown","ecc9cfad":"markdown","5c55046d":"markdown","7ef2761b":"markdown","66d83648":"markdown","623d6abc":"markdown","36d2064d":"markdown","f053c8f8":"markdown","a7617d75":"markdown","e892a854":"markdown","aa747e86":"markdown","dc995ed1":"markdown","d3efcb18":"markdown","01139dc4":"markdown","e5443040":"markdown","1ec0bb48":"markdown","f504d53b":"markdown","ba2685e1":"markdown","533b6edd":"markdown","aa7ca83e":"markdown"},"source":{"3f0f7652":"!pip install seaborn --upgrade\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import cross_val_predict\nimport missingno as msno\nimport seaborn as sns\nplt.style.use('seaborn-pastel')\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_validate\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68fa6406":"#Loading the data\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf.head()","41c77f6f":"#Dividing features from target variable\nX_train = df.drop('Survived', axis=1)\ny_train = df['Survived']\nX_test = df_test.copy()","130b4a38":"X_train.head()","fe2fa1ec":"X_train.info()","90c36d87":"#Check for missing values\nmsno.matrix(X_train, figsize=(15,10), sparkline=False, p=0);\ngray_patch = mpatches.Patch(color='gray', label='Not missing')\nwhite_patch = mpatches.Patch(color='white', label='Missing')\nplt.legend(handles=[gray_patch, white_patch], bbox_to_anchor=(1,0.5));","681df9ff":"#Visualising percent of missing values\nfeature_missing_percent = [X_train[feature].isna().sum()*100\/len(X_train) for feature in ('Age','Cabin','Embarked')]\nplt.figure(figsize=(15,8))\nplt.bar([0,1,2], feature_missing_percent, color=[0.5,0.5,0.5], edgecolor='k', tick_label=['Age','Cabin','Embarked']);\nplt.tick_params(axis='x', labelsize=18)\nplt.ylim([0,100])\nfor x,y in enumerate(feature_missing_percent):\n    plt.text(x-0.05, y+1, str(round(y,2)), va='bottom', weight='heavy')","f7c80668":"num_features = X_train.select_dtypes(exclude='object').columns\ncat_features = X_train.select_dtypes(include='object').columns","bdaae970":"print('Unique value count:', end='\\n'*2)\nfor i in cat_features:\n    print(i,':',X_train[i].nunique())","233f5efa":"#Visualising the variables via scatterplots\nplt.figure(figsize=(20,10))\nfor count, feature in enumerate(num_features):\n    plt.subplot(2,3, count+1)\n    plt.plot(X_train[feature], '.')\n    plt.xlabel(feature, fontsize=16)","07bac69e":"#Visualising the distribution of the two ratio type variables\nfig, axs = plt.subplots(1,2, figsize=(20,5))\naxs[0].hist(X_train['Age'], bins=50, edgecolor='k')\naxs[1].hist(X_train['Fare'], bins=50, edgecolor='k')\naxs[0].set_title('Age', fontsize=18)\naxs[1].set_title('Fare', fontsize=18);","67848657":"y_train.unique()","ed360923":"#Visualising the target variable\nplt.figure(figsize=(10,5))\nplt.bar([0,1], ((y_train==1).sum(),(y_train==0).sum()),color=[0.5,0.5,0.5], edgecolor='k', tick_label=['Survived','Not survived']);\nplt.tick_params(axis='x',labelsize=18)","55923f75":"#Visualising the count of all categorical variables\nplt.figure(figsize=(20,8))\nfor count, feature in enumerate(['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']):\n    count *= 2\n    if count == 6:\n        count = 0\n        plt.subplot2grid(shape=(2,6),loc=(1,count), colspan=3)\n    elif count == 8:\n        count = 3\n        plt.subplot2grid(shape=(2,6),loc=(1,count), colspan=3)\n    else:\n        plt.subplot2grid(shape=(2,6),loc=(0,count),colspan=2)\n    sns.countplot(X_train[feature])\n    plt.ylabel(None)\n    plt.xlabel(feature, fontsize=16)","7cea1876":"#Bi-variate visualistions of categorical variables with target variable\nplt.figure(figsize=(15,25))\nfor count, feature in enumerate(['Pclass','Sex','SibSp','Parch','Embarked']):\n    plt.subplot(5,1,count+1)\n    sns.countplot(X_train[feature], hue=y_train)\n    plt.ylabel(None)\n    plt.xlabel(feature, fontsize=16)\n    plt.legend(['Dead','Survived'],loc='upper right', fontsize='large')","b552304d":"#Distribution and bi-variate plotting of numerical variables against target variable\nfig,[[ax1,ax2],[ax3,ax4]] = plt.subplots(2,2, figsize=(25,15))\n\nsns.histplot(x =X_train['Age'], hue=y_train, ax=ax1, palette='dark')\nsns.histplot(x =X_train['Fare'], hue=y_train, ax=ax2, palette='dark')\nsns.violinplot(y_train, X_train['Age'], ax=ax3)\nsns.violinplot(y_train, X_train['Fare'], ax=ax4);","77c7dfc3":"#Calculations for survival rates\nPclass1 = y_train[(X_train['Pclass'] == 1)].value_counts()\nPclass2 = y_train[(X_train['Pclass'] == 2)].value_counts()\nPclass3 = y_train[(X_train['Pclass'] == 3)].value_counts()\nMales = y_train[(X_train['Sex'] == 'male')].value_counts()\nFemales = y_train[(X_train['Sex'] == 'female')].value_counts()\nSibsp_yes = y_train[(X_train['SibSp'] > 0)].value_counts()\nSibsp_no = y_train[(X_train['SibSp'] == 0)].value_counts()\nParch_yes = y_train[(X_train['Parch'] > 0)].value_counts()\nParch_no = y_train[(X_train['Parch'] == 0)].value_counts()\nEmbarked_C = y_train[(X_train['Embarked'] == 'C')].value_counts()\nEmbarked_Q = y_train[(X_train['Embarked'] == 'Q')].value_counts()\nEmbarked_S = y_train[(X_train['Embarked'] == 'S')].value_counts()\n\nprint('SURVIVAL RATES:','\\n'*2)\nprint('1st Class: ',(Pclass1[1]\/Pclass1.sum()*100).round(2),\"%\")\nprint('2nd Class: ',(Pclass2[1]\/Pclass2.sum()*100).round(2),\"%\")\nprint('3rd Class: ',(Pclass3[1]\/Pclass3.sum()*100).round(2),\"%\",end = '\\n'*2)\nprint('Males: ', (Males[1]\/Males.sum()*100).round(2),\"%\")\nprint('Females: ', (Females[1]\/Females.sum()*100).round(2),\"%\",end = '\\n'*2)\nprint('Passengers with siblings or spouses on-board: ', (Sibsp_yes[1]\/Sibsp_yes.sum()*100).round(2),\"%\")\nprint('Passengers with no siblings or spouses on-board: ', (Sibsp_no[1]\/Sibsp_no.sum()*100).round(2),\"%\",end = '\\n'*2)\nprint('Passengers with parents or children on-board: ', (Parch_yes[1]\/Parch_yes.sum()*100).round(2),\"%\")\nprint('Passengers with no parents or children on-board: ', (Parch_no[1]\/Parch_no.sum()*100).round(2),\"%\",end = '\\n'*2)\nprint('Embarked from Cherbourg: ',(Embarked_C[1]\/Embarked_C.sum()*100).round(2),\"%\")\nprint('Embarked from Queenstown: ',(Embarked_Q[1]\/Embarked_Q.sum()*100).round(2),\"%\")\nprint('Embarked from Southampton: ',(Embarked_S[1]\/Embarked_S.sum()*100).round(2),\"%\",end = '\\n'*2)\n\nprint('Age groups')\nfor age_group in (10,20,30,40,50,60,70,80):\n    age_group_value_count = y_train[(X_train['Age'] <= age_group) & (X_train['Age'] > age_group-10)].value_counts(sort=True)\n    print(str(age_group-10)+'-'+str(age_group)+': ', (age_group_value_count[1]\/age_group_value_count.sum()*100).round(2),'%')\n\nprint()\nlow_fare = y_train[(X_train['Fare'] <= 30)].value_counts(sort=True)\nhigh_fare = y_train[(X_train['Fare'] > 30)].value_counts(sort=True)\nprint('Low fare (<30): ',(low_fare[1]\/low_fare.sum()*100).round(2),'%')\nprint('High fare (>30): ',(high_fare[1]\/high_fare.sum()*100).round(2),'%')","2beb4a28":"#Dropping non-useful features\ndef drop_columns_from_dataset(dataset):\n    dataset.drop(['PassengerId','Ticket','Cabin', 'Name'], axis=1, inplace=True)\n    \n\ndrop_columns_from_dataset(X_train)\nX_train.head()","8a5ce149":"#Creating pipelines for feature transformation which can later be used on test data\nX_train_num = X_train.select_dtypes(exclude='object')\nX_train_cat = X_train.select_dtypes(include='object')\n\nnum_pipelines = [Pipeline([('num_imputer',SimpleImputer(strategy=strat)), ('scaler', StandardScaler())]) for strat in ('median','mean')]\ncat_pipelines = [Pipeline([('cat_imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(sparse=False, drop=drop))]) \n                 for drop in (None,'first','if_binary')]\n\nfull_pipelines = []\nnum_attribs = list(X_train_num)\ncat_attribs = list(X_train_cat)\n\nfor num_pipeline in num_pipelines:\n    for cat_pipeline in cat_pipelines:\n        full_pipeline = ColumnTransformer([('num', num_pipeline, num_attribs),('cat', cat_pipeline, cat_attribs)])\n        full_pipelines.append(full_pipeline)\n        \nX_train_datasets = [pipeline.fit_transform(X_train) for pipeline in full_pipelines]","b4890097":"#Getting all feature names in a list\ncat_attribs = full_pipelines[0].transformers_[1][1]['onehot'].get_feature_names(['Sex','Embarked'])\nall_attribs = num_attribs\nall_attribs.extend(cat_attribs)\nall_attribs","c4f48af9":"score_list = []\n\nlog_reg = LogisticRegression()\ndt_clf = DecisionTreeClassifier()\nsgd_clf = SGDClassifier()\nknn_clf = KNeighborsClassifier()\nsvc = SVC()\nrnd_clf = RandomForestClassifier()\next_clf = ExtraTreesClassifier()\n\nmodel_list = [log_reg, dt_clf, sgd_clf, knn_clf, svc, rnd_clf, ext_clf]\n\nfor model in model_list:\n    print()\n    print(model.__class__.__name__)\n    for X_train in X_train_datasets:\n        scores = cross_val_score(model, X_train, y_train, cv=5)\n        score_list.append(scores.mean())\n        print(scores.mean())","afc05cf4":"np.array(score_list).reshape(7,6).mean(axis=0)","6003230b":"X_train = X_train_datasets[0].copy()","1289c888":"#Appending two new models to the list\nlin_svc = LinearSVC()\nnb_clf = GaussianNB()\n\nmodel_list.append(lin_svc)\nmodel_list.append(nb_clf)\n\nmodel_list","90960088":"#Calculating cross validated mean and standard deviation for each model\nfor model in model_list:\n    scores = cross_val_score(model, X_train, y_train, cv=10)\n    print(model.__class__.__name__,':', scores.mean(),',', scores.std())","ba12973e":"#Splitting train set into small_train and validation set\nsplit = StratifiedShuffleSplit(test_size=0.2)\n\nfor train_index, val_index in split.split(X_train, y_train):\n    X_train_small, X_val = X_train[train_index], X_train[val_index]\n    y_train_small, y_val = y_train[train_index], y_train[val_index]","15728486":"#Lists created for plotting results later on\nfeature_importances = []\nconfusion_matrices = []\ny_preds = []","4f6563ac":"log_reg = LogisticRegression(random_state=42)\n\nlog_reg_cv = cross_validate(log_reg, X_train, y_train, cv=10, return_estimator=True)\n","32168db3":"a = []\n\nfor i in log_reg_cv['estimator']:\n    a.append(i.coef_.ravel())","fc6d4324":"np.array(a).mean(axis=0)","507ae3c1":"a","ff379315":"np.array(a)","61e3b9e1":"np.array(a).mean(axis=1)","96cdd0d6":"log_reg = LogisticRegression(random_state=42)\n\nlog_reg.fit(X_train_small, y_train_small)\ny_pred = log_reg.predict(X_val)\nprint('Score on training set:',log_reg.score(X_train_small,y_train_small))\nprint('Score on validation set:',log_reg.score(X_val, y_val))\n\nfeature_importances.append(log_reg.coef_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","8078e5af":"dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n\ndt_clf.fit(X_train_small, y_train_small)\ny_pred = dt_clf.predict(X_val)\nprint('Score on training set:',dt_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',dt_clf.score(X_val, y_val))\n\nfeature_importances.append(dt_clf.feature_importances_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","c683aed5":"sgd_clf = SGDClassifier(random_state=42)\n\nsgd_clf.fit(X_train_small, y_train_small)\ny_pred = sgd_clf.predict(X_val)\nprint('Score on training set:',sgd_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',sgd_clf.score(X_val, y_val))\n\nfeature_importances.append(sgd_clf.coef_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","cbca3147":"knn_clf = KNeighborsClassifier()\n\nknn_clf.fit(X_train_small, y_train_small)\ny_pred = knn_clf.predict(X_val)\nprint('Score on training set:',knn_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',knn_clf.score(X_val, y_val))\n\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","29acab58":"svc = SVC(kernel='linear')\n\nsvc.fit(X_train_small, y_train_small)\ny_pred = svc.predict(X_val)\nprint('Score on training set:',svc.score(X_train_small,y_train_small))\nprint('Score on validation set:',svc.score(X_val, y_val))\n\nfeature_importances.append(svc.coef_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","557cedb2":"rnd_clf = RandomForestClassifier(random_state=42, max_depth=5)\n\nrnd_clf.fit(X_train_small, y_train_small)\ny_pred = rnd_clf.predict(X_val)\nprint('Score on training set:',rnd_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',rnd_clf.score(X_val, y_val))\n\nfeature_importances.append(rnd_clf.feature_importances_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","887ac21c":"ext_clf = ExtraTreesClassifier(max_depth=5, random_state=42)\n\next_clf.fit(X_train_small, y_train_small)\ny_pred = ext_clf.predict(X_val)\nprint('Score on training set:',ext_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',ext_clf.score(X_val, y_val))\n\nfeature_importances.append(ext_clf.feature_importances_)\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","f3b67b0a":"nb_clf = GaussianNB()\n\nnb_clf.fit(X_train_small, y_train_small)\ny_pred = nb_clf.predict(X_val)\nprint('Score on training set:',nb_clf.score(X_train_small,y_train_small))\nprint('Score on validation set:',nb_clf.score(X_val, y_val))\n\nconfusion_matrices.append(confusion_matrix(y_val, y_pred))\ny_preds.append(y_pred)","ac6e45bc":"#Helper lists for plotting\nfeature_importances_plots = ['Logistic Regression','Decision Tree Classifier','SGD Classifier','SVC','Random Forest','Extra Trees']\nconfusion_matrix_plots = ['Logistic Regression','Decision Tree Classifier','SGD Classifier','KNN Classifier','SVC','Random Forest','Extra Trees', 'Gaussian Naive Bayes']","3479e72f":"#Helper function to even out difference of formats between .coef_ and .feature_importances_\ndef flatten(arrays):\n    flattened_array = []\n    for array in arrays:\n        if len(array) == 1:\n            array = array.flatten()\n            flattened_array.extend(array)\n        else:\n            flattened_array.extend(array)\n    return flattened_array","6ec1dafd":"#Converting array to desired plottable shape\nfeature_importances = flatten(feature_importances)\nfeature_importances = np.array(feature_importances).reshape(6,10)","b8088a6f":"#Plotting model feature importances\nplt.figure(figsize=(25,8))\nfor count,feature_importance in enumerate(feature_importances):\n    plt.subplot(2,3,count+1)\n    plt.title(feature_importances_plots[count]+' Feature Importances', size=18)\n    pd.Series(feature_importance, index=all_attribs).plot(kind='barh')","83fa10f9":"#Plotting model coefficient matrices\nlabel_list = []\nfor matrix in confusion_matrices:\n    values = [str(value) for value in list(matrix.flatten())]\n    names = ['True Negative','False Positive','False Negative','True Positive']\n    percentages = [str(percent) for percent in list((matrix.flatten()\/matrix.sum()*100).round(decimals=2))]\n    label_list.append([value+'\\n'+name+'\\n('+percent+'%)' for value,name,percent in zip(values, names,percentages)])\n\nlabel_list = [np.array(label).reshape(2,2) for label in label_list]\n\nplt.figure(figsize=(25,8))\nfor count, confusion_matrix in enumerate(confusion_matrices):\n    plt.subplot(2,4,count+1)\n    plt.title(confusion_matrix_plots[count]+' confusion matrix', size=14)\n    sns.heatmap(confusion_matrix, annot=label_list[count],fmt='', cbar=False)","219bb520":"del model_list[-2] #Deleting LinearSVC from list of models\n\n#Plotting ROC curves\nplt.figure(figsize=(25,8))\nplt.gca().set_prop_cycle('color',['#ff5733','#2ecc71','#f1c40f','#9b59b6','#34495e','#2471a3','#dc7633','#76d7c4'])\n\nfor count,y_pred in enumerate(y_preds):\n    fpr,tpr,_ = roc_curve(y_val, y_pred)\n    plt.plot(fpr,tpr, lw=2, label=(model_list[count].__class__.__name__ +' (area ' +str(roc_auc_score(y_val,y_pred).round(2))+')'))\n\nplt.plot([0,1],[0,1], '--')\nplt.axis([0,1,0,1])\nplt.legend()\nplt.title('ROC Curves', size=20);","b97fe83b":"cross_val_score(rnd_clf, X_train_small, y_train_small, cv=10)","5b2f0cd8":"y_pred = cross_val_predict(rnd_clf, X_train_small, y_train_small, cv=10)\n\nroc_auc_score(y_train_small,y_pred)","2d1932ea":"rnd_clf = RandomForestClassifier()\nsvc = SVC()\nknn = KNeighborsClassifier()","9ae415cc":"rnd_param = {'n_estimators':[10,50,100,200], 'max_depth':[5,10,20],\n             'min_samples_split':[2,3,4,5],'min_samples_leaf':[1,2,3,4,5]}\n\nrnd_grid = GridSearchCV(rnd_clf, rnd_param, cv=8)\nrnd_grid.fit(X_train,y_train)","adfca906":"rnd_grid.best_score_","d73c897a":"rnd_grid.best_params_","f109e7c6":"svc_param = {'kernel':['linear','poly','rbf'], 'C':[0.1,1,10,100],\n            'degree':[2,3,4,5]}\n\nsvc_grid = GridSearchCV(svc, svc_param, cv=8)\nsvc_grid.fit(X_train,y_train)","c974c6b1":"svc_grid.best_score_","252895b4":"svc_grid.best_estimator_","53f62596":"svc_grid.best_estimator_.probability = True","48268e58":"knn_param = {'n_neighbors':[3,4,5,6,7,8]}\n\nknn_grid = GridSearchCV(knn, knn_param, cv=8)\nknn_grid.fit(X_train, y_train)","b705f829":"knn_grid.best_score_","70d6c161":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(estimators=[('rf', rnd_grid.best_estimator_), ('knn', knn_grid.best_estimator_),\n                                         ('svc', svc_grid.best_estimator_)], voting='soft')\n\nvoting_clf.fit(X_train,y_train)","9f627a8c":"voting_clf.score(X_train, y_train)","4ba8495d":"X_test.head()","5775e441":"drop_columns_from_dataset(X_test)","eb1e694c":"full_pipelines[0].transform(X_test)","5aed5153":"#NOW\n\ndrop_columns_from_dataset(X_test)\nX_test = full_pipelines[0].transform(X_test)","78b1f632":"full_pipelines[0]","68a14edc":"full_pipelines[0].transform(df)","a86a8162":"X_test","39b1932e":"submit1 = test_passenger_id\nsubmit1['Survived'] = dt_test_pred\nsubmit1","7e4e5d5e":"submit1.to_csv('submission1.csv', index=False)","7506d53b":"params = [{'max_depth':[2,3,4], 'max_features':[None, 'auto']}]\n\ndt3 = DecisionTreeClassifier(random_state=42)\ngrid = GridSearchCV(dt3, params, cv=5, scoring='accuracy')\ngrid.fit(df, df_labels)","753a1609":"grid.best_params_","b6aa3567":"grid.best_estimator_","84b597ee":"accuracy_score(df_labels, grid.best_estimator_.predict(df))","8c2252e1":"submit5_pred = grid.best_estimator_.predict(df_test)\nsubmit5 = test_passenger_id\nsubmit5['Survived'] = submit5_pred\nsubmit5","d54243b3":"submit5.to_csv('submission5.csv', index=False)","e5a0cd25":"params = {'max_depth':[2,3,4], 'n_estimators':[2,4,6,8,10], 'max_features':[2,3,4,5,6,7,8]}\nrf2 = RandomForestClassifier(random_state=42)\n\ngrid2 = GridSearchCV(rf2, params, cv=6, scoring='accuracy')\ngrid2.fit(df, df_labels)","4ef02671":"grid2.best_estimator_","16978dc8":"submit6_pred = grid2.best_estimator_.predict(df_test)\nsubmit6 = test_passenger_id\nsubmit6['Survived'] = submit6_pred\nsubmit6","2b92e637":"submit6.to_csv('submission6.csv', index=False)","06965871":"params = {'max_depth':[2,4,8], 'n_estimators':[10,50,100], 'max_features':[2,3,4,5,6,7,8]}\nrf3 = RandomForestClassifier(random_state=42)\n\ngrid3 = GridSearchCV(rf3, params, cv=8, scoring='accuracy')\ngrid3.fit(df, df_labels)","ccca936c":"grid3.best_estimator_","a1a4b301":"submit7_pred = grid3.best_estimator_.predict(df_test)\nsubmit7 = test_passenger_id\nsubmit7['Survived'] = submit7_pred\nsubmit7.to_csv('submission7.csv', index=False)","b58ff13b":"df","4c04bd98":"df['Survived'].unique()","fe03c85c":"df.head()","1403125e":"### Gaussian Naive Bayes Classifier","93ecb3fa":"Now, we will train various models with standard parameters on the dataset and compare their performances. We will then pick the top models amongst them for further hyperparameter and other fine tuning.","e49287fc":"Submission 7","6beb004a":"### **New learnings:**\n\n1. The passengers of 1st and 2nd class have a better surviving ratio than those in the 3rd class.\n2. The ratio of survival of females is a lot more than that of men's.\n3. Those with a sibling or a spouse on the ship had better surviving odds.\n4. Those who had parents or children on the ship survived more than those who didn't.\n5. Those embarked from Cherbourg had a better surviving ratio than those embarked from other ports.\n6. The children survived more than the adults.\n7. Those who paid a very high fare have a noticeably better surviving ratio.\n\nLet's find out how much exactly the survival rates differed.","4000894e":"Let's first find out which of the 6 transformed datasets should we take further. For that, we will quickly calculate the cross validation score of all datasets on several models and find which one gives the best average performance.","ecc9cfad":"### Stochastic Gradient Descent Classifier","5c55046d":"## Modeling","7ef2761b":"The feature `Cabin` is majorly filled with missing values. It won't prove to be useful and will be dropped. Others can be imputed.\n\nLet's explore the categorical and numerical variables separately:","66d83648":"The scores are pretty close. That means model is not overfitting the training set.\n\n### Decision Tree Classifier","623d6abc":"## Data Preparation\n(Cleaning and transformation)","36d2064d":"Submission 6","f053c8f8":"The columns `Age`, `Cabin` and `Embarked` have missing values. Let's see how many:","a7617d75":"### Extra Trees Classifier","e892a854":"The features in the dataset are all either integers or floats or object types.\n\nLet's look if they have any missing values amongst them:","aa747e86":"The difference in the scores is very minimal. We will pick the one which gives the highest score i.e the first dataset. That is-\n\n`X_train_datasets[0]`","dc995ed1":"### Support Vector Machine Classifier","d3efcb18":"The feature `PassengerId` is not going to be useful for our prediction. Hence, it can be dropped. There seem to be no alarming outliers in any of the features that need be removed.\n\nNow, on to viewing the distribution of `Age` and `Fare`, the only two ratio type numerical data types.","01139dc4":"### Model building","e5443040":"The categorical variables `Name` and `Ticket` could be used to find out the passenger class (names with titles, anomalistic ticket numbers) but these are already explained by `PClass` and `Fare`. So, they can safely be dropped.","1ec0bb48":"We have created 6 variations of the same dataset to find out which one of those will give the best predictions.","f504d53b":"Now, on to fitting each model individually for greater precision and to compare their feature importances and error types-\n\n### Logistic Regression","ba2685e1":"### Random Forest Classifier","533b6edd":"### K Neighbors Classifier","aa7ca83e":"# Exploratory Data Analysis"}}