{"cell_type":{"ba8510f8":"code","1856918d":"code","01881089":"code","c7473410":"code","00554bb6":"code","2927e1a9":"code","71e8a6fa":"code","f7c94390":"code","35178de2":"code","ccb5fed8":"code","1539099d":"code","4b3ec9a1":"code","a257d5bc":"code","61824ede":"code","2b23cf2c":"code","4bf08b19":"code","4c30c501":"markdown","348319b0":"markdown","dcb6f365":"markdown","7dcd8299":"markdown","e78fdc60":"markdown","92d2789d":"markdown","d84dba0a":"markdown","90d9753f":"markdown","a97e5904":"markdown","1779be93":"markdown","2c41a77a":"markdown","69ab06ef":"markdown","ecf3263a":"markdown","dd589224":"markdown","a3b4d043":"markdown","e93fd54c":"markdown","e84a95c3":"markdown","37bdf683":"markdown","f09165ff":"markdown","1f0d7fbb":"markdown"},"source":{"ba8510f8":"import os\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import models, layers, regularizers, metrics, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16\nfrom scipy import stats\nimport shutil\nimport random","1856918d":"def dataset_basic_info(generator, name):\n        print('The ' + name + ' data set includes ' + str(generator.samples) + ' samples.')\n        print('The ' + name + ' image shapes is ' + str(generator.image_shape))\n        keys = [el for el in generator.class_indices.keys()]\n        print('The ' + name + ' data set includes the following labels: ')\n        print(keys)\n        labels     = generator.labels\n        cat_labels = []\n        for i in range(len(labels)):\n            for j in range(len(keys)):\n                if (labels[i] == j):\n                    cat_labels.append(keys[j])\n                    break\n        occurrences = []\n        for key in keys:\n            counter = 0\n            for i in range(len(cat_labels)):\n                if cat_labels[i] == key:\n                    counter += 1\n            occurrences.append(counter)\n        print(name + ' data set labels frequencies:')\n        weights = {}\n        for i in range(len(keys)):\n            print(keys[i] + ': ' + str(occurrences[i]) + ' (absolute), ' + str(round(occurrences[i]\/float(generator.samples), 3)) + ' (relative).' )\n            weights[i] = generator.samples\/np.array(occurrences[i])*(1.0\/float(len(keys)))\n        \n        return weights\n\n\ndef build_model_cnn(regression_problem, conv_filters, conv_filter_shape, conv_activation_function, conv_padding, conv_pooling_type, conv_pooling_shape, hidden_layers_neurons, hidden_activation_function, L1_coeffs, L2_coeffs, hidden_layers_dropout, final_layer_neurons, final_activation_function, shape, model_optimizer, loss_function, metrics):\n    \n    model = models.Sequential()\n    \n    for i in range(len(conv_activation_function)):\n        \n        if (i == 0):\n            model.add(layers.Conv2D(conv_filters[i],\n                                    (conv_filter_shape[i][0],conv_filter_shape[i][1]), \n                                    activation = conv_activation_function[i], \n                                    padding    = conv_padding[i],\n                                    input_shape = (shape[0],shape[1],shape[2])))             \n        else:\n            model.add(layers.Conv2D(conv_filters[i],\n                                    (conv_filter_shape[i][0],conv_filter_shape[i][1]), \n                                    activation = conv_activation_function[i],\n                                    padding    = conv_padding[i]))\n        \n        if (conv_pooling_type[i] == 'max'):\n            model.add(layers.MaxPooling2D((conv_pooling_shape[i][0],conv_pooling_shape[i][1])))\n        elif (conv_pooling_type[i] == 'avg'):\n            model.add(layers.AveragePooling2D((conv_pooling_shape[i][0],conv_pooling_shape[i][1])))\n        else:\n            'no pooling'\n            \n    model.add(layers.Flatten())\n    \n    for i in range(len(hidden_activation_function)):\n\n        model.add(layers.Dense(hidden_layers_neurons[i], \n                               kernel_regularizer = regularizers.l1_l2(l1 = L1_coeffs[i], l2 =  L2_coeffs[i]),  \n                               activation=hidden_activation_function[i]))\n        if (hidden_layers_dropout[i] > 0.0):\n            model.add(layers.Dropout(hidden_layers_dropout[i]))\n    if regression_problem:\n            model.add(layers.Dense(final_layer_neurons))\n    else:\n            model.add(layers.Dense(final_layer_neurons,activation = final_activation_function))\n            \n    model.compile(optimizer = model_optimizer, loss = loss_function, metrics = metrics)\n    \n    model.summary()\n    \n    return model\n\ndef build_model_pretrained_cnn(pre_trained_model, regression_problem, hidden_layers_neurons, hidden_activation_function, L1_coeffs, L2_coeffs, hidden_layers_dropout, final_layer_neurons, final_activation_function, model_optimizer, loss_function, metrics):\n    \n    model = models.Sequential()\n    model.add(pre_trained_model)\n    model.add(layers.Flatten())\n    \n    for i in range(len(hidden_activation_function)):\n\n        model.add(layers.Dense(hidden_layers_neurons[i], \n                               kernel_regularizer = regularizers.l1_l2(l1 = L1_coeffs[i], l2 =  L2_coeffs[i]),  \n                               activation=hidden_activation_function[i]))\n        if (hidden_layers_dropout[i] > 0.0):\n            model.add(layers.Dropout(hidden_layers_dropout[i]))\n    if regression_problem:\n            model.add(layers.Dense(final_layer_neurons))\n    else:\n            model.add(layers.Dense(final_layer_neurons,activation = final_activation_function))\n            \n    model.compile(optimizer = model_optimizer, loss = loss_function, metrics = metrics)\n    \n    model.summary()\n    \n    return model\n\ndef display_input_images(generator, max_n_figures, batch_size, grid_size, fig_size):\n    \n    fig_counter = 0\n    for image_batch, label_batch in generator: \n        plt.figure(figsize=(fig_size[0],fig_size[1]))\n        for j in range(batch_size):\n            ax   = plt.subplot(grid_size[0], grid_size[1], j + 1)\n            plt.imshow(image_batch[j])\n            if (label_batch[j] == 1):\n                    plt.title(\"Dog\")\n            else:\n                    plt.title(\"Cat\")\n            plt.axis(\"off\")\n        plt.show()\n        fig_counter += 1\n        if (fig_counter == max_n_figures): break\n\ndef analyze_performances(hst, epochs):\n    history_dict             = hst.history\n    loss_values              = history_dict['loss']\n    validation_loss_values   = history_dict['val_loss']\n    acc_values               = history_dict['accuracy']\n    validation_acc_values    = history_dict['val_accuracy']\n    prec_values              = history_dict['precision']\n    validation_prec_values   = history_dict['val_precision']\n    recall_values            = history_dict['recall']\n    validation_recall_values = history_dict['val_recall']\n    epochs                   = range(1,len(loss_values) + 1)\n    fig, axes                = plt.subplots(1,4,figsize = (40,10))\n    training_ts              = [loss_values, acc_values, prec_values, recall_values]\n    validation_ts            = [validation_loss_values, validation_acc_values, validation_prec_values, validation_recall_values]\n    metric_names             = ['loss', 'accuracy','precision','recall']\n    for i in range(len(axes)):\n        axes[i].plot(epochs,training_ts[i],color = 'r',label = 'training')\n        axes[i].plot(epochs,validation_ts[i],color = 'b',label = 'validation')\n        axes[i].set_xlabel('epoch')\n        axes[i].set_ylabel(metric_names[i])\n        axes[i].set_title(metric_names[i] + ' analysis')\n        axes[i].set_xticks(np.arange(0,epochs[-1] + 1,5))\n        axes[i].set_yticks(np.arange(0,1.1,0.1))\n        axes[i].set_xlim([1,epochs[-1]])\n        #axes[i].set_ylim([np.min([np.min(training_ts[i]),np.min(validation_ts[i])]),np.max([np.max(training_ts[i]),np.max(validation_ts[i])])])\n        axes[i].set_ylim([0,1])\n        axes[i].legend()\n    plt.show()\n        ","01881089":"training_path                  =  \"\/kaggle\/input\/dogs-cats-images\/dataset\/training_set\/\"\ntest_path                      =  \"\/kaggle\/input\/dogs-cats-images\/dataset\/test_set\/\"\nvalidation_split               = 0.2\nregression_problem             = False\ntarget_img_shape_1             = 180\ntarget_img_shape_2             = 180\ntarget_img_channels            = 3\nconv_filters                   = [32,64,128,128]      \nconv_filter_shape              = [[3,3]]*4\nconv_activation_function       = ['relu']*4\nconv_padding                   = ['valid']*4\nconv_pooling_type              = ['max']*4\nconv_pooling_shape             = [[2,2]]*4\naugment_data                   = True\nrotation_range                 = 0.3\nwidth_shift_range              = 0.2\nheight_shift_range             = 0.2\nshear_range                    = 0.2\nbrightness_range               = [0.95,1.05]\nzoom_range                     = 0.2\nhorizontal_flip                = True\nfill_mode                      = 'nearest'\nprint_sample_input             = True\nhidden_activation_function     = ['relu']\nhidden_layers_neurons          = [256]\nhidden_layers_L1_coeffs        = [0.00]\nhidden_layers_L2_coeffs        = [0.00]\nhidden_layers_dropout          = [0.00]\nfinal_activation_function      = 'sigmoid'\nfinal_layer_neurons            = 1\nmodel_optimizer                = 'Adam'\nloss_function                  = 'binary_crossentropy'\nmetrics                        = ['accuracy',metrics.Precision(name='precision'),metrics.Recall(name='recall')]\nn_epochs                       = 80\nbatch_size                     = 20\nsteps_per_epoch                = 100\nvalidation_steps               = 50\nvgg_hidden_activation_function = ['relu']\nvgg_hidden_layers_neurons      = [256]\nvgg_hidden_layers_L1_coeffs    = [0.00]\nvgg_hidden_layers_L2_coeffs    = [0.00]\nvgg_hidden_layers_dropout      = [0.00]\nvgg_final_activation_function  = 'sigmoid'\nvgg_final_layer_neurons        = 1\nvgg_model_optimizer            = optimizers.RMSprop(lr=2e-5)\nvgg_n_epochs                   = 35\nvgg_steps_per_epoch            = 100\nvgg_validation_steps           = 50","c7473410":"labels = ['cats','dogs']\nnew_training_path   = \"..\/files\/dogs-cats-images\/dataset\/training_set\/\"\nnew_validation_path = \"..\/files\/dogs-cats-images\/dataset\/validation_set\/\"\nnew_test_path       = \"..\/files\/dogs-cats-images\/dataset\/test_set\/\"\nshutil.rmtree(new_training_path, ignore_errors=True)\nshutil.rmtree(new_validation_path, ignore_errors=True) \nshutil.rmtree(new_test_path, ignore_errors=True)\n[os.makedirs(new_training_path + label,exist_ok=True) for label in labels]\n[os.makedirs(new_validation_path + label,exist_ok=True) for label in labels]\n[os.makedirs(new_test_path + label,exist_ok=True) for label in labels]\nfor label in labels:\n        training_filenames   = os.listdir(training_path + label + \"\/\") \n        validation_filenames = random.sample(training_filenames, int(len(training_filenames)*validation_split))\n        training_filenames   = [file for file in training_filenames if file not in validation_filenames]\n        test_filenames       = os.listdir(test_path + label + \"\/\") \n        for file in training_filenames:\n            shutil.copy(training_path + label + \"\/\" + file, new_training_path + label + \"\/\")\n        print('Training images transfer complete for label: ' + label + '. # transferred images: ' + str(len(training_filenames)))\n        for file in validation_filenames:\n            shutil.copy(training_path + label + \"\/\" + file, new_validation_path + label + \"\/\")\n        print('Validation images transfer complete for label: ' + label + '. # transferred images: '  + str(len(validation_filenames)))\n        for file in test_filenames:\n            shutil.copy(test_path + label + \"\/\" + file, new_test_path + label + \"\/\")\n        print('Test images transfer complete for label: ' + label + '. # transferred images: '  + str(len(test_filenames)))\n","00554bb6":"if augment_data:\n    train_datagen   = ImageDataGenerator(rescale            = 1.\/255,\n                                         rotation_range     = rotation_range,\n                                         width_shift_range  = width_shift_range,\n                                         height_shift_range = height_shift_range,\n                                         shear_range        = shear_range,\n                                         brightness_range   = brightness_range,\n                                         zoom_range         = zoom_range,\n                                         horizontal_flip    = horizontal_flip,\n                                         fill_mode          = fill_mode)\nelse:\n    train_datagen   = ImageDataGenerator(rescale = 1.\/255)\n\nvalidation_datagen   = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen         = ImageDataGenerator(rescale = 1.\/255)\ntrain_generator      = train_datagen.flow_from_directory(new_training_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"binary\")  \nvalidation_generator = validation_datagen.flow_from_directory(new_validation_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"binary\") \ntest_generator       = test_datagen.flow_from_directory(new_test_path,target_size = (target_img_shape_1, target_img_shape_2), batch_size = batch_size, class_mode = \"binary\") ","2927e1a9":"if print_sample_input:\n    display_input_images(train_generator, 5, batch_size, [4,5], [20,20])","71e8a6fa":"train_labels_weights_dict      = dataset_basic_info(train_generator, 'training')\nvalidation_labels_weights_dict = dataset_basic_info(validation_generator, 'validation')\ntest_labels_weights_dict       = dataset_basic_info(test_generator, 'test')","f7c94390":"model = build_model_cnn(regression_problem, \n                        conv_filters, \n                        conv_filter_shape, \n                        conv_activation_function, \n                        conv_padding, \n                        conv_pooling_type, \n                        conv_pooling_shape, \n                        hidden_layers_neurons, \n                        hidden_activation_function, \n                        hidden_layers_L1_coeffs, \n                        hidden_layers_L2_coeffs, \n                        hidden_layers_dropout, \n                        final_layer_neurons, \n                        final_activation_function, \n                        [target_img_shape_1, target_img_shape_2, target_img_channels], \n                        model_optimizer, \n                        loss_function, \n                        metrics)","35178de2":"early_exit      = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\nbest_checkpoint = ModelCheckpoint('.best_fit.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n\nhst = model.fit(train_generator, \n                steps_per_epoch = \n                steps_per_epoch, \n                epochs = n_epochs, \n                batch_size = batch_size,\n                validation_data = validation_generator, \n                validation_steps = validation_steps, \n                verbose = 0, \n                callbacks =[early_exit, best_checkpoint])\n        \nmodel.load_weights(filepath = '.best_fit.hdf5')","ccb5fed8":"analyze_performances(hst, n_epochs)","1539099d":"test_loss_1, test_acc_1, test_prec_1, test_rec_1 = model.evaluate(test_generator)\nprint('The loss of the predictions on the test data set is: ' + str(round(test_loss_1,4)))\nprint('The accuracy of the predictions on the test data set is: ' + str(round(test_acc_1,4)))","4b3ec9a1":"pre_trained_conv = VGG16(weights = 'imagenet', include_top = False, input_shape = (target_img_shape_1, target_img_shape_2, target_img_channels))","a257d5bc":"model_pt = build_model_pretrained_cnn(pre_trained_conv, regression_problem, vgg_hidden_layers_neurons, vgg_hidden_activation_function, vgg_hidden_layers_L1_coeffs, vgg_hidden_layers_L2_coeffs, vgg_hidden_layers_dropout, vgg_final_layer_neurons, vgg_final_activation_function, vgg_model_optimizer, loss_function, metrics)","61824ede":"early_exit      = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\nbest_checkpoint = ModelCheckpoint('.best_fit_pre_trained.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n\nvgg_hst = model_pt.fit(train_generator, \n                       steps_per_epoch = vgg_steps_per_epoch, \n                       epochs = vgg_n_epochs, \n                       validation_data = validation_generator, \n                       validation_steps = vgg_validation_steps, \n                       verbose = 0, \n                       callbacks =[early_exit, best_checkpoint])\n        \nmodel_pt.load_weights(filepath = '.best_fit_pre_trained.hdf5')","2b23cf2c":"analyze_performances(vgg_hst, vgg_n_epochs)","4bf08b19":"test_loss_2, test_acc_2, test_prec_2, test_rec_2 = model_pt.evaluate(test_generator)\nprint('The loss of the predictions on the test data set is: ' + str(round(test_loss_2,4)))\nprint('The accuracy of the predictions on the test data set is: ' + str(round(test_acc_2,4)))","4c30c501":"## BUILD A MODEL BY STACKING THE PRE-TRAINED CNN WITH AN USER DEFINED NEURAL NETWORK CLASSIFIER","348319b0":"## BUILD A CONVOLUTIONAL NEURAL NETWORK MODEL\n\nThe input parameters set by the user serve as arguments of a function which returns a Keras model object of a convolutional neural network.\n\n![1_vkQ0hXDaQv57sALXAJquxA.jpg](attachment:1_vkQ0hXDaQv57sALXAJquxA.jpg)\n\nSchematics of the typical structure of a neural network model for image recognition.\n\nImage source: https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","dcb6f365":"## SUMMARY\n\n**Context**\n\nThis dataset contains images of cats and dogs. The objective is to predict the label of these images.\n\n**Overview of the notebook**\n\nThis notebook implements a convolutional neural network model to predict the whether an image displays a cat or a dog.\n\nThe input dataset is available at https:\/\/www.kaggle.com\/chetankv\/dogs-cats-images.\n\nThe code allows the user to buy a customized, i.e., number of nodes, layers, activation functions etc., convolutional neural network. \n\nIn this experiment, our model achieves very satisfactory accuracy levels, i.e., RMSE < 0.04, in predicting the labels of previously unseen samples.\n\n![_111699029_gettyimages-1168451046.jpg](attachment:_111699029_gettyimages-1168451046.jpg)\n\n\nImage source: https:\/\/www.bbc.co.uk\/news\/explainers-52214609","7dcd8299":"## USE A PRE-TRAINED MODEL TO FURTHER IMPROVE PERFORMANCES\n\nFinally, we use pre trained models and compare how they perform with respect to our trained model.\n\nSee Chapter 5 of \"Deep Learning with Python\", Francois Chollet, Manning, for details on pre-trained convolutional neural networks.\n\n![image_0-8fa3b810.png](attachment:image_0-8fa3b810.png)\n\nSchematics of the architecture of the pre-trained VGG16 model.\n\nImage source: https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl","e78fdc60":"## ANALYZE THE PERFORMANCE OF THE MODEL\n\nWe display four different metrics to assess the performances of the model\n\n1. loss -> the value of the loss function in each epoch.\n2. accuracy  -> the count of true positives and true negatives over the number of samples in the test data set.\n3. precision -> the count of true positives over the number of predicted positives (e.g., true positives + false positives).\n4. recall    -> the count of true positives over the number of samples in the positive group (e.g., true positives + false negatives).\n\nThese metrics are recorded at the end of each training epoch and plotted in epoch vs. metric diagrams. Red lines represent the pattern of these metrics during training while blue lines represent the pattern of these metrics during the validation phase.","92d2789d":"## REFERENCES\n\n1. This script is strongly inspired by the contents of Chapter 5 of \"Deep Learning with Python\", Francois Chollet, Manning.\n2. I also found the notebook \"TensorFlow Pneumonia Classification on X-rays\" by Amy Jang very useful. See https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance","d84dba0a":"## DEFINE AUXILIARY FUNCTIONS\n\nThe following cells supplies auxiliary functions that can be used to achieve the objectives of the present script\n\n1. dataset_basic_info         -> display basic information on the data sets.\n2. build_model_ann_cnn        -> return a Keras model object of a convolutional neural network built according to input specs provided by the user.\n3. build_model_pretrained_cnn -> return a Keras model object of a convolutional neural network made of a pretrained convolutional base (specified by the user) and a dense classifier (standard neural network) built according to input specs provided by the user.\n4. display_input_images       -> display batches of images from the train or test data set, credits to Amy Jang, see https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance\n5. analyze_performances       -> plot the trajectories of four different performance metrics (i.e., loss, accuracy, precision and recall) recorded at the end of each epoch for both the testing and the validation phases.","90d9753f":"## FIT THE MODEL","a97e5904":"## ANALYZE THE PERFORMANCE OF THE MODEL","1779be93":"## CLASSIFY THE TEST IMAGES\n\nWe use our trained convolutional neural network to predict the labels of previously unseen images contained in the test folder.","2c41a77a":"## INPUT DASHBOARD\n\nWe let the user set several variables that affect the estimation of the model:\n\n* training_path -> specify the path of the training images. NOTE: the next level should include k folder, where k is the number of different possible labels (e.g. k = 2, dogs and cats).\n* test_path -> specify the path of the test images. NOTE: the next level should include k folder, where k is the number of different possible labels (e.g. k = 2, dogs and cats).\n* validation_split    -> fraction of the images in the test data folder that should be used as validation subset during the training of the model.\n* regression_problem  -> indicates whether we are facing a regression problem. If = True, the final layer of the densely connected neural network won't have any specified activation function.   \n* target_img_shape_1  -> the first desired dimension for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator.\n* target_img_shape_2  -> the second desired dimension for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator.\n* target_img_channels -> the number of desired channels for the images; an input for the Keras function flow_from_directory that is applied to an ImageDataGenerator. E.g. 3 channels for RGB images.\n* conv_filters        -> list including the number of different filters that are used in each convolution.\n* conv_filter_shape   -> list of k lists, where k is the number of desired convolutions. Each of these k lists includes two integers representing the dimensions (in number of cells) of each filter that slides across the picture.\n* conv_activation_function -> list including the names of the activation function that should be used in each convolution. Note: we have a product between a patch of the image and the filter. Then, the entries of the resulting matrix are summed up. This sum serves as the argument of the activation function g().\n* conv_padding -> list including the type of padding that should be used in each convolution. The string 'valid' means that no padding should be applied on the borders of the image. 'same' means that the image is padded such that the output of the convolution retains the same shape.\n* conv_pooling_type   -> list including the type of pooling that must be applied after each convolution. The string 'max' implements max pooling, the string 'avg' implements average pooling, any other string means no pooling after the current convolution.\n* conv_pooling_coeffs -> list of k lists, where k is the number of desired convolutions. Each of these k lists includes two integers representing the dimensions (in number of cells) of the pooling operation region that is applied across the convolved image.\n* augment_data -> boolean indicating whether the images in the training data set should be augmented by applying a broad range of actions, such as rotations, shearing, zooming, flipping etc.\n* rotation_range -> specifies the range (in degrees) of random rotations of the input images.\n* width_shift_range -> specifies the range (in fractions of the image width) of random horizontal shifts of the input images.\n* height_shift_range -> specifies the range (in fractions of the image height) of random vertical shifts of the input images.\n* shear_range -> specifies the symmetric range -x, x of random shears of the input images.\n* brightness_range -> list including the lower and upper extremes of the range of image brightness values that are randomly applied to the input images. \n* zoom_range -> specifies the symmetric range 1 - x, 1 + x of random zooms of the input images.\n* horizontal_flip -> boolean indicating whether the columns of the training images should be randomly flipped.\n* fill_mode -> name of the method that should be used to fill the new empty pixels emerged during data augmentation procedures.\n* print_sample_input -> boolean indicating whether a sample of the input images shall be printed.\n* hidden_activation_function -> list containing the name of the activation functions (available in Keras) used in the hidden layers of the densely connected neural network that follows the sequence of convolutions.\n* hidden_layers_neurons -> list containing the number of neurons forming each hidden layer of the densely connected neural network that follows the sequence of convolutions.\n* hidden_layers_L1_coeffs -> list containing scalars multiplying the L1-penalty terms for each hidden layer weights of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid L1-regularization.\n* hidden_layers_L2_coeffs -> list containing scalars multiplying the L2-penalty terms for each hidden layer weights of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid L2-regularization.\n* hidden_layers_dropouts ->  list containing the fractions of weights that are randomly set to zero in each hidden layer of the densely connected neural network that follows the sequence of convolutions. Set it to 0 to avoid dropout regularization.\n* final_activation_function -> name of the activation function (available in Keras) used in the terminal layer of the densely connected neural network that follows the sequence of convolutions.\n* final_layer_neurons -> number of neurons forming the terminal layer of the densely connected neural network that follows the sequence of convolutions.\n* model_optimizer -> name of the method (available in Keras) to iteratively update the search of the set of parameters that minimize the loss function.\n* loss_function -> name the loss function (available in Keras) that we seek to minimize.\n* metrics -> list containing the name of the metrics (available in Keras) that we use to assess the performances of the model.\n* n_epochs -> the times the optimization algorithm goes through the entire training data set.\n* batch_size -> the number of samples included in a single batch.\n* steps_per_epoch -> the number of batches that form an epoch during the training phase. \n* validation_steps -> the number of sample batches that form an epoch during the validation phase.\n* vgg_hidden_activation_function -> list containing the name of the activation functions (available in Keras) used in the hidden layers of the densely connected neural network classifier that follows the pretrained VGG16 CNN.\n* vgg_hidden_layers_neurons -> list containing the number of neurons forming each hidden layer of the densely connected neural network classifier that follows the pretrained VGG16 CNN.\n* vgg_hidden_layers_L1_coeffs -> list containing scalars multiplying the L1-penalty terms for each hidden layer weights of the densely connected neural network classifier that follows the pretrained VGG16 CNN. Set it to 0 to avoid L1-regularization.\n* vgg_hidden_layers_L2_coeffs -> list containing scalars multiplying the L2-penalty terms for each hidden layer weights of the densely connected neural network classifier that follows the pretrained VGG16 CNN. Set it to 0 to avoid L2-regularization.\n* vgg_hidden_layers_dropouts ->  list containing the fractions of weights that are randomly set to zero in each hidden layer of the densely connected neural network classifier that follows the pretrained VGG16 CNN. Set it to 0 to avoid dropout regularization.\n* vgg_final_activation_function -> name of the activation function (available in Keras) used in the terminal layer of the densely connected neural network classifier that follows the pretrained VGG16 CNN.\n* vgg_final_layer_neurons -> number of neurons forming the terminal layer of the densely connected neural network classifier that follows the pretrained VGG16 CNN.\n* vgg_n_epochs -> the times the optimization algorithm goes through the entire training data set when the VGG16 based CNN is trained.\n* vgg_steps_per_epoch -> the number of batches that form an epoch during the training phase when the VGG16 based CNN is trained. \n* vgg_validation_steps -> the number of sample batches that form an epoch during the validation phase when the VGG16 based CNN is trained.","69ab06ef":"## DISPLAY BASIC INFORMATION ON TRAIN AND TEST IMAGES\n\nWe use the function \"dataset_basic_info()\" to display some basic information, e.g., # samples, image shape, labels frequencies, for each dataset.","ecf3263a":"## CLASSIFY THE TEST IMAGES\n\nWe use our trained convolutional neural network (including the pre-trained convolutional component) to predict the labels of previously unseen images contained in the test folder.","dd589224":"## NOTES\n\nAuthor: Alberto Ciacci\n\nE-mail: alberto.ciacci16@imperial.ac.uk\n\nLanguage: python\n\nComments, corrections and suggestions are very much welcome.","a3b4d043":"## DISPLAY SOME SAMPLE IMAGES\n\nCredits to Amy Jang, see https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays\/notebook#5.-Correct-for-data-imbalance","e93fd54c":"## IMPORT LIBRARIES\n\nWe begin by importing the libraries needed to perform the incoming tasks.","e84a95c3":"## FIT THE MODEL","37bdf683":"## LOAD THE VGG16 PRE-TRAINED CNN","f09165ff":"## REORGANIZE IMAGES \n\nWe transfer the available images in three folders (training, validation, testing). Our goal is to obtain:\n\n    1. an augmentable training set.\n    2. a non-augmentable validation set which is drawn from training images.\n    3. an independent, unseen, non-augmentable test set.\n","1f0d7fbb":"## GENERATE TRAINING AND TEST IMAGES\n\nWe create three distinct generators (training, validation, test) that we will use later to train and assess the model."}}