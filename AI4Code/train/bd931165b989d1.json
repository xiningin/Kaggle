{"cell_type":{"b9a14b09":"code","76526177":"code","4d52d182":"code","ba56710b":"code","c33590bf":"code","1d80d332":"code","15538f1d":"code","7803ac35":"code","0f126bc1":"code","686c17fc":"code","0914a8c8":"code","f9f5a72d":"code","d32c58e2":"code","057357fa":"code","379f1ee8":"code","4d7249be":"markdown","e4937475":"markdown"},"source":{"b9a14b09":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.models import Model,Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Activation,Dropout,BatchNormalization,Input,Flatten\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical","76526177":"from keras.datasets import fashion_mnist","4d52d182":"(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()","ba56710b":"## We are rescaling the pixel values between 0 and 1. Pixel values range between 0 and 255. So we divide it by 255\nx_train=x_train\/255\nx_test=x_test\/255","c33590bf":"labels = {0:\"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", \n             4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\",\n             9: \"Ankle boot\"}","1d80d332":"fig,axs=plt.subplots(1,5,figsize=(20,20))\nfor ax,i in zip(axs,range(6)):\n    ax.imshow(x_train[i])\n    ax.set_title(labels[y_train[i]])\nplt.show()","15538f1d":"##Reshaping (60000,28,28) to (60000,28,28,1)\n\nx_train=x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)","7803ac35":"##Converts the labels to one-hot vector form: 9 -> array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n\ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)","0f126bc1":"model=Sequential()\n\nmodel.add(Conv2D(filters=32,input_shape=(28,28,1),kernel_size=3,padding='valid',activation='relu'))\nmodel.add(Conv2D(filters=32,kernel_size=3,padding='valid',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3,3)))\n\nmodel.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\nmodel.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3,3)))\n\nmodel.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\nmodel.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(10,activation='softmax'))","686c17fc":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","0914a8c8":"model.summary()","f9f5a72d":"callbacks=[keras.callbacks.ReduceLROnPlateau(mode='max',monitor='accuracy',patience=3,factor=0.005),\n          keras.callbacks.ModelCheckpoint(filepath='model.h5',save_best_only=True)]","d32c58e2":"model.fit(x_train,y_train,shuffle=True,batch_size=100,epochs=10,validation_split=0.3,callbacks=callbacks)","057357fa":"loss,accuracy=model.evaluate(x_test,y_test)","379f1ee8":"print('Loss: ',loss,'\\nAccuracy: ',accuracy)","4d7249be":"Output shape for a convolutional layer = ( ((n+2p-f)\/s +1) , ((n+2p-f)\/s +1) , No of filters)\n\nFor example:\n\nInput image shape=(28,28,1) -> n=28\n\nConv2D_Layer1 Features -> No of filters=32, strides(s)=1, padding(p)=0, Kernel size(f)=3\n\n(n+2p-f)\/s +1) = 26\n\nSo the output of first Conv2D layer is (26,26,32)\n\nFor MaxPooling also this formula remains the same. In place of kernel size use pool size\n\n( In the subsequent layers I have used padding='same'. So this ensures that the output shape of the layer is same as input shape. In this case since input is (28,28), output will also be (28,28) )","e4937475":"Fashion MNIST is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. "}}