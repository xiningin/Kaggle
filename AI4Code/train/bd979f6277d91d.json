{"cell_type":{"caa5ffd6":"code","cb9181d8":"code","1f930b42":"code","18141947":"code","e7cb08e8":"code","b4773e34":"code","78545328":"code","7ad6b828":"code","83d95b43":"code","5e667682":"code","26f216a0":"code","3e3de5d8":"code","96e43605":"code","07e620db":"code","02a4d0d1":"code","aa8a40ba":"code","0649afbe":"markdown","148740ec":"markdown","9ceb92c1":"markdown","9659da36":"markdown","62ad6dc0":"markdown","da7d8b9b":"markdown","28b7e42c":"markdown","ed3d9f65":"markdown","abcb0ac9":"markdown"},"source":{"caa5ffd6":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom numpy.random import seed\nseed(42)\n\nimport random\nimport os\n\nrandom.seed(42)\nos.environ['PYTHONHASHSEED'] = str(42)\n\n#from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler","cb9181d8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom keras.models import Model\n\nfrom tensorflow.random import set_seed\nset_seed(42)","1f930b42":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col = 'id')\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col = 'id')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\n\nY_train = df_train['target'].copy()","18141947":"#oof = prediction on train set using 10-fold CV\n#sub = prediction on test set\n\noof1 = pd.read_csv('..\/input\/30days-xgboost\/train_oof.csv')\nsub1 = pd.read_csv('..\/input\/30days-xgboost\/submission.csv')\n\noof2 = pd.read_csv('..\/input\/30days-xgboost-model\/train_oof.csv')\nsub2 = pd.read_csv('..\/input\/30days-xgboost-model\/submission.csv')\n\noof3 = pd.read_csv('..\/input\/30days-xgboost-3\/train_oof.csv')\nsub3 = pd.read_csv('..\/input\/30days-xgboost-3\/submission.csv')\n\noof4 = pd.read_csv('..\/input\/30days-xgboost-4\/train_oof.csv')\nsub4 = pd.read_csv('..\/input\/30days-xgboost-4\/submission.csv')\n\noof5 = pd.read_csv('..\/input\/30days-xgboost-5\/train_oof.csv')\nsub5 = pd.read_csv('..\/input\/30days-xgboost-5\/submission.csv')\n\noof6 = pd.read_csv('..\/input\/30days-lightgbm-blend-with-xgboost\/train_oof.csv')\nsub6 = pd.read_csv('..\/input\/30days-lightgbm-blend-with-xgboost\/submission_lgbm.csv')\n\noof7 = pd.read_csv('..\/input\/30days-random-forest-on-gpu-rapids\/train_oof.csv')\nsub7 = pd.read_csv('..\/input\/30days-random-forest-on-gpu-rapids\/submission.csv')\n\noof8 = pd.read_csv('..\/input\/30days-knn-on-gpu-rapids\/train_oof.csv')\nsub8 = pd.read_csv('..\/input\/30days-knn-on-gpu-rapids\/submission.csv')\n\n#oof9 = pd.read_csv('..\/input\/30days-knn-on-gpu-rapids\/train_oof.csv')\n#sub9 = pd.read_csv('..\/input\/30days-knn-on-gpu-rapids\/submission.csv')","e7cb08e8":"oof_train = pd.DataFrame([x for x in oof1['target']], columns = ['pred1'])\noof_train['pred2'] = [x for x in oof2['target']]\noof_train['pred3'] = [x for x in oof3['target']]\noof_train['pred4'] = [x for x in oof4['target']]\noof_train['pred5'] = [x for x in oof5['target']]\noof_train['pred6'] = [x for x in oof6['target']]\noof_train['pred7'] = [x for x in oof7['target']]\noof_train['pred8'] = [x for x in oof8['target']]\n#oof_train['pred9'] = [x for x in oof9['target']]\n\noof_train","b4773e34":"sub_test = pd.DataFrame([x for x in sub1['target']], columns = ['pred1'])\nsub_test['pred2'] = [x for x in sub2['target']]\nsub_test['pred3'] = [x for x in sub3['target']]\nsub_test['pred4'] = [x for x in sub4['target']]\nsub_test['pred5'] = [x for x in sub5['target']]\nsub_test['pred6'] = [x for x in sub6['target']]\nsub_test['pred7'] = [x for x in sub7['target']]\nsub_test['pred8'] = [x for x in sub8['target']]\n#sub_test['pred9'] = [x for x in sub9['target']]\n\nsub_test","78545328":"#Defining Callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    min_delta=0.0000001,\n    restore_best_weights=True,\n)\n\nplateau = callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor = 0.5,                                     \n    patience = 3,                                   \n    min_delt = 0.0000001,                                \n    cooldown = 0,                               \n    verbose = 1\n) ","7ad6b828":"#Model\n\ndef new_model():\n\n    inputs = layers.Input(shape = (8,))\n    \n    x = layers.BatchNormalization()(inputs) \n    \n    x = layers.Dense(units = 32, activation = 'relu', kernel_initializer = 'he_normal')(x)\n    x = layers.BatchNormalization()(x) \n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.Dense(units = 16, activation = 'relu', kernel_initializer = 'he_normal')(x)\n    x = layers.BatchNormalization()(x) \n    x = layers.Dropout(0.2)(x)\n    \n    #Final Layer (Output)\n    output = layers.Dense(1, activation = 'linear')(x)\n    \n    model = keras.Model(inputs=[inputs], outputs=output)\n    \n    return model","83d95b43":"def prediction (X_train, Y_train, X_test):\n    \n    keras.backend.clear_session()\n\n    kfold = KFold(n_splits = 10, shuffle=True, random_state = 42)\n\n    y_pred = np.zeros((len(X_test),1))\n    train_oof = np.zeros((len(X_train),1))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model = new_model()\n        model.compile(loss='mse', optimizer = keras.optimizers.Adam(learning_rate=0.02),\n                      metrics=[keras.metrics.RootMeanSquaredError()])\n        \n        model.fit(xtrain, ytrain,\n        batch_size = 256, epochs = 100,\n        validation_data=(xval, yval),\n        callbacks=[early_stopping, plateau]);\n\n        #create predictions\n        y_pred += model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n\n        # calculate and append rmsle\n        rmse = mean_squared_error(yval,val_pred, squared=False)\n        print('RMSE : {}'.format(rmse))\n  \n    return y_pred, train_oof","5e667682":"pred, train_oof = prediction (oof_train, Y_train, sub_test)","26f216a0":"print(\"OOF - RMSE: {0:0.6f}\".format(mean_squared_error(Y_train,train_oof, squared = False)))","3e3de5d8":"train_oof = pd.DataFrame(train_oof, columns = ['target'])\ntrain_oof.to_csv('train_oof_nn.csv', index=False)\n\ntrain_oof","96e43605":"#Submission File 1\nsubmission['target'] = pred\nsubmission.to_csv(\"submission_nn.csv\", index=False)\nsubmission","07e620db":"#Importing files from the xgboost meta-learner\noof_xgb = pd.read_csv('..\/input\/30days-stacking-w-xgboost\/train_oof.csv')\nsub_xgb = pd.read_csv('..\/input\/30days-stacking-w-xgboost\/submission.csv')","02a4d0d1":"#Checking OOF score\noof_blend = (0.70*train_oof + 0.30*oof_xgb) \nprint('OOF score: {}'.format(mean_squared_error(Y_train, oof_blend['target'], squared = False)))","aa8a40ba":"#Blending\nsub_blend = 0.70*submission + 0.30*sub_xgb\nsub_blend['id'] = submission['id']\n\n#Submission File 2\nsub_blend.to_csv(\"submission_blend.csv\", index=False)\nsub_blend","0649afbe":"## Making Predictions","148740ec":"## Defining the Neural Network","9ceb92c1":"## <center>If you find this notebook useful, support with an upvote!<center>","9659da36":"## <center>If you find this notebook useful, support with an upvote!<center>","62ad6dc0":"<br>    \n    \n![30days](https:\/\/i.imgur.com\/cNwIrEe.png)\n <center>Illustration of the Final Approach<center>\n<br>","da7d8b9b":"## Importing Packages and Datasets","28b7e42c":"## Blending with XGBoost Meta-Model","ed3d9f65":"# <center>30 Days of Machine Learning<center>\n## <center>Final Stacking<center>\n---\nThis is my final notebook on the 30 days of ML competition. The final approach consists of a stacking approach followed by a final blending. First, 9 different models were trained on the original train set and used to make predictions on the test set and on the train set itself through a 10-fold Cross Validation. These are the 9 models:\n1. XGBoost #1\n2. XGBoost #2\n3. XGBoost #3 (Similar parameters to [Kosta R's notebook](https:\/\/www.kaggle.com\/boneacrabonjac\/30-days-ml-xgbr-and-auto-eda))\n4. XGBoost #4 (Similar parameters to [Steven Ferrer's notebook](https:\/\/www.kaggle.com\/stevenrferrer\/30-days-of-ml-optimized-xgboost-5folds))\n5. XGBoost #5 (Similar parameters to [Abhishek Thakur's notebook](https:\/\/www.kaggle.com\/abhishek\/competition-part-4-optimized-xgboost))\n6. LightGBM\n7. Random Forest on GPU (RAPIDS\/cuML)\n8. K-Nearest Neighbors on GPU (RAPIDS\/cuML)\n9. Ridge Regression\n    \nAfter that, two meta-models were trained using only the predictions of (some of) those models as train and test sets. The Neural Network meta-model, presented in this notebook, was trained using the predictions of all models, except the Ridge Regression. The XGBoost meta-model, added in the last section of the notebook, was trained using the predictions of 4 XGBoost base models (2 to 5) + Random Forest and Ridge Regression. At the end, the results from both meta-models were used for a final blending.\n\nTwo submissions can be selected for the Private Leaderboard. The Submission #1 is the output from the Neural Network meta-model. The Submission #2 is the blended submission. The following image illustrates my strategy in this competition.\n    \n","abcb0ac9":"---\n    \nSuggestion of EDA notebook in this dataset:\n* [30 Days of ML - EDA](https:\/\/www.kaggle.com\/dwin183287\/30-days-of-ml-eda) by [Sharlto Cope](https:\/\/www.kaggle.com\/dwin183287)\n    \nMy other notebooks in this competition:\n* [30 Days of Machine Learning: Starter - Auto EDA + Base XGBoost (GPU)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-starter-auto-eda-base-xgboost-gpu)\n* [30 Days of Machine Learning: LightGBM Model (+ Blend with XGBoost)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-lightgbm-blend-with-xgboost)\n* [30 Days of Machine Learning: Random Forest on GPU (RAPIDS\/cuML)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-random-forest-on-gpu-rapids) "}}