{"cell_type":{"2fcee786":"code","941d0507":"code","7feb169d":"code","2b3f0f65":"code","930c72b4":"code","6dc75530":"code","7ff8572f":"code","9085569d":"code","468ccde5":"code","10960df9":"code","62685919":"code","baa807fe":"code","54647083":"code","775b0fad":"code","bf509e24":"code","3f267e2f":"code","afe404b4":"code","19657133":"code","3f9099a6":"code","4ec62d6a":"code","95db3bc2":"code","bc44f7cb":"code","c8be8e2a":"code","a5888c36":"code","cd442bca":"code","7effce08":"code","5950a085":"code","0101757f":"code","b62e9233":"code","65fe08fd":"code","2d190b81":"code","440a1bab":"code","be25cba1":"code","e0968e26":"code","ec2072f8":"code","35b0ddd5":"code","54b1454f":"code","9c0f5a3f":"code","93954cd9":"code","fd08e507":"code","d075a0dc":"code","b2ad83d1":"code","bac2db72":"code","6b6ebe3a":"code","978dd170":"code","977d55e5":"code","64ec0399":"code","0031fe93":"code","c34222db":"code","d7a24ba5":"code","2788de77":"code","2b1c8cb5":"code","a2deca27":"code","7e911771":"code","83224d18":"code","f81810fb":"code","5ce0d6f2":"code","a4aca481":"code","8dcaaae1":"code","53af0397":"code","d3a50adc":"code","22cc1166":"code","5ec10f50":"code","037db033":"code","d49a8d01":"code","91bc548b":"code","82d47ff7":"code","65d49309":"code","b635d0f5":"code","fb0887ee":"code","670ba1f0":"code","c9eafe9e":"code","363753d1":"code","d0b8bfc1":"code","c7cf7df4":"code","1ef125f3":"code","bc2b7375":"code","ab17c0d4":"code","2f553cf9":"code","b190d654":"code","4d1826e6":"code","92fb06f6":"code","81e95c29":"code","9345277d":"code","8f3de228":"code","af5dcb68":"code","d313a615":"code","14dc96e9":"markdown","91da692a":"markdown","fa0cdae1":"markdown","6ae7976d":"markdown","cf3788a4":"markdown","8014a4af":"markdown","eff80e70":"markdown","ae4f5718":"markdown","4ad1f580":"markdown","2307ce31":"markdown","e00c6acd":"markdown","7f918b45":"markdown","4e91e336":"markdown","9d46e365":"markdown","473bce7d":"markdown","4139e3eb":"markdown","d8252fea":"markdown","63e7625d":"markdown","366a2856":"markdown","3b174743":"markdown","a02841a1":"markdown","e00473a1":"markdown","9bdf4f2d":"markdown","c9c31c28":"markdown","dca49183":"markdown","ce9c2d03":"markdown","7b735106":"markdown","a4892a31":"markdown","b7bcf43c":"markdown","a7dbe1b0":"markdown","7341cddd":"markdown","a05993d6":"markdown","80eeec5d":"markdown","a81a2fb9":"markdown","201984fb":"markdown","67e162d8":"markdown","3917de4f":"markdown","8afa6ecb":"markdown","198b7803":"markdown"},"source":{"2fcee786":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","941d0507":"import numpy as np\nimport pandas as pd\nimport random\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score as r2\nfrom sklearn.model_selection import KFold, GridSearchCV\n\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom datetime import datetime\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7feb169d":"import warnings\nwarnings.filterwarnings('ignore')","2b3f0f65":"matplotlib.rcParams.update({'font.size': 14})","930c72b4":"def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):\n    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))\n    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))\n    \n    plt.figure(figsize=(18,10))\n    \n    plt.subplot(121)\n    sns.scatterplot(x=train_pred_values, y=train_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Train sample prediction')\n    \n    plt.subplot(122)\n    sns.scatterplot(x=test_pred_values, y=test_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Test sample prediction')\n\n    plt.show()","6dc75530":"TRAIN_DATASET_PATH = '..\/input\/real-estate-price-prediction-moscow\/train.csv'\nTEST_DATASET_PATH = '..\/input\/real-estate-price-prediction-moscow\/test.csv'","7ff8572f":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntrain_df.tail()","9085569d":"train_df.dtypes","468ccde5":"test_df = pd.read_csv(TEST_DATASET_PATH)\ntest_df.tail()","10960df9":"print('Lines in train dataset:', train_df.shape[0])\nprint('Lines in test dataset:', test_df.shape[0])","62685919":"train_df.shape[1] - 1 == test_df.shape[1]","baa807fe":"train_df.dtypes","54647083":"train_df['Id'] = train_df['Id'].astype(str)\ntrain_df['DistrictId'] = train_df['DistrictId'].astype(str)","775b0fad":"plt.figure(figsize = (16, 8))\n\ntrain_df['Price'].hist(bins=30)\nplt.ylabel('Count')\nplt.xlabel('Price')\n\nplt.title('Target distribution')\nplt.show()","bf509e24":"train_df.describe()","3f267e2f":"train_df.select_dtypes(include='object').columns.tolist()","afe404b4":"train_df['DistrictId'].value_counts()","19657133":"train_df['Ecology_2'].value_counts()","3f9099a6":"train_df['Ecology_3'].value_counts()","4ec62d6a":"train_df['Shops_2'].value_counts()","95db3bc2":"train_df['Rooms'].value_counts()","bc44f7cb":"train_df['Rooms_outlier'] = 0\ntrain_df.loc[(train_df['Rooms'] == 0) | (train_df['Rooms'] >= 6), 'Rooms_outlier'] = 1\ntrain_df.head()","c8be8e2a":"rooms_median = train_df['Rooms'].median()\ntrain_df.loc[train_df['Rooms'] == 0, 'Rooms'] = 1\ntrain_df.loc[train_df['Rooms'] >= 6, 'Rooms'] = rooms_median","a5888c36":"train_df['Rooms'].value_counts()","cd442bca":"train_df['KitchenSquare'].value_counts()","7effce08":"train_df['KitchenSquare'].quantile(.95), train_df['KitchenSquare'].quantile(.05)","5950a085":"kitchen_square_median = train_df['KitchenSquare'].median()\ncondition = (train_df['KitchenSquare'].isna()) \\\n             | (train_df['KitchenSquare'] > train_df['KitchenSquare'].quantile(.95))\n        \ntrain_df.loc[condition, 'KitchenSquare'] = kitchen_square_median\n\ntrain_df.loc[train_df['KitchenSquare'] < 5, 'KitchenSquare'] = 5","0101757f":"train_df['KitchenSquare'].value_counts()","b62e9233":"train_df['HouseFloor'].sort_values().unique()","65fe08fd":"train_df['Floor'].sort_values().unique()","2d190b81":"(train_df['Floor'] > train_df['HouseFloor']).sum()","440a1bab":"train_df['HouseFloor_outlier'] = 0\ntrain_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\ntrain_df.loc[train_df['Floor'] > train_df['HouseFloor'], 'HouseFloor_outlier'] = 1","be25cba1":"train_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor'] = train_df['HouseFloor'].median()","e0968e26":"floor_outliers = train_df.loc[train_df['Floor'] > train_df['HouseFloor']].index\nfloor_outliers","ec2072f8":"# If flat floor is higher than the max floor of house, this is the mistake in Floor or HouseFloor columns\n# We can fix it in different ways, for example, fill Floor HouseFloor value\ntrain_df.loc[floor_outliers, 'Floor'] = train_df.loc[floor_outliers, 'HouseFloor']","35b0ddd5":"(train_df['Floor'] > train_df['HouseFloor']).sum()","54b1454f":"train_df['HouseYear'].sort_values(ascending=False)","9c0f5a3f":"train_df.loc[train_df['HouseYear'] > 2020, 'HouseYear'] = 2020","93954cd9":"train_df.isna().sum()","fd08e507":"train_df[['Square', 'LifeSquare', 'KitchenSquare']].head(10)","d075a0dc":"train_df['LifeSquare_nan'] = train_df['LifeSquare'].isna() * 1\n\ncondition = (train_df['LifeSquare'].isna()) \\\n             & (~train_df['Square'].isna()) \\\n             & (~train_df['KitchenSquare'].isna())\n        \ncondition.sum()","b2ad83d1":"train_df.loc[condition, 'LifeSquare'] = train_df.loc[condition, 'Square'] \\\n                                            - train_df.loc[condition, 'KitchenSquare'] - 3","bac2db72":"condition = (train_df['LifeSquare'].isna()) \\\n             & (~train_df['Square'].isna()) \\\n             & (~train_df['KitchenSquare'].isna())\n        \ncondition.sum()","6b6ebe3a":"train_df['Healthcare_1'].value_counts().sort_index()","978dd170":"# We have only one 0 value in Healthcare_1, so, probably we can fill NaNs with 0 too\ntrain_df['Healthcare_1'].fillna(0, inplace=True)","977d55e5":"class DataPreprocessing:\n    \"\"\"Class for train and test datasets preprocessing\"\"\"\n\n    def __init__(self, kitchen_square_quantile=0.975, kitchen_square_min_threshold=3, balcony_square=3):\n        \"\"\"Preprocessing parameters\"\"\"\n        self.medians = None\n        self.kitchen_square_quantile = kitchen_square_quantile\n        self.kitchen_square_min_threshold = kitchen_square_min_threshold\n        self.balcony_square = balcony_square\n    \n    def fit(self, X):\n        \"\"\"Get statistics\"\"\"\n        self.medians = X.median()\n        self.kitchen_square_quantile = X['KitchenSquare'].quantile(self.kitchen_square_quantile)\n    \n    def transform(self, X):\n        \"\"\"Transform data\"\"\"\n\n        # Rooms\n        X['Rooms_outlier'] = 0\n        X.loc[(X['Rooms'] == 0) | (X['Rooms'] >= 6), 'Rooms_outlier'] = 1\n        \n        X.loc[X['Rooms'] == 0, 'Rooms'] = 1\n        X.loc[X['Rooms'] >= 6, 'Rooms'] = self.medians['Rooms']\n        \n        # KitchenSquare\n        condition = (X['KitchenSquare'].isna()) \\\n                    | (X['KitchenSquare'] > self.kitchen_square_quantile)\n        \n        X.loc[condition, 'KitchenSquare'] = self.medians['KitchenSquare']\n\n        X.loc[X['KitchenSquare'] < self.kitchen_square_min_threshold, 'KitchenSquare'] = self.kitchen_square_min_threshold\n        \n        # HouseFloor, Floor\n        X['HouseFloor_outlier'] = 0\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n        X.loc[X['Floor'] > X['HouseFloor'], 'HouseFloor_outlier'] = 1\n        \n        X.loc[X['HouseFloor'] == 0, 'HouseFloor'] = self.medians['HouseFloor']\n        \n        floor_outliers = X.loc[X['Floor'] > X['HouseFloor']].index\n        X.loc[floor_outliers, 'Floor'] = X.loc[floor_outliers, 'HouseFloor']\n        \n        # HouseYear\n        current_year = datetime.now().year\n        \n        X['HouseYear_outlier'] = 0\n        X.loc[X['HouseYear'] > current_year, 'HouseYear_outlier'] = 1\n        \n        X.loc[X['HouseYear'] > current_year, 'HouseYear'] = current_year\n        \n        # Healthcare_1\n        X['Healthcare_1'].fillna(0, inplace=True)\n        \n        # LifeSquare\n        X['LifeSquare_nan'] = X['LifeSquare'].isna() * 1\n        condition = (X['LifeSquare'].isna()) & \\\n                      (~X['Square'].isna()) & \\\n                      (~X['KitchenSquare'].isna())\n        \n        X.loc[condition, 'LifeSquare'] = X.loc[condition, 'Square'] - X.loc[condition, 'KitchenSquare'] - self.balcony_square\n        \n        \n        X.fillna(self.medians, inplace=True)\n        \n        return X","64ec0399":"binary_to_numbers = {'A': 0, 'B': 1}\n\ntrain_df['Ecology_2'] = train_df['Ecology_2'].replace(binary_to_numbers)\ntrain_df['Ecology_3'] = train_df['Ecology_3'].replace(binary_to_numbers)\ntrain_df['Shops_2'] = train_df['Shops_2'].replace(binary_to_numbers)","0031fe93":"district_size = train_df['DistrictId'].value_counts().reset_index() \\\n                    .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n\ndistrict_size.head()","c34222db":"train_df = train_df.merge(district_size, on='DistrictId', how='left')\ntrain_df.head()","d7a24ba5":"(train_df['DistrictSize'] > 100).value_counts()","2788de77":"train_df['IsDistrictLarge'] = (train_df['DistrictSize'] > 100).astype(int)","2b1c8cb5":"med_price_by_district = train_df.groupby(['DistrictId'], as_index=False).agg({'Price':'median'})\\\n                            .rename(columns={'Price':'MedPriceByDistrict'})\n\nmed_price_by_district.head()","a2deca27":"med_price_by_district.shape","7e911771":"train_df = train_df.merge(med_price_by_district, on=['DistrictId'], how='left')\ntrain_df.head()","83224d18":"def floor_to_cat(X):\n    bins = [X['Floor'].min(), 3, 5, 9, 15, X['Floor'].max()]\n    X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n    \n    X['floor_cat'].fillna(-1, inplace=True)\n    return X\n\ndef year_to_cat(X):\n    bins = [X['HouseYear'].min(), 1941, 1945, 1980, 2000, 2010, X['HouseYear'].max()]\n    X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n    \n    X['year_cat'].fillna(-1, inplace=True)\n    return X","f81810fb":"bins = [train_df['Floor'].min(), 3, 5, 9, 15, train_df['Floor'].max()]\npd.cut(train_df['Floor'], bins=bins, labels=False)","5ce0d6f2":"bins = [train_df['Floor'].min(), 3, 5, 9, 15, train_df['Floor'].max()]\npd.cut(train_df['Floor'], bins=bins)","a4aca481":"train_df = year_to_cat(train_df)\ntrain_df = floor_to_cat(train_df)\ntrain_df.head()","8dcaaae1":"med_price_by_floor_year = train_df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n                                            rename(columns={'Price':'MedPriceByFloorYear'})\nmed_price_by_floor_year.head()","53af0397":"train_df = train_df.merge(med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\ntrain_df.head()","d3a50adc":"class FeatureGenetator():\n    \"\"\"Class for feature generation for train and test datasets\"\"\"\n    \n    def __init__(self, large_district_threshold=100, med_price_by_district_grouping=[]):\n        \"\"\"Feature parameters\"\"\"\n        self.DistrictId_counts = None\n        self.binary_to_numbers = None\n        self.med_price_by_district = None\n        self.med_price_by_floor_year = None\n        self.house_year_max = None\n        self.floor_max = None\n        self.house_year_min = None\n        self.floor_min = None\n        self.district_size = None\n        \n        self.large_district_threshold = large_district_threshold\n        self.med_price_by_district_grouping = med_price_by_district_grouping\n        \n    def fit(self, X, y=None):\n        \"\"\"Get statistics\"\"\"\n        X = X.copy()\n        \n        # Binary features\n        self.binary_to_numbers = {'A': 0, 'B': 1}\n        \n        # DistrictID\n        self.district_size = X['DistrictId'].value_counts().reset_index() \\\n                               .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n        \n        # Target encoding\n        ## District, Rooms\n        df = X.copy()\n        \n        if y is not None:\n            df['Price'] = y.values\n            \n            self.med_price_by_district = df.groupby(self.med_price_by_district_grouping, as_index=False).agg({'Price':'median'}) \\\n                                            .rename(columns={'Price':'MedPriceByDistrict'})\n            \n            self.med_price_by_district_median = self.med_price_by_district['MedPriceByDistrict'].median()\n        \n        ## Median price for square meter in each district\n        if y is not None:\n            self.district_price_per_square = df.groupby(['DistrictId'], as_index=False).agg({'Price':'median'}) \\\n                                            .rename(columns={'Price':'DistrictPrice'})\n            \n            self.district_price_per_square_median = self.district_price_per_square['DistrictPrice'].median()\n        \n        ## floor, year\n        if y is not None:\n            self.floor_max = df['Floor'].max()\n            self.floor_min = df['Floor'].min()\n            self.house_year_max = df['HouseYear'].max()\n            self.house_year_min = df['HouseYear'].min()\n            df['Price'] = y.values\n            df = self.floor_to_cat(df)\n            df = self.year_to_cat(df)\n            self.med_price_by_floor_year = df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n                                            rename(columns={'Price':'MedPriceByFloorYear'})\n            self.med_price_by_floor_year_median = self.med_price_by_floor_year['MedPriceByFloorYear'].median()\n    \n    def transform(self, X):\n        \"\"\"Transform data\"\"\"\n        # Binary features\n        X['Ecology_2'] = X['Ecology_2'].map(self.binary_to_numbers)  # self.binary_to_numbers = {'A': 0, 'B': 1}\n        X['Ecology_3'] = X['Ecology_3'].map(self.binary_to_numbers)\n        X['Shops_2'] = X['Shops_2'].map(self.binary_to_numbers)\n        \n        # DistrictId, IsDistrictLarge\n        X = X.merge(self.district_size, on='DistrictId', how='left')\n        \n        X['new_district'] = 0\n        X.loc[X['DistrictSize'].isna(), 'new_district'] = 1\n        \n        X['DistrictSize'].fillna(5, inplace=True)\n        \n        X['IsDistrictLarge'] = (X['DistrictSize'] > self.large_district_threshold).astype(int)\n        \n        # More categorical features\n        X = self.floor_to_cat(X)\n        X = self.year_to_cat(X)\n        \n        # Target encoding\n        if self.med_price_by_district is not None:\n            X = X.merge(self.med_price_by_district, on=self.med_price_by_district_grouping, how='left')\n            X['MedPriceByDistrict'].fillna(self.med_price_by_district_median, inplace=True)\n        \n        if self.med_price_by_floor_year is not None:\n            X = X.merge(self.med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n            X['MedPriceByFloorYear'].fillna(self.med_price_by_floor_year_median, inplace=True)\n        \n        if self.district_price_per_square is not None:\n            X = X.merge(self.district_price_per_square, on=['DistrictId'], how='left')\n            X['DistrictPrice'].fillna(self.district_price_per_square_median, inplace=True)\n        \n        return X\n    \n    def floor_to_cat(self, X):\n        \"\"\"floor_cat feature\"\"\"\n        bins = [self.floor_min, 3, 5, 9, 15, self.floor_max]\n        X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n\n        X['floor_cat'].fillna(-1, inplace=True)\n        return X\n    \n    def year_to_cat(self, X):\n        \"\"\"year_cat feature\"\"\"\n        bins = [self.house_year_min, 1941, 1945, 1980, 2000, 2010, self.house_year_max]\n        X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n\n        X['year_cat'].fillna(-1, inplace=True)\n        return X","22cc1166":"train_df.columns.tolist()","5ec10f50":"# Exclude Floor and Healthcare_1 columns for better predictions\nfeature_names = ['Rooms', 'Square', 'LifeSquare', 'KitchenSquare', 'HouseFloor', 'HouseYear', # 'Floor',\n                 'Ecology_1', 'Ecology_2', 'Ecology_3', 'Social_1', 'Social_2', 'Social_3', # 'Healthcare_1',\n                 'Helthcare_2', 'Shops_1', 'Shops_2']\n\nnew_feature_names = ['Rooms_outlier', 'HouseFloor_outlier', 'HouseYear_outlier', 'LifeSquare_nan', 'DistrictSize',\n                     'DistrictPrice', 'new_district', 'IsDistrictLarge',  'MedPriceByDistrict', 'MedPriceByFloorYear']\n\ntarget_name = 'Price'","037db033":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)\n\nX = train_df.drop(columns=target_name)\ny = train_df[target_name]","d49a8d01":"test_df","91bc548b":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=21)","82d47ff7":"preprocessor = DataPreprocessing(kitchen_square_quantile=0.95, kitchen_square_min_threshold=3,\n                                 balcony_square=5)\npreprocessor.fit(X_train)\n\nX_train = preprocessor.transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ntest_df = preprocessor.transform(test_df)\n\nX_train.shape, X_valid.shape, test_df.shape","65d49309":"features_gen = FeatureGenetator(large_district_threshold=100, med_price_by_district_grouping=['DistrictId'])\nfeatures_gen.fit(X_train, y_train)\n\nX_train = features_gen.transform(X_train)\nX_valid = features_gen.transform(X_valid)\ntest_df = features_gen.transform(test_df)\n\nX_train.shape, X_valid.shape, test_df.shape","b635d0f5":"X_train = X_train[feature_names + new_feature_names]\nX_valid = X_valid[feature_names + new_feature_names]\ntest_df = test_df[feature_names + new_feature_names]","fb0887ee":"X_train.isna().sum().sum(), X_valid.isna().sum().sum(), test_df.isna().sum().sum()","670ba1f0":"# model = RandomForestRegressor(\n#     random_state=21,\n#     criterion='mse',\n#     max_depth=9,\n#     max_features=9,\n#     n_estimators=200,\n# )\n# model.fit(X_train, y_train)","c9eafe9e":"# parameters = [{\n#     'n_estimators': [100, 200, 400],\n#     'max_features': np.arange(5, 10),\n#     'max_depth': np.arange(4, 10),\n# }]\n\n# model = GridSearchCV(\n#     estimator=RandomForestRegressor(random_state=21, criterion='mse'),\n#     param_grid=parameters,\n#     scoring='r2',\n#     cv=3,\n# )\n# model.fit(X_train, y_train)\n\n# model.best_params_","363753d1":"model = CatBoostRegressor(\n    learning_rate=0.03,\n    depth=9,\n    l2_leaf_reg=2,\n    iterations=1000,\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    random_seed=21,\n    silent=True,\n)\nmodel.fit(X_train, y_train)","d0b8bfc1":"# model = CatBoostRegressor(\n#     loss_function='RMSE',\n#     eval_metric='RMSE',\n# )\n\n# grid = {\n#     'learning_rate': [0.03, 0.1],\n#     'depth': [4, 7, 10],\n#     'n_estimators': [200, 600, 1000],\n#     'l2_leaf_reg': [1, 5, 9]\n# }\n\n# grid_search_result = model.grid_search(\n#     grid,\n#     X=X_train,\n#     y=y_train,\n#     plot=True,\n# )","c7cf7df4":"# grid_search_result['params']","1ef125f3":"# model = XGBRegressor(n_estimators=400, max_depth=7, random_state=21)\n# model.fit(X_train, y_train)","bc2b7375":"# model = LGBMRegressor(\n#     n_estimators=400,\n#     random_state=21,\n#     max_depth=15\n# )\n# model.fit(X_train, y_train)","ab17c0d4":"y_train_preds = model.predict(X_train)\ny_test_preds = model.predict(X_valid)\n\nevaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)","2f553cf9":"cv_score = cross_val_score(model, X_train, y_train, scoring='r2', cv=KFold(n_splits=3, shuffle=True, random_state=21))\ncv_score","b190d654":"cv_score.mean()","4d1826e6":"feature_importances = pd.DataFrame(zip(X_train.columns, model.feature_importances_), \n                                   columns=['feature_name', 'importance'])\n\nfeature_importances.sort_values(by='importance', ascending=False)","92fb06f6":"test_df.shape","81e95c29":"test_df","9345277d":"submit = pd.read_csv('\/kaggle\/input\/real-estate-price-prediction-moscow\/sample_submission.csv')\nsubmit.head()","8f3de228":"predictions = model.predict(test_df)\npredictions","af5dcb68":"submit['Price'] = predictions\nsubmit.head()","d313a615":"submit.to_csv('submit.csv', index=False)","14dc96e9":"### **Learning**","91da692a":"All examples in this section should work. You can uncomment each model and check its evaluation.","fa0cdae1":"**RandomForestRegressor**","6ae7976d":"**GridSearchCV with RandomForestRegressor**","cf3788a4":"**LifeSquare**","8014a4af":"**Nominative variables**","eff80e70":"### **7. Model building**  <a class='anchor' id='modeling'>","ae4f5718":"**Quantitative variables**","4ad1f580":"#### **Cross-validation**","2307ce31":"**XGBRegressor**","e00c6acd":"### **2. Processing outliers**  <a class='anchor' id='outlier'>\nWhat we can do with outliers?\n1. Drop these data on train\n2. Replace them with medians, mean values, np.clip, etc\n3. Create an additional feature by outlier data\n4. Leave them alone :)","7f918b45":"**Dummies**","4e91e336":"### **4. Feature generation**  <a class='anchor' id='feature'>","9d46e365":"## **1. EDA**  <a class='anchor' id='eda'>\nWe use EDA for:\n- processing outliers\n- filling NaN gaps\n- finding ideas for new features","473bce7d":"### **6. Train and test split**  <a class='anchor' id='split'>","4139e3eb":"**CatBoostRegressor**","d8252fea":"**DistrictSize, IsDistrictLarge**","63e7625d":"**LGBMRegressor**","366a2856":"## **Loading data** <a class='anchor' id='load'>","3b174743":"### **3. Processing NaN values**  <a class='anchor' id='nan'>","a02841a1":"### **Types cast**","e00473a1":"**Feature importances**","9bdf4f2d":"**Grid search with CatBoostRegressor**","c9c31c28":"**Target variable**","dca49183":"**HouseYear**","ce9c2d03":"**Healthcare_1**","7b735106":"### **Dataset description**\n\n* **Id** - flat identifier in dataset\n* **DistrictId** - district identifier in dataset\n* **Rooms** - rooms number\n* **Square** - square of flat\n* **LifeSquare** - living square of flat\n* **KitchenSquare** - kitchen square of flat\n* **Floor** - floor of flat\n* **HouseFloor** - number of floors in house\n* **HouseYear** - year where the house was built\n* **Ecology_1, Ecology_2, Ecology_3** - ecology metrics of the area\n* **Social_1, Social_2, Social_3** - social metrics of the area\n* **Healthcare_1, Helthcare_2** - healthcare metrics of the area\n* **Shops_1, Shops_2** - metrics related to the presence of shops, shopping centers\n* **Price** - price of flat","a4892a31":"**Model evaluation**","b7bcf43c":"**Rooms**","a7dbe1b0":"### **8. Prediction on test dataset** <a class='anchor' id='prediction'>\n\n1. Make sure that evaluated data preprocesing and feature generation for test dataset too\n2. Make sure that you didn't lose or mix indices in predicted data\n3. Predictions should be evaluated for all rows from the test dataset","7341cddd":"**HouseFloor, Floor**","a05993d6":"### **Paths for train and test datasets**","80eeec5d":"### **Table of contents**\n* [Loading data](#load)\n* [1. EDA](#eda)\n* [2. Processing outliers](#outlier)\n* [3. Processing NaN values](#nan)\n* [4. Feature generation](#feature)\n* [5. Feature selection](#feature_selection)\n* [6. Train and test split](#split)\n* [7. Model building](#modeling)\n* [8. Prediction on test dataset](#prediction)","a81a2fb9":"### **5. Feature selection**  <a class='anchor' id='feature_selection'>","201984fb":"**MedPriceByFloorYear**","67e162d8":"### **Imports**","3917de4f":"**MedPriceByDistrict**","8afa6ecb":"# Real Estate Price Prediction\n\n### This is the course project for GeekBrains University\n\n* Use sklearn.metrics.r2_score metrics for model quality assessment\n* You need to get R2 > 0.6 on the Private Leaderboard\n* All CSV files should contain header with column names and comma as delimiter. The indexes should be removed\n\nRecommendations for working with the code:\n1. Notebook must contain headers and comments\n2. Use functions for repetitive operations\n3. Add charts for describing the data (about 3-5)\n4. Add only the best model, and don't include all possible models for solving the project\n5. Notebook must work from beginning to end (from loading data to getting predictions)\n6. The whole project must be in one notebook\n7. When using statistics (mean, median, etc.) as features, it is better to calculate them on the train dataset, and use it on validation and test datasets withhout recalculation\n8. The project should be evaluated in a reasonable time (no more than 10 minutes), so, do not include in the final version models with checking of a large number of combinations of parameters (GridSearch, etc).","198b7803":"**KitchenSquare** "}}