{"cell_type":{"7e39a193":"code","a7778169":"code","2605f365":"code","83a383cb":"code","2b1dcdc2":"code","f2364810":"code","07fadd7e":"code","4513466e":"code","f3ad9791":"code","d878c34a":"code","45a47da8":"code","c03aeef3":"code","2d5e5904":"code","2dfd9489":"markdown","5cf60de9":"markdown","847a7665":"markdown","a2685fc6":"markdown","062ece03":"markdown","246b3b7c":"markdown","3cf87d08":"markdown","67d96e93":"markdown","6ecfa617":"markdown","82eef1aa":"markdown","381aa205":"markdown","f6e76903":"markdown","b8b1c5c4":"markdown"},"source":{"7e39a193":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score \nfrom sklearn.model_selection import learning_curve, StratifiedKFold, train_test_split\nfrom sklearn.feature_selection import RFECV, SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz \n%matplotlib inline","a7778169":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ndata = np.c_[cancer.data, cancer.target]\ncolumns = np.append(cancer.feature_names, [\"target\"])\nsizeMeasurements = pd.DataFrame(data, columns=columns)\nX = sizeMeasurements[sizeMeasurements.columns[:-1]]\ny = sizeMeasurements.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\nprint('\\n Feature Names: \\n\\n', X.columns.values, \"\\n\")","2605f365":"sns.set_style(\"whitegrid\")\nplotOne = sns.FacetGrid(sizeMeasurements, hue=\"target\",aspect=2.5)\nplotOne.map(sns.kdeplot,'mean area',shade=True)\nplotOne.set(xlim=(0, sizeMeasurements['mean area'].max()))\nplotOne.add_legend()\nplotOne.set_axis_labels('mean area', 'Proportion')\nplotOne.fig.suptitle('Area vs Diagnosis (Blue = Malignant; Orange = Benign)')\nplt.show()\n\nsns.set_style(\"whitegrid\")\nplotTwo = sns.FacetGrid(sizeMeasurements, hue=\"target\",aspect=2.5)\nplotTwo.map(sns.kdeplot,'mean concave points',shade= True)\nplotTwo.set(xlim=(0, sizeMeasurements['mean concave points'].max()))\nplotTwo.add_legend()\nplotTwo.set_axis_labels('mean concave points', 'Proportion')\nplotTwo.fig.suptitle('# of Concave Points vs Diagnosis (Blue = Malignant; Orange = Benign)')\nplt.show()","83a383cb":"print(\"\\nFeature Correlation:\\n\")\ng = sns.heatmap(X_train.corr(),cmap=\"BrBG\",annot=False)","2b1dcdc2":"sizeMeasurements2 = sizeMeasurements.drop(['mean radius','mean perimeter',\n 'mean smoothness', 'mean compactness', 'mean concavity',\n 'mean concave points', 'mean fractal dimension',\n 'radius error', 'texture error', 'perimeter error', 'area error',\n 'smoothness error', 'compactness error', 'concavity error',\n 'concave points error', 'symmetry error', 'fractal dimension error',\n 'worst radius', 'worst perimeter', \n 'worst smoothness', 'worst compactness', \n 'worst concave points', 'worst symmetry', 'worst fractal dimension','worst texture', 'worst area',\n 'worst concavity'], axis=1)\nX2 = sizeMeasurements2[sizeMeasurements2.columns[:-1]]\ny2 = sizeMeasurements2.target\nX_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, y2, test_size=0.2)\nprint('\\n Feature Names: \\n\\n', X2.columns.values, \"\\n\")\nprint(\"\\nFeature Correlation:\\n\")\ng = sns.heatmap(X_train2.corr(),cmap=\"BrBG\",annot=False)","f2364810":"from sklearn.decomposition import PCA\nX3=X\ny3=y\nvariance_pct = 5 # Minimum percentage of variance we want to be described by the resulting transformed components\npca = PCA(n_components=variance_pct) # Create PCA object\nX_transformed = pca.fit_transform(X3,y3) # Transform the initial features\nX3pca = pd.DataFrame(X_transformed) # Create a data frame from the PCA'd data\nX_train3, X_test3, Y_train3, Y_test3 = train_test_split(X3pca, y3, test_size=0.2)\nprint('\\n Feature Names: \\n\\n', X3pca.columns.values, \"\\n\")\n#print('First Few Values, After PCA: \\n\\n,',X3pca.head(),'\\n\\n')\nprint(\"\\nFeature Correlation:\\n\")\ng = sns.heatmap(X_train3.corr(),cmap=\"BrBG\",annot=False)","07fadd7e":"clf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf1.fit(X_train, Y_train)\nclf2 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf2.fit(X_train2, Y_train2)\nclf3 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf3.fit(X_train3, Y_train3)\nprint('Accuracy of Decision Tree classifier on original training set: {:.2f}'.format(clf1.score(X_train, Y_train)))\nprint('Accuracy of Decision Tree classifier on original test set: {:.2f}'.format(clf1.score(X_test, Y_test)))\nprint('Accuracy of Decision Tree classifier on reduced training set: {:.2f}'.format(clf2.score(X_train2, Y_train2)))\nprint('Accuracy of Decision Tree classifier on reduced test set: {:.2f}'.format(clf2.score(X_test2, Y_test2)))\nprint('Accuracy of Decision Tree classifier on PCA-transformed training set: {:.2f}'.format(clf3.score(X_train3, Y_train3)))\nprint('Accuracy of Decision Tree classifier on PCA-transformed test set: {:.2f}'.format(clf3.score(X_test3, Y_test3)))","4513466e":"feature_names1 = X.columns.values\nfeature_names2 = X2.columns.values\nfeature_names3 = X3pca.columns.values\n\ndef plot_decision_tree1(a,b):\n    dot_data = tree.export_graphviz(a, out_file=None, \n                             feature_names=b,  \n                             class_names=['Malignant','Benign'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph \nplot_decision_tree1(clf1,feature_names1)","f3ad9791":"def plot_feature_importances(clf, feature_names):\n    c_features = len(feature_names)\n    plt.barh(range(c_features), clf.feature_importances_)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature name\")\n    plt.yticks(np.arange(c_features), feature_names)\nplot_feature_importances(clf1, feature_names1)","d878c34a":"plot_decision_tree1(clf2,feature_names2)","45a47da8":"plot_feature_importances(clf2, feature_names2)\nplt.show()","c03aeef3":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndict_characters = {0: 'Malignant', 1: 'Benign'}","2d5e5904":"(X1, y1) = load_breast_cancer(return_X_y = True)\nX_train1,X_test1,Y_train1,Y_test1=train_test_split(X1,y1,random_state=0)\nclf = RandomForestClassifier(max_features=8,random_state=0)\nclf.fit(X_train1,Y_train1)\nprint('Accuracy of Random Forest Classifier on training data: {:.2f}'.format(clf.score(X_train1,Y_train1)))\nprint('Accuracy of Random Forest Classifier on testing data: {:.2f}'.format(clf.score(X_test1,Y_test1)))\nmodel = clf\nprediction = model.predict(X_test1)\ncnf_matrix = confusion_matrix(Y_test1, prediction)\nplt.show()\nplot_learning_curve(model, 'Learning Curve For RF', X_train, Y_train, (0.80,1.1), 10)\nplt.show()\nplot_confusion_matrix(cnf_matrix, classes=dict_characters,title='Confusion matrix')\nplt.show()","2dfd9489":"## Step 6: Explore Decision Trees\n\n(1) No feature selection\n\n","5cf60de9":"## Step 2: Load and Describe Data\n\n","847a7665":"(3) retain only features that were produced through PCA transformation.\n\n","a2685fc6":"## Step 4: Feature Selection\nHere we will compare three methods for feature selection: (1) No feature selection; (2) retain only features that are not correlated; (3) use PCA transformation to select features and reduce feature correlation.\n\n(1) No feature selection\n\n","062ece03":"## Step 3: Plot Data\nHealthy cells typically have nuclei with a standard size and shape while cancer cells often have nuclei that are large and mishapen. As such, the size and shape of the nucleus should be a good predictor for whether or not a sample is cancerous.\n","246b3b7c":"(2) retain only features that are not correlated\n","3cf87d08":"(2) retain only features that are not correlated\n\n","67d96e93":"## Step 5: Evaluate Model Performance\n\n","6ecfa617":"(3) use PCA transformation to select features and reduce feature correlation.\n\n","82eef1aa":"## Step 7: Define Helper Functions for Learning Curve and Confusion Matrix\n\n","381aa205":"## Step 8: Evaluate Random Forest Classifier\n\n","f6e76903":"# Predicting Cancer From Nuclear Shape Measurements (Wisconsin Breast Cancer Dataset)\nTechnicians can use a microscope to observe tissue samples that were taken from patients who are suspected to have breast cancer. By looking at the size and shape of the nuclei present within these tissue samples, one can then predict whether a given sample appears to be cancerous. In this document I demonstrate an automated methodology to predict if a sample is benign or malignant given measurements of nuclear shape that were made from digital images of fine needle aspirates of breast tissue masses from clinical samples.\n## Step 1: Import Libraries\n","b8b1c5c4":"Here we predict whether or not a sample is cancerous with an accuracy rate of 99%.\n\n\nFor a more detailed analysis, see the following link: https:\/\/www.kaggle.com\/paultimothymooney\/predicting-breast-cancer-from-nuclear-shape"}}