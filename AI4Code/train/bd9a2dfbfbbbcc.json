{"cell_type":{"7ab484a0":"code","a2598e88":"code","641d9763":"code","8bca53e6":"code","cd7d61a1":"code","2083736b":"code","023ba4d6":"code","51345c78":"code","8c5bb19b":"code","56d73ad0":"code","6957b007":"code","09eab3b6":"code","b564da83":"code","4acce98c":"code","8709713e":"code","6d4879d1":"code","f62157ce":"code","4787d73f":"code","11dd8b7f":"code","4695ea87":"code","dd0f87d3":"code","08a83d04":"code","9184d017":"code","02b275d6":"code","e0ee416d":"code","bf5e4605":"code","4a2df77d":"code","e2039503":"code","6a196235":"code","ca18f1da":"code","d2e69646":"code","4438994e":"code","b42076b3":"code","c60e97ec":"code","3504b547":"code","096647b3":"code","4cb6826e":"code","9700155c":"code","0accd895":"code","3e042377":"code","ef7e7a0e":"code","bced7651":"code","b7132044":"code","4a756433":"code","d8df9c36":"code","45f56096":"code","efaf423e":"code","79ac4be9":"code","67e235b3":"code","e8fc0de4":"code","1d99d26d":"code","d5b36a31":"code","983577ae":"code","95c6b8d3":"code","131e13da":"code","0f97653f":"code","c6fc6ca9":"code","654563c7":"code","ff8d4977":"code","9b8e656a":"code","7c521746":"code","fe52c4dd":"code","5c59bddd":"code","b37a9686":"code","f03a7752":"code","ec72db85":"code","ddeaaedb":"code","99b1a379":"code","71399631":"code","67b9c543":"markdown","38b3c90f":"markdown","058df387":"markdown","9fb48f19":"markdown","5f80950a":"markdown","28331c3f":"markdown","d2b44c76":"markdown","f6656fe3":"markdown","a6215883":"markdown","b0707565":"markdown","65127b96":"markdown","5c4dbdb0":"markdown","14336390":"markdown","d74c8e04":"markdown","d8c3497a":"markdown","57aef417":"markdown"},"source":{"7ab484a0":"#Import all the required libraries\n\n#System libraries\nimport os, time\nfrom tqdm import tqdm\nimport glob\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\nimport collections, random, re\nfrom collections import Counter\n\n# Model building \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n\n#Read\/Display  images\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n\n# import tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import plot_model\n\n","a2598e88":"print(\"Tensorflow Version:\",tf.__version__)\nprint(\"Keras Version     :\",keras.__version__)","641d9763":"## Global Variables\nINPUT_PATH = \"..\/input\/flickr8k\/\"\nIMAGE_PATH = INPUT_PATH+'Images\/'\nCAPTIONS_FILE = INPUT_PATH+'captions.txt'\nOUTPUT_IMAGE_PATH = \"..\/working\/Image\/\"","8bca53e6":"#Import the dataset and read the image into a seperate variable\nall_imgs = glob.glob(IMAGE_PATH + '\/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","cd7d61a1":"# function to plot read and plot images\ndef plot_image(images, caption = None, cmap = None):\n    f, axes = plt.subplots(1, len(images), sharey = True)\n    f.set_figwidth(15)\n    \n    for ax, image in zip(axes, images):\n        ax.imshow(io.imread(image), cmap)","2083736b":"#Visualise both the images & text present in the dataset\nplot_image(all_imgs[0:5])","023ba4d6":"#Import the dataset and read the text file into a seperate variable\ndef load_doc(filename):\n    \n    file = open(filename)\n    text = file.read()\n    file.close()\n   \n    return text\n\ndoc = load_doc(CAPTIONS_FILE)\nprint(doc[:300])","51345c78":"def get_img_ids_and_captions(text):\n    keys=[]\n    values=[]\n    key_paths=[]\n    text=text.splitlines()[1:]\n    for line in text:\n        com_idx=line.index(\",\")\n        im_id,im_cap=line[:com_idx],line[com_idx+1:]\n        keys.append(im_id)\n        values.append(im_cap)\n        key_paths.append(IMAGE_PATH+'\/'+im_id)\n    return keys,key_paths, values","8c5bb19b":"# all_img_id= #store all the image id here\n# all_img_vector= #store all the image path here\n# annotations= #store all the captions here\n\nall_img_id, all_img_vector, annotations = get_img_ids_and_captions(doc)\n\ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n    \ndf.head()","56d73ad0":"df.info()","6957b007":"def plot_image_captions(Pathlist,captionsList,fig,count=2,npix=299,nimg=2):\n        image_load = load_img(Path,target_size=(npix,npix,3))\n        ax = fig.add_subplot(nimg,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        \n        count +=1\n        ax = fig.add_subplot(nimg,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(captions))\n        for i, caption in enumerate(captions):\n            ax.text(0,i,caption,fontsize=20)","09eab3b6":"# Images \nfig = plt.figure(figsize=(10,20))\ncount = 1\n    \nfor Path in df[:20].Path.unique():\n    captions = list(df[\"Captions\"].loc[df.Path== Path].values)\n    plot_image_captions(Path,captions,fig,count,299,5)\n    count +=2\nplt.show()","b564da83":"uni_filenames= np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\", Counter(Counter(df.ID.values).values()))","4acce98c":"#Create the vocabulary & the counter for the captions\ndef voc_fetcher(frame,column):\n    out=[]\n    for i in frame[column]:\n        out+=i.split(\" \")\n    return out\n\nvocabulary=voc_fetcher(df,\"Captions\")\nval_count=Counter(vocabulary)\nval_count","8709713e":"#Visualise the top 30 occuring words in the captions\nmost_occur = val_count.most_common(30)\nprint(most_occur)","6d4879d1":"df_word = pd.DataFrame.from_dict(val_count, orient = 'index')\ndf_word = df_word.sort_values(by = [0], ascending=False).reset_index()\ndf_word = df_word.rename(columns={'index':'word', 0:'count'})\n\ndf_word.head()","f62157ce":"#Visualise the top 30 occuring words in the captions\ndef plthist(index,words,count, title=\"The top 30 most frequently appearing words\"):\n    plt.figure(figsize=(20,4))\n    plt.bar(words,count,color='maroon', width =0.4)\n    plt.xlabel(\"Words\",  fontsize=20) \n    plt.ylabel(\"Word Count\",rotation=90,fontsize=20) \n   # plt.yticks(fontsize=20)\n    plt.xticks(index,words,rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n    \nwords = list(df_word[:30].word)\n\ncount =list(df_word['count'][:30])\nplthist(list(range(0,30)),words,count)","4787d73f":"#Create a list which contains all the captions\nannotations = df.Captions.apply(lambda z:\"<start>\"+\" \"+z+\" \"+\"<end>\") #add the <start> & <end> token to all those captions as well\n\n#Create a list which contains all the path to the images\nall_img_path = df.Path.to_list()\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_path)))","11dd8b7f":"# create the tokenizer\ntop_word_cnt = 5000\nspecial_chars = '!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ '\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_word_cnt,\n                                                  oov_token=\"<unk>\",\n                                                  filters=special_chars)\ntokenizer.fit_on_texts(annotations)\ntrain_seqs = tokenizer.texts_to_sequences(annotations)","4695ea87":"train_seqs[:5]","dd0f87d3":"annotations[:5]","08a83d04":"# Create word-to-index and index-to-word mappings.\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\ntrain_seqs = tokenizer.texts_to_sequences(annotations)","9184d017":"tokenizer.word_counts","02b275d6":"# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\nword_counts = tokenizer.word_counts\ndf_word = pd.DataFrame.from_dict(word_counts, orient = 'index')\ndf_word = df_word.sort_values(by = [0], ascending=False).reset_index()\ndf_word = df_word.rename(columns={'index':'word', 0:'count'})\n\nwords = list(df_word[:30].word)\ncount =list(df_word['count'][:30])\nplthist(list(range(0,30)),words,count, title='Top 30 occuring words after text processing')","e0ee416d":"max_len = max([len(cap) for cap in train_seqs])\nprint(\"shape of caption vector: \", len(train_seqs))\nprint(\"Maximum length of sequence: \",max_len)","bf5e4605":"# Pad each vector to the max_length of the captions ^ store it to a vairable\n\ncap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs,\n                                                          padding='post',\n                                                          maxlen=max_len)\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))","4a2df77d":"cap_vector","e2039503":"#write your code here\ndef preprocess_image(image_path, shape = (299, 299)):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image,channels=3)\n    image = tf.image.resize(image,shape)\n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n    return image, image_path","6a196235":"print(\"Shape after resize :\", preprocess_image(all_img_path[0])[0].shape)\nplt.imshow(preprocess_image(all_img_path[0])[0])","ca18f1da":"encode_train_set = sorted(set(all_img_vector))\n\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train_set)\nimage_dataset = image_dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\nimage_dataset","d2e69646":"# train-test split\nimage_train, image_test, caption_train, caption_test = train_test_split(all_img_vector,cap_vector,\n                                                                        test_size=0.2,random_state=42)","4438994e":"print(\"Training data for images: \" + str(len(image_train)))\nprint(\"Testing data for images: \" + str(len(image_test)))\nprint(\"Training data for Captions: \" + str(len(caption_train)))\nprint(\"Testing data for Captions: \" + str(len(caption_test)))","b42076b3":"sample_img_batch, sample_cap_batch = next(iter(image_dataset))\nprint(sample_img_batch.shape) #(batch_size, 299, 299, 3)\nprint(sample_cap_batch.shape) #(batch_size, max_len)","c60e97ec":"image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\nimage_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\nnew_input = image_model.input  # get the input of the image_model\nhidden_layer = image_model.layers[-1].output  # get the output of the image_model\n\nimage_features_extract_model = keras.Model(new_input, hidden_layer)  # build the final model using both input & output layer","3504b547":"image_features_extract_model.summary()","096647b3":"# write your code to extract features from each image in the dataset\nfeature_dict = {}\nfor image,path in tqdm(image_dataset):\n    batch_features = image_features_extract_model(image)\n    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n    for batch_f, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        feature_dict[path_of_feature] =  batch_f.numpy()","4cb6826e":"def map_function(image_name,capt):\n    image_tensor = feature_dict[image_name.decode('utf-8')]\n    return image_tensor,capt","9700155c":"\ndef generate_dataset(images_data, captions_data, BATCH_SIZE =32, BUFFER_SIZE = 1000):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n    dataset = dataset.shuffle(BUFFER_SIZE)\n\n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_function, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\n\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset","0accd895":"train_dataset=generate_dataset(image_train,caption_train)\ntest_dataset=generate_dataset(image_test,caption_test)","3e042377":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\nprint(sample_cap_batch.shape) #(batch_size,40)","ef7e7a0e":"BATCH_SIZE = 32\nembedding_dim = 256 \nunits = 512\nvocab_size = 5001 #top 5,000 words +1\ntrain_num_steps = len(image_train) \/\/ BATCH_SIZE\ntest_num_steps = len(image_test) \/\/ BATCH_SIZE","bced7651":"class Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        self.dense = tf.keras.layers.Dense(embed_dim)\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        \n    def call(self, features):\n        # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features =  self.dense(features) \n        features = tf.nn.relu(features)\n        \n        return features","b7132044":"encoder=Encoder(embedding_dim)","4a756433":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden, 1) # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) # build your score funciton to shape: (batch_size, 8*8, units)\n        attention_weights =  tf.keras.activations.softmax(self.V(score), axis=1) # extract your attention weights with shape: (batch_size, 8*8, 1)\n        context_vector = attention_weights * features #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = tf.reduce_sum(context_vector, axis = 1) # reduce the shape to (batch_size, embedding_dim)\n        \n\n        return context_vector, attention_weights","d8df9c36":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero =  False) #build your Embedding layer\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        \n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n        embed =  self.dropout(self.embed(x)) # embed your input to shape: (batch_size, 1, embedding_dim)\n        mask = self.embed.compute_mask(x)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed, mask = mask) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        output = self.d1(output)\n        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n        \n        return output,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","45f56096":"decoder=Decoder(embedding_dim, units, vocab_size)","efaf423e":"features=encoder(sample_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","79ac4be9":"optimizer = tf.keras.optimizers.Adam() #define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') #define your loss object","67e235b3":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","e8fc0de4":"checkpoint_path = \".\/checkpoints\/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","1d99d26d":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","d5b36a31":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n        avg_loss = (loss\/int(target.shape[1]))\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        \n    return loss, avg_loss","983577ae":"@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n        avg_loss = (loss \/ int(target.shape[1]))\n\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        \n    return loss, avg_loss","95c6b8d3":"def test_loss_cal(test_dataset):\n    total_loss = 0\n    \n    total_loss = 0\n    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n    avg_test_loss=total_loss\/test_num_steps\n    \n    return avg_test_loss","131e13da":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss \/ train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","0f97653f":"plt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.legend([\"Train\", \"Test\"], loc =\"best\")\nplt.show()","c6fc6ca9":"def evaluate(image):\n    max_length=max_len\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(preprocess_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions","654563c7":"def beam_evaluate(image, beam_index = 3):\n    max_length=max_len\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(preprocess_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n\n    while len(result[0][0]) < max_length:\n        i=0\n        temp = []\n        for s in result:\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:]\n          \n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n            \n                prob += np.log(predictions[0][w])\n                    \n                temp.append([next_cap, prob])\n        result = temp\n        result = sorted(result, reverse=False, key=lambda l: l[1])\n        result = result[-beam_index:]\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3):\n            dec_input = tf.expand_dims([prd_id], 0)  \n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","ff8d4977":"def plot_attmap(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(weights[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap\/\/2, len_cap\/\/2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","9b8e656a":"from nltk.translate.bleu_score import sentence_bleu","7c521746":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","fe52c4dd":"features_shape = batch_f.shape[1]\nattention_features_shape = batch_f.shape[0]","5c59bddd":"rid = np.random.randint(0, len(image_test))\ntest_image = image_test[rid]\n# test_image = '.\/images\/413231421_43833a11f5.jpg'\n# real_caption = '<start> black dog is digging in the snow <end>'\n\nreal_caption = ' '.join([tokenizer.index_word[i] for i in caption_test[rid] if i not in [0]])\nresult, attention_plot,pred_test = evaluate(test_image)\nreal_caption=filt_text(real_caption)      \npred_caption=' '.join(result).rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = pred_caption.split()\n\nprint ('Real Caption      :', real_caption)\nprint ('Prediction Caption:', pred_caption)\nprint ('')\nplot_attmap(result, attention_plot, test_image)\n\nImage.open(test_image)","b37a9686":"print ('Real Caption      :', real_caption)\nprint ('Prediction Caption:', pred_caption)\n\nscore1 = sentence_bleu(reference, candidate, weights=(1,0,0,0))\nscore2 = sentence_bleu(reference, candidate, weights=(0,1,0,0))\nscore3 = sentence_bleu(reference, candidate, weights=(0,0,1,0))\nscore4 = sentence_bleu(reference, candidate, weights=(0,0,0,1))\n\nprint(\"\\nBELU score: \")\nprint(f\"Individual 1-gram: {score1*100}\")\nprint(f\"Individual 2-gram: {score2*100}\")\nprint(f\"Individual 3-gram: {score3*100}\")\nprint(f\"Individual 4-gram: {score4*100}\")","f03a7752":"score1 = sentence_bleu(reference, candidate, weights=(1,0,0,0))\nscore2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\nscore3 = sentence_bleu(reference, candidate, weights=(0.33,0.33,0.33,0))\nscore4 = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n\nprint(f\"BELU score: \")\nprint(f\"Cumumlative 1-gram: {score1*100}\")\nprint(f\"Cumumlative 2-gram: {score2*100}\")\nprint(f\"Cumumlative 3-gram: {score3*100}\")\nprint(f\"Cumumlative 4-gram: {score4*100}\")","ec72db85":"captions=beam_evaluate(test_image)\nprint(captions)","ddeaaedb":"# install required library\n!pip install gTTS","99b1a379":"# converting text-to-speach \nfrom gtts import gTTS\nfrom IPython import display\n\ntts = gTTS(pred_caption, slow = False)\ntts.save('pred_caption.mp3')\n\nsound_file = 'pred_caption.mp3'\ndisplay.display(display.Audio(sound_file))","71399631":"# converting text-to-speach using beam_search\ntts = gTTS(captions, slow = False)\ntts.save('beam_evaluate.mp3')\n\nsound_file = 'beam_evaluate.mp3'\ndisplay.display(display.Audio(sound_file))","67b9c543":"## Pre-processing the images\n\n1.Resize them into the shape of (299, 299)\n\n3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. ","38b3c90f":"### Beam Search(optional)","058df387":"### Decoder","9fb48f19":"## Data understanding\n1.Import the dataset and read image & captions into two seperate variables\n\n2.Visualise both the images & text present in the dataset\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Create a dataframe which summarizes the image, path & captions as a dataframe\n\n5.Visualise the top 30 occuring words in the captions\n\n6.Create a list which contains all the captions & path\n","5f80950a":"## Create the train & test data \n1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n","28331c3f":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","d2b44c76":"Create a dataframe which summarizes the image, path & captions as a dataframe\n\nEach image id has 5 captions associated with it therefore the total dataset should have 40455 samples.","f6656fe3":"### Greedy Search","a6215883":"## Pre-Processing the captions\n1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.Replace all other words with the unknown token \"UNK\" .\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Pad all sequences to be the same length as the longest one.","b0707565":"### Attention model","65127b96":"## Model training & optimization\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","5c4dbdb0":"### Encoder","14336390":"## Load the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n\n","d74c8e04":"# EYE FOR BLIND\nPrepared by: Santh Raul\nEnvironment used: Kaggle\n\n**Problem statement:** To create a deep learning model that can explain the content of an image in the form of speech through caption generation with the attention mechanism on the Flickr8K data set \n\nThis type of model is a use case for blind people so that they can understand any image with the help of speech. The caption generated through a CNN-RNN model will be converted to speech using a text-to-speech library. \n\n \n\nThis problem statement is an application of both deep learning and natural language processing. The features of an image will be extracted by the CNN-based encoder, and this will be decoded by an RNN model.","d8c3497a":"Let's read the dataset","57aef417":"## Model Evaluation\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function usininit_stateeam search ( optional)\n\n3.Test it on a sample data using BLEU score"}}