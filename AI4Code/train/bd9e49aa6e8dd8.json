{"cell_type":{"d45127fd":"code","82645268":"code","4c2a270e":"code","180fc555":"code","433108da":"code","095d7218":"code","0c8b7691":"code","9ff04a1e":"code","521cf110":"code","af0f336a":"code","1d9144e3":"code","e702ed81":"code","db8fdfbf":"code","796c22dc":"code","5cfed056":"code","f5015404":"code","c092df9f":"code","ff64ac7f":"code","c407f37b":"code","45e07dc3":"code","f1a62df1":"code","a025a4af":"code","b1b77dcf":"code","2162fccc":"code","99f93a8a":"code","b4c07ac5":"code","4c882828":"code","6dc66ddd":"code","5e04135b":"code","a8028acf":"code","24dceb63":"code","d48cdfd0":"code","a83b3c84":"code","64fab8fe":"code","e68db263":"code","5011502b":"code","21a5558a":"code","8e82df13":"code","c4b59191":"code","002e942e":"code","2802385f":"code","fcd3a843":"code","622116dd":"code","43f5df33":"code","3680198b":"code","4ebbab3d":"code","e1e5f506":"code","b83481b5":"code","5cbd0501":"code","b946e5be":"code","64918924":"code","b80c5dc3":"code","72910600":"code","cb9fd2d7":"code","673ef894":"code","01080554":"code","9e9fd3cf":"code","e97f3b1e":"code","4e400faf":"code","161f6aa9":"code","678f6b6e":"code","0022b084":"code","ce76c38c":"code","05f74e75":"code","317bbeb2":"code","ccc4745c":"code","9c200cba":"code","f3955ed1":"code","31cf2ec8":"code","48a50170":"code","81c5c80b":"code","057fdd43":"code","ce56650e":"code","e3e8be83":"code","64d63107":"code","65d2a64d":"code","fb19e70f":"markdown","804947e4":"markdown","ace1b65a":"markdown","630a320d":"markdown","852ecb86":"markdown","3c0643a8":"markdown","7e6471a8":"markdown","064cc026":"markdown","57d08e6d":"markdown","e1a0f09a":"markdown","c999a5db":"markdown","e7d28dac":"markdown","8bd6881f":"markdown","a8e25dbe":"markdown","904c1800":"markdown","ee2f5962":"markdown","f11083d3":"markdown","91467d93":"markdown","2feaa8a1":"markdown","eded0c0e":"markdown","5e35b8e3":"markdown","0ca3b0bb":"markdown","0e8e1fc3":"markdown","a02eb3b4":"markdown","b3eef520":"markdown","fc79aeec":"markdown","7e0c9264":"markdown","d74388da":"markdown","5dcacecb":"markdown","4885b78f":"markdown","2cf96531":"markdown","02702a12":"markdown","a541ce3b":"markdown","96a10a2b":"markdown","54b234c5":"markdown","0ad581f7":"markdown","df8e5dde":"markdown","ca998439":"markdown"},"source":{"d45127fd":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re # regex\n\nimport warnings\nwarnings.filterwarnings('ignore')","82645268":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4c2a270e":"data_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndata_train.head()","180fc555":"print('Size of train data', data_train.shape)\ndata_train.isnull().sum()","433108da":"# drop columns: 'keyword', 'location', 'id'\ndata_train.drop(['keyword', 'location', 'id'], axis=1, inplace=True)\nprint('Columns {}, {} and {} have been dropped'.format('keyword', 'location', 'id'))","095d7218":"data_train","0c8b7691":"import nltk\nfrom nltk.corpus import stopwords\n# to stem derived words from the root word\nfrom nltk.stem.porter import PorterStemmer\nimport re","9ff04a1e":"stop = set(stopwords.words('english'))\npstem = PorterStemmer()\n\n\ndef stop_stem_lower(tweet):\n    try:\n        regex = re.compile('[^a-zA-Z]')\n        tweet = regex.sub(\" \", tweet)\n        tweet = tweet.lower()\n\n        # check for stop words and then stem\n        #tweet = [pstem.stem(word) for word in tweet.split() if word not in stop and len(word)>2]\n        tweet = [pstem.stem(word) for word in tweet.split() if word not in stop]\n        # join the words back\n        tweet = ' '.join(tweet)\n        return tweet\n    except:\n        return 0","521cf110":"# try on a sample\nsample = data_train.sample(frac=0.01, random_state=1)\nsample['Cleaned text'] = sample['text'].apply(lambda x: stop_stem_lower(x))\nsample","af0f336a":"# apply stop_stem_lower(tweet) on the entire train\ntemp = pd.DataFrame()\n\n# changing the width of the column\npd.set_option('display.max_colwidth', 100)\n\nimport time\nstart= time.time()\ntemp['original text'] = data_train['text'].copy()\ntemp['cleaned_text'] = data_train['text'].apply(lambda x: stop_stem_lower(x))\nprint('time taken: ', time.time() - start)\ntemp.head(10)","1d9144e3":"# to store the cleaned text\n# to use in the next stage for data preprocessing\n\ncorpus = temp['cleaned_text'].values.tolist()\nprint(corpus[:10])\ndel temp","e702ed81":"# CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncvect = CountVectorizer(stop_words='english') # the stop words were already removed, but i am just adding the paramater here\n\nstart = time.time()\ncvect_fit = cvect.fit_transform(corpus) # this return a 'scipy.sparse.csr.csr_matrix' (Compressed Sparsed row matrix)\nprint(type(cvect_fit))\nprint('Shape of the CSR matrix after the fit and transform method: ', cvect_fit.shape)\nprint('Number of documents in the train corpus: ', data_train.shape[0])\nprint('time taken for CountVectorizer on the cleaned corpus is', time.time() - start)","db8fdfbf":"# the numbers are not 'counts', but the positions in the sparse vector\ndisplay(cvect.vocabulary_)\n\nword_freq_csr = cvect_fit.sum(axis=0) # get the row wise sum of the frequencies of each feature\/words across all the documents","796c22dc":"# word frequency\n# the number here indicates the actual frequency of the word in the entire corpus of documents\nword_freq = [(word, word_freq_csr[0, indx]) for word, indx in cvect.vocabulary_.items()]","5cfed056":"word_freq_sorted = sorted(word_freq, key = lambda x: x[1], reverse=True)\n# the 10 most frequent words\nword_freq_sorted[:10]","f5015404":"corpus[:10]","c092df9f":"## Sample\n\ntemp=[]\nfor sent in corpus[:10]:\n    [temp.append(w) for w in sent.split()]\n    \nprint(temp)","ff64ac7f":"# for the entire train corpus\ntemp_counter= []\n\nfor sent in corpus:\n    [temp_counter.append(w) for w in sent.split()]\n    \n#print(temp_counter)\nprint('Number of words in the corpus is :', len(temp_counter))","c407f37b":"# On a sample\n\nfrom collections import Counter\nCounter(temp).most_common(10)","45e07dc3":"# on the complete train\n# temp_counter has the complete list\n\nCounter(temp_counter).most_common(10)","f1a62df1":"# On a sample\n\ncounter = Counter()\ncounter.update(temp)\ncounter.most_common(10)","a025a4af":"# on the complete train\n# temp_counter has the complete list\n\ncounter.update(temp_counter)\ncounter.most_common(10)","b1b77dcf":"WordFrequency=dict()\nfor sent in corpus:\n    for word in sent.split():\n        if word in WordFrequency.keys():\n            WordFrequency[word]+=1\n        else:\n            WordFrequency[word]=1\n            \n\n# convert dictionary to DataFrame\nWordFrequency_df = pd.DataFrame(WordFrequency.items(), columns=['word', 'frequency'])\nWordFrequency_df.sort_values('frequency', ascending=False).head(10)","2162fccc":"WordFrequency_df.sort_values('frequency', ascending=True)","99f93a8a":"plt.hist(WordFrequency_df['frequency'], range=(0,10))","b4c07ac5":"?plt.hist","4c882828":"# verifying the results\ndisplay(WordFrequency_df['frequency'].value_counts()[:10])\n\n# to see the results in percentages\ndisplay(WordFrequency_df['frequency'].value_counts(normalize=True)[:10])","6dc66ddd":"WordFrequency_df[WordFrequency_df['frequency'] == 1]['word'][:100]","5e04135b":"from nltk.corpus import words as nltk_words","a8028acf":"nltk_words.words()[:10]","24dceb63":"# start = time.time()\n# freq1 = WordFrequency_df[WordFrequency_df['frequency'] == 1]['word'].values.tolist()\n# valid_words_freq1 = [w for w in freq1 if w in nltk_words.words()]\n# print('time taken to find valid words with a frequency of 1 is', time.time() - start)\n# print('Number of valid words with a frequency of 1 is', len(valid_words_freq1))","d48cdfd0":"# valid_words_freq1","a83b3c84":"# save the contents of this list for later use\n# since the search is extensive from the previous cell: output as below\n# time taken to find valid words with a frequency of 1 is 1677.8927783966064 seconds\n# Number of valid words with a frequency of 1 is 1623\n\nimport pickle\n# pickle.dump(valid_words_freq1, open('valid_words_freq1.pkl', 'wb'))\n# \/kaggle\/input\/nlp-getting-started\/train.csv","64fab8fe":"# valid_words_freq1 = pickle.load(open('valid_words_freq1.pkl', 'wb'))\n# valid_words_freq1","e68db263":"WordFrequency_df20 = WordFrequency_df[WordFrequency_df['frequency'] >= 20]\nprint(WordFrequency_df20.shape)","5011502b":"WordFrequency_df20.sort_values('frequency', inplace=True, ascending=False)\nWordFrequency_df20.set_index('word', inplace=True)\nWordFrequency_df20","21a5558a":"countVec = CountVectorizer(max_features=WordFrequency_df20.shape[0])\nstart = time.time()\ncountVec_fit = countVec.fit_transform(corpus)\nprint('time taken:', time.time() - start)","8e82df13":"countVec_fit","c4b59191":"display(countVec_fit.toarray()), display(countVec_fit.todense())\nbagOfwords = countVec_fit.toarray()","002e942e":"X = bagOfwords\ny = data_train['target']","2802385f":"from sklearn.model_selection import cross_val_score","fcd3a843":"from sklearn.tree import DecisionTreeClassifier\n\n#dt = DecisionTreeClassifier(max_depth= 10, min_samples_split=10)\nstart = time.time()\ndt = DecisionTreeClassifier()\ndt_f1_scores = cross_val_score(dt, X, y, cv=5, scoring='f1')\ndt_roc_auc_scores = cross_val_score(dt, X, y, cv=5, scoring='roc_auc')\nprint('Time take for DecisionTreeClassifier is: ', time.time() - start)\nprint('Mean f1 score for DecisionTreeClassifier is: ', dt_f1_scores.mean())\nprint('Mean roc_auc_score for DecisionTreeClassifier is: ', dt_roc_auc_scores.mean())","622116dd":"# save the model\npickle.dump(dt, open('decisiontreemodel.pkl', 'wb'))","43f5df33":"# to view the parameters\n?DecisionTreeClassifier","3680198b":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nstart = time.time()\ngbc_f1_scores = cross_val_score(gbc, X, y, cv=5, scoring='f1')\ngbc_roc_auc_scores = cross_val_score(gbc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for GradientBoostingClassifier is: ', time.time() - start)\nprint('Mean f1 score for GradientBoostingClassifier is: ', gbc_f1_scores.mean())\nprint('Mean roc_auc_score for GradientBoostingClassifier is: ', gbc_roc_auc_scores.mean())","4ebbab3d":"# save the model\npickle.dump(gbc, open('gradientboostingmodel.pkl', 'wb'))","e1e5f506":"?GradientBoostingClassifier","b83481b5":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nstart = time.time()\nknn_f1_scores = cross_val_score(knn, X, y, cv=5, scoring='f1')\nknn_roc_auc_scores = cross_val_score(knn, X, y, cv=5, scoring='roc_auc')\nprint('Time take for KNeighborsClassifier is: ', time.time() - start)\nprint('Mean f1 score for KNeighborsClassifier is: ', knn_f1_scores.mean())\nprint('Mean roc_auc_score for KNeighborsClassifier is: ', knn_roc_auc_scores.mean())","5cbd0501":"# save the model\npickle.dump(knn, open('knnclassifiermodel.pkl', 'wb'))","b946e5be":"?KNeighborsClassifier","64918924":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=1)\nstart = time.time()\nlogreg_f1_scores = cross_val_score(logreg, X, y, cv=5, scoring='f1')\nlogreg_roc_auc_scores = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\nprint('Time take for LogisticRegression is: ', time.time() - start)\nprint('Mean f1 score for LogisticRegression is: ', logreg_f1_scores.mean())\nprint('Mean roc_auc_score for LogisticRegression is: ', logreg_roc_auc_scores.mean())","b80c5dc3":"# save the model\npickle.dump(knn, open('logregressionclassifiermodel.pkl', 'wb'))","72910600":"?LogisticRegression","cb9fd2d7":"from sklearn.linear_model import SGDClassifier\nsgdc = SGDClassifier()\nstart = time.time()\nsgdc_f1_scores = cross_val_score(sgdc, X, y, cv=5, scoring='f1')\nsgdc_roc_auc_scores = cross_val_score(sgdc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for SGDClassifier is: ', time.time() - start)\nprint('Mean f1 score for SGDClassifier is: ', sgdc_f1_scores.mean())\nprint('Mean roc_auc_score for SGDClassifier is: ', sgdc_roc_auc_scores.mean())","673ef894":"# save the model\npickle.dump(sgdc, open('SGDClassifier.pkl', 'wb'))","01080554":"?SGDClassifier","9e9fd3cf":"from sklearn.svm import SVC\nsvc = SVC()\nstart = time.time()\nsvc_f1_scores = cross_val_score(svc, X, y, cv=5, scoring='f1')\nsvc_roc_auc_scores = cross_val_score(svc, X, y, cv=5, scoring='roc_auc')\nprint('Time take for Support Vector Classifier is: ', time.time() - start)\nprint('Mean f1 score for Support Vector Classifier is: ', svc_f1_scores.mean())\nprint('Mean roc_auc_score for Support Vector Classifier is: ', svc_roc_auc_scores.mean())","e97f3b1e":"# save the model\npickle.dump(svc, open('SVCClassifier.pkl', 'wb'))","4e400faf":"?SVC","161f6aa9":"from sklearn.naive_bayes import BernoulliNB\nbernoulliNB = BernoulliNB()\nstart = time.time()\nbernoulliNB_f1_scores = cross_val_score(bernoulliNB, X, y, cv=5, scoring='f1')\nbernoulliNB_roc_auc_scores = cross_val_score(bernoulliNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for BernoulliNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for BernoulliNB Classifier is: ', bernoulliNB_f1_scores.mean())\nprint('Mean roc_auc_score for BernoulliNB Classifier is: ', bernoulliNB_roc_auc_scores.mean())","678f6b6e":"# save the model\npickle.dump(bernoulliNB, open('bernoulliNB.pkl', 'wb'))","0022b084":"from sklearn.naive_bayes import MultinomialNB\nmultinomialNB = MultinomialNB()\nstart = time.time()\nmultinomialNB_f1_scores = cross_val_score(multinomialNB, X, y, cv=5, scoring='f1')\nmultinomialNB_roc_auc_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for MultinomialNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for MultinomialNB Classifier is: ', multinomialNB_f1_scores.mean())\nprint('Mean roc_auc_score for MultinomialNB Classifier is: ', multinomialNB_roc_auc_scores.mean())","ce76c38c":"from sklearn.naive_bayes import GaussianNB\ngaussianNB = GaussianNB()\nstart = time.time()\ngaussianNB_f1_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='f1')\ngaussianNB_roc_auc_scores = cross_val_score(gaussianNB, X, y, cv=5, scoring='roc_auc')\nprint('Time take for GaussianNB Classifier is: ', time.time() - start)\nprint('Mean f1 score for GaussianNB Classifier is: ', gaussianNB_f1_scores.mean())\nprint('Mean roc_auc_score for GaussianNB Classifier is: ', gaussianNB_roc_auc_scores.mean())","05f74e75":"from sklearn.ensemble import VotingClassifier\nsvc = SVC(kernel='linear', probability=True)\n\nestimators = [('LogisiticRegression', logreg),\n              ('Support Vector Classifier', svc),\n              ('Bernoulli NB', bernoulliNB),\n              ('Gradient Boosting Classifier', gbc) \n             ]\n\nvotingclassifier = VotingClassifier(voting='soft', estimators = estimators)\nstart = time.time()\nvotingclassifier_f1_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='f1')\nvotingclassifier_roc_auc_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='roc_auc')\nprint('Time take for VotingClassifier is: ', time.time() - start)\nprint('Mean f1 score for VotingClassifier is: ', votingclassifier_f1_scores.mean())\nprint('Mean roc_auc_score for VotingClassifier is: ', votingclassifier_roc_auc_scores.mean())","317bbeb2":"svc = SVC(kernel='linear', probability=True) # setting \"probability=True\" enables the predict_proba method which is needed when using cross_validation\nsgdc = SGDClassifier(loss ='log') # this enables the predict_proba method\n\nestimators = [('LogisiticRegression', logreg),\n              ('Support Vector Classifier', svc),\n              ('Stochastic Gradient Classifier',sgdc),\n              ('Bernoulli NB', bernoulliNB),\n              ('Gradient Boosting Classifier', gbc)\n             ]\n\nvotingclassifier = VotingClassifier(voting='soft', estimators = estimators)\nstart = time.time()\nvotingclassifier_f1_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='f1')\nvotingclassifier_roc_auc_scores = cross_val_score(votingclassifier, X, y, cv=5, scoring='roc_auc')\nprint('Time take for VotingClassifier is: ', time.time() - start)\nprint('Mean f1 score for VotingClassifier is: ', votingclassifier_f1_scores.mean())\nprint('Mean roc_auc_score for VotingClassifier is: ', votingclassifier_roc_auc_scores.mean())","ccc4745c":"import pickle\npickle.dump(votingclassifier, open('votingclassifier_crossval.pkl', 'wb'))","9c200cba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)","f3955ed1":"# to a get a quick idea of the score using train, test splits\nfrom sklearn.metrics import f1_score, roc_auc_score\nvotingclassifier.fit(X_train, y_train)\nprediction = votingclassifier.predict(X_test)\nprint('f1 score: ', f1_score(y_test, prediction))\nprint('ROC-AUC score: ', roc_auc_score(y_test, prediction))","31cf2ec8":"import lightgbm as lgbm\ndtrain = lgbm.Dataset(X_train, label=y_train)\ndtest = lgbm.Dataset(X_test)\nnum_rounds=50\nparams = {'learning_rate': 0.001,\n          'objective': 'binary',\n          'metric': 'binary_logloss',\n          'max_depth': 30,\n          'num_leaves': 50,\n          'boosting_type': 'gbdt'\n         }","48a50170":"start= time.time()\nlgbm_model = lgbm.train(params, dtrain, num_boost_round=100)\nprint('time taken for the LGB Classifier model is: ', time.time() - start)\nprediction = lgbm_model.predict(X_test)\nprediction\n# print('f1 score: ', f1_score(y_test, prediction))\n# print('ROC-AUC score: ', roc_auc_score(y_test, prediction))","81c5c80b":"# Since the output of the prediction is an array of probabilities, I will convert that it to binary using threshold of 0.43\n# this threshold gives me the highest score\n\ny_pred =[]\nfor i in range(len(prediction)):\n    if prediction[i] >= 0.43:\n        prediction[i] = 1\n    else:\n        prediction[i] =0\n        \nprint('f1 score: ', f1_score(y_test, prediction))\nprint('ROC-AUC score: ', roc_auc_score(y_test, prediction))","057fdd43":"from lightgbm import LGBMClassifier\nLightGBMclf = LGBMClassifier()\nstart = time.time()\nLightGBMClassifier_f1_scores = cross_val_score(LightGBMclf, X, y, cv=5, scoring='f1')\nLightGBMClassifier_roc_auc_scores = cross_val_score(LightGBMclf, X, y, cv=5, scoring='roc_auc')\nprint('Time take for LightGBMClassifier is: ', time.time() - start)\nprint('Mean f1 score for LightGBMClassifier is: ', LightGBMClassifier_f1_scores.mean())\nprint('Mean roc_auc_score for LightGBMClassifier is: ', LightGBMClassifier_roc_auc_scores.mean())","ce56650e":"?LGBMClassifier","e3e8be83":"from xgboost import XGBClassifier\nxgboostclf = XGBClassifier()\nstart = time.time()\nXGBClassifier_f1_scores = cross_val_score(xgboostclf, X, y, cv=5, scoring='f1')\nXGBClassifier_roc_auc_scores = cross_val_score(xgboostclf, X, y, cv=5, scoring='roc_auc')\nprint('Time take for XGBClassifier is: ', time.time() - start)\nprint('Mean f1 score for XGBClassifier is: ', XGBClassifier_f1_scores.mean())\nprint('Mean roc_auc_score for XGBClassifier is: ', XGBClassifier_roc_auc_scores.mean())","64d63107":"# save the model\npickle.dump(xgboostclf, open('XGBClassifier.pkl', 'wb'))","65d2a64d":"xgbscores = {'f1': XGBClassifier_f1_scores.mean(), 'roc-auc': XGBClassifier_roc_auc_scores.mean()}\nlgbmscores = {'f1': LightGBMClassifier_f1_scores.mean(), 'roc-auc': LightGBMClassifier_roc_auc_scores.mean()}\nmultinomialscores = {'f1': multinomialNB_f1_scores.mean(), 'roc-auc': multinomialNB_roc_auc_scores.mean()}\nbernoulliscores = {'f1': bernoulliNB_f1_scores.mean(), 'roc-auc': bernoulliNB_roc_auc_scores.mean()}\nsvcscores = {'f1': svc_f1_scores.mean(), 'roc-auc': svc_roc_auc_scores.mean()}\nsgdcscores = {'f1': sgdc_f1_scores.mean(), 'roc-auc': sgdc_roc_auc_scores.mean()}\nlogregscores = {'f1': logreg_f1_scores.mean(), 'roc-auc': logreg_roc_auc_scores.mean()}\ngbcscores = {'f1': gbc_f1_scores.mean(), 'roc-auc': gbc_roc_auc_scores.mean()}\ndtscores = {'f1': dt_f1_scores.mean(), 'roc-auc': dt_roc_auc_scores.mean()}\n\nmodel_scores_list = [xgbscores,lgbmscores, multinomialscores, bernoulliscores, svcscores, sgdcscores, logregscores, gbcscores, dtscores]\nmodel_name =['xgboost', 'LightGBM', 'MultionomialNB', 'BernoulliNB', 'SVC', 'SGD', 'LogRegression', 'GradientBoosting', 'DecisionTree']\nsummary_df = pd.DataFrame(model_scores_list, index=model_name)\nsummary_df\nsummary_df.sort_values('roc-auc', ascending=False).to_csv('model_score.csv', index=True, header=True)","fb19e70f":"### Using the Counter function: Method 2","804947e4":"### 4.1 Decision Tree Model","ace1b65a":"### 4.10 Light GBM Classifier","630a320d":"### 4.6 Support Vector Machine Classifier (SVM Classifier)","852ecb86":"### Check if words with frequency: 1 are valid English words as per English dictionary and if they can be found from the NLTK corpus","3c0643a8":"## 4. Machine Models","7e6471a8":"### ** More than 69% of the words in the entire train corpus have a word frequency of only 1 **","064cc026":"### 2.3.1 From the list of words, consider removing some words or features to reduce dimensions:\n    - There are some words like 'co' and 'http' which doesn not add any valuable information\n    - There are also other words like 'via', 'u, which could also have been removed in the intial regex compilaton for selecting words that have a minimal length of 3\n    - Also we should consider removing words that are very rare as they do not add any value","57d08e6d":"## 3. Create a Bag of words, using a sparse matrix\n- Use a CountVectorizer, with max_features = number of unique words","e1a0f09a":"### Preprocessing text: Stage 1","c999a5db":"### Split the words in seach sentence and add them to a list, so that we can apply the Counter()","e7d28dac":"### Finding the most frequent words","8bd6881f":"### Using cross-validation with LightGBM\n* for cross validation I have to use the LightGBMClassifier as this has the fit method","a8e25dbe":"### 4.4 Logisitic Regression Classifier","904c1800":"### 2.1 Using CountVectorizer","ee2f5962":"### 2.3.2 ** from the histogram plot, there are more than 13K words which have a frequency of 1 **","f11083d3":"### 4.9 Voting Classifier\n- A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output\n- It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting\n- The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class\n\n- Voting Classifier supports two types of votings:\n    - Hard voting: the predicted output class is a class with the highest majority of votes\n    - Soft voting: the prediction is based on the average of probability given to that class\n- Ref: https:\/\/www.geeksforgeeks.org\/ml-voting-classifier-using-sklearn\/","91467d93":"### Each row in the CSR matrix represents a document and each column is a feature or a word\n- **Each column entry is the count for that feature in that document**\n\n### CountVectorizer methods:\n- After CountVectorizer has been fit on the corpus:\n    - CountVectorizer().vocabulary_ : returns the unique words and their positions in the sparse CSR matrix\n    - CountVectorizer().get_feature_names() : will list all the features or unique words\n    - CountVectorizer().fit_transform(corpus).toarray(): converts the sparse matrix into an array, where each row is a document in the corpus and each column entry is the occurence of the word in the document\n    - CountVectorizer().fit_transform(corpus).sum(axis=0): returns the frequency of the words\/features in the entire corpus. Shape: (1 X number of features)","2feaa8a1":"### 4.10 XGBoost Classifier","eded0c0e":"### Preprocessing text: Stage 2\n* **Check for words that are occur very less in our tweets**\n* **They should be removed from our Bag of words to reduce dimensionality**\n","5e35b8e3":"## 5. Summary\n* Bernoulli NB Algorithm has given the highest 'ROC-AUC' score and also the 'f1' score\n* From the base models with no hyperparamter tuning this is the best choice\n\n- Repeat the preprocessing steps for the given test data and submit to the model for prediction","0ca3b0bb":"### To view the contents of the CSR matrix, use either:\n - ** toarray() method **\n - ** todense() method **","0e8e1fc3":"### 4.7 Multinomial Naive Bayes Classifier","a02eb3b4":"## 3. Preprocessing of text data","b3eef520":"### Using the Counter function: Method 1","fc79aeec":"### 4.5 Stochastic Gradient Descent Classifier (SGD Classifier)","7e0c9264":"### Using the Counter function","d74388da":"### 2.2 Using Counter to count the number of frequent words\n- Ref: https:\/\/stackoverflow.com\/questions\/27488446\/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer","5dcacecb":"### 2.3 Word frequency using custom code or function","4885b78f":"### CountVectorizer does the following by default:\n- lowercases your text (set lowercase=false if you don\u2019t want lowercasing)\n- uses utf-8 encoding\n- performs tokenization (converts raw text to smaller units of text)\n- uses word level tokenization (meaning each word is treated as a separate token)\n- ignores single characters during tokenization (say goodbye to words like \u2018a\u2019 and \u2018I\u2019)","2cf96531":"### 2.3.3  For the below code, I will consider only those words that have frequency of > 20","02702a12":"### 4.8 Gaussian Naive Bayes Classifier","a541ce3b":"### 4.3 K-Nearest Neighbors Classifier(KNN)","96a10a2b":"### 4.2 Gradient Boosting model","54b234c5":"## 1. Load the data","0ad581f7":"* For the SGDClassifier() Error: Cannot use loss ='hinge'with SGDClassifer,hence I have removed it. \n* Using loss=\"log\" or loss=\"modified_huber\" enables the predict_proba method, which gives a vector of probability estimates per sample\n* Ref: https:\/\/scikit-learn.org\/stable\/modules\/sgd.html\n\n### Adding SGDClassifier to the Voting Classifier","df8e5dde":"### 4.6 Bernoulli Naive Bayes Classifier","ca998439":"## 2.Check for missing values"}}