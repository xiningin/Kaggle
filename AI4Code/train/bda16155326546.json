{"cell_type":{"096ba202":"code","ea5bd70f":"code","daf2de5e":"code","5921ea89":"code","7e667fd4":"code","6e5da87f":"code","a8c1f2ba":"code","e499f9c2":"code","d37f9ffe":"code","81db3834":"code","540b83f5":"code","9c390d5b":"code","fff78f02":"code","59b3ebb9":"code","7b81ebc7":"code","fea063ec":"code","71e3cd0e":"code","cbae743b":"code","4e1976e7":"code","09df841e":"code","af6048f2":"code","e38a450d":"code","61cf7306":"code","8e79a293":"code","2bf40b67":"code","fe1d31d5":"code","11e5c60c":"code","21266927":"code","be92acd0":"code","c21b1c40":"code","02da6597":"code","05e670a2":"code","d423a913":"code","bffb9d71":"code","e48e0f58":"code","924b38a2":"code","580a871e":"code","8106fbce":"code","6c69ba63":"code","b0d3276f":"code","327a82b7":"markdown"},"source":{"096ba202":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ea5bd70f":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import GRU\nfrom keras.initializers import random_uniform\nfrom keras.optimizers import Adagrad\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport tensorflow as tf\nimport datetime\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfont = {'family' : 'meiryo'}\nplt.rc('font', **font)","daf2de5e":"import random as rn\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(12345)\nsession_conf =  tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nfrom keras import backend as K\ntf.random.set_seed(1234)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","5921ea89":"train_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntrain_df = train_df.fillna(\"No State\")\ntrain_df","7e667fd4":"test_rate = 0.1\ntime_series_len = 18\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = scaler.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#Formatting the train data for a time series model\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #Areas with zero patients cannot be predicted \u21d2 Artificially predicted to be zero\n    if df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - time_series_len):\n            X.append(df[['ConfirmedCases_std']].iloc[i:(i+time_series_len)].values)\n            Y.append(df[['ConfirmedCases_std']].iloc[i+time_series_len].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","6e5da87f":"confirmedCases_std_min = train_df[\"ConfirmedCases_std\"].min()","a8c1f2ba":"def huber_loss(y_true, y_pred, clip_delta=1.0):\n  error = y_true - y_pred\n  cond  = tf.keras.backend.abs(error) < clip_delta\n\n  squared_loss = 0.5 * tf.keras.backend.square(error)\n  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n\n  return tf.where(cond, squared_loss, linear_loss)\n\ndef huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))","e499f9c2":"epochs_num = 20\nn_in = 1\n\nmodel = Sequential()\nmodel.add(GRU(100,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer=random_uniform(seed=1),\n               return_sequences=False\n             ))\nmodel.add(Dense(50))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(n_in, kernel_initializer=random_uniform(seed=1)))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","d37f9ffe":"callbacks = [ReduceLROnPlateau(monitor='loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='loss', patience=10)]\n\nhist = model.fit(X_train, Y_train, batch_size=20, epochs=epochs_num,\n                 callbacks=callbacks,shuffle=False)","81db3834":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test\nresult_std.plot(figsize=(25,6))\nplt.show()","540b83f5":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","9c390d5b":"predicted = scaler.inverse_transform(predicted_std)\nY_test2 = scaler.inverse_transform(Y_test)","fff78f02":"np.sqrt(mean_squared_log_error(predicted, Y_test2))","59b3ebb9":"result= pd.DataFrame(predicted)\nresult.columns = ['predict']\nresult['actual'] = Y_test2\nresult.plot(figsize=(25,6))\nplt.show()","7b81ebc7":"test_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")","fea063ec":"submission = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")","71e3cd0e":"temp = (datetime.datetime.strptime(\"2020-04-01\", '%Y-%m-%d') - datetime.timedelta(days=time_series_len)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","cbae743b":"check_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\").query(\"Date>'2020-04-01'and Date<='2020-04-14'\")\ncheck_df[\"ConfirmedCases_std\"] = scaler.transform(check_df[\"ConfirmedCases\"].values.reshape(len(check_df[\"ConfirmedCases\"].values),1))","4e1976e7":"confirmedCases_pred = []\nfor i in range(0,313*time_series_len,time_series_len):\n    temp_array = np.array(test_df[\"ConfirmedCases_std\"][i:i+time_series_len])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.array(check_df[\"ConfirmedCases_std\"])[int(i*13\/time_series_len)+j])\n        elif np.array(test_df[\"ConfirmedCases\"][i:i+time_series_len]).sum() == 0:\n            temp_array = np.append(temp_array,temp_array[-1])\n        else:\n            temp_array = np.append(temp_array,model.predict(temp_array[-time_series_len:].reshape(1,time_series_len,1)))\n    confirmedCases_pred.append(temp_array[-43:])","09df841e":"submission[\"ConfirmedCases\"] = np.abs(scaler.inverse_transform(np.array(confirmedCases_pred).reshape(313*43)))\nsubmission[\"ConfirmedCases_std\"] = np.array(confirmedCases_pred).reshape(313*43)\nsubmission","af6048f2":"submission.to_csv('.\/submission_c.csv')\nsubmission.to_csv('..\\output\\kaggle\\working\\submission_c.csv')","e38a450d":"test_rate = 0.1\ntime_series_len = 16\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"Fatalities_std\"] = scaler.fit_transform(train_df[\"Fatalities\"].values.reshape(len(train_df[\"Fatalities\"].values),1))\n\nss = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = ss.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#Formatting the train data for a time series model\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #Areas with zero patients cannot be predicted \u21d2 Artificially predicted to be zero\n    if df[\"Fatalities\"].sum() != 0 or df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - time_series_len):\n            X.append(df[['Fatalities_std','ConfirmedCases_std']].iloc[i:(i+time_series_len)].values)\n            Y.append(df[['Fatalities_std']].iloc[i+time_series_len].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","61cf7306":"fatalities_std_min = train_df[\"Fatalities_std\"].min()","8e79a293":"epochs_num = 21\nn_in = 2\n\nmodel = Sequential()\nmodel.add(GRU(100,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer=random_uniform(seed=1),\n               return_sequences=False))\nmodel.add(Dense(50))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1, kernel_initializer=random_uniform(seed=1)))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","2bf40b67":"callbacks = [ReduceLROnPlateau(monitor='loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='loss', patience=10)]\nhist = model.fit(X_train, Y_train, batch_size=16, epochs=epochs_num,\n                 callbacks=callbacks,shuffle=False)","fe1d31d5":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test","11e5c60c":"result_std.plot(figsize=(25,6))\nplt.show()","21266927":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","be92acd0":"predicted = scaler.inverse_transform(predicted_std)\nY_test = scaler.inverse_transform(Y_test)","c21b1c40":"X_test_ = scaler.inverse_transform(X_test)","02da6597":"np.sqrt(mean_squared_log_error(predicted, Y_test))","05e670a2":"temp = (datetime.datetime.strptime(\"2020-04-01\", '%Y-%m-%d') - datetime.timedelta(days=time_series_len)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","d423a913":"check_df[\"Fatalities_std\"] = scaler.transform(check_df[\"Fatalities\"].values.reshape(len(check_df[\"Fatalities\"].values),1))\ncheck_df","bffb9d71":"fatalities_pred = []\nfor i in range(0,313*time_series_len,time_series_len):\n    temp_array = np.array(test_df[[\"Fatalities_std\",\"ConfirmedCases_std\"]][i:i+time_series_len])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.append(np.array(check_df[\"Fatalities_std\"])[int(i*13\/time_series_len)+j],np.array(check_df[\"ConfirmedCases_std\"])[int(i*13\/time_series_len)+j]).reshape(1,2),axis=0)\n        elif np.array(test_df[[\"Fatalities\",\"ConfirmedCases\"]][i:i+time_series_len]).sum() == 0:\n            temp_array = np.append(temp_array,np.array(temp_array[-1]).reshape(1,2),axis=0)\n        else:\n            temp_array = np.append(temp_array,np.append(model.predict(temp_array[-time_series_len:].reshape(1,time_series_len,2)),submission[\"ConfirmedCases_std\"][i\/time_series_len*43+j]).reshape(1,2),axis=0)\n    fatalities_pred.append(temp_array[-43:])","e48e0f58":"submission[\"Fatalities\"] = np.abs(scaler.inverse_transform([i[0] for i in np.array(fatalities_pred).reshape(313*43,2)]))\nsubmission","924b38a2":"submission[[\"ConfirmedCases\",\"Fatalities\"]] = submission[[\"ConfirmedCases\",\"Fatalities\"]].round().astype(int)\nsubmission","580a871e":"submission = submission.drop(\"ConfirmedCases_std\",axis=1)","8106fbce":"submission = submission.set_index('ForecastId')","6c69ba63":"for i in range(313):\n    for j in range(2,44):\n        if submission[\"ConfirmedCases\"][i*43+j] < submission[\"ConfirmedCases\"][i*43+j-1]:\n            submission[\"ConfirmedCases\"][i*43+j] = submission[\"ConfirmedCases\"][i*43+j-1]\n        if submission[\"Fatalities\"][i*43+j] < submission[\"Fatalities\"][i*43+j-1]:\n            submission[\"Fatalities\"][i*43+j] = submission[\"Fatalities\"][i*43+j-1]","b0d3276f":"submission.to_csv('submission.csv')","327a82b7":"The expected result may be negative because the loss function is MSE.\n\nBecause,restore by taking an absolute value."}}