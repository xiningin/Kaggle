{"cell_type":{"342945f2":"code","c0c0aff8":"code","617a999c":"code","fe000103":"code","62fc6be7":"code","d9be15b5":"code","ea6f1e21":"code","a7e69661":"code","47d9c37e":"code","a0917f90":"code","9b7a50ef":"code","4abce316":"code","0e37cbda":"code","fa5d9178":"code","e37a3062":"code","ae05ab6e":"code","fa7cfdf9":"code","b219ebdf":"code","81d6a096":"code","c0259060":"code","0c20fc2e":"code","863b860c":"code","f4d4c3eb":"code","5ff6d6fb":"code","d1accf97":"code","6675aa1c":"code","0065ee70":"code","aed11cfa":"code","91a30078":"code","c83b291c":"code","14d8b35e":"code","997eea2e":"code","b05a61c9":"code","8d74110b":"code","310cc67a":"code","e1f6e37b":"code","ad5f329f":"code","97105400":"code","b2d416df":"code","191ae654":"code","efc98f99":"code","edfacdc7":"code","7a10d141":"code","bfa713b7":"markdown","fdfca769":"markdown","1b10e59a":"markdown","04379565":"markdown","793abf9f":"markdown","08b3ccda":"markdown","27162084":"markdown","9f5e8800":"markdown","7fb62603":"markdown","8b4c04c4":"markdown","b13aaed8":"markdown","78953c83":"markdown","6f2cb96a":"markdown","49f282ee":"markdown","aa3c8d92":"markdown","c665d05f":"markdown","c367a7d5":"markdown","e5acea9c":"markdown","9ba714e2":"markdown"},"source":{"342945f2":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c0c0aff8":"train = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/test.csv', index_col='id')","617a999c":"train.head(3).T","fe000103":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name', 'dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\nsummary(train)","62fc6be7":"# \u770b\u8fd9\u4e00\u884c\u4e2d\u6709\u591a\u5c11\u4e2aNULL\u503c\ntrain['missing_count'] = train.isnull().sum(axis=1)\ntest['missing_count'] = test.isnull().sum(axis=1)","d9be15b5":"missing_number = -99999\nmissing_string = 'MISSING_STRING'","ea6f1e21":"numerical_features = [\n    'bin_0', 'bin_1', 'bin_2',\n    'ord_0',\n    'day', 'month'\n]\n\nstring_features = [\n    'bin_3', 'bin_4',\n    'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5',\n    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n]","a7e69661":"def impute(train, test, columns, value):\n    for column in columns:\n        train[column] = train[column].fillna(value)\n        test[column] = test[column].fillna(value)","47d9c37e":"impute(train, test, numerical_features, missing_number)\nimpute(train, test, string_features, missing_string)","a0917f90":"#ord_5 \u662f\u7531\u4e24\u4e2a\u5b57\u6bcd\u7ec4\u6210\u7684\uff0c\u8fd9\u6bb5\u7684\u76ee\u7684\u662f\u5c06\u8fd9\u4e24\u4e2a\u5b57\u6bcd\u62c6\u5206\u6210\u4e24\u5217\uff0c\u4ecd\u7136\u4fdd\u7559\u7528 MISSING_STRING \u53bb\u586b\u5145\ntrain['ord_5_1'] = train['ord_5'].str[0]\ntrain['ord_5_2'] = train['ord_5'].str[1]\n\ntrain.loc[train['ord_5'] == missing_string, 'ord_5_1'] = missing_string\ntrain.loc[train['ord_5'] == missing_string, 'ord_5_2'] = missing_string\n\ntrain = train.drop('ord_5', axis=1)\n\n\ntest['ord_5_1'] = test['ord_5'].str[0]\ntest['ord_5_2'] = test['ord_5'].str[1]\n\ntest.loc[test['ord_5'] == missing_string, 'ord_5_1'] = missing_string\ntest.loc[test['ord_5'] == missing_string, 'ord_5_2'] = missing_string\n\ntest = test.drop('ord_5', axis=1)","9b7a50ef":"# \u8fd9\u91cc\u5c06 Feature \u4f5c\u4e3a 3\u7c7b\uff0csimple_features\uff0c ohe_features \u548c target_features \n# Apply Target to features that have many unique values \u8fd9\u91cc\u4ec5\u662f\u8bb2 \u8fd9\u4e00\u5217 Unique Value\u5982\u679c\u5f88\u5927\u7684\u8bdd\uff0c\u624d\u505aTarget Encoding\nsimple_features = [\n    'missing_count'\n]\n\noe_features = [\n    'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4',\n    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4',\n    'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5_1', 'ord_5_2'\n]\n\ntarget_features = [\n    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n]\n\ncyc_features = ['day', 'month']\n\n# ohe\u7684\u542b\u4e49\u662f One Hot Encoding\nohe_features = [\n    'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4',\n    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4',\n    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', # \u8fd9\u4e00\u884c\u7684\u8fd9\u4e9bFeature\u5305\u542b\u4e86\u5927\u91cf\u7684unique values\n    'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5_1', 'ord_5_2',\n    'day', 'month'\n]\n# \u7528\u4e8e Logit \u7684\u662f ohe_features + simple_features\n# \u7528\u4e8e Xgboost \u7684\u662f oe_features + cyc_features + simple_features + target_features","4abce316":"y_train = train['target'].copy()\nx_train = train.drop('target', axis=1)\ndel train\n\nx_test = test.copy()\n\ndel test","0e37cbda":"from sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\nsimple_x_train = scaler.fit_transform(x_train[simple_features])\nsimple_x_test = scaler.transform(x_test[simple_features])","fa5d9178":"type(simple_x_train)","e37a3062":"from sklearn.preprocessing import OneHotEncoder\n\n\nohe = OneHotEncoder(dtype='uint16', handle_unknown=\"ignore\")\nohe_x_train = ohe.fit_transform(x_train[ohe_features])\nohe_x_test = ohe.transform(x_test[ohe_features])","ae05ab6e":"# OneHotEncoder transfer \u4e4b\u540e\u7684\u9ed8\u8ba4\u7684\u662f\u4e00\u4e2a\u7a00\u758f\u77e9\u9635\uff0c\u53ef\u4ee5\u901a\u8fc7 to_array \u6216\u8005\u8bbe\u7f6e sparse=False \u7b49\u8f6c\u5316\u6210 \u6b63\u5e38\u7684 Array\ntype(ohe_x_train)","fa7cfdf9":"ohe_x_train.shape","b219ebdf":"ohe_x_train[:,0]","81d6a096":"def encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n#\u76f4\u63a5\u751f\u62102\u5217\n\nx_train = encode(x_train, 'month', 12)\nx_train = encode(x_train, 'day', 7)\n\nx_test = encode(x_test, 'month', 12)\nx_test = encode(x_test, 'day', 7)","c0259060":"cyclic_x_train = x_train[['month_sin','month_cos','day_sin','day_cos']]\ncyclic_x_test = x_test[['month_sin','month_cos','day_sin','day_cos']]","0c20fc2e":"cyclic_x_test.head(1).T","863b860c":"from sklearn.preprocessing import OrdinalEncoder\n\noe = OrdinalEncoder()\noe_x_train = oe.fit_transform(x_train[oe_features])\noe_x_test = oe.transform(x_test[oe_features])","f4d4c3eb":"from category_encoders import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\n# \u5f88\u9ad8\u7ea7\u7684\u4e00\u79cd\u505a\u6cd5\uff0c\u505a Target Encoding \u7684\u65f6\u5019\u62c6\u5206\u5f00\u6765\u505a\n# oof \u7684\u542b\u4e49\u662f out of fold\ndef transform(transformer, x_train, y_train, cv):\n    oof = pd.DataFrame(index=x_train.index, columns=x_train.columns)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        x_train_train = x_train.loc[train_idx]\n        y_train_train = y_train.loc[train_idx]\n        x_train_valid = x_train.loc[valid_idx]\n        transformer.fit(x_train_train, y_train_train)\n        oof_part = transformer.transform(x_train_valid)\n        oof.loc[valid_idx] = oof_part\n    return oof\n\ntarget = TargetEncoder(drop_invariant=True, smoothing=0.2)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# shuffle\uff1a\u5728\u6bcf\u6b21\u5212\u5206\u65f6\uff0c\u662f\u5426\u8fdb\u884c\u6d17\u724c\n# \u2460\u82e5\u4e3aFalses\u65f6\uff0c\u5176\u6548\u679c\u7b49\u540c\u4e8erandom_state\u7b49\u4e8e\u6574\u6570\uff0c\u6bcf\u6b21\u5212\u5206\u7684\u7ed3\u679c\u76f8\u540c\n# \u2461\u82e5\u4e3aTrue\u65f6\uff0c\u6bcf\u6b21\u5212\u5206\u7684\u7ed3\u679c\u90fd\u4e0d\u4e00\u6837\uff0c\u8868\u793a\u7ecf\u8fc7\u6d17\u724c\uff0c\u968f\u673a\u53d6\u6837\u7684\n# target_x_train \u7528\u7684\u662f oof \u6280\u672f\u8ba1\u7b97\u7684\ntarget_x_train = transform(target, x_train[target_features], y_train, cv).astype('float')\n# target_x_test \u7528\u7684\u662f train \u7684\u5168\u91cftarget\u53bb\u8ba1\u7b97\u7684\ntarget.fit(x_train[target_features], y_train)\n# \u7528 fit \u5b8c\u6210\u540e\u7684\u5bf9\u8c61\uff0c\u53bbtransfer x_test\ntarget_x_test = target.transform(x_test[target_features]).astype('float')\n\n#\u751f\u6210\u7684\u662f DF \u683c\u5f0f\n# type(target_x_test)","5ff6d6fb":"target.get_params()","d1accf97":"#\u751f\u6210\u7684\u662f DF\ntype(target_x_test)","6675aa1c":"#\u751f\u6210\u7684\u662f DF\ntype(target_x_train)","0065ee70":"target_x_test.head(1).T","aed11cfa":"import scipy\nx_train = scipy.sparse.hstack([ohe_x_train, simple_x_train]).tocsr()\nx_test = scipy.sparse.hstack([ohe_x_test, simple_x_test]).tocsr()","91a30078":"x_train.shape","c83b291c":"type(x_train)","14d8b35e":"type(ohe_x_train)","997eea2e":"# \u5bf9 logit \u505a\u7f51\u683c\u641c\u7d22\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogit_param_grid = {\n    'C': list(np.linspace(start = 0, stop = 0.1, num = 11))\n}\n\nlogit_grid = GridSearchCV(LogisticRegression(solver='lbfgs'), logit_param_grid,\n                          scoring='roc_auc', cv=5)\nlogit_grid.fit(x_train, y_train)\n\nbest_C = logit_grid.best_params_['C']\nbest_Score = logit_grid.best_score_\nprint('Best C:', best_C)\nprint('Best Score:', best_Score)","b05a61c9":"# \u7528 Best C predict x_test\nlogit = LogisticRegression(\n    C=best_C, \n    solver='lbfgs', \n    max_iter=10000)\nlogit.fit(x_train, y_train)\ny_pred_logit = logit.predict_proba(x_test)[:, 1]","8d74110b":"# \u770b\u5728 Train Set \u4e0a\u7684AUC\u7ed3\u679c\u5982\u4f55\ny_train_pred_logit = logit.predict_proba(x_train)[:, 1]\ntrain_auc_logit = roc_auc_score(y_train, y_train_pred_logit)\ntrain_auc_logit","310cc67a":"x_train = np.concatenate((oe_x_train, simple_x_train, target_x_train, cyclic_x_train), axis=1)\nx_test = np.concatenate((oe_x_test, simple_x_test, target_x_test, cyclic_x_test), axis=1)","e1f6e37b":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\nimport gc\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 12\n    count=1\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    # tss = TimeSeriesSplit(n_splits=FOLDS)\n#     y_preds = np.zeros(submission.shape[0]) #\u8fd9\u53e5\u770b\u8d77\u6765\u6ca1\u6709\u7528\n    # y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0 #\u521d\u59cb\u5316 mean\n    for tr_idx, val_idx in kf.split(x_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=1000, random_state=4, \n            verbose=True, \n            \n            tree_method='gpu_hist', # GPU\u52a0\u901f\n            **params #\u8fd9\u4e2a\u7528\u6cd5\u9700\u8981\u6ce8\u610f\n        )\n\n        X_tr, X_vl = x_train[tr_idx, :], x_train[val_idx, :] # \u9700\u8981\u6839\u636e x_train \u7684\u7c7b\u578b\u6765\u5224\u65ad\u662f\u5426\u7528 iloc\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr) #Training Set\u5185\u90e8\u7684K Fold # \u7528fit\u65b9\u6cd5\u6ca1\u6709\u65e9\u505c\n        \n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        \n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl) # \u81ea\u5b9a\u4e49\u4e00\u4e2ascore\uff0c\u522b\u7684\u9879\u76ee\u9700\u8981\u66ff\u6362\u6389\u8fd9\u4e2a\n        # plt.show()\n        score_mean += score # \u5148\u628a\u8fd9\u51e0\u6b21Fold\u7684mean\u52a0\u8d77\u6765\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1 # count \u5355\u7eaf\u662f\u4e3a\u4e86\u7528\u6765\u8ba1\u6570 \u8868\u793a\u662f\u7b2c\u51e0\u6b21cv\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect() #\u5185\u5b58\u56de\u6536\u673a\u5236\uff0c\u68c0\u67e5\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}') #\u5f97\u5230\u6700\u7ec8\u7684mean\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    \n    return -(score_mean \/ FOLDS) #\u8fd9\u91cc\u662f\u7531\u4e8e\u8981\u53d6\u635f\u5931\u51fd\u6570\u7684\u6700\u5c0f\u503c\uff0c\u56e0\u4e3ascore\u524d\u9762\u8981\u6709\u8d1f\u53f7\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.15),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 200, 5))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [.5, 0.6, 0.7, .8]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","ad5f329f":"best = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=50, \n            # trials=trials #trials \u662f\u4e3a\u4e86\u540e\u6765\u7ed8\u56fe\u6240\u7528\uff0c\u8be6\u7ec6\u8bfe\u4ef6 hyperopt \u6559\u7a0b\n           )\n# \u6574\u4e2a fmin \u4e4b\u540e\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u53c2\u6570\u7a7a\u95f4","97105400":"best_params = space_eval(space, best) #\u53ef\u80fd\u662f\u7531\u4e8e\u7d22\u5f15\u95ee\u9898\uff0c\u516c\u5f0f\u5f97\u5230\u7684best\uff0c\u9700\u8981\u91cd\u65b0\u52a0\u5de5\u4e00\u4e0b\u624d\u662f\u6700\u7ec8\u7684 best_params\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params","b2d416df":"clf = xgb.XGBClassifier(\n    n_estimators=500,\n    **best_params,\n    tree_method='gpu_hist'\n)\n\nclf.fit(x_train, y_train)\n\ny_pred_xgb = clf.predict_proba(x_test)[:,1] ","191ae654":"# \u770b\u5728 Train Set \u4e0a\u7684AUC\u7ed3\u679c\u5982\u4f55\ny_train_pred_xgb = clf.predict_proba(x_train)[:, 1]\ntrain_auc_xgb = roc_auc_score(y_train, y_train_pred_xgb)\ntrain_auc_xgb","efc98f99":"# Blending\ny_pred = np.add(y_pred_logit, y_pred_xgb) \/ 2","edfacdc7":"submission = pd.read_csv('..\/input\/cat-in-the-dat-ii\/sample_submission.csv', index_col='id')\nsubmission['target'] = y_pred\nsubmission.to_csv('logit_xgboost.csv')","7a10d141":"submission.head()","bfa713b7":"## XGB classifier","fdfca769":"## Load data","1b10e59a":"## Handle missing values","04379565":"## Logistic regression","793abf9f":"## Standard scaler for simple_features","08b3ccda":"## Target encoder","27162084":"Split 'ord_5' preserving missing values","9f5e8800":"#  (OHE + Logit)  + (Target, Cyclic Encoding+ Xgboost)\n\nIdeas:\n* Replace missing values with constant\n* Add number of missing values in row as a feature\n* Apply StandardScaler to created feature\n* Apply Target to features that have many unique values\n* Apply OHE for other features + Logit\n* Apply Target and Cyclic Encoding + Xgboost\n* Blend Logit and Xgboost","7fb62603":"Add number of missing values in row as a feature","8b4c04c4":"Replace missing values with constants","b13aaed8":"## Submit predictions","78953c83":"## Encoding cyclic features ","6f2cb96a":"## Feature engineering","49f282ee":"## OHE","aa3c8d92":"## Merge for XGB","c665d05f":"## Extract target variable","c367a7d5":"## Merge for Logit","e5acea9c":"\nIt's just some minor changes to the great kernel Cat in dat 2: Embeddings,Target + Keras\n<br>I just encode the date and month features with [cyclic encoding method](https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning) and change Keras to XGboost Model, and also add my learning note which is Chinese.\n<br>Please Upvote the [original kernel](https:\/\/www.kaggle.com\/pavelvpster\/cat-in-dat-2-embeddings-target-keras). \n<p><font color=\"blue\">Don't hesitate to give your suggestions in the comment section<\/font><\/p>\n<p><font color=\"blue\">Thank you...<\/font><\/p>\n\n","9ba714e2":"## Ordinal encoder"}}