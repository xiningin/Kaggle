{"cell_type":{"1396d9bd":"code","7e3b0ed2":"code","4342e249":"code","384618f0":"code","94f8868a":"code","748cff31":"code","968c4405":"code","20543093":"code","366b995a":"code","794c031f":"code","0ff647dd":"code","aed663f5":"code","f48cbe21":"code","44869b66":"code","565c28d4":"code","d7347151":"code","41f8febe":"code","e7638bef":"code","b9a2bf52":"code","3fcbd7aa":"code","bb94a0bf":"markdown","00603b6c":"markdown","b57ce9e4":"markdown","6c9f47b2":"markdown","b97ba64f":"markdown","09c72f7e":"markdown","9d073cfb":"markdown","bbef510e":"markdown","2433a9d3":"markdown","a2f3d1f3":"markdown"},"source":{"1396d9bd":"#Importing the dataset\nimport pandas as pd\nhousing = pd.read_csv('..\/input\/housing\/Housing.csv')\nhousing.head()","7e3b0ed2":"# Converting Yes to 1 and No to 0\nhousing['mainroad'] = housing['mainroad'].map({'yes': 1, 'no': 0})\nhousing['guestroom'] = housing['guestroom'].map({'yes': 1, 'no': 0})\nhousing['basement'] = housing['basement'].map({'yes': 1, 'no': 0})\nhousing['hotwaterheating'] = housing['hotwaterheating'].map({'yes': 1, 'no': 0})\nhousing['airconditioning'] = housing['airconditioning'].map({'yes': 1, 'no': 0})\nhousing['prefarea'] = housing['prefarea'].map({'yes': 1, 'no': 0})","4342e249":"#Converting furnishingstatus column to binary column using get_dummies\nstatus = pd.get_dummies(housing['furnishingstatus'],drop_first=True)\nhousing = pd.concat([housing,status],axis=1)\nhousing.drop(['furnishingstatus'],axis=1,inplace=True)","384618f0":"housing.head()","94f8868a":"# Normalisisng the data\nhousing = (housing - housing.mean())\/housing.std()\nhousing.head()","748cff31":"# Simple linear regression\n# Assign feature variable X\nX = housing['area']\n\n# Assign response variable to y\ny = housing['price']","968c4405":"# Conventional way to import seaborn\nimport seaborn as sns\n\n# To visualise in the notebook\n%matplotlib inline","20543093":"# Visualise the relationship between the features and the response using scatterplots\nsns.pairplot(housing, x_vars='area', y_vars='price',size=7, aspect=0.7, kind='scatter')","366b995a":"import numpy as np\nX = np.array(X)\ny = np.array(y)","794c031f":"# Implement gradient descent function\n# Takes in X, y, current m and c (both initialised to 0), num_iterations, learning rate\n# returns gradient at current m and c for each pair of m and c\n\ndef gradient(X, y, m_current=0, c_current=0, iters=1000, learning_rate=0.01):\n    N = float(len(y))\n    gd_df = pd.DataFrame( columns = ['m_current', 'c_current','cost'])\n    for i in range(iters):\n        y_current = (m_current * X) + c_current\n        cost = sum([data**2 for data in (y-y_current)]) \/ N\n        m_gradient = -(2\/N) * sum(X * (y - y_current))\n        c_gradient = -(2\/N) * sum(y - y_current)\n        m_current = m_current - (learning_rate * m_gradient)\n        c_current = c_current - (learning_rate * c_gradient)\n        gd_df.loc[i] = [m_current,c_current,cost]\n    return(gd_df)\n","0ff647dd":"# print gradients at multiple (m, c) pairs\n# notice that gradient decreased gradually towards 0\n# we have used 1000 iterations, can use more if needed\ngradients = gradient(X,y)\ngradients","aed663f5":"# plotting cost against num_iterations\ngradients.reset_index().plot.line(x='index', y=['cost'])","f48cbe21":"# Assigning feature variable X\nX = housing[['area','bedrooms']]\n\n# Assigning response variable y\ny = housing['price']","44869b66":"# Add a columns of 1s as an intercept to X.\n# The intercept column is needed for convenient matrix representation of cost function\n\nX['intercept'] = 1\nX = X.reindex_axis(['intercept','area','bedrooms'], axis=1)\nX.head()","565c28d4":"# Convert X and y to arrays\nimport numpy as np\nX = np.array(X)\ny = np.array(y)","d7347151":"# Theta is the vector representing coefficients (intercept, area, bedrooms)\ntheta = np.matrix(np.array([0,0,0])) \nalpha = 0.01\niterations = 1000","41f8febe":"# define cost function\n# takes in theta (current values of coefficients b0, b1, b2), X and y\n# returns total cost at current b0, b1, b2\n\ndef compute_cost(X, y, theta):\n    return np.sum(np.square(np.matmul(X, theta) - y)) \/ (2 * len(y))","e7638bef":"# gradient descent\n# takes in current X, y, learning rate alpha, num_iters\n# returns cost (notice it uses the cost function defined above)\n\ndef gradient_descent_multi(X, y, theta, alpha, iterations):\n    theta = np.zeros(X.shape[1])\n    m = len(X)\n    gdm_df = pd.DataFrame( columns = ['Bets','cost'])\n\n    for i in range(iterations):\n        gradient = (1\/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n        theta = theta - alpha * gradient\n        cost = compute_cost(X, y, theta)\n        gdm_df.loc[i] = [theta,cost]\n\n    return gdm_df","b9a2bf52":"# print costs with various values of coefficients b0, b1, b2\ngradient_descent_multi(X, y, theta, alpha, iterations)","3fcbd7aa":"# print cost\ngradient_descent_multi(X, y, theta, alpha, iterations).reset_index().plot.line(x='index', y=['cost'])","bb94a0bf":"In this Python notebook we will go through an example of implementing **Gradient Descent** in simple and multiple linear regression, for this we will be using housing dataset.","00603b6c":"![image.png](attachment:image.png)","b57ce9e4":"## Gradient Descent Implementation","6c9f47b2":"Now to apply gradient descent from scratch we need our X and y variables as numpy arrays, Let's convert them.","b97ba64f":"### Multiple Regression: Applying Gradient Descent for Multiple (>1) Features","09c72f7e":"Now we will apply partial derivative with respect to m and c and will equate it to zero to find the least value of m and c for which our cost function get the lowest value as possible.","9d073cfb":"![image.png](attachment:image.png)","bbef510e":"**For linear regression we use a cost function known as the mean squared error or MSE.**","2433a9d3":"![image.png](attachment:image.png)","a2f3d1f3":"More on [Numpy Matmul](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.matmul.html)"}}