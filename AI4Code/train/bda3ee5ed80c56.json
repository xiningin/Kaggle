{"cell_type":{"294ad6d6":"code","6de464f4":"code","81ff51ac":"code","2444ac30":"code","89b1f5b9":"code","7baa90dd":"code","37a8ebda":"code","0e10701f":"code","4a3afa44":"code","7f6cb500":"code","35f6899b":"code","8407ec3e":"code","a9720503":"code","0d00a306":"code","d7528012":"code","fa1108e9":"code","1190c828":"code","351bcc3d":"code","3a25b004":"code","42608b1d":"code","6eeb2727":"code","878eb0a7":"code","e52d83e9":"code","ab551857":"code","b44255e6":"code","5f199f85":"code","487c882f":"code","695d6522":"code","d9ce0aa6":"code","3dd9bd2e":"code","00395165":"code","90d743e8":"code","c1d24252":"code","84867411":"code","f12f2f5e":"code","bc845a75":"code","04e4e18c":"code","42fd6fdc":"code","d18d2dd2":"code","2ed1813d":"code","852fb075":"code","25eef298":"code","05e5e3ac":"code","47c29449":"code","759c6c38":"code","ff2e295f":"code","addecbba":"code","3f1212b2":"code","3bae0a35":"code","d6028d0e":"code","74308e4e":"code","85c927f3":"code","d1d16651":"code","f96d8cd0":"code","2860fd38":"code","42a81f4f":"code","b11d251e":"code","d664071c":"code","07d950f6":"code","530a4bd6":"code","f29d7b7a":"markdown","0592dc92":"markdown","3da5e0ee":"markdown","4e2afe59":"markdown","248e7ed7":"markdown","b951d6a0":"markdown","c1361c2e":"markdown","e46d3201":"markdown","665745cd":"markdown","a6884bb5":"markdown","10512f5f":"markdown","94970f19":"markdown","184e5db7":"markdown","e7de45f2":"markdown","0f3e13f8":"markdown","4d890fbe":"markdown","91f50318":"markdown","146c9f8a":"markdown","5fc19cfa":"markdown","90351628":"markdown","cbe3ae85":"markdown","028cb0b8":"markdown","47d4c73c":"markdown","03b3dbeb":"markdown","00f6dc8b":"markdown","ed7bda26":"markdown","9de15d55":"markdown","16b67963":"markdown","ee67d640":"markdown","972f793f":"markdown","c2406c43":"markdown","88420bcb":"markdown","d4df40a1":"markdown","a838514b":"markdown","504f1578":"markdown","ac7e47e1":"markdown","12075397":"markdown","1aef2c11":"markdown","adc96448":"markdown","52dfb488":"markdown","b56bb74c":"markdown","7ea4297c":"markdown","883828cf":"markdown","c498ed6f":"markdown","2c8280ed":"markdown","6f1d6dfd":"markdown","456e4f87":"markdown","b85b97a7":"markdown","d3778588":"markdown","6cd86471":"markdown","77e0f52f":"markdown","15920dfc":"markdown","5b1514ee":"markdown","b260d6d4":"markdown","72f0d4cd":"markdown","4b28dd2c":"markdown","010c55ec":"markdown","7e1b5d13":"markdown","b0b0ef92":"markdown","f38c8f66":"markdown","42df9a00":"markdown","093e2f85":"markdown","b872252a":"markdown"},"source":{"294ad6d6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTENC\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None  # default='warn'\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)","6de464f4":"df_eda = pd.read_csv('..\/input\/bank-marketing\/bank-additional-full.csv',sep=';') \n\ndf_eda.info()","81ff51ac":"df_eda.isnull().sum()","2444ac30":"len(df_eda)-len(df_eda.drop_duplicates())","89b1f5b9":"df_eda = df_eda.drop_duplicates()","7baa90dd":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"age\"].value_counts(bins=8).plot(kind='barh', color=\"C1\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"age\"].value_counts(bins=8).plot(kind='barh', ax = ax, color=\"C2\")\nax2 = (df_eda[df_eda[\"y\"]=='yes'][\"age\"].value_counts(bins=20) * 100 \/ df_eda[\"age\"].value_counts(bins=20) ).plot(kind='barh', color=\"C1\", ax=axs[1])\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Age Groups')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Age Groups')\naxs[1].title.set_text('Percentage of Customers (per Age Group) who Subscribed')","37a8ebda":"sns.boxplot(x=df_eda['age'])","0e10701f":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\nax = df_eda[\"job\"].value_counts().plot(kind='barh', color=\"C3\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"job\"].value_counts().plot(kind='barh', ax = ax, color=\"C4\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"job\"].value_counts() * 100 \/ df_eda[\"job\"].value_counts() ).plot(kind='barh', color=\"C3\", ax=axs[1])\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Job Categories')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Job Categories')\naxs[1].title.set_text('Percentage of Customers (per Job Categories) who Subscribed')","4a3afa44":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"marital\"].value_counts().plot(kind='barh', color=\"C5\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"marital\"].value_counts().plot(kind='barh', ax = ax, color=\"C6\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"marital\"].value_counts() * 100 \/ df_eda[\"marital\"].value_counts() ).plot(kind='barh', color=\"C5\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Marital Status')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Marital Status')\naxs[1].title.set_text('Percentage of Customers (per Marital Status) who Subscribed')","7f6cb500":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"education\"].value_counts().plot(kind='barh', color=\"C1\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"education\"].value_counts().plot(kind='barh', ax = ax, color=\"C2\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"education\"].value_counts() * 100 \/ df_eda[\"education\"].value_counts() ).plot(kind='barh', color=\"C1\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Education')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Education')\naxs[1].title.set_text('Percentage of Customers (per Education type) who Subscribed')","35f6899b":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"default\"].value_counts().plot(kind='barh', color=\"C3\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"default\"].value_counts().plot(kind='barh', ax = ax, color=\"C4\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"default\"].value_counts() * 100 \/ df_eda[\"default\"].value_counts() ).plot(kind='barh', color=\"C3\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Defaulter Category')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Defaulter Category')\naxs[1].title.set_text('Percentage of Customers (per Defaulter Category) who Subscribed')","8407ec3e":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"housing\"].value_counts().plot(kind='barh', color=\"C5\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"housing\"].value_counts().plot(kind='barh', ax = ax, color=\"C6\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"housing\"].value_counts() * 100 \/ df_eda[\"housing\"].value_counts() ).plot(kind='barh', color=\"C5\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Housing Loan')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Housing Loan')\naxs[1].title.set_text('Percentage of Customers (per Housing Loan Category) who Subscribed')","a9720503":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"loan\"].value_counts().plot(kind='barh', color=\"C1\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"loan\"].value_counts().plot(kind='barh', ax = ax, color=\"C2\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"loan\"].value_counts() * 100 \/ df_eda[\"loan\"].value_counts() ).plot(kind='barh', color=\"C1\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Personal Loan')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Personal Loan')\naxs[1].title.set_text('Percentage of Customers (per Personal Loan Category) who Subscribed')","0d00a306":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"contact\"].value_counts().plot(kind='barh', color=\"C3\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"contact\"].value_counts().plot(kind='barh', ax = ax, color=\"C4\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"contact\"].value_counts() * 100 \/ df_eda[\"contact\"].value_counts() ).plot(kind='barh', color=\"C3\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Contact Communication Type')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Contact Communication Type')\naxs[1].title.set_text('Percentage of Customers (per Contact Communication Type) who Subscribed')","d7528012":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"month\"].value_counts().plot(kind='barh', color=\"C5\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"month\"].value_counts().plot(kind='barh', ax = ax, color=\"C6\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"month\"].value_counts() * 100 \/ df_eda[\"month\"].value_counts() ).plot(kind='barh', color=\"C5\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Month')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Month')\naxs[1].title.set_text('Percentage of Customers (per Month) who Subscribed')","fa1108e9":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"day_of_week\"].value_counts().plot(kind='barh', color=\"C1\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"day_of_week\"].value_counts().plot(kind='barh', ax = ax, color=\"C2\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"day_of_week\"].value_counts() * 100 \/ df_eda[\"day_of_week\"].value_counts() ).plot(kind='barh', color=\"C1\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Day_of_week')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Day_of_week')\naxs[1].title.set_text('Percentage of Customers (per Day_of_week) who Subscribed')","1190c828":"ax = df_eda[\"duration\"].value_counts(bins=20).plot(kind='barh', color=\"C3\", figsize=(13,5))\ndf_eda[df_eda[\"y\"]=='yes'][\"duration\"].value_counts(bins=20).plot(kind='barh', ax = ax, color=\"C4\")","351bcc3d":"sns.boxplot(x=df_eda['duration'])","3a25b004":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"campaign\"].value_counts().plot(kind='barh', color=\"C5\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"campaign\"].value_counts().plot(kind='barh', ax = ax, color=\"C6\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"campaign\"].value_counts() * 100 \/ df_eda[\"campaign\"].value_counts() ).plot(kind='barh', color=\"C5\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Campaign')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Campaign')\naxs[1].title.set_text('Percentage of Customers (per Campaign) who Subscribed')","42608b1d":"sns.boxplot(x=df_eda['campaign'])","6eeb2727":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"previous\"].value_counts().plot(kind='barh', color=\"C1\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"previous\"].value_counts().plot(kind='barh', ax = ax, color=\"C2\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"previous\"].value_counts() * 100 \/ df_eda[\"previous\"].value_counts() ).plot(kind='barh', color=\"C1\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Previous')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Previous')\naxs[1].title.set_text('Percentage of Customers (per Previous Count) who Subscribed')","878eb0a7":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"poutcome\"].value_counts().plot(kind='barh', color=\"C3\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"poutcome\"].value_counts().plot(kind='barh', ax = ax, color=\"C4\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"poutcome\"].value_counts() * 100 \/ df_eda[\"poutcome\"].value_counts() ).plot(kind='barh', color=\"C3\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Poutcome Type')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Poutcome Type')\naxs[1].title.set_text('Percentage of Customers (per Poutcome Type) who Subscribed')","e52d83e9":"fig, axs = plt.subplots(1,2, figsize=(13,5))\nplt.tight_layout(pad=3)\n\nax = df_eda[\"pdays\"].value_counts().plot(kind='barh', color=\"C5\", ax=axs[0])\ndf_eda[df_eda[\"y\"]=='yes'][\"pdays\"].value_counts().plot(kind='barh', ax = ax, color=\"C6\")\nax = (df_eda[df_eda[\"y\"]=='yes'][\"pdays\"].value_counts() * 100 \/ df_eda[\"pdays\"].value_counts() ).plot(kind='barh', color=\"C5\", ax=axs[1])\n\naxs[0].set_xlabel('Count(Yes) & Total Counts')\naxs[0].set_ylabel('Pdays Count')\naxs[0].title.set_text('Count of Customers who said Yes & Total Counts ')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].set_xlabel('Percentage')\naxs[1].set_ylabel('Pdays Count')\naxs[1].title.set_text('Percentage of Customers (per Pdays Count) who Subscribed')","ab551857":"fig, axs = plt.subplots(1,3, figsize=(13,5))\nplt.tight_layout()\n\nfig.suptitle(\"Percentage of Customers who Subscribed\", fontsize=12)\nfig.subplots_adjust(top=0.88)\nax = (df_eda[df_eda[\"y\"]=='yes'][\"emp.var.rate\"].value_counts() * 100 \/ df_eda[\"emp.var.rate\"].value_counts() ).plot(kind='barh', color=\"C1\", ax=axs[0])\nax = (df_eda[df_eda[\"y\"]=='yes'][\"cons.price.idx\"].value_counts() * 100 \/ df_eda[\"cons.price.idx\"].value_counts() ).plot(kind='barh', color=\"C2\", ax=axs[1])\nax = (df_eda[df_eda[\"y\"]=='yes'][\"nr.employed\"].value_counts() * 100 \/ df_eda[\"nr.employed\"].value_counts() ).plot(kind='barh', color=\"C4\", ax=axs[2])\naxs[0].title.set_text('emp.var.rate')\naxs[1].title.set_text('cons.price.idx')\naxs[2].title.set_text('per nr.employed')\naxs[0].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[1].yaxis.set_major_formatter(FormatStrFormatter('%d'))\naxs[2].yaxis.set_major_formatter(FormatStrFormatter('%d'))\nfig.suptitle(\"Percentage of Customers who Subscribed\", fontsize=12)","b44255e6":"plt.scatter(x=df_eda['emp.var.rate'], y=df_eda['cons.price.idx'], alpha=0.5)","5f199f85":"plt.scatter(x=df_eda['euribor3m'], y=df_eda['nr.employed'], alpha=0.5)","487c882f":"df_enc = df_eda.copy()\ndf_cat = df_eda.copy()","695d6522":"df_eda.job = pd.Categorical(df_eda.job)\ndf_cat['job'] = df_eda.job.cat.codes\n\ndf_eda.marital = pd.Categorical(df_eda.marital)\ndf_cat['marital'] = df_eda.marital.cat.codes\n\ndf_eda.education = pd.Categorical(df_eda.education)\ndf_cat['education'] = df_eda.education.cat.codes\n\ndf_eda.default = pd.Categorical(df_eda.default)\ndf_cat['default'] = df_eda.default.cat.codes\n\ndf_eda.housing = pd.Categorical(df_eda.housing)\ndf_cat['housing'] = df_eda.housing.cat.codes\n\ndf_eda.loan = pd.Categorical(df_eda.loan)\ndf_cat['loan'] = df_eda.loan.cat.codes\n\ndf_eda.contact = pd.Categorical(df_eda.contact)\ndf_cat['contact'] = df_eda.contact.cat.codes\n\ndf_eda.month = pd.Categorical(df_eda.month)\ndf_cat['month'] = df_eda.month.cat.codes\n\ndf_eda.day_of_week = pd.Categorical(df_eda.day_of_week)\ndf_cat['day_of_week'] = df_eda.day_of_week.cat.codes\n\ndf_eda.poutcome = pd.Categorical(df_eda.poutcome)\ndf_cat['poutcome'] = df_eda.poutcome.cat.codes\n","d9ce0aa6":"encoded_jobs = pd.get_dummies(df_eda['job'])\ndf_enc = pd.concat([df_enc, encoded_jobs], axis=1)\ndf_enc.drop(['job'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_job'}, axis=1, inplace=True)\n\nencoded_marital = pd.get_dummies(df_eda['marital'])\ndf_enc = pd.concat([df_enc, encoded_marital], axis=1)\ndf_enc.drop(['marital'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_marital'}, axis=1, inplace=True)\n\nencoded_education = pd.get_dummies(df_eda['education'])\ndf_enc = pd.concat([df_enc, encoded_education], axis=1)\ndf_enc.drop(['education'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_edu'}, axis=1, inplace=True)\n\nencoded_default = pd.get_dummies(df_eda['default'])\ndf_enc = pd.concat([df_enc, encoded_default], axis=1)\ndf_enc.drop(['default'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_default'}, axis=1, inplace=True)\ndf_enc.rename({'yes': 'yes_default'}, axis=1, inplace=True)\ndf_enc.rename({'no': 'no_default'}, axis=1, inplace=True)\n\nencoded_housing = pd.get_dummies(df_eda['housing'])\ndf_enc = pd.concat([df_enc, encoded_housing], axis=1)\ndf_enc.drop(['housing'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_housing'}, axis=1, inplace=True)\ndf_enc.rename({'yes': 'yes_housing'}, axis=1, inplace=True)\ndf_enc.rename({'no': 'no_housing'}, axis=1, inplace=True)\n\nencoded_loan = pd.get_dummies(df_eda['loan'])\ndf_enc = pd.concat([df_enc, encoded_loan], axis=1)\ndf_enc.drop(['loan'], axis=1, errors='ignore',  inplace=True)\ndf_enc.rename({'unknown': 'unknown_loan'}, axis=1, inplace=True)\ndf_enc.rename({'yes': 'yes_loan'}, axis=1, inplace=True)\ndf_enc.rename({'no': 'no_loan'}, axis=1, inplace=True)\n\n\nencoded_contact = pd.get_dummies(df_eda['contact'])\ndf_enc = pd.concat([df_enc, encoded_contact], axis=1)\ndf_enc.drop(['contact'], axis=1, errors='ignore',  inplace=True)\n\nencoded_month = pd.get_dummies(df_eda['month'])\ndf_enc = pd.concat([df_enc, encoded_month], axis=1)\ndf_enc.drop(['month'], axis=1, errors='ignore',  inplace=True)\n\nencoded_day_of_week = pd.get_dummies(df_eda['day_of_week'])\ndf_enc = pd.concat([df_enc, encoded_day_of_week], axis=1)\ndf_enc.drop(['day_of_week'], axis=1, errors='ignore',  inplace=True)\n\nencoded_poutcome = pd.get_dummies(df_eda['poutcome'])\ndf_enc = pd.concat([df_enc, encoded_poutcome], axis=1)\ndf_enc.drop(['poutcome'], axis=1, errors='ignore',  inplace=True)\n\ndf_eda.y = pd.Categorical(df_eda.y)\ndf_cat['y'] = df_eda.y.cat.codes\ndf_enc['y'] = df_eda.y.cat.codes","3dd9bd2e":"corr = df_cat.corr()# plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)","00395165":"y_cat = df_cat['y'].copy()\ny_enc = df_enc['y'].copy()\n\ndf_cat.drop(['y'], axis=1, errors='ignore', inplace=True)\ndf_enc.drop(['y'], axis=1, errors='ignore', inplace=True)","90d743e8":"X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(df_cat, y_cat, stratify=y_cat, test_size=0.25)\nX_enc_train, X_enc_test, y_enc_train, y_enc_test = train_test_split(df_enc, y_enc, stratify=y_enc, test_size=0.25)","c1d24252":"X_enc_train_SMT = X_enc_train.copy()\ny_enc_train_SMT = y_enc_train.copy()\n\nsmote_nc = SMOTENC(categorical_features=[10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62], random_state=0)\nX_enc_train_SMT2, y_enc_train_SMT2 = smote_nc.fit_resample(X_enc_train_SMT, y_enc_train_SMT)\n\nX_enc_train = pd.DataFrame(X_enc_train_SMT2, columns=df_enc.columns)\ny_enc_train = y_enc_train_SMT2","84867411":"X_cat_train_SMT = X_cat_train.copy()\ny_cat_train_SMT = y_cat_train.copy()\n\nsmote_nc = SMOTENC(categorical_features=[2, 3, 4, 5, 6, 7, 8, 9], random_state=0)\nX_cat_train_SMT2, y_cat_train_SMT2 = smote_nc.fit_resample(X_cat_train_SMT, y_cat_train_SMT)\n\nX_cat_train = pd.DataFrame(X_cat_train_SMT2, columns=df_cat.columns)\ny_cat_train = y_cat_train_SMT2","f12f2f5e":"scaler = MinMaxScaler()\nX_cat_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler.fit_transform(X_cat_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\nX_cat_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler.fit_transform(X_cat_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\n\npd.options.mode.chained_assignment = None\nX_enc_std_train = X_enc_train.copy(deep=True)\nX_enc_std_test = X_enc_test.copy(deep=True)\n\nX_enc_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler.fit_transform(X_enc_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\nX_enc_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler.fit_transform(X_enc_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\n\nscaler2 = StandardScaler()\npd.options.mode.chained_assignment = None\nX_enc_std_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler2.fit_transform(X_enc_std_train[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\nX_enc_std_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']] = scaler2.fit_transform(X_enc_std_test[['age', 'duration','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])\n","bc845a75":"p_test3 = {'learning_rate':[1, 0.50,0.25], 'n_estimators':[70,100], 'max_depth':[3,8,12], \"min_samples_split\": np.linspace(0.1, 0.5, 2, 12),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 1, 12)}\n\n\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(subsample=1,max_features='sqrt', random_state=10), \n            param_grid = p_test3, scoring='f1',n_jobs=4,iid=False, cv=5)\ntuning.fit(X_cat_train, y_cat_train)\nprint(tuning.best_params_)\nprint(tuning.score(X_cat_train, y_cat_train))\nprint(tuning.best_score_)","04e4e18c":"gb_clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_features=5, max_depth=8, random_state=0) #0.548 0.72\n\ngb_clf2.fit(X_cat_train, y_cat_train)\npredictions = gb_clf2.predict(X_cat_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_cat_test, predictions))\nprint(\" \")\nprint(\"Classification Report\")\nprint(classification_report(y_cat_test, predictions))\nprint(\" \")\nprint(\"Kappa- \", cohen_kappa_score(y_cat_test, predictions))\nprint(\"F1- \", f1_score(y_cat_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_cat_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_cat_test, predictions))\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_cat_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_cat_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","42fd6fdc":"xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n                    silent=True, nthread=1)\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","d18d2dd2":"folds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='f1', n_jobs=4, cv=skf.split(X_cat_train, y_cat_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_cat_train, y_cat_train)\n","2ed1813d":"print(random_search.best_params_)\nprint(random_search.score(X_cat_train, y_cat_train))\nprint(random_search.best_score_)","852fb075":"xgb_clf = XGBClassifier(subsample=0.8, min_child_weight=1, max_depth=4, gamma=1, colsample_bytree=1.0) #0.53 0.70\n\nxgb_clf.fit(X_cat_train, y_cat_train)","25eef298":"predictions = xgb_clf.predict(X_cat_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_cat_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_cat_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_cat_test, predictions))\nprint(\"F1- \", f1_score(y_cat_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_cat_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_cat_test, predictions))\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_cat_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_cat_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","05e5e3ac":"score = xgb_clf.score(X_cat_test, y_cat_test)\nprint(score)","47c29449":"logreg=LogisticRegression()\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"], 'max_iter':[10000]}\n\nlogreg_cv=GridSearchCV(logreg,grid,scoring='f1', cv=10)\nlogreg_cv.fit(X_enc_std_train,y_enc_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)\n\nprint(logreg_cv.best_params_)\nprint(logreg_cv.score(X_enc_std_train,y_enc_train))\nprint(logreg_cv.best_score_)\n","759c6c38":"logreg2=LogisticRegression(C=100,penalty=\"l2\", max_iter=10000) #0.60 0.77 , solver='liblinear'\nlogreg2.fit(X_enc_train,y_enc_train)\nprint(\"score\",logreg2.score(X_enc_test,y_enc_test))","ff2e295f":"predictions = logreg2.predict(X_enc_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_enc_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_enc_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_enc_test, predictions))\nprint(\"F1- \", f1_score(y_enc_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_enc_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_enc_test, predictions))\n\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_enc_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_enc_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","addecbba":"lgbm=LGBMClassifier()\nlgbm.fit(X_cat_train,y_cat_train)\nprint(\"score\",lgbm.score(X_cat_test,y_cat_test))","3f1212b2":"predictions = lgbm.predict(X_cat_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_cat_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_cat_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_cat_test, predictions))\nprint(\"F1- \", f1_score(y_cat_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_cat_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_cat_test, predictions))\n\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_cat_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_cat_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","3bae0a35":"skf = StratifiedKFold(n_splits=10)\nparams = {}\n\nnb = GaussianNB() \ngs = GridSearchCV(nb, cv=skf, param_grid=params, scoring='f1', return_train_score=True)\ngs.fit(X_enc_train, y_enc_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",gs.best_score_)\n\nprint(gs.best_params_)\nprint(gs.score(X_enc_train,y_enc_train))\nprint(gs.best_score_)","d6028d0e":"gnb = GaussianNB() \n\ngnb.fit(X_enc_train, y_enc_train)\n\npredictions = gnb.predict(X_enc_test)","74308e4e":"print(\"Accuracy:\",metrics.accuracy_score(y_enc_test, predictions))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_enc_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_enc_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_enc_test, predictions))\nprint(\"F1- \", f1_score(y_enc_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_enc_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_enc_test, predictions))\n\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_enc_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_enc_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","85c927f3":"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1],\n                     'C': [1]},\n                    {'kernel': ['linear'], 'C': [1]}]\n\nclf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='precision')\nclf.fit(X_cat_train, y_cat_train)\n\nprint('The best model is: ', clf.best_params_)\nprint('Mean cross-validated score (precision) of', clf.best_score_)","d1d16651":"clf = svm.SVC(kernel='linear') \n\nclf.fit(X_cat_train, y_cat_train)\n\npredictions = clf.predict(X_cat_test)","f96d8cd0":"print(\"Accuracy:\",metrics.accuracy_score(y_cat_test, predictions))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_cat_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_cat_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_cat_test, predictions))\nprint(\"F1- \", f1_score(y_cat_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_cat_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_cat_test, predictions))\n\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_cat_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_cat_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","2860fd38":"neighbors = np.arange(1,19)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    knn.fit(X_cat_train, y_cat_train)\n    \n    train_accuracy[i] = knn.score(X_cat_train, y_cat_train)\n    \n    test_accuracy[i] = knn.score(X_cat_test, y_cat_test) ","42a81f4f":"plt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","b11d251e":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_cat_train, y_cat_train)\npredictions = knn.predict(X_cat_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_cat_test, predictions))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_cat_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_cat_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_cat_test, predictions))\nprint(\"F1- \", f1_score(y_cat_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_cat_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_cat_test, predictions))\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_cat_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_cat_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","d664071c":"def create_model():\n  model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(63,)),\n    tf.keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation='sigmoid')\n  ])\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  return model\n\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n\n# define the grid search parameters\nbatch_size = [100, 150]\nepochs = [10]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X_enc_train, y_enc_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","07d950f6":"\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(63,)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(), #from_logits=True\n              metrics=['AUC'])\n\nmodel.summary()\n\nhistory = model.fit(X_enc_train, y_enc_train, epochs=10)\n\ntest_loss, test_acc = model.evaluate(X_enc_test, y_enc_test, verbose=0)\npredictions = model.predict(X_enc_test)\n\npredictions[predictions>0.5] = 1\npredictions[predictions<=0.5] = 0","530a4bd6":"print(\"Accuracy:\",metrics.accuracy_score(y_enc_test, predictions))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_enc_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_enc_test, predictions))\n\nprint(\"Kappa- \", cohen_kappa_score(y_enc_test, predictions))\nprint(\"F1- \", f1_score(y_enc_test, predictions))\nprint(\"ROC_AUC_score- \", roc_auc_score(y_enc_test, predictions))\nprint(\"PR AUC- \", average_precision_score(y_enc_test, predictions))\n\nprint(\" \")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,5))\nfig.tight_layout()\n\nfpr, tpr, threshold = metrics.roc_curve(y_enc_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nax1.set_title('Receiver Operating Characteristic')\nax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax1.legend(loc = 'lower right')\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_xlim([0, 1])\nax1.set_ylim([0, 1])\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')\n\nlr_precision, lr_recall, _ = metrics.precision_recall_curve(y_enc_test, predictions)\nax2.set_title('Precision-Recall AUC Curve')\nax2.plot(lr_recall, lr_precision, marker='.')\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_xlim([0, 1])\nax2.set_ylim([0, 1])\n\nplt.show()","f29d7b7a":"Lets begin with some basic cleanup. \n\nCheck how many NULLs are present in the dataset","0592dc92":"accepts categorical data\n","3da5e0ee":"**Standardization**\n\nData needs to normaliszed to make it Machine Learn'able. \n\n*   We will use MinMaxScaler for all the models except Logistic Regression. All of the remaining models perform well with MinMaxScaler\n*   For Logistic Regression, StandardScaler is appropriate.\n\n","4e2afe59":"As expected, customers who have not defaulted have a higher probability of taking the Term Deposit. \n\nLets check further,\n\n*Housing* Loan is the next attribute.","248e7ed7":"Nothing here. \n\nLets evaluate *Contact* communication type","b951d6a0":"use one hot encoded dataframe","c1361c2e":"Lets begin! The Basics first!\n\n\nLoad all the necessary libraries.","e46d3201":"Next,\n\nLets explore the *Marital* status of customers.","665745cd":"Drop the duplicates in the dataset","a6884bb5":"## CONCLUSIONS\n\n\n---\n\n**Important Conclusions** - \n*   Interestingly, if a customer is contacted atleast a couple of times before a campaign begins, chances of the success of the campaign improve by 50% or more. In some cases upto 70%.\n*   Customers above the age of 73 have a 50% chance of saying YES.\n*   If the previous campaign was a success with a customer, there is a 60% chance the current campaign will succeed.\n\n---\n\nHere is the **consolidated list** of other conclusions derived from the analysis done earlier.\n\n*   Any customer above the age of 60 has atleast a 40% chance of subscribing for the Bank Term Deposit.\n*   Students and Retired customers have a good chance(> 25%) of creating a Fixed Deposit. \n*   Surprisingly, the Illiterate category has a higher chance(> 20%) of opting for the Term Deposit.\n*   Mar, Sep, Dec & Oct where great months for the campaign with a 40% or more probability of customers accepting the marketting offer. It generally seems like the last month of each quarter is a fruitful time for campaigning.\n*   Increasing the number of calls to a customer only reduces the chances of him accepting the product. \n\n---\n\n**Best Perfroming Models** - \n*   Light GBM has the top scores for all the metrics and outperforms XGBoost as expected.\n*   Its closely followed by XGBoost and Neural Networks. I didnt tune the Neural Network model for lack of more time. This has scope to be tuned and improved.\n\n---\n<h3><center>\nHope you liked the notebook. If you did, then give it a vote.\n<\/center><\/h3>\n\n\n---","10512f5f":"People with a Mobile phone have a higher probability of accepting the offer from the bank.","94970f19":"## EXPLORATORY DATA ANALYSIS\n\nData has been analyzed for - \n\n*   Correlations\n*   Univariate & Bivariate analysis\n*   Outliers","184e5db7":"*Conclusion:*\n\n*   If the previous campaign was a success with a customer, there is a 60% chance the current campaign will succeed.","e7de45f2":"## DATA PREPROCESSING","0f3e13f8":"*Conclusion:*\n\n*   Students and Retired customers have a good chance(> 25%) of creating a Fixed Deposit. \n\n","4d890fbe":"Split data into train and test.","91f50318":"There isn't much of a trend here. Lets move on. \n\nLets explore the *Education* of the customers.","146c9f8a":"**Round 2 :  XGBoost**\n","5fc19cfa":"Correlation can be further explored with df_cat.corr(). I have checked this and there is minimal feature correlation apart from the above two. \n\nLets preprocess the data now.","90351628":"## MODEL EVALUATION\n\n\n---\n\n\n\n\n\n\n**Hyperparameter Tuning:** To get the best hyperparameters, we will use GridsearchCV for all the models \n\n---\n**Improve Scores** - To improve the F1 Scores \/ ROC_AUC scores, SMOTE is used to increase the values of minority class for training. This improved the score for each of the models by around 2%\n\n---\n**ROC-AUC \/ PRAUC Curves** indicate which model performs better than the others.\n\n---\n**Models** - For classification, following models have been considered. \n\n*   Gradient Boosted Trees\n*   XGBosst\n*   Light GBM\n*   Logistic Regression\n*   Support Vector Machines\n*   Naive Bayes (Gaussian NB)\n*   KNN\n*   Neural Networks (Keras\/Tensorflow)\n---\n**Evaluation Metrics:** The dataset is imbalanced, there are more rows for Label y=NO than there are for y=YES, so accuracy will not be the appropriate pointer for a successful model. F1 score, ROC_AUC, Kappa & PR AUC are the appropriate metrics for a skewed dataset.\n\n---\n","cbe3ae85":"Setup One-hot Encoded dataframe","028cb0b8":"**Round 8 : Deep Learning**","47d4c73c":"*Conclusion:*\n\n*   Interestingly, if a customer is contacted atleast a couple of times before a campaign begins, chances of the success of the campaign improve by 50% or more. In some cases upto 70%.","03b3dbeb":"No trend noticed here. \n\nLets plot *Duration* too","00f6dc8b":"**Round 5 : Naive Bayes**","ed7bda26":"**Round 4 : Light GBM**","9de15d55":"*Conclusion:*\n\n*   Surprisingly, the Illiterate category has a higher chance(> 20%) of opting for the Term Deposit.","16b67963":"We will maintain 2 dataframes, \n*   one for categorical encoded data \n*   and another for one-hot-encoded data\n\n","ee67d640":"euribor3m & nr.employed too have a correlation","972f793f":"How do the features match up for correlation? ","c2406c43":"Next, lets check the numbers on whether the has customer *Defaulted* before or not.","88420bcb":"Load the data from csv and verify the structure and counts from the dataframe","d4df40a1":"Atleast a single contact improves the changes of the campaign drastically. Look for Pdays=999 ( which means customer was not contacted before)","a838514b":"*Conclusion:*\n\n*   Any customer above the age of 60 has atleast a 40% chance of subscribing for the Bank Term Deposit.\n*   Customers above the age of 73 have a 50% chance of saying YES.","504f1578":"**Round 7 : KNN**","ac7e47e1":"*Age*: Check how customers' behavior is per age group","12075397":"Pull up the best hyperparameters for GBTs","1aef2c11":"**Round 1 : Grandient Boosted Trees**\n","adc96448":"Setup Categorical dataframe","52dfb488":"## INTRODUCTION\n\n\n---\n\n\n**WHAT YOU WILL SEE IN THIS KERNEL :**\n\n*   **Neural Networks:** Keras & Tensorflow based model to predict the success of the marketting campaign. \n\n*   **Multi model evaluation:** Gradient Boosted Trees, XGBosst, Light GBM, Logistic Regression, Support Vector Machines, Naive Bayes (Gaussian NB), KNN, Neural Networks\n\n*   **SMOTE** to handle imbalanced data\n\n*   **Kappa** score to evaluate models for imbalanced data. Accuracy is ususally not a good measure for imbalanced datasets. Cohen\u2019s kappa statistic is a very good measure that can handle very well both multi-class and imbalanced class problems. It basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class.\n\n*   **ROC_AUC Curves and PR AUC Curves** for model evaluation\n\n*   **Additional Insights** in Exploratory Data Analysis\n\n---\n**DATASET:**\n\n\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n \n---\n**IMBALANCED DATA:** Since the data is imbalanced, appropriate steps have to be taken to analyze and predict skewed classes. \n\nSteps taken to handle imbalanced data - \n*   Accuracy is avoided as the model performance metric. Precision, Recall, F1 Score, Kappa , ROC-AUC & PR AUC have been considered to evaluate models.\n*   Generate Synthetic Samples using SMOTE for the minority class.\n*   Different algorithm were evaluated, and the one with best performance was selected.\n\n---","b56bb74c":"Next is *Day of the Week*","7ea4297c":"*Pdays*","883828cf":"Some campaigns went for too long.","c498ed6f":"Duration has outliers. Few people had long conversations!","2c8280ed":"Next,\n\nLets evaluate the *Job* categories for the different clients","6f1d6dfd":"use categorical dataframe","456e4f87":"**Round 3 : Logistic Regression**","b85b97a7":"| ML MODEL  | F1 SCORE  | ROC AUC  | KAPPA  | PR AUC  |\n|---|---|---|---|---|\n| Grandient Boosted Trees  | 0.57  | 0.78  | 0.51  | 0.37  |\n| XGBoost  | 0.61  | 0.83  | 0.54  | 0.41  |\n| Logistic Regression  | 0.59  | 0.76  | 0.54  | 0.40  |\n| Light GBM  | 0.62  | 0.85  | 0.56  | 0.43  |\n| Naive Bayes (GaussianNB)  | 0.32  | 0.66  | 0.19  | 0.17  |\n| Support Vector Machines  | 0.54  | 0.85  | 0.45  | 0.35  |\n| KNN  | 0.39  | 0.69  | 0.28  | 0.21  |\n| Neural Networks  | 0.58  | 0.77  | 0.52  | 0.38  |\n","d3778588":"**Round 6 : Support Vector Machines**","6cd86471":"*Conclusion:*\n\n*   Increasing the number of calls to a customer only reduces the chances of him accepting the product. ","77e0f52f":"uses one hot encoded data","15920dfc":"*Conclusion:*\n\n*   Mar, Sep, Dec & Oct where great months for the campaign with a 40% or more probability of customers accepting the marketting offer. It generally seems like the last month of each quarter is a fruitful time for campaigning.","5b1514ee":"Does prior contact with a customer improve conversions? \n\nLets explore *Previous* attribute\n","b260d6d4":"Time usually plays an important role with how decisions are made. Lets see how it plays out with this dataset.\n\nLets evaluate the *Month* when the last contact was made.","72f0d4cd":"Again, no trend noticed here. \n\nNext is Personal *Loan*","4b28dd2c":"**SMOTE**\n\nSince the data is imbalanced, lets balance it by generating some synthetic data for the minority class. This improves ML Models' scores and performance. \n\nThe F1 scores for all the models improved by about 2% by introducing SMOTE.","010c55ec":"Check for duplicates in the dataset","7e1b5d13":"emp.var.rate & cons.price.idx have a certain degree of correlation","b0b0ef92":"Age has some outliers","f38c8f66":"Campaign styles play an important role in converting a sale or not.","42df9a00":"Lets plot \n*   *emp.var.rate* \n*   *euribor3m*\n*   *cons.price.idx*\n*   *nr.employed*\n\n","093e2f85":"Does previous marketting campaigns affect the decision in the next campaign ?\n\nLets get an insight on *Poutcome*","b872252a":"## MODEL PERFORMANCE REPORT"}}