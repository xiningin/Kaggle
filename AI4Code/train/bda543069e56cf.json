{"cell_type":{"f7066e48":"code","0d0449c5":"code","8790669f":"code","038592de":"code","e12b680b":"code","13adb28f":"code","2494d0f2":"code","bcd045e1":"code","89129009":"code","b02c8d4f":"code","477272ae":"code","25fd9790":"code","dde20240":"code","b6170285":"code","29f1a4f5":"code","0dcb572a":"code","230c4320":"code","ba456864":"code","4f236270":"code","d55d30fa":"code","eb447a4e":"code","997a33c3":"code","a5d8de5d":"code","382f2c8d":"code","89257faa":"code","6a2c795f":"code","31e441e1":"code","b7ad7de3":"code","16c26461":"code","59f02cb6":"code","564161f8":"code","05f25aeb":"code","d8b39817":"code","5a9c9ccb":"code","427af4a4":"code","b8954586":"code","9cbc716b":"code","41715dcc":"code","2e0cc854":"code","433a824b":"code","11d6aba0":"code","ff501c6f":"code","dd17b58f":"code","3a186ae4":"code","bedd4fbc":"code","da472f66":"code","f71420e5":"code","52f3c624":"code","a73d3fb2":"code","278440af":"code","550ef4e1":"code","a089da14":"code","22184670":"code","72ec80d1":"code","1c89baa6":"code","ff3d6e98":"code","6b888e95":"code","7389eefb":"code","60e009fd":"code","0ceeb8a7":"code","f6632abd":"code","6a284eba":"code","257d35f7":"code","89216b16":"code","febffe84":"code","6bc974b9":"code","1fb263d7":"code","0a6f3d1a":"code","5d7aa810":"code","daf603d6":"code","dc72988e":"code","e1bc9b98":"code","14b126e9":"code","11f8ecd8":"code","edffc7bc":"code","48547871":"code","ac66c53e":"code","192c4da4":"code","32b42864":"code","01e20795":"code","ce829dab":"code","4be4ad6d":"code","8a6898c0":"code","156b55e8":"code","3de936d4":"code","ae7be63c":"code","461e29a9":"code","92c4959e":"code","5dcbcd53":"code","d98b389b":"code","18d019c5":"code","a2ef3936":"code","f1961e16":"code","87c4cef2":"code","603f7103":"code","b2862445":"code","bc65bb0c":"code","94564007":"code","ce20bdf0":"code","b5e8e431":"code","91308390":"code","1b69bbab":"code","edd577a6":"code","d05cf8dc":"code","2be5a72a":"code","59fffa09":"code","78ae2cf4":"code","6f83acb3":"code","8f21ff8d":"code","877968aa":"code","c3af6b07":"code","15044ca5":"code","295dd9ea":"code","a11094ca":"code","ceb103b3":"code","243a0326":"code","987f89ab":"code","98940153":"code","000dcbe5":"markdown","55a2cb3d":"markdown","3b9e842b":"markdown","42d7a2f6":"markdown","e1934e8a":"markdown","2f9398ad":"markdown","668bc170":"markdown","bf1689dc":"markdown","a77ce6f5":"markdown","4626975e":"markdown","77d25589":"markdown","05321ade":"markdown","34112655":"markdown","937dc6f2":"markdown","8bffe76b":"markdown","8168478a":"markdown","f00d829a":"markdown","ef375286":"markdown","3db23dc1":"markdown","9a6bff71":"markdown","43f29336":"markdown","456b6cd2":"markdown","dff33686":"markdown","83355888":"markdown","1609d889":"markdown","fcf92a7c":"markdown","6aca0ebd":"markdown","1b3d223a":"markdown","cf5176a0":"markdown","1a21fa31":"markdown","d9d77c90":"markdown","5b5388ef":"markdown","c10c9bbe":"markdown","c68fb2f2":"markdown","5b932510":"markdown","e3431cb6":"markdown","1d68612b":"markdown","9abf01ce":"markdown","901b2a7e":"markdown","adcb6f21":"markdown","9ad6cbcd":"markdown","fc7de931":"markdown","479e9420":"markdown","e8459414":"markdown","ed34f3f8":"markdown","89c50d55":"markdown","76cd0620":"markdown","ff6c2e27":"markdown","b0ad593f":"markdown","15876c8e":"markdown","ddbb6a5d":"markdown","091a8e57":"markdown","7c8f4a9e":"markdown","5ed037a1":"markdown"},"source":{"f7066e48":"import numpy as np\nimport pandas as pd\nimport warnings \nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import zscore\nimport statsmodels\nimport scipy.stats as stats\nimport statsmodels.stats.proportion as smpt\nfrom sklearn import model_selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix,accuracy_score, classification_report,f1_score,cohen_kappa_score","0d0449c5":"d1=pd.read_csv('..\/input\/bankmarketing\/Bank Marketing Data Set.csv')","8790669f":"d1.isnull().sum()","038592de":"num_col = ['int16','int32','int64','float16','float32','float64']\n\n#Filter out variables with numeric datatypes\ndf_numcols_only= d1.select_dtypes(include= num_col)","e12b680b":"cat_col=['object']\ndf_catcols_only=d1.select_dtypes(include=cat_col)","13adb28f":"for col in df_catcols_only:\n    plt.figure(figsize=(10,4))\n    sns.barplot(d1[col].value_counts().values, d1[col].value_counts().index)\n    plt.title(col)\n    plt.tight_layout()","2494d0f2":"significant_cat_variables = ['education','job']\nfor var in significant_cat_variables:\n    #df[var + '_un'] = 0\n    #df[var + '_un'][df[var]=='unknown'] = 1\n    #FIXME one-line coding\n    d1[var + '_un'] = (d1[var] == 'unknown').astype(int)","bcd045e1":"def cross_tab(df,f1,f2):\n    jobs=list(df[f1].unique())\n    edu=list(df[f2].unique())\n    dataframes=[]\n    for e in edu:\n        dfe=df[df[f2]==e]\n        dfejob=dfe.groupby(f1).count()[f2]\n        dataframes.append(dfejob)\n    xx=pd.concat(dataframes,axis=1)\n    xx.columns=edu\n    xx=xx.fillna(0)\n    return xx","89129009":"cross_tab(d1,'job','education')","b02c8d4f":"d1.loc[(d1['age']>60) & (d1['job']=='unknown'),'job']='retired'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='admin.'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='blue-collar'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='entrepreneur'), 'education'] = 'tertiary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='housemaid'), 'education'] =  'primary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='management'), 'education'] = 'tertiary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='retired'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='self-employed'), 'education'] = 'tertiary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='services'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='student'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='technician'), 'education'] = 'secondary'\nd1.loc[(d1['education']=='unknown') & (d1['job']=='unemployed'), 'education'] = 'secondary'","477272ae":"cross_tab(d1,'job','education')","25fd9790":"d1.loc[(d1['education']=='unknown') & (d1['job']=='unknown'), 'education'] = 'secondary'","dde20240":"d1.loc[(d1['education']=='secondary') & (d1['job']=='unknown'), 'job'] = 'blue-collar'\nd1.loc[(d1['education']=='tertiary') & (d1['job']=='unknown'), 'job'] = 'blue-collar'\nd1.loc[(d1['education']=='primary') & (d1['job']=='unknown'), 'job'] = 'management'","b6170285":"cross_tab(d1,'job','education')","29f1a4f5":"d1['pdays'].replace(to_replace=-1,value=0,inplace=True)","0dcb572a":"d1.rename(columns={'class':'deposit','campain':'campaign'},inplace=True)","230c4320":"d1['deposit'].replace(to_replace=[1,2],value=[0,1],inplace=True)","ba456864":"dist_age_balance = plt.figure(figsize = (10,6))\nra1 = dist_age_balance.add_subplot(1,2,1) \nra2 = dist_age_balance.add_subplot(1,2,2)\n\nra1.hist(d1['age'],color='orange')\nra1.set_title('The Distribution of Age')\n\nra2.hist(d1['balance'], color = 'orange')\nra2.set_title('The Distribution of Balance')\n\nplt.tight_layout() \nplt.show()","4f236270":"scatter_age_balance = d1.plot.scatter('age','balance',figsize = (7,5))\n\nplt.title('The Relationship between Age and Balance ')\nplt.show()","d55d30fa":"dur_cam = sns.lmplot(x='duration', y='campaign',data = d1,\n                     hue = 'deposit',\n                     fit_reg = False,\n                     scatter_kws={'alpha':0.6}, height =7)\n\nplt.axis([0,65,0,65])\nplt.ylabel('Number of Calls')\nplt.xlabel('Duration of Calls (Minutes)')\nplt.title('The Relationship between the Number and Duration of Calls (with Response Result)')\n\n# Annotation\nplt.axhline(y=5, linewidth=2, color=\"k\", linestyle='--')\nplt.annotate('Higher subscription rate when calls <5',xytext = (35,13),\n             arrowprops=dict(color = 'k', width=1),xy=(30,6))\n\nplt.show()","eb447a4e":"d1.drop(['education_un','job_un'],axis=1,inplace=True) # removing created feature required in eda part","997a33c3":"corr_data = d1\ncorr = corr_data.corr()\ncorr","a5d8de5d":"corr_data = d1\ncorr = corr_data.corr()\n\ncor_plot = sns.heatmap(corr,annot=True,cmap='RdYlGn',linewidths=0.1,annot_kws={'size':10})\nfig=plt.gcf()\nfig.set_size_inches(5,5)\nplt.xticks(fontsize=10,rotation=-30)\nplt.yticks(fontsize=10)\nplt.title('Correlation Matrix')\nplt.show()","382f2c8d":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"age\"] < 30,  'age_group'] = 20\n    column.loc[(column[\"age\"] >= 30) & (column[\"age\"] <= 39), 'age_group'] = 30\n    column.loc[(column[\"age\"] >= 40) & (column[\"age\"] <= 49), 'age_group'] = 40\n    column.loc[(column[\"age\"] >= 50) & (column[\"age\"] <= 59), 'age_group'] = 50\n    column.loc[column[\"age\"] >= 60, 'age_group'] = 60\n","89257faa":"count_age_response_pct = pd.crosstab(d1['deposit'],d1['age_group']).apply(lambda x: x\/x.sum() * 100)\ncount_age_response_pct = count_age_response_pct.transpose() ","6a2c795f":"age = pd.DataFrame(d1['age_group'].value_counts())\nage['% Contacted'] = age['age_group']*100\/age['age_group'].sum()\nage['% Subscription'] = count_age_response_pct[1]\nage.drop('age_group',axis = 1,inplace = True)\n\nage['age'] = [30,40,50,20,60]\nage = age.sort_values('age',ascending = True)","31e441e1":"plot_age = age[['% Subscription','% Contacted']].plot(kind = 'bar',\n                                              figsize=(8,6), color = ('green','red'))\nplt.xlabel('Age Group')\nplt.ylabel('Subscription Rate')\nplt.xticks(np.arange(5), ('<30', '30-39', '40-49', '50-59', '60+'),rotation = 'horizontal')\nplt.title('Subscription vs. Contact Rate by Age')\nplt.show()","b7ad7de3":"lst = [d1]\nfor column in lst:\n    column.loc[column[\"balance\"] <= 0,  'balance_group'] = 'no balance'\n    column.loc[(column[\"balance\"] > 0) & (column[\"balance\"] <= 1000), 'balance_group'] = 'low balance'\n    column.loc[(column[\"balance\"] > 1000) & (column[\"balance\"] <= 5000), 'balance_group'] = 'average balance'\n    column.loc[(column[\"balance\"] > 5000), 'balance_group'] = 'high balance'","16c26461":"count_balance_response_pct = pd.crosstab(d1['deposit'],d1['balance_group']).apply(lambda x: x\/x.sum() * 100)\ncount_balance_response_pct = count_balance_response_pct.transpose()","59f02cb6":"bal = pd.DataFrame(d1['balance_group'].value_counts())\nbal['% Contacted'] = bal['balance_group']*100\/bal['balance_group'].sum()\nbal['% Subscription'] = count_balance_response_pct[1]\nbal.drop('balance_group',axis = 1,inplace = True)\n\nbal['bal'] = [1,2,0,3]\nbal = bal.sort_values('bal',ascending = True)","564161f8":"plot_balance = bal[['% Subscription','% Contacted']].plot(kind = 'bar',\n                                               color = ('royalblue','skyblue'),\n                                               figsize = (8,6))\n\nplt.title('Subscription vs Contact Rate by Balance Level')\nplt.ylabel('Subscription Rate')\nplt.xlabel('Balance Category')\nplt.xticks(rotation = 'horizontal')\n\n# label the bar\nfor rec, label in zip(plot_balance.patches,\n                      bal['% Subscription'].round(1).astype(str)):\n    plot_balance.text(rec.get_x() + rec.get_width()\/2, \n                      rec.get_height() + 1, \n                      label+'%',  \n                      ha = 'center', \n                      color = 'black')","05f25aeb":"d1['response']=d1['deposit']\nd1['response'].replace(to_replace=[0,1],value=['no','yes'],inplace=True)","d8b39817":"age_balance1 = pd.DataFrame(d1.groupby(['age_group','balance_group'])['deposit'].sum())\nage_balance2 = pd.DataFrame(d1.groupby(['age_group','balance_group'])['response'].count())\n\nage_balance1['response'] = age_balance2['response']\nage_balance1['response_rate'] = age_balance1['deposit']\/ (age_balance1['response'])\nage_balance1 = age_balance1.drop(['deposit','response'],axis =1)\n\nage_balance1 = age_balance1.unstack()","5a9c9ccb":"age_bal = age_balance1.plot(kind='bar',figsize = (10,6))\n\n# Set x ticks\nplt.xticks(np.arange(5),('<30', '30-39', '40-49', '50-59', '60+'),rotation = 'horizontal')\n\n# Set legend\nplt.legend(['Average Balance','High Balance','Low Balance','No Balance'],loc = 'best',ncol = 1)\n\nplt.ylabel('Subscription Rate')\nplt.xlabel('Age Group')\nplt.title('The Subscription Rate of Different Balance Levels in Each Age Group')\nplt.show()","427af4a4":"count_job_response_pct = pd.crosstab(d1['response'],d1['job']).apply(lambda x: x\/x.sum() * 100)\ncount_job_response_pct = count_job_response_pct.transpose()","b8954586":"plot_job = count_job_response_pct['yes'].sort_values(ascending = True).plot(kind ='barh',\n                                                                           figsize = (12,6))\n                                                                               \nplt.title('Subscription Rate by Job')\nplt.xlabel('Subscription Rate')\nplt.ylabel('Job Category')\n\n# Label each bar\nfor rec, label in zip(plot_job.patches,\n                      count_job_response_pct['yes'].sort_values(ascending = True).round(1).astype(str)):\n    plot_job.text(rec.get_width()+0.8, \n                  rec.get_y()+ rec.get_height()-0.5, \n                  label+'%', \n                  ha = 'center', \n                  va='bottom')","9cbc716b":"#Change 'month' from words to numbers for easier analysis\nlst = [d1]\nfor column in lst:\n    column.loc[column[\"month\"] == \"jan\", \"month_int\"] = 1\n    column.loc[column[\"month\"] == \"feb\", \"month_int\"] = 2\n    column.loc[column[\"month\"] == \"mar\", \"month_int\"] = 3\n    column.loc[column[\"month\"] == \"apr\", \"month_int\"] = 4\n    column.loc[column[\"month\"] == \"may\", \"month_int\"] = 5\n    column.loc[column[\"month\"] == \"jun\", \"month_int\"] = 6\n    column.loc[column[\"month\"] == \"jul\", \"month_int\"] = 7\n    column.loc[column[\"month\"] == \"aug\", \"month_int\"] = 8\n    column.loc[column[\"month\"] == \"sep\", \"month_int\"] = 9\n    column.loc[column[\"month\"] == \"oct\", \"month_int\"] = 10\n    column.loc[column[\"month\"] == \"nov\", \"month_int\"] = 11\n    column.loc[column[\"month\"] == \"dec\", \"month_int\"] = 12","41715dcc":"count_month_response_pct = pd.crosstab(d1['response'],d1['month_int']).apply(lambda x: x\/x.sum() * 100)\ncount_month_response_pct = count_month_response_pct.transpose()","2e0cc854":"month = pd.DataFrame(d1['month_int'].value_counts())\nmonth['% Contacted'] = month['month_int']*100\/month['month_int'].sum()\nmonth['% Subscription'] = count_month_response_pct['yes']\nmonth.drop('month_int',axis = 1,inplace = True)\n\nmonth['Month'] = [5,7,8,6,11,4,2,1,10,9,3,12]\nmonth = month.sort_values('Month',ascending = True)","433a824b":"plot_month = month[['% Subscription','% Contacted']].plot(kind ='line',\n                                                          figsize = (10,6),\n                                                          marker = 'o')\n\nplt.title('Subscription vs. Contact Rate by Month')\nplt.ylabel('Subscription and Contact Rate')\nplt.xlabel('Month')\n\nticks = np.arange(1,13,1)\nplt.xticks(ticks)\n\n# Annotation: peak of contact\ny = month['% Contacted'].max()\nx = month['% Contacted'].idxmax()\nplt.annotate('May: Peak of contact', xy=(x+0.1, y+0.1), xytext=(x+1,y+4), arrowprops=dict(facecolor='black', headwidth=6, width=1, headlength=4), horizontalalignment='left', verticalalignment='top')\n\n# Annotation: peak of subscription rate\ny = month['% Subscription'].max()\nx = month['% Subscription'].idxmax()\nplt.annotate('March: Peak Subscription rate', xy=(x+0.1, y+0.1), xytext=(x+1,y+1), arrowprops=dict(facecolor='black', headwidth=6, width=1, headlength=4), horizontalalignment='left', verticalalignment='top')\n\nplt.show()","11d6aba0":"d2=d1.drop(['age_group','balance_group','response','month_int'],axis=1,inplace=True)","ff501c6f":"d2=d1\nnum_col = ['int16','int32','int64','float16','float32','float64']\n\n#Filter out variables with numeric datatypes\ndf_numcols_only= d2.select_dtypes(include= num_col)\ndf_numcols_only.drop(columns=['deposit'],axis=1,inplace=True)","dd17b58f":"columns=['age', 'balance', 'day','duration','campaign']\nfig,ax = plt.subplots(2,3,figsize=(16,20))\nax = ax.flatten()\nfor i,col in enumerate(columns):\n    sns.distplot(d1[col],ax=ax[i],color='red')\nplt.tight_layout()\nplt.show()","3a186ae4":"d2[['age','balance','duration','campaign','pdays','previous','day']].plot(kind= 'box' ,layout=(4,3),subplots=True, sharex=False, sharey=False, figsize=(20,25),color='blue')\nplt.show()","bedd4fbc":"num_col = ['int16','int32','int64','float16','float32','float64']\n\n#Filter out variables with numeric datatypes\ndf_numcols_only1= d2.select_dtypes(include= num_col)","da472f66":"df_numcols_only1.columns","f71420e5":"d2['age']=zscore(d2['age'])\nd2['balance']=zscore(d2['balance'])\nd2['duration']=zscore(d2['duration'])\nd2['campaign']=zscore(d2['campaign'])\nd2['pdays']=zscore(d2['pdays'])\nd2['previous']=zscore(d2['previous'])\nd2['day']=zscore(d2['day'])","52f3c624":"from sklearn.impute import KNNImputer","a73d3fb2":"d2.loc[d2.age > 3, 'age'] = np.nan\nnumeric1=d2[['age']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['age'] = imputer.fit_transform(numeric1)\n\n\nd2.loc[d2.age > 3, 'duration'] = np.nan\nnumeric2=d2[['duration']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['duration'] = imputer.fit_transform(numeric2)\n\n\nd2.loc[d2.age > 3, 'campaign'] = np.nan\nnumeric3=d2[['campaign']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['campaign'] = imputer.fit_transform(numeric3)\n\n\nd2.loc[d2.age > 3, 'pdays'] = np.nan\nnumeric4=d2[['pdays']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['pdays'] = imputer.fit_transform(numeric4)\n\n\nd2.loc[d2.age > 3, 'previous'] = np.nan\nnumeric5=d2[['previous']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['previous'] = imputer.fit_transform(numeric5)\n\n\nd2.loc[d2.age > 3, 'day'] = np.nan\nnumeric6=d2[['day']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['day'] = imputer.fit_transform(numeric6)\n\nd2.loc[d2.age > 3, 'balance'] = np.nan\nnumeric7=d2[['balance']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['balance'] = imputer.fit_transform(numeric7)\n\n\nd2.loc[d2.age < -3, 'balance'] = np.nan\nnumeric8=d2[['balance']]\nimputer = KNNImputer(missing_values=np.nan)\nd2['balance'] = imputer.fit_transform(numeric8)","278440af":"d2['age']=np.cbrt(d2['age'])\nd2['balance']=np.cbrt(d2['balance'])\nd2['duration']=np.cbrt(d2['duration'])\nd2['campaign']=np.cbrt(d2['campaign'])\nd2['pdays']=np.cbrt(d2['pdays'])\nd2['previous']=np.cbrt(d2['previous'])\nd2['day']=np.cbrt(d2['day'])","550ef4e1":"num_col = ['int16','int32','int64','float16','float32','float64']\n\n#Filter out variables with numeric datatypes\ndf_numcols_only1= d2.select_dtypes(include= num_col)\ndf_numcols_only1=df_numcols_only1.columns\ndf_numcols_only1","a089da14":"columns=['age', 'balance', 'day','duration','campaign']\nfig,ax = plt.subplots(2,3,figsize=(16,20))\nax = ax.flatten()\nfor i,col in enumerate(columns):\n    sns.distplot(d2[col],ax=ax[i])\nplt.tight_layout()\nplt.show()","22184670":"d2[['age','balance','duration','campaign','pdays','previous','day']].plot(kind= 'box' ,layout=(4,3),subplots=True, sharex=False, sharey=False, figsize=(20,25),color='red')\nplt.show()","72ec80d1":"cat_col=['object']\ndf_catcols_only=d2.select_dtypes(include=cat_col)","1c89baa6":"df_numcols_only.columns","ff3d6e98":"d2=pd.get_dummies(data=d2,columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n       'month', 'poutcome'],drop_first=True)","6b888e95":"d3=d2","7389eefb":"cat_col=['object']\ndf_catcols_only1=d1.select_dtypes(include=cat_col)","60e009fd":"def chisquare_test(target_variable, categorical_column):\n    print('H0: There is NO association between ' + target_variable.name + ' and ' + categorical_column.name)\n    print('Ha: There is an association between ' + target_variable.name + ' and ' + categorical_column.name)\n    print()\n    ct = pd.crosstab(target_variable, categorical_column)\n    print(ct)\n    chi2_ct = stats.chi2_contingency(ct)\n    chival = chi2_ct[0]\n    pval = chi2_ct[1]\n    print()\n    print('chi-val =', chival)\n    print('p-val =', pval)\n    print()\n    rejectH0 = pval < 0.05\n    if rejectH0:\n        print('Reject H0')\n    else:\n        print('Failed to reject H0')\n    print()\n    print('---------------------------------------------------------------')\n    return rejectH0","0ceeb8a7":"cat_cols = ['job', 'marital','education','contact','month','poutcome','default','housing','loan']\nreject = []\nnot_reject = []\n\nfor col in cat_cols:\n    chisquare_test(d1['deposit'], d1[col])\n    #if rejectH0:\n        #reject.append(col)\n    #else:\n        #not_reject.append(col)\n#print()   \n#print('Columns Fails to Reject H0 :', reject)\n#print()\n#print('Columns Rejected H0 :', not_reject)","f6632abd":"def two_sample_ttest(target_variable, numerical_column):\n    print('H0: The mean of ' + numerical_column.name + ' is equal for both categories of ' + target_variable.name)\n    print('Ha: The mean of ' + numerical_column.name + ' is NOT equal for both categories of ' + target_variable.name)\n    print()\n    grp0 = numerical_column[target_variable == 0]\n    grp1 = numerical_column[target_variable == 1]\n    ttest = stats.ttest_ind(grp0, grp1)\n    print(ttest)\n    rejectH0 = ttest[1] < 0.05\n    print()\n    #return rejectH0\n    if rejectH0:\n        print('Reject H0')\n        print('\\n')\n        print('-------------------------------------------------------------------------')\n    else:\n        print('Failed to Reject H0')         \n        print()\n        print('-------------------------------------------------------------------------')","6a284eba":"num_cols = ['age','balance','day', 'duration', 'campaign', 'pdays', 'previous']\n\nreject = []\nnot_reject = []\nfor col in num_cols:\n    rejectH0 = two_sample_ttest(d1['deposit'], d1[col])\n    \"\"\"\n    if rejectH0:\n        reject.append(col)\n    else:\n        not_reject.append(col)\n    \nprint('Columns Fails to reject H0 :', reject)\n\nprint()\nprint('Columns Reject H0 :', not_reject)\"\"\"","257d35f7":"X1=d3.drop(columns=['deposit'],axis=1)\ny1=d3['deposit']","89216b16":"from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X1, y1, test_size=0.3, random_state=1)\n\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","febffe84":"import statsmodels.api as sm","6bc974b9":"logit_model=sm.Logit(ytrain,Xtrain)\nresult=logit_model.fit()","1fb263d7":"result.summary()","0a6f3d1a":"X1.drop(columns=['day','previous','job_retired','marital_single','education_secondary','default_yes','contact_telephone','poutcome_other'],axis=1,inplace=True)","5d7aa810":"from statsmodels.stats.outliers_influence import variance_inflation_factor","daf603d6":"vif= pd.DataFrame()\nvif['Features'] = X1.columns\nvif['vif']=[variance_inflation_factor(X1.values,i) for i in range(X1.shape[1])]\nvif","dc72988e":"vif[vif['vif']>4]","e1bc9b98":"X1.drop(['poutcome_unknown','month_may'],axis=1,inplace=True)","14b126e9":"vif= pd.DataFrame()\nvif['Features'] = X1.columns\nvif['vif']=[variance_inflation_factor(X1.values,i) for i in range(X1.shape[1])]\nvif","11f8ecd8":"from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X1, y1, test_size=0.3, random_state=1)\n\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","edffc7bc":"from sklearn.linear_model import LogisticRegression\nmodel1 = LogisticRegression()\nmodel1.fit(Xtrain, ytrain)","48547871":"print('Training score =', model1.score(Xtrain, ytrain))\nprint('Test score =', model1.score(Xtest, ytest))","ac66c53e":"from sklearn.metrics import confusion_matrix\nypred = model1.predict(Xtest)\ncm = confusion_matrix(ytest, ypred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","192c4da4":"tn = cm[0,0]  #True Negative\ntp = cm[1,1]  #True Positives\nfp = cm[0,1]  #False Positives\nfn = cm[1,0]  #False Negatives\n\naccuracy = (tp+tn)\/(tp+fn+fp+tn)\nprecision = tp \/ (tp+fp)\nrecall = tp \/ (tp+fn)\nf1 = 2*precision*recall \/ (precision+recall)\n\nprint('Accuracy =',accuracy)\nprint('Precision =', precision)\nprint('Recall =', recall)\nprint('F1 Score =', f1)","32b42864":"from sklearn.metrics import roc_curve,roc_auc_score\nypred = model1.predict_proba(Xtest)\nfpr,tpr,threshold = roc_curve(ytest,ypred[:,1])\nroc_auc = roc_auc_score(ytest,ypred[:,1])\n\nprint('ROC AUC =', roc_auc)\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n#plt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","01e20795":"sns.countplot(ytrain)\nplt.xticks(np.arange(2),('Not Made fixed deposit','Made fixed deposit'),rotation=45)\nplt.show()\nprint(ytrain.value_counts())","ce829dab":"didnt = len(d3[d3['deposit'] == 0])\ndid = len(d3[d3['deposit'] == 1])\ntotal = didnt + did\n\nper_didnt = (didnt \/ total) * 100\nper_did = (did \/ total) * 100\n\nprint(per_didnt)\nprint(per_did)","4be4ad6d":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=2)\n\nX_resampled, y_resampled = sm.fit_sample(X1, y1)\n\nX_resampled = pd.DataFrame(X_resampled, columns=X1.columns)\n\nprint(X1.shape)\nprint(X_resampled.shape)\n\nsns.countplot(y_resampled)\nplt.xticks(np.arange(2),('Not Made fixed deposit','Made fixed deposit'),rotation=45)\nplt.show()","8a6898c0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","156b55e8":"from sklearn.linear_model import LogisticRegression\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)","3de936d4":"print('Training score =', model1.score(X_train, y_train))\nprint('Test score =', model1.score(X_test, y_test))","ae7be63c":"from sklearn.metrics import confusion_matrix\nypred = model1.predict(X_test)\ncm = confusion_matrix(y_test, ypred)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","461e29a9":"tn = cm[0,0]  #True Negative\ntp = cm[1,1]  #True Positives\nfp = cm[0,1]  #False Positives\nfn = cm[1,0]  #False Negatives\n\naccuracy = (tp+tn)\/(tp+fn+fp+tn)\nprecision = tp \/ (tp+fp)\nrecall = tp \/ (tp+fn)\nf1 = 2*precision*recall \/ (precision+recall)\n\nprint('Accuracy =',accuracy)\nprint('Precision =', precision)\nprint('Recall =', recall)\nprint('F1 Score =', f1)","92c4959e":"from sklearn.metrics import roc_curve,roc_auc_score\nypred = model1.predict_proba(Xtest)\nfpr,tpr,threshold = roc_curve(ytest,ypred[:,1])\nroc_auc = roc_auc_score(ytest,ypred[:,1])\n\nprint('ROC AUC =', roc_auc)\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n#plt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","5dcbcd53":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier","d98b389b":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nknn=KNeighborsClassifier()\nparam={'n_neighbors':np.arange(5,30),'weights':['uniform','distance']}\nGS=RandomizedSearchCV(knn,param,cv=3,scoring='f1_weighted',n_jobs=-1)\nGS.fit(X_train,y_train)","18d019c5":"GS.best_params_","a2ef3936":"dt=DecisionTreeClassifier(random_state=0)","f1961e16":"param={'max_depth':np.arange(3,50),'criterion':['entropy','gini'],'min_samples_leaf':np.arange(3,20)}\nGS=RandomizedSearchCV(dt,param,cv=3,scoring='f1_weighted')\nGS.fit(X_train,y_train)","87c4cef2":"GS.best_params_","603f7103":"LR=LogisticRegression()\nNB=GaussianNB()\nKNN=KNeighborsClassifier(n_neighbors=5,weights='distance')\nDT=DecisionTreeClassifier(criterion='entropy',max_depth=20,min_samples_leaf=18,random_state=0)\nRF=RandomForestClassifier(criterion='entropy',n_estimators=13,random_state=0)\nBag=BaggingClassifier(n_estimators=8,random_state=0)\nAB=AdaBoostClassifier(n_estimators=90,random_state=0)\n#ABL=AdaBoostClassifier(base_estimator=LR,n_estimators=50,random_state=0)\nGB=GradientBoostingClassifier(n_estimators=98)\n#svm=SVC(C=10,gamma=0.001,kernel='rbf')\nstacked = StackingClassifier(classifiers=[Bag,RF,AB], meta_classifier=KNN)","b2862445":"RF_var=[]\nfor val in np.arange(1,50):\n  RF=RandomForestClassifier(criterion='gini',n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(RF, X_train,y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  RF_var.append(np.var(cv_results,ddof=1))\n  ","bc65bb0c":"x_axis=np.arange(1,50)\nplt.plot(x_axis,RF_var)","94564007":"np.argmin(RF_var)","ce20bdf0":"Bag_var=[]\nfor val in np.arange(1,20):\n  Bag=BaggingClassifier(n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(Bag, X_train,y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  Bag_var.append(np.var(cv_results,ddof=1))\n  #print(val,np.var(cv_results,ddof=1))","b5e8e431":"x_axis=np.arange(1,20)\nplt.plot(x_axis,Bag_var)","91308390":"np.argmin(Bag_var)","1b69bbab":"Ada_bias=[]\nfor val in np.arange(1,100):\n  Ada=AdaBoostClassifier(n_estimators=val,random_state=0)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(Ada, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  Ada_bias.append(1-np.mean(cv_results))\n  #print(val,1-np.mean(cv_results))\n","edd577a6":"x_axis=np.arange(1,100)\nplt.plot(x_axis,Ada_bias)","d05cf8dc":"np.argmin(Ada_bias)","2be5a72a":"GB_bias=[]\nfor val in np.arange(1,100):\n  gb=GradientBoostingClassifier(n_estimators=val)\n  kfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n  cv_results = model_selection.cross_val_score(gb, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n  GB_bias.append(1-np.mean(cv_results))\n  #print(val,1-np.mean(cv_results))\n","59fffa09":"x_axis=np.arange(1,100)\nplt.plot(x_axis,GB_bias)","78ae2cf4":"np.argmin(GB_bias)","6f83acb3":"models = []\nmodels.append(('Logistic', LR))\nmodels.append(('NaiveBayes', NB))\nmodels.append(('KNN',KNN))\nmodels.append(('DecisionTree',DT))\nmodels.append(('RandomForest',RF))\nmodels.append(('BaggingClassifier',Bag))\nmodels.append(('AdaBoost',AB))\nmodels.append(('GBoost',GB))\nmodels.append(('Stacked',stacked))","8f21ff8d":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = model_selection.KFold(shuffle=True,n_splits=3,random_state=0)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train,cv=kfold, scoring='f1_weighted',n_jobs=-1)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"%s: %f (%f)\" % (name, np.mean(cv_results),np.var(cv_results,ddof=1)))\n   # boxplot algorithm comparison\nfig = plt.figure(figsize=(10,9))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,rotation=45)\nplt.show()","877968aa":"RF.fit(X_train,y_train)","c3af6b07":"print('Training score =', RF.score(X_train, y_train))\nprint('Test score =', RF.score(X_test, y_test))","15044ca5":"predictions = RF.predict(X_test)","295dd9ea":"print(accuracy_score(y_test, predictions))","a11094ca":"cm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.show()","ceb103b3":"# Classification Report\nprint(classification_report(y_test, predictions))","243a0326":"tn = cm[0,0]  #True Negative\ntp = cm[1,1]  #True Positives\nfp = cm[0,1]  #False Positives\nfn = cm[1,0]  #False Negatives\n\naccuracy = (tp+tn)\/(tp+fn+fp+tn)\nprecision = tp \/ (tp+fp)\nrecall = tp \/ (tp+fn)\nf1 = 2*precision*recall \/ (precision+recall)\n\nprint('Accuracy =',accuracy)\nprint('Precision =', precision)\nprint('Recall =', recall)\nprint('F1 Score =', f1)","987f89ab":"from sklearn.metrics import roc_curve,roc_auc_score\nypred = RF.predict_proba(Xtest)\nfpr,tpr,threshold = roc_curve(ytest,ypred[:,1])\nroc_auc = roc_auc_score(ytest,ypred[:,1])\n\nprint('ROC AUC =', roc_auc)\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC Curve (area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","98940153":"RF.fit(X1,y1)","000dcbe5":"## Part 7 Data Imbalance in the Target Varaible","55a2cb3d":"![3.png](attachment:3.png)\n\n### In the given data it is observed that there are different types of jobs such as admin, management, student, blue collar and others. Among these blue collar job records are high approximately 4,000 and subscription wise also blue collar job holders are doing more term deposits. While considering these job categories bank should also consider housing and personal loan into consideration. \n\n### From the above graph it is observed that the people with no loan are doing high term deposits compared to people with loan burdens. This signifies that bank should focus on doing marketing on blue collar job holder\u2019s followed by management, admin, and others who are not having loans. This leads to good number of the people turn towards subscriptions.\n","3b9e842b":"## 3.1 Imputation:\n### Now, to infer the missing values in 'job' and 'education', we make use of the cross-tabulation between 'job' and 'education'. Our hypothesis here is that 'job' is influenced by the 'education' of a person.\n\n### Hence, we can infer 'job' based on the education of the person. Moreover, since we are just filling the missing values, we are not much concerned about the causal inference. We, therefore, can use the job to predict the education.\n\n### Inferring education from jobs : From the cross-tabulation, it can be seen that people with management jobs usually have a university degree. Hence wherever 'job' = management and 'education' = unknown, we can replace 'education' with \u2018tertiary'. Similarly, 'job' = 'services' --> 'education' = \u2018secondary and 'job' = 'housemaid' --> 'education' = \u2018primary.\n\n### Inferring jobs from age : As we see, if 'age' > 60, then the 'job' is 'retired,' which makes sense.\n\n### While imputing the values for job and education, we were cognizant of the fact that the correlations should make real world sense. If it didn't make real world sense, we didn't replace the missing values. We haven\u2019t found such any hidden pattern between poutcome and contact and we haven\u2019t changed unknowns in these features. Change the \"response\" variable (yes\/no) to binary values (1\/0) for easier analysis.","42d7a2f6":"## Insights: target students and retired clients\n### As noted from the horizontal bar chart, students and retired clients account for more than 50% of subscription, which is consistent with the previous finding of higher subscription rates among the younger and older.\n","e1934e8a":"## KNN(Outlayer Treatment)","2f9398ad":"## 4.5 Scatter matrix and Correlation matrix","668bc170":"## Part 8 Base Model with oversampled data (SMOTE)","bf1689dc":"## 5.3 check for multicollinearity (VIF)\n\n### Variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.","a77ce6f5":"## 4.10 Visualize the subscription and contact rate by month","4626975e":"![1.png](attachment:1.png)\n\n### The contact plot signifies that most of the people were contacted by the bank through cellular network more than 25,000 and among them more than 4,000 people have subscribed term deposit. From this we can infer that the bank should focus on the people who are available to contact them on cellular network.\n\n### Even unknown contact type records are also of high number approximately 12,400. So, bank should take some initiatives to identify which contact type those unknowns are using and separate them into cellular, telephone or make a new cluster.\n\n### We noticed a hidden pattern from \u201cRecords with respect time plot\u201d. Majority of the records were identified in the month may followed by April, November and others. Similarly, lowest records were identified in the months March, December, June and others.  So, bank should not focus directly on the months which are having highest records instead of that they have to focus on the months at which most the subscriptions happening.\n","77d25589":"## 5.2 Two-sample t test\n\n## For all the numeric variables, A two-sample unpaired t tests was performed between values of the variable for two classes of target variables to compare their means. \n### Null Hypothesis H0: The means of the two samples are EQUAL \n### Alternate Hypothesis Ha: The means of the two samples are NOT EQUAL\n## If the means of the two samples are significantly different form each other, then we can conclude that the variable does have a significant relationship with the target variable.\n","05321ade":"### Before the outlier treatment it is observed form the data is not normally distributed and also many outliers are present in the data. In order to reduce the outliers and to make the data to be normally distributed we identified the outliers and performed knn- imputation on the outlier\u2019s and made outlier treatment.\n### In order to reduce the scale of the data we also performed cube root transformation on the data. We choose cube root transformation due to few reasons:\n* As data in some of the features are left skewed and square root transformation of left skewed data make data NaN.\n* Log transformation in order to reduce the skewness it is increasing the skewness of the some of the features such as \u201cbalance\u201d due to the data in the balance feature is left skewed (i.e negative values).\n\n### This makes us to choose higher end transformation techniques such as cube root and finally makes our data normally distributed compared to the data before outlier treatment and transformation.\n","34112655":"### From the above results we can infer that the features which are having multicollinearity are only three features. This resembles that there is very less multicollinearity in the data. So there is no need to go for PCA (Principal Component Analysis).","937dc6f2":"## Dummy Encoding\u00b6\n","8bffe76b":"## Part 9 Algorithms Comparison:\n### Four different classification algorithms (Logistic Regression, K-Neighbours Classifier, Decision Tree Classifier, and Gaussian NB, Random Forest, Ada boost, Gradient Boosting) were run on the dataset through K-fold cross validation and the best-performing one was (identified by observing bias and variance errors) and used to build the classification model.\n","8168478a":"## Part 5 Statistical Analysis\n## Statistical tests were performed to see the whether the independent variables have a significant relationship with the dependent variable, DEPOSIT\n","f00d829a":"## Checking the normality of the features","ef375286":"## Prepare Data for Classification\n### Select the most relevant customer information: job, education, age, balance, default, housing and loan and other features whose p-value <0.05 and vif<4. Since machine learning algorithms only take numerical values, all five categorical variables (job, education, default, housing and others) are transformed into dummy variables. \n\n### Dummy variables were used instead of continuous integers because these categorical variables are not ordinal. They simply represent different types rather than levels, so dummy variables are ideal to distinguish the effect of different categories. Feature selection: all customer statistics were selected as features while the deposit feature was set as target. 70% of the data was used to build the classification model and 30% was reserved for testing the model.\n","3db23dc1":"## 4.4 Visualize the relationship between phone call duration & the number of campaigns with respect to deposit","9a6bff71":"### From the above results it is observed that Accuracy, precision, recall and f1-score are approximately same and all are around 80% which indicates that model is having less bias and variance errors. This is due to generating balance in the target variable by using smote technique. So by doing oversampling the data there is an increase in the classification metrics (precision, recall, f1-score) along with accuracy. ","43f29336":"## Part 2 About the Data\n### There are 45,211 observations in the dataset, with no missing values. Each represents an existing customer that the bank reached via phone calls. For each observation, the dataset records 17 input variables that stand for both qualitative and quantitative attributes of the customers. There is a single binary output variable that denotes \u201cyes\u201d(1) or \u201cno(0)\u201d revealing the outcomes of the marketing phone calls. \"Yes\" means that a customer subscribed to term deposits","456b6cd2":"## As only three features are having vif>4 inferes that there is less multicollinearity in the data\n## Removing features having vif>4","dff33686":"### Based on this scatter plot, there is no clear relationship between client\u2019s age and balance level.\n### Nevertheless, over the age of 60, clients tend to have a significantly lower balance, mostly under 5,000 euros. This is due to the fact that most people retire after 60 and no longer have a reliable income source.\n","83355888":"## 4.6 Visualize the subscription and contact rate by customer age","1609d889":"## 4.7 Visualize the subscription rate by balance level","fcf92a7c":"## Outlyer Treatment and transformation","6aca0ebd":"### To investigate more about correlation, a correlation matrix was plotted with all qualitative variables. Clearly, \u201cdeposit\u201d has a strong correlation with \u201cduration\u201d, a moderate correlation with \u201cpdays\u201d, and mild correlations between \u201cprevious\u201d, and \u201ccampaign\u201d and \u201cbalance\u201d. This correlation matrix also infers that there is no such strong correlation (multi collinearity) among the features and it resembles that there is no much noise in the data.\n","1b3d223a":"## 4.9 Visualize the subscription rate by job","cf5176a0":"## Creating new variables (variable name + '_un') to capture the information if the missing values are at random or is there a pattern in the missing values.","1a21fa31":"![2.png](attachment:2.png)\n\n### In the given data it is observed that there are three types of educated people (primary, secondary, tertiary) and unknown. Among these secondary educated people are doing more subscriptions followed by tertiary, primary. So, the bank should focus their marketing on the tertiary educated people first and in the month of the may followed by other months. \n\n### In the given data it is observed that there are different types of jobs such as admin, management, student, blue collar and others. Among these blue collar job records are high approximately 4,000 and subscription wise also blue collar job holders are doing more term deposits. This signifies that bank should focus on doing marketing on blue collar job holder\u2019s followed by management, admin, and others which leads to good number of the people turn towards subscriptions.","d9d77c90":"## 5.1 Chi-square Test\n## For the Categorical Columns, a Chi-square Test of independence was performed with the target variable, DEPOSIT which is also a categorical column. \n### Null Hypothesis H0: There is NO association between the two variables \n### Alternate Hypothesis Ha: There is an association between the two variables\n","5b5388ef":"## 4.2 Visualize the distribution of customer age and balance levels:","c10c9bbe":"## Insights: target clients with average or high balance\n\n### To identify the trend more easily, clients are categorized into four groups based on their levels of balance:\n* No Balance: clients with a negative balance.\n* Low Balance: clients with a balance between 0 and 1000 euros\n* Average Balance: clients with a balance between 1000 and 5000 euros.\n* High Balance: clients with a balance greater than 5000 euros.\n\n### Unsurprisingly, this bar chart indicates a positive correlation between clients\u2019 balance levels and subscription rate. Clients with negative balances only returned a subscription rate of 6.9% while clients with average or high balances had significantly higher subscription rates, more than15%.\n### However, in this campaign, more than 50% of clients contacted only have a low balance level. In the future, the bank should shift its marketing focus to high-balance customers to secure more term deposits.\n","c68fb2f2":"## Oversampling the target variable by using SMOTE","5b932510":"## Insights: target older clients with high balance levels\n### While age represents a person\u2019s life stage and balance represents a person\u2019s financial condition, jointly evaluating the impact of these two factors enables us to investigate if there is a common trend across all ages, and to identify which combination of client features indicates the highest likelihood of subscription.\n### In order to investigate the combined effect of age and balance on a client\u2019s decision, we performed a two-layer grouping, segmenting customers according to their balance levels within each age group.\n### The graph tells the same story regarding the subscription rate for different age groups: the willingness to subscribe is exceptionally high for people aged above 60 and younger people aged below 30 also have a distinguishable higher subscription rate than those of other age groups.\n### Furthermore, the effect of balance levels on subscription decision is applicable to each individual age group: every age group shares a common trend that the percentage of subscription increases with balance.\n### In sum, the bank should prioritize its telemarketing to clients who are above 60 years old and have positive balances, because they have the highest acceptance rate of about 35%. The next group the bank should focus on is young clients with positive balances, who showed high subscription rates between 15% and 20%.\n","e3431cb6":"## The data is not balanced with approximately 90 and 10 percent .But we can do oversampling. This infers that the target variable is not evenly distributed","1d68612b":"## 4.3 Visualize the relationship between customer age and balance","9abf01ce":"## Part 1 Project Background\n\n### Nowadays, marketing expenditures in the banking industry are massive, meaning that it is essential for banks to optimize marketing strategies and improve effectiveness. Understanding customers\u2019 need leads to more effective marketing plans, smarter product designs and greater customer satisfaction.\n\n### Main Objectives: predict customers' responses to future marketing campaigns & increase the effectiveness of the bank's telemarketing campaign\n\n### This project will enable the bank to develop a more granular understanding of its customer base, predict customers' response to its telemarketing campaign and establish a target customer profile for future marketing plans.\n\n### By analysing customer features, such as demographics and transaction history, the bank will be able to predict customer saving behaviours and identify which type of customers is more likely to make term deposits. The bank can then focus its marketing efforts on those customers. This will not only allow the bank to secure deposits more effectively but also increase customer satisfaction by reducing undesirable advertisements for certain customers.","901b2a7e":"## Splitting the data","adcb6f21":"## Removing features which are having p>0.05\n\n### checked the p-values of the features p>0.05 by removing individual features having p>0.05 but changes in the p-value reduction is not observed. So, removing the features all at a time","9ad6cbcd":"## Insights: target the youngest and the oldest instead of the middle-aged\n### Green vertical bars indicate that clients with a age of 60+ have the highest subscription rate. About 17% of the subscriptions came from the clients aged between 18 to 29. More than 50% of the subscriptions are contributed by the youngest and the eldest clients.\n### It is not surprising to see such a pattern because the main investment objective of older people is saving for retirement while the middle-aged group tend to be more aggressive with a main objective of generating high investment income. Term deposits, as the least risky investment tool, are more preferable to the eldest.\n### The youngest may not have enough money or professional knowledge to engage in sophisticated investments, such as stocks and mutual funds. Term deposits provide liquidity and generate interest incomes that are higher than the regular saving account, so term deposits are ideal investments for students.\n### However, red vertical bars show that the bank focused its marketing efforts on the middle-aged group, which returned lower subscription rates than the younger and older groups. Thus, to make the marketing campaign more effective, the bank should target younger and older clients in the future\n","fc7de931":"## Part 6 Machine Learning: Classification\n### The main objective of this project is to identify the most responsive customers before the marketing campaign so that the bank will be able to efficiently reach out to them, saving time and marketing resources. To achieve this objective, classification algorithms will be employed. By analyzing customer statistics, a classification model will be built to classify all clients into two groups: \"yes\" to term deposits and \"no\" to term deposits.\n","479e9420":"## Insights: Initiate the telemarketing campaign in fall or spring\n### Besides customer characteristics, external factors may also have an impact on the subscription rate, such as seasons and the time of calling. So the month of contact is also analysed here.\n### This line chart displays the bank\u2019s contact rate in each month as well as clients\u2019 response rate in each month. One way to evaluate the effectiveness of the bank's marketing plan is to see whether these two lines have a similar trend over the same time horizon.\n### The bank contacted most clients between May and August. The highest contact rate is around 30%, which happened in May, while the contact rate is closer to 0 in March, September, October, and December. However, the subscription rate showed a different trend. The highest subscription rate occurred in March, which is over 50%, and all subscription rates in September, October, and December are over 40%.\n### Clearly, these two lines move in different directions which strongly indicates the inappropriate timing of the bank\u2019s marketing campaign. To improve the marketing campaign, the bank should consider initiating the telemarketing campaign in fall and spring when the subscription rate tends to be higher.\n### Nevertheless, the bank should be cautious when analysing external factors. More data from previous marketing campaign should be collected and analysed to make sure that this seasonal effect is constant over time and applicable to the future.\n","e8459414":"## Part 10 Conclusion\n### The main objective of this project is to increase the effectiveness of the bank's telemarketing campaign, which was successfully met through data analysis, visualization and analytical model building. A target customer profile was established while classification models were built to predict customers' response to the term deposit campaign.\n### According to previous analysis, a target customer profile can be established. The most responsive  customers possess these features:\n\n* Feature 1: age < 30 or age > 60\n* Feature 2: students or retired people\n* Feature 3: a balance of more than 5000 euros\n\n### By applying RF classifier algorithm, classification and estimation model were successfully built. With this model, the bank will be able to predict a customer's response to its telemarketing campaign before calling this customer. In this way, the bank can allocate more marketing efforts to the clients who are classified as highly likely to accept term deposits, and call less to those who are unlikely to make term deposits.\n### In addition, predicting duration before calling and adjusting marketing plan benefit both the bank and its clients. On the one hand, it will increase the efficiency of the bank\u2019s telemarketing campaign, saving time and efforts. On the other hand, it prevents some clients from receiving undesirable advertisements, raising customer satisfaction. With the aid of RF classifier model, the bank can enter a virtuous cycle of effective marketing, more investments and happier customers.","ed34f3f8":"## The distribution of age:\n### In its telemarketing campaigns, clients called by the bank have an extensive age range, from 18 to 95 years old. However, a majority of customers called is in the age of 30s and 40s (33 to 48 years old fall within the 25th to 75th percentiles). \n### The distribution of customer age is fairly normal with a small standard deviation.\n## The distribution of balance:\n### The distribution of balance having a minimum of -8019 to a maximum of 102127 euros. The distribution of balance has a huge standard deviation relative to the mean, suggesting large variabilities in customers' balance levels.\n","89c50d55":"## Part 4 Exploratory Data Analysis\n## 4.1 Bivariate Analysis","76cd0620":"## Part 11 Recommendations\n## 11.1. More appropriate timing\n### When implementing a marketing strategy, external factors, such as the time of calling, should also be carefully considered. The previous analysis points out that March, September, October and December had the highest success rates. Nevertheless, more data should be collected and analyzed to make sure that this seasonal effect is constant over time. If the trend has the potential to continue in the future, the bank should consider initiating its telemarketing campaign in fall and spring.\n## 11.2. Smarter marketing design\n### By targeting the right customers, the bank will have more and more positive responses, and the classification algorithms would ultimately eliminate the imbalance in the original dataset. Hence, more accurate information will be presented to the bank for improving the subscriptions. Meanwhile, to increase the likelihood of subscription, the bank should re-evaluate the content and design of its current campaign, making it more appealing to its target customers.\n## 11.3. Better services provision\n### With a more granular understanding of its customer base, the bank has the ability to provide better banking services. For example, marital status and occupation reveal a customer's life stage while loan status indicates his\/her overall risk profile. With this information, the bank can estimate when a customer might need to make an investment. In this way, the bank can better satisfy its customer demand by providing banking services for the right customer at the right time.\n","ff6c2e27":"## 4.8 Visualize the subscription rate by age and balance","b0ad593f":"### In this scatter plot, clients not subscribed to term deposits are denoted as \"0\" while clients subscribed are denoted as \"1\". Compared to \u201cno\u201d clients\u201d, \u201cyes\u201d clients were contacted by fewer times and had longer call duration. More importantly, after five campaign calls, clients are more likely to reject the term deposit unless the duration is high. Most \u201cyes\u201d clients were approached by less than 10 times. This suggests that the bank should resist calling a client for more than five times, which can be disturbing and increase dissatisfaction.\n","15876c8e":"## From the above results it is observed that RF Classifier is the best performing model.\n### By comparing all algorithms bias error and variance error, RF classifier is observed to be the best so it would be used to predict term depositor\u2019s. The test of RF classifier with base estimator (Decision Tree (DEFAULT), n_estimators=13) is the best model with best bias & variance error trade off\n","ddbb6a5d":"## 4.11 Normality & Outliers of the features after outlier\u2019s treatment","091a8e57":"## Finally","7c8f4a9e":"## Part 3 Data Cleaning\n### Several changes were made to the dataset to prepare it for analysis. There are unknown values for many variables in the data set which can be seen in the below plots. Variables with unknown\/missing values are: 'education', 'job', 'poutcome', 'contact'. By observing these features, we found way of doing an imputation where we use other independent variables to infer the value of the missing variable. \n\n### Therefore, we start with creating new variables for the unknown values in 'education', 'job'. We do this to see if the values are missing at random or is there a pattern in the missing values.\n","5ed037a1":"## From the above results it is observed that Accuracy is good but precision, recall and f1-score are very less compared to Accuracy. This is due to imbalance in the target variable. So by doing oversampling there is a chance to significantly increase the classification metrics (precision, recall, f1-score) along with accuracy."}}