{"cell_type":{"5771b296":"code","3f578711":"code","6efd213c":"code","d0385122":"code","c58564b0":"code","019aeab8":"code","2a2eeac9":"code","6674b80e":"code","eb69b213":"code","55a249a9":"code","bc8de1f0":"code","b2ad0282":"code","3d712db8":"code","5010d58d":"code","0907a815":"code","a2d3fb56":"code","d6bbec57":"code","93f57d8c":"code","321ed981":"code","dbcda78e":"code","b2666223":"code","2ba1eaea":"code","5a8399c9":"code","4195314d":"code","fa23213b":"code","4d15a928":"code","12b1618e":"code","b2474561":"code","c51e75d9":"code","cbcce386":"code","7ea812cc":"code","6af2cc57":"code","e443d2ee":"code","8fd4a85e":"code","9d9aebcf":"code","58fabeb0":"code","8455f63b":"code","73088b43":"code","46966059":"code","e4587da7":"code","45381a82":"markdown","23d9ec23":"markdown","19b1e99c":"markdown","2c9103ef":"markdown","4d9106c4":"markdown","b729b25a":"markdown","44093233":"markdown","2e80db44":"markdown","4a2a6bfb":"markdown","e951aa83":"markdown","525c98ab":"markdown","c6786b0b":"markdown","5fec420b":"markdown","fd7df4cc":"markdown","f48a1947":"markdown","1b8a767d":"markdown","dae9c2f1":"markdown","2831a2c5":"markdown","056d969f":"markdown","26f9b171":"markdown","17a3b02f":"markdown","866b73ee":"markdown","f2ad2053":"markdown","4ff77c2e":"markdown","6b274083":"markdown","a4c3cd22":"markdown","24c2672e":"markdown","7bb6a1e1":"markdown","f8f0e1bc":"markdown","54096f5a":"markdown","0c4bdcb3":"markdown"},"source":{"5771b296":"import numpy as np\nimport pandas as pd\nimport json\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge\nfrom scipy.sparse import csr_matrix, hstack\nfrom scipy.stats import probplot\nimport pickle\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport seaborn as sns \nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\n\ncolor = sns.color_palette()\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\nsns.palplot(color)\n\nimport os\nPATH = \"..\/input\"","3f578711":"!du -l ..\/input\/*","6efd213c":"def read_json_line(line=None):\n    result = None\n    try:        \n        result = json.loads(line)\n    except Exception as e:      \n        # Find the offending character index:\n        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n        # Remove the offending character:\n        new_line = list(line)\n        new_line[idx_to_replace] = ' '\n        new_line = ''.join(new_line)     \n        return read_json_line(line=new_line)\n    return result\n\nfrom html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()","d0385122":"from joblib import Parallel, delayed\nimport multiprocessing\nfrom tqdm import tqdm_notebook\n     \ndef process_line(line):\n    json_data = read_json_line(line)\n    content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n    content_no_html_tags = strip_tags(content)\n    published = json_data['published']['$date']\n    title = json_data['meta_tags']['title'].split('\\u2013')[0].strip() #'Medium Terms of Service \u2013 Medium Policy \u2013 Medium'\n    author = json_data['meta_tags']['author'].strip()\n    domain = json_data['domain']\n    url = json_data['url']\n\n    tags_str = []\n    soup = BeautifulSoup(content, 'lxml')\n    try:\n        tag_block = soup.find('ul', class_='tags')\n        tags = tag_block.find_all('a')\n        for tag in tags:\n            tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n        tags = ' '.join(tags_str)\n    except Exception:\n        tags = 'None'\n    return content_no_html_tags, published, title, author, domain, tags, url\n \ndef extract_features(path_to_data):\n    \n    content_list = [] \n    published_list = [] \n    title_list = []\n    author_list = []\n    domain_list = []\n    tags_list = []\n    url_list = []\n\n    with open(path_to_data, encoding='utf-8') as inp_json_file:\n        num_cores = multiprocessing.cpu_count()\n        results = Parallel(n_jobs=num_cores)(delayed(process_line)(line) for line in tqdm_notebook(inp_json_file))\n    return zip(*results) #content_list, published_list, title_list, author_list, domain_list, tags_list, url_list","c58564b0":"content_train, published_list, title_list, author_list, domain_list, tags_list, url_list = extract_features(os.path.join(PATH, 'how-good-is-your-medium-article\/train.json'))\ntrain = pd.DataFrame()\n#train['content'] = content_list\ntrain['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\ntrain['title'] = title_list\ntrain['author'] = author_list\ntrain['domain'] = domain_list\ntrain['tags'] = tags_list\ntrain['length'] = list(map(len, content_train))\ntrain['url'] = url_list\n\ntrain_target = pd.read_csv(os.path.join(PATH, 'how-good-is-your-medium-article\/train_log1p_recommends.csv'), index_col='id')\ny_train = train_target['log_recommends'].values","019aeab8":"content_test, published_list, title_list, author_list, domain_list, tags_list, url_list = extract_features(os.path.join(PATH, 'how-good-is-your-medium-article\/test.json'))\ntest = pd.DataFrame()\n#test['content'] = content_list\ntest['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\ntest['title'] = title_list\ntest['author'] = author_list\ntest['domain'] = domain_list\ntest['tags'] = tags_list\ntest['length'] = list(map(len, content_test))\ntest['url'] = url_list\ndel published_list, title_list, author_list, domain_list, tags_list, url_list\ngc.collect()","2a2eeac9":"idx_split = len(train)\ndf_full = pd.concat([train, test])\n\ndf_full['dow'] = df_full['published'].apply(lambda x: x.dayofweek)\ndf_full['year'] = df_full['published'].apply(lambda x: x.year)\ndf_full['month'] = df_full['published'].apply(lambda x: x.month)\ndf_full['hour'] = df_full['published'].apply(lambda x: x.hour)\ndf_full['number_of_tags'] = df_full['tags'].apply(lambda x: len(x.split()))\n\ntrain = df_full.iloc[:idx_split, :]\ntest = df_full.iloc[idx_split:, :]\n\ntrain['target'] = y_train\ntrain.sort_values(by='published', inplace=True)\ntrain.reset_index(drop=True, inplace=True)\n\nprint('TRAIN: {}'.format(train.shape))\nprint('TEST: {}'.format(test.shape))\ndel df_full\ngc.collect()","6674b80e":"train.head()","eb69b213":"plt.figure(figsize=(15,6))\nplt.suptitle(\"Target variable\",fontsize=20)\ngridspec.GridSpec(2,2)\n\nplt.subplot2grid((2,2),(0,0))\nplt.xlim(0, 12)\nsns.distplot(train.target.values, hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of claps (log1p transformed)\")\n\nplt.subplot2grid((2,2),(1,0))\nplt.xlim(0, 12)\nsns.boxplot(train.target.values)\n\nplt.subplot2grid((2,2),(0,1), rowspan=2)\nplt.ylim(0, 12)\n# plt.grid(False)\nprobplot(train.target.values, dist=\"norm\", plot=plt);","55a249a9":"train.sort_values(by='target', ascending=False).reset_index(drop=True).loc[0, 'url']","bc8de1f0":"plt.figure(figsize=(16,6))\nplt.suptitle(\"                       Posts distribution across years\",fontsize=20)\n\nax1 = plt.subplot2grid((1,5),(0,0), colspan=3)\nax1 = sns.countplot(x='year', data=train, alpha=0.8, color=color[2])\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Year', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,5),(0,3), colspan=2, sharey=ax1)\nax2 = sns.countplot(x='year', data=test, alpha=0.8, color=color[9])\nplt.xlabel('Year', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')","b2ad0282":"train = train[train.year >= 2015]","3d712db8":"temp=pd.concat([train.groupby(['year','month'])['hour'].count(), test.groupby(['year','month'])['hour'].count().iloc[:-1]])\nplt.figure(figsize=(12,4))\nsns.pointplot(temp.index,temp.values, alpha=0.8, color=color[1],)\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly posts variation', fontsize=15)\nplt.xticks(rotation='vertical');\n\ntemp=train.groupby(['year','month']).aggregate({'hour':np.size,'year':np.min,'month':np.min})\ntemp.reset_index(drop=True, inplace=True)\nplt.figure(figsize=(12,6))\nplt.plot(range(1,13),temp.iloc[0:12,0],label=\"2015\", marker='o')\nplt.plot(range(1,13),temp.iloc[12:24,0],label=\"2016\", marker='o')\nplt.plot(range(1,7),temp.iloc[24:30,0],label=\"2017-train\", marker='o')\nconnect_point = temp.iloc[29,0]\n\ntemp=test.groupby(['year','month']).aggregate({'hour':np.size,'year':np.min,'month':np.min})\ntemp.reset_index(drop=True, inplace=True)\nplt.plot(range(6,8),[connect_point,temp.iloc[0,0]], color='r',label=None)\nplt.plot(range(7,13),temp.iloc[0:6,0], color='r',label=\"2017-test\", marker='o')\nplt.plot(range(1,3),temp.iloc[6:8,0],label=\"2018-test\", marker='o')\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly posts variation', fontsize=15)\nplt.xticks(np.arange(1, 13, 1.0))\nplt.xlim(1, 12)\nplt.legend(loc='upper right', fontsize=11)\nplt.xticks(rotation='horizontal');","5010d58d":"temp=train.groupby(['year','month'])['target'].sum()\nplt.figure(figsize=(13,4))\nsns.pointplot(temp.index,temp.values, alpha=0.8, color=color[4],)\nplt.ylabel('Overall claps (log1p transformed)', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly claps variation', fontsize=15)\nplt.xticks(rotation='vertical');","0907a815":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='dow', data=train)\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title('Claps distribution across day of week', fontsize=15)\nax1.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\nplt.subplot(122)\ntemp = train.groupby('dow')['target'].sum()\nax2 = sns.barplot(temp.index,np.round(temp.values))\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title('Count of claps across day of week', fontsize=15)\nax2.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","a2d3fb56":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='hour', data=train, color=color[9])\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Hour', fontsize=12)\nplt.title('Claps distribution across hour', fontsize=15)\n\nplt.subplot(122)\ntemp = train.groupby('hour')['target'].sum()\nax2 = sns.barplot(temp.index,temp.values, alpha=0.8, color=color[9])\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Hour', fontsize=12)\nplt.title('Count of claps across hour', fontsize=15);","d6bbec57":"plt.figure(figsize=(15,10))\nplt.title('Claps distribution across hour and day of week', fontsize=20)\ntemp = train.pivot_table(index='dow', columns='hour', values='target', aggfunc='mean')\nax = sns.heatmap(temp, annot=True, fmt='.2f', cmap='viridis')\nax.set(xlabel='Hour', ylabel='Day of week')\nax.set_yticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.yticks(rotation='horizontal');","93f57d8c":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Posts distribution across domains\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='domain', data=train, alpha=0.8, color=color[2], order=train.domain.value_counts().iloc[:10].index)\nplt.ylabel('Number of posts', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='domain', data=test, alpha=0.8, color=color[9], order=test.domain.value_counts().iloc[:10].index)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","321ed981":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\ntemp = train[train.domain.isin(['medium.com', 'hackernoon.com'])]\nax1 = sns.boxplot(y='target',x='year', hue='domain', data=temp)\nplt.ylabel('Claps by post', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Claps distribution across domains', fontsize=15)\n\nplt.subplot(122)\ntemp = temp.groupby('domain')['target'].sum().iloc[:2]\nax2 = sns.barplot(temp.index,temp.values)\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Domain', fontsize=12)\nplt.title('Count of claps across domain', fontsize=15);","dbcda78e":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Count of posts across authors\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='author', data=train, alpha=0.8, color=color[2], order=train.author.value_counts().iloc[:10].index)\nplt.ylabel('Overall posts', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='author', data=test, alpha=0.8, color=color[9], order=test.author.value_counts().iloc[:10].index)\nplt.xlabel('Author', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","b2666223":"plt.figure(figsize=(18,6))\n\n\ntemp = train.groupby('author')['target'].sum().sort_values(ascending=False).iloc[:30]\nax1 = sns.barplot(temp.index,np.round(temp.values, 1), alpha=0.8, color=color[3])\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Number of claps across authors', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nplt.figure(figsize=(18,6))\n\n\ntemp = train.groupby('author')['target'].median().sort_values(ascending=False).iloc[:30]\nax2 = sns.barplot(temp.index,np.round(temp.values, 1), alpha=0.8, color=color[4])\nplt.ylabel('Median of claps by post', fontsize=12)\nplt.xlabel('Author', fontsize=12)\nplt.title('Median of claps across authors', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","2ba1eaea":"plt.figure(figsize=(15,6))\nplt.suptitle(\"Length of post distribution\",fontsize=20)\ngridspec.GridSpec(2,1)\n\nplt.subplot2grid((2,1),(0,0))\nplt.xlim(0, 450000)\nsns.distplot(train.length.values, hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of chars\")\n\nplt.subplot2grid((2,1),(1,0))\nplt.xlim(0, 450000)\nsns.boxplot(train.length.values);\n\nplt.figure(figsize=(15,6))\nplt.suptitle(\"Length of post distribution (log1p transformed)\",fontsize=20)\ngridspec.GridSpec(2,1)\n\nplt.subplot2grid((2,1),(0,0))\nsns.distplot(np.log1p(train.length.values), hist=False, color=color[0], kde_kws={\"shade\": True, \"lw\": 2})\nplt.title(\"Number of chars (log1p transformed)\")\n\nplt.subplot2grid((2,1),(1,0))\nsns.boxplot(np.log1p(train.length.values));","5a8399c9":"ax = sns.jointplot(x=np.log1p(train[\"length\"]), y=train[\"target\"], kind='kde', size=9)\nax.set_axis_labels(\"Length of article\", \"Number of claps\");","4195314d":"%%time\ncv_train_tags = CountVectorizer(ngram_range=(1, 1), min_df=5)\nX_train_tags = cv_train_tags.fit_transform(train.tags.values).toarray()\ncv_test_tags = CountVectorizer(ngram_range=(1, 1), min_df=5)\nX_test_tags = cv_test_tags.fit_transform(test.tags.values).toarray()\n\nmatrix_freq = X_train_tags.sum(axis=0).ravel()\nX_train_freq = np.array([np.array(cv_train_tags.get_feature_names()), matrix_freq])\nmatrix_freq = X_test_tags.sum(axis=0).ravel()\nX_test_freq = np.array([np.array(cv_test_tags.get_feature_names()), matrix_freq])\n\ndf_train_tags = pd.DataFrame()\ndf_train_tags['tag'] = X_train_freq[0]\ndf_train_tags['number_of_posts'] = X_train_freq[1]\ndf_train_tags['mean_claps'] = [0]*len(X_train_freq[1])\ndf_train_tags['sum_claps'] = [0]*len(X_train_freq[1])\n\ndf_test_tags = pd.DataFrame()\ndf_test_tags['tag'] = X_test_freq[0]\ndf_test_tags['number_of_posts'] = X_test_freq[1]\n\ndf=pd.DataFrame(X_train_tags)\ndf['target'] = train.target.values\nfor col in range(df.shape[1]-1):\n    temp=df[df[col]==1]\n    df_train_tags.loc[col,'mean_claps']=temp['target'].mean()\n    df_train_tags.loc[col,'sum_claps']=temp['target'].sum()\n    \ndf_train_tags['tag'] = df_train_tags['tag'].astype(str)\ndf_train_tags['number_of_posts'] = df_train_tags['number_of_posts'].astype(int)\ndf_test_tags['tag'] = df_test_tags['tag'].astype(str)\ndf_test_tags['number_of_posts'] = df_test_tags['number_of_posts'].astype(int)","fa23213b":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Top-15 tags by number of occurrences in posts\", fontsize=18)\n\nax1 = plt.subplot2grid((1,2),(0,0))\ntemp = df_train_tags.sort_values(by='number_of_posts', ascending=False).iloc[:15]\nax1 = sns.barplot(temp.tag, temp.number_of_posts, alpha=0.8, color=color[7])\nplt.ylabel('Number of occurrences', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\ntemp = df_test_tags.sort_values(by='number_of_posts', ascending=False).iloc[:15]\nax2 = sns.barplot(temp.tag, temp.number_of_posts, alpha=0.8, color=color[8])\nplt.xlabel('Tag', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","4d15a928":"plt.figure(figsize=(18,6))\ntemp = df_train_tags.sort_values(by='sum_claps', ascending=False).iloc[:30]\nax1 = sns.barplot(temp.tag, temp.sum_claps, alpha=0.8, color=color[3])\nplt.ylabel('Overall claps', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Top-30 tags by total number of claps', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom')\n    \nplt.figure(figsize=(18,6))\ntemp = df_train_tags.sort_values(by='mean_claps', ascending=False).iloc[:30]\nax2 = sns.barplot(temp.tag, temp.mean_claps, alpha=0.8, color=color[4])\nplt.ylabel('Median of claps by post', fontsize=12)\nplt.xlabel('Tag', fontsize=12)\nplt.title('Top-30 tags by median of claps', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(np.round(p.get_height(),1)), (x.mean(), y), ha='center', va='bottom');","12b1618e":"plt.figure(figsize=(16,6))\n\nplt.subplot(121)\nax1 = sns.boxplot(y='target',x='number_of_tags', data=train)\nplt.ylabel('Claps distribution', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Claps distribution across article with different number of tags', fontsize=15)\n\nplt.subplot(122)\ntemp = train.groupby('number_of_tags')['target'].sum()\nax2 = sns.barplot(temp.index,np.round(temp.values))\nplt.ylabel('Number of claps', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Count of claps across article with different number of tags', fontsize=15)\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(int(p.get_height())), (x.mean(), y), ha='center', va='bottom');","b2474561":"plt.figure(figsize=(16,6))\nplt.suptitle(\"Count of articles with different number of tags\",fontsize=20)\n\nax1 = plt.subplot2grid((1,2),(0,0))\nax1 = sns.countplot(x='number_of_tags', data=train, alpha=0.8, color=color[1])\nplt.ylabel('Overall articles', fontsize=12)\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Train data', fontsize=15)\nplt.grid(False)\n\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom')\n    \nax2 = plt.subplot2grid((1,2),(0,1), sharey=ax1)\nax2 = sns.countplot(x='number_of_tags', data=test, alpha=0.8, color=color[2])\nplt.xlabel('Number of tags in an article', fontsize=12)\nplt.title('Test data', fontsize=15)\nplt.yticks([])\nplt.ylabel('')\n\n\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax2.annotate('{}'.format(p.get_height()), (x.mean(), y), ha='center', va='bottom');","c51e75d9":"content_train = train['content'].values.tolist()\ntitle_train = train['title'].values.tolist()\ntags_train = train['tags'].values.tolist()\ny_train = train['target'].values\ntrain.drop(['content', 'title', 'target', 'tags', 'published', 'length', 'url'], axis=1, inplace=True)\n\ncontent_test = test['content'].values.tolist()\ntitle_test = test['title'].values.tolist()\ntags_test = test['tags'].values.tolist()\ntest.drop(['content', 'title', 'tags', 'published', 'length', 'url'], axis=1, inplace=True)","cbcce386":"train.columns","7ea812cc":"test.columns","6af2cc57":"%%time\n\nidx_split = len(train)\ndf_full = pd.concat([train, test])\n\nlist_to_dums = ['author', 'dow', 'month', 'hour', 'domain', 'year']\ndummies = pd.get_dummies(df_full, columns = list_to_dums, drop_first=True,\n                            prefix=list_to_dums, sparse=False)\n\nX_train_feats = dummies.iloc[:idx_split, :]\nX_test_feats = dummies.iloc[idx_split:, :]\n\nprint('TRAIN feats: {}'.format(X_train_feats.shape))\nprint('TEST feats: {}'.format(X_test_feats.shape))\ndel dummies, df_full\ngc.collect()","e443d2ee":"%%time\n# cv_title = CountVectorizer(max_features=30000)\ncv_content = CountVectorizer(max_features=50000)\ncv_tags = CountVectorizer(max_features=1000)\n\n# X_train_title = cv_title.fit_transform(title_train)\n# X_test_title = cv_title.transform(title_test)\nX_train_content = cv_content.fit_transform(content_train)\nX_test_content = cv_content.transform(content_test)\nX_train_tags = cv_tags.fit_transform(tags_train)\nX_test_tags = cv_tags.transform(tags_test)\n\nprint('TRAIN content: {}, tags: {}'.format(X_train_content.shape, X_train_tags.shape))\nprint('TEST content: {}, tags: {}'.format(X_test_content.shape, X_test_tags.shape))\ndel content_train, content_test, title_train, title_test, tags_train, tags_test\ngc.collect()","8fd4a85e":"# %%time\n# del train, test\n# X_train_sparse = csr_matrix(hstack([X_train_content, X_train_tags, X_train_feats.values])) \n# X_test_sparse = csr_matrix(hstack([X_test_content, X_test_tags, X_test_feats.values]))\n# print(X_train_sparse.shape, X_test_sparse.shape)","9d9aebcf":"def load_sparse_csr(filename):\n    loader = np.load(filename)\n    return csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n\nX_train_sparse = load_sparse_csr(os.path.join(PATH, 'mediumeda\/train_eda_csr.npz'))\nX_test_sparse = load_sparse_csr(os.path.join(PATH, 'mediumedatest\/test_eda_csr.npz'))\nprint(X_train_sparse.shape, X_test_sparse.shape)","58fabeb0":"%%time\ndef write_submission_file(prediction, path_to_sample=os.path.join(PATH, 'how-good-is-your-medium-article\/sample_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='id')\n    \n    submission['log_recommends'] = prediction\n    submission.to_csv('submission.csv')\n    \nridge = Ridge(random_state=17)                          \nridge_pred = ridge.fit(X_train_sparse, y_train).predict(X_test_sparse)      \nwrite_submission_file(ridge_pred)","8455f63b":"top30_plus = np.argsort(ridge.coef_)[-30:][::-1]\ntop30_plus","73088b43":"top30_minus = np.argsort(ridge.coef_)[:30]\ntop30_minus","46966059":"feats_plus=[]\nfeats_minus=[]\nfor idx in top30_plus:\n    if idx<X_train_content.shape[1]:\n        feats_plus.append(list(cv_content.vocabulary_.keys())[list(cv_content.vocabulary_.values()).index(idx)])\n#     elif (idx>=shape(X_train_content)[1] & idx<2*shape(X_train_content)[1]):\n#         feats_plus.append(list(cv_title.vocabulary_.keys())[list(cv_title.vocabulary_.values()).index(idx)])\n    elif idx>=(X_train_content.shape[1]+X_train_tags.shape[1]):\n        feats_plus.append(X_train_feats.columns[idx-(X_train_content.shape[1]+X_train_tags.shape[1])])\n    else:\n        feats_plus.append(list(cv_tags.vocabulary_.keys())[list(cv_tags.vocabulary_.values()).index(idx-X_train_content.shape[1])])\nfor idx in top30_minus:\n    if idx<X_train_content.shape[1]:\n        feats_minus.append(list(cv_content.vocabulary_.keys())[list(cv_content.vocabulary_.values()).index(idx)])\n#     elif (idx>=shape(X_train_content)[1] & idx<2*shape(X_train_content)[1]):\n#         feats_minus.append(list(cv_title.vocabulary_.keys())[list(cv_title.vocabulary_.values()).index(idx)])\n    elif idx>=(X_train_content.shape[1]+X_train_tags.shape[1]):\n        feats_minus.append(X_train_feats.columns[idx-(X_train_content.shape[1]+X_train_tags.shape[1])])\n    else:\n        feats_minus.append(list(cv_tags.vocabulary_.keys())[list(cv_tags.vocabulary_.values()).index(idx-X_train_content.shape[1])])","e4587da7":"plt.figure(figsize=(18,6))\nax1 = sns.barplot(feats_plus,ridge.coef_[top30_plus],color=color[2])\nplt.ylabel('Value', fontsize=12)\nplt.xlabel('Feature name', fontsize=12)\nplt.title('Feature importance (positive coefficients)', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12)\n    \nplt.figure(figsize=(18,6))\nax2 = sns.barplot(feats_minus,ridge.coef_[top30_minus],color=color[3])\nplt.ylabel('Value', fontsize=12)\nplt.xlabel('Feature name', fontsize=12)\nplt.title('Feature importance (negative coefficients)', fontsize=15)\nplt.grid(False)\nplt.xticks(rotation=90, fontsize=12);","45381a82":"The largest number of post falls in the last years. Let`s leave only 2015-2017.","23d9ec23":"## 2.1. Target variable\n\nThe target variable is number of claps and it was log1p transformed in advance. So, keep that in mind.","19b1e99c":"# 2. EDA\nThe five earliest articles on Medium:","2c9103ef":"Median and number of claps is greater to articles published earlier in the week. I think, that articles get the highest number of claps in the first few hours after publishing. Therefore the first thing is not surprising, people are much kinder after weekends :) The second thing is easy to understand too. What do you do on Monday at work? You are reading habr`s posts and like them :) And what do you do on weekend? Right, you are cycling, skydiving, swimming an\u0432 etc.","4d9106c4":"This kernel contains **multyprocess implementation** of data loading and preprocessing for\n[Clap it - Exploratory Data Analysis](https:\/\/www.kaggle.com\/jandevel\/clap-it-exploratory-data-analysis) kernel\nIt makes this operation several times faster.\nTake it as example that can be applied to other kernels as well","b729b25a":"Some posts have about 400 000 chars. That's the equivalent of about 100 pages in Microsoft Word with font Times New Roman and size 12. Fantastic! ","44093233":"Mean of claps is greater to posts published earlier in the week in the second half of the day. But it is interesting situation here. Posts published on Monday at night and on weekend in the afternoon have the high mean value of claps.","2e80db44":"## 2.4. Words features","4a2a6bfb":"There is almost no correlation between this two variables.","e951aa83":"There are new active authors and some old popular authors disappear. Thats normal to every popular platform. People change.\nAnd an interesting observation is that some authors are corporate blogs like ODS on the habr.","525c98ab":"Well, bad news. The distribution of the target variable is far from normal even though  it was log transformed. It means that we cant use parametric statistical tests in the future at least. Just non-parametric ones. Some articles was claped about 70 000 times. Maybe it's a mistake in the data? Lets look the most popular article and its number of claps.","c6786b0b":"The most popular author is Dina Leygermann. Lets look at her blog. In this way we make it possible to forge a common understanding of the Medium audience.\n\n<img src='https:\/\/image.prntscr.com\/image\/K0xjSl99SvuFt-qnsgtPXQ.png'>\n\nWell, sitcoms, politic... thats not the habr )","5fec420b":"There are TOP-10 domains by posts on the plot.  The most significant share is published on two ones - medium.com and hackermoon.com.","fd7df4cc":"Lately, blockchain and bitcoin become more important than politics )","f48a1947":"# 1. Data preprocessing\n## 1.1. Supplementary functions","1b8a767d":"And we can see the closely related situation to number of claps.","dae9c2f1":"## 2.2. Time series features","2831a2c5":" What we're seeing here? Medium`s posts get fewer and fewer claps year by year. And the mean value of claps on hackernoon is significant higher. But  the bulk of posts there is on medium.com.","056d969f":"## 1.2. Data extraction","26f9b171":"# Introduction:\nWhat is Medium? Medium is a dynamically developing international publishing platform for people to write, read and clap easily online. It is like the russian [habrahabr.ru](http:\/\/habrahabr.ru) just a little worse. We have two JSON files that contain published articles on Medium till 2018, March. There is number of claps to each article in the first file and there is no ones in the second file. Our goal is to predict the number of \"claps\" for articles in test. \nLet's start our EDA journey!","17a3b02f":"## 2.3. Other features","866b73ee":"**New mutiprocess stuff ends here: ====================================================== **    \nTake into account that where is no train['content'] and test['content'] columns more,   \nbut only content_train and content_test lists","f2ad2053":"The train data contains 62313 articles and the test one contains 34645.\nLet us look at the data a little closer.","4ff77c2e":"The similar situation like in previous plot.","6b274083":"![](http:\/\/)No mistake. It was really claped more than 79 000 times. ","a4c3cd22":"<img src='https:\/\/image.prntscr.com\/image\/OryvX61BTkCRzM2GTXi_PA.png'>","24c2672e":"As we can see, first articles were published about 50 years ago. Great :)","7bb6a1e1":"## 1.3. Feature engineering","f8f0e1bc":"# 3. Baseline and feature importance","54096f5a":"**New multiprocess stuff starts here: ====================================================== ** ","0c4bdcb3":"It's very interesting. First of all, Medium is rapidly becoming the popular platform. Then, April and September have less published articles than in previous month year after year. But in March, May and October is opposite situation. And the shocking upsurge of popularity in the 2018!"}}