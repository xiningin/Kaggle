{"cell_type":{"c4b0753e":"code","a3c3e342":"code","6cf3b770":"code","73aa7e6c":"code","d958050c":"code","13054b63":"code","6222939d":"code","db3a8396":"code","b21598ff":"code","1e279472":"code","0bebfd0c":"code","759918ec":"code","5b0c2051":"code","e396cfbb":"code","71f52cf0":"code","b6a49688":"code","4c19450a":"code","2f83697d":"code","e8325941":"code","90f0c54f":"code","d15a38c7":"code","44855bdc":"code","30bd5056":"code","e6107117":"code","0809b41f":"code","52bdba7f":"code","6de70d9a":"code","32754187":"code","beef88e5":"code","b710928d":"code","fd459638":"code","8fef444d":"code","c24d3e0a":"code","7c23ac71":"code","6ceae0a0":"code","39acfacb":"code","442eb36e":"code","4a577046":"code","12a57cd8":"code","5859df9b":"code","0e517eb3":"code","4e7a135b":"code","eabc2b92":"code","d29d7806":"code","ba6b43f2":"code","7b7c14a2":"code","4406dce9":"code","a1d0f584":"code","e7894dca":"code","1e59de7c":"code","b7460c7e":"code","8fcc0361":"code","b2fb4d9a":"code","4d52f24f":"code","ec55c3ec":"code","dc3164fc":"code","b7efaa6d":"code","fac01333":"code","94c537b7":"code","fc88a46b":"code","3134b4cd":"code","59bd2584":"code","ab989812":"code","8b7f257e":"code","2b642608":"code","6e261a5d":"code","8d553f14":"code","f4d63fc4":"code","7107d88c":"code","996f0bd9":"code","e5667fcc":"code","a32bf39c":"code","01c7fbf3":"code","28a9b1cd":"code","242b370c":"markdown","d7b634a3":"markdown","73063e65":"markdown","f180556d":"markdown","2a3a563a":"markdown","6b524475":"markdown","6f3c1390":"markdown","519c0d29":"markdown","26a49874":"markdown","86b4a4b9":"markdown","ad117dbd":"markdown","b218e6bb":"markdown","1b1e893e":"markdown","bc50f408":"markdown","c7bf845b":"markdown","b7ef0f14":"markdown","261f13d9":"markdown","b089978c":"markdown","03e8c7bb":"markdown","10a99afc":"markdown","9f00a18e":"markdown","48d74907":"markdown","7d58068b":"markdown","f035f28f":"markdown","d0a839db":"markdown","3f822e13":"markdown","365b25de":"markdown","0e44334b":"markdown","6c1b7093":"markdown"},"source":{"c4b0753e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3c3e342":"#Setting the environment\nimport math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv(\"..\/input\/amsterdam-house-price-prediction\/HousingPrices-Amsterdam-August-2021.csv\")","6cf3b770":"data.head()","73aa7e6c":"data= data.drop('Unnamed: 0',axis=1)\ndata.head()","d958050c":"data.shape","13054b63":"data.info()","6222939d":"data.Room.value_counts().plot.bar()","db3a8396":"data.Zip.value_counts()","b21598ff":"data[data.Room > 10]","1e279472":"data['Location'] = data['Address'].apply(lambda x: str(x.split(',')[0]))\ndata.head()","0bebfd0c":"def name_extraction(string):\n    list = string.split()\n    word = []\n    number = [] \n    for i in list:\n        if i.isalpha() == True: \n            word.append(i)\n        else:\n            break\n    word = ' '.join(word)\n    return word","759918ec":"data['Street_Name'] = data['Location'].apply(lambda x: name_extraction(x))\ndata.head()","5b0c2051":"data.Street_Name.unique()","e396cfbb":"data.isnull().sum()","71f52cf0":"data['ZipNo'] = data['Zip'].apply(lambda x: int(x.split(' ')[0]))\ndata['AreaCode'] = data['Zip'].apply(lambda x: str(x.split(' ')[1]))\ndata.head()","b6a49688":"data= data.drop(['Address','Zip'],axis=1)\ndata.head()","4c19450a":"data= data.drop('Location',axis=1)","2f83697d":"data.describe()","e8325941":"plt.figure(figsize=(8,5))\nsns.boxplot('Price',data=data)","90f0c54f":"q1 = data.describe()['Price']['25%']\nq3 = data.describe()['Price']['75%']\niqr = q3 - q1\nmax_price = q3 + 1.5 * iqr","d15a38c7":"outli = data[data['Price'] >= max_price]\noutliers_count = outli['Price'].count()\ndata_count = data['Price'].count()\nprint('Percentage removed: ' + str(round(outliers_count\/data_count * 100, 2)) + '%')","44855bdc":"data= data[data['Price'] <= max_price]","30bd5056":"plt.figure(figsize=(8,5))\nsns.boxplot('Price',data=data);","e6107117":"sns.pairplot(data)","0809b41f":"sns.relplot(x=\"Area\", y=\"Price\", hue=\"Room\", data=data)\nplt.show()","52bdba7f":"plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(), annot = True)","6de70d9a":"data= data.dropna()","32754187":"y = data['Price']\nX = data.drop('Price', axis = 1)\n\nX.head()","beef88e5":"# Labelencoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod = ['Street_Name','AreaCode']\nfor i in var_mod:\n    X[i] = le.fit_transform(X[i])","b710928d":"X.head()","fd459638":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression","8fef444d":"# 25% data as validation set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)","c24d3e0a":"model = LinearRegression(normalize=True)\nmodel.fit(X_train,y_train)\n\npred= model.predict(X_test)","7c23ac71":"model.score(X_train, y_train)","6ceae0a0":"model.score(X_test, y_test)","39acfacb":"#Root Mean Square Error\nfrom sklearn import metrics\nimport math\nprint(math.sqrt(metrics.mean_squared_error(y_test, pred)))","442eb36e":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","4a577046":"algos = [LinearRegression(normalize=True),  Ridge(normalize=True), Lasso(normalize=True),\n          KNeighborsRegressor(), DecisionTreeRegressor(),RandomForestRegressor(),XGBRegressor()]\n\nnames = ['Linear Regression', 'Ridge Regression', 'Lasso Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor','Random Forest Regressor','XGBoost Regressor']\n\nrmse_list = []","12a57cd8":"for name in algos:\n    model = name\n    model.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    MSE= metrics.mean_squared_error(y_test,pred)\n    rmse = np.sqrt(MSE)\n    rmse_list.append(rmse)","5859df9b":"evaluation = pd.DataFrame({'Model': names,\n                           'RMSE': rmse_list})\n\nevaluation","0e517eb3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error","4e7a135b":"model= LinearRegression(normalize=True)\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nR2_scores = cross_val_score(model, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(model, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","eabc2b92":"R2_scores.mean(),mae","d29d7806":"Rd= Ridge(normalize=True)\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nR2_scores = cross_val_score(Rd, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(Rd, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","ba6b43f2":"R2_scores.mean(),mae","7b7c14a2":"KN= KNeighborsRegressor()\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nR2_scores = cross_val_score(KN, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(KN, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","4406dce9":"R2_scores.mean(),mae","a1d0f584":"Dtree= DecisionTreeRegressor()\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nR2_scores = cross_val_score(Dtree, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(Dtree, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","e7894dca":"R2_scores.mean(),mae","1e59de7c":"RF= RandomForestRegressor()\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nR2_scores = cross_val_score(RF, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(RF, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","b7460c7e":"R2_scores.mean(),mae","8fcc0361":"XGB= XGBRegressor()\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nR2_scores = cross_val_score(XGB, X_train, y_train, cv=kf,scoring = 'r2')\npred = cross_val_predict(XGB, X_train, y_train, cv=kf)\nmae= mean_absolute_error(y_train, pred)","b2fb4d9a":"R2_scores.mean(),mae","4d52f24f":"rmse_RFR=[]\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in kf.split(X,y):\n    xtr, xvl, ytr, yvl = X.iloc[train_index], X.iloc[test_index], \\\n                                       y.iloc[train_index], y.iloc[test_index]\n    model = RandomForestRegressor()\n    model.fit(xtr,ytr)\n    pred=  model.predict(xvl)\n    score= math.sqrt(metrics.mean_squared_error(yvl,pred))\n    rmse_RFR.append(score)","ec55c3ec":"np.average(rmse_RFR)","dc3164fc":"data.head()","b7efaa6d":"data['Price\/m\u00b2'] = data['Price']\/data['Area']\ndata['Price\/Room'] = data['Price']\/data['Room']\ndata['m\u00b2\/Room'] = data['Area']\/data['Room']\n\ndata.head()","fac01333":"plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),annot = True,vmax=.8,square=True,cmap=\"BuPu\")","94c537b7":"y = data['Price']\nX = data.drop('Price', axis = 1)\nX.head()","fc88a46b":"# Labelencoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod = ['Street_Name','AreaCode']\nfor i in var_mod:\n    X[i] = le.fit_transform(X[i])","3134b4cd":"rmse_RFR=[]\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in kf.split(X,y):\n    xtr, xvl, ytr, yvl = X.iloc[train_index], X.iloc[test_index], \\\n                                       y.iloc[train_index], y.iloc[test_index]\n    model = RandomForestRegressor()\n    model.fit(xtr,ytr)\n    pred=  model.predict(xvl)\n    score= math.sqrt(metrics.mean_squared_error(yvl,pred))\n    rmse_RFR.append(score)","59bd2584":"np.average(rmse_RFR)","ab989812":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [2,4]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","8b7f257e":"# Create the param grid\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(param_grid)","2b642608":"from sklearn.model_selection import GridSearchCV\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nmodel = RandomForestRegressor()\n\nclf = GridSearchCV(estimator = model, param_grid = param_grid, cv = kf, verbose=True, n_jobs = 4)","6e261a5d":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1)","8d553f14":"best_clf = clf.fit(X_train,y_train)","f4d63fc4":"clf.best_params_","7107d88c":"print (f'Train Accuracy - : {clf.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {clf.score(X_test,y_test):.3f}')","996f0bd9":"model= RandomForestRegressor(\n                        n_estimators= 56,max_depth=4,bootstrap=True,\n                        max_features='auto',min_samples_leaf=1,min_samples_split=2)","e5667fcc":"model.fit(X_train,y_train)\npred= model.predict(X_test)\nmodel.score(X_train, y_train)","a32bf39c":"model.score(X_test, y_test)","01c7fbf3":"print(math.sqrt(metrics.mean_squared_error(y_test, pred)))","28a9b1cd":"from sklearn.metrics import r2_score\n\nprint('Coefficient of determination: %.2f' % r2_score(y_test, pred))","242b370c":"The housing prices have been obtained from Pararius.nl as a snapshot in August 2021. We'll have to build a model that predicts the housing prices given the feature variables.\n\nTo begin with, we'll import the dataset and also load the necesssary packages.","d7b634a3":"# Final Thoughts","73063e65":"Based on our domain knowledge, we can come up with new features that might affect our target variable and thus place us at an even better position to make a better price prediction.","f180556d":"Let us understand the data first. We've been given Housing Price which is our target variable which needs to be predicted. The feature parameters are as follows.\n\n- Address: Residential address\n- Zip: Residential Zip code\n- Area: Residential area in square meters.\n- Room: Number of rooms at residence.\n- Lon: Longitude coordinates.\n- Lat: Latitude coordinates.","2a3a563a":"We need to convert the categorical features into numerical ones, before training the model with it. There are three ways we could achieve this.\n- Label Encoding\n- One Hot Encoding\n- Ordinal Encoding\n\nOne Hot Encoding or using pd.dummies will not be used as due to high cardinality it'll lead to too many feature variables which would render them ineffective.\nOrdinal Encoding is pretty much useless out here as there's no ranked ordering between values to preserve.\nThus, we'll use Label Encoding.","6b524475":"### Building a Linear Regression Model","6f3c1390":"# Cross Validation and Model Selection\n\nWe'll try different models to our dataset to find out their performance and RMSE. The algorithm giving us the least RMSE should be the one we'll be chosing for our model building.","519c0d29":"# Feature Engineering","26a49874":"We're not able to make much out of the address column as there exists high degree of cardinality. The same goes true for Zip Code. We'll try to reduce them down as far as possible.","86b4a4b9":"Thus, we've managed to pull out the street name from address column. High cardinality still exists but at least it is less that what we had before.","ad117dbd":"Doing the same with Zip codes...","b218e6bb":"We've managed to achieve \n- R-square of 91% \n& \n- Root mean Square Error (RMSE): 69813.926\n\nIt is important to note that this certainly isn't an exhaustive analysis of the dataset.\n\nDo let me know your thoughts on this project.","1b1e893e":"# Model Building: I","bc50f408":"The dataset mostly contains houses that have <6 rooms. This is understandable as finding houses having more than 6 rooms are very rare and will not be able to help us in our analysis.","c7bf845b":"# Model Building- II","b7ef0f14":"Here, we tried to figure Price, Area and Rooms and plot them to see if we find a relation. We find that houses that have a larger area and more number of rooms typically demand a higher price. The converse hold true as well.\n\nThis validates our hypothesis.","261f13d9":"We lost around 8% of the data, which isn't much.","b089978c":"![image.png](attachment:9d6bad3f-c285-427e-939c-8edbbae7e521.png)","03e8c7bb":"# Hyperparameter Tuning\n\nWe will try to improve the accuracy by tuning the hypermeters for this model. For that, we'll use **Grid Search** to get the optimised values of hypermeters. \n\nWe'll give the algorithm necessary options for the parameters to choose from and the algorithm will tune us the optimised set of parameter choices for the model.","10a99afc":"The model accuracy and RMSE doesn't look to promising. We'll need to make further improvements or try a different model for the same.","9f00a18e":"We find that Random Forest Regressor gives us a better overall performance on our data. Thus, we'll proceed with this algorithm.","48d74907":"Removing the null values..","7d58068b":"# Hypothesis Generation\n\nAfter looking at the problem statement, we'll now move into hypothesis generation. It is the process of listing out all the possible factors that can affect the target variable. So it is important to ask what are the factors that could generally determine our price variable. Some of the factors for the same can be listed as below.\n- Area (or Residential Area) of the house. Generally a house with a large area\/size would demand a higher price.\n- Room (or the number of rooms) in the house could be an important factor in predicting price. It wouldn't be unusual to assume that a 1 room house would cost less than a 2 room house keeping other factors constant.\n- Address(or location) of the house. Prime location houses are usually more expensive than the others. Also, houses that are at the heart of the city are more expensive compared to the ones in surburbs.\n\nThese certainly don't apply in all situations as there may be many other factors that are not taken in our dataset. But we'll not consider them as they are beyond the scope of this project. So we'll stick to these parameters and will try to validate each of these hypothesis based on the dataset.","f035f28f":"There seems to be presence of outliers in the Price column. Thus we need to treat these outliers first and then proceed with our task.","d0a839db":"We'll use a K-fold cross validation on different models. A diagramatic representation of K-fold has been given below.","3f822e13":"# Understanding the Data","365b25de":"We'll use train_test_split function from sklearn to divide our train dataset.\nWe'll build the model on training dataset and make predictions on the test dataset.","0e44334b":"# Problem Statement","6c1b7093":"Once we get the optimal parameters, we use them in our model and the find ist performance."}}