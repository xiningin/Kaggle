{"cell_type":{"765fab2d":"code","fd47e74a":"code","8336f28b":"code","728418c9":"code","35d2e830":"code","53929812":"code","fcabd0f0":"code","909a77fb":"code","fdfe6204":"code","629c29e7":"code","2c9b225d":"code","ffb41d93":"code","db99b48c":"code","9ad5429e":"code","452fe26f":"code","bd6ce037":"code","92e1f1ba":"code","2a8b23e4":"markdown","b120b4f3":"markdown","5e5fb78c":"markdown","e24bb0d6":"markdown","712a18a0":"markdown","dd64c389":"markdown","1c96d2f9":"markdown"},"source":{"765fab2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd47e74a":"train = pd.read_csv('..\/input\/coleridge-initiative-preprocessed-to-csv-data\/df_train_publications.csv')\ntest = pd.read_csv('..\/input\/coleridge-initiative-preprocessed-to-csv-data\/df_test_publications.csv')\nlabels = pd.read_csv('..\/input\/coleridge-initiative-preprocessed-to-csv-data\/labels.csv')\ntrain_meta = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\n\ndef concat(column):\n    res = ' '\n    for st in column:\n        if type(st) == str:\n            res += st\n    return res\n\ntrain_gr = train.groupby('pub_id')['text'].apply(concat).reset_index()\ntrain_gr.loc[train['pub_id'].isin(train_meta['Id']), 'cleaned_label'] = train_meta.loc[train_meta['Id'].isin(train_gr['pub_id']),'cleaned_label']","8336f28b":"# !python3 -m spacy download en_core_web_lg\nimport spacy\nsp_lg = spacy.load('en_core_web_lg')\n\ndocument = train_gr.loc[0, 'text']\nprint({(ent.text.strip(), ent.label_) for ent in sp_lg(document).ents})","728418c9":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\ncorpus = train_gr['text']\n\nX = vectorizer.fit_transform(corpus)","35d2e830":"X.shape","53929812":"vectorizer_label = CountVectorizer()\nlabels_corpus = train_gr['cleaned_label']\ny = vectorizer_label.fit_transform(labels_corpus)\n\ny.shape","fcabd0f0":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\nsvd.fit(X)\nX_tr = svd.transform(X)\n\nsvd_y = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\nsvd_y.fit(y)\ny_tr = svd_y.transform(y)","909a77fb":"X_tr","fdfe6204":"y_tr","629c29e7":"X_tr.shape","2c9b225d":"y_tr.shape","ffb41d93":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","db99b48c":"encoded_input = tokenizer(train_gr.loc[0, 'text'])\nprint(encoded_input)","9ad5429e":"import gensim\nw2v_model = gensim.models.Word2Vec(train_gr.loc[:10, 'text'].str.split().to_list(), size=100, min_count=1, window=5, iter=50)","452fe26f":"w2v_model.train(train_gr.loc[:10, 'text'].str.split().to_list(), total_examples=1, epochs=50)","bd6ce037":"train_gr.loc[0, 'text']","92e1f1ba":"w2v_model.wv['DOT', 'Lebanon\\'s' ,'ICT' ,'training', 'program']","2a8b23e4":"Here we have to decide what entities are important for us, for example, organizations, and go through the train_gr with the loop.","b120b4f3":"## NER Feature extraction","5e5fb78c":"## Some pre-trained tokenizers","e24bb0d6":"Also we can use, for example, TfIdf Vectorizer.","712a18a0":"We will use spacy pre-trained model to extract some features.","dd64c389":"## Count Vectorizer and dimensionality reduction","1c96d2f9":"## Gensim Word2Vec"}}