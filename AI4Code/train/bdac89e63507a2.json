{"cell_type":{"836cc9af":"code","1206a613":"code","e9b0b25b":"code","ba7aa23e":"code","d7abddde":"code","1e8a1b92":"code","0c22a6e4":"code","ce2d392e":"code","bf139f19":"code","55c300d9":"code","d7308750":"code","f3b61ecc":"code","89d715cb":"code","80fd43fb":"code","cf20c05a":"code","c9ec5f08":"code","44e73450":"code","2756d747":"code","4a97dc72":"code","18cff009":"code","6afd6f4f":"code","a0156341":"code","308143ec":"code","2899a136":"code","d80f7102":"code","24367ffb":"code","c8ec9041":"code","b2814105":"code","7bd31559":"code","f1322667":"code","67f7f3ed":"code","8b177b61":"code","edad9542":"code","b7e123e3":"code","a4f12acf":"code","c9011dc4":"code","7a7adeb6":"code","cdd4c33f":"code","2aa04b35":"code","2ba7ffcd":"code","10bf79c7":"code","d9ada8a6":"code","806299dc":"code","cdaf9dc8":"code","d551ed11":"code","32e2bdf6":"code","812f9493":"code","942669f1":"markdown","8e27be64":"markdown","b3a23b58":"markdown","ede0ca1f":"markdown","849d586a":"markdown","2f0eebf3":"markdown","e5d97e1a":"markdown","c109c34b":"markdown","bd5c9152":"markdown","d1dbf991":"markdown","cac70c94":"markdown","73224c95":"markdown","d81e2c37":"markdown","4f335f47":"markdown","c840830e":"markdown","8e3fc473":"markdown","c810fd65":"markdown"},"source":{"836cc9af":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import preprocessing,model_selection","1206a613":"#reading data into dfs\ndata = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\ndata.head()","e9b0b25b":"data.info()\n","ba7aa23e":"print(data.isnull().sum())","d7abddde":"columns = ['city','gender','relevent_experience','enrolled_university','education_level','major_discipline','experience','last_new_job','company_size','company_type']\nfor col in columns:\n  cont = pd.crosstab(data['target'],data[col])\n  chi_val = stats.chi2_contingency(cont)\n  print('p-value for:',col,chi_val[1])","1e8a1b92":"data = data.drop(['major_discipline','company_size'],axis = 'columns')\ndata_test = data_test.drop(['major_discipline','company_size'],axis = 'columns')\n","0c22a6e4":"print(data.isnull().sum())","ce2d392e":"data['experience'].fillna(\"1\",inplace = True)\ndata_test['experience'].fillna(\"1\",inplace = True)","bf139f19":"data['gender'].fillna('Male',inplace = True)\ndummies_gender = pd.get_dummies(data['gender'])\n\ndata_test['gender'].fillna('Male',inplace = True)\ndummies_gender_test = pd.get_dummies(data_test['gender'])\n","55c300d9":"data['relevent_experience'].fillna('Has relevent experience',inplace = True)\ndummies_relexp = pd.get_dummies(data['relevent_experience'])\n\ndata_test['relevent_experience'].fillna('Has relevent experience',inplace = True)\ndummies_relexp_test = pd.get_dummies(data_test['relevent_experience'])\n\n","d7308750":"data['enrolled_university'].fillna('no_enrollment',inplace = True)\ndummies_enruniv = pd.get_dummies(data['enrolled_university'])\n\ndata_test['enrolled_university'].fillna('no_enrollment',inplace = True)\ndummies_enruniv_test = pd.get_dummies(data_test['enrolled_university'])","f3b61ecc":"data['education_level'].fillna('Graduate',inplace = True)\ndummies_edlevel = pd.get_dummies(data['education_level'])\n\ndata_test['education_level'].fillna('Graduate',inplace = True)\ndummies_edlevel_test = pd.get_dummies(data_test['education_level'])","89d715cb":"data['last_new_job'].fillna(1,inplace = True)\ndummies_lastjob = pd.get_dummies(data['last_new_job'],prefix = 'last_')\n\ndata_test['last_new_job'].fillna(1,inplace = True)\ndummies_lastjob_test = pd.get_dummies(data_test['last_new_job'],prefix = 'last_')","80fd43fb":"data['experience'].replace({\"1\":1,\"2\":1,\"3\":0,\"4\":0,\"19\":0,\"20\":0,\">20\":0,\"<1\":1,\"5\":0,\"6\":0,\"7\":0,\"8\":0,\"9\":0,\"10\":0,\"11\":0,\"12\":0,\"13\":0,\"14\":0,\"15\":0,\"16\":0,\"17\":0,\"18\":0},inplace = True)\ndata_test['experience'].replace({\"1\":1,\"2\":1,\"3\":0,\"4\":0,\"19\":0,\"20\":0,\">20\":0,\"<1\":1,\"5\":0,\"6\":0,\"7\":0,\"8\":0,\"9\":0,\"10\":0,\"11\":0,\"12\":0,\"13\":0,\"14\":0,\"15\":0,\"16\":0,\"17\":0,\"18\":0},inplace = True)\n","cf20c05a":"data['company_type'].fillna('Pvt Ltd',inplace = True)\ndummies_ctype = pd.get_dummies(data['company_type'])\n\ndata_test['company_type'].fillna('Pvt Ltd',inplace = True)\ndummies_ctype_test = pd.get_dummies(data_test['company_type'])","c9ec5f08":"merged = pd.concat([data,dummies_gender,dummies_relexp,dummies_enruniv,dummies_edlevel,dummies_ctype,dummies_lastjob],axis = 'columns')\ndata = merged.drop(['city','gender','relevent_experience','enrolled_university','education_level','company_type','last_new_job'],axis = 'columns')\n\nmerged_test = pd.concat([data_test,dummies_gender_test,dummies_relexp_test,dummies_enruniv_test,dummies_edlevel_test,dummies_ctype_test,dummies_lastjob_test],axis = 'columns')\ndata_test = merged_test.drop(['city','gender','relevent_experience','enrolled_university','education_level','company_type','last_new_job'],axis = 'columns')\n\n","44e73450":"data.head()","2756d747":"data_test.head()","4a97dc72":"\ncol_list = data.columns[1:]\ncol_list_test = data_test.columns[1:]\n\ncol = col_list.to_list()\ncol_test = col_list_test.to_list()\nprint(col,col_test)","18cff009":"\nscaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndata[col] = scaler.fit_transform(data[col])\ndata = pd.DataFrame(data)\ncol.insert(0,'enrollee_id')\ndata.columns = col\n\ndata_test[col_test] = scaler.fit_transform(data_test[col_test])\ndata_test = pd.DataFrame(data_test)\ncol_test.insert(0,'enrollee_id')\ndata_test.columns = col_test","6afd6f4f":"data.head()","a0156341":"print(\"target data:\",data['target'].value_counts())\n","308143ec":"from sklearn.utils import resample\ndf_majority = data[data.target==0]\ndf_minority = data[data.target==1]\nprint(df_majority,df_minority)\n# Upsample minority class\n\n ","2899a136":"df_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=15934,    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndata = pd.concat([df_majority, df_minority_upsampled])\ndata.info()","d80f7102":"from sklearn.model_selection import train_test_split\nx = data.drop(['enrollee_id','target','last__1'], axis = 'columns')\n\ny = data['target']\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2,random_state = 42,stratify = y)\n\nprint(x_train.shape, y_train.shape)\nprint(x_val.shape, y_val.shape)","24367ffb":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nlr = LogisticRegression(C=0.001,random_state = 0)\nlr.fit(x_train,y_train)\n\nlr_probs = lr.predict_proba(x_val)\nlr_probs = lr_probs[:, 1]\nlr_auc = roc_auc_score(y_val, lr_probs)\n\nlr_auc\n","c8ec9041":"lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_probs)\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","b2814105":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 50, min_samples_leaf = 1,random_state = 0)\nrf.fit(x_train,y_train)\n\nrf_probs = rf.predict_proba(x_val)\nrf_probs = rf_probs[:, 1]\nrf_auc = roc_auc_score(y_val, rf_probs)\n\nrf_auc","7bd31559":"rf_fpr, rf_tpr, _ = roc_curve(y_val, rf_probs)\nplt.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","f1322667":"#Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\ndt_probs = dt.predict_proba(x_val)\ndt_probs = dt_probs[:, 1]\ndt_auc = roc_auc_score(y_val, dt_probs)\n\ndt_auc","67f7f3ed":"dt_fpr, dt_tpr, _ = roc_curve(y_val, dt_probs)\nplt.plot(dt_fpr, dt_tpr, marker='.', label='Decision Tree')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","8b177b61":"from sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB(binarize=0.55)\nbnb.fit(x_train,y_train)\nbnb_probs = bnb.predict_proba(x_val)\nbnb_probs = bnb_probs[:, 1]\nbnb_auc = roc_auc_score(y_val, bnb_probs)\n\nbnb_auc","edad9542":"bnb_fpr, bnb_tpr, _ = roc_curve(y_val, bnb_probs)\nplt.plot(bnb_fpr, bnb_tpr, marker='.', label='Bernoulli')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","b7e123e3":"from sklearn.svm import SVC\nsvc = SVC(probability=True,tol = 0.01)\nsvc.fit(x_train,y_train)\nsvc_probs = svc.predict_proba(x_val)\nsvc_probs = svc_probs[:, 1]\nsvc_auc = roc_auc_score(y_val, svc_probs)\n\nsvc_auc","a4f12acf":"svc_fpr, svc_tpr, _ = roc_curve(y_val, svc_probs)\nplt.plot(svc_fpr, svc_tpr, marker='.', label='Support Vector')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","c9011dc4":"from sklearn.neural_network import MLPClassifier\np = MLPClassifier(random_state=42,\n              max_iter=100,tol = 0.0001)\np.fit(x_train,y_train)\np_probs = p.predict_proba(x_val)\np_probs = p_probs[:, 1]\np_auc = roc_auc_score(y_val, p_probs)\n\np_auc","7a7adeb6":"p_fpr, p_tpr, _ = roc_curve(y_val, p_probs)\nplt.plot(p_fpr, p_tpr, marker='.', label='Perceptron')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","cdd4c33f":"auc_scores = [lr_auc,rf_auc,bnb_auc,svc_auc,dt_auc,p_auc]\nmodels =['Logistic Regression','Random Forest','Bernoulli NaiveBayes','Support Vector','Decision Tree','MultiLayerPerceptron']\nprint(\"=====================ROC_AUC Scores=========================\")\nfor score,model  in zip(auc_scores,models):\n  print(model,\": \",score)","2aa04b35":"data_test.head()","2ba7ffcd":"x_test = data_test.drop(['enrollee_id','last__1'], axis = 'columns')","10bf79c7":"pred_lr = lr.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_lr, return_counts=True)\nprint(unique_elements,counts_elements)","d9ada8a6":"pred_rf = rf.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_rf, return_counts=True)\nprint(unique_elements,counts_elements)","806299dc":"pred_dt = dt.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_dt, return_counts=True)\nprint(unique_elements,counts_elements)","cdaf9dc8":"pred_svc = svc.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_svc, return_counts=True)\nprint(unique_elements,counts_elements)","d551ed11":"pred_bnb = bnb.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_bnb, return_counts=True)\nprint(unique_elements,counts_elements)","32e2bdf6":"pred_p = p.predict(x_test)\nunique_elements, counts_elements = np.unique(pred_p, return_counts=True)\nprint(unique_elements,counts_elements)","812f9493":"auc_scores = [0.5957,0.5162,0.5950,0.6008,0.5168,0.6133]\nmodels =['Logistic Regression','Random Forest','Bernoulli NaiveBayes','Support Vector','Decision Tree','MultiLayerPerceptron']\nprint(\"=====================Final Scores=========================\")\nfor score,model  in zip(auc_scores,models):\n  print(model,\": \",score)","942669f1":"*We will be imputing the values using for different features with different methods.These are:*\n\n*   Imputing by mode\n*   Imputing by creating categorical bundles and assigning numerical values\n\nWe will also be performing one-hot encoding to convert the categoricl features into numerical\n","8e27be64":"**As we assumed, the models with abnormally high score training(Random Forest,Decision Tree) performed the worst on test set. This is because of overfitting.**\n\nHere is a link explaining what is overfitting and how it can be reduced:\n[Overfit and Underfit in ML](https:\/\/towardsdatascience.com\/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690)","b3a23b58":"*Since the parameter on which scores will be decided is roc_auc_score we will also rate the models on the same*","ede0ca1f":"# Implementing Machine Learning models","849d586a":"\n**Observations**\n\n\n*   Our label will be 'Target'\n*   We have 10 categorical features which will be needed to converted into numerical through encoding.\n\n\n*   Lots of columns have null values as well. We need to impute the values.\n\n\n\n\n\n\n","2f0eebf3":"*RandomForest and DecisionTree have exceptionally high scores, this maybe because of overfitting*\n\nLet us now predict our target and consolidate the final scores","e5d97e1a":"**Now that our analysis is complete and dataset is balanced, let us move to the next step**","c109c34b":"*Clearly there is a lot of imbalance between categories, which may cause biasing in the models. There is a need to balance the dataset.\nWe will use upsampling of minority class*","bd5c9152":"# Predicting Test dataset\n","d1dbf991":"# Data Analysis\n\n**In this section we will take a look at train and test data, find relations between features and labels,handle missing values and try to create a balanced dataset**","cac70c94":"**Let's consolidate the  scores of each model**","73224c95":"# Problem Statement\nA training institute which conducts training for analytics\/ data science wants to expand their business to manpower recruitment (data science only) as well. \n \nCompany gets large number of signups for their trainings. Now, company wants to connect these enrollees with their clients who are looking to hire employees working in the same domain. Before that, it is important to know which of these candidates are really looking for a new employment. They have student information related to demographics, education, experience and features related to training as well.\n\n------------------------------------------------------------------------\n\n# AIM\n\nWe need to design a model that uses the current credentials\/demographics\/experience to predict the probability of an enrollee to look for a new job.\n\n------------------------------------------------------------------------\n\nThis notebook is structured as follows:\n\n1. **Data Analysis** : In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other. \n2. **Feature Engineering,Categorical Encoding and Data Balancing** : Conduct some feature engineering as well as encode all our categorical features into dummy variables\n3. **Implementing Machine Learning models** : Implement machine learning models for predictions","d81e2c37":"**Let us look for imbalance in our dataset. We will calculate the unique count of label categories in the dataset **","4f335f47":"\n **Gender, enrolled_university, education_level,experience,company_type ,last_new_job has missing values**","c840830e":"*Considering confidence interval to be 0.1; we can see major_discipline and company_size does not make for a good candidate.Also they have a lot of NULL values. Hence we will be dropping them.*","8e3fc473":"#Feature Engineering,Categorical Encoding and Data Balancing\n\n\n\n","c810fd65":"**Let us now perform Chi-square to see relationship of categorical features with label.We will take the p-value as the estimate**"}}