{"cell_type":{"79e22ab5":"code","81223b99":"code","bdfa7df1":"code","35cc7f9d":"code","e6c78cb3":"code","0cf7391b":"code","3306a0fa":"code","abdad4e3":"code","dbfb0904":"code","09fa410f":"code","91eb1f33":"code","0da70b23":"code","17de3120":"code","9e1539bf":"code","057fe214":"code","fdf572d3":"code","7fd4ab87":"code","9a76c5ae":"code","dc905fc8":"code","dd6be5bf":"markdown","192858a7":"markdown","70d20a4e":"markdown","92e724bd":"markdown","13955a01":"markdown","b3b22686":"markdown","116b65d3":"markdown","c03cd3c8":"markdown","bb809fb7":"markdown","3628b84b":"markdown","eeb7c53d":"markdown","05dea764":"markdown","87853318":"markdown","65f2414b":"markdown","cce1b91a":"markdown","55c94c06":"markdown","f8766acf":"markdown","beeecfdf":"markdown","25514ab8":"markdown","b1d2b75d":"markdown","1cabda6a":"markdown","6af3f721":"markdown","d7a32c1d":"markdown","96cc9c50":"markdown","8fffefbc":"markdown","d2e8debd":"markdown","6c444962":"markdown","df13881d":"markdown","430e730b":"markdown"},"source":{"79e22ab5":"## importing packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport pycountry\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column, row\nfrom bokeh.models import LinearAxis\nfrom bokeh.palettes import Spectral11\nfrom bokeh.plotting import figure\nfrom bokeh.models.ranges import Range1d\n\nfrom plotly import graph_objects as go\n\noutput_notebook()\n\n## reading data\ndf = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\", skiprows=[1])\ndf_2018 = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2018\/freeFormResponses.csv\", skiprows=[1])\n\n## creating numeric feature for salary\ndict_salary = dict({\"$0-999\": 500, \"1,000-1,999\": 1500, \"2,000-2,999\": 2500,\n                    \"3,000-3,999\": 3500, \"4,000-4,999\": 4500, \"5,000-7,499\": 6250,\n                    \"7,500-9,999\": 8750, \"10,000-14,999\": 12500, \"15,000-19,999\": 17500,\n                    \"20,000-24,999\": 22500, \"25,000-29,999\": 27500, \"30,000-39,999\": 35000,\n                    \"40,000-49,999\": 45000, \"50,000-59,999\": 55000, \"60,000-69,999\": 65000,\n                    \"70,000-79,999\": 75000, \"80,000-89,999\": 85000, \"90,000-99,999\": 95000,\n                    \"100,000-124,999\": 112500, \"125,000-149,999\": 137500, \"150,000-199,999\": 175000,\n                    \"200,000-249,999\": 225000, \"250,000-299,999\": 275000, \"300,000-500,000\": 400000,\n                    \"> $500,000\": 500000})\ndf[\"salary\"] = df.Q10.map(dict_salary)\n\n## creating numeric feature for expense\ndict_expense = dict({\"$0 (USD)\": 0, \"$1-$99\": 50, \"$100-$999\": 550, \"$1000-$9,999\": 5500,\n                     \"$10,000-$99,999\": 55000, \"> $100,000 ($USD)\": 100000})\ndf[\"expense\"] = df.Q11.map(dict_expense)\n\n## tagging practitioner types\ndf[\"practitioner_type\"] = \"Non-DL Practitioner\"\ndf.loc[~(df.Q24_Part_6.isna() & df.Q24_Part_7.isna() & df.Q24_Part_8.isna() & df.Q24_Part_9.isna() & df.Q24_Part_10.isna()), \"practitioner_type\"] = \"DL Practitioner\"\ndf.loc[(df.Q24_Part_1.isna() & df.Q24_Part_2.isna() & df.Q24_Part_3.isna() & df.Q24_Part_4.isna() & df.Q24_Part_5.isna() &\n        df.Q24_Part_6.isna() & df.Q24_Part_7.isna() & df.Q24_Part_8.isna() & df.Q24_Part_9.isna() & df.Q24_Part_10.isna()), \"practitioner_type\"] = \"Unknown\"\n\n## splitting dataset by type\ndf_dl = df[df.practitioner_type == \"DL Practitioner\"]\ndf_nondl = df[df.practitioner_type == \"Non-DL Practitioner\"]\ndf_dl_nondl = df[df.practitioner_type != \"Unknown\"]\n","81223b99":"v = figure(plot_width = 700, plot_height = 300, x_range = np.unique(df.practitioner_type.values), title = \"Practitioner Distribution\")\nv.vbar(x = np.unique(df.practitioner_type.values), top = df.practitioner_type.value_counts().sort_index().values, width = 0.9, color = Spectral11[1], legend_label = \"# Participants\")\nv.legend.location = \"top_center\"\nv.legend.click_policy = \"hide\"\n\nshow(v)\n","bdfa7df1":"v = figure(plot_width = 700, plot_height = 300, x_range = np.unique(df_dl.Q1.values), title = \"Age Distribution\")\nv.vbar(x = np.unique(df_dl.Q1.values), top = df_dl.Q1.value_counts().sort_index().values, width = 0.9, color = Spectral11[1], legend_label = \"# DL Practitioners\")\nv.line(np.unique(df_dl.Q1.values), df_dl.Q1.value_counts().sort_index().values * 100 \/ df_dl_nondl.Q1.value_counts().sort_index().values, color = Spectral11[10], legend_label = \"% DL Practitioners\", y_range_name=\"Percentages\")\nv.extra_y_ranges = {\"Percentages\": Range1d(start = 20, end = 80)}\nv.add_layout(LinearAxis(y_range_name = \"Percentages\"), \"right\")\nv.legend.location = \"top_right\"\nv.legend.click_policy = \"hide\"\n\nshow(v)\n","35cc7f9d":"v = figure(plot_width = 700, plot_height = 400, x_range = np.unique(df_dl.Q4.values), title = \"Education Distribution\")\nv.vbar(x = np.unique(df_dl.Q4.values), top = df_dl.Q4.value_counts().sort_index().values, width = 0.9, color = Spectral11[1], legend_label = \"# DL Practitioners\")\nv.line(np.unique(df_dl.Q4.values), df_dl.Q4.value_counts().sort_index().values * 100 \/ df_dl_nondl.Q4.value_counts().sort_index().values, color = Spectral11[10], legend_label = \"% DL Practitioners\", y_range_name=\"Percentages\")\nv.extra_y_ranges = {\"Percentages\": Range1d(start = 20, end = 80)}\nv.add_layout(LinearAxis(y_range_name = \"Percentages\"), \"right\")\nv.legend.location = \"top_right\"\nv.legend.click_policy = \"hide\"\nv.xaxis.major_label_orientation = 145\n\nshow(v)\n","e6c78cb3":"v = figure(plot_width = 700, plot_height = 300, x_range = np.unique(df_dl.Q2.values), title = \"Gender Distribution\")\nv.vbar(x = np.unique(df_dl.Q2.values), top = df_dl.Q2.value_counts().sort_index().values, width = 0.9, color = Spectral11[8], legend_label = \"# DL Practitioners\")\nv.line(np.unique(df_dl.Q2.values), df_dl.Q2.value_counts().sort_index().values * 100 \/ df_dl_nondl.Q2.value_counts().sort_index().values, color = Spectral11[1], legend_label = \"% DL Practitioners\", y_range_name=\"Percentages\")\nv.extra_y_ranges = {\"Percentages\": Range1d(start = 20, end = 80)}\nv.add_layout(LinearAxis(y_range_name = \"Percentages\"), \"right\")\nv.legend.location = \"top_right\"\nv.legend.click_policy = \"hide\"\n\nshow(v)\n","0cf7391b":"## mapping country codes\ndef get_country_code(country_name):\n    \"\"\"\n    Mapping country name to 3-digit country code.\n    \"\"\"\n    \n    if country_name == \"Russia\":\n        country_name = \"Russian Federation\"\n    if country_name == \"South Korea\":\n        country_name = \"Korea, Republic of\"\n    if country_name == \"Hong Kong (S.A.R.)\":\n        country_name = \"Hong Kong\"\n    if country_name == \"Taiwan\":\n        country_name = \"Taiwan, Province of China\"    \n    if country_name == \"Republic of Korea\":\n        country_name = \"Democratic People's Republic of Korea\"\n    if country_name == \"Iran, Islamic Republic of...\":\n        country_name = \"Iran, Islamic Republic of\"\n    \n    country_data = pycountry.countries.get(name=country_name)\n    \n    if country_data is None:\n        country_data = pycountry.countries.get(official_name=country_name)\n    \n    if country_data is None:\n        return np.nan\n    return country_data.alpha_3\n\ndf_dl_country = pd.DataFrame(df_dl.Q3.value_counts()).reset_index().rename(columns={\"index\": \"country\", \"Q3\": \"dl_count\"})\ndf_dl_country[\"country_code\"] = df_dl_country.country.apply(lambda x: get_country_code(x))\n\nf = go.Figure(data=go.Choropleth(\n    locations=df_dl_country.country_code,\n    z=df_dl_country.dl_count,\n    locationmode=\"ISO-3\",\n    text=df_dl_country.country,\n    colorscale=\"Blues\",\n    autocolorscale=False,\n    marker_line_width=0.5,\n    colorbar_tickprefix=\"#\",\n    colorbar_title=\"# DL Practitioners\"\n))\n\nf.update_layout(\n    title={\n        \"text\": \"Global # DL Practitioners\",\n        \"y\":0.9,\n        \"x\":0.475,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\"}\n)\n\nf.show()","3306a0fa":"df_dl_country.sort_values(\"dl_count\", ascending=False).head(10)","abdad4e3":"df_dl_nondl_country = pd.DataFrame(df_dl_nondl.Q3.value_counts()).reset_index().rename(columns={\"index\": \"country\", \"Q3\": \"dl_nondl_count\"})\ndf_dl_nondl_country[\"country_code\"] = df_dl_nondl_country.country.apply(lambda x: get_country_code(x))\ndf_dl_nondl_country = df_dl_nondl_country.merge(df_dl_country[[\"country_code\", \"dl_count\"]], how=\"left\", on=\"country_code\")\ndf_dl_nondl_country[\"dl_percentage\"] = round(df_dl_nondl_country.dl_count * 100 \/ df_dl_nondl_country.dl_nondl_count)\n\nf = go.Figure(data=go.Choropleth(\n    locations=df_dl_nondl_country.country_code,\n    z=df_dl_nondl_country.dl_percentage,\n    locationmode=\"ISO-3\",\n    text=df_dl_nondl_country.country,\n    colorscale=\"Blues\",\n    autocolorscale=False,\n    marker_line_width=0.5,\n    colorbar_ticksuffix=\"%\",\n    colorbar_title=\"% DL Practitioners\"\n))\n\nf.update_layout(\n    title={\n        \"text\": \"Global % DL Practitioners\",\n        \"y\":0.9,\n        \"x\":0.475,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\"}\n)\n\nf.show()","dbfb0904":"df_dl_nondl_country.sort_values(\"dl_percentage\", ascending=False).head(10)","09fa410f":"df_dl_role = pd.DataFrame(df_dl.Q5.value_counts()).reset_index().rename(columns={\"index\": \"Role\", \"Q5\": \"dl_count\"})\ndf_dl_nondl_role = pd.DataFrame(df_dl_nondl.Q5.value_counts()).reset_index().rename(columns={\"index\": \"Role\", \"Q5\": \"dl_nondl_count\"})\n\ndf_dl_role = df_dl_role.merge(df_dl_nondl_role)\ndf_dl_role[\"dl_percentage\"] = df_dl_role.dl_count * 100 \/ df_dl_role.dl_nondl_count\ndf_dl_role.sort_values(\"dl_percentage\", ascending=False, inplace=True)\n\nf = figure(x_range=df_dl_role.Role.values, plot_width=700, plot_height=300, title=\"Role Distribution\")\nf.vbar(x=df_dl_role.Role.values, top=df_dl_role.dl_count.values, width=0.9, color=Spectral11[9], legend_label=\"# DL Practitioners\")\nf.line(df_dl_role.Role.values, df_dl_role.dl_percentage.values, color=Spectral11[1], legend_label=\"% DL Practitioners\", y_range_name=\"Percentages\")\nf.extra_y_ranges = {\"Percentages\": Range1d(start=20, end=80)}\nf.add_layout(LinearAxis(y_range_name=\"Percentages\"), \"right\")\nf.legend.location=\"top_right\"\nf.legend.click_policy=\"hide\"\nf.xaxis.major_label_orientation=145\nshow(f)","91eb1f33":"import seaborn as sns\nf = sns.FacetGrid(df_dl_nondl, col=\"practitioner_type\")\nf.map(plt.hist, \"salary\")\nf.add_legend()\nplt.show()","0da70b23":"bp = sns.boxplot(x=\"practitioner_type\", y=\"salary\", data=df_dl_nondl, palette=\"Set2\").set_title(\"Salary Distribution\")","17de3120":"import seaborn as sns\nf = sns.FacetGrid(df_dl_nondl, col=\"practitioner_type\")\nf.map(plt.hist, \"expense\")\nf.add_legend()\nplt.show()","9e1539bf":"bp = sns.boxplot(x=\"practitioner_type\", y=\"expense\", data=df_dl_nondl, palette=\"Set3\").set_title(\"Expense Distribution\")","057fe214":"media_list = {\n    \"Twitter\": sum(~df_dl.Q12_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"HackerNews\": sum(~df_dl.Q12_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"Reddit\": sum(~df_dl.Q12_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"Kaggle\": sum(~df_dl.Q12_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"Forums\": sum(~df_dl.Q12_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"YouTube\": sum(~df_dl.Q12_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Podcasts\": sum(~df_dl.Q12_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"Blogs\": sum(~df_dl.Q12_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"Journals\": sum(~df_dl.Q12_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"Slack\": sum(~df_dl.Q12_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q12_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q12_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_media = pd.DataFrame.from_dict(media_list, orient=\"index\", columns=[\"media_percentage\"]).reset_index().rename(columns={\"index\": \"media\"})\ndf_dl_media.sort_values(\"media_percentage\", ascending=False, inplace=True)\n\nplatform_list = {\n    \"Udacity\": sum(~df_dl.Q13_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"Coursera\": sum(~df_dl.Q13_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"edX\": sum(~df_dl.Q13_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"DataCamp\": sum(~df_dl.Q13_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"DataQuest\": sum(~df_dl.Q13_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"Kaggle\": sum(~df_dl.Q13_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Fast.ai\": sum(~df_dl.Q13_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"Udemy\": sum(~df_dl.Q13_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"LinkedIn\": sum(~df_dl.Q13_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"University\": sum(~df_dl.Q13_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q13_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q13_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_platform = pd.DataFrame.from_dict(platform_list, orient=\"index\", columns=[\"platform_percentage\"]).reset_index().rename(columns={\"index\": \"platform\"})\ndf_dl_platform.sort_values(\"platform_percentage\", ascending=False, inplace=True)\n\nf1 = figure(x_range=df_dl_media.media, plot_width=700, plot_height=400, title=\"Media Distribution\")\nf1.vbar(x=df_dl_media.media, top=df_dl_media.media_percentage, width=0.9, color=Spectral11[6], legend_label=\"% Media Usage\")\nf1.legend.location=\"top_right\"\nf1.legend.click_policy=\"hide\"\nf1.xaxis.major_label_orientation=145\n\nf2 = figure(x_range=df_dl_platform.platform, plot_width=700, plot_height=400, title=\"Platform Distribution\")\nf2.vbar(x=df_dl_platform.platform, top=df_dl_platform.platform_percentage, width=0.9, color=Spectral11[6], legend_label=\"% Platform Usage\")\nf2.legend.location=\"top_right\"\nf2.legend.click_policy=\"hide\"\nf2.xaxis.major_label_orientation=145\n\nshow(column(f1, f2))","fdf572d3":"ide_list = {\n    \"Jupyter\": sum(~df_dl.Q16_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"RStudio\": sum(~df_dl.Q16_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"PyCharm\": sum(~df_dl.Q16_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"Atom\": sum(~df_dl.Q16_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"MATLAB\": sum(~df_dl.Q16_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"VisualStudio\": sum(~df_dl.Q16_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Spyder\": sum(~df_dl.Q16_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"Vim\": sum(~df_dl.Q16_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"Notepad++\": sum(~df_dl.Q16_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"SublimeText\": sum(~df_dl.Q16_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q16_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q16_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_ide = pd.DataFrame.from_dict(ide_list, orient=\"index\", columns=[\"ide_percentage\"]).reset_index().rename(columns={\"index\": \"ide\"})\ndf_dl_ide.sort_values(\"ide_percentage\", ascending=False, inplace=True)\n\nnotebook_list = {\n    \"KaggleNotebooks\": sum(~df_dl.Q17_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"GoogleColab\": sum(~df_dl.Q17_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"MicrosoftAzureNotebooks\": sum(~df_dl.Q17_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"GoogleCloudNotebooks\": sum(~df_dl.Q17_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"Paperspace\/Gradient\": sum(~df_dl.Q17_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"FloydHub\": sum(~df_dl.Q17_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Binder\/JupyterHub\": sum(~df_dl.Q17_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"IBMWatsonStudio\": sum(~df_dl.Q17_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"CodeOcean\": sum(~df_dl.Q17_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"AWSNotebooks\": sum(~df_dl.Q17_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q17_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q17_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_notebook = pd.DataFrame.from_dict(notebook_list, orient=\"index\", columns=[\"notebook_percentage\"]).reset_index().rename(columns={\"index\": \"notebook\"})\ndf_dl_notebook.sort_values(\"notebook_percentage\", ascending=False, inplace=True)\n\nlanguage_list = {\n    \"Python\": sum(~df_dl.Q18_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"R\": sum(~df_dl.Q18_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"SQL\": sum(~df_dl.Q18_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"C\": sum(~df_dl.Q18_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"C++\": sum(~df_dl.Q18_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"Java\": sum(~df_dl.Q18_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Javascript\": sum(~df_dl.Q18_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"TypeScript\": sum(~df_dl.Q18_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"Bash\": sum(~df_dl.Q18_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"MATLAB\": sum(~df_dl.Q18_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q18_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q18_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_language = pd.DataFrame.from_dict(language_list, orient=\"index\", columns=[\"language_percentage\"]).reset_index().rename(columns={\"index\": \"language\"})\ndf_dl_language.sort_values(\"language_percentage\", ascending=False, inplace=True)\n\nf1 = figure(x_range=df_dl_ide.ide, plot_width=700, plot_height=400, title=\"IDE Distribution\")\nf1.vbar(x=df_dl_ide.ide, top=df_dl_ide.ide_percentage, width=0.9, color=Spectral11[3], legend_label=\"% IDE Usage\")\nf1.legend.location=\"top_right\"\nf1.legend.click_policy=\"hide\"\nf1.xaxis.major_label_orientation=145\n\nf2 = figure(x_range=df_dl_notebook.notebook, plot_width=700, plot_height=400, title=\"Notebook Distribution\")\nf2.vbar(x=df_dl_notebook.notebook, top=df_dl_notebook.notebook_percentage, width=0.9, color=Spectral11[3], legend_label=\"% Notebook Usage\")\nf2.legend.location=\"top_right\"\nf2.legend.click_policy=\"hide\"\nf2.xaxis.major_label_orientation=145\n\nf3 = figure(x_range=df_dl_language.language, plot_width=700, plot_height=400, title=\"Language Distribution\")\nf3.vbar(x=df_dl_language.language, top=df_dl_language.language_percentage, width=0.9, color=Spectral11[3], legend_label=\"% Language Usage\")\nf3.legend.location=\"top_right\"\nf3.legend.click_policy=\"hide\"\nf3.xaxis.major_label_orientation=145\n\nshow(column(f1, f2, f3))","7fd4ab87":"hardware_list = {\n    \"CPUs\": sum(~df_dl.Q21_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"GPUs\": sum(~df_dl.Q21_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"TPUs\": sum(~df_dl.Q21_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q21_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q21_Part_5.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_hardware = pd.DataFrame.from_dict(hardware_list, orient=\"index\", columns=[\"hardware_percentage\"]).reset_index().rename(columns={\"index\": \"hardware\"})\ndf_dl_hardware.sort_values(\"hardware_percentage\", ascending=False, inplace=True)\n\nmodel_list = {\n    \"DenseNN\": sum(~df_dl.Q24_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"CNN\": sum(~df_dl.Q24_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"GAN\": sum(~df_dl.Q24_Part_8.isna()) * 100 \/ df_dl.shape[0],\n    \"RNN\": sum(~df_dl.Q24_Part_9.isna()) * 100 \/ df_dl.shape[0],\n    \"TransformerNetworks\": sum(~df_dl.Q24_Part_10.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q24_Part_11.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q24_Part_12.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_model = pd.DataFrame.from_dict(model_list, orient=\"index\", columns=[\"model_percentage\"]).reset_index().rename(columns={\"index\": \"model\"})\ndf_dl_model.sort_values(\"model_percentage\", ascending=False, inplace=True)\n\ntool_list = {\n    \"AutoAugmentation\": sum(~df_dl.Q25_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"AutoFeatureSelection\": sum(~df_dl.Q25_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"AutoModelSelection\": sum(~df_dl.Q25_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"AutoModelArchitectureSearch\": sum(~df_dl.Q25_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"AutoHyperparameterTuning\": sum(~df_dl.Q25_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"AutoML\": sum(~df_dl.Q25_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q25_Part_7.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q25_Part_8.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_tool = pd.DataFrame.from_dict(tool_list, orient=\"index\", columns=[\"tool_percentage\"]).reset_index().rename(columns={\"index\": \"tool\"})\ndf_dl_tool.sort_values(\"tool_percentage\", ascending=False, inplace=True)\n\nf1 = figure(x_range=df_dl_hardware.hardware, plot_width=700, plot_height=400, title=\"Hardware Distribution\")\nf1.vbar(x=df_dl_hardware.hardware, top=df_dl_hardware.hardware_percentage, width=0.9, color=Spectral11[0], legend_label=\"% Hardware Usage\")\nf1.legend.location=\"top_right\"\nf1.legend.click_policy=\"hide\"\nf1.xaxis.major_label_orientation=145\n\nf2 = figure(x_range=df_dl_model.model, plot_width=700, plot_height=400, title=\"Model Distribution\")\nf2.vbar(x=df_dl_model.model, top=df_dl_model.model_percentage, width=0.9, color=Spectral11[0], legend_label=\"% Model Usage\")\nf2.legend.location=\"top_right\"\nf2.legend.click_policy=\"hide\"\nf2.xaxis.major_label_orientation=145\n\nf3 = figure(x_range=df_dl_tool.tool, plot_width=700, plot_height=400, title=\"Tool Distribution\")\nf3.vbar(x=df_dl_tool.tool, top=df_dl_tool.tool_percentage, width=0.9, color=Spectral11[0], legend_label=\"% Tool Usage\")\nf3.legend.location=\"top_right\"\nf3.legend.click_policy=\"hide\"\nf3.xaxis.major_label_orientation=145\n\nshow(column(f1, f2, f3))","9a76c5ae":"cv_list = {\n    \"Regular Methods\": sum(~df_dl.Q26_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"Image Segmentation\": sum(~df_dl.Q26_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"Object Detection\": sum(~df_dl.Q26_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"Image Classification\": sum(~df_dl.Q26_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"Image Generation\": sum(~df_dl.Q26_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q26_Part_6.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q26_Part_7.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_cv = pd.DataFrame.from_dict(cv_list, orient=\"index\", columns=[\"cv_percentage\"]).reset_index().rename(columns={\"index\": \"cv\"})\ndf_dl_cv.sort_values(\"cv_percentage\", ascending=False, inplace=True)\n\nf = figure(x_range=df_dl_cv.cv, plot_width=700, plot_height=400, title=\"Computer Vision techniques Distribution\")\nf.vbar(x=df_dl_cv.cv, top=df_dl_cv.cv_percentage, width=0.9, color=Spectral11[8], legend_label=\"% Computer Vision techniques Usage\")\nf.legend.location=\"top_right\"\nf.legend.click_policy=\"hide\"\nf.xaxis.major_label_orientation=145\n\nshow(f)","dc905fc8":"nlp_list = {\n    \"Word Embeddings\": sum(~df_dl.Q27_Part_1.isna()) * 100 \/ df_dl.shape[0],\n    \"Sequence Models\": sum(~df_dl.Q27_Part_2.isna()) * 100 \/ df_dl.shape[0],\n    \"Contextualized Embeddings\": sum(~df_dl.Q27_Part_3.isna()) * 100 \/ df_dl.shape[0],\n    \"Language Models\": sum(~df_dl.Q27_Part_4.isna()) * 100 \/ df_dl.shape[0],\n    \"None\": sum(~df_dl.Q27_Part_5.isna()) * 100 \/ df_dl.shape[0],\n    \"Other\": sum(~df_dl.Q27_Part_6.isna()) * 100 \/ df_dl.shape[0]\n}\n\ndf_dl_nlp = pd.DataFrame.from_dict(nlp_list, orient=\"index\", columns=[\"nlp_percentage\"]).reset_index().rename(columns={\"index\": \"nlp\"})\ndf_dl_nlp.sort_values(\"nlp_percentage\", ascending=False, inplace=True)\n\nf = figure(x_range=df_dl_nlp.nlp, plot_width=700, plot_height=400, title=\"NLP techniques Distribution\")\nf.vbar(x=df_dl_nlp.nlp, top=df_dl_nlp.nlp_percentage, width=0.9, color=Spectral11[8], legend_label=\"% NLP techniques Usage\")\nf.legend.location=\"top_right\"\nf.legend.click_policy=\"hide\"\nf.xaxis.major_label_orientation=145\n\nshow(f)","dd6be5bf":"# 2 Gender Inequality","192858a7":"We have about **7.1K DL practitioners** and **5.7K non-DL practitioners** in the survey. The **remaining 6.9K** will not be used.","70d20a4e":"# 5 Income and Expenses don't change","92e724bd":"# 6 Blogs, Coursera and YouTube are a goldmine of information","13955a01":"* This again clearly shows that there are DL practitioners across varying levels of education (and invariably age too).\n* A lot of this could be due to the fact that the modern research in DL is widely available on the internet leading to the **democratization of information**.\n\n> Deep Learning is for anyone and everyone","b3b22686":"* **Egypt! \ud83c\uddea\ud83c\uddec Iran! \ud83c\uddee\ud83c\uddf7 Romania! \ud83c\uddf7\ud83c\uddf4** A large proportion of practitioners in Data Science in these countries are working in Deep Learning. Looking at the top-10 list, there are many unexpected names. It is wonderful to see DL being adopted in so many countries across continents.\n* If we consider at least 100 respondants to Q24 from a country to be reliable enough data, we have **Taiwan \ud83c\uddf9\ud83c\uddfc, China \ud83c\udde8\ud83c\uddf3 and Turkey \ud83c\uddf9\ud83c\uddf7** who have the largest proportion of DL practitioners.\n* Just for comparison, India has 56% and US has 48% and they are much lower in the list.\n\n> You might want to relocate to Egypt if you are a DL enthusiast.","116b65d3":"# 8 GPU usage on the rise","c03cd3c8":"* The GPU usage is close to the CPU usage due to the fact that running DL models can require a **lot of computing power** and GPUs help in scaling horizontally.\n* **CNN is the most popular model** used by DL practitioners.\n* AutoML techniques are widely used for structured data without DL models. They are not yet mature to deal with data that is unstructured and to automatically work with DL methods and architectures.\n\n> Upgrading to GPU is worth it","bb809fb7":"# Setup\nWe will be using the [2019 Kaggle ML & DS Survey data](https:\/\/www.kaggle.com\/c\/kaggle-survey-2019\/data). Since there is no direct question to flag a person as a DL practitioner or not, we will be using responses to *Q24: Which of the following ML algorithms do you use on a regular basis?* with options:\n- Linear or Logistic Regression\n- Decision Trees or Random Forests\n- Gradient Boosting Machines (xgboost, lightgbm, etc)\n- Bayesian Approaches\n- Evolutionary Approaches\n- **Dense Neural Networks (MLPs, etc)**\n- **Convolutional Neural Networks**\n- **Generative Adversarial Networks**\n- **Recurrent Neural Networks**\n- **Transformer Networks (BERT, gpt-2, etc)**\n\nA DL practitioner is tagged as someone who has selected at least one of the last 5 options. This will be the subset of data used the most.   \nA non-DL practitioner is tagged as someone who has selected at least one of the first 5 options and not selected any of the last 5 options.   \n\nThere are some who have not answered this question or have answered **None** or have answered **Other**. Since there is no concrete information about them to classify correctly, we will be eliminating them for the purpose of this analysis.\n\n![ml-dl.jpg](attachment:ml-dl.jpg)","3628b84b":"# 3 Egypt, Iran and Romania rise as unexpected winners","eeb7c53d":"* The numbers say it all. A whopping **>95% of DL practitioners use Python**, and combined with **>80% of Jupyter** and **>40% of Colab**, these easily form the most popular and convenient set of tools available today.\n* Almost every popular library in DL is based in Python so the 5% folks who are not Python users might be missing the bus.\n\n> Using the latest and best can give an upper edge","05dea764":"* There is a clear gap between the **43% of females** vs **57% of males** in Data Science who work in Deep Learning. But, the number of female respondants to the survey are much lower than men and this percentage may not be very reliable.\n* You could also go through the [Geek Girls Rising notebook](https:\/\/www.kaggle.com\/parulpandey\/geek-girls-rising-myth-or-reality) that explores the female responses in detail by [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey).\n\n> A balanced ensemble of females and males can lead to wonders","87853318":"# 10 A new era of NLP is coming","65f2414b":"# 4 Research and Engineering are the recipes for Deep Learning","cce1b91a":"* Deep Learning requires a lot of research and implementation and the plot above is a great confirmation of the same. People in roles of **Research Scientist, Software Engineer and Data Scientist** are almost twice as likely **(>60%)** to be working in DL compared to those in roles of **Business Analyst, Statistician and Data Analyst (~35%)**.\n* In today's world, most DL projects in the industry require skills of research as well engineering together for success. Defining, transforming and pipelining a data science problem into a deep learning framework is as important and crucial as implementing, productionizing and solutioning of the problem. They go hand-in-hand like two sides of a coin.\n\n> Deep Learning has to be learnt and done hands-on","55c94c06":"# 7 The deadly combo of Jupyter + Colab + Python","f8766acf":"* One of the biggest use-cases of DL models in the industry is on image\/video data. With the ability to process raw pixel data of images to build solutions that are capable of matching human performance and even beating it in some cases, **Image Classification** is making breakthroughs in many ways.\n* Earlier we saw CNN as the most popular algorithm being used and not surprisingly, **CNNs** are also the most frequent method used for image classification problems.\n* **Image Segmentation** and **Object Detection** are slowly but surely catching up. They are a bit more complex in nature, but the annual [Open Images](https:\/\/opensource.google\/projects\/open-images-dataset) dataset and competition on Kaggle is helping its development.\n\n> DL models are improving with every passing day","beeecfdf":"The top-10 countries with largest proportion of DL practitioners among those in Data Science:","25514ab8":"* **India, US, Japan, Brazil and China**, the usual suspects, lead the list of DL practitioners. It is very intuitive and expected due to the population of the countries and corresponding number of active users on Kaggle.\n\nInstead, let's look at the % of DL practitioners within each country among those in Data Science. The results are very different.","b1d2b75d":"# 1 Age and Education no bar for Deep Learning!","1cabda6a":"# 9 CNN breakthrough in Image Classification","6af3f721":"# A deep learning of Deep Learning\n![dl.jpeg](attachment:dl.jpeg)\n*Pic Credits: Getty*\n\n**Deep Learning (DL)** is progressing by leaps and bounds at a phenomenally fast pace with so much new research, papers, ideas, models being developed throughout the world. The last few years have seen a lot of solutions being built using deep learning methods and frameworks and this is expected to only increase in the future.\n\nThis notebook analyses the deep learning practitioners from the [2019 Kaggle ML & DS Survey](https:\/\/www.kaggle.com\/c\/kaggle-survey-2019) to understand patterns, get insights, learn challenges and maybe even answer some questions regarding the current and future landscape of deep learning:\n\n* Who are these deep learning practitioners? Are they concentrated in a certain geography or age or background?\n* Are deep learning practitioners very different from other machine learning practitioners? Do they have a different career path or have different salaries?\n* Does deep learning require a lot of money and resources? Are the tools, products and languages used very different requiring special skills?\n* What does the future of deep learning hold for us? Will it significantly change industries or even modeling and solutioning methods?   \n\n...and many more.\n\n**P.S.**: The insights shared in this notebook are based on the 2019 Kaggle Survey data only and not all of them necessarily would be similar in the real world.","d7a32c1d":"The top-10 countries with largest number of DL practitioners:","96cc9c50":"* The salary distribution as well as the statistical measures like mean, median, quartiles and variance are almost identical between DL practitioners and non-DL practitioners.\n* The expenses made for work has a very similar distribution as well.\n\n> Deep Learning is for the passionate","8fffefbc":"# Overall Summary\nWe've seen a variety of outputs for Deep Learning practitioners through slicing\/dicing of the data with numerous comparisons, results, questions and probable reasonings. Here's a summary of the key insights:\n\n* **Deep Learning is everywhere:** Deep Learning is being practised across ages, education levels and geographies. Egypt, Iran and Romania have the highest proportion of DL practitioners among those building Data Science models.\n* **Male Dominance:** Females have lower participation and prominence than men. Having more data to support or accept this is required.\n* **Research + Engineering ensemble:** Research and Engineering are strongly correlated to working in the DL space. These areas of expertise are the essential components for building successing solutions in the industry.\n* **Passion not Money:** Salary and Expenditure is consistent irrespective of the type of work.\n* **Online Content:** Blogs, Coursera and YouTube are the top-3 sources that are used by DL practitioners. A lot of the latest content is available at these online sources and most for free.\n* **Python all the way:** Python is and seems to be the favourite language of DL practitioners. And it doesn't seem to be changing anytime soon.\n* **Computing power:** GPUs are becoming a dire need for running DL models. TPUs haven't yet reached the scale of availability and adoption compared to GPUs. It will take time to establish a set of libraries and a community that can help run many of the DL workflows.\n* **Unstructured Victories:** CNNs made a big breakthrough in image processing and since then DL has been widely used to solve computer vision problems. New pre-trained language models like BERT that use transformer architecture are making waves recently due to their improved performance over traditional approaches. These are expected to be adopted more especially if it's scoring latency can be enhanced.","d2e8debd":"* We see that about **50%-55%** of practitioners in Data Science work in Deep Learning across all age-groups. So whether you are a college student or a working professional or maybe even retired and looking to get your hands into some new technology and innovation, deep learning is independent of age!\n* This resonates with the fact that a lot of successful DL practitioners as well as many top Kagglers are **self-taught** in this field. And something that can be learned through motivation, effort and time has no constraint on age.","6c444962":"* Word Embeddings like **TFIDF, Word2Vec and Glove** were the go-to approaches when working on NLP problems and are still being used. But a lot has changed in recent years.\n* Language models like **BERT, XLNET and GPT-2** are models that are performing exceptionally well on various problems using textual data. These are new and it's adoption is expected to increase in the coming years.\n* One of the drawbacks of the newer language models are it's scoring latency which might be a reason it may not easily be adopted in real-time production systems. This could improve in the future too.\n\n> NLP models are understanding human languages better","df13881d":"Let's just quickly check the number of practitioners in the final dataset.","430e730b":"* Let's ignore the Kaggle bars since these are already biased with Kaggle users. **~65% of DL practitioners read blogs, followed by ~60% who do Coursera courses**. And why not? Both are extremely rich sources of information and a lot of the latest developments in DL are shared and discussed here.\n* [Andrew Ng](https:\/\/en.wikipedia.org\/wiki\/Andrew_Ng), one of the biggest figures in AI co-founded [Coursera](https:\/\/www.coursera.org\/) and as of 2019, the two most popular courses on the platform is, without surprise, [Machine Learning](https:\/\/www.coursera.org\/learn\/machine-learning) and [Deep Learning](https:\/\/www.coursera.org\/learn\/neural-networks-deep-learning). Well, of course everyone is going to be on Coursera!\n* It is probably one of the first set of material that any DL practitioner should study.\n\n> Knowledge is power"}}