{"cell_type":{"499d3531":"code","6fd3be9b":"code","85f145d3":"code","ecf58ed4":"code","820473bf":"code","ad931047":"code","ea408ed8":"code","ebb5bb2b":"code","6de1feaf":"code","63101381":"code","290a3547":"code","53a864dd":"code","cfa8e94b":"code","1aff2fc7":"code","7f7f7fc2":"code","86bb55d2":"code","1440ff9a":"code","71baa8f3":"code","fd12915d":"code","b4a1c120":"code","6a2126aa":"code","e65cf0eb":"code","f4fff6dd":"code","3e9fbd4a":"code","f4316360":"code","a2a8fa24":"code","fefc4226":"code","9a1d5f50":"code","edf03406":"code","26585438":"code","542df0e3":"code","7199e3c9":"code","6fc58794":"markdown","8277269e":"markdown","d5aae227":"markdown","e7ebc96e":"markdown","f313d12f":"markdown","dbe2b0d1":"markdown","1c3da871":"markdown","65c07321":"markdown","cb25fe50":"markdown","aa15cdd9":"markdown","9c4957db":"markdown","3a8dd03c":"markdown","2bc615aa":"markdown","3923db12":"markdown","7b44f15a":"markdown","f076a16c":"markdown","d6eb3bfc":"markdown","38184dd8":"markdown","93ae9c42":"markdown","60df52c7":"markdown","2cba9184":"markdown","eadb66dd":"markdown","8db5deba":"markdown","e301e889":"markdown","d43a9fac":"markdown","99338708":"markdown","3b047550":"markdown","f6a2980e":"markdown","50c8a1d2":"markdown"},"source":{"499d3531":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,plot_confusion_matrix\nfrom sklearn import tree\n\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.model_selection import KFold\nimport time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score","6fd3be9b":"rock_data = pd.read_csv(\"..\/input\/emg-4\/0.csv\", header=None )\nscissors_data = pd.read_csv(\"..\/input\/emg-4\/1.csv\", header=None )\npaper_data = pd.read_csv(\"..\/input\/emg-4\/2.csv\", header=None )\nok_data = pd.read_csv(\"..\/input\/emg-4\/3.csv\", header=None )\ndata = pd.concat([rock_data, scissors_data, paper_data, ok_data], axis = 0)\n\n\nX = data.drop(data.columns[-1],axis=1)\ny = data[data.columns[-1]]\n\n#Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train.head(5)","85f145d3":"#Check the number of times each gesture occurs in training data.\ny_train.value_counts()\nprint(\"Dataset samples:\",len(y))","ecf58ed4":"#https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Predictive Accuracy\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True,verbose=2)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    print(\"CV Scores:\",test_scores_mean)\n\n    return plt","820473bf":"def r2(a,b):\n    this_correlation = np.corrcoef(a, b)[0,1]\n    this_r2 = this_correlation**2\n    return this_r2","ad931047":"def plot_confusion(title, model, X_train, y_train, X_test, y_test):\n    svm_confusion_matrix = plot_confusion_matrix(model, X_train, y_train,\n                      display_labels=['Rock','Scissors', 'Paper', 'Ok'],\n                      cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Training Set)\")\n    plt.show()\n\n    svm_confusion_matrix = plot_confusion_matrix(model, X_test, y_test,\n                          display_labels=['Rock','Scissors', 'Paper', 'Ok'],\n                          cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Testing Set)\")\n    plt.show()","ea408ed8":"#Estimate parameters\ndecision_tree = tree.DecisionTreeClassifier(max_depth=10, \n                                  criterion='entropy',\n                                  min_samples_leaf=10,\n                                  min_samples_split=5,\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\n\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Initial Decision Tree Learning Rate\", \n                    X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","ebb5bb2b":"dt_params = {\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(5,30),\n    \"min_samples_leaf\":range(1,5),\n    \"min_samples_split\":range(1,5)\n}\ndecision_tree = tree.DecisionTreeClassifier()\n\n\ngrid = GridSearchCV(decision_tree,\n                    param_grid = dt_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#Best params:\n#{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 3}","6de1feaf":"start_time = time.time();\ndecision_tree = tree.DecisionTreeClassifier(max_depth=20,\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3)\ndecision_tree.fit(X_train, y_train)\nprint(\"Training time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Decision Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","63101381":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nestimator = tree.DecisionTreeClassifier(max_depth=20,\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(estimator, \"Decision Tree Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.6, 1.01),\n                    cv=cv, n_jobs=4)","290a3547":"#Initial model\nsvm = SVC(kernel='poly',\n          C=1,\n          degree=3,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Initial Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.25, 0.8),\n                    cv=cv, n_jobs=4)","53a864dd":"svm_params1 = {\n    \"kernel\":['poly'],\n    \"degree\":range(1,5),\n    \"C\":[0.01,0.1,1,10]\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'C': 10, 'degree': 2, 'kernel': 'poly'}","cfa8e94b":"svm_params1 = {\n    \"kernel\":['rbf'],\n    \"gamma\":['scale','auto']\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'gamma': 'scale', 'kernel': 'rbf'}","1aff2fc7":"start_time = time.time();\nsvm = SVC(kernel='rbf',\n          gamma='scale',\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"\\nUsing RBF Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","7f7f7fc2":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"RBF Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.5, 1.01),\n                    cv=cv, n_jobs=4)","86bb55d2":"start_time = time.time();\nsvm = SVC(kernel='poly',\n          C=10,\n          degree=2,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","1440ff9a":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)","71baa8f3":"#Initial parameter estimates\nknn = KNeighborsClassifier(n_neighbors=10,\n                           algorithm='auto',\n                           leaf_size=10)\nknn.fit(X_train, y_train)\n\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint('KNN Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"Initial KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.85),\n                    cv=cv, n_jobs=4)","fd12915d":"knn_params = {\n    \"n_neighbors\":range(1,10),\n    \"leaf_size\":range(25,35),\n    \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn,\n                    param_grid = knn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'algorithm': 'auto', 'leaf_size': 33, 'n_neighbors': 4}","b4a1c120":"start_time = time.time();\nknn = KNeighborsClassifier(n_neighbors=4,\n                           algorithm='auto',\n                           leaf_size=33)\nknn.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\n\nstart_time = time.time();\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"K = 4\")\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"KNN Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","6a2126aa":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.8),\n                    cv=cv, n_jobs=4)","e65cf0eb":"#Initial estimates\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=2, \n                                  criterion='entropy',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  random_state=0),\n    n_estimators=50,\n    learning_rate=0.1,\n    random_state=0) \nbooster.fit(X_train, y_train)\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"Initial ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.55, 0.95),\n                    cv=cv, n_jobs=4)","f4fff6dd":"start_time = time.time();\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=6,#Tested values between 1 and 20\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3),\n    n_estimators=500,\n    learning_rate=0.5,#0.5: 0.908  #tested values between 0.01 and 1\n    random_state=0)\nbooster.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Boosted Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Boosted Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","3e9fbd4a":"if False:#Turn to true to run cell.  Takes > 15 minutes.\n    start_time = time.time();\n    train_accuracies = [];\n    test_accuracies = [];\n    for i in range(1,1001,50):\n        booster = AdaBoostClassifier(\n            tree.DecisionTreeClassifier(max_depth=6,#Tested values between 1 and 20\n                                      criterion='gini',\n                                      random_state=0,\n                                      min_samples_leaf=1,\n                                      min_samples_split=3),\n            n_estimators=i,\n            learning_rate=0.5,\n            random_state=0) \n        booster.fit(X_train, y_train)\n        y_test_pred = booster.predict(X_test)\n        y_train_pred = booster.predict(X_train)\n        train_accuracies.append(accuracy_score(y_train, y_train_pred))\n        test_accuracies.append(accuracy_score(y_test, y_test_pred))\n        print(i,end=\" \")\n    print(\"Training Scores:\",train_accuracies)\n    print(\"Testing Scores:\",test_accuracies)","f4316360":"plt.plot(range(1,1001,50),train_accuracies,marker='o',label=\"Training Set\")\nplt.plot(range(1,1001,50),test_accuracies,marker='o',label=\"Testing Set\")\nplt.xlabel('Estimators')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Adaptive Boosting Accuracy by Estimator Count\")\nplt.legend()\nplt.show()","a2a8fa24":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.8, 1.01),\n                    cv=cv, n_jobs=4)","fefc4226":"#Initial parameters\nnn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(32,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=50,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Initial Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","9a1d5f50":"nn_params = {\n    \"hidden_layer_sizes\":[(64,),(),(64,32),(32,)],\n    \"activation\":['identity', 'logistic', 'tanh', 'relu'],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"verbose\":[True]\n}\n\nnn = MLPClassifier()\ngrid = GridSearchCV(nn,\n                    param_grid = nn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'activation': 'relu', 'hidden_layer_sizes': (64,), 'solver': 'lbfgs', 'verbose': True}","edf03406":"start_time = time.time();\nnn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(64,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=200,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Train r2 score:',r2(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Neural Network Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","26585438":"nn_scores_train = [];\nnn_scores = [];\nfor i in range(20):\n    nn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(64,),\n                   solver='lbfgs',\n                   verbose=False,\n                   max_iter=i*10+1,\n                   random_state=1)\n    \n    nn.fit(X_train,y_train);\n    y_test_pred = nn.predict(X_test)\n    nn_scores_train.append(accuracy_score(y_train, y_train_pred))\n    nn_scores.append(accuracy_score(y_test, y_test_pred))\n    print(\"I\",i)\nprint(\"Training Scores:\",nn_scores_train)\nprint(\"Testing Scores:\",nn_scores)","542df0e3":"plt.plot(range(1,201,10),nn_scores_train,marker='o',label=\"Training Set\")\nplt.plot(range(1,201,10),nn_scores,marker='o',label=\"Testing Set\")\nplt.xlabel('Network Iterations')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Neural Network Accuracy by Iteration Number\")\nplt.legend()\nplt.show()","7199e3c9":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","6fc58794":"Run Poly SVM","8277269e":"Function to plot learning curves for each model","d5aae227":"Plot KNN Learning rate","e7ebc96e":"Plot Poly kernel SVM learning rate","f313d12f":"Calculate network accuracy by iteration","dbe2b0d1":"# Neural Network","1c3da871":"GridSearch poly kernel SVM parameters","65c07321":"# Gesture Recognition","cb25fe50":"Calculate Boosting accuracy by iterations","aa15cdd9":"GridSearch Rbf parameters","9c4957db":"import libraries","3a8dd03c":"# BOOSTING","2bc615aa":"Function to plot confusion matrices","3923db12":"Run Rbf Kernel SVM","7b44f15a":"Calculate Decision Tree Learning Rate","f076a16c":"Draw network learning rate","d6eb3bfc":"Run neural network","38184dd8":"Run Boosting algorithm","93ae9c42":"Run Decision tree","60df52c7":"Show Booster Learning Rate","2cba9184":"Gridsearch Network parameters","eadb66dd":"Plot RBF Learning Curve","8db5deba":"## Decision Tree","e301e889":"Read dataset and split into training and testing sets","d43a9fac":"GridSearch knn parameters","99338708":"Grid Search decision tree parameters","3b047550":"# KNN Algorithm","f6a2980e":"# SVM Model","50c8a1d2":"Run KNN model"}}