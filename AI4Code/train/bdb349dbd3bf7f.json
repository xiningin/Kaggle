{"cell_type":{"aba8d065":"code","587305fb":"code","6c1367ec":"code","58f83837":"code","55e2f0cf":"code","952e5616":"code","dee7a3ea":"code","bb621b4b":"code","b15a7322":"code","c10300ff":"code","8e2713c8":"code","e7d2c44e":"code","76186924":"code","47c91b62":"code","00f3fae8":"code","0da3b45d":"code","a29ccbbf":"code","9cb33645":"code","20bc3b59":"code","c66acede":"code","a55567fe":"code","c335a8ae":"code","6897617e":"code","26a56e2e":"code","802ca3bd":"code","a1f92e7e":"code","db9d547e":"code","0c77241a":"code","0c8d7c55":"code","24b352bd":"code","d350e29f":"code","ca7eaad2":"code","5224f4b4":"code","b232c590":"code","1a5ea28a":"code","24cd618e":"code","023310b6":"code","8990da79":"code","45190f84":"code","f11f1ffb":"code","7d9c3968":"code","174252ae":"code","62201a3a":"code","63f4ab55":"code","2a09319d":"code","595c64de":"code","2f71d05e":"code","c985ee9a":"code","36c80cbd":"code","897dc463":"code","de35cd1c":"code","9c693e31":"code","8242bdc1":"code","97d9bda3":"code","7bebe2c6":"code","5ad16b7b":"code","aff3e99a":"code","e66c3d05":"code","b9dfbe0e":"code","55774855":"code","bc2201c7":"code","70b45add":"code","4f44a884":"code","104cfdca":"code","cdd0b832":"code","599ff680":"markdown","50559b30":"markdown","2fd4b4a3":"markdown","3ab8c173":"markdown","e6c6439e":"markdown","4846647c":"markdown","22d406da":"markdown","2bc03996":"markdown","159c336f":"markdown","0c543e70":"markdown","285c6493":"markdown","9497c189":"markdown","3a5b48a5":"markdown","dfeed40e":"markdown","14cf3bf8":"markdown","2766f22f":"markdown","28679a2d":"markdown","5a12a105":"markdown","cb58addf":"markdown","6f955fdb":"markdown","db38d2e7":"markdown","6bc67a61":"markdown","c71b33a0":"markdown","3ad81c43":"markdown","e9850104":"markdown","df200ab1":"markdown","15378e0c":"markdown","6ba98897":"markdown","010ddf50":"markdown","2ed045d8":"markdown","af177c45":"markdown","960c1540":"markdown","03335bfb":"markdown","618b949b":"markdown","56eccc20":"markdown","18448d6d":"markdown","d6b10c95":"markdown","df7963b3":"markdown","40e11766":"markdown","da80886e":"markdown","bc31bd78":"markdown","baae6b90":"markdown","f455b1b2":"markdown","2dd7a55d":"markdown","a2287d24":"markdown","460c2234":"markdown","02d5afd7":"markdown","5ba45efb":"markdown","2ebd1d47":"markdown","81928c48":"markdown","0539b48b":"markdown","137b7ad0":"markdown","a1fcc132":"markdown","62b160e0":"markdown","0023c3b0":"markdown","ab21ff75":"markdown","864bd9b8":"markdown","d548f2a4":"markdown","588dfd0d":"markdown","dc7354c6":"markdown","bc71db13":"markdown","caf69a7e":"markdown","9a726b86":"markdown","096a091a":"markdown","5ede6b23":"markdown","633b1299":"markdown","87bdaf34":"markdown","8c3aae65":"markdown","58afc2f5":"markdown","d432f054":"markdown","bde97b43":"markdown","39b979ac":"markdown","6c9654e7":"markdown","4b39b930":"markdown"},"source":{"aba8d065":"import numpy as np  # Importing NumPy library\nimport pandas as pd  # Importing Pandas library\nimport matplotlib.pyplot as plt  # Importing Matplotlib library's \"pyplot\" module\nimport seaborn as sns  # Imorting Seaborn library\n\n# Ignore all warnings:\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# This lines for Kaggle:\nimport os\nprint(os.listdir(\"..\/input\"))","587305fb":"data = pd.read_csv(\"..\/input\/mushrooms.csv\")  # Read CSV file and load into \"data\" variable\ndata.info()  # Show detailed information for dataset columns(attributes)","6c1367ec":"data.head()  # Prints first 5 entries of the dataset","58f83837":"data.tail()  # Prints last 5 entries of the dataset","55e2f0cf":"from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder class\n\nlabel_encoder = LabelEncoder()  # Create a instance for the label encoder\nencoded_data = pd.DataFrame()  # Create empty DataFrame\n\nfor column in data.columns:\n    encoded_data[column] = label_encoder.fit_transform(data[column])  # Iterate all columns and transform its values","952e5616":"encoded_data.head()  # Print first 5 record for the encoded data","dee7a3ea":"encoded_data.describe()  # Print some statistics for data","bb621b4b":"encoded_data.corr()  # Prints correlation matrix","b15a7322":"fig, axes = plt.subplots(figsize=(18, 18))  # This method creates a figure and a set of subplots\nsns.heatmap(data=encoded_data.corr(), annot=True, linewidths=.5, cmap=\"coolwarm\", ax=axes)  # Figure out heatmap\nplt.show()  # Shows only plot and remove other informations","c10300ff":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 13))  # Adjust subplots\n\n# Draw frequency of the \"bruises\" values according to \"class\":\nbruises_bar = sns.countplot(x=\"bruises\", hue=\"class\", data=data, ax=axes[0][0]);\nbruises_bar.set_xticklabels([\"True\", \"False\"])\n\n# Draw frequency of the \"gill-spacing\" values according to \"class\":\ngill_spacing_bar = sns.countplot(x=\"gill-spacing\", hue=\"class\", data=data, ax=axes[0][1]);\ngill_spacing_bar.set_xticklabels([\"Close\", \"Crowded\", \"Distant\"])\n\n# Draw frequency of the \"gill-size\" values according to \"class\":\ngill_size_bar = sns.countplot(x=\"gill-size\", hue=\"class\", data=data, ax=axes[0][2]);\ngill_size_bar.set_xticklabels([\"Narrow\", \"Broad\"])\n\n# Draw frequency of the \"gill-color\" values according to \"class\":\ngill_color_bar = sns.countplot(x=\"gill-color\", hue=\"class\", data=data, ax=axes[1][0]);\ngill_color_bar.set_xticklabels([\"Black\", \"Brown\", \"Gray\", \"Pink\", \"White\", \"Choco\", \"Purple\", \"Red\", \"Buff\", \"Green\", \"Yellow\", \"Orange\"], rotation=60)\n\n# Draw frequency of the \"stalk-root\" values according to \"class\":\nstalk_root_bar = sns.countplot(x=\"stalk-root\", hue=\"class\", data=data, ax=axes[1][1]);\nstalk_root_bar.set_xticklabels([\"Equal\", \"Club\", \"Bulbous\", \"Rooted\", \"None\"], rotation=60)\n\n# Draw frequency of the \"stalk-surface-above-ring\" values according to \"class\":\nstalk_sar_bar = sns.countplot(x=\"stalk-surface-above-ring\", hue=\"class\", data=data, ax=axes[1][2]);\nstalk_sar_bar.set_xticklabels([\"Smooth\", \"Fibrous\", \"Silky\", \"Scaly\"], rotation=60)\n\n# Draw frequency of the \"stalk-surface-below-ring\" values according to \"class\":\nstalk_sbr_bar = sns.countplot(x=\"stalk-surface-below-ring\", hue=\"class\", data=data, ax=axes[2][0]);\nstalk_sbr_bar.set_xticklabels([\"Smooth\", \"Fibrous\", \"Silky\", \"Scaly\"], rotation=60)\n\n# Draw frequency of the \"ring-type\" values according to \"class\":\nring_type_bar = sns.countplot(x=\"ring-type\", hue=\"class\", data=data, ax=axes[2][1]);\nring_type_bar.set_xticklabels([\"Pendant\", \"Evanescent\", \"Large\", \"Flaring\", \"None\"], rotation=60)\n\n# Draw frequency of the \"population\" values according to \"class\":\npopulation_bar = sns.countplot(x=\"population\", hue=\"class\", data=data, ax=axes[2][2]);\npopulation_bar.set_xticklabels([\"Scattered\", \"Numerous\", \"Abundant\", \"Several\", \"Solitary\", \"Clustered\"], rotation=60)\n\nfig.tight_layout()  # Slightly spacing between axis labels and values\nplt.show()","8e2713c8":"poisonous_count = len(data[data[\"class\"] == \"p\"].index)  # Get poisonous count\nedible_count = len(data[data[\"class\"] == \"e\"].index)  # Get edible count\n\n# Draw Pie Chart:\nplt.pie([poisonous_count, edible_count], labels=[\"Poisonous\", \"Edible\"], autopct='%1.1f%%', radius=2.0, shadow=True, colors=[\"r\", \"g\"])\nplt.show()","e7d2c44e":"fig, ax = plt.subplots(figsize=(12,8))  # For specify figure size\ndata.groupby(['habitat', 'class']).size().unstack().plot.bar(stacked=True, ax=ax)  # Draw Stacked Bar Chart\nplt.show()","76186924":"encoded_data.drop([\"cap-shape\", \"veil-type\"], axis=1, inplace=True)  # Drop \"cap-shape\" and \"veil-type\" columns from dataset\nencoded_data.columns","47c91b62":"X = encoded_data.drop([\"class\"], axis=1)  # Put all data (except \"class\" column) to the X variable\ny = encoded_data[\"class\"] # Put only \"class\" column to the Y variable","00f3fae8":"X.head()","0da3b45d":"y.head()","a29ccbbf":"from sklearn.preprocessing import StandardScaler  # Import StandartScaler class\nstd_scaler = StandardScaler()  # Create instance for scaler\n\nX = std_scaler.fit_transform(X)  # Fit and transform data with scaler","9cb33645":"from sklearn.decomposition import PCA  # Import class for PCA\n\nfor i in range(2, 20):\n    pca = PCA(n_components=i)  # n_components = Specify the number of features you want to reduce.\n    pca.fit_transform(X)\n    print(\"Sum of Variance Ratio in \" + str(i) + \" Components: \", sum(pca.explained_variance_ratio_))","20bc3b59":"pca = PCA(n_components=13)  # We will reduce the feature count to the 13\nX = pca.fit_transform(X)  # Fit and transform with data\n\nprint(sum(pca.explained_variance_ratio_))  # Print variance ratio","c66acede":"from sklearn.model_selection import train_test_split  # Import \"train_test_split\" method\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Parameters:\n# test_size : It decides how many test data in percentage.\n# random_state : This parameter can take any value. This value decides randomness seed.","a55567fe":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","c335a8ae":"logistic_regression_cls = LogisticRegression(random_state=13)  # Create instance for model\nlogistic_regression_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for Logistic Regression: \", logistic_regression_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for Logistic Regression: \", logistic_regression_cls.score(x_test, y_test))  # Print Test Score","6897617e":"logistic_regression_cls.get_params()  # Print hyperparameters and their values for the model","26a56e2e":"naive_bayes_cls = GaussianNB()  # Create instance for model\nnaive_bayes_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for Gaussian Naive Bayes: \", naive_bayes_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for Gaussian Naive Bayes: \", naive_bayes_cls.score(x_test, y_test))  # Print Test Score","802ca3bd":"naive_bayes_cls.get_params()  # Print hyperparameters and their values for the model","a1f92e7e":"decision_tree_cls = DecisionTreeClassifier(random_state=13)  # Create instance for model\ndecision_tree_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for Decision Tree: \", decision_tree_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for Decision Tree: \", decision_tree_cls.score(x_test, y_test))  # Print Test Score","db9d547e":"decision_tree_cls.get_params()  # Print hyperparameters and their values for the model","0c77241a":"random_forest_cls = RandomForestClassifier(random_state=13)  # Create instance for model\nrandom_forest_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for Random Forest: \", random_forest_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for Random Forest: \", random_forest_cls.score(x_test, y_test))  # Print Test Score","0c8d7c55":"random_forest_cls.get_params()  # Print hyperparameters and their values for the model","24b352bd":"support_vector_cls = SVC(random_state=13)  # Create instance for model\nsupport_vector_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for SVC: \", support_vector_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for SVC: \", support_vector_cls.score(x_test, y_test))  # Print Test Score","d350e29f":"support_vector_cls.get_params()  # Print hyperparameters and their values for the model","ca7eaad2":"knn_cls = KNeighborsClassifier()  # Create instance for model\nknn_cls.fit(x_train, y_train)  # Fit data with model\n\nprint(\"Train Score for K-NN: \", knn_cls.score(x_train, y_train))  # Print Train Score\nprint(\"Test Score for K-NN: \", knn_cls.score(x_test, y_test))  # Print Test Score","5224f4b4":"knn_cls.get_params()  # Print hyperparameters and their values for the model","b232c590":"# Specifying hyperparameters' range for the model:\nparameters_LR = {\"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                 \"penalty\" : [\"l1\", \"l2\"]}\n\n# Create a Grid Search Cross Validation instance (\"n_jobs=-1\" means use all cores of the processor):\ngrid_search_LR = GridSearchCV(logistic_regression_cls, parameters_LR, cv=6, n_jobs=-1, return_train_score=True)\n\n# Fit Grid Search model with data:\ngrid_search_LR.fit(x_train, y_train)\n\n# Show results for all combinations:\npd.DataFrame(grid_search_LR.cv_results_)[[\"params\", \"mean_fit_time\", \"mean_train_score\", \"mean_test_score\"]]","1a5ea28a":"grid_search_LR.best_params_  # Print best hyperparameters for the model","24cd618e":"# Specifying hyperparameters' range for the model:\nparameters_DT = {\"max_depth\" : np.arange(3, 20)}\n\n# Create a Grid Search Cross Validation instance (\"n_jobs=-1\" means use all cores of the processor):\ngrid_search_DT = GridSearchCV(decision_tree_cls, parameters_DT, cv=6, n_jobs=-1, return_train_score=True)\n\n# Fit Grid Search model with data:\ngrid_search_DT.fit(x_train, y_train)\n\n# Show results for all combinations:\npd.DataFrame(grid_search_DT.cv_results_)[[\"params\", \"mean_fit_time\", \"mean_train_score\", \"mean_test_score\"]]","023310b6":"grid_search_DT.best_params_  # Print best hyperparameters for the model","8990da79":"# Specifying hyperparameters' range for the model:\nparameters_RF = {\"max_depth\" : np.arange(5, 15),\n                 \"n_estimators\" : [100, 200, 300]}\n\n# Create a Grid Search Cross Validation instance (\"n_jobs=-1\" means use all cores of the processor):\ngrid_search_RF = GridSearchCV(random_forest_cls, parameters_RF, cv=6, n_jobs=-1, return_train_score=True)\n\n# Fit Grid Search model with data:\ngrid_search_RF.fit(x_train, y_train)\n\n# Show results for all combinations:\npd.DataFrame(grid_search_RF.cv_results_)[[\"params\", \"mean_fit_time\", \"mean_train_score\", \"mean_test_score\"]]","45190f84":"grid_search_RF.best_params_  # Print best hyperparameters for the model","f11f1ffb":"# Specifying hyperparameters' range for the model:\nparameters_SVC = {\"C\" : [0.1, 1, 10, 100],\n                 \"gamma\" : [0.001, 0.01, 0.1, 1]}\n\n# Create a Grid Search Cross Validation instance (\"n_jobs=-1\" means use all cores of the processor):\ngrid_search_SVC = GridSearchCV(support_vector_cls, parameters_SVC, cv=6, n_jobs=-1, return_train_score=True)\n\n# Fit Grid Search model with data:\ngrid_search_SVC.fit(x_train, y_train)\n\n# Show results for all combinations:\npd.DataFrame(grid_search_SVC.cv_results_)[[\"params\", \"mean_fit_time\", \"mean_train_score\", \"mean_test_score\"]]","7d9c3968":"grid_search_SVC.best_params_  # Print best hyperparameters for the model","174252ae":"# Specifying hyperparameters' range for the model:\nparameters_KNN = {\"n_neighbors\" : np.arange(2, 30)}\n\n# Create a Grid Search Cross Validation instance (\"n_jobs=-1\" means use all cores of the processor):\ngrid_search_KNN = GridSearchCV(knn_cls, parameters_KNN, cv=6, n_jobs=-1, return_train_score=True)\n\n# Fit Grid Search model with data:\ngrid_search_KNN.fit(x_train, y_train)\n\n# Show results for all combinations:\npd.DataFrame(grid_search_KNN.cv_results_)[[\"params\", \"mean_fit_time\", \"mean_train_score\", \"mean_test_score\"]]","62201a3a":"grid_search_KNN.best_params_  # Print best hyperparameters for the model","63f4ab55":"# Logistic Regression Classifier:\nlogistic_regression_cls_tuned = LogisticRegression(C=0.1, penalty=\"l2\", random_state=13)\nlogistic_regression_cls_tuned.fit(x_train, y_train)\n\n# Gaussian Naive Bayes Classifier (Not Changed):\nnaive_bayes_cls_tuned = GaussianNB()\nnaive_bayes_cls_tuned.fit(x_train, y_train)\n\n# Decision Tree Classifier:\ndecision_tree_cls_tuned = DecisionTreeClassifier(max_depth=14, random_state=13)\ndecision_tree_cls_tuned.fit(x_train, y_train)\n\n# Random Forest Classifier:\nrandom_forest_cls_tuned = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=13)\nrandom_forest_cls_tuned.fit(x_train, y_train)\n\n# Support Vector Classifier:\nsupport_vector_cls_tuned = SVC(kernel=\"rbf\", C=1, gamma=0.1, random_state=13)\nsupport_vector_cls_tuned.fit(x_train, y_train)\n\n# K-Nearest Neighbors Classifier:\nknn_cls_tuned = KNeighborsClassifier(n_neighbors=3)\nknn_cls_tuned.fit(x_train, y_train)\n\n# Find test accuracy for all models:\nprint(\"Test Score for Logistic Regression: \", logistic_regression_cls_tuned.score(x_test, y_test))\nprint(\"Test Score for Gaussian Naive Bayes: \", naive_bayes_cls_tuned.score(x_test, y_test))\nprint(\"Test Score for Decision Tree: \", decision_tree_cls_tuned.score(x_test, y_test))\nprint(\"Test Score for Random Forest: \", random_forest_cls_tuned.score(x_test, y_test))\nprint(\"Test Score for SVC: \", support_vector_cls_tuned.score(x_test, y_test))\nprint(\"Test Score for K-NN: \", knn_cls_tuned.score(x_test, y_test))","2a09319d":"from sklearn.metrics import confusion_matrix  # For find confusion matrix\nfrom sklearn.metrics import classification_report  # For print evaluation report\nfrom sklearn.metrics import roc_curve  # For drawing ROC curve\nfrom sklearn.metrics import auc  # For find AUC","595c64de":"# Prediction of test dataset:\ny_pred_LR = logistic_regression_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_LR = confusion_matrix(y_test, y_pred_LR)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_LR = pd.DataFrame(confusion_matrix_LR, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_LR, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"Logistic Regression\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","2f71d05e":"report_LR = pd.DataFrame(classification_report(y_test, y_pred_LR, \n                                               output_dict=True, \n                                               target_names=[\"Edible\", \"Poisonous\"]))\nreport_LR","c985ee9a":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_LR, true_positive_rate_LR, thresholds_LR = roc_curve(y_test, y_pred_LR)\n\n# Find AUC value:\nauc_LR = auc(false_positive_rate_LR, true_positive_rate_LR)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_LR, true_positive_rate_LR, label=\"AUC = %0.2f\"%auc_LR)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for Logistic Regression\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","36c80cbd":"# Prediction of test dataset:\ny_pred_GNB = naive_bayes_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_GNB = confusion_matrix(y_test, y_pred_GNB)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_GNB = pd.DataFrame(confusion_matrix_GNB, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_GNB, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"Gaussian Naive Bayes\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","897dc463":"report_GNB = pd.DataFrame(classification_report(y_test, y_pred_GNB, \n                                                output_dict=True, \n                                                target_names=[\"Edible\", \"Poisonous\"]))\nreport_GNB","de35cd1c":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_GNB, true_positive_rate_GNB, thresholds_GNB = roc_curve(y_test, y_pred_GNB)\n\n# Find AUC value:\nauc_GNB = auc(false_positive_rate_GNB, true_positive_rate_GNB)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_GNB, true_positive_rate_GNB, label=\"AUC = %0.2f\"%auc_GNB)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for Gaussian Naive Bayes\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","9c693e31":"# Prediction of test dataset:\ny_pred_DT = decision_tree_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_DT = confusion_matrix(y_test, y_pred_DT)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_DT = pd.DataFrame(confusion_matrix_DT, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_DT, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"Decision Tree Classifier\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","8242bdc1":"report_DT = pd.DataFrame(classification_report(y_test, y_pred_DT, \n                                               output_dict=True, \n                                               target_names=[\"Edible\", \"Poisonous\"]))\nreport_DT","97d9bda3":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_DT, true_positive_rate_DT, thresholds_DT = roc_curve(y_test, y_pred_DT)\n\n# Find AUC value:\nauc_DT = auc(false_positive_rate_DT, true_positive_rate_DT)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_DT, true_positive_rate_DT, label=\"AUC = %0.2f\"%auc_DT)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for Decision Tree Classifier\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","7bebe2c6":"# Prediction of test dataset:\ny_pred_RF = random_forest_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_RF = confusion_matrix(y_test, y_pred_RF)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_RF = pd.DataFrame(confusion_matrix_RF, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_RF, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"Random Forest Classifier\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","5ad16b7b":"report_RF = pd.DataFrame(classification_report(y_test, y_pred_RF, \n                                               output_dict=True, \n                                               target_names=[\"Edible\", \"Poisonous\"]))\nreport_RF","aff3e99a":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_RF, true_positive_rate_RF, thresholds_RF = roc_curve(y_test, y_pred_RF)\n\n# Find AUC value:\nauc_RF = auc(false_positive_rate_RF, true_positive_rate_RF)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_RF, true_positive_rate_RF, label=\"AUC = %0.2f\"%auc_RF)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for Random Forest Classifier\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","e66c3d05":"# Prediction of test dataset:\ny_pred_SVC = support_vector_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_SVC = confusion_matrix(y_test, y_pred_SVC)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_SVC = pd.DataFrame(confusion_matrix_SVC, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_SVC, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"SVC\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","b9dfbe0e":"report_SVC = pd.DataFrame(classification_report(y_test, y_pred_SVC, \n                                                output_dict=True, \n                                                target_names=[\"Edible\", \"Poisonous\"]))\nreport_SVC","55774855":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_SVC, true_positive_rate_SVC, thresholds_SVC = roc_curve(y_test, y_pred_SVC)\n\n# Find AUC value:\nauc_SVC = auc(false_positive_rate_SVC, true_positive_rate_SVC)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_SVC, true_positive_rate_SVC, label=\"AUC = %0.2f\"%auc_SVC)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for SVC\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","bc2201c7":"# Prediction of test dataset:\ny_pred_KNN = knn_cls_tuned.predict(x_test)\n\n# Find confusion matrix for this model:\nconfusion_matrix_KNN = confusion_matrix(y_test, y_pred_KNN)\n\n# Plot confusion matrix with Heatmap:\ncm_dataframe_KNN = pd.DataFrame(confusion_matrix_KNN, index=[\"Edible\", \"Poisonous\"], columns=[\"Edible\", \"Poisonous\"])\nsns.heatmap(cm_dataframe_KNN, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\nplt.title(\"K-NN Classifier\")\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show()","70b45add":"report_KNN = pd.DataFrame(classification_report(y_test, y_pred_KNN, \n                                                output_dict=True, \n                                                target_names=[\"Edible\", \"Poisonous\"]))\nreport_KNN","4f44a884":"# Find parameters for drawing ROC curve:\nfalse_positive_rate_KNN, true_positive_rate_KNN, thresholds_KNN = roc_curve(y_test, y_pred_KNN)\n\n# Find AUC value:\nauc_KNN = auc(false_positive_rate_KNN, true_positive_rate_KNN)\n\n# Draw ROC curve:\nplt.figure(figsize=(12, 12))\nplt.plot(false_positive_rate_KNN, true_positive_rate_KNN, label=\"AUC = %0.2f\"%auc_KNN)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.legend(loc='lower right')\nplt.title(\"Receiver Operating Characteristic(ROC) for K-NN Classifier\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")","104cfdca":"# Create list that keep model names:\nmodel_names = [\"Logistic Regression\", \n               \"Gaussian Naive Bayes\",\n               \"Decision Tree Classification\",\n               \"Random Forest Classification\",\n               \"Support Vector Classification\",\n               \"K-NN Classifiaction\"]\n\n# Shortening models' variables names for quick usage:\nLR_model = logistic_regression_cls_tuned\nGNB_model = naive_bayes_cls_tuned\nDT_model = decision_tree_cls_tuned\nRF_model = random_forest_cls_tuned\nSVC_model = support_vector_cls_tuned\nKNN_model = knn_cls_tuned\n\n# Get model hyperparameters into variables:\nLR_model_C = str(LR_model.get_params()[\"C\"])\nLR_model_penalty = str(LR_model.get_params()[\"penalty\"])\nDT_mode_max_depth = str(DT_model.get_params()[\"max_depth\"])\nRF_model_n_estimators = str(RF_model.get_params()[\"n_estimators\"])\nRF_model_max_depth = str(RF_model.get_params()[\"max_depth\"])\nSVC_model_kernel = str(SVC_model.get_params()[\"kernel\"])\nSVC_model_C = str(SVC_model.get_params()[\"C\"])\nSVC_model_gamma = str(SVC_model.get_params()[\"gamma\"])\nKNN_model_n_neighbors = str(KNN_model.get_params()[\"n_neighbors\"])\n\n# Create list that keep model hyperparameters:\nmodel_params = [\"C=\" + LR_model_C + \", penalty=\" + LR_model_penalty,\n                \"None\",\n                \"max_depth=\" + DT_mode_max_depth,\n                \"n_estimators=\" + RF_model_n_estimators + \", max_depth=\" + RF_model_max_depth,\n                \"kernel=\" + SVC_model_kernel + \", C=\" + SVC_model_C + \", gamma=\" + SVC_model_gamma,\n                \"n_neighbors=\" + KNN_model_n_neighbors]\n\n# Create list that keep models' training accuracies:\nmodel_training_accuracies = [LR_model.score(x_train, y_train),\n                             GNB_model.score(x_train, y_train),\n                             DT_model.score(x_train, y_train),\n                             RF_model.score(x_train, y_train),\n                             SVC_model.score(x_train, y_train),\n                             KNN_model.score(x_train, y_train)]\n\n# Create list that keep models' testing accuracies:\nmodel_testing_accuracies = [LR_model.score(x_test, y_test),\n                             GNB_model.score(x_test, y_test),\n                             DT_model.score(x_test, y_test),\n                             RF_model.score(x_test, y_test),\n                             SVC_model.score(x_test, y_test),\n                             KNN_model.score(x_test, y_test)]\n\n# Create list that keep models' F1 scores:\nmodel_f1_scores = [report_LR.iloc[0][\"weighted avg\"],\n                   report_GNB.iloc[0][\"weighted avg\"],\n                   report_DT.iloc[0][\"weighted avg\"],\n                   report_RF.iloc[0][\"weighted avg\"],\n                   report_SVC.iloc[0][\"weighted avg\"],\n                   report_KNN.iloc[0][\"weighted avg\"]]\n\n# Create list that keep models' precisions:\nmodel_precisions = [report_LR.iloc[1][\"weighted avg\"],\n                   report_GNB.iloc[1][\"weighted avg\"],\n                   report_DT.iloc[1][\"weighted avg\"],\n                   report_RF.iloc[1][\"weighted avg\"],\n                   report_SVC.iloc[1][\"weighted avg\"],\n                   report_KNN.iloc[1][\"weighted avg\"]]\n\n# Create list that keep models' recalls:\nmodel_recalls = [report_LR.iloc[2][\"weighted avg\"],\n                report_GNB.iloc[2][\"weighted avg\"],\n                report_DT.iloc[2][\"weighted avg\"],\n                report_RF.iloc[2][\"weighted avg\"],\n                report_SVC.iloc[2][\"weighted avg\"],\n                report_KNN.iloc[2][\"weighted avg\"]]\n\n# Create list that keep models' AUC values:\nmodel_AUC_values = [auc_LR, auc_GNB, auc_DT, auc_RF, auc_SVC, auc_KNN]\n\n# Generate table data with column names:\ntable_data = {\"Parameters\" : model_params,\n              \"Training Accuracy\" : model_training_accuracies,\n              \"Testing Accuracy\" : model_testing_accuracies,\n              \"F1 Score\" : model_f1_scores,\n              \"Precision\" : model_precisions,\n              \"Recall\" : model_recalls,\n              \"AUC\" : model_AUC_values}\n\n# Create and print result table:\ntable_dataframe = pd.DataFrame(data=table_data, index=model_names)\ntable_dataframe","cdd0b832":"table_dataframe.iloc[:, 1:].plot(kind=\"bar\", ylim=[0.8, 1.0], figsize=(14, 9)) # Y Limit: 0.8 - 1.0\nplt.legend(loc='lower right')\nplt.show()","599ff680":"Now let's list the parameters and current values of K-NN Classifier model:","50559b30":"<a id=\"43\"><\/a>\n## 4.3. Scaling of Data\n\nHere we need to standardize them so that we can use the data properly. To do this, we need to apply the Scaling process to the data. Scikit-Learn's \"_StandardScaler_\" class can be used for this process:","2fd4b4a3":"<a id=\"64\"><\/a>\n## 6.4. Evaluating Random Forest Classifier Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","3ab8c173":"Now let's list the parameters and current values of the Logistic Regression model:","e6c6439e":"<a id=\"521\"><\/a>\n### 5.2.1. Logistic Regression","4846647c":"<a id=\"9\"><\/a>\n# REFERENCES\n\n* https:\/\/datascienceplus.com\/mushrooms-classification-part-1\/\n* https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/\n\n_Best regards..._ <br>\n**Mustafa YEMURAL**","22d406da":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","2bc03996":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","159c336f":"<a id=\"32\"><\/a>\n## 3.2. Label Encoding\n\nFor Label Encoding we will use the \"_LabelEncoder_\" class of the Scikit-Learn library. With the \"*fit_transform*\" method of this class, we will be able to cycle and transform values through all columns. Let's write our code as follows:","0c543e70":"Now let's print the various statistics that determine the performance of our model as a table:","285c6493":"There are 8124 records in this dataset. Each record is indexed from 0 to 8123. In addition, there is no missing value in this dataset. In the \"_stalk-root_\" column, the data indicated by \"_?_\" will be taken as \"_None_\". Therefore, there is no need to work for missing values in data cleaning. There are 23 columns in total and all of them are \"_object_\" type. Now, let's explain what the columns mean:\n\n* **class :** It is the column that we will use as the target, indicating whether the mushroom is edible or poisonous. The values and the meaning of values of this column are as follows:\n\t* _e :_ Edible\n\t* _p :_ Poisonous\n* **cap-shape :** It specifies the shape of the mushroom cap. The values that can be taken are:\n\t* _b :_ Bell\n\t* _c :_ Conical\n\t* _x :_ Convex\n\t* _f :_ Flat\n\t* _k :_ Knobbed\n\t* _s :_ Sunken or Depressed\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-cap-shape.jpg)\n<br>\n* **cap-surface :** It specifies the shape on the surface of the cap of the mushroom. The values that can be taken are:\n\t* _f :_ Fibrous\n\t* _g :_ Grooves\n\t* _y :_ Scaly\n\t* _s :_ Smooth\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-cap-surface.jpg)\n<br>\n* **cap-color :** It specifies the color of the cap of the mushroom. The values that can be taken are:\n\t* _n :_ Brown\n\t* _b :_ Buff\n\t* _c :_ Cinnamon\n\t* _g :_ Gray\n\t* _r :_ Green\n\t* _p :_ Pink\n\t* _u :_ Purple\n\t* _e :_ Red\n\t* _w :_ White\n\t* _y :_ Yellow\n* **bruises :** Specifies whether the mushroom has bruises. The values that can be taken are:\n\t* _t :_ Exists\n\t* _f :_ Not Exists\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/df\/Gyroporus_cyanescens_37226.jpg\/640px-Gyroporus_cyanescens_37226.jpg)\n<br>\n* **odor :** Indicates the smell of mushrooms. The values that can be taken are:\n\t* _a :_ Almond\n\t* _l :_ Anise\n\t* _c :_ Creosote\n\t* _y :_ Fishy\n\t* _f :_ Foul\n\t* _m :_ Musty\n\t* _n :_ None\n\t* _p :_ Pungent\n\t* _s :_ Spicy\n* **gill-attachment :** It specifies the connection between mushroom gill and its stem. The values that can be taken are:\n\t* _a :_ Attached\n\t* _d :_ Descending\n\t* _f :_ Free\n\t* _n :_ Notched\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-gill-attachment.jpg)\n<br>\n* **gill-spacing :** It specifies the spaces among mushroom gills. The values that can be taken are:\n\t* _c :_ Close\n\t* _w :_ Crowded\n\t* _d :_ Distant\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-gill-spacing.jpg)\n<br>\n* **gill-size :** It specifies the size of mushroom gills. The values that can be taken are:\n\t* _b :_ Broad\n\t* _n :_ Narrow\n* **gill-color :** It specifies the color of mushroom gills. The values that can be taken are:\n\t* _k :_ Black\n\t* _n :_ Brown\n\t* _b :_ Buff\n\t* _h :_ Chocolate\n\t* _g :_ Gray\n\t* _r :_ Green\n\t* _o :_ Orange\n\t* _p :_ Pink\n\t* _u :_ Purple\n\t* _e :_ Red\n\t* _w :_ White\n\t* _y :_ Yellow\n* **stalk-shape :** It specifies the shape of the mushroom stem. The values that can be taken are:\n\t* _e :_ Enlarging\n\t* _t :_ Tapering\n* **stalk-root :** It specifies the shape of the mushroom root. The values that can be taken are:\n\t* _b :_ Bulbous\n\t* _c :_ Club Shaped\n\t* _u :_ Cup or Volva\n\t* _e :_ Equal\n\t* _z :_ Rhizomorphs\n\t* _r :_ Rooted\n\t* _? :_ Missing or None\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-stalk.jpg)\n<br>\n* **stalk-surface-above-ring :** It specifies the surface of the body piece above the ring. The values that can be taken are:\n\t* _f :_ Fibrous\n\t* _y :_ Scaly\n\t* _k :_ Silky\n\t* _s :_ Smooth\n* **stalk-surface-below-ring :** It specifies the surface of the body piece below the ring. The values that can be taken are:\n\t* _f :_ Fibrous\n\t* _y :_ Scaly\n\t* _k :_ Silky\n\t* _s :_ Smooth\n* **stalk-color-above-ring :** It specifies the color of the body piece above the ring. The values that can be taken are:\n\t* _n :_ Brown\n\t* _b :_ Buff\n\t* _c :_ Cinnamon\n\t* _g :_ Gray\n\t* _o :_ Orange\n\t* _p :_ Pink\n\t* _e :_ Red\n\t* _w :_ White\n\t* _y :_ Yellow\n* **stalk-color-below-ring :** It specifies the color of the body piece below the ring. The values that can be taken are:\n\t* _n :_ Brown\n\t* _b :_ Buff\n\t* _c :_ Cinnamon\n\t* _g :_ Gray\n\t* _o :_ Orange\n\t* _p :_ Pink\n\t* _e :_ Red\n\t* _w :_ White\n\t* _y :_ Yellow\n* **veil-type :** It specifies the veil type. The values that can be taken are:\n\t* _p :_ Partial\n\t* _u :_ Universal\n* **veil-color :** It specifies the veil color. The values that can be taken are:\n\t* _n :_ Brown\n\t* _o :_ Orange\n\t* _w :_ White\n\t* _y :_ Yellow\n* **ring-number :** It specifies the ring numbers in stem of the mushroom. The values that can be taken are:\n\t* _n :_ None\n\t* _o :_ One\n\t* _t :_ Two\n* **ring-type :** It specifies the shape of the ring in stem of the mushroom. The values that can be taken are:\n\t* _c :_ Cobwebby\n\t* _e :_ Evanescent\n\t* _f :_ Flaring\n\t* _l :_ Large\n\t* _n :_ None\n\t* _p :_ Pendant\n\t* _s :_ Sheathing\n\t* _z :_ Zone\n![](https:\/\/datascienceplus.com\/wp-content\/uploads\/2018\/02\/mushroom-ring-type.jpg)\n<br>\n* **spore-print-color :** It specifies the color of mushroom spores. The values that can be taken are:\n\t* _k :_ Black\n\t* _n :_ Brown\n\t* _b :_ Buff\n\t* _h :_ Chocolate\n\t* _r :_ Green\n\t* _o :_ Orange\n\t* _u :_ Purple\n\t* _w :_ White\n\t* _y :_ Yellow\n* **population :** It specifies the rate at which the mushroom is present in the population. The values that can be taken are:\n\t* _n :_ Numerous\t\n\t* _a :_ Abundant\n\t* _v :_ Several\n\t* _c :_ Clustered\n\t* _s :_ Scattered\n\t* _y :_ Solitary\n* **habitat :** It specifies the habitat the mushroom is in. The values that can be taken are:\n\t* _g :_ Grasses\n\t* _l :_ Leaves\n\t* _m :_ Meadows\n\t* _p :_ Paths\n\t* _u :_ Urban\n\t* _w :_ Waste\n\t* _d :_ Woods\n\nWe gave the necessary information about dataset. Now, looking at the first 5 and last 5 entries of dataset, what are the values that are being held:","9497c189":"<a id=\"523\"><\/a>\n### 5.2.3. Random Forest","3a5b48a5":"<a id=\"44\"><\/a>\n## 4.4 Principal Component Analysis (PCA)\n\nAlthough the PCA method is used for many different purposes, it is usually used to reduce the number of features. We will also use it here for this purpose. I would like to remind you that after the last steps we have 20 features. Now we'll reduce the number of these features, but we will try to keep the variance high while doing so. Because if the variance value of our data falls, information loss occurs and this is undesirable. Then let's first see how much variance you have with PCA on how many features:","dfeed40e":"<a id=\"525\"><\/a>\n### 5.2.5. K-NN","14cf3bf8":"<a id=\"34\"><\/a>\n## 3.4. Various Visualizations from Dataset\n\nIn this section we will perform various visualization operations related to dataset. First, let's take a look at the statistics of the columns with a correlation greater than 0.3 and less than -0.3 and the \"_class_\" column:","2766f22f":"<a id=\"513\"><\/a>\n### 5.1.3. Decision Tree\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:","28679a2d":"Now let's print the various statistics that determine the performance of our model as a table:","5a12a105":"Now let's list the parameters and current values of Gaussian Naive Bayes Classifier model:","cb58addf":"Now let's list the parameters and current values of the SVC model:","6f955fdb":"<a id=\"4\"><\/a>\n# 4. DATA PREPARATION AND CLEANING\n\nIn this section, we will perform data cleaning and preprocessing. Since I'm going to use Column names as a string, I won't make any edits to them. Also, because missing values are not available in this dataset, I also skip \"Handling Missing Values\". In this section we will see the PCA (Principal Component Analysis) topic.","db38d2e7":"<a id=\"41\"><\/a>\n## 4.1. Drop Irrelevant Columns\n\nWe'll delete columns from dataset, which doesn't mean much to us here. In this section and later sections, we will go through \"encoded_data\" which has Label Encoding process and fit to the models accordingly. Now, delete the \"_cap-shape_\" and \"_veil-type_\" columns that don't have much to do with other attributes from the dataset:","6bc67a61":"<a id=\"5\"><\/a>\n# 5. BUILDING MODELS\n\nIn this section we will fit various classification models with our data. We will also perform the process of tunning the hyperparameter of the models. In doing so, we will use the Grid Search method. We will create new models that are tune with the parameters we found and we will fit our data to them. The classification algorithms that we will use in this kernel are:\n\n1. Logistic Regression\n1. Gaussian Naive Bayes Classification\n1. Decision Tree Classification\n1. Random Forest Classification\n1. Support Vector Classification\n1. K-Nearest Neighbour Classification\n\nNow let's do our import operations for these models and for Grid Search Cross Validation:","c71b33a0":"We will only perform the \"*n_neighbors*\" parameter.","3ad81c43":"As you can see, the Gaussian Naive Bayes model does not have any hyperparameter to tune, so we don't need to take any action on it.","e9850104":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","df200ab1":"<a id=\"524\"><\/a>\n### 5.2.4. SVC (with RBF)","15378e0c":"<a id=\"45\"><\/a>\n## 4.5. Preparation of Test and Train Data\n\nThe final process here is the smooth and random separation of test and train data. For this, we will benefit from the method named \"train_testsplit\" from the Scikit-Learn library. I would like to use 20% of our data for testing and 80% for training purposes. The process is very simple:","6ba98897":"<a id=\"516\"><\/a>\n### 5.1.6. K-NN\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:","010ddf50":"For correlation and many statistical analyzes, data must be of a numerical type. So before going on to the next step, we'll do the Label Encoding event here. Label Encoding is the conversion of categorical data to the corresponding integers.","2ed045d8":"We will only perform operations related to the \"*max_depth*\" parameter.","af177c45":"<a id=\"522\"><\/a>\n### 5.2.2. Decision Tree","960c1540":"<a id=\"51\"><\/a>\n## 5.1. Building Models with Default Hyperparameters\n\nIn this section, we will fit the models by creating them with the default hyperparameters. After creating these models and fit with our data, we will calculate train and test scores. Last but not least, we will determine the hyperparameters that we will tune.","03335bfb":"Finally, let's show with the Stacked Bar Chart how much poisonous mushrooms are found in the habitats:","618b949b":"As seen above, we can maintain approximately 93% of our data by using 13 features. For this reason, we will give the number of components as 13:","56eccc20":"Now let's print the various statistics that determine the performance of our model as a table:","18448d6d":"<a id=\"514\"><\/a>\n### 5.1.4. Random Forest\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:","d6b10c95":"Now let's list the parameters and current values of Decision Tree Classifier model:","df7963b3":"<a id=\"66\"><\/a>\n## 6.6. Evaluating K-Nearest Neighbors Classifier Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","40e11766":"<a id=\"52\"><\/a>\n## 5.2. Hyperparameter Tuning with Grid Search\n\nIn this section, we will use the Grid Search technique to tune the hyperparameter of the models. In addition, 6-Fold Cross Validation process will find the scores of the parameter combinations. Finally, we will decide what the best parameters are for the model.","da80886e":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","bc31bd78":"We will do the operations related to \"*C*\" and \"*gamma*\" parameters. The \"kernel\" parameter will remain \"*rbf*\" by default.","baae6b90":"Now let's print the various statistics that determine the performance of our model as a table:","f455b1b2":"## TABLE OF CONTENTS\n\n* [1. INTRODUCTION](#1)\n* [2. USED LIBRARIES](#2)\n* [3. DATA EXPLORATION](#3)\n    * [3.1. Detailed Information of the Dataset](#31)\n    * [3.2. Label Encoding](#32)\n    * [3.3. Correlation of Columns(Attributes)](#33)\n    * [3.4. Various Visualizations from Dataset](#34)\n* [4. DATA PREPARATION AND CLEANING](#4)\n    * [4.1. Drop Irrelevant Columns](#41)\n    * [4.2. Split Data and Target](#42)\n    * [4.3. Scaling of Data](#43)\n    * [4.4. Principal Component Analysis (PCA)](#44)\n    * [4.5. Preparation of Test and Train Data](#45)\n* [5. BUILDING MODELS](#5)\n    * [5.1. Building Models with Default Hyperparameters](#51)\n        * [5.1.1. Logistic Regression](#511)\n        * [5.1.2. Gaussian Naive Bayes](#512)\n        * [5.1.3. Decision Tree](#513)\n        * [5.1.4. Random Forest](#514)\n        * [5.1.5. SVC](#515)\n        * [5.1.6. K-NN](#516)\n    * [5.2. Hyperparameter Tuning with Grid Search](#52)\n        * [5.2.1. Logistic Regression](#521)\n        * [5.2.2. Decision Tree](#522)\n        * [5.2.3. Random Forest](#523)\n        * [5.2.4. SVC (with RBF)](#524)\n        * [5.2.5. K-NN](#525)\n    * [5.3. Rebuild Models with Tuned Hyperparameters](#53)\n* [6. EVALUATING MODELS](#6)\n    * [6.1. Evaluating Logistic Regression Model](#61)\n    * [6.2. Evaluating Gaussian Naive Bayes Model](#62)\n    * [6.3. Evaluating Decision Tree Classifier Model](#63)\n    * [6.4. Evaluating Random Forest Classifier Model](#64)\n    * [6.5. Evaluating Support Vector Classifier Model](#65)\n    * [6.6. Evaluating K-Nearest Neighbors Classifier Model](#66)\n* [7. EXPLORATION OF RESULTS](#7)\n* [8. CONCLUSION](#8)\n* [REFERENCES](#9)","2dd7a55d":"We kept them in separate variables. Because sometimes categorical data will be useful in some statistics. Finally, let's examine the output of our \"_describe_\" function and go to the next section:","a2287d24":"<a id=\"8\"><\/a>\n# 8. CONCLUSION\n\nThis kernel contains 6 different classification algorithms and works on \"Mushroom Classification\" dataset. In addition, EDA (Exploratory Data Analysis) was performed for dataset. I tried to give you the following information on this kernel:\n\n* How to make a dataset's EDA (Exploratory Data Analysis) process simply\n* How libraries such as Pandas, Matplotlib, Seaborn and Scikit-Learn are used in classification models\n* How to convert category data to numerical data with Label Encoding\n* How we can graphically show the relationship between the various columns\n* How to draw graphics such as Pie Chart, Bar Chart, Stacked Bar Chart and Heatmap with Seaborn and Matplotlib\n* How to perform Scaling on data\n* What is Principal Component Analysis (PCA) and how to do it simply\n* Installation, use, optimization and evaluation of models such as Logistic Regression, Gaussian Naive Bayes, Decision Tree Classification, Random Forest Classification, SVC and K-NN Classification\n* Performing Hyperparameter Tuning to the models with the Grid Search method and finding the most suitable parameters\n* Extraction of Confusion Matrix for classification models and detailed description of Confusion Matrix\n* Calculation of classification models such as F1 Score, Recall, Precision and Accuracy\n* Drawing of ROC Curves of classification models and calculation of AUC (Area Under Curve) values\n* Simple comparison of classification models and their results\n\nYou can use the codes in this kernel as desired. Please read to its license for dataset usage. About 5 hours of writing this kernel was deleted and I had to write them all over again. So if you see some of my mistakes in between, that's why. Good work...","460c2234":"Now let's print the various statistics that determine the performance of our model as a table:","02d5afd7":"<a id=\"515\"><\/a>\n### 5.1.5. SVC\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:","5ba45efb":"<a id=\"63\"><\/a>\n## 6.3. Evaluating Decision Tree Classifier Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","2ebd1d47":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","81928c48":"Now we visualize this correlation matrix with Heatmap:","0539b48b":"Now let's list the parameters and current values of Random Forest Classifier model:","137b7ad0":"<a id=\"3\"><\/a>\n# 3. DATA EXPLORATION\n\nIn this section, detailed information about dataset will be given and various visualizations will be made about features in dataset. In addition, during the \"Data Preprocessing\" step, the Label Encoding process will be performed. Because we need to convert them from \"_object_\" to numeric types so we can find the correlation between features. As you can see from this section, there will also be a correlation between features and will be plotted with the Heatmap.","a1fcc132":"We cannot see any statistical data or any correlation in the \"*veil-type*\" column. This is because this attribute has a single value for all records. This means that it is a useless column for us. In the future we will drop this column from dataset.","62b160e0":"<a id=\"33\"><\/a>\n## 3.3. Correlation of Columns(Attributes)\n\nIn this section, we'll find the correlation matrix between the columns and we'll visualize it into a Heatmap. In this way, we will be able to see the relationship between the attributes more clearly and visualize them in the future.","0023c3b0":"<a id=\"512\"><\/a>\n### 5.1.2. Gaussian Naive Bayes\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:","ab21ff75":"Now let's show the distribution of the edible and poisonous classes with the Pie Chart:","864bd9b8":"<a id=\"1\"><\/a>\n# 1. INTRODUCTION\n\nIn this study, EDA (Exploratory Data Analysis) related to \"Mushroom Classification\" dataset will be made and various classification models such as Logistic Regression, Gaussian Naive Bayes, Decision Tree, Random Forest, SVM and K-NN will be created. Then the dataset will be fit with these models and the results will be observed. In addition, in this study, a simple level of dimensionality reduction will be performed by PCA (Principal Component Analysis) method which plays an important role in reducing the number of features. In addition to this, the hyperparameter will be tuned with the Grid Search technique for each model. We will plot Confusion Matrix and ROC Curve for each model. Dataset has 8124 mushrooms with 23 different attributes. We'll create models that find poisonous or edible mushrooms given feature with classification algorithms.","d548f2a4":"<a id=\"65\"><\/a>\n## 6.5. Evaluating Support Vector Classifier Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","588dfd0d":"<a id=\"7\"><\/a>\n# 7. EXPLORATION OF RESULTS\n\nIn this section we will examine the results from the models. First, let's show the results of all models in a single table:","dc7354c6":"<a id=\"42\"><\/a>\n## 4.2. Split Data and Target\n\nHere we separate the data and target parts of the dataset and assign them to variables:","bc71db13":"<a id=\"61\"><\/a>\n## 6.1. Evaluating Logistic Regression Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","caf69a7e":"You can find out what letter in the above description. Let's move on to the next topic without much visualization. Because this is the main subject of the kernel classification algorithms and their comparison to the foreground should be more important.","9a726b86":"We will do the operations related to \"*C*\" and \"_penalty_\" parameters.","096a091a":"<a id=\"53\"><\/a>\n## 5.3. Rebuild Models with Tuned Hyperparameters\n\nIn this section, models with the best hyperparameter will be rebuilded and fitted with the data. Then the test data and test scores will be calculated. Let's do all this with one code:","5ede6b23":"As you can see, the results are quite nice. The reason for this is that the dataset has a sufficient number of data, or it can be tuned hyperparameters or PCA (Principal Component Analysis). Finally, let's graph all these results and finish the section:","633b1299":"Finally, draw the ROC Curve and find the AUC (Area Under Curve) value:","87bdaf34":"Now let's take a look at the data we encode:","8c3aae65":"<a id=\"62\"><\/a>\n## 6.2. Evaluating Gaussian Naive Bayes Model\n\nAfter giving the model test data and prediction, let's take out the Confusion Matrix for our model:","58afc2f5":"<a id=\"2\"><\/a>\n# 2. USED LIBRARIES\n\nThis section will give information about Python libraries to be used in the study and these libraries will be imported into the project. Here are the libraries and explanations we will use:\n\n* **NumPy :** This library is actually a dependency for other libraries. The main purpose of this library is to provide a variety of mathematical operations on matrices and vectors in Python. Our project will be used this library to provide support to other libraries.\n* **Pandas :** This library performs import and processing of dataset in Python. In our project, it will be used to include the CSV extension dataset in the project and to perform various operations on it.\n* **Matplotlib :** This library, which is usually used to visualize data. It will perform the same task in our project.\n* **Seaborn :** This library which has similar features to Matplotlib is another library used for data visualization in Python. In our project, it will be used for the implementation of various features not included in the Matplotlib library.\n* **Sckit-Learn :** This library includes the implementation of various machine larning algorithms. With this library, we will perform all operations from building to evaluation of regression models using functions and classes in this library.\n\nNow let's import NumPy, Pandas, Matplotlib and Seaborn libraries into our project and get them ready for use:","d432f054":"<a id=\"6\"><\/a>\n# 6. EVALUATING MODELS\n\nIn this section we will do various measurements to test the performance of the models. Confusion Matrix for each model to be printed. Then there will be values such as precision and f1 score. To do this, we will take advantage of a function found in the Scikit-Learn library, which finds all of these values at the same time. Finally, the ROC Curve of the corresponding model will be drawn. Let's start by talking about some basic concepts. First, let's see what the Confusion Matrix is:\n\n![](https:\/\/cdncontribute.geeksforgeeks.org\/wp-content\/uploads\/Confusion_Matrix1_1.png)\n\nThe \"Actual\" lines that you see in this image indicate the actual values of the classes, while the \"Predicted\" columns specify the estimates of the classes from the model. In this case, the values and meanings of the table are as follows:\n\n* **True Positive(TP) :** Specifies that the data count belonging to the \"Class 1\" class is estimated to be \"Class 1\".\n* **True Negative(TN) :** Specifies that the data count belonging to the \"Class 2\" class is estimated to be \"Class 2\".\n* **False Positive(FP) :** Specifies that the data count belonging to the \"Class 2\" class is estimated to be \"Class 1\". (Type I Error)\n* **False Negative(FN) :** Specifies that the data count belonging to the \"Class 1\" class is estimated to be \"Class 2\". (Type II Error)\n\nSome important information and formulas can be extracted from this matrix. Some of these formulas and their meanings are as follows:\n\n* **Accuracy :** Specifies the percentage of total correct estimates. Formula: \"_Accuracy = (TP + TN) \/ (TP + TN + FP + FN)_\"\n* **Classification Error :** Specifies the percentage of total wrong estimates. Formula: \"_Classification Error = (FP + FN) \/ (TP + TN + FP + FN)_\"\n* **Sensitivity (True Positive Rate, Recall) :** Assuming that all values tested are \"Class 1\", it indicates the percentage of correct estimates. Formula: \"_Recall = (TP) \/ (TP + FN)_\"\n* **Specificity :** Assuming that all values tested are \"Class 2\", it indicates the percentage of correct estimates. Formula: \"_Specificity = (TN) \/ (TN + FP)_\"\n* **False Positive Rate :** Assuming that all values tested are \"Class 2\", it indicates the percentage of wrong estimates. Formula: \"_True Positive Rate = (FP) \/ (TN + FP)_\"\n* **Precision :** Specifies the proportion of correct estimates only in a case where the class \"Class 1\" is estimated. Formula: \"_Precision = (TP) \/ (TP + FP)_\"\n* **F1 Score :** Specifies the harmonic mean of Recall and Precision values. Formula: \"_F1 Score = (2 * Precision * Recall) \/ (Precision + Recall)_\"\n\nBesides these values, there is a value called \"Threshold\" or \"Cut-off\". This value is usually valid in Binary Classification. Specifies the probability that the prediction in Binary Classification is true and the default value is 0.5. We will not take any action on optimization of the Threshold value here. Let's start by importing the library and functions we will use now:","bde97b43":"Now let's print the various statistics that determine the performance of our model as a table:","39b979ac":"<a id=\"31\"><\/a>\n## 3.1. Detailed Information of the Dataset\n\nFirst of all we import our dataset and look at the quantities related to the columns with the \"_info_\" method:","6c9654e7":"We will do the operations with parameters \"*n_estimators*\" and \"*max_depth*\".","4b39b930":"<a id=\"511\"><\/a>\n### 5.1.1. Logistic Regression\n\nFirst, we create the default model and fit it with our data. Then we calculate train and test scores:"}}