{"cell_type":{"56f42008":"code","92960c31":"code","76efd221":"code","d2a3e0f0":"code","c9334c73":"code","b2a49431":"code","f4c83073":"code","52393024":"code","eef17488":"code","06fb9e26":"code","eddb1d52":"code","6ea2e3ce":"code","a99c17e9":"code","6877ba5a":"code","7c768911":"code","0d6766fb":"code","a5bc1388":"code","e1040359":"code","769771f7":"markdown","f44786a4":"markdown","50473f67":"markdown","518f0188":"markdown","39b44736":"markdown","6642c16c":"markdown"},"source":{"56f42008":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport plotly.graph_objects as go\nimport plotly.express as px\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","92960c31":"nRowsRead = None #1000 # specify 'None' if want to read whole file\nusa_jobs = pd.read_csv('\/kaggle\/input\/data_scientist_united_states_job_postings_jobspikr.csv', delimiter=',', nrows = nRowsRead)\nusa_jobs.dataframeName = 'data_scientist_united_states_job_postings_jobspikr.csv'\nnRow, nCol = usa_jobs.shape\nprint(f'There are {nRow} rows and {nCol} columns')","76efd221":"usa_jobs.head(5)","d2a3e0f0":"usa_jobs['crawl_timestamp'] = pd.to_datetime(usa_jobs['crawl_timestamp'])\n#Finding earliest and latest posting\nprint(f\"Earliest job post in the set: {min(usa_jobs['crawl_timestamp'])}\")\nprint(f\"Latest post in the set: {max(usa_jobs['crawl_timestamp'])}\")","c9334c73":"requirements = {\"powerbi\":0, \" r \":0, \"tableau\":0, \"qlikview\":0, \"python\":0, \"sql\":0, \"machine learning\":0,'linux':0, 'c#':0, \\\n\" ml \":0, \"hive\":0, \"spark\":0, \"hadoop\":0, \"java\":0, \"scala\":0, \"kafka\":0, \"bachelor\":0, \"master\":0, \"phd\":0, 'year':0, 'years':0, \"c++\":0}\nfor i in range(len(usa_jobs)):\n    job_description = usa_jobs.job_description[i].lower().replace(\"\\n\", \" \")\n    for k in requirements:\n        if k in job_description:\n            requirements[k] += 1\nrequirements['machine learning'] += requirements[' ml ']\nrequirements['year'] += requirements['years']\nrequirements['years experience'] = requirements.pop('year')\ndel requirements['years']\ndel requirements[' ml ']","b2a49431":"from collections import OrderedDict\nsorted_req = OrderedDict(sorted(requirements.items(), key=lambda x:x[1]))\nplt.figure(figsize=(10, 10))\nplt.bar(range(len(sorted_req)), list(sorted_req.values()), align='center')\nplt.xticks(range(len(sorted_req)), list(sorted_req.keys()), rotation='vertical')\nplt.xlabel(\"job requirement\")\nplt.ylabel(\"Number of posts\")\nplt.show()","f4c83073":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english'))\ndescription_example = usa_jobs.job_description[2].lower()\nword_tokens = word_tokenize(description_example)\nfiltered_description = [w for w in word_tokens if not w in stop_words]\nfiltered_description = \" \".join(filtered_description)","52393024":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom matplotlib.pyplot import plot\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","eef17488":"aggregate_descriptions = \" \".join(job_description.lower() \n                      for job_description in usa_jobs.job_description)\nstopwords = set(STOPWORDS)","06fb9e26":"stopwords.update(['experience', 'following', 'candidates', 'big', 'background','developing', 'characteristics', 'data', 'team', 'data', 'scientist', 'strong', 'project', \n                  'solution', 'technology', 'science', 'model', 'knowledge','skill', 'work', 'build', 'will', 'knowledge', 'application','gender', 'identity', 'equal',\n                  'opportunity','related','field', 'without', 'regard', 'national', 'origin', 'religion', 'sex', 'race', 'color', 'veteran', 'status','sexual',\n                  'orientation','opportunity', 'employer', 'qualified','applicant','skills', 'job', 'summary', 'advanced', 'system', 'applicants', 'receive', 'large', 'best', 'practice', 'problem'\n                 , 'processing', 'affirmative', 'action', 'employment', 'consideration', 'receive', 'united', 'state', 'programming', 'computer', 'working', 'saying', \n                  'preferred', 'qualification', 'disability', 'protected', 'structured', 'unstructured', 'problems', 'technical', 'internal', 'external', 'non',\n                 'subject', 'matter', 'please', 'apply', 'using', 'dental', 'reasonable', 'accomodation', 'join', 'us', 'tools', 'individuals', 'disabilities'\n                 , 'type', 'full', 'wide', 'range', 'duties', 'responsibilities', 'stakeholder', 'oral', 'written', 'ideal', 'candidate', 'ability', 'qualifications', 'well',\n                  'must', 'able', 'unit', 'member', 'posted', 'today', 'service', 'clearance', 'days', 'ago', 'high', 'quality', 'level', 'every', 'use', 'case', 'additional'])\nwordcloud = WordCloud(stopwords=stopwords, background_color='white',\n                     width=1000, height=700).generate(aggregate_descriptions)\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","eddb1d52":"states = usa_jobs.inferred_state.unique()\nsum_in_states = []\nfor state in states:\n    total_jobs_state = len(usa_jobs[usa_jobs['inferred_state']==state])\n    sum_in_states.append(int(total_jobs_state))\njobs_in_states = {'state':states, 'Total jobs':sum_in_states}\njobs_in_states = pd.DataFrame(jobs_in_states)\njobs_in_states = jobs_in_states.sort_values(by='Total jobs', ascending=False)\njobs_in_states = jobs_in_states.reset_index(drop=True)\njobs_in_states = jobs_in_states.drop(jobs_in_states.index[len(jobs_in_states)-1])\njobs_in_states[:10]","6ea2e3ce":"latitude = [32.318231,35.20105,34.048928,36.778261,39.550051,41.603221,\n38.905985,38.910832,27.664827,32.157435,19.898682,41.878003,44.068202,\n40.633125,40.551217,39.011902,37.839333,31.244823,42.407211,39.045755,\n45.253783,44.314844,46.729553,37.964253,32.354668,46.879682,35.759573,\n47.551493,41.492537,43.193852,40.058324,34.97273,38.80261,43.299428,\n40.417287,35.007752,43.804133,41.203322,41.580095,33.836081,43.969515,\n35.517491,31.968599,39.32098,37.431573,44.558803,47.751074,43.78444,\n38.597626,43.075968, 38.895]\nlongitude = [-86.902298,-91.831833,-111.093731,-119.417932,-105.782067,\n-73.087749,-77.033418,-75.52767,-81.515754,-82.907123,-155.665857,-93.097702,\n-114.742041,-89.398528,-85.602364,-98.484246,-84.270018,-92.145024,-71.382437,\n-76.641271,-69.445469,-85.602364,-94.6859,-91.831833,-89.398528,-110.362566,\n-79.0193,-101.002012,-99.901813,-71.572395,-74.405661,-105.032363,-116.419389,\n-74.217933,-82.907123,-97.092877,-120.554201,-77.194525,-71.477429,-81.163725,\n-99.901813,-86.580447,-99.901813,-111.093731,-78.656894,-72.577841,-120.740139,\n-88.787868,-80.454903,-107.290284, -77.0366]\nstate_names = ['Alabama','Arkansas','Arizona','California','Colorado','Connecticut',\n'District of columbia','Delaware','Florida','Georgia','Hawaii','Iowa',\n'Idaho','Illinois','Indiana','Kansas','Kentucky','Louisiana','Massachusetts',\n'Maryland','Maine','Michigan','Minnesota','Missouri','Mississippi',\n'Montana','North carolina','North dakota','Nebraska','New hampshire',\n'New jersey','New mexico','Nevada','New york','Ohio','Oklahoma','Oregon',\n'Pennsylvania','Rhode island','South carolina','South dakota','Tennessee',\n'Texas','Utah','Virginia','Vermont','Washington','Wisconsin','West virginia',\n'Wyoming', 'Washington d.c.']\nstate_dict = {'state':state_names, 'latitude':latitude, 'longitude':longitude}\nstate_df = pd.DataFrame(state_dict, columns=['state', 'latitude', 'longitude'])\nstate_coords = pd.merge(state_df, jobs_in_states, how='right', on='state')\nstate_coords = state_coords.sort_values(by='Total jobs', ascending=False)\nstate_coords = state_coords.reset_index(drop=True)","a99c17e9":"fig = px.scatter_geo(data_frame=state_coords, lat='latitude', scope='north america', hover_name='state',\n                    lon='longitude', size='Total jobs', projection='hammer')\nfig.show()","6877ba5a":"month_of_posting = []\nfor i in range(len(usa_jobs)):\n    month_of_posting.append(usa_jobs['crawl_timestamp'][i].month)\nusa_jobs['month'] = month_of_posting\nmonths = [x for x in range(2, 11)]\nsum_in_months = []\nfor month in months:\n    total_jobs_in_month = len(usa_jobs[usa_jobs['month']==month])\n    sum_in_months.append(total_jobs_in_month)\njobs_in_months = {'month':months, 'Total jobs':sum_in_months}\njobs_in_months = pd.DataFrame(jobs_in_months)\n\njobs_in_months = jobs_in_months.drop([8])\njobs_in_months","7c768911":"months_plot = go.Figure()\nmonths_plot.add_trace(go.Scatter(x=jobs_in_months.month, \n                                y=jobs_in_months['Total jobs']))\nmonths_plot.update_layout(title='US job posts in Data science by month in 2019',\n                         xaxis_title='Month', yaxis_title='Amount of job posts')\nmonths_plot.show()","0d6766fb":"import re\nfrom word2number import w2n\nimport statistics\ndef search_text_left_of_word(text, word, n):\n    \"\"\"Searches for a text and retrieves n words on left side of the text\"\"\"\n    words = re.findall(r'\\w+', text)\n    try:\n        index = words.index(word)\n    except ValueError:\n        return \" \"\n    return words[index - n:index]\ndef search_year_word(text):\n    return text.find('year')\ndef search_number_around_word(word_surroundings):         \n    word_surroundings = \" \".join(word_surroundings)\n    word_surroundings = word_tokenize(word_surroundings)\n    pos_tags = nltk.pos_tag(word_surroundings)\n    numbers_list = []\n    for a in pos_tags:\n        if a[1] in 'CD':\n            if a[0].isalpha(): \n                try:\n                    numbers_list.append(w2n.word_to_num(a[0]))\n                except ValueError:\n                    return \"\"\n            else:\n                numbers_list.append(a[0])\n    return numbers_list\nyears_experience_req = []\n\ndef convert_to_int(list_elem):\n    try:\n        converted_int = int(list_elem)\n        if converted_int <= 10:\n            return int(list_elem)\n    except ValueError:\n        return\nfor post_index in range (len(usa_jobs)):\n    current_job = usa_jobs.job_description[post_index]\n    word_surroundings = search_text_left_of_word(current_job, 'years', 2)\n    if current_job.find(' year ') > -1:\n        years_experience_req.append(['1'])\n    years_experience_req.append(search_number_around_word(word_surroundings))\n    #print(post_index, search_number_around_word(word_surroundings))\nyears_experience_req = [convert_to_int(item) for sublist in years_experience_req for item in sublist]\nyears_experience_req = [i for i in years_experience_req if i != None]\n#print(years_experience_req)\nprint(\"An average of \", statistics.mean(years_experience_req), \"  years is required in most job offerings. \")","a5bc1388":"#Just checking if the last 7's in the data actually correspond to \nusa_jobs.job_description[10]","e1040359":"statistics.mean([3, 4])","769771f7":"Above we can see the top 10 states with the most jobs. Let's see them plotted on the US map as well!","f44786a4":"Next we are going to create a wordcloud with the most sought-after skills in the Data Science domain!","50473f67":"This dataset is rather small, collected by scraping data from different job boards\/platforms for US in the year 2019 (from Feb 2019 to October 2019).","518f0188":"Let's take a quick look at what the data looks like and see the first and last dates of the posts:","39b44736":"Observing the demand of various hard skills and programming languages. Each time a word is mentioned in the job description, its values in the dictionary are incremented.\n","6642c16c":"It is necessary to remove all the irrelevant stopwords which usually appear in most of the job posts. I have done this by iteratively removing words which I considered not interesting for the required skill set"}}