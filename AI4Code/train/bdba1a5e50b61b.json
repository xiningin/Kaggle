{"cell_type":{"e5fd7112":"code","0b9f9931":"code","ac2d00ee":"code","5bfb1169":"code","7416a05a":"code","e282d065":"code","80258ded":"code","697dc862":"code","3f20d7fb":"code","ee361dcb":"code","d5fa7820":"code","592e7a7d":"code","3c15093d":"code","28159acc":"code","0b93a711":"code","24c68394":"code","4cf45ca5":"code","a75298b4":"code","0cea7c15":"code","2df5a13a":"code","5d6343a2":"code","cb54a765":"markdown","b90eccc7":"markdown","b7171c15":"markdown","0fa9e474":"markdown","2613c7ac":"markdown","1f199941":"markdown","e87458b1":"markdown","2771a33e":"markdown","9314e7d0":"markdown","533132e3":"markdown","2d05c58c":"markdown","59274ddd":"markdown","9c80d81d":"markdown","7f647738":"markdown","1055089b":"markdown","025c9267":"markdown","e1f1a281":"markdown","f74440e5":"markdown","cb765d72":"markdown","b5f98d0f":"markdown","378b58a2":"markdown","665103fd":"markdown","ba43c2e0":"markdown"},"source":{"e5fd7112":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re, string\nimport emoji\nimport nltk\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix","0b9f9931":"train_dataset = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding=\"ISO-8859-1\")\ntest_dataset = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding=\"ISO-8859-1\")\n\nprint(train_dataset.head())\nprint(test_dataset.head())","ac2d00ee":"train_dataset = train_dataset[['OriginalTweet','Sentiment']]\ntest_dataset = test_dataset[['OriginalTweet','Sentiment']]","5bfb1169":"def cleaner(tweet):\n    # remove links\n    tweet = \"\".join(re.sub(\"(\\w+:\\\/\\\/\\S+)\",\" \", tweet))\n    \n    # remove hashtags\n    tweet = \"\".join(re.sub(\"(#[A-Za-z0-9_]+)\",\" \", tweet))\n    \n    # remove user mention\n    tweet = \"\".join(re.sub(\"(@[A-Za-z0-9_]+)\",\" \", tweet))\n    \n    # remove none alphanumeric and aposthrope\n    tweet = \"\".join(re.sub(\"([^0-9A-Za-z \\t'])\",\" \", tweet))\n    \n    # remove extra whitespace\n    tweet = \" \".join(tweet.split())\n    \n    # remove emoji unicode \n    tweet = \"\".join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n    \n    # remove leading and trailing space\n    tweet = tweet.strip()\n    return tweet","7416a05a":"train_dataset['OriginalTweet'] = train_dataset['OriginalTweet'].apply(lambda x: cleaner(x))\ntest_dataset['OriginalTweet'] = test_dataset['OriginalTweet'].apply(lambda x: cleaner(x))\nprint(train_dataset.head())\nprint(test_dataset.head())","e282d065":"train_dataset = train_dataset[train_dataset['OriginalTweet'].apply(lambda x: len(x.split()) > 5)]\ntest_dataset = test_dataset[test_dataset['OriginalTweet'].apply(lambda x: len(x.split()) > 5)]\nprint(train_dataset.head())\nprint(test_dataset.head())","80258ded":"train_dataset['Sentiment'] = train_dataset['Sentiment'].map(\\\n             {'Extremely Negative':0,'Negative':1,'Neutral':2,'Positive':3,'Extremely Positive':4})\ntest_dataset['Sentiment'] = test_dataset['Sentiment'].map(\\\n            {'Extremely Negative':0,'Negative':1,'Neutral':2,'Positive':3,'Extremely Positive':4})","697dc862":"train_dataset['Sentiment'].value_counts()","3f20d7fb":"x_train = train_dataset['OriginalTweet'].values\ny_train = train_dataset['Sentiment'].values\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, stratify=y_train, random_state=43)\nx_test = test_dataset['OriginalTweet'].values\ny_test = test_dataset['Sentiment'].values\n\nprint(f\"Train Count: {x_train.shape[0]}\\nValidation Count: {x_valid.shape[0]}\\nTest Count: {x_test.shape[0]}\" )","ee361dcb":"ohe = preprocessing.OneHotEncoder()\ny_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\ny_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\ny_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()\n\nprint(y_train.shape)\nprint(y_valid.shape)\nprint(y_test.shape)","d5fa7820":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","592e7a7d":"MAX_LEN=128\n\ndef bert_tokenize(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for tweet in data:\n        encoded = tokenizer.encode_plus(\n            tweet,\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","3c15093d":"train_input_ids, train_attention_masks = bert_tokenize(x_train, MAX_LEN)\nval_input_ids, val_attention_masks = bert_tokenize(x_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = bert_tokenize(x_test, MAX_LEN)","28159acc":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","0b93a711":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    # parameter\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    bert = bert_model([input_ids,attention_masks])[1]\n    dropout = tf.keras.layers.Dropout(0.2)(bert)\n    output = tf.keras.layers.Dense(5, activation=\"softmax\")(dropout)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n    model.compile(optimizer, loss=loss, metrics=accuracy)\n    \n    return model","24c68394":"model = create_model(bert_model, MAX_LEN)\nmodel.summary()","4cf45ca5":"EPOCHS = 10\nBATCH_SIZE = 32\ncheckpoint_filepath = '\/BERT'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_categorical_accuracy',\n    mode='max',\n    save_best_only=True)\n\ntrain_history = model.fit([train_input_ids,train_attention_masks], \n                          y_train, \n                          validation_data=([val_input_ids,val_attention_masks], y_valid),\n                          epochs=EPOCHS, \n                          batch_size=BATCH_SIZE,\n                          callbacks=[model_checkpoint_callback])","a75298b4":"plt.plot(train_history.history['categorical_accuracy'])\nplt.plot(train_history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","0cea7c15":"plt.plot(train_history.history['loss'])\nplt.plot(train_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","2df5a13a":"# model.load_weights(checkpoint_filepath)\n\ny_pred = model.predict([test_input_ids,test_attention_masks])\n\ny_pred = np.argmax(y_pred, axis=1)\ny_test = np.argmax(y_test, axis=1)","5d6343a2":"labels = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n\naccuracy = accuracy_score(y_true=y_test, y_pred=y_pred) # Also gives the accuracy for the two lists actual and pred\nprint(\"Accuracy: %.2lf \" % (accuracy*100))\n\nprint()\nprint()\nprint(\"Classification Report,\")\nprint(classification_report(y_test,y_pred, target_names=labels))\n\nprint()\nprint()\nprint(\"Confusion Matrix,\")\ncf_matrix = confusion_matrix(y_test, y_pred)\ncf_matrix = pd.DataFrame(cf_matrix, index = labels,\n                  columns = labels)\nplt.figure(figsize = (10,7))\nsns.heatmap(cf_matrix, annot=True)","cb54a765":"# Training the architecture\n\nI trained the model for 10 loop using batch size of 32 and the epoch with best validation accuracy will be saved.","b90eccc7":"**Removing Tweet that are less than or equal to 5 words**\n\nSince i did some cleaning to the dataset, there is a possibility that after the cleaning is done there will be a tweet with small count of word. Therefore, i decided to drop these rows since it might not contains any information.","b7171c15":"**Defining Function to bert_tokenize all tweet in the dataset**","0fa9e474":"**Defining Bert Model that will be used as the encoder**\n\ni used bert-base-uncased since it is smaller compared to the large one and will be faster to train.","2613c7ac":"# BertTokenizer\n\nIn NLP, tokenizer is used to break down a whole sentence into word (sentence to word) and this could be done just by splitting the string. \n\nIn BERT, before a sentence is fed into the network, each sentence will go through some step called BertTokenizer. \n1. Each sentence will be tokenized using an algorithm called wordpiece tokenizer. The difference between this algorithm with the usual one is not only each sentence will be splitted into a number of word. Each word later on will be splitted into a number of subword. This is done because of the limitation of vocabulary in the pretrained model, usually unknown vocabulary will be converted into [UNK] token \/ unknown. If there are many unknown token in the dataset, this will create an information loss. As an example: I am sleeping, in the regular tokenizer the word sleeping will stay as it is, but in the wordpiece algorithm, the word sleeping will be splitted into \"sleep\" and \"##ing\" which is more commonly seen in the vocabulary. \n2. Adding [CLS] in front of the string which means start of string\n3. Adding [SEP] at the end of the string which means the end of the string\n4. Adding [PAD] this is done if the string length is less than max length given in the input. Because the model only receive input with same length\n5. Lastly, convert all token into corresponding ids.","1f199941":"**Creating the model and showing the model summary**","e87458b1":"# Defining Function to create the model\n\n<a href=\"https:\/\/arxiv.org\/pdf\/1810.04805.pdf\"> Paper <\/a>\n\n    \nIn the model is used Adam Optimizer which is used by the creator of the pretrained model, using learning rate of 2e-5 which is recommended by the paper (5e-5, 4e-5, 3e-5, 2e-5). The loss function that will be used is CategoricalCrossentropy since the task is multiclass classification (one prediction from 5 class). The accuracy metrics that i used is CategoricalAccuracy since the output will be vector of [5,1].\n\nThe model will be:\nInput (id and mask) -> Bert -> dropout layer (0,2 dropout to reduce overfitting) -> Output (Dense with Softmax function which is recommended for multiclass classification)","2771a33e":"**Plotting the Training Metrics**\n\nFrom the plot, i could see that the highest training gain is in epoch 1 and epoch 2. After that the accuracy stays between 87%-89%.","9314e7d0":"**Since we only need the original tweet and the sentiment for the classification, we will drop other column**","533132e3":"# Conclusion\n\n1. Based on the Accuracy, the model already achieved good result at 86% accuracy\n2. Based on the Recall score, the model is having trouble in predicting an Extremely Negative Label\n3. The statement in point 2 is strengthened with the evaluation from confusion matrix where the model misclassify  many Extremely Negative as Negative (about 110 missprediction which is the highest).\n4. This might happen because the tweet in the dataset is not fully english (some of the tweet not in english).\n5. Based on the classification Report the model already have a good result.\n6. Further adjustment that could be made is cleaning the dataset to remove tweet that are not in english\n7. Other adjustment could be made by adding more tweet in the label other than neutral to make the dataset more balanced.","2d05c58c":"# Creating Function for Cleaning\/Preprocessing\n\nSince there are so many things in the tweet that could create noise, i tried to clean the data using some preprocessing. List of the preprocessing that has been done to the dataset could be seen below:\n* Removing links such as https:\/\/.... , http:\/\/.... , etc\n* Removing hashtags \n* Removing user mention\n* Removing unnecessary character (non alphanumeric except aposthrope)\n* Removing extra whitespace\n* Removing emoji unicode such as \\xF0\\x9F\\x98\\x81, \\xF0\\x9F\\x98\\x82, etc\n* Lastly, Removing leading and trailing space\n\nIn the cleaning step i used regex which is a set of characters that specify a pattern in a text\ndetail for each line of code\/regex could be seen in the comment below","59274ddd":"# Splitting Train Dataset into train dataset and validation dataset\n\nSince the dataset given already contains test data which have around 3800 tweets and its label. I splitted the train data to get validation dataset, I splitted the data with 90% for training and 10% validation. I choose 10% since the amount of validation data will be similar with the test data.","9c80d81d":"**Applying cleaning process for all tweet both in train_dataset and test_dataset**","7f647738":"**BertTokenize all of the dataset (train, validation, test)**\n\nI used 128 as the max_len since its is close to the maximum word count in one tweet from this dataset.","1055089b":"**Converting labels into onehot representation**\n\nThis is done because there are more than 2 class of data. If we don't use one hot encoding the prediction later will be between 2 class, therefore i used one hot encoding so later on the prediction of the model will be the probability for each 1 from 5 class and not only between 2 classes.","025c9267":"# Import Some Library that will be used","e1f1a281":"# **Performance Analysis**","f74440e5":"# Read .csv file containing the dataset","cb765d72":"# **4. B. Develop the classifier and Train the model**\n\n# Network Architecture","b5f98d0f":"# 2301865741 - Edgard Jonathan Putra Pranoto","378b58a2":"# **4. A. Performing preprocess and prepare the dataset using BertTokenizer**","665103fd":"**After i work with the tweets, next i convert the label from the dataset into integer**\n\nThe converted label will be\n* 0: Extremely Negative\n* 1: Negative\n* 2: Neutral\n* 3: Positive\n* 4: Extremely Positive","ba43c2e0":"**The model is evaluated using accuracy score, precision, recall, f1, and Confusion Matrix**"}}