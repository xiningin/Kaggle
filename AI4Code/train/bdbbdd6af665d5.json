{"cell_type":{"9dce4810":"code","13ed7595":"code","ef3f9a2a":"code","b004a387":"code","d8313bc5":"code","0d394404":"code","258ac27a":"code","65d3516c":"code","220f35ad":"code","75579db6":"code","4443250e":"code","d7bd2e73":"code","8a631eca":"code","240bb128":"code","49b94b41":"code","d81f6901":"markdown","9a772ee2":"markdown","4e79a77e":"markdown","56e6ea52":"markdown","9526f51c":"markdown","ef9f4593":"markdown"},"source":{"9dce4810":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","13ed7595":"!pip uninstall -y kaggle\n!pip install --upgrade pip\n!pip install kaggle==1.5.6\n!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\n!ls -lha kaggle.json\n!chmod 600 ~\/.kaggle\/kaggle.json\n!kaggle competitions download -c sejongyjelectricpowerprediction\n!unzip sejongyjelectricpowerprediction.zip","ef3f9a2a":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\nimport random\n\nfrom sklearn import preprocessing","b004a387":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nrandom.seed(777)\ntorch.manual_seed(777)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)","d8313bc5":"learning_rate = 0.5\ntraining_epochs = 50\nbatch_size =10\nScaler = preprocessing.StandardScaler()","0d394404":"train_data=pd.read_csv('electric_power_train_data.csv')\n#test_data=pd.read_csv('electric_power_test_data.csv')\ntest_data=pd.read_csv('electric_power_test_data.csv')\ntrain_data['Date'] = train_data['Date']%1000000\/10000\ntest_data['Date'] = test_data['Date']%1000000\/10000\n\nx_train_data=train_data.loc[:,[i for i in train_data.keys()[:-1]]]\ny_train_data=train_data[train_data.keys()[-1]]\n\nx_train_data=np.array(x_train_data)\ny_train_data=np.array(y_train_data)\nx_train_data = Scaler.fit_transform(x_train_data)  ####baseline\uacfc \ucc28\uc774\uc810 \uc5ec\uae30\uc11c Scaler \uc0ac\uc6a9\ud574\uc90c\n\nx_train_data=torch.FloatTensor(x_train_data)\ny_train_data=torch.FloatTensor(y_train_data)","258ac27a":"train_dataset = torch.utils.data.TensorDataset(x_train_data, y_train_data)\n\ndata_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          drop_last=True)","65d3516c":"linear1 = torch.nn.Linear(3,3,bias=True)\nlinear2 = torch.nn.Linear(3,1,bias=True)\nrelu= torch.nn.ReLU()\n\ntorch.nn.init.kaiming_normal_(linear1.weight)\ntorch.nn.init.kaiming_normal_(linear2.weight)","220f35ad":"model = torch.nn.Sequential(linear1, relu,\n                            linear2).to(device)","75579db6":"loss = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","4443250e":"total_batch = len(data_loader)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.to(device)\n        Y = Y.to(device)\n\n        # \uadf8\ub798\ub514\uc5b8\ud2b8 \ucd08\uae30\ud654\n        optimizer.zero_grad()\n        # Forward \uacc4\uc0b0\n        hypothesis = model(X)\n        # Error \uacc4\uc0b0\n        cost = loss(hypothesis, Y)\n        # Backparopagation\n        cost.backward()\n        # \uac00\uc911\uce58 \uac31\uc2e0\n        optimizer.step()\n\n        # \ud3c9\uade0 Error \uacc4\uc0b0\n        avg_cost += cost \/ total_batch\n\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning finished')","d7bd2e73":"with torch.no_grad():\n\n  x_test_data=test_data.loc[:,[i for i in test_data.keys()[:]]]\n  x_test_data=np.array(x_test_data)\n  x_test_data = Scaler.transform(x_test_data)\n  x_test_data=torch.from_numpy(x_test_data).float().to(device)\n\n  prediction = model(x_test_data)","8a631eca":"correct_prediction = prediction.cpu().numpy().reshape(-1,1)","240bb128":"submit=pd.read_csv('electric_power_submit_data.csv')\nfor i in range(len(correct_prediction)):\n  submit['ElectricPower'][i]=correct_prediction[i].item()\n\nsubmit\nsubmit.to_csv('submit.csv', index = None, header=True)","49b94b41":"!kaggle competitions submit -c sejongyjelectricpowerprediction -f submit.csv -m \"Message\"","d81f6901":"# \ud30c\ub77c\ubbf8\ud130 \uc124\uc815","9a772ee2":"# \ub370\uc774\ud130 \ub85c\ub4dc","4e79a77e":"# \uac00\uc911\uce58 \ucd08\uae30\ud654 \ubaa8\ub378 3-> 3-> 1","56e6ea52":"# \ubaa8\ub378 \ud559\uc2b5","9526f51c":"# import","ef9f4593":"# \uc608\uce21\uac12 \uad6c\ud558\uae30"}}