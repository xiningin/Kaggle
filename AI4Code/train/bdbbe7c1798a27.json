{"cell_type":{"d3e2b6c9":"code","733b20e1":"code","54e77092":"code","5840d422":"code","8c062b02":"code","8f20068f":"code","c02bb792":"code","aa298378":"code","ab9fde36":"code","beb014d9":"code","30b17dc1":"code","46be0f44":"code","9acc10b0":"code","0058a75c":"code","22ff7029":"code","6fb7b27a":"code","60c739f1":"code","4534ed93":"code","97b701e6":"code","c60e84a6":"code","992245e4":"code","51896e98":"code","63c2dec4":"code","a89a3444":"code","25766228":"code","8119bb28":"code","38a85fee":"code","bd4fc8c6":"code","d1b11f09":"code","e921043b":"code","8583ad6e":"code","4c9f818d":"code","ce9c535c":"code","73c776c9":"code","958b01ca":"code","ff1b51c4":"code","cf437354":"code","531d06be":"code","bb8351ca":"code","e7255749":"code","e1ba8eda":"code","67e29d44":"code","8b63e8a6":"code","0c804ae6":"code","de3b1813":"code","154536f8":"code","8737bb4e":"code","d7103c46":"code","a86a5069":"code","28916815":"code","1f37f6e8":"code","228e64a3":"code","e8102913":"code","b3d5de5e":"code","f4cf10f1":"code","760bc2e9":"code","d49fbbfd":"code","af0a7223":"code","07e7b080":"code","9e07be3c":"code","e9ed5eb2":"code","ec0ebed0":"markdown","57d9605d":"markdown","c4569fe7":"markdown","95bc05de":"markdown","fb82e0b4":"markdown","92135a07":"markdown","e51f758a":"markdown","cfe54819":"markdown","8ef6c561":"markdown","6e576d89":"markdown","e7cebfce":"markdown","e3a01056":"markdown","c18a6c5a":"markdown","b4720a87":"markdown"},"source":{"d3e2b6c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","733b20e1":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","54e77092":"train_data.info()","5840d422":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nsns.heatmap(train_data.corr(), center = 0)\nplt.title(\"Correlations Between Columns\")\nplt.show()","8c062b02":"y = train_data.SalePrice\nX = train_data.drop(columns=[\"SalePrice\"], axis=1)","8f20068f":"y.shape, X.shape, test_data.shape","c02bb792":"corr_matrix = train_data.corr()","aa298378":"corr_matrix['SalePrice'][(corr_matrix[\"SalePrice\"] > 0.40) | (corr_matrix[\"SalePrice\"] < -0.40)]","ab9fde36":"important_num_cols = list(corr_matrix['SalePrice'][(corr_matrix[\"SalePrice\"] > 0.5) | (corr_matrix[\"SalePrice\"] < -0.5)].index)\n\nimportant_num_cols.remove('SalePrice')\nlen(important_num_cols)","beb014d9":"important_num_cols","30b17dc1":"X_num_only = X[important_num_cols]","46be0f44":"X_num_only.shape","9acc10b0":"plt.figure(figsize=(10,8))\nsns.heatmap(X_num_only.corr(), center = 0)\nplt.title(\"Correlations Between Columns\")\nplt.show()","0058a75c":"corr_X = X_num_only.corr()\nlen(corr_X)","22ff7029":"\nfor i in range(0, len(corr_X) - 1):\n    for j in range(i + 1, len(corr_X)):\n        if(corr_X.iloc[i, j] < -0.6 or corr_X.iloc[i, j] > 0.6):\n            print(corr_X.iloc[i, j], i, j, corr_X.index[i], corr_X.index[j])\n            ","6fb7b27a":"# Based on the above information, we further discard the features 1stFlrSF, FullBath, TotRmsAbvGrd, GarageArea\n#num_cols = [i for i in X_modified.columns if i not in ['1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'GarageArea']]\nnum_cols = [i for i in X_num_only.columns if i not in ['1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'GarageArea']]\n","60c739f1":"# Categorical columns - choose the important ones\n\ncat_cols = [\"MSZoning\", \"Utilities\",\"BldgType\",\"Heating\",\"KitchenQual\",\"SaleCondition\",\"LandSlope\"]","4534ed93":"X_final = X[num_cols]","97b701e6":"X_final.shape","c60e84a6":"X_final['YearRemodAdd'] = X_final['YearRemodAdd'] - X_final['YearBuilt']","992245e4":"X_final.head()","51896e98":"X_final.isna().sum()","63c2dec4":"#X_final['MasVnrArea'] = X_final['MasVnrArea'].fillna(X_final['MasVnrArea'].median())","a89a3444":"X[cat_cols].isna().sum()","25766228":"X_categorical_df = pd.get_dummies(X[cat_cols], columns=cat_cols)","8119bb28":"X_categorical_df","38a85fee":"# Create final dataframe","bd4fc8c6":"X_final = X_final.join(X_categorical_df)","d1b11f09":"X_final","e921043b":"from sklearn import preprocessing\nstandardize = preprocessing.StandardScaler().fit(X_final[num_cols])","8583ad6e":"#See mean per column\nstandardize.mean_","4c9f818d":"#transform\nX_final[num_cols] = standardize.transform(X_final[num_cols])","ce9c535c":"X_final","73c776c9":"X_final.head()","958b01ca":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_final, y, test_size=0.2, random_state=1)","ff1b51c4":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","cf437354":"from sklearn.metrics import r2_score \nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import PolynomialFeatures","531d06be":"perf = []\nmethod = []","bb8351ca":"from sklearn.metrics import mean_squared_log_error","e7255749":"# Linear Regression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\npredictions = lin_reg.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nmethod.append('Linear Regression')\nperf.append(rmsle)\n","e1ba8eda":"# Ridge regression\nridge = Ridge()\nridge.fit(X_train, y_train)\npredictions = ridge.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('Ridge Regression')\n\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","67e29d44":"# Ridge regression\nlasso = Lasso()\nlasso.fit(X_train, y_train)\npredictions = lasso.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('Lasso Regression')\n\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","8b63e8a6":"# support vector regression\nfrom sklearn.svm import SVR\nsvr = SVR(C=1000000)\nsvr.fit(X_train, y_train)\npredictions = svr.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\n#method.append('SVM')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\n#perf.append(rmsle)","0c804ae6":"svr_rbf = SVR(kernel=\"rbf\", C=1000000, gamma=0.01, epsilon=0.1)\nsvr_rbf.fit(X_train, y_train)\npredictions = svr_rbf.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\n\nmethod.append('SVR')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","de3b1813":"#Random forest regressor\nfor i in range(50 , 500, 50):\n    random_forest = RandomForestRegressor(n_estimators=i)\n    random_forest.fit(X_train, y_train)\n    predictions = random_forest.predict(X_val)\n\n    r_squared = r2_score(predictions, y_val)\n\n    print(\"R2 Score:\", r_squared)\n    method.append('Random Forest Regressor')\n    rmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\n    print(\"RMSLE:\", rmsle)\n    perf.append(rmsle)","154536f8":"# xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\nxgb.fit(X_train, y_train)\npredictions = xgb.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('XGBoost Regressor')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","8737bb4e":"# ANN\n'''\nimport math\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import MeanSquaredLogarithmicError\n\n\n\nhidden_units1 = 400\n#hidden_units2 = 480\nhidden_units3 = 256\nlearning_rate = 0.01\n# Creating model using the Sequential in tensorflow\ndef build_model_using_sequential():\n    model = Sequential([\n        Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n        Dropout(0.2),\n        Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n        Dense(1, kernel_initializer='normal', activation='linear')\n      ])\n    return model\n# build the model\nmodel = build_model_using_sequential()\n\n# loss function\nmsle = MeanSquaredLogarithmicError()\nmodel.compile(\n    loss=msle, \n    optimizer=Adam(learning_rate=learning_rate), \n    metrics=[msle]\n)\n\n# train the model\nhistory = model.fit(\n    X_final.values, \n    y.values, \n    epochs=1000, \n    batch_size=64,\n    validation_split=0.2\n)\npredictions = model.predict(X_val)\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nmethod.append('ANN')\nperf.append(rmsle)\n'''","d7103c46":"# Compare performances of models\nplt.barh(method, perf)\nplt.title('RMSLE comparison of models')","a86a5069":"# Test Data Preprocessing\n\nX_test = test_data[num_cols + cat_cols]\nX_test['YearRemodAdd'] = X_test['YearRemodAdd'] - X_test['YearBuilt']","28916815":"X_test.shape","1f37f6e8":"# Encode categorical similar to train\nX_test = pd.get_dummies(X_test)","228e64a3":"X_test","e8102913":"# Add missed columns missed due to get dummies on X_test\nX_test = X_test.reindex(columns = X_final.columns, fill_value=0)","b3d5de5e":"X_test","f4cf10f1":"#transform\nX_test[num_cols] = standardize.transform(X_test[num_cols])","760bc2e9":"X_test","d49fbbfd":"X_test.isna().sum()","af0a7223":"# we will use median for missing values\nX_test['TotalBsmtSF'] = X_test['TotalBsmtSF'].fillna(train_data['TotalBsmtSF'].median())","07e7b080":"# mode for cars\nX_test['GarageCars'] = X_test['GarageCars'].fillna(train_data['GarageCars'].mode()[0])","9e07be3c":"# Submission using SVR\n\npreds = svr_rbf.predict(X_test)\nsubmit = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': preds})\nsubmit.to_csv('submission.csv',index=False)\n","e9ed5eb2":"# Submission using ANN\n'''\npreds = model.predict(X_test)\npreds_2 = [i[0] for i in preds]\nout = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': preds_2}) \nout.to_csv('submission.csv',index=False)\n'''","ec0ebed0":"## Remove the feautures which are highly correlated with each other","57d9605d":"# Testing","c4569fe7":"# Regression Using Machine Learning ","95bc05de":"## Split training data into training and validation","fb82e0b4":"# Exploratory Data Analysis","92135a07":"## Split input and target variables","e51f758a":"## Handling missing data in test data","cfe54819":"# Feature Engineering","8ef6c561":"# Encoding Categorical data","6e576d89":"## Load the data","e7cebfce":"## Choose only the significant features, discard those with correlation score < 0.5 with the target variable","e3a01056":"## Modify 'YearRemodAdd' feature - make it more informative","c18a6c5a":"# Handling missing data","b4720a87":"# Normalizing the data"}}