{"cell_type":{"136d1a67":"code","d203e8e6":"code","78981b3e":"code","94265417":"code","33f4784a":"code","d31c5410":"code","19b5a181":"code","e977fb7c":"code","50e87baa":"markdown","5191df98":"markdown","13d6783c":"markdown","038e651a":"markdown"},"source":{"136d1a67":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom textblob import TextBlob\nfrom tqdm import tqdm\nfrom math import log\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom itertools import dropwhile, takewhile\n\nplt.style.use('seaborn-talk')\ntqdm.pandas()","d203e8e6":"reviews_df = pd.read_csv('..\/input\/movie-review\/movie_review.csv')\nreviews_df.head()","78981b3e":"reviews_df.tag.hist()\nprint(reviews_df.tag.value_counts())","94265417":"reviews_df['text_len'] = reviews_df.text.progress_apply(lambda txt: len(TextBlob(txt).words))\nreviews_df.text_len.hist(bins=50)\nreviews_df.text_len.describe()","33f4784a":"## TODO\n## probably we need to remove too short reviews\nreviews_df[reviews_df.text_len <= 3].head()","d31c5410":"puncts = set(\"!#$%&\\(),-.\/:;<=>?@[\\\\]{|}\")\ndef normalize_text(text):\n    text = text.lower()\n    words = TextBlob(text).tokens\n    start_negation = False\n    processed_words = []\n    for i, word in enumerate(words):\n        word = word.lemma\n        if puncts & set(word):\n            start_negation = False\n            word = None\n        elif word in [\"n't\", 'not', 'no', 'never']:\n            start_negation = True\n        elif start_negation:\n            word = f'NOT_{word}'\n        \n        if word:\n            processed_words.append(word)\n    \n    return processed_words\n\n    \ntext = \"I don't like these ones... , but I very like other things.\"\nnormalize_text(text)","19b5a181":"class Vocab:\n    def __init__(self, counter: Counter):\n        self.counter = counter\n        self.indx_to_word = list(counter.keys())\n        self.word_to_indx = {word:indx for indx, word in enumerate(self.indx_to_word)}\n        self.size = len(self.indx_to_word)\n        \n    def to_word(self, indx):\n        return self.indx_to_word(indx)\n    \n    def to_indx(self, word):\n        return self.word_to_indx[word]\n    \n    def is_known_word(self, word):\n        return self.counter[word] > 0\n    \nclass Classes:\n    def __init__(self, classes):\n        self.indx_to_class = classes\n        self.class_to_indx = {classs:indx for indx, classs in enumerate(self.indx_to_class)}\n        self.size = len(self.indx_to_class)\n        \n    def to_class(self, indx):\n        return self.indx_to_class[indx]\n    \n    def to_indx(self, classs):\n        return self.class_to_indx[classs]\n    \n    \nclass NaiveBayes:\n    def __init__(self, smooth_factor=1.):\n        self.smooth_factor = smooth_factor\n        self.logprior = None\n        self.loglikelihood = None\n        \n    def train(self, train_df, vocab: Vocab, classes:Classes, verbose=False):\n        n_docs = len(train_df)\n        logprior = np.zeros(classes.size)\n\n        loglikelihood = np.zeros((vocab.size, classes.size))\n\n        for classs in classes.indx_to_class:\n            if verbose:\n                print(f'Processing class [{classs}]')\n            class_indx = classes.class_to_indx[classs]\n\n            class_train_df = train_df[train_df.tag==classs]\n            n_docs_in_class = len(class_train_df)\n            logprior[class_indx] = log(n_docs_in_class\/n_docs)\n            if verbose:\n                print('\\tcreating vocabulary for the class..')\n            class_counter = Counter()\n            for text in class_train_df.text.values:\n                class_counter.update(set(normalize_text(text)))\n\n            class_doc_words_cnt = sum(class_counter.values())\n            for word in tqdm(vocab.indx_to_word):\n                word_indx = vocab.to_indx(word)\n                word_in_class_cnt = class_counter[word]\n\n                loglikelihood[word_indx, class_indx] = log((class_counter[word] + self.smooth_factor) \/ (class_doc_words_cnt + vocab.size * self.smooth_factor))\n        \n        self.logprior = logprior\n        self.loglikelihood = loglikelihood\n        \n\n\n    def predict(self, text, vocab: Vocab, classes:Classes):\n        probs = np.zeros(classes.size)\n        text_words = set(normalize_text(text))\n        for classs in classes.indx_to_class:\n            class_indx = classes.to_indx(classs)\n            probs[class_indx] = self.logprior[class_indx]\n\n            for word in text_words:\n                if vocab.is_known_word(word):\n                    word_indx = vocab.to_indx(word)\n                    probs[class_indx] += self.loglikelihood[word_indx, class_indx]\n\n        return classes.to_class(np.argmax(probs)), probs","e977fb7c":"\naccuracies = []\nfor fold in range(10):\n    print('Fold:', fold)\n    \n    val_folds = [fold]\n    val_df = reviews_df.query('fold_id in @val_folds')\n    train_df = reviews_df.query('fold_id not in @val_folds')\n#     print('\\tTrain set size:', len(train_df))\n#     print('\\tVal set size:', len(val_df))\n\n    counter = Counter()\n\n    for text in tqdm(train_df.text.values):\n        counter.update(set(normalize_text(text)))\n    \n    # remove most common words\n    for key, count in takewhile(lambda key_count: key_count[1] > 5000, counter.most_common()):\n        del counter[key]\n        \n    # remove most uncommon words\n    for key, count in dropwhile(lambda key_count: key_count[1] > 10, counter.most_common()):\n        del counter[key]\n\n    vocab = Vocab(counter)\n#     print('\\tVocabulary size:', vocab.size)\n    classes = Classes(['pos', 'neg'])\n    NB = NaiveBayes()\n    NB.train(train_df, vocab, classes)\n\n    bool_preds = []\n    for text, tag in tqdm(val_df.loc[:, ['text', 'tag']].values):\n        pred_tag, probs = NB.predict(text, vocab, classes)\n        bool_preds.append(pred_tag==tag)\n\n    accuracy = sum(bool_preds) \/ len(bool_preds)\n    print(f'\\tAccuracy: {int(accuracy * 100)}% [{accuracy}]')\n    accuracies.append(accuracy)\n    \navg_accuracy = sum(accuracies)\/len(accuracies)\nprint(f'Avg accuracy: {int(avg_accuracy * 100)}% [{avg_accuracy}]')","50e87baa":"# Simple baseline for sentiment classification with Naive Bayes\n","5191df98":"## Build useful structures","13d6783c":"## CV train","038e651a":"## Text normallization"}}