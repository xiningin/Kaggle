{"cell_type":{"e52f601a":"code","de2cfc19":"code","11e75ebb":"code","c9c4911f":"code","cb0139f9":"code","f23ed521":"code","c483f024":"code","1f52cd2d":"code","33db673c":"code","b2749f20":"code","6f18655d":"code","e1e3c325":"code","11033847":"code","9ba8f990":"code","db7b12b4":"code","324fd2c3":"code","b3d0bccd":"code","3469079f":"code","b314d579":"code","c4bc0b2b":"code","76dc73ba":"code","11750133":"code","2ae41424":"code","b53a2efa":"code","61046f78":"code","596eec34":"code","7e054700":"code","84be5022":"code","d627ad76":"code","28d090ea":"code","20b301e8":"code","98ff229f":"code","466bfad7":"code","b265f7a2":"code","430ced10":"code","366c76f0":"code","25f40879":"code","7e4e0a22":"code","89236dc6":"code","7fb3e80d":"code","2ce831b1":"code","cc158ce1":"code","9c29349e":"code","0bedb8f4":"code","a81c011a":"code","562277cc":"code","5d3e74fe":"code","f9df211b":"code","4af98c48":"code","b1547204":"code","201138b3":"code","a7097994":"code","b3efe52e":"code","f203d38d":"code","957a7c68":"code","ca009382":"code","86a17215":"code","11aeb78f":"code","618eec9a":"code","30acddd9":"code","a9089a67":"code","d62c9e3f":"code","91154b9b":"code","036d44b6":"code","3e4466aa":"code","b36bd0f9":"code","78559656":"code","f3585d06":"code","6f764abc":"code","2b0c2c75":"code","59644e51":"code","21f91b8a":"code","163aea89":"code","6c66df35":"code","1917d3bb":"code","17f0f376":"markdown","da7b1f72":"markdown","3c086268":"markdown","792b6d28":"markdown","368a3686":"markdown","4d003203":"markdown","18fc5163":"markdown","5a777796":"markdown","746ccc97":"markdown","3cb091ad":"markdown","81f7e542":"markdown","36da6288":"markdown","864941be":"markdown","5a9ffe15":"markdown","37968427":"markdown","75aa6574":"markdown","528f8462":"markdown","455ab258":"markdown","be9accfc":"markdown","61eef6db":"markdown","5b1c35e0":"markdown","99681648":"markdown","0bd217d4":"markdown","8b700d26":"markdown","88c63872":"markdown"},"source":{"e52f601a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de2cfc19":"df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","11e75ebb":"\nrslt_df = df[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nrslt_df2 = df[(df['toxic'] == 1) | (df['severe_toxic'] == 1) | (df['obscene'] == 1) | (df['threat'] == 1) | (df['insult'] == 1) | (df['identity_hate'] == 1)]\nnew1 = rslt_df[['id', 'comment_text', 'toxic']].iloc[:23891].copy() \nnew2 = rslt_df2[['id', 'comment_text', 'toxic']].iloc[:946].copy()\nnew = pd.concat([new1, new2], ignore_index=True)","c9c4911f":"new.tail()","cb0139f9":"#test train split\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new[\"comment_text\"], new['toxic'], test_size=0.33)","f23ed521":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5)\nX1 = vectorizer.fit_transform(X_train)\nX_test1= vectorizer.transform(X_test)","c483f024":"class_names = ['nontoxic', 'toxic']","1f52cd2d":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf2 = LogisticRegression(C=0.1, solver='sag')\nscores = cross_val_score(clf2, X1,y_train, cv=5,scoring='f1_weighted')","33db673c":"y_p1 = clf2.fit(X1, y_train).predict(X_test1)","b2749f20":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p1)\nprint('Accuracy: %f' % accuracy)","6f18655d":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc = make_pipeline(vectorizer, clf2)","e1e3c325":"new[\"comment_text\"][0]","11033847":"print(c.predict_proba([new[\"comment_text\"][0]]))","9ba8f990":"from lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=class_names)","db7b12b4":"X_test = X_test.tolist()","324fd2c3":"X_test[0]","b3d0bccd":"type(y_test)","3469079f":"y_test = y_test.tolist()","b314d579":"y_test = np.array(y_test)","c4bc0b2b":"type(y_test)","76dc73ba":"idx = 0\nexp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])\n\n","11750133":"exp.as_list()","2ae41424":"print('Original prediction:', clf2.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['you']] = 0\ntmp[0,vectorizer.vocabulary_['thanks']] = 0\nprint('Prediction removing some features:', clf2.predict_proba(tmp)[0,1])\nprint('Difference:', clf2.predict_proba(tmp)[0,1] - clf2.predict_proba(X_test1[idx])[0,1])","b53a2efa":"%matplotlib inline\nfig = exp.as_pyplot_figure()","61046f78":"exp.show_in_notebook(text=False)","596eec34":"exp.save_to_file('\/tmp\/oi.html')","7e054700":"exp.show_in_notebook(text=True)","84be5022":"print('Original dataset shape %s' % Counter(y_train))\nsm = SMOTE(random_state=12)\nx_train_res, y_train_res = sm.fit_sample(X1, y_train)\nprint('Resampled dataset shape %s' % Counter(y_train_res))","d627ad76":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=0.1, solver='sag')\nscores = cross_val_score(clf, x_train_res,y_train_res, cv=5,scoring='f1_weighted')","28d090ea":"y_p2 = clf.fit(x_train_res, y_train_res).predict(X_test1)","20b301e8":"\n\nfrom sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p2)\nprint('Accuracy: %f' % accuracy)\n\n","98ff229f":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc2 = make_pipeline(vectorizer, clf)","466bfad7":"print(c2.predict_proba([new[\"comment_text\"][0]]))","b265f7a2":"idx = 0\nexp = explainer.explain_instance(X_test[idx], c2.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c2.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","430ced10":"exp.as_list()","366c76f0":"print('Original prediction:', clf.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['article']] = 0\ntmp[0,vectorizer.vocabulary_['you']] = 0\nprint('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\nprint('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(X_test1[idx])[0,1])","25f40879":"%matplotlib inline\nfig = exp.as_pyplot_figure()","7e4e0a22":"exp.show_in_notebook(text=False)","89236dc6":"exp.save_to_file('\/tmp\/oi.html')","7fb3e80d":"exp.show_in_notebook(text=True)","2ce831b1":"from imblearn.under_sampling import NearMiss\nnm = NearMiss()\nX_d, y_d = nm.fit_resample(X1, y_train)\nprint('Resampled dataset shape %s' % Counter(y_d))","cc158ce1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf1 = LogisticRegression(C=0.1, solver='sag')\nscores = cross_val_score(clf1, X_d,y_d, cv=5,scoring='f1_weighted')","9c29349e":"y_p3 = clf1.fit(X_d, y_d).predict(X_test1)","0bedb8f4":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p3)\nprint('Accuracy: %f' % accuracy)","a81c011a":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc3 = make_pipeline(vectorizer, clf1)","562277cc":"print(c3.predict_proba([new[\"comment_text\"][0]]))","5d3e74fe":"idx = 0\nexp = explainer.explain_instance(X_test[idx], c3.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c3.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","f9df211b":"exp.as_list()","4af98c48":"print('Original prediction:', clf1.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['article']] = 0\ntmp[0,vectorizer.vocabulary_['you']] = 0\nprint('Prediction removing some features:', clf1.predict_proba(tmp)[0,1])\nprint('Difference:', clf1.predict_proba(tmp)[0,1] - clf1.predict_proba(X_test1[idx])[0,1])","b1547204":"%matplotlib inline\nfig = exp.as_pyplot_figure()","201138b3":"exp.show_in_notebook(text=False)","a7097994":"exp.save_to_file('\/tmp\/oi.html')","b3efe52e":"exp.show_in_notebook(text=True)","f203d38d":"#printing ids of comments which are toxic\ncount=-1\nfor x in y_test:\n    count=count+1\n    if x==1:\n        print(count)","957a7c68":"idx = 141\nexp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","ca009382":"exp.as_list()","86a17215":"print('Original prediction:', clf2.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['shit']] = 0\ntmp[0,vectorizer.vocabulary_['bastard']] = 0\nprint('Prediction removing some features:', clf2.predict_proba(tmp)[0,1])\nprint('Difference:', clf2.predict_proba(tmp)[0,1] - clf2.predict_proba(X_test1[idx])[0,1])","11aeb78f":"%matplotlib inline\nfig = exp.as_pyplot_figure()","618eec9a":"exp.show_in_notebook(text=False)","30acddd9":"exp.save_to_file('\/tmp\/oi.html')","a9089a67":"exp.show_in_notebook(text=True)","d62c9e3f":"idx = 141\nexp = explainer.explain_instance(X_test[idx], c2.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c2.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","91154b9b":"exp.as_list()","036d44b6":"print('Original prediction:', clf.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['shit']] = 0\ntmp[0,vectorizer.vocabulary_['bastard']] = 0\nprint('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\nprint('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(X_test1[idx])[0,1])","3e4466aa":"%matplotlib inline\nfig = exp.as_pyplot_figure()","b36bd0f9":"exp.show_in_notebook(text=False)","78559656":"exp.save_to_file('\/tmp\/oi.html')","f3585d06":"exp.show_in_notebook(text=True)","6f764abc":"idx = 141\nexp = explainer.explain_instance(X_test[idx], c3.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c3.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","2b0c2c75":"exp.as_list()","59644e51":"print('Original prediction:', clf1.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['shit']] = 0\ntmp[0,vectorizer.vocabulary_['bastard']] = 0\nprint('Prediction removing some features:', clf1.predict_proba(tmp)[0,1])\nprint('Difference:', clf1.predict_proba(tmp)[0,1] - clf1.predict_proba(X_test1[idx])[0,1])","21f91b8a":"%matplotlib inline\nfig = exp.as_pyplot_figure()","163aea89":"exp.show_in_notebook(text=False)","6c66df35":"exp.save_to_file('\/tmp\/oi.html')","1917d3bb":"exp.show_in_notebook(text=True)","17f0f376":"We will now  generate an explanation with at most 10 features for an arbitrary document in the test set.","da7b1f72":"#### Visualizing explanations of toxic comment in imbalance dataset","3c086268":"##### Logistic Regression","792b6d28":"> we have converted X_test to list and y_test to ndarray because explain_instance takes input like this.","368a3686":"#### Visualizing explanations non-toxic comment on unbalanced dataset","4d003203":"The classifier got this example right (it predicted non-toxic).\nThe explanation is presented below as a list of weighted features.","18fc5163":"![image](https:\/\/blogs.sas.com\/content\/subconsciousmusings\/files\/2018\/10\/flupredictionLIME.jpg)","5a777796":"## LIME","746ccc97":"OVERSAMPLE","3cb091ad":"features contributing for classification","81f7e542":"NOW for TOXIC Comment ","36da6288":"These weighted features are a linear model, which approximates the behaviour of the logistic Regression classifier in the vicinity of the test example. Roughly, if we remove 'you' and 'thanks' from the document , the prediction should move towards the opposite class by about the sum of the weights for both features.","864941be":"#### Visualizing explanations for toxic comment after Oversampling the dataset","5a9ffe15":"##### Tf\/Idf","37968427":"#### Visualizing explanations for non-toxic comment after Undersampling the dataset","75aa6574":"> here we have to pass the raw text and it gives the prob for non-toxic and toxic class for perticular comment at index 0","528f8462":"#### Visualizing explanations for non-toxic comment after Oversampling the dataset","455ab258":"After Undersampling","be9accfc":"After Oversampling ","61eef6db":"#### Visualizing explanations for toxic comment after Undersampling the dataset","5b1c35e0":"UNDERSAMPLE","99681648":"## Example1","0bd217d4":"### EXPLAINING PREDICTION USING LIME","8b700d26":"# Example 2","88c63872":"For reference please see the [paper](https:\/\/arxiv.org\/abs\/1602.04938) "}}