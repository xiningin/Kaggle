{"cell_type":{"e6f7efb7":"code","e8fbc0b3":"code","63bde3f4":"code","b3019f25":"code","e85d2dcf":"code","b702dbc7":"code","1bb7c976":"code","b4b96960":"code","ffc29d38":"code","56a0d3ef":"code","72825d9c":"code","0399fc6f":"code","b1e52e42":"code","d0f9deb9":"code","dbe8a632":"code","3d8abd0f":"code","9b053118":"code","17028c93":"code","2e8549e4":"code","e7afec01":"code","9a77c133":"code","cb81e54b":"code","498fba93":"code","417673c6":"code","29cd04f4":"code","adfb5288":"code","7cf0f163":"code","4bda1803":"code","2648e336":"code","caa1fa64":"code","b70e09be":"code","a0999b12":"code","7860cbec":"code","c8bd7dd9":"code","b84c4add":"code","c90c7988":"code","3a99987b":"code","048247cd":"code","5028dba6":"code","5d692927":"code","1baa722f":"code","6ac90cc8":"code","508742d3":"code","278ac76f":"code","a6f2ab3b":"code","0bced366":"code","d1716c70":"code","544bfe27":"code","25d74775":"code","4427b2e3":"code","71119407":"code","bcf49f8b":"code","19ff4b34":"markdown","dc7b21bc":"markdown","21b3853c":"markdown","9db134b9":"markdown","be539741":"markdown","b85589c7":"markdown","6bf1b7f4":"markdown","f4c0fc53":"markdown","bf27382c":"markdown","4783e393":"markdown","03b9c9a2":"markdown","257668a1":"markdown","6ae0802d":"markdown","045da98e":"markdown","8c6addc6":"markdown","9c8c310a":"markdown","8a683b5b":"markdown","81229234":"markdown","373adcea":"markdown","bbbbc199":"markdown","41b7ffd9":"markdown","8bd6e060":"markdown","d828e600":"markdown","5f99b6d6":"markdown","1a622cab":"markdown","7b050312":"markdown","21a135fc":"markdown","3ac48199":"markdown","2ef79d3d":"markdown","80b7fa30":"markdown","baff0a11":"markdown","165a6632":"markdown"},"source":{"e6f7efb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier \nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, accuracy_score, f1_score, roc_curve, roc_auc_score\n","e8fbc0b3":"df= pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","63bde3f4":"x= df.loc[:, df.columns!='target']\ny=df['target']\n\n\n","b3019f25":"f,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","e85d2dcf":"x_train, x_test, y_train , y_test= train_test_split(x, y, test_size=0.2)\nselect_feature= SelectKBest(chi2, k=5).fit(x_train, y_train)\nfeature_importances_kbest= pd.DataFrame(columns=['scores'], index= x.columns)\nfeature_importances_kbest['scores']= select_feature.scores_\nfeature_importances_kbest=feature_importances_kbest.sort_values('scores', ascending= False)\nprint(feature_importances_kbest)\n\n","b702dbc7":"num_features=['3 features', '4 features', '5 features', '6 features', '7 features', '8 features', '9 features', '10 features', '11 features', '12 features', '13 features' ]\nresults_rf_features_kbest=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc score'], index=num_features)","1bb7c976":"rf_model= RandomForestClassifier()\nfor ind in range(3, len(feature_importances_kbest.index)+1):\n    \n    x_rf_kbest_train= x_train.loc[:, feature_importances_kbest.index[0:ind]]\n    x_rf_kbest_test= x_test.loc[:, feature_importances_kbest.index[0:ind]]\n    rf_model.fit(x_rf_kbest_train, y_train)\n    y_pred_kbest= rf_model.predict(x_rf_kbest_test)\n    \n    results_rf_features_kbest.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 1]=recall_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 2]=precision_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 3]=f1_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred_kbest)\n\n","b4b96960":"results_rf_features_kbest","ffc29d38":"results_rf_features_kbest.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","56a0d3ef":"results_rf_features_kbest.sort_values(['recall score'], ascending= False).iloc[0:2,:]","72825d9c":"results_rf_features_kbest.sort_values(['roc score'], ascending= False).iloc[0:2,:] ","0399fc6f":"\nfeature_importances_rf= pd.DataFrame(columns=['scores'], index= x.columns)\nfeature_importances_rf['scores']= rf_model.feature_importances_\nfeature_importances_rf=feature_importances_rf.sort_values('scores', ascending= False)\nfeature_importances_rf\n\n\n","b1e52e42":"results_rf_features_rf=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc auc score'], index=num_features)\n\nfor ind in range(1, len(feature_importances_rf.index)+1):\n        \n    x_rf_rf_train=x_train.loc[:, feature_importances_rf.index[0:ind]]\n    x_rf_rf_test= x_test.loc[:, feature_importances_rf.index[0:ind]]\n    \n    rf_model.fit(x_rf_rf_train, y_train)\n    y_pred3= rf_model.predict(x_rf_rf_test)\n\n\n    results_rf_features_rf.iloc[ind-3, 0]=accuracy_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 1]=recall_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 2]=precision_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 3]=f1_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred3)\n\n    \n\n","d0f9deb9":"results_rf_features_rf","dbe8a632":"results_rf_features_rf.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n\n","3d8abd0f":"results_rf_features_rf.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n","9b053118":"results_rf_features_rf.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","17028c93":"rf_model=RandomForestClassifier()\nrfe= RFECV(estimator=rf_model, cv=5, step=1, scoring='accuracy')\nrfe=rfe.fit(x_train, y_train)\nprint('choosen best  features ', x_train.columns[rfe.support_])\nfeature_importances_rfe= pd.DataFrame(columns=['important features'] )\nfeature_importances_rfe['important features'] = x_train.columns[rfe.support_]\nfeature_importances_rfe\n","2e8549e4":"results_rfecv_features=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc auc score'], index=num_features)\nfor ind in range(3, len(feature_importances_rfe.index)+1):\n    \n    \n    x_f4_train=x_train.loc[:, feature_importances_rfe['important features'] [0:ind]]\n    x_f4_test= x_test.loc[:, feature_importances_rfe['important features'] [0:ind]]\n    \n    rf_model.fit(x_f4_train, y_train)\n    y_pred4= rf_model.predict(x_f4_test)\n    \n    \n    results_rfecv_features.iloc[ind-3, 0]=accuracy_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 1]=recall_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 2]=precision_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 3]=f1_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred4)\n    \n","e7afec01":"results_rfecv_features","9a77c133":"results_rfecv_features.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n\n","cb81e54b":"results_rfecv_features.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n","498fba93":"results_rfecv_features.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","417673c6":"results_random_forest=pd.DataFrame(columns=[ 'RF_features_importances','RF_KBEST' ,'RF_rfecv'], index=['accuracy_score', 'recall_score', 'precision_score', 'f1score','roc_score'])\nresults_rf_features_sorted= results_rf_features_rf.sort_values('accuracy score', ascending=False)\nresults_rf_features_kbest_sorted= results_rf_features_kbest.sort_values('accuracy score', ascending= False)\nresults_rfecv_features_sorted= results_rfecv_features.sort_values('accuracy score', ascending= False)\n\n\nresults_random_forest.iloc[0,0]=results_rf_features_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,0]=results_rf_features_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,0]=results_rf_features_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,0]=results_rf_features_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,0]=results_rf_features_sorted.iloc[0,:][4]\n\nresults_random_forest.iloc[0,1]=results_rf_features_kbest_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,1]=results_rf_features_kbest_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,1]=results_rf_features_kbest_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,1]=results_rf_features_kbest_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,1]=results_rf_features_kbest_sorted.iloc[0,:][4]\n\nresults_random_forest.iloc[0,2]=results_rfecv_features_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,2]=results_rfecv_features_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,2]=results_rfecv_features_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,2]=results_rfecv_features_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,2]=results_rfecv_features_sorted.iloc[0,:][4]\n\nresults_random_forest","29cd04f4":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_random_forest[results_random_forest.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'f1score', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   \n","adfb5288":"xgb_model= XGBClassifier(random_state= 22, use_label_encoder=False)\nxgb_model.fit(x_train, y_train)\nf_imp_xgboost= pd.DataFrame(columns= ['feature importances'], index= x_train.columns)\nresults_xgboost_scores= pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'roc auc score'], index= num_features)\n\n\nf_imp_xgboost['feature importances']=abs( xgb_model.feature_importances_)    \nf_imp_xgboost=f_imp_xgboost.sort_values('feature importances', ascending= False)    \nf_imp_xgboost\n\n","7cf0f163":"\nfor ind in range(3, len(f_imp_xgboost.index)+1):\n        \n    x_xgb_train2=x_train.loc[:, f_imp_xgboost.index[0:ind]]\n    x_xgb_test2= x_test.loc[:, f_imp_xgboost.index[0:ind]]\n    \n    xgb_model.fit(x_xgb_train2, y_train)\n    y_pred_xgb2= xgb_model.predict(x_xgb_test2)\n\n\n    results_xgboost_scores.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgb2)\n\n\n\n ","4bda1803":"results_xgboost_scores","2648e336":"results_xgboost_scores.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","caa1fa64":"results_xgboost_scores.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n ","b70e09be":"results_xgboost_scores.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","a0999b12":"results_xgb_kbest= pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'roc auc score'], index= num_features)\nfor ind in range(3,  len(feature_importances_kbest.index)+1):\n        \n    x_xgb_train1=x_train.loc[:,feature_importances_kbest.index[0:ind]]\n    x_xgb_test1= x_test.loc[:, feature_importances_kbest.index[0:ind]]\n    \n    xgb_model.fit(x_xgb_train1, y_train)\n    y_pred_xgb1= xgb_model.predict(x_xgb_test1)\n\n\n    results_xgb_kbest.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgb1)\n\n\n\n","7860cbec":"results_xgb_kbest.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","c8bd7dd9":"results_xgb_kbest.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n  ","b84c4add":"results_xgb_kbest.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","c90c7988":"xgboost_model=XGBClassifier(random_state= 22, use_label_encoder=False)     \n\nrfe2=RFE(estimator=xgboost_model, n_features_to_select=13, step=1)\n\nrfe2=rfe2.fit(x_train, y_train)\n\nfeature_importances_rfe2= pd.DataFrame(columns=['important features'] )\nfeature_importances_rfe2['important features'] = x_train.columns[rfe2.support_]\n","3a99987b":"feature_importances_rfe2","048247cd":"results_rfecv_features2=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score',  'roc auc score'], index=num_features)\nfor ind in range(3, len(feature_importances_rfe2.index)+1):\n    \n    \n    x_gboost_train=x_train.loc[:, feature_importances_rfe2['important features'] [0:ind]]\n    x_gboost_test= x_test.loc[:, feature_importances_rfe2['important features'] [0:ind]]\n    \n    xgboost_model.fit(x_gboost_train, y_train)\n    y_pred_xgboost2= xgboost_model.predict(x_gboost_test)\n    \n    \n    results_rfecv_features2.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgboost2)\n    \n\n","5028dba6":"results_rfecv_features2","5d692927":"results_rfecv_features2.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","1baa722f":"results_rfecv_features2.sort_values(['recall score'], ascending= False).iloc[0:2,:]","6ac90cc8":"results_rfecv_features2.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]","508742d3":"results_xgboost=pd.DataFrame(columns=[ 'XGBOOST_features_importances','XGBOOST_KBEST' ,'XGBOOST_rfecv'], index=['accuracy_score', 'recall_score', 'precision_score','roc_score'])\nresults_xgboost_features_sorted= results_xgboost_scores.sort_values('accuracy score', ascending=False)\nresults_xgboost_features_kbest_sorted= results_xgb_kbest.sort_values('accuracy score', ascending= False)\nresults_xgboost_rfecv_features_sorted= results_rfecv_features2.sort_values('accuracy score', ascending= False)\n\n\nresults_xgboost.iloc[0,0]=results_xgboost_features_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,0]=results_xgboost_features_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,0]=results_xgboost_features_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,0]=results_xgboost_features_sorted.iloc[0,:][3]\n\n\nresults_xgboost.iloc[0,1]=results_xgboost_features_kbest_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,1]=results_xgboost_features_kbest_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,1]=results_xgboost_features_kbest_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,1]=results_xgboost_features_kbest_sorted.iloc[0,:][3]\n\n\nresults_xgboost.iloc[0,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][3]\n\n\n","278ac76f":"results_xgboost","a6f2ab3b":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_xgboost[results_xgboost.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   \n","0bced366":"results_xgboost_random_forest=pd.DataFrame(columns=['xgboost', 'RandomForest'], index= ['accuracy_score', 'recall_score', 'precision_score', 'roc auc score'])","d1716c70":"results_xgboost","544bfe27":"results_random_forest","25d74775":"results_xgboost_random_forest.iloc[0,0]= results_xgboost.iloc[0, 2]\nresults_xgboost_random_forest.iloc[1,0]=results_xgboost.iloc[1, 2]\nresults_xgboost_random_forest.iloc[2,0]=results_xgboost.iloc[2, 2]\nresults_xgboost_random_forest.iloc[3,0]=results_xgboost.iloc[3, 2]\n\nresults_xgboost_random_forest.iloc[0,1]=results_random_forest.iloc[0, 1]\nresults_xgboost_random_forest.iloc[1,1]=results_random_forest.iloc[1, 1]\nresults_xgboost_random_forest.iloc[2,1]=results_random_forest.iloc[2, 1]\nresults_xgboost_random_forest.iloc[3,1]=results_random_forest.iloc[4, 1]\nresults_xgboost_random_forest","4427b2e3":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_xgboost_random_forest[results_xgboost_random_forest.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   ","71119407":"x_test_xgb_3=x_test.loc[: ,feature_importances_rfe2['important features'][0:12].to_list()]\nxgb_model2= XGBClassifier(random_state= 22, use_label_encoder=False)\nxgb_model2.fit(x_train.loc[: ,feature_importances_rfe2['important features'][0:12].to_list()], y_train)\n\ny_pred_xgb_3=xgb_model2.predict(x_test_xgb_3)\nxgb_cm= confusion_matrix(y_test, y_pred_xgb_3)\n\nsns.heatmap(xgb_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No heart disease', 'heart disease'], xticklabels = ['Predicted no heat disease', 'Predicted heart disease'])\nplt.yticks(rotation = 0)\nplt.show()","bcf49f8b":"x_test_rf_11=x_test.loc[: ,feature_importances_kbest.index[0:9]]\nrf_model2= RandomForestClassifier()\nrf_model2.fit(x_train.loc[: ,feature_importances_kbest.index[0:9]], y_train)\n\ny_pred_rf_11=rf_model2.predict(x_test_rf_11)\nrf_cm= confusion_matrix(y_test, y_pred_rf_11)\n\nsns.heatmap(rf_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No heart disease', 'heart disease'], xticklabels = ['Predicted no heat disease', 'Predicted heart disease'])\nplt.yticks(rotation = 0)\nplt.show()","19ff4b34":"**I'm going to fit RandomForest model on different combination of features starting with only 3 top features recommended by Scikitlearn to all 13 features and repeat the same steps with the 2 other features selection methods, after that I will put the results in a DataFrame so we can visualize it and compare between different combinations**","dc7b21bc":"For the seek of better results I'm going to repeat these steps with XGboost","21b3853c":"in this approach  we got the best results with all features.","9db134b9":"well, fitting for the second time both models with the selected features gives relatively same results ","be539741":"**now lets compare the highest results achieved in both xgboost and RandomForest**","b85589c7":"2. XGBoost with SelectKBest","6bf1b7f4":"1. XGBOOST Feature importances","f4c0fc53":"Here, including all features shows greatest results.","bf27382c":"3. XGBOOST with RFECV","4783e393":"Now I'm going to compare between the 3 methods of features selection and take the best combination for each method and compare these 3 selected ones","03b9c9a2":"**Load Libraries**","257668a1":"2. Feature_importances ","6ae0802d":"1. SelectKBest","045da98e":"**In this task will try different features selection approaches such as SelectKBest, feature selections provided by models and RFECV**","8c6addc6":"12 features provide the best scores\n","9c8c310a":"But what about selecting features with recursive feature elimination with cross-validation ? let's see ","8a683b5b":"**finally just to get better insights I will use heatmap to see confusion matrix for both models**","81229234":"for this type of features selection, 8 features give the highest results","373adcea":"we got the best results with 8 top features\n","bbbbc199":"**Now let's try the second method which is provided by the model and repeat the same steps as we did before** ","41b7ffd9":"**DATASET COLUMNS EXPLANATION** :\n\n* Age: (age in years)\n* Sex: (1 = male; 0 = female)\n* CP: (chest pain type)\n* TRESTBPS: (resting blood pressure (in mm Hg on admission to the hospital))\n* CHOL: (serum cholestoral in mg\/dl)\n* FPS: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* RESTECH: (resting electrocardiographic results)\n* THALACH:  (maximum heart rate achieved)\n* EXANG : (exercise induced angina (1 = yes; 0 = no))\n* OLDPEAK : (ST depression induced by exercise relative to rest)\n* SLOPE : (the slope of the peak exercise ST segment)\n* CA : (number of major vessels (0-3) colored by flourosopy)\n* THAL : (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* TARGET :(1 or 0)","8bd6e060":"the best reuslts with XGBOOST are  with 12 features.\n","d828e600":"\n**Now I'm going to sort only the two best combinations of features in terms of accuracy, recall and auc score**\n","5f99b6d6":"**as we can see above, eliminating the least good feature 'thalassemia' shows the greatest scores**","1a622cab":"**I will take XGboost with rfe features selection and RandomFOrest with SelectKBest features selections and compare them**","7b050312":"As we can see, RF did slightly better with accuracy and Auc score.","21a135fc":"2. Confusion Matrix with RandomForest","3ac48199":"**Conclusion**\n\nI think both models did well to be used for diagnosing Heart Disease based on this analysis maybe RandomForest slightly better.. \n\nthe RandomForest with the top 9 features ['thalach', 'ca', 'cp', 'oldpeak', 'exang', 'age', 'chol', 'trestbps','slope'].\n\nXGBOOST with 12 top features['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca']\n\nThis is my first notebook, please give me your feedback or some advice into how to improve the quality of my work and Thank you for your time. \n\n\n\n\n\n","2ef79d3d":"**I will use heatmap to see if there is some correlations with features**","80b7fa30":"1. Confusion Matrix with XGBOOST","baff0a11":"3. RFCV","165a6632":"as we can observe, RandomForest is giving the best results using KBest feature selections which include 8 features \n"}}