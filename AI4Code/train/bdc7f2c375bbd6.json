{"cell_type":{"5f6c9da1":"code","16373489":"code","573ba6e4":"code","87a09232":"code","e4f6d170":"code","f461ceaa":"code","c0b37c62":"code","5eef0a54":"code","ff9ec5b8":"code","89e1b72b":"code","0cfabbac":"code","0421008f":"code","5143c624":"code","2f46b935":"code","f97f3203":"code","4bf3ad8d":"code","b538914f":"code","f8c61125":"code","c407e817":"code","4dfe5de3":"code","f16be829":"code","5f2b8a90":"code","65581383":"markdown","88cd81f9":"markdown","491be058":"markdown","93a0cb9b":"markdown","bd05d0e3":"markdown","5bde53cc":"markdown","57707e62":"markdown","c5ef00ff":"markdown","df3a4cea":"markdown","6cf5c78e":"markdown","b98152ff":"markdown","f9cf659f":"markdown","bd738d95":"markdown","77cbd3cd":"markdown","7a3113d0":"markdown","aba23846":"markdown","6281fadb":"markdown","e43c790f":"markdown","0b57a86b":"markdown","be03906e":"markdown","506cac66":"markdown"},"source":{"5f6c9da1":"#Importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\n%matplotlib inline\nfrom statsmodels.tsa.stattools import ARMA, adfuller\nfrom datetime import datetime\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n","16373489":"\n#loading the data\nseries = pd.Series.from_csv(\"..\/input\/electricityproduction\/Electric-Production.csv\", header = 0)\n\nplt.figure(figsize = (15, 7))\nplt.plot(series)\nplt.xlabel('Year')\nplt.ylabel('Power Production in GigaWatt (GW)')\nplt.show()","573ba6e4":"series.index","87a09232":"def stationarity(ts, window, n):\n    roll_mean = ts.rolling(window).mean()\n    roll_std = ts.rolling(window).std()\n    \n    plt.figure(figsize = (16, 8))\n    \n    plt.plot(ts[:n], label = 'Original Data', color = 'red')\n    plt.plot(roll_mean[:n], label = 'Rolling Mean', color = 'blue')\n    plt.plot(roll_std[:n], label = 'Rolling Standard Deviation', color = 'green')\n    plt.title(\"Rolling Mean and Standard Deviation for the first %d observations\"%(n))\n    plt.legend(loc = 'best')\n    plt.show(block = False)\n    \n    dftest = adfuller(ts)\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n    \nstationarity(series,12,len(series))","e4f6d170":"ts_log = np.log(series)\nts_log_diff = ts_log - ts_log.shift() #differencing step\nts_log_diff.dropna(inplace = True)\n\nstationarity(ts_log_diff,365,len(ts_log_diff))","f461ceaa":"def predict(coef, history):\n    yhat = 0.0\n    for i in range(1, len(coef)+1):\n        yhat += coef[i-1] * history[-i]\n    return yhat\n\nX = ts_log_diff.values\nsize = len(X) - 100\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,0,0))\n    model_fit = model.fit(trend='nc', disp=False)\n    ar_coef = model_fit.arparams\n    #print(ar_coef)\n    yhat = predict(ar_coef, history)\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    #print('>predicted=%.3f, expected=%.3f' % (yhat, obs))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nplt.figure(figsize=(12,8))\nplt.plot(test, label = 'Original Data')\nplt.plot(predictions, label = 'Predicted Data')\nplt.legend(loc = 'best')\nplt.show()","c0b37c62":"X = ts_log_diff.values\nsize = len(X) - 100\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nresiduals = list()\n\nfor t in range(len(test)):\n    model = ARIMA(history, order=(0,0,1))\n    model_fit = model.fit(trend='nc', disp=False)\n    ma_coef = model_fit.maparams\n    resid = model_fit.resid\n    yhat = predict(ma_coef, resid)\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    #print('>predicted=%.3f, expected=%.3f' % (yhat, obs))\n    error = obs - yhat # expected-predicted\n    residuals.append(error)\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nplt.figure(figsize=(12,8))\nplt.plot(test, label = 'Original Data')\nplt.plot(predictions, label = 'Predicted Data')\nplt.legend(loc = 'best')","5eef0a54":"type(X)","ff9ec5b8":"from statsmodels.tsa.stattools  import acf, pacf\n\nlag_acf = acf(X, nlags = 12)\nlag_pacf = pacf(X, nlags = 12)","89e1b72b":"#plt.subplot(121)\nplt.figure(figsize=(12,8))\nplt.plot(lag_acf)\nplt.axhline(linestyle = '--', color = 'gray', y = 0)\nplt.axhline(linestyle = '--', color = 'gray', y = -1.96\/np.sqrt(len(X)))\nplt.axhline(linestyle = '--', color = 'gray', y = 1.96\/np.sqrt(len(X)))\nplt.title('Autocorrelation Function')\nplt.show()\n\n#plt.subplot(122)\nplt.figure(figsize=(12,8))\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(X)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(X)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.show()","0cfabbac":"type(ts_log_diff)","0421008f":"X = ts_log_diff.values\nsize = len(X) - 100\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n\tmodel = ARMA(history, order=(2,0,1))\n\tmodel_fit = model.fit(trend='nc', disp=False)\n\tar_coef, ma_coef = model_fit.arparams, model_fit.maparams\n\tresid = model_fit.resid\n\tyhat = predict(ar_coef, history) + predict(ma_coef, resid)\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\t#print('>predicted=%.3f, expected=%.3f' % (yhat, obs))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nplt.figure(figsize=(12,8))\nplt.plot(test, label = 'Original Data')\nplt.plot(predictions, label = 'Predicted Data')\nplt.legend(loc = 'best')","5143c624":"def difference(dataset):\n\tdiff = list()\n\tfor i in range(1, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - 1]\n\t\tdiff.append(value)\n\treturn np.array(diff)","2f46b935":"#ts_log_diff = Series.from_csv('daily-minimum-temperatures.csv', header=0)\nX = series.values\nsize = len(X) - 100\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\n#print(history)\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(1,1,1))\n    model_fit = model.fit(trend='nc', disp=False)\n    ar_coef, ma_coef = model_fit.arparams, model_fit.maparams\n    resid = model_fit.resid\n    diff = difference(history)\n    yhat = history[-1] + predict(ar_coef, diff) + predict(ma_coef, resid)\n    predictions.append(yhat)\n    #print(t, test[t])\n    obs = test[t]\n    history.append(obs)\n    #print('>predicted=%.3f, expected=%.3f' % (yhat, obs))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nplt.figure(figsize=(12,8))\nplt.plot(test, label = 'Original Data')\nplt.plot(predictions, label = 'Predicted Data')\nplt.legend(loc = 'best')","f97f3203":"#ts_log_diff = Series.from_csv('daily-minimum-temperatures.csv', header=0)\nX = ts_log_diff.values\nsize = len(X) - 100\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(2,1,0))\n\tmodel_fit = model.fit(trend='nc', disp=False)\n\tar_coef, ma_coef = model_fit.arparams, model_fit.maparams\n\tresid = model_fit.resid\n\tdiff = difference(history)\n\tyhat = history[-1] + predict(ar_coef, diff) + predict(ma_coef, resid)\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\t#print('>predicted=%.3f, expected=%.3f' % (yhat, obs))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\nplt.figure(figsize=(12,8))\nplt.plot(test, label = 'Original Data')\nplt.plot(predictions, label = 'Predicted Data')\nplt.legend(loc = 'best')","4bf3ad8d":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\n#plt.subplot(411)\nplt.figure(figsize = (10,7))\n#plt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n#plt.tight_layout()","b538914f":"pd.plotting.autocorrelation_plot(series)\nplt.show()","f8c61125":"from statsmodels.tsa.arima_model import ARIMA\n# fit model\nmodel = ARIMA(series, order=(5,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n# plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nplt.plot(residuals)\nplt.show()\nresiduals.plot(kind='kde')\nplt.show()\nprint(residuals.describe())","c407e817":"from sklearn.metrics import mean_squared_error\n\nX = series.values\nsize = int(len(X) * 0.66)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(5,1,0))\n\tmodel_fit = model.fit(disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\t#print('predicted=%f, expected=%f' % (yhat, obs))\nerror = np.sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % error)\n# plot\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()","4dfe5de3":"#divide into train and validation set\ntrain = series[:int(0.66*(len(series)))]\nvalid = series[int(0.66*(len(series))):]\n\n#plotting the data\ntrain.plot()\nvalid.plot()","f16be829":"#building the model\nfrom pyramid.arima import auto_arima\nmodel = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\nmodel.fit(train)\n\nforecast = model.predict(n_periods=len(valid))\nforecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.plot(train, label='Train')\nplt.plot(valid, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","5f2b8a90":"#calculate rmse\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\nrms = sqrt(mean_squared_error(valid,forecast))\nprint(rms)","65581383":"### 2. Moving Average Model","88cd81f9":"### 3. ARMA Model\n\n* Combination of **AR** and **MA** model\n* ARMA Model can work only on time series that are stationary.","491be058":"### References\n\n\n---\n\n\n\n\n1.  Palit, Ajoy K., and Dobrivoje Popovic. Computational Intelligence in Time Series Forecasting Theory and Engineering Applications. Springer, 2005.\n\n2.   https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n\n3.   https:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/complete-tutorial-time-series-modeling\/\n\n4. https:\/\/towardsdatascience.com\/understanding-the-hyperparameters-of-a-simple-time-series-model-631f26c46c9\n\n5. https:\/\/people.duke.edu\/~rnau\/411arim3.htm\n\n","93a0cb9b":"Models used here are types of **Regressive Models** which are built using Regression Analysis.\nWe'll be looking at four of these popular models","bd05d0e3":"* Here the Test Statistic Value is greater than the critical values. \n* Hence this series is **non-stationary**\n* To make it stationary we apply differencing one time and perform the test again.","5bde53cc":"### Forecasting a Time Series using Models\n\n#### ACF and PACF plots\n\n","57707e62":"### Dickey Fuller Test\n\n\n*   We perform this test to find if the series is stationary or not\n\n[Wikipedia Source](https:\/\/en.wikipedia.org\/wiki\/Dickey%E2%80%93Fuller_test)\n\n","c5ef00ff":"\n*  ARIMA model can used in non-stationary time series data also\n*  Differencing is a method of transforming a non-stationary time ts_log_diff into a stationary one\n*   This is an important step in preparing data to be used in an ARIMA model\n","df3a4cea":"### Decomposition\n\n* Here we try to decompose the series into its different characteristic components we've seen at the beginning.","6cf5c78e":"### Making a Time Series Stationary\n\n\n\n* A time series is said to be stationary if series' properties like mean and variance remain a constant over a period of time\n\n*   Because, if a series is stationary, we can say that with high probabilty over time it will follow the same in the future\n\n\n\n","b98152ff":"### 4. ARIMA Model","f9cf659f":"### ARIMA modelling for the Non-Stationary Series","bd738d95":"#### Characteristics of a Time Series Data\n\n*   Stationarity\n*   Trend\n*   Seasonality\n*   Residuals","77cbd3cd":"### 1. Auto Regressive Model","7a3113d0":"* Autoregression models express the current value of a time series by a finite linear aggregate of its previous values.\n* A crucial problem in modelling of autoregressive time series is the selection of the order of the model to be built. \n* A useful approach in this case is the analysis of the  related **partial  autocorrelation (PAC)** function. We'll be visualizing them in the next section.","aba23846":"### Auto Arima\n* It is a python library that finds the optimal values of p, q, d where\n  * p - autoregression parameter\n  * d - differencing parameter \n  * q - moving average window parameter","6281fadb":"* Here the test statistic value is lesser than the 1% critical value. \n* Hence we can conclude with 99% confidence that this differenced time series is **stationary** in nature.","e43c790f":"#### Importing Libraries and Loading Data","0b57a86b":"As the name suggests, this model gives us the average value of the batch of the time series data preceding a current data point over a specified order q","be03906e":"### ARIMA modelling after converting into a stationary series","506cac66":"### Dataset Description\nThe dataset used here for visualization is Electricity Production in America from Jan, 1939 to April, 2019 - monthly data"}}