{"cell_type":{"9c07e1b3":"code","0836ba6e":"code","0ac068b7":"code","0326923d":"code","12750910":"code","b1248208":"code","4a2ebfe2":"code","17d64757":"code","2a576188":"code","3b419c6b":"code","43f7e4dc":"code","cbeb2fa3":"code","d6d11841":"code","7bf25533":"code","2eeac2d0":"code","f8a7b6a0":"code","fead20b2":"code","69e0c01a":"code","0ba7fba2":"code","76627045":"markdown"},"source":{"9c07e1b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/weather-dataset'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0836ba6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow_hub as hub\n\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport os, cv2, json\nfrom PIL import Image\nfrom random import randint","0ac068b7":"# File Parameters\nWORK_DIR = \"\/kaggle\/input\/weather-dataset\"\nlabel_col = \"label\"\nimg_col = \"filename\"\ntrain_folder = \"train\"\ntest_folder = \"test\"\n\nos.listdir(WORK_DIR)","0326923d":"print('Train images: %d' %len(os.listdir(\n    os.path.join(WORK_DIR, train_folder))))\n\nprint('Test images: %d' %len(os.listdir(\n    os.path.join(WORK_DIR, test_folder))))","12750910":"train_df = pd.read_csv(os.path.join(WORK_DIR, \"Training_set.csv\"))\ntrain_df[\"label\"]=train_df[\"label\"].apply(lambda x:x.split(\",\"))\nlabel_names = train_df[label_col].value_counts().index\ndisplay(train_df.head())","b1248208":"test_df = pd.read_csv(os.path.join(WORK_DIR, \"Testing_set.csv\"))\ndisplay(test_df.head())","4a2ebfe2":"# Main parameters\nBATCH_SIZE = 32\nSTEPS_PER_EPOCH = np.ceil(len(train_df)*0.8 \/ BATCH_SIZE)\nVALIDATION_STEPS = np.ceil(len(train_df)*0.2 \/ BATCH_SIZE)\nEPOCHS = 300\nTARGET_SIZE = 224 # required size for MobileNet\n\nprint(STEPS_PER_EPOCH, VALIDATION_STEPS)","17d64757":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                     validation_split = 0.8,\n                                     preprocessing_function = None,\n                                     rotation_range = 45,\n                                     zoom_range = 0.2,\n                                     horizontal_flip = True,\n                                     vertical_flip = True,\n                                     fill_mode = 'nearest',\n                                     shear_range = 0.1,\n                                     height_shift_range = 0.2,\n                                     width_shift_range = 0.2)\n\ntrain_generator = train_datagen.flow_from_dataframe(train_df,\n                         directory = os.path.join(WORK_DIR, train_folder),\n                         subset = \"training\",\n                         x_col = img_col,\n                         y_col = label_col,\n                         seed=42,\n                         shuffle=True,\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         classes = ['sunrise', 'cloudy', 'foggy', 'rainy', 'shine'],\n                         class_mode = \"categorical\",)\n","2a576188":"validation_datagen = ImageDataGenerator(rescale=1.\/255,\n                                         validation_split = 0.2,\n                                         preprocessing_function = None,\n                                         rotation_range = 45,\n                                         zoom_range = 0.2,\n                                         horizontal_flip = True,\n                                         vertical_flip = True,\n                                         fill_mode = 'nearest',\n                                         shear_range = 0.1,\n                                         height_shift_range = 0.2,\n                                         width_shift_range = 0.2)\n\nvalidation_generator = validation_datagen.flow_from_dataframe(train_df,\n                         directory = os.path.join(WORK_DIR, train_folder),\n                         subset = \"validation\",\n                         x_col = img_col,\n                         y_col = label_col,\n                         seed=42,\n                         shuffle=True,\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         classes = ['sunrise', 'cloudy', 'foggy', 'rainy', 'shine'],\n                         class_mode = \"categorical\")","3b419c6b":"test_datagen=ImageDataGenerator(rescale=1.\/255.)\n\ntest_generator=test_datagen.flow_from_dataframe(\n                        dataframe=test_df,\n                        directory=os.path.join(WORK_DIR, test_folder),\n                        x_col=\"filename\",\n                        batch_size=1,\n                        seed=42,\n                        shuffle=False,\n                        class_mode=None,\n                        target_size=(TARGET_SIZE,TARGET_SIZE))","43f7e4dc":"from tensorflow import keras","cbeb2fa3":"## Extracting MobileNet Pre-trained model\n## Option 1\nURL = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/2\"\nfeature_extractor = hub.KerasLayer(URL,\n                                   input_shape=(TARGET_SIZE, TARGET_SIZE,3))\n\n# Setting the model to non-trainable to maintain original weights from imagenet\nfeature_extractor.trainable = False","d6d11841":"model = tf.keras.Sequential([\n    feature_extractor,\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(5, \n                 activation='softmax')\n])\n\nmodel.summary()","7bf25533":"## model 1\nmodel.compile(tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6),\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])","2eeac2d0":"STEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = validation_generator.n\/\/validation_generator.batch_size\nSTEP_SIZE_TEST = test_generator.n\/\/test_generator.batch_size\n\nprint(\"========== Hyperparameters ==========\")\nprint(\"Train Step: \", STEP_SIZE_TRAIN, \"Validation step: \", STEP_SIZE_VALID, \"Test Step: \", STEP_SIZE_TEST)\nprint(\"=====================================\")","f8a7b6a0":"early_stopping = EarlyStopping(monitor=\"val_loss\",\n                                min_delta=0.001,\n                                patience=10,\n                                verbose=1,\n                                restore_best_weights=True)\n\nhistory = model.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=validation_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=EPOCHS,\n#                     callbacks = [early_stopping]\n)","fead20b2":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.set_style(\"white\")\nplt.suptitle('Train history', size = 15)\n\nax1.plot(epochs, acc, label = \"Training acc\")\nax1.plot(epochs, val_acc, label = \"Validation acc\")\nax1.set_title(\"Training and validation acc\")\nax1.legend()\n\nax2.plot(epochs, loss, label = \"Training loss\")\nax2.plot(epochs, val_loss, label = \"Validation loss\")\nax2.set_title(\"Training and validation loss\")\nax2.legend()\n\nplt.show()","69e0c01a":"test_generator.reset()\npred=model.predict_generator(test_generator,\nsteps=STEP_SIZE_TEST,\nverbose=1)","0ba7fba2":"predictions=[]\nlabels = train_generator.class_indices\nlabels = dict((v,k) for k,v in labels.items())\nfor row in pred:\n    pred_index = np.argmax(row)\n    weather_pred = labels[pred_index]\n    predictions.append(weather_pred)\n# print(predictions)\nfilenames=test_generator.filenames\nresults=pd.DataFrame({\"filename\":filenames,\n                      \"label\":predictions})\nresults.to_csv(\"submission.csv\",index=False)\ndisplay(results)","76627045":"# Building a model"}}