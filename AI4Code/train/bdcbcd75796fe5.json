{"cell_type":{"da584665":"code","09966f70":"code","a18bc4b6":"code","d35403c7":"code","3ffb4d95":"code","6fa937b0":"code","8fafee05":"code","093faaa1":"code","2deeacef":"code","9e68b8bd":"code","438b738c":"code","7dc3a068":"code","18bc518e":"code","dbbbbad2":"code","4ae2f431":"code","ec768f49":"code","0a2be838":"code","ebbdfe69":"code","50038476":"code","43e3a40e":"code","890dc192":"code","a626a555":"code","f874661f":"code","c0d9aca5":"code","25d65969":"code","05c71f58":"code","8a10a157":"code","1f00358e":"code","809d041b":"code","c8c712f5":"code","7ce05587":"code","df6ccd39":"code","5e94b673":"code","9062e462":"code","ef80028f":"code","af847c1f":"code","328371e3":"code","f9df6fbf":"code","a4fbb29c":"code","1bb424b6":"code","f572012f":"code","f2f476e2":"code","9592aea6":"code","66369f90":"code","3ca874e5":"code","c115587d":"code","0278240a":"code","70ab854f":"code","15cadfc3":"code","6af8f81d":"code","4092a681":"code","8256c00a":"markdown","dd143e45":"markdown","e44e9b37":"markdown","a8392b8b":"markdown","a8596229":"markdown","520fe1ee":"markdown","06fbce55":"markdown","98bd4cf5":"markdown","785194bd":"markdown","bf3deac3":"markdown","4d3aa57d":"markdown","56f60e6c":"markdown","734f5ad1":"markdown","84913135":"markdown","fbc1de85":"markdown","b13bd4c5":"markdown","48d17735":"markdown","85c78b35":"markdown","daa6fba4":"markdown"},"source":{"da584665":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","09966f70":"data = pd.read_csv('..\/input\/mercedesbenz-greener-manufacturing\/train.csv',usecols=['X1','X2','X3','X4','X5','X6'])\ndata.head()","a18bc4b6":"#To check the Unique values\nfor i in data.columns:\n    print(i,' : ',len(data[i].unique()), 'labels')","d35403c7":"#If we perform one hot encoding, lets see how many columns will be generated\npd.get_dummies(data,drop_first =True ).shape\n\n#Its says it will generate 117 columns","3ffb4d95":"#Finding top 10 most frequent values of X2\na = data['X2'].value_counts()\nb = a.sort_values(ascending = False)\nb.head(10)\n#OR\n#data['X2'].value_counts().sort_values(ascending = False).head(10)","6fa937b0":"#Making a list of the index values of top 10 frequent values\nl = []\nfor i in data['X2'].value_counts().sort_values(ascending = False).head(10).index:\n    l.append(i)\nprint(l)\n#OR\n#l = [i for i in data['X2'].value_counts().sort_values(ascending = False).head(10).index]\n","8fafee05":"# get whole set of dummy variables, for all the categorical variables\ndef one_hot_encoding_top_x(data, variable, l):\n    # function to create the dummy variables for the most frequent labels\n    # we can vary the number of most frequent labels that we encode\n    for label in l:\n        data[label] = np.where(data[variable]==label,1,0)\none_hot_encoding_top_x(data, 'X2', l)\ndata.head(20)","093faaa1":"#You can now do the same thing for all the other columns\n#After completing the for all the columns, drop the initial columns (X1,X2,X3,X4,X5,X6)","2deeacef":"df = pd.read_csv('..\/input\/mercedesbenz-greener-manufacturing\/train.csv',usecols=['X1','X2'])\ndf.head()","9e68b8bd":"#Take the count of unique values and convert into a dictionary\ndict_var = df.X2.value_counts().to_dict()\ndict_var","438b738c":"df['X2'].head()","7dc3a068":"df.X2 = df.X2.map(dict_var)\ndf","18bc518e":"import datetime","dbbbbad2":"#Today's date\ntoday = datetime.datetime.today()\ntoday","4ae2f431":"#Difference between today's date and the no. of days mentioned(In our case - 2)\ntoday-datetime.timedelta(2)","ec768f49":"#Taking 15 days data in a list comprehension\ndays = [today-datetime.timedelta(x) for x in range(15)]","0a2be838":"data = pd.DataFrame(days)\ndata.columns = ['Day']","ebbdfe69":"data","50038476":"#To get the day's value\ndata['Weekday']=data['Day'].dt.strftime(\"%A\")","43e3a40e":"data.head()","890dc192":"#Creating a dictionary to give ranks to each day\ndict = {'Moday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7}\n#Creating a new column and appending the ranks based on the day\ndata['ordinal_rank'] = data['Weekday'].map(dict)","a626a555":"data.head()","f874661f":"df=pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Cabin','Survived'])\ndf.head()","c0d9aca5":"#Filled the missing values with the word 'Missing'\ndf['Cabin'].fillna('Missing',inplace=True)","25d65969":"#Taking only the first letter of the word (For eg if C85 then only C will be taken)\ndf['Cabin']=df['Cabin'].astype(str).str[0]","05c71f58":"df.head()","8a10a157":"#Checking the unique values in cabin column now\ndf.Cabin.unique()","1f00358e":"#Taking the mean of the Cabin value along with the survived column to check for the percentages\ndf.groupby(['Cabin'])['Survived'].mean()","809d041b":"#Taking the index of Cabin column\ndf.groupby(['Cabin'])['Survived'].mean().sort_values().index","c8c712f5":"#Adding the sorted values in ordinal_labels\nordinal_labels=df.groupby(['Cabin'])['Survived'].mean().sort_values().index\nordinal_labels","7ce05587":"#Assigning ranks as per the sorted percentage\n#The ranks range from 0 to length of ordinal_labels\nordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\nordinal_labels2","df6ccd39":"#Creating a new column containing ranks\ndf['Cabin_ordinal_labels']=df['Cabin'].map(ordinal_labels2)\ndf.head()","5e94b673":"df=pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Cabin','Survived'])\ndf.head()","9062e462":"#Filled the missing values with the word 'Missing'\ndf['Cabin'].fillna('Missing',inplace=True)","ef80028f":"#Taking only the first letter of the word (For eg if C85 then only C will be taken)\ndf['Cabin']=df['Cabin'].astype(str).str[0]","af847c1f":"#Checking the unique values in cabin column now\ndf.Cabin.unique()","328371e3":"#Take the mean and store the values in dict\nmean_ordinal=df.groupby(['Cabin'])['Survived'].mean().to_dict()\nmean_ordinal","f9df6fbf":"#Creating a new column and appending the values according to the mean\ndf['mean_ordinal_encode']=df['Cabin'].map(mean_ordinal)\ndf.head()","a4fbb29c":"df=pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Cabin','Survived'])\ndf.head()","1bb424b6":"#Filled the missing values with the word 'Missing'\ndf['Cabin'].fillna('Missing',inplace=True)","f572012f":"#Taking only the first letter of the word (For eg if C85 then only C will be taken)\ndf['Cabin']=df['Cabin'].astype(str).str[0]","f2f476e2":"#Checking the unique values in cabin column now\ndf.Cabin.unique()","9592aea6":"#Create a varible to take the mean of survived group by cabin\nmean_prob=df.groupby(['Cabin'])['Survived'].mean()\nmean_prob","66369f90":"#Creating a new dataframe and storing the above data\na = pd.DataFrame(mean_prob)\na","3ca874e5":"#Creating the Died Column\na['Died']=1-a['Survived']","c115587d":"a.head()","0278240a":"#Creating a column and appending the calculated probablity values\na['Prob'] = a['Survived']\/a['Died']","70ab854f":"a.head()","15cadfc3":"#Creating a new variable and passing the Prob column of dataset a into a dictionary\nprobablity_encoded = a.Prob.to_dict()","6af8f81d":"#Creating a new column in original dataframe (df) and adding the probablity values using map function\ndf['Cabin_encoded'] = df['Cabin'].map(probablity_encoded)","4092a681":"df.head()","8256c00a":"# 4. Ordinal Number Encoding","dd143e45":"### Apply one hot encoding on top 10 values of each column (Showing for X2 first)","e44e9b37":"## One Hot Encoding for Categorical data ","a8392b8b":"# 3. Replacing the variables with their count","a8596229":"# 2. One hot encoding on top 10 values\n","520fe1ee":"### Disadvantages","06fbce55":"* Does not add any information that makes the variables more predictable\n* Does not keep the information of the ignored variables","98bd4cf5":"* Does not require hours of variable exploration\n* Straightforward Implementation\n* Does not expand the no. of columns massively","785194bd":"# 6. Mean Encoding","bf3deac3":"### Advantages of one hot encoding on top variables","4d3aa57d":"# 1. One hot encoding on all the values","56f60e6c":"## Importing Libraries","734f5ad1":"* If the labels have same count, they will get the same values. It may lead to important information loss","84913135":"# 5. Target Guided Ordinal Encoding\n1. Ordering the labels according to the target\n2. Replace the labels by the joint probability of being 1 or 0","fbc1de85":"## Disadvantages ","b13bd4c5":"## Advantages","48d17735":"* Easy to implement\n* Does not increase the feature dimension size (columns)","85c78b35":"## 1. One hot encoding on all the values\n## 2. One hot encoding on top 10 values\n## 3. Replacing the variables with their count\n## 4. Ordinal Number Encoding\n## 5. Target Guided Ordinal Encoding\n## 6. Mean Encoding\n## 7. Probablity Ratio Encoding","daa6fba4":"# 7. Probablity Ratio Encoding"}}