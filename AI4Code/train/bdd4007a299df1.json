{"cell_type":{"274336e6":"code","b543781b":"code","b5343184":"code","5673fd83":"code","28431c3a":"code","02af2167":"code","fe88a727":"code","5ca00034":"code","d75e365c":"code","1109ada9":"code","db794247":"code","58d74d35":"code","fe31c7ff":"code","01f11600":"code","89cda192":"code","8cc7a5ce":"code","d933e03a":"code","1cbf193a":"code","e769e03b":"code","33600dd5":"code","c604923a":"code","4b3f411d":"code","a58b7167":"code","81d3bf4f":"code","86b77bff":"code","5c633e38":"code","c50b9af0":"code","391c05dc":"code","af4bdbbc":"code","c25f3258":"code","9debcf7a":"code","4ae52859":"code","a39a98cc":"code","8e8af705":"code","7198cdb3":"code","800ac617":"code","70d8fb6d":"code","815b54b1":"code","13bb04c4":"code","32fff890":"code","f5a05b09":"code","0bd211a6":"code","870f0a04":"code","7e877c60":"code","fb45bc4e":"code","76a1ea62":"code","be462702":"code","fef4c96d":"code","e15774d6":"code","b819ca9f":"code","d9e3664c":"code","e99c5ed9":"code","31ba24b0":"code","d5aad318":"code","51aaecc8":"code","0cbf3db8":"code","70d06710":"code","275458eb":"code","7cb0fedb":"code","92258af7":"code","9bf92517":"code","2aa82ca4":"code","77ba9101":"code","42173f21":"code","f83a60a5":"code","e65b2e82":"code","5096bbe2":"code","0e685e10":"code","8e62fbfb":"code","a0263874":"code","f9c6da07":"code","64ef1f19":"code","34c968eb":"code","e16c7ba2":"code","53110a67":"code","7fc74189":"code","c5337414":"code","083aecc2":"code","15edc51d":"code","b83c2c6b":"code","2410ed5c":"code","537fa3e5":"code","2924e182":"code","c905bede":"code","4b3157da":"code","a21c2f86":"code","9ea6fb2e":"code","e44e35ee":"code","8b200f55":"markdown","15b7ea95":"markdown","f008f6a3":"markdown","8a6180be":"markdown","981ea78b":"markdown","b38e62d8":"markdown","4db8acf3":"markdown","71b96c9b":"markdown","86b18f23":"markdown","b81d4bc3":"markdown","f8098dee":"markdown","6d3e8e7f":"markdown","68b60e4d":"markdown","33a30667":"markdown","767192e7":"markdown","81b90144":"markdown","a65d9903":"markdown","251e2cc7":"markdown","14605411":"markdown","90a07c0d":"markdown","fd52cdf0":"markdown","f4a63058":"markdown","cba940bc":"markdown","398e9843":"markdown","71582ecd":"markdown","d0890ab4":"markdown","499f0c71":"markdown","e2837ebf":"markdown","abb17ccd":"markdown","fcd4b86a":"markdown","4f92a3ed":"markdown","9192d68e":"markdown","83a96133":"markdown","5ef31e6b":"markdown","62ce1c03":"markdown","df9f9594":"markdown","3886ab15":"markdown","a325855b":"markdown","4342d0b8":"markdown","429bd449":"markdown","353ac836":"markdown","bc1bc912":"markdown","a1065688":"markdown","edc7cb99":"markdown","25b05e0b":"markdown","a82546ac":"markdown","f9e05273":"markdown","d3cdefa4":"markdown","376958c4":"markdown","10c53dc4":"markdown","063dff09":"markdown","f2e9e999":"markdown","4e263401":"markdown","e7dbb845":"markdown","b9c0fcb7":"markdown","6b010f3f":"markdown","ef9d0d83":"markdown","5e0bf89d":"markdown","9eb97e53":"markdown","a5abe443":"markdown","a13767e4":"markdown","56fb7be4":"markdown","76fb6fb2":"markdown","46fbb91b":"markdown","3b4f89de":"markdown","2cdb3ba8":"markdown","48e5eab3":"markdown"},"source":{"274336e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n%matplotlib inline","b543781b":"# '..\/input\/house-prices-advanced-regression-techniques' for Kaggle\ndef load_housing_data(data_name):\n    csv_path = os.path.join('..\/input\/house-prices-advanced-regression-techniques', data_name)\n    return pd.read_csv(csv_path)","b5343184":"train, test = load_housing_data(\"train.csv\"), load_housing_data(\"test.csv\")","5673fd83":"train.head()","28431c3a":"train.shape","02af2167":"train.select_dtypes(include='number').columns","fe88a727":"train.select_dtypes(include='object').columns","5ca00034":"train['YrSold'].value_counts()","d75e365c":"def to_str(feature, data=train):\n    data[feature] = data[feature].astype(str)","1109ada9":"to_str('YrSold')\nto_str('MoSold')\nto_str('MSSubClass')\nto_str('YrSold',test)\nto_str('MoSold',test)\nto_str('MSSubClass',test)","db794247":"train.dtypes.value_counts()","58d74d35":"sns.set_style(\"darkgrid\")","fe31c7ff":"def plot_SalePrice(data=train['SalePrice']):    \n    sns.distplot(data)\n    plt.ylabel(\"Frequency\")\n    plt.show()","01f11600":"plot_SalePrice()","89cda192":"#skewness and kurtosis\nprint(\"Skewness: \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis: \" + str(train['SalePrice'].kurt()))","8cc7a5ce":"corr_matrix = train.corr()\n(corr_matrix[\"SalePrice\"]**2).sort_values(ascending=False)","d933e03a":"corr = train.corr()**2\nplt.subplots(figsize=(12,12))\nsns.heatmap(corr, vmax=0.9, square=True)\nplt.show()","1cbf193a":"def scatterplot(x, y='SalePrice', data=train):\n#     plt.subplots(figsize=(12,8))\n    sns.scatterplot(x=x, y=y, data=data)\n    plt.show()","e769e03b":"sns.boxplot(x='OverallQual', y='SalePrice', data=train)\nplt.show()","33600dd5":"scatterplot('GrLivArea')","c604923a":"scatterplot('GarageCars')","4b3f411d":"scatterplot('GarageArea')","a58b7167":"scatterplot('TotalBsmtSF')","81d3bf4f":"scatterplot('1stFlrSF')","86b77bff":"scatterplot('TotRmsAbvGrd')","5c633e38":"scatterplot('YearBuilt')","c50b9af0":"scatterplot('YearRemodAdd')","391c05dc":"scatterplot('MasVnrArea')","af4bdbbc":"train.sort_values('GrLivArea', ascending=False).head(2)['GrLivArea']","c25f3258":"train = train[train['GrLivArea'] < 4676]","9debcf7a":"train.shape","4ae52859":"train.sort_values('GrLivArea', ascending=False).head(2)['GrLivArea']","a39a98cc":"plot_SalePrice()","8e8af705":"train['SalePrice'] = np.log1p(train['SalePrice'])","7198cdb3":"plot_SalePrice(train['SalePrice'])","800ac617":"test_dummy = test.copy()\ntest_dummy['SalePrice'] = np.zeros(len(test))\ncombined = pd.concat([train,test_dummy])","70d8fb6d":"combined.shape","815b54b1":"def missing_percent(data, n=35):\n    num_of_nulls = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\n    result = pd.concat([num_of_nulls, percent], axis=1, keys=['Number', 'Percent'])\n    return result.head(n)","13bb04c4":"missing_percent(train)","32fff890":"missing_percent(test)","f5a05b09":"missing_percent(combined)","0bd211a6":"import warnings\nwarnings.filterwarnings(\"ignore\")","870f0a04":"def fill_missing(df):\n    \n    # Filling 'Functional' according to the data description\n    df['Functional'] = df['Functional'].fillna('Typ')\n    \n    # Filling some categorical values with mode\n    cats = ['Electrical','SaleType','Exterior2nd','KitchenQual','Exterior1st','MSZoning']\n    for cat in cats:\n        df[cat] = df[cat].fillna(df[cat].mode()[0])\n    \n    # Filling LotFrontage by grouping by neighborhood and taking the median\n    df['LotFrontage'] = df['LotFrontage'].fillna(\n        df.groupby('Neighborhood')['LotFrontage'].transform('median'))\n    \n    # Filling the rest of the categorical value with 'None'\n    # (For some features like those of basement and garage, NA means None.\n    # But for some features we don't know so let's just use None)\n    df_cat = df[list(df.select_dtypes(include='object').columns)]\n    df.update(df_cat.fillna('None'))\n    \n    # Filling the rest of the numerical values with 0\n    # (For some features like alley and LotFrontage, NA means 0.\n    # But for some features we don't know so let's just use 0)\n    df_num = df[list(df.select_dtypes(include='number').columns)]\n    df.update(df_num.fillna(0))\n    \n    return df","7e877c60":"fill_missing(train).isnull().sum().sum()","fb45bc4e":"fill_missing(test).isnull().sum().sum()","76a1ea62":"test_dummy = test.copy()\ntest_dummy['SalePrice'] = np.zeros(len(test))\ncombined = pd.concat([train,test_dummy])","be462702":"missing_percent(combined,1)","fef4c96d":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\ndef fix_skew(df):\n    \n    skew_index = []\n\n    # A function to show the top 10 skewed features\n    def show_skew(df):\n        df_num = df[list(df.select_dtypes(include='number').columns)]\n\n        skew_features = df_num.apply(lambda x: x.skew()).sort_values(ascending=False)\n\n        high_skew = skew_features[skew_features > 0.6]\n        nonlocal skew_index\n        skew_index = high_skew.index\n\n        print(\"{} features have skew > 0.6 :\".format(high_skew.shape[0]))\n        skewness = pd.DataFrame({'Skew' :high_skew})\n        print(skew_features.head(10))\n        \n    # Before transformation\n    show_skew(df)\n\n    # Transformation\n    for i in skew_index:\n        df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))\n        \n    # After transformation\n    show_skew(df)","e15774d6":"fix_skew(combined)","b819ca9f":"combined.select_dtypes(include='number').columns","d9e3664c":"combined.select_dtypes(include='object').columns","e99c5ed9":"combined['Has2ndFlr'] = combined['2ndFlrSF'].apply(lambda x : 1 if x > 0 else 0)\n\ncombined['HasWoodDeck'] = combined['WoodDeckSF'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasOpenPorch'] = combined['OpenPorchSF'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasEnclosedPorch'] = combined['EnclosedPorch'].apply(lambda x : 1 if x > 0 else 0)\ncombined['Has3SsnPorch'] = combined['3SsnPorch'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasScreenPorch'] = combined['ScreenPorch'].apply(lambda x : 1 if x > 0 else 0)","31ba24b0":"def binarize(column, data=combined):\n    print(combined[column].value_counts())\n    combined[column] = combined[column].apply(lambda x : 0 if x == 'None' else 1)\n    print(combined[column].value_counts())","d5aad318":"binarize('PoolQC')","51aaecc8":"combined['HouseQualAdd'] = combined['OverallQual'] + combined['OverallCond']\ncombined['HouseQualProd'] = combined['OverallQual'] * combined['OverallCond']","0cbf3db8":"combined['TotalSqrFt'] = (combined['BsmtFinSF1'] + combined['BsmtFinSF2']\n                    + combined['1stFlrSF'] + combined['2ndFlrSF'])\ncombined['TotalSF'] = (combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF'])\ncombined['TotalPorchSF'] = (combined['WoodDeckSF'] + combined['OpenPorchSF']\n                         + combined['EnclosedPorch'] + combined['3SsnPorch'] \n                         + combined['ScreenPorch'])\ncombined['TotalBath'] = (combined['FullBath'] \n                      + (0.5 * combined['HalfBath']) \n                      + combined['BsmtFullBath'] \n                      + (0.5 * combined['BsmtHalfBath']))","70d06710":"combined.drop(['Street', 'Utilities'], axis=1, inplace=True)","275458eb":"combined.shape","7cb0fedb":"combined_encoded = pd.get_dummies(combined).reset_index(drop=True)","92258af7":"combined_encoded.shape","9bf92517":"combined_encoded = combined_encoded.loc[:,~combined_encoded.columns.duplicated()]","2aa82ca4":"combined_encoded.shape","77ba9101":"len(train)","42173f21":"y_test, X_test = (combined_encoded[len(train):][\"SalePrice\"], \n                 combined_encoded.drop([\"SalePrice\"], axis=1)[len(train):])","f83a60a5":"y_test.shape, X_test.shape","e65b2e82":"y_train_full, X_train_full = (combined_encoded[:len(train)][\"SalePrice\"], \n                             combined_encoded.drop([\"SalePrice\"], axis=1)[:len(train)])","5096bbe2":"y_train_full.shape, X_train_full.shape","0e685e10":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, test_size=0.20, random_state=42)","8e62fbfb":"X_train.shape, X_valid.shape","a0263874":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nimport hyperopt\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, hp, anneal, Trials\nfrom sklearn.preprocessing import RobustScaler","f9c6da07":"def rmsle(y, y_pred):\n    # y and y_pred are already logarithmic as we've used the log1p transform\n    return np.sqrt(mean_squared_error(y, y_pred))","64ef1f19":"kfolds = KFold(n_splits=10, random_state=42, shuffle=True)\n\ndef cv_rmse(model, X=X_train_full):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train_full, \n                                    scoring='neg_mean_squared_error', cv=kfolds))\n    return (rmse)","34c968eb":"def mse_cv(params, cv=kfolds, X=X_train_full, y=y_train_full):\n    # the function gets a set of variable parameters in \"params\"\n    params = {'max_depth': int(params['max_depth']),\n              'learning_rate': params['learning_rate'],\n              'gamma': params['gamma'],\n              'colsample_bytree': params['colsample_bytree'], \n              'subsample': params['subsample']\n             }\n    \n    # we use this params to create a new LGBM Regressor\n    model = XGBRegressor(objective='reg:linear', n_estimators=200,\n                          random_state=42, \n                          **params)\n    \n    # and then conduct the cross validation with the same folds as before\n    score = np.sqrt(-cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean())\n\n    return score","e16c7ba2":"# possible values of parameters\nspace={'max_depth' : hp.quniform('max_depth', 2, 10, 1),\n       'learning_rate': hp.loguniform('learning_rate', -5, 0), \n       'gamma': hp.loguniform('gamma', -1, 0), \n       'colsample_bytree': hp.loguniform('colsample_bytree', -1, 0), \n       'subsample': hp.loguniform('subsample', -1, 0)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=mse_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=200, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(42) # fixing random state for the reproducibility\n         )","53110a67":"# computing the score on the test set\nxgb_reg = XGBRegressor(random_state=42, n_estimators=5000,\n                     max_depth=int(best['max_depth']),learning_rate=best['learning_rate'], \n                     gamma=best['gamma'], colsample_bytree=best['colsample_bytree'], \n                     subsample=best['subsample'], objective='reg:linear', \n                     nthread=-1, scale_pos_weight=1)\n\nprint(np.mean(cv_rmse(xgb_reg))) \n\nprint(\"Best MSE {:.3f} params {}\".format( mse_cv(best), best)) ","7fc74189":"xgb_reg = XGBRegressor(random_state=42,\n                       n_estimators=7000, \n                       max_depth=2, \n                       learning_rate=0.138170657, \n                       gamma=0.38498075, \n                       colsample_bytree=0.599437614, \n                       objective='reg:linear', \n                       nthread=-1,\n                       scale_pos_weight=1,\n                       subsample=0.534382267)","c5337414":"lgbm_reg = LGBMRegressor(random_state=42, \n                         n_estimators=7000, \n                         num_leaves=3, \n                         learning_rate=0.099384860, \n                         bagging_seed=5, \n                         feature_fraction_seed=5, \n                         bagging_fraction=0.407679661, \n                         feature_fraction=0.563479974, \n                         min_sum_hessian_in_leaf=20)","083aecc2":"svr_reg = make_pipeline(RobustScaler(), SVR(C=11,\n                                            gamma=0.006742321288,\n                                            epsilon=0.010861719298))","15edc51d":"gb_reg = GradientBoostingRegressor(max_depth=21,\n                                   min_samples_leaf=11,\n                                   min_samples_split=8,\n                                   learning_rate=0.038954801112873964, \n                                   loss='huber', max_features='sqrt',\n                                   n_estimators=7000, random_state=42)","b83c2c6b":"ridge_alphas = [1e-10, 1e-8, 1e-5, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, \n                0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n\nlasso_alphas = [5e-5, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001]\nelastic_l1ratio = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 1]","2410ed5c":"ridge_reg = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kfolds))\nlasso_reg = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=lasso_alphas, \n                                              random_state=42, cv=kfolds))\nelasticnet_reg = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                                                            alphas=elastic_alphas, \n                                                            cv=kfolds, \n                                                            l1_ratio=elastic_l1ratio))                                ","537fa3e5":"stack_gen = StackingCVRegressor(regressors=(ridge_reg, lasso_reg, elasticnet_reg, xgb_reg, lgbm_reg, gb_reg, svr_reg),\n                                meta_regressor=xgb_reg,\n                                use_features_in_secondary=True)","2924e182":"scores = {}\n\nscore = cv_rmse(lgbm_reg)\nprint(\"lgbm_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgbm'] = (score.mean(), score.std())\n\nscore = cv_rmse(xgb_reg)\nprint(\"xgb_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())\n\nscore = cv_rmse(svr_reg)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())\n\nscore = cv_rmse(ridge_reg)\nprint(\"ridge_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())\n\nscore = cv_rmse(lasso_reg)\nprint(\"lasso_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())\n\nscore = cv_rmse(elasticnet_reg)\nprint(\"elasticnet_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['elasticnet'] = (score.mean(), score.std())\n\nscore = cv_rmse(gb_reg)\nprint(\"gb_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gb'] = (score.mean(), score.std())","c905bede":"%%time\nprint('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X_train_full), np.array(y_train_full))\n\nprint('elasticnet')\nelastic_reg_model = elasticnet_reg.fit(X_train_full, y_train_full)\n\nprint('Lasso')\nlasso_reg_model = lasso_reg.fit(X_train_full, y_train_full)\n\nprint('Ridge') \nridge_reg_model = ridge_reg.fit(X_train_full, y_train_full)\n\nprint('Svr')\nsvr_reg_model = svr_reg.fit(X_train_full, y_train_full)\n\nprint('GradientBoosting')\ngb_reg_model = gb_reg.fit(X_train_full, y_train_full)\n\nprint('xgboost')\nxgb_reg_model = xgb_reg.fit(X_train_full, y_train_full)\n\nprint('lightgbm')\nlgbm_reg_model = lgbm_reg.fit(X_train_full, y_train_full)\n\nprint('END FIT')","4b3157da":"def blender(X):\n    return ((0.1 * elastic_reg_model.predict(X)) + \\\n            (0.1 * lasso_reg_model.predict(X)) + \\\n            (0.1 * ridge_reg_model.predict(X)) + \\\n            (0.1 * svr_reg_model.predict(X)) + \\\n            (0.1 * gb_reg_model.predict(X)) + \\\n            (0.1 * xgb_reg_model.predict(X)) + \\\n            (0.1 * lgbm_reg_model.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","a21c2f86":"print('RMSLE score on train data:')\nprint(rmsle(y_train_full, blender(X_train_full)))","9ea6fb2e":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blender(X_test)))","e44e35ee":"submission.to_csv(\"submission.csv\", index=False)","8b200f55":"#### GarageCars and GarageArea","15b7ea95":"## Scores for Each Model","f008f6a3":"### Train and Valid","8a6180be":"Also, let's use boxcox1p to transform the skewed features (again, 1p to avoid sadness because of zeros)","981ea78b":"## Making New Features!","b38e62d8":"Let's use Seaborn to see the distribution of SalePrice, our target feature.","4db8acf3":"#### PoolQC","71b96c9b":"There's 2 very obvious outliers!","86b18f23":"#### Conclusion\n* The target feature `SalePrice` is obviously not normally distributed.\n* The distribution is right-skewed.\n* We will have to fix this later on.","b81d4bc3":"### Using Multiple Features","f8098dee":"## Ridge, Lasso and ElasticNet","6d3e8e7f":"Looks good!","68b60e4d":"#### YearBuilt and YearRemodAdd","33a30667":"There are features where the values are null for a reason. We'll fill these with their appropriate values according to the data description, and for the rest we will decide how to impute. ","767192e7":"### Scatterplots","81b90144":"### From GrLivArea\nWe found 2 outliers while plotting GrLivArea. Let's remove those.","a65d9903":"## Stacking","251e2cc7":"## Looking at Individual Correlations","14605411":"SalePrice increases with OverallQual.","90a07c0d":"#### MasVnrArea","fd52cdf0":"#### 1stFlrSF","f4a63058":"## Correlations","cba940bc":"## SVR","398e9843":"### Is It Normal?","71582ecd":"# Splitting the Dataset","d0890ab4":"### Test","499f0c71":"### Cross-Validation Metric","e2837ebf":"#### TotalRmsAbvGrd","abb17ccd":"## Hyperopt\nIf you don't know much about this library, I encourage you to go through this kernel: https:\/\/www.kaggle.com\/ilialar\/hyperparameters-tunning-with-hyperopt\n<br><br>\nAt the time of making this notebook, Hyperopt was a new library for me too. If you find any way to optimize this code, please let me know. ","fcd4b86a":"Methods used:\n* Stacking\n* Blending\n* Cross-Validation\n* Robust Scaling\n* RMSLE (Which is the competition metric)\n* Hyperopt","4f92a3ed":"## Fixing the SalePrice Skew","9192d68e":"# Submitting","83a96133":"There might be some parts of the house for which just the existence of that part is important. For example, does only *having* a fireplace raise the house value over houses that have no fireplaces? ","5ef31e6b":"# Blending","62ce1c03":"### Numerical Features","df9f9594":"# Model Training\nLet's do some cool stuff now!<br>\nModels used:\n* XGBoost\n* LGBM\n* Gradient Boosting Regressor\n* SVR\n* Ridge\n* Lasso\n* Elastic Net","3886ab15":"## LGBM ","a325855b":"### These Features are Categorical ","4342d0b8":"## The Target Feature","429bd449":"GarageYrBuilt was a similar graph, and it felt kinda useless so I'm excluding it to save space.","353ac836":"# Loading the Dataset","bc1bc912":"## Encoding Categorical Features","a1065688":"## Feature Types","edc7cb99":"### Does Size Matter?","25b05e0b":"Let's list the features we can use","a82546ac":"#### Conclusions\n* There are outliers we need to remove.\n* There is heteroscedasticity.","f9e05273":"#### OverallQual","d3cdefa4":"#### Conclusion\n* That's 19 columns with missing values in the training set and 33 in the test set.","376958c4":"#### TotalBsmtSF","10c53dc4":"## Missing %","063dff09":"# Exploring and Preprocessing","f2e9e999":"## Metrics\n### Main Metric","4e263401":"## Fitting","e7dbb845":"#### GrLivArea","b9c0fcb7":"Let's use logarithmic transformation (log1p is log(1+x) so that x=0 poses no problems)","6b010f3f":"Also, we have `MoSold` which is the month in which the house was sold. This is obviously categorical as there isn't any reason for July to be better than June. <br>\nAlso, `MSSubClass` just describes the type of the dwelling. Categorical again. ","ef9d0d83":"For features like basement, fireplace, garage, and pool we already have a 'None' category.\nLet's see some other features.","5e0bf89d":"## Fixing Skewed Features\nI came across this from the \"How I Made Top 0.3% on a Kaggle Competition\" by Lavanya Shukla's (@lavanyashukla01) notebook. This wasn't something I'd learned beforehand, so I had to do some research. From Erik Bruin's (@erikbruin) R notebook \"House prices: Lasso, XGBoost, and a detailed EDA\" I was able to understand this topic better. As he explains it, \"As a rule of thumb, skewness should be between -1 and 1. In this range, data are considered fairly symmetrical.\" <br><br> The former notebook fixes features with skew > 0.5, and the latter uses 0.8. I'll take inspiration from some of the Python code of the former and use skew > 0.6 for no reason.","9eb97e53":"## Deleting Outliers","a5abe443":"## GradientBoostingRegressor","a13767e4":"We want to take care of missing data both in the train and the test set. So let's write a function for it. ","56fb7be4":"#### Perfect!","76fb6fb2":"### Dropping Street and Utilities ","46fbb91b":"## XGBoost","3b4f89de":"Making features like total porch area, total house quality, etc.","2cdb3ba8":"## Fixing What's Missing","48e5eab3":"### Categorical Features"}}