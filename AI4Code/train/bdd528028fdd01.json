{"cell_type":{"0f541e2b":"code","2341ef62":"code","bc3ee833":"code","bdecfb16":"code","bcf2c014":"code","49d2e350":"code","653d7599":"code","bdd8655c":"code","c0eff448":"code","e927af9a":"code","b61b5eb0":"code","ee280a8a":"code","5f0304c9":"code","b3aff026":"code","f911dcff":"code","8aebd025":"code","8e502b2e":"code","5e73b9c4":"code","ec0ab075":"code","9256c1e4":"code","78d4a252":"code","13f4fc20":"code","1f6ca72f":"code","77b97d27":"code","3ae97682":"code","8f5dbc2f":"code","90d23b9f":"code","d04d74a3":"code","584f9b39":"code","6ecc3027":"code","16cb5993":"code","e5fbddbf":"code","173c6f47":"code","e84e233a":"code","dcc64ce8":"code","f447a828":"code","342fef83":"code","62ca647d":"code","09e1d291":"code","dfe4bb89":"code","1be96645":"code","4f2918d0":"code","bef8b591":"code","bf0c6b10":"code","8dbaa3b6":"code","759c81e3":"code","f2aa1847":"code","d27ca6b5":"code","fa5c9afa":"code","28c11997":"code","70a6a47e":"code","f08b4b0b":"code","0d19c318":"code","db85c3f7":"code","9b42dc99":"code","55c30fcd":"code","1e409735":"code","82788f81":"code","3152cecc":"code","08c99e8a":"code","c074c0ef":"code","4271ffcc":"code","694c9273":"code","9cb4082f":"code","f858de49":"code","7f86285b":"code","34091c75":"code","463166d3":"code","db9aad78":"code","f74e621c":"code","adec855e":"code","ff8339de":"code","a91ebeac":"code","f3d4d966":"code","1b4aa08b":"code","edfc7b6d":"code","8a9091ae":"code","89cbf93c":"code","8b9d33c5":"code","6328e6c2":"code","ef17e2e2":"code","68506023":"code","550bc52f":"code","8c2c159f":"code","335a1172":"code","9b76b5e1":"code","7ccf77aa":"markdown","7cc6b780":"markdown","b145103a":"markdown","dd0f5a67":"markdown","83f9a385":"markdown","9dbfe024":"markdown","3a6dbea5":"markdown","47bb05af":"markdown","0b7656ca":"markdown","4812f04d":"markdown","4f6c2b71":"markdown","d18fdb3e":"markdown","63e6ef07":"markdown","add56809":"markdown","67e19851":"markdown","9c936522":"markdown","f2a6c92a":"markdown","65bc7c63":"markdown","28d0396e":"markdown","32123c3c":"markdown","7d5bf998":"markdown","ef4d53cb":"markdown","1488afcd":"markdown","cbed3d5c":"markdown","1b446615":"markdown","6edb29ef":"markdown","21f3a8f9":"markdown","175a94a6":"markdown","cf801898":"markdown","77de5564":"markdown","9ae87082":"markdown","49ce9805":"markdown","8d652b34":"markdown","e1f6a131":"markdown","e29ffdee":"markdown","a6e3cb20":"markdown","aab86255":"markdown","32971f1d":"markdown","d1f1ca8e":"markdown","a692efaf":"markdown","a8ec1f43":"markdown","b068b079":"markdown","90360ff5":"markdown","72fe66b9":"markdown","abf7df14":"markdown","e683f1cf":"markdown","6101ec79":"markdown","deeb8418":"markdown","c31d67e7":"markdown","67a7b1ea":"markdown","8829569e":"markdown","02adca82":"markdown","c4818e9a":"markdown","a86986eb":"markdown","f948f7ee":"markdown","8e4afdf6":"markdown","a5129d93":"markdown","a082d3da":"markdown","eb4e32d4":"markdown","526070c4":"markdown","0ff1a115":"markdown","ad4203af":"markdown","14cb80de":"markdown","9fc206e0":"markdown","4497e853":"markdown","0990e98a":"markdown","e88e48da":"markdown","4a44007f":"markdown","914aeb3c":"markdown","e74c50fc":"markdown","9b931bfe":"markdown","ddb3cfb5":"markdown","07724ba4":"markdown","47445d2e":"markdown"},"source":{"0f541e2b":"from typing import List, Dict, List, Tuple\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom inspect import signature\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_recall_curve, \\\nroc_auc_score, average_precision_score, precision_score, make_scorer, f1_score, roc_curve, auc\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n \nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n!pip install mglearn\nimport mglearn\n","2341ef62":"DIR = r\"..\/input\/titanic\"\ndf_train = pd.read_csv(DIR + '\/' + 'train.csv')\ndf_test = pd.read_csv(DIR + '\/' + 'test.csv')\ndf_combine = [df_train, df_test]","bc3ee833":"# identify different data type of the variables. check numerical variable and categorical variables\n\nprint(df_train.info(), '\\n')\ndf_train.head(5)","bdecfb16":"# look at the values in all variables. we can potentially know some values in the categorical variable might need to be adjusted.\n\nfor j in range(df_train.shape[1]):\n    print(df_train.columns[j])\n    print(df_train.iloc[:, j].value_counts())\n    print('\\n')","bcf2c014":"# missing values\n\ndef missing_percent(df: pd.DataFrame) -> Dict:\n    \"\"\"Calculate the percentage of the missing data in each columns\"\"\"\n    percent = {}\n    for column_name in list(df.columns):\n        percent[column_name] = sum(df[column_name].isnull()) \/ len(df[column_name])\n    return percent\n\npercent = missing_percent(df_train)\npercent \n","49d2e350":"percent = missing_percent(df_test)\npercent ","653d7599":"# descriptive analysis of the numerical variables\n\ndf_train.describe()","bdd8655c":"# plot the distribution\n\ndef plot_numeric(df, feature_list):\n    fig, axis = plt.subplots(1, len(feature_list), figsize=(16, 4))\n    for i in range(len(feature_list)):\n        df[feature_list[i]].plot.hist(ax=axis[i])\n        axis[i].set_title(feature_list[i])","c0eff448":"feature_list = ['Age', 'SibSp', 'Parch', 'Fare']\nplot_numeric(df_train, feature_list)","e927af9a":"df_train.describe(include=[np.object])","b61b5eb0":"# plot the unique counts for the categorical variables\n\ndef plot_categorical(df, categorical_variables_list: List):\n    nrows = 1\n    ncols = int(np.ceil(len(categorical_variables_list) \/ nrows))\n    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 4))\n\n    for i in range(len(categorical_variables_list)):\n        df.loc[:, categorical_variables_list[i]].value_counts().plot.barh(ax=axes[i])\n        axes[i].set_title(categorical_variables_list[i])\n\nplot_categorical(df_train, ['Survived', 'Sex', 'Pclass','Embarked', 'Survived'])\n","ee280a8a":"df_train.groupby('Survived').mean()","5f0304c9":"def plot_numerical_target(df, feature_list, target):\n    fig, axis = plt.subplots(1, len(feature_list), figsize=(10, 4))\n    for i in range(len(feature_list)):\n        sns.catplot(ax=axis[i], data=df, x=target, y=feature_list[i], kind='bar')\n        axis[i].set_title(feature_list[i])\n    \nplot_numerical_target(df_train, ['Age', 'Fare', 'SibSp', 'Parch'], 'Survived')\nplt.close(2)\nplt.close(3)\nplt.close(4)\nplt.close(5)\n","b3aff026":"g = sns.FacetGrid(data=df_train, col='Survived')\ng.map(plt.hist, 'Age')\n\nsns.catplot(data=df_train, x='Survived', y='Age', kind='box')\n","f911dcff":"# We can combine multiple features for identifying correlations using a single plot. \n# This can be done with numerical and categorical features which have numeric values.\n\ng = sns.FacetGrid(data=df_train, row='Pclass', col='Survived')\ng.map(plt.hist, 'Age')\n","8aebd025":"g = sns.FacetGrid(data=df_train, row='Sex', col='Survived')\ng.map(plt.hist, 'Age')","8e502b2e":"g = sns.FacetGrid(data=df_train, col='Survived')\ng.map(plt.hist, 'Fare')\n\nsns.catplot(data=df_train, x='Survived', y='Fare', kind='box')\n","5e73b9c4":"sns.regplot(data=df_train, x='Age', y='Fare')","ec0ab075":"g = sns.FacetGrid(data=df_train, col='Survived')\ng.map(plt.hist, 'SibSp')\n\nsns.catplot(data=df_train, x='Survived', y='SibSp', kind='box')","9256c1e4":"g = sns.FacetGrid(data=df_train, col='Survived')\ng.map(plt.hist, 'Parch')\nsns.catplot(data=df_train, x='Survived', y='Parch', kind='box')\n","78d4a252":"def cat_target(df, feature, target):\n    for i in range(len(feature)):\n        print(df.groupby(by=feature[i]).mean().sort_values(by=target, ascending=False), '\\n')  ","13f4fc20":"cat_target(df_train, ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'], 'Survived')","1f6ca72f":"df_survived = df_train[df_train['Survived'] == 1]\ndf_notsurvived = df_train[df_train['Survived'] == 0]","77b97d27":"plot_categorical(df_notsurvived, [ 'Sex', 'Pclass','Embarked'])","3ae97682":"plot_categorical(df_survived, ['Sex', 'Pclass','Embarked'])\n","8f5dbc2f":"grid = sns.FacetGrid(df_train, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","90d23b9f":"df_train_ = df_train.copy()\ndf_test_ = df_test.copy()","d04d74a3":"df_train_.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)\ndf_test_.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True) # keep consistant\n\nprint('before', df_train.shape, df_test.shape)\nprint('after', df_train_.shape, df_test_.shape)","584f9b39":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(df_train_, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","6ecc3027":"#Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.\nguess_ages = np.zeros((2,3))\nguess_ages\nsex_list = ['male', 'female']\ndf_combine_ = [df_train_, df_test_]\n\n#Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\nfor dataset in df_combine_:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == sex_list[i]) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            #print(age_guess)\n            guess_ages[i,j] = (age_guess\/0.5 + 0.5 ) * 0.5\n            #guess_ages[i,j] = int(age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == sex_list[i]) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ndf_train_.head()","16cb5993":"# before, age has missing values in the training set\ndf_train[df_train.index.isin(df_train[df_train.Age.isnull()].index)].head(5)","e5fbddbf":"# after \ndf_train_[df_train.index.isin(df_train[df_train.Age.isnull()].index)].head(5)","173c6f47":"df_test_['Fare'].fillna(df_test_.Fare.dropna().median(), inplace=True)\ndf_test_.Fare.isnull().any()","e84e233a":"freq =  df_train_.Embarked.dropna().mode()[0]\nprint(freq)\n\ndf_train_['Embarked'].fillna(freq, inplace=True)\ndf_test_['Embarked'].fillna(freq, inplace=True)\n","dcc64ce8":"# before\ndf_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f447a828":"# after\ndf_train_[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","342fef83":"sns.heatmap(df_train_.isnull())","62ca647d":"sns.heatmap(df_test_.isnull())","09e1d291":"# Let us create Age bands and determine correlations with Survived.\ndf_train_['AgeBand1'] = pd.cut(df_train_['Age'], bins=5)\n\nprint(df_train_[['AgeBand1', 'Survived']].groupby(['AgeBand1'], as_index=False).mean().sort_values(by='AgeBand1', ascending=True))\n","dfe4bb89":"df_train_['AgeBand1'] = pd.cut(df_train_['Age'], bins=5, labels=False)\ndf_train_.head()","1be96645":"# use the same bins as the trainning set\ndf_test_['AgeBand1'] = pd.cut(df_test_['Age'], bins=[-0.08, 16, 32, 48, 64, 200], labels=False) \ndf_train_.head()","4f2918d0":"bins=[-0.1, 10, 20, 40, 60, 80, 200] \ndf_train_['AgeBand2'] = pd.cut(df_train_['Age'], bins=bins)\nprint(df_train_[['AgeBand2', 'Survived']].groupby(['AgeBand2'], as_index=False).mean().sort_values(by='AgeBand2', ascending=True))\n","bef8b591":"df_train_['AgeBand2'] = pd.cut(df_train_['Age'], bins=bins, labels=False)\ndf_test_['AgeBand2'] = pd.cut(df_test_['Age'], bins=bins, labels=False) \n","bf0c6b10":"# Let us create Fare bands and determine correlations with Survived.\n\ndf_train_['FareBand1'] = pd.qcut(df_train_['Fare'], q=4)\nprint(df_train_[['FareBand1', 'Survived']].groupby(['FareBand1'], as_index=False).mean().sort_values(by='FareBand1', ascending=True))\n","8dbaa3b6":"df_train_['FareBand1'] = pd.qcut(df_train_['Fare'], q=4, labels=False)\ndf_train_.head()","759c81e3":"df_test_['FareBand1'] = pd.cut(df_test_['Fare'], bins=[-0.001, 7.91, 14.454, 31, 100000], labels=False)","f2aa1847":"bins = [-0.1, 100, 200, 1000]\ndf_train_['FareBand2'] = pd.cut(df_train_['Fare'], bins=bins)\nprint(df_train_[['FareBand2', 'Survived']].groupby(['FareBand2'], as_index=False).mean().sort_values(by='FareBand2', ascending=True))\n","d27ca6b5":"df_train_['FareBand2'] = pd.cut(df_train_['Fare'], bins=bins, labels=False)\ndf_test_['FareBand2'] = pd.cut(df_test_['Fare'], bins=bins, labels=False)","fa5c9afa":"df_train_['FamilySize'] = df_train_['SibSp'] + df_train_['Parch'] + 1\ndf_test_['FamilySize'] = df_test_['SibSp'] + df_test_['Parch'] + 1\n\ndf_train_[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","28c11997":"df_combine_ = [df_train_, df_test_]\nfor dataset in df_combine_:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf_train_[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","70a6a47e":"df_combine_ = [df_train_, df_test_]\nfor dataset in df_combine_:\n    dataset['AgeBand1*Class'] = dataset.AgeBand1 * dataset.Pclass\n\ndf_train_.loc[:, ['AgeBand1*Class', 'AgeBand1', 'Pclass']].head(10)","f08b4b0b":"df_combine_ = [df_train_, df_test_]\nfor dataset in df_combine_:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n","0d19c318":"df_train_.columns","db85c3f7":"def split_to_features_labels(df: pd.DataFrame, feature_list: List, label_list: List) -> Tuple[pd.DataFrame, pd.Series]:\n    \"\"\" Split the data into feature matrix and the labels vector\"\"\"\n    df_features = df[feature_list]\n    df_label = df[label_list]\n    return df_features, df_label","9b42dc99":"feature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']","55c30fcd":"label_list = 'Survived'\ndf_features, df_label = split_to_features_labels(df_train_, feature_list, label_list)","1e409735":"X_trainval, X_test, y_trainval, y_test = train_test_split( df_features, df_label, test_size=0.2, random_state=0)","82788f81":"param_grid = {'penalty': ['l1', 'l2'],\n              'C': [0.001, 0.1, 1, 10, 100, 1000],\n             'solver': ['liblinear']}\nscorer = make_scorer(f1_score)\n\nmodel_lg = GridSearchCV(estimator=LogisticRegression(), \n                        scoring = scorer,\n                        param_grid = param_grid,\n                        cv=5)\n\nmodel_lg.fit(X_trainval, y_trainval)\n","3152cecc":"mean_test_score = model_lg.cv_results_['mean_test_score']\nprint('mean_test_score for different paramters \\n', mean_test_score)\nprint('best scores \\n', model_lg.best_score_)    \nprint('best estimator \\n', model_lg.best_estimator_)\nprint('best parameters \\n', model_lg.best_params_)","08c99e8a":"results = pd.DataFrame(model_lg.cv_results_)\nresults.head(5)","c074c0ef":"scores = np.array(results.mean_test_score).reshape(2, 6)\nprint(scores)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='C', xticklabels=param_grid['C'],\n                      ylabel='penalty', yticklabels=param_grid['penalty'], cmap=\"viridis\")","4271ffcc":"coeff_df = pd.DataFrame(feature_list)\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(model_lg.best_estimator_.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","694c9273":"probs = model_lg.predict_proba(X_test)\npredicted = model_lg.predict(X_test)","9cb4082f":"accuracy = accuracy_score(y_test, predicted)\nprint('accuracy for the test data is: ', accuracy)","f858de49":"print(classification_report(y_test, predicted))","7f86285b":"def plot_precision_recall_curve(y_test: pd.DataFrame, probability: pd.DataFrame, predicted: pd.DataFrame ):\n    \"\"\"Plot the recall precision tradeoff\"\"\"\n   \n    precision, recall, threshold = precision_recall_curve(y_test, probability)\n    #print('precision is', precision)\n    #print('recall is', recall)\n    \n    precision_ = precision_score(y_test, predicted)\n    print('precision is ', precision_)\n    recall_ = recall_score(y_test, predicted)\n    print('recall is', recall_)\n    f1 = f1_score(y_test, predicted)\n    print('f1 score is', f1)\n    \n    average_precision = average_precision_score(y_test, probability)\n    print('average precision is', average_precision)\n    \n    # calculate precision-recall AUC (area under the curve)\n    auc_ = auc(recall, precision)\n    print('precision-recall AUC is', auc_)\n    \n    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n    step_kwargs = ({'step': 'post'}\n                    if 'step' in signature(plt.fill_between).parameters\n                    else {})\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n              average_precision))\n","34091c75":"predicted = model_lg.predict(X_test)\nplot_precision_recall_curve(y_test, probs[:,1], predicted)","463166d3":"def plot_roc_curve(y_test: pd.DataFrame, probability: pd.DataFrame):\n    plt.figure()\n    fpr, tpr, threshold = roc_curve(y_test, probability)\n    roc_auc = auc(tpr, fpr)    # Compute Area Under the Curve (AUC) using the trapezoidal rule\n    \n    # Plotting\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    print('ROC AUC(area under the curve) is', roc_auc_score(y_test, probability)) \n    print('threshold values :', threshold[0: 20])","db9aad78":"plot_roc_curve(y_test, np.ravel(probs[:,1]))","f74e621c":"#param_grid = {'n_estimators': [1, 30, 100], 'max_features': ['auto', 'log2'],  \n#              'max_depth': [2, 4, 8, 16], 'criterion': ['gini', 'entropy']\n#}\n\nparam_grid = {'n_estimators': [1, 30, 100, 500, 1000], 'max_features': ['log2'],  \n              'max_depth': [8, 16], 'criterion': ['gini']\n}\n\nscorer = make_scorer(f1_score)\n\nmodel_rf = GridSearchCV(estimator=RandomForestClassifier(), \n                        scoring = scorer,\n                        param_grid = param_grid,\n                        cv=5)\n\nmodel_rf.fit(X_trainval, y_trainval)\n","adec855e":"mean_test_score = model_rf.cv_results_['mean_test_score']\nprint('mean_test_score for different paramters \\n', mean_test_score)\nprint('best scores \\n', model_rf.best_score_)    \nprint('best estimator \\n', model_rf.best_estimator_)\nprint('best parameters \\n', model_rf.best_params_)","ff8339de":"results = pd.DataFrame(model_rf.cv_results_)\nprint(results[['param_max_depth', 'param_n_estimators', 'mean_test_score']])\nscores = np.array(results.mean_test_score).reshape(2, 5)\nprint(scores)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='n_estimators', xticklabels=param_grid['n_estimators'],\n                      ylabel='depth', yticklabels=param_grid['max_depth'], cmap=\"viridis\")","a91ebeac":"coeff_df = pd.DataFrame(feature_list)\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(model_rf.best_estimator_.feature_importances_)\ncoeff_df.sort_values(by='Correlation', ascending=False)\n","f3d4d966":"probs = model_rf.predict_proba(X_test)\npredicted = model_rf.predict(X_test)","1b4aa08b":"accuracy = accuracy_score(y_test, predicted)\nprint('accuracy for the test data is: ', accuracy)","edfc7b6d":"accuracy_score(y_test, predicted)","8a9091ae":"print(classification_report(y_test, predicted))","89cbf93c":"predicted = model_rf.predict(X_test)\nplot_precision_recall_curve(y_test, probs[:,1], predicted)","8b9d33c5":"plot_roc_curve(y_test, np.ravel(probs[:,1]))","6328e6c2":"presicion_lg, recall_lg, thresholds_lg = precision_recall_curve(y_test, model_lg.predict_proba(X_test)[:, 1])\nplt.plot(presicion_lg, recall_lg, label='lg')\n\npresicion_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\nplt.plot(presicion_rf, recall_rf, label='rf')\nplt.xlabel('precision')\nplt.ylabel('recall')\nplt.legend(loc='best')","ef17e2e2":"fpr_lg, tpr_lg, thresholds_lg = roc_curve(y_test, model_lg.predict_proba(X_test)[:, 1])\nplt.plot(fpr_lg, tpr_lg, label='lg')\n\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\nplt.plot(fpr_rf, tpr_rf, label='rf')\nplt.xlabel('FPR')\nplt.ylabel('TPR recall')\nplt.legend(loc='best')","68506023":"# add the engineering features\n\"\"\"\nfeature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Embarked', 'AgeBand1',  'FareBand1' ,\n       'FamilySize', 'IsAlone', 'AgeBand1*Class']\n\"\"\"","550bc52f":"# add another  engineering features\n\"\"\"\nfeature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Embarked',  'AgeBand2',  'FareBand2',\n       'FamilySize', 'IsAlone']\n\"\"\"","8c2c159f":"# add the engineering features and drop the original features\n\"\"\"\nfeature_list = ['Pclass', 'Sex',\n       'Embarked', 'AgeBand1',  'FareBand1' ,\n       'FamilySize', 'IsAlone', 'AgeBand1*Class']\n\"\"\"","335a1172":"X_test_2 = df_test_[feature_list]\ny_test_2 = model_rf.predict(X_test_2)","9b76b5e1":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": y_test_2\n    })\nsubmission.to_csv('result.csv', index=False)","7ccf77aa":"##### 'SibSp'","7cc6b780":"two features","b145103a":"#### (2) classification report","dd0f5a67":"#### (2) classification report","83f9a385":"## Random Forest","9dbfe024":"mean","3a6dbea5":"### (4) Summary","47bb05af":"**Summary**\n* Most of the people pay less than 100\n* there are more people pay between 200-300 in the Survived group \n* a large number of people who pay less than 50 didn't survive\n* Thus, I will include Fare into the feature\n* can consider create fare categorical varialbe as a engineering feature. <100, 100-200, > 200","0b7656ca":"##### Fare","4812f04d":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. \nThis can be done by calculating the coefficient of the features in the decision function. Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).","4f6c2b71":"###  compare logistic regression with random forest","d18fdb3e":"### (1) Overview of all features","63e6ef07":"#### (a) Numerical variable vs. target variable","add56809":"###### for 'Age'","67e19851":"### This workbook is about building a predictive model for titanic data - a classic classification problem\n\nThe structure of this workbook:\n\n1. Get the data\n2. Analysis\n3. Wrangle data\n4. Model training\n5. Try different engineering features\n6. Submission\n7. References\n\n","9c936522":"**summary**\n* F1 score for lable 1 is 0.73 in the training set, and 0.75 for the test set. The model generalize well. ","f2a6c92a":"#### (1) accuracy ","65bc7c63":"**Summary**\n\n* Age vs Survived: Yonger people tend to survive. Since it is only 2 years yonger, it may not be significant.\n* Fare vs Survived: people who pay higher fee tend to survive. It may be significant since they pay twice more.\n* Age and Fare should be included into the model for further analysis\n\n* Pclass vs Survived, Sex vs Survived: both Pclass and Sex have strong correlation with the survival. We should include them into the model\n* SibSp vs Survived, Parch vs Survived, Embarked vs Survived: I am not sure the correlation.  It may be best to derive a feature or a set of features from these individual features.","28d0396e":"####  (c) Categorical variable vs. target variable","32123c3c":"### evaluate the performance in the test data set","7d5bf998":"# 4. Model training","ef4d53cb":"Now we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https:\/\/en.wikipedia.org\/wiki\/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","1488afcd":"**Summary**\n* some facotors are favorable to survive. female, higher class, Embarked on C,Q ports\n* I will include them into the features","cbed3d5c":"# References\n* https:\/\/www.kaggle.com\/c\/titanic\/overview\/tutorials\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* Introduction to Machine Learning with Python by Sarah Guido, Andreas C. M\u00fcller, Publisher: O'Reilly Media, Inc., Release Date: October 2016, ISBN: 9781449369880\n\n","1b446615":"#### (3) precision_recall_curve","6edb29ef":"### Completing categorical features","21f3a8f9":"**Summary**\n\n(1)  feature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n* lg:  F1 score for lable 1 is 0.73 in the training set, and 0.75 for the test set.\n* rf:  F1 score for lable 1 is 0.75 in the training set, and 0.77 for the test set.\n\n\n\n(2) feature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'AgeBand1',  'FareBand1' , 'FamilySize', 'IsAlone', 'AgeBand1*Class'] \n\t   \n* lg:\tF1 score for lable 1 is 0.72 in the training set, and 0.71 for the test set. \n* Rf: \tF1 score for lable 1 is 0.75 in the training set, and 0.77 for the test set. \n\n\n(3) feature_list = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',  'AgeBand2',  'FareBand2', 'FamilySize', 'IsAlone']\n\t   \n* lg: F1 score for lable 1 is 0.75 in the training set, and 0.74 for the test set. \n* rf: F1 score for lable 1 is 0.74 in the training set, and 0.75 for the test set.\n\n(4) feature_list = ['Pclass', 'Sex','Embarked', 'AgeBand1',  'FareBand1' ,'FamilySize', 'IsAlone', 'AgeBand1*Class']\n\n* lg: F1 score for lable 1 is 0.72 in the training set, and 0.75 for the test set.\n* rf: F1 score for lable 1 is 0.72 in the training set, and 0.78 for the test set. \n\n\n* For logistic regression, if I add the engineering features (features list3) , the f1 score for the training set is the best, however the test set is not. But we don't use test set to influce on the features we choose. Test set is just for evaluate the generalization of the model. \n\n* for random forest, the original features are good enough, adding the engineering features didn't help or even decrease the perfrmance. If we drop the original features, random forest perform worse.\n\n* Overal, random forest with the original features perform better than the logistic regression model. ","175a94a6":"##### Parch","cf801898":"#####  Age","77de5564":"# 6.  Submission","9ae87082":"###  (1) Drop some columns or remove some categories","49ce9805":"We can create another feature called IsAlone.","8d652b34":"##### descriptive","e1f6a131":"###  evaluate the parameters","e29ffdee":"#### visulization","a6e3cb20":"### Completing  numerical features","aab86255":"### (5) separate features and target","32971f1d":"### (5) Analyze correlation between features and target\n\n","d1f1ca8e":"##### Fare\nfix missing value in the test set\n","a692efaf":"## Logistic regression ","a8ec1f43":"##### Family size\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","b068b079":"**Summary**\n* the features importance is different from the result from the logistic regresssion. However, Sex is still the number one factor. ","90360ff5":"**Summary**\n* very yonge children and around 80 years old people tend to survive\n* most of the people are within 20-40.\n* Large number of 15-25 year olds did not survive.\n* Thus, Age feature need to be included\n* can consider create age category engineering feature: 0-10, 10-20, 20-40, 40-60, 60-80, > 80\n* first class cabinet tends to have older people and tend to survive. class 3 is the opposite. should consider Pclass as the feature\n* among people who didn't survive, man is dominant. More men die than survive, less women die than survive. Gender should be included into the feature.","72fe66b9":"### (3) Analyze categorical variables","abf7df14":"We can also create an artificial feature combining Pclass and Age.","e683f1cf":" * Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224). There are 418 samples on the test set. So the training set is 2\/3 of the total data available. \n\n**Which features are available in the dataset?**\n\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https:\/\/www.kaggle.com\/c\/titanic\/data).\n\n\n **Which features have missing values?**\n * Age, Cabin, and Embarked in the training set, Age, Cabin, and Fare in the test set.\n\n\n **What are the numeric variables and categorical variables?**\n\n * Numerical variables: (1) continuous: 'Age', 'Fare'; (2) discrete: 'SibSp', 'Parch'. \n * Categorial variables: (1) ordinal: 'Pclass'; (2) nominal: 'PassengerId',  'Name', 'Sex',  'Survived', 'Ticket', 'Cabin', 'Embarked'.\n\n\n **What is the distribution of numerical feature values across the samples?**\n\n * Age: The range of  age is very wide. It is from infant to very old people.\n * SibSp, Parch: From the number of the sibling, parents, and children, we can tell most of the people travel alone. \n * Fare: Fare vary a lot\n\n\n **What is the distribution of categorical features?**\n\n * Survived: Survived variable is the target variable. 1 means survived. 38% of the people are survived. The data is relatively balanced.\n * Pclass: Lowest class cabinet contains most people, almost 50%.\n * PassengerId: Name of the passenger is unique. it contains the gender information. Mr. Ms. However, it may contain some type  errors.\n * Sex: There are almost twice men than women.\n * Ticket: The format of the ticket number vary a lot. It is a mix of alphanumeric. There are duplicate ticket numbers. They are wrong. \n * Cabin: cabin name may contain some errors.\n * Embarked: most of the people embarked from S.\n * Survived: less people survive\n\n\n\n**Which features are mixed data types?**\n  Numerical, alphanumeric data within same feature. These are candidates for correcting goal.\n* Ticket: a mix of numeric and alphanumeric data types. \n* Cabin: alphanumeric.\n\n\n**Which features may contain errors or typos?**\n* Name: it may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n\n\n\n**which feature I should select? preliminary inspection**\n* passenger ID, Name, and Ticket ID may not useful because they are kind of unique for each passenger, the model \nmight not learn anything. \n\n* I may also consider dropping 'Cabin' features. \n* I will verify the features selection by some analysis in the following steps.\n\n**the test set(I skip here)**\n* The variables have similar property as in the training set. Thus, I assume the model trained on the training set will havea similar performance on the test set\n\n","6101ec79":"**summary**\n* For the recall-precision curve, the Area Under the Curve from random forest is larger than the from the logistic regression. Which means with precision increases, the recall drop less in the randome forest.\n* for the ROC curve, the curve from random forest is to the upper left of the curve from logistic regression. It means with the same recall, it has less false positive rate.\n* As a result, Random forest has better performance than the logistic regression","deeb8418":"##### Age","c31d67e7":"### (6) split into train set and test set\n","67a7b1ea":"#### (4) ROC curve\n","8829569e":"### (2) Analyze numerical variables ","02adca82":"\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing.**\n\nSince Age and Fare are important, we need to fill the missing value. I also need to complete Embarked. Cabin has a lot of missing value. Since I will drop Cabin feature, don't worry about it.\n\n**Correcting.**\n * we can drop some features.\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating.**\n\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying.**\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\n1. Women (Sex=female) were more likely to have survived.\n2. Children (Age<?) were more likely to have survived. \n3. The upper-class passengers (Pclass=1) were more likely to have survived.","c4818e9a":"**Summary**\n* L2 has better performance.\n* the model perform similar with different set of regularization\n* Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n\n","a86986eb":"**Summary**\n* People who have less sibling tend to survive, there are less people have have more than 4 sibings\n* On the other hand, people who have more parent and children tend to survive.\n* SibSp and Parch should be included into the model for further analysis.\n* We can consider add SibSp and Parch together to create a engineering feature.","f948f7ee":"### (4) Converting a categorical feature","8e4afdf6":"##### try another bins","a5129d93":"# 1. Get the data","a082d3da":"#### descriptive","eb4e32d4":"##### visulization","526070c4":"# 3.  Wrangle data","0ff1a115":"### (2) missing values","ad4203af":"### evaluate the performance in the test data set","14cb80de":"#### (3) precision_recall_curve","9fc206e0":"##### try another bins","4497e853":"# 5. Try different engineering features\ntry different features, then re run the above models again","0990e98a":"**summary**\n* F1 score for lable 1 is 0.75 in the training set, and 0.77 for the test set. The model generalize well. ","e88e48da":"### (3)  Create engineering features","4a44007f":"#### (4) ROC curve\n","914aeb3c":"**Summary**\n* most of the people pay fee at the lower range\n* people who pay lower fee, they spread across different ages.\n* Thus, although people who is yonger and pay higher tend to survive, I cannot say younger people pay higher fee","e74c50fc":"##### 'Fare'","9b931bfe":"distribution ","ddb3cfb5":"# 2. Analysis","07724ba4":"### (6) Assumtions based on data analysis\n","47445d2e":"#### (1) accuracy "}}