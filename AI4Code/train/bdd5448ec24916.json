{"cell_type":{"7ea87c40":"code","54e9a21a":"code","4325235b":"code","5af236ad":"code","69106c31":"code","a616c57e":"code","d7527f23":"code","d1565bca":"code","1e21f984":"code","4a760452":"markdown","11704488":"markdown","aed2c9de":"markdown","1fb54d33":"markdown","a4c1ee28":"markdown","6b633bcc":"markdown","e7a27f96":"markdown","e8867979":"markdown","6fe41e31":"markdown","f29e895a":"markdown","e7b23a21":"markdown","c26d5363":"markdown","4549d565":"markdown","918f162b":"markdown","52a80b2a":"markdown","f3a0a91e":"markdown","0944c7d3":"markdown","a1bced14":"markdown","5d1cc7eb":"markdown","1b15021a":"markdown"},"source":{"7ea87c40":"!pip install pyspark","54e9a21a":"import pyspark\nfrom pyspark.sql.types import StructType,StructField\nfrom pyspark.sql.types import StringType, IntegerType, TimestampType\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nimport re\nimport time\nimport datetime\nfrom pyspark import SparkFiles\n\nspark = SparkSession.builder.getOrCreate()\nsc = SparkContext.getOrCreate()","4325235b":"df = spark.read.csv(\"..\/input\/*.txt\")","5af236ad":"\n#Fun\u00e7\u00e3o para coletar a coluna Host\ndef getValueHost(text):\n    result = text.split(\" \")\n    return str(result[0])\n\n#Fun\u00e7\u00e3o para coletar a coluna Timestamp\ndef getValueBetween(text):\n    try:\n        s = re.search('\\[(.*)\\]', text)\n        s = str(s.group(0)).replace('[','')\n        s = s.replace(']','')\n        subStr = datetime.datetime.strptime(s, \"%d\/%b\/%Y:%H:%M:%S %z\")\n        return subStr\n    except:\n        return None\n\n#Fun\u00e7\u00e3o para coletar a coluna Bytes\ndef getValueCodes(text,num):\n    try:\n        result = text.split(\" \")\n        return int(result[len(result) - num])\n    except:\n        return None\n\n#Fun\u00e7\u00e3o para coletar a coluna Status\ndef getValueStatus(text):\n    try:\n        result = text.split('\"')\n        result = result[len(result) - 1].split(' ')\n        try:\n            result.remove('')\n        except ValueError:\n            pass\n        try:\n            result.remove(' ')\n        except ValueError:\n            pass\n\n        return int(result[0])\n    except:\n        return None\n\n#Fun\u00e7\u00e3o para coletar a coluna Request\ndef getValueRequest(text):\n    try:\n        match = re.search('\"([^\"]*)\"', text)\n        return str(match.group(0)).replace('\"','')\n    except:\n        return \"\"\n\n#Acionando a fun\u00e7\u00e3o da coluna Host\nudf_funcHost = udf(lambda x: getValueHost(x),returnType=StringType())\ndfFinal = df.withColumn('host', udf_funcHost('_c0'))\n\n#Acionando a fun\u00e7\u00e3o da coluna Timestamp\nudf_funcTimeStamp = udf(lambda x: getValueBetween(x),returnType=TimestampType())\ndfFinal = dfFinal.withColumn('timestamp', udf_funcTimeStamp('_c0'))\n\n#Acionando a fun\u00e7\u00e3o da coluna Request\nudf_funcRequest = udf(lambda x: getValueRequest(x),returnType=StringType())\ndfFinal = dfFinal.withColumn('request', udf_funcRequest('_c0'))\n\n#Acionando a fun\u00e7\u00e3o da coluna Status\nudf_funcStatus = udf(lambda x: getValueStatus(x),returnType=IntegerType())\ndfFinal = dfFinal.withColumn('status', udf_funcStatus('_c0'))\n\n#Acionando a fun\u00e7\u00e3o da coluna bytes\nudf_funcBytes = udf(lambda x: getValueCodes(x, 1),returnType=IntegerType())\ndfFinal = dfFinal.withColumn('bytes', udf_funcBytes('_c0'))\n\n#Retirando a coluna que guardava todos os dados em uma String \ndfFinal = dfFinal.drop('_c0')","69106c31":"re1 = (dfFinal.groupBy(\"host\").count()).filter(\"count = 1\")","a616c57e":"re2 = (dfFinal.groupBy(\"status\").count()).filter(\"status = 404\")","d7527f23":"re3 = (dfFinal.groupBy(\"host\",\"status\").count()).filter(\"status = 404\").limit(5).orderBy('count', ascending=False)","d1565bca":"dfFinal.createOrReplaceTempView(\"data\")\nre4 = spark.sql(\"select DATE(timestamp),status, count(*) as count from data where status = 404 group by DATE(timestamp),status order by DATE(timestamp)\")","1e21f984":"re5 = dfFinal.groupBy().sum(\"bytes\")","4a760452":"**Quantidade de erros 404 por dia.**\n\nComo voc\u00ea consegue ver, nesse momento eu usei o SQL SparkSession em vez do SparkContext para conseguir criar uma query usando Data com mais facilidade ","11704488":"**Insights**\n    \nAqui \u00e9 onde come\u00e7o a tirar alguns dos Insights deste Dataset.\nN\u00e3o vou conseguir exibir direto no Kaggle pois estou obtendo","aed2c9de":"**N\u00famero de hosts \u00fanicos**","1fb54d33":"![Capturar3.PNG](attachment:Capturar3.PNG)","a4c1ee28":"**Tratamento dos dados**\n\nAqui comecei a identificar os padr\u00f5es de cada linha do Dataset para conseguir coletar cada coluna dela individualmente usando fun\u00e7\u00f5es do Pyspark para acelerar o processo.\n\nEsse processo poderia ser feito com o python, por\u00e9m o Spark \u00e9 usado para Big Data e est\u00e1 crescendo muito no mercado, se compararmos a performance dos dois, teremos uma diferen\u00e7a enorme, principalmente pelo fato do Dataset ser relativamente grande.","6b633bcc":"**Como voc\u00eas podem ver, os dados est\u00e3o muito bagun\u00e7ados, sem nenhuma estrutura do que \u00e9 o que.**","e7a27f96":"**CONCLUS\u00c3O****\n\nEsses foram apenas alguns Insights b\u00e1sicos que podem ser retirados deste Dataset mas o intuito era mostrar como estruturar esses dados.\nInfelizmente n\u00e3o consegui executar no Notebook do Kaggle por um erro no Java, mas se voc\u00ea instalar o Pyspark no seu computador corretamente dificilmente n\u00e3o vai funcionar.\nVou continuar atualizando e melhorando esse Notebook e aceito sugest\u00f5es para poder melhorar cada vez mais e se puder avaliar de alguma forma, vou continuar postando e melhorando meu conte\u00fado cada vez mais.\n\nObrigado.","e8867979":"![Capturar4.PNG](attachment:Capturar4.PNG)","6fe41e31":"**Importa\u00e7\u00f5es**\n\nEstou Iniciando tanto o SparkContext quanto o SparkSession pois foi muito dinamico usar os dois na hora de gerar os Insights.","f29e895a":"**Os 5 URLs que mais causaram erro 404.**","e7b23a21":"![Capturar1.PNG](attachment:Capturar1.PNG)","c26d5363":"**NASA KENNEDY SPACE CENTER - FL\u00d3RIDA**\n\nSei que n\u00e3o \u00e9 a melhor pr\u00e1tica para estrutura\u00e7\u00e3o de dados, mas o resultado foi muito bom.\n\nEsses dois conjuntos de dados possuem todas as requisi\u00e7\u00f5es HTTP para o servidor da NASA Kennedy Space Center WWW na Fl\u00f3rida para um per\u00edodo espec\u00edfico.\n\nFonte oficial do dateset: http:\/\/ita.ee.lbl.gov\/html\/contrib\/NASA-HTTP.html\n\nEles est\u00e3o dispon\u00edveis nestes links :\n\n[Jul 01 to Jul 31, ASCII format, 20.7 MB gzip compressed, 205.2 MB.](ftp:\/\/ita.ee.lbl.gov\/traces\/NASA_access_log_Jul95.gz)\n[Aug 04 to Aug 31, ASCII format, 21.8 MB gzip compressed, 167.8 MB.](ftp:\/\/ita.ee.lbl.gov\/traces\/NASA_access_log_Aug95.gz)\n\nOs dois Dataset vieram bagun\u00e7ados, deixei eles como .txt para trata-los e depois tirar pequenos Insights deles.\n\nDentro desse Notebook vou mostrar como fiz esse para estruturar esses dados que vieram bagun\u00e7ados:\n\n*  Host fazendo a requisi\u00e7\u00e3o. Um hostname quando poss\u00edvel, caso contr\u00e1rio o endere\u00e7o de internet se o nome n\u00e3o puder ser identificado.\n\n*  Timestamp no formato \"DIA\/M\u00caS\/ANO:HH:MM:SS TIMEZONE\"\n\n*  Requisi\u00e7\u00e3o (entre aspas)\n\n*  C\u00f3digo do retorno HTTP\n\n*  Total de bytes retornados\n\nPara come\u00e7ar, vamos usar o Pyspark.","4549d565":"**O total de bytes retornados.**","918f162b":"**O total de erros 404.**","52a80b2a":"![Capturar6.PNG](attachment:Capturar6.PNG)","f3a0a91e":"![Capturar2.PNG](attachment:Capturar2.PNG)","0944c7d3":"**Lendo todos os dados**\n\nColoquei os dois arquivos dentro de uma pasta para conseguir ler os dois juntos.","a1bced14":"**Esse \u00e9 o resultado final ap\u00f3s a estrutura\u00e7\u00e3o dos dados**","5d1cc7eb":"![Capturar7.PNG](attachment:Capturar7.PNG)","1b15021a":"![Capturar5.PNG](attachment:Capturar5.PNG)"}}