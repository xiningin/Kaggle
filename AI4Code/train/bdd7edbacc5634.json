{"cell_type":{"faa6c758":"code","2c50d429":"code","837387ef":"code","48cbc60a":"code","fce56357":"code","a2e5d926":"code","c9ad61c4":"code","c943abf8":"code","7459596e":"code","2a503006":"code","c996e637":"code","5049310c":"code","bc9606f8":"code","b95588ce":"code","f333cf0b":"code","8826da2d":"code","94b4d497":"code","56e33448":"code","8fef59a0":"code","ea841579":"code","c91c31e6":"code","7f0fe141":"code","fe4534dc":"code","1d9fc894":"code","cd403cfc":"code","aea79ae7":"code","55d56916":"code","9ece315c":"code","5ef96606":"code","93077216":"code","c6884f4e":"code","64588212":"code","78903f38":"code","b23ba3ba":"code","6f3c579d":"code","f7d624cf":"code","986571b9":"code","a23fdab5":"code","03ccb303":"code","bb6bfaa9":"code","5f7d3e46":"code","c1eec703":"code","cf23895c":"code","25deb9e1":"code","834047f4":"code","10a57e82":"code","11a75067":"code","625e8d26":"code","47d517a2":"code","01d8262a":"code","cd21e703":"code","37cefa19":"code","c5b6a566":"code","30ecee41":"code","587e5653":"code","4f6abcd0":"code","e849ea4f":"code","c847d4d5":"code","a37cf904":"code","e6b476d8":"code","47978bbb":"code","aa39647e":"code","a7b58349":"code","afe619bb":"code","a59f0c66":"code","e1a30549":"code","e6f7ba0f":"code","1f9aceb6":"code","e973b7c9":"code","2b894753":"code","b572ca43":"code","596e4b9d":"code","904a04c8":"code","5eb56526":"code","72188b7a":"code","fb4411ca":"markdown","ea0f5e62":"markdown","d2ad8a2f":"markdown","7f8fcbd1":"markdown","5d2a1bf2":"markdown","296fbcfa":"markdown","bf75a736":"markdown","574789de":"markdown","1ec36e8e":"markdown","852669af":"markdown","e74e89f1":"markdown","88902e4e":"markdown","b5ec1130":"markdown","1be32ec8":"markdown","20962f20":"markdown","1910adf9":"markdown","1a649d2b":"markdown"},"source":{"faa6c758":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c50d429":"#load the data:\ndf = pd.read_excel('..\/input\/concrete-dataset\/Concrete_Data.xls')\ndf.head()","837387ef":"df.shape","48cbc60a":"df.info()","fce56357":"df.isnull().sum()","a2e5d926":"df.columns","c9ad61c4":"df=df.rename(columns={'Cement (component 1)(kg in a m^3 mixture)':'Cement','Blast Furnace Slag (component 2)(kg in a m^3 mixture)':'Blast_Slag','Concrete compressive strength(MPa, megapascals) ':'Strength'})","c943abf8":"cor=df.corr()","7459596e":"plt.figure(figsize=(7,6))\nsns.heatmap(cor,annot=True,cmap='seismic')","2a503006":"cor_target=abs(cor['Strength'])","c996e637":"Main_features=cor_target[cor_target>0.1]\nMain_features","5049310c":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nscaled=pd.DataFrame(sc.fit_transform(df),columns=df.columns)","bc9606f8":"scaled.describe()","b95588ce":"X = df.drop('Strength', axis=1)\ny= df['Strength']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')","f333cf0b":"from sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8826da2d":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","94b4d497":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nX_constant = sm.add_constant(X)\nlin_reg = sm.OLS(y,X_constant).fit()\nlin_reg.summary()","56e33448":"df1=df.copy()","8fef59a0":"df1.skew()","ea841579":"l=[]\nfor i in df1.columns:\n    if ((df1[i].skew()<0.1) or (df1[i].skew()>0.2) and (i!='Strength')):\n        l.append(i)\nl","c91c31e6":"import scipy.stats as st\nfor i in df1.columns:\n    if i in l:\n        df1[i]=list(st.boxcox(df1[i]+1)[0])\ndf1.skew()","7f0fe141":"df1['Superplasticizer (component 5)(kg in a m^3 mixture)'].transform(lambda x:x**4)","fe4534dc":"df1.skew()","1d9fc894":"df2=df.copy()","cd403cfc":"df2=df2.transform(lambda x:x**2)","aea79ae7":"X =df2.drop('Strength',axis=1)\ny =df2.Strength\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","55d56916":"df3=df.copy()","9ece315c":"df3=df3.transform(lambda x:x**0.5)","5ef96606":"X =df3.drop('Strength',axis=1)\ny =df3.Strength\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","93077216":"df4=df.copy()","c6884f4e":"df4=df3.transform(lambda x:np.log1p(x))","64588212":"X =df4.drop('Strength',axis=1)\ny =df4.Strength\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","78903f38":"df5=df.copy()","b23ba3ba":"df5=df.transform(lambda x:np.log1p(x))","6f3c579d":"X =df5.drop('Strength',axis=1)\ny =df5.Strength\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","f7d624cf":"import statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso","986571b9":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues","a23fdab5":"cols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","03ccb303":"model = LinearRegression()","bb6bfaa9":"#Initializing RFE model\nrfe = RFE(model,4)","5f7d3e46":"#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","c1eec703":"X.columns","cf23895c":"#no of features\nnof_list=np.arange(1,9)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","25deb9e1":"cols = list(X.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 8)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","834047f4":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","10a57e82":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","11a75067":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","625e8d26":"## Building of simple OLS model.\nX = df5.drop('Strength',1)\ny = df5.Strength\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","47d517a2":"### calculating the vif values as multicollinearity exists (as stated by warning 2)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n[variance_inflation_factor(X.values, j) for j in range(1, X.shape[1])]","01d8262a":"# removing collinear variables\n# function definition\n\ndef calculate_vif(x):\n    thresh = 5.0\n    output = pd.DataFrame()\n    k = x.shape[1]\n    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    for i in range(1,k):\n        print(\"Iteration no.\")\n        print(i)\n        print(vif)\n        a = np.argmax(vif)\n        print(\"Max VIF is for variable no.:\")\n        print(a)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = x.drop(x.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)","cd21e703":"## passing X to the function so that the multicollinearity gets removed.\ntrain_out = calculate_vif(X)","37cefa19":"## includes only the relevant features\ntrain_out.head()","c5b6a566":"lr = LinearRegression()\nlr.fit(X_train, y_train)","30ecee41":"# higher the alpha value, more restriction on the coefficients; \n# low alpha > more generalization, coefficients are barely\nrr = Ridge(alpha=0.01) \n# restricted and in this case linear and ridge regression resembles\nrr.fit(X_train, y_train)","587e5653":"rr100 = Ridge(alpha=100) #  comparison with alpha value\nrr100.fit(X_train, y_train)","4f6abcd0":"train_score=lr.score(X_train, y_train)\ntest_score=lr.score(X_test, y_test)","e849ea4f":"Ridge_train_score = rr.score(X_train,y_train)\nRidge_test_score = rr.score(X_test, y_test)","c847d4d5":"Ridge_train_score100 = rr100.score(X_train,y_train)\nRidge_test_score100 = rr100.score(X_test, y_test)","a37cf904":"print(\"linear regression train score:\", train_score)\nprint(\"linear regression test score:\", test_score)\nprint(\"ridge regression train score low alpha:\", Ridge_train_score)\nprint(\"ridge regression test score low alpha:\", Ridge_test_score)\nprint(\"ridge regression train score high alpha:\", Ridge_train_score100)\nprint(\"ridge regression test score high alpha:\", Ridge_test_score100)","e6b476d8":"from sklearn.linear_model import Lasso","47978bbb":"lasso = Lasso()\nlasso.fit(X_train,y_train)\ntrain_score=lasso.score(X_train,y_train)\ntest_score=lasso.score(X_test,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)","aa39647e":"print(\"training score:\"), train_score \nprint(\"test score: \"), test_score\nprint(\"number of features used: \"), coeff_used","a7b58349":"lasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train,y_train)","afe619bb":"train_score001=lasso001.score(X_train,y_train)\ntest_score001=lasso001.score(X_test,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)","a59f0c66":"print(\"training score for alpha=0.01:\"), train_score001 \nprint(\"test score for alpha =0.01: \"), test_score001\nprint(\"number of features used: for alpha =0.01:\"), coeff_used001","e1a30549":"lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train,y_train)","e6f7ba0f":"train_score00001=lasso00001.score(X_train,y_train)\ntest_score00001=lasso00001.score(X_test,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)","1f9aceb6":"print(\"training score for alpha=0.0001:\"), train_score00001 \nprint(\"test score for alpha =0.0001: \"), test_score00001\nprint(\"number of features used: for alpha =0.0001:\"), coeff_used00001","e973b7c9":"lr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_train_score=lr.score(X_train,y_train)\nlr_test_score=lr.score(X_test,y_test)","2b894753":"print(\"LR training score:\"), lr_train_score \nprint(\"LR test score: \"), lr_test_score","b572ca43":"# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\nfrom sklearn.metrics import r2_score\n\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')","596e4b9d":"cv_model.fit(X_train, y_train)","904a04c8":"print('Optimal alpha: %.8f'%cv_model.alpha_)\nprint('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\nprint('Number of iterations %d'%cv_model.n_iter_)","5eb56526":"# train model with best parameters from CV\nmodel = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nmodel.fit(X_train, y_train)","72188b7a":"print(r2_score(y_test, model.predict(X_test))) # test data performance","fb4411ca":"### Standard Scaling","ea0f5e62":"### Backward Elimination","d2ad8a2f":"Build a regression model to predict the compressive strength of concrete based on the the given features in the dataset.","7f8fcbd1":"### Checking correlation with target variable:","5d2a1bf2":"### Importing necessary libraries:","296fbcfa":"### Lasso Feature selection","bf75a736":"## Feature Selection","574789de":"### Raw linear regression model","1ec36e8e":"### RFE","852669af":"### Applying boxcox transformation to reduce the skewness from the dataset:","e74e89f1":"### Checking skewness of the dataset:","88902e4e":"### VIF","b5ec1130":"### Problem Statement:","1be32ec8":"### Elastic Net","20962f20":"### Checking null values:","1910adf9":"### Lasso:","1a649d2b":"## Regularization\n### Ridge"}}