{"cell_type":{"c3afd542":"code","7b56b7d6":"code","9a7f38aa":"code","f55fd87b":"code","cc38a607":"code","2b0dd3b5":"code","a2769a43":"code","7eed4992":"code","6ef537ec":"code","6ab9a20c":"code","7c5fad75":"code","7611a9b9":"code","3de09209":"code","76779dc6":"code","4c7756b2":"code","a590ef4b":"code","8c9c2515":"code","ea400c28":"code","c720ac84":"code","61853e17":"code","b77c970f":"code","170833cb":"code","d7e8364b":"code","a78c1340":"code","c34bbf29":"code","b37cb135":"code","089abad6":"code","c9132f78":"code","dd484d4c":"code","8515bd44":"code","a8987882":"code","f8970657":"code","044b082a":"code","a3ecbb3c":"code","3eb71886":"code","62f2dc29":"code","fc792ac4":"code","848435c3":"code","ea3e6ba5":"code","f447ccbe":"code","0d2992e7":"code","eb34ccfb":"code","4e97e77e":"code","bbf38115":"code","753759e6":"code","3cd7e2eb":"code","6921b840":"code","8261305f":"code","389d99ce":"code","c1b9448e":"code","6697e896":"code","519be5cb":"code","6ad38409":"code","2c3e9051":"code","6bea4fa3":"code","f09a3998":"code","6abbd202":"code","95c7706e":"code","689a600b":"code","0e878d9b":"code","e4254579":"code","c097a5aa":"code","a7b44776":"code","12ec491f":"code","770c8988":"code","c5e4d444":"code","8d26996f":"code","4334c1d6":"code","1ca57c28":"code","370553b8":"code","2dc45ce3":"code","8685ae0a":"code","cff78bd2":"code","ec198297":"code","dc0b66ea":"code","6fde9a06":"markdown","d9617e93":"markdown","806a01de":"markdown","5eba8ae3":"markdown","e6e0feaa":"markdown","57efc6a3":"markdown","56793c42":"markdown","67287578":"markdown","cbf3a622":"markdown","a511ffde":"markdown","f0fa6355":"markdown","91f58ffc":"markdown","96d60337":"markdown","2dfe040d":"markdown","03bd8ff7":"markdown","64159374":"markdown","316e037c":"markdown","8d6f9fb7":"markdown","e262bceb":"markdown","d2db4849":"markdown","7caf17a2":"markdown","9696c749":"markdown"},"source":{"c3afd542":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import pearsonr\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nimport keras\nfrom keras.layers.core import Dense, Dropout\nfrom keras.layers import Input , LSTM, Embedding\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.regularizers import l2\n\nfrom os.path import join\nfrom os import listdir\n","7b56b7d6":"SEED = 1234\nfrom numpy.random import seed\nseed(SEED)\nfrom tensorflow import set_random_seed\nset_random_seed(SEED)","9a7f38aa":"print(listdir('..\/input'))","f55fd87b":"# dataset = '\/content\/gdrive\/My Drive\/Colab Notebooks\/DataSet-Jaipur'\n\n# Location of Dataset\ndataset = '..\/input'\nfilename = 'JaipurRawData3.csv'\nfilename = join(dataset, filename)","cc38a607":"df = pd.read_csv(filename, index_col='date')\ndf.head()","2b0dd3b5":"print('Shape of Dataset: {}'.format(df.shape))\nprint('Shape of Each Row: {}'.format(df.iloc[0].shape))\nprint(df.dtypes)","a2769a43":"plt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.savefig('correlationheatmap.png')\nplt.show()\n","7eed4992":"def min_max_to_mean(df, input_columns, output_columnname):\n    df[output_columnname] = (df[input_columns[0]] + df[input_columns[1]])\/ 2\n    df.drop(input_columns, axis=1, inplace=True)\n    return df","6ef537ec":"df = min_max_to_mean(df, ['maxhumidity', 'minhumidity'], 'meanhumidity')\ndf.drop(['maxtempm', 'mintempm', 'maxdewptm', 'mindewptm', 'maxpressurem', 'minpressurem'], axis=1, inplace=True)","6ab9a20c":"plt.figure(figsize=(10,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.savefig('correlationheatmap.png')\nplt.show()\n","7c5fad75":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number])\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] \n    columnNames = list(df)\n    if len(columnNames) > 10: \n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.savefig('scatterplot.png')\n\n    plt.show()\n\n    \nplotScatterMatrix(df, 10, 10)\n","7611a9b9":"def showplots(x):\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, \n                                        gridspec_kw={\"height_ratios\": (.15, .85)})\n\n    sns.boxplot(x, ax=ax_box)\n    sns.distplot(x, ax=ax_hist)\n    ax_box.set(yticks=[])\n    sns.despine(ax=ax_hist)\n    sns.despine(ax=ax_box, left=True)","3de09209":"showplots(df['meanhumidity'])","76779dc6":"showplots(df['meandewptm'])","4c7756b2":"scaler = MinMaxScaler(feature_range=(0, 1))\ndf_scaled = scaler.fit_transform(df)","a590ef4b":"df = pd.DataFrame(df_scaled, columns=df.columns)","8c9c2515":"def pad_nth_day_feature(df, feature, N):\n    rows = df.shape[0]\n    nth_prior_meassurements = [None]*N + [df[feature][i-N] for i in range(N, rows)]\n    col_name = \"{}_{}\".format(feature, N)\n    df[col_name] = nth_prior_meassurements","ea400c28":"df.columns","c720ac84":"for column in df.columns:\n#     if column != 'precipm':\n    for n in range(1, 3):\n        pad_nth_day_feature(df, column, n)","61853e17":"df.head()","b77c970f":"# Changes in Shape\nprint('Shape of Dataset: {}'.format(df.shape))\nprint('Shape of Each Row: {}'.format(df.iloc[0].shape))","170833cb":"df.info()","d7e8364b":"# Check if there is only one value in the column remove that feature\ndef check_uniqueness(dataframe):\n    for column in dataframe.columns:\n        if len(pd.Series.unique(dataframe[column])) == 1:\n            dataframe.drop(column, inplace=True, axis=1)\n\n            \n    return dataframe\n\ndf = check_uniqueness(df)","a78c1340":"# Drop Na Columns\ndf.dropna(inplace=True)","c34bbf29":"df.describe().T","b37cb135":"y_data = df['precipm']\nx_data = df.drop(['precipm'], axis=1)","089abad6":"print('Shape of X: {}'.format(x_data.shape))\nprint('Shape of Y: {}'.format(y_data.shape))","c9132f78":"# Split into Training and Test Set\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=SEED)","dd484d4c":"# Change them all to numpy array for faster computation\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","8515bd44":"# Print Final Shapes of Sets\nprint('Training Set : X -> {}, Y -> {}'.format(x_train.shape, y_train.shape))\nprint('Testing Set: X -> {}, Y -> {}'.format(x_test.shape, y_test.shape))","a8987882":"from sklearn.ensemble import RandomForestRegressor","f8970657":"rfg = RandomForestRegressor(random_state=SEED)\nrfg.fit(x_train, y_train)\n","044b082a":"y_test_pred = rfg.predict(x_test)","a3ecbb3c":"# Root Mean Square Error\nrmf_rmse = np.round(np.sqrt(mean_squared_error(y_test,y_test_pred)), 5)\nprint('Root Mean Square Error: {}'.format(rmf_rmse))","3eb71886":"lin_reg = LinearRegression()\nlin_reg.fit(x_train, y_train)","62f2dc29":"y_test_predicted = lin_reg.predict(x_test)","fc792ac4":"# Root Mean Square Error\nlin_rmse = np.round(np.sqrt(mean_squared_error(y_test,y_test_predicted)), 5)\nprint('Root Mean Square Error: {}'.format(lin_rmse))\n","848435c3":"from sklearn.preprocessing import PolynomialFeatures","ea3e6ba5":"polynomial_history = []","f447ccbe":"degree_array = [2,4,5]","0d2992e7":"for degree in degree_array:\n\n    polynomial_features= PolynomialFeatures(degree=degree)\n    x_train_poly = polynomial_features.fit_transform(x_train)\n    x_test_poly = polynomial_features.fit_transform(x_test)\n    lin_reg = LinearRegression()\n    lin_reg.fit(x_train_poly, y_train)\n    y_test_poly_predicted = lin_reg.predict(x_test_poly)\n    rmse = np.round(np.sqrt(mean_squared_error(y_test, y_test_poly_predicted)), 5)\n    print('Root Mean Square Error: {}'.format(rmse))\n    \n    polynomial_history.append((degree, rmse))\n    ","eb34ccfb":"def plot_RMSE(x_axis, y_axis, figsize=(12,8)):\n    plt.figure(figsize=figsize)\n    plt.title('Linear Regression and Polynomial Regression with their RMSE Value')\n    plt.ylabel('Root Mean Square Error')\n    bar_heights = plt.bar(x_axis, y_axis)\n    for rect in bar_heights:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()\/2.0, height, '{}'.format(height), ha='center', va='bottom')\n    plt.show()\n\n\n\n\n","4e97e77e":"rmse_x = ['Random Forest', 'Linear Regression'] + ['Polynomial of Degree {}'.format(x[0]) for x in polynomial_history]\nrmse_y = [rmf_rmse, lin_rmse] + [x[1] for x in polynomial_history]\n\nplot_RMSE(rmse_x, rmse_y)","bbf38115":"lin_reg_fs = LinearRegression()\nrfe = RFE(lin_reg_fs, 10)\nfit = rfe.fit(x_data, y_data)","753759e6":"print(\"Num Features: {}\".format(fit.n_features_))\nprint(\"Selected Features: {}\".format(fit.support_))\nprint(\"Feature Ranking: {}\".format(fit.ranking_))","3cd7e2eb":"x_feature_selected_data = rfe.transform(x_data)\nx_feature_selected_data.shape","6921b840":"x_train_fs, x_test_fs , y_train_fs, y_test_fs = train_test_split(x_feature_selected_data, y_data, test_size=0.2, random_state=SEED)\nprint(x_train_fs.shape, y_train_fs.shape, x_test_fs.shape, y_test_fs.shape)","8261305f":"lin_reg_fs.fit(x_train_fs, y_train_fs)","389d99ce":"y_predicted = lin_reg_fs.predict(x_test_fs)","c1b9448e":"lin_rmse_fs = np.round(np.sqrt(mean_squared_error(y_test_fs,y_predicted)), 5)\nprint('Root Mean Square Error: {}'.format(lin_rmse_fs))\n","6697e896":"rmse_x = rmse_x + ['Feature Selected Regression']\nrmse_y = rmse_y + [lin_rmse_fs]\n\nplot_RMSE(rmse_x, rmse_y, (18,8))","519be5cb":"lin_reg_requ = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])","6ad38409":"lin_reg_requ.fit(x_train_fs, y_train_fs)","2c3e9051":"y_pred_regu = lin_reg_requ.predict(x_test_fs)","6bea4fa3":"lin_rmse_regu = np.round(np.sqrt(mean_squared_error(y_test_fs,y_pred_regu)), 5)\nprint('Root Mean Square Error: {}'.format(lin_rmse_regu))","f09a3998":"rmse_x = rmse_x + ['Ridge Regularization']\nrmse_y = rmse_y + [lin_rmse_regu]","6abbd202":"plot_RMSE(rmse_x, rmse_y, (20,8))","95c7706e":"model = Sequential()\n\nmodel.add(Dense(35, activation='relu', input_shape=x_train[0].shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()","689a600b":"history = model.fit(x_train, y_train, epochs=200, verbose=0)","0e878d9b":"y_predicted_nn = model.predict(x_test)","e4254579":"nn_rmse = np.round(np.sqrt(mean_squared_error(y_test,y_predicted_nn)), 5)\nprint('Root Mean Square Error: {}'.format(nn_rmse))","c097a5aa":"rmse_x = rmse_x + ['Deep Neural Network']\nrmse_y = rmse_y + [nn_rmse]","a7b44776":"plot_RMSE(rmse_x, rmse_y, (22,8))","12ec491f":"model_reg = Sequential()\n\nmodel_reg.add(Dense(35, activation='relu', input_shape=x_train[0].shape, kernel_regularizer=l2(0.001)))\nmodel_reg.add(Dropout(0.3))\nmodel_reg.add(Dense(100, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_reg.add(Dropout(0.2))\nmodel_reg.add(Dense(200, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_reg.add(Dropout(0.2))\nmodel_reg.add(Dense(100, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_reg.add(Dropout(0.2))\nmodel_reg.add(Dense(50, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_reg.add(Dropout(0.2))\nmodel_reg.add(Dense(1))\n\nmodel_reg.compile(loss='mean_squared_error', optimizer='adam')\nmodel_reg.summary()","770c8988":"history_reg = model_reg.fit(x_train, y_train, epochs=200, verbose=0)","c5e4d444":"y_predicted_nn_reg = model_reg.predict(x_test)\nnn_rmse_reg = np.round(np.sqrt(mean_squared_error(y_test,y_predicted_nn_reg)), 5)\nprint('Root Mean Square Error: {}'.format(nn_rmse_reg))","8d26996f":"rmse_x = rmse_x + ['DNN Regularized']\nrmse_y = rmse_y + [nn_rmse_reg]","4334c1d6":"plot_RMSE(rmse_x, rmse_y, (24,12))","1ca57c28":"model_ts = Sequential()\n\nmodel_ts.add(Embedding(541, 100, input_length=len(x_train[0])))\nmodel_ts.add(Dropout(0.3))\nmodel_ts.add(LSTM(100, activation='relu', kernel_regularizer=l2(0.001), return_sequences=True))\nmodel_ts.add(Dropout(0.2))\nmodel_ts.add(LSTM(200, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_ts.add(Dropout(0.2))\nmodel_ts.add(Dense(100, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_ts.add(Dropout(0.2))\nmodel_ts.add(Dense(50, activation='relu', kernel_regularizer=l2(0.001)))\nmodel_ts.add(Dropout(0.2))\nmodel_ts.add(Dense(1))\n\nmodel_ts.compile(loss='mean_squared_error', optimizer='adam')\nmodel_ts.summary()","370553b8":"history_ts = model_ts.fit(x_train, y_train, epochs=200, verbose=0)","2dc45ce3":"y_predicted_ts = model_ts.predict(x_test)\nnn_rmse_ts = np.round(np.sqrt(mean_squared_error(y_test,y_predicted_ts)), 5)\nprint('Root Mean Square Error: {}'.format(nn_rmse_ts))","8685ae0a":"rmse_x = rmse_x + ['RNN Regularized']\nrmse_y = rmse_y + [nn_rmse_ts]","cff78bd2":"plot_RMSE(rmse_x, rmse_y, (26,12))","ec198297":"result_frame = pd.DataFrame({'Model': rmse_x, 'RMSE' : rmse_y}, columns=['Model', 'RMSE']).sort_values('RMSE').reset_index(drop=True)\nresult_frame.index = np.arange(1, len(result_frame) + 1)\nresult_frame.index.names = ['Rank']","dc0b66ea":"result_frame","6fde9a06":"#### Therefore, In case of Regression the Minimum Value of Root Mean Square Error is seen in ***Linear Regression***","d9617e93":"##### Pad Historic data for past 2 days with each row","806a01de":"##### We see that Maximum Correlation is from Humidity and Dew Point, Looking at their distribution","5eba8ae3":"### Feature Engineering","e6e0feaa":"#### Now we have Training Set and Testing Set","57efc6a3":"# Problem :\n## Jaipur Rainfall Data\n#### The problem is that Jaipur is one of the regions in India that has very limited amount of rainfall throught the year, So we will try to implement data analysis to predict what amount of rainfall will be recieved\nSome methods like padding dates were taken from other kernels. \nStill you will see some new methods and analysis and the reason behind them in this kernels.","56793c42":"##### 1. Applying Linear Regression","67287578":"## Conclusion : \nFor this dataset and This hyperparameters Models can be ranked liked this","cbf3a622":"# Lets Analysis The response of Neural Netoworks in This Scenario","a511ffde":"## Feature Selection Reduced the Mean Square Error thus with Feature Selection we got a better fit","f0fa6355":"##### Applying RandomForestRegressor","91f58ffc":"For Precipitation Prediction","96d60337":"#### Plotting Linear Regression with Polynomial Regression RMSE","2dfe040d":"Applying Ridge Regularization in this neural network and retraining it with output we get","03bd8ff7":"#### Split Train and Test Data\n","64159374":"#### Applying Ridge Regularization in Linear Regression","316e037c":"### Preparing Data","8d6f9fb7":"##### 2. Fitting Polynomial Regression","e262bceb":"##### Doing Some Data Cleaning Operations","d2db4849":"##### Thus There were no outliners as such, data comes from a distribution\n\n### Normalizing the Data we get","7caf17a2":"##### 2. Recursive Feature Selection on Linear Regression\nWe will use Wrapper Method of Recursive Feature Selection","9696c749":"Now Splitting the data to train and test set"}}