{"cell_type":{"f2673548":"code","dfdb1d1a":"code","0485323e":"code","0412fe00":"code","4356e904":"code","d87dfecd":"code","5a8fe458":"code","f5c7d0c1":"code","456a33a7":"code","a5d93da9":"code","70b85b2b":"code","60c4b76b":"code","78ec2454":"code","42396284":"code","9f127b74":"code","3ef1d619":"code","ec8d4023":"code","95cfe8df":"code","8050fdf3":"code","47bd2f67":"code","baf1d940":"code","b7534cc3":"code","fa6682a1":"code","e0b74850":"code","1d66c172":"code","d47c59ad":"code","d208e712":"code","341e5b6e":"code","fef238e8":"code","82b7c415":"code","de12eed8":"code","2216587a":"code","5daa8a79":"code","c2f37124":"code","e7af4f96":"code","02523f2d":"code","0f8e3716":"code","8eb12e0e":"code","a810769b":"code","6f40340f":"code","7fe2945a":"code","f52c9e39":"code","f48eddf9":"code","cfd78bb9":"code","c32a8efc":"code","08310d59":"code","4d00e8c2":"code","a0ee0a71":"code","74f66f9e":"code","417686a5":"code","710f0932":"code","1c56eb98":"code","9357bacc":"code","8ade4712":"code","42e8a472":"code","e6083848":"code","b4e7dc90":"code","33813e1c":"code","e26d3d57":"code","982fd3b2":"code","c141ff73":"code","67500a72":"code","3d769f53":"code","c1c51304":"code","e595fc3f":"code","66b5ae5f":"code","15fe53a2":"code","e1cb5cc5":"code","87e90f9a":"code","0bd10d07":"code","89db7fb0":"code","e78c6508":"code","50d9d2a3":"code","0f908a2a":"code","5a1dfda8":"code","d414f6fa":"code","584730aa":"code","d53554fb":"code","db713684":"code","afb80549":"code","bd7521bd":"code","970f7c5c":"code","853e2f0f":"code","a2466a93":"code","e7330699":"code","cb0652f1":"code","4e3a4e12":"code","7eeba0a5":"code","2eeac1f0":"code","f66b2ffb":"code","75c899b8":"code","71628119":"code","3d835c6f":"code","13f39402":"code","48268567":"code","57830c40":"code","c0e340c7":"code","10d2c013":"code","7c377a96":"code","1deb3d90":"code","dc6cc5e2":"code","50f9d796":"code","1d48c649":"code","4acce90b":"code","48cd29b0":"code","d0a6780f":"code","f05934b2":"code","9cbfd2c1":"code","0e3831be":"code","b49eeec8":"code","ed0f43ea":"code","d16d1beb":"code","fb4aac3e":"markdown","370e8955":"markdown","740ff728":"markdown","72d1a5f2":"markdown","1f5438be":"markdown","3faa96a8":"markdown","267191d7":"markdown","eee6274f":"markdown","f2d4371e":"markdown","16b8bd58":"markdown","eae36410":"markdown","f045439e":"markdown","16722033":"markdown","8809dd92":"markdown","131003ff":"markdown","3a8f2fd5":"markdown","0315798d":"markdown","f3367371":"markdown","7d95d361":"markdown","5b15ef5d":"markdown","88a4be1f":"markdown","7073d89f":"markdown","86e368d2":"markdown","653a592b":"markdown","2c520cd4":"markdown","d821d4c1":"markdown","0059597e":"markdown","e0c01d80":"markdown","b1ae70b4":"markdown","b2d286cd":"markdown","70014ab4":"markdown","09b6ccda":"markdown","a16ea1f7":"markdown","b3ec1e7d":"markdown","4423beba":"markdown","15a1d3a2":"markdown","75f4bb07":"markdown","4a94e069":"markdown","88339664":"markdown","d318ef1c":"markdown","a5c8eeb4":"markdown","676ef9ec":"markdown","9196a663":"markdown","c18bc51d":"markdown","e424bf6e":"markdown","49c24ac4":"markdown","f143e151":"markdown","930f26ed":"markdown","9e587c92":"markdown"},"source":{"f2673548":"!pip install -U scikit-learn","dfdb1d1a":"import sklearn\nsklearn.__version__","0485323e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","0412fe00":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","4356e904":"df.head()","d87dfecd":"df = df.drop(['id'], axis=1)","5a8fe458":"len(df)","f5c7d0c1":"df.info()","456a33a7":"fig = px.histogram(df, x='stroke', color='stroke')\nfig.update_layout(bargap=0.2)\nfig.update_layout(showlegend=False)\nfig.show()","a5d93da9":"df['stroke'].value_counts(normalize=True)","70b85b2b":"df.describe()","60c4b76b":"df.columns","78ec2454":"fig = px.histogram(df, x='gender', color='gender')\nfig.update_layout(bargap=0.2)\nfig.update_layout(showlegend=False)\nfig.show();","42396284":"df['gender'].value_counts()","9f127b74":"df['gender'].value_counts(normalize=True)","3ef1d619":"df[df['gender']=='Other']","ec8d4023":"df = df.drop(df[df['gender'] == 'Other'].index)","95cfe8df":"df[df['gender']=='Other']","8050fdf3":"fig = px.histogram(df, x='gender', color='gender')\nfig.update_layout(bargap=0.2)\nfig.update_layout(showlegend=False)\nfig.show();","47bd2f67":"df['gender'].value_counts(normalize=True)","baf1d940":"fig = px.histogram(df, x='age', color='gender')\nfig.show()","b7534cc3":"mean_age_stroke = df[df['stroke'] == 1]['age'].mean()\nmean_age_no_stroke = df[df['stroke'] == 0]['age'].mean()\n\nfig = px.histogram(df, x=\"age\", color=\"stroke\")\n\n# Adds vertical line for mean value of patients that had a stroke\nfig.add_vline(x=mean_age_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"darkblue\",\n              annotation_text=\"Mean age: \" + str(round(mean_age_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"darkblue\")\n\n# Adds vertical line for mean value of patients that did not have a stroke\nfig.add_vline(x=mean_age_no_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"crimson\",\n              annotation_text=\"Mean age: \" + str(round(mean_age_no_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"crimson\")\n\nfig.show()","fa6682a1":"fig = px.violin(df,\n                y='age',\n                x='stroke',\n                color='stroke',\n                points='all',\n                box=True)\nfig.show()","e0b74850":"fig = px.violin(df,\n                y='age',\n                x='stroke',\n                color='gender')\nfig.show()","1d66c172":"df[(df['stroke'] == 1) & (df['age'] < 30)]","d47c59ad":"df = df.drop(df[(df['stroke'] == 1) & (df['age'] == 1.32)].index)","d208e712":"df[(df['stroke'] == 1) & (df['age'] < 30)]","341e5b6e":"fig = px.histogram(df, x='hypertension', color='hypertension')\nfig.update_layout(bargap=0.2)\nfig.update_layout(showlegend=False)\nfig.show();","fef238e8":"fig = px.histogram(df, x='hypertension', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","82b7c415":"fig = px.histogram(df, x='heart_disease', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","de12eed8":"fig = px.histogram(df, x='ever_married', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","2216587a":"fig = px.histogram(df, x='work_type', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","5daa8a79":"fig = px.histogram(df, x='Residence_type', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","c2f37124":"fig = px.violin(df,\n                y='avg_glucose_level',\n                x='stroke',\n                color='stroke',\n                points='all',\n                box=True)\nfig.show()","e7af4f96":"mean_gluc_stroke = df[df['stroke'] == 1]['avg_glucose_level'].mean()\nmean_gluc_no_stroke = df[df['stroke'] == 0]['avg_glucose_level'].mean()\n\nfig = px.histogram(df, x=\"avg_glucose_level\", color=\"stroke\")\n\n# Adds vertical line for mean value of patients that had a stroke\nfig.add_vline(x=mean_gluc_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"darkblue\",\n              annotation_text=\"Mean avg glucose level: \" + str(round(mean_gluc_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"darkblue\",\n              annotation_y = 0.5)\n\n# Adds vertical line for mean value of patients that did not have a stroke\nfig.add_vline(x=mean_gluc_no_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"crimson\",\n              annotation_text=\"Mean avg glucose level: \" + str(round(mean_gluc_no_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"crimson\")\n\nfig.show()","02523f2d":"fig = px.violin(df,\n                y='bmi',\n                x='stroke',\n                color='stroke',\n                points='all',\n                box=True)\nfig.show()","0f8e3716":"fig = px.violin(df,\n                y='bmi',\n                x='stroke',\n                color='gender')\nfig.show()","8eb12e0e":"mean_bmi_stroke = df[df['stroke'] == 1]['bmi'].mean()\nmean_bmi_no_stroke = df[df['stroke'] == 0]['bmi'].mean()\n\nfig = px.histogram(df, x=\"bmi\", color=\"stroke\")\n\n# Adds vertical line for mean value of patients that had a stroke\nfig.add_vline(x=mean_bmi_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"darkblue\",\n              annotation_text=\"Mean bmi: \" + str(round(mean_bmi_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"darkblue\",\n              annotation_y = 0.5)\n\n# Adds vertical line for mean value of patients that did not have a stroke\nfig.add_vline(x=mean_bmi_no_stroke,\n              line_width=3,\n              line_dash=\"dash\",\n              line_color=\"crimson\",\n              annotation_text=\"Mean bmi: \" + str(round(mean_bmi_no_stroke, 2)),\n              annotation_font_size=18,\n              annotation_font_color=\"crimson\")\n\nfig.show()","a810769b":"fig = px.histogram(df, x='smoking_status', color='stroke', barmode='group')\nfig.update_layout(bargap=0.2)\nfig.show()","6f40340f":"df[df['smoking_status']=='never smoked']['stroke'].value_counts(normalize=True)","7fe2945a":"df[df['smoking_status']=='smokes']['stroke'].value_counts(normalize=True)","f52c9e39":"df[df['smoking_status']=='formerly smoked']['stroke'].value_counts(normalize=True)","f48eddf9":"df.isnull().sum()","cfd78bb9":"# df[df['bmi'].isnull()]","c32a8efc":"df[df['bmi'].isnull()]['stroke'].value_counts(normalize=True)","08310d59":"# data manipulation\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom xgboost import XGBClassifier\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix","4d00e8c2":"df_num = df.copy()","a0ee0a71":"df_num = df_num.drop('Residence_type', axis = 1)","74f66f9e":"# set up preprocessing for numeric columns\nscaler = StandardScaler()\nimp_knn = KNNImputer(n_neighbors=5)\n\n# set up preprocessing for categorical columns\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# select columns for transformer\nnum_cols = ['age', 'bmi', 'avg_glucose_level']\ncat_cols = ['gender', 'ever_married', 'work_type', 'smoking_status']\nremainder_cols = ['hypertension', 'heart_disease']\n\n# do all preprocessing\npreprocessor = make_column_transformer(\n    (make_pipeline(scaler, imp_knn), num_cols),\n    (ohe, cat_cols),\n    remainder='passthrough')","417686a5":"X = df_num.drop('stroke', axis = 1)\ny = df_num['stroke'].astype(np.uint8)","710f0932":"preprocessor.fit_transform(X)\n\n# Get transformed DF\ntransformed_column_names = (num_cols +\n                            preprocessor.named_transformers_['onehotencoder'].\n                            get_feature_names_out().tolist() + remainder_cols)\n\n\nX = pd.DataFrame(preprocessor.fit_transform(X),\n                 columns=transformed_column_names)","1c56eb98":"X_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=y)","9357bacc":"from imblearn.over_sampling import SMOTE\n\nover = SMOTE(random_state=42)","8ade4712":"X_train_os, y_train_os = over.fit_resample(X_train, y_train)","42e8a472":"model_dict = {\n    'Logistic Reg':\n    LogisticRegression(random_state=42, max_iter=500, class_weight='balanced'),\n    'Naive Bayes':\n    GaussianNB(),\n    'Stochastic Grad Descent':\n    SGDClassifier(random_state=42, class_weight='balanced'),\n    'Random Forest Classifier':\n    RandomForestClassifier(random_state=42, class_weight='balanced'),\n    'Gradient Boosting Classifier':\n    GradientBoostingClassifier(random_state=42),\n    'Support Vector Machine':\n    SVC(random_state=42, class_weight='balanced'),\n    'K Nearest Classifier':\n    KNeighborsClassifier(),\n    'Decison Tree':\n    DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n    'XGBClassifier':\n    XGBClassifier(learning_rate=0.1,\n                  objective='binary:logistic',\n                  random_state=0,\n                  eval_metric='mlogloss')\n}","e6083848":"# Lists to create metrics dataframe later on\nmodel_list = []\ntrain_acc_list = []\ncounter_list = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_f1=[]\n\n# Main model loop\nfor model, clf in model_dict.items():\n    print('Running on '+model)\n    print(40*'=')\n    # Fitting the model to train data\n    clf.fit(X_train_os, y_train_os)\n    \n    # Making predictions\n    predictions = clf.predict(X_test)\n    \n    # Getting model accuracy on test data\n    acc = accuracy_score(y_test, predictions)\n    \n    # Getting train predictions\n    train_pred = clf.predict(X_train_os)\n    \n    # Training accuracy\n    train_acc = accuracy_score(y_train_os, train_pred)\n    \n    # Counting predicted target classes\n    counter = Counter(predictions)\n    \n    # Getting precision, recall and f1\n    report = precision_recall_fscore_support(y_test,\n                                             predictions,\n                                             average='binary')\n    \n    # Printing classification report\n    print('Classification report for '+model)\n    print(classification_report(y_test, predictions))\n    print(40*'=')\n\n    # Saving info to lists\n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    counter_list.append(counter)\n    test_acc.append(acc)\n    test_precision.append(report[0])\n    test_recall.append(report[1])\n    test_f1.append(report[2])\n\n# Putting it all together in the dataframe\nresults = pd.DataFrame({\n    \"model\": model_list,\n    \"train_accuracy\": train_acc_list,\n    'test_acc': test_acc,\n    'test_precision': test_precision,\n    'test_recall': test_recall,\n    'test_f1': test_f1,\n    'counter': counter_list\n})","b4e7dc90":"results","33813e1c":"# Lists to create metrics dataframe later on\nmodel_list = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_f1 = []\ntest_roc = []\n\nover = SMOTE(random_state=42)\n\nfor model, clf in model_dict.items():\n    print('Running on ' + model)\n    print(40 * '=')\n\n    # Defining the pipeline\n    pipeline = make_pipeline(over, clf)\n\n    # Stratified cv\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n\n    scores = cross_validate(pipeline,\n                            X,\n                            y,\n                            scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'),\n                            cv=cv,\n                            n_jobs=-1)\n\n    model_list.append(model)\n    test_acc.append(scores['test_accuracy'].mean())\n    test_precision.append(scores['test_precision'].mean())\n    test_recall.append(scores['test_recall'].mean())\n    test_f1.append(scores['test_f1'].mean())\n    test_roc.append(scores['test_roc_auc'].mean())\n\n# Putting it all together in the dataframe\nresults_cv = pd.DataFrame({\n    \"model\": model_list,\n    'test_acc': test_acc,\n    'test_precision': test_precision,\n    'test_recall': test_recall,\n    'test_f1': test_f1,\n    'test_roc_auc': test_roc\n})","e26d3d57":"results_cv","982fd3b2":"X = df.drop(['stroke'], axis = 1)\ny = df['stroke']","c141ff73":"from sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\n# NOTE THE make_pipeline needs to be from imblearn!!!\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold","67500a72":"# set up preprocessing for numeric columns\nscaler = StandardScaler()\nimp_knn = KNNImputer(n_neighbors=5, add_indicator=True)\n\n# set up preprocessing for categorical columns\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# select columns for transformer\nnum_cols = ['age', 'bmi', 'avg_glucose_level']\ncat_cols = [\n    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n]\n\n# do all preprocessing\npreprocessor = make_column_transformer(\n    (make_pipeline(scaler, imp_knn), num_cols),\n    (make_pipeline(imp_constant, ohe), cat_cols),\n    remainder='passthrough')","3d769f53":"# Over sampling with SMOTE:\nfrom imblearn.over_sampling import SMOTE","c1c51304":"over = SMOTE(random_state=42)","e595fc3f":"model_dict = {\n    'Logistic Reg':\n    LogisticRegression(random_state=42, max_iter=500, class_weight='balanced'),\n    'Naive Bayes':\n    GaussianNB(),\n    'Stochastic Grad Descent':\n    SGDClassifier(random_state=42, class_weight='balanced'),\n    'Random Forest Classifier':\n    RandomForestClassifier(random_state=42, class_weight='balanced'),\n    'Gradient Boosting Classifier':\n    GradientBoostingClassifier(random_state=42),\n    'Support Vector Machine':\n    SVC(random_state=42, class_weight='balanced'),\n    'K Nearest Classifier':\n    KNeighborsClassifier(),\n    'Decison Tree':\n    DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n    'XGBClassifier':\n    XGBClassifier(learning_rate=0.1,\n                  objective='binary:logistic',\n                  random_state=0,\n                  eval_metric='mlogloss')\n}","66b5ae5f":"# Lists to create metrics dataframe later on\nmodel_list = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_f1 = []\ntest_roc = []\n\nover = SMOTE(random_state=42)\n\nfor model, clf in model_dict.items():\n    print('Running on ' + model)\n    print(40 * '=')\n\n    # Defining the pipeline\n    pipeline = make_pipeline(preprocessor, over, clf)\n\n    # Stratified cv\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n\n    scores = cross_validate(pipeline,\n                            X,\n                            y,\n                            scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'),\n                            cv=cv,\n                            n_jobs=-1)\n    \n\n    model_list.append(model)\n    test_acc.append(scores['test_accuracy'].mean())\n    test_precision.append(scores['test_precision'].mean())\n    test_recall.append(scores['test_recall'].mean())\n    test_f1.append(scores['test_f1'].mean())\n    test_roc.append(scores['test_roc_auc'].mean())\n\n# Putting it all together in the dataframe\nresults_cv = pd.DataFrame({\n    \"model\": model_list,\n    'test_acc': test_acc,\n    'test_precision': test_precision,\n    'test_recall': test_recall,\n    'test_f1': test_f1,\n    'test_roc_auc': test_roc\n})","15fe53a2":"results_cv","e1cb5cc5":"# Tuning\nover = SMOTE()\npipeline = make_pipeline(preprocessor, over, LogisticRegression(max_iter=500))","87e90f9a":"# pipeline.steps","0bd10d07":"params = {}\nparams['logisticregression__C'] = [100, 10, 1.0, 0.1, 0.01]\nparams['logisticregression__penalty'] = ['l2']\nparams['logisticregression__solver'] = ['newton-cg', 'lbfgs', 'liblinear']","89db7fb0":"from sklearn.model_selection import GridSearchCV\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\ngrid = GridSearchCV(pipeline, params, cv=cv, scoring='recall', n_jobs=-1)\ngrid_result = grid.fit(X, y)","e78c6508":"# what was the best score found during the search?\ngrid.best_score_","50d9d2a3":"# which combination of parameters produced the best score?\ngrid.best_params_","0f908a2a":"# summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# convert results into a DataFrame\ndf_grid = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","5a1dfda8":"pd.set_option('display.max_colwidth', None)","d414f6fa":"df_grid.sort_values('rank_test_score')","584730aa":"best_params_logreg = {\n    'logisticregression__C': 0.01,\n    'logisticregression__penalty': 'l2',\n    'logisticregression__solver': 'lbfgs'\n}","d53554fb":"from sklearn.model_selection import RandomizedSearchCV","db713684":"# Tuning\nover = SMOTE()\npipeline = make_pipeline(\n    preprocessor, over,\n    SGDClassifier(random_state=42, class_weight='balanced', max_iter=10000))","afb80549":"# pipeline.steps","bd7521bd":"params = {}\nparams['sgdclassifier__loss'] = [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"]\nparams['sgdclassifier__alpha'] = [0.0001, 0.001, 0.01, 0.1]\nparams['sgdclassifier__penalty'] = [\"l2\", \"l1\", \"none\"]","970f7c5c":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\ngrid = GridSearchCV(pipeline, params, cv=cv, scoring='recall', n_jobs=-1)\n# grid = RandomizedSearchCV(pipeline, params, n_iter=20, cv=cv, scoring='recall', n_jobs=-1)\ngrid_result = grid.fit(X, y)","853e2f0f":"# which combination of parameters produced the best score?\ngrid.best_params_","a2466a93":"# convert results into a DataFrame\ndf_grid = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","e7330699":"df_grid.sort_values('rank_test_score').head(10)","cb0652f1":"best_params_sgd = {\n    'sgdclassifier__alpha': 0.1,\n    'sgdclassifier__loss': 'hinge',\n    'sgdclassifier__penalty': 'l1'\n}","4e3a4e12":"X_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=y)","7eeba0a5":"over = SMOTE(random_state=42)\n\nlogreg_pipeline = make_pipeline(\n    preprocessor, over,\n    LogisticRegression(C=0.01, penalty='l2', solver='lbfgs', max_iter=500))","2eeac1f0":"logreg_pipeline.fit(X_train, y_train);","f66b2ffb":"predictions = logreg_pipeline.predict(X_test)","75c899b8":"print(classification_report(y_test, predictions))","71628119":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay","3d835c6f":"disp = ConfusionMatrixDisplay.from_estimator(logreg_pipeline,\n                                             X_test,\n                                             y_test,\n                                             display_labels=logreg_pipeline.classes_);","13f39402":"from sklearn.metrics import roc_curve, auc\nfpr_lr, tpr_lr, _ = roc_curve(y_test, logreg_pipeline.predict_proba(X_test)[:,1])\n\nplt.figure(figsize=(12,8));\n\nplt.plot(fpr_lr, tpr_lr);\nplt.xlabel('False Positive Rate', fontsize=16);\nplt.ylabel('True Positive Rate', fontsize=16);\nplt.title('ROC curve', fontsize=16);\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--');\n#plt.axes().set_aspect('equal');\nsns.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False);\n\nprint('Auc : ', auc(fpr_lr, tpr_lr))","48268567":"### One-liner\n# disp = RocCurveDisplay.from_estimator(logreg_pipeline,\n#                                       X_test,\n#                                       y_test,\n#                                       name='LogReg')","57830c40":"over = SMOTE(random_state=42)\n\nsgd_pipeline = make_pipeline(\n    preprocessor, over,\n    SGDClassifier(alpha=0.1,\n                  loss='hinge',\n                  penalty='l1',\n                  random_state=42,\n                  class_weight='balanced',\n                  max_iter=10000))","c0e340c7":"sgd_pipeline.fit(X_train, y_train)\npredictions = sgd_pipeline.predict(X_test)\nprint(classification_report(y_test, predictions))","10d2c013":"disp = ConfusionMatrixDisplay.from_estimator(\n    sgd_pipeline, X_test, y_test, display_labels=sgd_pipeline.classes_)","7c377a96":"fpr_lr, tpr_lr, _ = roc_curve(y_test, sgd_pipeline.decision_function(X_test))\n\nplt.figure(figsize=(12,8));\n\nplt.plot(fpr_lr, tpr_lr);\nplt.xlabel('False Positive Rate', fontsize=16);\nplt.ylabel('True Positive Rate', fontsize=16);\nplt.title('ROC curve', fontsize=16);\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--');\n#plt.axes().set_aspect('equal');\nsns.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False);\n\nprint('Auc : ', auc(fpr_lr, tpr_lr))","1deb3d90":"# ### One-liner\n# disp = RocCurveDisplay.from_estimator(sgd_pipeline,\n#                                       X_test,\n#                                       y_test,\n#                                       name='SGD')","dc6cc5e2":"preprocessor.named_transformers_['pipeline-2']['onehotencoder']","50f9d796":"# Get the names of each feature\n\nnum_cols = ['age', 'bmi', 'avg_glucose_level']\ncat_cols = [\n    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n]\nremainder_cols = ['hypertension', 'heart_disease']\n\ntransformed_column_names = (\n    num_cols + preprocessor.named_transformers_['pipeline-2']\n    ['onehotencoder'].get_feature_names_out().tolist() + remainder_cols)\n\ntransformed_column_names","1d48c649":"coefs = logreg_pipeline['logisticregression'].coef_.flatten()","4acce90b":"# Zip coefficients and names together and make a DataFrame\nzipped = zip(transformed_column_names, coefs)\n\nfeatures_df = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n\n# Sort the features by the absolute value of their coefficient\nfeatures_df[\"abs_value\"] = features_df[\"value\"].apply(lambda x: abs(x))\nfeatures_df[\"colors\"] = features_df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\nfeatures_df = features_df.sort_values(\"abs_value\", ascending=False)","48cd29b0":"import seaborn as sns\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 7))\nsns.barplot(x=\"feature\",\n            y=\"value\",\n            data=features_df.head(20),\n           palette=features_df.head(20)[\"colors\"])\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)\nax.set_title(\"Top 20 Features\", fontsize=25)\nax.set_ylabel(\"Coef\", fontsize=22)\nax.set_xlabel(\"Feature Name\", fontsize=22)","d0a6780f":"coefs = sgd_pipeline['sgdclassifier'].coef_.flatten()\n\n# Zip coefficients and names together and make a DataFrame\nzipped = zip(transformed_column_names, coefs)\n\nfeatures_df = pd.DataFrame(zipped, columns=[\"feature\", \"value\"])\n\n# Sort the features by the absolute value of their coefficient\nfeatures_df[\"abs_value\"] = features_df[\"value\"].apply(lambda x: abs(x))\nfeatures_df[\"colors\"] = features_df[\"value\"].apply(lambda x: \"green\"\n                                                   if x > 0 else \"red\")\nfeatures_df = features_df.sort_values(\"abs_value\", ascending=False)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 7))\nsns.barplot(x=\"feature\",\n            y=\"value\",\n            data=features_df.head(20),\n            palette=features_df.head(20)[\"colors\"])\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)\nax.set_title(\"Top 20 Features\", fontsize=25)\nax.set_ylabel(\"Coef\", fontsize=22)\nax.set_xlabel(\"Feature Name\", fontsize=22)","f05934b2":"# set up preprocessing for numeric columns\nscaler = StandardScaler()\nimp_knn = KNNImputer(n_neighbors=5)\n\n# set up preprocessing for categorical columns\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# select columns for transformer\nnum_cols = ['age', 'bmi', 'avg_glucose_level']\ncat_cols = [\n    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n]\n\n# do all preprocessing\npreprocessor = make_column_transformer(\n    (make_pipeline(scaler, imp_knn), num_cols),\n    (make_pipeline(imp_constant, ohe), cat_cols),\n    remainder='passthrough')\n\n# Over sampler\nover = SMOTE(random_state=42)\n\n# Main pipeline\nlogreg_pipeline = make_pipeline(\n    preprocessor, over,\n    LogisticRegression(C=0.01, penalty='l2', solver='lbfgs', max_iter=500))\n\n# Fit the model on full data\nlogreg_pipeline.fit(X, y)\n\n# Metrics\npredictions = logreg_pipeline.predict(X)\nprint(classification_report(y, predictions))","9cbfd2c1":"disp = ConfusionMatrixDisplay.from_estimator(logreg_pipeline,\n                                             X,\n                                             y,\n                                             display_labels=logreg_pipeline.classes_);","0e3831be":"# Prediction function\ndef stroke_prediction(patient_info):\n    '''\n    Given patient information in a dictionary with keys matching those of X.columns,\n    return predicted probability of having a stroke.\n    Prints the probabilities and returns them as array (probability of 0, probability of 1)\n    '''\n    user = pd.DataFrame(patient_info)\n\n    prediction = logreg_pipeline.predict_proba(user)\n\n    print(40 * '=')\n    print('Predicted probability of patient having a stroke:')\n    print(str(100 * round(prediction[0][1], 2)) + '%')\n\n    return prediction","b49eeec8":"Patient1 = {\n    'gender': ['Male'],\n    'age': [32.0],\n    'hypertension': [0],\n    'heart_disease': [1],\n    'ever_married': ['Yes'],\n    'work_type': ['Private'],\n    'Residence_type': ['Urban'],\n    'avg_glucose_level': [100.00],\n    'bmi': [24.6],\n    'smoking_status': ['never smoked']\n}","ed0f43ea":"stroke_prediction(Patient1)","d16d1beb":"Patient2 = {\n    'gender': ['Female'],\n    'age': [60.0],\n    'hypertension': [0],\n    'heart_disease': [0],\n    'ever_married': ['Yes'],\n    'work_type': ['Private'],\n    'Residence_type': ['Urban'],\n    'avg_glucose_level': [150.00],\n    'bmi': [30.0],\n    'smoking_status': ['never smoked']\n}\n\nstroke_prediction(Patient2)","fb4aac3e":"A violin plot may be better here to visualize the difference in these two distributions:","370e8955":"### Stochastic Gradient Descent","740ff728":"For the `bmi` column, we'll use **KNNImputer** in our pipeline instead of the usual **SimpeImputer**. According to the [CDC](https:\/\/www.cdc.gov\/healthyweight\/assessing\/bmi\/adult_bmi\/index.html):\n\n> BMI is interpreted differently for children and teens even though it is calculated with the same formula. Due to changes in weight and height with age, as well as their relation to body fatness, BMI levels among children and teens are expressed relative to other children of the same sex and age.\n\nSo, even though *BMI* is calculated with the same formula for all ages, it's meaning (the number's meaning) is different for different ages and gender. This motivates the use of **KNNImputer** because it uses information on other features to impute missing values. According to the [docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html):\n\n> Each sample\u2019s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.","72d1a5f2":"## Age column","1f5438be":"This patient has a high chance of having a stroke (again, according to our simple model).","3faa96a8":"## Preprocessing","267191d7":"## Oversampling the minority class","eee6274f":"So, both our outliers candidates are female patients. Let's look at them closer","f2d4371e":"Let's consider the task before deciding which one of these did better: we are trying to predict wether a patient is likely to have a stroke or not. To this end, it is much safer to over classify patients as probable stroke cases and miss than the other way around. In other words, we'll take some false positives over some false negatives! In (yet) other words, we prefer models with *higher recall*.\n\nIf we blindly follow this rule, we see that the Naive Bayes classifier has a recall of almost 1.0. Now, we have not done any cross validation and the remaining metrics for this model are not the best. We should also notice that all precisions are really low, which means we have a lot of false positives. We should consider the \"cost\" of false positives if we use this in a real-world scenario...\n\n### Cross validating\n\nLet's run cross validation on some of this models to get a better view of the metrics. We'll do a (stratified) k-fold cross-validation on our data. The correct way to do this with oversampling is to apply the method on the training data only, then get metrics for the model on the stratified but non-transformed test data. Check [here](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/) for a great tutorial on the matter.\n\nWe can do this with a (`imblearn`) pipeline! To create it, we first transform the training data with `SMOTE` and then fit the model.\n\n","16b8bd58":"Let's get a prediction on a (fictitious) patient: Male, 32 years-old, history of hear disease, married, non-smoker and with normal BMI.","eae36410":"## Hypertension column","f045439e":"# EDA","16722033":"## Ever Married column","8809dd92":"## Train, test split","131003ff":"## Gender column","3a8f2fd5":"## Residence Type column","0315798d":"This is a highly imbalanced classification task!","f3367371":"# Stroke Prediction Dataset\n\nFind it [here](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset).\n\n---\n\n## Context\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n## Attribute Information\n\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","7d95d361":"The 14 years-old patient will **not** be dropped as an outlier here because of her `bmi`value. According to the [CDC clinical growth chart](https:\/\/www.cdc.gov\/growthcharts\/data\/set1clinical\/cj41l024.pdf), a `bmi` value of 30.9 for a female patient of 14 years of age is well above the 95th percentile for `bmi` - meaning that her `bmi` is greater than that of 95% of similarly aged girls in this reference population. This would place the patient as obese. Quoting the [CDC](https:\/\/www.cdc.gov\/healthyweight\/assessing\/bmi\/adult_bmi\/index.html#InterpretedAdults)\n\n> Obesity among 2- to 19-year-olds is defined as a BMI at or above the 95th percentile of children of the same age and sex(...).\n\nSo, for our purposes (predicting a stroke), it will be important to keep this record as the obesity might be strongly correlated with a stroke in such a young age.","5b15ef5d":"# Thanks for reading this far!\n\n## Let's connect: \n\n* [My portfolio](https:\/\/brunobvr.github.io\/my_portfolio\/)\n* [LinkedIn](https:\/\/www.linkedin.com\/in\/bruno-vieira-ribeiro-a1b1b259\/)\n* [GitHub](https:\/\/github.com\/BrunoBVR)\n\nStay safe everyone!","88a4be1f":"## Column transformers\n\nWe'll do the following transformations:\n* For `age`, `bmi` and `avg_glucose_level` we'll scale the data using StandardScaler.\n* We'll do imputation using KNNImputer (for reasons explained above) to the missing values in the `bmi` column.\n* We'll One Hot encode categorical columns (`gender`, `ever_married`, `work_type`, `smoking_status`).\n* We'll keep `hypertension` and `heart_disease` as binary features.","7073d89f":"Our fictitious fellow has a very low chance of having a stroke (according to our simple model).\nLet's try another one: Female, 60 years-old, married and with high BMI:","86e368d2":"### SGD","653a592b":"## Smoking Status column","2c520cd4":"### Oversampling X and y only on training data","d821d4c1":"## BMI column","0059597e":"## Function to get prediction on patient:","e0c01d80":"## Avg Glucose Level column","b1ae70b4":"For the updated figure:","b2d286cd":"Look at how uniform those points on the \"no stroke\" patients look! We can see, using plotly's awesome interactivity that there are two points that appear to be outliers in the \"stroke\" patients distributions:\n* One 14 years old patient\n* One patient with an age of 1.32.\n\nWe'll explore these two patients to see if their other characteristics and warning (such as high BMI or Glucose). If they are, then their contribution is important for the model and should not be rejected as outliers based on this distribution alone!\n\nLet's inspect a bit more to see what can we tell about these possible outliers:","70014ab4":"The patient with an age of 1.32 does not have a recorded `bmi` and a low blood glucose level (check [this](https:\/\/www.nationwidechildrens.org\/family-resources-education\/health-wellness-and-safety-resources\/resources-for-parents-and-kids\/managing-your-diabetes\/chapter-three-monitoring-blood-glucose) reference for the expected ranges of glucose level in children). We will drop this record for the remaining analysis:","09b6ccda":"The `stroke` class 0 (patient did not have a stroke) represents over 95% of all data points.","a16ea1f7":"## Selecting features and targets","b3ec1e7d":"## Feature importance\n\n### Logistic Regression","4423beba":"### Stochastic Gradient Descent","15a1d3a2":"## Target value: `stroke`","75f4bb07":"With our cross-validated metrics, we can choose some models to tune some hyperparameters and improve performance. We see that Logistic Regression and SGD have recalls of over 70% and a ROC-AUC socre of over 0.83, so we can pick this two to move forward.\n\n---\n\n## Putting it all in a single pipeline\n\nUntil now, we have done the preprocessing of our data in a separate pipeline than the modeling. However, we can link together our `preprocessor` object with our oversampler and our classifiers in a single pipeline. This way, we do not create a new features matrix and can simplify the modeling process (basically getting the data \"as is\" and feeding it to our pipeline).\n\nLet's do this before our tuning. We'll start from our original `df` and redefine the preprocessor (just a copy of the code above, for clarity).","4a94e069":"## Testing algorithms\n\nWe'll create a dictionary of classifiers to test our task. To keep track of performance, we'll save some metrics for each model into a dataframe.\n\n### Getting performance of classifiers on a single run","88339664":"Distribution of age according to patient having a stroke or not:","d318ef1c":"## Best models\n\n### Logistic Regression","a5c8eeb4":"# Training model on full data and making predictions\n\nNow we can pick a tuned model and train it on the full dataset. We'll use Logistic Regression, for simplicity (but the process would be similar for SGD):","676ef9ec":"We can drop the `Residence_type` column as the cases of stroke appear to be equally distributed among both classes (Urban and Rural).","9196a663":"## Heart Disease column","c18bc51d":"## Dealing with missing values","e424bf6e":"There is only one record of `gender` as `Other`. We will drop this row for further analysis.","49c24ac4":"# Modeling\n\nWe'll start our modeling with no further feature engineering.","f143e151":"## Dropping the id column","930f26ed":"## Hyperparameter tuning\n\n### Logistic Regression\n","9e587c92":"## Work Type column"}}