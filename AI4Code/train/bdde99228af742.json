{"cell_type":{"5fa03d33":"code","599ee6cf":"code","b5b16804":"code","beed1841":"code","26244ed3":"code","9743e2ca":"code","53b662c5":"code","8c45e0c3":"code","93d3dcfb":"code","edf066e0":"markdown","792d1612":"markdown"},"source":{"5fa03d33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","599ee6cf":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport keras.backend as K\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\nimport os","b5b16804":"!wget https:\/\/www.tbi.univie.ac.at\/RNA\/download\/ubuntu\/ubuntu_18_04\/viennarna_2.4.15-1_amd64.deb\n!apt-get install .\/viennarna_2.4.15-1_amd64.deb -y\n!git clone https:\/\/github.com\/DasLab\/arnie\n\n!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!git clone https:\/\/www.github.com\/DasLab\/draw_rna draw_rna_pkg\n!cd draw_rna_pkg && python setup.py install\n\n!yes '' | cpan -i Graph\n!git clone https:\/\/github.com\/hendrixlab\/bpRNA","beed1841":"import sys\n\n!echo \"vienna_2: \/usr\/bin\" > arnie.conf\n!echo \"TMP: \/kaggle\/working\/tmp\" >> arnie.conf\n!mkdir -p \/kaggle\/working\/tmp\nos.environ[\"ARNIEFILE\"] = f\"\/kaggle\/working\/arnie.conf\"\nsys.path.append('\/kaggle\/working\/draw_rna_pkg\/')\nsys.path.append('\/kaggle\/working\/draw_rna_pkg\/ipynb\/')\npkg = 'vienna_2'","26244ed3":"from multiprocessing import Pool\nfrom arnie.pfunc import pfunc\nfrom arnie.mea.mea import MEA\nfrom arnie.free_energy import free_energy\nfrom arnie.bpps import bpps\nfrom arnie.mfe import mfe\nimport arnie.utils as utils\nfrom tqdm.notebook import tqdm as tqdm\n\nn_candidates = 2\n# turn off for all data\ndebug = True","9743e2ca":"!grep processor \/proc\/cpuinfo | wc -l\n\nMAX_THRE = 4\n","53b662c5":"train = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\nif debug:\n    train = train[:20]\n    test = test[:20]\ntarget_df = train.append(test)","8c45e0c3":"def proc1(arg):\n    sequence = arg[0]\n    id = arg[1]\n    log_gamma = arg[2]\n    bp_matrix = bpps(sequence, package=pkg)\n    mea_mdl = MEA(bp_matrix,gamma=10**log_gamma)\n    return id, sequence, mea_mdl.structure, log_gamma, mea_mdl.score_expected()[2]\n\nli = []\nfor log_gamma in range(10):\n    for i, arr in enumerate(target_df[['sequence','id']].values):\n        li.append([arr[0], arr[1], log_gamma])\n\np = Pool(processes=MAX_THRE)\nresults = []\nfor ret in tqdm(p.imap(proc1, li),total=len(li)):\n    results.append(ret)\n    #print(f'done for {ret[0]}')\ndf = pd.DataFrame(results, columns=['id', 'sequence', 'structure', 'log_gamma', 'score'])\n\ndf_tmp = target_df[['id', 'sequence', 'structure']].copy()\ndf_tmp['log_gamma'] = 100\ndf_tmp['score'] = 100\ndf = df.append(df_tmp).sort_values('score', ascending=False).reset_index(drop=True)\n\nnew_df = pd.DataFrame()\nfor id in df['id'].unique():\n    unq_df = df[df['id'] == id].drop_duplicates('structure')\n    unq_df['cnt'] = unq_df.shape[0]\n    new_df = new_df.append(unq_df[1:min(n_candidates,len(unq_df))])","93d3dcfb":"!mkdir -p tmp_files\ndef get_predicted_loop_type(id, sequence, structure, debug=False):\n    structure_fixed = structure.replace('.','0').replace('(','1').replace(')','2')\n    pid = os.getpid()\n    tmp_in_file = f'tmp_files\/{id}_{structure_fixed}_{pid}.dbn'\n    tmp_out_file = f'{id}_{structure_fixed}_{pid}.st'\n    !echo $sequence > $tmp_in_file\n    !echo \"$structure\" >> $tmp_in_file\n    !export PERL5LIB=\/root\/perl5\/lib\/perl5 && perl bpRNA\/bpRNA.pl $tmp_in_file\n    result = [l.strip('\\n') for l in open(tmp_out_file)]\n    if debug:\n        print(sequence)\n        print(structure)\n        print(result[5])\n    else:\n        !rm $tmp_out_file $tmp_in_file\n    return id, structure, result[5]\n\ndef proc2(arg):\n    result = get_predicted_loop_type(arg[0], arg[1], arg[2], debug=False)\n    return result\n\nli = []\nfor i, arr in enumerate(new_df[['id', 'sequence', 'structure']].values):\n    li.append(arr)\n\np = Pool(processes=MAX_THRE)\nresults_loop_type = []\nfor ret in tqdm(p.imap(proc2, li),total=len(li)):\n    results_loop_type.append(ret)\n    #print(f'done for {ret[0]}')\n\nnew_df = new_df.merge(pd.DataFrame(results_loop_type, columns=('id', 'structure', 'predicted_loop_type')), on=['id','structure'], how='left')\nnew_df.to_csv('aug_data.csv', index=False)","edf066e0":"# Start","792d1612":"# Data Augmentation"}}