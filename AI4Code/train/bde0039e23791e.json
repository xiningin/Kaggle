{"cell_type":{"35365edd":"code","2f13dd67":"code","0ac99e46":"code","d5cffc5f":"code","f09a1d25":"code","372be8c0":"code","3f57eec3":"code","894b2103":"code","5e047524":"code","8c7ad526":"code","9d503be0":"code","f0a6b3f7":"code","25464c3a":"code","02592ebb":"code","25352a84":"code","c9c01210":"code","36cd1bff":"code","d6980dde":"code","5bab33a1":"code","11b48a6f":"markdown","e822d4fe":"markdown","350f1190":"markdown","b00ae9c3":"markdown","be8220d8":"markdown","b81a69f8":"markdown","f769ffbe":"markdown"},"source":{"35365edd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f13dd67":"heart = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nheart.head()","0ac99e46":"print(\"The number of training examples(data points) = %i \" % heart.shape[0])\nprint(\"The number of features = %i \" % heart.shape[1])","d5cffc5f":"heart.info(); ","f09a1d25":"heart.isnull().sum()","372be8c0":"duplicate = heart[heart.duplicated()]\n","3f57eec3":"heart.drop_duplicates()","894b2103":"heart.target.value_counts()","5e047524":"#sns.pairplot(heart) #diag_kind can be changed to see a different type of graph along the diagonal","8c7ad526":"\ncorr = heart.corr()\nf, ax = plt.subplots(figsize=(20, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,linewidths=.5, annot= True)\n","9d503be0":"target = heart['target']\nvariables = heart.drop('target',axis=1)\nvariables.head()\n\nx_train,x_test,y_train,y_test = train_test_split(variables, target, test_size=0.2, random_state=42)","f0a6b3f7":"x_train=np.array(x_train)\ny_train=np.array(y_train)\nx_test=np.array(x_test)\ny_test=np.array(y_test)\n\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","25464c3a":"clf = DecisionTreeClassifier(max_depth=15, random_state=12)\n\nclf.fit(x_train, y_train)\n\nclf_score_train = clf.score(x_train, y_train) #Accuracy\nclf_score_test =clf.score(x_test, y_test)\n\n\nprint('train accuracy: {}%'.format(clf_score_train*100)) \nprint('test accuracy: {}%'.format(clf_score_test*100))","02592ebb":"#from sklearn import tree\n#tree.plot_tree(clf)","25352a84":"rf = RandomForestClassifier(n_estimators=7, max_depth=7)\nrf.fit(x_train, y_train)\n\nclf_score_train = rf.score(x_train, y_train) #Accuracy\nclf_score_test =rf.score(x_test, y_test)\n\nprint('train accuracy: {}%'.format(clf_score_train*100)) \nprint('test accuracy: {}%'.format(clf_score_test*100))","c9c01210":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 10000)\nlr.fit(x_train,y_train)\n\ny_pred = lr.predict(x_test)\n\nscore_train = lr.score(x_train, y_train) #Coefficient of determination\nscore_test =lr.score(x_test, y_test)\n\n\nprint('train score: {}'.format(score_train)) \nprint('test score: {}'.format(score_test))\n","36cd1bff":"from sklearn.svm import SVC\nscaler=MinMaxScaler()\n\nSc_X_train=scaler.fit_transform(x_train)\nSc_X_test=scaler.fit_transform(x_test)\n\nclf=SVC(kernel='rbf')\nclf.fit(Sc_X_train, y_train)\n\nclf_score_train = clf.score(Sc_X_train, y_train) #accuracy\nclf_score_test =clf.score(Sc_X_test, y_test)\n\nprint('train accuracy: {}%'.format(clf_score_train*100)) \nprint('test accuracy: {}%'.format(clf_score_test*100))","d6980dde":"#using svc to predict \n\nclf.predict([[21,0,0,97,160,1,0,150,0,0.5,2,2,3]])\n","5bab33a1":"#using lr to predict \n\nlr.predict([[21,0,0,97,160,1,0,150,0,0.5,2,2,3]])","11b48a6f":"# Splitting Data ","e822d4fe":"# Random Forest","350f1190":"# Data Exploration","b00ae9c3":"# Logistic Regression","be8220d8":"# ***Different Classification Models***","b81a69f8":"# SVC","f769ffbe":"# Decision Tree"}}