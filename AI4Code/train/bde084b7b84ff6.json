{"cell_type":{"feef5a60":"code","a3699cd6":"code","93ec0ffb":"code","6ceb4e18":"code","7a0ab9ae":"code","420fc394":"code","58ae8217":"code","87cac4fd":"code","0b4def64":"code","76e12fbb":"code","9f3d44be":"code","7775be96":"code","861f7789":"code","b0c51627":"code","ff3286d1":"code","7b4269a5":"code","1c12bffb":"code","e8dec67c":"code","a2a22166":"code","01233831":"code","b9344c99":"code","9e5068e0":"code","f1ee12d4":"code","1abe38e8":"code","7e80a39a":"code","5a24f407":"code","9ca401c9":"code","2c58f263":"code","5f26ed36":"code","f9a2e4ad":"markdown","8dc37e94":"markdown","701eae3f":"markdown","03f008b3":"markdown","87a0057a":"markdown","00d8f912":"markdown","08613a67":"markdown"},"source":{"feef5a60":"import pandas as pd\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv', na_filter=False)\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv', na_filter=False)","a3699cd6":"train_df.sample(10)","93ec0ffb":"train_df.shape","6ceb4e18":"train_df['sentiment'].value_counts()","7a0ab9ae":"test_df.sample(10)","420fc394":"test_df.shape","58ae8217":"def jaccard(str1, str2):\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if len(a) + len(b) == len(c):\n        return 0.0\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","87cac4fd":"def eval_jaccard(y_pred, y_true):\n    return sum(jaccard(sp, st) for sp, st in zip(y_pred, y_true)) \/ len(y_true)","0b4def64":"eval_jaccard(train_df[\"text\"], train_df[\"selected_text\"])","76e12fbb":"SEED = 2020","9f3d44be":"def to_spacy_format(X, Y, S):\n    data = []\n    for x, y, s in zip(X, Y, S):\n        if not x:\n            print(f\"'{x}': no tweet given\")\n            continue\n        if y and (s == \"positive\" or s == \"negative\"):            \n            start = x.find(y)\n            if start < 0:\n                print(\"Can't find a phrase: skipping...\")\n                continue\n            ex = (x, {\"entities\": [(start, start + len(y), s)]})\n        else:\n            ex = (x, {\"entities\": []})\n        data.append(ex)\n    return data","7775be96":"def from_spacy_format(data):\n    X, Y = [], []\n    for x, ann in data:\n        X.append(x)\n        if ann[\"entities\"]:\n            s = []\n            for e in ann[\"entities\"]:\n                s.append(x[e[0]: e[1]])                \n            Y.append(\" \".join(s))\n        else:\n            Y.append(x)\n    return X, Y","861f7789":"def predict_spacy(nlp, X):\n    Y = []\n    for x in X:\n        doc = nlp(x)\n        if doc.ents:\n            y = \" \".join([e.text for e in doc.ents])\n        else:\n            y = x\n        Y.append(y)\n    return Y","b0c51627":"def baseline(data):\n    X, Y = from_spacy_format(data)\n    return eval_jaccard(X, Y)","ff3286d1":"from pathlib import Path\n\nmodel_dir = Path(\"\/kaggle\/working\/model\/\")\nmodel_dir.mkdir(exist_ok=True)","7b4269a5":"import random\nimport numpy as np\nfrom spacy.util import minibatch, compounding, decaying\n\n\ndef train_model(nlp, train_data, model_dir,\n                valid_data=None, blank=False,\n                epochs=100, \n                dropouts=decaying(0.6, 0.2, 1e-6), \n                batch_sizes=compounding(1.0, 16.0, 1.0 + 1e-4)\n               ):\n    \n    x_train, y_train = from_spacy_format(train_data)    \n    \n    if valid_data:\n        best_score = 0\n        x_valid, y_valid = from_spacy_format(valid_data)\n    \n    random.seed(SEED)\n    \n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    else:\n        ner = nlp.get_pipe(\"ner\")\n        \n    ner.add_label(\"positive\")\n    ner.add_label(\"negative\")\n    \n    # get names of other pipes to disable them during training\n    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    \n    # only train NER\n    with nlp.disable_pipes(*other_pipes):\n        \n        # reset and initialize the weights randomly\n        if blank:\n            optimizer = nlp.begin_training()\n        else:\n            optimizer = nlp.resume_training()\n        \n        for i in range(epochs):\n            random.shuffle(train_data)\n            losses = {}\n            \n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(train_data, size=batch_sizes)\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                dropout = next(dropouts)\n                \n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    sgd=optimizer,\n                    drop=dropout,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            \n            loss = losses['ner']\n            batch_size = next(batch_sizes)\n            dropout = next(dropouts)\n            \n            y_pred = predict_spacy(nlp, x_train)\n            tr_score = eval_jaccard(y_pred, y_train)\n            \n            message = f\"epoch {i + 1}: batch_size={batch_size:.1f}, dropout={dropout:.3f}, loss={loss:.3f}, tr_score={tr_score:.3f}\"\n            \n            if valid_data:\n                y_pred = predict_spacy(nlp, x_valid)\n                val_score = eval_jaccard(y_pred, y_valid)          \n                \n                if val_score > best_score:\n                    best_score = val_score\n                    if not model_dir.exists():\n                        model_dir.mkdir(parents=True)\n                    nlp.to_disk(model_dir)\n                \n                message = f\"{message}, val_score={val_score:.3f}\"\n            \n            print(message)","1c12bffb":"import spacy\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n\ndef cross_validate(df, model_dir, folds=10, epochs=10, blank=True):\n    print(f\"Cross-validation: folds={folds}\")\n        \n    data = to_spacy_format(df.text, df.selected_text, df.sentiment)\n    \n    models = []    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, valid_idx) in enumerate(skf.split(data, df.sentiment), start=1): \n        print(f'Fold: {fold}')\n        \n        train_set = set(train_idx)\n        valid_set = set(valid_idx)\n        \n        train_data = [x for i, x in enumerate(data) if i in train_set]\n        valid_data = [x for i, x in enumerate(data) if i in valid_set]\n        \n        baseline_score = baseline(valid_data)\n        print(f\"Baseline={baseline_score:.3f}\")\n        \n        if blank:\n            model = spacy.blank(\"en\")\n        else:\n            model = spacy.load(\"en_core_web_sm\")\n        \n        dropouts=decaying(0.5, 0.3, 1e-6)\n        batch_sizes=compounding(4.0, 16.0, 1.0 + 1e-4)\n        \n        model_path = Path(model_dir.joinpath(f\"spacy_fold_{fold}\"))\n        model_path.mkdir(parents=True)\n        \n        train_model(model, train_data, model_path, valid_data,\n                    dropouts=dropouts, batch_sizes=batch_sizes, \n                    blank=blank, epochs=epochs)\n        \n        models.append(model)\n    \n    return models","e8dec67c":"train_pn_df = train_df[(train_df[\"sentiment\"] == \"positive\") | (train_df[\"sentiment\"] == \"negative\")]","a2a22166":"%%time\n\nmodels = cross_validate(train_pn_df, model_dir, folds=10, epochs=7, blank=True)","01233831":"import numpy as np\nimport math\n\ndef mode(array, prefer_min=True):\n    (values, counts) = np.unique(array, return_counts=True)\n    max_count = np.max(counts)\n    best = math.inf if prefer_min else - math.inf\n    for i in range(len(values)):\n        if counts[i] == max_count:\n            if prefer_min and best > values[i]:\n                best = values[i]\n            if not prefer_min and best < values[i]:\n                best = values[i]\n    return best","b9344c99":"def majority_vote(ys, x):\n    starts, ends = [], []\n    for y in ys:\n        start = x.find(y)\n        end = start + len(y)\n        starts.append(start)\n        ends.append(end)\n    y = x[mode(starts, prefer_min=True): mode(ends, prefer_min=False)]\n    return y","9e5068e0":"def predict_all(models):\n    Y = []\n    for x, s in zip(test_df.text, test_df.sentiment):\n        if s == \"neutral\":\n            Y.append(x)\n        else:\n            ys = []\n            for model in models:\n                ys.append(predict_spacy(model, [x])[0])\n            Y.append(majority_vote(ys, x))\n    return Y","f1ee12d4":"%%time\n\nY_pred = predict_all(models)","1abe38e8":"test_df[\"selected_text\"] = Y_pred","7e80a39a":"test_df.sample(10)","5a24f407":"sub_df = pd.DataFrame({\n    'textID': test_df['textID'],\n    'selected_text': Y_pred\n})","9ca401c9":"sub_df.to_csv('\/kaggle\/working\/submission.csv', index=False)","2c58f263":"sub_df.sample(10)","5f26ed36":"import shutil\nshutil.rmtree(model_dir)","f9a2e4ad":"## Data preparation","8dc37e94":"In this notebook, I treat the task of selecting a supporting phrase for a given tweet as a named entity recognition (NER) problem and train spaCy NER models.\n* No text pre-processing is necessary.\n* I train a blank spaCy model only on positive and negative sentiment.\n* As others pointed out, just using the whole text as the selection for neutral sentiment gives pretty good results, so I don't train a model on this subset.\n* I track Jaccard scores on both training and validation data. \n* I make a model ensemble using cross-validation.\n* I use majority voting to make a final prediction of the ensemble.","701eae3f":"## Predicting","03f008b3":"## Data loading","87a0057a":"## Baseline","00d8f912":"## Training","08613a67":"So, if we just use the full text field as our selected text, we can get pretty high already: ~0.6 Jaccard (mostly due to the neutral sentiment data)."}}