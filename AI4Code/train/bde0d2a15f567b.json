{"cell_type":{"818b1356":"code","337217c0":"code","cb6dc4e3":"code","329f7c67":"code","f6064632":"code","16d531dd":"code","29b81e8d":"code","4615a493":"code","4fc1524a":"code","83e983a8":"code","1a113e1d":"code","0e70bb00":"code","98b046a6":"code","38ad1798":"code","ba6c6ae2":"code","c40fe554":"code","45f1b14f":"code","7864cb53":"code","0f355a18":"code","f37ef95c":"code","c762b520":"code","33a5c148":"code","8df36d3e":"code","e4b0bcf8":"code","fac7274d":"code","fdadb988":"code","498b240e":"code","cd2b8836":"code","5e7db237":"code","603c85dd":"markdown","2d1d0d84":"markdown","91b55042":"markdown","65d1d776":"markdown","077713c1":"markdown","c41a1976":"markdown","d85fd4b1":"markdown","6731c6f8":"markdown","4217a24d":"markdown","1268b9f9":"markdown"},"source":{"818b1356":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm","337217c0":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","cb6dc4e3":"# Drop ID column for modelling\ntrain_id = train['Id']\ntest_id = test['Id']\ntrain = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)","329f7c67":"# remove outliers and confirm that they were removed correctly\nf, axs = plt.subplots(2, 1, sharex=True, sharey=True)\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, ax=axs[0])\ntrain = train[(train['GrLivArea'] < 4500) | (train['SalePrice'] > 300000)]\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, ax=axs[1])\naxs[0].set_title('Before Outlier Removal')\naxs[1].set_title('After Outlier Removal')","f6064632":"# drop features that had >40% missing values\nto_drop = ['PoolQC', 'MiscFeature', 'Alley', 'FireplaceQu']\ntrain = train.drop(to_drop, axis=1)\ntest = test.drop(to_drop, axis=1)","16d531dd":"# Define feature engineering\n\n# features to log transform\nLOG_FEATURES = ['LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'PoolArea']\n\ndef engineer_features(data):\n    #data['Age'] = data['YrSold']-data['YearBuilt']\n    #MSSubClass ordering doesn't seem like its ordering is particularly relevant, sow we will convert it to a categorical feature\n    data['MSSubClass'] = data['MSSubClass'].astype(str)\n    # We'll also convert year and month sold to strings:\n    data['YrSold'] = data['YrSold'].astype(str)\n    data['MoSold'] = data['MoSold'].astype(str)\n    # data['TotalSF'] = data['TotalBsmtSF']+data['1stFlrSF']+data['2ndFlrSF']\n    data['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    for feature in LOG_FEATURES:\n        data[feature] = np.log1p(data[feature])\n    return data","29b81e8d":"# Apply feature engineering\ntrain = engineer_features(train)\ntest = engineer_features(test)","4615a493":"# imports for modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder, StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso, RidgeCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, make_scorer\nimport xgboost\nimport lightgbm as lgb","4fc1524a":"# Split training data into features and target\nX_train, y_train = train.drop('SalePrice', axis=1), train['SalePrice']","83e983a8":"categorical = [i for i in X_train.columns if train.dtypes[i] == 'object']\nnumerical = [i for i in X_train.columns if train.dtypes[i] != 'object']","1a113e1d":"# Meaning of missing values from Serigne's notebook (https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard):\n#- For these features, NA means or probably means 'None': PoolQC, MiscFeature, Alley, Fence, FirePlaceQu, GarageType, GarageFinish, GarageQual, GarageCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2, MasVnrType, MSSubClass\n#- For these features, NA means or probably means 0: GarageArea, GarageCars, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, MasVnrArea\n#- For these features, NA means or probably means 'unknown' or undefined: MSZoning, Utilities, Electrical, KitchenQual, Exterior1st, Exterior2nd, SaleType, GarageYrBuilt\n#- For 'Functional', NA means 'Typ'\n\n# define missng values preprocessor:\nmissing_na = ['Fence', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass']\nmissing_zero = ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\nother_categorical = np.setdiff1d(categorical, missing_na)\nother_categorical = np.setdiff1d(other_categorical, ['Functional'])\nother_categorical = np.setdiff1d(other_categorical, missing_zero)\nother_numerical = np.setdiff1d(numerical, missing_na)\nother_numerical = np.setdiff1d(other_numerical, ['Functional'])\nother_numerical = np.setdiff1d(other_numerical, missing_zero)\nmissing_preprocessor = ColumnTransformer([\n    ('NA', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='NA'), missing_na),\n    ('Functional', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='Typ'), ['Functional']),\n    ('zero', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0), missing_zero),\n    ('unknown_categorical', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), other_categorical),\n    ('unknown_numerical', SimpleImputer(missing_values=np.nan, strategy='median'), other_numerical)\n])\n\n# names of columns output by missing value preprocessor:\nmissing_columns_output = np.hstack([missing_na, ['Functional'], missing_zero, other_categorical, other_numerical])","0e70bb00":"# Make encoding preprocessor\ndef col_indicies(names):\n    \"\"\"Return boolean array that is true when column name is in names\"\"\"\n    return np.isin(missing_columns_output, names)\n\nordinal = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond']\nnon_ordinal_categorical = np.setdiff1d(categorical, ordinal)\nhousing_encoder = ColumnTransformer([\n    ('quality', OrdinalEncoder(categories=[['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex']]*len(ordinal)), col_indicies(ordinal)),\n    ('LandSlope', OrdinalEncoder(categories=[['Gtl', 'Mod', 'Sev']]), col_indicies(['LandSlope'])),\n    ('BsmtExposure', OrdinalEncoder(categories=[['NA', 'No', 'Mn', 'Av', 'Gd']]), col_indicies(['BsmtExposure'])),\n    ('Functional', OrdinalEncoder(categories=[['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ']]), col_indicies(['Functional'])),\n    ('GarageFinish', OrdinalEncoder(categories=[['NA', 'Unf', 'RFn', 'Fin']]), col_indicies(['GarageFinish'])),\n    ('Fence', OrdinalEncoder(categories=[['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']]), col_indicies(['Fence'])),\n    ('BsmtFinType', OrdinalEncoder(categories=[['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']]*2), col_indicies(['BsmtFinType1', 'BsmtFinType2'])),\n    ('PavedDrive', OrdinalEncoder(categories=[['N', 'P', 'Y']]), col_indicies(['PavedDrive'])),\n    ('categorical', OneHotEncoder(handle_unknown='ignore'), col_indicies(non_ordinal_categorical))\n], remainder='passthrough')\n\nhousing_all_hot_encoder = ColumnTransformer([\n    ('categorical', OneHotEncoder(handle_unknown='ignore'), col_indicies(categorical))\n], remainder='passthrough')","98b046a6":"# Ridge\nridge = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('ridge', Ridge(alpha=11.905772393787833))\n])\nridge = TransformedTargetRegressor(ridge, func=np.log1p, inverse_func=np.expm1)\n\n\nhot_ridge = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_all_hot_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('ridge', Ridge(alpha=11.905772393787833))\n])\nhot_ridge = TransformedTargetRegressor(hot_ridge, func=np.log1p, inverse_func=np.expm1)\n","38ad1798":"# ridge hyperparameter tuning\n\"\"\"\nalphas = np.logspace(0.5, 2, 30)\nparam_grid = {\n    'regressor__ridge__alpha': alphas\n}\ntuned_ridge = GridSearchCV(ridge, param_grid, n_jobs=-1, cv=3, verbose=10, scoring='neg_mean_squared_log_error')\ntuned_ridge.fit(X_train, y_train)\nplt.figure()\nplt.semilogx(alphas, tuned_ridge.cv_results_['mean_test_score'])\nprint(tuned_ridge.best_params_)\n# best alpha: 11.905772393787833\n\"\"\"","ba6c6ae2":"# Lasso\nlasso = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('lasso', Lasso(alpha=0.000565620644760902))\n])\nlasso = TransformedTargetRegressor(lasso, func=np.log1p, inverse_func=np.expm1)\n\nhot_lasso = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_all_hot_encoder),\n    #('scale', StandardScaler(with_mean=False)),\n    ('scale', RobustScaler(with_centering=False)),\n    ('lasso', Lasso(alpha=0.000565620644760902))\n])\nhot_lasso = TransformedTargetRegressor(hot_lasso, func=np.log1p, inverse_func=np.expm1)","c40fe554":"# lasso hyperparameter tuning\n\"\"\"\nalphas = np.logspace(-3.5, -2.5, 100)\nparam_grid = {\n    'regressor__lasso__alpha': alphas\n}\ntuned_lasso = GridSearchCV(lasso, param_grid, n_jobs=-1, cv=3, verbose=10, scoring='neg_mean_squared_log_error')\ntuned_lasso.fit(X_train, y_train)\nplt.figure()\nplt.semilogx(alphas, tuned_lasso.cv_results_['mean_test_score'])\nprint(tuned_lasso.best_params_)\n# best alpha: 0.000565620644760902\n\"\"\"","45f1b14f":"# Kernel Ridge Regression\nkernel_ridge = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('kridge', KernelRidge(kernel='poly', degree=2, alpha=48.93900918477494))\n])\nkernel_ridge = TransformedTargetRegressor(kernel_ridge, func=np.log1p, inverse_func=np.expm1)","7864cb53":"# kernel ridge hyperparameter tuning\n\"\"\"\nalphas = np.logspace(0.5, 2, 30)\nparam_grid = {\n    'regressor__kridge__alpha': alphas\n}\ntuned_kernel_ridge = GridSearchCV(kernel_ridge, param_grid, n_jobs=-1, cv=3, verbose=10, scoring='neg_mean_squared_log_error')\ntuned_kernel_ridge.fit(X_train, y_train)\nplt.figure()\nplt.semilogx(alphas, tuned_kernel_ridge.cv_results_['mean_test_score'])\nprint(tuned_kernel_ridge.best_params_)\nprint(tuned_kernel_ridge.best_score_)\n# best alpha: 48.93900918477494\n\"\"\"","0f355a18":"# k-Nearest Neighbors\nknn = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('knn', KNeighborsRegressor(n_neighbors=9, weights='distance', p=1))\n])\nknn = TransformedTargetRegressor(knn, func=np.log1p, inverse_func=np.expm1)\n","f37ef95c":"# knn hyperparameter tuning\n\"\"\"\nn_neighbors = [4, 5, 6, 7, 8, 9, 10, 12, 16]\nparam_grid = {\n    'regressor__knn__n_neighbors': n_neighbors\n}\ntuned_knn = GridSearchCV(knn, param_grid, n_jobs=-1, cv=3, verbose=10, scoring='neg_mean_squared_log_error')\ntuned_knn.fit(X_train, y_train)\nplt.figure()\nplt.semilogx(n_neighbors, tuned_knn.cv_results_['mean_test_score'])\nprint(tuned_knn.best_params_)\n# best n_neighbors: 6\n\"\"\"","c762b520":"# Random Forest\nrf = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('rf', RandomForestRegressor(n_jobs=-1))\n])\nrf = TransformedTargetRegressor(rf, func=np.log1p, inverse_func=np.expm1)\n#_ = rf.fit(X_train, y_train)","33a5c148":"# Random Forest hyperparameter tuning\nparam_grid = {\n    'regressor__rf__max_depth': [10, 20, 30, 40, 50, 60, None],\n    'regressor__rf__max_features': ['auto', 'sqrt'],\n    'regressor__rf__min_samples_leaf': [1, 2, 4],\n    'regressor__rf__n_estimators': [200, 400, 600, 800, 1000, 1200]\n}\ntuned_rf = RandomizedSearchCV(rf, param_grid, n_jobs=-1, cv=3, n_iter=10)\n#tuned_rf.fit(X_train, y_train)","8df36d3e":"# xgboost\nparam_init = {\n    \"max_depth\": 5, # default: 3 only for depthwise\n    \"n_estimators\": 3000, # default: 500\n    \"learning_rate\": 0.01, # default: 0.05\n    \"subsample\": 0.5,\n    \"colsample_bytree\": 0.7,  # default:  1.0\n    \"min_child_weight\": 1.5,\n    \"reg_alpha\": 0.75,\n    \"reg_lambda\": 0.4,\n    \"seed\": 42,\n#     \"eval_metric\": \"rmse\"\n}\nparam_fit = {\n    \"xgb__eval_metric\": \"rmse\",\n    #\"xgb__early_stopping_rounds\": 500, # default: 100\n    \"xgb__verbose\": 200\n    #\"xgb__eval_set\": [(X_val, y_val)]\n}\nxgb_model = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('xgb', xgboost.XGBRegressor(**param_init))\n])\nxgb_model = TransformedTargetRegressor(xgb_model, func=np.log1p, inverse_func=np.expm1)","e4b0bcf8":"# lgbm\nparam_init = {\n    'objective': 'regression',\n    'num_leaves': 5,\n    'learning_rate': 0.05,\n    'n_estimators': 720,\n    'max_bin': 55,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'feature_fraction': 0.2319,\n    'feature_fraction_seed': 9,\n    'bagging_seed': 9,\n    'min_data_in_leaf': 6,\n    'min_sum_hessian_in_leaf': 11\n}\nlgbm_model = Pipeline([\n    ('fill_missing', missing_preprocessor),\n    ('encoding', housing_encoder),\n    ('scale', RobustScaler(with_centering=False)),\n    ('lgbm', lgb.LGBMRegressor(**param_init))\n])\nlgbm_model = TransformedTargetRegressor(lgbm_model, func=np.log1p, inverse_func=np.expm1)","fac7274d":"# define individual models to assess\nmodels = {\n    'ridge': ridge,\n    #'hot_ridge': hot_ridge,\n    'lasso': lasso,\n    #'hot_lasso': hot_lasso,\n    'kernel ridge': kernel_ridge,\n    'knn': knn,\n    'random forest': rf,\n    'xgboost': xgb_model,\n    'lgbm': lgbm_model\n}","fdadb988":"# assess models performances\nfor name, model in models.items():\n    print(f'{name}:')\n    print(np.sqrt(np.mean(cross_val_score(model, X_train, y_train, cv=4, scoring=make_scorer(mean_squared_log_error), n_jobs=-1))))","498b240e":"# Make stacked model using better performing models\nestimators = [('lasso', lasso),\n              ('ridge', ridge),\n              ('xgb', xgb_model),\n              ('lgbm', lgbm_model)]\nstacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\nprint(np.sqrt(np.mean(cross_val_score(stacking_regressor, X_train, y_train, cv=3, scoring=make_scorer(mean_squared_log_error), n_jobs=-1))))","cd2b8836":"# Refit best model on all training+validation data\nbest_model = stacking_regressor    # although the stacked model didn't have as high of CV as lasso, it did do better on the test data\n_ = best_model.fit(train.drop('SalePrice', axis=1), train['SalePrice'])","5e7db237":"# make submission test file\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = best_model.predict(test)\nsub.to_csv('submission.csv', index=False)","603c85dd":"## 5. Define how to Encode Categorical Features","2d1d0d84":"## 2. Drop Features with Excessive Missing Values","91b55042":"## 3. Feature Engineering","65d1d776":"## 1. Outlier Removal","077713c1":"## 6. Evaluate Common Machine Learning Models","c41a1976":"## 8. Generate Test Set Predictions","d85fd4b1":"## Load Data","6731c6f8":"## 4. Define how to Handle Missing Values","4217a24d":"# Modeling Ames, Iowa Housing Data\n\nOur procedure is\n\n1. Remove a few outliers from training data as defined in documentation\n2. Drop features with excessive missing values\n3. Add engineered features\n4. Make pipeline step for filling missing values according to the meaning of being missing for each feature\n5. Make pipeline steps for encoding categorical features\n5. Evaluate common machine learning models with training data\n6. Combine the best performing models using stacking\n7. Generate test data predictions from stacked model","1268b9f9":"## 7. Combine Best Performing Individual Models with Stacking"}}