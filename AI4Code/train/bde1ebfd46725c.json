{"cell_type":{"09b0ec72":"code","79246a72":"code","aa831491":"code","08f5b3fc":"code","e84dcc22":"code","2557397d":"code","6eac9d9b":"code","b72b0ba6":"code","e559ac39":"code","4b73633a":"code","503e57c0":"markdown","5c2ca653":"markdown","a1415afc":"markdown","c96a9ebf":"markdown","40cb8536":"markdown","916b7587":"markdown","2576c829":"markdown","e6cec36a":"markdown"},"source":{"09b0ec72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79246a72":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder","aa831491":"data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest_data = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\ndata.shape, test_data.shape","08f5b3fc":"y = data.SalePrice\nX_init = data.drop('SalePrice', axis=1)\nto_transform = pd.concat([X_init, test_data]) # i want to transform all features data of (train + valid, test) together\nto_transform.head()","e84dcc22":"preprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(),StandardScaler()),      # Fill missing data, Standardization\n     make_column_selector(dtype_include=np.number)),       # Select numerical features\n    (make_pipeline(SimpleImputer(strategy='constant'),OneHotEncoder(sparse=False)),  # Fill missing data, Standardization\n     make_column_selector(dtype_include=object)),          # Select categorical features\n)\ntransformed = preprocessor.fit_transform(to_transform)     # tarnsform data\ntransfomed_X = transformed[:X_init.shape[0]]       # select transformed inputs (train + valid)\ntransfomed_X_test = transformed[X_init.shape[0]:]    # select transformed inputs (test)\nX_init.shape, test_data.shape, to_transform.shape, transformed.shape","2557397d":"X = transfomed_X            # inputs of model\ninput_shape = [X.shape[1]]  # shape of inputs\n\nmodel = keras.Sequential([\n    #layers.BatchNormalization(),\n    layers.Dense(units=16, activation='relu', input_shape=input_shape),\n    layers.Dense(units=16, activation='relu'),\n    layers.Dense(units=16, activation='relu'),\n    layers.Dense(units=1),    \n])\nmodel.compile(\n    optimizer='adam', \n    loss=\"mae\",\n)","6eac9d9b":"early_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True, \n)","b72b0ba6":"history = model.fit(\n    X, y,\n    validation_split=0.05,    # because of submition i want to use most of data to train model\n    batch_size=64, \n    #callbacks=[early_stopping],\n    epochs=500,\n    verbose=0\n    )\npd.DataFrame(history.history).plot()\nplt.show()","e559ac39":"print(\"Minimum validation loss: {}\".format(min(history.history['val_loss'])))","4b73633a":"test_results = model.predict(transfomed_X_test)\nsubmission = pd.DataFrame({'Id':test_data['Id'], 'SalePrice':[el[0] for el in test_results]})\nsubmission.to_csv('submission.csv', index=False)","503e57c0":"# Preprocessing","5c2ca653":"# Early stopping","a1415afc":"# Make predictions","c96a9ebf":"# Input\/Output","40cb8536":"# Import packages","916b7587":"# Load data","2576c829":"# Define a model","e6cec36a":"# Train the model"}}