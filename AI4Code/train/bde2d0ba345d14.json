{"cell_type":{"bd521872":"code","77b1a2f5":"code","879d9b36":"code","468839b1":"code","942d947c":"code","294fbe5c":"code","d661b039":"code","7919e992":"code","af0b645d":"code","6df97c97":"code","1dcc5451":"code","f24e8209":"code","77a2f4ad":"code","91f332a1":"code","9bbf2d49":"code","97a205c4":"code","5ef19479":"code","8a4b368b":"code","3e0a098c":"code","ca5ab93d":"code","2b586f15":"code","4a3bb7c2":"code","30175312":"code","04600f38":"markdown","f9201570":"markdown","7c9e1f98":"markdown","afef0ce3":"markdown","476f8676":"markdown","c5bc9aab":"markdown","a6109972":"markdown","9f2ac4dc":"markdown","9cb1b91b":"markdown","b97dc664":"markdown","731fa2f5":"markdown","6763cf1d":"markdown","dc6b5bff":"markdown","b15235ad":"markdown","c4529b5a":"markdown","4ea84fda":"markdown","0b39f377":"markdown"},"source":{"bd521872":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","77b1a2f5":"df_train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv').drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Street','MasVnrType','Id'],axis=1)\n\n\ndf_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv').drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Street','MasVnrType','Id'],axis=1)\n\n\ndf=pd.concat([df_train,df_test],keys=['train', 'test'])\ndf.head()","879d9b36":"train_num=df_train.select_dtypes(include=[np.number])\n\ntrain_cat=df_train.select_dtypes(include=[object])","468839b1":"def fillmode(x,y):\n    x[y].fillna(x[y].mode()[0],inplace=True)\n    \ndef fillmean(x,y):\n    x[y].fillna(x[y].mean(),inplace=True)\n    \n    \ntrain_num_null=train_num.columns[train_num.isnull().any()]\n\ntrain_cat_null=train_cat.columns[train_cat.isnull().any()]\n\nfor i in train_num_null:\n        fillmean(train_num,i)\n    \nfor i in train_cat_null:\n    fillmode(train_cat,i)\n\n\n\n\nsalesprice=train_num['SalePrice']\ntrain_cat = pd.concat([train_cat,salesprice], axis = 1)","942d947c":"#! pip install dython","294fbe5c":"#from dython.nominal import associations\n\n#categorical_corr=associations(train_cat, theil_u=True, figsize=(20, 20))\n#categorical_corr","d661b039":"import seaborn as sns\nf, ax = plt.subplots(figsize=(20, 20))\n\ncorr = train_num.corr()\nsns.heatmap(corr, cmap=\"Blues\", annot=True)","7919e992":"top_features = corr.index[abs(corr[\"SalePrice\"]>0.3)]\ntop_features","af0b645d":"df=df[['Neighborhood','HeatingQC','GarageType','BsmtFinType1','ExterQual','Foundation',\n       'BsmtQual','KitchenQual','GarageFinish','LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n       'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'SalePrice']]\n\n","6df97c97":"predictors=df.columns[df.isnull().any()]\n\nfor i in predictors:\n    if (i=='SalePrice'):\n        break\n    \n    if(df[i].dtype==object or i=='YearBuilt' or i=='YearRemodAdd'):\n        fillmode(df,i)\n    else:\n        fillmean(df,i)","1dcc5451":"df.head(20)","f24e8209":"df.info()","77a2f4ad":"df.isnull().sum()","91f332a1":"df = pd.get_dummies(df, columns=['Neighborhood','HeatingQC','GarageType','BsmtFinType1','ExterQual','Foundation','BsmtQual','KitchenQual','GarageFinish'])\ndf.head()","9bbf2d49":"train_data=df.loc[\"train\"]\ntrain_data.drop(train_data.tail(1).index,inplace=True)\ntest_data=df.loc[\"test\"]\n\ntest_data=test_data.drop(\"SalePrice\",axis=1)","97a205c4":"X=train_data.drop([\"SalePrice\"],axis=True)\ny=train_data[\"SalePrice\"]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","5ef19479":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\n\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\nprint(\"RMSLE: \",np.sqrt(mean_squared_log_error(y_test, y_pred)))\n\ny_model_pred=regressor.predict(test_data)\ny_model_pred=np.around(y_model_pred,0)\nprediction=np.array(y_model_pred).tolist()","8a4b368b":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom yellowbrick.regressor import residuals_plot\nfrom yellowbrick.regressor import prediction_error\n\n# Instantiate an XGBoost object with hyperparameters\nxgb_reg = xgb.XGBRegressor(max_depth=3, n_estimators=100, n_jobs=2,\n                           objectvie='reg:squarederror', booster='gbtree',\n                           random_state=42, learning_rate=0.05)\n\n# Train the model with train data sets\nxgb_reg.fit(X_train, y_train)\n\ny_pred = xgb_reg.predict(X_test) # Predictions\ny_model_pred=xgb_reg.predict(test_data)\n\ntest_output=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\noutput = pd.DataFrame({'Id': test_output.Id, 'SalePrice': y_model_pred})\noutput.to_csv(\"houseprice_submission.csv\", index=False)\n\nprint(\"\\nRMSLE: \", np.sqrt(mean_squared_log_error(y_test, y_pred)))\n\nprint(\"\\nPrediction Error Plot\")\nprint(prediction_error(xgb_reg, X_train, y_train, X_test, y_test))\n\nprint(\"\\nResiduals Plot\")\nprint(residuals_plot(xgb_reg, X_train, y_train, X_test, y_test))","3e0a098c":"from sklearn import preprocessing\nimport tensorflow as tf\nmin_max_scaler = preprocessing.MinMaxScaler()\nX = min_max_scaler.fit_transform(X)\ntest_data_output=min_max_scaler.fit_transform(test_data)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","ca5ab93d":"def model():\n    \n\n    # Create the model\n    model = tf.keras.Sequential([\n      tf.keras.layers.Dense(100,activation='relu'),\n      tf.keras.layers.Dense(100,activation='relu'),\n      tf.keras.layers.Dense(1,activation='linear'),\n    ])\n\n    # Compile the model\n    model.compile(loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n                              optimizer=tf.keras.optimizers.Adamax(learning_rate=0.01),\n                              metrics=['mse'])\n    return model\n","2b586f15":"from keras.wrappers.scikit_learn import KerasRegressor\n\nestimator = KerasRegressor(build_fn=model, epochs=200, batch_size=32, verbose=0)\n","4a3bb7c2":"from sklearn.metrics import mean_squared_log_error\nestimator.fit(X_train, y_train)\ny_pred= estimator.predict(X_test)","30175312":"from sklearn.metrics import r2_score\nprint(\"R2: \", r2_score(y_test, y_pred))\nprint(\"\\nRMSLE: \", np.sqrt(mean_squared_log_error(y_test, y_pred)))","04600f38":"**Here i used a library that calculates correlation on categorical data. For some weird reason kaggle doesnt install this, so run this command in your notebook first time running**","f9201570":"**Set XGBoostregressor with depth 3, 100 trees and a learningrate of 0.05. Plot data with yellowbrick library to visualize predicted outcome**","7c9e1f98":"### Fitting the model","afef0ce3":"### An improvement from 0.1542 to 0.1446 on root mean logarithmic square error\n### R2: 0.88\n### Future work: Go through data again and perform a different feature engineering","476f8676":"**Here use keys that were assigned to df to slice back training and testing data since cleaning and feature selection is done**","c5bc9aab":"**Run randomforest and calculate mean squared logarithmic error. Use this just as a comparison with XGBoost**","a6109972":"**Define model with kerasregressor**","9f2ac4dc":"**R2=0.896, MSLE: 0.1501**","9cb1b91b":"**Fit on testing data**","b97dc664":"### Run dataset with tensorflow keras for a better mean squared log error (MSLE)\n**- First normalize the training data**","731fa2f5":"**Go through train_num and train_cat and fill with mode or mean**","6763cf1d":"### Feature selection\n\n**divide the training data to numerical and categorical dataframes**","dc6b5bff":"### Choose directory and datasets","b15235ad":"**Heatmap for numerical data**","c4529b5a":"**Get dummies for categorical data**","4ea84fda":"**Extract correlations>0.3 from above figures from original df and then handle with missing data in this new df**","0b39f377":"**Drop columns that have lots of nulls (analysis were done later but i then dropped them from beginning)**\n**. Merge training and test data and set a key on these to work on one dataframe**"}}