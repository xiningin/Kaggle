{"cell_type":{"9a14aed2":"code","d5ec360b":"code","d4005aba":"code","a4795517":"code","791e1aac":"code","d379acbd":"code","2d5e0b2a":"code","64f73395":"code","4fe0a495":"code","47a5ffd1":"code","e305643a":"code","c376d3c8":"code","5fff4ce0":"code","550d2a77":"code","6ff09fd5":"code","6314c816":"code","2ee07671":"code","2b738fcc":"code","aa73d2c1":"code","f695a0c7":"code","0ef29d68":"code","f4660a31":"code","529c881a":"code","875ec4b1":"code","ff72abeb":"code","eeab5bd7":"code","b6e075f9":"code","e87fc288":"code","6bc43d8d":"code","80bb0cbd":"code","6b0f568f":"code","99d55b41":"code","ca9709d7":"code","dc623f2e":"code","68e4b840":"code","8e1f84d5":"code","b1bd6b88":"code","1ca65331":"code","d76582a5":"code","d462e3e4":"code","f9964ffe":"code","3da8d32d":"code","f95b833c":"code","52dab9e4":"code","554886c1":"code","ab978cfd":"code","419cc0f7":"code","f34b3e7f":"markdown","5420f8d7":"markdown","8c4299de":"markdown","fca38553":"markdown","9f334b03":"markdown","c5f0eef2":"markdown","1cfa1c9c":"markdown","f4ca91c0":"markdown","400dc247":"markdown","3132f097":"markdown","796c1bab":"markdown","17367d4c":"markdown","1988f604":"markdown","bead82cd":"markdown","00de17d5":"markdown","b2f79501":"markdown","72432488":"markdown","6b88ef84":"markdown","5e8894f2":"markdown","117f15d1":"markdown","6c92face":"markdown","b9c0099c":"markdown","cef7d15b":"markdown","4c555255":"markdown"},"source":{"9a14aed2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5ec360b":"import pandas as pd\n# Sometimes the scientific numbers get on my nerves @#$#$#%&e07 \npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport numpy as np\nimport plotly\nfrom plotly import tools\n\n\ndf = pd.read_csv(\"..\/input\/amex-competition\/dataset\/train.csv\")\ndf.head()","d4005aba":"df.describe()","a4795517":"df.info()","791e1aac":"print('Number of nulls per column')\ndf[df.columns[df.isnull().any()]].isnull().sum()","d379acbd":"print('Percentage of nulls per column')\ndf[df.columns[df.isnull().any()]].isnull().sum() * 100 \/ df.shape[0]","2d5e0b2a":"print(f'Total dataframe values {df.size} with unique values as below :')\nfor c in df.columns.to_list():\n    if df[c].dtype == object and isinstance(df.iloc[0][c], str):\n        print(f'{c} : {df[c].nunique()} : Sample : {df[c].unique()[:3]}')","64f73395":"import plotly.express as px\ns = df['credit_card_default']\nfig = px.pie(s, values=s.value_counts().values, names=s.value_counts().index)\nfig.update_traces(hoverinfo='label', textinfo='value+percent')\nfig.show()","4fe0a495":"dfp = df.groupby(['gender','credit_card_default']).size().reset_index(name='gender_default')\nfig = px.bar(dfp,\n             x=\"gender\",\n             y=\"gender_default\",\n             color='credit_card_default',\n             barmode='stack')\nfig.show()","47a5ffd1":"print('Percentage of defaults for each gender')\ntable = pd.pivot_table(df, values='customer_id', index=['gender'],\n                    columns=['credit_card_default'], aggfunc=\"count\",margins=True)\ntable2 = table.div( table.iloc[:,-1], axis=0 )\ntable2","e305643a":"df[['occupation_type', 'credit_card_default']].groupby(['occupation_type']).agg(['count'])","c376d3c8":"print('Percentage of defaults for each occupation')\ntable = pd.pivot_table(df, values='gender', index=['occupation_type'],\n                    columns=['credit_card_default'], aggfunc=\"count\",margins=True)\n\ntable2 = table.div( table.iloc[:,-1], axis=0 )\ntable2\n","5fff4ce0":"import plotly.express as px\nfig = px.histogram(df, x=\"credit_limit_used(%)\",  color=\"credit_card_default\", marginal=\"rug\",\n                   hover_data=df.columns)\nfig.show()","550d2a77":"import seaborn as sb\nsb.histplot(data=df, x='credit_score', hue=\"credit_card_default\", multiple=\"stack\")","6ff09fd5":"sb.scatterplot(data=df, x=\"credit_score\", y=\"credit_limit_used(%)\", hue=\"credit_card_default\")","6314c816":"    \nsb.histplot(data=df, x='prev_defaults', hue=\"credit_card_default\", multiple=\"stack\")","2ee07671":"sb.histplot(data=df, x='default_in_last_6months', hue=\"credit_card_default\", multiple=\"stack\")","2b738fcc":"df['customer_id'].is_unique","aa73d2c1":"df[(df['credit_limit_used(%)']>70) & (df['credit_score']<700) & (df['credit_score']>150)]['credit_card_default'].value_counts()","f695a0c7":"#Feature engineering the new column - although i thimk logistic regression would have taken care of this \ndef f(row):\n    if (row['credit_limit_used(%)'] >70) & (row['credit_score']<700):\n        val = 1\n    else:\n        val = 0\n    return val\n\ndf['c_defaultchance'] = df.apply(f, axis=1) \n","0ef29d68":"df.info()","f4660a31":"#Simple Regression clasifier\ndf2 = df.drop(['customer_id','name'], axis = 1)\ndf2['gender'].replace('F',1, inplace=True)\ndf2['gender'].replace('M',0, inplace=True)\ndf2['gender'].replace('XNA',0, inplace=True)\n\ndf2['owns_car'].replace(['Y','N'],[1,0],inplace=True)\ndf2['owns_house'].replace(['Y','N'],[1,0],inplace=True)\n\nfor i in df2.columns[df2.isnull().any(axis=0)]:     #---Applying Only on variables with NaN values\n    df2[i].fillna(df2[i].mean(),inplace=True)","529c881a":"#df2 = df2.drop(df2[df2.gender == 'XNA'].index)\n#df2 = df2.drop(df2[df2.no_of_days_employed > 365000].index)\n\ndf2.loc[df['no_of_days_employed'] > 365000, 'no_of_days_employed'] = 2454.371\ndf2.loc[df['net_yearly_income'] > 1009012, 'net_yearly_income'] = 1009012.371\ndf2.loc[df['credit_limit'] > 259012, 'credit_limit'] = 259012.371\n\ndf2['credit_score'] = 1000 - df2['credit_score']\n\ndf2[\"gender\"] = pd.to_numeric(df2[\"gender\"])","875ec4b1":"#https:\/\/towardsdatascience.com\/beware-of-the-dummy-variable-trap-in-pandas-727e8e6b8bde\ndf3 = pd.get_dummies(df2, drop_first=True)","ff72abeb":"ax1 = df2.plot.scatter(x='no_of_children',\n                      y='no_of_days_employed',\n                      c='credit_card_default',\n                       colormap='viridis')","eeab5bd7":"#dataplot = sb.heatmap(df3.corr(), cmap=\"YlGnBu\", annot=True)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(20,20))         # Sample figsize in inches\n\n\nfrom matplotlib import cm\ninitial_cmap = cm.get_cmap('gray')\nreversed_cmap=initial_cmap.reversed()\n\nsns.heatmap(df3.corr(), annot=True, linewidths=.5, ax=ax,cmap=reversed_cmap)\n","b6e075f9":"#stratified sampling by target variable\ndf_sample = df.groupby('credit_card_default', group_keys=False).apply(lambda x: x.sample(1000))\n\ng=sns.pairplot(df_sample, hue=\"credit_card_default\",plot_kws={'alpha':0.6})\ng.fig.set_size_inches(30,30)\n","e87fc288":"import sklearn as sk\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot","6bc43d8d":"#https:\/\/stackabuse.com\/classification-in-python-with-scikit-learn-and-pandas\/\ny = df3.credit_card_default\nX = df3.drop(['credit_card_default'], axis = 1)\n\nmodel = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced').fit(X, y)\n\ny_pred=model.predict(X)\n\nround(model.score(X,y), 4)\n\n# get importance\nname  = model.coef_[0]\nroll_no = X.columns.to_list()\nzipper = zip(name, roll_no)\n\nres = sorted(zipper, key = lambda x: x[0],reverse=True)\n\nfor i, (name, age) in enumerate(res):\n    print(i, round(name,3), age)","80bb0cbd":"from sklearn import metrics\nprint('Final Score : ' + str(100*(metrics.f1_score(y, y_pred, average= \"macro\" ))))","6b0f568f":"#from sklearn.datasets import load_breast_cancer\n#X, y = data = load_breast_cancer(return_X_y=True)","99d55b41":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8675309)","ca9709d7":"# Compare Algorithms\nimport pandas\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.metrics import f1_score\nscoring = make_scorer(f1_score, average='macro')\n\n\n\n# load dataset\n#url = \"..\/input\/testdiabetescsv\/pima-indians-diabetes.data.csv\"\n#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n#dataframe = pandas.read_csv(url, names=names)\n#array = dataframe.values\n#X = array[:,0:8]\n#Y = array[:,8]\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear',class_weight='balanced', max_iter=1000))) # Bug : https:\/\/stackoverflow.com\/questions\/65682019\/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0,class_weight='balanced')))\nmodels.append(('XGB', XGBClassifier(eval_metric='mlogloss',use_label_encoder=False)))\nmodels.append(('ADA',AdaBoostClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    #kf = KFold(n_splits=10, random_state=seed,shuffle=True)\n    kf = StratifiedKFold(n_splits=10, random_state=seed,shuffle=True)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kf, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    model.fit(X,y)\n    y_pred = model.predict(X)\n    print('Final Score : ' + str(100*(metrics.f1_score(y, y_pred, average= \"macro\" ))))\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\n","dc623f2e":"X.shape","68e4b840":"y.shape","8e1f84d5":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nparam_grid = { \n    'n_estimators': [200],\n    'max_features': ['auto'],\n    'max_depth' : [4,5],\n    'criterion' :['gini']\n}\n","b1bd6b88":"from sklearn.model_selection import GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nrfc = RandomForestClassifier()\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\n\n","1ca65331":"\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)","d76582a5":"rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=5, criterion='gini')\nrfc1.fit(X_train, y_train)","d462e3e4":"y_pred=rfc1.predict(X_test)","f9964ffe":"print('Final Score : ' + str(100*(metrics.f1_score(y_test, y_pred, average= \"macro\" ))))","3da8d32d":"import pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]} \nparam_grid = {'C': [0.1], 'gamma': [1]} \ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n#preds = grid.fit(X_train,y_train).predict(X_test)\n\nprint('SVM Train accuracy %s' % model.score(X_train, y_train)) \nprint('SVM Test accuracy %s' % accuracy_score(y_pred, y_test)) \n\nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred)) ","f95b833c":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\n\n#===========================================================================\n# parameters for keras\n#===========================================================================\ninput_dim   = len(X_train.columns) # number of neurons in the input layer\nn_neurons   = 20            # number of neurons in the first hidden layer\nepochs      = 10           # number of training cycles\n\n#===========================================================================\n# keras model\n#===========================================================================\nmodel = Sequential()         # a model consisting of successive layers\n# input layer\nmodel.add(Dense(n_neurons, input_dim=input_dim, activation='relu'))\n# output layer, with one neuron\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#===========================================================================\n# train the model\n#===========================================================================\nmodel.fit(X_train, y_train, epochs=epochs, verbose=1)\n","52dab9e4":"from sklearn.ensemble import RandomForestRegressor\nimport xgboost\nimport pandas as pd\nimport numpy as np\nfrom boruta import BorutaPy","554886c1":"\nrcf = RandomForestRegressor()\nrcf.fit(X_train, y_train)\n\nbase_score = rcf.score(X_test, y_test)","ab978cfd":"boruta_selector = BorutaPy(rcf, n_estimators='auto', verbose = 2)\n\nboruta_selector.fit(np.array(X_train), np.array(y_train))\nX_train = boruta_selector.transform(np.array(X_train))\nX_test = boruta_selector.transform(np.array(X_test))\n\nrcf = RandomForestRegressor()\nrcf.fit(X_train, y_train)\n\nnew_score = rcf.score(X_test, y_test)\n\nchange = (new_score -base_score)*100\/base_score\nprint(f'Your model accuracy has been affect by {change} percent')","419cc0f7":"y = df3.credit_card_default\nX = df3.drop(['credit_card_default'], axis = 1)\n\nmodel = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced',max_iter=1000).fit(X, y)\n\ny_pred=model.predict(X)\n# get importance\ncoeff  = model.coef_[0]\ncols = X.columns.to_list()\nzipper = zip(cols, coeff)\n\nres = sorted(zipper, key = lambda x: abs(x[1]),reverse=True)\n\nfor i, (cols, coeff) in enumerate(res):\n    #print(i, round(coeff,3), cols)\n    pass\n    \nremoval_list = [cols for [cols,coeff] in res if abs(coeff)< 0.01 ]\n\nprint(removal_list)\n\nprint('Final Score : ' + str(100*(metrics.f1_score(y, y_pred, average= \"macro\" ))))\n\n\n\nX = df3.drop(['credit_card_default'], axis = 1)\nX = X.drop(removal_list, axis = 1)\nmodel = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced',max_iter=1000).fit(X, y)\n\ny_pred=model.predict(X)\n\nprint('Final Score : ' + str(100*(metrics.f1_score(y, y_pred, average= \"macro\" ))))","f34b3e7f":"Phew!! you can look at the imbalance ~3 to 41, <br>\nWell atleast one good thing, only ~4 in 45 people default as per the statistic","5420f8d7":"* References : \n* #https:\/\/www.hackerearth.com\/challenges\/competitive\/amexpert-code-lab\/machine-learning\/credit-card-default-risk-5-95cbc85f\/\n* #https:\/\/github.com\/rajat5ranjan\n* #https:\/\/medium.com\/analytics-vidhya\/classification-model-for-loan-default-risk-prediction-98c2cc7ef1bf\n* #https:\/\/www.kaggle.com\/janiobachmann\/predicting-grades-for-the-school-year\/notebook\n* #https:\/\/medium.com\/geekculture\/python-seaborn-statistical-data-visualization-in-plot-graph-f149f7a27c6e\n* #https:\/\/www.kaggle.com\/kanncaa1\/plotly-tutorial-for-beginners#Scatter\n* \n* #https:\/\/www.kaggle.com\/getting-started\/114661\n* #https:\/\/github.com\/ShuaiW\/kaggle-classification\n* #https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n* #https:\/\/www.kaggle.com\/sociopath00\/random-forest-using-gridsearchcv\n* #https:\/\/www.kaggle.com\/general\/200076\n* #https:\/\/www.ritchieng.com\/machine-learning-evaluate-classification-model\/\n* #https:\/\/medium.com\/swlh\/feature-importance-hows-and-why-s-3678ede1e58f\n* \n* # AutoFeat Engineering\n* #https:\/\/www.kaggle.com\/vbmokin\/titanic-autofeat-automatic-fe","8c4299de":"See that XNA, who would have though of that\n\n\nAlso its alos a good idea to get to know about the balance of the dataset, generally the better balance we have it the better for algorithms else the bais is clearly seen in predictions, if not catered for early on.\n\nAlso I am not an expert in plotly, you can find more information [Kaggle page](https:\/\/www.kaggle.com\/saurav9786\/plotly-tutorial-for-everyone)","fca38553":"Hmmm !!! all the defaults are concentrated in this region ","9f334b03":"Males being less in number still have higher default percentage **Ouuch!!!**","c5f0eef2":"Hmm lots of columns, Although column names seems to be self explanatory, a formal description is in order \n\n<table border=\"1\" align=\"left\">\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t<p><strong>Column name<\/strong><\/p>\n\t\t\t<\/td>\n\t\t\t<td>\n\t\t\t<p><strong>Description<\/strong><\/p>\n\t\t\t<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>customer_id<\/td>\n\t\t\t<td>Represents the unique identification of a customer<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>name<\/td>\n\t\t\t<td>Represents the name of a customer<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>age<\/td>\n\t\t\t<td>Represents the age of a customer ( in years )&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>gender<\/td>\n\t\t\t<td>Represents the gender of a customer( F means Female and M means Male &nbsp;)<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>owns_car<\/td>\n\t\t\t<td>Represents whether a customer owns a car ( Y means Yes and N means No &nbsp;)&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>owns_house<\/td>\n\t\t\t<td>Represents whether a customer owns a house ( Y means Yes and N means No &nbsp;)<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>no_of_children<\/td>\n\t\t\t<td>Represents the number of children of a customer&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>net_yearly_income<\/td>\n\t\t\t<td>Represents the net yearly income of a customer ( in USD )&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>no_of_days_employed<\/td>\n\t\t\t<td>Represents the no of days employed<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>occupation_type<\/td>\n\t\t\t<td>Represents the occupation type of a customer ( IT staff, Managers, Accountants, Cooking staff, etc )<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>total_family_members<\/td>\n\t\t\t<td>Represents the number of family members of a customer<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>migrant_worker<\/td>\n\t\t\t<td>Represents whether a customer is a migrant worker( 1 means Yes and 0 means No &nbsp;)&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>yearly_debt_payments<\/td>\n\t\t\t<td>Represents the yearly debt payments of a customer &nbsp;( in USD )<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>credit_limit<\/td>\n\t\t\t<td>Represents the credit limit of a customer &nbsp;( in USD )&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>credit_limit_used(%)<\/td>\n\t\t\t<td>Represents the percentage of credit limit used by a customer<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>credit_score<\/td>\n\t\t\t<td>Represents the credit score of a customer<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>prev_defaults<\/td>\n\t\t\t<td>Represents the number of previous defaults<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>default_in_last_6months<\/td>\n\t\t\t<td>Represents whether a customer has defaulted in the last 6 months ( 1 means Yes and 0 means No &nbsp;)&nbsp;<\/td>\n\t\t<\/tr>\n\t\t<tr>\n\t\t\t<td>credit_card_default<\/td>\n\t\t\t<td>Represents whether there will be credit card default &nbsp;( 1 means Yes and 0 means No &nbsp;)&nbsp;<\/td>\n\t\t<\/tr>\n\t<\/tbody>\n<\/table>","1cfa1c9c":"<h4>Check String Values<\/h4>\nNumbers are always easy to handle, its the text that causes more problems, it would always be a good idea to go over those and these bad boys identify themselves before they can cause damange.\n\nBelow code would quickly run over string columns and show us the details of those columns ","f4ca91c0":"Lower skills have a higher default rates, converting these into ordinal data would be a good idea , but then where would unkowns fit. for now we can use the universl dummies and get along with it\nIT Staff has the lowest percentage defaults. ","400dc247":"Again loads of information from here , but the important information that : \n* credit_limit_used\n* credit score","3132f097":"Created a new feature to store this **knowledge**","796c1bab":"Lets first identify the nulls right away, Here we would see what can be done with these columns :\n* Simplest way is to remove ( but wheres the fun in that !!)\n* One way is to impute them ( Simpleimputer - Mean - Median - most frequent etc )  \n* if these are critical columns ( that you would identify based on feature importance later we may use more statistical methods )\n\nMore details [towardsdatascience](https:\/\/towardsdatascience.com\/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779) & [MachinelearningMastery](https:\/\/machinelearningmastery.com\/statistical-imputation-for-missing-values-in-machine-learning\/)\n\nAs we are just doing EDA here, lets just continue","17367d4c":"<h3>Few - Things to note<\/h3>\n\n* First of all Nine children :)\n* Second yearly income of 140 Million , holy cow!! sus\n* Number of days employed 365252 that is nearly 1000 years \n* Credit limit of 31 Million, seems this should be for 140M guy(or gal )","1988f604":"<h1>Exploratory Data Analysis<\/h1>","bead82cd":"**The how** : We are going to stucture this analysis in the below \n* Extract the data & general information\n* Visualize the data and outliers\n* Data transformation\n* Data analysis and Visualization\n* Data Cleaning\n* Conclusion","00de17d5":"Seems once you default, you are very likely to **default again !!!!**","b2f79501":"## Classification Model for Loan Default (AMEX Dataset)","72432488":"**Almost Seperable !!!!** this is an amazing discovery right there my man!!!","6b88ef84":"This is my favorite plot, many things you can tell from this \n* Laborers and Drivers are mostly males \n* Mostly males owns cars\n* Credit limit & yearly debt payments is highly corelated with Yearly income \n* number of family members and no of children is highly corelated \n\nAnd most importantly , credi tdefault depends on below columns numbers :\n 14  credit_limit_used(%)     \n 15  credit_score             \n 16  prev_defaults           \n 17  default_in_last_6months  \n 19  c_defaultchance  ","5e8894f2":"<h3>Modelling<h3\/>","117f15d1":"Well, plotly is amazing but heavy, I see the page lagging now, I am going to continue with seaborn, but you get the idea now.\n\nAlso do you see defaults mostly seem to happen when credit limit exceeds 70% \n\n**So make sure you do not reach that stage** , if you do , remember its a warning sign","6c92face":"I added another featuere based on the above analysis that defaults are mostly when credit_score < 700 and credit limit used > 70","b9c0099c":"Where there is no gender inequality here, **Go Ladies !!!!** <br>\nSeems males are only half of the total polutation","cef7d15b":"![image.png](attachment:2d381a24-5637-4324-9b3a-c24749ef9020.png)\n\n**The what** : We are all familier that banks provide loans to individuals, the individual gets financial help he needs while the banks get interest. The individual repays the loan + intrest and story ends when the full loan is repaid. Their is one critical **if** here, **what if** the loan is not repaid. And that is why we have this dataset.\n\n**The why** : Not being able to repay the loan, be it Unintentional or Intentional has ramifications to both the individual and the bank. The individual suffers as the credit score gets hit, which is a fancy way of saying he will not get any credit ( student loans \/ car loans etc) and rental accomodation etc. The bank suffers as well as there are additional costs and challenges involved, the loan needs to be restructured etc. It may end up as an NPA to the bank (Non-performing Asset) which hurts the bank and its stakeholders.\n\n","4c555255":"And also make sure that you improve your **credit score beyond 700**\n\nBot the above graphs have made me curious, seems default is strongly a function of score and credit limit used "}}