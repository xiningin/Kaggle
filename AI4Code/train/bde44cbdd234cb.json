{"cell_type":{"9c67c3d5":"code","78a15851":"code","cccd8619":"code","7268ba29":"code","2aef66f8":"code","29907b3c":"code","fcfc18b7":"code","b4e2f614":"code","1900bc2c":"code","d52b06f8":"code","ca5244cb":"code","4d54defd":"code","52443ba2":"code","6af90983":"code","76b136e4":"code","c795ab20":"code","8fea39ac":"code","20633ed3":"code","5bafe82c":"code","3a8a9750":"code","bcdfeef4":"code","64c30e52":"code","35f15dac":"code","f6a96490":"code","9f0aaa8e":"code","81f37876":"code","c72a231a":"code","e85e3d92":"code","36cf8c7c":"code","45f50e96":"code","765aa116":"code","dc997b66":"code","65653f58":"code","fa02509a":"code","fb7a27a1":"code","68e05fba":"code","3824b4e6":"code","880cd3d0":"code","03229460":"code","186b8ca1":"code","6c3c4fae":"code","8568e7f2":"code","d3e27bd3":"markdown","90d73cc5":"markdown","18d86b94":"markdown","ab45db70":"markdown","0b17cb6a":"markdown"},"source":{"9c67c3d5":"%matplotlib inline","78a15851":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nimport tensorflow as tf\n\nfrom keras.utils import to_categorical\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Input, Embedding\nfrom keras.models import Model\n\nnp.random.seed(456789)","cccd8619":"movie_data = pd.read_csv(\"..\/input\/wikipedia-movie-plots\/wiki_movie_plots_deduped.csv\")","7268ba29":"movie_data.head()","2aef66f8":"movie_data.shape","29907b3c":"count_by_genre = movie_data['Genre'].groupby(movie_data['Genre']) \\\n                             .count() \\\n                             .reset_index(name='count') \\\n                             .sort_values(['count'], ascending=False) \\\n                             .head(20)","fcfc18b7":"count_by_genre","b4e2f614":"genres = ['drama', 'comedy', 'horror', 'action', 'thriller',\n          'romance', 'western', 'crime', 'adventure', 'musical',\n          'crime drama', 'romantic comedy', 'science fiction', 'mystery', 'animation']","1900bc2c":"len(genres)","d52b06f8":"case = movie_data[\"Genre\"].isin(genres)\nmovie_data_selected = movie_data[case]\nmovie_data_selected.reset_index(inplace=True)\nmovie_data_selected.shape","ca5244cb":"movie_data_selected = movie_data_selected.sort_values(\"Genre\")","4d54defd":"movie_data_selected.head()","52443ba2":"wordcounter = movie_data_selected['Plot'].apply(lambda x: x.count(' '))\nprint(\"Average number of words per plot: \", int(wordcounter.mean()))\nprint(\"Standard deviation of the words: \", int(wordcounter.std()))","6af90983":"plt.hist(wordcounter, bins='fd')\nplt.show()","76b136e4":"nltk.download('stopwords')","c795ab20":"stopwords = stopwords.words('english')","8fea39ac":"def clean_text(text):\n    '''\n    Clean a string input and prepare it for next steps.\n    '''\n    text = text.lower()\n    # Find and clear all ('s)\n    pattern_s = re.compile(\"\\'s\")\n    text = re.sub(pattern_s, '', text)\n    # Find and clear all (\\r\\n)\n    pattern_rn = re.compile(\"\\\\r\\\\n\")\n    text = re.sub(pattern_rn, '', text)\n    # Find and remove all parentheses and their contents\n    pattern_parentheses = re.compile(\"\\(.*?\\)\")\n    text = re.sub(pattern_parentheses, '', text)\n    # Find and remove punctuation and special characters\n    pattern_punct = re.compile(r\"[^\\w\\s]\")\n    text = re.sub(pattern_punct, '', text)\n    # Broke into tokens and remove stopwords\n    tokens = [w for w in text.split() if not w in stopwords]\n    # Remove short words (under 3 characters) from the tokens\n    long_words = []\n    for token in tokens:\n        if len(token) >= 3:\n            long_words.append(token)\n    # Join the tokens back together\n    cleaned_text = (\" \".join(long_words)).strip()\n    return cleaned_text","20633ed3":"# Clean the plot text and add it to the dataframe\ncleaned_plot = []\nfor plot in movie_data_selected[\"Plot\"]:\n    cleaned_plot.append(clean_text(plot))","5bafe82c":"movie_data_selected[\"cleaned_plot\"] = cleaned_plot","3a8a9750":"stemmer = PorterStemmer()\nmovie_data_selected[\"stemmed_plot\"] = movie_data_selected[\"cleaned_plot\"].str.split().apply(lambda x: ' '.join([stemmer.stem(w) for w in x]))","bcdfeef4":"movie_data_selected.head()","64c30e52":"movie_data_selected.groupby(movie_data_selected[\"Genre\"]).size()","35f15dac":"print(f\"80% of the data for training: {int(movie_data_selected.shape[0]*0.8)} samples\")\nprint(f\"10% for training and 10% for validation: {int(movie_data_selected.shape[0]*0.1)} samples each\")","f6a96490":"grouped_by_genre = movie_data_selected.groupby(movie_data_selected[\"Genre\"], group_keys=False)\n\ntrain_df = pd.DataFrame()\nval_df = pd.DataFrame()\ntest_df = pd.DataFrame()\n# Not exactly what I need, but the general idea is here\nfor g in genres:\n    train_range = int(grouped_by_genre.get_group(g).shape[0]*0.8)\n    val_range = int(grouped_by_genre.get_group(g).shape[0]*0.9)\n    train_df = train_df.append(grouped_by_genre.get_group(g).iloc[0:train_range, :])\n    val_df = val_df.append(grouped_by_genre.get_group(g).iloc[train_range:val_range, :])\n    test_df = test_df.append(grouped_by_genre.get_group(g).iloc[val_range:, :])\n# Combine in one dataframe\ncomb_df = pd.DataFrame()\ncomb_df = comb_df.append(train_df)\ncomb_df = comb_df.append(val_df)\ncomb_df = comb_df.append(test_df)","9f0aaa8e":"print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)\nprint(comb_df.shape)","81f37876":"# Initalise tokenizer with the original data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(list(comb_df[\"Plot\"]))\nsequences = tokenizer.texts_to_sequences(list(comb_df[\"Plot\"]))\nmax_len = np.max([len(sequence) for sequence in sequences])\nprint(\"Maximum length sequence is\", max_len)\nword_index = tokenizer.word_index\nprint(f\"{len(word_index)} unique tokens have been found.\")\ntoken_data = pad_sequences(sequences, maxlen=max_len, padding='post')\nprint(\"Shape of the token data tensor:\", token_data.shape)","c72a231a":"# With the cleaned data\ntokenizer_clean = Tokenizer()\ntokenizer_clean.fit_on_texts(list(comb_df[\"stemmed_plot\"]))\nsequences_clean = tokenizer_clean.texts_to_sequences(list(comb_df[\"stemmed_plot\"]))\nmax_len_clean = np.max([len(sequence) for sequence in sequences_clean])\nprint(\"Maximum length sequence is\", max_len_clean)\nword_index_clean = tokenizer_clean.word_index\nprint(f\"{len(word_index_clean)} unique tokens have been found.\")\ntoken_data_clean = pad_sequences(sequences_clean, maxlen=max_len_clean, padding='post')\nprint(\"Shape of the token data tensor:\", token_data_clean.shape)","e85e3d92":"token_data[1952]","36cf8c7c":"sanity_check_index = {v: k for k, v in tokenizer.word_index.items()}\nprint(sequences[100])\nprint(' '.join([sanity_check_index[word_index] for word_index in sequences[100]]))\nprint(token_data[100][0])\nprint(token_data[100][-1])\nprint(' '.join([sanity_check_index[word_index] for word_index in token_data[100] if word_index!=0]))","45f50e96":"train_data = token_data[0:train_range]\nval_data = token_data[train_range:val_range]\ntest_data = token_data[val_range:]\n\ntrain_labels = train_df[\"Genre\"]\nval_labels = val_df[\"Genre\"]\ntest_labels = test_df[\"Genre\"]","765aa116":"train_labels.value_counts()","dc997b66":"val_labels.value_counts()","65653f58":"test_labels.value_counts()","fa02509a":"train_labels = pd.factorize(train_labels)\nval_labels = pd.factorize(val_labels)\ntest_labels = pd.factorize(test_labels)","fb7a27a1":"train_labels = to_categorical(train_labels[0], num_classes=len(genres))\nval_labels = to_categorical(val_labels[0], num_classes=len(genres))\ntest_labels = to_categorical(test_labels[0], num_classes=len(genres))","68e05fba":"train_labels.shape","3824b4e6":"embeddings = {}\nindex = 0\nwith open ('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt') as file:\n    for embedding_line in file:\n        line_split = embedding_line.split()\n        coefs = np.asarray(line_split[1:], dtype='float32')\n        embeddings[line_split[0]] = coefs\n        index += 1","880cd3d0":"embeddings_matrix = np.zeros((len(word_index)+1, len(embeddings['a'])))\nfor word, i in word_index.items():\n    if word in embeddings:\n        embeddings_matrix[i] = embeddings[word]","03229460":"print('Word #125', sanity_check_index[125])\nprint('Index of if', word_index['if'])\nprint('Embedding in embeddings list: ', embeddings['if'][:5])\nprint('Embedding in embeddings matrix: ', embeddings_matrix[125][:5])","186b8ca1":"embedding_layer = Embedding(len(word_index)+1, \n                            len(embeddings['a']), \n                            weights=[embeddings_matrix], \n                            input_length=max_len, \n                            trainable=False)\nembedding_layer_without_GloVe = Embedding(len(word_index)+1, \n                                          len(embeddings['a']), \n                                          weights=[embeddings_matrix], \n                                          input_length=max_len)","6c3c4fae":"# Check the layer\nsequence_input = Input(shape=(max_len,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nembedding_only_Model = Model(sequence_input, embedded_sequences)\n\nprint('Manual Embeddings Result: ', [list(embeddings[sanity_check_index[x]][:3]) if sanity_check_index[x] in embeddings else [0, 0, 0] for x in sequences[500]][-5:])\nprint()\nprint('Model Embeddings Result: ', embedding_only_Model.predict(np.array(token_data[500]).reshape(1, max_len))[0, -5:, :3])","8568e7f2":"token_data[500]","d3e27bd3":"Text cleaning:\n1. Convert everything to lowercase\n2. Remove (\\\\'s)\n3. Remove (\\r\\n)\n4. Remove the text inside parenthesis ()\n5. Remove punctuations and special characters\n6. Remove stopwords\n7. Remove short words","90d73cc5":"It looks like the tokenizer with the cleaned data found a little more unique tokens than the other. Maybe for now we will use the one with the original data.","18d86b94":"### GloVe","ab45db70":"# Movie Genre Classification\n## Using Deep learning to predict the genre of a movie based on it's plot","0b17cb6a":"TODO:\n1. Sanity check of the tokenizer - DONE\n2. Train-test-split - DONE\n3. GloVe emeddings\n4. LSTM-CNN model\n"}}