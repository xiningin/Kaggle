{"cell_type":{"b9ec5062":"code","dc01d265":"code","25e00370":"code","faeec64b":"code","eeef62c8":"code","c25be870":"code","47820ffd":"code","47597388":"code","bd3b7b7c":"code","90199d8a":"code","d9f3f37f":"code","ddad4076":"code","4f49b4ac":"code","e17e5d7c":"code","02cfba53":"code","3e732eb7":"code","28faf00b":"code","f0805110":"code","9470070d":"code","10083cbf":"code","12cb4a56":"code","fe30c5ec":"code","d73651fd":"code","027fce87":"code","73f81118":"code","fee0d115":"code","a6c3d414":"code","0678dc79":"code","aad10aab":"code","7e3e773b":"code","9789275a":"code","d39960bf":"code","2f9e4f40":"code","6148361a":"code","d29baca8":"code","5218ba91":"code","53203f09":"code","94cc93e3":"markdown","9f3f3b2d":"markdown","94643273":"markdown","0e298694":"markdown","b28c8af5":"markdown","31645707":"markdown","041fddb5":"markdown","bf050946":"markdown"},"source":{"b9ec5062":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport math\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nfrom logging import getLogger, StreamHandler, FileHandler, INFO, Formatter\n\nfrom time import time\nimport datetime\nimport os\n\nimport lightgbm as lgb\nimport gc\nimport warnings\nwarnings.simplefilter('ignore')","dc01d265":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as f\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","25e00370":"trainF = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest  = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrainTs = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrainTn = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","faeec64b":"trainF.info()\nprint('\\n')\ntest.info()","eeef62c8":"display(trainF.describe())\ndisplay(trainTs.describe())\ntest.describe()","c25be870":"display(trainF.head().T)\ndisplay(trainTs.sample(6))\ndisplay(test.head())\nsub.head()","47820ffd":"print('total missing values in dataset = ', trainF.isna().sum().sum())\n#categorical features\ncat_feat = trainF.columns[trainF.dtypes == 'object'].tolist()\ncat_feat","47597388":"target_cols = [col for col in trainTs.columns if col != 'sig_id']\ntrain = trainF.merge(trainTs, on= 'sig_id')\ntrain.shape, trainTs.shape","bd3b7b7c":"target_cols = [col for col in trainTs.columns if col != 'sig_id']\nc_feats = ['cp_type', 'cp_time', 'cp_dose']\nfor feat in c_feats:\n    col = target_cols + [feat]\n    c_sumTs = train[col].groupby([feat]).sum().sum(1)\n    sns.countplot(c_sumTs) ;\n    sns.barplot(c_sumTs.index, c_sumTs.values) ;\n    plt.show()","90199d8a":"train[col+['cp_type']].groupby('cp_type').sum().sum(1)","d9f3f37f":"def cat2num(df):\n    df.loc[:, 'cp_time'] = df['cp_time'].map({24: 1, 48: 2, 72: 3})\n    df.loc[:, 'cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    return df\ntrain = cat2num(train)\ntest = cat2num(test)","ddad4076":"print('Number of different labels:', len(target_cols))\n\ntrain = train[train['cp_type']!= 1].reset_index(drop=True)\nX_test = test.copy()\ntest = test[test['cp_type']!= 1].reset_index(drop=True)\n\nnum_feat = [x for x in train.columns if x not in trainTs]\ntargets = train[target_cols].values\nprint(train.shape, test.shape, targets.shape)","4f49b4ac":"df = pd.concat([train[num_feat], test[num_feat]], axis= 0)\n\nfeatures_g = list(train.columns[4:776])\nfeatures_c = list(train.columns[776:876])\ntrain_feat = []\nhalf_g = len(features_g)\/\/2\nhalf_c = len(features_c)\/\/2\ngc_fe = ['g_sum', 'g_mean', 'g_kurt', 'g_skew', 'c_sum', 'c_mean', 'c_std','c_kurt','c_skew','gc_sum',\n         'gc_mean','gc_std','gc_kurt', 'gc_skew', 'g_initials_mean', 'g_initials_std','g_finals_mean',\n         'g_finals_std']\n#\ndf['g_sum'] = df[features_g].sum(axis=1)\ndf['g_mean'] = df[features_g].mean(axis=1)\ndf['g_std'] = df[features_g].std(axis=1)\ndf['g_kurt'] = df[features_g].kurtosis(axis = 1)\ndf['g_skew'] = df[features_g].skew(axis = 1)\ndf['c_sum'] = df[features_c].sum(axis = 1)\ndf['c_mean'] = df[features_c].mean(axis = 1)\ndf['c_std'] = df[features_c].std(axis = 1)\ndf['c_kurt'] = df[features_c].kurtosis(axis = 1)\ndf['c_skew'] = df[features_c].skew(axis = 1)\ndf['gc_sum'] = df[features_g + features_c].sum(axis = 1)\ndf['gc_mean'] = df[features_g + features_c].mean(axis = 1)\ndf['gc_std'] = df[features_g + features_c].std(axis = 1)\ndf['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\ndf['gc_skew'] = df[features_g + features_c].skew(axis = 1)\ndf['gc_skew'] = df[features_g + features_c].skew(axis = 1)\ndf['g_initials_mean'] = df[features_g[:half_g]].mean(axis=1)\ndf['g_initials_std'] = df[features_g[:half_g]].std(axis=1)\ndf['g_finals_mean'] = df[features_g[half_g:]].mean(axis=1)\ndf['g_finals_std'] = df[features_g[half_g:]].std(axis=1)\ndf['c_initials_mean'] = df[features_c[:half_c]].mean(axis=1)\ndf['c_initials_std'] = df[features_c[:half_c]].std(axis=1)\ndf['c_finals_mean'] = df[features_c[half_c:]].mean(axis=1)\ndf['c_finals_std'] = df[features_c[half_c:]].std(axis=1)\n\n\ntrain[gc_fe] = df[gc_fe].iloc[:train.shape[0],:]\ntest[gc_fe] = df[gc_fe].iloc[train.shape[0]:, :]\nnum_feat = num_feat + gc_fe\n\ndel df","e17e5d7c":"def get_logger(filename='log'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()","02cfba53":"def seed_everything(seed = 42):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(7)","3e732eb7":"import sys\nsys.path.append('\/kaggle\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","28faf00b":"folds = train.copy()\nFold = MultilabelStratifiedKFold(n_splits = 5, shuffle= True, random_state= 42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[target_cols])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype('int')","f0805110":"class TrainDataset(Dataset):\n    def __init__(self, df, num_features, labels):\n        self.cont_values = df[num_features].values\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.cont_values)\n\n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        label = torch.tensor(self.labels[idx]).float()\n        \n        return cont_x, label\n    \n\nclass TestDataset(Dataset):\n    def __init__(self, df, num_features):\n        self.cont_values = df[num_features].values\n        \n    def __len__(self):\n        return len(self.cont_values)\n    \n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        \n        return cont_x","9470070d":"class CFG:\n    max_grad_norm = 5\n    gradient_accumulation_steps=1\n    hidden_size=512\n    dropout=0.5\n    lr=1e-2\n    weight_decay=1e-6\n    batch_size=32\n    epochs= 20\n    num_features=num_feat\n    target_cols=target_cols","10083cbf":"class TabularNN(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.mlp = nn.Sequential(\n                            nn.Linear(len(cfg.num_features), cfg.hidden_size),\n                            nn.BatchNorm1d(cfg.hidden_size),\n                            nn.Dropout(cfg.dropout),\n                            nn.PReLU(),\n                            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n                            nn.BatchNorm1d(cfg.hidden_size),\n                            nn.Dropout(cfg.dropout),\n                            nn.PReLU(),\n                            nn.Linear(cfg.hidden_size, len(cfg.target_cols))\n                            )\n    def forward(self, cont_x):\n        x = self.mlp(cont_x)\n        return x","12cb4a56":"def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n    losses = AverageMeter()\n    model.train()\n    \n    for step, (cont_x, y) in enumerate(train_loader):\n        cont_x, y = cont_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n        pred = model(cont_x)\n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        losses.update(loss.item(), batch_size)\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), CFG.max_grad_norm)\n        \n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scheduler.step()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n    return losses.avg\n\ndef validate_fn(valid_loader, model, device):\n    losses = AverageMeter()\n    model.eval()\n    val_preds = []\n    \n    for epoch, (cont_x, y) in enumerate(valid_loader):\n        cont_x, y = cont_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n        pred = model(cont_x)\n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        # losses.update is a function from AverageMeter() used to accumulate the loss form all epoches\n        losses.update(loss.item(), batch_size)\n        \n        val_preds.append(pred.sigmoid().detach().cpu().numpy())\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_step\n    val_preds = np.concatenate(val_preds)\n    \n    return losses.avg, val_preds\n\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    preds = []\n    \n    for epoch, (cont_x) in enumerate(test_loader):\n        cont_x = cont_x.to(device)\n        \n        with torch.no_grad():\n            pred = model(cont_x)\n        preds.append(pred.sigmoid().detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    \n    return preds\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","fe30c5ec":"def run_single_nn(cfg, train, test, folds, num_features, target, device, fold_num=0, seed=42):\n    \n    # Set seed\n    logger.info(f'Set seed {seed}')\n    seed_everything(seed=seed)\n\n    # loader\n    trn_idx = folds[folds['fold'] != fold_num].index\n    val_idx = folds[folds['fold'] == fold_num].index\n    train_folds = train.loc[trn_idx].reset_index(drop=True)\n    valid_folds = train.loc[val_idx].reset_index(drop=True)\n    train_target = target[trn_idx]\n    valid_target = target[val_idx]\n    train_dataset = TrainDataset(train_folds, num_features, train_target)\n    valid_dataset = TrainDataset(valid_folds, num_features, valid_target)\n    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, \n                              num_workers=4, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False, \n                              num_workers=4, pin_memory=True, drop_last=False)\n\n    # model\n    model = TabularNN(cfg)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n\n    # log\n    log_df = pd.DataFrame(columns=(['EPOCH']+['TRAIN_LOSS']+['VALID_LOSS']) )\n\n    # train & validate\n    best_loss = np.inf\n    for epoch in range(cfg.epochs):\n        train_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, device)\n        valid_loss, val_preds = validate_fn(valid_loader, model, device)\n        log_row = {'EPOCH': epoch, \n                   'TRAIN_LOSS': train_loss,\n                   'VALID_LOSS': valid_loss,\n                  }\n        log_df = log_df.append(pd.DataFrame(log_row, index=[0]), sort=False)\n        \n        if valid_loss < best_loss:\n            logger.info(f'epoch{epoch} save best model... {valid_loss}')\n            best_loss = valid_loss\n            oof = np.zeros((len(train), len(cfg.target_cols)))\n            oof[val_idx] = val_preds\n            torch.save(model.state_dict(), f\"fold{fold_num}_seed{seed}.pth\")\n\n    # predictions\n    test_dataset = TestDataset(test, num_features)\n    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n                             num_workers=4, pin_memory=True)\n    model = TabularNN(cfg)\n    model.load_state_dict(torch.load(f\"fold{fold_num}_seed{seed}.pth\"))\n    model.to(device)\n    predictions = inference_fn(test_loader, model, device)\n    \n    # del\n    torch.cuda.empty_cache()\n\n    return oof, predictions\n\n\ndef run_kfold_nn(cfg, train, test, folds, num_features, target, device, n_fold=5, seed=42):\n\n    oof = np.zeros((len(train), len(cfg.target_cols)))\n    predictions = np.zeros((len(test), len(cfg.target_cols)))\n\n    for _fold in range(n_fold):\n        logger.info(\"Fold {}\".format(_fold))\n        _oof, _predictions = run_single_nn(cfg,\n                                           train,\n                                           test,\n                                           folds,\n                                           num_features, \n                                           target, \n                                           device,\n                                           fold_num=_fold,\n                                           seed=seed)\n        oof += _oof\n        predictions += _predictions \/ n_fold\n\n    score = 0\n    for i in range(target.shape[1]):\n        _score = log_loss(target[:,i], oof[:,i])\n        score += _score \/ target.shape[1]\n    logger.info(f\"CV score: {score}\")\n    \n    return oof, predictions","d73651fd":"#seed average for solid results\noof = np.zeros((len(train), len(CFG.target_cols)))\npredictions = np.zeros((len(test), len(CFG.target_cols)))\n\nSEED = [0, 1, 2]\nfor seed in SEED:\n    _oof, _predictions = run_kfold_nn(CFG, \n                                      train, test, folds, \n                                      num_feat, targets,\n                                      device,\n                                      n_fold=5, seed=seed)\n    oof += _oof \/len(SEED)\n    predictions += _predictions \/ len(SEED)\n    \nscore = 0\nfor i in range(targets.shape[1]):\n    _score = log_loss(targets[:, i], oof[:, i])\n    score += _score \/ targets.shape[1]\nlogger.info(f'saved average CV score: {score}')","027fce87":"train1 = train[['sig_id']].copy()\ntest1 = test[['sig_id']].copy()\ntrain1[target_cols] = oof\ntrain1[['sig_id']+target_cols].to_csv('oof.csv', index=False)\n\ntest1[target_cols] = predictions\ntest1[['sig_id']+target_cols].to_csv('pred.csv', index=False)","73f81118":"# Final result with 'cp_type'=='ctl_vehicle' data\nresult = trainTs.drop(columns=target_cols)\\\n            .merge(train1[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\ny_true = trainTs[target_cols].values\ny_pred = result[target_cols].values\nscore = 0\nfor i in range(y_true.shape[1]):\n    \n    _score = log_loss(y_true[:,i], y_pred[:,i])\n    score += _score \/ y_true.shape[1]\nlogger.info(f\"Final result: {score}\")\n\ndel result, y_true, y_pred\ngc.collect()","fee0d115":"sub = sub.drop(columns=target_cols).merge(test1[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission1.csv', index=False)\nsub.head()","a6c3d414":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': 3,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          #'is_unbalanced': True,\n          \"metric\": 'binary_logloss',\n          #'device': 'gpu',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'save_binary': True,\n          'num_threads': 4,\n          'random_state': 47\n         }","0678dc79":"\naccumulative_loss = 0\nskf = StratifiedKFold(n_splits = 3, random_state= 47, shuffle= True)\nprint('Execution time | Model number | logloss | new logloss | best coeff')\n\nfold_num = 0\ntrain2 = train[['sig_id']].copy()\ntest2 = test[['sig_id']].copy()\n\ntrain_folds = train[num_feat]\nfor model, target in enumerate(target_cols, 1):\n    y = train[target]\n    start_time = time()    \n    \n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(train.shape[0])\n    fold_num = 0\n    \n    for trn_idx, test_idx in skf.split(train[num_feat], y):\n        \n        trn_data = lgb.Dataset(train_folds.iloc[trn_idx], label = y.iloc[trn_idx])\n        val_data = lgb.Dataset(train_folds.iloc[test_idx], label = y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = val_data,\n                        verbose_eval=0, early_stopping_rounds= 20)\n        clf.save_model(f'lgb_{target}_{fold_num}.txt', num_iteration= clf.best_iteration)\n        clf = lgb.Booster(model_file = f'lgb_{target}_{fold_num}.txt')\n        fold_num += 1\n        oof[test_idx] = clf.predict(train_folds.loc[test_idx])\n        preds += clf.predict(test[num_feat])\n    loss = log_loss(y, oof)\n    \n    #Hacking the metrics\n    coeffs = [3, 2, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]\n    best_coeff = 0\n    best_loss = loss\n    for coeff in coeffs:\n        new_oof = oof.copy()\n        new_oof[new_oof < new_oof.mean() \/ coeff] = 0\n        new_loss = log_loss(y, new_oof)\n        if new_loss < loss:\n            preds[preds < preds.mean() \/ best_coeff] = 0\n            best_coeff = coeff\n            best_loss = new_loss\n            \n    if best_coeff:\n        preds[preds < preds.mean() \/ best_coeff] = 0\n    \n    train2[f'{target}_2'] = new_oof\n    test2[f'{target}_2'] = preds\n        \n    accumulative_loss += best_loss\n    print('{}\\t\\t{}\\t\\t{:.5f}\\t\\t{:.5f}\\t\\t{}'.format(str(datetime.timedelta(seconds = time() - start_time))[:-7],\n                                                   model, loss, best_loss, best_coeff))\n    del preds, oof, start_time, y, loss, best_loss, new_oof\n    gc.collect()\n","aad10aab":"train2.to_csv('train2.csv', index = False) \ntest2.to_csv('test2.csv', index = False)","7e3e773b":"print('Overall mean loss: {:.5f}'.format(accumulative_loss \/ 206))","9789275a":"target_cols2 = [f'{col}_2' for col in target_cols]\nsub1 = sub.drop(columns=target_cols).merge(test2[['sig_id']+target_cols2], on='sig_id', how='left').fillna(0)\nsub1.to_csv('submission2.csv', index = False)","d39960bf":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.multioutput import MultiOutputClassifier\n\nkf = KFold(n_splits = 5, random_state= 42, shuffle=True)\nn_folds = 5\n\nclf = XGBClassifier()","2f9e4f40":"df_train = pd.concat([train1, train2], axis = 1)\ndf_test = pd.concat([test1, test2], axis = 1)\ntargets = train[target_cols].values\n\ndf_train = df_train.drop(['sig_id'], axis=1)\ndf_test = df_test.drop(['sig_id'], axis= 1)","6148361a":"#stacking\noof = np.zeros(train[target_cols].shape)\naccumulative_loss = 0\ntest3 = np.zeros((test.shape[0], len(target_cols)))\nn_folds = 5\nfor target_i, target in enumerate(target_cols):\n    oof_losses = []\n    oof = np.zeros(train.shape[0])\n    y = train[target].values\n    X = df_train[[target, f'{target}_2']].values\n    X_test2 = df_test[[target, f'{target}_2']].values\n    for i, (trn_idx, test_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X[trn_idx], X[test_idx]\n        y_train, y_val = y[trn_idx], y[test_idx]\n        clf.fit(X_train, y_train)\n        val_preds = clf.predict_proba(X_val)\n\n        val_preds = np.array(val_preds)[:, 1].T \n        val_preds = val_preds.astype('float64')\n        oof[test_idx] = val_preds\n\n        preds = clf.predict_proba(X_test2)\n        preds = np.array(preds)[:,1].T\n        test3[:, target_i] += preds\/n_folds\n    oof_losses = log_loss(y, oof)\n    accumulative_loss += np.mean(oof_losses)\n    print(f'loss {target}', np.mean(oof_losses))\n    print(f'Mean OOF loss of folds {target}', np.mean(oof_losses))\n\n    del X_train, X_val, y_train, y_val, val_preds, preds\n    gc.collect()\n    ","d29baca8":"print('overall mean loss =', accumulative_loss\/206)","5218ba91":"sub3 = sub.copy()\ncontrol_mask = X_test['cp_type'] == 1\nsub3.iloc[~control_mask, 1:] = test3\nsub3.iloc[control_mask, 1:] = 0\nsub3.to_csv('submission.csv', index = False)","53203f09":"sub3.head()","94cc93e3":"## Feature engineering","9f3f3b2d":"## CV Split","94643273":"## stacking","0e298694":"## Analysing cp- features","b28c8af5":"## About this Competition\n\nscientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\nHence, our task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.\n\nBased on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.","31645707":"## References\n*  https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n*  https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter\n*  https:\/\/www.kaggle.com\/nroman\/moa-lightgbm-206-models\n*  https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\n*  https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-feature-engineering-0-01846\n","041fddb5":"***train_features.csv*** \/ ***test_features.csv*** -Features for the training set. \n<br>Features g- signify gene expression data, and \nc- signify cell viability data. \ncp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; \ncp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n<br>***train_targets_scored.csv*** - The binary MoA targets that are scored.\n<br>***sample_submission.csv*** - A submission file in the correct format","bf050946":"I would be grateful for any correction, suggestion or discussion ):"}}