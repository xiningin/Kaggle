{"cell_type":{"4857dd6d":"code","8af5ea43":"code","787dacd8":"code","bf5831b8":"code","93e6a269":"code","86aabb7d":"code","7f763922":"code","9c3d412f":"code","f5c52e9e":"code","2f0314c1":"code","146354dd":"code","e49b944d":"code","ead3d778":"code","cde93d49":"code","4f1af4b0":"code","42bcebbf":"code","75f82821":"code","2bf3b2b8":"code","9d8f4c8a":"code","81613557":"code","0b7b950b":"markdown","4d817cd9":"markdown","00146701":"markdown","526c6514":"markdown","537705bf":"markdown","b0d3a63a":"markdown","0375ba3d":"markdown","8ae0cbd7":"markdown","774ff49c":"markdown","07fc6480":"markdown","e31055ad":"markdown","9a50df00":"markdown","84f401a1":"markdown","668e8810":"markdown","b5b19916":"markdown","a4d05cd9":"markdown"},"source":{"4857dd6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n# Set random seed\nnp.random.seed(0)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8af5ea43":"data = pd.read_csv(\"\/kaggle\/input\/the-spotify-hit-predictor-dataset\/dataset-of-70s.csv\")\ndata.head()","787dacd8":" data.target.value_counts()","bf5831b8":"data = data.sample(frac=1)\ndata.head()","93e6a269":"data.info()","86aabb7d":"data.drop([\"track\",\"artist\",\"uri\"],axis=1,inplace=True)","7f763922":"unscaled_inputs = data.iloc[:,0:-1]\ntarget = data.iloc[:,[-1]]","9c3d412f":"scaled_inputs = preprocessing.scale(unscaled_inputs)","f5c52e9e":"samples_count = scaled_inputs.shape[0]\n#\ntrain_samples_count = int(0.8*samples_count)\nvalidation_samples_count = int(0.1*samples_count)\ntest_samples_count = samples_count - train_samples_count - validation_samples_count","2f0314c1":"# train:\ntrain_inputs = scaled_inputs[:train_samples_count]\ntrain_targets = target[:train_samples_count]","146354dd":"# validation:\nvalidation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = target[train_samples_count:train_samples_count+validation_samples_count]","e49b944d":"# test:\ntest_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\ntest_targets = target[train_samples_count+validation_samples_count:]","ead3d778":"# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) \/ train_samples_count)\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) \/ validation_samples_count)\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) \/ test_samples_count)","cde93d49":"# Save the three datasets in *.npz.\n# We will see that it is extremely valuable to name them in such a coherent way!\n\nnp.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('Spotify_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)","4f1af4b0":"npz = np.load('Spotify_data_train.npz')\n# we extract the inputs using the keyword under which we saved them\n# to ensure that they are all floats, let's also take care of that\ntrain_inputs = npz['inputs'].astype(np.float)\n# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\ntrain_targets = npz['targets'].astype(np.int)\n\n# we load the validation data in the temporary variable\nnpz = np.load('Spotify_data_validation.npz')\n# we can load the inputs and the targets in the same line\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# we load the test data in the temporary variable\nnpz = np.load('Spotify_data_test.npz')\n# we create 2 variables that will contain the test inputs and the test targets\ntest_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)","42bcebbf":"# Set the input and output sizes\ninput_size = 15 # count of features\noutput_size = 2 # count of targets\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 50 # counts of neurons\n    \n# define how the model will look like\nmodel = tf.keras.Sequential([\n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3nd hidden layer\n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])","75f82821":"# we define the optimizer we'd like to use, \n# the loss function, \n# and the metrics we are interested in obtaining at each iteration\n#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])","2bf3b2b8":"# That's where we train the model we have built.\n# set the batch size\nbatch_size = 300\n# set a maximum number of training epochs\nmax_epochs = 6\n\n# fit the model\n# note that this time the train, validation and test data are not iterable\nhistory = model.fit(  train_inputs, # train inputs\n                      train_targets, # train targets\n                      batch_size=batch_size, # batch size\n                      epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n                      # callbacks are functions called by a task when a task is completed\n                      # task here is to check if val_loss is increasing\n                      #callbacks=[early_stopping], # early stopping\n                      validation_data=(validation_inputs, validation_targets), # validation data\n                      verbose = 2 # making sure we get enough information about the training process\n          )  ","9d8f4c8a":"# Get training and test loss histories\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, validation_loss, 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();","81613557":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\nprint('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","0b7b950b":"### Data","4d817cd9":"## Save the three datasets in *.npz","00146701":"### Model","526c6514":"<a id = \"2\"><\/a><br>\n## Load Data\n* I will use just 70s that has much more datas.","537705bf":"<a id = \"7\"><\/a><br>\n## Create The Deep Learning Algorithm","b0d3a63a":"<a id = \"5\"><\/a><br>\n## Standardize The Inputs","0375ba3d":"### Context\n* This is a dataset consisting of features for tracks fetched using Spotify's Web API. \n* The tracks are labeled '1' or '0' ('Hit' or 'Flop') depending on some criteria of the author.\n* This dataset can be used to make a classification model that predicts whether a track would be a 'Hit' or not.\n<br>\n<br>\n<font color = 'blue'>\n1. Content\n    * [Load Libraries](#1)\n    * [Load Dataset](#2)    \n    * [Balance The Dataset](#3)\n    * [Shuffle The Data](#4)\n    * [Standardize The Inputs](#5)\n    * [Split The Dataset into Train,Validation and Test](#6)\n    * [Create The Deep Learning Algorithm](#7)\n    * [Visualize Neural Network Loss History](#8)\n    * [Test The Model](#9)","8ae0cbd7":"### Choose the optimizer and the loss function","774ff49c":"<a id = \"4\"><\/a><br>\n## Shuffle The Data","07fc6480":"<a id = \"6\"><\/a><br>\n## Split The Dataset into Train,Validation and Test\n* 80% , 10% , 10%","e31055ad":"<a id = \"9\"><\/a><br>\n## Test The Model","9a50df00":"<a id = \"8\"><\/a><br>\n## Visualize Neural Network Loss History","84f401a1":"<a id = \"1\"><\/a><br>\n## Load Libraries","668e8810":"### Drop Categorical Features","b5b19916":"<a id = \"3\"><\/a><br>\n## Balance The Dataset\n* This is a balanced dataset","a4d05cd9":"### Training"}}