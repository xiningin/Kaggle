{"cell_type":{"2f1a9645":"code","765d893b":"code","4b2e5a30":"code","bc372e87":"code","6c803493":"code","e19b70ef":"code","913af2c2":"code","b2a006a8":"code","aa9569e8":"code","78078a37":"code","5352a8e2":"code","d8fdf517":"code","1f060472":"code","9d1d7f75":"code","4408ce4e":"code","ea647742":"code","c26173b9":"code","1458a837":"code","59b3372d":"code","3a9be686":"code","928dd847":"code","c1febeef":"code","8f80b6c8":"code","659efe75":"code","da7a16c3":"code","2a458660":"code","b71e39cd":"code","693aa419":"code","b274da1e":"code","aaa7d22f":"code","40290492":"code","3a351f13":"code","b32ebf50":"code","1293d031":"code","e45d0d0a":"code","e95eee7f":"code","795f14fb":"code","efbd438c":"code","0bc4f2b5":"code","cf9630f5":"code","497382d8":"code","fb7db184":"code","9a7571a2":"markdown","2bc4d7c2":"markdown","dd89fd77":"markdown","115aa4e0":"markdown"},"source":{"2f1a9645":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport shutil\nimport matplotlib.pyplot as plt\n\nfrom keras.applications import MobileNet\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import Adam, SGD\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import imagenet_utils\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\nfrom IPython.display import Image\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,Input\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image \nfrom keras.layers.normalization import BatchNormalization\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.models import load_model\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport os\nfrom keras.preprocessing import image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical","765d893b":"#Load data file\ndata_file = pd.read_csv(\"\/kaggle\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv\")\ndata_file","4b2e5a30":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n# train_dir\n    # 10 subfolders\n# val_dir\n    # 10 subfolders\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nc0 = os.path.join(train_dir, 'c0')\nos.mkdir(c0)\nc1 = os.path.join(train_dir, 'c1')\nos.mkdir(c1)\nc2 = os.path.join(train_dir, 'c2')\nos.mkdir(c2)\nc3 = os.path.join(train_dir, 'c3')\nos.mkdir(c3)\nc4 = os.path.join(train_dir, 'c4')\nos.mkdir(c4)\nc5 = os.path.join(train_dir, 'c5')\nos.mkdir(c5)\nc6 = os.path.join(train_dir, 'c6')\nos.mkdir(c6)\nc7 = os.path.join(train_dir, 'c7')\nos.mkdir(c7)\nc8 = os.path.join(train_dir, 'c8')\nos.mkdir(c8)\nc9 = os.path.join(train_dir, 'c9')\nos.mkdir(c9)\n\n\n# create new folders inside val_dir\nc0 = os.path.join(val_dir, 'c0')\nos.mkdir(c0)\nc1 = os.path.join(val_dir, 'c1')\nos.mkdir(c1)\nc2 = os.path.join(val_dir, 'c2')\nos.mkdir(c2)\nc3 = os.path.join(val_dir, 'c3')\nos.mkdir(c3)\nc4 = os.path.join(val_dir, 'c4')\nos.mkdir(c4)\nc5 = os.path.join(val_dir, 'c5')\nos.mkdir(c5)\nc6 = os.path.join(val_dir, 'c6')\nos.mkdir(c6)\nc7 = os.path.join(val_dir, 'c7')\nos.mkdir(c7)\nc8 = os.path.join(val_dir, 'c8')\nos.mkdir(c8)\nc9 = os.path.join(val_dir, 'c9')\nos.mkdir(c9)","bc372e87":"# check that the folders have been created\nos.listdir('base_dir\/train_dir')\n","6c803493":"os.listdir('base_dir\/val_dir')","e19b70ef":"val_drivers = random.choices(data_file['subject'].unique().tolist(), k=4)\nval_drivers","913af2c2":"imgs = list(data_file['img'])\n\ni = 0\n\nfor img in imgs:\n    i += 1\n    label = data_file.loc[data_file['img'] == img, 'classname'].item()\n    if data_file.loc[data_file['img'] == img, 'subject'].item() in val_drivers:\n        # source path to image\n        src = os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/train', label, img)\n        # destination path to image\n        dst = os.path.join(val_dir, label, img)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n    else:\n        # source path to image\n        src = os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/train', label, img)\n        # destination path to image\n        dst = os.path.join(train_dir, label, img)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        \n    if i%1000 == 0:\n        print(i)\nprint(i)","b2a006a8":"# check how many train images we have in each folder\nval = len(os.listdir('base_dir\/train_dir\/c0')) + len(os.listdir('base_dir\/train_dir\/c1')) + len(os.listdir('base_dir\/train_dir\/c2')) + len(os.listdir('base_dir\/train_dir\/c3')) + len(os.listdir('base_dir\/train_dir\/c4')) + len(os.listdir('base_dir\/train_dir\/c5')) + len(os.listdir('base_dir\/train_dir\/c6')) + len(os.listdir('base_dir\/train_dir\/c7')) + len(os.listdir('base_dir\/train_dir\/c8')) + len(os.listdir('base_dir\/train_dir\/c9'))\n\nprint(len(os.listdir('base_dir\/train_dir\/c0')) + len(os.listdir('base_dir\/train_dir\/c1')) + len(os.listdir('base_dir\/train_dir\/c2')) + len(os.listdir('base_dir\/train_dir\/c3')) + len(os.listdir('base_dir\/train_dir\/c4')) + len(os.listdir('base_dir\/train_dir\/c5')) + len(os.listdir('base_dir\/train_dir\/c6')) + len(os.listdir('base_dir\/train_dir\/c7')) + len(os.listdir('base_dir\/train_dir\/c8')) + len(os.listdir('base_dir\/train_dir\/c9')))\nprint(len(os.listdir('base_dir\/val_dir\/c0')) + len(os.listdir('base_dir\/val_dir\/c1')) + len(os.listdir('base_dir\/val_dir\/c2')) + len(os.listdir('base_dir\/val_dir\/c3')) + len(os.listdir('base_dir\/val_dir\/c4')) + len(os.listdir('base_dir\/val_dir\/c5')) + len(os.listdir('base_dir\/val_dir\/c6')) + len(os.listdir('base_dir\/val_dir\/c7')) + len(os.listdir('base_dir\/val_dir\/c8')) + len(os.listdir('base_dir\/val_dir\/c9')))","aa9569e8":"#Load val Images\n        \nval_image = []\nval_label = []\n\nfor i in range(10):\n    print('Loading images from folder C',i)\n    imgs = os.listdir('base_dir\/val_dir\/c' + str(i))\n    for j in range(len(imgs)):\n\n        img_name = 'base_dir\/val_dir\/c'+str(i)+\"\/\"+imgs[j]\n        img = cv2.imread(img_name)\n        img = cv2.resize(img, (224,224))\n        val_image.append(img)\n        val_label.append(i)","78078a37":"# #Load Images\n# import cv2\n        \n# train_image = []\n# image_label = []\n\n# for i in range(10):\n#     print('Loading images from folder C',i)\n#     imgs = os.listdir('..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/c' + str(i))\n#     for j in range(len(imgs)):\n\n#         img_name = '..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/c'+str(i)+\"\/\"+imgs[j]\n#         img = cv2.imread(img_name)\n#         img = cv2.resize(img, (224,224))\n#         label = i\n#         driver = data_file[data_file['img'] == imgs[j]]['subject'].values[0]\n#         train_image.append([img,label,driver])\n#         image_label.append(i)\n        \n\n# import random\n# random.shuffle(train_image)","5352a8e2":"# ## getting list of driver names\n\n# D = []\n# for features,labels,drivers in train_image:\n#     D.append(drivers)\n\n# ## Deduplicating drivers\n\n# deduped = []\n\n# for i in D:\n#     if i not in deduped:\n#         deduped.append(i)\n    \n\n# ## selecting random drivers for the validation set\n# driv_selected = []\n# import random\n# driv_nums = random.sample(range(len(deduped)), 6)\n# for i in driv_nums:\n#     driv_selected.append(deduped[i])","d8fdf517":"# X_train= []\n# y_train = []\n# X_test = []\n# y_test = []\n# D_train = []\n# D_test = []\n\n# for features,labels,drivers in train_image:\n#     if drivers in driv_selected:\n#         X_test.append(features)\n#         y_test.append(labels)\n#         D_test.append(drivers)\n    \n#     else:\n#         X_train.append(features)\n#         y_train.append(labels)\n#         D_train.append(drivers)\n    \n# print (len(X_train),len(X_test))\n# print (len(y_train),len(y_test))","1f060472":"# from keras.utils import to_categorical\n# x_train, y_train = np.asarray(X_train), np.asarray(y_train)\n# x_test, y_test = np.asarray(X_test), np.asarray(y_test)\n\n# x_train = np.array(x_train, dtype=np.uint8).reshape(-1,224,224,3)\n# x_test = np.array(x_test, dtype=np.uint8).reshape(-1,224,224,3)\n\n# # x_train = x_train.astype('float32')\n# # x_test = x_test.astype('float32')\n\n# # # normalize to range 0-1\n# # x_train \/= 255.0\n# # x_test \/= 255.0\n\n# y_train = to_categorical(y_train)\n# y_test = to_categorical(y_test)","9d1d7f75":"# print(x_train[0], y_train.shape)\n# print(x_test.shape, y_test.shape)","4408ce4e":"# print('Unique drivers: ', data_file['subject'].nunique())\n# print('Number of images per driver')\n# print(data_file['subject'].value_counts())","ea647742":"# import matplotlib.pyplot as plt\n# plt.imshow(cv2.cvtColor(x_train[100], cv2.COLOR_BGR2RGB))\n# plt.show()\n# print(y_train[100])","c26173b9":"# from keras.utils import to_categorical\n# from keras.models import Sequential\n# from keras.layers import *\n# from keras.optimizers import RMSprop\n# import keras\n# from keras.callbacks import EarlyStopping\n\n\n# model = Sequential()\n\n# size = 16\n\n# model.add(Conv2D(size*2, (3, 3), padding='same', input_shape=(224,224,1)))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*2, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n# model.add(Conv2D(size*4, (3, 3), padding='same'))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*4, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n# model.add(Conv2D(size*8, (3, 3), padding='same'))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*8, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n\n# model.add(Flatten())\n# model.add(Dense(size*16))\n# model.add(Activation('elu'))\n# model.add(Dropout(0.5))\n# model.add(Dense(10))\n# model.add(Activation('softmax'))\n\n# # initiate RMSprop optimizer\n# opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n\n# # Let's train the model using RMSprop\n# model.compile(loss='categorical_crossentropy',\n#               optimizer=opt,\n#               metrics=['accuracy'])\n","1458a837":"# model.summary()","59b3372d":"# history = model.fit(x_train, y_train, \n#           validation_data=(x_test, y_test),\n#           epochs=20, batch_size=32, callbacks = EarlyStopping(monitor='val_loss', mode='min', patience=5))","3a9be686":"# plt.figure(figsize=(8,8))\n# plt.subplot(211)\n# plt.title('Cross Entropy Loss')\n# plt.plot(history.history['loss'], color='blue', label='train')\n# plt.plot(history.history['val_loss'], color='orange', label='test')\n# # plot accuracy\n# plt.subplot(212)\n# plt.title('Classification Accuracy')\n# plt.plot(history.history['accuracy'], color='blue', label='train')\n# plt.plot(history.history['val_accuracy'], color='orange', label='test')","928dd847":"# from keras.models import load_model\n\n# model.save('CNN_SF_distracted_drivers_2.h5')","c1febeef":"# _, acc = model.evaluate(x_test, y_test, verbose=10)\n# print(acc)","8f80b6c8":"# base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n\n# final_layer = base_model.output\n# final_layer = GlobalAveragePooling2D()(final_layer)\n\n# pred=Dense(10,activation='softmax')(final_layer) #final layer with softmax activation\n\n# model = Model(inputs=base_model.input, outputs=pred)\n\n# model.summary()","659efe75":"# opt = optimizers.SGD(lr = 0.005)\n# # opt = optimizers.Adam(lr=0.001)\n\n\n# model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy']) ","da7a16c3":"# filepath = \"mobilenet_sgd0.001_4_v2.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\n# datagen_val = ImageDataGenerator()\n\n# datagen_train = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30)\n\n# train_path = 'base_dir\/train_dir'\n# valid_path = 'base_dir\/val_dir'\n\n# IMAGE_SIZE = 224\n\n# train_gen = datagen_train.flow_from_directory(train_path,\n#                                         target_size=(IMAGE_SIZE,IMAGE_SIZE),\n#                                         batch_size=64,\n#                                         class_mode='categorical')\n\n# val_gen = datagen_val.flow_from_directory(valid_path,\n#                                         target_size=(IMAGE_SIZE,IMAGE_SIZE),\n#                                         batch_size=64,\n#                                         class_mode='categorical')\n\n# #datagen.fit(X_train)\n# # data_generator = datagen.flow(x_train, y_train, batch_size = 4)\n\n# # Fits the model on batches with real-time data augmentation:\n# mobilenet_model = model.fit_generator(train_gen,steps_per_epoch = val \/ 64, callbacks=[checkpoint,earlystopper],\n#                         epochs = 25, verbose = 1, validation_data = val_gen)","2a458660":"# plt.figure(figsize=(8,8))\n# plt.subplot(211)\n# plt.title('Cross Entropy Loss')\n# plt.plot(mobilenet_model.history['loss'], color='blue', label='train')\n# plt.plot(mobilenet_model.history['val_loss'], color='orange', label='test')\n# # plt.ylim(0,1)\n# # plot accuracy\n# plt.subplot(212)\n# plt.title('Classification Accuracy')\n# plt.plot(mobilenet_model.history['accuracy'], color='blue', label='train')\n# plt.plot(mobilenet_model.history['val_accuracy'], color='orange', label='test')\n# plt.ylim(0,1)","b71e39cd":"# #delete base_dir\n\n# shutil.rmtree('base_dir')","693aa419":"# val_image = np.array(val_image, dtype=np.uint8).reshape(-1,224,224,3)\n\n# val_label = to_categorical(val_label)","b274da1e":"\n# predictions = model.predict(val_image, steps=len(val_image), verbose=1)","aaa7d22f":"# pred = predictions.argmax(axis=1)\n# pred_val = val_label.argmax(axis=1)","40290492":"# cm = confusion_matrix(pred_val, pred)\n# print(cm)","3a351f13":"from keras.layers import Input\nvgg16_input = Input(shape = (224, 224, 3), name = 'Image_input')\n\n\n## The VGG model\n\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n\n#Get back the convolutional part of a VGG network trained on ImageNet\nmodel_vgg16_conv = VGG16(weights='imagenet', include_top=False, input_tensor = vgg16_input)\nmodel_vgg16_conv.summary()","b32ebf50":"#Use the generated model \n\noutput_vgg16_conv = model_vgg16_conv(vgg16_input)\n\n#Add the fully-connected layers \n\nx = Flatten(name='flatten')(output_vgg16_conv)\n\nx = Dense(10, activation='softmax', name='predictions')(x)\n\nvgg16_pretrained = Model(inputs = vgg16_input, outputs = x)\nvgg16_pretrained.summary()\n\n# Compile CNN model\nsgd = optimizers.SGD(lr = 0.005)\nvgg16_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])","1293d031":"datagen_val = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30,rescale=1.\/255)\n\ndatagen_train = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30, rescale=1.\/255)\n\ntrain_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nIMAGE_SIZE = 224\n\ntrain_gen = datagen_train.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=64,\n                                        class_mode='categorical')\n\nval_gen = datagen_val.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=64,\n                                        class_mode='categorical')","e45d0d0a":"checkpoint = ModelCheckpoint(\"vgg16_sgd0.001_4_{epoch:02d}_{val_accuracy:.2f}.h5\", monitor='val_accuracy', verbose=1, mode='max', save_freq='epoch')\nearlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\n# Fits the model on batches with real-time data augmentation:\nvgg16_model = vgg16_pretrained.fit(train_gen,steps_per_epoch = val \/ 64, callbacks=[checkpoint,earlystopper],\n                        epochs = 25, verbose = 1, validation_data = val_gen)","e95eee7f":"plt.figure(figsize=(8,8))\nplt.subplot(211)\nplt.title('Cross Entropy Loss')\nplt.plot(vgg16_model.history['loss'], color='blue', label='train')\nplt.plot(vgg16_model.history['val_loss'], color='orange', label='test')\n# plt.ylim(0,1)\n# plot accuracy\nplt.subplot(212)\nplt.title('Classification Accuracy')\nplt.plot(vgg16_model.history['accuracy'], color='blue', label='train')\nplt.plot(vgg16_model.history['val_accuracy'], color='orange', label='test')\nplt.ylim(0,1)","795f14fb":"#delete base_dir\n\nshutil.rmtree('base_dir')","efbd438c":"val_image = np.array(val_image, dtype=np.uint8).reshape(-1,224,224,3)\nval_label = to_categorical(val_label)\nprint(val_label)","0bc4f2b5":"val_image = val_image.astype('float32')","cf9630f5":"pred_valid = val_label.argmax(axis=1)\ncm1 = confusion_matrix(pred_valid, pred_valid)\nprint(cm1)","497382d8":"# model = load_model('..\/input\/state-farm-distracted-drivers-detection\/vgg16_sgd0.001_4_10_0.79.h5')\n\npredictions = vgg16_pretrained.predict(val_image, steps=len(val_image), verbose=1)\n\npred = predictions.argmax(axis=1)\npred_val = val_label.argmax(axis=1)\n\ncm = confusion_matrix(pred_val, pred)\nprint(cm)","fb7db184":"# test_data_dir = r'..\/input\/state-farm-distracted-driver-detection\/imgs\/test\/'\n\n# class_labels = [\n#     \"normal driving\",\n#     \"texting - right\",\n#     \"talking on the phone - right\",\n#     \"texting - left\",\n#     \"talking on the phone - left\",\n#     \"operating the radio\",\n#     \"drinking\",\n#     \"reaching behind\",\n#     \"hair and makeup\",\n#     \"talking to passenger\"]\n\n# file_names = np.random.choice(os.listdir(test_data_dir),50)\n\n# img_arrays = []\n\n# for file_name in file_names:\n#     img = image.load_img(os.path.join(test_data_dir, file_name), target_size=(224, 224))\n#     img_array = image.img_to_array(img)\n#     img_arrays.append(img_array)\n\n# img_arrays = np.array(img_arrays)\n# # img_arrays = img_arrays.astype('float32')\n# predictions = model.predict(np.array(img_arrays).reshape(-1,224,224,3))\n# # predictions = model.predict(img_arrays)\n\n\n# label_indexes = np.argmax(predictions, axis=1)\n# probabilities = np.max(predictions, axis=1)\n\n# for (file_name, label_index, probability) in zip(file_names, label_indexes, probabilities):\n\n#     label_with_probability = \"{}: {:.2f}%\".format(class_labels[label_index], probability * 100)\n\n#     import cv2\n\n#     image = cv2.imread(os.path.join(test_data_dir, file_name))\n\n#     cv2.putText(image, label_with_probability.upper(), (100, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n    \n#     plt.imshow(image)\n#     plt.show()","9a7571a2":"# CNN Model","2bc4d7c2":"# Exploring the dataset","dd89fd77":"# VGG16 Model","115aa4e0":"# **MobileNet model**"}}