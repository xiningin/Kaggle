{"cell_type":{"9b4fb020":"code","88c18d81":"code","223c60fb":"code","db79e79f":"code","78b171ef":"code","2686f43e":"code","8784e5e2":"code","4358be8c":"code","88297edb":"code","2928b118":"code","7cd5b3e9":"code","3a44840b":"code","6bd064b7":"code","97587e80":"code","6f51b772":"code","802af816":"code","b5603230":"code","ea8c70b8":"code","e22654be":"code","71e128a3":"code","34f4725a":"code","101f12f2":"code","97b13807":"code","7d776669":"code","f69406cb":"code","a37088a0":"code","6a4e6d64":"code","96dec0de":"code","4c5e3784":"code","7077b784":"code","06388b64":"code","f9070c6c":"code","b092bfe9":"code","48fe163d":"code","0fbdfb06":"code","00aef1f4":"code","d87b7d2b":"code","c0660771":"code","292ade16":"code","6cee59b7":"code","a3fa4099":"code","5af0f176":"code","34068f83":"code","94907649":"code","edff2fb8":"code","438e6096":"code","fca32f7a":"code","e93204be":"code","b17918a1":"code","f71f703b":"code","2add25be":"code","d969d870":"code","1bdbe9b0":"code","1c26ec82":"code","177c8755":"code","e6cd4afd":"code","2be02296":"code","46ca2fa9":"code","079d3ffa":"code","8d1c559e":"code","44fae7ab":"code","61f8fcc3":"code","c044ae39":"code","bdb6ad97":"code","96ff7a7e":"code","f397cce6":"code","d4ebee63":"code","cd17d727":"code","47b35932":"code","2b69db77":"code","be9a7a79":"code","be1099ae":"code","0363795f":"code","5288050a":"code","2e414735":"code","c93325f8":"code","264a6823":"code","edd14107":"code","d173ecd4":"code","77dc585b":"code","63309c44":"code","0029d05d":"code","626d73be":"code","0c66af6e":"code","a3233d7e":"code","51fc1e2c":"code","4ad66564":"code","f868832a":"code","d5b7e64b":"code","c17606bf":"code","741f45b6":"code","1b85d229":"code","22cdd099":"markdown","42e45fac":"markdown","e0da7f1f":"markdown","310091e9":"markdown","7e1fbbbd":"markdown","6abb90bf":"markdown","e7957aac":"markdown","f835bc43":"markdown","ff5d71da":"markdown","c6432c24":"markdown","eb364188":"markdown","a160a258":"markdown","793c93eb":"markdown","b1f23520":"markdown","c67b0841":"markdown","cd0b7553":"markdown","256bba65":"markdown","67788cda":"markdown","d678b631":"markdown","7d0f3c6d":"markdown","277c2127":"markdown","798c2bf5":"markdown","3d076f3e":"markdown","c431a797":"markdown","c4fe11a1":"markdown","dc177cb4":"markdown","3fd43830":"markdown","bd3db369":"markdown","f9ebdae1":"markdown","9393ac53":"markdown","3efee422":"markdown","39ecc926":"markdown","368d121e":"markdown","aff7f1a5":"markdown","74088dd1":"markdown","3e3f153a":"markdown","0b055bdd":"markdown","132de150":"markdown","4cc8d55d":"markdown","cf2517a5":"markdown"},"source":{"9b4fb020":"cd \/kaggle\/input","88c18d81":"# importing libs\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport datetime as dt\nimport squarify\nfrom IPython.display import Image","223c60fb":"# reading the dataset\ndata=pd.read_csv('indian-startup-funding\/startup_funding.csv',encoding='utf-8')","db79e79f":"data.head(10)","78b171ef":"data.info()","2686f43e":"#visualizing the null values for each attribute\nplt.figure(figsize=(10,5)) \nsns.heatmap(data.isnull(),cmap='magma',yticklabels=False,cbar=False)\nplt.show()","8784e5e2":"#getting the unique value in Remark attribute.\ndata['Remarks'].unique()","4358be8c":"# checking other attributes have those unicode symbol or not\n# i am specifying some the row from 150-200 for showing\ndata['Industry Vertical'].unique()[150:200]","88297edb":"#checking how many null values are their in `Amount in USD(Funding)` attribute\ndata[data['Amount in USD'].isnull()]","2928b118":"#filling null value with \"others\" for attributes other than 'Amount in USD'.\ndata[1:-2].fillna('others',inplace=True)","7cd5b3e9":"data['Amount in USD'].unique()","3a44840b":"# checking how many values are presen with value undisclosed\ndata[(data['Amount in USD']=='undisclosed')]","6bd064b7":"## checking how many values are presen with value unknown\ndata[(data['Amount in USD']=='unknown')]","97587e80":"data[(data['Amount in USD']=='Undisclosed')]","6f51b772":"data[data['Amount in USD'].isnull()]","802af816":"# drop rows with non numerical funding amount \ndata.drop(data.loc[(data['Amount in USD']=='undisclosed') | (data['Amount in USD']=='Undisclosed') | (data['Amount in USD'].isnull()) | (data['Amount in USD']=='unknown')].index,inplace=True)","b5603230":"data['Remarks'].fillna('None',inplace=True)","ea8c70b8":"#replacing unneccesary word with others\ndata['Investors Name'].replace('undisclosed','others',inplace=True)","e22654be":"data.shape","71e128a3":"#checking wheather we have processed all the null values\ndata.isnull().sum()","34f4725a":"data.dropna(inplace=True)","101f12f2":"data.shape","97b13807":"t=\"sequoia capital, rohit bansal, kunal bahl, sandeep tandon, kunal shah, zishaan hayath,\\\\\\ abhi\"\nt=re.sub(r\"\\\\+\",\"\",t)\nt","7d776669":"# defining func for the removing of unicodes from each cell of dataframe\ndef unicode(x):\n    x=re.sub(r\"\\\\x..\",\"\",x)     ## removing unicode as it contains 2 char followed by '\\x'\n    x=re.sub(r\"\\\\\",\"\",x)     ## removing the backslashes\n    x=x.lower()                   ## lower the strings\n    # print(x)\n    return x.strip()","f69406cb":"# process each cell of dataframe to remove unicode\nfor col in data.columns[1:]:\n    data[col]=data[col].apply(unicode)","a37088a0":"data","6a4e6d64":"# removing unnecessary char\ndata[\"Amount in USD\"] = data[\"Amount in USD\"].apply(lambda x:(str(x).replace(\",\",\"\").replace(\"+\",\"\").replace(r'n\/a',\"others\")))\n","96dec0de":"data=data[data['Amount in USD']!='others']\ndata['Amount in USD']=pd.to_numeric(data['Amount in USD']) #converting funding attribute to numeric(float) type ","4c5e3784":"data['Date dd\/mm\/yyyy'].unique()","7077b784":"#removing the '.' and replacing them with '\/'\ndata['Date dd\/mm\/yyyy']=data['Date dd\/mm\/yyyy'].apply(lambda x: x.replace(\".\",\"\/\"))","06388b64":"#converting some dates to their proper format\ndata['Date dd\/mm\/yyyy'].replace('22\/01\/\/2015','22\/01\/2015',inplace=True)\ndata['Date dd\/mm\/yyyy'].replace('01\/07\/015','01\/07\/2015',inplace=True)\ndata['Date dd\/mm\/yyyy'].replace('05\/072018','05\/07\/2018',inplace=True)","f9070c6c":"# getting some feature like 'date','month' and 'year' from Date.\ndata['year']=pd.to_datetime(data['Date dd\/mm\/yyyy'],format='%d\/%m\/%Y').dt.year\ndata['month']=pd.to_datetime(data['Date dd\/mm\/yyyy'],format='%d\/%m\/%Y').dt.month\ndata['day']=pd.to_datetime(data['Date dd\/mm\/yyyy'],format='%d\/%m\/%Y').dt.day","b092bfe9":"data[\"yearmonth\"] = (pd.to_datetime(data['Date dd\/mm\/yyyy'],format='%d\/%m\/%Y').dt.year*100)+(pd.to_datetime(data['Date dd\/mm\/yyyy'],format='%d\/%m\/%Y').dt.month)","48fe163d":"# checking what kind of value does the 'City Location' have\ndata['City  Location'].unique()","0fbdfb06":"# getting angel type of funding \ndata['InvestmentnType']=data['InvestmentnType'].apply(lambda x: x.split(\"\/\")[-1])","00aef1f4":"# getting the city name where we have 2 names available\ndata['City  Location']=data['City  Location'].apply(lambda x: x.split(\"\/\")[0].strip())","d87b7d2b":"## function to change those common city names\ndef map(x):\n    if x=='gurugram':\n        return 'gurgaon'\n    elif x=='bengaluru':\n        return 'bangalore'\n    else:\n        return x\ndata['City  Location']=data['City  Location'].apply(map)","c0660771":"data","292ade16":"## creating new list to having startups with their total funding\ntopfiftystart=[]\nfor startup in data['Startup Name'].unique():\n    df=data[data['Startup Name']==startup]    ## get the dataframe for each startup\n    sum=np.sum(df['Amount in USD'])         ## sum total funding of startup\n    topfiftystart.append([startup,sum])\n\nstartup=pd.DataFrame(topfiftystart,columns=['startup','Revenue'])  #converting the list to dataframe and sort them by the fundin amount\nstartup.sort_values(by='Revenue',ascending=False,inplace=True)","6cee59b7":"startup","a3fa4099":"data.shape","5af0f176":"## diving amount by 1M to show data in term of Millions\nstartup['Revenue']\/=1000000\nstartup=startup[:51]","34068f83":"## visualising the amount spend on the top 50 startups\nplt.figure(figsize=(20,50))\nax = sns.barplot(x=\"Revenue\", y=\"startup\",data=startup, palette=\"husl\",linewidth = 2,label='big')\nfor i,j in enumerate(startup[\"Revenue\"]):\n    ax.text(10, i, j, weight=\"bold\", color = 'black', fontsize =15,ha='left')\nplt.title(\"50 most revenue  startups \",fontsize=20)\nax.set_xlabel(xlabel = 'Amount(in Millions)', fontsize = 20)\nax.set_ylabel(ylabel = 'Startup company', fontsize = 30)\nplt.show()\ndel startup","94907649":"company=data.copy()","edff2fb8":"company.sort_values(by='Amount in USD',ascending=False,inplace=True) ## sort the investor according to their fundings","438e6096":"company['Amount in USD']\/=1000000\ncompany=company[company['Investors Name']!='others']\ncompany=company[:10]","fca32f7a":"# visualizing the biggest investors\nplt.figure(figsize=(15,10))\nax = sns.barplot(x=\"Amount in USD\", y=\"Investors Name\",data=company, palette=\"husl\",linewidth = 2,label='big')\nfor i,j in enumerate(company[\"Amount in USD\"]):\n    ax.text(10, i, j, weight=\"bold\", color = 'black', fontsize =15,ha='left')\nplt.title(\"10 bigest investor \",fontsize=20)\nax.set_xlabel(xlabel = 'Amount(in Millions)', fontsize = 20)\nax.set_ylabel(ylabel = 'Investors Name', fontsize = 30)\nplt.show()\ndel company","e93204be":"## visualizing the most valued industries prefered for funding \nindustry = data['Industry Vertical'].value_counts().head(10)\nplt.figure(figsize=(15,8))\nsns.barplot(industry.index, industry.values, alpha=0.9, palette=\"husl\")\nplt.xticks(rotation=45)\nplt.xlabel('Industry of startups', fontsize=12)\nplt.ylabel('freq of industries for the startup', fontsize=12)\nplt.title(\"Most valued industries for funding\", fontsize=16)\nplt.show()","b17918a1":"## investor func to name undisclosed investors to others\ndef investor(x):\n    x=x.strip()\n    if x=='undisclosed' or x=='undisclosed investors' or x=='undisclosed investor':\n        return 'others'\n    else:\n        return x\ndata[\"Investors Name\"]=data[\"Investors Name\"].apply(investor)","f71f703b":"# getting the names of investors\nnames = data[data['Investors Name']!='others'][\"Investors Name\"]","2add25be":"# creating the wordcloud to observe the most frequent investor\nwordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='cyan').generate(' '.join(names))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Frequent investors\", fontsize=35)\nplt.axis(\"off\")\nplt.show()","d969d870":"#in which sector there are most startups\nd=data[data['Industry Vertical']!='others']['Industry Vertical'].value_counts().head(6)\nexplode = (0.1, 0, 0, 0,0,0)\nfig1,ax1=plt.subplots(figsize=(20,10))\n\nax1.pie(d.values,explode=explode, labels=d.index,autopct='%1.1f%%', shadow=True, startangle=140)\nax1.axis('equal')\nplt.title(\"Famous industries of startup\",fontsize=30)\nplt.show()","1bdbe9b0":"# plot the locations according to the funding given to the starups\nplt.figure(figsize=(17,12))\nmean_amount = data.groupby('City  Location').sum()[\"Amount in USD\"].sort_values(ascending=False).head(15)\nsquarify.plot(sizes=mean_amount.values,label=mean_amount.index, value=mean_amount.values)\nplt.title('Distribution of Startups across Top cities')","1c26ec82":"corr = data.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)","177c8755":"data['Investors Name'].unique()","e6cd4afd":"data.columns","2be02296":"### aggregate the revenue as per the startup name \n## get the list of all locations \nfor i,row in data.iterrows():\n    data.at[i,'Location']=list(set(data[data['Startup Name']==row['Startup Name']]['City  Location'].values))\n    data.at[i,'Revenue']=float(np.sum(data[data['Startup Name']==row['Startup Name']]['Amount in USD'].values))\n    # print(row['Startup Name'])","46ca2fa9":"## remove the duplicate startups rows\ndata.drop_duplicates(subset='Startup Name',keep=False,ignore_index=True,inplace=True)","079d3ffa":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()","8d1c559e":"##convert location to the one-hot  encoding\nres = pd.DataFrame(mlb.fit_transform(data['Location']),columns=mlb.classes_,index=data['Location'].index)","44fae7ab":"res","61f8fcc3":"data=pd.concat([data,res],axis=1)  ## concatinating both the dataframe","c044ae39":"data","bdb6ad97":"# ## splitting the names of invstors separated by ','\n# data['investors']=data['Investors Name'].apply(lambda x: set(x.split(',')))","96ff7a7e":"# ## creating vocab for the unique names of investors\n# investor=[]\n# for each in data['investors']:\n#     for i in each:\n#         if i!=\"\":\n#             investor.append(i.strip())\n\n# investor=list(set(investor))\n# print(investor[:10])  ","f397cce6":"# # creating ranks for the names of investors vocab \n# investors={}\n# i=1\n# for inv in investor:a\n#     investors[inv]=i\n#     i+=1\n","d4ebee63":"# ## creating func val for mapping the sum of rank of investors name\n# def val(x):\n#     v=0\n#     for i in x:\n#         if i:\n#             v+=investors[i.strip()]\n#     return v","cd17d727":"# ## applying func to map the names as the sum of rank of investors name\n# data['investor val']=data['investors'].apply(val)","47b35932":"from sklearn.feature_extraction.text import TfidfVectorizer","2b69db77":"data.columns","be9a7a79":"v = TfidfVectorizer(stop_words='english',analyzer='word',max_features=500) ## vector should create vector for 800 most weightage words\nx = v.fit_transform(data['SubVertical'])","be1099ae":"data.reset_index(drop=True,inplace=True) ","0363795f":"t=x.toarray()   # converting vector list to array","5288050a":"dt=pd.DataFrame(t)","2e414735":"dt","c93325f8":"df=pd.concat([data,dt],axis=1)   ## concatenating the tf-idf vectors with prevoius dataframe","264a6823":"from sklearn import preprocessing","edd14107":"le = preprocessing.LabelEncoder()","d173ecd4":"df['Startup Name']=le.fit_transform(df['Startup Name'])\ndf['Industry Vertical']=le.fit_transform(df['Industry Vertical'])\ndf['InvestmentnType']=le.fit_transform(df['InvestmentnType'])\n","77dc585b":"## drop the non-featured attributes\ndf.drop(labels=['Date dd\/mm\/yyyy','Industry Vertical','SubVertical','Investors Name','yearmonth','Remarks','Amount in USD','Location','City  Location'],inplace=True,axis=1)","63309c44":"df","0029d05d":"df['Revenue']\/=1000000","626d73be":"Image(\"\/kaggle\/input\/images\/linear.png\",width = 600, height = 300)","0c66af6e":"Image(\"\/kaggle\/input\/images\/ridge.png\",width = 600, height = 300)","a3233d7e":"Image(\"\/kaggle\/input\/images\/alpha.png\",width = 600, height = 300)","51fc1e2c":"## importing the libraries\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nimport pickle","4ad66564":"model= Ridge(normalize=True,alpha=0.1) ## normalize the value for faster convergence while gradient descent\ncv = RepeatedKFold(n_splits=20, n_repeats=3, random_state=32)   # k-fold cross validation with k=20\nscores = cross_val_score(model,df.loc[:, df.columns != 'Revenue'],df['Revenue'], scoring='neg_median_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","f868832a":"# pickle.dump(model,open('\/kaggle\/input\/indian-startup-funding\/ridge_reg.pkl','wb'))\nwith open(\"\/kaggle\/ridge_reg.sav\",mode='wb') as model_f:\n    pickle.dump(model,model_f)","d5b7e64b":"Image(\"\/kaggle\/input\/images\/data.png\",width = 1000, height = 300)","c17606bf":"#importing lib for lasso regression\nfrom sklearn import linear_model","741f45b6":"clf = linear_model.Lasso(alpha=0.1,normalize=True)\ncv = RepeatedKFold(n_splits=20, n_repeats=3, random_state=32)\nscores = cross_val_score(clf,df.loc[:, df.columns != 'Revenue'],df['Revenue'], scoring='neg_median_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","1b85d229":"# pickle.dump(clf,open('lasso_reg.pkl','wb'))\nwith open(\"\/kaggle\/lasso_reg.sav\",mode='wb') as model_f:\n    pickle.dump(clf,model_f)","22cdd099":"Conclusion:\n* The most funded revenue startups are `Flipkart` and `Rapido ride taxi`\n* The highest funded investores are `Westbridge capital` and `softbank`.\n* The industry most preferred by investors for funding the startup are `Consumer internet` and `Technology`. The new startups should focus on these field for better funding and growth.\n* And the most preffered location for the startups is Banglore .","42e45fac":"Their are many missing values in the Revenue attribute and in `Remarks` attribute. Inorder to further process , lets see what are some non-null value in it.","e0da7f1f":"### Data exploring ","310091e9":"from above we can see that `Amount in USD` has less correlation with other attributes , so we need to find some more features for training.","7e1fbbbd":"lambda is represented as alpha in the image.\n\nAs increase the lambda the slope of the regression line reduces and becomes more horizontal and the model becomes less sensitive to variation of the independent variable X , thus slope of the line is reduced and a small change in the X would have less impact in the change of Y and we can avoid the overfitting of the model.\n\nNow the question arises how do chose the value of lambda?<br>\nAns:For this we will use k-fold cross validation.\n\n### K-fold cross validation\n\nCondider the dataset:\n\n|sr.|X|Y|\n|---|---|---|\n|1|101|2|\n|2|120|3|\n|3|135|3|\n|...|...|...|\n|...|...|...|\n|1000|256|8|\n\nLets say we divid the dataset into 5 parts i.e. k=5, so get the partion as:<br>\n1st set have datapoints from \\[1,2...200\\]<br>\n2nd set have datapoints from \\[201,202...400\\]<br>\n3rd set have datapoints from \\[401,402...600\\]<br> \n4th set have datapoints from \\[601,602...800\\]<br>\n5th set have datapoints from \\[801,802...1000\\]\n\nNow make set 1-4 as train set and 5 as the test set and do this 4 more time for other sets as well, finally we would train our model over all the combination of test and train set and we get error from each of the combination say: E1,E2,E3,E4 and E5. We aggregate those results can conclude the final value of lambda which has the less error value in k-fold cross validation.\n\nI am using sklearn for ridge regression","6abb90bf":"Now we can process those unicode symbol, here is the a example that how we regex to remove those symbols","e7957aac":"The `Amount in USD`(Funding) attribute has *undisclosed* and *unknown* value so we can remove them if they are less.","f835bc43":"While converting the `Date` attribute from object to  datetime  i get some error regaring the char ,so i check the values we have in `Date` attribute. As we can see some of the dates do not have '\/' some have '.' in it.<br>\nEx:  05\/072018  ,   22\/01\/\/2015","ff5d71da":"from above we can also see that some of the cities like gurgaon and gurugram points to same locations, so we can map it to same kind of name.","c6432c24":"To predict the Funding amount ,we need to have the features with numerical value because the system only understands the numbers.\n\nSo we have the attribute `City  Location` having the name of location for each startups. So we need to convert it to numerical value. If we look at the investors name then each of the startup have more than one investor, but need to focus more on the revenue and location. So , we will need to aggregate all the revenue for each startup and each need to list all the location for it.\nEx:\n\n|Startup|Location|revenue|\n|---|---|---|\n|byju's|banglore|2000|\n|mamearth|gurgram|3500|\n|pandos|chennai|4000|\n|byju's|gurugram|3000|\n|mamearth|gurgram|2500|\n|...|...|...|\n\n\nthen after aggregating the total revenue and list all the location we would left with:\n\n\n|Startup|Location|revenue|\n|---|---|---|\n|byju's|\\[banglore,gurugram\\]|5000|\n|mamearth|gurgram|6000|\n|pandos|chennai|4000|\n|...|...|...|\n\nThen we can convert the list of location to [one hot enconding](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) to convert them to numerical value.\n","eb364188":"So first we need to remove these unicde for making some meaninful out of it. We can remove these using `regex` pattern from string, but before removing them we need to handle the null values.","a160a258":"We still remain with null values so remove them by droping null value from dataframe","793c93eb":"Their are many cities which has 2 places mentioned , we can took one of them for making for visualizing as almost all of them contains city on left side of backslash and country on right side , so we go with cities as other locations are also the name of some cities and this is the same case for `InvestmentnType`.<br>\nEx: Seed\/ Angel Funding, here the angel and seed Funding are same type of funding , more details can be found [here](https:\/\/www.investopedia.com\/terms\/a\/angelinvestor.asp).\nSo we can remove the term seed from it.\n","b1f23520":"Their are any null value in `Amount in USD` attribute , their are many null values in other attributes, so we need to fill them as well.","c67b0841":"It can be seen from above that some of attributes have less value than others, it shows that their are some missing values. So,lets visualize which attributes have null value.","cd0b7553":"The number of non numerical value for `Amount in USD` attribute are 7 , so we can remove them.","256bba65":"### Data cleaning\nTheir are some strange words present in the dataset like: `\\xc2` ,`\\xa0` etc.. If we  search for it then we found those are the [unicode](https:\/\/www.codetable.net\/hex\/a0) symbols. <br>\nEx: \"\\xa0\" represents 'no-break space'","67788cda":"To know more about the regex one can follow over [here](https:\/\/regexr.com)","d678b631":"Their are many char in `Amount in USD` like ',' ,  '+'  and 'n\/a' , inorder to convert them to float datatype we need to remove these character.","7d0f3c6d":"Consider those red datapoints as the poitns from the training set , and the red line fit perfectly to those points but if try to predict new points(green points) then we get high variance from the red line. So we try to reduce this variance by introducing some bias to the sum of square error and get the new line which is represented by blue colour by reducing some variance.This provides a better long term prediction params. <br>\nThis introducing of some bias is also known as penalty(regularization term), <br>represented as =\n$\\lambda * {\\theta}^2$<br>\n$\\theta$=penalty to loss function<br>\n$\\lambda$ represents how severe the penalty is\n\nSo the now our new loss function would become:<br>\n$J= (y\\_given-(\\theta * X +b))^2+\\lambda * {\\theta}^2 $\n","277c2127":"We got remain with attributes : `Startup Name` ,`Industry Vertical`, `City  Location` and `InvestmentnType`. We can map the values to number like we have done with ranks dict created above for the investors name. <br>\nI am using `LabelEncoder` for this.","798c2bf5":"We can see the top 50 highest funded startups and Flipkart is one of the biggest startup with the highest funded amount.\n\nNow will see who are the biggest investors in the market.","3d076f3e":"Their are many null values in Funding attribute , i explore some of the sites to collect them like [crunchbase](https:\/\/www.crunchbase.com\/) , but we have need the details regarding the investors , that how much a particular investor has fund in a particular startup , but the site does not contains it ,instead it contains the total funding a particular startup has , and the specific amount is not available on the other sites as well. Their are many ways to fill those values :\n* By filling them with 0\n* By filling them with avg <br>\n\nBut those would not help much , because the number of missing values are 1\/3 of the total dataset and we try the model to predict the funding amount it would not work properly because we would have a lot of variance in it.So , instead we need to drop it for better prediction of funding amount.\n","c431a797":"From above we can see that the overall Mean absolute error by Lasso Regression is less than the Ridge Regression.","c4fe11a1":"We need to now map the `Subvertical` to numerical value.\nAs it contains small discription of the Industry ,so we need to extract some important terms which has more value in a sentence.<br>\nEx: Online Marketplace For Mother and Babycare Products<br>\nHere can see the word babycare , Marketplace and mother has more weightage than other words. One of the example is `google search engine`.<br>\nTo find this weightage of term we are using TF*IDF algo.<br>\n1. TF(term frequencey)(t,d)=count of word t in the document(d)\/number of words in d <br>\n2. IDF(inverse document frequencey)= $\\log (\\frac{N}{df+1})$<br>\nwhere N=occurence of term t in document set N <br>\ndf=occurences of term t in document set N.\n\nTF-IDF=TF(t,d) x $\\log (\\frac{N}{df+1})$<br>\nFor more details click [here](https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n\nI am using Sklearn for creating TF-IDF vectors.","dc177cb4":"\n## Training model\n\nAs we need to predict the `Amount in USD` which is continuous value ,we can predict them with regression. Consider this Image:","3fd43830":"### Visualization\nTo answer our first question that **what are top 50 funded startup?** we visualise them by barplot","bd3db369":"If we look at the names of investors then we can find that their are more than one investor for some of the startups and they are separated by ',' , so we can create a list of those names as to extract unique names form it.","f9ebdae1":"\n\n\nThe line fitting those datapoints has the equation , J(y_pred)= $\\theta * X +b$<br>\nJ is also known as loss function<br>\nb = intercept<br>\nX= feature from the datapoints<br>\n$\\theta$= slope of the line\n\nWe can see that the line does not fit all the datapoints , and we can their is some error between each points and the line . We can find the total error by sum of square of those error i.e. $S=(y\\_given^{i}-J^{i})^2$ <br>\nand we use the square term because some of points above the line might have positive error and some of the points below the line has negaive error and we sum them we might get the actual error or the total error get decreased.<br>\nNow the goal is that we use S to update the theta such that Sum of square error could be minized: <br>\n$\\theta= \\theta - LR* \\frac{dS}{d\\theta}$<br>\n$b=b-LR* \\frac{dS}{db}$<br>\nLR is learning rate at which the update the weights , one can visualise them as the  we do the [gradient descent](https:\/\/blog.clairvoyantsoft.com\/the-ascent-of-gradient-descent-23356390836f) to find the global optimal. \n\nSome of these line get overfit the datasets i.e. high variance, one can also understand it like the Training model get well trained over the given dataset but might not perform well for new coming datapoints. So we address this problem using **Rigde regression**.\n\n### Rigde regression (L2 regularization)\nConsider the Image:\n\n","9393ac53":"Now we look wo are the most frequent investors in the market. If we see in the investors name then we can observe that their are some name like undisclosed, undisclosed investos , which represents nothing so we can put them under the category of 'others'.","3efee422":"### Feature engineering","39ecc926":"### Data preprocessing","368d121e":"\nWhen we train the model the using Ridge Regression ,it has the term ${\\theta}^2$ as the penalty , and if see from above image we have many features in it. Some of the feature like `Location`,`startup name` might have value for our training model but some features like `489`,`402` ,`Sr.no.` etc. might not have much value\/effect for our training model , so as we increase the lambda value the penalty for the Ridge Regression goes Asymptotically to 0 ,and those features remain in out training set with some less impact on the sum of squares error but the **Lasso Regression** has the norm term as penalty and as we increase the lambda then those less important features goes straight forward to 0 and those features has no effect for the trainig model and we obtain the model with less sum of squares of error than the Ridge Regression . Thus Lasso Regression perform better than Ridge Regression by remove the effect of some non useful features for the training model.","aff7f1a5":"\nTheir are many features available after converting the text data to numerical value, their might be some features which are less useful while training the model ,so we can neglect of those features by introducing Lasso Regression.\n\n### Lasso Regression(L1 Regression)\nIt is same as the Ridge Regression except the difference is that it uses norm instead of square for penalty term i.e:<br>\n$\\lambda * {|\\theta|}$<br>\nSo our final function would be :\n$J= (y\\_given-(\\theta * X +b))^2+\\lambda * {|\\theta|} $\n\nHow it affect the features?<br>\nConsider The training model : <br>\n","74088dd1":"# Top 50 startups\n\nStartup is the first phase of the company, it is founded or lead by entrepreneurs who develop some product or service according to need of society. Their are many things we need for a proper Startup like:\n* Business plan \/ Idea\n* Location\n* Raw materials\n* Market knowledge\n* Right people\n\nBut one of the most importing things is funding for proper growth and expansion of the startup. We need to look investors , and the investors look for business plan and the future of the startup and the field or tech on which startup is focusing on. <br>\nHere we have dataset from [Kaggle](https:\/\/www.kaggle.com\/sudalairajkumar\/indian-startup-funding) which has the info regarding different startups, their funding and investors. We explore it to know :\n* Which are some biggest funded startups?\n* What are the most popular industries for startups?\n* Who are the biggest investors?\n* Which Location is preffered for startups? \n\n","3e3f153a":"Banglore is  the most preffered industry for the startups , that's why we can also say that Banglore is the IT hub,  the location also matters so one should also think about the location in terms of funding and kind of industry statups are working on.\n\n**Now we have answer to all the question we have mentioned at the starting**.","0b055bdd":"The most of the startups are based on:\n* Consumer internet\n* Technology\n* e-commerce industries\n\nNow we also look at what are the most preferred locations for the startups.","132de150":"From above we can see the investors who has invested more than one startups are :\n* venture partner\n* sequoia capital\n* accesl partner","4cc8d55d":"We get the overall mean absolute error of 16.897 from our model.","cf2517a5":"We can see the biggest investor for the startups and the biggest investors are `Westbridge capital` and `softbank`.\n"}}