{"cell_type":{"ba2fd4ad":"code","ca41544a":"code","3e9f93e3":"code","75f28151":"code","320d5c35":"code","a3e77420":"code","140d75e9":"code","bd1c830b":"code","66a09d30":"code","6fab159a":"code","a0aed55c":"code","a727db35":"code","d4cb94c8":"code","4be1bf10":"code","254e04b0":"code","23da16b6":"code","eaf7e9f5":"code","d0495e74":"code","a0c614fc":"code","56e616ad":"code","934334a6":"code","65a4ad18":"code","85505b56":"code","2f074c81":"code","40f35493":"code","d9b0ebeb":"code","79dfeaab":"code","9a405605":"code","18228b21":"code","589d7b4b":"code","fc74798d":"code","3f091a1b":"code","1a0eccc0":"code","831f2009":"code","bc78c30b":"markdown","8c112b45":"markdown","526fb59a":"markdown","6d52e2f1":"markdown","9ddb832a":"markdown","8f7419be":"markdown","e92cd598":"markdown","7946f814":"markdown","f92e44b5":"markdown","214ea157":"markdown","8e060091":"markdown","cfd29c1c":"markdown","f5d2ca8e":"markdown","a74c54b9":"markdown","56ce54e0":"markdown","b59295a6":"markdown","13315348":"markdown","b9f79330":"markdown","67b9d815":"markdown"},"source":{"ba2fd4ad":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca41544a":"data = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')\nprint(data.shape)\ndata.head()","3e9f93e3":"data.info()","75f28151":"data.describe()","320d5c35":"sns.countplot(data['price_range'])","a3e77420":"plt.figure(figsize=(16,12))\nsns.heatmap(data.corr(),annot=True,square=True)","140d75e9":"corr = data.corr()\nNum = corr['price_range'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette('cyan',as_cmap=True)\ns = Num.style.background_gradient(cmap=cm)\ns","bd1c830b":"plt.figure(figsize=(14,6))\n\nplt.subplot(2,2,1)\nsns.barplot(x='price_range',y='battery_power',data=data,palette='Reds')\nplt.subplot(2,2,2)\nsns.barplot(x='price_range',y='px_height',data=data,palette='Blues')\nplt.subplot(2,2,3)\nsns.barplot(x='price_range',y='px_width',data=data,palette='Greens')\nplt.subplot(2,2,4)\nsns.barplot(x='price_range',y='ram',data=data,palette='Oranges')","66a09d30":"sns.relplot(x='price_range',y='ram',data=data,kind='line')","6fab159a":"from sklearn.model_selection import train_test_split\n\nfeatures = data.drop(columns=['price_range'])\ntarget = data['price_range']\nX_train,X_test,y_train,y_test = train_test_split(features,target,test_size=0.2,random_state=0)","a0aed55c":"cols = features.columns","a727db35":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train,columns=[cols])\nX_test = pd.DataFrame(X_test,columns=[cols])","d4cb94c8":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf_rf = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf,n_features_to_select=5,step=1)\nrfe = rfe.fit(X_train,y_train)","4be1bf10":"print('Chosen best 5 feature by rfe:',X_train.columns[rfe.support_])","254e04b0":"pick_col = ['battery_power','mobile_wt','px_height','px_width','ram']\nX_train_pick = X_train[pick_col]\nX_test_pick = X_test[pick_col]","23da16b6":"from sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score","eaf7e9f5":"rfc = RandomForestClassifier()\nrfc.fit(X_train_pick,y_train)\ny_pred1 = rfc.predict(X_test_pick)\n\nacc = accuracy_score(y_test,y_pred1)\nprint('Accuracy is: ',acc)\ncm = confusion_matrix(y_test,y_pred1)\nsns.heatmap(cm,annot=True,fmt=\"d\",cmap='YlGnBu')","d0495e74":"from sklearn.feature_selection import RFECV\n\nclf_rf2 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf2,step=1,cv=5,scoring='accuracy') \nrfecv = rfecv.fit(X_train,y_train)\n\nprint('Optimal number of features :',rfecv.n_features_)\nprint('Best features :',X_train.columns[rfecv.support_])","a0c614fc":"plt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1,len(rfecv.grid_scores_)+1),rfecv.grid_scores_)\nplt.show()","56e616ad":"select_col = ['battery_power','px_height','px_width','ram']\nX_train_selected = X_train[select_col]\nX_test_selected = X_test[select_col]","934334a6":"rfc = RandomForestClassifier()\nrfc.fit(X_train_selected,y_train)\ny_pred2 = rfc.predict(X_test_selected)\n\nacc = accuracy_score(y_test,y_pred2)\nprint('Accuracy is: ',acc)\ncm = confusion_matrix(y_test,y_pred2)\nsns.heatmap(cm,annot=True,fmt=\"d\",cmap='YlGn')","65a4ad18":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report","85505b56":"def score_of_model(models,X_train,X_test,y_train,y_test):\n    np.random.seed(0)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[name] = model.score(X_test,y_test)\n\n    model_scores = pd.DataFrame(model_scores, index=['Score']).transpose()\n    model_scores = model_scores.sort_values('Score')\n        \n    return model_scores","2f074c81":"models = {'LogisticRegression': LogisticRegression(max_iter=10000),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'RandomForestClassifier': RandomForestClassifier(),\n          'XGBClassifier': XGBClassifier()}","40f35493":"model_score = score_of_model(models,X_train_selected,X_test_selected,y_train,y_test)","d9b0ebeb":"cm = sns.color_palette('coolwarm',as_cmap=True)\nscore = model_score.style.background_gradient(cmap=cm)\nscore","79dfeaab":"# instantiate classifier with default hyperparameters\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nsvc = SVC() \nsvc.fit(X_train_selected,y_train)\ny_pred = svc.predict(X_test_selected)\nprint('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test,y_pred)))","9a405605":"# instantiate classifier with rbf kernel and C=100\nsvc100 = SVC(C=100.0) \nsvc100.fit(X_train_selected,y_train)\ny_pred = svc100.predict(X_test_selected)\nprint('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","18228b21":"# instantiate classifier with linear kernel and C=1.0\nlinear_svc = SVC(kernel='linear',C=1.0) \nlinear_svc.fit(X_train_selected,y_train)\ny_pred = linear_svc.predict(X_test_selected)\nprint('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test,y_pred)))","589d7b4b":"# instantiate classifier with polynomial kernel and C=1.0\npoly_svc = SVC(kernel='poly',C=1.0) \npoly_svc.fit(X_train_selected,y_train)\ny_pred = poly_svc.predict(X_test_selected)\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test,y_pred)))","fc74798d":"# instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc = SVC(kernel='sigmoid',C=1.0) \nsigmoid_svc.fit(X_train_selected,y_train)\ny_pred = sigmoid_svc.predict(X_test_selected)\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","3f091a1b":"from sklearn.metrics import classification_report,plot_confusion_matrix ","1a0eccc0":"model = SVC() \nmodel.fit(X_train_selected,y_train)\ny_pred = model.predict(X_test_selected)\nprint(classification_report(y_test,y_pred))","831f2009":"plot_confusion_matrix(model,X_test_selected,y_test,cmap='OrRd')","bc78c30b":"#### Run SVM with linear kernel ","8c112b45":"## 2. Exploratory Data Analysis","526fb59a":"#### Run SVM with rbf kernel and C=100.0","6d52e2f1":"#### Recursive feature elimination (RFE) with random forest","9ddb832a":"#### Run SVM with default hyperparameters ","8f7419be":"## 1. Reading Dataset","e92cd598":"## 4. Building Model","7946f814":"How we see - SVC gives the best results","f92e44b5":"### (A) Feature Scaling","214ea157":"## 5. Model Evalution","8e060091":"#### Recursive feature elimination with cross validation(RFECV) and random forest classification","cfd29c1c":"We get maximum accuracy with **rbf kernel** with C=1.0 and the accuracy is **0.9675**. Based on the above analysis we can conclude that our classification model accuracy is very good. ","f5d2ca8e":"## 3. Feature Engineering","a74c54b9":"Let's look at best accuracy in plot.","56ce54e0":"#### Run SVM with sigmoid kernel ","b59295a6":"**Conclusion:**\n\nThe chosen model was SVC since it\u00b4s the most accurate, and got a perfecf accuracy.\n\nIn this project, we use RFECV to select the features.","13315348":"We will use the features selected by RFECV to build the model.","b9f79330":"#### Run SVM with polynomial kernel","67b9d815":"### (B) Feature Selection"}}