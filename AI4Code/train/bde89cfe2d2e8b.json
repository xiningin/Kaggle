{"cell_type":{"42ee8a3f":"code","05bf8afe":"code","38eca073":"code","f1e4d5fc":"code","03ac912b":"code","ad95916f":"code","6dce058d":"code","9eae3c45":"code","46489e55":"code","f0d37450":"code","e9ffdc2e":"code","160b9729":"code","095bea47":"code","b09a1950":"code","35cccebd":"code","4711fa41":"code","2ca09417":"code","1c0a9b86":"markdown","81e8b381":"markdown","b80589e7":"markdown","d6cde4c7":"markdown","f2757080":"markdown","3b72f026":"markdown","8d54d8e6":"markdown","d253d6ea":"markdown","4a2931a9":"markdown","4714d3bc":"markdown","7956d31a":"markdown","df5dc04e":"markdown","328b012b":"markdown","394ea6e8":"markdown","af14f320":"markdown","36fa8b1b":"markdown","b45ab128":"markdown"},"source":{"42ee8a3f":"import numpy as np\nimport pandas as pd\nimport spacy\n\n# Import the english language model\nnlp = spacy.load('en')","05bf8afe":"df = pd.read_csv(\"..\/input\/fake.csv\")\ndf.shape","38eca073":"df.head()","f1e4d5fc":"df[\"title\"].head()","03ac912b":"txt = df[\"title\"][1009]\ntxt","ad95916f":"doc = nlp(txt)    \nolist = []\nfor token in doc:\n    l = [token.text,\n        token.idx,\n        token.lemma_,\n        token.is_punct,\n        token.is_space,\n        token.shape_,\n        token.pos_,\n        token.tag_]\n    olist.append(l)\n    \nodf = pd.DataFrame(olist)\nodf.columns= [\"Text\", \"StartIndex\", \"Lemma\", \"IsPunctuation\", \"IsSpace\", \"WordShape\", \"PartOfSpeech\", \"POSTag\"]\nodf","6dce058d":"doc = nlp(txt)\nolist = []\nfor ent in doc.ents:\n    olist.append([ent.text, ent.label_])\n    \nodf = pd.DataFrame(olist)\nodf.columns = [\"Text\", \"EntityType\"]\nodf","9eae3c45":"from spacy import displacy\ndisplacy.render(doc, style='ent', jupyter=True)","46489e55":"txt = df[\"title\"][3003]\ndoc = nlp(txt)\ncolors = {'GPE': 'lightblue', 'NORP':'lightgreen'}\noptions = {'ents': ['GPE', 'NORP'], 'colors': colors}\ndisplacy.render(doc, style='ent', jupyter=True, options=options)","f0d37450":"txt = df[\"title\"][2012]\nprint(txt)","e9ffdc2e":"doc = nlp(txt)\nolist = []\nfor chunk in doc.noun_chunks:\n    olist.append([chunk.text, chunk.label_, chunk.root.text])\nodf = pd.DataFrame(olist)\nodf.columns = [\"NounPhrase\", \"Label\", \"RootWord\"]\nodf","160b9729":"doc = nlp(df[\"title\"][1009])\nolist = []\nfor token in doc:\n    olist.append([token.text, token.dep_, token.head.text, token.head.pos_,\n          [child for child in token.children]])\nodf = pd.DataFrame(olist)\nodf.columns = [\"Text\", \"Dep\", \"Head text\", \"Head POS\", \"Children\"]\nodf","095bea47":"displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})","b09a1950":"doc = nlp(df[\"title\"][3012])\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance': 60})","35cccebd":"nlp = spacy.load('en_core_web_lg')","4711fa41":"from scipy import spatial\ncosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n\nqueen = nlp.vocab['Queen'].vector\ncomputed_similarities = []\nfor word in nlp.vocab:\n    # Ignore words without vectors\n    if not word.has_vector:\n        continue\n    similarity = cosine_similarity(queen, word.vector)\n    computed_similarities.append((word, similarity))\n\ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint([w[0].text for w in computed_similarities[:10]])","2ca09417":"queen = nlp.vocab['Queen']\nelizabeth = nlp.vocab['Elizabeth']\nbritain = nlp.vocab['Britain']\ndolphin = nlp.vocab['Dolphin']\nking = nlp.vocab['King']\n \nprint(\"Word similarity score between Queen and Elizabeth : \",queen.similarity(elizabeth))\nprint(\"Word similarity score between Queen and Britain : \",queen.similarity(britain))\nprint(\"Word similarity score between Queen and Dolphin : \",queen.similarity(dolphin))\nprint(\"Word similarity score between Queen and King : \",queen.similarity(king))","1c0a9b86":"In this notebook, we will explore the [Spacy](https:\/\/spacy.io\/) tool by using Fake News dataset. \n\nLet us import spacy and also import the 'english' language model.","81e8b381":"**Dependency Parser**\n\nA dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads - [Stanford NLP](https:\/\/nlp.stanford.edu\/software\/nndep.html)\n\nSpacy can be used to create these dependency parsers which can be used in a variety of tasks. ","b80589e7":"\"King\" is the most similar word followed by \"Elizabeth\" and \"Britain\".","d6cde4c7":"**Named Entity Recognition:**\n\nA named entity is a \"real-world object\" that's assigned a name \u2013 for example, a person, a country, a product or a book title. \n\nWe also get named entity recognition as part of spacy package. It is inbuilt in the english language model and we can also train our own entities if needed.","f2757080":"**References:**\n1. [Complete Guide to Spacy](https:\/\/nlpforhackers.io\/complete-guide-to-spacy\/)\n2. [Spacy documentation](https:\/\/spacy.io\/)\n\n**More to come. Stay tuned.!**","3b72f026":"So using \"nlp\" we got a lot of information. The details are as follows:\n\n* Text - Tokenized word\n* StartIndex - Index at which the word starts in the sentence\n* Lemma - Lemma of the word (we need not do lemmatization separately)\n* IsPunctuation - Whether the given word is a punctuation or not\n* IsSpace - Whether the given word is just a white space or not\n* WordShape - Gives information about the shape of word (If all letters are in upper case, we will get XXXXX, if all in lower case then xxxxx, if the first letter is upper and others lower then Xxxxx and so on)\n* PartOfSpeech - Part of speech of the word\n* POSTag - Tag for part of speech of word","8d54d8e6":"The description of the columns are as follows:\n\n* uuid - Unique identifier\n* ord_in_thread\n* author - author of story\n* published - date published\n* title - title of the story\n* text - text of story\n* language - data from webhose.io\n* crawled - date the story was archived\n* site_url - site URL from BS detector\n* country - data from webhose.io\n* domain_rank - data from webhose.io\n* thread_title\n* spam_score - data from webhose.io\n* main_img_url - image from story\n* replies_count - number of replies\n* participants_count - number of participants\n* likes - number of Facebook likes\n* comments - number of Facebook comments\n* shares - number of Facebook shares\n* type - type of website (label from BS detector)\n\nNow let us look at the top few rows of the dataset to gain some more understanding.","d253d6ea":"**Word-Level Attributes:**\n\nJust calling the function \"nlp\" on the text column gets us a lot of information. Let us take an example row from the dataset and then apply the same.","4a2931a9":"**Word Similarity:**\n\nSpacy has word vector model as well. So we can use the same to find similar words. The list of available models can be seen [here](https:\/\/spacy.io\/models\/).\n\nFor our case, let us use the 'en_core_web_lg' model available in spacy (more details about the model can be accessed in this [link](https:\/\/spacy.io\/models\/en#en_core_web_lg)). First step is to load the model.","4714d3bc":"Let us look at the number of rows and columns present in the dataset.","7956d31a":"The description of the columns are\n* Text: The original token text.\n* Dep: The syntactic relation connecting child to head.\n* Head text: The original text of the token head.\n* Head POS: The part-of-speech tag of the token head.\n* Children: The immediate syntactic dependents of the token.\n\nThe best way to understand the dependency parser is to visualize the same and looking at it.","df5dc04e":"Wow. This one looks cool. We can also take one more example and visualize the same. ","328b012b":"Now we can use the cosine similarity to find the words that are similar to the word \"Queen\".","394ea6e8":"**Noun Phrase Chunking:**\n\nNoun chunks are \"base noun phrases\" \u2013 flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun \u2013 for example, \"the lavish green grass\" or \"the world\u2019s largest tech fund\". \n\nNow let us look at how to do noun phrase chunking using spacy. In addition to noun phrase chunking, spacy also gets us the root of the noun.","af14f320":"Different versions of king and queen came out as the top similar words. Now let us take the other important words from the sentence \"Elizabeth\", \"Britain\", \"Dolphin\" and also \"King' and check the similarity.","36fa8b1b":"Columns \"title\", \"text\" and \"thread_title\" has textual data. For this introduction, let us concentrate on the 'title' column. So let us look at the top few rows of the columns alone","b45ab128":"The complete list of different entity types can be seen [here](https:\/\/spacy.io\/usage\/linguistic-features#entity-types)\n\nSpacy also includes a [displacy visualizer](displaCy visualizer with Jupyter support) with jupyter notebook support. This can be used to visualize the named entity recognition data."}}