{"cell_type":{"cbbde55b":"code","7261be3d":"code","f4d10794":"code","2665a0f0":"code","b88e86b1":"code","15b3a950":"code","0f33c9b5":"code","65ced82d":"code","fd7cea23":"code","70b683af":"code","8d74efef":"code","78ff887e":"code","dc24db34":"code","590ba930":"code","36bab480":"code","6ef35fe6":"code","490f2392":"code","5e566041":"code","336cf263":"code","1a4e32f6":"code","f161fece":"code","8ec077c7":"code","3a4302e4":"markdown","89ee110c":"markdown","a869cd36":"markdown","3398a2b9":"markdown","35c4be80":"markdown","583df5dd":"markdown","d968a762":"markdown","3dffa1a0":"markdown","d1d97b1e":"markdown","327c3087":"markdown","b7b3d5ee":"markdown","9501b1ef":"markdown","3321e6e6":"markdown","da81fb7c":"markdown","994abdd5":"markdown","3243ea4a":"markdown","ad04aef9":"markdown","d07e21ea":"markdown","8ba1bcd5":"markdown","037427f2":"markdown","7694ef4b":"markdown","0983271d":"markdown","664fdcdd":"markdown","03aad812":"markdown","48f944ec":"markdown","4907d685":"markdown","8f43200e":"markdown","99aa0c54":"markdown","2dd12aaf":"markdown","480880b6":"markdown","101c49bb":"markdown","38e9ded6":"markdown","4043b836":"markdown","15e302d7":"markdown","88bf6504":"markdown","82ab1113":"markdown"},"source":{"cbbde55b":"import pandas as pd\nimport numpy as np","7261be3d":"!pip install pyldavis","f4d10794":"import nltk\nnltk.download('punkt')","2665a0f0":"import re\nimport string\nfrom gensim import corpora, models, similarities ","b88e86b1":"datafile = '..\/input\/turkish-reviews-dataset\/Reviews.csv'","15b3a950":"query_df = pd.read_csv(datafile,error_bad_lines=False)\nquery_df = query_df.iloc[:8000]\nquery_df.head()","0f33c9b5":"query_df['Review'] = query_df['Review'].astype(str)\nquery_df['Review'].head()","65ced82d":"def deEmoji(text):\n\n  emoji_pattern = re.compile(\"[\"\n          u\"\\U0001F600-\\U0001F64F\"  # duygular (emoticons)\n          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                            \"]+\", flags=re.UNICODE)\n  return str(emoji_pattern.sub('', text) ) # no emoji","fd7cea23":"with open('..\/input\/stop-words-in-28-languages\/turkish.txt', 'r') as f:\n    myList = [line.strip() for line in f]","70b683af":"def word_tokenize(sentence):\n    \n    acronym_each_dot = r\"(?:[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]\\.){2,}\"\n    acronym_end_dot = r\"\\b[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]{2,3}\\.\"\n    suffixes = r\"[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]{3,}' ?[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]{0,3}\"\n    numbers = r\"\\d+[.,:\\d]+\"\n    any_word = r\"[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]+\"\n    punctuations = r\"[a-z\u011f\u00e7\u015f\u00f6\u00fc\u0131]*[.,!?;:]\"\n    word_regex = \"|\".join([acronym_each_dot,\n                           acronym_end_dot,\n                           suffixes,\n                           numbers,\n                           any_word,\n                           punctuations])\n    \n    sentence = re.compile(\"%s\"%word_regex, re.I).findall(sentence)\n    return sentence","8d74efef":"def initial_clean(text):\n     \n     text = text.translate(str.maketrans('', '', string.punctuation))\n     text = text.lower() # lower case text\n     text = word_tokenize(text)\n     return text","78ff887e":"def remove_stop_words(text):\n     stop_words = myList\n     return [word for word in text if word not in stop_words]","dc24db34":"def apply_all(text):\n    \n     return remove_stop_words(initial_clean(deEmoji(text)))","590ba930":"import time   \nt1 = time.time()   \nquery_df['tokenized_texts'] = query_df['Review'].apply(apply_all) #g\u00fcr\u00fclt\u00fcl\u00fc text verisi---->normalize edildi\nt2 = time.time()  \nprint(\"Time to clean and tokenize\", len(query_df), \"texts:\", (t2-t1)\/60, \"min\") #Time to clean and tokenize 3209 reviews: 0.21254388093948365 min","36bab480":"import gensim\nimport pyLDAvis.gensim","6ef35fe6":"#Tokenized datadan Gensim k\u00fct\u00fcphanesini olu\u015fturuyoruz\ntokenized = query_df['tokenized_texts']\n#corpora, kelimeler ve integer idler aras\u0131nda mapping yaparak corpusu olu\u015fturur?\ndictionary = corpora.Dictionary(tokenized)\n#Filter terms which occurs in less than 1 query and more than 80% of the queries.\ndictionary.filter_extremes(no_below=1, no_above=0.8)\n#convert the dictionary to a bag of words corpus \n#doc2bow: Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples.\ncorpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\nprint(corpus[:1])","490f2392":"[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]","5e566041":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 8, id2word=dictionary, passes=15)\nldamodel.save('mOdel.gensim') #modeli kaydettik\ntopics = ldamodel.print_topics(num_words=30) #her bir topic grubu i\u00e7in 30 kelime atad\u0131k","336cf263":"for topic in topics:\n   print(topic)","1a4e32f6":"get_document_topics = ldamodel.get_document_topics(corpus[0])\nprint(get_document_topics)","f161fece":"lda_viz = gensim.models.ldamodel.LdaModel.load('mOdel.gensim')#lda modelini y\u00fckledik\nlda_display = pyLDAvis.gensim.prepare(lda_viz, corpus, dictionary, sort_topics=True)\npyLDAvis.display(lda_display)","8ec077c7":"def dominant_topic(ldamodel,corpus,content):\n     #Her bir sorgu i\u00e7in bask\u0131n konuyu (topic) bulan fonksiyon\n     sent_topics_df = pd.DataFrame() \n     # Her bir sorgu i\u00e7in ana topic\n     for i, row in enumerate(ldamodel[corpus]):\n         row = sorted(row, key=lambda x: (x[1]), reverse=True)\n         # Her bir sorgu i\u00e7in dominant topic, y\u00fczde katk\u0131s\u0131 ve anahtar kelimeleri bulur\n         for j, (topic_num, prop_topic) in enumerate(row):\n             if j == 0:  # =&gt; dominant topic\n                 wp = ldamodel.show_topic(topic_num,topn=30)\n                 topic_keywords = \", \".join([word for word, prop in wp])\n                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n             else:\n                 break\n     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n     contents = pd.Series(content)#noisy data\n     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n     return(sent_topics_df)\ndf_dominant_topic = dominant_topic(ldamodel=ldamodel, corpus=corpus, content=query_df['Review']) \ndf_dominant_topic.head(10)","3a4302e4":"**Yani corpus i\u00e7in kelime-frekans mapping yap\u0131yor diyebiliriz**","89ee110c":"**Tan\u0131mlad\u0131\u011f\u0131m\u0131z t\u00fcm yard\u0131mc\u0131 fonksiyonlar\u0131 apply_all i\u00e7ine at\u0131yoruz**","a869cd36":"**Preprocessing ve LDA i\u00e7in gerekli k\u00fct\u00fcphaneleri y\u00fckl\u00fcyoruz**","3398a2b9":"**PyLDAvis ile topic gruplar\u0131n\u0131 g\u00f6rselle\u015ftirelim:**","35c4be80":"?**stopwordleri myList i\u00e7ine ta\u015f\u0131d\u0131k?**","583df5dd":"**Sonu\u00e7lar\u0131 analiz etmemiz gerekirse, 0'dan 7'ye 8 tane topic belirlenmi\u015ftir. Her topicte o topici en iyi tan\u0131mlayan 30 kelime vard\u0131r.**","d968a762":"    Args:\n        sentence (str): any sentence.\n    Returns:\n        list: each item is a word.\n        ","3dffa1a0":"**Dictionary, num_topics (t\u00fcm corpus i\u00e7in olu\u015fturdu\u011fumuz toplam topic say\u0131s\u0131) ve corpusu input olarak ald\u0131k.**","d1d97b1e":"*******************************************************************************************************************","327c3087":"**Corpusun ilk indexindeki (0,1)\u2019de ifade edilmek istenen, word_id\u2019si 0 olan kelimenin ilk text verisinde ka\u00e7 kere tekrarlam\u0131\u015f oldu\u011fu.**","b7b3d5ee":"**Corpustaki ilk verimizi en iyi ifade eden topici ve verinin bu gruba yak\u0131nl\u0131\u011f\u0131n\u0131 belirten olas\u0131l\u0131\u011f\u0131 yukar\u0131daki kod ile bulabiliriz**","9501b1ef":"PyLDAvis, kullan\u0131c\u0131lar\u0131n bir topic modelindeki konular\u0131 bir metin verisi toplulu\u011funa uygun olarak yorumlamalar\u0131na yard\u0131mc\u0131 olmak i\u00e7in tasarlanm\u0131\u015ft\u0131r. LDA topic modelinden ald\u0131\u011f\u0131 bilgilerle etkile\u015fimli bir web tabanl\u0131 g\u00f6rselle\u015ftirme sunar.","3321e6e6":"**4. topic grubu 0.78'lik bir oranla verimiz i\u00e7in en uygun grup diyebiliriz.**","da81fb7c":".strip() kesme fonk (hem ba\u015ftan hem sondan)","994abdd5":"G\u00f6kmen,B\u00fc\u015fra,Latent Dirichlet Allocation(LDA) Kullanarak Nas\u0131l Topic Modeling Yap\u0131l\u0131r?,medium.com,2020","3243ea4a":"**Gensim k\u00fct\u00fcphanesi text verisindeki kelimeleri token olarak ifade eder ve her token i\u00e7in index numaras\u0131 olu\u015fturur ve dictionaryi i\u00e7inde bu indexler \u201cid\u201d olarak yer al\u0131r.**","ad04aef9":"**PyLDAvis, kullan\u0131c\u0131lar\u0131n bir topic modelindeki konular\u0131 bir metin verisi toplulu\u011funa uygun olarak yorumlamalar\u0131na yard\u0131mc\u0131 olmak i\u00e7in tasarlanm\u0131\u015ft\u0131r. LDA topic modelinden ald\u0131\u011f\u0131 bilgilerle etkile\u015fimli bir web tabanl\u0131 g\u00f6rselle\u015ftirme sunar.**","d07e21ea":"**LDA algoritmas\u0131 Dok\u00fcman-Topic matrixi ve Topic-Kelime matrixi olu\u015fturur. Topic-Kelime matrixi kelimelerin topiclere da\u011f\u0131l\u0131m olas\u0131l\u0131klar\u0131n\u0131 tutar.**","8ba1bcd5":"**Texti temizleyip tokenized_text ad\u0131nda yeni bir kolon olarak at\u0131yoruz**","037427f2":"**Normalizasyon Fonksiyonlar\u0131 tan\u0131mlan\u0131r.**","7694ef4b":"**Daha sonra modeli mOdel.gensim olarak kaydettik. Her topic i\u00e7in bulunma oran\u0131 en fazla olan kelimeleri num_words parametresi ile yazd\u0131rabiliriz.**","0983271d":"**Textteki emojileri kald\u0131ran bir fonk tan\u0131ml\u0131yoruz**","664fdcdd":"**initial_clean, text \u00fczerindeki noktalama i\u015faretlerini temizler (punctuations) ve b\u00fcy\u00fck harfleri k\u00fc\u00e7\u00fck harflere \u00e7evirir. word_tokenize fonksiyonunu da initial_clean i\u00e7ine koyarak texti bo\u015fluklara g\u00f6re par\u00e7al\u0131yoruz.**","03aad812":"**Do\u011fal dil i\u015fleme k\u00fct\u00fcphanesi olan NLTK'yi import ediyoruz**","48f944ec":"**word_tokenize ile textteki kelimeler ve semboller bo\u015flu\u011fa g\u00f6re ayr\u0131l\u0131r.**","4907d685":"Bu corpus outputuna **Document Term Matrix(Dok\u00fcman Terim Matrixi)** de deniyor ve LDA topic modelimiz i\u00e7in **input** matrixi olacak.","8f43200e":"**Corpusun ilk indexinin kelimeli halini inceleyecek olursak:**","99aa0c54":"**GENSIM K\u00dcT\u00dcPHANES\u0130 VE CORPUSUNU OLU\u015eTURALIM**","2dd12aaf":"**Dok\u00fcman topic matrisi, dok\u00fcmanlar\u0131n topiclere da\u011f\u0131l\u0131m olas\u0131l\u0131\u011f\u0131n\u0131 verir. Yani hangi dok\u00fcman\u0131n hangi matrise ait oldu\u011funu bulabiliriz bu yolla.**","480880b6":"**\u015eimdi topic modelimizi olu\u015fturmaya ba\u015fl\u0131yoruz**","101c49bb":"**?????????????????????????????**","38e9ded6":"**T\u00fcrk\u00e7e i\u00e7in stopwords listesini getiriyoruz**","4043b836":"**Yani dictionary kelimeler ve onlara ait \u201cid\u201d numaralar\u0131n\u0131 bar\u0131nd\u0131ran bir s\u00f6zl\u00fck yap\u0131s\u0131d\u0131r. Corpus ise bu kelimelerin id\u2019lerinden ve text verisi i\u00e7indeki frekanslar\u0131ndan olu\u015fur.**","15e302d7":"**Standart k\u00fct\u00fcphanelerimizi tan\u0131ml\u0131yoruz**","88bf6504":"**Dok\u00fcmanlar\u0131n bask\u0131n topiclerini belirlemek i\u00e7in:**","82ab1113":"**CSV dosyas\u0131n\u0131 okutuyoruz**"}}