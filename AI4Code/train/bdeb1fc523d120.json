{"cell_type":{"51dd40dd":"code","9753b1f1":"code","0e410f59":"code","b0b97fe9":"code","6dcd40eb":"code","477bbb27":"code","69024426":"code","fc01ee9f":"code","c44d8d40":"code","45288db1":"code","84ad3588":"code","bdd80d80":"code","0b10c053":"code","81fa5a18":"code","e082d9c4":"code","75f9e322":"code","be9dea06":"code","33c83c20":"code","2166fa63":"code","1fe83e91":"code","a7c75d80":"code","87e7bacf":"code","bba41b5e":"code","57d5029a":"code","941101c9":"code","022a869c":"code","0a1c6517":"code","d646f670":"code","14fa1a4b":"code","3a710bc6":"code","b7080cd7":"code","3b161eb7":"code","27a300b3":"code","5d0718eb":"code","c2551b1b":"code","dc597ab4":"code","a1fa2e2b":"code","e7aa4af4":"code","966ad264":"code","22f6f412":"code","b4f28602":"code","3078e12d":"code","bf0253a2":"code","8eff05d1":"markdown","59fd02e2":"markdown","2d0f1b9e":"markdown","e387c1b5":"markdown","3e744819":"markdown","d25cd678":"markdown","9259c1ac":"markdown","e3fa15c7":"markdown","b33660cd":"markdown","a2ba0e84":"markdown","e72d45de":"markdown","035d5b4b":"markdown","0052c6bb":"markdown","5ad95f78":"markdown","e8ffb52b":"markdown","4ab9d6e1":"markdown","35d927f3":"markdown","fd85c104":"markdown","02f793f7":"markdown","88906937":"markdown","70b6a702":"markdown","7fc82cc0":"markdown","159bceac":"markdown","c5627b6e":"markdown","7bc8b957":"markdown","5d55a17d":"markdown","81c68d31":"markdown","60e8902a":"markdown","13553733":"markdown","db6132c2":"markdown","ec26a9f8":"markdown","1f9354a5":"markdown","013bdae9":"markdown","c9679233":"markdown","068fbbd9":"markdown","ac1d9115":"markdown","12fe5b40":"markdown","583bde78":"markdown","287b893d":"markdown","21151dce":"markdown","452af328":"markdown","c931b014":"markdown","011f5d67":"markdown","1a9b8409":"markdown","c4254a8d":"markdown","3d691ac2":"markdown","1c3fbb3c":"markdown","40c129bd":"markdown"},"source":{"51dd40dd":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/titanic.csv')\ndata.head(10)","9753b1f1":"print(data.columns.values)","0e410f59":"drop_cat=['boat','body','home.dest']\ndata.drop(drop_cat, inplace=True, axis=1)\ndata.head(10)","b0b97fe9":"data['survived'].mean()","6dcd40eb":"print('Survival Rate by Sex')\nprint(data['survived'].groupby(data['sex']).mean())\nprint('\\n\\nSex Ratio of Passengers')\nprint(data['sex'].value_counts(normalize=True))","477bbb27":"hist = data['age'].hist(bins=30)","69024426":"data['survived'].groupby(pd.cut(data['age'], 20)).mean().plot(kind='bar')","fc01ee9f":"data['survived'].groupby(pd.cut(data['fare'], [0,5,10,20,40,70,100,1000])).mean().plot(kind='bar')","c44d8d40":"data['survived'].groupby(data['pclass']).mean().plot(kind='bar')","45288db1":"data['first name'] = data['name'].str.split(',|\\\\.',expand = True)[2] #expand set to True to return a df instead of series\ndata['first name'] = data['first name'].str.strip() #strip leading and trailing white spaces\ndata['last name'] = data['name'].str.split(',|\\\\.',expand = True)[0] #expand set to True to return a df instead of series\ndata['last name'] = data['last name'].str.strip()\ndata['title'] = data['name'].str.split(',|\\\\.',expand = True)[1] #expand set to True to return a df instead of series\ndata['title'] = data['title'].str.strip()\n\ndata['title'].value_counts() #just display the name column summary","84ad3588":"status_map={'Capt':'Military',\n            'Col':'Military',\n            'Don':'Noble',\n            'Dona':'Noble',\n            'Dr':'Dr',\n            'Jonkheer':'Noble',\n            'Lady':'Noble',\n            'Major':'Military',\n            'Master':'Common',\n            'Miss':'Common',\n            'Mlle':'Common',\n            'Mme':'Common',\n            'Mr':'Common',\n            'Mrs':'Common',\n            'Ms':'Common',\n            'Rev':'Clergy',\n            'Sir':'Noble',\n            'the Countess':'Noble',\n            }\n\ndata['social status'] = data['title'].map(status_map)","bdd80d80":"data['family members'] = data['parch'] + data['sibsp']","0b10c053":"data['deck'] = data['cabin'].str.replace('[0-9]','').str.split(' ', expand=True)[0]\n#1. delete cabin number, leaving leading letter (deck); 2. since multiple cabins are assigned, just get the first one, they are all on the same deck","81fa5a18":"data['name length'] = data['name'].apply(lambda x: len(x))\ndata['ticket length'] = data['ticket'].apply(lambda x: len(x))","e082d9c4":"data['survived'].groupby(data['social status']).mean().plot(kind='bar')","75f9e322":"data['survived'].groupby(data['family members']).mean().plot(kind='bar')","be9dea06":"data.isnull().any()","33c83c20":"data['age available'] = ~data['age'].isnull()\ndata['age'] = data['age'].fillna(data['age'].mean())","2166fa63":"data['fare available'] = ~data['fare'].isnull()\ndata['fare'] = data['fare'].fillna(data['fare'].mean())","1fe83e91":"data['deck'] = data['deck'].fillna('NA')","a7c75d80":"data['embarked'] = data['embarked'].fillna('NA')","87e7bacf":"data.isnull().any()","bba41b5e":"import numpy as np\ncat_used=['survived','pclass','sex','age','sibsp','parch','fare','embarked','title','social status','family members','deck','name length','ticket length','age available', 'fare available']\ndata_used=pd.DataFrame()\nfor cat in cat_used:\n    data_used[cat_used] = data[cat_used]\n\ndata_used.columns.values","57d5029a":"categorical_data = ['sex','pclass','embarked','title','social status','deck','age available', 'fare available']\n\nfor cat in categorical_data:\n    data_used = pd.concat((data_used, pd.get_dummies(data_used[cat], prefix = cat)), axis = 1)\n    data_used.drop(cat, inplace=True, axis=1)","941101c9":"test_data_split = 0.3\nmsk = np.random.rand(len(data_used)) < test_data_split \n\ntest = data_used[msk]\ntrain = data_used[~msk]","022a869c":"Y_train = train['survived']\nX_train = train.drop(['survived'], axis=1)\n\nY_test = test['survived']\nX_test = test.drop(['survived'], axis=1)","0a1c6517":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train_minmax = min_max_scaler.fit_transform(X_train)\nX_test_minmax = min_max_scaler.fit_transform(X_test)","d646f670":"Y_pred_base=np.zeros((len(Y_test)))\ncompare=Y_pred_base==Y_test\nacc_base = sum(compare)\/len(compare)\nprint('Base Line Test Accuracy = ',acc_base)","14fa1a4b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.get_params().keys()","3a710bc6":"param_grid = { \"min_samples_split\" : [ 2,3,4], \"min_samples_leaf\": [1,2,5,10,20,30,40,50,60,70,80,90,100]}\n\ngrid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\ngrid_search = grid_search.fit(X_train, Y_train)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)\n\n","b7080cd7":"param_grid = { \"min_samples_split\" : [2], \"min_samples_leaf\": range(60,80)}\n\ngrid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\ngrid_search = grid_search.fit(X_train, Y_train)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","3b161eb7":"dt_classifier = DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=73)\ndt_classifier.fit(X_train, Y_train)\ndt_classifier.score(X_train,Y_train)\nY_pred_dt = dt_classifier.predict(X_test)\ncompare=Y_pred_dt==Y_test\nacc_dt = sum(compare)\/len(compare)\nprint('Decision Tree Test Accuracy = ',acc_dt)","27a300b3":"from sklearn.externals.six import StringIO    \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(dt_classifier, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,\n                feature_names = X_test.columns.values)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(\"decision.png\")","5d0718eb":"pd.concat((pd.DataFrame(X_train.columns, columns = ['variable']), \n           pd.DataFrame(dt_classifier.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:10]","c2551b1b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier(criterion= 'gini', min_samples_leaf=1, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nparam_grid = { \"min_samples_split\" : [ 12, 14, 16], \"n_estimators\": [40,50, 60, 80]}\n\ngrid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\ngrid_search = grid_search.fit(X_train, Y_train)\n\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","dc597ab4":"rf_classifier = RandomForestClassifier(n_estimators=60, max_features='auto', criterion='gini', min_samples_split=14, min_samples_leaf = 1, oob_score=True, random_state=1, n_jobs=-1)\nrf_classifier.fit(X_train, Y_train)\nrf_classifier.oob_score_\n\npd.concat((pd.DataFrame(X_train.columns, columns = ['variable']), \n           pd.DataFrame(rf_classifier.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:10]\n\nY_pred_rf = rf_classifier.predict(X_test)\ncompare=Y_pred_rf==Y_test\nacc_rf = sum(compare)\/len(compare)\nprint('Random Forest Test Accuracy = ',acc_rf)","a1fa2e2b":"pd.concat((pd.DataFrame(X_train.columns, columns = ['variable']), \n           pd.DataFrame(rf_classifier.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:10]","e7aa4af4":"from sklearn.linear_model import LogisticRegression\n\nlr_classifierlogreg = LogisticRegression()\nlr_classifierlogreg.fit(X_train_minmax, Y_train)\n\nlr_classifierlogreg.score(X_train_minmax, Y_train)\nY_pred_lr = lr_classifierlogreg.predict(X_test_minmax)\ncompare=Y_pred_lr==Y_test\nacc_lr = sum(compare)\/len(compare)\nprint('Linear Regression Test Accuracy = ',acc_lr)","966ad264":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import plot_model\n\nmodel = Sequential()\n\n# Add new layers\nmodel.add(Dense(units=20, activation='linear',input_dim=52))\nmodel.add(Dense(units=10, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\nprint(model.summary())\nplot_model(model, to_file='model.png',show_shapes=True)\nmodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])","22f6f412":"history = model.fit(x=X_train_minmax,\n                    y=Y_train, \n                    batch_size=128, \n                    epochs=50, \n                    verbose=0, \n                    validation_split=0.2, \n                    shuffle=True)","b4f28602":"import matplotlib.pyplot as plt\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","3078e12d":"Y_pred_nn = model.predict(X_test_minmax)\ncompare=np.squeeze(np.round(Y_pred_nn))==np.array(Y_test)\nacc_nn = sum(compare)\/len(compare)\nprint('Neural Network Test Accuracy = ',acc_nn)","bf0253a2":"%%HTML\n<style>\ndiv.prompt {display:none} #div.prompt {display:\"\"} div.prompt {display:none} \n<\/style>","8eff05d1":"Our baseline prediction is pretty good, with an accuracy of 65%. \n\n### 4.2 Decision Tree\n\nWe briefly introduce decision trees.\n\n- Nonlinear classification model\n- Decision trees generate an approximate solution via greedy, top-down, recursive partitioning. \n    - Greedy: grow the tree by recursively splitting the samples in the leaf $R_i$ according to $X_j > s$, such that $(R_i , X_j , s)$ maximize the drop in entropy.\n    - The method is top-down because we start with the original input space X and split it into two child regions by thresholding on a single feature. We then take one of these child regions and can partition via a new threshold. \n    - We continue the training of our model in a recursive manner, always selecting a leaf node, a feature, and a threshold to form a new split.\n\n- Stopping criteria we could use to determine when to halt the growth of a tree:\n    - The simplest criteria involves \u201dfully\u201d growning the tree: we continue until each leaf region contains exactly one training data point. \n    - This technique however leads to a high variance and low bias model, and we therefore turn to various stopping heuristics for regularization. Some common ones include:\n        - Minimum Leaf Size \u2013 Do not split R if its cardinality falls below a fixed threshold. \n        - Maximum Depth \u2013 Do not split R if more than a fixed threshold of splits were already taken to reach R. \n        - Maximum Number of Nodes \u2013 Stop if a tree has more than a fixed threshold of leaf nodes\n\n\n\nThe benefit of decision trees is that it can be easily visualized. At each node, we know which feature is being used to decide the splits. The disadvantage is that its a greedy process, and will not perform as well as some of the other models.\n\n#### 4.2.1 Hyper-parameter Tuning\n\nThere are several hyper-parameters we can tune to improve performance. Lets see them","59fd02e2":"5. Finally we drop the \"cabin\" column since we already gleened \"deck\" information from it, and we are not using the room numbers\n\nCheck once again which column have missing data. We will drop the \"cabin\" column in the next section","2d0f1b9e":"We further split our data into output labels and input data.","e387c1b5":"We see that our best score lies within the middle of our scan range. If the best score lies at either end, we must increase our range and scan again. Also, we can scan another round in finer grainarities around our best parameter .","3e744819":"### 4.3 Random Forest\n\nIn random forest, we use many decision trees, each using only part of the data samples.\n\n- We fit a decision tree to different samples. \n    - When growing the tree, we select a random sample of m < M predictors to consider in each step. \n    - This will lead to very different (or \u201cuncorrelated\u201d) trees from each sample. \n    - Finally, average the prediction of each tree.\n- Each tree is grown as follows:\n    1. If the number of cases in the training set is N, sample N cases at random - but\u00a0with replacement, from the original data. This sample will be the training set for growing the tree.\n    2. If there are M input variables, a number m<<M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.\n    3. Each tree is grown to the largest extent possible. There is no pruning.\n\n#### 4.3.1 Hyper-Parameter Tuning\n\nWe first scan through some hyper-parameters.","d25cd678":"4. For \"embarked\" column, we also use NA to indicate missing values","9259c1ac":"Well, it looks like if you are nobility, you escaped with high probability. On the other hand, if you are clergy, you may have stayed behind. What about family size?","e3fa15c7":"### 3.2 Data Transformation\n\nOnce all missing data is filled in. We will extract the columns that we will use in our model.","b33660cd":"It looks like \"age\", \"fare\", \"cabin\", \"embarked\", and \"deck\" have missing values. We must take care filling these values. For categorical data, we can just leave it blank, since \"not available\" is also a category and may indeed convey some information. Filling a missing categorical data could generate some artificial information which could be harmful to the model. For non-categorical data, we may use the mean value, but we would like to note that this entry is artificiall generated. So we create another column indicating this.\n\n1. for \"age\" column, we use mean value to fill, and use another column to indicate this value is artificial.","a2ba0e84":"2. For \"fare\" data we also use mean value to fill, and use another column to indicate this value is artificial.","e72d45de":"Training a neural network model involves many iterations or epochs. The accuracy and error (loss) increase and decrease over time respectively. ","035d5b4b":"Our model accuracy is 82%.\n\n## 5. Model Comparison\n\nWe compare the test accuracy of our models below. For simple datasets, all models perform equally well. In particular, simple models such as decision tree is able to perform well when we feed it engineered features.","0052c6bb":"What about the lenght of the name, or length of the ticket number? Could they hold some mysteries meaning that we can't discern but a machine model can extract?","5ad95f78":"## 2. Feature Engineering\n\nOnce we have a good understanding of the dataset we are working with, we can engineer some new features from it to make our predictions more accurate. We first extract the first, last names and the title from the \"name\" column.","e8ffb52b":"# Feature Engineering, Data Exploration, and Classification using Titanic Dataset \n\nIn machine learing, what perhaps is more important than defining a model or a neural network is data exploration. After we get the dataset, that is hopefully representative of the problem we are trying to solve, the first thing we do is to examine the data and see what characteristics and features are present. Are there any problems in the data? Are some categories under-represented? Are there missing values? Are there significant correlations between some features and outcome? Can we easily engineer new features from existing ones that will correlate better with the outcome? Data exploration not only helps to discover and eliminates some potential problems before model training begins, but will also makes our model more accurate by injecting some human intuition into the solution.\n\nIn this tutorial, we will use the Titanic passenger survival dataset to illustrate the concept of data exploration and feature engineering. We choose this dataset because it is simple, and thus allows us to focus on engineering the features. We will show how engineered features play an important role in prediction by being the most important feature in machine learning models. Then, we will illustrate compare several methods of binary classification that can used to predict whether a passenger survives the disaster.\n\nThe Titanic dataset can be [download from this website](https:\/\/storage.googleapis.com\/kaggle-forum-message-attachments\/66979\/2180\/titanic.csv). Prediction Titanic passenger survival is also a [Kaggle challege](https:\/\/www.kaggle.com\/c\/titanic). Note that because the passenger survival information is public, the Kaggle challenge leaderboard is spammed with people submitting actual real-world data as machine learning results, making it meaningless.\n\n## 1. Data Exploration\n\nWe first load the \"titanic.csv\" file into a Pandas dataframe. We will immediately print the first few lines of the file.","4ab9d6e1":"3. For \"deck\" column, since this is categorical, we use NA to indicate missing values","35d927f3":"Once the grid search for hyperparameter is done, we choose the best one and train our model","fd85c104":"## 4. Model Definition and Training\n\nWe are finally ready to define our machine learning model and do some prediction. We compare the prediction accuracy of several classes of binary classification model.\n\n\n### 4.1 Baseline Model\n\nFor our baseline, we will always predict that the passenger will not survive. We will compare the accuracy of this prediciton with the predictions of our machine learning models.","02f793f7":"As one can see, \"title\", \"passenger class\", \"assignment of deck\", \"embarked port\", and \"fare\" are the most important features to indicate survival. With the **engineered feature** of \"title\" being overwhelmingly the most important. \n\nWe can see from decision tree that indeed ones sex and wealth played an important role in one's survival rate. Age is not a good indicator. \n\nThe benefit of decision trees is that it allows us visualize and explain our finds. On other machine learning models, such as neural networks, and random forest, obtaining an intuition on how the features are used by the model is very difficult. Although they may be more accurate. ","88906937":"For neural network and regression models, since we are doing mathematical operations on the input data, we need to normalize the input data to similar ranges, so that no single category data will dominate the calculated result. We use the MinMaxScaler() function in sklearn to scale our inputs to (0,1).","70b6a702":"We have dropped the \"cabin\" column along with any other column not used.\n\nCategorical data needs to be transformed into **one-hot** vector representation for the model. Assigning an integer to each category does not work because we cannot do number operations on categorical data. Rather, we need an indicator for these data, which is what one-hot vectors are best suited for.","7fc82cc0":"Lets scan through the \"min_samples_split\" and \"min_samples_leaf\" to maximize accuracy.","159bceac":"Lets use the best parameters in our model (\"min_samples_split\"=2 and \"min_samples_leaf\"=77) and fit using training data. After fitting we will use test data to gauge the accuracy of the model","c5627b6e":"#### 4.4.2 Visualization\n\nWe can visualize our model here.\n\n![](model.png)","7bc8b957":"From these categories, will will drop \"boat\", \"body\" from the dataset since they cannot be used as predictors and they are also not the outcome. We will also drop the \"home.dest\" column since we already have many other good indicators of socio-economic status. Keeping the \"home.dest\" column will not cause harm, we just want to keep our model a simpler.","5d55a17d":"What about the deck the passenger resides in. Would a higher deck offer more chances of escape?","81c68d31":"We display the titles gathered from the \"name\" column. Note that aside from the common titles, there are some religions titles (\"Rev\"), some noble titles (\"Jonkheer\", \"Don\", etc), and some military titles (\"Capt\", etc). We will sort these titles into social status. Our intuition is that the social status of a person have an impact on their survival rate, and can increase our prediction accuracy when used.","60e8902a":"I definitly see a trend here. If you paid more than \\$70 for your ticket, your chances of survival are above 60%. If you paied less than \\$10, your chances are only at 20%. Lets look at passenger class.","13553733":"Let's see if any of the newly engineered features are good indicators of survival. Lets first look at deck.","db6132c2":"Like the decision tree model, the title is the most important feature considered. Fare of the passengers follows. **Again, an engineered feature is the most important here.** This shows the importance of feature engineering.","ec26a9f8":"#### 4.4.2 Visualization\n\nOnly the weights are available for us to examine. But in general gaining intuition from them is difficult. Thus we do not visualize the weights here","1f9354a5":"The accuracy of the model in our is 81%. \n\n### 4.2.2 Visualization\n\nLets generate the decision tree used by our model and visualize how the features are used to generate a prediction","013bdae9":"Our prediction accuracy is 81%","c9679233":"![Decision Tree](..\/input\/decision.png\/decision.png)\n\nThe model uses the \"title\", which is an **engineered feature**, at the top of the tree. The title of the passenger not only includes information on sex, but also soci-economic status of the passenger. We are very happy that a engineered feature is the most useful in the model. The second level uses \"deck\" and \"passenger class\". Deck is another engineered feature. So  deck information of a passenger indeed is a useful feature to determine rate survival. However, this is used in unexpected ways, that is, if deck information is available or not. \n\nWe will generate the \"importance\" of each feature below in detail below.","068fbbd9":"We notice that 72% of the female passengers survived, despite making up only 35% of the total passengers. So indeed women had a larger survival rate than men. But what about age? Lets plot the passenger age historgram.","ac1d9115":"It does look like children have a higher survival rate. The few elderly on the ship also survived. \n\nHow about soci-economic status. Do rich people have a survival rate higher than the poor? Lets look at fare first.","12fe5b40":"About 40% of the passengers survived. We know from historical records that women and children were given priority on the lifeboats. Was this true, and did it reflect in their survival rates? Lets take a look.","583bde78":"#### 4.3.2 Visualization\n\nLets look at the decision imprtance of the features used. ","287b893d":"So it looks like smaller families survived with higher rate, parents probably left with their children. Single passengers probably stayed behind more. Interestingly, larger family have lower survival rate. Perhaps they could not all fit into a single lifeboat and chose to stay behind together. \n\nThe newly engineered features are indeed good indicators to the survival rate. We will use them as part of our model to increase model accuracy. These features are easily obtained using human intuition, but is hard for a model to generate by itself.\n\n## 3. Data Augmentation\n\n### 3.1 Fill in Missing Values\n\nBefore we feed our features into any machine model, we need to fill in the missing values and make sure the data format is correct. First, let's see which columns has missing values.","21151dce":"Let use directly goto the point and see how many passengers survived the disaster.","452af328":"We immediately notice that the second column \"survived\" is our prediction result. The \"name\" column contains not only the first and last name, but also the **title**, which could be an important indicator of socio-economic status, which could be a factor in the survival probability. The \"sibsp\" columns contains the number of siblings and spouses a passenger have on the ship. The \"parch\" column is the number of parent or guardian the passenger have on the ship. The two columns added together is the size of the family together on the ship. The \"ticket\" column is the ticket number, which is unique for vast majority of the passengers. Some tickets have alphabets in front. The \"embarked\" column is the port of departure. This could be another indicator of socio-economic status along with the \"fare\" column. The \"boat\" and \"body\" column is information that should not be used to predict survival rate, since it is statistics gathered after the event. These two columns should be dropped. The \"cabin\" column is very interesting, it contains the **deck** information and the room number. Which deck the passenger resides in could be an indicator of survival rate, and should be extracted. Room number on the other hand, whithout knowing the actual layout of the ship, is not very useful. Finally, the \"home.dest\" column shows the home and destination of the passenger, which could be another indicator of socio-economic status. \n\nWe also immdediatly see that some data is missing. We need to examine in detail which data is missing and determine the best way to fill in these missing data without skewing the prediction results.\n\nWe can use the folloing command to list the categories in the dataset.","c931b014":"### 4.5 Neural Network\n\nNeural network allows the *non-linear* combination of features to predict an outcome. This is achieved using non=linear activations such as \"relu\". Furthermore, we can have multiple layers of neural networks and re-combine the already combined features again. This makes neural networks more powerful than logistic regression. However, for simple datasets like the Titanic dataset, neural network will not signifcantly out perform other models.\n\n#### 4.5.1 Hyper-Parameter Tuning\n\nMany parameters can be tuned in neural networks. This include the number of layers, size of each layer, activation of each layer, among others. For sake of simplicity, we will not tune hyper-parameters here. This is a topic that is too complicated to be covered here.","011f5d67":"Looks like most of the pasengers are in their twenties and thirties. There are lots of children too. How about their survival rate? ","1a9b8409":"Clearly 1st class passengers have an advantage of 3rd class.\n\nNow that we have explored our dataset we have fairly good idea on what features are good indicators of survival rate. We maybe able to increase the accuracy of our prediction by engineer new features. For example, maybe doctors have higher survival rate, and military personel have lower survival rate. We can engineer additional features from the features that are already given to us. These additional features represent human intuition, and is very difficult for a machine model to discern. ","c4254a8d":"What about the size of the family? Would a larger family have more chances of survival?","3d691ac2":"We split our data into test data and training data.","1c3fbb3c":"### 4.4 Logistic Regression\n\nLogic regression combines features together in a linear fashion to predict the outcome. Each feature is weighted differently and summed together. The optimum weights are trained using gradient descent.\n\nIn linear regression, the inputs should be normalized to the same range. This is so that no single feature will dominate the weighted combinations.\n\n#### 4.4.1 Hyper-Parameter Tuning\n\nWe will not tune any hyper-parameters for linear regression","40c129bd":"|        Model        | Test Accuracy |\n|:-------------------:|:-------------:|\n|       Baseline      |     0.6512          |\n|    Decision Tree    |    0.8102         |\n|    Random Forest    |      0.8272         |\n| Logistic Regression |      0.8102         |\n|    Neural Network   |    0.8036           |"}}