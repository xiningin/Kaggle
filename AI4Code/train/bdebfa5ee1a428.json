{"cell_type":{"7bc64247":"code","191b8d48":"code","6c341bb5":"code","edf225f1":"code","73fe1efe":"code","d284bdf0":"code","76df7604":"code","e903a359":"code","02cb0e5e":"code","58cf5378":"code","a5d58f3b":"code","679f0072":"code","3279aac9":"code","7a7dc235":"code","2f2abb72":"code","4927e99d":"code","a17678fb":"code","a5ff1818":"code","3cab9b17":"code","2f163d7f":"code","c94bf6ea":"code","6cdbe480":"code","96f453e7":"code","bd918c59":"code","4f69937f":"code","13bdf6c2":"markdown","57914192":"markdown","3df92409":"markdown","82c8e128":"markdown","881e11ff":"markdown","405fd73c":"markdown","87685ad1":"markdown","609c52f9":"markdown","a0a00246":"markdown","755f7b71":"markdown","b22ca0f3":"markdown","cbad1ecb":"markdown","be0b998e":"markdown","b7f6c325":"markdown","41a11fe8":"markdown","a560d891":"markdown","d86c63ff":"markdown","7e97cb7f":"markdown","af454e58":"markdown","9b736e2b":"markdown","b233ef6f":"markdown","5820b891":"markdown","8fa320e3":"markdown"},"source":{"7bc64247":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob \nimport skimage.draw","191b8d48":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\n","6c341bb5":"os.chdir('Mask_RCNN')","edf225f1":"ROOT_DIR = '\/kaggle\/project'","73fe1efe":"sys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","d284bdf0":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    NAME = 'fruits'\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8\n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 4  # background + 3 fruit classes\n    \n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n    TRAIN_ROIS_PER_IMAGE = 32\n\n    STEPS_PER_EPOCH = 25\n    \nconfig = DetectorConfig()\nconfig.display()","76df7604":"class DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n    \n    def load_labels(self, labels_list):\n        for i, label in enumerate(labels_list):\n            self.add_class('fruits', i + 1, label)\n            \n    def load_dataset(self, images_obj):\n        for image_obj in images_obj:\n            image_id = image_obj['image_id']\n            image_path = image_obj['image_path']\n            num_ids = image_obj['num_ids']\n            polygons = image_obj['polygons']\n            width = image_obj['width']\n            height = image_obj['height']\n            self.add_image(\"fruits\", image_id=image_id, path=image_path,\n                           width=width, height=height, polygons=polygons,num_ids=num_ids)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def draw_shape(self, image, shape, dims, color):\n        \"\"\"Draws a shape from the given specs.\"\"\"\n        # Get the center x, y and the size s\n        x, y, s = dims\n        if shape == 'square':\n            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n        elif shape == \"circle\":\n            cv2.circle(image, (x, y), s, color, -1)\n        elif shape == \"triangle\":\n            points = np.array([[(x, y-s),\n                                (x-s\/math.sin(math.radians(60)), y+s),\n                                (x+s\/math.sin(math.radians(60)), y+s),\n                                ]], dtype=np.int32)\n            cv2.fillPoly(image, points, color)\n        return image\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        info = self.image_info[image_id]\n        num_ids = info['num_ids']\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        num_ids = np.array(num_ids, dtype=np.int32)\n        return mask, num_ids","e903a359":"train_image_path = os.path.join('..\/..\/input', 'train_zip')\ntest_image_path = os.path.join('..\/..\/input', 'test_zip')","02cb0e5e":"!pip install xmltodict","58cf5378":"labels = [\"apple\", \"banana\", \"orange\"]","a5d58f3b":"def parse_single_annotation(label_obj):\n    #print(label_obj)\n    name = label_obj['name']\n    # Get label\n    num_id = labels.index(name) + 1\n    bb_box = label_obj['bndbox']\n    # Extract the xmin xmax ymin and ymax of bounding box\n    xmin = int(bb_box['xmin'])\n    xmax = int(bb_box['xmax'])\n    ymin = int(bb_box['ymin'])\n    ymax = int(bb_box['ymax'])\n    # Convert it into polygon format. So we need 5 points for both x and y\n    all_points_x = [xmin, xmax, xmax, xmin, xmin]\n    all_points_y = [ymin, ymin, ymax, ymax, ymin]\n    return all_points_x, all_points_y, num_id\n    ","679f0072":"import xmltodict\nimport json\ntrain_images = []\ndef transform_annotations(image_path):\n    # Start the index from 100\n    curr_idx = 100\n    images_list = []\n    # List the files in the training or test path\n    for i in os.listdir(os.path.join(image_path)):\n        # Get the image path\n        img_path = os.path.join(image_path, i)\n        split_img_path = i.split('.')\n        # check if the file is a .jpg ext. We ignore .xml file as they will be parsed based on .jpg file name\n        if split_img_path[1] == 'jpg':\n            # Define dict key value pair required in coco dataset\n            polygons = []\n            num_ids = []\n            # Read the image file \n            file_data = cv2.imread(img_path)\n            # Get the heigh and width. OpenCV shape is in the format h, w, depth\n            height, width, _ = file_data.shape\n            # Open the xml file which has the same name of the image we have opened for this iteration\n            with open(os.path.join(image_path, split_img_path[0] + '.xml')) as fd:\n                # Load the xml -> convert xml to dict -> convert to json\n                bb_file = json.loads(json.dumps(xmltodict.parse(fd.read())))\n                # There are two case - bb_file['annotation']['object'] can exist as a single dict or as a list of dict.\n                # Thus, we need to do a check to see whether it is a list or not.\n                # If the value is a data type of list:\n                if isinstance(bb_file['annotation']['object'], list):\n                    # Loop through each dict in the list\n                    for obj in bb_file['annotation']['object']:\n                        # Parse each annotation individually\n                        all_points_x, all_points_y, num_id = parse_single_annotation(obj)\n                        # Append the points into polygon list\n                        polygons.append({\n                            'all_points_x': all_points_x,\n                            'all_points_y': all_points_y\n                        })\n                        # Append the id into the num_ids list\n                        num_ids.append(num_id)\n                # If the ['object'] key only contains a dict value\n                else:\n                    # We just need to parse a single annotation\n                    all_points_x, all_points_y, num_id = parse_single_annotation(bb_file['annotation']['object'])\n                    # Append it into polygon and num_ids list\n                    polygons.append({\n                        'all_points_x': all_points_x,\n                        'all_points_y': all_points_y\n                    })\n                    num_ids.append(num_id)\n            # For this image, we need to create a dict to represent it and all the corresponding annotations represented by polygons and num_ids key list\n            image_label = {\n                'image_path': img_path,\n                'image_id': curr_idx,\n                'polygons': polygons,\n                'num_ids': num_ids,\n                'height': height,\n                'width': width\n            }\n            curr_idx = curr_idx + 1\n            # Append it into the images_list\n            images_list.append(image_label)\n    return images_list\n\n","3279aac9":"train_images = transform_annotations(os.path.join(train_image_path, 'train'))\nprint(train_images[0:5])\ndataset_train = DetectorDataset()\ndataset_train.load_labels(labels)\ndataset_train.load_dataset(train_images)\ndataset_train.prepare()","7a7dc235":"test_images = transform_annotations(os.path.join(test_image_path, 'test'))\ndataset_val = DetectorDataset()\ndataset_val.load_labels(labels)\ndataset_val.load_dataset(test_images)\ndataset_val.prepare()","2f2abb72":"from mrcnn import visualize\nfor i in range(5):\n    image_id = train_images[i]['image_id']\n    mask, num_id = dataset_train.load_mask(i)\n    img_data = cv2.imread(train_images[i]['image_path'])\n    num_id = [x - 1 for x in num_id]\n    visualize.display_top_masks(img_data, mask, num_id, labels)","4927e99d":"!cd \/kaggle && mkdir project\nsys.path.append(ROOT_DIR)\nMODEL_DIR = os.path.join(ROOT_DIR, 'logs')\n# Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)\n\n","a17678fb":"# Create model in training mode\nmodel = modellib.MaskRCNN(mode=\"training\", config=config,\n                          model_dir=MODEL_DIR)","a5ff1818":"# Which weights to start with?\ninit_with = \"coco\"  # imagenet, coco, or last\n\nif init_with == \"imagenet\":\n    model.load_weights(model.get_imagenet_weights(), by_name=True)\nelif init_with == \"coco\":\n    # Load weights trained on MS COCO, but skip layers that\n    # are different due to the different number of classes\n    # See README for instructions to download the COCO weights\n    model.load_weights(COCO_MODEL_PATH, by_name=True,\n                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n                                \"mrcnn_bbox\", \"mrcnn_mask\"])\nelif init_with == \"last\":\n    # Load the last model you trained and continue training\n    model.load_weights(model.find_last(), by_name=True)","3cab9b17":"# Train the head branches\n# Passing layers=\"heads\" freezes all layers except the head\n# layers. You can also pass a regular expression to select\n# which layers to train by name pattern.\nmodel.train(dataset_train, dataset_val, \n            learning_rate=config.LEARNING_RATE, \n            epochs=30, \n            layers='heads')","2f163d7f":"os.listdir(MODEL_DIR)","c94bf6ea":"dir_names = os.listdir(MODEL_DIR)\ndir_names = sorted(dir_names)\n\nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(MODEL_DIR, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else: \n      \n      checkpoint = os.path.join(dir_name, checkpoints[-1])\n      fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","6cdbe480":"class InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","96f453e7":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    class_ids = [x - 1 for x in class_ids]\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","bd918c59":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\nstart_idx = 0\nfor i in range(start_idx, start_idx + 3):\n    \n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    plt.subplot(6, 2, 2*(i-start_idx) + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*(i-start_idx) + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","4f69937f":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf \/kaggle\/working\/Mask_RCNN","13bdf6c2":"## Now we can start training the actual dataset","57914192":"# Create Configuration class for Detector","3df92409":"# Store annotations into the respective train and test dataset","82c8e128":"## Parse the images and annotations and it in a single array","881e11ff":"## Load the test dataset and prepare ","405fd73c":"# Start training","87685ad1":"# Predict output","609c52f9":"## Load and visualize the pixel mask for the first 5 objects","a0a00246":"# Convert xml file information to dict","755f7b71":"## Define labels","b22ca0f3":"## Load the weights of the COCO dataset","cbad1ecb":"# Set root directory for training and test dataset","be0b998e":"# Download Mask RCNN library","b7f6c325":"\n# Visualize image output","41a11fe8":"## Create the model with the specified configuration defined at the top of the file","a560d891":"## Download the coco weights","d86c63ff":"# Create inference configuration","7e97cb7f":"# Import MaskRCNN dependencies","af454e58":"## Parse a single annoation\n- Extract the dict object and convert it into cocos format\n\nExample of format:\n{\n    \"all_points_x\": [...],\n    \"all_points_y\": [...],\n    \"num_id\": 3\n}","9b736e2b":"## Load the training dataset and prepare","b233ef6f":"# Extend existing Dataset class to customize methods","5820b891":"# Import dependencies","8fa320e3":"## Download xmltodict helper dependencies"}}