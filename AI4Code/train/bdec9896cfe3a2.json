{"cell_type":{"0159280d":"code","59a21536":"code","81b4c978":"code","cb17d5ce":"code","ac2b9efc":"code","9422dfd5":"code","9c34516d":"code","d7d5de5c":"code","8b56f48b":"code","a1e52ea7":"code","dd9a76eb":"code","7bde1b84":"code","65564491":"code","17e8a254":"code","c72ef15a":"code","8c9c51bc":"code","094b81ff":"code","c524fa24":"code","912d3c9c":"code","d037c671":"code","593e6ba2":"markdown"},"source":{"0159280d":"# IMPORTING THE GOOD STUFF\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection\nimport sklearn.svm as svm\nimport sklearn.metrics","59a21536":"train_dataframe=pd.read_csv(\"..\/input\/train.csv\")\ntest_dataframe=pd.read_csv(\"..\/input\/test.csv\")\ngender_submission = pd.read_csv(\"..\/input\/gender_submission.csv\")\nprint(\"Got them!\")\n\n# cleansing as before. Define Cabin? as cabin known (or did they have one? presumably they did)\ntrain_dataframe['Cabin?'] = np.where(pd.isnull(train_dataframe['Cabin']), 0,1)\npredictors = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Cabin?', 'Embarked']\n\n","81b4c978":"# missing values for age - impute from medians for sex and class\ndef impute_age(cols):\n    age = cols[0]\n    sex = cols[1]\n    pclass = cols[2]\n    if pd.isnull(age):\n        if sex == 'female':\n            if pclass == 1:\n                return 35\n            elif pclass == 2:\n                return 28\n            elif pclass == 3:\n                return 21.5\n            else:\n                print('error! pclass should be 1, 2, or 3 but it is '+pclass+'!')\n                return np.nan\n        elif sex == 'male':\n            if pclass == 1:\n                return 40\n            elif pclass == 2:\n                return 30\n            elif pclass == 3:\n                return 25\n            else:\n                print('error! pclass should be 1, 2, or 3 but it is '+pclass+'!')\n                return np.nan\n        else: print('error! sex should be female or male but it is '+sex+'!')\n    else:\n        return age\n\n    \ntrain_dataframe['Age']=train_dataframe[['Age','Sex','Pclass']].apply(impute_age,axis=1)","cb17d5ce":"# FOR SOME MODELS, WILL NEED TO HAVE ONLY NUMERICAL FEATURES\n# sex and embarked are categorical\nmale = pd.get_dummies(train_dataframe['Sex'], drop_first=True)\nport = pd.get_dummies(train_dataframe['Embarked'], drop_first=True)\ntrain = pd.concat([train_dataframe, male, port],axis=1)\ntrain[pd.isnull(train['Embarked'])==True]\n# just change those to 0.33 don't know\ntrain.loc[61,'Q']=0.33\ntrain.loc[61,'S']=0.33\ntrain.loc[829,'Q']=0.33\ntrain.loc[829,'S']=0.33\ntrain[pd.isnull(train['Embarked'])==True]\npredictors_num = ['Pclass', 'male', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin?', 'Q','S']\n#train = train_dataframe","ac2b9efc":"# Train test split the training data\nX_tr, X_test, y_tr, y_test = sklearn.model_selection.train_test_split(train[predictors_num],train['Survived'], random_state = 1)","9422dfd5":"# define and try model\n\n# NOTES TO SELF:\n# there are several types of support vector machine in sklearn. I've only read about a general case\n# Linear SVC only has a linear kernel. I think this won't be the best kind of kernel for my complicated data,\n# although some variables are binary and others have cut-off points where things change (e.g. age) so it might be good\n\n# SVC is what I studied; \n# NuSVC is a reparamaterisation of SVC so that you can control nu, the upper bound on the fraction of training errors\n# (which means you force more support vectors)\n\n# SVR is regression rather than categorical so considers the length of the y variables rather than just whether they are 1 or 0\n# (so you get difference, not dot products, in the definition)\n\n# In this case I want categorical responses 0 or 1 so use SVC or NuSVC\n# I'm going to try SVC\n# C is the upper bound of the support vector coefficients\n# kernels: \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. rbf is radial basis function, a function of the distance of the \n# points from the origin (or from a given point...)\n# gamma is the kernel coefficient. It used to be 1\/n_features but will now also divide by the standard dev of X which I think is good\n# (check - in this case it will be set to 'scale'. If not, set gamma='scale')\n# for poly or sigmoid can also set coef0 and for poly can set degree.\n\nsup_vec = svm.SVC(gamma = 'scale')\nsup_vec.fit(X_tr, y_tr)","9c34516d":"# make and test some predictions!!\npredictions = sup_vec.predict(X_test)\nsklearn.metrics.confusion_matrix(y_test,predictions)\n# Not doing very well at knowing who survived - thinks too many died.","d7d5de5c":"# How about if it's linear?\n# needs more iterations to converge - doesn't seem to be working too well as default is 1000 and it won't converge on 1000000\nsup_vec = svm.LinearSVC(max_iter=1000000)\nsup_vec.fit(X_tr, y_tr)\npredictions = sup_vec.predict(X_test)\nsklearn.metrics.confusion_matrix(y_test,predictions)\n\n# but even though it doesn't converge, it still predicts better than the radial basis function one...","8b56f48b":"# My intuition tells me that too many features will damage this model\npreds = ['Pclass', 'male', 'Age', 'Fare', 'Cabin?']\n# should be more than enough. Now let's look at both of those models again\nsup_vec_C = svm.SVC(gamma = 'scale')\nsup_vec_C.fit(X_tr[preds], y_tr)\npredictions = sup_vec_C.predict(X_test[preds])\nsklearn.metrics.confusion_matrix(y_test,predictions)\n\n# worse than before!\n","a1e52ea7":"sup_vec_L = svm.LinearSVC(max_iter=1000000)\nsup_vec_L.fit(X_tr[preds], y_tr)\npredictions = sup_vec_L.predict(X_test[preds])\nsklearn.metrics.confusion_matrix(y_test,predictions)\n\n# but even so it still makes better predictions","dd9a76eb":"# does the SVC think it fitted correctly? 0 means no problem, 1 means problem\nprint(\"RBF: \"+str(sup_vec_C.fit_status_))","7bde1b84":"# the linear one makes better predictions but doesn't converge!\n# how about quadratic? Next best thing\nsup_vec_Q = svm.SVC(gamma = 'scale', kernel='poly', degree=2)\nsup_vec_Q.fit(X_tr[preds], y_tr)\npredictions = sup_vec_Q.predict(X_test[preds])\nprint(sklearn.metrics.confusion_matrix(y_test,predictions))\nprint('fit status: '+str(sup_vec_Q.fit_status_))\n# Yes!","65564491":"# And cubic, just to be sure\n# this one takes a VERY long time and actually gives less good results than the quadratic (as expected) so going to get rid of it\n# sup_vec_Cu = svm.SVC(gamma = 'scale', kernel='poly', degree=3)\n# sup_vec_Cu.fit(X_tr[preds], y_tr)\n# predictions = sup_vec_Cu.predict(X_test[preds])\n# print(sklearn.metrics.confusion_matrix(y_test,predictions))\n# print('fit status: '+str(sup_vec_Cu.fit_status_))","17e8a254":"# For some strange reason, although the linear model didn't work, this did - but it gives less good predictions.\n# Maybe because of gamma? See below without gamma\nsup_vec_Lin = svm.SVC(gamma = 'scale', kernel='linear')\nsup_vec_Lin.fit(X_tr[preds], y_tr)\npredictions = sup_vec_Lin.predict(X_test[preds])\nprint(sklearn.metrics.confusion_matrix(y_test,predictions))\nprint('fit status: '+str(sup_vec_Lin.fit_status_))\n\n# but I'm going to stick with the quadratic\n","c72ef15a":"sup_vec_Lin = svm.SVC(kernel='linear')\nsup_vec_Lin.fit(X_tr[preds], y_tr)\npredictions = sup_vec_Lin.predict(X_test[preds])\nprint(sklearn.metrics.confusion_matrix(y_test,predictions))\nprint('fit status: '+str(sup_vec_Lin.fit_status_))\n# Makes no difference whatsoever","8c9c51bc":"sup_vec_Lin.coef_\n# for ['Pclass', 'male', 'Age', 'Fare', 'Cabin?'] - so sex is by FAR the most important, then class and cabin\n# fare is basically irrelevant\n# what happens if we drop fare?","094b81ff":"pre = ['Pclass', 'male', 'Age', 'Cabin?']\nmodels = [1,2,3]\nfor i in models:\n    if i == 1:\n        sup_vec = svm.SVC(gamma = 'scale', kernel='linear')\n    else:\n        sup_vec = svm.SVC(gamma = 'scale', kernel = 'poly', degree = i)\n    sup_vec.fit(X_tr[pre], y_tr)\n    predictions = sup_vec.predict(X_test[pre])\n    print('Model degree '+str(i)+' confusion matrix:')\n    print(sklearn.metrics.confusion_matrix(y_test,predictions))\n    print('fit status: '+str(sup_vec.fit_status_))","c524fa24":"# they're all pretty crap but the quadratic model with fare in there was the best so let's roll that out onto all the data\nsup_vec_Q = svm.SVC(gamma = 'scale', kernel='poly', degree=2)\nsup_vec_Q.fit(train[preds], train['Survived'])\n# before getting the predictions need to apply cleansing to test data\ntest_dataframe['Cabin?'] = np.where(pd.isnull(test_dataframe['Cabin']), 0,1)\n\n# for age, I found the medians unchanged by using all the data\ntest_dataframe['Age']=test_dataframe[['Age','Sex','Pclass']].apply(impute_age,axis=1)\n\nsex = pd.get_dummies(test_dataframe['Sex'])\nport = pd.get_dummies(test_dataframe['Embarked'])\nsex.drop('female', axis = 1, inplace=True)\nport.drop('C', axis = 1, inplace = True)\ntest = pd.concat([test_dataframe, sex, port],axis=1)\n# and there are no missing values for port this time\n\n# but there was one missing fare\n# I gave him the median fare for his class\ntest['Fare']=np.where(pd.isnull(test['Fare'])==True, 7.8958, test['Fare'])\n\npredictions = sup_vec_Q.predict(test[preds])\nfinal = pd.DataFrame(predictions, columns = ['Survived'])","912d3c9c":"output = pd.concat([test['PassengerId'],final], axis=1)\noutput","d037c671":"output.to_csv('csv_to_submit.csv', index=False)","593e6ba2":"I am learning using the Titanic data.\n\nI'm going to get ready to try applying all of the models I know and compare what they do\n- logistic regression\n- decision trees and random forests\n- support vector machine\n- k nearest neighbours\n- neural network\n\nIn this kernel, I'll start by getting ready and considering which features to use.\n\nI'll also use the feature selection of some models to determine which features should be used by the others."}}