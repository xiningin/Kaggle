{"cell_type":{"c6dd000c":"code","466b9692":"code","a0c99ba4":"code","fc694c36":"code","833df617":"code","16714171":"code","4d673f9e":"code","69c02a10":"code","ed366bdb":"code","bac0dde4":"code","adc9cd81":"code","3795c1af":"code","6f4f1e2e":"code","6e378ab8":"code","7fc73ea1":"code","96fe6274":"code","65a25cd0":"code","0d9bcd0f":"code","c3d919e4":"code","60e1f122":"code","debd9fdf":"code","5fd0f9c8":"code","4fcff491":"code","2f079e73":"code","a78379d5":"code","dde796ce":"code","62288a71":"code","32de4e2e":"code","6a7167f8":"code","f348b255":"code","2a9b4c68":"code","8146ece1":"code","757aadac":"code","95415fac":"code","ec30b912":"code","d2a19fd7":"code","44ae83b3":"code","793467d7":"code","aaa42219":"code","60800b39":"code","15b8dc7f":"code","d4dc58eb":"code","9dcb9b0d":"code","a86762b6":"code","f45ab37c":"code","3dfd3b3c":"code","480f343a":"code","48146c69":"code","b09c3183":"code","8c7078dc":"code","7c15bfc5":"code","c80f53df":"code","1a2bbd9b":"code","1500aab4":"code","05a9b3f4":"code","89fc3120":"code","cc506e73":"code","4f23c559":"code","3ffc89a9":"code","26c74f64":"code","cd848abc":"markdown","cfb78d82":"markdown","f89b1129":"markdown","3ca2ef1f":"markdown","93255db0":"markdown","2ee53577":"markdown","c2c30e4c":"markdown","d4d24353":"markdown","cddd883e":"markdown","bc8a823b":"markdown","84d7c094":"markdown","3c94e0bf":"markdown","d82b61d1":"markdown","b1237267":"markdown","760c5a6c":"markdown","a56654a2":"markdown","5937d265":"markdown","b66b874d":"markdown","edca61d6":"markdown","99315e87":"markdown","39af8351":"markdown","8db979b3":"markdown","b4a58dfc":"markdown","dacbff99":"markdown","c784cb02":"markdown","eb1e6ffc":"markdown","2faa8413":"markdown","ba2c0e33":"markdown","eeb2aee4":"markdown","c87c66a1":"markdown","7cf1f7d9":"markdown","f0f0d6ef":"markdown","51649e43":"markdown","c1f5ba09":"markdown","74e441ee":"markdown","f69dc160":"markdown","9ee6a2c8":"markdown","c2d06319":"markdown"},"source":{"c6dd000c":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nprint(os.listdir(\"..\/input\/kernel-files\"))","466b9692":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape, test.shape","a0c99ba4":"# Preview the data we're working with\ntrain.head()","fc694c36":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","833df617":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","16714171":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train.columns:\n    if train[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","4d673f9e":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","69c02a10":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","ed366bdb":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","bac0dde4":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));","adc9cd81":"data = pd.concat([train['SalePrice'], train['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","3795c1af":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","6f4f1e2e":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","6e378ab8":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","7fc73ea1":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","96fe6274":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","65a25cd0":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","0d9bcd0f":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","c3d919e4":"# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","60e1f122":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","debd9fdf":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","5fd0f9c8":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_features = handle_missing(all_features)","4fcff491":"# Let's make sure we handled all the missing values\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","2f079e73":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","a78379d5":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","dde796ce":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","62288a71":"# Normalize skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","32de4e2e":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","6a7167f8":"all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","f348b255":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","2a9b4c68":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","8146ece1":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","757aadac":"all_features.head()","95415fac":"all_features.shape","ec30b912":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]","d2a19fd7":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape","44ae83b3":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 150))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(X[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","793467d7":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","aaa42219":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","60800b39":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","15b8dc7f":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","d4dc58eb":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","9dcb9b0d":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","a86762b6":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","f45ab37c":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","3dfd3b3c":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","480f343a":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))","48146c69":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, train_labels)","b09c3183":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, train_labels)","8c7078dc":"print('Svr')\nsvr_model_full_data = svr.fit(X, train_labels)","7c15bfc5":"print('Ridge')\nridge_model_full_data = ridge.fit(X, train_labels)","c80f53df":"print('RandomForest')\nrf_model_full_data = rf.fit(X, train_labels)","1a2bbd9b":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, train_labels)","1500aab4":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","05a9b3f4":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","89fc3120":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","cc506e73":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","4f23c559":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))","3ffc89a9":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression1.csv\", index=False)","26c74f64":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_regression2.csv\", index=False)","cd848abc":"## Features: a deep dive","cfb78d82":"## Create interesting features","f89b1129":"## The Goal\n\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features.\n- Our models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses will affect our score equally.\n\n## Key features of the model training process in this kernel:\n- **Cross Validation:** Using 12-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- **Blending:** All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.","3ca2ef1f":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","93255db0":"## Recreate training and test sets","2ee53577":"<p align=center>\n<img src=\"https:\/\/journal.firsttuesday.us\/wp-content\/uploads\/CA-Sales-Home-Volume.png\" width=\"50%\"><\/p>\n","c2c30e4c":"Visualize some of the features we're going to train our models on.","d4d24353":"## Key features of the model training process:\n- **Cross Validation:** Using 12-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- **Blending:** All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.","cddd883e":"## Fill missing values","bc8a823b":"## The Goal\n\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features.","84d7c094":"and plot how the features are correlated to each other, and to SalePrice","3c94e0bf":"## Blend models and get predictions","d82b61d1":"We can now move through each of the features above and impute the missing values for each of them.","b1237267":"## Fix skewed features","760c5a6c":"Let's visualize some of the features in the dataset","a56654a2":"## Identify the best performing model","5937d265":"Let's plot how SalePrice relates to some of the features in the dataset","b66b874d":"Let's take a look at the distribution of the SalePrice.","edca61d6":"## Setup models","99315e87":"# EDA","39af8351":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I'll use for making the final predictions.","8db979b3":"## Train models","b4a58dfc":"### Get cross validation scores for each model","dacbff99":"Let's plot the SalePrice again.","c784cb02":"All the features look fairly normally distributed now.","eb1e6ffc":"## Now that we have some context, let's get started!","2faa8413":"# Feature Engineering","ba2c0e33":"### Fit the models","eeb2aee4":"## Encode categorical features","c87c66a1":"## Feature transformations\nLet's create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won't be able to reliably tell if log(feature) or feature^2 is a predictor of the SalePrice.","7cf1f7d9":"# Train a model","f0f0d6ef":"The SalePrice is now normally distributed, excellent!","51649e43":"There are no missing values anymore!","c1f5ba09":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data.","74e441ee":"ML models have trouble recognizing more complex patterns (and we're staying away from neural nets for this competition), so let's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house.","f69dc160":"Numerically encode categorical features because most models can only handle numerical features.","9ee6a2c8":"## Setup cross validation and define error metrics","c2d06319":"## Submit predictions"}}