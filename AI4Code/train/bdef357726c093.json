{"cell_type":{"70039f9f":"code","2729338a":"code","7bb23557":"code","3b22bfe2":"code","893db902":"code","6b7d3c70":"code","29316338":"code","9605fcf9":"code","535fe4e3":"code","b4fee76a":"code","76fada29":"code","9e011335":"code","0466ecf5":"code","e10df372":"code","1c358960":"code","2f8b04a8":"code","74d954d1":"code","fd7cf5a5":"code","c21905e8":"code","6bffc3a5":"code","e4e3b3d6":"code","91888f91":"code","6331c0b1":"code","1d00989f":"code","220cf9c1":"code","9ecad424":"code","06ee580e":"code","249d42b8":"code","527c3911":"code","ad7ea076":"code","54065c0b":"code","ad13eaa6":"code","61af2552":"code","fbcac917":"markdown","6b856c6f":"markdown","36ca4b19":"markdown","38d9308b":"markdown","fb21cfd5":"markdown","94186c04":"markdown","7518064c":"markdown","cefa6f87":"markdown","fd6d442a":"markdown"},"source":{"70039f9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2729338a":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport scipy.stats as st \nimport os","7bb23557":"df=pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","3b22bfe2":"df.info()","893db902":"pd.set_option('display.max_columns',40)\ndf.describe()","6b7d3c70":"df1=df.drop(['EmployeeCount','EmployeeNumber','StandardHours'],axis=1)\nordinal=['Education','EnvironmentSatisfaction','JobInvolvement','JobLevel','JobSatisfaction','PerformanceRating',\n 'RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance']","29316338":"df.describe(include='object')","9605fcf9":"cat=[i for i in df1.columns if df[i].dtype=='O']\nnumerical=[i for i in df1.columns if i not in cat+ordinal]","535fe4e3":"len(cat)+len(numerical)+len(ordinal)","b4fee76a":"fig, axes=plt.subplots(3,6,figsize=(15,10))\naxes=axes.ravel()\nind=0\nfor i in numerical:\n    sns.boxplot(x=df1.Attrition,y=df[i],ax=axes[ind])\n    ind=ind+1\n    #plt.show()\n    ","76fada29":"fig, axes=plt.subplots(3,6,figsize=(20,10))\naxes=axes.ravel()\nind=0\nfor i in cat+ordinal:\n    plt.xticks(rotation=90)\n    sns.countplot(df[i],hue=df.Attrition,ax=axes[ind])\n    ind=ind+1\n   # plt.show()","9e011335":"df1['Attrition']=df1['Attrition'].map({'Yes':1,'No':0})\ndf1.Attrition.value_counts()\n","0466ecf5":"x=df1.drop('Attrition',axis=1)\ny=df1.Attrition","e10df372":"cat.remove('Attrition')\nx=pd.get_dummies(x,columns=cat,drop_first=True)","1c358960":"x.info()","2f8b04a8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV,KFold,cross_val_score,StratifiedKFold\nfrom sklearn.metrics import roc_curve,roc_auc_score,accuracy_score,classification_report,confusion_matrix,recall_score","74d954d1":"def mod_score(algo,x,y):\n    sfold=StratifiedKFold(random_state=7,shuffle=True)\n    mod=algo.fit(x,y)\n    cv1=cross_val_score(algo,x,y,cv=sfold,scoring='accuracy')\n    cv2=cross_val_score(algo,x,y,cv=sfold,scoring='roc_auc')\n    cv3=cross_val_score(algo,x,y,cv=sfold,scoring='recall')\n    print('\\nAccuracy : ',cv1.mean())\n    print('ROC AUC score : ',cv2.mean())\n    print('Recall: ',cv3.mean() )\n    return mod\n\n    ","fd7cf5a5":"def rand_search(algo,params,x,y):\n    rs=RandomizedSearchCV(algo,param_distributions=params,random_state=0,n_jobs=-1,n_iter=100,scoring='roc_auc',cv=10)\n    mod=rs.fit(x,y)\n    print(mod.best_score_)\n    return mod.best_params_","c21905e8":"def get_models(x,y):\n    rbp=rand_search(RandomForestClassifier(),rfc_params,x,y)\n    lbp=rand_search(LGBMClassifier(),lgb_params,x,y)\n    kbp=rand_search(KNeighborsClassifier(),knn_params,x,y)\n    lr=LogisticRegression(solver='liblinear')\n    rfc=RandomForestClassifier(**rbp)\n    lgb=LGBMClassifier(**lbp)\n    ss=StandardScaler()\n    knn=KNeighborsClassifier(**kbp)\n    x_ss=ss.fit_transform(x)\n    models={'Logistic Regression':lr,'Random Forest':rfc,'Light GBM':lgb,'knn':knn}\n    m={}\n    for i in models:\n        if i!='knn':\n            print('\\n',i)\n            m[i]=mod_score(models[i],x,y)\n        else:\n            print('\\n',i)\n            m[i]=mod_score(models[i],x_ss,y)\n            \n    return m","6bffc3a5":"rfc_params={'n_estimators':st.randint(50,300),\n    'criterion':['gini','entropy'],\n    'max_depth':st.randint(2,20),\n    'min_samples_split':st.randint(2,100),\n    'min_samples_leaf':st.randint(2,100)}\nlgb_params={ 'num_leaves':st.randint(31,60),\n   'max_depth':st.randint(2,20),\n    'learning_rate':st.uniform(0,1),\n    'n_estimators':st.randint(50,300),\n    'min_split_gain':st.uniform(0,0.3)}\nknn_params={'n_neighbors': st.randint(5,30),\n    'leaf_size':st.randint(20,70) }","e4e3b3d6":"m0=get_models(x,y)","91888f91":"x1=x\nss=StandardScaler()\nd=ss.fit_transform(x)\nx1['sum']=d.sum(axis=1)\nx1['min']=d.min(axis=1)\nx1['max']=d.max(axis=1)\nx1['skew']=st.skew(d,axis=1)\nx1['kurt']=st.kurtosis(d,axis=1)\nx1['std']=d.std(axis=1)","6331c0b1":"m1=get_models(x1,y)","1d00989f":"plt.figure(figsize=[10,20])\nplt.xticks(rotation=90)\nsns.barplot(x=x1.columns,y=m1['Random Forest'].feature_importances_)","220cf9c1":"plt.figure(figsize=[10,20])\nplt.xticks(rotation=90)\nsns.barplot(x=x1.columns,y=m1['Light GBM'].feature_importances_)","9ecad424":"\nx2=x\nx2['MonInc\/ed']=x['MonthlyIncome']\/x['Education']\nx2['SalHike']=(x['PercentSalaryHike']\/100)*x['MonthlyIncome']\n","06ee580e":"cat.append('EducationField')","249d42b8":"m2=get_models(x2,y)","527c3911":"from imblearn.over_sampling import SMOTE,KMeansSMOTE,SMOTENC\n\n","ad7ea076":"def score(mod,x,y,samp_frac=0.5):\n    sfold=StratifiedKFold(random_state=7,shuffle=True)\n    #mod=algo.fit(x,y)\n    smote=SMOTE(sampling_strategy=samp_frac,random_state=7)\n    fold_auc=[]\n    fold_acc=[]\n    fold_recall=[]\n    for i,j in sfold.split(x,y):\n        xtr=x.iloc[i]\n        ytr=y.iloc[i]\n        xts=x.iloc[j]\n        yts=y.iloc[j]\n        x_sm,y_sm=smote.fit_resample(xtr,ytr)\n        m=mod.fit(x_sm,y_sm)\n        pred=m.predict(xts)\n        prob=m.predict_proba(xts)[:,1]\n        fold_acc.append(accuracy_score(yts,pred))\n        fold_auc.append(roc_auc_score(yts,prob))\n        fold_recall.append(recall_score(yts,pred))\n    print('\\nAccuracy : ',np.array(fold_acc).mean())\n    print('ROC AUC score : ',np.array(fold_auc).mean())\n    print('Recall: ',np.array(fold_recall).mean() )\n    \n\ndef smote_models(x,y):\n    rbp=rand_search(RandomForestClassifier(),rfc_params,x,y)\n    lbp=rand_search(LGBMClassifier(),lgb_params,x,y)\n    kbp=rand_search(KNeighborsClassifier(),knn_params,x,y)\n    lr=LogisticRegression(solver='liblinear')\n    rfc=RandomForestClassifier(**rbp)\n    lgb=LGBMClassifier(**lbp)\n    ss=StandardScaler()\n    knn=KNeighborsClassifier(**kbp)\n    \n    models={'Logistic Regression':lr,'Random Forest':rfc,'Light GBM':lgb,'knn':knn}\n    m={}\n    for i in models:\n\n        print('\\n',i)\n        m[i]=score(models[i],x,y)\n","54065c0b":"smote_models(x,y)","ad13eaa6":"smote_models(x1,y)","61af2552":"x","fbcac917":"Let us build an initial base model without any feature elimination or feature engineering to evalute and establish a baseline model which can be improved upon","6b856c6f":"## SMOTE","36ca4b19":"The main problem as we see is that recall is not increasing. This is because of imbalance in the data set. The models are biased towards 0 and are not predicting any 1. This is because the model is giving prominence to only 0. This is a common problem in imbalanced datasets. \n\nThis can be solved by under sampling or over sampling.\nIn undersampling we may lose important data that may be required for creating rules. Hence we will do over sampling\nHere we use a technique called SMOTE(Synthetic Minority oversampling technique) which will create duplicate points for target 1 which will be close to the existing ones and solve the imbalance.","38d9308b":"Now we use plots to check for significant features based on which we may be able to create new features to improve accuracy\nWe use Box plots for numerical variables and count plots for categorical variables","fb21cfd5":"We can pick the required model and use it as required. ","94186c04":"# Building Models","7518064c":"# Exploratory Data Analysis\n\nWe will check the data for trends and draw insights. We will separate nominal and ordinal categorical variables. ","cefa6f87":"Now we try to make new features and see if they may have any affect on the results","fd6d442a":"# Fixing Imbalance"}}