{"cell_type":{"432ca17b":"code","c33d3f35":"code","ce5e419f":"code","d7f5a8e1":"code","73680830":"code","e5dd0b87":"code","fcc2a3c4":"code","08634e77":"code","f29b4d16":"code","771f608a":"code","06eeddaf":"code","ad636c40":"code","4fcb71d2":"code","f9b513a4":"code","5d85c6b6":"code","b1864d85":"code","544f4786":"code","73544fed":"code","315aeb40":"code","b5a3013c":"code","3a252c11":"code","7815f421":"code","a4d6b7be":"code","df2de596":"code","07ad05da":"code","4e771380":"code","1704f27c":"code","7a97d844":"code","5fdc1db3":"code","764f77c0":"code","3b8bfcd8":"code","3baf2662":"code","fd5e019b":"code","b6c73cc6":"code","41e90544":"code","15297f5b":"code","f8afd966":"code","ae50d90f":"code","d81df9b3":"code","05382adb":"code","cd9d1022":"code","ddf01843":"code","a8ecdc84":"code","69f1d9c1":"code","d0e21d6a":"code","0aff2290":"code","a85964d8":"code","679a704f":"code","a9ea6178":"code","f1664de1":"code","9179644c":"code","c3373598":"code","f2d8ba3a":"markdown","1f905bb3":"markdown","427a34fa":"markdown","3cce7d19":"markdown","ca7b25a5":"markdown","16636875":"markdown","6b55e17f":"markdown","0414e7a5":"markdown","f83238cc":"markdown","3e394611":"markdown","9bfabad0":"markdown","3d6788e5":"markdown","680ff314":"markdown","2ac6cb24":"markdown","56b7bf5e":"markdown","7e7ed9a6":"markdown","aaa08160":"markdown","5d0c8e24":"markdown","574c1e68":"markdown","8f812b35":"markdown","aa578d56":"markdown","97701656":"markdown","deee4a89":"markdown","07322236":"markdown","01d79ac0":"markdown","f911ffed":"markdown","b77ba064":"markdown","bf3e6603":"markdown","1659bb72":"markdown","c6725a02":"markdown","8c0dc700":"markdown","8c2b4ce2":"markdown","f10b8a1d":"markdown","a668948d":"markdown","bbda1376":"markdown","eeafae2d":"markdown","39b18677":"markdown","0469a1ca":"markdown","af3079f8":"markdown","8efaccee":"markdown","5358a062":"markdown","54eba38c":"markdown","c2f943fd":"markdown"},"source":{"432ca17b":"#Importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom math import * # module math\nimport matplotlib.pyplot as plt # visualization\nfrom PIL import Image\nimport seaborn as sns # visualization\nimport itertools\nimport io\nimport plotly.offline as py # visualization\npy.init_notebook_mode(connected=True) # visualization\nimport plotly.graph_objs as go # visualization\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff # visualization\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","c33d3f35":"telcom = pd.read_csv(r\"..\/input\/telecom-churn-datasets\/churn-bigml-80.csv\")\ntelcom_test = pd.read_csv(r\"..\/input\/telecom-churn-datasets\/churn-bigml-20.csv\")\ntelcom.head()","ce5e419f":"def dataoveriew(df, message):\n    print(f'{message}:\\n')\n    print(\"Rows:\", df.shape[0])\n    print(\"\\nNumber of features:\", df.shape[1])\n    print(\"\\nFeatures:\")\n    print(telcom.columns.tolist())\n    print(\"\\nMissing values:\", df.isnull().sum().values.sum())\n    print(\"\\nUnique values:\")\n    print(df.nunique())","d7f5a8e1":"dataoveriew(telcom, 'Overiew of the training dataset')","73680830":"dataoveriew(telcom_test, 'Overiew of the test dataset')","e5dd0b87":"trace = go.Pie(labels = telcom[\"Churn\"].value_counts().keys().tolist(),\n               values = telcom[\"Churn\"].value_counts().values.tolist(),\n               marker = dict(colors = ['royalblue','lime'],\n                             line = dict(color = \"white\", width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer churn in training data\",\n                        plot_bgcolor = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","fcc2a3c4":"#Separating columns to be visualized\nout_cols = list(set(telcom.nunique()[telcom.nunique()<6].keys().tolist()\n                    + telcom.select_dtypes(include='object').columns.tolist()))\nviz_cols = [x for x in telcom.columns if x not in out_cols] + ['Churn']\n\nsns.pairplot(telcom[viz_cols], diag_kind=\"kde\")\nplt.show()","08634e77":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#Removing correlated and unneccessary columns\ncol_to_drop = ['State', 'Area code', 'Total day charge', 'Total eve charge', \n               'Total night charge', 'Total intl charge']\n   \ntelcom = telcom.drop(columns = col_to_drop, axis = 1)\ntelcom_test = telcom_test.drop(columns = col_to_drop, axis = 1)\n\n#target column\ntarget_col = [\"Churn\"]\n\n#number of levels in feature to be a categorical feature\nnlevels = 6\n\n#Separating categorical and numerical columns\n#categorical columns\ncat_cols = list(set(telcom.nunique()[telcom.nunique()<nlevels].keys().tolist() \n                    + telcom.select_dtypes(include='object').columns.tolist()))\ncat_cols = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols = [x for x in telcom.columns if x not in cat_cols + target_col]\n#Binary columns with 2 values\nbin_cols = telcom.nunique()[telcom.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols:\n    telcom[i] = le.fit_transform(telcom[i])\n    telcom_test[i] = le.transform(telcom_test[i])\n\n#combining the train and test datasets\ntrainsize = telcom.shape[0]\ncomb = pd.concat((telcom, telcom_test), sort=False)\n\n#Duplicating columns for multi value columns\ncomb = pd.get_dummies(data = comb, columns = multi_cols)\n\n#Separating the train and test datasets\ntelcom = comb[:trainsize]\ntelcom_test = comb[trainsize:]\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(telcom[num_cols])\nscaled = pd.DataFrame(scaled, columns=num_cols)\n\nscaled_test = std.transform(telcom_test[num_cols])\nscaled_test = pd.DataFrame(scaled_test, columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_telcom_og = telcom.copy()\ntelcom = telcom.drop(columns = num_cols, axis = 1)\ntelcom = telcom.merge(scaled, left_index=True, right_index=True, how = \"left\")\n\ndf_telcom_test_og = telcom_test.copy()\ntelcom_test = telcom_test.drop(columns = num_cols, axis = 1)\ntelcom_test = telcom_test.merge(scaled_test, left_index=True, right_index=True, how = \"left\")","f29b4d16":"summary = (df_telcom_og[[i for i in df_telcom_og.columns]].\n           describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['#119DFF']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [200,60,100,100,60,60,80,80,80])\nlayout = go.Layout(dict(title = \"Training variable Summary\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)","771f608a":"#correlation\ncorrelation = telcom.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   colorscale = \"Viridis\",\n                   colorbar = dict(title = \"Pearson Correlation coefficients\", titleside = \"right\"),\n                  )\nlayout = go.Layout(dict(title = \"Correlation matrix\",\n                        autosize = False,\n                        height = 720,\n                        width = 800,\n                        margin = dict(r = 0, l = 210, t = 25, b = 210),\n                        yaxis = dict(tickfont = dict(size = 9)),\n                        xaxis = dict(tickfont = dict(size = 9))\n                       )\n                  )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","06eeddaf":"def pca_scatter(pcadf, targetfeature, targetlabel, color):\n    tracer = go.Scatter(x = pcadf[pcadf[targetfeature]==targetlabel][\"PC1\"],\n                        y = pcadf[pcadf[targetfeature]==targetlabel][\"PC2\"],\n                        name = targetlabel, mode = \"markers\",\n                        marker = dict(color = color, line = dict(width = .5), symbol = \"diamond-open\"),\n                       )\n    return tracer","ad636c40":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\n\nX = telcom[[i for i in telcom.columns if i not in target_col]]\nY = telcom[target_col]\n\nprincipal_components = pca.fit_transform(X)\npca_data = pd.DataFrame(principal_components, columns = [\"PC1\", \"PC2\"])\npca_data = pca_data.merge(Y, left_index=True, right_index=True, how=\"left\")\npca_data[\"Churn\"] = pca_data[\"Churn\"].replace({1: \"Churn\", 0: \"Not churn\"})\n\nlayout = go.Layout(dict(title = \"Visualizing data with PCA\",\n                        plot_bgcolor = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"first principal component (PC1)\",\n                                     zerolinewidth=1, ticklen=5, gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"second principal component (PC2)\",\n                                     zerolinewidth=1, ticklen=5, gridwidth=2),\n                        height = 400\n                       )\n                  )\ntrace1 = pca_scatter(pca_data, 'Churn', 'Churn', 'red')\ntrace2 = pca_scatter(pca_data, 'Churn', 'Not churn', 'royalblue')\ndata = [trace2, trace1]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","4fcb71d2":"def plot_radar(df, aggregate, title):\n    data_frame = df[df[\"Churn\"] == aggregate] \n    data_frame_x = data_frame[bi_cs].sum().reset_index()\n    data_frame_x.columns = [\"feature\", \"yes\"]\n    data_frame_x[\"no\"] = data_frame.shape[0] - data_frame_x[\"yes\"]\n    data_frame_x = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n    \n    #count of 1's (yes)\n    trace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill = \"toself\", \n                             name = \"count of 1's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            )\n    #count of 0's (no)\n    trace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill = \"toself\", \n                             name = \"count of 0's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            ) \n    layout = go.Layout(dict(polar = dict(radialaxis = dict(visible = True,\n                                                           side = \"counterclockwise\",\n                                                           showline = True,\n                                                           linewidth = 2,\n                                                           tickwidth = 2,\n                                                           gridcolor = \"white\",\n                                                           gridwidth = 2),\n                                         angularaxis = dict(tickfont = dict(size = 10),\n                                                            layer = \"below traces\"\n                                                           ),\n                                         bgcolor = \"rgb(243,243,243)\",\n                                        ),\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            title = title, height = 600, width = 600))\n    \n    data = [trace2, trace1]\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)","f9b513a4":"#separating binary columns\nbi_cs = telcom.nunique()[telcom.nunique() == 2].keys()\ndat_rad = telcom[bi_cs]\n\n#plotting radar chart for churn and not churn customers (binary variables)\nplot_radar(dat_rad, 1, \"Churn customers\")\nplot_radar(dat_rad, 0, \"Not churn customers\")","5d85c6b6":"def telecom_churn_prediction(algorithm, training_x, testing_x, training_y, testing_y, cf, threshold_plot):\n    #model\n    algorithm.fit(training_x, training_y)\n    predictions = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n        \n    print('Algorithm:', type(algorithm).__name__)\n    print(\"\\nClassification report:\\n\", classification_report(testing_y, predictions))\n    print(\"Accuracy Score:\", accuracy_score(testing_y, predictions))\n    \n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y, predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y, predictions) \n    print(\"Area under curve:\", model_roc_auc,\"\\n\")\n    \n    fpr, tpr, thresholds = roc_curve(testing_y, probabilities[:,1])\n     \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix,\n                        x = [\"Not churn\", \"Churn\"],\n                        y = [\"Not churn\", \"Churn\"],\n                        showscale = False, colorscale = \"Picnic\",\n                        name = \"Confusion matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr, y = tpr,\n                        name = \"Roc: \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'), width = 2))\n    trace3 = go.Scatter(x = [0,1], y = [0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'), width = 2,\n                        dash = 'dot'))\n    \n    if cf in ['coefficients', 'features']:\n        if cf == 'coefficients':\n            coefficients = pd.DataFrame(algorithm.coef_.ravel())\n        elif cf == 'features':\n            coefficients = pd.DataFrame(algorithm.feature_importances_)\n        \n        column_df = pd.DataFrame(training_x.columns.tolist())\n        coef_sumry = (pd.merge(coefficients, column_df, left_index=True, \n                               right_index=True, how=\"left\"))\n        coef_sumry.columns = [\"coefficients\", \"features\"]\n        coef_sumry = coef_sumry.sort_values(by = \"coefficients\", ascending=False)\n        \n        #plot coeffs\n        trace4 = go.Bar(x = coef_sumry[\"features\"], y = coef_sumry[\"coefficients\"], \n                        name = \"coefficients\",\n                        marker = dict(color = coef_sumry[\"coefficients\"],\n                                      colorscale = \"Picnic\",\n                                      line = dict(width = .6, color = \"black\")\n                                     )\n                       )\n        #subplots\n        fig = make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                                subplot_titles=('Confusion matrix',\n                                                'Receiver operating characteristic',\n                                                'Feature importances')\n                           )  \n        fig.append_trace(trace1,1,1)\n        fig.append_trace(trace2,1,2)\n        fig.append_trace(trace3,1,2)\n        fig.append_trace(trace4,2,1)\n        fig['layout'].update(showlegend=False, title=\"Model performance\",\n                             autosize=False, height = 900, width = 800,\n                             plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                             paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                             margin = dict(b = 195))\n        fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n        fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n        fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True, tickfont = dict(size = 10), tickangle = 90))\n        \n    elif cf == 'None':\n        #subplots\n        fig = make_subplots(rows=1, cols=2,\n                            subplot_titles=('Confusion matrix',\n                                            'Receiver operating characteristic')\n                           )\n        fig.append_trace(trace1,1,1)\n        fig.append_trace(trace2,1,2)\n        fig.append_trace(trace3,1,2)\n        fig['layout'].update(showlegend=False, title=\"Model performance\",\n                         autosize=False, height = 500, width = 800,\n                         plot_bgcolor = 'rgba(240,240,240,0.95)',\n                         paper_bgcolor = 'rgba(240,240,240,0.95)',\n                         margin = dict(b = 195))\n        fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n        fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))  \n        \n    py.iplot(fig)\n    \n    if threshold_plot == True: \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()","b1864d85":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score, roc_curve,scorer, f1_score, precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\nimport statsmodels.api as sm\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n#defining the studied or used independent features (columns) as well the target  \ncols = [i for i in telcom.columns if i not in target_col]\ntarget_col = ['Churn']\n\n#splitting the principal training dataset to subtrain and subtest datasets\nx_train, x_test, y_train, y_test = train_test_split(telcom[cols], telcom[target_col], \n                                                    test_size = .25, random_state = 111)\n\n#splitting the no scaled principal training dataset to subtrain and subtest datasets\nx_train_og, x_test_og, y_train_og, y_test_og = train_test_split(df_telcom_og[cols], telcom[target_col],\n                                                                test_size = .25, random_state = 111)","544f4786":"from sklearn.linear_model import LogisticRegression\n\n#Baseline model        \nlogit = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n                           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n                           verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit, x_train, x_test, y_train, y_test, \"coefficients\", threshold_plot=True)","73544fed":"from imblearn.over_sampling import SMOTE\n\n#oversampling minority class using smote\nsmote = SMOTE(random_state = 0)\nx_smote, y_smote = smote.fit_sample(x_train, y_train)\nx_smote = pd.DataFrame(data = x_smote, columns=cols)\ny_smote = pd.DataFrame(data = y_smote, columns=target_col)\n\nlogit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                                 intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n                                 penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n                                 verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit_smote, x_smote, x_test, y_smote, y_test, \"coefficients\", threshold_plot=True)","315aeb40":"from sklearn.feature_selection import RFE\n\nlogit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                               intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n                               penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n                               verbose=0, warm_start=False)\n\n\nrfe = RFE(logit_rfe, 10)\nrfe = rfe.fit(x_train, y_train.values.ravel())\n\n#identified columns Recursive Feature Elimination\nidc_rfe = pd.DataFrame({\"rfe_support\": rfe.support_,\n                        \"columns\": cols,\n                        \"ranking\": rfe.ranking_,\n                       })\ncols_rfe = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n\n#applying model\ntelecom_churn_prediction(logit_rfe, x_train[cols_rfe], x_test[cols_rfe], y_train, y_test, \"coefficients\", threshold_plot=True)\n\ntable_rk = ff.create_table(idc_rfe)\npy.iplot(table_rk)","b5a3013c":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\n#dataframe with non negative values\nx_df = df_telcom_og[cols]\ny_df = df_telcom_og[target_col]\n\n#fit model with k= 3\nselect = SelectKBest(score_func = chi2, k = 3)\nselect = select.fit(x_df, y_df)\n\n#create dataframe\nscore = pd.DataFrame({\"features\": cols, \"scores\": select.scores_, \"p_values\": select.pvalues_ })\nscore = score.sort_values(by = \"scores\", ascending=False)\n\n#createing new label for categorical and numerical columns\nscore[\"feature_type\"] = np.where(score[\"features\"].isin(num_cols), \"Numerical\", \"Categorical\")\n\ntable_score = ff.create_table(score)\npy.iplot(table_score)\n\n#plot\ntrace1 = go.Scatter(x = score[score[\"feature_type\"]==\"Categorical\"][\"features\"],\n                   y = score[score[\"feature_type\"]==\"Categorical\"][\"scores\"],\n                   name = \"Categorial\", mode = \"lines+markers\",\n                   marker = dict(color = \"red\", line = dict(width =1))\n                   )\n\ntrace2 = go.Bar(x = score[score[\"feature_type\"]==\"Numerical\"][\"features\"],\n                y = score[score[\"feature_type\"]==\"Numerical\"][\"scores\"], name = \"Numerical\",\n                marker = dict(color = \"royalblue\", line = dict(width =1)),\n                xaxis = \"x2\", yaxis = \"y2\"\n               )\nlayout = go.Layout(dict(title = \"Scores for Categorical & Numerical features\",\n                        plot_bgcolor = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     tickfont = dict(size =10),\n                                     domain=[0, 0.7],\n                                     tickangle = 90, zerolinewidth=1,\n                                     ticklen=5, gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"scores\",\n                                     zerolinewidth=1, ticklen=5, gridwidth=2),\n                        margin = dict(b=200),\n                        xaxis2=dict(domain=[0.8, 1], tickangle = 90, gridcolor = 'rgb(255, 255, 255)'),\n                        yaxis2=dict(anchor='x2', gridcolor = 'rgb(255, 255, 255)')\n                        )\n                  )\n\ndata = [trace1, trace2]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","3a252c11":"def treeplot(classifier, cols, classnames):\n    #plot decision tree\n    graph = Source(tree.export_graphviz(classifier, out_file=None, \n                                        rounded=True, proportion=False,\n                                        feature_names = cols, \n                                        precision = 2,\n                                        class_names = classnames,\n                                        filled = True)\n                  )\n    display(graph)","7815f421":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG, display\n\ndecision_tree = DecisionTreeClassifier(max_depth = 9, random_state = 123,\n                                       splitter = \"best\", criterion = \"gini\")\n\ntelecom_churn_prediction(decision_tree, x_train, x_test, y_train, y_test, \"features\", threshold_plot=True)\n\n#plot decision tree\ntreeplot(decision_tree, cols, [\"Not churn\", \"Churn\"])","a4d6b7be":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n                           weights='uniform')\ntelecom_churn_prediction(knn, x_train, x_test, y_train, y_test, 'None', threshold_plot=True)","df2de596":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 100, random_state = 123,\n                             max_depth = 9, criterion = \"gini\")\n\ntelecom_churn_prediction(rfc, x_train, x_test, y_train, y_test, 'features', threshold_plot=True)","07ad05da":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB(priors=None)\n\ntelecom_churn_prediction(gnb, x_train, x_test, y_train, y_test, 'None', threshold_plot=True)","4e771380":"from sklearn.svm import SVC\n\n#Support vector classifier using linear hyper plane\nsvc_lin  = SVC(C=1.0, kernel='linear', probability=True, random_state=124)\n\ntelecom_churn_prediction(svc_lin, x_train, x_test, y_train, y_test, \"coefficients\", threshold_plot=True)","1704f27c":"#support vector classifier using non-linear hyper plane (\"rbf\")\nsvc_rbf  = SVC(C=10.0, kernel='rbf', gamma=0.1, probability=True, random_state=124)   \n\ntelecom_churn_prediction(svc_rbf, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","7a97d844":"from lightgbm import LGBMClassifier\n\nlgbmc = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\ntelecom_churn_prediction(lgbmc, x_train, x_test, y_train, y_test, \"features\", threshold_plot=True)","5fdc1db3":"from xgboost import XGBClassifier\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n                    max_depth=7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n\ntelecom_churn_prediction(xgc, x_train, x_test, y_train, y_test, \"features\", threshold_plot=True)","764f77c0":"from sklearn.gaussian_process import GaussianProcessClassifier\n\ngpc = GaussianProcessClassifier(random_state=124)\n\ntelecom_churn_prediction(gpc, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","3b8bfcd8":"from sklearn.ensemble import AdaBoostClassifier\n\nadac = AdaBoostClassifier(random_state=124)\n\ntelecom_churn_prediction(adac, x_train, x_test, y_train, y_test, \"features\", threshold_plot=True)","3baf2662":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(random_state=124)\n\ntelecom_churn_prediction(gbc, x_train, x_test, y_train, y_test, \"features\", threshold_plot=True)","fd5e019b":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\n\ntelecom_churn_prediction(lda, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","b6c73cc6":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\ntelecom_churn_prediction(qda, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","41e90544":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(alpha=1, max_iter=1000, random_state=124)\n\ntelecom_churn_prediction(mlp, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","15297f5b":"from sklearn.ensemble.bagging import BaggingClassifier\n\nbgc = BaggingClassifier(random_state=124)\n\ntelecom_churn_prediction(bgc, x_train, x_test, y_train, y_test, \"None\", threshold_plot=True)","f8afd966":"#putting all the model names, model classes and the used columns in a dictionary\nmodels = {'Logistic (Baseline)': [logit, cols],\n          'Logistic (SMOTE)': [logit_smote, cols], \n          'Logistic (RFE)': [logit_rfe, cols_rfe], \n          'Decision Tree': [decision_tree, cols], \n          'KNN Classifier': [knn, cols], \n          'Random Forest': [rfc, cols], \n          'Naive Bayes': [gnb, cols], \n          'SVM (linear)': [svc_lin, cols], \n          'SVM (rbf)': [svc_rbf, cols], \n          'LGBM Classifier': [lgbmc, cols], \n          'XGBoost Classifier': [xgc, cols], \n          'Gaussian Process': [gpc, cols], \n          'AdaBoost': [adac, cols], \n          'GradientBoost': [gbc, cols], \n          'LDA': [lda, cols], \n          'QDA': [qda, cols], \n          'MLP Classifier': [mlp, cols], \n          'Bagging Classifier': [bgc, cols],\n         }","ae50d90f":"#gives model report in dataframe\ndef model_report(model, training_x, testing_x, training_y, testing_y, name):\n    model = model.fit(training_x, training_y)\n    predictions = model.predict(testing_x)\n    accuracy = accuracy_score(testing_y, predictions)\n    recallscore = recall_score(testing_y, predictions)\n    precision = precision_score(testing_y, predictions)\n    roc_auc = roc_auc_score(testing_y, predictions)\n    f1score = f1_score(testing_y, predictions) \n    kappa_metric = cohen_kappa_score(testing_y, predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy\"        : [accuracy],\n                       \"Recall\"          : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1-score\"        : [f1score],\n                       \"Roc_auc\"         : [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df","d81df9b3":"#outputs for all models over the training dataset\nmodel_performances_train = pd.DataFrame() \nfor name in models:\n    if name == 'Logistic (SMOTE)':\n        model_performances_train = model_performances_train.append(model_report(models[name][0], \n                                                                                x_smote[models[name][1]], x_test[models[name][1]], \n                                                                                y_smote, y_test, name), ignore_index=True)\n    else:\n        model_performances_train = model_performances_train.append(model_report(models[name][0], x_train[models[name][1]], \n                                                                                x_test[models[name][1]], \n                                                                                y_train, y_test, name), ignore_index=True)\n        \ntable_train = ff.create_table(np.round(model_performances_train, 4))\npy.iplot(table_train)","05382adb":"def output_tracer(df, metric, color):\n    tracer = go.Bar(y = df[\"Model\"],\n                    x = df[metric],\n                    orientation = \"h\", name = metric ,\n                    marker = dict(line = dict(width =.7), color = color)\n                   )\n    return tracer\n\ndef modelmetricsplot(df, title):\n    layout = go.Layout(dict(title = title,\n                        plot_bgcolor = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5, gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1, ticklen=5, gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n    trace1 = output_tracer(df, \"Accuracy\", \"#6699FF\")\n    trace2 = output_tracer(df, 'Recall', \"red\")\n    trace3 = output_tracer(df, 'Precision', \"#33CC99\")\n    trace4 = output_tracer(df, 'f1-score', \"lightgrey\")\n    trace5 = output_tracer(df, 'Roc_auc', \"magenta\")\n    trace6 = output_tracer(df, 'Kappa_metric', \"#FFCC99\")\n\n    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)","cd9d1022":"modelmetricsplot(df=model_performances_train, title=\"Model performances over the training dataset\")","ddf01843":"def confmatplot(modeldict, df_train, df_test, target_train, target_test, figcolnumber):\n    fig = plt.figure(figsize=(4*figcolnumber, 4*ceil(len(modeldict)\/figcolnumber)))\n    fig.set_facecolor(\"#F3F3F3\")\n    for name, figpos in itertools.zip_longest(modeldict, range(len(modeldict))):\n        plt.subplot(ceil(len(modeldict)\/figcolnumber), figcolnumber, figpos+1)\n        if name=='Logistic (SMOTE)':\n            model = modeldict[name][0].fit(df_train[1][modeldict[name][1]], target_train[1])\n            predictions = model.predict(df_test[modeldict[name][1]])\n            conf_matrix = confusion_matrix(target_test, predictions)\n            sns.heatmap(conf_matrix, annot=True, fmt = \"d\", square = True,\n                        xticklabels=[\"Not churn\", \"Churn\"],\n                        yticklabels=[\"Not churn\", \"Churn\"],\n                        linewidths = 2, linecolor = \"w\", cmap = \"Set1\")\n            plt.title(name, color = \"b\")\n            plt.subplots_adjust(wspace = .3, hspace = .3)\n        else:\n            model = modeldict[name][0].fit(df_train[0][modeldict[name][1]], target_train[0])\n            predictions = model.predict(df_test[modeldict[name][1]])\n            conf_matrix = confusion_matrix(target_test, predictions)\n            sns.heatmap(conf_matrix, annot=True, fmt = \"d\", square = True,\n                        xticklabels=[\"Not churn\", \"Churn\"],\n                        yticklabels=[\"Not churn\", \"Churn\"],\n                        linewidths = 2, linecolor = \"w\", cmap = \"Set1\")\n            plt.title(name, color = \"b\")\n            plt.subplots_adjust(wspace = .3, hspace = .3)","a8ecdc84":"confmatplot(modeldict=models, df_train=[x_train, x_smote], df_test=x_test, \n             target_train=[y_train, y_smote], target_test=y_test, figcolnumber=3)","69f1d9c1":"def rocplot(modeldict, df_train, df_test, target_train, target_test, figcolnumber):\n    fig = plt.figure(figsize=(4*figcolnumber, 4*ceil(len(modeldict)\/figcolnumber)))\n    fig.set_facecolor(\"#F3F3F3\")\n    for name, figpos in itertools.zip_longest(modeldict, range(len(modeldict))):\n        qx = plt.subplot(ceil(len(modeldict)\/figcolnumber), figcolnumber, figpos+1)\n        if name=='Logistic (SMOTE)':\n            model = modeldict[name][0].fit(df_train[1][modeldict[name][1]], target_train[1])\n            probabilities = model.predict_proba(df_test[modeldict[name][1]])\n            predictions = model.predict(df_test[modeldict[name][1]])\n                        \n            fpr, tpr, thresholds = roc_curve(target_test, probabilities[:,1])\n            plt.plot(fpr, tpr, linestyle = \"dotted\",\n                     color = \"royalblue\", linewidth = 2,\n                     label = \"AUC = \" + str(np.around(roc_auc_score(target_test, predictions), 3)))\n            plt.plot([0,1],[0,1], linestyle = \"dashed\",\n                     color = \"orangered\", linewidth = 1.5)\n            plt.fill_between(fpr, tpr, alpha = .1)\n            plt.fill_between([0, 1], [0, 1], color = \"b\")\n            plt.legend(loc = \"lower right\",\n                       prop = {\"size\" : 12})\n            qx.set_facecolor(\"w\")\n            plt.grid(True, alpha = .15)\n            plt.title(name, color = \"b\")\n            plt.xticks(np.arange(0, 1, .3))\n            plt.yticks(np.arange(0, 1, .3))\n       \n        else:\n            model = modeldict[name][0].fit(df_train[0][modeldict[name][1]], target_train[0])\n            probabilities = model.predict_proba(df_test[modeldict[name][1]])\n            predictions = model.predict(df_test[modeldict[name][1]])\n                        \n            fpr, tpr, thresholds = roc_curve(target_test, probabilities[:,1])\n            plt.plot(fpr, tpr, linestyle = \"dotted\",\n                     color = \"royalblue\", linewidth = 2,\n                     label = \"AUC = \" + str(np.around(roc_auc_score(target_test, predictions), 3)))\n            plt.plot([0,1],[0,1], linestyle = \"dashed\",\n                     color = \"orangered\", linewidth = 1.5)\n            plt.fill_between(fpr, tpr, alpha = .1)\n            plt.fill_between([0, 1], [0, 1], color = \"b\")\n            plt.legend(loc = \"lower right\",\n                       prop = {\"size\" : 12})\n            qx.set_facecolor(\"w\")\n            plt.grid(True, alpha = .15)\n            plt.title(name, color = \"b\")\n            plt.xticks(np.arange(0, 1, .3))\n            plt.yticks(np.arange(0, 1, .3))","d0e21d6a":"rocplot(modeldict=models, df_train=[x_train, x_smote], df_test=x_test, \n             target_train=[y_train, y_smote], target_test=y_test, figcolnumber=3)","0aff2290":"def prcplot(modeldict, df_train, df_test, target_train, target_test, figcolnumber):\n    fig = plt.figure(figsize=(4*figcolnumber, 4*ceil(len(modeldict)\/figcolnumber)))\n    fig.set_facecolor(\"#F3F3F3\")\n    for name, figpos in itertools.zip_longest(modeldict, range(len(modeldict))):\n        qx = plt.subplot(ceil(len(modeldict)\/figcolnumber), figcolnumber, figpos+1)\n        if name=='Logistic (SMOTE)':\n            model = modeldict[name][0].fit(df_train[1][modeldict[name][1]], target_train[1])\n            probabilities = model.predict_proba(df_test[modeldict[name][1]])\n            predictions = model.predict(df_test[modeldict[name][1]])\n            \n            recall, precision, thresholds = precision_recall_curve(target_test, probabilities[:,1])\n            plt.plot(recall, precision, linewidth = 1.5,\n                     label = (\"avg_pcn: \"+str(np.around(average_precision_score(target_test, predictions), 3))))\n            plt.plot([0, 1], [0, 0], linestyle = \"dashed\")\n            plt.fill_between(recall, precision, alpha = .1)\n            plt.legend(loc = \"lower left\", prop = {\"size\": 10})\n            qx.set_facecolor(\"w\")\n            plt.grid(True, alpha = .15)\n            plt.title(name, color = \"b\")\n            plt.xlabel(\"recall\", fontsize=7)\n            plt.ylabel(\"precision\", fontsize=7)\n            plt.xlim([0.25,1])\n            plt.yticks(np.arange(0, 1, .3))\n        else:\n            model = modeldict[name][0].fit(df_train[0][modeldict[name][1]], target_train[0])\n            probabilities = model.predict_proba(df_test[modeldict[name][1]])\n            predictions = model.predict(df_test[modeldict[name][1]])\n            \n            recall, precision, thresholds = precision_recall_curve(target_test, probabilities[:,1])\n            plt.plot(recall, precision, linewidth = 1.5,\n                     label = (\"avg_pcn: \"+str(np.around(average_precision_score(target_test, predictions), 3))))\n            plt.plot([0, 1], [0, 0], linestyle = \"dashed\")\n            plt.fill_between(recall, precision, alpha = .1)\n            plt.legend(loc = \"lower left\", prop = {\"size\": 10})\n            qx.set_facecolor(\"w\")\n            plt.grid(True, alpha = .15)\n            plt.title(name, color = \"b\")\n            plt.xlabel(\"recall\", fontsize=7)\n            plt.ylabel(\"precision\", fontsize=7)\n            plt.xlim([0.25,1])\n            plt.yticks(np.arange(0, 1, .3))","a85964d8":"prcplot(modeldict=models, df_train=[x_train, x_smote], df_test=x_test, \n             target_train=[y_train, y_smote], target_test=y_test, figcolnumber=3)","679a704f":"#outputs for all models over the principal test dataset\nmodel_performances_test = pd.DataFrame() \nfor name in models:\n    if name == 'Logistic (SMOTE)':\n        model_performances_test = model_performances_test.append(model_report(models[name][0], \n                                                                              x_smote[models[name][1]], telcom_test[models[name][1]], \n                                                                              y_smote, telcom_test[target_col], name), ignore_index=True)\n    else:\n        model_performances_test = model_performances_test.append(model_report(models[name][0], \n                                                                              x_train[models[name][1]], telcom_test[models[name][1]], \n                                                                              y_train, telcom_test[target_col], name), ignore_index=True)\n        \ntable_test = ff.create_table(np.round(model_performances_test, 4))\npy.iplot(table_test)","a9ea6178":"modelmetricsplot(df=model_performances_test, title=\"Model performances over the principal test dataset\")","f1664de1":"confmatplot(modeldict=models, df_train=[x_train, x_smote], df_test=telcom_test[cols], \n             target_train=[y_train, y_smote], target_test=telcom_test[target_col], figcolnumber=3)","9179644c":"rocplot(modeldict=models, df_train=[x_train, x_smote], df_test=telcom_test[cols], \n             target_train=[y_train, y_smote], target_test=telcom_test[target_col], figcolnumber=3)","c3373598":"prcplot(modeldict=models, df_train=[x_train, x_smote], df_test=telcom_test[cols], \n             target_train=[y_train, y_smote], target_test=telcom_test[target_col], figcolnumber=3)","f2d8ba3a":"## <a id='2.2'>2.2. Variable distributions<\/a>","1f905bb3":"## <a id='4.12'>4.12. Gaussian Process Classifier<\/a> ","427a34fa":"# <a id='3'>3. Data preprocessing<\/a>","3cce7d19":"## <a id='6.5'>6.5. Precision recall curves<\/a>","ca7b25a5":"# <a id='5'>5. Model performances over the training dataset<\/a>","16636875":"# <a id='1'>1. Data overview<\/a>","6b55e17f":"## <a id='5.3'>5.3. Confusion matrices for models<\/a>","0414e7a5":"## <a id='4.3'>4.3. Recursive Feature Elimination<\/a>\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","f83238cc":"- <a href='#1'>1. Data overview<\/a>\n- <a href='#2'>2. Exploratory Data Analysis<\/a>\n    - <a href='#2.1'>2.1. Customer churn in data<\/a>\n    - <a href='#2.2'>2.2. Variable distributions<\/a>\n- <a href='#3'>3. Data preprocessing<\/a>\n    - <a href='#3.1'>3.1. Variable summary<\/a>\n    - <a href='#3.2'>3.2. Correlation matrix<\/a>\n    - <a href='#3.3'>3.3. Visualizing data with principal components<\/a>\n    - <a href='#3.4'>3.4. Binary variable distributions in customer churn (Radar Chart)<\/a>\n- <a href='#4'>4. Model Building<\/a>\n    - <a href='#4.1'>4.1. Baseline model<\/a>\n    - <a href='#4.2'>4.2. Synthetic Minority Oversampling TEchnique (SMOTE)<\/a>\n    - <a href='#4.3'>4.3. Recursive Feature Elimination<\/a>\n    - <a href='#4.4'>4.4. Univariate Selection<\/a>\n    - <a href='#4.5'>4.5. Decision Tree Classifier<\/a> \n    - <a href='#4.6'>4.6. KNN Classifier<\/a>\n    - <a href='#4.7'>4.7. Random Forest Classifier<\/a>\n    - <a href='#4.8'>4.8. Gaussian Naive Bayes<\/a>\n    - <a href='#4.9'>4.9. Support Vector Machine<\/a>\n        - <a href='#4.9.1'>4.9.1. Support Vector Machine (linear)<\/a>\n        - <a href='#4.9.2'>4.9.2. Support Vector Machine (rbf)<\/a>\n    - <a href='#4.10'>4.10. LightGBM Classifier<\/a>\n    - <a href='#4.11'>4.11. XGBoost Classifier<\/a>\n    - <a href='#4.12'>4.12. Gaussian Process Classifier<\/a> \n    - <a href='#4.13'>4.13. AdaBoost Classifier<\/a> \n    - <a href='#4.14'>4.14. GradientBoosting Classifier<\/a>\n    - <a href='#4.15'>4.15. Linear Discriminant Analysis<\/a> \n    - <a href='#4.16'>4.16. Quadratic Discriminant Analysis<\/a> \n    - <a href='#4.17'>4.17. Multi-layer Perceptron Classifier<\/a> \n    - <a href='#4.18'>4.18. Bagging Classifier<\/a>\n- <a href='#5'>5. Model performances over the training dataset<\/a>\n    - <a href='#5.1'>5.1. Model performance metrics<\/a>\n    - <a href='#5.2'>5.2. Compare model metrics<\/a>\n    - <a href='#5.3'>5.3. Confusion matrices for models<\/a>\n    - <a href='#5.4'>5.4. ROC - Curves for models<\/a>\n    - <a href='#5.5'>5.5. Precision recall curves<\/a>\n- <a href='#6'>6. Model performances over the principal test dataset<\/a>\n    - <a href='#6.1'>6.1. Model performance metrics<\/a>\n    - <a href='#6.2'>6.2. Compare model metrics<\/a>\n    - <a href='#6.3'>6.3. Confusion matrices for models<\/a>\n    - <a href='#6.4'>6.4. ROC - Curves for models<\/a>\n    - <a href='#6.5'>6.5. Precision recall curves<\/a>\n    ","3e394611":"# <a id='4'>4. Model Building<\/a>","9bfabad0":"## <a id='5.5'>5.5. Precision recall curves<\/a>","3d6788e5":"## <a id='4.15'>4.15. Linear Discriminant Analysis<\/a> ","680ff314":"## <a id='3.2'>3.2. Correlation matrix<\/a>","2ac6cb24":"# <a id='6'>6. Model performances over the principal test dataset<\/a>\n## <a id='6.1'>6.1. Model performance metrics<\/a>","56b7bf5e":"## <a id='4.2'>4.2. Synthetic Minority Oversampling TEchnique (SMOTE)<\/a>\n* Randomly pick a point from the minority class.\n* Compute the k-nearest neighbors (for some pre-specified k) for this point.\n* Add k new points somewhere between the chosen point and each of its neighbors","7e7ed9a6":"## <a id='4.18'>4.18. Bagging Classifier<\/a> ","aaa08160":"## <a id='4.14'>4.14. GradientBoosting Classifier<\/a> ","5d0c8e24":"## <a id='6.2'>6.2. Compare model metrics<\/a>","574c1e68":"## <a id='4.7'>4.7. Random Forest Classifier<\/a>\nRandom forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement.","8f812b35":"## <a id='6.4'>6.4. ROC - Curves for models<\/a>","aa578d56":"## <a id='4.15'>4.16. Quadratic Discriminant Analysis<\/a> ","97701656":"## <a id='4.4'>4.4. Univariate Selection<\/a>\n* Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n* uses the chi squared ($\\chi^2$) statistical test for non-negative features to select the best features","deee4a89":"# Customer churn prediction: Telecom Churn Dataset\n\nCustomer churn, also known as customer retention, customer turnover, or customer defection, is the loss of clients or customers.\n\nTelephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics  because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n\nCompanies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n\npredictive analytics  use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.\n\n**Please upvote for this kernel as well as the Telecom Churn Dataset if you find them useful.**","07322236":"### <a id='4.9.2'>4.9.2. Support Vector Machine (rbf)<\/a>","01d79ac0":"## <a id='2.1'>2.1. Customer churn in data<\/a>","f911ffed":"## <a id='3.3'>3.3. Visualizing data with principal components<\/a>","b77ba064":"## <a id='3.4'>3.4. Binary variable distribution in customer churn (Radar Chart)<\/a>","bf3e6603":"## <a id='4.8'>4.8. Gaussian Naive Bayes<\/a>","1659bb72":"## <a id='4.6'>4.6. KNN Classifier<\/a>\n","c6725a02":"## <a id='4.9'>4.9. Support Vector Machine<\/a>\n\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges.   it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space .where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes.\n\n### <a id='4.9.1'>4.9.1. Support Vector Machine (linear)<\/a>","8c0dc700":"## <a id='4.17'>4.17. Multi-layer Perceptron Classifier<\/a> ","8c2b4ce2":"## <a id='5.4'>5.4. ROC - Curves  for models<\/a>","f10b8a1d":"## <a id='4.11'>4.11. XGBoost  Classifier<\/a>","a668948d":"# <a id='2'>2. Exploratory Data Analysis<\/a>","bbda1376":"Several of the numerical data are very correlated. (Total day minutes and Total day charge), (Total eve minutes and Total eve charge), (Total night minutes and Total night charge) and lastly (Total intl minutes and Total intl charge) are alo correlated. We only have to select one of them.","eeafae2d":"## <a id='5.1'>5.1. Model performance metrics<\/a>","39b18677":"## <a id='4.10'>4.10. LightGBM Classifier<\/a>","0469a1ca":"## <a id='3.1'>3.1. Variable summary<\/a>","af3079f8":"## <a id='4.13'>4.13. AdaBoost Classifier<\/a> ","8efaccee":"## <a id='4.1'>4.1. Baseline model<\/a>","5358a062":"## <a id='6.3'>6.3. Confusion matrices for models<\/a>","54eba38c":"## <a id='4.5'>4.5. Decision Tree Classifier<\/a>\n","c2f943fd":"## <a id='5.2'>5.2. Compare model metrics<\/a>"}}