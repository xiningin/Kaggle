{"cell_type":{"c7ea2cff":"code","4e2d07c1":"code","82280055":"code","1aed2c6b":"code","c2e32517":"code","3dfae950":"code","7c168011":"code","ec53562a":"code","2bde3a86":"code","2ab9c0ba":"code","97b35aa4":"code","b35f5f09":"code","5f3416b3":"code","cfe59576":"code","63139ce3":"code","c1bebe13":"code","e6d26d03":"code","a11b5104":"code","b68dedcf":"code","cf974cfb":"code","4532928e":"code","d4bd994b":"code","41245436":"code","5adac15d":"code","0a9f3680":"code","35416922":"code","3d833b06":"code","66b8675d":"code","f5fa98f5":"markdown","2caeed58":"markdown","f7ddec0f":"markdown","f5935f3c":"markdown","ff0fbf2b":"markdown","ac87c709":"markdown","5b2a7929":"markdown","90578ada":"markdown","c58d1365":"markdown","99259e35":"markdown","0c379f1c":"markdown","867ac857":"markdown","b6ecd84b":"markdown","4ede3499":"markdown","f4971fed":"markdown","f3aac077":"markdown","f1582456":"markdown","9702da74":"markdown","e5db2335":"markdown","28d5578d":"markdown","e98a0bc4":"markdown","1e7e3661":"markdown","98aa4a36":"markdown","65b6d64b":"markdown","2c0c504e":"markdown","386ea69f":"markdown","0aff9e98":"markdown","54c6ae7e":"markdown","de3cd9da":"markdown","a943ba88":"markdown","133151f3":"markdown","5c5f9ff4":"markdown","85355353":"markdown","bcd7c5d6":"markdown","e9ede2ba":"markdown"},"source":{"c7ea2cff":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom IPython.display import Image, clear_output\n\nsns.set()\ntf.__version__","4e2d07c1":"data = tf.keras.datasets.fashion_mnist\n\n(X_train_full, y_train_full), (X_test, y_test) = data.load_data()\nclass_names = ['T-shirt\/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n\nclear_output()\nprint(\"X_train_full shape :\",X_train_full.shape)\nprint(\"X_test shape :\", X_test.shape)","82280055":"X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","1aed2c6b":"# Always do normalize\nX_valid, X_train = X_valid\/255., X_train\/255.\n\n# Specify the model's architecture\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape = [28,28]),\n    tf.keras.layers.Dense(300, activation='relu'),\n    tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax'),\n])\n\n# Specify the loss fuction, optimizer, metrics\nmodel.compile(\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer = tf.keras.optimizers.SGD(),\n    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train, epochs=10,\n    validation_data = (X_valid, y_valid),\n    verbose=0\n)","c2e32517":"model.summary()","3dfae950":"print(\"list of layers :\",model.layers)\nprint(\"\\nmodel.layers[1].name :\",model.layers[1].name)\nprint(\"\\nmodel.get_layer('dense_1') :\",model.get_layer('dense_1'))","7c168011":"hidden1 = model.layers[1]\n\nweights, bias = hidden1.get_weights()","ec53562a":"pd.DataFrame(history.history).plot()\nplt.xlabel(\"epoch\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)\nplt.show()","2bde3a86":"loss, acc = model.evaluate(X_test, y_test)","2ab9c0ba":"X_new = X_test[:3]\ny = model.predict(X_new)\ny.round(3)","97b35aa4":"from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = fetch_california_housing()\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(data.data, data.target)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(X_test)","b35f5f09":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(30, activation='relu', input_shape=(None, X_train.shape[1])),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(\n    loss = 'mean_squared_error',\n    optimizer = 'SGD',\n    metrics = ['mean_squared_error']\n)\n\nhistory = model.fit(X_train, y_train, \n                    epochs=10, \n                    validation_data = (X_valid, y_valid), \n                    verbose=0)","5f3416b3":"pd.DataFrame({\n    'y_true':y_test,\n    'y_pred':model.predict(X_test).ravel()\n}\n).T","cfe59576":"Image('\/kaggle\/input\/nonseq1\/Image 1.png',width = 200)","63139ce3":"input_layer = tf.keras.Input(shape = (None, X_train.shape[1]))\nhidden1 = tf.keras.layers.Dense(30, activation='relu')(input_layer)\nhidden2 = tf.keras.layers.Dense(30, activation='relu')(hidden1)\nconcat = tf.keras.layers.Concatenate()([input_layer, hidden2])\noutput_layer = tf.keras.layers.Dense(1)(concat)\n\nmodel = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])","c1bebe13":"Image('\/kaggle\/input\/nonseq2\/Image 2.png',width = 220)","e6d26d03":"X_train_a, X_train_b = X_train[:,:5], X_train[:,3:]\n\ninput_layer_A = tf.keras.layers.Input(shape = (None,X_train_a.shape[1]))\ninput_layer_B = tf.keras.layers.Input(shape = (None,X_train_b.shape[1]))\n\nhidden1 = tf.keras.layers.Dense(30, activation='relu')(input_layer_B)\nhidden2 = tf.keras.layers.Dense(30, activation='relu')(hidden1)\n\nconcat = tf.keras.layers.concatenate([input_layer_A, hidden2])\noutput_layer = tf.keras.layers.Dense(1)(concat)\n\nmodel = tf.keras.models.Model(inputs=[input_layer_A, input_layer_B], outputs=[output_layer])","a11b5104":"model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n\nX_valid_a, X_valid_b = X_valid[:,:5], X_valid[:,3:]\nX_test_a, X_test_b = X_test[:,:5], X_test[:,3:]","b68dedcf":"history = model.fit((X_train_a, X_train_b), \n                    y_train, \n                    epochs=10, \n                    validation_data=((X_valid_a, X_valid_b), y_valid),\n                    verbose=0)","cf974cfb":"loss, mse = model.evaluate((X_test_a, X_test_b), y_test)","4532928e":"pd.DataFrame({\n    'y_true':y_test,\n    'y_pred':model.predict((X_test_a,X_test_b)).ravel()\n}\n).T","d4bd994b":"Image('..\/input\/nonseq2\/Image 3.png', width=300)","41245436":"X_train_a, X_train_b = X_train[:,:5], X_train[:,3:]\n\ninput_layer_A = tf.keras.layers.Input(shape = (None,X_train_a.shape[1]))\ninput_layer_B = tf.keras.layers.Input(shape = (None,X_train_b.shape[1]))\n\nhidden1 = tf.keras.layers.Dense(30, activation='relu')(input_layer_B)\nhidden2 = tf.keras.layers.Dense(30, activation='relu')(hidden1)\n\nconcat = tf.keras.layers.concatenate([input_layer_A, hidden2])\noutput_layer = tf.keras.layers.Dense(1)(concat)\n\naux_output = tf.keras.layers.Dense(1)(hidden2)\n\nmodel = tf.keras.models.Model(inputs=[input_layer_A, input_layer_B], outputs=[output_layer, aux_output])","5adac15d":"model.compile(loss=['mse','mse'], loss_weights=[0.9,0.1], optimizer='sgd')","0a9f3680":"history = model.fit(\n                [X_train_a, X_train_b], [y_train, y_train],\n                epochs=20,\n                validation_data = ([X_valid_a, X_valid_b], [y_valid, y_valid]),\n                verbose=0\n            )","35416922":"total_loss, main_loss, aux_loss = model.evaluate([X_test_a,X_test_b],[y_test,y_test])","3d833b06":"class our_model(tf.keras.models.Model):\n    def __init__(self, units, activation, **kwargs):\n        super().__init__(**kwargs)  # For other args\n        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n        self.main_output = tf.keras.layers.Dense(1)\n        self.aux_output = tf.keras.layers.Dense(1)\n    \n    def call(self, inputs):\n        input_a, input_b = inputs\n        hidden1 = self.hidden1(input_b)\n        hidden2 = self.hidden2(hidden1)\n        \n        aux_output = self.aux_output(hidden2)\n        concat = tf.keras.layers.concatenate([input_a, hidden2])\n        main_output = self.main_output(concat)\n        \n        return main_output, aux_output","66b8675d":"model = our_model(units=30, activation='relu')","f5fa98f5":"We also can get, set the weights and biases of each layer.","2caeed58":"Once we satisfy with the validation performance, we should evaluate the model on the test set.","f7ddec0f":"If we want to send a subset of features through the wide path, and a different(possibly overlapping) through the deep path, like an image above, we can use **multiple inputs**","f5935f3c":"Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, then you can feed the model some data for training.<br>\nThis has many advantages: \n- The model can easily be saved, cloned, shared.\n- Its structure can be displayed and analyzed, so errors can be caught early (before any data ever goes through the model). \n- It\u2019s also fairly easy to debug, since the whole model is just a static graph of layers. \n\nBut the flip side is just that: it\u2019s static. <br>\nSome models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or if you prefer a more imperative programming style, the Subclassing API is for you.","ff0fbf2b":"However, this extra flexibility comes at a cost: \n- Keras cannot easily inspect the model, it cannot save or clone it.\n- Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. \n\nSo unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API.\n\n___","ac87c709":"First of all, neural network requires the input data to be normalized.","5b2a7929":"Note : \n1. Dense layer initialized the weight randomly and biases were initilized to zeros. We also can use a different initialization scheme by setting `kernel_initializer`, `bias_initializer` when creating layer object.\n2. If the data is highly imbalanced, we can set the `class_weight` param when calling `fit()` method. These weights will be used by Keras when computing the loss.","90578ada":"# 3. Subclassing API","c58d1365":"## 1.2 Regression MLP","99259e35":"Non-sequential neural network (like an image above) makes it possible for neural network to learn both *deep patterns*(using deept path) and *simple rules*(through the short path). In contrast, a sequential MLP forces all the data to flow through the full stack of layers, thus simple patterns are diluted along the way.\n","0c379f1c":"-> All we do is subclass the `Model` class, create layers as needed in constructor, pick any layer to perform the computations in `call()` method.","867ac857":"**X_test** should be kept unseen from the model until the final evaluation. So we need validation dataset to evaluate the model's performance along the experiment.","b6ecd84b":"## 1.1 Multi-layer Perceptron Classifier\nWe are to build the image classifier on the **Fashion-Mnist** dataset.","4ede3499":"Then we treat it as a regular model we built earlier(compile, train, evaluate, make predictions)","f4971fed":"`inputs=[input_layer_A, input_layer_B]` when creating the model. We then can compile model as usual, but when `fit()` we must pass `model.fit((X_train_A, X_train_B)), y_train, validation_data=((X_valid_A, X_valid_B), y_valid))`","f3aac077":"Note : \n1. Instead of passing a `validation_data`, we can set `validation_split` and it will automatically split the validation set for us.\n2. Loss function <br>\n    2.1) We use the `SparseCategoricalCrossentropy` because we have sparse labels(0,1,2,..,9) <br>\n    2.2) If we have one-hot vectors as labels, we should use `CategoricalCrossentropy` <br>\n    2.3) If we have binary labels, we should use `BinaryCrossentropy` <br>\n3. To do One-Hot encoding, simply using `tf.keras.utils.to_categorical()`. And to go the opposite way, use `np.argmax(..., axis=1)`","f1582456":"When we train the model, we need to provide labels to each output. Same as `validation_data`.","9702da74":"There are also many use cases we may want to have multiple outputs\n- We may have multiple independent tasks to perform based on the same data.\n- Regularization technique (constraint to reduce overfitting, improve generalization)\n- Other than the final prediction that data runs throughout all the layers, we may want to see the output that runs through some parts(some layers) of the model.\n\nAll we have to do is connecting extra outputs to to the appropriate layers and add them to the model's outputs list.","e5db2335":"Lastly, we will use the model to predict.","28d5578d":"## 2.1 Multiple Input\n\n","e98a0bc4":"**Important note !!!** <br>\n- Hyperparams must be tunned on the validation set, **not the test set**. (i.e. we tune the hyperparam in oder to improve the model's performance on validation set)","1e7e3661":"Then we can compile it, evaluate it, use it to make predictions like we just did. <br>","98aa4a36":"**Quick recap** <br>\nIn [part 1 notebook](https:\/\/www.kaggle.com\/sathianpong\/neural-network-basic-1), I shown the basic of Deep learning including **Perceptron, Liner regession, and gradient descent**. I also introduced the **Tensorflow autodiff** which can compute the gradient of arbitrary function for us and make *Backpropagation* process much more easier (we don't need to compute gradient's close-form).\n\nIn this notebook we will go deeper into one of the most famous high-level DeepLearning API, **Keras**. ","65b6d64b":"We can get the model's list of layers.","2c0c504e":"Each output need its own loss function. So, when we compile the model we should pass a list of losses. (If passing a single loss, Keras will assume the same loss for all outputs) <br>\nBy default, Keras will compute all the losses and add them up to get the final loss used for training. However we can give Keras a `loss_weights` to control  the influence of each output to the loss used to train and give the model an idea on which output it should pay more attention.","386ea69f":"As we can see, we can build any sort of architecture with the Functional API.","0aff9e98":"## 2.2 Multiple Output","54c6ae7e":"UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '","de3cd9da":"\nWe do not need to create the inputs, we just use the input argument to the `call()` method. <br>\nWe separate the creation of the layers in the `constructor` from their usage in the `call()` method. <br>\n\nWe can do pretty much anything in `call()` method: for loops, if statements, low-level TensorFlow operations. This makes it a great API for researchers experimenting with new ideas.","a943ba88":"We can inspect the fit history to see how the model perform in each epoch. <br>\nIf validation curves are quite close to the training curves,it means that the model is not too overfitting which is good. <br>\nIf the validtion loss is still going down, we should continue training by calling fit() method again(epoch can be adjusted).","133151f3":"Reference : <br>\n`Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition`\n","5c5f9ff4":"# 1. Sequential API","85355353":"In Keras we can build the model in several ways\n1. **Sequential API** : As the name suggest, this API is to build the model in sequential style, i.e. output of one layer will be fed to the later layer.\n2. **Function API** : This API allows us to completely control the flow of data (doesn't need to be sequential).\n3. **Subclassig the tf.keras.models class** : This will gives a complete control of the overall model. We can have loops, conditional branching, and other dynamic behaviors in the model.","bcd7c5d6":"# 2. Functional API","e9ede2ba":"We we evaluate the model, Keras will return total loss, and all the individual losses."}}