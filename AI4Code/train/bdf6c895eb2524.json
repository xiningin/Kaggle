{"cell_type":{"04aa2c28":"code","5e0c8181":"code","230c36bb":"code","e4304de1":"code","ebd21c44":"code","6d41a563":"code","eb8fd583":"markdown","4f055b39":"markdown","2bd11734":"markdown","5a0ebd6f":"markdown","19fe4c09":"markdown","5c85eea5":"markdown","61275b66":"markdown","91ccda68":"markdown"},"source":{"04aa2c28":"!pip install cleantext\nimport pandas as pd \nimport calendar\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\n\nfrom gensim.corpora import Dictionary\n\nfrom gensim import  models\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nfrom pprint import pprint\n\nimport cleantext\n\nfrom nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer()\nimport nltk \n\nfrom tqdm import tqdm\n\n#metadata\nmetadata=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nprint('shape' ,metadata.shape)\nprint('info',metadata.info())\n","5e0c8181":"#journals analysis\ncols = ['journal']\ndata=metadata[metadata[cols].notna().all(1)]\nprint(data.shape)\ndf=data['journal'].value_counts()\nx=df.iloc[[0,1,2,3,4,5,6,7,8,9]]\nx.plot.pie(figsize = (10,10),autopct = '%.2f%%',\n title = 'Top Ten Journals')\nplt.title(\"Top Ten Journals\", bbox={'facecolor':'0.9', 'pad':5})\nplt.show()\n\n    \n#publication year analysis  \ncols = ['publish_time']\ndata=metadata[metadata[cols].notna().all(1)]\nprint(data.shape)\nlistd=[]\nfor d in data['publish_time'].tolist():\n    d=str(d)\n    d=d.replace(\"-\",\" \")\n    listd.append(str(d).strip().split()[0])\n#del data['publish_time']\ndata['publish_year'] = listd\n\ndf=data['publish_year'].value_counts()\nx=df.iloc[list(range(len(df)))]\nx.plot(kind='bar',figsize=(20,20),x='year',y='number of publications')\nplt.title(\"Number of publications per year\", bbox={'facecolor':'0.9', 'pad':5})\nplt.show()\n\n\n#publication 2020 year analysis\n#print(data['publish_time'].tolist())\ndic=dict((v,k) for k,v in enumerate(calendar.month_abbr))\nlistd=[]\nfor d in data['publish_time'].tolist():\n    d=str(d).strip()\n    d=d.replace(\"-\",\" \")\n    z=d.split()\n    if len(z)>1 and z[0]=='2020':\n        m=z[1]\n        if m.isdigit():\n            m=int(m)\n            listd.append(calendar.month_name[m])\n        else:\n            listd.append(m)\n                \n\nxlabel=[calendar.month_name[i] for i in range(1,12)]\nylabel=[listd.count(i) for i in xlabel]\nplt.figure(figsize=(10,10))\nplt.barh(xlabel, ylabel)\nplt.xlabel('Number of Publications', fontsize=10)\nplt.ylabel('Month', fontsize=10)\n#plt.title('Number of publications in each month in 2020')\nplt.title(\"Number of publications in each month in 2020\", bbox={'facecolor':'0.9', 'pad':5})\n\nplt.show()\n\n#author analysis\ncols = ['authors']\ndata=metadata[metadata[cols].notna().all(1)]\nprint(data.shape)\ndf=data['authors'].value_counts()\nx=df.iloc[[0,1,2,3,4,5,6,7,8,9]]\nx.plot.pie(figsize = (10,10),autopct = '%.2f%%',\n title = 'Top Ten authors')\nplt.title(\"Top Ten authors\", bbox={'facecolor':'0.9', 'pad':5})\nplt.show()\n","230c36bb":"#get titles and abstracts\nmetadata['titleabstract']=metadata.title+'. '+metadata.abstract\n#remove NaN from both titles and abstracts  \ncols = ['titleabstract']\ndata=metadata[metadata[cols].notna().all(1)]\nprint(\"Total Number of papers to be processed=\",len(list(data['titleabstract'])))  \n\nprint(\"PreProcessing Progress:\")  \n\n#processing   \nlistdocs=[]\ntags=['NN','NNS','NNP','NNPS']\nbar=tqdm(list(data['titleabstract']))\nfor d in bar :\n    docclean=cleantext.clean(d, extra_spaces=True,stopwords=True,lowercase=True,numbers=True,punct=True)\n    text = nltk.word_tokenize(docclean)\n    tagged_text=nltk.pos_tag(text)\n    #filter out words with length=1 or 2\n    words=[]\n    for w in tagged_text:\n        if len(w[0])>2 and w[1] in tags:\n            words.append(lemmatizer.lemmatize(w[0]))      \n    listdocs.append(words)\n    pass\n  \ndct = Dictionary(listdocs)\ndct.filter_extremes(no_below=1000, no_above=0.5)\ndct.save('dct.dict')\ndd=[dct.doc2bow(d) for d in listdocs]\nprint(dct)","e4304de1":"tdfreq=dict()\n\ntdfreq={}\n\nfor i in range(len(dct)):\n    tdfreq[dct[i]]=dct.dfs[i]\n\nl = sorted(tdfreq.items() , reverse=True, key=lambda x: x[1])\n\nwordcloud = WordCloud(background_color='white',\n                      width=1500,\n                      height=1000\n                      ).generate_from_frequencies(tdfreq)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title(\"Wordcloud of the used vocabulary\", bbox={'facecolor':'0.9', 'pad':5})\nplt.show()\n\n\n#show the topmost 50 terms\nxlabel=[l[i][0] for i in range(1,50)]\nylabel=[l[i][1] for i in range(1,50)]\nplt.figure(figsize=(25,15))\nplt.barh(xlabel, ylabel)\nplt.xlabel('Number of documents', fontsize=10)\nplt.ylabel('Term', fontsize=10)\n#plt.title('Number of publications in each month in 2020')\nplt.title(\"Number of papers where the topmost 50 terms appear\", bbox={'facecolor':'0.9', 'pad':5})\n\nplt.show()\n","ebd21c44":"#Create Train and save doc2vec models (model0=DBOW, model1=PV-DM)\nprint(\"---------CREATE AND TRAIN DOC2VEC MODELS----------\")\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(listdocs)]\nmodel0 = Doc2Vec(documents, dm=0, vec_size=25, window=2, min_count=5, workers=4)\nmodel1 = Doc2Vec(documents, dm=1, vec_size=25, window=2, min_count=5, workers=4)\n\nmax_epochs = 10\nbar=tqdm(range(max_epochs))\nfor epoch in bar:\n    #Train with 10 epochs\n    model0.train(documents, total_examples=model0.corpus_count, epochs=model0.iter)\n    model1.train(documents, total_examples=model1.corpus_count, epochs=model1.iter)\n    pass\n#save the models    \nmodel0.save(\"doc2vecmodel0.dict\")\nmodel1.save(\"doc2vecmodel1.dict\")\nprint(\"------MODELS CREATED AND SAVED---------\")\n","6d41a563":"#Testing\nprint(\"-------------------Choose a Doc2Vec Model------------\")\nprint(\"1- Distributed Bag Of Words (DBOW)\")\nprint(\"2- Distributed Memory (PV-DM)\")\nchoosenmodel = input(\"Choose a model:\")\nif choosenmodel==1:\n    model = Doc2Vec.load(\"doc2vecmodel0.dict\")\nelse:\n    model = Doc2Vec.load(\"doc2vecmodel1.dict\")\nprint(\"------LOAD MODEL-------\")\ntags=['NN','NNS','NNP','NNPS']\nTASKS=[\n'What is known about transmission, incubation, and environmental stability?',\n'What do we know about COVID-19 risk factors?',\n'What do we know about virus genetics, origin, and evolution?',\n'What do we know about vaccines and therapeutics?',\n'What do we know about non-pharmaceutical interventions?',\n'What has been published about medical care?',\n'What do we know about diagnostics and surveillance?',\n'What has been published about information sharing and inter-sectoral collaboration?',\n'What has been published about ethical and social science considerations?']\n\nlisttasks=[]\n\nfor d in TASKS:\n    docclean=cleantext.clean(d, extra_spaces=True,stopwords=True,lowercase=True,numbers=True,punct=True)\n    text = nltk.word_tokenize(docclean)\n    tagged_text=nltk.pos_tag(text)\n    #filter out words with length=1 or 2\n    words=[]\n    for w in tagged_text:\n        if len(w[0])>2 and w[1] in tags:\n            words.append(lemmatizer.lemmatize(w[0]))      \n    listtasks.append(words)\n\nprint(\"------------------------ TASK KEY WORDS ------------------------\")\ni=0\nfor t in listtasks:\n    print(\"Task\",i+1,\" :\")\n    print(listtasks[i])\n    i=i+1\n    \ntaskdocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(listtasks)]\n\n\n\ntestdocs=[]\nfor t in taskdocs:\n    testdocs.append(model.infer_vector(t.words))\n\n\nprint(\"----------Visualization with t-SNE----------\")\n\nfrom sklearn.decomposition import TruncatedSVD\n\n\ndoc_embeddings = model.docvecs.vectors_docs[:len(model.docvecs)]\n\n\n#tsne = TSNE(perplexity=30,n_components=3, random_state=0)\n#tsne = TSNE(perplexity=15)\ntsne=TruncatedSVD(n_components=15, n_iter=10, random_state=42)\nY = tsne.fit_transform(doc_embeddings)\nY1 = tsne.fit_transform(testdocs)\n\nplt.figure(figsize=(15,15))\nx_coords1 = Y[:, 0]\ny_coords1 = Y[:, 1]\n\nx_coords2 = Y1[:, 0]\ny_coords2 = Y1[:, 1]\n# display scatter plot\nplt.scatter(x_coords1, y_coords1)\nplt.scatter(x_coords2, y_coords2)\nplt.title(\"t-SNE reduction for documents embeddings visualization (tasks are red circles and blue circles are our papers\", bbox={'facecolor':'0.9', 'pad':5})\nplt.show()\n\nimport csv\n\nprint(\"----------The 10 topmost similar papers for each task----------\") \nfor i in range(9):\n    print(\"********Task\",i+1,\"**********\")\n    vector = model.infer_vector(listtasks[i])\n    sim = model.docvecs.most_similar([vector],topn=20)\n    filename=\"task\"+str(i)+\".csv\"\n    csv_file=open(filename, mode='w',encoding=\"utf-8\",newline='') \n    writer = csv.writer(csv_file)\n    fieldnames = ['doi','source_x','publish_time','journal','title','abstract']\n    writer.writerow(fieldnames)\n    \n    for elem in sim:\n        row=[]\n        row.append(data['doi'].tolist()[elem[0]])\n        row.append(data['source_x'].tolist()[elem[0]])\n        row.append(data['publish_time'].tolist()[elem[0]])\n        row.append(data['journal'].tolist()[elem[0]])\n        row.append(data['title'].tolist()[elem[0]])\n        row.append(data['abstract'].tolist()[elem[0]])\n        writer.writerow(row)\n    csv_file.close()\n    resultdata=pd.read_csv(filename)\n    display(resultdata)\n\n#other task\nnewtask = input(\"Enter your new task: \") \n\n#preprocess new task\ndocclean=cleantext.clean(newtask, extra_spaces=True,stopwords=True,lowercase=True,numbers=True,punct=True)\ntext = nltk.word_tokenize(docclean)\ntagged_text=nltk.pos_tag(text)\n#filter out words with length=1 or 2\nwords=[]\nfor w in tagged_text:\n    if len(w[0])>2 and w[1] in tags:\n        words.append(lemmatizer.lemmatize(w[0])) \n\nvector = model.infer_vector(words)\nsim = model.docvecs.most_similar([vector],topn=20)\nfilename=\"newtask.csv\"\ncsv_file=open(filename, mode='w',encoding=\"utf-8\",newline='') \nwriter = csv.writer(csv_file)\nfieldnames = ['doi','source_x','publish_time','journal','title','abstract']\nwriter.writerow(fieldnames)\n    \nfor elem in sim:\n    row=[]\n    \n    row.append(data['doi'].tolist()[elem[0]])\n    row.append(data['source_x'].tolist()[elem[0]])\n    row.append(data['publish_time'].tolist()[elem[0]])\n    row.append(data['journal'].tolist()[elem[0]])\n    row.append(data['title'].tolist()[elem[0]])\n    row.append(data['abstract'].tolist()[elem[0]])\n    writer.writerow(row)\n\ncsv_file.close()\nresultdata=pd.read_csv(\"newtask.csv\")\ndisplay(resultdata)\n","eb8fd583":"# **Preprocessing phase**","4f055b39":"# **Approach**\nOur approach uses only the title and the abstract of each paper to identify the top 10 most similar papers in which you can find the answer of any task listed in the competition. In this system two Doc2Vec embedding techniques are implemented (Distributed Memory (PV-DM) and Distributed Bag Of Words (DBOW)). \n1. After getting titles and abstracts from **metadata.csv** file, we cleaned and processed all documents to remove digits, punctuation and stop words and lemmatize the remaining words to keep only nouns\n* Filter out the words that appear in less than 1000 papers and in more than 50% of the papers\n2.\tWe created two Doc2Vec models (PV-DM and DBOW) and we trained them\n3.\tWe saved models in files to be queried later on.\n4.\tFor each task we saved the top 20 similar papers in csv files to be displayed to the user\n\n# **Features**\n1. Visualization of the percentage of publications by journal, by author, by publication year\n2. Visualize the number of publications in 2020 and for each month\n3. Show a word cloud of all terms used\n4. Show the top most terms used. For each term shows the number of documents where it appears.\n5. Visualization of documents and tasks using T-SNE \n6. Use Doc2Vec embedding to get more semantics\n7. We used only title and abstract to reduce the training and testing time\n8. It is possible to propose any new task and get the most similar papers\n\n# **Pros:**\n1. The generated Doc2Vec models are saved to reduce the querying time. So once trained, the model is easy and fast to apply\n2. Helps to use the semantic of words when searching for the most similar papers for any task\n\n# **Cons:**\n\n1. It is not possible to get the best numbers of epochs to train the model. Here we choose 10 epochs.","2bd11734":"# **Display used terms** ","5a0ebd6f":"# **Training phase**","19fe4c09":"# **Document embeddings to find the most similar papers for each task**","5c85eea5":"# **Introduction** \nCOVID-19 is an infectious disease which was first identified in 2019 in Wuhan-China and after that it has spread globally, resulting in the ongoing coronavirus pandemic. In front of this pandemic, thousands of research papers have been published especially in 2020. This papers cover many research questions related to coronavirus and other viruses. This competition lists some examples of research questions as tasks to be solved.\nIn this notebook,  we present a system to identify the top most related papers that answer any task including the tasks of the  current competition. Our system uses only the titles and the abstracts and therefore it does not require too much time.  Our system uses two document embeddings techniques (Distributed Memory (PV-DM) and Distributed Bag Of Words (DBOW)). This system is very useful for any one  interested to know about the COVID-19 pandemic including researchers, hosiptal doctors, nurses, decision makers, etc.\n","61275b66":"# **Testing phase**","91ccda68":"# **Data analysis**"}}