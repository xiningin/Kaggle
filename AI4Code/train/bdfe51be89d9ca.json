{"cell_type":{"f71901ee":"code","26cc14d9":"code","6370ad41":"code","bb01f13b":"code","ba2a27de":"code","dddfe764":"code","8ab7325a":"code","cb353e84":"code","6f265b00":"code","5bfe1e6b":"code","57f61de4":"code","1f158ce2":"code","6ce67206":"code","d84a9413":"code","b49baf9b":"code","38d73ae0":"code","bb21634b":"code","5eac5ffa":"code","9bec1fa7":"code","382eaa7d":"code","8241166f":"code","e23d2fa0":"code","c77a835f":"code","396ebb9a":"code","ebdfd77e":"code","f935210c":"code","2326ecce":"code","ae57c459":"code","022f62c1":"code","6ad53d42":"code","7413a828":"code","77e7cd4a":"code","c0f61cbc":"code","e804a1e2":"code","9a6bf8d5":"code","17ebee25":"code","c5424012":"code","57e60f6d":"markdown","53552f4f":"markdown","c97b843c":"markdown","046f4a22":"markdown","249cb3d3":"markdown","fe92dd34":"markdown","92ae3f36":"markdown","d5553fde":"markdown","8a278a8c":"markdown","e2715569":"markdown"},"source":{"f71901ee":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('..\/input\/fullcsv\/full.csv')","26cc14d9":"data.head()","6370ad41":"df = data.copy()\ndf.shape","bb01f13b":"df.info()","ba2a27de":"df.describe(include='all')","dddfe764":"df.dtypes.value_counts()","8ab7325a":"df.dtypes.value_counts().plot.pie()","cb353e84":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.isna(), cbar=False)","6f265b00":"(df.isna().sum() \/ df.shape[0]).sort_values()","5bfe1e6b":"df = df[df.columns[df.isna().sum()\/df.shape[0] <= 0.9]]\ndf.head()","57f61de4":"plt.figure(figsize=(20,10))\nsns.heatmap(df.isna(), cbar=False)","1f158ce2":"df = df.drop(['id_mutation', 'code_departement', 'nom_commune', 'code_commune', 'id_parcelle', 'date_mutation', 'adresse_code_voie', 'adresse_nom_voie', 'longitude', 'latitude', 'adresse_numero', 'nature_culture'], axis=1)","6ce67206":"'''for col in df.select_dtypes('float'):\n    plt.figure()\n    sns.distplot(df[col])'''","d84a9413":"#sns.pairplot(df.select_dtypes(exclude='object'))","b49baf9b":"for col in df.select_dtypes('object').columns:\n    print(f'{col:-<30} {df[col].unique()}')","38d73ae0":"import seaborn as sns\nsns.heatmap(df.select_dtypes(exclude='object').corr(), vmin=0)","bb21634b":"sns.clustermap(df.select_dtypes('float').corr(), vmin=0)","5eac5ffa":"df['type_local'].value_counts()","9bec1fa7":"df = df[df['type_local']=='Local industriel. commercial ou assimil\u00e9']","382eaa7d":"df.head()","8241166f":"df['code_type_local'].value_counts()","e23d2fa0":"df['nombre_pieces_principales'].value_counts()","c77a835f":"df = df.drop(['type_local','code_type_local','nombre_pieces_principales'], axis=1)","396ebb9a":"df.head()","ebdfd77e":"df.shape","f935210c":"df.dtypes.value_counts().plot.pie()","2326ecce":"(df.isna().sum() \/ df.shape[0]).sort_values()","ae57c459":"df_categorial = df.select_dtypes('object')","022f62c1":"df_categorial.count()","6ad53d42":"df_numerical = df.select_dtypes(exclude = 'object')","7413a828":"df_numerical.count()","77e7cd4a":"from sklearn.impute import KNNImputer, SimpleImputer\n\ndef impute_numerical(df):\n    imputer = KNNImputer()\n    df = pd.DataFrame(imputer.fit_transform(df), index = df.index, columns=df.columns)\n    return df\n\ndef impute_categorial(df):\n    imputer = SimpleImputer(missing_values='nan' , strategy='most_frequent')\n    df = pd.DataFrame(imputer.fit_transform(df.astype(str)),index = df.index, columns=df.columns)\n    return df\n\n'''#Nous pouvons faire plus simple avec cette fonction \u00e0 la place des deux fonctions indiqu\u00e9es ci-dessus\n\ndef impute(df):\n    df = df.fillna(-999)\n    return df'''","c0f61cbc":"from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n\ndef encodage(df):\n    encoder = OrdinalEncoder()\n    df = pd.DataFrame(encoder.fit_transform(df.astype(str)),index = df.index, columns=df.columns)\n    return df\n\n'''\n#Nous pouvons egalement utilis\u00e9 OneHotEncoder o\u00f9 nous d\u00e9composons les variables initiales en plusieurs sous \n#variables, cr\u00e9ant donc autant de colonnes que de classes qu'on a dans cette variable initiale.\n\ndef encodage_1(df):   \n    encoder = OneHotEncoder(sparse=False)\n    df = pd.DataFrame(encoder.fit_transform(df), index = df.index)\n    return df\n\n#Comme nous pouvons faire plus simple avec pandas sans passer par sklearn\n\nfor col in df.select_dtypes('object').columns:\n        df.loc[:,col] = df[col].astype('category').cat.codes'''","e804a1e2":"def preprocessing(df_numerical, df_categorial):\n\n    df_numerical = impute_numerical(df_numerical)\n    df_categorial = impute_categorial(df_categorial)\n    df_categorial = encodage(df_categorial)\n    df = pd.concat((df_numerical,df_categorial), axis=1)\n    \n    return df","9a6bf8d5":"preprocessing(df_numerical, df_categorial)","17ebee25":"df = preprocessing(df_numerical, df_categorial)","c5424012":"from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=2)\nmodel.fit(df)\ncentroid = model.cluster_centers_\n\nplt.scatter(df['surface_reelle_bati'], df['valeur_fonciere'], c=model.labels_, )\nplt.scatter(centroid[:,0], centroid[:,1], c='r')","57e60f6d":"# 2) Data pre-processing\n## Nettoyage - Encodage","53552f4f":"##  Visulation initiale - Elimination des colonnes inutiles","c97b843c":"## Relation Variables \/ Variables ","046f4a22":"# Analyse plus d\u00e9taill\u00e9e :\n## Filtrer le jeu de donn\u00e9es","249cb3d3":"# 3) Modelling\n## Entrainement d'un mod\u00e8le de Machine Learnig","fe92dd34":"## Variables Qualitatives ","92ae3f36":"# Analyse de Fond :","d5553fde":"# 1) Exploratory Data Analysis\n## Analyse de Forme :","8a278a8c":"# Comment g\u00e9rer un projet de Data Science: \n\nUn projet de data science se fait en 3 grandes parties: \nD\u2019abord l\u2019analyse et l\u2019exploration de donn\u00e9es(EDA), cette partie est consacr\u00e9e pour se mettre \u00e0 l\u2019aise avec le dataset et comprendre au maximum nos variables afin de d\u00e9finir une strat\u00e9gie de mod\u00e9lisation. Puis vient la partie de preprocessing qui est le nettoyage de donn\u00e9es, dont l\u2019objectif est de transformer le dataset pour le mettre dans un format propice au d\u00e9veloppement de mod\u00e8les de Machine Learning. Et enfin arrive l\u2019\u00e9tape de mod\u00e9lisation o\u00f9 nous entrainons finalement notre mod\u00e8le pour arriver \u00e0 notre objectif.\n\nIci nous somme dans le cas de l'apprentissage non supervis\u00e9 et face \u00e0 un probl\u00e8me de classification, o\u00f9 la machine essaye elle m\u00eame d'pprendre \u00e0 regrouper les donn\u00e9es selon leurs ressemblances. Pour cela, nous utilisons le kmeans clustering qui est un algorithme it\u00e9ratif qui cherche \u00e0 minimiser la fonction inertia, et fonctionne en 2 \u00e9tapes:\n- Affectation des points du dataset au centroid le plus proche\n- Deplacement du centroid au milieu(la moyenne) du cluster \n\nLa question n'\u00e9tant pas tr\u00e8s claire en ce qui concerne le jeu de donn\u00e9es \u00e0 utiliser, je me suis servie du jeu de donn\u00e9es qui pr\u00e9sente les transactions immobili\u00e8res de 2020.\nJe tiens quand m\u00eame \u00e0 pr\u00e9ciser que c'est possible de fusionner tous les jeux de donn\u00e9es de 2014 \u00e0 2020 (ce qui rend l'ex\u00e9cution beaucoup plus lente) en utilisant :\n- [filenames=['full14.csv','full15.csv','full16.csv','full17.csv','full18.csv','full19.csv','full20.csv']\n- combined_csv = pd.concat( [ pd.read_csv(f) for f in filenames ] )]\n\net proc\u00e9der de la m\u00eame mani\u00e8re que ci-dessous:\n\n\n# 1) Exploratory Data Analysis\n## Objectif \n\n  - Comprendre du mieux possible nos donn\u00e9es \n  - D\u00e9velopper une premiere strat\u00e9gie de mod\u00e9lisation\n\n## Checklist de base\n### Analyse de Forme :\n\n  - **Variable target** : type_local\n  - **Lignes et colonnes** : 827105, 40\n  - **Types de variables** : qualitatives : 19, quantitatives : 21\n  - **Analyse des valeurs manquantes** :\n       - Beaucoup de NaN (40% de variables > 90% de NaN)\n       \n### Analyse de Fond :\n\n  - **Visulation initiale - Elimination des colonnes inutiles**\n       - Suppression des colonnes qui contiennent plus de 90% de valeurs manquantes\n       - Suppression des colonnes 'id_mutation','code_departement', 'nom_commune', 'code_commune', 'id_parcelle', 'date_mutation', 'adresse_code_voie', 'adresse_nom_voie', 'longitude', 'latitude', 'adresse_numero', 'nature_culture' qui n'aident pas sp\u00e9cialement notre mod\u00e8le de classification dans son developpement     \n  - **Signification des variables** \n       - Variables continues d\u00e9j\u00e0 normalis\u00e9es, donc inutile d'utiliser StandardScaler() ou Robustscaler()\n       - Variables qualitatives : besoin d'\u00eatre encod\u00e9es dans la partie pr\u00e9-processing\n  - **Relation Variables \/ Variables**  \n       - Faible corr\u00e9lation\n\n### Analyse plus d\u00e9taill\u00e9e :\n  - **Filtrer le jeu de donn\u00e9es**\n       - Retirer Maisons - Appartements - D\u00e9pendances de la colonne 'type_local', ce qui nous laisse l'unique classe Local industriel. commercial ou assimil\u00e9 >> la colonne 'type_local' peut donc \u00eatre supprim\u00e9e car elle n'influe pas sur les r\u00e9sultats du mod\u00e8le \n       - Apr\u00e8s ce filtre, nous constatons que la colonne 'nombre_pieces_principales' et 'type_local' ne contient qu'une valeur unique, donc elles peuvent \u00eatre \u00e0 leur tour supprim\u00e9es\n       - Dataset r\u00e9duit \u00e0 **30674 lignes** et **9 colonnes**\n  - **NaN analyse**  \n      - Nous remarquons qu'il nous manque des donn\u00e9es, donc il faudrait les remplir par d\u00e9faut, ce qui peut changer la r\u00e9alit\u00e9! Ou les supprimer, quit \u00e0 perdre quelques donn\u00e9es que de corompre la base (parfois c'est plus recommand\u00e9), sauf qu'ici, si nous supprimons les valeurs manquantes, nous nous retrouverons uniquement avec 12642 donn\u00e9es contre 30674, ce qui ferait presque 60% de donn\u00e9es perdues\n      \n# 2) Data pre-processing\n   - **Nettoyage - Encodage** \n       - Remplir les valeurs manquantes du dataset, et remplacer les chaines de caract\u00e8res par des valeurs num\u00e9riques, l\u2019objectif ici est de transformer le dataset pour le mettre dans un format propice au d\u00e9veloppement de mod\u00e8les de Machine Learning\n       \n# 3) Modelling\n   - **Entrainement d'un mod\u00e8le de Machine Learnig**\n       - Utilisation du kMeans pour trouver nos 2 classes 'espace Logistique\/Local d'activit\u00e9 ' et 'un commerce \/ bureau'","e2715569":"## NaN analyse et cr\u00e9ation des sous-ensembles numerical et categorial"}}