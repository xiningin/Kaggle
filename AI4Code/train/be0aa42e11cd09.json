{"cell_type":{"7d07f5ba":"code","7c69f60b":"code","0cef640b":"code","0c78ab27":"code","28697463":"code","4aa250dd":"code","9088ab24":"code","ecc3a49a":"code","3f95f0b0":"code","38812cd6":"code","3621b166":"code","a3466ba5":"code","99ee8729":"code","4b69ac8d":"code","8a4c9275":"code","a7064e78":"code","439ecace":"code","7404e64d":"code","100b2536":"code","e9008702":"code","b24b4057":"code","2d0c5a45":"code","d428f8d1":"code","b9e93b1c":"code","fff743ef":"markdown","a46a9c38":"markdown","6cdb6853":"markdown","87265841":"markdown","4d30ad61":"markdown","a83d62d4":"markdown","37d02d88":"markdown","54179030":"markdown","e1ffca90":"markdown","8bedc038":"markdown","4ea62b3c":"markdown","652bb4b4":"markdown","172ae317":"markdown"},"source":{"7d07f5ba":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\nimport xgboost as xgb\n\nimport lightgbm as lgbm\nfrom sklearn.utils import shuffle, resample\nfrom sklearn.preprocessing import StandardScaler\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n# Similarly LGBMRegressor can also be imported for a regression model.\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\n\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nfrom skopt.plots import plot_convergence","7c69f60b":"train = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/train.csv\")\ntest = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/test.csv\")","0cef640b":"test.drop(columns=\"ID\", inplace=True)\ntest.head(2)","0c78ab27":"test[\"TARGET\"] = 2\nall_data = pd.concat([train,test],axis=0)\nall_data.shape","28697463":"all_data.drop(columns=[\"TXN_TRM\",\"MC_NAME\"], inplace=True)","4aa250dd":"cat_cols = ['CST_NR', 'CC_NR', 'DAY_OF_MONTH',\n       'TXN_SOURCE', 'TXN_ENTRY', 'CITY', 'COUNTRY',\n   'MC_ID', 'MCC_CODE']","9088ab24":"all_data[cat_cols] = all_data[cat_cols].fillna(\"missing\")","ecc3a49a":"# Label encoding categorical features\nfrom sklearn import preprocessing\nfor col in cat_cols:\n    print(col)\n    le = preprocessing.LabelEncoder()\n    all_data[col] = le.fit_transform(all_data[col].values)","3f95f0b0":"all_data = all_data.sort_values(by=[\"DAY_OF_MONTH\",\"TXN_TIME\"])\nall_data[\"top_islem1\"] = 1\nall_data['date_block_num'] = range(933739)","38812cd6":"def lag_feature(all_data, lags, col):\n    tmp = all_data[['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        all_data = pd.merge(all_data, shifted, on=['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num'], how='left')\n    return all_data","3621b166":"all_data = lag_feature(all_data, [1],\"top_islem1\")\nall_data[\"loc_sum\"] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1_lag_1'].transform('sum') ","a3466ba5":"all_data['TXN_AMNT'] = StandardScaler().fit_transform(all_data['TXN_AMNT'].values.reshape(-1, 1))","99ee8729":"all_data[\"top_islem1\"] = 1","4b69ac8d":"all_data[\"weekend\"]  = np.where((all_data[\"DAY_OF_WEEK\"] < 5.5)&(all_data[\"DAY_OF_WEEK\"] > 1.5),0 ,1)","8a4c9275":"all_data['top_isyeri_islem'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\n#all_data['top_isyeri_saniyelik_islem'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME', 'MC_ID'])['top_islem1'].transform('sum')  #\u00fcye i\u015fyeri ba\u015f\u0131na saniyelik toplam i\u015flem\nall_data['top_mus_islem'] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['top_kart_islem'] = all_data.groupby(['DAY_OF_MONTH','CC_NR'])['top_islem1'].transform('sum') #kart no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['isyeri_top_tutar'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['TXN_AMNT'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam tutar\nall_data['top_mussaniyelik_islem'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik toplam i\u015flem\nall_data['top_mussaniyelik_ort_tutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['TXN_AMNT'].transform('mean') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik ortalama tutar\nall_data['top_mussaniyelik_tutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['TXN_AMNT'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik toplam tutar\n\nall_data['isyeri_weekend_ort_tutar'] = all_data.groupby(['weekend','MC_ID'])['TXN_AMNT'].transform('mean') #\u00fcye i\u015fyeri ba\u015f\u0131na hsonu veya hi\u00e7i ortalama tutar\nall_data['isyeri_weekendsaniye_ort_tutar'] = all_data.groupby(['weekend','TXN_TIME','MC_ID'])['TXN_AMNT'].transform('mean')\nall_data['isyeri_weekend_ort_islem'] = all_data.groupby(['weekend','MC_ID'])['top_islem1'].transform('mean') #\u00fcye i\u015fyeri ba\u015f\u0131na hsonu veya hi\u00e7i ortalama islem\n\nall_data['isyeri_ort_tutar'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['TXN_AMNT'].transform('mean')  #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck ortalama tutar\nall_data['isyeri_ort_saniyetutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME', 'MC_ID'])['TXN_AMNT'].transform('mean')\nall_data['mus_saniyeislem_yuku'] = all_data['top_mussaniyelik_tutar'] \/ all_data['top_mussaniyelik_ort_tutar']  # ki\u015finin tutar sapmas\u0131\nall_data['isyeri_saniyetutar_yuku'] = all_data['TXN_AMNT'] \/ all_data['isyeri_weekendsaniye_ort_tutar']\nall_data['isyeri_tutar_yuku'] = all_data['TXN_AMNT'] \/ all_data['isyeri_weekend_ort_tutar']\nall_data['isyeri_islem_yuku'] = all_data['top_isyeri_islem'] \/ all_data['isyeri_weekend_ort_islem']\nall_data['isyeri_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['isyeri_ort_tutar'] # ki\u015finin tutar\u0131 \/ \u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck ortalama tutar\n#all_data['isyeri_saniye_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['isyeri_ort_saniyetutar']\n#all_data['mus\/kart'] = all_data['top_mus_islem'] \/ all_data['top_kart_islem'] # m\u00fc\u015fteri no i\u015flem say\u0131s\u0131\/ kart no i\u015flem say\u0131s\u0131\nall_data['top_mus_islem'] = all_data.groupby(['CC_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na  toplam i\u015flem\nall_data['top_isyeri_islem'] = all_data.groupby(['MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na  toplam i\u015flem\nall_data['m\u00fcsteri_isyerinde_islem'] = all_data.groupby(['CC_NR','MC_ID'])['top_islem1'].transform('sum') #M\u00fc\u015fterinin o i\u015fyerinde ka\u00e7 i\u015flemi var\n#all_data['kacinci_islem'] = all_data.groupby(['CC_NR','MC_ID'])['top_islem1'].transform('sum') #M\u00fc\u015fterinin o i\u015fyerindeki ka\u00e7\u0131nc\u0131 i\u015flemi\nall_data['mus_tutar_top'] = all_data.groupby(['CC_NR'])['TXN_AMNT'].transform('sum') #m\u00fc\u015fteri tutar topmal\u0131\nall_data['mus_tutar_top'].fillna(0, inplace = True)\nall_data['mus_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['mus_tutar_top'] # ki\u015finin tutar\u0131 \/ m\u00fc\u015fteri tutar topmal\u0131\nall_data['mus_tutar\/ort'].fillna(0, inplace = True)\nall_data['isyeri_ort_tutar'] = all_data.groupby(['MC_ID'])['TXN_AMNT'].transform('mean')  #\u00fcye i\u015fyeri ba\u015f\u0131na ortalama tutar\nall_data['isyeri_ort_tutar'].fillna(0, inplace = True)\nall_data['gunlultop_mus_islem'] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['gunluktop_kart_islem'] = all_data.groupby(['DAY_OF_MONTH','CC_NR'])['top_islem1'].transform('sum') #kart no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['gunluktop_isyeri_islem'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem","a7064e78":"all_data.drop(columns = [\"DAY_OF_WEEK\",\"DAY_OF_MONTH\"], inplace=True)","439ecace":"all_data['TARGET'] = all_data['TARGET'].apply(np.int32)","7404e64d":"# Splitting back to train and test\ntest = all_data.iloc[len(train):]\ntrain = all_data.iloc[:len(train)]\ntrain.shape,test.shape","100b2536":"# separate classes into different datasets\nnormal_class = train.query('TARGET == 0')\nfraudulent_class = train.query('TARGET == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=1210)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=1210)\nresampled = normal_class.sample(n=int(len(fraudulent_class)*4.4), random_state=1210)\ntrain = pd.concat([fraudulent_class,resampled])","e9008702":"train.drop(columns = [\"top_islem1\"], inplace=True)\ntest.drop(columns =[\"top_islem1\"], inplace=True)","b24b4057":"y=train[\"TARGET\"]\nX=train[['TXN_SOURCE',\"CC_NR\",\"CST_NR\",\n       'TXN_ENTRY', 'TXN_AMNT', 'CITY', 'COUNTRY', 'MC_ID', 'MCC_CODE', 'weekend', 'top_isyeri_islem',\n        'top_mus_islem', 'top_kart_islem','loc_sum',\n       'isyeri_top_tutar', 'top_mussaniyelik_islem',\n       'top_mussaniyelik_ort_tutar', 'top_mussaniyelik_tutar',\n       'isyeri_weekend_ort_tutar', 'isyeri_weekendsaniye_ort_tutar',\n       'isyeri_weekend_ort_islem', 'isyeri_ort_tutar',\n       'isyeri_ort_saniyetutar', 'mus_saniyeislem_yuku',\n       'isyeri_saniyetutar_yuku', 'isyeri_tutar_yuku', 'isyeri_islem_yuku',\n       'isyeri_tutar\/ort', \n       'm\u00fcsteri_isyerinde_islem', 'mus_tutar_top', 'mus_tutar\/ort',\n       'gunlultop_mus_islem', 'gunluktop_kart_islem',\n       'gunluktop_isyeri_islem']]","2d0c5a45":"import numpy ","d428f8d1":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, lambda_l1,lambda_l2,early_stopping_round):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['lambda_l1'] = int(round(lambda_l1))\n        params['lambda_l2'] = int(round(lambda_l2))\n        params['early_stopping_round'] = int(round(early_stopping_round))\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =50,num_boost_round=200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.04, 0.65),\n                                            'num_leaves': (8, 80),\n                                            'feature_fraction': (0.21, 0.4),\n                                            'bagging_fraction': (0.2, 0.4),\n                                            'max_depth': (4, 25),\n                                            'max_bin':(5,45),\n                                            'early_stopping_round' : (10,80),\n                                            'lambda_l1': (0, 10),\n                                            'lambda_l2':  (0, 10)}, random_state=200)                                   \n                                      \n\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6)","b9e93b1c":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\n#opt_params[1]['is_unbalance']=True\n#opt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","fff743ef":"<a id=\"3\"><\/a> <br>\n# **Feature Engineering**\n<br>","a46a9c38":"Referance : https:\/\/www.kaggle.com\/somang1418\/tuning-hyperparameters-under-10-minutes-lgbm","6cdb6853":"<a id=\"5\"><\/a> <br>\n# **BEST PARAMS**\n<br>\n","87265841":"![](https:\/\/media.giphy.com\/media\/3oz8xKaR836UJOYeOc\/giphy.gif)","4d30ad61":"Here are some academic papers about Bayesian Optimization just in case you are interested in:\n\n1) http:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf\n\n2) https:\/\/arxiv.org\/pdf\/1012.2599v1.pdf","a83d62d4":"# TIME IS IMPORTANT","37d02d88":"<a id=\"1\"><\/a> <br>\n\n# **What Is Bayesian Optimization and Why Do We Care?**","54179030":"<a id=\"5\"><\/a> <br>\n# **Referance**\n<br>\n","e1ffca90":"<a id=\"2\"><\/a> <br>\n# **Loading Library and Dataset**","8bedc038":"<a id=\"4\"><\/a> <br>\n# **UNDERSAMPLING**\n<br>","4ea62b3c":"**Bayesian Optimization** is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. [(source)](https:\/\/towardsdatascience.com\/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)<br> It is very effective with real-world applications in high-dimensional parameter-tuning for complex machine learning algorithms. Bayesian optimization utilizes the Bayesian technique of setting a prior over the objective function and\ncombining it with evidence to get a posterior function. I attached one graph that demonstrates Bayes\u2019 theorem below. <br> <br>\n\n<img src=\"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/06\/12-768x475.jpg\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n<br> <br> \n The prior belief is our belief in parameters before modeling process. The posterior belief is our belief in our parameters after observing the evidence.\n<br> Another way to present the Bayes\u2019 theorem is: \n\n<img src=\"https:\/\/www.maths.ox.ac.uk\/system\/files\/attachments\/Bayes_0.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/> <br> \n\nFor continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made, which means that we need to give range of values of hyperparameters (ex. learning rate range from 0.1 to 1).  So, in our case, the Gaussian process gives us a prior distribution on functions. Gaussian process approach is a non-parametric approach, in that it finds a distribution over the possible functions \nf(x) that are consistent with the observed data. Gaussian processes have proven to be useful surrogate models for computer experiments and good\npractices have been established in this context for sensitivity analysis, calibration and prediction While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. [(source)](http:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n\n<br>\n**Well..why do we care about it?** <br> \nAccording to the [study](http:\/\/proceedings.mlr.press\/v28\/bergstra13.pdf), hyperparameter tuning by Bayesian Optimization of machine learnin models is more efficient than Grid Search and Random Search. Bayesian Optimization has better overall performance on the test data and takes less time for optimization. Also, we do not need to set a certain values of parameters like we do in Random Search and Grid Search. For Bayesian Optimization tuning, we just give a range of a hyperparameter. \n\n","652bb4b4":"<br>\n  CONTENTS\n\n1. [What Is Bayesian Optimization and Why Do We Care?](#1)\n2. [Loading Library and Dataset](#2)\n3. [Feature Engineering ](#3)\n4. [Undersampling](#4)\n5. [BAYESIAN OPTIMIZATION](#5)\n6. [Best Params](#6)\n7. [Referance](#7)","172ae317":"<a id=\"5\"><\/a> <br>\n# **BAYESIAN OPTIMIZATION**\n<br>\n"}}