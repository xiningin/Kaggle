{"cell_type":{"b0ea262b":"code","a1947e4a":"code","a4eeaabe":"code","d21265dd":"code","1b898b0a":"code","b48bc5e3":"code","4624a10d":"code","7e73fcde":"code","21d365d2":"markdown","b5262756":"markdown","e5286c37":"markdown","b0cd9074":"markdown","8667abdc":"markdown","ae813c92":"markdown","9639e05a":"markdown","f391710d":"markdown","26118c16":"markdown","e239193e":"markdown","28e8960b":"markdown"},"source":{"b0ea262b":"import os\nimport sys\nimport tqdm\nimport gzip\nimport random\nimport math\nimport numpy as np\n\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torchtext import data, datasets, vocab\n\n# a funciton to get the device used either cpu or gpu\ndef d(tensor=None):\n    \"\"\"\n    Returns a device string either for the best available device,\n    or for the device corresponding to the argument\n    :param tensor:\n    :return:\n    \"\"\"\n    if tensor is None:\n        return 'cuda' if torch.cuda.is_available() else 'cpu'\n    return 'cuda' if tensor.is_cuda else 'cpu'\n\n\n# Watch this for explaination on how torchtext works\n# https:\/\/www.youtube.com\/watch?v=KRgq4VnCr7I\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)","a1947e4a":"def mask_(matrices, maskval=0.0, mask_diagonal=True):\n    \"\"\"\n    Masks out all values in the given batch of matrices where i <= j holds,\n    i < j if mask_diagonal is false\n    In place operation\n    :param tns:\n    :return:\n    \"\"\"\n\n    b, h, w = matrices.size()\n\n    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n    matrices[:, indices[0], indices[1]] = maskval","a4eeaabe":"class SelfAttentionNarrow(nn.Module):\n\n    def __init__(self, emb, heads=8, mask=False):\n        \"\"\"\n        :param emb:\n        :param heads:\n        :param mask:\n        \"\"\"\n\n        super().__init__()\n\n        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n\n        self.emb = emb\n        self.heads = heads\n        self.mask = mask\n\n        s = emb \/\/ heads\n        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n\n        self.tokeys    = nn.Linear(s, s, bias=False)\n        self.toqueries = nn.Linear(s, s, bias=False)\n        self.tovalues  = nn.Linear(s, s, bias=False)\n\n        self.unifyheads = nn.Linear(heads * s, emb)\n\n    def forward(self, x):\n\n        b, t, e = x.size()\n        h = self.heads\n        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n\n        s = e \/\/ h\n        x = x.view(b, t, h, s)\n\n        keys    = self.tokeys(x)\n        queries = self.toqueries(x)\n        values  = self.tovalues(x)\n\n        assert keys.size() == (b, t, h, s)\n        assert queries.size() == (b, t, h, s)\n        assert values.size() == (b, t, h, s)\n\n        # Compute scaled dot-product self-attention\n\n        # - fold heads into the batch dimension\n        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n\n        queries = queries \/ (e ** (1\/4))\n        keys    = keys \/ (e ** (1\/4))\n        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n        #   This should be more memory efficient\n\n        # - get dot product of queries and keys, and scale\n        dot = torch.bmm(queries, keys.transpose(1, 2))\n\n        assert dot.size() == (b*h, t, t)\n\n        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n\n        dot = F.softmax(dot, dim=2)\n        # - dot now has row-wise self-attention probabilities\n\n        # apply the self attention to the values\n        out = torch.bmm(dot, values).view(b, h, t, s)\n\n        # swap h, t back, unify heads\n        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n\n        return self.unifyheads(out)","d21265dd":"class TransformerBlock(nn.Module):\n\n    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=False):\n        super().__init__()\n        if wide:\n            raise NotImplementedError\n        self.attention = SelfAttentionNarrow(emb, heads=heads, mask=mask)\n        self.mask = mask\n\n        self.norm1 = nn.LayerNorm(emb)\n        self.norm2 = nn.LayerNorm(emb)\n\n        self.ff = nn.Sequential(\n            nn.Linear(emb, ff_hidden_mult * emb),\n            nn.ReLU(),\n            nn.Linear(ff_hidden_mult * emb, emb)\n        )\n\n        self.do = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        attended = self.attention(x)\n\n        x = self.norm1(attended + x)\n\n        x = self.do(x)\n\n        fedforward = self.ff(x)\n\n        x = self.norm2(fedforward + x)\n\n        x = self.do(x)\n\n        return x","1b898b0a":"class CTransformer(nn.Module):\n    \"\"\"\n    Transformer for classifying sequences\n    \"\"\"\n    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n        \"\"\"\n        :param emb: Embedding dimension\n        :param heads: nr. of attention heads\n        :param depth: Number of transformer blocks\n        :param seq_length: Expected maximum sequence length\n        :param num_tokens: Number of tokens (usually words) in the vocabulary\n        :param num_classes: Number of classes.\n        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n                         average pooling.\n        \"\"\"\n        super().__init__()\n\n        self.num_tokens, self.max_pool = num_tokens, max_pool\n\n        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n\n        tblocks = []\n        for i in range(depth):\n            tblocks.append(\n                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout, wide=wide))\n\n        self.tblocks = nn.Sequential(*tblocks)\n\n        self.toprobs = nn.Linear(emb, num_classes)\n\n        self.do = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: A batch by sequence length integer tensor of token indices.\n        :return: predicted log-probability vectors for each token based on the preceding tokens.\n        \"\"\"\n        tokens = self.token_embedding(x)\n        b, t, e = tokens.size()\n\n        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n        x = tokens + positions\n        x = self.do(x)\n\n        x = self.tblocks(x)\n\n        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n\n        x = self.toprobs(x)\n\n        return F.log_softmax(x, dim=1)","b48bc5e3":"vocab_size = 50000\nbatch_size = 4\n\n\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\n\nTEXT.build_vocab(train, max_size=vocab_size - 2)\nLABEL.build_vocab(train)\n\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size, device=d())\n\n\nprint(f'- nr. of training examples {len(train_iter)}')\nprint(f'- nr. of test examples {len(test_iter)}')","4624a10d":"max_length = 512\nmx = max_length\nembedding_size = 128\nnum_heads = 8\ndepth = 6\nmax_pool = True\nlr = .0001\nlr_warmup = 10000\nnum_epochs = 1\nNUM_CLS = 2\n\n# create the model\nmodel = CTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=mx, num_tokens=vocab_size,\n                            num_classes=NUM_CLS, max_pool=max_pool)\nif torch.cuda.is_available():\n    model.cuda()\n    \n\nopt = torch.optim.Adam(lr=lr, params=model.parameters())\nsch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i \/ (lr_warmup \/ batch_size), 1.0))","7e73fcde":"seen = 0\ngradient_clipping = 1.0\nfor e in range(num_epochs):\n\n    print(f'\\n epoch {e}')\n    model.train(True)\n\n    for batch in tqdm.tqdm(train_iter):\n\n        opt.zero_grad()\n\n        input = batch.text[0]\n        label = batch.label - 1\n\n        if input.size(1) > mx:\n            input = input[:, :mx]\n        out = model(input)\n        loss = F.nll_loss(out, label)\n\n        loss.backward()\n\n        # clip gradients\n        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n        if gradient_clipping > 0.0:\n            nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\n        opt.step()\n        sch.step()\n\n        seen += input.size(0)\n\n    with torch.no_grad():\n\n        model.train(False)\n        tot, cor= 0.0, 0.0\n\n        for batch in test_iter:\n\n            input = batch.text[0]\n            label = batch.label - 1\n\n            if input.size(1) > mx:\n                input = input[:, :mx]\n            out = model(input).argmax(dim=1)\n\n            tot += float(input.size(0))\n            cor += float((label == out).sum().item())\n\n        acc = cor \/ tot\n        print(f'-- {\"test\"} accuracy {acc:.3}')\n","21d365d2":"\n## 1. load the IMDB data\nFor now we will only split train and test","b5262756":"Let's make it interactive:\n\nQ1: What do you understand from this pic?\n\n![att1.png](attachment:att1.png)\n\nA1: I understand that selfattention is the first part of the transformer block\n\nFor now read [this](http:\/\/peterbloem.nl\/blog\/transformers)  I will complete the QA later.","e5286c37":"# Transformer Block\n","b0cd9074":"This is far from complete but a working example of transformers. \nMany parts of it are taken from https:\/\/github.com\/pbloem\/former and http:\/\/peterbloem.nl\/blog\/transformers with the intention of making further modifications and explanations to make it more clear. Upvote to support me.","8667abdc":"# The Classification transformer\nWe will use a transformer to classify reviews as positive or negative \n\n![trans.png](attachment:trans.png)","ae813c92":"# Import libraries","9639e05a":"## 2. Create the model","f391710d":"# Define the mask\nThe values in the upper triangle in the matrix will be set to 0\n![mask.png](attachment:mask.png)","26118c16":"## 3. training loop\n","e239193e":"# Train the classifier","28e8960b":"# Narrow multihead self attention\nThe default multihead self attention is narrow which means :\n> if the embedding vector has 256 dimensions, and we have 8 attention heads, we cut it into 8 chunks of 32 dimensions. For each chunk, we generate keys, values and queries of 32 dimensions each. This means that the matrices \ud835\udc16rq, \ud835\udc16rk,\ud835\udc16rv are all 32\u00d732.\n\n\n"}}