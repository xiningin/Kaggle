{"cell_type":{"9118dd2a":"code","0029dce5":"code","2788af47":"code","96a948e6":"code","64ca85b8":"code","1f078111":"code","1d742704":"code","2a0c2ace":"code","566cf7c1":"code","b470e0a3":"code","9cf0e5aa":"code","347803ef":"code","e21986b3":"markdown","9054b38f":"markdown","4b0e5d91":"markdown","58871ad3":"markdown","f79484b2":"markdown","b6c93f7f":"markdown","f116ca73":"markdown","2fdabd76":"markdown","3b8633ad":"markdown","d924a3a9":"markdown"},"source":{"9118dd2a":"import pandas\ntraining_data = pandas.read_csv('..\/input\/train.csv')\ncivil_comments = training_data[\n    training_data['target'] < 0.5\n]\ntoxic_comments = training_data[\n    training_data['target'] >= 0.5\n]\n\ntoxicity_types = [\n    'severe_toxicity',\n    'obscene',\n    'threat',\n    'insult',\n    'identity_attack',\n    'sexual_explicit']\n\nidentity_types = [\n    # Gender\n    'male', 'female', \n    'transgender', 'other_gender', # Not targets\n    # Sexuality\n    'homosexual_gay_or_lesbian',\n    'heterosexual', 'bisexual', 'other_sexual_orientation', # Not targets\n    # Religion\n    'christian', 'jewish', 'muslim',\n    'hindu', 'buddhist', 'atheist', 'other_religion', # Not targets\n    # Race\/Ethnicity\n    'black', 'white',\n    'asian', 'latino', 'other_race_or_ethnicity', # Not targets\n    # Disability\/Illness\n    'psychiatric_or_mental_illness',\n    'physical_disability', 'other_disability', 'intellectual_or_learning_disability' # Not targets\n]\n\ntarget_identity_types = [\n      # Gender\n    'male', 'female',\n    # Sexuality\n    'homosexual_gay_or_lesbian',\n    # Religion\n    'christian', 'jewish', 'muslim',\n    # Race\/Ethnicity\n    'black', 'white',\n    # Disability\/Illness\n    'psychiatric_or_mental_illness',\n]","0029dce5":"training_data[\n    training_data['id'].isin(\n        [59856, 59859, 59861])\n]","2788af47":"positions = []\nheights = []\nlabels = []\ncolors = []\nfor i, toxicity_type in enumerate(toxicity_types):\n    positions.extend([2*i, 2*i+1])\n    heights.extend([\n        civil_comments[toxicity_type].mean(),\n        toxic_comments[toxicity_type].mean()\n    ])\n    labels.extend([\n        toxicity_type + ' (civil)', \n        toxicity_type + ' (toxic)'\n    ])\n    colors.extend(['C0', 'C1'])\nplt.xticks(positions, labels, rotation='vertical')\nplt.ylim(0)\nplt.bar(positions, heights, color=colors)\nplt.show()","96a948e6":"positions = []\nheights = []\nlabels = []\ncolors = []\nfor i, identity_type in enumerate(target_identity_types):\n    positions.extend([2*i, 2*i+1])\n    heights.extend([\n        civil_comments.dropna()[identity_type].mean(),\n        toxic_comments.dropna()[identity_type].mean()\n    ])\n    labels.extend([\n        identity_type + ' (civil)', \n        identity_type + ' (toxic)'\n    ])\n    colors.extend(['C0', 'C1'])\nplt.xticks(positions, labels, rotation='vertical')\nplt.ylim(0, 0.4)\nplt.bar(positions, heights, color=colors)\nplt.show()","64ca85b8":"positions = []\nheights = []\nlabels = []\ncolors = []\n\nfor i, identity_type in enumerate(set(identity_types)-set(target_identity_types)):\n    positions.extend([2*i, 2*i+1])\n    heights.extend([\n        civil_comments.dropna()[identity_type].mean(),\n        toxic_comments.dropna()[identity_type].mean()\n    ])\n    labels.extend([\n        identity_type + ' (civil)', \n        identity_type + ' (toxic)'\n    ])\n    colors.extend(['C0', 'C1'])\nplt.xticks(positions, labels, rotation='vertical')\nplt.ylim(0, 0.4)\nplt.bar(positions, heights, color=colors)\nplt.figure(figsize=(200, 400))\nplt.show()","1f078111":"print('Civil Comments Character Length:',\n      civil_comments['comment_text'].apply(lambda c: len(c)).mean())\nprint('Toxic Comments Character Length:',\n      toxic_comments['comment_text'].apply(lambda c: len(c)).mean())","1d742704":"print('Civil Comments Word Count:',\n      civil_comments['comment_text'].apply(lambda c: len(c.split(' '))).mean())\nprint('Toxic Comments Word Count:',\n      toxic_comments['comment_text'].apply(lambda c: len(c.split(' '))).mean())","2a0c2ace":"import numpy as np\nprint('Civil Comments Word Length:',\n      civil_comments['comment_text'].apply(lambda c: np.mean([\n          len(word) for word in c.split(' ')])).mean())\nprint('Toxic Comments Word Length:',\n      toxic_comments['comment_text'].apply(lambda c: np.mean([\n          len(word) for word in c.split(' ')])).mean())","566cf7c1":"civil_word_counts = civil_comments['comment_text'].apply(lambda c: len(c.split(' ')))\ntoxic_word_counts = toxic_comments['comment_text'].apply(lambda c: len(c.split(' ')))\n\npunctuation_marks = [\n    '.', ',', ';', ':', '?', '!', '*', \n    '(', ')', '[', ']', '{', '}'\n]\n\nfor punctuation in punctuation_marks:\n    print(f'Civil Comments Number Punctuation ({punctuation}):',\n          (civil_comments['comment_text'].apply(\n              lambda c: c.count(punctuation))\/\n              civil_word_counts).mean()\n        )\n    print(f'Toxic Comments Number Punctuation ({punctuation}):',\n          (toxic_comments['comment_text'].apply(\n              lambda c: c.count(punctuation))\/\n              toxic_word_counts).mean()\n        )","b470e0a3":"def add_noses(emojis):\n    noses = ['^', '<', 'o']\n    emojis_with_noses = []\n    for emoji in emojis:\n        for nose in noses:\n            emojis_with_noses.append(\n                emoji[:1] + nose + emoji[1:])\n    return emojis_with_noses\n\npositive_emojis = (\n    add_noses([':)', '=)', ';)']) +\n    ['<3'] + [':D', '=D', ':3'] +\n    [':P', '=P'] +\n    ['xD', 'XD'] +\n    ['^.^', '^^']\n)\nnegative_emojis = (\n    add_noses([':(', '=(', ':\/', ':|', '=\/', '=|']) +\n    [\":'(\", \"='(\"] +\n    [\"-.-\", 'O.o'])\n\nemojis = positive_emojis + negative_emojis","9cf0e5aa":"def contains_emoji(text, emojis):\n    return any(emoji in text for emoji in emojis)\n\n\nprint('Civil Comments Emoji Frequency (all):',\n      civil_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, emojis))).mean()\n)\nprint('Toxic Comments Emoji Frequency (all):',\n      toxic_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, emojis))).mean()\n)\n\nprint('Civil Comments Emoji Frequency (positive):',\n      civil_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, positive_emojis))).mean()\n)\nprint('Toxic Comments Emoji Frequency (positive):',\n      toxic_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, positive_emojis))).mean()\n)\n\nprint('Civil Comments Emoji Frequency (negative):',\n      civil_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, negative_emojis))).mean()\n)\nprint('Toxic Comments Emoji Frequency (negative):',\n      toxic_comments['comment_text'].apply(\n          lambda c: int(contains_emoji(c, negative_emojis))).mean()\n)","347803ef":"civil_emojis = []\ntoxic_emojis = []\nambigious_emojis = []\n\nfor emoji in emojis:\n    toxic_use_count = toxic_comments['comment_text'].apply(\n        lambda c: int(contains_emoji(c, [emoji]))).sum()\n    toxic_use_frequency = toxic_use_count\/len(toxic_comments)\n    \n    civil_use_count = civil_comments['comment_text'].apply(\n        lambda c: int(contains_emoji(c, [emoji]))).sum()\n    civil_use_frequency = civil_use_count\/len(civil_comments)\n\n    if toxic_use_frequency + civil_use_frequency == 0:\n        continue\n\n    if toxic_use_frequency >= civil_use_frequency:\n        confidence = (toxic_use_frequency*100\/\n            (civil_use_frequency*100 + toxic_use_frequency*100))\n        toxic_emojis.append(\n            (emoji, confidence, toxic_use_count + civil_use_count)\n        )\n    else:\n        confidence = (civil_use_frequency*100\/\n            (civil_use_frequency*100 + toxic_use_frequency*100))\n        civil_emojis.append(\n            (emoji, confidence, toxic_use_count + civil_use_count)\n        )\nprint('Civil emojis', civil_emojis)\nprint('Toxic emojis', toxic_emojis)","e21986b3":"## What identity mentions are indicative of toxicity?\nWith the exception of Christian identity mention in the comments, identity mentions among the target identities are indicative of toxicity.\n\nAmong the non-target identity mentions, there does not appear to be a tendency for Asian identity mentions.","9054b38f":"## Which emojis are civil and which are toxic?","4b0e5d91":"## Is there a difference in how emojis are used in civil and toxic comments?\n* Emojis are used in roughly 1\/2000 comments.\n* Civil comments are ~40% more likely to use emojis\n* Civil comments are more likely to use positive emojis\n* Negative emojis are rare, but are used at roughly the same frequence.","58871ad3":"Toxic comments have poor spelling\/grammar, e.g., comment 59856, 59859, and 59861.\n\nSome of these comments also lack identity annotation. This might be an opportunity for data imputation\/multi-task learning.\n\nThe targets are continuous variables, which may suggest that, e.g., regular cross entropy is not necessarily ideal, but perhaps an L1\/L2 loss. Maybe hinge loss?","f79484b2":"### Do toxic comments tend to contain shorter words?\nYes, but not substantially.","b6c93f7f":"## How do the toxicity type distributions look for civil and toxic comments? How do they differ?\nMost toxic comments contain insults, identity attacks, and obscenity. Civil comments are unlikely to contain any of the toxicity.","f116ca73":"## Do toxic comments use less punctuation than civil comments (per word)?\n* There is a small tendency for civil comments to use dots and semi-colons.\n* There is a small tendency for toxic comments to use more commas.\n* Civil comments are more likely to use question marks, whereas toxic comments are more likely to use exclamation marks. The latter tendency is not quite as strong as the first.\n* Civil comments are more likely to use colons.\n* Toxic comments are more likely to use asterisk, presumably to spell obscene words, e.g., \"sh\\*tty\" as shown previously\n* Civil comments are more likely use brackets.","2fdabd76":"### Do toxic comments contain fewer words?\nYes, but not substantially.","3b8633ad":"### Do toxic comments contain fewer characters?\nYes, but not substantially.","d924a3a9":"## Is there a relationship between comment length and toxicity?"}}