{"cell_type":{"920663cc":"code","2b3d3dc7":"code","30d02323":"code","323b6ef2":"code","477a8249":"code","4aa67a47":"code","7ec08dbb":"code","288cb5ca":"code","3d1ba621":"code","7a56c50b":"code","53a0b1de":"markdown","8b3be454":"markdown","ae1c9924":"markdown","d4534f3f":"markdown","f9c9dae6":"markdown","a2a6eb49":"markdown","8cb9cd50":"markdown","dac98e47":"markdown","50bf6d61":"markdown","f002c326":"markdown","7685d81f":"markdown","1a148acb":"markdown","55bd7e10":"markdown","792b79ae":"markdown"},"source":{"920663cc":"import pandas as pd\nimport numpy as np\nimport math\nfrom sklearn import datasets","2b3d3dc7":"'''SkLearn Datasets is used to load the IRIS DATABASE that includes information about\ndifferent classes of the IRIS plant species. These species are SETOSA, VIRSICOLOR and \nVIRGINICA. The features that are used to classify them are SEPAL_WIDTH, SEPAL_LENGTH,\nPETAL_WIDTH, PETAL_LENGTH.'''\n\n'''All the data like petal length etc are stored in numpy array DATA and the target,\nie : category of species is stored in TARGET. Hence X, Y have made using them.'''\n\ndf = datasets.load_iris()\nX1 = df.data\nY1 = df.target","30d02323":"class TreeNode:\n    def __init__(self, data, output):\n        # data - Represents the feature upon which the decision tree node was split.\n        # data is NULL for leaf node.\n        self.data = data\n        # children of a node are stored as a dicticionary with key being the value of feature upon which the node was split.\n        # and the corresponding value stores the child TreeNode.\n        self.children = {}\n        # output represents the class with current majority at this instance of the decision tree.\n        self.output = output\n        self.index = -1\n        \n    def add_child(self, feature_value, obj):\n        self.children[feature_value] = obj\n    # add_child is used add objects to the children dictionary corresponding to a particular feature.","323b6ef2":"class DecisionTreeClassifier:\n    def __init__(self):\n        # root represents the root node of the data after fitting data.\n        self.root = None\n    \n    def count_unique(self, Y):\n        # This function will take Y (Classes) as input and then return a dictionary with\n        # keys as unique values of Y and corresponding value as its frequency in Y.\n        dicti = {}\n        for i in Y:\n            if i not in dicti:\n                dicti[i]=1\n            else:\n                dicti[i]+=1\n        return dicti\n    \n    def entropy(self, Y):\n        # This function will return the entropy of the node.\n        freq_dicti = self.count_unique(Y)\n        entropy = 0\n        length = len(Y)\n        for i in freq_dicti:\n            prob = freq_dicti[i] \/ length\n            entropy = entropy + ( (-prob) * math.log2(prob) )\n        return entropy\n    \n    def gain_ratio(self, X, Y, feature):\n        # Returns the gain ratio\n        orig_entropy = self.entropy(Y) # orig_entropy represents entropy before splitting\n        new_entropy = 0  # new_entropy represents entropy after splitting upon the selected feature\n        split_info = 0\n        values = set(X[:,feature])\n        df = pd.DataFrame(X)\n        # Adding Y values as the last column in the dataframe \n        df[df.shape[1]] = Y\n        initial_size = df.shape[0] \n        for i in values:\n            df1 = df[df[feature] == i]\n            current_size = df1.shape[0]\n            new_entropy += (current_size\/initial_size)*self.entropy(df1[df1.shape[1]-1])\n            split_info += (-current_size\/initial_size)*math.log2(current_size\/initial_size)\n        # To handle the case when split info = 0 which leads to division by 0 error\n        if split_info == 0 :\n            return math.inf\n        # Otherwise\n        entropy_gain = orig_entropy - new_entropy\n        gain_ratio = entropy_gain \/ split_info\n        return gain_ratio\n    \n    def gini_index(self, Y):\n        # Returns the gini index \n        freq_dicti = self.count_unique(Y)\n        gini_index = 1\n        length = len(Y)\n        for i in freq_dicti:\n            p = freq_dicti[i] \/ length\n            gini_index = gini_index - p**2\n        return gini_index\n    \n    def gini_gain(self, X, Y, feature):\n        # Returns the gini gain\n        gini_orig = self.gini_index(Y)   # gini_orig represents gini index before splitting\n        gini_split_f = 0                 # gini_split_f represents gini index after splitting upon the selected feature\n        values = set(X[:,feature])\n        df = pd.DataFrame(X)\n        # Adding Y values as the last column in the dataframe \n        df[df.shape[1]] = Y\n        initial_size = df.shape[0] \n        for i in values:\n            df1 = df[df[feature] == i]\n            current_size = df1.shape[0]\n            gini_split_f += (current_size\/initial_size)*self.gini_index(df1[df1.shape[1]-1])\n        gini_gain = gini_orig - gini_split_f\n        return gini_gain\n    \n    def decision_tree(self, X, Y, features, level, metric, classes):\n        '''Returns the root of the Decision Tree (which consists of Class TreeNodes) built after fitting the training data.\n        Here Nodes are printed as in PREORDER traversal. Classes represents the different classes present in the classification problem. \n        Metric can take value gain_ratio or gini_index.\n        Level represents depth of the tree.\n        We split a node on a particular feature only once to avoid overlap.'''\n           \n        # If the node consists of only 1 class.\n        if (len(set(Y)) == 1):\n            print(\"Level\", level)\n            output = None\n            for i in classes:\n                if i in Y:\n                    output = i\n                    print(\"Count of\",i,\"=\",len(Y))\n                else:\n                    print(\"Count of\",i,\"=\",0)\n            if metric == \"gain_ratio\":\n                print(\"Current Entropy is =  0.0\")\n            elif metric == \"gini_index\":\n                print(\"Current Gini Index is =  0.0\")\n            print(\"Reached leaf Node\")\n            print()\n            return TreeNode(None,output)\n        \n        # If we have run out of features to split upon.\n        # In this case we will output the class with maximum count. This will be the final result.\n        if len(features) == 0:\n            print(\"Level\",level)\n            freq_dicti = self.count_unique(Y)\n            output = None\n            max_count = -math.inf\n            for i in classes:\n                if i not in freq_dicti:\n                    print(\"Count of\",i,\"=\",0)\n                else :\n                    if freq_dicti[i] > max_count :\n                        output = i\n                        max_count = freq_dicti[i]\n                    print(\"Count of\",i,\"=\",freq_dicti[i])\n\n            if metric == \"gain_ratio\":\n                print(\"Current Entropy  is =\",self.entropy(Y))\n            elif metric == \"gini_index\":\n                print(\"Current Gini Index is =\",self.gini_index(Y))            \n            print(\"Reached leaf Node\")\n            print()\n            return TreeNode(None,output)\n        \n        # Finding the best feature to split upon further.\n        max_gain = -math.inf\n        final_feature = None\n        for f in features :\n            if metric == \"gain_ratio\":\n                current_gain = self.gain_ratio(X,Y,f)\n            elif metric ==\"gini_index\":\n                current_gain = self.gini_gain(X,Y,f)\n            if current_gain > max_gain:\n                max_gain = current_gain\n                final_feature = f\n                \n        print(\"Level\",level)\n        freq_dicti = self.count_unique(Y)\n        output = None\n        max_count = -math.inf\n        \n        # Printin count of all features at that node.\n        for i in classes:\n            if i not in freq_dicti:\n                print(\"Count of\",i,\"=\",0)\n            else :\n                if freq_dicti[i] > max_count :\n                    output = i\n                    max_count = freq_dicti[i]\n                print(\"Count of\",i,\"=\",freq_dicti[i])\n        # Using input metric to determine feature to be split on.    \n        if metric == \"gain_ratio\" :        \n            print(\"Current Entropy is =\",self.entropy(Y))\n            print(\"Splitting on feature  X[\",final_feature,\"] with gain ratio \",max_gain,sep=\"\")\n            print()\n        elif metric == \"gini_index\":\n            print(\"Current Gini Index is =\",self.gini_index(Y))\n            print(\"Splitting on feature  X[\",final_feature,\"] with gini gain \",max_gain,sep=\"\")\n            print()\n            \n        unique_values = set(X[:,final_feature]) # unique_values represents the unique values of the feature selected.\n        df = pd.DataFrame(X)\n        # Adding Y values as the last column in the dataframe\n        df[df.shape[1]] = Y\n        current_node = TreeNode(final_feature,output)\n\n        # Now removing the selected feature from the list as we do not want to split on one feature more \n        # than once(in a given root to leaf node path).\n        index  = features.index(final_feature)\n        features.remove(final_feature)\n        for i in unique_values:\n            # Creating a new dataframe with value of selected feature = i\n            df1 = df[df[final_feature] == i]\n            # Segregating the X and Y values and recursively calling on the splits\n            node = self.decision_tree(df1.iloc[:,0:df1.shape[1]-1].values,df1.iloc[:,df1.shape[1]-1].values,features,level+1,metric,classes)\n            current_node.add_child(i,node)\n        # Add the removed feature     \n        features.insert(index,final_feature)\n        return current_node\n    \n    def fit(self, X, Y, metric=\"gain_ratio\"):\n        # Fits to the given training data.\n        # Metric can take value gain_ratio or gini_index. Default value will be gain_ratio.\n        features = [i for i in range(len(X[0]))]\n        classes = set(Y)\n        level = 0\n        if metric != \"gain_ratio\" :\n            if metric != \"gini_index\":\n                metric=\"gain_ratio\"  # If user entered a value which was neither gini_index nor gain_ratio\n        self.root = self.decision_tree(X,Y,features,level,metric,classes)\n        \n    def predict_helper(self, data, node):\n        # predicts the class for a given testing point and returns the answer.\n        # If we have reached a leaf node :      \n        if len(node.children) == 0 :\n            return node.output\n        val = data[node.data] # represents the value of feature on which the split was made       \n        if val not in node.children :\n            return node.output\n        # Recursively call on the splits.\n        return self.predict_helper(data,node.children[val])\n\n    def predict(self, X):\n        # This function returns Y predicted.\n        # X should be a 2-D np array.\n        Y = np.array([0 for i in range(len(X))])\n        for i in range(len(X)):\n            Y[i] = self.predict_helper(X[i],self.root)\n        return Y\n    \n    def score(self,X,Y):\n        # Returns the mean accuracy of predictions.\n        Y_pred = self.predict(X)\n        count = 0\n        for i in range(len(Y_pred)):\n            if Y_pred[i] == Y[i]:\n                count+=1\n        return count\/len(Y_pred)","477a8249":"clf1 = DecisionTreeClassifier()\nx = np.array([[0,0],\n              [0,1],\n              [1,0],\n              [1,1]])\ny = np.array([0,\n              1,\n              1,\n              1]) \nclf1.fit(x,y)\nY_pred = clf1.predict(x)\nprint(\"Predictions :\",Y_pred)\nprint()\nprint(\"Score :\",clf1.score(x,y)) # Score on training data\nprint()","4aa67a47":"'''USING MY DECISION TREE CLASSIFIER'''\n\nfrom sklearn import datasets\n# Generating a random dataset\nX, Y = datasets.make_classification(n_samples=100, n_features=5, n_classes=3, n_informative=3, random_state=0)\n# To reduce the values a feature can take ,converting floats to int\nfor i in range(len(X)):\n    for j in range(len(X[0])):\n        X[i][j] = int(X[i][j])\n        \nclf2 = DecisionTreeClassifier()\nclf2.fit(X,Y,metric='gini_index')\nY_pred2 = clf2.predict(X)\nprint(\"Predictions : \",Y_pred2)\nprint()\nour_score = clf2.score(X,Y)\nprint(\"Score :\",our_score) # score on training data\nprint()","7ec08dbb":"'''USING SKLEARN INBUILT CLASSIFIER'''\n\nimport sklearn.tree\nclf3 = sklearn.tree.DecisionTreeClassifier()\nclf3.fit(X,Y)\nY_pred3 = clf3.predict(X)\nprint(\"Predictions\",Y_pred3)\nsklearn_score = clf3.score(X,Y)\nprint(\"Score :\",sklearn_score)","288cb5ca":"'''SCORE OF BOTH CLASSFIERS IS SAME. BOTH PERFORM WELL ON TRAINING DATA'''","3d1ba621":"'''USING MY DECISION TREE CLASSIFIER'''\n# Use train_test_split for IRIS Data.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X1, Y1, random_state = 0)\n\n# TRAINING DATA\nclf = DecisionTreeClassifier()\nclf.fit(X_train,Y_train,metric='gini_index')\nY_pred_train = clf.predict(X_train)\nprint(\"Predictions for train data : \",Y_pred_train)\nprint()\ntrain_score = clf.score(X_train,Y_train)\nprint(\"Score :\", train_score)\nprint()\n\n# TEST DATA\nY_pred_test = clf.predict(X_test)\nprint(\"Predictions for test data : \",Y_pred_test)\nprint()\ntest_score = clf.score(X_test,Y_test)\nprint(\"Score :\", test_score)\nprint()","7a56c50b":"'''USING SKLEARN INBUILT CLASSIFIER'''\n\nimport sklearn.tree\nclfsk = sklearn.tree.DecisionTreeClassifier()\nclfsk.fit(X_train,Y_train)\nY_pred_train_sk = clfsk.predict(X_train)\nprint(\"Predictions for train\",Y_pred_train_sk)\nsklearn_score_train = clfsk.score(X_train,Y_train)\nprint(\"Score for training :\",sklearn_score_train)\nY_pred_test_sk = clfsk.predict(X_test)\nprint(\"Predictions for test\",Y_pred_test_sk)\nsklearn_score_test = clfsk.score(X_test,Y_test)\nprint(\"Score for training :\",sklearn_score_test)","53a0b1de":"## --------------------------------------------------------------------------------------------------------","8b3be454":"### Importing Libraries","ae1c9924":"## --------------------------------------------------------------------------------------------------------","d4534f3f":"## --------------------------------------------------------------------------------------------------------","f9c9dae6":"# INDEX","a2a6eb49":"### Testing Implemented Tree on Random Data and comparing with SKLearn","8cb9cd50":"# FINAL IMPLEMENTATION ON IRIS DATABASE","dac98e47":"### Testing Implemented Tree on OR GATE","50bf6d61":"### Loading Database and Preprocessing Data","f002c326":"##### RESULT USING SKLearn - Training Score is 1.0 and testing score is 0.947. \n##### Performance on training data is similar in both. SKlearn performs slightly better than my classifier.","7685d81f":"##### RESULT USING MY CLASSIFIER - Training Score is 1.0 and testing score is 0.815","1a148acb":"## --------------------------------------------------------------------------------------------------------","55bd7e10":"### Implementing Tree","792b79ae":"##### 1. Importing Necessery Libraries.\n##### 2. Loading IRIS DATA and PREPROCESSING.\n##### 3. Implementing Decision Tree Classifier.\n##### 4. Test on OR GATE.\n##### 5. Test on Random Database to compare with SKLearn.\n##### 6. Final Implementation on IRIS DATA.\n##### 7. Using Graph_viz to build Decision Tree for IRIS and SAMPLE DATA and save as PDF."}}