{"cell_type":{"cc8ca80c":"code","b4dc79fe":"code","71b38038":"code","c1747df4":"code","fa9eed99":"code","00d77a38":"code","099e6e83":"code","18663230":"code","5c0240c5":"code","cb1f524d":"code","4e076bd6":"code","5788b616":"code","8595d466":"code","d20566fe":"code","5275296a":"code","fab56e2d":"code","f91c9b33":"markdown","40777f2a":"markdown","aa7d3678":"markdown","3daeec37":"markdown"},"source":{"cc8ca80c":"if False:\n    !pip install -qq selenium\n    !wget -q https:\/\/chromedriver.storage.googleapis.com\/2.45\/chromedriver_linux64.zip -O ..\/working\/chromedriver.zip\n    !cd ..\/working; unzip -q chromedriver.zip\n    chrome_driver = '..\/working\/chromedriver'\n    from selenium import webdriver\n    driver = webdriver.Chrome(chrome_driver)\nelse:\n    import requests\n    class RequestsDriver:\n        def __init__(self):\n            self.page_source=''\n            self.error=False\n        def get(self, url):\n            try:\n                self.page_source = requests.get(url).text\n                self.error = False\n            except Exception as e:\n                self.page_source = 'Page cannot be loaded'\n                self.error = True\n        def quit(self):\n            pass\n    driver = RequestsDriver()","b4dc79fe":"from bs4 import BeautifulSoup\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nfrom six.moves.urllib_parse import urlparse, urljoin\nimport pandas as pd\nimport numpy as np\nfrom bs4.element import Comment\nfrom langdetect import detect\ndef tag_visible(element):\n    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    return True","71b38038":"parent_url = \"https:\/\/www.uzh.ch\/\"\ndomain = urlparse(parent_url).netloc\nparent_domain = 'uzh.ch'","c1747df4":"def crawl_url(driver, url, black_list = None):\n    \"\"\"get contents and url from a page\"\"\"\n    if black_list is None:\n        black_list = []\n    # Once the url is parsed, add it to crawled url list\n    driver.get(url)\n    if driver.error:\n        text_output = driver.page_source\n        title_txt = '404'\n        urls = []\n    else:\n        html = driver.page_source.encode(\"utf-8\")\n        soup = BeautifulSoup(html, 'lxml')\n        # parse text\n        texts = soup.findAll(text=True)\n        visible_texts = filter(tag_visible, texts)  \n        text_output = u\" \".join(t.strip() for t in visible_texts)\n        title_txt = soup.title.string if soup.title is not None else ''\n        \n        urls = soup.findAll(\"a\")\n    \n    url_list = []\n    # Even if the url is not part of the same domain, it is still collected\n    # But those urls not in the same domain are not parsed\n    for a in urls:\n        c_url = a.get(\"href\")\n        if (c_url):\n            frag_parse = urlparse(c_url)\n            if (len(frag_parse.path)>0) or (len(frag_parse.query)>0) or (len(frag_parse.netloc)>0):\n                if len(frag_parse.netloc)==0:\n                    c_url = urljoin(url, c_url)\n                if (c_url not in url_list) and (c_url not in black_list):\n                    url_list.append(c_url)\n    \n    return set(url_list), (title_txt, text_output)","fa9eed99":"links, (page_title, page_text) = crawl_url(driver, 'https:\/\/www.fartyfacybagoon.ixm')\nprint('Links:', len(links))\nprint('Language:', detect(page_text), 'Title:', page_title, 'Content:', page_text[:80])","00d77a38":"links, (page_title, page_text) = crawl_url(driver, parent_url)\nprint('Links:', len(links))\nprint('Language:', detect(page_text), 'Title:', page_title, 'Content:', page_text[:80])","099e6e83":"link_queue = {parent_url}\ncrawled_urls = dict()\nedge_list = set()\nsearch_depth = 0\nMAX_ARTICLES = 5000\nwhile len(link_queue)>0:\n    if len(crawled_urls)>MAX_ARTICLES:\n        break\n    finished_pages = len(crawled_urls)\n    next_links = set()\n    # breadth first crawl\n    for c_link in link_queue:\n        if c_link not in crawled_urls:\n            links, page_text = crawl_url(driver, c_link)\n            crawled_urls[c_link] = page_text\n            if len(crawled_urls)>MAX_ARTICLES:\n                break\n            for r_link in links:\n                edge_list.add((c_link, r_link))\n                if parent_domain in urlparse(r_link).netloc:\n                    next_links.add(r_link)\n    print(finished_pages, ':', len(link_queue), '->',  len(next_links))\n    link_queue = next_links.copy()\n    search_depth += 1","18663230":"# Finally quit the browser\ndriver.quit()\nprint(\"Edges\", len(edge_list))\nprint(\"URLs Crawled\", len(crawled_urls))","5c0240c5":"webtext_df = pd.DataFrame([{'url': a, 'contents': contents, \n               'title': title} \n              for a,(title, contents) in crawled_urls.items()])\ndef _soft_detect(in_text):\n    try:\n        return detect(in_text)\n    except:\n        return ''\nwebtext_df['language'] = webtext_df['contents'].map(_soft_detect)\nwebtext_df['clean_url'] = webtext_df['url'].map(lambda x: x.replace('www.', ''))\nwebtext_df['domain'] = webtext_df['clean_url'].map(lambda x: urlparse(x).netloc)\nwebtext_df['path'] = webtext_df['clean_url'].map(lambda x: urlparse(x).path)\nwebtext_df.to_json('full_crawl.json')\nwebtext_df.sample(3)","cb1f524d":"webtext_df.groupby('language').size().plot.bar()","4e076bd6":"edge_df = pd.DataFrame([{'src': a, 'dst': b} for a, b in edge_list])\nedge_df['src'] = edge_df['src'].map(lambda x: x.replace('www.', ''))\nedge_df['dst'] = edge_df['dst'].map(lambda x: x.replace('www.', ''))\nedge_df['src_parse'] = edge_df['src'].map(urlparse)\nedge_df['dst_parse'] = edge_df['dst'].map(urlparse)\nfor base in ['src', 'dst']:\n    for node in ['netloc', 'path', 'params', 'query']:\n        edge_df[f'{base}_{node}'] = edge_df[f'{base}_parse'].map(lambda x: getattr(x, node))\nedge_df.drop(['src_parse', 'dst_parse'], axis=1).to_csv('edges.csv')\nedge_df.sample(3)","5788b616":"domain_df = pd.DataFrame({'netloc': edge_df['src_netloc'].values.tolist()+\n                          edge_df['dst_netloc'].values.tolist()}).\\\n    groupby('netloc').size().reset_index(name='count').\\\n    sort_values('count', ascending=False)\ndomain_df = domain_df[domain_df.iloc[:, 0].map(len)>0]\ntop_domains_df = domain_df.head(25)\ntop_domains_df","8595d466":"dom_edge_df = edge_df.\\\n    groupby(['src_netloc', 'dst_netloc']).\\\n    size().\\\n    reset_index(name='count').\\\n    sort_values('count', ascending=False)\ngraph_df = pd.merge(\n    pd.merge(dom_edge_df, \n             top_domains_df, \n             left_on='src_netloc', \n             right_on='netloc', \n             suffixes=('', '_src')).drop(['netloc'],axis=1),\n    top_domains_df, \n    left_on='dst_netloc', \n    right_on='netloc', \n    suffixes=('', '_dst')\n).drop(['netloc'],axis=1)\ngraph_df.sample(3)","d20566fe":"import pydot\nimport networkx as nx\ng = nx.MultiDiGraph()\nfor _, c_row in graph_df.iterrows():\n    a_name = c_row.iloc[0]\n    b_name = c_row.iloc[1]\n    g.add_edge(a_name, b_name, weight=c_row.iloc[2])","5275296a":"fig, ax1 = plt.subplots(1, 1, figsize = (20, 15))\nax1.axis('off')\n#pos = nx.nx_agraph.graphviz_layout(g)\npos = nx.spring_layout(g, iterations=20)\npos = nx.kamada_kawai_layout(g)\n#pos = nx.circular_layout(g)\n#pos = nx.spectral_layout(g)\nnx.draw_networkx_edges(g, \n                       pos, \n                       alpha=0.3, \n                       ax=ax1, \n                       edge_color='g',\n                      width = np.log2(graph_df.iloc[:, 2].values))\nnx.draw_networkx_nodes(g, \n                       pos, \n                       node_color='r', \n                       alpha=0.4, \n                       ax=ax1)\n#nx.draw_networkx_edges(g, pos, alpha=0.4, node_size=0, width=1, edge_color='k', ax=ax1)\nnx.draw_networkx_labels(g, pos, fontsize=14, ax=ax1);","fab56e2d":"d = nx.drawing.nx_pydot.to_pydot(g)\nImage(d.create_png())","f91c9b33":"## Extract edges","40777f2a":"# Visualize Graphs\nWe can visualize the connenctivity between different pages and institutes","aa7d3678":"# Export Results\nHere we export the results to text files to process later","3daeec37":"# Take the top 25 domains"}}