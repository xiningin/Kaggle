{"cell_type":{"359acb6a":"code","114272ba":"code","036cfc14":"code","8aba992d":"code","b50a4f1d":"code","48d58302":"code","9d587fe1":"code","44854da5":"code","fcdade6a":"code","051c9ad6":"code","b23b65d8":"code","6c0a35d6":"code","8397bf34":"code","db0c21fb":"code","a7506268":"code","c9ad5fec":"code","dcfa36a3":"code","6a843783":"code","9e916448":"code","f3f414a4":"code","1b5c7638":"code","d4e9ce90":"code","9aa41636":"code","0bbe8abb":"code","fb74d4bf":"code","59b6a1df":"code","bf702646":"code","f8d7de40":"code","787068cc":"code","3cdd683d":"code","7770053d":"code","37250dfb":"code","4daff9d6":"code","91f762a9":"code","dbf3283a":"code","af3d2616":"code","23cc46fe":"code","129ee6c0":"code","f6d5b1e0":"code","c12afb66":"code","a15cfa96":"code","f4540ccd":"code","877ed394":"code","802f4fff":"code","14eb90cc":"code","9a759db6":"code","5581efa5":"code","8bcbc916":"code","b5af633f":"markdown","b881e338":"markdown","9dcadbb8":"markdown","6c705073":"markdown","3a5a9ccf":"markdown","c67749c6":"markdown","47e79d11":"markdown","e0cbd6e9":"markdown","2adc3e47":"markdown","02b25144":"markdown","35a92894":"markdown","f62c5712":"markdown","716c18fd":"markdown","e0c840cd":"markdown","32365383":"markdown","641ae26b":"markdown","96aaea9e":"markdown","e27152cb":"markdown","fbf64f1e":"markdown","5299c60e":"markdown","239e5088":"markdown","d539d5a9":"markdown","f38eecb6":"markdown"},"source":{"359acb6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport pickle\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import AgglomerativeClustering\nimport Levenshtein\nimport xgboost as xgb\nfrom scipy.spatial.distance import squareform\nfrom scipy.spatial.distance import pdist\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","114272ba":"# gc.set_debug(gc.DEBUG_LEAK)","036cfc14":"def import_data():\n    \"\"\"Import all data from csv files\"\"\"\n    sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n    item_cat = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n    items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n    # sub_sample = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\n    shops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\n    test = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\n    return sales, item_cat, items, shops, test\n\n\ndef downcast_dtypes(df):\n    \"\"\"Downcast float columns to float32 and int columns to int16\"\"\"\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","8aba992d":"SALES, ITEM_CAT, ITEMS, SHOPS, TEST = import_data()","b50a4f1d":"def pre_process_item_data(df):\n    \"\"\"create new item id 'item_lab' to account for duplicates. Of 22k items there are roughly\n    100 duplicates\"\"\"\n    df['name_pre'] = df.item_name.str.replace(r'[\\*\\!\\.\/,]', '')\n    df['name_pre'] = df.name_pre.str.lower()\n    # df['name_pre'] = df.name_pre.str.replace(r'\\((.*?)\\)', '')  # delete all () brackets\n    # df['name_pre'] = df.name_pre.str.replace(r'\\[(.*?)\\]', '')  # delete all [] brackets\n    df['name_pre'] = df.name_pre.str.replace(r'd$', '').str.strip()\n    df['item_lab'] = df.name_pre.factorize(sort=True)[0]","48d58302":"pre_process_item_data(ITEMS)\nSALES = SALES.merge(ITEMS[['item_id', 'item_category_id', 'item_lab']], how='left', on='item_id')\n# SALES.head()","9d587fe1":"# drop all thats not needed for aggr, merge again later\nSALES.drop(['item_id', 'date', 'item_category_id'], axis=1, inplace=True)","44854da5":"def clip_fillna_prices(df):\n    # clip item prices with 99 percentile\n    quantile_99 = df.item_price.quantile(0.99)\n    df.item_price.clip(upper=quantile_99, inplace=True)\n\n    # Fix single item price == -1 value with mean of same month, shop and item\n    idx = df[df.item_price < 0].index.tolist()\n    df.at[idx[0], 'item_price'] = (2499. + 1249.) \/ 2.\n\n\ndef aggregate_item_count(df):\n    # Aggregate item count to sum per month and item price to mean\n    train = df.groupby(['date_block_num', 'shop_id', 'item_lab']).agg(\n        item_price=('item_price', 'mean'), item_cnt_month=('item_cnt_day', 'sum')\n    ).reset_index()\n    \n    # clip item_cnt_month to min\/max in test set\n    train.item_cnt_month.clip(lower=0, upper=20, inplace=True)\n    \n    return train\n\n\ndef add_missing_item_shop_pairs(df, price):\n    # set item count for missing shop \/ item pairs per month to zero\n    df.set_index(['date_block_num', 'shop_id', 'item_lab'], inplace=True)\n    idx = []\n    for month in df.index.unique('date_block_num'):\n        shops_unique = df.loc[month].index.unique('shop_id')\n        items_unique = df.loc[month].index.unique('item_lab')\n        idx.append(pd.MultiIndex.from_product([[month], shops_unique, items_unique], names=['date_block_num', 'shop_id', 'item_lab']))\n\n    idx = idx[0].append(idx[1:])\n    df = df.reindex(idx, fill_value=0.)\n\n    # fill missing item prices with mean\n    df = df.reset_index()\n    \n    df = pd.merge(df, price, how='left', on='item_lab', suffixes=('', '_'))\n    df['item_price'] = np.where(df.item_price > 0, df.item_price, df.item_price_)\n    del df['item_price_']\n    return df","fcdade6a":"clip_fillna_prices(SALES)\nPRICE = SALES[['item_lab', 'item_price']][SALES.item_price > 0].groupby('item_lab').agg('mean')\nMATRIX = aggregate_item_count(SALES)\nMATRIX = add_missing_item_shop_pairs(MATRIX, PRICE)\n\nMATRIX = MATRIX.merge(ITEMS[['item_category_id', 'item_lab']].drop_duplicates(), how='left', on='item_lab')","051c9ad6":"# MATRIX.head()","b23b65d8":"TEST['date_block_num'] = 34\nTEST.drop('ID', axis=1, inplace=True)\nTEST['item_cnt_month'] = np.nan\n\nTEST = TEST.merge(ITEMS[['item_id', 'item_lab', 'item_category_id']], how='left', on='item_id')\nTEST = pd.merge(TEST.drop('item_id', axis=1), PRICE, how='left', on='item_lab')\n\nMATRIX = MATRIX.append(TEST, ignore_index=True)\nMATRIX['item_category_id'] = MATRIX.item_category_id.astype('int16')\n# MATRIX.head()","6c0a35d6":"def fillna_means(df, col):\n    \"\"\"Fill with means of item_lab, if not available, use means of category\"\"\"\n    \n    # df[col] = df.groupby('item_id')[col].transform(lambda x: x.fillna(x.mean()))\n    df[col] = df.groupby('item_lab')[col].transform(lambda x: x.fillna(x.mean()))\n    df[col] = df.groupby('item_category_id')[col].transform(lambda x: x.fillna(x.mean()))","8397bf34":"fillna_means(MATRIX, 'item_price')","db0c21fb":"def pre_process_item_cat_data(df):\n    \"\"\"Do pre processing of item_cat data frame\"\"\"\n    \n    # split name at '-' to seperate to categories and sub-categories\n    cat = df.item_category_name.str.split('-', n=1, expand=True)\n    df['cat1'] = cat[0].str.strip().str.lower()\n    df['cat2'] = cat[1].str.strip().str.lower()\n\n    df.cat1.fillna('', inplace=True)\n    df.cat2.fillna('', inplace=True)\n    \n    df['cat1_lab'] = df.cat1.factorize(sort=True)[0]\n    df['cat2_lab'] = df.cat2.factorize(sort=True)[0]","a7506268":"def get_string_correlation(array):\n    def dist(x, y):\n        lev = Levenshtein.distance(x[0],y[0])\n        m = np.mean([len(x[0]), len(y[0])])\n        return lev \/ m\n    cor = pdist(array.reshape(-1, 1), dist)\n    return squareform(cor)\n\n\ndef agglomerative_clustering(distance_matrix, num_clusters=None, threshold=None):\n    model = AgglomerativeClustering(n_clusters=num_clusters, affinity='precomputed', linkage='average', distance_threshold=threshold)\n    model.fit(distance_matrix)\n    return model.labels_\n\n\ndef add_levenshtein_feature(df, cols):\n    for col in cols:\n        categories = df[col].unique()\n        corr = get_string_correlation(categories)\n        labels = agglomerative_clustering(corr, threshold=0.5)\n        features = pd.DataFrame(np.array([categories, labels]).T, columns=[col, col + '_lev'])\n        df = df.merge(features, how='left', on=col)\n        df[col + '_lev'] = df[col + '_lev'].astype('int32')\n    return df","c9ad5fec":"pre_process_item_cat_data(ITEM_CAT)\nITEM_CAT = add_levenshtein_feature(ITEM_CAT, ['cat1', 'cat2'])\nMATRIX = MATRIX.merge(ITEM_CAT[['item_category_id', 'cat1_lab', 'cat2_lab', 'cat1_lev', 'cat2_lev']], how='left',\n                      on='item_category_id')\n","dcfa36a3":"# MATRIX.head()","6a843783":"def pre_process_shop_data(df):\n    split = df.shop_name.str.split(n=1, expand=True)\n    df['city'] = split[0].str.lower().str.strip()\n    df['city_id'] = df.city.factorize(sort=True)[0]\n    df['shopping_center'] = split[1].str.lower().str.contains('\u0422\u0426'.lower())\n    ","9e916448":"pre_process_shop_data(SHOPS)\nMATRIX = MATRIX.merge(SHOPS[['shop_id', 'city_id', 'shopping_center']], how='left', on='shop_id')","f3f414a4":"def add_shop_revenue(df):\n    # tmp = df.loc[:, ['shop_id', 'date_block_num', 'item_price', 'item_cnt_month']]\n    df['revenue'] = df.item_price * df.item_cnt_month\n    df['shop_revenue_month'] = df.groupby(['shop_id', 'date_block_num'])['revenue'].transform('sum')\n    del df['revenue']\n\n# Delete this feature later on, only use lag feature (since revenue will not be available for test set)\n    ","1b5c7638":"add_shop_revenue(MATRIX)\n# MATRIX.head()","d4e9ce90":"def lag_feature(df, lags, col):\n    \"\"\"Add lag features based on date_block_num to test and train df\"\"\"\n    tmp = df[['date_block_num', 'shop_id', 'item_lab', col]]\n    \n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num', 'shop_id', 'item_lab', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        \n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_lab'], how='left')\n        df[col+'_lag_'+str(i)].fillna(0, inplace=True)\n    \n    return df","9aa41636":"MATRIX = lag_feature(MATRIX, (1, 2, 12), 'item_cnt_month')\nMATRIX = lag_feature(MATRIX, (1, 2, 12), 'shop_revenue_month')\ndel MATRIX['shop_revenue_month']\n# MATRIX.head()","0bbe8abb":"MATRIX['month'] = MATRIX.date_block_num.mod(12)\n# MATRIX.head()","fb74d4bf":"MATRIX['release'] = MATRIX.item_lab.map(\n    MATRIX[MATRIX.item_cnt_month > 0].groupby(['item_lab']).agg(release=('date_block_num', 'min')).loc[:,'release']\n)\nMATRIX['since_release'] = MATRIX.date_block_num - MATRIX.release\n\n# substitute nans with 0 (since_release == nan -> release == nan -> item not seen before)\nMATRIX['since_release'] = MATRIX.since_release.fillna(0)\n\n# delete all entries where since_release is negative (item not released yet)\n# those entries stem from the added item\/shop pairs with item_cnt_month == 0\nMATRIX = MATRIX[MATRIX.since_release >= 0]\ndel MATRIX['release']\n","59b6a1df":"MATRIX['mean_item_cnt_month'] = MATRIX.groupby('date_block_num')['item_cnt_month'].transform('mean')\nMATRIX[MATRIX.item_cnt_month == 34].mean_item_cnt_month = 0.28394\n# MATRIX.head()","bf702646":"IDX_TRAIN = (MATRIX.date_block_num > 11) & (MATRIX.date_block_num < 33)\nIDX_VAL = MATRIX.date_block_num == 33\nIDX_TEST = MATRIX.date_block_num == 34","f8d7de40":"def add_max_cnt(df):\n    max_cnt = df.loc[IDX_TRAIN, ['item_lab', 'shop_id', 'item_cnt_month']].groupby(['item_lab', 'shop_id']).agg(\n        max_cnt=('item_cnt_month', 'max')).reset_index()\n    df = df.merge(max_cnt, how='left', on=['item_lab', 'shop_id'])\n    return df","787068cc":"MATRIX = add_max_cnt(MATRIX)\nfillna_means(MATRIX, 'max_cnt')\n\nIDX_TRAIN = (MATRIX.date_block_num > 11) & (MATRIX.date_block_num < 33)\nIDX_VAL = MATRIX.date_block_num == 33\nIDX_TEST = MATRIX.date_block_num == 34","3cdd683d":"MATRIX.shape","7770053d":"MATRIX.loc[IDX_TRAIN, 'max_cnt'].shape","37250dfb":"def get_mean_encodings_kfold(df, idx_train, idx_val, idx_test, feature_names, target_name, folds=5):\n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True)\n    global_mean = df.loc[idx_train, target_name].mean()\n    \n    for feature_name in feature_names:\n        print(feature_name)\n        \n        df.loc[:, feature_name + '_mean'] = np.nan\n        data = df.loc[idx_train, [feature_name, feature_name + '_mean', target_name]]\n\n        for idx_1, idx_2 in skf.split(data[[feature_name]], data[feature_name]):\n            # use means from set 1 for mean encoding of set 2\n            x_1 = data.iloc[idx_1].loc[:, [feature_name, feature_name + '_mean', target_name]]\n            x_2 = data.iloc[idx_2].loc[:, [feature_name, feature_name + '_mean', target_name]]\n            means = x_1.groupby(feature_name).agg(mean_target=(target_name, 'mean')).loc[:, 'mean_target']\n            x_2[feature_name + '_mean'] = x_2[feature_name].map(means)\n            data.update(x_2)\n\n        data[feature_name + '_mean'].fillna(global_mean, inplace=True)\n        df.update(data)\n    \n    # use means of complete training set for mean encoding of both validation and test set\n    for feature_name in feature_names:\n        x_1 = df.loc[idx_train, [feature_name, target_name]]\n        x_2 = df.loc[np.logical_or(idx_val, idx_test), [feature_name, target_name]]\n        means = x_1.groupby(feature_name).agg(mean_target=(target_name, 'mean')).loc[:, 'mean_target']\n        x_2[feature_name + '_mean'] = x_2[feature_name].map(means)\n        df.update(x_2)\n    \n    return df","4daff9d6":"MATRIX = get_mean_encodings_kfold(\n    MATRIX, IDX_TRAIN, IDX_VAL, IDX_TEST,\n    ['shop_id', 'city_id', 'item_lab', 'cat1_lab', 'cat2_lab', 'cat1_lev', 'cat2_lev', 'month'],\n    'item_cnt_month')\n\n# MATRIX[IDX_TRAIN].head()","91f762a9":"MATRIX = downcast_dtypes(MATRIX)\nMATRIX.info()","dbf3283a":"TRAIN_X = MATRIX[IDX_TRAIN].drop('item_cnt_month', axis=1)\nTRAIN_Y = MATRIX[IDX_TRAIN].item_cnt_month\nVAL_X = MATRIX[IDX_VAL].drop('item_cnt_month', axis=1)\nVAL_Y = MATRIX[IDX_VAL].item_cnt_month\nTEST_X = MATRIX[IDX_TEST].drop('item_cnt_month', axis=1)","af3d2616":"del SALES, ITEM_CAT, ITEMS, SHOPS, TEST","23cc46fe":"gc.collect()","129ee6c0":"\n# fit xgb regressor\nregressor = xgb.XGBRegressor(n_estimators = 100,\n                             learning_rate = 0.08,\n                             max_depth = 5,\n                             subsample = 0.8,\n                             colsample_bytree = 0.8,\n                             n_jobs = 8\n                            )\nreg = regressor.fit(\n    X=TRAIN_X,\n    y=TRAIN_Y,\n    eval_metric='rmse',\n    eval_set=[(TRAIN_X, TRAIN_Y), (VAL_X, VAL_Y)],\n    verbose=True,\n    early_stopping_rounds=8\n)","f6d5b1e0":"pickle.dump(reg, open(\"xgb.pickle.dat\", \"wb\"))","c12afb66":"plt.plot(reg.evals_result()['validation_0']['rmse'], label='validation_0')\nplt.plot(reg.evals_result()['validation_1']['rmse'], label='validation_1')\nplt.show()","a15cfa96":"importance = pd.Series(reg.feature_importances_, index=TRAIN_X.columns)","f4540ccd":"importance.sort_values(ascending=False)","877ed394":"TEST = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nPRED = TEST_X[['item_lab', 'shop_id']]\nPRED['item_cnt_month'] = reg.predict(TEST_X)\nPRED.drop_duplicates(inplace=True)\nprint(TEST.shape, PRED.shape)","802f4fff":"ITEMS = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\npre_process_item_data(ITEMS)\nTEST = TEST.merge(ITEMS[['item_id', 'item_lab']], how='left', on='item_id')\nprint(TEST.shape)","14eb90cc":"TEST = TEST.merge(PRED, how='left', on=['item_lab', 'shop_id'])\n# TEST.drop_duplicates(ignore_index=True, inplace=True)\nTEST.shape","9a759db6":"TEST.item_cnt_month.mean()","5581efa5":"TEST.item_cnt_month.clip(lower=0, upper=20, inplace=True)","8bcbc916":"TEST[['ID', 'item_cnt_month']].sort_values('ID').to_csv('submission.csv', index=False, header=True)","b5af633f":"Fill missing item prices with mean values","b881e338":"Use XGBRegressor for predictions. Hyperparameters were slowly adapted over several runs:\n\n* n_estimators: started with 50, increased to 100\n* max_depth: started with 3, increased to 5\n* learning_rate: started with 0.1, decreased to 0.08","9dcadbb8":"Max count of item in whole data set","6c705073":"Check importance of features","3a5a9ccf":"Add target mean as feature. To get the correct value for the test set, use leaderbord probing:\n(https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/79142)\n\n* N=214200\n* Predict 0: mse_0 = 1.25011 ** 2\n* Predict 1: mse_1 = 1.41241 ** 2\n\nsum_y_test = (mse_1 - mse_0 - 1) \/ -2\n\nmean_y_test = sum_y_true \/ N\n\nmean_y_test = 0.28394","c67749c6":"Do some preprocessing of item category data. Category name is split into category 1 and 2 names.","47e79d11":"Preproessing of shop data. Features city_id and shopping_center are created.","e0cbd6e9":"Take into account, that some items appear to be the same despite of having slightly different names. Special characters are deleted from the item names and item_id is replaced by item_lab.","2adc3e47":"# Add some advanced features","02b25144":"Define train, validation and test matrices and target vectors.","35a92894":"The shop revenue per month is added. Since the shop revenue will not be able for the 34th month, only a shop revenue lag feature will be used","f62c5712":"Include test set into matrix, so that feature generation will be consistent over train and test set.","716c18fd":"Split data into train, validation and test set. Since the 12 month lag features are only available starting in date_block_num == 12, the previous data is not used for training.\nValidation is done with month number 33, the test set is month number 34.","e0c840cd":"Deal with missing values, aggregate the item count per day to item count per month and add zeros for missing shop \/ item pairs. \n\nIn the test set are 5100 items * 42 shops = 214200 pairs which suggests, that entries with item_cnt_month == 0 are also included. The train set however does not contain any zeros.","32365383":"Add lag features for 1, 2 and 12 months.","641ae26b":"Plot RMSE for train and validation set","96aaea9e":"Add month","e27152cb":"Time since item was first released","fbf64f1e":"Create submission file. Some merging needs to be done since the item_id was changed to item_lab.","5299c60e":"Add mean encoding with K-Fold","239e5088":"Using Levenshteining, the category names are clustered into groups with similar category names. The new features cat1_lev and cat2_lev are created. They contain labels of the clusters.","d539d5a9":"Downcast types to save some RAM.","f38eecb6":"Save model"}}