{"cell_type":{"968e1afb":"code","2dface33":"code","5cc9b57e":"code","58489c10":"code","74665dce":"code","74c8490c":"code","d61d6c57":"code","93e1c5bd":"code","4983f1d7":"code","db787967":"code","bc5fef9e":"code","ed5028fe":"code","7945620b":"code","7e46574e":"code","b7645bac":"code","30c2d95a":"code","bb28e564":"markdown","0495cd3b":"markdown","fa8e6b2d":"markdown","3332cf3c":"markdown","04192726":"markdown","c202625d":"markdown","19f5a574":"markdown","be185f78":"markdown","bea90129":"markdown","8638ca29":"markdown","b2d0c398":"markdown","445af421":"markdown","6b192573":"markdown","a2e3c007":"markdown"},"source":{"968e1afb":"!pip install -U tensorflow==2.6","2dface33":"import tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Dropout, Conv2D, Conv2DTranspose, MaxPool2D, LeakyReLU, Flatten\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.datasets import mnist\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nfrom tqdm import tqdm","5cc9b57e":"tf.__version__","58489c10":"(train_images,_),(_,_) = mnist.load_data()","74665dce":"train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\ntrain_images = (train_images - 127.5) \/ 127.5","74c8490c":"plt.figure(figsize=(10,10))\r\nfor i in range(25):\r\n    plt.subplot(5,5,i+1)\r\n    plt.xticks([])\r\n    plt.yticks([])\r\n    plt.grid(False)\r\n    plt.imshow(train_images[i].squeeze(), cmap=plt.cm.binary)\r\nplt.show()","d61d6c57":"BUFFER_SIZE = 60000\r\nBATCH_SIZE = 256","93e1c5bd":"train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","4983f1d7":"def create_generator():\r\n\r\n    generator = Sequential()\r\n    generator.add(Dense(7*7*256,input_shape=(100,),use_bias=False))\r\n    generator.add(BatchNormalization())\r\n    generator.add(LeakyReLU())\r\n\r\n    generator.add(Reshape((7,7,256)))\r\n    generator.add(Conv2DTranspose(128,5,1,padding='same',use_bias=False))\r\n    generator.add(BatchNormalization())\r\n    generator.add(LeakyReLU())\r\n\r\n    generator.add(Conv2DTranspose(64,5,2,padding='same',use_bias=False))\r\n    generator.add(BatchNormalization())\r\n    generator.add(LeakyReLU())\r\n\r\n    generator.add(Conv2DTranspose(1,5,2,padding='same',activation='tanh',use_bias=False))\r\n\r\n    return generator\r\n","db787967":"def create_discriminator():\r\n    discriminator = Sequential()\r\n    discriminator.add(Conv2D(64,4,2,padding='same',input_shape=(28,28,1)))\r\n    discriminator.add(LeakyReLU())\r\n    discriminator.add(Dropout(0.3))\r\n\r\n    discriminator.add(Conv2D(128,4,2,padding='same'))\r\n    discriminator.add(LeakyReLU())\r\n    discriminator.add(Dropout(0.3))\r\n\r\n    discriminator.add(Flatten())\r\n    \r\n    discriminator.add(Dense(1))\r\n\r\n    return discriminator","bc5fef9e":"class GAN():\r\n    def __init__(self):\r\n        self.generator = create_generator()\r\n        self.discriminator = create_discriminator()\r\n        self.loss = keras.losses.BinaryCrossentropy(from_logits=True)\r\n        self.noise_dim = 100\r\n        self.generator_optimizer = tf.keras.optimizers.Adam(0.0002,0.5)\r\n        self.discriminator_optimizer = tf.keras.optimizers.Adam(0.0002,0.5)\r\n\r\n    def discriminator_loss(self,real_outputs,fake_outputs):\r\n        real_loss = self.loss(tf.ones_like(real_outputs),real_outputs)\r\n        fake_loss = self.loss(tf.zeros_like(fake_outputs),fake_outputs)\r\n        total_loss = real_loss + fake_loss\r\n        return total_loss\r\n\r\n    def generator_loss(self,fake_outputs):\r\n        return self.loss(tf.ones_like(fake_outputs),fake_outputs)\r\n\r\n    \r\n    @tf.function\r\n    def train_step(self,images):\r\n        noise = tf.random.normal([BATCH_SIZE,self.noise_dim])\r\n    \r\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n            generated_images = self.generator(noise,training=True)\r\n\r\n            real_outputs = self.discriminator(images,training=True)\r\n            fake_outputs = self.discriminator(generated_images,training=True)\r\n\r\n            gen_loss = self.generator_loss(fake_outputs)\r\n            disc_loss = self.discriminator_loss(real_outputs,fake_outputs)\r\n\r\n        generator_gradients = gen_tape.gradient(gen_loss,self.generator.trainable_variables)\r\n        discriminator_gradients = disc_tape.gradient(disc_loss,self.discriminator.trainable_variables)\r\n\r\n        self.generator_optimizer.apply_gradients(zip(generator_gradients,self.generator.trainable_variables))\r\n        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients,self.discriminator.trainable_variables))\r\n\r\n        return gen_loss, disc_loss\r\n\r\n\r\n    def fit(self,dataset,epochs):\r\n        history = {'gen_loss':[],'disc_loss':[]}\r\n        for epoch in range(epochs):\r\n            print(f\"Epoch:{epoch+1} of {epochs} epochs\")\r\n            for batch in tqdm(dataset):\r\n                gen_loss, disc_loss = self.train_step(batch)\r\n            print(f\"Generator loss: {gen_loss},\\ndiscriminator_loss: {disc_loss}\")\r\n            history['gen_loss'].append(gen_loss)\r\n            history['disc_loss'].append(disc_loss)\r\n\r\n        return history","ed5028fe":"!nvidia-smi\r#  physical_devices = tf.config.list_physical_devices(\"GPU\")# # tf.config.experimental.set_memory_growth(physical_devices[0], True)","7945620b":"gan = GAN()\r\nhistory = gan.fit(train_dataset,25)","7e46574e":"x = [i for i in range(25)]\r\nplt.plot(x,history['gen_loss'])\r\nplt.plot(x,history['disc_loss'])\r\nplt.title('change in loss over epochs')\r\nplt.legend(['gen_loss','dis_loss'])\r\nplt.xlabel('epochs')\r\nplt.ylabel('loss')\r\nplt.show()","b7645bac":"noise = np.random.randn(32,100)\r\npred = gan.generator.predict(noise)","30c2d95a":"plt.figure(figsize=(10,10))\r\nfor i in range(25):\r\n    plt.subplot(5,5,i+1)\r\n    plt.xticks([])\r\n    plt.yticks([])\r\n    plt.grid(False)\r\n    plt.imshow(pred[i].squeeze(), cmap=plt.cm.binary)\r\nplt.show()","bb28e564":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">Quick introduction into GANS<\/h1>\r\n\r\n<p style=\"text-align:center\">In this notebook we will cover the basics of GANS. We will use tensorflow 2.6.0<\/p>","0495cd3b":"# CHECKING TENSORFLOW VERSION","fa8e6b2d":"# PLOTTING THE RESULTS OF OUR GENERATOR","3332cf3c":"# VISUALISING OUR DATASET","04192726":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">What are GANS?<\/h1>\r\n\r\n<p style=\"text-align:center\">GAN (General advesarial network) is a neural network model that is used for generating new unseen data from a latent dimention.<br>It consist of two separate networks that we call generator and discriminator. The generators job is to generate new unseen data (hence the name) so that <br>the discriminator could not decide if the generated data sample is a real sample or generated one. The discriminators job is to tell us is the passed<br>\r\n data sample generated or not. This whole process we can describe as one big min-max game between these two networks.<\/p>\r\n\r\n<div style=\"text-align:center\">\r\n <img src=\"https:\/\/bs-uploads.toptal.io\/blackfish-uploads\/uploaded_file\/file\/191419\/image-1582299090634-df900b01178a9a4c10f96028ae03f354.png\"\r\n style=\"width:900px;height:500px\"\/>\r\n<\/div>","c202625d":"\r\n\r\n<div style=\"text-align:center\"><h1 style=\"text-align:center;background-color:#4313c9;color:white\">Loss functions for discriminator and generator<\/h1><br>\r\n<img src=\"https:\/\/i.stack.imgur.com\/QAtuJ.png\"\/><\/div>","19f5a574":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">Summary<\/h1>\r\n\r\n<p style=\"text-align:center\">We can see that the generated pictures are not perfect but we trained the network for only 25 epochs. Also GANS are highly<br> sensitive to hyperparameter changes so we could play with those to get better results.<br>This notebook was intended to simply explain gans and because i wanted to learn more about them (and what better way exist than explaining it to other people).<br>The next notebook i will post is about image colorization with gans where our generator model will be a modified image segmentation network (unet)<br><br>\r\n<h1 style=\"text-align:center\">I hope you liked this notebook, if you have any questions or suggestions feel free to comment!<\/h1>\r\n<\/p>","be185f78":"# IMPORTS","bea90129":"# NETWORK TRAINING","8638ca29":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">GAN class<\/h1>\r\n\r\n<p style=\"text-align:center\">This class is used to group all of our functions in one object. We will have our custom training loop where i added tqdm so the<br>output of our training is more transparent. Also training function is called fit (i wanted it to be simmilar to keras api) and it returns history of network training (losses of generator and discriminator)<\/p>","b2d0c398":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">Generator<\/h1>\r\n\r\n<p style=\"text-align:center\">As stated above, the generator should create a new data sample from a latent space. We will use Conv2DTranspose layers for<br> upsampling with 2X2 strides (This will increase the dimention of our data sample by two). At the end we will have a 28X28 image with one color channel<br><\/p>","445af421":"# HISTORY PLOTTING","6b192573":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">Discriminator<\/h1>\r\n\r\n<p style=\"text-align:center\">The discriminator will be a convolutional network for binary classification (Because we only need to check if the input image is a generated or real one)<\/p>","a2e3c007":"<h1 style=\"text-align:center;background-color:#4313c9;color:white\">Data loading<\/h1>\r\n\r\n<p style=\"text-align:center\">For this example we will use mnist dataset, and we will rescale pixel values from 0 - 255 to -1 - 1<\/p>"}}