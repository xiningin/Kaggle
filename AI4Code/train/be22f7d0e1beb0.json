{"cell_type":{"39f912db":"code","18f1f117":"code","819ab915":"code","7a6e6f4f":"code","8af362d5":"code","a99587fa":"code","a0292ee6":"code","ba779962":"code","11a87b91":"code","5a5421ed":"code","1f3728ed":"code","f86dd089":"code","3250dc24":"code","5e8b7175":"code","d806c633":"markdown","c8158f95":"markdown"},"source":{"39f912db":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n#!pip install -U --no-build-isolation --no-deps ..\/input\/transformers-master\/\n","18f1f117":"import sys\nsys.path.append(\"..\/input\/tez-lib\/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","819ab915":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n                {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n                }\n            )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out[0]\n        logits = self.output(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","7a6e6f4f":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }","8af362d5":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","a99587fa":"def postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions","a0292ee6":"#arch = '..\/input\/muril-large-pt\/muril-large-cased' \narch = \"..\/input\/xlmrob\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(arch)","ba779962":"pad_on_right = tokenizer.padding_side == \"right\"\nmax_length = 400 #384 #400\ndoc_stride = 135  #128\n\ntest_data = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\n","11a87b91":"\"\"\"\ntest_tamil = test_data[test_data['language'] == 'tamil'].reset_index(drop=True)\ntest_data = test_tamil\nprint(test_data.shape)\n\"\"\"","5a5421ed":"test_dataset = Dataset.from_pandas(test_data)\ntest_features = test_dataset.map(\n    partial(\n        prepare_validation_features, \n        tokenizer=tokenizer,\n        pad_on_right=pad_on_right, \n        max_length=max_length,\n        doc_stride=doc_stride\n    ),\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(\n    lambda example: example, remove_columns=['example_id', 'offset_mapping']\n)\n\nfin_start_logits = None\nfin_end_logits = None\n\n#for fold_ in tqdm(range(5)):\nuse_folds = [0,1,2,3,4,5,6,7,8]\nfor fold_ in use_folds: \n    model_path = f\"..\/input\/xlrm-large-tez-1015-10fold\/xlm-roberta-large-squad2_fold_{fold_}.bin\"\n    #model_path = f\"..\/input\/chaii-tez-muril-large-1103-400\/pytorch_model_fold_{fold_}.bin\"\n    #print(f'Inference for fold {fold_} using model {model_path}')\n    model = ChaiiModel(model_name=arch, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n    model.load(model_path, weights_only=True)\n    model.to(\"cuda\")\n    model.eval()\n    data_loader = torch.utils.data.DataLoader(\n        ChaiiDataset(test_feats_small), \n        batch_size=128,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n    start_logits = []\n    end_logits = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output, _, _ = model(**data)\n            start = output[0].detach().cpu().numpy()\n            end = output[1].detach().cpu().numpy()\n            start_logits.append(start)\n            end_logits.append(end)\n\n    start_logits = np.vstack(start_logits)\n    end_logits = np.vstack(end_logits)\n    \n    if fin_start_logits is None:\n        fin_start_logits = start_logits\n        fin_end_logits = end_logits\n    else:\n        fin_start_logits += start_logits\n        fin_end_logits += end_logits\n        \n    del model\n    torch.cuda.empty_cache()","1f3728ed":"len(use_folds)","f86dd089":"fin_start_logits \/= len(use_folds)\nfin_end_logits \/= len(use_folds)","3250dc24":"predictions = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))","5e8b7175":"submission = []\nfor p1, p2 in predictions.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n\nsample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\ntest_data =pd.merge(left=test_data,right=sample,on='id')\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\ntamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\ntamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\ntamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\nhindi_ad = \"\u0908\"\nhindi_bc = \"\u0908.\u092a\u0942\"\nhindi_ad1 = \"\u090f.\u0921\u0940\"\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n\n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_ad1), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n    cleaned_preds.append(pred)\n\ntest_data[\"PredictionString\"] = cleaned_preds\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)\nprint(test_data[\"PredictionString\"])","d806c633":"sub_df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\n\nsub_df_final = pd.merge(sub_df['id'], test_data[['id', 'PredictionString']], left_on='id', right_on='id', how='left')\nsub_df_final.fillna('', inplace=True)\nsub_df_final.to_csv('submission.csv', index=False)\nprint(sub_df_final)","c8158f95":"#### Single language pred"}}