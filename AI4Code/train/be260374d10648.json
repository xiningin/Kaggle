{"cell_type":{"ec5b11c3":"code","be5b9fff":"code","0a349c50":"code","f35e01fe":"code","58411018":"code","db1b02b9":"code","323370d4":"code","5aefc741":"code","4451f8af":"code","fabe1e56":"code","d75aa026":"code","47405ed2":"code","4b83c074":"code","b4c55a88":"code","489b8a0c":"code","421a45ed":"code","1ce69fcc":"code","5843bea5":"code","de4f2d88":"code","c9cc68a8":"code","8a9d8abf":"code","7659988b":"code","16d279b2":"code","744c29e4":"code","3452da1f":"code","110bb5fa":"code","04a4b903":"code","a34c24e9":"code","1727e288":"code","433fb83d":"code","30ad0fb2":"code","fadf8f9f":"code","51e61f07":"code","c21d2494":"code","88d614e3":"code","02ba9094":"code","57060f3a":"code","854d1742":"code","6f641e88":"code","0e32b097":"code","30c67003":"code","8d226fc0":"code","18baa450":"code","bc1657d0":"code","5325ce4d":"markdown","ecb3d88b":"markdown","6a4ec88d":"markdown","3c1e3906":"markdown","cfb52b92":"markdown","ad5299d2":"markdown","601761a8":"markdown","d1b17442":"markdown","2d213f7e":"markdown","33e8e3e6":"markdown","b582e40e":"markdown","67cc15a3":"markdown"},"source":{"ec5b11c3":"!pip install imutils\n!pip install image-classifiers==1.0.0b1","be5b9fff":"# import the necessary packages\nimport tensorflow as tf\nimport gc\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16, DenseNet169\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Activation, BatchNormalization, Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,TensorBoard,TerminateOnNaN, LearningRateScheduler\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\nfrom tensorflow.keras.layers import Lambda, Reshape, DepthwiseConv2D, ZeroPadding2D, Add, MaxPooling2D,Activation, Flatten, Conv2D, Dense, Input, Dropout, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nimport shutil\nimport cv2\nimport os\nfrom classification_models.tfkeras import Classifiers\nfrom datetime import datetime\n%load_ext tensorboard","0a349c50":"dataset_path = '.\/dataset'\nlog_path = '.\/logs'","f35e01fe":"%%bash\nrm -rf dataset\nmkdir -p dataset\/covid\nmkdir -p dataset\/normal\n#mkdir -p dataset\/pneumonia\nmkdir -p logs","58411018":"len(os.listdir('..\/input\/covid19radiographycovidnormal\/covid19_radiography_dataset'))","db1b02b9":"samples = 500","323370d4":"covid_dataset_path = '..\/input\/covid19radiographycovidnormal\/covid19_radiography_dataset'","5aefc741":"normal_dataset_path ='..\/input\/covid19radiographycovidnormal\/covid19_radiography_dataset'","4451f8af":"basePath = os.path.sep.join([normal_dataset_path,\"NORMAL\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:samples]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/normal\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","fabe1e56":"samples = 500","d75aa026":"basePath = os.path.sep.join([covid_dataset_path, \"COVID\"])\nimagePaths = list(paths.list_images(basePath))\n\n# randomly sample the image paths\nrandom.seed(42)\nrandom.shuffle(imagePaths)\nimagePaths = imagePaths[:samples]\n\n# loop over the image paths\nfor (i, imagePath) in enumerate(imagePaths):\n    # extract the filename from the image path and then construct the\n    # path to the copied image file\n    filename = imagePath.split(os.path.sep)[-1]\n    outputPath = os.path.sep.join([f\"{dataset_path}\/covid\", filename])\n\n    # copy the image\n    shutil.copy2(imagePath, outputPath)","47405ed2":"len(os.listdir('..\/working\/dataset\/normal'))","4b83c074":"len(os.listdir('..\/working\/dataset\/covid'))","b4c55a88":"def ceildiv(a, b):\n    return -(-a \/\/ b)\n\ndef plots_from_files(imspaths, figsize=(10,5), rows=1, titles=None, maintitle=None):\n    \"\"\"Plot the images in a grid\"\"\"\n    f = plt.figure(figsize=figsize)\n    if maintitle is not None: plt.suptitle(maintitle, fontsize=10)\n    for i in range(len(imspaths)):\n        sp = f.add_subplot(rows, ceildiv(len(imspaths), rows), i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        img = plt.imread(imspaths[i])\n        plt.imshow(img, cmap = 'gray')","489b8a0c":"normal_images = list(paths.list_images(f\"{dataset_path}\/normal\"))\ncovid_images = list(paths.list_images(f\"{dataset_path}\/covid\"))","421a45ed":"plots_from_files(normal_images[0:10], maintitle=\"Normal X-ray images\")","1ce69fcc":"plots_from_files(covid_images[0:10], maintitle=\"Covid-19 X-ray images\")","5843bea5":"class_to_label_map = {'covid' : 1, 'normal' : 0}","de4f2d88":"# grab the list of images in our dataset directory, then initialize\n# the list of data (i.e., images) and class images\nprint(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(dataset_path))\ndata = []\nlabels = []\n# loop over the image paths\nfor imagePath in imagePaths:\n    # extract the class label from the filename\n    label = imagePath.split(os.path.sep)[-2]\n    # load the image, swap color channels, and resize it to be a fixed\n    # 224x224 pixels while ignoring aspect ratio\n    image = cv2.imread(imagePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n    # update the data and labels lists, respectively\n    data.append(image)\n    labels.append(class_to_label_map[label])\n# convert the data and labels to NumPy arrays while scaling the pixel\n# intensities to the range [0, 1]\ndata = np.array(data) \/ 255.0\nlabels = np.array(labels)","c9cc68a8":"# perform one-hot encoding on the labels\n# perform one-hot encoding on the labels\n#labels = to_categorical(labels)\n# partition the data into training and testing splits using 80% of\n# the data for training and the remaining 20% for testing\n(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.15, stratify=labels, random_state=42)\n# initialize the training data augmentation object\ntrain_datagen = ImageDataGenerator(\n                                   rotation_range=60,\n                                   horizontal_flip = True ,\n                                   vertical_flip = True ,\n                                   fill_mode='nearest')\n\nval_datagen = ImageDataGenerator()","8a9d8abf":"trainYSparse = trainY\ntrainY = to_categorical(trainY)","7659988b":"def get_baseline_model(img_size):\n  \n    weight_decay = 1e-4\n\n    visible = Input(shape=(img_size, img_size, 3), dtype=tf.float32)\n\n    conv = Conv2D(32, (5, 5))(visible)\n    conv_act = Activation('relu')(conv)\n    conv_act_batch = BatchNormalization()(conv_act)\n    conv_maxpool = MaxPooling2D()(conv_act_batch)\n    conv_dropout = Dropout(0.1)(conv_maxpool)\n\n    conv = Conv2D(64, (3, 3))(conv_dropout)\n    conv_act = Activation('relu')(conv)\n    conv_act_batch = BatchNormalization()(conv_act)\n    conv_maxpool = MaxPooling2D()(conv_act_batch)\n    conv_dropout = Dropout(0.2)(conv_maxpool)\n\n    conv = Conv2D(128, (3, 3))(conv_dropout)\n    conv_act = Activation('relu')(conv)\n    conv_act_batch = BatchNormalization()(conv_act)\n    conv_maxpool = MaxPooling2D()(conv_act_batch)\n    conv_dropout = Dropout(0.3)(conv_maxpool)\n\n    conv = Conv2D(256, (3, 3))(conv_dropout)\n    conv_act = Activation('relu')(conv)\n    conv_act_batch = BatchNormalization()(conv_act)\n    conv_maxpool = MaxPooling2D()(conv_act_batch)\n    conv_dropout = Dropout(0.4)(conv_maxpool)\n\n    conv = Conv2D(512, (3, 3))(conv_dropout)\n    conv_act = Activation('relu')(conv)\n    conv_act_batch = BatchNormalization()(conv_act)\n    conv_maxpool = MaxPooling2D()(conv_act_batch)\n    conv_dropout = Dropout(0.5)(conv_maxpool)\n    \n    gap2d = GlobalAveragePooling2D()(conv_dropout)\n    act = Activation('relu')(gap2d)\n    batch = BatchNormalization()(act)\n    dropout = Dropout(0.3)(batch)\n\n    fc1 = Dense(256)(dropout)\n    act = Activation('relu')(fc1)\n    batch = BatchNormalization()(act)\n    dropout = Dropout(0.4)(batch)\n\n    # and a logistic layer\n    predictions = Dense(2, activation='softmax')(dropout)\n    \n    # Create model.\n    model = tf.keras.Model(visible, predictions, name='baseline')\n\n    return model\n","16d279b2":"model = get_baseline_model(224)","744c29e4":"from math import floor\nN_FOLDS = 8\nEPOCHS = 8\nINIT_LR = 3e-4\nT_BS = 16\nV_BS = 16\ndecay_rate = 0.95\ndecay_step = 1\n\nskf = StratifiedKFold(n_splits=N_FOLDS, random_state=None,)\nlog_dir = \".\/logs\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\ncallbacks = [ModelCheckpoint(filepath='vgg16_weights_tf_dim_ordering_tf_kernels.h5', monitor='val_loss',mode='min',verbose=1,save_best_only=True,save_weights_only=True),\n             LearningRateScheduler(lambda epoch : INIT_LR * pow(decay_rate, floor(epoch \/ decay_step))), tensorboard_callback]","3452da1f":"resnet34, _ = Classifiers.get('densenet121')\nconv_base = resnet34(weights='imagenet',\n              include_top=False,\n              input_shape=(224, 224, 3))\n\nconv_base.trainable = False\n\n\ngap2d = GlobalAveragePooling2D()(conv_base.output)\nact = Activation('relu')(gap2d)\nbatch = BatchNormalization()(act)\ndropout = Dropout(0.3)(batch)\n\nfc1 = Dense(256)(dropout)\nact = Activation('relu')(fc1)\nbatch = BatchNormalization()(act)\ndropout = Dropout(0.4)(batch)\npredictions = Dense(2, activation='softmax')(dropout)\n\nmodel = Model(conv_base.input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=INIT_LR),\n              metrics=['acc', tf.keras.metrics.AUC()])","110bb5fa":"submission_predictions = []\nfor epoch, skf_splits in zip(range(0,N_FOLDS),skf.split(trainX,trainYSparse)):\n\n    train_idx = skf_splits[0]\n    val_idx = skf_splits[1]\n    \n    # Create Model\n    resnet34, _ = Classifiers.get('densenet121')\n    conv_base = resnet34(weights='imagenet',\n                  include_top=False,\n                  input_shape=(224, 224, 3))\n\n    conv_base.trainable = False\n\n\n    gap2d = GlobalAveragePooling2D()(conv_base.output)\n    act = Activation('relu')(gap2d)\n    batch = BatchNormalization()(act)\n    dropout = Dropout(0.3)(batch)\n\n    fc1 = Dense(256)(dropout)\n    act = Activation('relu')(fc1)\n    batch = BatchNormalization()(act)\n    dropout = Dropout(0.4)(batch)\n    predictions = Dense(2, activation='softmax')(dropout)\n\n    model = Model(conv_base.input, predictions)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=INIT_LR),\n                  metrics=['acc', tf.keras.metrics.AUC()])\n    \n    if epoch != 0:\n        # Load Model Weights\n        model.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels.h5') \n    \n    history = model.fit(\n                train_datagen.flow(trainX[train_idx], trainY[train_idx], batch_size=T_BS),\n                steps_per_epoch=len(train_idx) \/\/ T_BS,\n                epochs=EPOCHS,\n                validation_data = val_datagen.flow(trainX[val_idx], trainY[val_idx], batch_size=V_BS),\n                validation_steps = len(val_idx) \/\/ V_BS,\n                callbacks=callbacks)\n    \n    if epoch >= 1:\n        preds = model.predict(testX, batch_size=V_BS)\n        submission_predictions.append(preds)\n    \n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='valid')\n    plt.title(\"model loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"number of epochs\")\n    plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n    plt.savefig('loss_performance'+'_'+str(epoch)+'.png')\n    plt.clf()\n    plt.plot(history.history['acc'], label='train')\n    plt.plot(history.history['val_acc'], label='valid')\n    plt.title(\"model acc\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"number of epochs\")\n    plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n    plt.savefig('acc_performance'+'_'+str(epoch)+'.png')\n    \n    #del history\n    #del model\n    gc.collect()\n    ","04a4b903":"model.summary()","a34c24e9":"class GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n\n        # if the layer name is None, attempt to automatically find\n        # the target output layer\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n\n    def find_target_layer(self):\n        # attempt to find the final convolutional layer in the network\n        # by looping over the layers of the network in reverse order\n        for layer in reversed(self.model.layers):\n            # check to see if the layer has a 4D output\n            if len(layer.output_shape) == 4:\n                return layer.name\n\n        # otherwise, we could not find a 4D layer so the GradCAM\n        # algorithm cannot be applied\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n\n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output, \n                     \n                self.model.output])\n\n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type, pass the\n            # image through the gradient model, and grab the loss\n            # associated with the specific class index\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n\n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, convOutputs)\n\n        # compute the guided gradients\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n\n        # the convolution and guided gradients have a batch dimension\n        # (which we don't need) so let's grab the volume itself and\n        # discard the batch\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n\n        # compute the average of the gradient values, and using them\n        # as weights, compute the ponderation of the filters with\n        # respect to the weights\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n\n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer \/ denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n\n        # return the resulting heatmap to the calling function\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap=cv2.COLORMAP_JET):\n        # apply the supplied color map to the heatmap and then\n        # overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n\n        # return a 2-tuple of the color mapped heatmap and the output,\n        # overlaid image\n        return (heatmap, output)","1727e288":"model.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels.h5')","433fb83d":"np.where(testY == 0)","30ad0fb2":"covid1 = testX[10]\ncovid2 = testX[28]\ncovid3 = testX[29]\ncovid_list = [covid1, covid2, covid3]\n#pneumonia1 = testX[2]\n#pneumonia2 = testX[43]\n#pneumonia3 = testX[52]\n#pneumonia_list = [pneumonia1, pneumonia2, pneumonia3]","fadf8f9f":"image = testX[7]\npreds = model.predict(image[np.newaxis,...])\npreds","51e61f07":"\npreds = model.predict(image[np.newaxis,...])\ni = np.argmax(preds[0])\n\n# decode the ImageNet predictions to obtain the human-readable label\n\n# # initialize our gradient class activation map and build the heatmap\ncam = GradCAM(model, i)\nheatmap = cam.compute_heatmap(image[np.newaxis,...])\n\nimg_copy = np.copy(image)\nimg_copy -= img_copy.min((0,1))\nimg_copy = (255*img_copy).astype(np.uint8)\n# resize the resulting heatmap to the original input image dimensions\n# and then overlay heatmap on top of the image\nheatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n(heatmap, output) = cam.overlay_heatmap(heatmap, img_copy, alpha=0.5)","c21d2494":"plt.imshow(output, cmap = 'gray')","88d614e3":"def plot_map(img_list):\n    fig, axes = plt.subplots(len(img_list), 2, figsize=(15, 15))\n    fig.suptitle('Covid 19 Grad-CAM\\n',fontsize=20)\n    \n    for i, img in enumerate(img_list):\n        preds = model.predict(img[np.newaxis,...])\n        axes[i,0].imshow(img, cmap = 'bone')\n        axes[i,0].set_xticks([])\n        axes[i,0].set_yticks([])\n      #  axes[i,0].set_title(f'{class_label[np.argmax(preds[:, 1:]) + 1]} \/ {class_label[np.argmax(label[:, 1:]) + 1]} \/ {np.max(preds[:, 1:]):.4f}')\n        heatmap = cam.compute_heatmap(img[np.newaxis,...])\n        img_copy = np.copy(img)\n        img_copy -= img_copy.min((0,1))\n        img_copy = (255*img_copy).astype(np.uint8)\n        # resize the resulting heatmap to the original input image dimensions\n        # and then overlay heatmap on top of the image\n        heatmap = cv2.resize(heatmap, (img_copy.shape[1], img_copy.shape[0]))\n        (heatmap, output) = cam.overlay_heatmap(heatmap, img_copy, alpha=0.5)\n        axes[i,1].imshow(output)\n        axes[i,1].set_xticks([])\n        axes[i,1].set_yticks([])\n        #axes[i,1].set_title(\"heatmap showing hemorrhage location\")\n    plt.subplots_adjust(wspace=1, hspace=0.2)\n    plt.savefig('CovidGradCAM.png')","02ba9094":"plot_map(covid_list)","57060f3a":"#plot_map(pneumonia_list)","854d1742":"predY = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])","6f641e88":"class_to_label_map = {1 : 'covid', 0 : 'normal'}","0e32b097":"cm_mat = confusion_matrix(testY, np.argmax(predY, axis = -1))","30c67003":"import numpy as np\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize = 'xx-large')\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n    \nplot_confusion_matrix(cm_mat, \n                      normalize = False,\n                      target_names = ['normal', 'covid'],\n                      title        = \"Confusion Matrix\")","8d226fc0":"print(classification_report(testY, np.argmax(predY, axis = -1), target_names = ['normal', 'covid']))","18baa450":"# compute the confusion matrix and and use it to derive the raw\n# accuracy, sensitivity, and specificity\ncm = confusion_matrix(testY, np.argmax(predY, axis = -1))\ntotal = sum(sum(cm))\nacc = (cm[0, 0] + cm[1, 1]) \/ total\nsensitivity = cm[0, 0] \/ (cm[0, 0] + cm[0, 1])\nspecificity = cm[1, 1] \/ (cm[1, 0] + cm[1, 1])\n# show the confusion matrix, accuracy, sensitivity, and specificity\nprint(cm)\nprint(\"acc: {:.4f}\".format(acc))\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\nprint(\"specificity: {:.4f}\".format(specificity))","bc1657d0":"!rm -rf dataset\n!rm -rf logs","5325ce4d":"### Training","ecb3d88b":"### Covid xray dataset","6a4ec88d":"## Plot x-rays","3c1e3906":"Helper function to plot the images in a grid","cfb52b92":"## Build Dataset","ad5299d2":"## Model","601761a8":"### Build normal xray dataset","d1b17442":"## Data preprocessing","2d213f7e":"## Setup","33e8e3e6":"## Grad-CAM","b582e40e":"### Evaluation","67cc15a3":"#### Confusion matrix"}}