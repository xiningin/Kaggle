{"cell_type":{"df5c38ff":"code","c6b22a50":"code","d0755cdf":"code","74e99993":"code","4a6240b9":"code","a1c8e028":"code","f4037c96":"code","af3fb7da":"code","e613ad65":"code","fb526e9d":"code","45b9a2b3":"code","a8149e8e":"code","7c077367":"code","8d9e6b4b":"code","b63b4c70":"code","53c9659a":"code","201e6efb":"code","d891e781":"code","a4a7badb":"code","74f7cb4f":"code","5b613162":"code","10364ea2":"code","8c073c0c":"code","c4946d61":"code","a8e2dd2f":"code","c2d1d07d":"code","f722786f":"code","83fb6260":"code","280e913c":"code","6c80e47f":"code","93f0cfb2":"code","77ffeb7c":"code","1164b5c0":"code","ad1439a7":"code","0da9bcf6":"code","ae0e5892":"code","414199cf":"code","7e565114":"code","4bd60cb5":"code","3e0f1cfa":"code","171792b2":"code","5a5d1ef4":"code","84ab7287":"code","dc7eade5":"code","3d430b11":"code","dbdc8f29":"code","185ac79b":"code","31938e56":"code","3cdce7a6":"code","dee721e9":"code","8f9fd122":"code","69eee411":"code","3f9138ad":"code","5186ae4c":"code","b23bf802":"code","034ad2db":"code","eb90d5ce":"code","9b46bb32":"code","0e8857a7":"code","715277d5":"code","66c2de8a":"code","955b79b5":"code","9af1c078":"code","19492224":"code","f8d7041e":"code","659efe4a":"code","557cfb8d":"code","ca28cdf6":"markdown","32fff7ac":"markdown","3fbe289b":"markdown","4579324e":"markdown","f40616d9":"markdown","aa7a3a84":"markdown","adb2027a":"markdown","5731ba1c":"markdown","9fda92a9":"markdown","830e33a5":"markdown","b45add99":"markdown","516f25b6":"markdown","97c335f0":"markdown","04acc0ff":"markdown","97d35716":"markdown","9e977640":"markdown"},"source":{"df5c38ff":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n","c6b22a50":"train = pd.read_csv('..\/input\/hackerearth-carnival-wars-challenge-problem-1\/train.csv')\ntest = pd.read_csv('..\/input\/hackerearth-carnival-wars-challenge-problem-1\/test.csv')","d0755cdf":"train.head()","74e99993":"train.dtypes","4a6240b9":"# For train data\ntrain['instock_date']= pd.to_datetime(train['instock_date'])\ntrain['year'] = train['instock_date'].dt.year\ntrain['month'] =  train['instock_date'].dt.month\ntrain['quarter'] = train['instock_date'].dt.quarter\ntrain['day of the week'] = train['instock_date'].dt.dayofweek # Monday = 0 & Sunday = 6\ntrain['Dayofyear'] = train['instock_date'].dt.dayofyear\ntrain[\"hour\"] = train['instock_date'].dt.hour\n","a1c8e028":"# For test data\ntest['instock_date']= pd.to_datetime(test['instock_date'])\ntest['year'] = test['instock_date'].dt.year\ntest['month'] =  test['instock_date'].dt.month\ntest['quarter'] = test['instock_date'].dt.quarter\ntest['day of the week'] = test['instock_date'].dt.dayofweek # Monday = 0 & Sunday = 6\ntest['Dayofyear'] = test['instock_date'].dt.dayofyear\ntest[\"hour\"] = test['instock_date'].dt.hour","f4037c96":"train.head()","af3fb7da":"train.shape","e613ad65":"train.info()","fb526e9d":"# Finding number of unique values in each columns\nfor col in train.columns:\n  print(col,train[col].nunique())","45b9a2b3":"# Finding number of null values in each column\ntrain.isnull().sum()\n","a8149e8e":"# Excluding rows where train['Selling_Price'] is null\ntrain = train[train['Selling_Price'].notna()]\n","7c077367":"# Replacing null values in train['Customer_name'] with mode\ntrain['Customer_name'] = train['Customer_name'].fillna(train['Customer_name'].mode()[0])\n\n","8d9e6b4b":"# After applying Label Encoding to categorical columns: train['Customer_name'], train['Loyalty_customer'], train['Product_Category']\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n# Encode labels in column 'Customer_name'. \ntrain['Customer_name']= label_encoder.fit_transform(train['Customer_name']) \n\n# Encode labels in column 'Loyalty_customer'. \ntrain['Loyalty_customer']= label_encoder.fit_transform(train['Loyalty_customer']) \n\n# Encode labels in column 'Product_Category'. \ntrain['Product_Category']= label_encoder.fit_transform(train['Product_Category']) \n\n\n\n","b63b4c70":"train.head()","53c9659a":"train.Stall_no.describe()","201e6efb":"# Converting the negative values in \"Selling_Price\" to its absolute value (positive value)\ntrain['Selling_Price'] = train['Selling_Price'].abs()\n","d891e781":"sns.boxplot( y=\"Selling_Price\",color='m', data=train)","a4a7badb":"train.Selling_Price.describe()","74f7cb4f":"train['Loyalty_customer'].unique()","5b613162":"train['Product_Category'].unique()","10364ea2":"train.Discount_avail.unique()","8c073c0c":"# Replacing nan values of other columns using mean , median mode\ntrain['Discount_avail'] = train['Discount_avail'].fillna(train['Discount_avail'].mean())\ntrain['charges_1'] = train['charges_1'].fillna(train['charges_1'].mean())\ntrain['charges_2 (%)'] = train['charges_2 (%)'].fillna(train['charges_2 (%)'].mean())\ntrain['Minimum_price'] = train['Minimum_price'].fillna(train['Minimum_price'].mean())\ntrain['Maximum_price'] = train['Maximum_price'].fillna(train['Maximum_price'].mean())\ntrain['Stall_no'] = train['Stall_no'].fillna(train['Stall_no'].median())\n","c4946d61":"# Rechecking the presence of null values\ntrain.isnull().sum()\n","a8e2dd2f":"# Distplot of Charges_1\nsns.distplot(train.charges_1,color='g')\nplt.grid('True')\nfor x in [0.25,0.5,0.75]:\n    plt.axvline(train.charges_1.quantile(x),c='r',lw=1.5)\nplt.show()","c2d1d07d":"# Distplot of charges_2 (%)\nsns.distplot(train['charges_2 (%)'],color='g')\nplt.grid('True')\nfor x in [0.25,0.50,0.75]:\n    plt.axvline(train['charges_2 (%)'].quantile(x),color='m',lw=1.5)\nplt.show()","f722786f":"train.Minimum_price.describe()\n","83fb6260":"# Distplot of Minimum Price\nsns.distplot(train.Minimum_price,color='r')\nplt.grid('True')\nfor x in [0.25,0.50,0.75]:\n    plt.axvline(train.Minimum_price.quantile(x),c='g',lw=1.5)\nplt.show()","280e913c":"# Create charges_1 (%) by dividing each column value in charges_1 by 100\ntrain['charges_1 (%)'] = train['charges_1'] \/100\n# Create charges_2 by multiplying each column value in charges_2 (%) by 100\ntrain['charges_2'] = train['charges_2 (%)'] *100\n# Adding charges_1 and charges_2 to get total charges\ntrain[\"total_charges\"]= train['charges_1']+train['charges_2']\n# Maximum - minium price = range\ntrain[\"range\"] = train['Maximum_price']-train['Minimum_price']","6c80e47f":"train.head()","93f0cfb2":"train =train.drop([\"instock_date\",\"Product_id\"],axis =1)\n","77ffeb7c":"x = train\nx = x.drop([\"Selling_Price\"],axis =1)\ny = train['Selling_Price']","1164b5c0":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 40)","ad1439a7":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","0da9bcf6":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import mean_squared_error","ae0e5892":"gbr = GradientBoostingRegressor(random_state=0)\nxgb = xgboost.XGBRegressor(n_jobs=-1)\net = ExtraTreesRegressor(n_jobs=-1)\nrf = RandomForestRegressor(n_jobs=-1)\nds = DecisionTreeRegressor()","414199cf":"reg = {\n\"LinearRegression\": LinearRegression(),\n\"KNeighborsRegressor\":KNeighborsRegressor(n_neighbors=2),\n\"AdaBoostRegressor\":AdaBoostRegressor(random_state=0, n_estimators=100),\n\"LGBMRegressor\":LGBMRegressor(),\n\"Ridge\": Ridge(alpha=1.0),\n\"ElasticNet\":ElasticNet(random_state=0),\n\"GradientBoostingRegressor\":GradientBoostingRegressor(random_state=0),\n\"DecisionTreeRegressor\": DecisionTreeRegressor(),\n\"ExtraTreesRegressor\": ExtraTreesRegressor(n_jobs=-1),\n\"RandomForestRegressor\": RandomForestRegressor(n_jobs=-1),\n\"XGBRegressor\":xgboost.XGBRegressor(n_jobs=-1)\n}","7e565114":"%%time\ndic =  {\"Model\":[],\"R2_Train\":[],\"RMSE_Train\":[]}\nfor name, model in reg.items():\n\n   \n  model.fit(x_train, y_train)\n  y_train_pre = model.predict(x_test)\n  r2 = r2_score(y_test, y_train_pre)\n  rmse  = np.sqrt(mean_squared_error(y_test, y_train_pre))\n  print(\"--------------------------------------------------------------\")\n  print(\"Model:\", name)\n  print(\"-----Training Data Evalution-----\")\n  print(\"R2 Value: \", r2_score(y_test, y_train_pre))\n  print(\"RMSE: \",np.sqrt(mean_squared_error(y_test, y_train_pre)))\n  dic[\"Model\"].append(name)\n  dic[\"R2_Train\"].append(r2)\n  dic[\"RMSE_Train\"].append(rmse)\n\n\n\n\n","4bd60cb5":"final_data = pd.DataFrame(dic)\nfinal_data","3e0f1cfa":"final_data.sort_values(\"RMSE_Train\", axis = 0, ascending = True)","171792b2":"from sklearn.ensemble import RandomForestRegressor \n# create regressor object \nrf = RandomForestRegressor(n_jobs=-1)\n# fit the regressor with x and y data \nrf.fit(x_train, y_train)","5a5d1ef4":"pred_rf = rf.predict(x_test)\npred_rf = pd.DataFrame(pred_rf)","84ab7287":"from sklearn.metrics import mean_squared_error \nprint(mean_squared_error(y_test, pred_rf)**0.5)","dc7eade5":"# Seeing the feature importance \nimportance = rf.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","3d430b11":"# Recursive Feature Elimination to select important features\nfrom sklearn.feature_selection import RFE\nsel2 = RFE(RandomForestRegressor(), n_features_to_select =5)\nsel2.fit(x_train,y_train)","dbdc8f29":"sel2.get_support()","185ac79b":"features2 = x_train.columns[sel2.get_support()]\nfeatures2","31938e56":"x_train_rfe = sel2.transform(x_train)\nx_test_rfe = sel2.transform(x_test)\nclf = RandomForestRegressor()\nclf.fit(x_train_rfe,y_train)\ny_pred_rfe =clf.predict(x_test_rfe)\nfrom sklearn.metrics import mean_squared_error \nprint(mean_squared_error(y_test, y_pred_rfe)**0.5)","3cdce7a6":"et = ExtraTreesRegressor(n_jobs=-1)\net.fit(x_train,y_train)\ny_pred_et =et.predict(x_test)\nfrom sklearn.metrics import mean_squared_error \nprint(mean_squared_error(y_test, y_pred_et)**0.5)","dee721e9":"importance = et.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","8f9fd122":"# Recursive Feature Elimination to select important features \nsel3 = RFE(ExtraTreesRegressor(n_jobs=-1),n_features_to_select =5)\nsel3.fit(x_train,y_train)\nsel3.get_support()\nfeatures3 = x.columns[sel3.get_support()]\nfeatures3\n","69eee411":"# I used VotingRegressor with base regressors as ExtraTreesRegressor and RandomForestRegressor as they both gave the best predictions as described in previous cells\n\n\nfrom sklearn.ensemble import VotingRegressor\nmodel = VotingRegressor([('et', et),('rf',rf)],n_jobs=-1)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nr2_test = r2_score(y_test, y_pred)\nrmse_test  = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"R2 Value: \", r2_test)\nprint(\"RMSE: \",rmse_test)","3f9138ad":"\n\nmodel = VotingRegressor([('et', et),('rf',rf)],n_jobs=-1)\n\nmodel.fit(x,y)\n\ny_pred = model.predict(x)\nr2_test = r2_score(y, y_pred)\nrmse_test  = np.sqrt(mean_squared_error(y, y_pred))\nprint(\"R2 Value: \", r2_test)\nprint(\"RMSE: \",rmse_test)","5186ae4c":"test.head()","b23bf802":"# Finding number of null values in each column\ntest.isnull().sum()\n","034ad2db":"test['Customer_name'] = test['Customer_name'].fillna(test['Customer_name'].mode()[0])\n","eb90d5ce":"test.head()","9b46bb32":"# Applying Label Encoding to categorical columns\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n# Encode labels in column 'Customer_name'. \ntest['Customer_name']= label_encoder.fit_transform(test['Customer_name']) \n\n# Encode labels in column 'Loyalty_customer'. \ntest['Loyalty_customer']= label_encoder.fit_transform(test['Loyalty_customer']) \n\n# Encode labels in column 'Product_Category'. \ntest['Product_Category']= label_encoder.fit_transform(test['Product_Category']) \n","0e8857a7":"test['charges_1'] = test['charges_1'].fillna(test['charges_1'].mean())\ntest['charges_2 (%)'] = test['charges_2 (%)'].fillna(test['charges_2 (%)'].mean())\ntest['Minimum_price'] = test['Minimum_price'].fillna(test['Minimum_price'].mean())\ntest['Stall_no'] = test['Stall_no'].fillna(test['Stall_no'].median())\n","715277d5":"test['charges_1 (%)'] = test['charges_1'] \/100\ntest['charges_2'] = test['charges_2 (%)'] *100\ntest[\"total_charges\"]= test['charges_1']+train['charges_2']","66c2de8a":"test[\"range\"] = test['Maximum_price']-test['Minimum_price']","955b79b5":"test[\"total_charges\"] = test[\"total_charges\"].fillna(test[\"total_charges\"].mean())\n","9af1c078":"p_id = test[\"Product_id\"]\ntest = test.drop([\"Product_id\",\"instock_date\"],axis =1)","19492224":"prediction = model.predict(test)","f8d7041e":"prediction = pd.DataFrame(prediction)\nprediction","659efe4a":"Prediction = pd.concat([p_id,prediction],axis =1)","557cfb8d":"Prediction","ca28cdf6":"When I used ExtraTreesRegressor along with recursive feature elimination to predict the given test data, my public score on the leaderboard was 89.82776.","32fff7ac":"When I used RandomForestRegressor along with recursive feature elimination to predict the given test data, my public score on the leaderboard was 89.83489.   ","3fbe289b":"#### ExtraTreesRegressor\n","4579324e":"#### Randomforest regression","f40616d9":"### Importing Modules","aa7a3a84":"### Loading the dataset","adb2027a":"### Feature Engineering","5731ba1c":"Finally, I used the VotingRegressor as my final model and trained it on the entire given training data. ","9fda92a9":"The final model led to a public score of 90.03197 and leaderboard rank 75.","830e33a5":"### Test data","b45add99":"### Model Building","516f25b6":"#### New Features using datetime from 'instock_date'","97c335f0":"From the above we find that the best preictions are given by ExtraTreesRegressor and RandomForestRegressor.","04acc0ff":"###  Split the dataset into training and testing","97d35716":"#### VotingRegressor","9e977640":"###### Feature selection"}}