{"cell_type":{"fdd6a190":"code","4d798f09":"code","d77b93f7":"code","30872205":"code","0ee2efd6":"code","919115bb":"code","ead6ae69":"code","a5174b5a":"code","e3836fa2":"code","dd6f4e88":"code","8d58ca98":"code","75456f8f":"code","ae5ac39b":"code","52173960":"code","4f2c55b6":"markdown","fd58204d":"markdown","2b1e08cf":"markdown","f2c6d715":"markdown","da5c25f4":"markdown","5404181a":"markdown"},"source":{"fdd6a190":"!pip install datatable==0.11.0 > \/dev\/null\n","4d798f09":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport gc\nimport pyarrow.parquet as pq\nimport pyarrow\nimport datatable as dt\nfrom datatable import f\n","d77b93f7":"%%time\n\ndt_data = dt.fread(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.jay\")\ndt_data.shape","30872205":"# make a single column which contains a unique id for each [user_id, task_container_id] combination\ndt_data['tc_id'] = dt_data[:,dt.str32(f['user_id'])+'_'+f['task_container_id']]","0ee2efd6":"%%time\n\n# make a separate frame with just questions, and one row per tc_id\nquestions = dt_data[f[\"content_type_id\"]==0,:]\nq_task_containers = questions[\n    (f['tc_id']!=dt.shift(f['tc_id'])) , :]\nq_task_containers.shape  # expected shape: (76483597, 11)","919115bb":"# this_* is prior_* shifted once by tc_id, within user\nq_task_containers['this_question_elapsed_time'] = q_task_containers[:,dt.shift(f['prior_question_elapsed_time'], -1),dt.by(f['user_id'])]['prior_question_elapsed_time']\nq_task_containers['this_question_had_explanation'] = q_task_containers[:,dt.shift(f['prior_question_had_explanation'], -1),dt.by(f['user_id'])]['prior_question_had_explanation']","ead6ae69":"# sanity check - this_* values are null in the last row for a user\nq_task_containers[f['user_id']==115, :]","a5174b5a":"# drop all columns except the newest ones\nq_task_containers = q_task_containers[:,'tc_id':]","e3836fa2":"%%time\n\n# key by tc_id for joining\nq_task_containers.key='tc_id'","dd6f4e88":"%%time\n\n# left outer join into original data table\ndt_data = dt_data[:,:,dt.join(q_task_containers)]","8d58ca98":"# Sanity check: last 3 task_container_id bundles for a particular user\ndt_data[(f['user_id']==2147012157) & (f['task_container_id'] > 4675),:]","75456f8f":"# Make room in RAM\ndel q_task_containers\ngc.collect()\n","ae5ac39b":"%%time\n\ndt_data.to_csv('riiid_train_with_qdata.csv')","52173960":"%%time\n\ndt_data.to_jay('riiid_train_with_qdata.jay')","4f2c55b6":"## Compute question_elapsed_time and question_had_explanation for *this* question \n\n\nThis value is the same within each bundle of questions(set of questions asked of a given user with a given `task_container_id`). So, the whole bundle either got explanations or didn't; and the elapsed time is averaged over the bundle. To get explanation\/elapsed time for *this* bundle we get the applicable bundles (those that represent groups of questions, not lectures) in the right order, then shift `prev_*` backwards. \n\nThen, for `had_explanation`, ffill the NaNs. (the only time that NaNs need to be filled is for the last question, beause there was no `prev_*` to get the data from; ffill makes sense in this context for had_explanation, since the only time users generally *dont't* get an explanation is at the beginning, when they are being asked diagnostic questions.)\n\n\nJudging by user 115 (for whom the order of `timestamp` and `task_container_id` do not match), it is `timestamp` which determines what the \"previous\" task container was: the first task container should have `prior_question_had_explanation == None`, and this is true for `timestamp` 0, not `task_container_id` 0.","fd58204d":"This notebook augments the Riiid input dataset with new columns: `this_question_had_explanation` and `this_question_elapsed_time`. These are essentially shifted and ffilled versions of `prior_question_had_explanation` and `prior_question_elapsed_time` from the orinal dataset.\nThey tracks whether **this** question provided feedback to the user and how long did **this** question take, rather than the previous one.\n\nI'm using alternative input\/output formats based on info in [this notebook](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets\/data)","2b1e08cf":"## Read in training data from .jay file to datatable\n\n","f2c6d715":"# Now less wrong!\n\nI've switched from using Pandas to datatable, because Pandas can't do what needs to be done without running out of memory.\n\nThis enabled me to fix two bugs:\n\n- Group by user before shifting prev_* to make this_*\n- Actually join this_* information properly so that it applies to each row, not just the first row per (user_id, task_container_id) combination.","da5c25f4":"## Write back out ","5404181a":"## Put computed columns back into training data"}}