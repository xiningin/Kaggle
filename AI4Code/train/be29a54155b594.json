{"cell_type":{"0425263d":"code","559945b2":"code","f5fbc728":"code","802360cb":"code","7bd19526":"code","8ab29e35":"code","e37d3ddd":"code","62bb15ab":"code","4c6da4b7":"code","80fb7568":"code","d73dc2b4":"code","8239c9f7":"code","69d9695a":"code","c1f15367":"code","03c79f3c":"code","a4780c6d":"code","97282cd1":"code","71482bef":"code","723241e3":"code","049b2d72":"code","a7480d39":"code","d79d54b4":"code","3fd34cc8":"code","e18a76d5":"code","de23b819":"code","b2c82f31":"code","087a60db":"code","a45139fd":"code","a9ec5d84":"code","57e4f966":"code","91e5c228":"code","4665a35b":"code","4c5565e0":"code","9e4ff582":"code","599fcb84":"code","6ca0948a":"code","19021bea":"code","c9a229c1":"markdown","dd5a0537":"markdown","e185a5fd":"markdown","4feb2d1c":"markdown","e1f0d6e4":"markdown","7b8ff3fd":"markdown","ab4124b7":"markdown","b097a2be":"markdown","ef31d0b3":"markdown","28a424fa":"markdown","4fcf3480":"markdown","526f325d":"markdown","42ebf02a":"markdown","39ad5298":"markdown","27084f44":"markdown","cde09602":"markdown","403d0563":"markdown","40e8566d":"markdown","aac0b152":"markdown","e1a1e871":"markdown","e66e2a51":"markdown","329a1c85":"markdown","9fe611c7":"markdown","75a3046e":"markdown","d9ba25dd":"markdown","e78ed562":"markdown","b3d3ee18":"markdown","ab852722":"markdown","d8d8b84f":"markdown","d9e9ad82":"markdown","273264ea":"markdown","b8b6f681":"markdown","31a4772d":"markdown","b6b4127c":"markdown"},"source":{"0425263d":"# LOAD LIBRARY\nimport gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.show()\n\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# plotly\n!pip install chart_studio\nimport plotly.express as px\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(world_readable=True, theme='polar')\n\nimport lightgbm as lgb\n\nfrom time import time\nfrom tqdm import tqdm_notebook\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","559945b2":"# SET RANDOM SEED\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\n\n# DATA CHECK\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# DEFINE BASE PATH\nBASE_PATH = '\/kaggle\/input\/kakr-4th-competition\/'","f5fbc728":"# LOAD DATASET\ndf_train = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\ndf_test  = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))","802360cb":"# CHECK TRAIN DATASET\nprint('Training data shape is: ', df_train.shape)\ndf_train.head(10)","7bd19526":"# CHECK NULL VALUES AND DATA TYPES\nprint('Train dataset')\nprint(df_train.info())\nprint('')\nprint('')\nprint('Test dataset')\nprint(df_test.info())","8ab29e35":"print(df_train['id'].count())\nprint(df_train['id'].value_counts().shape[0])","e37d3ddd":"df_train.groupby(['income']).count()['id'].to_frame()","62bb15ab":"df_train['income'].value_counts(normalize=True).iplot(kind='bar',\n                                                      linecolor='black',\n                                                      opacity=0.6,\n                                                      color='red',\n                                                      bargap=0.8,\n                                                      gridcolor='white',\n                                                      xTitle='Income',\n                                                      yTitle='Percentage',\n                                                      title='Distribution of the Target column in the training dataset')","4c6da4b7":"df_train['age'].iplot(kind='hist',\n                      bins=15,\n                      color='yellow',\n                      xTitle='age',\n                      yTitle='Count',\n                      title='Distribution of Final weight')","80fb7568":"sns.kdeplot(df_train.loc[df_train['income'] == '<=50K', 'age'], label='<=50K', shade=True)\nsns.kdeplot(df_train.loc[df_train['income'] == '>50K', 'age'], label='>50K', shade=True)\n\nplt.xlabel('Age')\nplt.ylabel('Density')","d73dc2b4":"df_train['sex'].value_counts()","8239c9f7":"df_train['sex'].value_counts(normalize=True).iplot(kind='bar',\n                                                   linecolor='black',\n                                                   opacity=0.6,             \n                                                   color='blue',\n                                                   bargap=0.8,\n                                                   gridcolor='white',\n                                                   xTitle='Gender',\n                                                   yTitle='Percentage',\n                                                   title='Distribution of the Gender column in the training set')","69d9695a":"df_gender_target = df_train.groupby(['income', 'sex'])['id'].count().to_frame().reset_index()\ndf_gender_target.style.background_gradient(cmap='Reds')","c1f15367":"sns.catplot(x='income', y='id', hue='sex', data=df_gender_target, kind='bar')\nplt.ylabel('Count')\nplt.xlabel('Income')","03c79f3c":"df_train['race'].value_counts(normalize=True).sort_values(ascending=False)","a4780c6d":"df_train['race'].value_counts(normalize=True).iplot(kind='barh',\n                                                    linecolor='black',\n                                                    opacity=0.7,\n                                                    color='orange',\n                                                    bargap=0.8,\n                                                    gridcolor='white',\n                                                    xTitle='Percentage',\n                                                    yTitle='Race',\n                                                    title='Distribution of the race column in the training set')","97282cd1":"df_race_target = df_train.groupby(['income', 'race'])['id'].count().to_frame().reset_index()\ndf_race_target.style.background_gradient(cmap='Reds')","71482bef":"sns.catplot(x='race', y='id', hue='income', data=df_race_target, kind='bar')\n\n# plt.gcf().set_size_inches(10, 8)\nplt.xlabel('Type of race')\nplt.xticks(rotation=45, fontsize='10', horizontalalignment='right')\nplt.ylabel('Count')","723241e3":"df_train['fnlwgt'].iplot(kind='hist',\n                         bins=15,\n                         color='green',\n                         xTitle='fnlwgt',\n                         yTitle='Count',\n                         title='Distribution of Final weight')","049b2d72":"sns.kdeplot(df_train.loc[df_train['income'] == '<=50K', 'fnlwgt'], label='<=50K', shade=True)\nsns.kdeplot(df_train.loc[df_train['income'] == '>50K', 'fnlwgt'], label='>50K', shade=True)\n\nplt.xlabel('Final weight')\nplt.ylabel('Density')","a7480d39":"df_train['hours_per_week'].iplot(kind='hist',\n                                 bins=15,\n                                 color='red',\n                                 xTitle='Hours per week',\n                                 yTitle='Count',\n                                 title='Distribution of Hours per week')","d79d54b4":"sns.kdeplot(df_train.loc[df_train['income'] == '<=50K', 'hours_per_week'], label='<=50K', shade=True)\nsns.kdeplot(df_train.loc[df_train['income'] == '>50K', 'hours_per_week'], label='>50K', shade=True)\n\nplt.xlabel('Hours per week')\nplt.ylabel('Density')","3fd34cc8":"df_train['workclass'].value_counts()","e18a76d5":"df_train['workclass'].value_counts(normalize=True).sort_values().iplot(kind='barh',\n                                                                       linecolor='black',\n                                                                       opacity=0.7,\n                                                                       color='skyblue',\n                                                                       theme='pearl',\n                                                                       bargap=0.2,\n                                                                       gridcolor='white',\n                                                                       xTitle='Percentage',\n                                                                       yTitle='Workclass',\n                                                                       title='Distribution of Workclass')","de23b819":"df_workclass_target = df_train.groupby(['workclass', 'income'])['id'].count().to_frame().reset_index()\ndf_workclass_target.style.background_gradient(cmap='Reds')","b2c82f31":"sns.catplot(x='workclass', y='id', hue='income', data=df_workclass_target, kind='bar')\n\n# plt.gcf().set_size_inches(10, 8)\nplt.xlabel('Type of workclass')\nplt.xticks(rotation=45, fontsize='10', horizontalalignment='right')\nplt.ylabel('Count')","087a60db":"df_train['income'] = np.where(df_train['income'] == '>50K', 1, 0)\ny = df_train['income']\ndf_train = df_train.drop('income', axis=1)","a45139fd":"# LABEL ENCODING\nfor col in df_train.columns:\n    if df_train[col].dtype.name == 'object' or df_test[col].dtype.name == 'object':\n        le = LabelEncoder()\n        le.fit(list(df_train[col].values) + list(df_test[col].values))\n        df_train[col] = le.transform(list(df_train[col].values))\n        df_test[col]  = le.transform(list(df_test[col].values))","a9ec5d84":"# reduce_mem_usage()\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2   \n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n                    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n        \n    return df","57e4f966":"# REDUCE MEMORY USAGE\ndf_train = reduce_mem_usage(df_train)\ndf_test  = reduce_mem_usage(df_test)","91e5c228":"X_train = df_train.drop(['id'], axis=1)\nX_test  = df_test.drop(['id'], axis=1) \ny_train = y\n\nprint(\"X_train:\", X_train.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"X_test:\",  X_test.shape)","4665a35b":"def lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat  = np.round(y_hat)\n    \n    return 'f1', f1_score(y_true, y_hat, average='micro'), True","4c5565e0":"params = {\n          'objective': 'binary',\n          'max_depth': -1,\n          'n_jobs': -1,\n          'learning_rate': 0.01,\n          'num_leaves': 2**6,\n          'min_data_in_leaf': 30,\n          'boosting_type': 'gbdt',\n          'subsample_freq': 1,\n          'subsample': 0.7,\n          'n_estimators': 10000,\n          'verbose': -1,\n          'random_state': SEED,\n          }","9e4ff582":"%%time\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X_train.columns\nsplits = folds.split(X_train, y_train)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n\nfor fold_n, (trn_idx, val_idx) in enumerate(splits):\n    X_trn, X_val = X_train[columns].iloc[trn_idx], X_train[columns].iloc[val_idx]\n    y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n    \n    dtrain = lgb.Dataset(X_trn, label=y_trn)\n    dvalid = lgb.Dataset(X_val, label=y_val)\n    \n    clf = lgb.train(\n        params,\n        dtrain,\n        valid_sets = [dtrain, dvalid],\n        verbose_eval = 200,\n        early_stopping_rounds = 100,\n        feval = lgb_f1_score\n    )\n    \n    feature_importances[f'fold_{fold_n+1}'] = clf.feature_importance()\n    \n    y_pred_val = clf.predict(X_val) \n#     y_pred_val = np.where(y_pred_val >= 0.5, 1, 0)\n    y_pred_val = [int(v >= 0.5) for v in y_pred_val]\n    \n    y_oof[val_idx] = y_pred_val\n    print(f\"Fold {fold_n + 1} | F1 Score: {f1_score(y_val, y_pred_val, average='micro')}\")\n    \n    score += f1_score(y_val, y_pred_val, average='micro') \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_trn, X_val, y_trn, y_val\n    gc.collect()\n    \nprint(f\"\\nMean F1 score = {score}\")\nprint(f\"OOF F1 score = {f1_score(y, y_oof, average='micro')}\")","599fcb84":"sample_submission = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))\ny_preds = np.where(y_preds >= 0.5, 1, 0)\nsample_submission['prediction'] = y_preds\n\nsample_submission.head(10)","6ca0948a":"sample_submission.to_csv('baseline_submission.csv', index=False)","19021bea":"feature_importances['average'] = feature_importances[[f'fold_{fold_n+1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False), x='average', y='feature')\nplt.title(f'Feature Importances over {folds.n_splits} folds average')","c9a229c1":"## [KaKr] EDA with plotly + LGBM \n\n\uc608\uc804 \uce90\uae00 \ub300\ud68c\ub97c \ud1b5\ud574 \ubc30\uc6b4 plotly\uc640 seaborn \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \ud65c\uc6a9\ud574\uc11c \ub370\uc774\ud130 \uc2dc\uac01\ud654\ub97c \uc9c4\ud589\ud574\ubcf4\uace0 <br>\n\ud2b9\ubcc4\ud55c \uc804\ucc98\ub9ac \uc791\uc5c5\uc5c6\uc774 \uac00\uc7a5 \uae30\ubcf8\uc801\uc778 LightGBM Model\uc744 \ub9cc\ub4e4\uc5b4\ubd24\uc2b5\ub2c8\ub2e4. <br>\n\uac04\ub2e8\ud558\uac8c \uc791\uc131\ud55c \ub0b4\uc6a9\uc774\ub77c \ub2e4\ub978 \ubd84\ub4e4 notebook\uacfc \uac19\uc774 \ucc38\uace0\ud558\uc2dc\uba74\uc11c \ub300\ud68c\ub97c \uc9c4\ud589\ud558\uc2dc\uba74 \ub420 \uac83 \uac19\uc2b5\ub2c8\ub2e4. <br>\n\uc9c8\ubb38 \ubc0f \uc870\uc5b8\uc740 \uc5b8\uc81c\ub098 \ud658\uc601\uc785\ub2c8\ub2e4! ","dd5a0537":"### Workclass & Target","e185a5fd":".info() \ud568\uc218\ub97c \ud1b5\ud574 feature\uc758 type\uacfc \ud568\uaed8 null \uac12\uc774 \uc5c6\ub294 \uc815\ubcf4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4!","4feb2d1c":"### Target column","e1f0d6e4":"\uc131\ubcc4\ub9c8\ub2e4 \uc804\uccb4 \ube44\uc728\uc744 \uace0\ub824\ud588\uc744 \ub54c, \ub0a8\uc131\uc758 \uc18c\ub4dd\uc774 \ub192\uc740 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4.","7b8ff3fd":"\ub300\uccb4\uc801\uc73c\ub85c \ub098\uc774\uac00 \ub9ce\uc740 \uc0ac\ub78c\uc774 \uc18c\ub4dd\ub3c4 \ub9ce\uc740 \uacbd\ud5a5\uc744 \ubcf4\uc774\ub124\uc694!","ab4124b7":"## 2. Data Exploration","b097a2be":"- `id`\n- `age` : \ub098\uc774\n- `workclass` : \uace0\uc6a9 \ud615\ud0dc\n- `fnlwgt` : \uc0ac\ub78c \ub300\ud45c\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uac00\uc911\uce58 (final weight\uc758 \uc57d\uc790)\n- `education` : \uad50\uc721 \uc218\uc900\n- `education_num` : \uad50\uc721 \uc218\uc900 \uc218\uce58\n- `marital_status`: \uacb0\ud63c \uc0c1\ud0dc\n- `occupation` : \uc5c5\uc885\n- `relationship` : \uac00\uc871 \uad00\uacc4\n- `race` : \uc778\uc885\n- `sex` : \uc131\ubcc4\n- `capital_gain` : \uc591\ub3c4 \uc18c\ub4dd\n- `capital_loss` : \uc591\ub3c4 \uc190\uc2e4\n- `hours_per_week` : \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n- `native_country` : \uad6d\uc801\n- `income` : \uc218\uc775 (\uc608\uce21\ud574\uc57c \ud558\ub294 \uac12)\n    - `>50K` : 1\n    - `<=50K` : 0","ef31d0b3":"## 3. Modeling","28a424fa":"\uc8fc\ub2f9 40\uc2dc\uac04\uc815\ub3c4\ub85c \uc77c\ud558\ub294 \uadfc\ub85c\uc790\ub4e4\uc774 \ub9ce\uc2b5\ub2c8\ub2e4. <br>\n\ub300\uccb4\ub85c \uc18c\ub4dd\uc774 \ub192\uc740 \uc0ac\ub78c\ub4e4\uc740 40\uc2dc\uac04 \uc774\uc0c1 \uc77c\uc744 \ud558\ub294 \ud3b8\uc774\ub124\uc694.","4fcf3480":"\uc758\ubbf8\uac00 \uac00\uc7a5 \uad81\uae08\ud55c \ubcc0\uc218\uc5ec\uc11c \ub9c8\ucc2c\uac00\uc9c0\ub85c target \ubcc0\uc218\uc640 \ube44\uad50\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","526f325d":"## 1. Import libraries and Load datasets.","42ebf02a":"### Unique ID","39ad5298":"### Gender & Target","27084f44":"\uadf8\ub798\ud504\uc758 \uc6b0\uce21 \ud558\ub2e8\uc5d0 \uc788\ub294 `Export to plot.ly`\ub97c \ub204\ub974\uc2dc\uba74 \uc6d0\ud558\ub294\ub300\ub85c \uadf8\ub798\ud504\ub97c \uc880 \ub354 \ud29c\ub2dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br>\n\ud55c \ubc88 \ud574\ubcf4\uc2dc\ub294\uac78 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4~!","cde09602":"\ub530\ub85c \uc804\ucc98\ub9ac \uacfc\uc815\uc5c6\uc774 LightGBM Model\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","403d0563":"## 4. Submission","40e8566d":"- \uc5ec\ub7ec \uc790\ub8cc\ub97c \ucc38\uace0\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc758 label \ube44\uc728\uc774 \ubd88\uade0\ud615\ud558\uc9c0 \uc54a\ub2e4\uace0 \ud310\ub2e8\ud574 F1 score\uc758 \uacc4\uc0b0 \ubc29\ubc95\uc744 weighted \ud3c9\uade0\uc5d0\uc11c micro \ud3c9\uade0\uc73c\ub85c \uc218\uc815\ud588\uc2b5\ub2c8\ub2e4.","aac0b152":"### Hours per week & Target","e1a1e871":"### Age Distribution\n\n\uba3c\uc800 \uac00\uc7a5 \ub9cc\ub9cc\ud55c \ub098\uc774\uc640 \uc131\ubcc4\ubd80\ud130 \uac74\ub4dc\ub824\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","e66e2a51":"\ud2b9\uc774\ud558\uac8c\ub3c4 \ubd84\ud3ec\uc758 \ucc28\uc774\uac00 \uac70\uc758 \uc5c6\uc2b5\ub2c8\ub2e4. <br>\n\ud06c\uac8c \uc911\uc694\uc2dc\ub420 \ub9cc\ud55c \ubcc0\uc218\ub294 \uc544\ub2cc \uac83 \uac19\ub124\uc694.","329a1c85":"\ubc31\uc778\uc758 \ube44\uc728\uc774 \uc555\ub3c4\uc801\uc73c\ub85c \ub9ce\ub124\uc694. <br>\n\uadf8\ub798\ub3c4 target \ubcc0\uc218\uc640 \ube44\uad50\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","9fe611c7":"\uc81c notebook\uc5d0\uc11c \ub2e4\ub8e8\uc9c0 \uc54a\uc740 feature\ub4e4\uc740 \uc9c1\uc811 \ud55c\ubc88 \ud574\ubcf4\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4~!","75a3046e":"\ub370\uc774\ud130\uc758 \ud06c\uae30\ub3c4 \uc791\uc740 \ud3b8\uc774\uace0 feature\uc758 \uac1c\uc218\ub3c4 \uc801\uc740 \uac04\ub2e8\ud55c dataset\uc785\ub2c8\ub2e4.","d9ba25dd":"### Race & Target","e78ed562":"### Hours per week Distribution","b3d3ee18":"### Final weight & Target","ab852722":"\uc911\ubcf5\ub41c \ub370\uc774\ud130 \ud3ec\uc778\ud2b8\ub294 \uc5c6\uae30\uc5d0 \uc774\ub97c \ud1b5\ud574 target \ubcc0\uc218\uc758 \ubd84\ud3ec\ub97c \ud655\uc778\ud574\ubcf8 \uacb0\uacfc, \ub300\ub7b5 3\ubc30\uc815\ub3c4 \ucc28\uc774\uac00 \ub098\ub124\uc694. <br>\n\uc774\ub97c Plotly \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud574 \uc2dc\uac01\ud654\ud574\ubd05\uc2dc\ub2e4.","d8d8b84f":"### Age & Target","d9e9ad82":"\uc778\uc885\ubcc4 \ubd88\uade0\ud615\uc774 \uc2ec\ud558\ub2e4\ubcf4\ub2c8 \ud06c\uac8c \uc8fc\ubaa9\ud560\ub9cc\ud55c \uc694\uc18c\ub294 \ubcf4\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.","273264ea":"### Race Distribution","b8b6f681":"### Workclass","31a4772d":"### Gender Distribution","b6b4127c":"### Final weight Distribution"}}