{"cell_type":{"8371af89":"code","46f4dd0e":"code","6d7a093a":"code","e3399d84":"code","50b53052":"code","903492cb":"code","73749aaf":"code","28176034":"code","a20b7d79":"code","2f876cd6":"code","041efe1b":"code","7972541a":"code","58c79ec1":"code","a8bcedc2":"code","dc181188":"code","d7e711fc":"code","b4866bb3":"code","ecdeda36":"code","9115c643":"code","fc0346b2":"code","3f86c562":"code","c2c0ed44":"code","2450379d":"code","acf0ee56":"code","ad7fe513":"code","b60a24fd":"code","70e4303c":"code","c2c7aac2":"code","bbed4d19":"code","1d5c6f97":"code","b1d852a1":"code","6a46af46":"code","91d04bec":"code","6ee07771":"code","c9864a50":"code","3d2ff44e":"code","b5a09800":"code","620b0d75":"code","34fec187":"code","aae91a45":"code","8b344acd":"code","8d692c43":"code","e2d9fd95":"code","b79abe98":"code","1accf2c1":"code","1dbc7fd3":"code","148639b7":"code","5d486fad":"code","530d89a5":"code","b46a3d54":"code","c80eae80":"code","5a0cbc9e":"code","7b090058":"code","93305662":"code","105670d3":"markdown","473e3427":"markdown","4f02d58e":"markdown","ea5f15af":"markdown","e183b42a":"markdown","be07bd2f":"markdown"},"source":{"8371af89":"DATA_DIR = '..\/input\/animefacedataset'","46f4dd0e":"image_size = 64\nbatch_size = 128\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)","6d7a093a":"import os\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nimport torch\nimport torch.nn as nn\nimport cv2\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e3399d84":"train_ds = ImageFolder(DATA_DIR, transform=tt.Compose([\n    tt.Resize(image_size),\n    tt.CenterCrop(image_size),\n    tt.ToTensor(),\n    tt.Normalize(*stats)]))","50b53052":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)","903492cb":"def denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]\n\ndef show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break","73749aaf":"show_batch(train_dl)","28176034":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","a20b7d79":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","2f876cd6":"train_dl = DeviceDataLoader(train_dl, device)","041efe1b":"discriminator = nn.Sequential(\n    # in: 3 x 64 x 64\n\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(),\n    nn.Sigmoid())","7972541a":"discriminator = to_device(discriminator, device)","58c79ec1":"latent_size = 128","a8bcedc2":"generator = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 64 x 64\n)","dc181188":"xb = torch.randn(batch_size, latent_size, 1, 1) \nfake_images = generator(xb)\nprint(fake_images.shape)\nshow_images(fake_images)","d7e711fc":"generator = to_device(generator, device)","b4866bb3":"sample_dir = 'generated'\nos.makedirs(sample_dir, exist_ok=True)","ecdeda36":"fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)","9115c643":"fixed_latent.shape","fc0346b2":"def save_samples(index, latent_tensors, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = 'generated-images.png'.format(index)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n    print('Saving', fake_fname)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n    return fake_images.cpu().detach()","3f86c562":"model = {\n    \"discriminator\": discriminator.to(device),\n    \"generator\": generator.to(device)\n}\n\ncriterion = {\n    \"discriminator\": nn.BCELoss(),\n    \"generator\": nn.BCELoss()\n}\nlr = 0.0002\nepochs = 40\n","c2c0ed44":"!nvidia-smi","2450379d":"model[\"discriminator\"].train()\nmodel[\"generator\"].train()\ntorch.cuda.empty_cache()\n\n# Losses & scores\nlosses_g = []\nlosses_d = []\nreal_scores = []\nfake_scores = []\nsaved_samples = []\n\n# Create optimizers\noptimizer = {\n    \"discriminator\": torch.optim.Adam(model[\"discriminator\"].parameters(), \n                                      lr=lr, betas=(0.5, 0.999)),\n    \"generator\": torch.optim.Adam(model[\"generator\"].parameters(),\n                                  lr=lr, betas=(0.5, 0.999))\n}\n\nfor epoch in range(epochs):\n    loss_d_per_epoch = []\n    loss_g_per_epoch = []\n    real_score_per_epoch = []\n    fake_score_per_epoch = []\n    for real_images, _ in tqdm(train_dl):\n        # Train discriminator\n        # Clear discriminator gradients\n        optimizer[\"discriminator\"].zero_grad()\n\n        # Pass real images through discriminator\n        real_preds = model[\"discriminator\"](real_images)\n        real_targets = torch.ones(real_images.size(0), 1, device=device)\n        real_loss = criterion[\"discriminator\"](real_preds, real_targets)\n        cur_real_score = torch.mean(real_preds).item()\n\n        # Generate fake images\n        latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n        fake_images = model[\"generator\"](latent)\n\n        # Pass fake images through discriminator\n        fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n        fake_preds = model[\"discriminator\"](fake_images)\n        fake_loss = criterion[\"discriminator\"](fake_preds, fake_targets)\n        cur_fake_score = torch.mean(fake_preds).item()\n\n        real_score_per_epoch.append(cur_real_score)\n        fake_score_per_epoch.append(cur_fake_score)\n\n        # Update discriminator weights\n        loss_d = real_loss + fake_loss\n        loss_d.backward()\n        optimizer[\"discriminator\"].step()\n        loss_d_per_epoch.append(loss_d.item())\n\n\n        # Train generator\n        # Clear generator gradients\n        optimizer[\"generator\"].zero_grad()\n\n        # Generate fake images\n        latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n        fake_images = model[\"generator\"](latent)\n\n        # Try to fool the discriminator\n        preds = model[\"discriminator\"](fake_images)\n        targets = torch.ones(batch_size, 1, device=device)\n        loss_g = criterion[\"generator\"](preds, targets)\n\n        # Update generator weights\n        loss_g.backward()\n        optimizer[\"generator\"].step()\n        loss_g_per_epoch.append(loss_g.item())\n\n    # Record losses & scores\n    losses_g.append(np.mean(loss_g_per_epoch))\n    losses_d.append(np.mean(loss_d_per_epoch))\n    real_scores.append(np.mean(real_score_per_epoch))\n    fake_scores.append(np.mean(fake_score_per_epoch))\n\n    # Log losses & scores (last batch)\n    print(\"Epoch [{}\/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n        epoch+1, epochs, \n        losses_g[-1], losses_d[-1], real_scores[-1], fake_scores[-1]))\n\n    # Save generated images\n    if epoch+1 in [5, 10, 20, 40]:\n        saved_samples.append(generator(fixed_latent).cpu().detach().numpy())\n        print('Samples are saved')","acf0ee56":"saved_samples = np.array(saved_samples)\nsaved_samples.shape","ad7fe513":"real_imgs = []\nfor images, _ in train_dl:\n    real_imgs = images[:64].cpu().detach().numpy()\n    break\nreal_imgs.shape","b60a24fd":"real_imgs.shape","70e4303c":"show_images(torch.from_numpy(real_imgs)) # real images","c2c7aac2":"show_images(torch.from_numpy(saved_samples[0])) # generated images when batch = 5","bbed4d19":"show_images(torch.from_numpy(saved_samples[1])) # generated images when batch = 10","1d5c6f97":"show_images(torch.from_numpy(saved_samples[2])) # generated images when batch = 20","b1d852a1":"show_images(torch.from_numpy(saved_samples[3])) # generated images when batch = 40","6a46af46":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport pandas as pd\n# for loading\/processing the images  \nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.applications.vgg16 import preprocess_input \n\n# models \nfrom keras.applications.vgg16 import VGG16 \nfrom keras.models import Model\n\n# clustering and dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# for everything else\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport pandas as pd\nimport pickle","91d04bec":"import keras\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, UpSampling2D, Conv2D, MaxPooling2D, Activation, Flatten\n\nbase_model = VGG16(weights='imagenet', input_shape=(64, 64, 3), include_top=False)\n\nbase_out = base_model.output\nflt = Flatten()(base_out)\n\n# dns = Dense(units=100)(flt)\n\nmodel = Model(base_model.input, flt) #Let's leave it this way so that we keep al the information about image, also stardatization will be made further\n\n\n\n# model.layers[-1:][0].get_weights()[0]\n\n# shape = model.layers[-1:][0].get_weights()[0].shape #get shape of the last dense layer\n# print(shape)\n\n# weights = np.ones(shape) #(2048, 100) shaped ones\n# bias = np.zeros((100,)) #(100,) shaped zeros\n# model.layers[-1].set_weights([weights, bias]) #setting new weights to the last dense layer","6ee07771":"model.summary()","c9864a50":"real_imgs = real_imgs.transpose(0, 2, 3, 1)\nreal_imgs.shape # DO NOT RUN THESE ONES SEVERAL TIMES","3d2ff44e":"saved_samples = saved_samples.transpose(0, 1, 3, 4, 2)\nsaved_samples.shape # DO NOT RUN THESE ONES SEVERAL TIMES","b5a09800":"all_classes = np.append(saved_samples, [real_imgs], axis=0)\nall_classes.shape","620b0d75":" def extract_features(img, model):\n    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n    reshaped_img = img.reshape(1,64,64,3) \n    # prepare image for model\n    imgx = preprocess_input(reshaped_img)\n    # get the feature vector\n    features = model.predict(imgx, use_multiprocessing=True)\n    return features","34fec187":"dataset = []\nfor i in range(len(all_classes)):\n    print(i)\n    for j in range(len(all_classes[0])):\n        dataset.append(extract_features(all_classes[i,j], model)[0])","aae91a45":"dataset = np.array(dataset)\ndataset.shape","8b344acd":"dataset.mean()","8d692c43":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndataset = scaler.fit_transform(dataset)\ndataset.mean()","e2d9fd95":"kmeans = KMeans(n_clusters=5, random_state=22)","b79abe98":"%%time\nkmeans.fit(dataset)","1accf2c1":"clusters = kmeans.predict(dataset)\nclasses = np.array([0]*64+[1]*64+[2]*64+[3]*64+[4]*64)","1dbc7fd3":"np.unique(clusters, return_counts=True)","148639b7":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nconfusion_matrix(classes, clusters)","5d486fad":"clusters","530d89a5":"new_clusters = np.empty(5, dtype=int)\nnew_clusters[[0,1,2,3,4]]=[2,4,1,3,0]\nclusters = new_clusters[clusters] # The values have been replaced","b46a3d54":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(confusion_matrix(classes, clusters))\nprint(classification_report(classes, clusters))","c80eae80":"kmeans = KMeans(n_clusters=2, random_state=22)\nkmeans.fit(dataset[-128:])\npred = kmeans.predict(dataset[-128:])","5a0cbc9e":"pred","7b090058":"classes = np.array([1]*64+[0]*64)\nclasses","93305662":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(confusion_matrix(pred, classes), '\\n\\n')\nprint(classification_report(pred, classes))","105670d3":"### Results are not that bad, now let's see if it can distinguish between real images and generated by 40 epochs images","473e3427":"#### I am using Kaggle notebook so that I don't have to download the data and GPUs here are much faster","4f02d58e":"##### Judging by the confusion matrix cluster values should be adjusted (calibrated)\n##### Let's do this as follows: \n##### cluster=0 gets value 2 (because 21 observations of 64 of 2nd class are concentrated at cluster 0)\n#####                           cluster=1 gets value 4\n#####                           cluster=2 gets value 1\n#####                           cluster=3 gets value 3\n#####                           cluster=4 gets value 0","ea5f15af":"#### Actually samples are not saved they are just recorded into python list","e183b42a":"#### Now let's cluster these images","be07bd2f":"### Great results! It does not struggle identifying fake images at all, although there are some missclassifications of real images"}}