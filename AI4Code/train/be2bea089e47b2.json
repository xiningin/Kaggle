{"cell_type":{"4f3d9e65":"code","8052b1f2":"code","b2fbcae5":"code","8f54aee7":"code","8a8db134":"code","92d35adf":"code","8da39b13":"code","4ae1a07d":"code","b01ed3c5":"code","c52bc9ab":"code","064c3877":"code","785cc9d8":"code","1dea48ac":"code","43bd40ee":"code","cfefc7c9":"code","d2706bed":"markdown","b82d7b38":"markdown","bf3c260e":"markdown"},"source":{"4f3d9e65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8052b1f2":"import os\nimport pandas as pd\nimport re","b2fbcae5":"mydata = pd.read_csv(\"..\/input\/Text - Input.csv\", header = 0)\nprint(mydata.shape)\n\n# Use 10000 for testing\nmydata = mydata[:10000] \nmydata.head()","8f54aee7":"def clean_text(string_in):\n    string_in = re.sub(r\"@\\w+\", \"\", string_in) # Remove twitter handle\n    string_in = re.sub(r\"\\d\", \"\", string_in)   # Remove numbers  \n    string_in = re.sub(r\"_+\", \"\", string_in)   # Remove consecutive underscores\n    string_in = string_in.lower()              # Tranform to lower case    \n    \n    return string_in.strip()\n\nmydata[\"tweet_text_cleaned\"] = mydata.tweet_text.apply(clean_text)","8a8db134":"mydata","92d35adf":"import nltk","8da39b13":"from nltk.tokenize import RegexpTokenizer\npreprocessed = [\" \".join(RegexpTokenizer(r'\\w+').\\\n                         tokenize(mydata.tweet_text_cleaned[idx])) \\\n                for idx in mydata.index]","4ae1a07d":"preprocessed","b01ed3c5":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text \n\ncustom_stop_words = []\nmy_stop_words = text.ENGLISH_STOP_WORDS.union(custom_stop_words)\n\nvectorizer = TfidfVectorizer(min_df = 1, ngram_range = (1,1), \n                             stop_words = my_stop_words)\n\ntfidf = vectorizer.fit_transform(preprocessed)\nprint(\"Created document-term matrix of size %d x %d\" % (tfidf.shape[0],tfidf.shape[1]))","c52bc9ab":"my_stop_words","064c3877":"from sklearn import decomposition\nimport numpy as np\nnmf = decomposition.NMF(init = 'nndsvd', n_components = 3, max_iter = 200)\nW = nmf.fit_transform(tfidf)\nH = nmf.components_\nprint(\"Generated factor W of size %s and factor H of size %s\" % ( str(W.shape), str(H.shape)))\n\nfeature_names = vectorizer.get_feature_names()\nn_top_words = 10\n\n# Print top words in each topic\nfor topic_idx, topic in enumerate(H):\n    print(\"Topic #%d:\" % topic_idx)\n    print(\" \".join([feature_names[i]\n                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()","785cc9d8":"mydf = pd.DataFrame({\"feature_name\": feature_names})\n\nfor topic_idx, topic in enumerate(H):\n    mydf[\"topic_\" + str(topic_idx)] = topic\n\nmylist = list(mydf.itertuples())\n\nmywords_topic1 = []\nmywords_topic2 = []\nmywords_topic3 = []\n\nfor order_id, key, num1, num2, num3 in mylist:\n    mywords_topic1.append((key, num1))\n    mywords_topic2.append((key, num2))\n    mywords_topic3.append((key, num3))\n\nmywords_topic1 = sorted(mywords_topic1, key=lambda myword: myword[1], reverse=True)\nmywords_topic2 = sorted(mywords_topic2, key=lambda myword: myword[1], reverse=True)\nmywords_topic3 = sorted(mywords_topic3, key=lambda myword: myword[1], reverse=True)\n","1dea48ac":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndef draw_wordcloud(dict, topic_number):\n    wc = WordCloud(max_words=1000)    \n    wordcloud = WordCloud().generate_from_frequencies(dict)\n    \n    plt.title('Topic %s' %str(topic_number), size = 16)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")        \n    plt.show()\n\ndraw_wordcloud(dict(mywords_topic1), topic_number=1)\ndraw_wordcloud(dict(mywords_topic2), topic_number=2)\ndraw_wordcloud(dict(mywords_topic3), topic_number=3)","43bd40ee":"# Prediction example\ntext_new = preprocessed[0:5]\ntfidf_new = vectorizer.transform(text_new)\nW_new = nmf.transform(tfidf_new)","cfefc7c9":"W_new","d2706bed":"# Tokenization","b82d7b38":"# Identifying Topics","bf3c260e":"# Making Predictions"}}