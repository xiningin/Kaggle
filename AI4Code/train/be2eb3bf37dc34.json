{"cell_type":{"1e9bf067":"code","1a163d44":"code","b70b7326":"code","848fe0a9":"code","e4c54c7a":"code","daf36c29":"code","76dab359":"code","fddc1f3e":"code","8e9e199a":"code","37914cc9":"code","dde7ecf9":"code","3056bafa":"code","a3f1fb6b":"code","d67d86d9":"code","3a4062b2":"code","4e8ddb9f":"code","8bd797d4":"code","741d1979":"code","a7f9692f":"code","138b552f":"code","a08ed1de":"code","c84d1e7f":"code","b673602d":"code","1ce74d36":"code","a25f45df":"code","8669328d":"code","b7ae4fe9":"code","0d121a9d":"code","31cdca90":"code","a262f732":"code","3d64ed7a":"code","7e90d6b9":"code","8c86cab5":"code","02a62596":"code","82903d27":"code","5c7c8710":"code","71fd987c":"code","4d630000":"code","9f8cf4f3":"code","f04070d6":"code","259ad63c":"code","521f1f8e":"code","403908e4":"code","c09f63ee":"code","cae796ad":"code","71b84a18":"code","aeee8621":"code","93fa70ac":"code","5446a038":"code","46d794ed":"code","e6b855e1":"code","b79e7fd2":"code","e8bbf5c6":"code","dd0563fe":"code","f77b8819":"code","3446a77b":"code","d1de8b94":"code","1af249ee":"code","c08f0009":"code","87209493":"code","5f321935":"code","9154337d":"code","06e66efd":"code","aad6eb31":"code","42348f41":"code","ca311968":"code","b2c56a35":"code","7ef9ce82":"code","c806a7dd":"code","65150a64":"code","30078403":"code","e66db9ca":"code","742c96c0":"code","ca22f171":"code","6db1f969":"code","6853b517":"code","e26f000b":"code","8fd1e723":"code","11b15c07":"code","a0489c24":"code","589de6a2":"code","51bd0fae":"code","893c82e6":"code","24a85eac":"code","b54c9610":"code","8d045cfc":"code","d49caea7":"code","3c692e88":"code","90a4a807":"code","be78f699":"code","7cc77e8c":"code","35a8f09e":"code","4b1800d1":"code","e6d58c32":"code","0a0ea602":"code","b5090345":"code","2c523027":"code","6c5291c6":"code","239758c4":"code","1fa898a6":"code","e2107859":"code","5d65c079":"code","4bf9e1d6":"code","094ed2a8":"code","d2caa112":"code","9a5df3fb":"code","094493fe":"code","baf82d28":"code","a9eb26a6":"code","0be4c7dd":"code","2ee6847e":"code","323713bc":"code","49cfa05a":"code","66b72d4c":"code","a01fc707":"code","05a46dce":"code","c221955c":"code","990e3a00":"code","9b3b0be9":"code","2e807999":"code","8d644a0f":"code","73f8462c":"code","f3290504":"code","39ecade8":"code","92527bad":"code","e4bc5aba":"code","b20b544a":"code","9656e086":"code","9c7ff15b":"markdown","0261712d":"markdown","b10d6eea":"markdown","b1f2969e":"markdown","64818401":"markdown","72de1880":"markdown","73337a19":"markdown","869cb7ec":"markdown","b0b00e86":"markdown","8ba97346":"markdown","eca33df7":"markdown","3b554580":"markdown","607a53da":"markdown","da3cf3fc":"markdown","f384dc31":"markdown","7bb2d9ea":"markdown","082de617":"markdown","4bc652c2":"markdown","c1ad8d55":"markdown","d6392740":"markdown","33c6e390":"markdown","251f5e30":"markdown","a1fb1a0e":"markdown","91640928":"markdown","cd94a513":"markdown","5d5d1285":"markdown","f781cd19":"markdown","f3312671":"markdown","19811fcd":"markdown","528d9103":"markdown","c3e55567":"markdown","fb233f45":"markdown","2c08a39a":"markdown","e8fd9c77":"markdown","f2d6ee1a":"markdown","bcd8a8f5":"markdown","863885bc":"markdown","5c945222":"markdown","aa5cc150":"markdown","58644d8b":"markdown","ba21a36f":"markdown","a092bacf":"markdown","545dd2a2":"markdown","2b8981ee":"markdown"},"source":{"1e9bf067":"pip install dataprep","1a163d44":"#Importing the Required Librarires\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\nfrom dataprep.eda import plot, plot_correlation, create_report, plot_missing\npd.options.display.max_columns = 100\npd.options.display.max_rows = 900\npd.set_option('float_format', '{:f}'.format)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n%matplotlib inline\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, cohen_kappa_score,auc\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b70b7326":"#Reading two data sets and printing top 5 rows\ndf= pd.read_csv(\"..\/input\/fraud-detection\/fraudTrain.csv\")\ndf.head()","848fe0a9":"df.info()","e4c54c7a":"#Checking null values\ndf.isnull().sum()","daf36c29":"# print the shape of dataset\ndf.shape","76dab359":"df_fraud=df[['is_fraud','trans_date_trans_time']].groupby('is_fraud').count().reset_index()\ndf_fraud.columns=['is_fraud','count']\ndf_fraud['percentage']=(df_fraud['count']\/df_fraud['count'].sum())*100\ndf_fraud","fddc1f3e":"#Finiding unique values in each column\ndf.nunique()","8e9e199a":"# plot(df)","37914cc9":"#create_report(df)","dde7ecf9":"# plot(df, \"state\")","3056bafa":"plot(df, \"amt\", \"state\")","a3f1fb6b":"#  Converting data type of trans_date_trans_time to datetime\ndf['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n\ndf.dtypes['trans_date_trans_time']","d67d86d9":"# Derive 'Transaction Hour' Feature from 'Transaction Time' Feature\ndf['trans_hour'] = df['trans_date_trans_time'].dt.hour\ndf['trans_hour']","3a4062b2":"# Derive 'Day of Week' Feature from 'Transaction Time' Feature\ndf['day_of_week'] = df['trans_date_trans_time'].dt.day_name()\ndf['day_of_week']","4e8ddb9f":"# let's print top 5 rows from the dataset\ndf.head()","8bd797d4":"# Derive 'Year Month' Feature from 'Transaction Time' Feature\ndf['year_month'] = df['trans_date_trans_time'].dt.to_period('M')\ndf['year_month']","741d1979":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\ndf['category'].value_counts().plot.bar();\nplt.subplot(1,2,2)\ndf['day_of_week'].value_counts().plot.bar();","a7f9692f":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\ndf['gender'].value_counts().plot.bar();\nplt.subplot(1,2,2)\ndf['year_month'].value_counts().plot.bar();\n","138b552f":"df.state.value_counts(normalize=True)","a08ed1de":"df.job.value_counts(normalize=True,ascending=False)","c84d1e7f":"df['dob'] = pd.to_datetime(df['dob'])\n\ndf['age'] = np.round((df['trans_date_trans_time'] - df['dob'])\/np.timedelta64(1,'Y'))\n\ndf['age'].describe()","b673602d":"##  Distribution of amt\npd.concat(\n[df['amt'].describe(percentiles = [0.5,0.95,0.999])\\\n.reset_index().rename(columns={'index': 'Row Type', 'amt':'Overall Amt Distribution'}),\ndf.loc[df['is_fraud']==0,['amt']].describe(percentiles = [0.5,0.95,0.999])\\\n.reset_index(drop = 1).rename(columns={'amt':'Non Fraud Amt Distribution'}),\ndf.loc[df['is_fraud']==1,['amt']].describe(percentiles = [0.5,0.95,0.999])\\\n.reset_index(drop = 1).rename(columns={'amt':'Fraud Amt Distribution'})],\naxis=1\n)","1ce74d36":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nax[0].hist(df[df['amt']<=1500]['amt'], bins=50)\nax[1].hist(df[(df['is_fraud']==0) & (df['amt']<=1500)]['amt'], bins=50)\nax[2].hist(df[(df['is_fraud']==1) & (df['amt']<=1500)]['amt'], bins=50)\n\nax[0].set_title('Overall Amt Distribution')\nax[1].set_title('Non Fraud Amt Distribution')\nax[2].set_title('Fraud Amt Distribution')\n\nax[0].set_xlabel('Transaction Amount')\nax[0].set_ylabel('#.of Transactions')\n\nax[1].set_xlabel('Transaction Amount')\nax[2].set_xlabel('Transaction Amount')\nplt.show()","a25f45df":"# check outliers in graph\nnum_cols=['amt']\nplt.figure(figsize=[10,8])\nfor ind, col in enumerate(num_cols):\n    plt.subplot(1,2,ind+1)\n    df[col].plot.box()\n    plt.title(col)\nplt.show()","8669328d":"# Year Month vs Number of Transactions\ndf_timeline01 = df.groupby(df['year_month'])[['trans_num','cc_num']].nunique().reset_index()\ndf_timeline01.columns = ['year_month','num_of_transactions','customers']\ndf_timeline01","b7ae4fe9":"x = np.arange(0,len(df_timeline01),1)\n\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nax.plot(x,df_timeline01['num_of_transactions'])\nax.set_xticks(x)\nax.set_xticklabels(df_timeline01['year_month'])\n\nax.set_xlabel('Year Month')\nax.set_ylabel('Num of Transactions')\nplt.show()\n","0d121a9d":"## Year Month vs Number of Customers Done the Transactions\nx = np.arange(0,len(df_timeline01),1)\n\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nax.plot(x,df_timeline01['customers'])\nax.set_xticks(x)\nax.set_xticklabels(df_timeline01['year_month'])\n\nax.set_xlabel('Year Month')\nax.set_ylabel('Num of Customers')\nplt.show()\n","31cdca90":"## Fraud Transactions\ndf_fraud_transactions = df[df['is_fraud']==1]\n\ndf_timeline02 = df_fraud_transactions.groupby(df_fraud_transactions['year_month'])[['trans_num','cc_num']].nunique().reset_index()\ndf_timeline02.columns = ['year_month','num_of_fraud_transactions','fraud_customers']\ndf_timeline02","a262f732":"print(df_timeline02['num_of_fraud_transactions'].mean())","3d64ed7a":"x = np.arange(0,len(df_timeline02),1)\n\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nax.plot(x,df_timeline02['num_of_fraud_transactions'])\nax.set_xticks(x)\nax.set_xticklabels(df_timeline02['year_month'])\n\nax.set_xlabel('Year Month')\nax.set_ylabel('Num of Transactions')\nplt.show()","7e90d6b9":"x = np.arange(0,len(df_timeline02),1)\n\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nax.plot(x,df_timeline02['fraud_customers'])\nax.set_xticks(x)\nax.set_xticklabels(df_timeline02['year_month'])\n\nax.set_xlabel('Year Month')\nax.set_ylabel('Num of Customers')\nplt.show()","8c86cab5":"df_gender = df[['gender','trans_num']].groupby(['gender']).count().reset_index()\ndf_gender.columns = ['Gender','gender_count']\n\ndf_gender['percent'] = (df_gender['gender_count']\/df_gender['gender_count'].sum())*100\n\ndf_gender","02a62596":"plt.bar(df_gender['Gender'], df_gender['gender_count'], color=(0.2, 0.4, 0.6, 0.6),  \n        width = 0.4)\n\nplt.show()","82903d27":"df_fraud_gender = df[['gender','is_fraud','trans_num']].groupby(['gender','is_fraud']).count().reset_index()\ndf_fraud_gender.columns = ['Gender','is_fraud','count']\n\ndf_fraud_gender = df_fraud_gender.merge(df_gender[['Gender','gender_count']],how='inner',\\\n                                  left_on='Gender',right_on='Gender')\n\n\ndf_fraud_gender['percent_grp'] = (df_fraud_gender['count']\/df_fraud_gender['gender_count'])*100\n\n\ndf_fraud_gender","5c7c8710":"df_category = df[['category','trans_num']].groupby(['category']).count().reset_index()\ndf_category.columns = ['Category','category_count']\n\ndf_category['percent'] = (df_category['category_count']\/df_category['category_count'].sum())*100\n\ndf_category.sort_values(by = ['percent'], ascending=False)","71fd987c":"df_category = df_category.sort_values(by = ['percent'], ascending=False).reset_index()\ndf_category","4d630000":"fig = plt.figure(figsize = (20, 6)) \n\n\nplt.bar(df_category['Category'], df_category['category_count'], color=(0.2, 0.4, 0.6, 0.6),  \n        width = 0.4)\n\nplt.show()","9f8cf4f3":"df_fraud_category = df[['category','is_fraud','trans_num']].groupby(['category','is_fraud']).count().reset_index()\ndf_fraud_category.columns = ['Category','is_fraud','count']\n\ndf_fraud_category = df_fraud_category.merge(df_category[['Category','category_count','percent']],how='inner',\\\n                                  left_on='Category',right_on='Category')\n\n\ndf_fraud_category['percent_grp'] = (df_fraud_category['count']\/df_fraud_category['category_count'])*100\n","f04070d6":"df_fraud_category.sort_values(by = ['category_count'], ascending=False)","259ad63c":"df_fraud=df_fraud_category[df_fraud_category['is_fraud'] == 1].sort_values(by = ['percent_grp'])\ndf_fraud","521f1f8e":"fig = plt.figure(figsize = (20, 6)) \n\n\nplt.bar(df_fraud['Category'] , df_fraud['percent_grp'], color=(0.2, 0.4, 0.6, 0.6),  \n        width = 0.4)\n\nplt.show()","403908e4":"df.merchant.value_counts(normalize=True, ascending=False)","c09f63ee":"df_merchant = df[['merchant','trans_num']].groupby(['merchant']).count().reset_index()\ndf_merchant.columns = ['Merchant','merchant_count']\n\ndf_merchant['percent'] = (df_merchant['merchant_count']\/df_merchant['merchant_count'].sum())*100\n\ndf_merchant.sort_values(by = ['percent'], ascending=False)","cae796ad":"df_fraud_merchant = df[['merchant','is_fraud','trans_num']].groupby(['merchant','is_fraud']).count().reset_index()\ndf_fraud_merchant.columns = ['Merchant','is_fraud','count']\n\ndf_fraud_merchant = df_fraud_merchant.merge(df_merchant[['Merchant','merchant_count','percent']],how='inner',\\\n                                  left_on='Merchant',right_on='Merchant')\n\n\ndf_fraud_merchant['percent_grp'] = (df_fraud_merchant['count']\/df_fraud_merchant['merchant_count'])*100","71b84a18":"df_fraud_merchant[df_fraud_merchant['is_fraud'] == 1].sort_values(by = ['percent_grp'],ascending=False)","aeee8621":"category_onehot = pd.get_dummies(df.category, prefix='category', drop_first=True)\ngender_onehot = pd.get_dummies(df.gender, prefix='gender', drop_first=True)\nday_of_week_onehot = pd.get_dummies(df.day_of_week, prefix='week',drop_first=True)","93fa70ac":"df1 = pd.concat([df, category_onehot,gender_onehot,day_of_week_onehot], axis=1)","5446a038":"df1.head()","46d794ed":"def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees or in radians)\n\n    All (lat, lon) coordinates must have numeric dtypes and be of equal length.\n\n    \"\"\"\n    if to_radians:\n        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n\n    a = np.sin((lat2-lat1)\/2.0)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)\/2.0)**2\n\n    return earth_radius * 2 * np.arcsin(np.sqrt(a))","e6b855e1":"df1['dist'] = \\\n    haversine(df1['lat'], df1['long'],\n                 df1['merch_lat'], df1['merch_long'])","b79e7fd2":"df1['dist'].describe()","e8bbf5c6":"df1.dtypes","dd0563fe":"### Historic Variables\ndf1.index = pd.to_datetime(df1['trans_date_trans_time'])\ndf1 = df1.rename_axis(index={'trans_date_trans_time': 'time_index'})\ndf1 = df1.sort_index()\ndf1.head()","f77b8819":"df1['val_for_agg'] = 1","3446a77b":"##  60 days transactions by customer\ndf_hist_trans_60d = \\\n    df1 \\\n    .groupby(['cc_num'])['val_for_agg']\\\n    .rolling('60D')\\\n    .count()\\\n    .shift()\\\n    .reset_index()\\\n    .fillna(0)\n\ndf_hist_trans_60d.columns = ['cc_num','trans_date','hist_trans_60d']","d1de8b94":"df_hist_trans_60d['trans_date'] = df_hist_trans_60d['trans_date'].dt.date","1af249ee":"df_hist_trans_60d = df_hist_trans_60d.groupby(['cc_num','trans_date'])['hist_trans_60d'].min().reset_index()","c08f0009":"df_hist_trans_60d.head()","87209493":"## 24 hours order by customer\ndf_hist_orders_24h = \\\n    df1 \\\n    .groupby(['cc_num'])['val_for_agg']\\\n    .rolling('24H')\\\n    .count()\\\n    .shift()\\\n    .reset_index()\\\n    .fillna(0)\n\ndf_hist_orders_24h.columns = ['cc_num','trans_date_trans_time','hist_trans_24h']","5f321935":"df_hist_orders_24h.head()","9154337d":"## 24 hrs fraud historic transactions\ndf_hist_fraud_trans_24h = \\\n    df1[df1['is_fraud']== 1]\\\n    .groupby(['cc_num'])['val_for_agg']\\\n    .rolling('24H')\\\n    .count()\\\n    .shift()\\\n    .reset_index()\\\n    .fillna(0)\n\ndf_hist_fraud_trans_24h.columns = ['cc_num','trans_date_trans_time','hist_fraud_trans_24h']","06e66efd":"df_hist_fraud_trans_24h.head()","aad6eb31":"## 2 Hours Fraud by Customers\ndf_hist_fraud_trans_2h = \\\n    df1[df1['is_fraud']== 1]\\\n    .groupby(['cc_num'])['val_for_agg']\\\n    .rolling('2H')\\\n    .count()\\\n    .shift()\\\n    .reset_index()\\\n    .fillna(0)\n\ndf_hist_fraud_trans_2h.columns = ['cc_num','trans_date_trans_time','hist_fraud_trans_2h']","42348f41":"df_hist_fraud_trans_2h.head()","ca311968":"df_hist_trans_amt_avg_60d = \\\n    df1 \\\n    .groupby(['cc_num'])['amt']\\\n    .rolling('60D')\\\n    .mean()\\\n    .shift(1)\\\n    .reset_index()\\\n    .fillna(0)\n\ndf_hist_trans_amt_avg_60d.columns = ['cc_num','trans_date','hist_trans_avg_amt_60d']","b2c56a35":"df_hist_trans_amt_avg_60d['trans_date'] = df_hist_trans_amt_avg_60d['trans_date'].dt.date\n","7ef9ce82":"df_hist_trans_amt_avg_60d = df_hist_trans_amt_avg_60d.groupby(['cc_num','trans_date'])\\\n['hist_trans_avg_amt_60d'].min().reset_index()","c806a7dd":"df_hist_trans_amt_avg_60d.head(10)","65150a64":"df1['trans_date'] = df1['trans_date_trans_time'].dt.date","30078403":"df2 = df1.merge(df_hist_trans_60d,left_on = ['cc_num','trans_date'], \\\n          right_on = ['cc_num','trans_date'],how = 'left')","e66db9ca":"df2 = df2.merge(df_hist_orders_24h,left_on = ['cc_num','trans_date_trans_time'], \\\n          right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n\ndf2 = df2.merge(df_hist_fraud_trans_24h,left_on = ['cc_num','trans_date_trans_time'], \\\n          right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n\ndf2 = df2.merge(df_hist_fraud_trans_2h,left_on = ['cc_num','trans_date_trans_time'], \\\n          right_on = ['cc_num','trans_date_trans_time'],how = 'left')\n\ndf2 = df2.merge(df_hist_trans_amt_avg_60d,left_on = ['cc_num','trans_date'], \\\n          right_on = ['cc_num','trans_date'],how = 'left')","742c96c0":"df2[['hist_trans_60d','hist_trans_24h','hist_fraud_trans_24h','hist_fraud_trans_2h','hist_trans_avg_amt_60d']]= \\\ndf2[['hist_trans_60d','hist_trans_24h','hist_fraud_trans_24h','hist_fraud_trans_2h','hist_trans_avg_amt_60d']].fillna(0)","ca22f171":"df2.head()","6db1f969":"df2['hist_fraud_trans_24h'] = df2['hist_fraud_trans_24h'] - df2['hist_fraud_trans_2h']","6853b517":"cols = ['amt','city_pop', 'trans_hour',\n       'age', 'category_food_dining', 'category_gas_transport',\n       'category_grocery_net', 'category_grocery_pos',\n       'category_health_fitness', 'category_home', 'category_kids_pets',\n       'category_misc_net', 'category_misc_pos', 'category_personal_care',\n       'category_shopping_net', 'category_shopping_pos', 'category_travel',\n       'gender_M','week_Monday','week_Tuesday', 'week_Wednesday','week_Thursday',\n        'week_Saturday', 'week_Sunday','dist','hist_trans_60d','hist_trans_24h',\n       'hist_fraud_trans_24h','hist_trans_avg_amt_60d','is_fraud']","e26f000b":"corr = df2[cols].corr()","8fd1e723":"import seaborn as sn\n\nfig, ax = plt.subplots(figsize=(20,20))\nsn.heatmap(corr, annot=True)\nplt.show()","11b15c07":"import plotly\nimport plotly.graph_objects as go ","a0489c24":"df2.head()","589de6a2":"df_fraud_merchant = df[['merchant','is_fraud','trans_num']].groupby(['merchant','is_fraud']).count().reset_index()\ndf_fraud_merchant.columns = ['Merchant','is_fraud','count']\n\ndf_fraud_merchant = df_fraud_merchant.merge(df_merchant[['Merchant','merchant_count','percent']],how='inner',\\\n                                  left_on='Merchant',right_on='Merchant')\n\n\ndf_fraud_merchant['percent_grp'] = (df_fraud_merchant['count']\/df_fraud_merchant['merchant_count'])*100","51bd0fae":"df_job = df[['job','trans_num']].groupby(['job']).count().reset_index()\ndf_job.columns = ['Job','tran_count_by_job']\n\ndf_job['percent'] = (df_job['tran_count_by_job']\/df_job['tran_count_by_job'].sum())*100\n\ndf_job.sort_values(by = ['percent'], ascending=False)","893c82e6":"df_fraud_job = df[['job','is_fraud','trans_num']].groupby(['job','is_fraud']).count().reset_index()\ndf_fraud_job.columns = ['Job','is_fraud','count']\n\ndf_fraud_job =  df_fraud_job.merge(df_job[['Job','tran_count_by_job','percent']],how='inner',\\\n                                  left_on='Job',right_on='Job')\n\n\ndf_fraud_job['percent_grp'] = (df_fraud_job['count']\/df_fraud_job['tran_count_by_job'])*100","24a85eac":"job_plt_data = df_fraud_job.sort_values(by = [\"tran_count_by_job\"], ascending = False).head(20)","b54c9610":"job_plt_data","8d045cfc":"job_plt_data['label'] = 'Not Fraud'\njob_plt_data.loc[job_plt_data['is_fraud']==1,['label']]= 'Fraud'\njob_plt_data","d49caea7":"ne_grp = job_plt_data['Job'].unique()\nprint(ne_grp)\n\nrm_grp = job_plt_data['label'].unique()\nprint(rm_grp)","3c692e88":"# split features into Categorical and Numerical \ncatogrical = [x for x in df2.columns if df2[x].dtype == \"object\"]\nnumeric = [x for x in df2.columns if df2[x].dtype == \"float64\"]","90a4a807":"# filling null or missing values with mean or average \n\nfor i in numeric:\n    df2[i].fillna(df2[i].mean(), inplace = True)","be78f699":"df[\"dob\"] = pd.to_datetime(df['dob'])\ndf[\"dob_year\"] = df[\"dob\"].dt.year\ndf[\"dob_day\"] = df[\"dob\"].dt.day\ndf[\"dob_month\"] = df[\"dob\"].dt.month","7cc77e8c":"df[\"trans_date_trans_time\"] = pd.to_datetime(df['trans_date_trans_time'])\ndf[\"year\"] = df[\"trans_date_trans_time\"].dt.year\ndf[\"day\"] = df[\"trans_date_trans_time\"].dt.day\ndf[\"month\"] = df[\"trans_date_trans_time\"].dt.month","35a8f09e":"# dropping un-necessary features\ndf2.drop(['Unnamed: 0','trans_date_trans_time','dob','year_month'],axis=1,inplace=True)\n","4b1800d1":"df2.head()","e6d58c32":"# checking outliers \n#df2.plot(kind='box',subplots=True,layout=(2,23))","0a0ea602":"#Removing Outliers\n#from scipy.stats import zscore\n#z=np.abs(zscore(df2))\n\n#threshold=3\n#np.where((z>3))\n#df2_new=df2[(z<3).all(axis=1)]#removing outliers\n\n#print('Original dataset shape',df2.shape())\n#print('after removing outliers dataset shape',df2_new.shape())","b5090345":"#from sklearn.preprocessing import LabelEncoder\n\n#df3 = df2.copy()\n#label_encoder = LabelEncoder()\n#for col in catogrical:\n #   df[col] = label_encoder.fit_transform(df[col])\n    \n#df","2c523027":"from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.svm import SVC, LinearSVC\nimport seaborn as sns\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\n#Building XG Boost Model\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_curve,roc_auc_score\n","6c5291c6":"X_cols = ['amt','city_pop', 'trans_hour',\n       'age', 'category_food_dining', 'category_gas_transport',\n       'category_grocery_net', 'category_grocery_pos',\n       'category_health_fitness', 'category_home', 'category_kids_pets',\n       'category_misc_net', 'category_misc_pos', 'category_personal_care',\n       'category_shopping_net', 'category_shopping_pos', 'category_travel',\n       'gender_M','week_Monday','week_Tuesday', 'week_Wednesday','week_Thursday',\n        'week_Saturday', 'week_Sunday','dist','hist_trans_60d','hist_trans_24h',\n       'hist_fraud_trans_24h','hist_trans_avg_amt_60d'] #,\n\nY_cols = ['is_fraud']","239758c4":"#Feature Selection using LassoCV\n\nfrom sklearn.linear_model import LassoCV\n\n#Feature Selection\nX = df2[X_cols]  #Feature Matrix\ny = df2[Y_cols]         #Target Variable\n\nreg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  \n      str(sum(coef == 0)) + \" variables\")\n\nimp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","1fa898a6":"X = X.drop(['dist','category_shopping_net','city_pop','hist_trans_60d'],1)   #Feature Matrix","e2107859":"x=df2[X_cols]\ny=df2[Y_cols]","5d65c079":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)","4bf9e1d6":"def evaluate_model(model, x_test, y_test):\n    from sklearn import metrics\n\n    # Predict Test Data \n    y_pred = model.predict(x_test)\n\n    # Calculate accuracy, precision, recall, f1-score, and kappa score\n    acc = metrics.accuracy_score(y_test, y_pred)\n    prec = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n\n    # Calculate area under curve (AUC)\n    y_pred_proba = model.predict_proba(x_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n\n    # Display confussion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n\n    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, \n            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}","094ed2a8":"lr=LogisticRegression(max_iter=10000)\nlr.fit(X_train,y_train)\n\n# Evaluate Model\nlr_eval = evaluate_model(lr, X_test, y_test)\n\n# Print result\nprint('Accuracy:', lr_eval['acc'])\nprint('Precision:', lr_eval['prec'])\nprint('Recall:', lr_eval['rec'])\nprint('F1 Score:', lr_eval['f1'])\nprint('Cohens Kappa Score:', lr_eval['kappa'])\nprint('Area Under Curve:', lr_eval['auc'])\nprint('Confusion Matrix:\\n', lr_eval['cm'])","d2caa112":"y_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\nprint(confusion_matrix(y_train, y_train_pred))\nprint(classification_report(y_train, y_train_pred))","9a5df3fb":"dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\n# Evaluate Model\ndt_eval = evaluate_model(dt, X_test, y_test)\n\n# Print result\nprint('Accuracy:', dt_eval['acc'])\nprint('Precision:', dt_eval['prec'])\nprint('Recall:', dt_eval['rec'])\nprint('F1 Score:', dt_eval['f1'])\nprint('Cohens Kappa Score:', dt_eval['kappa'])\nprint('Area Under Curve:', dt_eval['auc'])\nprint('Confusion Matrix:\\n', dt_eval['cm'])","094493fe":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\nprint(confusion_matrix(y_train, y_train_pred))\nprint(classification_report(y_train, y_train_pred))","baf82d28":"#Building Random Forest Model\nrf_clf = RandomForestClassifier(n_estimators = 50,max_depth = 20,\n                                random_state=345, verbose = 1)\nrf_clf.fit(X_train, y_train)\n\nprint(\"Train Results\")\n# Evaluate Model\nrf_eval = evaluate_model(rf_clf, X_test, y_test)\n\n# Print result\nprint('Accuracy:', rf_eval['acc'])\nprint('Precision:', rf_eval['prec'])\nprint('Recall:', rf_eval['rec'])\nprint('F1 Score:', rf_eval['f1'])\nprint('Cohens Kappa Score:', rf_eval['kappa'])\nprint('Area Under Curve:', rf_eval['auc'])\nprint('Confusion Matrix:\\n', rf_eval['cm'])","a9eb26a6":"y_train_pred = rf_clf.predict(X_train)\ny_test_pred = rf_clf.predict(X_test)\nprint(confusion_matrix(y_train, y_train_pred))\nprint(classification_report(y_train, y_train_pred))","0be4c7dd":"#Building XG Boost Model\nfrom xgboost import XGBClassifier\n\n# fit model no training data\nxbt_model = XGBClassifier(n_estimators = 100, learning_rate = 0.1, max_depth = 3, random_state=345, verbose = 1)\nxbt_model.fit(X_train, y_train)\n# Evaluate Model\nxg_eval = evaluate_model(xbt_model, X_test, y_test)\n\n# Print result\nprint('Accuracy:', xg_eval['acc'])\nprint('Precision:', xg_eval['prec'])\nprint('Recall:', xg_eval['rec'])\nprint('F1 Score:', xg_eval['f1'])\nprint('Cohens Kappa Score:', xg_eval['kappa'])\nprint('Area Under Curve:', xg_eval['auc'])\nprint('Confusion Matrix:\\n', xg_eval['cm'])","2ee6847e":"y_train_pred = xbt_model.predict(X_train)\ny_test_pred = xbt_model.predict(X_test)\nprint(confusion_matrix(y_train, y_train_pred))\nprint(classification_report(y_train, y_train_pred))","323713bc":"#k_means_classification --> k_means_clustering, confsion_matrix, reassigning\nkmeans=KMeans(n_clusters=2,random_state=0,algorithm=\"elkan\",max_iter=10000,n_jobs=-1)\nkmeans.fit(X_train)\nkmeans_predicted_train_labels=kmeans.predict(X_train)\n#confusion matrix\n# tn fp\n# fn tp\nprint(\"tn --> true negatives\")\nprint(\"fp --> false positives\")\nprint(\"fn --> false negatives\")\nprint(\"tp --> true positives\")\ntn,fp,fn,tp=confusion_matrix(y_train,kmeans_predicted_train_labels).ravel()\nreassignflag=False\nif tn+tp<fn+fp:\n\t# clustering is opposite of original classification\n\treassignflag=True\nkmeans_predicted_test_labels=kmeans.predict(X_test)\nif reassignflag:\n\tkmeans_predicted_test_labels=1-kmeans_predicted_test_labels\n#calculating confusion matrix for kmeans\ntn,fp,fn,tp=confusion_matrix(y_test,kmeans_predicted_test_labels).ravel()\n#scoring kmeans\nkmeans_accuracy_score=accuracy_score(y_test,kmeans_predicted_test_labels)\nkmeans_precison_score=precision_score(y_test,kmeans_predicted_test_labels)\nkmeans_recall_score=recall_score(y_test,kmeans_predicted_test_labels)\nkmeans_f1_score=f1_score(y_test,kmeans_predicted_test_labels)\nkmeans_kappa_score=cohen_kappa_score(y_test,kmeans_predicted_test_labels)\nfpr, tpr, _=roc_curve(y_test,kmeans_predicted_test_labels)\nkmeans_auc=roc_auc_score(y_test,kmeans_predicted_test_labels)\n#printing\nprint(\"\")\nprint(\"K-Means\")\nprint(\"Confusion Matrix\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)\nprint(\"Scores\")\nprint(\"Accuracy -->\",kmeans_accuracy_score)\nprint(\"Precison -->\",kmeans_precison_score)\nprint(\"Recall -->\",kmeans_recall_score)\nprint(\"F1 -->\",kmeans_f1_score)\nprint(\"Kappa -->\",kmeans_kappa_score)\nprint('Area Under Curve:',kmeans_auc)\nprint(\"fpr = \", fpr , \"tpr = \", tpr)\n\n\n","49cfa05a":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\ninput_dim = X_train.shape[1]\nencoding_dim = 14\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, activation=\"tanh\",\n                activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoder = Dense(int(encoding_dim \/ 2), activation=\"relu\")(encoder)\ndecoder = Dense(int(encoding_dim \/ 2), activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n","66b72d4c":"nb_epoch = 100\nbatch_size = 32\nautoencoder.compile(optimizer='adam',\n                    loss='mean_squared_error',\n                    metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='\/media\/old-tf-hackers-7\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history\nautoencoder = load_model('model.h5')","a01fc707":"#Evaluation\n\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');","05a46dce":"# Intitialize figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Model Comparison', fontsize=16, fontweight='bold')\nfig.set_figheight(7)\nfig.set_figwidth(14)\nfig.set_facecolor('white')\n\n# First plot\n## set bar size\nbarWidth = 0.2\ndtc_score = [dt_eval['acc'], dt_eval['prec'], dt_eval['rec'], dt_eval['f1'], dt_eval['kappa']]\nrf_score = [rf_eval['acc'], rf_eval['prec'], rf_eval['rec'], rf_eval['f1'], rf_eval['kappa']]\nxg_score = [xg_eval['acc'], xg_eval['prec'], xg_eval['rec'], xg_eval['f1'], xg_eval['kappa']]\nlr_score = [lr_eval['acc'], lr_eval['prec'], lr_eval['rec'], lr_eval['f1'], lr_eval['kappa']]\nk_score = [kmeans_accuracy_score, kmeans_precison_score, kmeans_recall_score, kmeans_f1_score, kmeans_kappa_score]\n\n\n## Set position of bar on X axis\nr1 = np.arange(len(dtc_score))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\nr5 = [x + barWidth for x in r4]\n\n\n## Make the plot\nax1.bar(r1, dtc_score, width=barWidth, edgecolor='white', label='Decision Tree')\nax1.bar(r2, rf_score, width=barWidth, edgecolor='white', label='Random Forest')\nax1.bar(r3, xg_score, width=barWidth, edgecolor='white', label='XGBClassifier')\nax1.bar(r4, lr_score, width=barWidth, edgecolor='white', label='Logistic Regression')\nax1.bar(r4, lr_score, width=barWidth, edgecolor='white', label='K-Means')\n\n\n## Configure x and y axis\nax1.set_xlabel('Metrics', fontweight='bold')\nlabels = ['Accuracy', 'Precision', 'Recall', 'F1', 'Kappa']\nax1.set_xticks([r + (barWidth * 1.5) for r in range(len(dtc_score))], )\nax1.set_xticklabels(labels)\nax1.set_ylabel('Score', fontweight='bold')\nax1.set_ylim(0, 1)\n\n## Create legend & title\nax1.set_title('Evaluation Metrics', fontsize=14, fontweight='bold')\nax1.legend()\n\n# Second plot\n## Comparing ROC Curve\nax2.plot(dt_eval['fpr'], dt_eval['tpr'], label='Decision Tree, auc = {:0.5f}'.format(dt_eval['auc']))\nax2.plot(rf_eval['fpr'], rf_eval['tpr'], label='Random Forest, auc = {:0.5f}'.format(rf_eval['auc']))\nax2.plot(xg_eval['fpr'], xg_eval['tpr'], label='XGBClassifier, auc = {:0.5f}'.format(xg_eval['auc']))\nax2.plot(lr_eval['fpr'], lr_eval['tpr'], label='Logistic Regression, auc = {:0.5f}'.format(lr_eval['auc']))\nax2.plot(fpr, tpr, label='K-Means, auc = {:0.5f}'.format(kmeans_auc))\n\n\n## Configure x and y axis\nax2.set_xlabel('False Positive Rate', fontweight='bold')\nax2.set_ylabel('True Positive Rate', fontweight='bold')\n\n## Create legend & title\nax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\nax2.legend(loc=4)\n\nplt.show()","c221955c":"#As this dataset is highly imbalance we have to balance this by over sampling\ncnt_non_fraud = df2[df2['is_fraud'] == 0]['amt'].count()\ndf2_class_fraud = df2[df2['is_fraud'] == 1]\ndf2_class_nonfraud = df2[df2['is_fraud'] == 0]","990e3a00":"#OverSampling\ndf2_class_fraud_oversample = df2_class_fraud.sample(cnt_non_fraud, replace=True)\ndf2_oversampled = pd.concat([df2_class_nonfraud, df2_class_fraud_oversample], axis=0)\n\nprint('Random over-sampling:')\nprint(df2_oversampled['is_fraud'].value_counts())","9b3b0be9":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(df2_oversampled[X_cols],df2_oversampled[Y_cols] , train_size=0.7, test_size=0.3, random_state=42)","2e807999":"def evaluate_model(model, x_test, y_test):\n    from sklearn import metrics\n\n    # Predict Test Data \n    y_pred = model.predict(x_test)\n\n    # Calculate accuracy, precision, recall, f1-score, and kappa score\n    acc = metrics.accuracy_score(y_test, y_pred)\n    prec = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n\n    # Calculate area under curve (AUC)\n    y_pred_proba = model.predict_proba(x_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n\n    # Display confussion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n\n    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, \n            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}","8d644a0f":"lr=LogisticRegression(max_iter=10000)\nlr.fit(X_train,y_train)\n\n# Evaluate Model\nlr_eval = evaluate_model(lr, X_test, y_test)\n\n# Print result\nprint('Accuracy:', lr_eval['acc'])\nprint('Precision:', lr_eval['prec'])\nprint('Recall:', lr_eval['rec'])\nprint('F1 Score:', lr_eval['f1'])\nprint('Cohens Kappa Score:', lr_eval['kappa'])\nprint('Area Under Curve:', lr_eval['auc'])\nprint('Confusion Matrix:\\n', lr_eval['cm'])","73f8462c":"dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\n# Evaluate Model\ndt_eval = evaluate_model(dt, X_test, y_test)\n\n# Print result\nprint('Accuracy:', dt_eval['acc'])\nprint('Precision:', dt_eval['prec'])\nprint('Recall:', dt_eval['rec'])\nprint('F1 Score:', dt_eval['f1'])\nprint('Cohens Kappa Score:', dt_eval['kappa'])\nprint('Area Under Curve:', dt_eval['auc'])\nprint('Confusion Matrix:\\n', dt_eval['cm'])","f3290504":"#Building Random Forest Model\nrf_clf = RandomForestClassifier(n_estimators = 50,max_depth = 20,\n                                random_state=345, verbose = 1)\nrf_clf.fit(X_train, y_train)\n\nprint(\"Train Results\")\n# Evaluate Model\nrf_eval = evaluate_model(rf_clf, X_test, y_test)\n\n# Print result\nprint('Accuracy:', rf_eval['acc'])\nprint('Precision:', rf_eval['prec'])\nprint('Recall:', rf_eval['rec'])\nprint('F1 Score:', rf_eval['f1'])\nprint('Cohens Kappa Score:', rf_eval['kappa'])\nprint('Area Under Curve:', rf_eval['auc'])\nprint('Confusion Matrix:\\n', rf_eval['cm'])","39ecade8":"#Building XG Boost Model\nfrom xgboost import XGBClassifier\n\n# fit model no training data\nxbt_model = XGBClassifier(n_estimators = 100, learning_rate = 0.1, max_depth = 3, random_state=345, verbose = 1)\nxbt_model.fit(X_train, y_train)\n# Evaluate Model\nxg_eval = evaluate_model(xbt_model, X_test, y_test)\n\n# Print result\nprint('Accuracy:', xg_eval['acc'])\nprint('Precision:', xg_eval['prec'])\nprint('Recall:', xg_eval['rec'])\nprint('F1 Score:', xg_eval['f1'])\nprint('Cohens Kappa Score:', xg_eval['kappa'])\nprint('Area Under Curve:', xg_eval['auc'])\nprint('Confusion Matrix:\\n', xg_eval['cm'])","92527bad":"# Intitialize figure with two plots\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Model Comparison', fontsize=16, fontweight='bold')\nfig.set_figheight(7)\nfig.set_figwidth(14)\nfig.set_facecolor('white')\n\n# First plot\n## set bar size\nbarWidth = 0.2\ndtc_score = [dt_eval['acc'], dt_eval['prec'], dt_eval['rec'], dt_eval['f1'], dt_eval['kappa']]\nrf_score = [rf_eval['acc'], rf_eval['prec'], rf_eval['rec'], rf_eval['f1'], rf_eval['kappa']]\nxg_score = [xg_eval['acc'], xg_eval['prec'], xg_eval['rec'], xg_eval['f1'], xg_eval['kappa']]\nlr_score = [lr_eval['acc'], lr_eval['prec'], lr_eval['rec'], lr_eval['f1'], lr_eval['kappa']]\n\n## Set position of bar on X axis\nr1 = np.arange(len(dtc_score))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\n\n## Make the plot\nax1.bar(r1, dtc_score, width=barWidth, edgecolor='white', label='Decision Tree')\nax1.bar(r2, rf_score, width=barWidth, edgecolor='white', label='Random Forest')\nax1.bar(r3, xg_score, width=barWidth, edgecolor='white', label='XGBClassifier')\nax1.bar(r4, lr_score, width=barWidth, edgecolor='white', label='Logistic Regression')\n\n## Configure x and y axis\nax1.set_xlabel('Metrics', fontweight='bold')\nlabels = ['Accuracy', 'Precision', 'Recall', 'F1', 'Kappa']\nax1.set_xticks([r + (barWidth * 1.5) for r in range(len(dtc_score))], )\nax1.set_xticklabels(labels)\nax1.set_ylabel('Score', fontweight='bold')\nax1.set_ylim(0, 1)\n\n## Create legend & title\nax1.set_title('Evaluation Metrics', fontsize=14, fontweight='bold')\nax1.legend()\n\n# Second plot\n## Comparing ROC Curve\nax2.plot(dt_eval['fpr'], dt_eval['tpr'], label='Decision Tree, auc = {:0.5f}'.format(dt_eval['auc']))\nax2.plot(rf_eval['fpr'], rf_eval['tpr'], label='Random Forest, auc = {:0.5f}'.format(rf_eval['auc']))\nax2.plot(xg_eval['fpr'], xg_eval['tpr'], label='XGBClassifier, auc = {:0.5f}'.format(xg_eval['auc']))\nax2.plot(lr_eval['fpr'], lr_eval['tpr'], label='Logistic Regression, auc = {:0.5f}'.format(lr_eval['auc']))\n\n## Configure x and y axis\nax2.set_xlabel('False Positive Rate', fontweight='bold')\nax2.set_ylabel('True Positive Rate', fontweight='bold')\n\n## Create legend & title\nax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\nax2.legend(loc=4)\n\nplt.show()","e4bc5aba":"import numpy as np \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2],\n               'max_features': ['auto', 'sqrt'],\n               'min_samples_leaf': [4, 6, 8],\n               'min_samples_split': [5, 7,10],\n               'n_estimators': [20]}\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 10, \n                               cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\nmodel.fit(X_train,y_train)","b20b544a":"table = pd.pivot_table(pd.DataFrame(model.cv_results_),\n    values='mean_test_score', index='param_n_estimators', \n                       columns='param_criterion')\n     \nsns.heatmap(table)","9656e086":"predictionforest = model.best_estimator_.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc3 = accuracy_score(y_test,predictionforest)","9c7ff15b":"## About the Dataset\n   This is a simulated credit card transaction dataset containing legitimate and fraud transactions from the duration 1st Jan 2019 - 31st Dec 2020. It covers credit cards of 1000 customers doing transactions with a pool of 800 merchants.\n## Data Description\n\n* trans_date_trans_time: date and time of the transaction\n* cc_num: credit card number of customer\n* merchant: customer is paying to which merchant(like Amazon, Walmart)\n## category:\n* amt: the amount of transaction\n* first: first name of customer\n* last: last anme of customer\n* gender: gender of the customer\n* street, city, state: address of the customer\n* zip: zip code of the transaction\n* lat: latitude of the customer\n* long: longitude of customer\n* city_pop: population of the city he is living\n* job: job of cus\n* dob: date of birth of the customer\n* trans_num: unique transaction number for each and every transaction\n* unix_time: time of the transaction in Unix( generally not used in our analysis as it is unique mostly)\n* merch_lat: merchant lattitude\n* merch_long: merchant longitude\n* is_fraud: whether transaction is fraud or no (1- fraud, 0- not fraud)","0261712d":"##### Derive Age of the Customer: \n\n````\nAge of Customer = Trasaction Date - DOB\n````","b10d6eea":"## Auto Encoding","b1f2969e":"## Observations :\n* Sunday and Monday of the week have highest credit card transactions\n* For the Gas_transport purpose most of them used Credit card for their transactions ..\n* Leastly for Travelling purpose they used credit card transactions ... \n","64818401":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Building Machine Learning Model\ud83d\udcda<\/p>","72de1880":"## Under sampling on Dataset","73337a19":"gas_transport holds for 10% of the transactions ","869cb7ec":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Data Vizualization\ud83c\udfa8<\/p>","b0b00e86":"## Machine Learning model on Original Dataset","8ba97346":"## Logistic Regression","eca33df7":"## Observations :\n* Fraud Transaction mean is way higher than non-fraud transaction\n* Mean of Non Fraud Transactions: 67.6\n* Mean of Fraud Transactions: 530.6","3b554580":"##### One Hot-Encoding","607a53da":"## Feature Selection","da3cf3fc":"grocery_pos and shopping_net has more number of fraud transactions compared to other categories","f384dc31":"## Decision Tree Classifier","7bb2d9ea":"## Observations\n* From above we can observe that there is alot of outliers present in almost most of the features .\n* Now i am going to deal with these outliers .\n* Outliers can be deal in different ways one of the way is applying Z-score on the dataset","082de617":"## K-MEANS CLUSTERING","4bc652c2":"## Observations :\n* 33-57 age people are 50% of our customers\n* Minimum age of customer count is 14 \n* Maximum age of customer count is 96","c1ad8d55":"###### Merge Historical Variables with Transactions by ['cc_num','trans_date_trans_time']","d6392740":"## Logistic Regression","33c6e390":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Cleaning\ud83d\udd27<\/p>","251f5e30":"## XGBClassifier","a1fb1a0e":"## Decision Tree Classifier","91640928":"##### Category","cd94a513":"## Observations :\n* Highest number of Transactions are in month of December\n* Lowest number of Transactions happens in February \n* Most of them who used credit card transactions are Females when compared to Males ..\n","5d5d1285":"We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model.","f781cd19":"### Observations :\n* From above we can observe that there is no null or missing values present in this dataset ..","f3312671":"## XGBClassifier","19811fcd":"## Random Forest Classifier","528d9103":"Females customers are higher than male","c3e55567":"hist_fraud_trans_24h is highly correlated with is_fraud - 0.77\n\nhist_trans_24h is also correlated with hist_trans_60d   - 0.56","fb233f45":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">1. Importing necessary modules and libraries\ud83d\udcda<\/p>","2c08a39a":"## Random Forest Classifier","e8fd9c77":"#### Gender","f2d6ee1a":"<a id='1'><\/a>\n# <p style=\"background-color:red; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Table of Content<\/p>\n* [1. Importing necessary modules and libraries\ud83d\udcda](#1)\n* [2. Data Exploration\ud83d\udd0d](#2)\n* [3. Data Cleaning\ud83d\udd27](#3)\n* [4. Data Vizualization\ud83c\udfa8](#4)\n* [5. Machine Learning model training\ud83d\udcdd](#5)","bcd8a8f5":"##### 60 Day Orders Amt Avg by Customers","863885bc":"<a id='0'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Credit Card fraud Detection\ud83d\udd8b\ud83d\udcdd - EDA\ud83d\udcda & Baseline Model\ud83c\udfaf <\/p>","5c945222":"### Observations :\n* From above we can observe that there is 23 features and 1296675 rows ","aa5cc150":"## Hyper parmeter tuning","58644d8b":"## Model Comparission on Original Dataset","ba21a36f":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data Exploration\ud83d\udd0d<\/p>","a092bacf":"This is highly imbalanced class data, so later we need to balance the dependent variable","545dd2a2":"### Distribution of Dependent Variable","2b8981ee":"## Over sampling on Dataset"}}