{"cell_type":{"f7eafe5a":"code","bc936c6d":"code","d5a9b326":"code","e7b5245b":"code","cbc43fb6":"code","1d799b98":"code","9d60c90d":"code","924d7c41":"code","43412977":"code","ba3e8b81":"code","3a1ab30b":"code","47518a03":"code","bfd05993":"code","b385fba4":"code","7f22e672":"code","02d20e7b":"code","88063d0e":"code","9c33db73":"code","47b9265e":"code","5154d5eb":"code","fbb8c503":"code","c6a99dff":"code","8d207c14":"code","bfa06e8c":"code","8cc3631f":"code","c45abc48":"code","93615eef":"code","a4f5ab8b":"code","c51ade98":"code","49dab462":"code","344f8bc9":"code","30b8afdf":"code","0b773074":"code","44264baf":"code","d89a3ab1":"code","5b91c4cc":"code","c28307f3":"code","4d301562":"markdown","145804e2":"markdown","d7eb5dac":"markdown","f68424b8":"markdown","515807b8":"markdown","7876f421":"markdown","90baa0e8":"markdown","4df6b6c9":"markdown","1df8f727":"markdown","dc8cc15a":"markdown","6435e95d":"markdown","c8afc7d5":"markdown","2fcb149f":"markdown","6245b00e":"markdown","eabe0317":"markdown","3f682dbe":"markdown","78dad2b8":"markdown","57b1a354":"markdown","cccb56a8":"markdown","0be93488":"markdown","9f37626c":"markdown","426341e2":"markdown","03d65998":"markdown","4aeb753b":"markdown","ca1e7f87":"markdown","bd94cbd8":"markdown","6def6588":"markdown","91b19a56":"markdown"},"source":{"f7eafe5a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#to scale the data using z-score \nfrom sklearn.preprocessing import StandardScaler\n\n#importing PCA and TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","bc936c6d":"data = pd.read_csv('..\/input\/autompg-dataset\/auto-mpg.csv')","d5a9b326":"data.head()","e7b5245b":"data.info()","cbc43fb6":"data[\"car name\"].nunique()","1d799b98":"# dropping car_name\ndata1 = data.copy()\ndata = data.drop(['car name'], axis=1)","9d60c90d":"# checking if there are values other than digits in the column 'horsepower' \nhpIsDigit = pd.DataFrame(data.horsepower.str.isdigit())  # if the string is made of digits store True else False\n\n# print isDigit = False!\ndata[hpIsDigit['horsepower'] == False]   # from temp take only those rows where hp has false","924d7c41":"#Relacing ? with np.nan\ndata = data.replace('?', np.nan)\ndata[hpIsDigit['horsepower'] == False]","43412977":"# Imputing the missing values with median value\ndata.horsepower.fillna(data.horsepower.median(), inplace=True)\ndata['horsepower'] = data['horsepower'].astype('float64')  # converting the hp column from object data type to float","ba3e8b81":"data.describe()","3a1ab30b":"# Uncomment and complete the code by filling the blanks \n\nfor col in data.columns:\n     print(col)\n     print('Skew :',round(data[col].skew(),2))\n     plt.figure(figsize=(15,4))\n     plt.subplot(1,2,1)\n     data[col].hist()\n     plt.ylabel('count')\n     plt.subplot(1,2,2)\n     sns.boxplot(x= data[col])\n     plt.show()","47518a03":"plt.figure(figsize=(8,8))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","bfd05993":"# scaling the data\nscaler=StandardScaler()\ndata_scaled=pd.DataFrame(scaler.fit_transform(data), columns=data.columns)","b385fba4":"data_scaled.head()","7f22e672":"#Defining the number of principal components to generate \nn=data_scaled.shape[1]\n\n#Finding principal components for the data\npca = PCA(n_components=n,random_state = 1) #Applying the PCA algorithm with random state = 1\ndata_pca1 = pd.DataFrame(pca.fit_transform(data_scaled)) #Fitting and transforming the pca function on scaled data\n\n#The percentage of variance explained by each principal component\nexp_var = pca.explained_variance_ratio_","02d20e7b":"# visualizing the explained variance by individual components\nplt.figure(figsize = (10,10))\nplt.plot(range(1,9), exp_var.cumsum(), marker = 'o', linestyle = '--')\nplt.title(\"Explained Variances by Components\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")","88063d0e":"# finding the least number of components that can explain more than 90% variance\nsum = 0\nfor ix, i in enumerate(exp_var):\n  sum = sum + i\n  if(sum>0.90):\n    print(\"Number of PCs that explain at least 90% variance: \", ix+1)\n    break","9c33db73":"pca.components_","47b9265e":"pc_comps = ['PC1','PC2','PC3','PC4']\ndata_pca = pd.DataFrame(np.round(pca.components_[:4,:],2),index=pc_comps,columns=data_scaled.columns)\ndata_pca.T","5154d5eb":"def color_high(val):\n    if val <= -0.40: # you can decide any value as per your understanding\n        return 'background: pink'\n    elif val >= 0.40:\n        return 'background: skyblue'   \n    \ndata_pca.T.style.applymap(color_high)","fbb8c503":"plt.figure(figsize = (7,7))\nsns.scatterplot(x=data_pca1[0],y=data_pca1[1])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()","c6a99dff":"df_concat = pd.concat([data_pca1, data], axis=1)\n\nplt.figure(figsize = (7,7))\n#Create a scatter plot with x=0 and y=1 using df_concat dataframe\nsns.scatterplot(x = 0, y = 1, data=df_concat, hue = 'cylinders')\n\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")","8d207c14":"tsne = TSNE(n_components = 2, random_state = 1)  #Applying the TSNE algorithm with random state = 1\ndata_tsne = tsne.fit_transform(data_scaled) #Fitting and transforming tsne function on the scaled data","bfa06e8c":"data_tsne.shape","8cc3631f":"data_tsne = pd.DataFrame(data = data_tsne, columns = ['Component 1', 'Component 2'])","c45abc48":"data_tsne.head()","93615eef":"sns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1])","a4f5ab8b":"# Let's see scatter plot of the data w.r.t number of cylinders\nsns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1],hue=data.cylinders)","c51ade98":"# Let's assign points to 3 different groups\ndef grouping(x):\n    first_component = x['Component 1']\n    second_component = x['Component 2']\n    if (first_component> 0) and (second_component >0): \n        return 'group_1'\n    if (first_component >-20 ) and (second_component >-10):\n        return 'group_2'\n    else: \n        return 'group_3'","49dab462":"data_tsne['groups'] = data_tsne.apply(grouping,axis=1)","344f8bc9":"sns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1],hue=data_tsne.iloc[:,2])","30b8afdf":"data['groups'] = data_tsne['groups'] ","0b773074":"all_col = data.columns.tolist()\nplt.figure(figsize=(20, 20))\n\nfor i, variable in enumerate(all_col):\n    if i==7:\n        break\n    plt.subplot(4, 2, i + 1)\n    #Create boxplot with groups on the x-axis and variable on the y-axis (use the dataframe data)\n    sns.boxplot(x = data['groups'], y = data[variable])\n    plt.tight_layout()\n    plt.title(variable)\nplt.show()","44264baf":"data2 = data1.copy()\ndata2['groups'] = data_tsne['groups']\ndata2.head()","d89a3ab1":"data2[data2['groups'] == 'group_1'].head()","5b91c4cc":"data2[data2['groups'] == 'group_2'].head()","c28307f3":"data2[data2['groups'] == 'group_3'].head()","4d301562":"**Observations:**\n\n- There are 398 observations and 8 columns in the data.\n- All variables except horsepower and car name are of numeric data type.\n- The horsepower must be a numeric data type. We will explore this further.","145804e2":"#### Summary Statistics","d7eb5dac":"**Observations:**\n\n* There don't seem to be any obvious suspicious values\n\n* The earliest car model is from 1970 whereas the latest is from 1982\n\n* The average MPG across the cars is 23.51, and the average horsepower is around 104.3\n\n* The cars seem to have approximately 5.4 cylinders on average\n\n","f68424b8":"## Data Preprocessing and Exploratory Data Analysis","515807b8":"## Principal Component Analysis","7876f421":"**Observations:**\n\n* We get clear distinctions between each group across the different variables\n\n* Group 1 seems to be more fuel efficient, simpler cars since they have higher MPG, lower weight, lower horsepower, and lower cylinders. One such example is the datsun pl510 which is a relatively smaller car. These might be the cars meant more for every day use and affordability. \n\n* Group 2 seems to represent the \"intermediate\" cars. Ones that don't have very good mpg but not as low as group 3. They seem to be mid sized, with a mid-range horsepower and cylinders. An example would be the plymouth duster. \n\n* Group 3 seems to be the heavier cars and faster cars that have a lot of horsepower and cylinders, they are less fuel efficient than the other categories since their MPG numbers are lower. An example would be the chevrolet chevelle malibu.   ","90baa0e8":"**Let's look at the coefficients of three principal components from the below dataframe**","4df6b6c9":"#### Loading data","1df8f727":"#### Scaling the data","dc8cc15a":"#### Checking correlation","6435e95d":"**Observations:**\n- The variable mpg has strong negative correlation with cylinders, displacement, horsepower, and weight.\n- horsepower and acceleration are negatively correlated.\n- The variable weight has strong positively correlation with horsepower, displacement and cylinders\n- model year is positively correlated with mpg.","c8afc7d5":"**Observations:**\n\n* The first principle component seems to have high coefficients for cylinders, displacement, horsepower, and weight and a very low coefficient for mpg- this seems to be associated with cars that are bigger and heavier with more cylinders and less fuel efficient. \n\n* PC2 has a very low coefficient for model year, so it seems to be associated with cars that have typical values for older cars. \n\n* PC3 has a low coefficient for acceleration ","2fcb149f":"#### We can also visualize the data in 2 dimensions using first two principal components ","6245b00e":"## Importing necessary libraries and overview of the dataset","eabe0317":"**Observations:**\n\nWe see 3 different clusters:\n* one of them has 3-5 cylinders\n* the other has 6 cylinders \n* the last one has 8 cylinders\n\nSome values overlap","3f682dbe":"#### Let's check the distribution and outliers for each column in the data","78dad2b8":"**Observations:**\n- There are 6 observations where horsepower is ?.\n- We can consider these values as missing values.\n- Let's impute these missing values and change the data type of horsepower column.\n- First we need to replace the ? with np.nan.","57b1a354":"## Auto MPG: PCA and tSNE\n\n-----------------------------\n\n-----------------------------\n## Objective: \n-----------------------------\nThe objective of this notebook is to explore the data and reduce the number of features by using dimensionality reduction techniques like PCA and TSNE and generate meaningful insights. \n\n-----------------------------\n## Dataset: \n-----------------------------\nThere are 8 variables in the data: \n\n- mpg: miles per gallon\n- cyl: number of cylinders\n- disp: engine displacement (cu. inches) or engine size\n- hp: horsepower\n- wt: vehicle weight (lbs.)\n- acc: time taken to accelerate from O to 60 mph (sec.)\n- yr: model year\n- car name: car model name","cccb56a8":"**Observations:**\n\n* The vizualized groups seem to be formed in a similar way as PCA however they are better separated with larger distances between clusters to make the distinctions clear","0be93488":"#### Checking the info of the data","9f37626c":"## t-SNE","426341e2":"- The column 'car name' is of object data type containing a lot of unique entries and would not add values to our analysis. We can drop this column.","03d65998":"#### Checking values in horsepower column","4aeb753b":"**Observations:**\n\n* Some of the distributions seem to be skewed to the right, namely: weight, displacement, horsepower, and MPG\n\n* Acceleration resembles a normal distribution\n\n* A significant portion of the cars have 4 cylinders. ","ca1e7f87":"\n**We will apply the TSNE embedding with 2 components for the dataframe data_scaled**\n  ","bd94cbd8":"\n**Let's take a better look at the different groups w.r.t different variables**","6def6588":"**Observations:**\n\n* We can see that a large percentage of the variation can be explained by 4 principal components. More specifically, they explain almost 95% of the variance as we can see in the above results. \n\n* This lets us reduce the number of dimensions significantly. ","91b19a56":"**Let's try adding hue to the scatter plot**"}}