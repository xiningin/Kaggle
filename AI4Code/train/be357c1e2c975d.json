{"cell_type":{"2de9c506":"code","3770af58":"code","03428491":"code","b69485c9":"code","083eebbc":"code","e8f9827d":"code","14e945b4":"code","574aa0e3":"markdown","8e69890c":"markdown","f79ea5e4":"markdown","d83cbe37":"markdown","51c457d5":"markdown","6acbdbf3":"markdown","4dd66159":"markdown"},"source":{"2de9c506":"!pip install livelossplot","3770af58":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport random\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nfrom IPython.display import SVG, Image\nfrom livelossplot import PlotLossesKerasTF\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","03428491":"# Randomly view 5 images in each category\n\npath = \"..\/input\/facial-expressions\/\"\npath_train = os.path.join(path, \"train\")\npath_valid = os.path.join(path, \"test\")\n\nlabels = os.listdir(path_train)\n\nfig, axs = plt.subplots(len(labels), 5, figsize = (15, 15))\n\nclass_len = {}\nfor i, c in enumerate(labels):\n    class_path = os.path.join(path_train, c)\n    all_images = os.listdir(class_path)\n    sample_images = random.sample(all_images, 5)\n    class_len[c] = len(all_images)\n    \n    for j, image in enumerate(sample_images):\n        img_path = os.path.join(class_path, image)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axs[i, j].imshow(img)\n        axs[i, j].set(xlabel = c, xticks = [], yticks = [])\n\nfig.tight_layout()","b69485c9":"img_size = 48\nbatch_size = 64\n\ndatagen_train = ImageDataGenerator(horizontal_flip=True)\n\ntrain_generator = datagen_train.flow_from_directory(\n    path_train,\n    target_size=(img_size,img_size),\n    color_mode=\"grayscale\",\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\ndatagen_validation = ImageDataGenerator(horizontal_flip=True)\nvalidation_generator = datagen_validation.flow_from_directory(\n    path_valid,\n    target_size=(img_size,img_size),\n    color_mode=\"grayscale\",\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)","083eebbc":"# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(7, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","e8f9827d":"%%time\n\nepochs = 30\nsteps_per_epoch = train_generator.n\/\/train_generator.batch_size\nvalidation_steps = validation_generator.n\/\/validation_generator.batch_size\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.1,\n    patience=2, \n    min_lr=0.00001, \n    mode='auto'\n)\n\ncheckpoint = ModelCheckpoint(\n    \"model_weights.h5\", \n    monitor='val_accuracy',\n    save_weights_only=True, \n    mode='max', \n    verbose=1\n)\n\ncallbacks = [PlotLossesKerasTF(), checkpoint, reduce_lr]\n\nhistory = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data = validation_generator,\n    validation_steps = validation_steps,\n    callbacks=callbacks\n)","14e945b4":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","574aa0e3":"### Task 2: Plot Sample Images","8e69890c":"### Task 6: Train and Evaluate Model","f79ea5e4":"<h2 align=center> Facial Expression Recognition<\/h2>","d83cbe37":"### Task 7: Represent Model as JSON String","51c457d5":"### Task 3: Generate Training and Validation Batches","6acbdbf3":"### Task 1: Import Libraries","4dd66159":"### Task 4: Create CNN Model"}}