{"cell_type":{"ddfd5e35":"code","d6e5e9fc":"code","def50e97":"code","dc286745":"code","07892433":"code","4b6ad210":"code","735b8a9c":"code","de95f13d":"code","7b71b233":"code","e16b6a67":"code","ab7f1675":"code","ce3ae3e8":"code","e2fd9e32":"code","2077d485":"code","45dcea12":"code","c76a955f":"code","151fed76":"code","08052e79":"code","97a1cf16":"code","bcee740d":"code","0b86e554":"code","bf226d22":"code","949f7ab4":"code","417defbe":"code","fe49a186":"code","4b84837b":"code","1b70ba21":"code","aee8c57a":"code","9659af55":"code","edc7ba6c":"code","8761cf03":"markdown","ce68dab8":"markdown","689db01b":"markdown","c514e09d":"markdown","7f0ff327":"markdown","f66c827f":"markdown","5ff863ef":"markdown","a2dcd1c7":"markdown","2302a8d4":"markdown","ab985292":"markdown","555be5ca":"markdown","aa2cae03":"markdown","dfd15a6e":"markdown","79d54668":"markdown","6ec2f9c9":"markdown"},"source":{"ddfd5e35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n        print(dirname)\n\n# Any results you write to the current directory are saved as output.","d6e5e9fc":"base_path = \"..\/input\/kuzushiji-recognition\/\"\ntrain = pd.read_csv(\"..\/input\/kuzushiji-recognition\/train.csv\")","def50e97":"train.head(5)","dc286745":"print(train[\"labels\"][0])","07892433":"import cv2\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,20))\ns = train[\"image_id\"][0]\nimg_0_path = base_path + \"train_images\/\" + s + \".jpg\"\nimg_0 = cv2.imread(img_0_path)\nimg_0 = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\nplt.imshow(img_0)","4b6ad210":"#preprocessing on a single image\n\nplt.figure(figsize=(20,20))\nimg_0_orig = cv2.imread(img_0_path)\nimg_0_orig = cv2.cvtColor(img_0_orig, cv2.COLOR_BGR2RGB)\nimg_0 = cv2.imread(img_0_path)\nimg_0 = cv2.cvtColor(img_0, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(img_0,(3,3),0)\nsharp_mask = np.subtract(img_0, blur)\nimg_0 = cv2.addWeighted(img_0,1, sharp_mask,10, 0)\nret,th = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nkernel_1 = np.ones((5,5),np.uint8)\nkernel_2 = np.ones((1,1),np.uint8)\nopening = cv2.morphologyEx(th, cv2.MORPH_OPEN, kernel_1)\nclosing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel_2)\nmask = cv2.bitwise_not(closing)\nmask = cv2.cvtColor(closing, cv2.COLOR_GRAY2RGB)\nimg = cv2.add(img_0_orig,mask)\nblur_1 = cv2.GaussianBlur(img, (13,13), 0)\nsharp_mask_1 = np.subtract(img,blur_1)\nsharp_mask_1 = cv2.GaussianBlur(sharp_mask_1, (7,7), 0)\nimg = cv2.addWeighted(img,1,sharp_mask_1,-10, 0)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nprint(img_0_orig.shape)","735b8a9c":"pwd = os.getcwd()\nprint(pwd)","de95f13d":"train_imgs_path = base_path + \"train_images\/\"\nfilelist = os.listdir(train_imgs_path)\n#print(filelist[:5])\n","7b71b233":"trainlist = train[\"image_id\"].tolist()\ntrainlist = [x + \".jpg\" for x in trainlist]","e16b6a67":"#print(trainlist[:5])","ab7f1675":"train_out = train[\"labels\"].tolist()\n#train_out[:5]\n","ce3ae3e8":"yolo_labels = []\nimg_label = []\nfor l in train_out:\n    voc_out = str(l).split()\n    for i in range(len(voc_out)\/\/5):\n        start_idx = 5*i\n        img_label.append(voc_out[start_idx:start_idx+5])\n    yolo_labels.append(img_label)\n    img_label = []\n#print(yolo_labels[:5])","e2fd9e32":"trainlist[0]","2077d485":"def pre(Image):\n    img_0_orig = cv2.imread(Image)\n    img_0_orig = cv2.cvtColor(img_0_orig, cv2.COLOR_BGR2RGB)\n    img_0 = cv2.imread(Image)\n    img_0 = cv2.cvtColor(img_0, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(img_0,(3,3),0)\n    sharp_mask = np.subtract(img_0, blur)\n    img_0 = cv2.addWeighted(img_0,1, sharp_mask,10, 0)\n    ret,th = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    kernel_1 = np.ones((5,5),np.uint8)\n    kernel_2 = np.ones((1,1),np.uint8)\n    opening = cv2.morphologyEx(th, cv2.MORPH_OPEN, kernel_1)\n    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel_2)\n    mask = cv2.bitwise_not(closing)\n    mask = cv2.cvtColor(closing, cv2.COLOR_GRAY2RGB)\n    img = cv2.add(img_0_orig,mask)\n    blur_1 = cv2.GaussianBlur(img, (13,13), 0)\n    sharp_mask_1 = np.subtract(img,blur_1)\n    sharp_mask_1 = cv2.GaussianBlur(sharp_mask_1, (7,7), 0)\n    img = cv2.addWeighted(img,1,sharp_mask_1,-10, 0)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return img\n    ","45dcea12":"plt.imshow(pre(train_imgs_path+trainlist[0]), cmap = 'gray')\nprint(pre(train_imgs_path+trainlist[0]).shape)","c76a955f":"#yolo_model = make_yolov3_model()\n#yolo_model.load_weights(\"yolo.h5\")","151fed76":"#yolo_model.summary()","08052e79":"uni_lab = pd.read_csv(\"..\/input\/kuzushiji-recognition\/unicode_translation.csv\")","97a1cf16":"#uni_lab.head()","bcee740d":"#uni_lab.tail()","0b86e554":"uni_list = uni_lab[\"Unicode\"].to_list()\n#print(uni_list[:5])","bf226d22":"with open('class.names', 'w') as f:\n    for item in uni_list:\n        f.writelines(item+\"\\n\")\n#!ls","949f7ab4":"#open('class.names', 'w').close()\nwith open(\"class.names\", \"r\") as f_r:\n    test_list = f_r.readlines()\n#print(test_list[0][:-1])","417defbe":"uni_dict = ((uni_list[i],i) for i in range(len(uni_list)))\nuni_dict = dict(uni_dict)        ","fe49a186":"#uni_dict[\"U+0031\"]","4b84837b":"for labels in yolo_labels:\n    for label in labels:\n        label[0] = uni_dict[label[0]]\n        label[1:5] = list(map(int, label[1:5]))\nyolo_labels[:5]","1b70ba21":"for i in range(len(trainlist)):\n    labels = yolo_labels[i]\n    img = trainlist[i]\n    img_path = base_path + \"train_images\/\" + img\n    image = Image.open(img_path)\n    w, h = image.size\n    for label in labels:\n        id_1 = label[0]\n        label[0],label[2] = list(map(int,[label[1]-label[3]\/2,label[1]+label[3]\/2]))\n        label[1],label[3] = list(map(int,[label[2]-label[4]\/2,label[2]+label[4]\/2]))\n        label[4] = id_1","aee8c57a":"#yolo_labels[:3]","9659af55":"#str(yolo_labels[0][0][0])","edc7ba6c":"lines = [train_imgs_path+img_name for img_name in trainlist]\n#print(lines[:3])\nannot = []\nfor labels in yolo_labels:\n    ann_fin = \"\"\n    for label in labels:\n        ann = \" \" + str(label[0])+','+str(label[1])+','+str(label[2])+','+str(label[3])+','+str(label[4])\n        ann_fin += ann\n    annot.append(ann_fin)\n#print(annot[0])\nfor i in range(len(lines)):\n    lines[i] = lines[i]+annot[i]\n#print(lines[0])","8761cf03":"So before putting any image we need to preprocess our image so that the print on the back page is not passed when the model is predicting.\n\nIntuitively, I can see that the intensity values for that are higher than the Kuzushiji print on a page so, by using Otsu's Thresholding and I do binary thresholding of the images and I get the above result. The binary thresholding involves passing the image through a (3,3) Gaussian filter and then Otsu's thresholding is applied. On observing, we were right and we can no more see any from the opposite side of the page. To further improve the quality of the input, the image is passed through 2 morphological transformations, i.e. opening using a (5,5) one's kernel and a (1,1) one's closing kernel.","ce68dab8":"***The following is just a rough draft\/incomplete solution to the problem. I don't know if I'll be able to complete on time or not becuase of University stuff. But here it is as of now. I put heavy emphasis on the preprocessing. A lot of the preprocessing is stuff I learnt from my college course \"Digital Image Processing\".***","689db01b":"We observe some issues, which have been stated in the problem statement also.\n\n1. You can occasionally see through especially thin paper and read characters from the opposite side of the page. Those characters should also be ignored.\n2. Also, the annotations which are in smaller font size, they need to be ignored.\n\nThe first problem can be tackled by using thresholding. Let's see what happens thresholding is applied.","c514e09d":"On observing we see that, each of the of labels gives us bounding boxes in the format - \"class id\", \"X_c\", \"Y_c\", \"width\", \"height\". ","7f0ff327":"**Making a list of all training images**","f66c827f":"**Saving all of these in a class.names file**","5ff863ef":"This is all I could do before the end of the competition. Hope this helps someone.","a2dcd1c7":"**Importing the training CSV file**","2302a8d4":"Let's observe the labels of any one element","ab985292":"Let's take a look at one of the training images. ","555be5ca":"**End of all preprocessing**","aa2cae03":"**Creating a dictionary to map class_id to class_name**","dfd15a6e":"*Converting the annotations into VOC format*","79d54668":"**Creating a dictionary which maps image ids to bounding boxes\/detections**","6ec2f9c9":"**Changing the classnames to class ids in the yolo_labels list including the groundtruths**"}}