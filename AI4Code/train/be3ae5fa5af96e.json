{"cell_type":{"1ce78fc6":"code","ee99bf77":"code","1bb63d70":"code","966c80bf":"code","b7b232e4":"code","2aedd30a":"code","d6942b40":"code","ae86e5c8":"code","d909f7d2":"code","e235562c":"code","12fc4cdf":"code","96984b3b":"code","7a64b0df":"code","376b3035":"code","a1bc3c91":"code","6a03c5ab":"code","5555eb70":"code","3be3a90d":"markdown","2c4ba9c6":"markdown","52262bf9":"markdown","b9c3f49f":"markdown","d6d15658":"markdown"},"source":{"1ce78fc6":"import pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure","ee99bf77":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1bb63d70":"app_data = pd.read_csv('\/kaggle\/input\/loan-defaulter\/application_data.csv')","966c80bf":"print(list(app_data.columns))\napp_data.head(5)","b7b232e4":"input_data = app_data[['TARGET', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', \n                        'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT', 'REGION_POPULATION_RELATIVE', \n                        'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', \n                        'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'AMT_REQ_CREDIT_BUREAU_HOUR', \n                        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', \n                        'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']].fillna(0).head(10000)","2aedd30a":"input_data","d6942b40":"class SoftLeaderClustering:\n    def __init__(self,data: pd.DataFrame, threshold: float):\n        self.__data = data\n        self.__threshold = threshold\n        self.__data_matrix = data.to_numpy()\n        self.__cluster_repr_matrix = None\n        self.__assignment_matrix = None\n        self.__product_matrix = None\n        self.__error = None\n    \n    def shuffle_data(self):\n        random.shuffle(self.__data_matrix)\n        \n    def do_clustering(self):\n        self.__build_cluster_repr_matrix()\n        self.__build_assignment_matrix()\n        self.__build_product_matrix()\n        self.__calculate_error()\n    \n    def get_error(self) -> float:\n        return self.__error\n    \n    def get_number_of_clusters(self) -> int:\n        return len(self.__cluster_repr_matrix)\n    \n    def __build_cluster_repr_matrix(self):\n        cluster_leaders = []\n        for data_point in self.__data_matrix:\n            if len(cluster_leaders) == 0:\n                cluster_leaders.append(data_point)\n                continue\n            else:\n                is_new_leader = True\n                for leader in cluster_leaders:\n                    distance_from_leader = distance.euclidean(leader, data_point)\n                    if distance_from_leader < self.__threshold: \n                        is_new_leader = False\n                        break\n                if is_new_leader: \n                    cluster_leaders.append(data_point)\n        self.__cluster_repr_matrix = cluster_leaders\n        \n    def __build_assignment_matrix(self):\n        assignment_matrix = []\n        for data_point in self.__data_matrix:\n            distance_arr = [np.exp(-distance.euclidean(data_point, p)) for p in self.__cluster_repr_matrix]\n            sum_val = np.sum(list(filter(lambda d: d < self.__threshold, distance_arr)))\n            if(sum_val != 0):\n                assignment_matrix.append(np.array([np.true_divide(p, sum_val)*(1 if p < self.__threshold else 0) for p in distance_arr]))\n            else:\n                assignment_matrix.append(np.array(list(map(lambda v: 1 if np.array_equal(v,data_point) else 0, distance_arr))))\n        self.__assignment_matrix = assignment_matrix\n    \n    def __build_product_matrix(self):\n        self.__product_matrix = np.dot(self.__assignment_matrix, self.__cluster_repr_matrix)\n    \n    def __calculate_error(self):\n        row_length = len(self.__data_matrix)\n        col_length = len(self.__data_matrix[0])\n        error_val = 0\n        for row in range(row_length):\n            for col in range(col_length):\n                error_val += np.square(self.__data_matrix[row][col] - self.__product_matrix[row][col])\n        self.__error = error_val\n        \n        \n    ","ae86e5c8":"threshold_vals = [100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000]\ncluster_count = []\nerror_val_for_thresholds = []\nfor threshold in threshold_vals:\n    soft_cluster = SoftLeaderClustering(input_data, threshold)\n    soft_cluster.do_clustering()\n    cluster_count.append(soft_cluster.get_number_of_clusters())\n    error_val_for_thresholds.append(soft_cluster.get_error())\ncluster_count_df = pd.DataFrame({'threshold_val': threshold_vals, 'cluster_count': cluster_count, 'error': error_val_for_thresholds})","d909f7d2":"cluster_count_df","e235562c":"cluster_count_df.plot(x='threshold_val', y = 'cluster_count', kind=\"line\", figsize=(5,4))\ncluster_count_df.plot(x='threshold_val', y = 'error', kind=\"line\", figsize=(5,4))\ncluster_count_df.plot(x='cluster_count', y = 'error', kind=\"line\", figsize=(5,4))","12fc4cdf":"soft_cluster = SoftLeaderClustering(input_data, 200000)\nnum_clusters = []\nerror_vals = []\nfor shuffle_num in range(10):\n    soft_cluster.shuffle_data()\n    soft_cluster.do_clustering()\n    num_clusters.append(soft_cluster.get_number_of_clusters())\n    error_vals.append(soft_cluster.get_error())\nsoft_leader_clustering_results = pd.DataFrame({'shuffle_num': list(range(1,11)), 'num_clusters': num_clusters, \n                                               'error_vals': error_vals})","96984b3b":"soft_leader_clustering_results","7a64b0df":"soft_leader_clustering_results.plot(x='num_clusters', y = 'error_vals', kind=\"line\", figsize=(5,4))","376b3035":"class KMeansClustering:\n    def __init__(self, data: pd.DataFrame, k: int):\n        self.__data = data\n        self.__k_value = k\n        self.__data_matrix = data.to_numpy()\n        self.__cluster_repr_matrix = None\n        self.__assignment_matrix = None\n        self.__product_matrix = None\n        self.__error = None\n    \n    def do_clustering(self):\n        kmeans = KMeans(n_clusters=self.__k_value, init='k-means++', random_state=0).fit(self.__data_matrix)\n        self.__cluster_repr_matrix = kmeans.cluster_centers_\n        self.__build_assignment_matrix(kmeans.labels_)\n        self.__build_product_matrix()\n        self.__calculate_error()\n    \n    def get_error(self) -> float:\n        return self.__error\n        \n    \n    def __build_assignment_matrix(self, labels):\n        assignment_matrix = []\n        for label in labels:\n            label_arr = [0]*self.__k_value\n            label_arr[label] = 1\n            assignment_matrix.append(label_arr)\n        self.__assignment_matrix = assignment_matrix\n        \n        \n    def __build_product_matrix(self):\n        self.__product_matrix = np.dot(self.__assignment_matrix, self.__cluster_repr_matrix)\n    \n    def __calculate_error(self):\n        row_length = len(self.__data_matrix)\n        col_length = len(self.__data_matrix[0])\n        error_val = 0\n        for row in range(row_length):\n            for col in range(col_length):\n                error_val += np.square(self.__data_matrix[row][col] - self.__product_matrix[row][col])\n        self.__error = error_val","a1bc3c91":"k_values = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nkmeans_error_vals = []\nfor k in k_values:\n    kmeans_cluster = KMeansClustering(input_data, k)\n    kmeans_cluster.do_clustering()\n    kmeans_error_vals.append(kmeans_cluster.get_error())\nkmeans_clustering_results = pd.DataFrame({'k_value': k_values, 'error': kmeans_error_vals})","6a03c5ab":"kmeans_clustering_results","5555eb70":"kmeans_clustering_results.plot(x='k_value', y = 'error', kind=\"line\", figsize=(5,4))","3be3a90d":"# Part B: K-Means clustering","2c4ba9c6":"# Part A: Soft leader clustering","52262bf9":"### For this assignment, I'll be taking the first 10k records and use the 25 numerical columns with most number of non-null values.","b9c3f49f":"### Based on the above values, let's take 200000 as the threshold value for different permutations of this data","d6d15658":"# Dataset prep"}}