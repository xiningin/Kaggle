{"cell_type":{"75123ea2":"code","e7e367de":"code","b97d55f8":"code","584e36ca":"code","41ce4925":"code","9fe07318":"code","06d8b990":"code","596104a0":"code","5fb4203f":"code","aa0d00d0":"code","74c3dd11":"code","00bdbecf":"code","884b5e05":"code","724a0de3":"code","219886b6":"code","cc22133e":"code","ef7b786e":"code","4278fc14":"markdown"},"source":{"75123ea2":"import time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom scipy.stats import skew, boxcox\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom pandas import DataFrame\nfrom scipy.stats.stats import pearsonr\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport scipy.stats as stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","e7e367de":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b97d55f8":"def read_data(file_path):    \n    print('Loading datasets...')\n    X_train = pd.read_csv(file_path + 'train.csv', sep=',')\n    print('Datasets loaded')\n    return X_train\nPATH = '..\/input\/allstate-claims-severity\/'\nDATA = read_data(PATH)","584e36ca":"DATA","41ce4925":"all_categorical_covariates = [x for x in DATA.select_dtypes(include=['object']).columns if x not in ['id','loss', 'log_loss', 'log_loss_+_200']]\ncontinuous_covariates = [x for x in DATA.select_dtypes(exclude=['object']).columns if x not in ['id','loss', 'log_loss', 'log_loss_+_200']]        \nbinary_categorical_covariates = [x for x in DATA.columns if len(DATA[x].unique()) == 2 and DATA[x].dtype == 'object']\nnon_bynary_categorical_covariates = [x for x in DATA.columns if len(DATA[x].unique()) > 2 and DATA[x].dtype == 'object']\n\nprint(\"List of Binary Categorical Covariates:\\n\")\nprint(binary_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of Non Binary Categorical Covariates:\\n\")\nprint(non_bynary_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of All Categorical Covariates:\\n\")\nprint(all_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of Continuous Covariates:\\n\")\nprint(continuous_covariates)","9fe07318":"for x in all_categorical_covariates:\n    lb = LabelEncoder()\n    lb.fit(DATA[x].unique())\n    DATA[x] = lb.transform(DATA[x])","06d8b990":"DATA","596104a0":"seed = 1\n\ntrainx = DATA.columns[1:73]\ntrainy = DATA.columns[-2]\n\nX = DATA[trainx]\nY = DATA[trainy]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n\nmodel1 = LinearRegression(n_jobs=-1)\nresults1 = cross_val_score(model1, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(-1*results1.mean(), results1.std()))","5fb4203f":"X2 = sm.add_constant(X)\nmodel = sm.OLS(Y, X2)\nmodel_ = model.fit()\nprint(model_.summary())","aa0d00d0":"model2 = Ridge(alpha=1,random_state=seed)\nresults2 = cross_val_score(model2, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Ridge (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results2.mean(), results2.std()))","74c3dd11":"start_time = time.clock()\n\nclf = Ridge()\ncoefs = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(40, 20))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights would be shrinked to zero (see proof below)', \nxy=(0.1, -0.6), xytext=(0.1, -0.4), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","00bdbecf":"start_time = time.clock()\n\nclf = Ridge()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    #clf.fit(X2, Y)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1).mean())\n\nplt.figure(figsize=(40, 20))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights would be \\nshrinked to zero (see proof below)', \nxy=(0.1, -0.68), xytext=(0.1, -0.65), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('', \nxy=(100, -0.51), xytext=(100, -0.54), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.48])\n#plt.xlim([-17.5,17.5])\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","884b5e05":"model3 = Lasso(alpha=0.0001,random_state=seed)\nresults3 = cross_val_score(model3, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Lasso (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results3.mean(), results3.std()))","724a0de3":"start_time = time.clock()\n\nclf = Lasso()\ncoefs = []\nalphas = np.logspace(-6, 2, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(40, 20))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Lasso coefficients as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('', \nxy=(0.1, -0.6), xytext=(0.1, -0.4), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","219886b6":"start_time = time.clock()\n\nclf = Lasso()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    #clf.fit(X2, Y)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1).mean())\n\nplt.figure(figsize=(40, 20))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights are \\nshrinked to zero', \nxy=(0.1, -0.60), xytext=(0.1, -0.58), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('Ridge still perform good at that point', \nxy=(100, -0.49), xytext=(100, -0.51), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.48])\n#plt.xlim([-17.5,17.5])\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","cc22133e":"model4 = ElasticNet(alpha=0.0001,l1_ratio=0.5,random_state=seed)\nresults4 = cross_val_score(model4, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Elastic Net (Manual Tuning): ({0:.3f}) +\/- ({1:.3f})\".format(results4.mean(), results4.std()))","ef7b786e":"lasso = Lasso(alpha=0.0001)\nlasso.fit(X_train, y_train)\nridge = Ridge(alpha=1,random_state=seed)\nridge.fit(X_train, y_train)\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)\nenet = ElasticNet(alpha=0.0001, l1_ratio=0.5)\nenet.fit(X_train, y_train)\n\nplt.figure(figsize = (40, 20))\nplt.plot(enet.coef_, color='red', linewidth=2,label='Elastic net coefficients with \u03b1 = 0.0001 & l1 Ratio = 0.5')\nplt.plot(lasso.coef_, color='black', linewidth=2,label='Lasso coefficients with \u03b1 = 0.0001')\nplt.plot(ridge.coef_, color='green', linewidth=2,label='Ridge coefficients with \u03b1 = 1')\nplt.plot(linear.coef_, color='blue', linewidth=2,label='Linear coefficients with no regularization')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.8,0.9])\nplt.xlim([-5,75])\nplt.legend(loc='best')\nplt.title('Coefficient Distribution According to the Linear Regression type')\nplt.xlabel('Binary Variable Ranked (cat1, cat2,...,cat72)')\nplt.ylabel('Estimated Coefficient Value')\nplt.show()","4278fc14":"# Ridge & Lasso coefficients as a function of the regularization Alpha parameter"}}