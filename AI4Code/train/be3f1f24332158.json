{"cell_type":{"9e4570c1":"code","595b91a2":"code","4dab21a4":"code","12d9c043":"code","095d221d":"code","a3570c82":"code","d942e09d":"code","59fc5718":"code","dbeed0d9":"code","f9b54668":"code","0013d7bf":"code","24a82e45":"code","fccfb891":"code","93af76e0":"code","01442ac6":"code","a2b3c942":"code","6d437f42":"code","a009d4ca":"code","59d084e8":"code","8a252ac0":"code","279a157c":"code","8911b3f3":"code","f855b6bd":"code","d7184bbe":"code","a798bb8b":"code","277ebd9d":"markdown","2ee1b0c1":"markdown","98bc4e0a":"markdown","51e52852":"markdown","f45a09e6":"markdown","2b9a57a0":"markdown","7d97ce69":"markdown","750b750c":"markdown","a3dc1eb3":"markdown","6d6ec2f4":"markdown","7f9e9ac9":"markdown","4983fccd":"markdown","0846fa68":"markdown","640dc490":"markdown","bd2a31ce":"markdown"},"source":{"9e4570c1":"#This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# importing pckgs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","595b91a2":"# loading tran & test data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\") #training set\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\") #test set\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']","4dab21a4":"# Exploring train set\ntrain.info()\ntrain.describe(include='all')","12d9c043":"# Exploring test set\ntest.info()\ntest.describe(include='all')","095d221d":"train.isna().sum()","a3570c82":"# Taken from user Pegasus - https:\/\/www.kaggle.com\/ritesh1993\/titanic-basic\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n#train.head(5)\n#test.head(5)\n\nfull_data = [train, test] # combine dataset (for cleaning\/feature engineering on both datasets)","d942e09d":"# Feature engineering steps taken from Sina - https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","59fc5718":"# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    ","dbeed0d9":"# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    ","f9b54668":"# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)","0013d7bf":"# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)","24a82e45":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","fccfb891":"# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)","93af76e0":"# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","01442ac6":"# data cleaning\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","a2b3c942":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp'] # elements to drop\n\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nX = train.values # Creates an array of the train data\ntest = test.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=15)\n\n\n","6d437f42":"from sklearn.metrics import accuracy_score\ny_train.shape, X_test.shape, X_train.shape, y_test.shape","a009d4ca":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nrf_predictions = random_forest.predict(test)\n\n# Evaluate acc\ny_pred = random_forest.predict(X_test)\nacc_rf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_rf)","59d084e8":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nlg_pred = model.predict(test)\n\n# Evaluate acc\ny_pred = model.predict(X_test)\nacc_lr = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_lr)","8a252ac0":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train,y_train)\n\ndt_pred = decision_tree.predict(test)\n\n# Evaluate acc\ny_pred = decision_tree.predict(X_test)\nacc_dt = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_dt)","279a157c":"# import xgboost as xgb\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nclf = XGBClassifier()\n\nxgbc_model = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)\nxgbc_model.fit(X_train,y_train,verbose=True)\n\nxgbc_pred = xgbc_model.predict(test)\n\n# Evaluate acc\ny_pred = xgbc_model.predict(X_test)\nacc_xgbc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_xgbc)","8911b3f3":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngaussian_pred = gaussian.predict(test)\n\n# Evaluate acc\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gaussian)","f855b6bd":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ngbk_pred = gbk.predict(test)\n\n# Evaluate acc\ny_pred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","d7184bbe":"# Generate Submission File\ndef submit(x,y):\n    submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': x })\n    \n    submission.to_csv(f\"Submission_{y}.csv\", index=False)\n\n    \nsubmit(rf_predictions, 'randomforest') # submit randomforest\nsubmit(lg_pred, 'logReg') # submit logistic regression\nsubmit(dt_pred, 'decisionTree') # submit decision tree\nsubmit(xgbc_pred, 'xgbc') # submit xgbc\nsubmit(gaussian_pred, 'gaussianNB') # submit gaussian naive bayes\nsubmit(gbk_pred, 'gradientBoost') # submit gradient boost classifier","a798bb8b":" from IPython.display import FileLink\n    FileLink(r'df_name.csv')","277ebd9d":"## Gaussian Naive Bayes","2ee1b0c1":"## Decision Tree","98bc4e0a":"# (Quick) Data exploration","51e52852":"<a href=\"\/kaggle\/input\/titanic\/Submission_{y}.csv\"> Download File <\/a>","f45a09e6":"# Loading dataset\nThis is my first kernel. \n\n**Please feel free to comment your inputs\/suggestions if you have any. I strive to learn and your feedback is much appreciated :) **","2b9a57a0":"NOTE: Age and Cabin seems too have a lot of missing values","7d97ce69":"## Gradient Boosting Classifer","750b750c":"# Further improvement\n\n* I will update this to include ensembles. Maybe stacking with weights. \n* Try other models \n* Try SMOTE?\n* Finetune feature engineering & cleaning section through visualization techniques\n\nSuggestions is much appreciated. ","a3dc1eb3":"# Feature engineering & Data Cleaning\n\nThis section was taken from:\n* Sina - https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\n> (Please look at their notebooks to get great feature engineering\/data cleaning insights)\n\nIn this section we will try to extract new information from the given dataset.","6d6ec2f4":"# Submitting\n\nOur predictions are now finally ready to be submitted\n","7f9e9ac9":"## XGBClassifier","4983fccd":"## RandomForestClassifier","0846fa68":"## Logistic Regression","640dc490":"# Modeling\n* RandomForestClassifier\n* LogisticRegression\n* DecisionTree\n* XGBClassifier\n* GaussianNB\n* GradientBoostingClassifier","bd2a31ce":"Score: \n* RandomForest       - 0.76076\n* LogisticRegression - 0.78648 (Best score)\n* DecisionTree       - 0.72727\n* XGBClassifier      - 0.77990\n* GaussianNB.        - 0.71291"}}