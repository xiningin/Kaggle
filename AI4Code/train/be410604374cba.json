{"cell_type":{"38eef227":"code","26c370f3":"code","06c08b27":"code","7af480aa":"code","becdb3dd":"code","2566c229":"code","fd1cff74":"code","91df82f9":"code","a732c7be":"code","8992a4f9":"code","98648a16":"code","0ab2d7ee":"code","8ccb909a":"code","f55e4d11":"code","1459fee7":"code","d9924a5d":"code","df0c5bb9":"code","627538b2":"code","3c51e69a":"code","e966828c":"markdown","46dad2b7":"markdown","e209d65e":"markdown","de1570b4":"markdown","aacb46dc":"markdown","9a567c43":"markdown","360a0a4a":"markdown"},"source":{"38eef227":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom surprise import CoClustering\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import accuracy\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","26c370f3":"# Load Data\nratings = pd.read_csv('\/kaggle\/input\/miniproject2\/data.txt', sep='\\t', header=None)\ngenres = pd.read_csv('\/kaggle\/input\/miniproject2\/movies.txt', sep='\\t', header=None)\n\nratings.columns = ['User Id', 'Movie Id', 'Rating']\ngenres.columns = ['Movie Id', 'Movie Title', 'Unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']","06c08b27":"counts = genres['Movie Title'].value_counts()\nprint(f'There are {len(counts[counts > 1])} movies with multiple ids:')\ncounts[counts > 1]","7af480aa":"print('Number of Ids = {}'.format(genres['Movie Id'].nunique()))\nprint('Number of Ids with at least one rating = {}'.format(ratings['Movie Id'].nunique()))\nprint('All movies have at least one rating.')","becdb3dd":"# Add a 'Movie Titles' Column to ratings\ntitles = []\n\n# for i in range(ratings.shape[0]):\n#     titles.append(genres.iloc[ratings.iloc[i,:]['Movie Id'] - 1]['Movie Title'])\n    \ntitles = [genres.iloc[ratings.iloc[i, :]['Movie Id'] - 1]['Movie Title'] for i in range(ratings.shape[0])]\n\n# titles = np.array(genres.loc[ratings['Movie Id'], 'Movie Title'])\n    \nratings['Movie Title'] = titles\n\n# Drop duplicates from genres and assign new ids\ngenres = genres.drop_duplicates(subset='Movie Title', keep='first')\n\n# for i in range(genres.shape[0]):\n#     genres.iloc[i, genres.columns.get_loc('Movie Id')] = i + 1\n    \ngenres.loc[:, 'Movie Id'] = np.arange(genres.shape[0]) + 1","2566c229":"# Assign new ids to ratings using the 'Movie Titles' Column\nr_m_id = ratings.columns.get_loc('Movie Id')\nr_m_title = ratings.columns.get_loc('Movie Title')\ng_m_id = genres.columns.get_loc('Movie Id')\n\n# Give each movie in ratings the appropriate id.\nfor i in range(ratings.shape[0]):\n    ratings.iloc[i, r_m_id] = genres[genres['Movie Title'] == ratings.iloc[i, r_m_title]].iloc[0, g_m_id]\n\nratings.drop(['Movie Title'], axis=1, inplace=True)","fd1cff74":"# Create a histogram with all of the movie data.\nplt.figure()\nplt.hist(ratings['Rating'], bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], density=True)\nplt.xlabel('Movie Rating')\nplt.ylabel('Frequency of Rating')\nplt.title('Frequency of Rating for All Movies')\nplt.show()","91df82f9":"# Create a histogram with only the most popular movies.\npopular_indices = ratings['Movie Id'].value_counts()[:10].index\npopular = ratings[ratings['Movie Id'].isin(popular_indices)]\n\nplt.figure()\nplt.hist(popular['Rating'], bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], density=True)\nplt.xlabel('Movie Rating')\nplt.ylabel('Frequency of Rating')\nplt.title('Frequency of Rating for 10 Most Popular Movies')\nplt.show()","a732c7be":"# Create a histogram with only the best (highest mean rating) movies.\nbest_data = ratings.set_index(['Movie Id']).sort_index()\nbest_data = pd.DataFrame([[i, best_data.loc[i, 'Rating'].mean()] for i in np.unique(best_data.index)], columns=['Movie Id', 'Mean Rating'])\nbest_indices = best_data.sort_values(by=['Mean Rating'], ascending=False)['Movie Id'][:10]\nbest = ratings[ratings['Movie Id'].isin(best_indices)]\n\nplt.figure()\nplt.hist(best['Rating'], bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], density=True)\nplt.xlabel('Movie Rating')\nplt.ylabel('Frequency of Rating')\nplt.title('Frequency of Rating for 10 Best Movies')\nplt.show()","8992a4f9":"# Create a histogram for three genres.\ngen = ['Comedy', 'Drama', 'Film-Noir']\n\nfor g in gen:\n    g_ratings = ratings[ratings['Movie Id'].isin(genres[genres[g] == 1]['Movie Id'])]\n    \n    plt.figure()\n    plt.hist(g_ratings['Rating'], bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], density=True)\n    plt.xlabel('Movie Rating')\n    plt.ylabel('Frequency of Rating')\n    plt.title(f'Frequency of Rating for {g} genre')\n    plt.show()","98648a16":"def visualize(V):\n    '''\n    Plot 10 random movies, the 10 most popular movies, the 10 best movies,\n    and 10 movies from the given genres on a 2-dimensional projection of\n    the collaborative filtering model.\n    \n    args:\n        V. The matrix corresponding to movies in the collaborative filtering\n           model.\n    '''\n    \n    # Get the SVD of V\n    A, S, B = np.linalg.svd(V.T, full_matrices=False)\n    A = A[:, :2]\n\n    # Calculate the 2-dimensional projections of U and V.\n    V_proj = (V @ A).T\n\n    # Normalise the U and V projection to have zero mean and unit variance.\n    V_proj = (V_proj - V_proj.mean(axis=1).reshape((-1, 1))) \/ V_proj.std(axis=1).reshape((-1, 1))\n\n    # Get ten random movies, and ten movies from each genre in the genre list.\n    random_indices = np.random.choice(ratings['Movie Id'], size=10, replace=False)\n    genre_indices = np.unique(np.array([np.array(genres[genres[g] == 1]['Movie Id'])[:10] for g in gen]).reshape(-1))\n    \n    graphs = [\n        (random_indices, 'Visualization of 10 Random Movies'),\n        (popular_indices, 'Visualization of 10 Most Popular Movies'),\n        (best_indices, 'Visualization of 10 Best Movies'),\n        (genre_indices, f'Visualization of 10 Movies from {\", \".join(gen)} Genres')\n    ]\n    for indices, title in graphs:\n        Vp = V_proj[:, indices-1]\n        titles = np.array(genres[genres['Movie Id'].isin(indices)].loc[:, 'Movie Title'])\n\n        # Plot the projections on the plane.\n        plt.figure(figsize=(10, 10))\n        plt.scatter(Vp[0], Vp[1])\n\n        # Annotate all of the data points with the movie title.\n        for i, pt in enumerate(Vp.T):\n            plt.annotate(titles[i-1], pt, textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n        plt.title(title)\n        plt.show()","0ab2d7ee":"def grad_U(Ui, Yij, Vj, reg, eta):\n    \"\"\"\n    Takes as input Ui (the ith row of U), a training point Yij, the column\n    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to Ui multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to u_i.\n    reg_term = reg * Ui\n    pred_term = -(Yij - np.dot(Ui, Vj)) * Vj\n    return eta * (reg_term + pred_term)\n\ndef grad_V(Vj, Yij, Ui, reg, eta):\n    \"\"\"\n    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n    Ui (the ith row of U), reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to Vj multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to v_i.\n    reg_term = reg * Vj\n    pred_term = -(Yij - np.dot(Ui, Vj)) * Ui\n    return eta * (reg_term + pred_term)\n    \ndef get_err(U, V, Y, reg=0.0):\n    \"\"\"\n    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n    j is the index of a movie, and Y_ij is user i's rating of movie j and\n    user\/movie matrices U and V.\n\n    Returns the mean regularized squared-error of predictions made by\n    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n    \"\"\"\n    \n    # Calculate the error of the model.\n    reg_term = 0.5 * reg * (np.linalg.norm(U)**2 + np.linalg.norm(V)**2)\n    pred_term = 0.5 * np.mean([(Yij - np.dot(U[i-1], V[j-1]))**2 for i, j, Yij in Y])\n    \n    return reg_term + pred_term\n\n    \n\ndef train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300, verbose=False):\n    \"\"\"\n    Given a training data matrix Y containing rows (i, j, Y_ij)\n    where Y_ij is user i's rating on movie j, learns an\n    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n    by (UV^T)_ij.\n\n    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n    MSE between epochs is smaller than a fraction <eps> of the decrease in\n    MSE after the first epoch.\n\n    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n    of the model.\n    \"\"\"\n    \n    if verbose: print(f'Training latent factor model for {max_epochs} epochs: ', end='')\n    \n    # Initialise the U and V matrices to uniform random numbers.\n    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n    \n    err = get_err(U, V, Y, reg)\n    # Run SGD on the training data Y.\n    for ep in range(max_epochs):\n        if verbose: print('.', end='')\n        \n        # Update the matrices.\n        for i, j, Yij in np.random.permutation(Y):\n            gradu = grad_U(U[i-1], Yij, V[j-1], reg, eta)\n            gradv = grad_V(V[j-1], Yij, U[i-1], reg, eta)\n            \n            U[i-1] = U[i-1] - gradu\n            V[j-1] = V[j-1] - gradv\n            \n        # Calculate the change in error, and break if exit condition met.\n        new_err = get_err(U, V, Y, reg)\n        if ep == 0:\n            delta_first_err = err - new_err\n        elif err - new_err < eps * delta_first_err:\n            if verbose: print('\\nStopping condition reached.', end='')\n            break\n        err = new_err\n    \n    if verbose: print()\n    return U, V, get_err(U, V, Y)","8ccb909a":"# Split the data into training and testing set.\nt = 4 * ratings.shape[0] \/\/ 5\nY_train = np.array(ratings.iloc[:t])\nY_test = np.array(ratings.iloc[t:])\n\n# The maximum user id and movie id.\nM = max(ratings.iloc[:, 0])  # max user id.\nN = max(ratings.iloc[:, 1])  # max movie id.\n\nK = 20\nreg = 0.0\neta = 0.03\nE_in = []\nE_out = []\n\n# Run the model and compute Ein and Eout\nU, V, err = train_model(M, N, K, eta, reg, Y_train)\nE_in = err\nE_out = get_err(U, V, Y_test)\n\n# Report the final results\nprint(f'Train error: {E_in}')\nprint(f'Test error:  {E_out}')","f55e4d11":"visualize(V)","1459fee7":"def grad_U(Ui, Yij, Vj, mu, ai, bj, reg, eta):\n    \"\"\"\n    Takes as input Ui (the ith row of U), a training point Yij, the column\n    vector Vj (jth column of V^T), the user bias term ai, the movie bias term bj,\n    the average of all observation mu, reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to Ui multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to u_i.\n    reg_term = reg * Ui\n    pred_term = -(Yij - mu - np.dot(Ui, Vj) - ai - bj) * Vj\n    return eta * (reg_term + pred_term)\n\ndef grad_V(Ui, Yij, Vj, mu, ai, bj, reg, eta):\n    \"\"\"\n    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n    Ui (the ith row of U), reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to Vj multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to v_i.\n    reg_term = reg * Vj\n    pred_term = -(Yij - mu - np.dot(Ui, Vj) - ai - bj) * Ui\n    return eta * (reg_term + pred_term)\n\ndef grad_a(Ui, Yij, Vj, mu, ai, bj, reg, eta):\n    \"\"\"\n    Takes as input Ui (the ith row of U), a training point Yij, the column\n    vector Vj (jth column of V^T), the user bias term ai, the movie bias term bj,\n    the average of all observation mu, reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to ai multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to u_i.\n    reg_term = reg * ai\n    pred_term = -(Yij - mu - np.dot(Ui, Vj) - ai - bj)\n    return eta * (reg_term + pred_term)\n\ndef grad_b(Ui, Yij, Vj, mu, ai, bj, reg, eta):\n    \"\"\"\n    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n    Ui (the ith row of U), reg (the regularization parameter lambda),\n    and eta (the learning rate).\n\n    Returns the gradient of the regularized loss function with\n    respect to bj multiplied by eta.\n    \"\"\"\n    \n    # Calculate the gradient with respect to v_i.\n    reg_term = reg * bj\n    pred_term = -(Yij - mu - np.dot(Ui, Vj) - ai - bj)\n    return eta * (reg_term + pred_term)\n    \ndef get_err(U, V, Y, mu, a, b, reg=0.0):\n    \"\"\"\n    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n    j is the index of a movie, and Y_ij is user i's rating of movie j and\n    user\/movie matrices U and V.\n\n    Returns the mean regularized squared-error of predictions made by\n    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n    \"\"\"\n    \n    # Calculate the error of the model.\n    reg_term = 0.5 * reg * (np.linalg.norm(U)**2 + np.linalg.norm(V)**2 + np.linalg.norm(a)**2 + np.linalg.norm(b)**2)\n    pred_term = 0.5 * np.mean([(Yij - mu - np.dot(U[i-1], V[j-1]) - a[i-1] - b[j-1])**2 for i, j, Yij in Y])\n    \n    return reg_term + pred_term\n\ndef train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300, verbose=False):\n    \"\"\"\n    Given a training data matrix Y containing rows (i, j, Y_ij)\n    where Y_ij is user i's rating on movie j, learns an\n    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n    by (UV^T)_ij.\n\n    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n    MSE between epochs is smaller than a fraction <eps> of the decrease in\n    MSE after the first epoch.\n\n    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n    of the model.\n    \"\"\"\n    \n    if verbose: print(f'Training latent factor model for {max_epochs} epochs: ', end='')\n    \n    # Initialise the U and V matrices to uniform random numbers.\n    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n    \n    # Find mu, and initialise a and b vectors.\n    mu = Y[:, 2].mean()\n    a = np.zeros(M)\n    b = np.zeros(N)\n    \n    err = get_err(U, V, Y, mu, a, b, reg)\n    # Run SGD on the training data Y.\n    for ep in range(max_epochs):\n        if verbose: print('.', end='')\n        \n        # Update the matrices.\n        for i, j, Yij in np.random.permutation(Y):\n            # Calculate all of the gradients.\n            gradu = grad_U(U[i-1], Yij, V[j-1], mu, a[i-1], b[j-1], reg, eta)\n            gradv = grad_V(V[j-1], Yij, U[i-1], mu, a[i-1], b[j-1], reg, eta)\n            grada = grad_a(U[i-1], Yij, V[j-1], mu, a[i-1], b[j-1], reg, eta)\n            gradb = grad_b(U[i-1], Yij, V[j-1], mu, a[i-1], b[j-1], reg, eta)\n            \n            # Update all of the parameters according to their gradients.\n            U[i-1] = U[i-1] - gradu\n            V[j-1] = V[j-1] - gradv\n            a[i-1] = a[i-1] - grada\n            b[j-1] = b[j-1] - gradb\n            \n        # Calculate the change in error, and break if exit condition met.\n        new_err = get_err(U, V, Y, mu, a, b, reg)\n        if ep == 0:\n            delta_first_err = err - new_err\n        elif err - new_err < eps * delta_first_err:\n            if verbose: print('\\nStopping condition reached.', end='')\n            break\n        err = new_err\n    \n    if verbose: print()\n    return U, V, a, b, get_err(U, V, Y, mu, a, b)","d9924a5d":"# Split the data into training and testing set.\nt = 4 * ratings.shape[0] \/\/ 5\nY_train = np.array(ratings.iloc[:t])\nY_test = np.array(ratings.iloc[t:])\n\n# The maximum user id and movie id.\nM = max(ratings.iloc[:, 0])  # max user id.\nN = max(ratings.iloc[:, 1])  # max movie id.\n\nK = 20\nreg = 0.0\neta = 0.03\nE_in = []\nE_out = []\n\n# Run the model and compute Ein and Eout\nU, V, a, b, err = train_model(M, N, K, eta, reg, Y_train, verbose=False)\nE_in = err\n\nmu = Y_test[:, 2].mean()\nE_out = get_err(U, V, Y_test, mu, a, b)\n\n# Report the final results\nprint(f'Train error: {E_in}')\nprint(f'Test error:  {E_out}')","df0c5bb9":"# Visualise the V matrix.\nvisualize(V)","627538b2":"# Using Surprise SVD\nsvd = SVD(n_factors=20)\nreader = Reader(rating_scale=(1, 5))\n\n# Load the dataset into a Surprise Dataset\ndata = Dataset.load_from_df(ratings[['User Id', 'Movie Id', 'Rating']], reader)\n\n# Automatically split the data into training and testing sets\ntrainset, testset = train_test_split(data, test_size=0.1)\n\n# Fit the model to the training set\nsvd.fit(trainset)\n\n# Predict the training and test sets\ntrain_predictions = svd.test(trainset)\ntest_predictions = svd.test(testset)","3c51e69a":"# Retrieve the training and testing errors\nE_in = accuracy.rmse(train_predictions)\nE_out = accuracy.rmse(test_predictions)\n\nprint(f'Train error: {E_in}')\nprint(f'Test error:  {E_out}')","e966828c":"## Visualization Code","46dad2b7":"## Approach 2","e209d65e":"# Data Loading\/Cleaning","de1570b4":"# Matrix Factorization","aacb46dc":"# Basic Visualizations","9a567c43":"## Approach 3","360a0a4a":"## Approach 1"}}