{"cell_type":{"9f6d7c99":"code","b4920897":"code","ef5e869f":"code","e764aabf":"code","72d570dd":"code","270fbaf9":"code","d0644d21":"code","2db95e4e":"code","53ac8c49":"code","8b121335":"code","3778816a":"code","4fd73097":"code","4de892da":"code","088b80ea":"code","65b05c2c":"code","a0088019":"code","19267f5b":"markdown","6276f92f":"markdown","62a4e9f7":"markdown","f082afdf":"markdown","e3ae2a74":"markdown","65f29ba7":"markdown","4cd43d96":"markdown","6e4e2539":"markdown","ffe0ee4f":"markdown","6eda31f3":"markdown","f3db6c2a":"markdown","3b6555f4":"markdown","cf0b51d7":"markdown"},"source":{"9f6d7c99":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b4920897":"df= pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","ef5e869f":"df.shape","e764aabf":"df.head()","72d570dd":"df.info()","270fbaf9":"sns.displot(df['Y house price of unit area'], kde=True, aspect=2, color='purple')\nplt.show()","d0644d21":"fig, axes= plt.subplots(nrows=3, ncols=2, figsize=(15,15))\nfig.subplots_adjust(wspace=0.3, hspace=0.3)\n\nfor i in range(1, df.shape[1]-1):\n    axes[(i-1)\/\/2, (i+1)%2].set_title(f'chart {i}').set_size(20)\n    sns.scatterplot(data=df, x=df.iloc[:, i], y='Y house price of unit area', ax=axes[(i-1)\/\/2, (i+1)%2])","2db95e4e":"fig = plt.figure(figsize=(10,5))\nsns.heatmap(df.iloc[:, 1:].corr(), annot=True, cmap='Greens')\nplt.show()","53ac8c49":"X = df.drop(['Y house price of unit area', 'No'],axis=1)\ny = df['Y house price of unit area']","8b121335":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","3778816a":"# Train List of RMSE per degree\ntrain_RMSE_list=[]\n#Test List of RMSE per degree\ntest_RMSE_list=[]\n\nfor d in range(1,10):\n    \n    #Preprocessing\n    #create poly data set for degree (d)\n    polynomial_converter= PolynomialFeatures(degree=d)\n    poly_features= polynomial_converter.fit(X)\n    poly_features= polynomial_converter.transform(X)\n    \n    #Split the dataset\n    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n    \n    #Train the Model\n    polymodel=LinearRegression()\n    polymodel.fit(X_train, y_train)\n    \n    #Predicting on both Train & Test Data\n    y_train_pred=polymodel.predict(X_train)\n    y_test_pred=polymodel.predict(X_test)\n    \n    #RMSE of Train set\n    train_RMSE=np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n    \n    #RMSE of Test Set\n    test_RMSE=np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n    \n    #Append the RMSE to the Train and Test List\n    train_RMSE_list.append(train_RMSE)\n    test_RMSE_list.append(test_RMSE)","4fd73097":"display(pd.DataFrame({'degree': list(range(1, 10)),'train_RMSE': train_RMSE_list,'test_RMSE':test_RMSE_list}).set_index('degree'))\n\nfig = plt.figure(figsize=(10,5))\nplt.plot(range(1,5), train_RMSE_list[:4], label='Train RMSE')\nplt.plot(range(1,5), test_RMSE_list[:4], label='Test RMSE')\n\nplt.xlabel('Polynomial Degree')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()","4de892da":"#create poly data set for degree 2\npolynomial_converter= PolynomialFeatures(degree=2)\npoly_features= polynomial_converter.fit(X)\npoly_features= polynomial_converter.transform(X)\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n\n#Train the Model\npolymodel=LinearRegression()\npolymodel.fit(X_train, y_train)\n\n#Predicting on both Train & Test Data\ny_train_pred=polymodel.predict(X_train)\ny_test_pred=polymodel.predict(X_test)","088b80ea":"test_residuals = y_test - y_test_pred","65b05c2c":"fig = plt.figure()\nax1 = fig.add_axes([0, 0, 1, 1])\nax2 = fig.add_axes([1.2, 0, 1, 1])\n\nsns.scatterplot(x=y_test, y=test_residuals, ax=ax1)\nax1.axhline(y=0, color='r', ls='--')\n\nsns.kdeplot(test_residuals, color='purple', ax=ax2)\nplt.show()","a0088019":"#Note: this model is a over fit model!\n\n#create poly data set for degree 8\npolynomial_converter= PolynomialFeatures(degree=8)\npoly_features= polynomial_converter.fit(X)\npoly_features= polynomial_converter.transform(X)\n\n#Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n\n#Train the Model\npolymodel=LinearRegression()\npolymodel.fit(X_train, y_train)\n\n#Predicting on both Train & Test Data\ny_train_pred=polymodel.predict(X_train)\ny_test_pred=polymodel.predict(X_test)\n\ntest_residuals = y_test - y_test_pred\n\nsns.scatterplot(x=y_test, y=test_residuals)\nplt.axhline(y=0, color='r', ls='--')\nplt.show()","19267f5b":"if you see the above chart of RMSEs, after 2 degree we have low error for training data, but in test data we have high error. in this situation our model will be __overfit__.\n\nfor example, if we assume degree equals to 8, the scatter plot will be like below chart. you can see that dots are in one line and have a pattern. this model is not a good model and is __overfit__.  ","6276f92f":"# Compare the RMSEs:","62a4e9f7":"# Exploratory Data Analysis:\nas you can see, this below chart shows us the distribution of 'house price of unit area'. based on this chart, mean of 'house price of unit area' is about 40. the maximum of price is about 120.","f082afdf":"# Data Overview:\ndataset that i worked on, is about house prices based on these 6 parameters:\n* 1-transaction date\n* 2-house age\n* 3-distance to the nearest MRT station\n* 4-number of convenience stores\n* 5-latitude\n* 6-longitude","e3ae2a74":"# Import the Dataset:","65f29ba7":"based on above chart, the best degree for our model is 2. in this point, test_RMSE is lowest.\n\nnow we must make a model with degree 2 and check the 'test_residuals' and 'y_test' scatter plot.","4cd43d96":"As you can see,the mean of 'test_residuals' is about 0.\nthe scatter plot has no pattern and the dots are distributed almost randomly above and below the red line. If the dots had a pattern, our model would not be a good model.","6e4e2539":"to better understand  the correlations you, can see the last row of this chart. as mentioned, 'house age' and 'distance to the nearest MRT station' have negative correlation with house price. but the 'number of convenience store' and 'geographical location' have positive correlation with house price.\n\n__Note__: Green is shown for positive correlation and white for negative correlation.","ffe0ee4f":"# what is polynomial regression?\nPolynomial regression is a special case of linear regression where we fit a polynomial equation on the data with a curvilinear relationship between the target variable and the independent variables.\nIn a curvilinear relationship, the value of the target variable changes in a non-uniform manner with respect to the predictor (s).\nIn Linear Regression, with a single predictor, we have the following equation:\nlinear regression equation\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/pr2.png\" alt=\"drawing\" width=\"200\"\/>\nwhere,\n\n*          Y is the target,\n*          x is the predictor,\n*         \ud835\udf030 is the bias,\n*         and \ud835\udf031 is the weight in the regression equation\n          \nThis linear equation can be used to represent a linear relationship. But, in polynomial regression, we have a polynomial equation of degree n represented as:\npolynomial regression equation\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/pr3.png\" alt=\"drawing\" width=\"500\"\/>\nHere:\n\n*          \ud835\udf030 is the bias,\n*         \ud835\udf031, \ud835\udf032, \u2026, \ud835\udf03n are the weights in the equation of the polynomial regression,\n*         and n is the degree of the polynomial\n\nThe number of higher-order terms increases with the increasing value of n, and hence the equation becomes more complicated.\n\n## But what if we have more than one predictor?\n\nFor 2 predictors, the equation of the polynomial regression becomes:\n\ntwo degree polynomial regression\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/pr10.png\" alt=\"drawing\" width=\"500\"\/>\n\nwhere,\n\n* Y is the target,\n\n* x1, x2 are the predictors,\n\n* \ud835\udf030 is the bias,\n\n* and, \ud835\udf031, \ud835\udf032, \ud835\udf033, \ud835\udf034, and \ud835\udf035 are the weights in the regression equation\n\nFor n predictors, the equation includes all the possible combinations of different order polynomials. This is known as Multi-dimensional Polynomial Regression.\n\nfor more information you can checkout these websites:\n\n[Introduction to Polynomial Regression (with Python Implementation)](https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/polynomial-regression-python\/).\n\n[polynomial regression(wikipedia)](https:\/\/en.wikipedia.org\/wiki\/Polynomial_regression).","6eda31f3":"# Make and Train the Model:\nin this part we use a for loop to make a model with degree from 1 to 9 and then train the model. next we compare the RMSEs and then choose best degree for model to predict the house price.","f3db6c2a":"# Import all necessary Libraries:\nat first we import the libraries.","3b6555f4":"## correlation: \nto check the correlation of parameters and house price, i displayed the 6 scatter plots to see is there any correlation or not.\n* chart 1: we cannot see the impressive correlation between transaction date and house price\n* chart 2:there is small negative correlation between house age and house price\n* chart 3: as you can see there is a negative corrolation between distance to the nearest MRT station and house price. this means if the 'distance to the nearest MRT station' become more, the house price become less.\n* chart 4: there is a positive correlation. it means for more number of convenience stores, the house price become more.\n* chart 5 and 6: for these charts, there is a positive correlation.","cf0b51d7":"# what is over fitting?"}}