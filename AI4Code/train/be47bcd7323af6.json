{"cell_type":{"ba89f97b":"code","9fdfcdf4":"code","61a6bd14":"code","9af23343":"code","c21f775b":"code","8fec2b48":"code","96fb2d94":"code","57f69758":"code","0bb58bae":"code","3e143f41":"code","55e07a0e":"code","3d56354b":"code","6918f959":"code","b309009d":"code","154504e7":"code","f8c3fccf":"code","a5ff814a":"code","abb9a6fb":"code","c821ee46":"code","214ce986":"code","30d1b682":"code","eb56d6ff":"code","f34cf35f":"code","de77d40e":"code","3753d1d9":"code","238dc51a":"code","5b140d91":"code","f3ff4745":"code","6c150c9d":"code","15a9bd2d":"code","9d4a7823":"code","40373d61":"code","335d1330":"markdown"},"source":{"ba89f97b":"from collections import Counter\nimport json\nimport os\nfrom pprint import pprint\nimport string\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport spacy\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom unidecode import unidecode\n\nsns.set()\nsns.set_context('talk')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9fdfcdf4":"# split corpus file\n!split -l 100000 -d \/kaggle\/input\/kdwd-make-sentences\/wikipedia_intros_sentences.txt wikipedia_intros_sentences_chunks_","61a6bd14":"!ls -lh \/kaggle\/*\/*","9af23343":"!pip uninstall -y tensorflow\n!pip install transformers==2.5.1\n!pip install tokenizers==0.5.2","c21f775b":"from tokenizers import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing","8fec2b48":"VOCAB_SIZE = 50_000\nMAX_FREQUENCY = 2\nMAX_LENGTH = 512\nWORK_DIR = 'WikipediaRoberta'","96fb2d94":"!mkdir {WORK_DIR}","57f69758":"!ls -lh \/kaggle\/*\/*","0bb58bae":"corpus_fname = '\/kaggle\/input\/kdwd-make-sentences\/wikipedia_intros_sentences.txt'","3e143f41":"!head \/kaggle\/input\/kdwd-make-sentences\/wikipedia_intros_sentences.txt","55e07a0e":"tokenizer = ByteLevelBPETokenizer()","3d56354b":"tokenizer.train(\n    files=[corpus_fname], \n    vocab_size=VOCAB_SIZE, \n    min_frequency=MAX_FREQUENCY, \n    special_tokens=[\n        \"<s>\",\n        \"<pad>\",\n        \"<\/s>\",\n        \"<unk>\",\n        \"<mask>\",\n])","6918f959":"tokenizer.save(WORK_DIR)","b309009d":"!ls -lh \/kaggle\/*\/*","154504e7":"encoded = tokenizer.encode('I am a sentence.')\nprint(encoded.ids)\nprint(encoded.tokens)\nprint(encoded.type_ids)\nprint(encoded.offsets)\nprint(encoded.attention_mask)\nprint(encoded.special_tokens_mask)\nprint(encoded.overflowing)\nprint(encoded.original_str)\nprint(encoded.normalized_str)","f8c3fccf":"vocab = json.load(open(os.path.join(WORK_DIR, 'vocab.json'), 'r'))","a5ff814a":"list(vocab.keys())[0:20]","abb9a6fb":"!head {WORK_DIR}\/merges.txt","c821ee46":"tokenizer = ByteLevelBPETokenizer(\n    f\"{WORK_DIR}\/vocab.json\",\n    f\"{WORK_DIR}\/merges.txt\",\n)\ntokenizer._tokenizer.post_processor = BertProcessing(\n    (\"<\/s>\", tokenizer.token_to_id(\"<\/s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=MAX_LENGTH)\n\nencoded = tokenizer.encode(unidecode(\"My name is Gabriel.\"))\nprint(encoded)\nprint(encoded.tokens)","214ce986":"import torch\ntorch.cuda.is_available()","30d1b682":"!wget -c https:\/\/raw.githubusercontent.com\/huggingface\/transformers\/v2.5.1\/examples\/run_language_modeling.py","eb56d6ff":"!ls -lh \/kaggle\/*\/*","f34cf35f":"import json\nconfig = {\n    \"architectures\": [\"RobertaForMaskedLM\"],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"max_position_embeddings\": 514,\n    \"model_type\": \"roberta\",\n    \"num_attention_heads\": 12,\n    \"num_hidden_layers\": 6,\n    \"type_vocab_size\": 1,\n    \"vocab_size\": VOCAB_SIZE\n}\nwith open(os.path.join(WORK_DIR, 'config.json'), 'w') as fp:\n    json.dump(config, fp)\n\ntokenizer_config = {\n    \"max_len\": MAX_LENGTH\n}\nwith open(os.path.join(WORK_DIR, 'tokenizer_config.json'), 'w') as fp:\n    json.dump(tokenizer_config, fp)","de77d40e":"!ls -lh \/kaggle\/*\/*","3753d1d9":"cmd = \"\"\"\n  python run_language_modeling.py\n  --train_data_file {}\n  --output_dir {}\n  --model_type roberta\n  --mlm\n  --config_name {}\n  --tokenizer_name {}\n  --do_train\n  --line_by_line\n  --learning_rate 2.5e-5\n  --num_train_epochs 1\n  --save_total_limit 2\n  --save_steps 2000\n  --per_gpu_train_batch_size 4\n  --seed 42\n\"\"\".replace(\"\\n\", \" \").format(\n    '\/kaggle\/working\/wikipedia_intros_sentences_chunks_00', \n    WORK_DIR + '-small-v1', \n    WORK_DIR, \n    WORK_DIR\n)","238dc51a":"!rm -rf WikipediaRoberta-small-v1","5b140d91":"%%time\n!{cmd}","f3ff4745":"!ls -lh \/kaggle\/*\/*","6c150c9d":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model = WORK_DIR + '-small-v1',\n    tokenizer = WORK_DIR + '-small-v1'\n)","15a9bd2d":"result = fill_mask(\"The sun <mask>.\")\nresult","9d4a7823":"result = fill_mask(\"This is the beginning of a beautiful <mask>.\")\nresult","40373d61":"result = fill_mask(\"Playing music is <mask> for your ears.\")\nresult","335d1330":"# Kensho Derived Wikimedia Dataset - Hugging Face Language Model from Scratch\n\nbased on this colab notebook https:\/\/colab.research.google.com\/github\/huggingface\/blog\/blob\/master\/notebooks\/01_how_to_train.ipynb"}}