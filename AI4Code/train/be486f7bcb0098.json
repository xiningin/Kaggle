{"cell_type":{"5e287b1e":"code","3eabd4ee":"code","903141b4":"code","2603075a":"code","b0a3e4f9":"code","7719606c":"code","d1619a6a":"code","16fcf2fd":"code","cac4c509":"code","c69c56d2":"code","4c1c0e29":"code","e896f56b":"code","cc83255f":"code","475c31eb":"markdown","8068d7d9":"markdown"},"source":{"5e287b1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3eabd4ee":"TRAINING = False","903141b4":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\n\ntrain = train.query('weight>0').reset_index(drop=True)\n\ntrain.fillna(train.median(),inplace=True)\n\ntrain['feature_stock_id_sum'] = train['feature_41'] + train['feature_42'] + train['feature_43']\ntrain['feature_1_2_cross'] = train['feature_1']\/(train['feature_2']+1e-5)\n\n\nNUM_TRAIN_EXAMPLES = len(train)","2603075a":"features = [c for c in train.columns if 'feature' in c]\n\nf_mean = np.nanmean(train[features[1:]].values,axis=0)","b0a3e4f9":"\nPATH = '..\/input\/densenetneu'","7719606c":"resp_cols = [c for c in train.columns if 'resp' in c]","d1619a6a":"import tensorflow_addons as tfa\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_model(input_shape):\n    \n    inp = tf.keras.layers.Input(input_shape)\n    tmp = tf.keras.layers.BatchNormalization()(inp)\n    xs = [tmp]\n    for _ in range(10):\n        if len(xs) > 1:\n            tmp = tf.keras.layers.Concatenate(axis=-1)(xs)\n        else:\n            tmp = xs[0]\n        tmp = tf.keras.layers.Dense(64,activation='mish')(tmp)\n        tmp = tf.keras.layers.BatchNormalization()(tmp)\n        tmp = tf.keras.layers.Dropout(0.2)(tmp)\n        xs.append(tmp)\n    \n    output = tf.keras.layers.Dense(len(resp_cols),activation='sigmoid')(tf.keras.layers.Concatenate()(xs))\n    model = tf.keras.models.Model(inp,output)\n    optimizer = tfa.optimizers.RectifiedAdam(1e-3)\n    model.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n                    metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n    return model","16fcf2fd":"import random\nimport os\ndef set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","cac4c509":"#Designed to do all features at the same time, but Kaggle kernels are memory limited.\nclass NeutralizeTransform:\n    def __init__(self,proportion=1.0):\n        self.proportion = proportion\n    \n    def fit(self,X,y):\n        self.lms = []\n        self.mean_exposure = np.mean(y,axis=0)\n        self.y_shape = y.shape[-1]\n        for x in X.T:\n            scores = x.reshape((-1,1))\n            exposures = y\n            exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n            \n            transform = np.linalg.lstsq(exposures, scores, rcond=None)[0]\n            self.lms.append(transform)\n            \n    def transform(self,X,y=None):\n        out = []\n        for i,transform in enumerate(self.lms):\n            x = X[:,i]\n            scores = x.reshape((-1,1))\n            exposures = np.repeat(self.mean_exposure,len(x),axis=0).reshape((-1,self.y_shape))\n            exposures = np.concatenate([exposures,np.array([np.mean(scores)] * len(exposures)).reshape((-1,1))],axis=1)\n            correction = self.proportion * exposures.dot(transform)\n            out.append(x - correction.ravel())\n            \n        return np.asarray(out).T\n    \n    def fit_transform(self,X,y):\n        self.fit(X,y)\n        return self.transform(X,y)\n        \n","c69c56d2":"%%time\nif TRAINING:\n    mask = train[features].isna()\n    train.fillna(0,inplace=True)\n    for feature in features:\n        nt = NeutralizeTransform(proportion=0.25)\n        train[feature] = nt.fit_transform(train[feature].values.reshape((-1,1)),\n                                          train['resp'].values.reshape((-1,1)))\n        pd.to_pickle(nt,f'NeutralizeTransform_{feature}.pkl')\n    train[mask] = np.nan\n    \nelse:\n    nts = []\n    for feature in features:\n        nt = pd.read_pickle(f'{PATH}\/NeutralizeTransform_{feature}.pkl')\n        nts.append(nt)","4c1c0e29":"import gc\ngc.collect()","e896f56b":"X_tr = train.query('date<440')[features].values\ny_tr = (train.query('date<440')[resp_cols].values > 0).astype(int)\n    \nX_val = train.query('date>460')[features].values\ny_val = (train.query('date>460')[resp_cols].values > 0).astype(int)\n\ndel train\ngc.collect()\n\n\nif TRAINING:\n    metric = {}\n    \n    for seed in [6, 28, 496, 8128]:\n        set_all_seeds(seed)\n\n        model = create_model(X_tr.shape[-1])\n        hist = model.fit(X_tr,y_tr,\n                         validation_data=(X_val,y_val),\n                         epochs=200,\n                         batch_size=8192,\n                         callbacks=[tf.keras.callbacks.EarlyStopping('val_binary_accuracy',mode='max',patience=20,restore_best_weights=True),\n                                   tf.keras.callbacks.ReduceLROnPlateau('val_binary_accuracy',mode='max',patience=10,cooldown=5)])\n        \n        model.save_weights(f'model_{seed}.tf')\n        metric[seed] = max(hist.history['val_binary_accuracy'])\n        \n        tf.keras.backend.clear_session()\n        \n    print(metric)        \nelse:\n    models = []\n    for seed in [6, 28, 496, 8128]:\n        model = create_model(X_tr.shape[-1])\n        model.load_weights(f'{PATH}\/model_{seed}.tf')\n        model.call = tf.function(model.call, experimental_relax_shapes=True)\n        models.append(model)\n","cc83255f":"from tqdm import tqdm\nif not TRAINING:\n    f = np.median\n    import janestreet\n    janestreet.competition.make_env.__called__ = False\n    env = janestreet.make_env()\n    th = 0.50\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        if test_df['weight'].item() > 0:\n            \n            test_df['feature_stock_id_sum'] = test_df['feature_41'] + test_df['feature_42'] + test_df['feature_43']\n            test_df['feature_1_2_cross'] = test_df['feature_1']\/(test_df['feature_2']+1e-5)\n            \n            x_tt = test_df.loc[:, features].values\n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n \n            for i in range(len(nts)):\n                x_tt[:,i] = nts[i].transform(np.expand_dims(x_tt[:,i],0))\n            \n            p = f(np.mean([model(x_tt,training=False).numpy() for model in models],axis=0))\n    \n            pred_df.action = np.where(p > th, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","475c31eb":"One of the adavantages of DenseNet is the ability to have a deep narrow network without loss in performance.","8068d7d9":"See https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/215305"}}