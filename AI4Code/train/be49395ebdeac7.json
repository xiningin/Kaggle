{"cell_type":{"73791131":"code","7281b1c5":"code","bd59da7e":"code","7d36cc30":"code","19f0a832":"code","3b877f0e":"code","06a39467":"code","5ca4415c":"code","bccb9a09":"code","5d3d5b5b":"code","cf8f8dd2":"code","39ad32fe":"code","acccf326":"code","be618a50":"code","ae54b351":"code","975cab83":"markdown","71297639":"markdown"},"source":{"73791131":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7281b1c5":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ntest_Id = test_df['Id']","bd59da7e":"train_df.info()","7d36cc30":"test_df.info()","19f0a832":"print (train_df.shape, test_df.shape)\ntrain_df.head()","3b877f0e":"# lets bar plot to see the frequencies of targets\ntrain_df['Target'].value_counts().plot.bar()","06a39467":"def preprocess(train_df, test_df, dropna = False, drop_obtype = True, replace_nan_with = 0, return_with_top100 = False, lgbfactor = 0):\n\n    train_df.drop(columns = ['Id'],inplace = True, errors=\"ignore\")\n    test_df.drop(columns = ['Id'], inplace = True, errors = \"ignore\")\n    nanvalues = train_df.isnull().sum(axis = 0).values\n    nullvalues = pd.DataFrame({\"Column\":train_df.columns, \"Count\":nanvalues})\n    nullvalues.sort_values(by=[\"Count\"], ascending = False, inplace = True)\n    #nullvalues.head()\n    nancols = nullvalues[nullvalues['Count']>0][\"Column\"].values.tolist()\n    \n    # columns with object type. We are dropping them for time being.\n    objcols = train_df[train_df.columns[(train_df.dtypes == 'object').values]].columns.values.tolist()\n    \n    if (drop_obtype == True):\n        train_df.drop(columns = objcols,inplace = True)\n        test_df.drop(columns = objcols,inplace = True)\n    if (dropna == True):\n        train_df.drop(columns = nancols, inplace = True)\n        test_df.drop(columns = nancols, inplace = True)\n    if (dropna == False):\n        train_df.replace(np.nan, replace_nan_with, inplace = True)\n        test_df.replace(np.nan, replace_nan_with, inplace = True)\n    \n    \n    labels = train_df['Target']\n    train_df.drop(columns = ['Target'],inplace = True, errors=\"ignore\")\n\n    \n    # Below we use Random Forest to select top 100 columns. Actually in LightGBM classifier I pass all features.\n    from sklearn.ensemble import RandomForestClassifier\n    rfc = RandomForestClassifier()\n    rfc.fit(train_df, labels)\n    \n    scores = rfc.feature_importances_.tolist()\n    report = pd.DataFrame({\"feature\":train_df.columns, \"score\":scores})\n    report.sort_values(by = [\"score\"],ascending = False, inplace = True)\n    top100cols = report.head(100)['feature'].values.tolist()\n    \n    \n    if (return_with_top100 == True):\n        train_df = train_df[top100cols]\n        test_df = test_df[top100cols]\n    \n    # Since the classes are imbalance. We need to give weights to each class and pass this parameter in LightGBM classifier.\n    from sklearn.utils import class_weight\n    class_weights = class_weight.compute_class_weight('balanced',\n                                                     np.unique(labels),\n                                                     labels)\n    # You might be wondering what is lgbfactor here. Actually LightGBM takes labels starting from 0 i.e 0,1,2,3 instead of 1,2,3,4\n    # so we have to subtract 1 from labels while we use LightGBM.\n    # Note that while submitting the predictions, we have to add 1 again to the predictions so that our labels are from 1,2,3,4\n    class_weights = dict(zip(np.unique(labels.values-lgbfactor), class_weights))\n    \n    \n    return train_df, test_df, labels, nancols, objcols, report, top100cols, class_weights\n    ","5ca4415c":"train, test, labels, nancols,objcols, report, top100cols, class_weightss = preprocess(train_df.copy(), test_df.copy(),dropna = False, lgbfactor = 1)","bccb9a09":"print (train.shape, test.shape)","5d3d5b5b":"import lightgbm as lgb\nimport sklearn.model_selection as model_selection\nfrom sklearn.metrics import f1_score, make_scorer","cf8f8dd2":"lgmodel = lgb.LGBMClassifier(class_weight=class_weightss, metric = \"multi_logloss\",num_class = 4)\n#rf_classif = RandomForestClassifier()\n","39ad32fe":"def get_score(model, train, label, fold):\n    score = model_selection.cross_val_score(model, train , label, cv = fold, scoring = make_scorer(f1_score, average = \"macro\"))\n    return score.mean()","acccf326":"kf = model_selection.KFold(n_splits=5, shuffle=True,random_state=2017)\ntemplabel = labels.loc[train['parentesco1']==1]\ntemptrain = train[train['parentesco1']==1]\nheads_scores = get_score(lgmodel, temptrain, templabel-1, kf) # subtract 1 because of LightGBM, remember the expalantion above 0,1,2,3\noverall_scores = get_score(lgmodel, train, labels-1, kf) # subtract 1 because of LightGBM, remember the expalantion above 0,1,2,3\nprint (\"Heads mean score:\",heads_scores.mean())\nprint (\"Overall mean score:\",overall_scores.mean())","be618a50":"lgmodel.fit(train, labels-1)\nlgpreds = lgmodel.predict(test)","ae54b351":"# Making a submission file #\nsub_df = pd.DataFrame({\"Id\":test_Id.values})\nsub_df[\"Target\"] = lgpreds + 1 # ----------> note here we add 1 to the predictions making the predictions start from 1\nsub_df.to_csv(\"LightGBM.csv\", index=False)","975cab83":"Hello there,  this is my first kernel ever in Kaggle. So I will be happy to get any suggestions from you.\n\nI have used LightGBM Classifier for the classification. This is all what I have done untill now:\n* Replaced NaN values with 0\n* Dropped object type columns for time being\n* Used Class weights to cater the class imbalance problem\n* Leaderboard score is **0.415**","71297639":"**I know there is still much work remaining like Preprocessing, feature selection, feature engineering, one hot encoding the object type columns.**"}}