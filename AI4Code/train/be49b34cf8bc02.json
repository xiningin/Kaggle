{"cell_type":{"54a1d7b8":"code","40ae6af7":"code","120eddd9":"code","0338639c":"code","6e667c4a":"code","a93ac7ba":"code","dd9baca3":"code","7c6475ec":"code","e46f8e59":"code","5ba8a54f":"code","1d815a47":"code","1a3a0f8a":"code","bdf04b48":"code","23669737":"code","18b73f59":"code","043b8a76":"code","292a18aa":"code","54718ffe":"code","e0d65961":"code","0cad7318":"code","e0fcf0e8":"code","a955da08":"code","49fe90e6":"code","2b978979":"code","2f3ce3fb":"code","96700bad":"code","cfcd3846":"code","8541a037":"code","26e8d830":"code","9c57e888":"code","f0dc0a94":"code","08efc7e9":"code","113afb11":"code","91094c05":"code","04f97612":"code","5f2998a2":"code","d0a1f400":"code","6a04502c":"code","ed186108":"code","c990262a":"code","cf7d9b40":"code","0fb12d76":"code","61619e1b":"code","48b2aa4f":"code","6a125eaf":"code","79d1cec5":"code","4fdf2f48":"code","e0d738fc":"code","19566f1c":"code","0ff4b807":"code","ac414b87":"code","900eb196":"code","065cae3b":"code","46d885d5":"code","853be81a":"code","76c85201":"code","462a5d59":"code","3589c287":"code","4db93a6d":"code","4ee9572d":"code","dd7e620d":"code","30f55812":"code","b79194b5":"code","cf40c9df":"code","b5e09632":"code","f14ecfbb":"code","db1f2e94":"code","7191ba8b":"code","aab1e35f":"code","a4079bdc":"code","96a081b5":"code","3b176b4f":"code","f2987720":"code","a108ad92":"code","1d62915d":"code","defe1f10":"code","3512ead2":"code","563593e8":"code","0052cdf1":"code","58dd0b32":"code","42c8a1f5":"code","aa56a6d5":"code","4cd64d0a":"code","4a5dffb1":"code","d797af98":"code","2c8a0730":"code","04e9904a":"code","fa94d2aa":"code","b87daddc":"code","c61ec0ef":"code","2316e9df":"code","a4a88672":"code","e964d6c5":"code","429908fb":"code","73056a68":"code","0dc5d693":"code","1ab26085":"code","0e71c6bc":"code","2c0a0195":"code","14ba8e03":"code","df60d5ed":"code","ab4fddeb":"code","303be04c":"code","1a337e89":"code","e43d70ec":"code","7c10c263":"code","6ec92504":"code","fcc5a2a5":"code","a4c9e850":"code","b9928df4":"code","d33f1514":"code","3e177162":"code","9ab3f57f":"code","780372ef":"code","0c99bc2c":"code","df16f322":"code","10b618f9":"code","416ba3db":"code","6b376256":"code","dcc6f24a":"code","2691a763":"code","c78bd5e6":"code","71ae6699":"code","633ed29e":"code","d4d88afe":"code","f639a126":"code","0611efeb":"code","d6161b87":"code","4ee6fcbd":"code","a9356cf8":"code","8e6ad23c":"code","370d8748":"code","f57811f5":"code","0e5d7b1f":"code","ad16ab46":"code","2d60ff9e":"code","93604fee":"code","f0976312":"code","c39af783":"code","c79028d3":"code","0683b446":"code","cf636bba":"code","144951e9":"code","df782f9e":"code","304447fb":"code","8ebec555":"code","004bed84":"code","6e00ebd7":"code","95573691":"code","8f34985e":"code","0445412b":"code","630783a6":"code","5561d984":"code","340e3e21":"code","0dad660b":"code","c72ccb57":"code","f68c82de":"code","f6e17e43":"code","6cbc185d":"code","cd3ca94c":"code","a7f4e221":"code","7b74d394":"code","5dc1661d":"code","56194aa8":"code","020a1348":"code","6b913dfb":"code","bdf6e257":"code","31dda233":"code","48610e3f":"code","60eff0c8":"code","ba153dd6":"code","40f3d888":"code","98c0149a":"code","7abe59d2":"code","1f594557":"code","ffd2e6ce":"code","dd45bd2a":"code","d0b88f19":"code","4b6c2cea":"code","9bfaf96c":"code","282c130a":"code","d4c8009d":"code","a97da78a":"code","a87edc21":"code","a0d12e9b":"code","fb919df2":"code","20556706":"code","8ce8d072":"code","86776e6d":"code","3a246e5d":"code","96fac274":"code","01dd7e65":"code","d6202799":"code","26642195":"code","5a125e7e":"code","493bfc17":"code","1729de46":"code","09aeaeb9":"code","9ccc3830":"code","b0685a49":"code","c74fd825":"code","ca1e381c":"code","b3a52960":"code","7b41ed43":"code","e135664b":"code","4c8e27d1":"code","784d2f27":"code","b0836987":"code","db1f8192":"code","dbbf29e0":"code","17231138":"code","4d2ab848":"code","b1fecd3b":"code","73123274":"code","fe6dbd6d":"code","9a803e1a":"code","15cfc444":"code","fd4765aa":"code","947b64e3":"code","4f253a3b":"code","82012a20":"code","60f6a9f3":"code","3fe67af7":"code","2d939b54":"code","493d2d76":"code","c38e19e7":"code","daaecfc0":"code","660d4197":"code","a532d9d1":"code","fd49a5b0":"code","4e48dec1":"code","676b572d":"code","05c6fbe8":"code","352479bc":"code","c1914be2":"code","3e0791e9":"code","3b7c486d":"markdown","1b267003":"markdown","50805bb7":"markdown","a0b39b2b":"markdown","f638ed88":"markdown","d41c10e4":"markdown","bb81276f":"markdown","58ab1f84":"markdown","0ff46db2":"markdown","3c0caca1":"markdown","85926c36":"markdown","33b296bc":"markdown","6195073c":"markdown","30a2c7c5":"markdown","34d7e741":"markdown","2bd378a5":"markdown","d3d40b92":"markdown","f04793f7":"markdown","899905ab":"markdown","d09b8235":"markdown","97cb4d93":"markdown","41b86a26":"markdown","4d20723e":"markdown","3c980cb4":"markdown","c30c8c50":"markdown","6685dea1":"markdown","5ad8591e":"markdown","3ddafd15":"markdown","03168ea4":"markdown","38ae98ba":"markdown","4db970b6":"markdown","bb2d86b6":"markdown","58257be4":"markdown","e4fb7aac":"markdown","e3d5be51":"markdown","cd7c1fc9":"markdown","5c7f43cc":"markdown","7787c6a3":"markdown","6d70585f":"markdown","c4b1c909":"markdown","be1c4fc3":"markdown","7238909b":"markdown","555fb5d8":"markdown","1ed630ff":"markdown","5a80356f":"markdown","2532df04":"markdown","6803130e":"markdown","1896ad25":"markdown","d4f621f2":"markdown"},"source":{"54a1d7b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40ae6af7":"df = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)","120eddd9":"cols = ['Fare', 'Embarked', 'Sex', 'Age']\nX = df[cols]","0338639c":"X","6e667c4a":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer","a93ac7ba":"ohe = OneHotEncoder()\nimp = SimpleImputer()","dd9baca3":"ct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),  # apply OneHotEncoder to Embarked and Sex\n    (imp, ['Age']),              # apply SimpleImputer to Age\n    remainder='passthrough')     # include remaining column (Fare) in the output","7c6475ec":"# column order: Embarked (3 columns), Sex (2 columns), Age (1 column), Fare (1 column)\nct.fit_transform(X)","e46f8e59":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)","5ba8a54f":"cols = ['Fare', 'Embarked', 'Sex', 'Age']\nX = df[cols]","1d815a47":"X","1a3a0f8a":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer  # new in 0.20\nfrom sklearn.compose import make_column_selector     # new in 0.22","bdf04b48":"ohe = OneHotEncoder()","23669737":"# all SEVEN of these produce the same results\nct = make_column_transformer((ohe, ['Embarked', 'Sex']))\nct = make_column_transformer((ohe, [1, 2]))\nct = make_column_transformer((ohe, slice(1, 3)))\nct = make_column_transformer((ohe, [False, True, True, False]))\nct = make_column_transformer((ohe, make_column_selector(pattern='E|S')))\nct = make_column_transformer((ohe, make_column_selector(dtype_include=object)))\nct = make_column_transformer((ohe, make_column_selector(dtype_exclude='number')))","18b73f59":"# one-hot encode Embarked and Sex (and drop all other columns)\nct.fit_transform(X)","043b8a76":"import pandas as pd\nX = pd.DataFrame({'Shape':['square', 'square', 'oval', 'circle'],\n                  'Class': ['third', 'first', 'second', 'third'],\n                  'Size': ['S', 'S', 'L', 'XL']})","292a18aa":"# \"Shape\" is unordered, \"Class\" and \"Size\" are ordered\nX","54718ffe":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder","e0d65961":"# left-to-right column order is alphabetical (circle, oval, square)\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(X[['Shape']])","0cad7318":"# category ordering (within each feature) is defined by you\noe = OrdinalEncoder(categories=[['first', 'second', 'third'], ['S', 'M', 'L', 'XL']])\noe.fit_transform(X[['Class', 'Size']])","e0fcf0e8":"import pandas as pd\nX = pd.DataFrame({'col':['A', 'B', 'C', 'B']})\nX_new = pd.DataFrame({'col':['A', 'C', 'D']})","a955da08":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')","49fe90e6":"X","2b978979":"# three columns represent categories A, B, and C\nohe.fit_transform(X[['col']])","2f3ce3fb":"# category D was not learned by OneHotEncoder during the \"fit\" step\nX_new","96700bad":"# category D is encoded as all zeros\nohe.transform(X_new[['col']])","cfcd3846":"import pandas as pd\nimport numpy as np","8541a037":"train = pd.DataFrame({'feat1':[10, 20, np.nan, 2], 'feat2':[25., 20, 5, 3], 'label':['A', 'A', 'B', 'B']})\ntest = pd.DataFrame({'feat1':[30., 5, 15], 'feat2':[12, 10, np.nan]})","26e8d830":"from sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","9c57e888":"imputer = SimpleImputer()\nclf = LogisticRegression()","f0dc0a94":"# 2-step pipeline: impute missing values, then pass the results to the classifier\npipe = make_pipeline(imputer, clf)","08efc7e9":"train","113afb11":"test","91094c05":"features = ['feat1', 'feat2']","04f97612":"X, y = train[features], train['label']\nX_new = test[features]","5f2998a2":"# pipeline applies the imputer to X before fitting the classifier\npipe.fit(X, y)\n\n# pipeline applies the imputer to X_new before making predictions\n# note: pipeline uses imputation values learned during the \"fit\" step\npipe.predict(X_new)","d0a1f400":"import pandas as pd\nimport numpy as np","6a04502c":"X = pd.DataFrame({'Age':[20, 30, 10, np.nan, 10]})","ed186108":"X","c990262a":"from sklearn.impute import SimpleImputer","cf7d9b40":"# impute the mean\nimputer = SimpleImputer()\nimputer.fit_transform(X)","0fb12d76":"# impute the mean and add an indicator matrix (new in 0.21)\nimputer = SimpleImputer(add_indicator=True)\nimputer.fit_transform(X)","61619e1b":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)","48b2aa4f":"cols = ['Fare', 'Embarked', 'Sex']\nX = df[cols]\ny = df['Survived']","6a125eaf":"from sklearn.model_selection import train_test_split","79d1cec5":"X","4fdf2f48":"# any positive integer can be used for the random_state value\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\nX_train","e0d738fc":"\n# using the SAME random_state value results in the SAME random split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\nX_train","19566f1c":"# using a DIFFERENT random_state value results in a DIFFERENT random split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\nX_train","0ff4b807":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)","ac414b87":"cols = ['SibSp', 'Fare', 'Age']\nX = df[cols]","900eb196":"X","065cae3b":"# new in 0.21, and still \"experimental\" so it must be enabled explicitly\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","46d885d5":"impute_it = IterativeImputer()\nimpute_it.fit_transform(X)","853be81a":"# new in 0.22\nfrom sklearn.impute import KNNImputer","76c85201":"impute_knn = KNNImputer(n_neighbors=2)\nimpute_knn.fit_transform(X)","462a5d59":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)","3589c287":"cols = ['Embarked', 'Sex', 'Age', 'Fare']\nX = df[cols]","4db93a6d":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression","4ee9572d":"ohe = OneHotEncoder()\nimp = SimpleImputer()\nclf = LogisticRegression()","dd7e620d":"from sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline","30f55812":"ct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (imp, ['Age']),\n    remainder='passthrough')","b79194b5":"pipe = make_pipeline(ct, clf)","cf40c9df":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","b5e09632":"\nct = ColumnTransformer(\n    [('encoder', ohe, ['Embarked', 'Sex']),\n     ('imputer', imp, ['Age'])],\n    remainder='passthrough')","f14ecfbb":"pipe = Pipeline([('preprocessor', ct), ('classifier', clf)])\npipe","db1f2e94":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=6)\ndf = df[['Age', 'Pclass', 'Survived']]","7191ba8b":"df","aab1e35f":"X = df[['Age', 'Pclass']]\ny = df['Survived']","a4079bdc":"from sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","96a081b5":"pipe = make_pipeline(SimpleImputer(), LogisticRegression())","3b176b4f":"# use semicolon to suppress output in IPython\npipe.fit(X, y);","f2987720":"# display the imputation values for \"Age\" and \"Pclass\"\npipe.named_steps.simpleimputer.statistics_","a108ad92":"# display the model coefficients for \"Age\" and \"Pclass\"\npipe.named_steps.logisticregression.coef_","1d62915d":"import pandas as pd\ntrain = pd.read_csv('http:\/\/bit.ly\/kaggletrain')\ntest = pd.read_csv('http:\/\/bit.ly\/kaggletest', nrows=175)","defe1f10":"train = train[['Survived', 'Age', 'Fare', 'Pclass']]\ntest = test[['Age', 'Fare', 'Pclass']]","3512ead2":"# count the number of NaNs in each column\ntrain.isna().sum()","563593e8":"test.isna().sum()","0052cdf1":"label = train.pop('Survived')","58dd0b32":"# new in 0.22: this estimator (experimental) has native support for NaNs\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier","42c8a1f5":"clf = HistGradientBoostingClassifier()","aa56a6d5":"# no errors, despite NaNs in train and test!\nclf.fit(train, label)\nclf.predict(test)","4cd64d0a":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","4a5dffb1":"cols = ['Sex', 'Name']\nX = df[cols]\ny = df['Survived']","d797af98":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer","2c8a0730":"ohe = OneHotEncoder()\nvect = CountVectorizer()\nct = make_column_transformer((ohe, ['Sex']), (vect, 'Name'))","04e9904a":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver='liblinear', random_state=1)","fa94d2aa":"from sklearn.pipeline import make_pipeline\npipe = make_pipeline(ct, clf)","b87daddc":"from sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()","c61ec0ef":"# specify parameter values to search\nparams = {}\nparams['columntransformer__countvectorizer__min_df'] = [1, 2]\nparams['logisticregression__C'] = [0.1, 1, 10]\nparams['logisticregression__penalty'] = ['l1', 'l2']","2316e9df":"# try all possible combinations of those parameter values\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\ngrid.fit(X, y);","a4a88672":"# what was the best score found during the search?\ngrid.best_score_","e964d6c5":"# which combination of parameters produced the best score?\ngrid.best_params_","429908fb":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","73056a68":"# use single brackets around \"Name\" because CountVectorizer expects 1-D input\nX = df['Name']\ny = df['Survived']","0dc5d693":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline","1ab26085":"pipe = make_pipeline(CountVectorizer(), MultinomialNB())","0e71c6bc":"# cross-validate the pipeline using default parameters\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()","2c0a0195":"# specify parameter values to search (use a distribution for any continuous parameters)\nimport scipy as sp\nparams = {}\nparams['countvectorizer__min_df'] = [1, 2, 3, 4]\nparams['countvectorizer__lowercase'] = [True, False]\nparams['multinomialnb__alpha'] = sp.stats.uniform(scale=1)","14ba8e03":"# try \"n_iter\" random combinations of those parameter values\nfrom sklearn.model_selection import RandomizedSearchCV\nrand = RandomizedSearchCV(pipe, params, n_iter=10, cv=5, scoring='accuracy', random_state=1)\nrand.fit(X, y);","df60d5ed":"# what was the best score found during the search?\nrand.best_score_","ab4fddeb":"# which combination of parameters produced the best score?\nrand.best_params_","303be04c":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","1a337e89":"X = df[['Pclass', 'Sex', 'Name']]\ny = df['Survived']","e43d70ec":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline","7c10c263":"ohe = OneHotEncoder()\nvect = CountVectorizer()\nclf = LogisticRegression(solver='liblinear', random_state=1)","6ec92504":"ct = make_column_transformer((ohe, ['Sex']), (vect, 'Name'), remainder='passthrough')\npipe = Pipeline([('preprocessor', ct), ('model', clf)])","fcc5a2a5":"# specify parameter values to search\nparams = {}\nparams['model__C'] = [0.1, 1, 10]\nparams['model__penalty'] = ['l1', 'l2']","a4c9e850":"# try all possible combinations of those parameter values\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\ngrid.fit(X, y);","b9928df4":"\n# convert results into a DataFrame\nresults = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","d33f1514":"# sort by test score\nresults.sort_values('rank_test_score')","3e177162":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","9ab3f57f":"cols = ['Pclass', 'Fare']\nX = df[cols]\ny = df['Survived']","780372ef":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix","0c99bc2c":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","df16f322":"\nclf = LogisticRegression()\nclf.fit(X_train, y_train);","10b618f9":"# pass it a trained model: it makes predictions for X_test and compares them to y_test\ndisp = plot_confusion_matrix(clf, X_test, y_test, cmap='Blues', values_format='d')","416ba3db":"# print the \"normal\" confusion matrix\ndisp.confusion_matrix","6b376256":"\nimport pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","dcc6f24a":"cols = ['Pclass', 'Fare', 'SibSp']\nX = df[cols]\ny = df['Survived']","2691a763":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_roc_curve","c78bd5e6":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","71ae6699":"lr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()","633ed29e":"lr.fit(X_train, y_train);\ndt.fit(X_train, y_train);\nrf.fit(X_train, y_train);","d4d88afe":"disp = plot_roc_curve(lr, X_test, y_test)\nplot_roc_curve(dt, X_test, y_test, ax=disp.ax_);\nplot_roc_curve(rf, X_test, y_test, ax=disp.ax_);","f639a126":"from sklearn.datasets import load_diabetes\ndataset = load_diabetes()","0611efeb":"X, y = dataset.data, dataset.target\nfeatures = dataset.feature_names","d6161b87":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y);","4ee6fcbd":"model.intercept_","a9356cf8":"model.coef_","8e6ad23c":"# display the feature names with the coefficients\nlist(zip(features, model.coef_))","370d8748":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})","f57811f5":"\nfeatures = ['Pclass', 'Fare', 'Sex']\nX = df[features]\ny = df['Survived']","0e5d7b1f":"classes = ['Deceased', 'Survived']","ad16ab46":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=2, random_state=0)\ndt.fit(X, y);","2d60ff9e":"import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text  # both are new in 0.21","93604fee":"plt.figure(figsize=(8, 6))\nplot_tree(dt, feature_names=features, class_names=classes, filled=True);","f0976312":"print(export_text(dt, feature_names=features, show_weights=True))","c39af783":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})","c79028d3":"features = ['Pclass', 'Fare', 'Sex', 'Parch']\nX = df[features]\ny = df['Survived']","0683b446":"\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score","cf636bba":"# default tree has 331 nodes\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(X, y).tree_.node_count","144951e9":"cross_val_score(dt, X, y, cv=5, scoring='accuracy').mean()","df782f9e":"# pruned tree has 121 nodes\ndt = DecisionTreeClassifier(ccp_alpha=0.001, random_state=0)\ndt.fit(X, y).tree_.node_count","304447fb":"# pruning improved the cross-validated accuracy\ncross_val_score(dt, X, y, cv=5, scoring='accuracy').mean()","8ebec555":"import pandas as pd\ndf = pd.DataFrame({'feature':list(range(8)), 'target':['not fraud']*6 + ['fraud']*2})","004bed84":"\nX = df[['feature']]\ny = df['target']","6e00ebd7":"from sklearn.model_selection import train_test_split","95573691":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)","8f34985e":"y_train","0445412b":"y_test","630783a6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=y)\n","5561d984":"y_train","340e3e21":"y_test","0dad660b":"import pandas as pd\nimport numpy as np\nX = pd.DataFrame({'Shape':['square', 'square', 'oval', 'circle', np.nan]})","c72ccb57":"\nX","f68c82de":"from sklearn.impute import SimpleImputer","f6e17e43":"imputer = SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(X)","6cbc185d":"imputer = SimpleImputer(strategy='constant', fill_value='missing')\nimputer.fit_transform(X)","cd3ca94c":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","a7f4e221":"cols = ['Embarked', 'Sex']","7b74d394":"df = pd.read_csv('http:\/\/bit.ly\/kaggletrain', nrows=10)\nX = df[cols]\ny = df['Survived']","5dc1661d":"df_new = pd.read_csv('http:\/\/bit.ly\/kaggletest', nrows=10)\nX_new = df_new[cols]","56194aa8":"ohe = OneHotEncoder()\nlogreg = LogisticRegression(solver='liblinear', random_state=1)","020a1348":"pipe = make_pipeline(ohe, logreg)","6b913dfb":"\npipe.fit(X, y)\npipe.predict(X_new)","bdf6e257":"# save the pipeline to a file\nimport joblib\njoblib.dump(pipe, 'pipe.joblib')","31dda233":"# load the pipeline from a file\nsame_pipe = joblib.load('pipe.joblib')","48610e3f":"# use it to make the same predictions\nsame_pipe.predict(X_new)","60eff0c8":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","ba153dd6":"X = df[['Name', 'Cabin']].dropna()","40f3d888":"from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()","98c0149a":"from sklearn.compose import make_column_transformer\nct = make_column_transformer((vect, 'Name'), (vect, 'Cabin'))\nct.fit_transform(X)","7abe59d2":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', usecols=['Embarked', 'Survived']).dropna()","1f594557":"X = df[['Embarked']]\ny = df['Survived']","ffd2e6ce":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline","dd45bd2a":"pipe = Pipeline([('ohe', OneHotEncoder()), ('clf', LogisticRegression())])\npipe.fit(X, y);","d0b88f19":"pipe.named_steps.clf.coef_","4b6c2cea":"pipe.named_steps['clf'].coef_","9bfaf96c":"pipe['clf'].coef_","282c130a":"pipe[1].coef_\n","d4c8009d":"import pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import cross_val_score","a97da78a":"# set up the regression problem\nX_reg, y_reg = load_diabetes(return_X_y=True)\nreg = LinearRegression()","a87edc21":"\n# set up the classification problem\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')\nX_clf = df[['Pclass', 'Fare', 'SibSp']]\ny_clf = df['Survived']\nclf = LogisticRegression()","a0d12e9b":"\nfrom sklearn.model_selection import KFold, StratifiedKFold","fb919df2":"kf = KFold(5, shuffle=True, random_state=1)\ncross_val_score(reg, X_reg, y_reg, cv=kf, scoring='r2')","20556706":"skf = StratifiedKFold(5, shuffle=True, random_state=1)\ncross_val_score(clf, X_clf, y_clf, cv=skf, scoring='accuracy')","8ce8d072":"from sklearn.datasets import load_wine\nX, y = load_wine(return_X_y=True)","86776e6d":"# only keep two features in order to make this problem harder\nX = X[:, 0:2]","3a246e5d":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()","96fac274":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","01dd7e65":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nclf.fit(X_train, y_train)\ny_score = clf.predict_proba(X_test)","d6202799":"# use 'ovo' (One-vs-One) or 'ovr' (One-vs-Rest)\nroc_auc_score(y_test, y_score, multi_class='ovo')","26642195":"from sklearn.model_selection import cross_val_score","5a125e7e":"# use 'roc_auc_ovo' (One-vs-One) or 'roc_auc_ovr' (One-vs-Rest)\ncross_val_score(clf, X, y, cv=5, scoring='roc_auc_ovo').mean()","493bfc17":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.compose import make_column_transformer","1729de46":"X = pd.DataFrame({'Fare':[200, 300, 50, 900],\n                  'Code':['X12', 'Y20', 'Z7', np.nan],\n                  'Deck':['A101', 'C102', 'A200', 'C300']})","09aeaeb9":"from sklearn.preprocessing import FunctionTransformer","9ccc3830":"clip_values = FunctionTransformer(np.clip, kw_args={'a_min':100, 'a_max':600})","b0685a49":"# extract the first letter from each string\ndef first_letter(df):\n    return df.apply(lambda x: x.str.slice(0, 1))","c74fd825":"\nget_first_letter = FunctionTransformer(first_letter)","ca1e381c":"\nct = make_column_transformer(\n    (clip_values, ['Fare']),\n    (get_first_letter, ['Code', 'Deck']))","b3a52960":"X","7b41ed43":"ct.fit_transform(X)","e135664b":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain')","4c8e27d1":"X = df['Name']\ny = df['Survived']","784d2f27":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score","b0836987":"vect = CountVectorizer()\nclf = LogisticRegression()","db1f8192":"pipe = make_pipeline(vect, clf)\ncross_val_score(pipe, X, y, scoring='accuracy').mean()","dbbf29e0":"from sklearn.feature_selection import SelectPercentile, chi2","17231138":"# keep 50% of features with the best chi-squared scores\nselection = SelectPercentile(chi2, percentile=50)","4d2ab848":"pipe = make_pipeline(vect, selection, clf)\ncross_val_score(pipe, X, y, scoring='accuracy').mean()","b1fecd3b":"import pandas as pd\ndf = pd.read_csv('http:\/\/bit.ly\/kaggletrain', usecols=['Survived', 'Pclass', 'Fare'])","73123274":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()","fe6dbd6d":"X = df[['Pclass', 'Fare']]\ny = df['Survived']","9a803e1a":"print(type(X))\nprint(type(y))","15cfc444":"# there's no need to use X.values or y.values\nclf.fit(X, y)","fd4765aa":"from sklearn.svm import SVC","947b64e3":"# positional arguments\nclf = SVC(0.1, 'linear')","4f253a3b":"\n# keyword arguments\nclf = SVC(C=0.1, kernel='linear')","82012a20":"\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline","60f6a9f3":"df = pd.read_csv('http:\/\/bit.ly\/kaggletrain')\nX = df[['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']]\ny = df['Survived']","3fe67af7":"imp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder()","2d939b54":"imp_ohe = make_pipeline(imp_constant, ohe)\nvect = CountVectorizer()\nimp = SimpleImputer()","493d2d76":"# pipeline step 1\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))","c38e19e7":"# pipeline step 2\nselection = SelectPercentile(chi2, percentile=50)","daaecfc0":"\n# pipeline step 3\nlogreg = LogisticRegression(solver='liblinear')","660d4197":"# display estimators as diagrams\nfrom sklearn import set_config\nset_config(display='diagram')","a532d9d1":"pipe = make_pipeline(ct, selection, logreg)\npipe","fd49a5b0":"\n# export the diagram to a file\nfrom sklearn.utils import estimator_html_repr\nwith open('pipeline.html', 'w') as f:  \n    f.write(estimator_html_repr(pipe))","4e48dec1":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer","676b572d":"df = pd.read_csv('http:\/\/bit.ly\/kaggletrain').dropna()","05c6fbe8":"\n# select 4 features\nX = df[['Embarked', 'Sex', 'Parch', 'Fare']]","352479bc":"# one-hot encode \"Embarked\" and \"Sex\", and passthrough \"Parch\" and \"Fare\"\nct = make_column_transformer(\n    (OneHotEncoder(), ['Embarked', 'Sex']),\n    remainder='passthrough')","c1914be2":"# ColumnTransformer outputs 7 columns\nct.fit_transform(X).shape","3e0791e9":"# get the names of those 7 features\nct.get_feature_names()","3b7c486d":"**Stratified**\n\n* Class proportions are the SAME in y_train and y_test. (This is good!)","1b267003":"# 8.New_imputers\n\n* Need something better than SimpleImputer for missing value imputation?\n\n* Try KNNImputer or IterativeImputer (inspired by R's MICE package). Both are multivariate approaches (they take other features into account!)","50805bb7":"# Can i get one vote\ud83d\ude03\ud83d\ude03....It will motivate me to create more kernel.....","a0b39b2b":"# 12.Pipeline_cross_validation\n\n* You can cross-validate and grid search an entire pipeline!\n\n* Preprocessing steps will automatically occur AFTER each cross-validation split, which is critical if you want meaningful scores.","f638ed88":"# 27.Function_transformer\n\nWant to do feature engineering within a ColumnTransformer or Pipeline?\n\n* Select an existing function (or write your own)\n* Convert it into a transformer using FunctionTransformer","d41c10e4":"# 21.Impute_categorical_features\nTwo options:\n\n* Impute the most frequent value\n* Impute the value \"missing\", which treats it as a separate category","bb81276f":"# 16.Plot_roc_curve\n\n* New in scikit-learn 0.22: Easily compare multiple ROC curves in a single plot!\n\n* Automatically displays the AUC for each model as well.","58ab1f84":"# 4.Handle_unknown_categories\n\nQ: For a one-hot encoded feature, what can you do if new data contains categories that weren't seen during training?\n\nA: Set handle_unknown='ignore' to encode new categories as all zeros.","0ff46db2":"# 17.Linear_model_coefficients\n\nQ: How do you display the intercept & coefficients for a linear model?\n\nA: They are stored as attributes of the model: intercept_ and coef_","3c0caca1":"**Use KFold with regression problems:**","85926c36":"# 11.Handle_missing_value\n\nFour options for handling missing values (NaNs):\n\n* Drop rows containing NaNs\n* Drop columns containing NaNs\n* Fill NaNs with imputed values\n* Use a model that natively handles NaNs (NEW!)","33b296bc":"# 10.Examine_pipeline_steps\n\nQ: How do you examine the intermediate steps in a Pipeline?\n\nA: By using the \"named_steps\" attribute:\n\npipe.named_steps.STEP_NAME.ATTRIBUTE","6195073c":"# 20.Stratified_train_test_split\n\n* Are you using train_test_split with a classification problem?\n\n* Be sure to set \"stratify=y\" so that class proportions are preserved when splitting.\n\n* Especially important if you have class imbalance!","30a2c7c5":"# 29.Pass_pandas_object\n\nThere's no need to use \".values\" when passing a DataFrame or Series to scikit-learn... it knows how to access the underlying NumPy array!","34d7e741":"**Use StratifiedKFold with classification problems:**","2bd378a5":"# 7.Random_state\n\nQ: Why set a value for \"random_state\"?\n\nA: Ensures that a \"random\" process will output the same results every time, which makes your code reproducible (by you and others!)","d3d40b92":"**Multiclass AUC with train\/test split**","f04793f7":"# 31.Pipeline_diagram\n\nNew in version 0.23: Create interactive diagrams of Pipelines (and other estimators) in Jupyter!\n\nClick on any element to see more details. You can even export the diagram to an HTML file!","899905ab":"**Four ways to display the model coefficients:**","d09b8235":"# 25.kfold_shuffle\n* If you use cross-validation and your samples are NOT in an arbitrary order, shuffling may be required to get meaningful results.\n\n* Use KFold or StratifiedKFold in order to shuffle!","97cb4d93":"# 6.Add_missing_indicator.\n\nWhen imputing missing values, you can preserve info about which values were missing and use THAT as a feature!\n\nWhy? Sometimes there's a relationship between \"missingness\" and the target\/label you are trying to predict.","41b86a26":"**Pipeline without feature selection**","4d20723e":"**Multiclass AUC with cross-validation**","3c980cb4":"**Pipeline with feature selection**","c30c8c50":"![](data:image\/png;base64,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)","6685dea1":"# 22.Joblib\n\n* Want to save a model (or pipeline) for later use? Use joblib!\n\n* Warning: You must load it into an identical environment, and only load objects you trust \ud83d\ude07","5ad8591e":"# 3.Encode_categorical_features\nTwo common ways to encode categorical features:\n\n* OneHotEncoder for unordered (nominal) data\n* OrdinalEncoder for ordered (ordinal) data","3ddafd15":"# 30.Keyword_arguments\n\nNew in version 0.23: Most parameters are now expected to be passed as keyword arguments.\n\nThey will raise a warning \u26a0\ufe0f if passed positionally, and will error \ud83d\uded1 starting in 0.25.","03168ea4":"# 23.Vectorize_two_columns\n\n* Want to vectorize two text columns in a ColumnTransformer?\n\n* You can't pass them in a list, but you can pass the vectorizer twice! (They'll learn separate vocabularies.)","38ae98ba":"# 14.Hyperparameter_search.\n\n* Hyperparameter search results (from GridSearchCV or RandomizedSearchCV) can be converted into a pandas DataFrame.\n\n* Makes it far easier to explore the results!","4db970b6":"# 28.Feature_selection\nIt's simple to add feature selection to a Pipeline:\n\n* Use SelectPercentile to keep the highest scoring features\n* Add feature selection after preprocessing but before model building","bb2d86b6":"# 32.Get_feature_names\n\nNeed to get the feature names output by a ColumnTransformer?\n\nUse get_feature_names(), which now works with \"passthrough\" columns (new in version 0.23)!","58257be4":"# 9.Pipeline_vs_Make_pipeline\n\nQ: What's the difference between Pipeline and make_pipeline?\n\nA: Pipeline requires naming of steps, make_pipeline does not.\n\n(Same applies to ColumnTransformer vs make_column_transformer)","e4fb7aac":"# 13.Randomized_search\n\n* GridSearchCV taking too long? Try RandomizedSearchCV with a small number of iterations.\n\n* Make sure to specify a distribution (instead of a list of values) for continuous parameters!","e3d5be51":"**Convert custom function into a transformer:**","cd7c1fc9":"# 1.Use Column_Transformer to apply different preprocessing to different columns:\n\n* select from DataFrame columns by name.\n* passthrough or drop unspecified columns.","5c7f43cc":"**Convert existing function into a transformer:**","7787c6a3":"# 18.Decision_tree_visualization\nTwo new functions in scikit-learn 0.21 for visualizing decision trees:\n\n* plot_tree: uses Matplotlib (not Graphviz!)\n* export_text: doesn't require any external libraries","6d70585f":"# 19.Decision_tree_pruning\nNew in scikit-learn 0.22: Pruning of decision trees to avoid overfitting!\n\n* Uses cost-complexity pruning\n* Increase \"ccp_alpha\" to increase pruning (default value is 0)","c4b1c909":"# 26.Multiclass_auc\n* AUC is an excellent evaluation metric for binary classification, especially if you have class imbalance.\n\n* New in scikit-learn 0.22: AUC can be used with multiclass problems! Supports \"one-vs-one\" and \"one-vs-rest\" strategies.","be1c4fc3":"**Apply the transformations**","7238909b":"* Find optimal tuning parameters for the entire pipeline","555fb5d8":"# 15.Plot_confusion_matrix\n\n* New in scikit-learn 0.22: Plot a confusion matrix in one line of code!\n\n* Highly customizable, including the colormap, display labels, and value formatting.","1ed630ff":"*  Cross-validate the entire pipeline (not just the model)","5a80356f":"# 24.Examine_pipeline_steps\n\n* There are FOUR ways to examine the steps of a Pipeline!\n\n* (I prefer method 1 since you can autocomplete the step & parameter names... but method 4 is SO short!)","2532df04":"**Not stratified**\n* y_train contains NONE of the minority class, whereas y_test contains ALL of the minority class. (This is bad!)","6803130e":"# 2.Select_columns\nThere are SEVEN ways to select columns using ColumnTransformer:\n\n* column name\n* integer position\n* slice\n* boolean mask\n* regex pattern\n* dtypes to include\n* dtypes to exclude","1896ad25":"# 5.Pipeline\n\nQ: What does \"pipeline\" do?\n\nA: Chains together multiple steps: output of each step is used as input to the next step.","d4f621f2":"****Include them in a ColumnTransformer:****"}}