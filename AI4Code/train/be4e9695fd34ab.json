{"cell_type":{"dd4d5666":"code","42c77029":"code","3d8564fa":"code","7ea94b81":"code","abab91b1":"code","914b78b7":"code","143b19e1":"code","1fe0f9f2":"code","1fca10d5":"code","bf711d6b":"code","b4acf0bf":"code","d1ec8566":"code","6d0b0958":"code","72eaf38c":"code","8d981264":"code","4c305ff9":"code","19b2f8c0":"code","c079383a":"code","095a0fe8":"code","d7b929bc":"code","491a2f42":"code","94407b03":"code","ca16deb1":"code","b8724c65":"code","1ebecf9b":"code","80ad1713":"code","bceab5a0":"code","abb1c0e9":"code","721e513e":"code","eed966de":"code","02dccb97":"code","4b56e024":"code","6462a4fd":"code","487a8c19":"code","a56caeca":"code","2b22830b":"code","d40a026b":"markdown","21365af2":"markdown","466bb54a":"markdown","638ae2fb":"markdown","0ffa2e57":"markdown","76693e60":"markdown","d9afa933":"markdown","6db3d8ae":"markdown","d7bc954e":"markdown","2142cfc7":"markdown","66ba8863":"markdown","9a4d20b9":"markdown"},"source":{"dd4d5666":"%matplotlib inline\nfrom memory_profiler import memory_usage\nimport os\nimport pandas as pd\nfrom glob import glob\nimport numpy as np","42c77029":"%%capture\n!apt-get install libav-tools -y","3d8564fa":"from fastai.vision import *\nimport librosa\nimport librosa.display\nimport pylab\nimport matplotlib\nimport gc","7ea94b81":"!mkdir \/kaggle\/working\/train\n!mkdir \/kaggle\/working\/test","abab91b1":"def create_spectrogram(filename,name):\n    plt.interactive(False)\n    clip, sample_rate = librosa.load(filename, sr=None)\n    fig = plt.figure(figsize=[0.72,0.72])\n    ax = fig.add_subplot(111)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_frame_on(False)\n    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n    filename  = Path('\/kaggle\/working\/train\/' + name + '.jpg')\n    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n    plt.close()    \n    fig.clf()\n    plt.close(fig)\n    plt.close('all')\n    del filename,name,clip,sample_rate,fig,ax,S","914b78b7":"def create_spectrogram_test(filename,name):\n    plt.interactive(False)\n    clip, sample_rate = librosa.load(filename, sr=None)\n    fig = plt.figure(figsize=[0.72,0.72])\n    ax = fig.add_subplot(111)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_frame_on(False)\n    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n    filename  = Path('\/kaggle\/working\/test\/' + name + '.jpg')\n    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n    plt.close()    \n    fig.clf()\n    plt.close(fig)\n    plt.close('all')\n    del filename,name,clip,sample_rate,fig,ax,S","143b19e1":"Data_dir=np.array(glob(\"..\/input\/train\/Train\/*\"))","1fe0f9f2":"%load_ext memory_profiler","1fca10d5":"%%memit \ni=0\nfor file in Data_dir[i:i+1500]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram(filename,name)","bf711d6b":"gc.collect()","b4acf0bf":"%%memit \ni=1500\nfor file in Data_dir[i:i+1500]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram(filename,name)","d1ec8566":"gc.collect()","6d0b0958":"%%memit \ni=3000\nfor file in Data_dir[i:i+1500]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram(filename,name)","72eaf38c":"gc.collect()","8d981264":"%%memit \ni=4500\nfor file in Data_dir[i:]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram(filename,name)","4c305ff9":"gc.collect()","19b2f8c0":"path = Path('\/kaggle\/working\/')\nnp.random.seed(42)\ndata = ImageDataBunch.from_csv(path,csv_labels='..\/input\/train.csv', folder=\"train\", valid_pct=0.2, suffix='.jpg',\n        ds_tfms=get_transforms(), size=224, num_workers=0).normalize(imagenet_stats)","c079383a":"data.classes","095a0fe8":"learn = create_cnn(data, models.resnet34, metrics=accuracy)","d7b929bc":"learn.fit_one_cycle(4)","491a2f42":"learn.save('stage-1')","94407b03":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","ca16deb1":"learn.fit_one_cycle(4, max_lr=slice(1e-4,1e-3))","b8724c65":"learn.save('stage-2')","1ebecf9b":"learn.lr_find()\nlearn.recorder.plot()","80ad1713":"learn.fit_one_cycle(1, max_lr=slice(1e-6,2e-6))","bceab5a0":"learn.save('stage-3')","abb1c0e9":"Test_dir=np.array(glob(\"..\/input\/test\/Test\/*\"))","721e513e":"%%memit \ni=0\nfor file in Test_dir[i:i+1500]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram_test(filename,name)","eed966de":"gc.collect()","02dccb97":"%%memit \ni=1500\nfor file in Test_dir[i:]:\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_spectrogram_test(filename,name)","4b56e024":"gc.collect()","6462a4fd":"learn.load('stage-3')\ntest_csv = pd.read_csv('..\/input\/test.csv')","487a8c19":"with open('output.csv',\"w\") as file:\n    file.write(\"ID,Prediction\\n\")\n    for test in test_csv.ID:\n        img = open_image('\/kaggle\/working\/test\/'+str(test)+'.jpg')\n        prediction = str(learn.predict(img)[0]).split()[0]\n        file.write(str(test)+','+prediction)\n        file.write('\\n')","a56caeca":"output = pd.read_csv('output.csv')\noutput.head()","2b22830b":"%%capture\n!apt-get install zip\n!zip -r train.zip \/kaggle\/working\/train\/\n!zip -r test.zip \/kaggle\/working\/test\/\n!rm -rf train\/*\n!rm -rf test\/*","d40a026b":"\n### Installing libav-tools to get Librosa Working Properly","21365af2":"### Splitting the Conversion in different cells and collecting the garbage to avoid Ram Overflow","466bb54a":"# Urban Sound Classification\n8732 labeled sound excerpts of urban sounds from 10 classes","638ae2fb":"## <b>Context<\/b>\n    \nThe automatic classification of environmental sound is a growing research field with multiple applications to largescale, content-based multimedia indexing and retrieval. In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled by multimedia sensor networks, as well as by large quantities of online multimedia content depicting urban scenes.\n\nHowever, while there is a large body of research in related areas such as speech, music and bioacoustics, work on the analysis of urban acoustic environments is relatively scarce.Furthermore, when existent, it mostly focuses on the classification of auditory scene type, e.g. street, park, as opposed to the identification of sound sources in those scenes, e.g.car horn, engine idling, bird tweet.\n\nThere are primarily two major challenges with urban sound research namely\n\n<ul><li>Lack of labeled audio data. Previous work has focused on audio from carefully produced movies or television tracks from specific environments such as elevators or office spaces and on commercial or proprietary datasets . The large effort involved in manually annotating real-world data means datasets based on field recordings tend to be relatively small (e.g. the event detection dataset of the IEEE AASP Challenge consists of 24 recordings per each of 17 classes).<\/li><\/ul>\n<ul><li>Lack of common vocabulary when working on urban sounds.This means the classification of sounds into semantic groups may vary from study to study, making it hard to compare results so the objective of this notebook is to address the above two mentioned challenges.<\/li><\/ul>\n\n## <b>Content<\/b>\nThe dataset is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: - The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, namely: Air Conditioner Car Horn Children Playing Dog bark Drilling Engine Idling Gun Shot Jackhammer Siren Street Music The attributes of data are as follows: ID \u2013 Unique ID of sound excerpt Class \u2013 type of sound\n\n## <b>Acknowledgements<\/b>\nSource of the dataset : https:\/\/drive.google.com\/drive\/folders\/0By0bAi7hOBAFUHVXd1JCN3MwTEU\n\nSource of research document : https:\/\/serv.cusp.nyu.edu\/projects\/urbansounddataset\/salamon_urbansound_acmmm14.pdf\n\nSource of Technique used : [CNN ARCHITECTURES FOR LARGE-SCALE AUDIO CLASSIFICATION](https:\/\/arxiv.org\/pdf\/1609.09430.pdf)","0ffa2e57":"## Training The Model","76693e60":"### Importing Fast.ai for Deep Learning and Librosa for Creating Spectrogram","d9afa933":"## Defining the Create Spectrogram Function","6db3d8ae":"## Making predictions and writing it to CSV","d7bc954e":"## Importing Basic Libraries and Installing Tools Required","2142cfc7":"### Making Temporary Working Directories for Storing the Audio Conversions","66ba8863":"### Removing Unwanted Folders","9a4d20b9":"## Creating Data Bunch for Training and Creating a Resnet34 Model"}}