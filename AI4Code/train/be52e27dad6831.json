{"cell_type":{"0d3f135d":"code","8f815c59":"code","477c339b":"code","8b5c23bf":"code","c192ee53":"code","9fe41af3":"code","21387975":"code","bbd6ab8c":"code","4877fadb":"code","13873b87":"code","a4e4ed0b":"code","10577171":"code","bb4dd88e":"markdown","0d7d62f6":"markdown","b0cc3e4c":"markdown","d7f32933":"markdown"},"source":{"0d3f135d":"!pip install --no-warn-conflicts -q --upgrade xgboost","8f815c59":"from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import RidgeCV\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom xgboost import DMatrix\nimport lightgbm as lgbm\nimport xgboost as xgb \nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')","477c339b":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')","8b5c23bf":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\nX_test = test.drop(['id'], axis=1)","c192ee53":"cat_cols = [feature for feature in train.columns if 'cat' in feature]\ncont_cols = [feature for feature in train.columns if 'con' in feature]\n\nfor feature in cat_cols:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    X_train[feature] = le.transform(X_train[feature])\n    X_test[feature] = le.transform(X_test[feature])\n","9fe41af3":"seed = 0\nn_splits = 6\nshuffle=True\niterations = 50000\nearly_stopping_rounds = 400\nverbose_eval = 0\nbaseline_rounds = 1\ncb_learning_rate = 0.006\nxgb_learning_rate = 0.01","21387975":"split = KFold(n_splits=n_splits, shuffle=True, random_state=seed)","bbd6ab8c":"cb_params = {'iterations':iterations,\n             'learning_rate':cb_learning_rate,\n             'depth':7,\n             'bootstrap_type':'Bernoulli',\n             'random_strength':1,\n             'min_data_in_leaf':10,\n             'l2_leaf_reg':3,\n             'loss_function':'RMSE', \n             'eval_metric':'RMSE',\n             'random_seed':seed,\n             'grow_policy':'Depthwise',\n             'max_bin':1024, \n             'model_size_reg': 0,\n             'task_type': 'GPU',\n             'od_type':'IncToDec',\n             'od_wait':100,\n             'metric_period':500,\n             'verbose':verbose_eval,\n             'subsample':0.8,\n             'od_pval':1e-10,\n             'max_ctr_complexity': 8,\n             'has_time': False,\n             'simple_ctr' : 'FeatureFreq',\n             'combinations_ctr': 'FeatureFreq'\n            }\n\nxgb_params= {'objective': 'reg:squarederror',\n             'max_depth': 6,\n             'eta': xgb_learning_rate,\n             'colsample_bytree': 0.4,\n             'subsample': 0.6,\n             'reg_alpha' : 6,\n             'min_child_weight': 100,\n             'n_jobs': 2,\n             'seed': 2001,\n             'tree_method': 'gpu_hist',\n             'gpu_id': 0,\n             'predictor': 'gpu_predictor',\n            }\n\nlgbm_params = {'max_depth': 16,\n               'subsample': 0.8032697250789377, \n               'colsample_bytree': 0.21067140508531404,\n               'learning_rate': 0.009867383057779643,\n               'reg_lambda': 10.987474846877767,\n               'reg_alpha': 17.335285595031994,\n               'min_child_samples': 31, \n               'num_leaves': 66,\n               'max_bin': 522,\n               'cat_smooth': 81,\n               'cat_l2': 0.029690334194270022,\n               'metric': 'rmse',\n               'n_jobs': -1, \n               'verbose':-1,\n               'n_estimators': iterations\n              }\n","4877fadb":"preds_list = []\noof_cb = np.zeros((len(train)))\noof_xgb = np.zeros((len(train)))\noof_cbx = np.zeros((len(train)))\noof_xgbx = np.zeros((len(train)))\noof_lgb = np.zeros((len(train)))\noof_lgb_incremental = np.zeros((len(train)))\nstack_oof = np.zeros((len(train)))\nstack_preds = np.zeros((len(test)))\n\nfor fold, (train_idx, val_idx) in enumerate(split.split(X_train)):\n    print(f'Fold {fold+1}')\n    X_tr = X_train.iloc[train_idx]\n    X_val = X_train.iloc[val_idx]\n    y_tr = y_train.iloc[train_idx]\n    y_val = y_train.iloc[val_idx]\n    fold_stack_oof = np.zeros((len(X_val), 6))\n    fold_stack_preds = np.zeros((len(test), 6))\n    ptrain = Pool(data=X_tr, label=y_tr, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    pvalid = Pool(data=X_val, label=y_val, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    ptest = Pool(data=X_test, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    CModel = CatBoostRegressor(**cb_params)\n    CModel.fit(ptrain,\n               eval_set=pvalid,\n               use_best_model=True,\n               early_stopping_rounds=early_stopping_rounds)\n    temp_fold_preds = CModel.predict(pvalid)\n    oof_cb[val_idx] = temp_fold_preds\n    first_cb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of CB model is {first_cb_rmse}')\n    baseline_preds_tr_cb = CModel.predict(ptrain)\n    baseline_preds_vl_cb = temp_fold_preds\n    test_preds_cb = CModel.predict(ptest)   \n    fold_stack_oof[:,0] = temp_fold_preds\n    fold_stack_preds[:,0] = test_preds_cb\n    xtrain = DMatrix(data=X_tr, label=y_tr, nthread=2)\n    xvalid = DMatrix(data=X_val, label=y_val, nthread=2)\n    xtest = DMatrix(data=X_test, nthread=2)\n    XModel = xgb.train(xgb_params, xtrain,\n                       evals=[(xvalid,'validation')],\n                       verbose_eval=verbose_eval,\n                       early_stopping_rounds=early_stopping_rounds,\n                       xgb_model=None,\n                       num_boost_round=iterations)\n    temp_fold_preds = XModel.predict(xvalid)\n    oof_xgb[val_idx] = temp_fold_preds\n    first_xgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of XGB model is {first_xgb_rmse}')\n    baseline_preds_tr_xgb = XModel.predict(xtrain)\n    baseline_preds_vl_xgb = temp_fold_preds\n    test_preds_xgb = XModel.predict(xtest)\n    fold_stack_oof[:,1] = temp_fold_preds\n    fold_stack_preds[:,1] = test_preds_xgb\n    ltrain = lgbm.Dataset(X_tr, label=y_tr, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    lvalid = lgbm.Dataset(X_val, label=y_val, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    ltest =  lgbm.Dataset(X_test, label=y_val, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    LModel = lgbm.train(lgbm_params,\n                        train_set=ltrain,\n                        num_boost_round=iterations,\n                        valid_sets=lvalid, \n                        init_model=None,\n                        early_stopping_rounds=early_stopping_rounds,\n                        verbose_eval=verbose_eval)           \n    temp_fold_preds = LModel.predict(X_val)\n    oof_lgb[val_idx] = temp_fold_preds\n    fold_stack_oof[:,2] = temp_fold_preds\n    fold_stack_preds[:,2] = LModel.predict(X_test)\n    first_lgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of LGBM model is {first_lgb_rmse}')\n    params = lgbm_params.copy()     \n    params.update({'learning_rate': 0.003})\n    for i in range(1, 9):\n        if i > 2:                      \n            params['reg_lambda'] *= 0.9\n            params['reg_alpha']  *= 0.9\n            params['num_leaves'] += 40                   \n        \n        LModel = lgbm.train(lgbm_params,\n                            train_set=ltrain,\n                            num_boost_round=iterations,\n                            valid_sets=lvalid, \n                            init_model=LModel,\n                            early_stopping_rounds=early_stopping_rounds,\n                            verbose_eval=verbose_eval)           \n    temp_fold_preds = LModel.predict(X_val)\n    oof_lgb_incremental[val_idx] = temp_fold_preds\n    second_lgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of LGBM model is {second_lgb_rmse}')\n    print(f'LGBM improvement using Incremental Improvements {first_lgb_rmse - second_lgb_rmse}')\n    baseline_preds_tr_lgb = LModel.predict(X_tr)\n    baseline_preds_vl_lgb = temp_fold_preds\n    test_preds_lgb = LModel.predict(X_test)\n    fold_stack_oof[:,3] = temp_fold_preds\n    fold_stack_preds[:,3] = test_preds_lgb\n    \n    baseline_train = (baseline_preds_tr_xgb+baseline_preds_tr_lgb+baseline_preds_tr_cb)\/3\n    baseline_valid = (baseline_preds_vl_xgb+baseline_preds_vl_lgb+baseline_preds_vl_cb)\/3\n    baseline_test = (test_preds_xgb+test_preds_lgb+test_preds_cb)\/3\n    \n    for baseline in range(baseline_rounds):\n        ptrain = Pool(data=X_tr, label=y_tr, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_train)\n        pvalid = Pool(data=X_val, label=y_val, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_valid)\n        ptest = Pool(data=X_test, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_test)\n        cb_params_ = cb_params.copy()\n        cb_params_.update({'learning_rate': cb_learning_rate*(1\/(baseline+1))})\n        CModel = CatBoostRegressor(**cb_params_)\n        CModel.fit(ptrain, \n                   eval_set=pvalid,\n                   use_best_model=True,\n                   early_stopping_rounds=early_stopping_rounds)\n        temp_fold_preds = CModel.predict(pvalid)\n        oof_cbx[val_idx] = temp_fold_preds\n        second_cb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n        print(f'RMSE of CB model with baseline round {baseline+1} {second_cb_rmse}')   \n        baseline_train = CModel.predict(ptrain)\n        baseline_valid = CModel.predict(pvalid)\n        baseline_test = CModel.predict(ptest)\n        if baseline == baseline_rounds - 1:\n            fold_stack_oof[:,4] = temp_fold_preds\n            fold_stack_preds[:,4] = baseline_test\n        \n        xtrain = DMatrix(data=X_tr, label=y_tr, base_margin=baseline_train)\n        xvalid = DMatrix(data=X_val, label=y_val, base_margin=baseline_valid)\n        xtest =  DMatrix(data=X_test, base_margin=baseline_test)\n        xgb_params_ = xgb_params.copy()\n        xgb_params_.update({'learning_rate': xgb_learning_rate*(1\/(baseline+1))})\n        XModel = xgb.train(xgb_params_, xtrain,\n                           evals=[(xvalid,'validation')],\n                           verbose_eval=verbose_eval,\n                           early_stopping_rounds=early_stopping_rounds,\n                           xgb_model=None,\n                           num_boost_round=iterations)\n        temp_fold_preds = XModel.predict(xvalid)\n        oof_xgbx[val_idx] = temp_fold_preds\n        baseline_train = XModel.predict(xtrain)\n        baseline_valid = temp_fold_preds\n        baseline_test = XModel.predict(xtest)\n        if baseline == baseline_rounds - 1:\n            fold_stack_oof[:,5] = temp_fold_preds\n            fold_stack_preds[:,5] = baseline_test\n        second_xgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n        print(f'RMSE of XGB model with baseline round {baseline+1} {second_xgb_rmse}')\n        print(f'CB Improvement  using Baseline round {baseline+1}: {first_cb_rmse - second_cb_rmse}')\n        print(f'XGB Improvement using Baseline round {baseline+1}: {first_xgb_rmse - second_xgb_rmse}')\n        first_cb_rmse = second_cb_rmse\n        first_xgb_rmse = second_xgb_rmse\n    \n    stacker = RidgeCV().fit(fold_stack_oof, y_val)\n    temp_stack_fold_pred = stacker.predict(fold_stack_oof)\n    stack_oof[val_idx] = temp_stack_fold_pred\n    stack_rmse = mean_squared_error(y_val, temp_stack_fold_pred, squared=False)\n    print(f'RMSE of stack model  {stack_rmse}')\n    stack_preds += stacker.predict(fold_stack_preds)\/n_splits\n    print('-' * 100)\n    print('',end='\\n')\n    ","13873b87":"first_cb_rmse = mean_squared_error(y_train, oof_cb, squared=False)\nfirst_xgb_rmse = mean_squared_error(y_train, oof_xgb, squared=False)\nfirst_lgb_rmse = mean_squared_error(y_train, oof_lgb, squared=False)\nsecond_cb_rmse = mean_squared_error(y_train, oof_cbx, squared=False)\nsecond_xgb_rmse = mean_squared_error(y_train, oof_xgbx, squared=False)\nsecond_lgb_rmse = mean_squared_error(y_train, oof_lgb_incremental, squared=False)\nprint(f'RMSE for CB model is {first_cb_rmse}')\nprint(f'RMSE for XGB model is {first_xgb_rmse}')\nprint(f'RMSE for LGBM model is {first_lgb_rmse}')\nprint(f'RMSE for CB model with XGB baseline is {second_cb_rmse}')\nprint(f'RMSE for XGB model with CB baseline is {second_xgb_rmse}')\nprint(f'RMSE for LGBM model with Incremental Improvement is {second_lgb_rmse}')\nprint(f'RMSE for CB and XGB blend is {mean_squared_error(y_train, (oof_cbx+oof_xgbx)\/2, squared=False)}')\nprint(f'RMSE for CB, XGB and LGBM blend is {mean_squared_error(y_train, (oof_cbx+oof_xgbx+oof_lgb_incremental)\/3, squared=False)}')\nprint(f'RMSE for CB, XGB and LGBM stack is {mean_squared_error(y_train, stack_oof, squared=False)}')","a4e4ed0b":"preds = np.mean(preds_list, axis=0)\nsubmission_mean = pd.DataFrame({'id':test.id,'target':stack_preds})\nsubmission_mean.to_csv('submission_mean.csv', index=False)\nsubmission_mean.head()","10577171":"sub_best = pd.read_csv('..\/input\/comparative-method-tabular-feb-301\/submission.csv')\nsub_final = submission_mean.copy()\nsub_final['target'] = sub_best.target * 0.9 + submission_mean.target * 0.1\nsub_final.to_csv('blend_mean.csv', index=False)\nsub_final.head()","bb4dd88e":"This is a fun competition for testing ideas and probably **Overfiting**! Using baseline predictions for Catboost and XGboost might improve their performance. I'm testing this idea in this notebook. XGboost parameters are from [https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap](https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap) and the LightGBM model is from [https:\/\/www.kaggle.com\/awwalmalhi\/extreme-fine-tuning-lgbm-using-7-step-training](https:\/\/www.kaggle.com\/awwalmalhi\/extreme-fine-tuning-lgbm-using-7-step-training)","0d7d62f6":"Lightgbm also has an **init_score** in it's **Dataset** but it's not working the same way as Catboost and XGBoost and providing high score predictions would lead to interference with model's learning process(It's based on my tests and maybe somebody could make it work!)","b0cc3e4c":"Another nice idea for more **overfit**. Blending with [**top public blend**](https:\/\/www.kaggle.com\/somayyehgholami\/comparative-method-tabular-feb-301)!","d7f32933":"Final predictions are a simple **stack** of **XGBoost**, **LightGBM** and **Catboost** models."}}