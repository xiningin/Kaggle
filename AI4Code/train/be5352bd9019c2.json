{"cell_type":{"33974d8c":"code","5c93aade":"code","809dfcca":"code","1467c87b":"code","d4e819bb":"code","9cca2461":"code","e7d8fd97":"code","e48a22bc":"code","ab332b55":"code","090b687b":"code","e0458c64":"code","23199b9b":"code","43efaf79":"code","fc0dde96":"code","319e9a58":"code","e426d204":"code","bb1195b5":"code","dbf811a4":"code","4a1ef165":"code","166a704c":"code","90984a72":"code","e3e8a93a":"code","7208833a":"code","0cb6b1eb":"code","d13ff314":"code","d13a3852":"code","1cbdb162":"code","ca95e0e3":"code","8ec89158":"code","65e58ef6":"code","7ec8b9b9":"code","ef1dd85e":"markdown","526dd1aa":"markdown","c8586aa5":"markdown","24c2e8cd":"markdown","2fd3091f":"markdown","31fbd20a":"markdown","e5aa1581":"markdown","99f42e8a":"markdown","382de113":"markdown","b751ff36":"markdown","01e648a6":"markdown","361e7504":"markdown","8b6e6368":"markdown","3fd0a083":"markdown","a0663f9a":"markdown","9cc5d7da":"markdown","c7477490":"markdown","8bc58db8":"markdown","47954d90":"markdown","390e12d7":"markdown","c1a874a2":"markdown","0196ca16":"markdown","82a0187c":"markdown","309bd426":"markdown","6711e4e7":"markdown","9dd33232":"markdown","249585ba":"markdown","4d04c665":"markdown"},"source":{"33974d8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nimport seaborn as sns\nsns.set_palette(\"bwr\")\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline","5c93aade":"df = pd.read_csv(\"\/kaggle\/input\/early-stage-diabetes-risk-prediction-dataset\/diabetes_data_upload.csv\")\ndf.head()","809dfcca":"df.describe(include=\"all\")","1467c87b":"sns.heatmap(df.isnull(), cbar=False)","d4e819bb":"# Make column name lowercase and convert space to underscore\ndf.columns = map(str.lower, df.columns)\ndf.columns = df.columns.str.strip()\ndf.columns = df.columns.str.replace(\" \", \"_\")","9cca2461":"# Map yes\/no values\none_values = [\"Male\", \"Positive\", \"Yes\"]\nzero_values = [\"Female\", \"Negative\", \"No\"]\n\nfor column in df.columns:\n    df[column] = df[column].replace(to_replace=[one_values], value=1)\n    df[column] = df[column].replace(to_replace=[zero_values], value=0)","e7d8fd97":"df = df.rename({\"class\": \"status\"}, axis = \"columns\")\ndf.head()","e48a22bc":"# Defining a function to plot a simple pie chart\ndef plotPie(value, title, label):\n    plt.figure(figsize=(4,4))\n    plt.pie(\n        value.value_counts(),\n        startangle=90,\n        labels = label,\n        autopct=(lambda p:f'{p:.2f}%\\n{p*sum(value.value_counts())\/100 :.0f} items')\n    )\n    plt.title(title)\n    plt.show()\n\nplotPie(df[\"status\"], \"Status distribution\", [\"Positive\", \"Negative\"])\nplotPie(df[\"gender\"], \"Gender distribution\", [\"Male\", \"Female\"])","ab332b55":"plt.figure(figsize=(5,5))\n\nax = sns.distplot(df[\"age\"], color=\"r\")","090b687b":"ax = sns.countplot(x=\"status\", data=df, hue=\"gender\")","e0458c64":"ax = sns.violinplot(x=\"status\", y=\"age\", data=df)","23199b9b":"# Divide data into positive and negative class data\ndf_pos = df[df[\"status\"] == 1]\ndf_neg = df[df[\"status\"] == 0]","43efaf79":"print(\"Average positive age:\", df_pos[\"age\"].mean())\nprint(\"Average negative age:\", df_neg[\"age\"].mean())","fc0dde96":"df_symptoms = df[df.columns.difference([\"age\", \"status\", \"gender\"])]\n\nfor column in df_symptoms.columns:\n    plt.figure(figsize=(4,4))\n    ax = sns.barplot(x=column, y=\"status\", data=df)\n    ax.set_xticklabels([\"No\", \"Yes\"])\n    ax.set_ylabel(\"Diabetes risk\")\n    ax.set_xlabel(None)\n    title = column.capitalize()\n    plt.title(title)\n    plt.show()","319e9a58":"# Select only the symptom columns\ndf_symptoms = df[df.columns.difference([\"age\", \"status\", \"gender\"])]\nplt.figure(figsize=(5,5))\nfor column in df_symptoms.columns:\n    plotPie(df_symptoms[column], column.capitalize(), [\"Yes\", \"No\"])","e426d204":"plt.figure(figsize=(8,8))\n\ncorr = df.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 160, n=256),\n    square=True,\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=50,\n    horizontalalignment=\"right\"\n);","bb1195b5":"feat_corr = df.corr()[\"status\"].to_frame()\nfeat_corr","dbf811a4":"# Sort values with highest correlation\nfeat_corr[\"status\"] = abs(feat_corr[\"status\"])\nfeat_corr = feat_corr.sort_values(by=\"status\", ascending=False).reset_index(drop=False)\nfeat_corr = feat_corr[1:11][\"index\"].to_numpy()\nfeat_corr","4a1ef165":"from sklearn.model_selection import train_test_split\n\nx = df[feat_corr]\ny = df[\"status\"]\n\n(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state=1)","166a704c":"from sklearn.preprocessing import StandardScaler\n\nscl = StandardScaler()\nx_train = scl.fit_transform(x_train)\nx_test = scl.transform(x_test)","90984a72":"# Defining objects for the models and creating a list to iterate the process\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nnb = GaussianNB()\nlr = LogisticRegression(max_iter = 2000)\ndt = tree.DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier(random_state = 1)\nsvc = SVC(probability = True)\nknn = KNeighborsClassifier()\nxgb = XGBClassifier(random_state =1)\nvot = VotingClassifier(\n    estimators = [('nb',nb), ('lr',lr), ('dt',dt), ('rf',rf), ('svc',svc), ('knn',knn), ('xgb',xgb)],\n    voting = 'soft'\n)\n\nmodels = [nb, lr, dt, rf, svc, knn, xgb, vot]\nmodels_name = [\n    \"Naive Bayes\",\n    \"Logistic Regression\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"SVM\",\n    \"K-Nearest Neighbor\",\n    \"XGBoost\",\n    \"Voting\"\n]","e3e8a93a":"results_base = {}\n\nfor index, model in enumerate(models):\n    cv = cross_val_score(model, x_train, y_train, cv=10)\n    results_base[models_name[index]] = cv.mean() * 100.0\n    print(\"Baseline using\", models_name[index], \"=\", cv.mean() * 100.0, \"%\", \"with std:\", cv.std())","7208833a":"from sklearn.metrics import accuracy_score, confusion_matrix\n\nresults = {}\n\nfor index, model in enumerate(models):\n    model.fit(x_train, y_train)\n    predict = model.predict(x_test)\n    confuse = confusion_matrix(y_test, predict)\n    accur = accuracy_score(y_test, predict)\n    results[models_name[index]] = accur * 100.0\n    \n    title = models_name[index] + \": \" + \"{:.3f}%\".format(accur*100) + \" accurate\\n\"\n    ax = sns.heatmap(confuse\/np.sum(confuse), annot=True, fmt='.1%', cmap=\"Greens\")\n    ax.set_title(title)\n    plt.show()","0cb6b1eb":"x = np.arange(len(results))\n\nplt.figure(figsize=(9,5))\nax = plt.subplot(111)\nax.bar(x, results_base.values(), width=0.4, color=\"c\", align=\"center\")\nax.bar(x+0.4, results.values(), width=0.4, color=\"r\", align=\"center\")\nax.legend((\"Base\", \"Real\"))\nplt.ylim((85, 100))\nplt.xticks(x+0.4, results_base.keys())\nplt.title(\"Performance comparison\")\nplt.xticks(rotation=40, horizontalalignment=\"right\")\nplt.show()","d13ff314":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\nx = df[df.columns.difference([\"status\"])]\ny = df[\"status\"]\n\nfeat_chi = SelectKBest(score_func=chi2, k=10)\nfit = feat_chi.fit(x, y)\nfeat_chi = pd.concat([pd.DataFrame(x.columns), pd.DataFrame(fit.scores_)], axis=1)\nfeat_chi.columns = [\"column\", \"score\"]\nfeat_chi = feat_chi.sort_values(by=\"score\", ascending=False).reset_index(drop=False)\nfeat_chi = feat_chi[0:10][\"column\"].to_numpy()\nprint(feat_chi)","d13a3852":"print(feat_corr)\nprint()\nprint(\"Difference:\", list(set(feat_corr).symmetric_difference(set(feat_chi))))","1cbdb162":"x = df[feat_chi]\ny = df[\"status\"]\n\n(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state=1)","ca95e0e3":"scl = StandardScaler()\nx_train = scl.fit_transform(x_train)\nx_test = scl.transform(x_test)","8ec89158":"results_base = {}\n\nfor index, model in enumerate(models):\n    cv = cross_val_score(model, x_train, y_train, cv=10)\n    results_base[models_name[index]] = cv.mean() * 100.0\n    print(\"Baseline using\", models_name[index], \"=\", cv.mean() * 100.0, \"%\", \"with std:\", cv.std())","65e58ef6":"results = {}\n\nfor index, model in enumerate(models):\n    model.fit(x_train, y_train)\n    predict = model.predict(x_test)\n    confuse = confusion_matrix(y_test, predict)\n    accur = accuracy_score(y_test, predict)\n    results[models_name[index]] = accur * 100.0\n    \n    title = models_name[index] + \": \" + \"{:.3f}%\".format(accur*100) + \" accurate\\n\"\n    ax = sns.heatmap(confuse\/np.sum(confuse), annot=True, fmt='.1%', cmap=\"Greens\")\n    ax.set_title(title)\n    plt.show()","7ec8b9b9":"x = np.arange(len(results))\n\nplt.figure(figsize=(9,5))\nax = plt.subplot(111)\nax.bar(x, results_base.values(), width=0.4, color=\"c\", align=\"center\")\nax.bar(x+0.4, results.values(), width=0.4, color=\"r\", align=\"center\")\nax.legend((\"Base\", \"Real\"))\nplt.ylim((85, 100))\nplt.xticks(x+0.4, results_base.keys())\nplt.title(\"Performance comparison\")\nplt.xticks(rotation=40, horizontalalignment=\"right\")\nplt.show()","ef1dd85e":"# Importing Library and Dataset","526dd1aa":"### (2.d) Predicting data","c8586aa5":"### (2.e) Comparing performance with baseline","24c2e8cd":"### Status in relation with gender","2fd3091f":"### (1.d) Predicting data\n\nWe'll also take a look at the confusion matrix of each model.","31fbd20a":"### Age distribution","e5aa1581":"# Exploring Our Data\n\n### Status and gender distribution","99f42e8a":"Compare it to the list of feature selection we've created earlier using Pearson's correlation coefficient. There is some difference between both of them. Let's check.","382de113":"### Taking a look on our dataset","b751ff36":"### Standardize column names and map boolean values","01e648a6":"### (1.c) Baseline validation\n\nWe'd like to see how different models perform with default parameters. We'll be using ten fold cross validation to get a baseline.","361e7504":"# Cleaning Data\n\n### Checking null values","8b6e6368":"### (1.a) Dividing dataset into training and test set","3fd0a083":"We will also rename \"class\" column into \"status\"","a0663f9a":"We can briefly see that polyuria and polydipsia have the strongest positive correlations with status, and gender has the strongest negative correlation.","9cc5d7da":"### Status in relation with age","c7477490":"### Correlation heatmap","8bc58db8":"# Building Models\n\n## (1) Features selection: with Pearson's\n\nWe will select top 10 features with the highest absolute value of Pearson's correlation coefficient.","47954d90":"# Conclusion\n\nSo, in this notebook, I experimented with different features selection methods and models. It turns out that using chi-squared method is best for categorical input and categorical output. After comparing the models above, the best models for predicting diabetes in this dataset are XGBoost, Random Forest, and Decision Tree using both features selection methods.\n\nThe best accuracy I can get is with XGBoost and Random Forest, both with 97.1% accuracy with feature selection done using chi-squared.\n\nAny upvote, comment, and suggestion will be very appreciated! Thank you.","390e12d7":"### (1.e) Comparing performance with baseline","c1a874a2":"### Occurences of symptoms in all patients","0196ca16":"# Getting Started\n\nThis is what I'm doing in this notebook:\n\n* Doing some exploratory data analysis.\n* Making predictions on the outcome of diabetes diagnosis based on the symptoms shown on patients using 7 different models + 1 voting\n* Comparing each model's prediction performance\n* Experimenting using different feature selection methods\n\nWith this notebook, I hope to learn more about data visualization and predicting outcomes using Python. Any upvote, comment, and suggestion will be very appreciated!","82a0187c":"### Correlation visualization between symptoms and diabetes status","309bd426":"### (1.b) Scaling and standardizing the data","6711e4e7":"### (2.a) Dividing dataset into training and test set","9dd33232":"## (2) Features selection: with Chi-Squared test\n\nWe will select top 10 features with the highest Chi-squared value.","249585ba":"### (2.b) Standardizing and scaling the data","4d04c665":"### (2.c) Baseline validation"}}