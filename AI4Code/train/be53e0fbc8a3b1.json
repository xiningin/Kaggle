{"cell_type":{"2ddbffc7":"code","eef5432e":"code","86f3a0d4":"code","4982ef37":"code","bf5e1d35":"code","627a23cc":"code","e0c65e66":"code","3f6e3d29":"code","201fe27f":"code","b0da73c3":"code","7986bd21":"code","6b0d3508":"code","0a389abe":"code","34948346":"code","fedfbcac":"code","c330e283":"code","61c70b56":"code","7fe3aae6":"code","1c5ed7a0":"code","d08ddffc":"code","fdc69e7a":"code","1227b560":"code","8a5ace97":"code","77cbcc35":"code","d8bb6e0d":"code","01d939b3":"code","9b254f2a":"code","5d8d41a6":"code","abc9b8a8":"code","48093147":"code","32845cb8":"code","4dc84f82":"code","f3c4a78e":"markdown","bcc5c1cd":"markdown","f9d3e994":"markdown","4062f6f5":"markdown","e206d98b":"markdown"},"source":{"2ddbffc7":"import numpy as np\nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","eef5432e":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns","86f3a0d4":"df=pd.read_csv('\/kaggle\/input\/clustering-categorical-peoples-interests\/kaggle_Interests_group.csv')","4982ef37":"df.info()\nsns.countplot(x='group',data=df)","bf5e1d35":"#see the distinct values per column\nfor col in df:\n    print(df[col].unique())","627a23cc":"#see the columns that real missing value, since the dataset also use NaN to indicate the value equal to zero\nfor col in df:\n    if df[col].nunique()>1:\n        print(col)\n    else:\n        pass","e0c65e66":"#fill the NaN value with zero, only for columns use NaN to indicate the value equal to zero\na=[]\nfor col in df:\n    if df[col].nunique()==1:\n        a.append(col)\n    else:\n        pass\ndf[a] = df[a].fillna(0)\n","3f6e3d29":"df.isnull().sum()","201fe27f":"#drop the columns with the number of missing value >10% of total data\nthreshold=0.1*6340\n\nb=[]\nfor col in df:\n    if df[col].isnull().sum()>threshold:\n        b.append(col)\n    else:\n        pass\ndf=df.drop(b,axis=1)","b0da73c3":"#drop the rows with the number of missing value from columns with the number of missing value <10% of total data\nc=[]\nfor col in df:\n    if (df[col].isnull().sum()<threshold) & (df[col].isnull().sum()>0):\n        c.append(col)\n    else:\n        pass\ndf = df.dropna(subset=c)","7986bd21":"for col in df:\n    print(df[col].unique())","6b0d3508":"df['group'] = df['group'].map({'C':0,'P':1,'R':2,'I':3})","0a389abe":"df #cleaned dataset","34948346":"x=df.drop('group',axis=1).values \ny=df['group'].values","fedfbcac":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=101)","c330e283":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","61c70b56":"scores=[]\nfor i in range(1,50):\n  tree=DecisionTreeClassifier(max_depth = i) \n  tree.fit(x_train, y_train) \n  scores.append(tree.score(x_test,y_test)) \nplt.plot(range(1,50),scores) \nplt.show()\n","7fe3aae6":"tree=DecisionTreeClassifier(max_depth =5) \ntree.fit(x_train, y_train) \ntree.score(x_test,y_test)","1c5ed7a0":"predictions = tree.predict(x_test) \nfrom sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(y_test,predictions)) \nprint(confusion_matrix(y_test,predictions))","d08ddffc":"from sklearn.ensemble import RandomForestClassifier\n\n\nscores=[]\nfor i in (np.arange(100,2000,100)):\n  classifier = RandomForestClassifier(n_estimators =i, max_depth=10, random_state =101) \n  classifier.fit(x_train, y_train) \n  scores.append(classifier.score(x_test,y_test)) \nplt.plot(np.arange(100,2000,100),scores) \nplt.show()","fdc69e7a":"classifier = RandomForestClassifier(n_estimators =700,max_depth=10, random_state =101)\nclassifier.fit(x_train, y_train)\nprint(classifier.score(x_test,y_test))","1227b560":"predictions = classifier.predict(x_test) \nfrom sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(y_test,predictions)) \nprint(confusion_matrix(y_test,predictions))","8a5ace97":"from sklearn.neighbors import KNeighborsClassifier\n\naccuracies=[]\nfor k in range(1,101):\n  classifier = KNeighborsClassifier(n_neighbors = k)\n  classifier.fit(x_train, y_train)\n  accuracies.append(classifier.score(x_test, y_test)) \n  \nk_list=list(range(1,101)) \n\nplt.plot(k_list,accuracies)\nplt.show() ","77cbcc35":"classifier = KNeighborsClassifier(n_neighbors =54)\nclassifier.fit(x_train, y_train)\nclassifier.score(x_test, y_test) ","d8bb6e0d":"predictions = classifier.predict(x_test) \nfrom sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(y_test,predictions)) \nprint(confusion_matrix(y_test,predictions))","01d939b3":"from tensorflow.keras.utils import to_categorical\ny_cat_train = to_categorical(y_train) \ny_cat_test = to_categorical(y_test)","9b254f2a":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping","5d8d41a6":"from tensorflow.keras.layers import Dropout\nmodel = Sequential()\nmodel.add(Dense(units=160,activation='relu',kernel_regularizer=tf.keras.regularizers.l1_l2(0.01)))\n\nmodel.add(Dense(units=4,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])","abc9b8a8":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n\nmodel.fit(x_train,y_cat_train,epochs=300,validation_data=(x_test,y_cat_test),verbose=1)","48093147":"print(model.metrics_names)\nprint(model.evaluate(x_test,y_cat_test,verbose=0))","32845cb8":"losses = pd.DataFrame(model.history.history)\nlosses[['accuracy','val_accuracy']].plot()\nlosses[['loss','val_loss']].plot()","4dc84f82":"from sklearn.metrics import classification_report,confusion_matrix\n\npredictions = model.predict_classes(x_test)\nprint(classification_report(y_test,predictions))\n","f3c4a78e":"**KNN**","bcc5c1cd":"Hello, I want to share my humble work regarding this dataset. I tried to make ML models to predict the groups based on the interests. Here you can see the performance difference between the ML algorithms (decision tree, random forest, KNN, ANN).","f9d3e994":"**Decision Tree**","4062f6f5":"**Random Forest**","e206d98b":"**ANN**"}}