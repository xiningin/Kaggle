{"cell_type":{"d846f9c6":"code","ceb504db":"code","db0e8faf":"code","245f8c3d":"code","bddc6dd9":"code","b571414b":"code","d91ebe4e":"code","ef068173":"code","5f05198e":"code","c75e0288":"code","87326a8a":"code","e21d61af":"code","ab416be0":"code","bd6b897b":"code","65246e97":"code","52950b12":"code","9cf7a91a":"code","d0b51890":"code","2e8a5989":"code","e357b0bf":"code","72c0a756":"code","bd79a5fe":"code","53da41f9":"code","ad8ec1f3":"code","9e4d93c9":"code","bb707cf6":"code","7dc0baf2":"code","fed3625e":"code","3015d07e":"code","d196642c":"code","ecbb4171":"code","966d2860":"code","06c3f653":"code","c21fc4c9":"code","a6838769":"code","c7b11042":"code","57b17807":"code","7c64931e":"code","312df8a2":"code","57071e69":"code","9cf7c624":"code","f3cc7608":"markdown","267c43e0":"markdown","579a2ee4":"markdown","2bdae805":"markdown","1d5a566f":"markdown","b0bad6a5":"markdown","98e42cd4":"markdown","a60bbc5e":"markdown","42eddcc1":"markdown","70b87b72":"markdown","478d3092":"markdown","de968d74":"markdown","7a9a71a5":"markdown","14bb9e9f":"markdown","f68142c8":"markdown","be1bedd1":"markdown","f4d11f78":"markdown","9ca0190c":"markdown","cbfe89d2":"markdown","cb247d54":"markdown","2e9d298e":"markdown","40295467":"markdown","f95be607":"markdown","a0f9a1f5":"markdown","e36b45a4":"markdown","4c1d0f45":"markdown","5ca04428":"markdown","dd4e86dc":"markdown","db717029":"markdown"},"source":{"d846f9c6":"import pandas as pd                  # A fundamental package for linear algebra and multidimensional arrays\nimport numpy as np                   # Data analysis and data manipulating tool\nimport random                        # Library to generate random numbers\nfrom collections import Counter      # Collection is a Python module that implements specialized container datatypes providing \n                                     # alternatives to Python\u2019s general purpose built-in containers, dict, list, set, and tuple.\n                                     # Counter is a dict subclass for counting hashable objects\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To ignore warnings in the notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ceb504db":"fraud_data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Imbalanced_classes\/master\/fraud_data.csv\")","db0e8faf":"fraud_data.head()","245f8c3d":"fraud_data.info()","bddc6dd9":"fraud_data.describe()","b571414b":"# Taking a look at the target variable\nfraud_data.isFraud.value_counts()","d91ebe4e":"fraud_data.isFraud.value_counts(normalize = True)      # Normalize = True will find the proportion of fraud transaction and not fraud transaction ","ef068173":"# we can also use countplot form seaborn to plot the above information graphically.\nsns.countplot(fraud_data.isFraud)","5f05198e":"def miss_val_info(df):\n  \"\"\"\n  This function will take a dataframe and calculates the frequency and percentage of missing values in each column.\n  \"\"\"\n  missing_count = df.isnull().sum().sort_values(ascending = False)\n  missing_percent = round(missing_count \/ len(df) * 100, 2)\n  missing_info = pd.concat([missing_count, missing_percent], axis = 1, keys=['Missing Value Count','Percent of missing values'])\n  return missing_info[missing_info['Missing Value Count'] != 0]\n","c75e0288":"miss_val_info(fraud_data)      # Display the frequency and percentage of data missing in each column","87326a8a":"fraud_data = fraud_data[fraud_data.columns[fraud_data.isnull().mean() < 0.2]]","e21d61af":"# filling missing values of numerical columns with mean value.\nnum_cols = fraud_data.select_dtypes(include=np.number).columns      # getting all the numerical columns\n\nfraud_data[num_cols] = fraud_data[num_cols].fillna(fraud_data[num_cols].mean())   # fills the missing values with mean","ab416be0":"cat_cols = fraud_data.select_dtypes(include = 'object').columns    # getting all the categorical columns\n\nfraud_data[cat_cols] = fraud_data[cat_cols].fillna(fraud_data[cat_cols].mode().iloc[0])  # fills the missing values with maximum occuring element in the column","bd6b897b":"# Let's have a look if there still exist any missing values\nmiss_val_info(fraud_data)","65246e97":"fraud_data = pd.get_dummies(fraud_data, columns=cat_cols)    # earlier we have collected all the categorical columns in cat_cols","52950b12":"fraud_data.head()","9cf7a91a":"# Separate input features and output feature\nX = fraud_data.drop(columns = ['isFraud'])       # input features\nY = fraud_data.isFraud      # output feature\n\nfrom sklearn.model_selection import train_test_split\n\n# Split randomly into 70% train data and 30% test data\nX_train, X_Test, Y_train, Y_Test = train_test_split(X, Y, test_size = 0.3, random_state = 123)","d0b51890":"# import SMOTE \nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 25, ratio = 1.0)   # again we are eqalizing both the classes","2e8a5989":"# fit the sampling\nX_train, Y_train = sm.fit_sample(X_train, Y_train)","e357b0bf":"np.unique(Y_train, return_counts=True)","72c0a756":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(criterion='entropy')","bd79a5fe":"rfc.fit(X_train, Y_train)","53da41f9":"rfc.score(X_Test, Y_Test)","ad8ec1f3":"from sklearn.feature_selection import SelectKBest, f_classif","9e4d93c9":"selector = SelectKBest(f_classif, k=10)     # Let's say we select 10 best features","bb707cf6":"X_new = selector.fit_transform(X, Y)","7dc0baf2":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_new, Y, test_size = 0.2, random_state = 42)","fed3625e":"rfc.fit(X_train, Y_train)","3015d07e":"rfc.score(X_test, Y_test)","d196642c":"# We will use here k - fold cross validation technique\nfrom sklearn.model_selection import cross_validate","ecbb4171":"cv_results = cross_validate(rfc, X_new, Y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\ncv_results","966d2860":"print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())","06c3f653":"from sklearn.model_selection import GridSearchCV","c21fc4c9":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()","a6838769":"# Different parameters in random forest\n\ncriterion = ['gini', 'entropy']        # what criteria to consider\n\nn_estimators = [100, 200, 300]       # Number of trees in random forest\n\nmax_features = ['auto', 'sqrt']       # Number of features to consider at every split\n\nmax_depth = [10, 20]      # Maximum number of levels in tree. Hope you remember linspace function from numpy session\n\nmax_depth.append(None)     # also appendin 'None' in max_depth i.e. no maximum depth to be considered.\n\nparams = {'criterion': criterion,\n          'n_estimators': n_estimators,\n          'max_features': max_features,\n          'max_depth': max_depth}\n","c7b11042":"params","57b17807":"gs = GridSearchCV(rfc, param_grid=params, n_jobs=2)","7c64931e":"gs.fit(X_train, Y_train)    # this will take a lot of time to execute; have some patience","312df8a2":"gs.best_params_","57071e69":"gs.best_score_","9cf7c624":"gs.score(X_test, Y_test)","f3cc7608":"### Getting Basic Idea About Data","267c43e0":"There are only 3% of the data which are fraud and the rest 97% are not fraud. This is clearly a class imbalance problem. In this notebook we will look to solve this type of problems.","579a2ee4":"### Building Random Forest Model\nA Random Forest \ud83c\udf32\ud83c\udf32\ud83c\udf32 is actually just a bunch of Decision Trees \ud83c\udf32 bundled together (ohhhhh that\u2019s why it\u2019s called a forest). In this notebook we will learn how to build Random Forest Model.","2bdae805":"Out of 434 columns, 414 have some missing values.","1d5a566f":"Even with cross validation we are getting approx 96.9% of accurate results.","b0bad6a5":"# Dealing with Imbalanced Data\nMost machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.\n","98e42cd4":"**Conclusion**\n*  We observed that the dataset contained missing values. We removed some columns and filled missing values for numerical column with mean and categorical column with mode (i.e. the maximum occuring value).\n*  We observed that the dataset was imblanced. We used 'SMOTE' to generate new data to deal with the problem of imbalanced data.\n*  We build Random Forest model, got accuracy score of 97.53%.\n*  Then we selected only 10 most important features using selectKBest and f_classif. Here the model complexity is reduced a lot (which is very good) with very little decrease in accuracy\n*  Cross Validation and Hyper parameter tunning gave nearly 96.8% of accurate results which is not bad. Most of the times the default values for hyper parameters of the models are same that we get through the hyper parameter tunning. That's the reason there is not much difference in normal model and tunned model.","a60bbc5e":"**Note:** Obviously we can notice there is 0.7% decrease in accuracy score. The point to note is that we had got 97.5% accuracy with 249 features, then after that we selected only 10 features of 249 features and still able to get 96.9% accurate results. The conclusion is that we have reduced a lot of computational complexities which is very good as our aim is not only to increase the performance of the model at any cost but also to reduce the computational complexity of our model.","42eddcc1":"The count of both the classes are equal.","70b87b72":"Eliminate columns with more than 20% missing values","478d3092":"### Missing values\nGenerally datasets always have some missing values. May be done during data collection, or due to some data validation rule.\n","de968d74":"There are 434 columns with 59054 observations.","7a9a71a5":"## Loading Data\nPandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data.\n\n**Disclaimer:** Loading fraud data might take time due to the nature of its size","14bb9e9f":"#### Separate Input Features and Output Features","f68142c8":"### Dealing with Missing Values\n*  Filling the missing values with right technique can change our results drastically. \n*  Also, there is no fixed rule of filling the missing values.\n*  No method is perfect for filling the missing values. We need to use our common sense, our logic, or may need to see what works for that particular data set.\n\n### Ways of dealing with missing values:\n\n**Default value:** One can fill the missing value by default value on the basis of one's 1) understanding of variable, 2) context \/ data insight or 3) common sense \/ logic. \n\n**Deleting:** Suppose in our dataset we have too many missing values in\n\n*  Column, we can drop the column\n*  Row, drop the row. Usually we do this for a large enough dataset.\n\n**Mean\/Median\/Mode - Imputation:** We fill missing values by mean or median or mode(i.e. maximum occuring value). Generally we use mean but if there are some outliers, we fill missing values with median. Mode is used to fill missing values for categorical column.","be1bedd1":"Here we will fill missing values with mean value for the numerical column.","f4d11f78":"## Introduction\nThe dataset used in this notebook is of '[IEEE-CIS Fraud Detection](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/data)'. This notebook will introduce you to class imbalance problem.\n\nData set link: [Fraud Dataset](https:\/\/drive.google.com\/file\/d\/1q8SYcjOJULdSkETv5S_gd7xNq1GrBHAO\/view)\n\n#### Imbalanced Problem\nImbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. Class imbalance can be found in many different areas including medical diagnosis, spam filtering, and fraud detection.\n\n#### Agenda\n*  Loading Libraries\n*  Loading Data\n*  Getting Basic Idea About Data\n*  Missing Values and Dealing with Missing Values\n*  One Hot Encoding (Creating dummies for categorical columns)\n*  Standardization \/ Normalization\n*  Splitting the dataset into train and test data\n*  Dealing with Imbalanced Data\n    *  Generate Synthetic Samples\n\n","9ca0190c":"We can notice, of 57049 observations \/ records only 2005 were fraud transactions.","cbfe89d2":"If you notice, a lot of dummy variables are created like; **P_emaildomain_hotmail.com, P_emaildomain_hotmail.de,** etc.","cb247d54":"To know about different parameters of random forest visit here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html","2e9d298e":"### Feature Selection\nFeature selection is the process of reducing the number of input variables when developing a predictive model.\n\nIt is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n\nHere we are using 'slectKBest' from 'sklearn.feature_selection' which selects features according to the k highest scores. It takes two parameters:\n\n* **score_func:** **callable**\n\n  Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below \u201cSee also\u201d). The default function only works with classification tasks.\n\n* **k:  optional, default=10**\n  \n  Number of top features to select.\n\n\nWhen we have numerical features and categorical output we can use 'f_classif' from 'sklearn.feature_selection'.\n\nFurther reading about feature selection: https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/\n\n","40295467":"3. **Generate Synthetic Samples:** Here we will use imblearn\u2019s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n\nAgain, it\u2019s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.","f95be607":"## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).","a0f9a1f5":"### One Hot Encoding (Creating dummies for categorical columns)\nIn this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column. In Python there is a class 'OneHotEncoder' in 'sklearn.preprocessing' to do this task, but here we will use pandas function 'get_dummies()'. This get_dummies() does the same work as done by 'OneHotEncoder' form sklearn.preprocessing.\n\nSo, let's create dummy variables.","e36b45a4":"### Hyper parameter tunning\nHyperparameters are important parts of the ML model and can make the model gold or trash. Here we have discussed one of the popular hyperparameter tunning method i.e. using Grid Search CV.","4c1d0f45":"### Cross - Validation\nUsually, our data is divided into Train and Test Sets. The Train set is further divided into Train and Validation set.\n\nThe Validation Set helps us in selecting good parameters\/tune the parameters for our model.","5ca04428":"References:\n1. [Dealing with Imbalanced Data by Tara Boyle](https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n2. [Data Pre-processing - Handling missing values and dealing with class imbalance by Bharat Ram Ammu](https:\/\/www.youtube.com\/watch?v=vksQx1JNo8Y)\n3. https:\/\/www.kaggle.com\/drgilermo\/a-tutorial-for-complete-beginners\n4. https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/","dd4e86dc":"Filling categorical columns with mode (i.e. maximum occuring element in the column)","db717029":"Notice, now we don't have any column with missing value."}}