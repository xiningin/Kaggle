{"cell_type":{"95d7dab8":"code","c8a7e0eb":"code","d7d1b7bb":"code","a9dce2a8":"code","20edfd82":"code","c7be4c1d":"code","31476784":"code","fbcee451":"markdown","2bbb5eca":"markdown"},"source":{"95d7dab8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom keras.layers import Input, Dense, Conv2D, Flatten\nfrom keras.models import Model\nfrom keras.optimizers import SGD, Adam\n\nfrom keras.utils import np_utils","c8a7e0eb":"#et voici le r\u00e9sultat de la transformation.\nnp_utils.to_categorical([0,1,2,1,0])\n\n#On peut voir 3 colonnes, chaquue colonne correspond \u00e0 une classe, (premi\u00e8re colonne est la classe 0,\n#deuxi\u00e8me colonne est la classe 1, troisi\u00e8me colonne est la classe 2).\n#On voit que la premi\u00e8re colonne contient des 1 pour les lignes appartenant \u00e0\n#la classe 0 (premi\u00e8re et derni\u00e8re ligne), etc\n\n#cette technique de transformation est appel\u00e9e le One Hot Encoding (ohe)","d7d1b7bb":"df = pd.read_csv('..\/input\/mnist-digit-recognizer\/train.csv')\n\ndfX = df.drop('label', axis=1).values\ndfY = df.label.values\ndfX = dfX\/255.\n\ndfY_ohe = np_utils.to_categorical(dfY) #Application du One Hot Encoding\nprint(dfY_ohe.shape)# Nous avons bien 10 vecteurs","a9dce2a8":"def create_dense_model():\n    inpt = Input ( (784,) )\n    \n    x = Dense(128, activation='relu', name='couche1')(inpt)\n    x = Dense(128, activation='relu', name='couche2')(x)\n    \n    x = Dense(10, activation='softmax', name='output')(x) # la couche de sortie contient 10 neurones\n    #chaque neurones va apprendre \u00e0 pr\u00e9dire la classe qui lui correspond\n    #Dans un probl\u00e8me multi_classification, on utilise l'activation softmax au lieu de la sigmoid\n    \n    model = Model( inpt, x )\n    return model\n\nmodel = create_dense_model()\nmodel.summary()\n\n\n","20edfd82":"cv = StratifiedKFold(n_splits=2)\nfor train_idx, test_idx in cv.split(dfX, dfY):\n    model = create_dense_model()\n    model.compile( loss='mse' , optimizer=Adam(), metrics=['accuracy'])\n    \n    es = EarlyStopping(patience=5, monitor='val_accuracy', mode='max')\n    mc = ModelCheckpoint('.\/weights.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n    \n    trainX = dfX[train_idx]\n    trainY = dfY_ohe[train_idx] #on utilise dfY_ohe et non dfY\n    \n    testX  = dfX[test_idx]    \n    testY  = dfY_ohe[test_idx]\n    \n    model.fit( trainX, trainY, validation_data=[testX, testY], callbacks = [es,mc],\n              epochs=1000)\n    \n    \n    model.load_weights('.\/weights.h5')#On charge les meilleurs poids sauvegard\u00e9s par le ModelCheckpoint\n    #on pr\u00e9dit le Test\n    preds = model.predict(testX)\n    score_test = accuracy_score( dfY[test_idx], np.argmax(preds, axis=1) )#j'expliquerai au cours\n    print (' LE SCORE DE TEST : ', score_test)\n    print('')","c7be4c1d":"def create_cnn_model():\n    inpt = Input ( (28, 28, 1) )\n    \n    x = Conv2D(filters=16, kernel_size=(4,4), strides=(2, 2), activation='relu')(inpt)\n    x = Conv2D(filters=32, kernel_size=(4,4), strides=(2, 2), activation='relu')(x)\n    x = Conv2D(filters=64, kernel_size=(4,4), strides=(2, 2), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)  \n    x = Dense(10, activation='softmax', name='output')(x)     \n    model = Model( inpt, x )\n    return model\n\nmodel = create_cnn_model()\nmodel.summary()\n\n\n","31476784":"cv = StratifiedKFold(n_splits=2)\nfor train_idx, test_idx in cv.split(dfX, dfY):\n    model = create_cnn_model()\n    model.compile( loss='mse' , optimizer=Adam(), metrics=['accuracy'])\n    \n    es = EarlyStopping(patience=5, monitor='val_accuracy', mode='max')\n    mc = ModelCheckpoint('.\/weights.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n    \n    trainX = dfX[train_idx]\n    trainY = dfY_ohe[train_idx] #on utilise dfY_ohe et non dfY\n    trainX = np.reshape( trainX, (-1,28,28, 1) )#on transforme les vecteurs de pixels en matrices repr\u00e9sentant les images\n    \n    testX  = dfX[test_idx]    \n    testY  = dfY_ohe[test_idx]\n    testX = np.reshape( testX, (-1,28,28, 1) )\n    \n    model.fit( trainX, trainY, validation_data=[testX, testY], callbacks = [es,mc],\n              epochs=1000)\n    \n    \n    model.load_weights('.\/weights.h5')#On charge les meilleurs poids sauvegard\u00e9s par le ModelCheckpoint\n    #on pr\u00e9dit le Test\n    preds = model.predict(testX)\n    score_test = accuracy_score( dfY[test_idx], np.argmax(preds, axis=1) )#j'expliquerai au cours\n    print (' LE SCORE DE TEST : ', score_test)\n    print('')","fbcee451":"* ceci est un exemple des r\u00e9sultats d'un epoch du r\u00e9seau de neurone :\n#### loss: 0.0167 - accuracy: 0.8902 - val_loss: 0.0097 - val_accuracy: 0.9363\n\n#### puisque nous avons fix\u00e9 loss='mse' \n* loss         : c'est l'erreur mse du TRAIN\n* val_loss     : c'est l'erreur mse du TEST\n* accuracy     : c'est la performance du r\u00e9seau en utilisant la mesure (metric) accuracy sur le TRAIN\n* val_accuracy : c'est la performance du r\u00e9seau en utilisant la mesure (metric) accuracy sur le TEST\n#### ---------------------------------------\n* Par d\u00e9faut, le earlystopping va monitorer (surveiller) l'\u00e9volution du val_loss du TEST, avec mode='min' puiqu'on souhaite minimiser le loss.\n* Or on souhaite maximiser l'accuracy du TEST. On va donc dire \u00e0 EarlyStopping de surveiller val_accuracy avec mode='max'\n* Le m\u00eame principe s'applique \u00e0 ModelCheckpoint. Mais on doit indiquer save_best_only=True afin que seuls les meilleurs poids soit sauvegard\u00e9s dans le fichier, sinon ils seront remplac\u00e9s par les epochs suivantes\n","2bbb5eca":"* On va devoir classifier des images contenant des chiffres \u00e9crits \u00e0 la main en 10 classes (0, 1, 2 ... 9), il s'agit d'un probl\u00e8me de clasiification multi-classe.\n* Le r\u00e9seau de neurones va avoir une couche de sortie contenant 10 neurones. Chaque neurone correspond \u00e0 une classe (un chiffre). On doit donc lui donner 10 vecteurs, chaque vecteur repr\u00e9sentera un chiffre\n* les donn\u00e9es que nous avons contiennent une colonne \"label\" avec des valeurs entre 0 et 9 indiquant le contenu de l'image. On ne peut pas utiliser directement ce vecteur au niveau de la sortie du r\u00e9seau de neurones, on doit le restructurer de mani\u00e8re \u00e0 ce que chaque classe soit encoder dans un vecteur diff\u00e9rent. Pour cela, on utilisera la fonction to_categorical de keras.\n* Voici un exemple d'utilisation, soit y = [0,1,2,1,0]. Ce vecteur poss\u00e8de 3 classes 0, 1 et 2\n\n"}}