{"cell_type":{"dba95f47":"code","48b041c0":"code","b5a597ca":"code","84dfb6eb":"code","63a5256c":"code","7eaf827e":"code","ae392a06":"code","93469963":"code","8eb7672b":"code","6cb9a776":"code","db069678":"code","4a58ee2e":"code","0717d831":"code","8371fa26":"code","11757dc2":"code","03158dd3":"code","2133466a":"code","e54a5025":"code","bd7ce313":"code","e20f1491":"code","a07f035c":"code","d2065682":"code","c9b3ee33":"code","f8c7ab6f":"code","eed1a9cf":"code","3807bba1":"code","ce9d1141":"code","13f5be42":"code","32ac7b80":"code","da0c9348":"code","1ee4ba2d":"code","0d289234":"code","b6f901ec":"code","43f8ab11":"code","d7970238":"code","19885554":"code","27484323":"code","a609c7bb":"code","06ed6f01":"code","092440f9":"code","e2f8dc27":"code","7f392fd4":"code","a3653d6a":"code","7ede83a3":"code","de589381":"code","26ac7a9b":"code","38623567":"code","6dbc7b91":"code","e29483d8":"code","93cea113":"markdown","e91437cf":"markdown","28337f08":"markdown","f3da5354":"markdown","562b44b9":"markdown","f6507304":"markdown","33017817":"markdown","200eb8c2":"markdown","59ae9c85":"markdown"},"source":{"dba95f47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48b041c0":"#usual imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nassert sys.version_info >= (3,5)\n#visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#consistent sized plot\nfrom pylab import rcParams\nrcParams['figure.figsize']= 12,5\nrcParams['axes.labelsize']= 12\nrcParams['xtick.labelsize']= 12\nrcParams['ytick.labelsize']= 12\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(action='ignore')\n#import nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n#regular expressions\nimport re\nimport string\nfrom string import punctuation\nfrom string import digits\n#sklearn for text manipulation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#data preparation for modeling\nfrom sklearn.model_selection import train_test_split\n#dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\n#evaluation metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss","b5a597ca":"train = pd.read_csv('..\/input\/ugam-decode-code-words\/train.csv',delimiter=',',engine='python')\ntrain.head()","84dfb6eb":"'''check basic info of the dataset '''\ntrain.info()","63a5256c":"'''check for the null values in the dataset'''\ntrain.isnull().sum()","7eaf827e":"#drop the id column\ntrain.drop('Id',axis=1,inplace=True)\n#create a list of all the numerical features\nnum_features = train.select_dtypes(exclude='object').columns.to_list()\nprint('Numerical features\\n {}'.format(num_features))","ae392a06":"'''check if all the targets are binary or have different labels'''\nfor feature in num_features:    \n    print(train[feature].value_counts())","93469963":"#stacked bar graph of all binary numerical features. In this dataset all the features except the review feature are target features\nzero_count = []\none_count = []\nfor col in num_features:\n    zero_count.append((train[col]==0).sum())\n    one_count.append((train[col]==1).sum())\n\nN = len(num_features)\nind = np.arange(N)\nwidth = 0.25\n\nplt.figure(figsize=(10,10))\np1 = plt.barh(ind, zero_count, width, color='red',alpha=0.5)\np2 = plt.barh(ind, one_count, width, left=zero_count, color='blue',alpha=0.5)\nplt.yticks(ind, num_features)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.ylabel('Numerical Features')\nplt.show()","8eb7672b":"train.head(3)","6cb9a776":"'''new feature which is a row wise sum of the target features'''\ntrain['concat'] = train.sum(axis=1)","db069678":"train.head()","4a58ee2e":"train['concat'].value_counts()","0717d831":"num_features.append('concat')","8371fa26":"'''Assign to X and y as input and output features'''\nX =  train[['Review']]\ny = train[num_features]","11757dc2":"X_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,stratify=train['concat'],random_state=42)","03158dd3":"y_train['concat'].value_counts()\/len(y_train)","2133466a":"y_valid['concat'].value_counts()\/len(y_valid)","e54a5025":"'''drop the concat feature'''\ny_train.drop('concat',axis=1,inplace=True)\ny_valid.drop('concat',axis=1,inplace=True)","bd7ce313":"num_features.remove('concat')","e20f1491":"'''distribution of the target features in the y_train'''\nzero_count = []\none_count = []\nfor col in num_features:\n    zero_count.append((y_train[col]==0).sum())\n    one_count.append((y_train[col]==1).sum())\n\nN = len(num_features)\nind = np.arange(N)\nwidth = 0.25\n\nplt.figure(figsize=(10,10))\np1 = plt.barh(ind, zero_count, width, color='red',alpha=0.5)\np2 = plt.barh(ind, one_count, width, left=zero_count, color='blue',alpha=0.5)\nplt.yticks(ind, num_features)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.ylabel('Numerical Features')\nplt.show()","a07f035c":"'''distribution of the target features in the y_valid'''\nzero_count = []\none_count = []\nfor col in num_features:\n    zero_count.append((y_valid[col]==0).sum())\n    one_count.append((y_valid[col]==1).sum())\n\nN = len(num_features)\nind = np.arange(N)\nwidth = 0.25\n\nplt.figure(figsize=(10,10))\np1 = plt.barh(ind, zero_count, width, color='red',alpha=0.5)\np2 = plt.barh(ind, one_count, width, left=zero_count, color='blue',alpha=0.5)\nplt.yticks(ind, num_features)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.ylabel('Numerical Features')\nplt.show()","d2065682":"X_train.head()","c9b3ee33":"'''randomly check one of the text'''\nX_train['Review'].iloc[np.random.randint(0,len(X_train))]","f8c7ab6f":"'''text cleaning'''\nstemmer = PorterStemmer()\ndef text_cleaning(text):\n    #conver to lower case\n    text = text.lower()\n    #remove the string punctuation\n    text = ' '.join([txt for txt in word_tokenize(text) if txt not in string.punctuation])\n    #remove all non alphabetical characters\n    text = ' '.join([txt for txt in word_tokenize(text) if txt.isalpha()])\n    #replace multiple whitespaces with a single whitespace\n    text = text.replace(r'\\s+',' ')\n    #remove the leading and trailing whitespaces\n    text = text.replace(r'^\\s+|\\s+?$', '')\n    #remove the stop words\n    text = ' '.join([txt for txt in word_tokenize(text) if txt not in stopwords.words('english')])\n    #apply stemmer\n    text = ' '.join([stemmer.stem(txt) for txt in word_tokenize(text)])\n    \n    #return the cleaned text\n    return text   \n","eed1a9cf":"def simplify(text):\n    '''Function to handle the diacritics in the text'''\n    import unicodedata\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError:\n        pass\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    return str(text)","3807bba1":"'''\nClean the text values in train and valid set\n'''\npreprocesses = [simplify,text_cleaning]\n\nfor preprocess in preprocesses:\n    X_train['Review'] = X_train['Review'].apply(preprocess)\n    X_valid['Review'] = X_valid['Review'].apply(preprocess)","ce9d1141":"X_train.head(3)","13f5be42":"'''encode the cleaned review text'''\nvectorizer = TfidfVectorizer(ngram_range=(1,2))\ncounts = vectorizer.fit_transform(X_train['Review'])\nvocab = vectorizer.vocabulary_","32ac7b80":"valid_counts = vectorizer.transform(X_valid['Review'])","da0c9348":"'''base model'''\nclf = MultiOutputClassifier(RandomForestClassifier(max_depth=5))","1ee4ba2d":"clf.fit(counts,y_train)","0d289234":"#get the predictions on the valid set\nvalid_predictions = clf.predict_proba(valid_counts)","b6f901ec":"#get the predictions on the train set\ntrain_predictions = clf.predict_proba(counts)","43f8ab11":"'''test data'''\ntest = pd.read_csv('..\/input\/ugam-decode-code-words\/test.csv',engine='python',delimiter=',')\ntest.head(3)","d7970238":"X_test = test[['Review']]","19885554":"'''clean the texts in the test set'''\nfor preprocess in preprocesses:\n    X_test['Review'] = X_test['Review'].apply(preprocess)","27484323":"'''vectorize the text'''\ntest_counts = vectorizer.transform(X_test['Review'])","a609c7bb":"'''predictions'''\ntest_predictions = clf.predict_proba(test_counts)\n","06ed6f01":"#extract the probability of target 1, ie the label is selected\npreds_cal = []\nfor item in test_predictions:\n    preds_cal.append( [local_item[1] for local_item in item] )","092440f9":"df_test_predictions = pd.DataFrame(np.array(preds_cal).T, columns=num_features)\ndf_test_predictions.head(3)","e2f8dc27":"df_test_predictions.to_csv('submission_rf.csv',index=False)","7f392fd4":"import tensorflow as tf","a3653d6a":"#convert the sparse matrices to dense arrays\ncount = counts.todense()\nvalid_count = valid_counts.todense()","7ede83a3":"count.shape","de589381":"#instantiate a sequential model\nmodel = tf.keras.models.Sequential()\n#add the layers\nmodel.add(tf.keras.layers.Dense(512,input_dim= count.shape[1],activation='tanh'))\nmodel.add(tf.keras.layers.Dense(256,activation='tanh'))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(128,activation='tanh'))\nmodel.add(tf.keras.layers.Dense(64,activation='tanh'))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(32,activation='tanh'))\nmodel.add(tf.keras.layers.Dense(12,activation='sigmoid'))\n#compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='nadam',metrics=['accuracy'])","26ac7a9b":"history = model.fit(count,y_train,epochs=500,batch_size=64,validation_data=(valid_count,y_valid),callbacks=tf.keras.callbacks.EarlyStopping(patience=400,\n                                                                                                                                            restore_best_weights=True))","38623567":"nn_test_predictions = model.predict(test_counts)","6dbc7b91":"df_test_predictions = pd.DataFrame(data=nn_test_predictions, columns=num_features)\ndf_test_predictions.head(3)","e29483d8":"df_test_predictions.to_csv('submission_nn_tanh.csv',index=False)","93cea113":"# _Load the data_","e91437cf":"- _Overall there is more of Positive sentiments compared to the negative sentiments depicted by the Polarity feature_\n- _Mostly the target labels are imbalanced in this dataset_\n","28337f08":"_There are no null values in the dataset. Except for the text column all the columns are of numeric type_","f3da5354":"# _Modeling_","562b44b9":"# Import Libraries\n","f6507304":"# _Deep Learning Model_","33017817":"_Create a new feature which is a concatenation of all the target features. Using this feature, the datase will be stratified split into training and validation set_","200eb8c2":"# _Split the data based on the various target features_","59ae9c85":"# _Handle the text columns_"}}