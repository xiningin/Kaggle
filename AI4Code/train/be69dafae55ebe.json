{"cell_type":{"021e2de0":"code","6d0dc9c6":"code","454f3249":"code","a8636805":"code","a7089969":"code","d7f7618f":"code","138728c6":"code","0076d592":"code","1d8cc6b7":"code","44b1c852":"code","3da84d59":"code","593d1848":"code","80c680e3":"code","eb11d379":"code","fde5c00d":"code","84d2d4af":"code","98de002c":"code","d8a90a50":"code","9514969d":"code","dfc541f8":"code","bb3c7ecc":"code","9a2e0679":"code","cfce2832":"code","26fbc572":"code","15ec0783":"code","8073f4c0":"code","5890994d":"code","c3901894":"code","48c5030c":"code","587f271c":"code","09ffaf48":"code","037de44e":"code","869fbc7c":"code","9c7e3272":"code","cbf4eaf0":"code","81ef6ad6":"code","8154b3c2":"code","90e42106":"code","b6699426":"code","a0257d7a":"code","bd3c4a74":"code","9d3de390":"code","7ee732b2":"markdown","d8493e89":"markdown","d3782589":"markdown","76358eca":"markdown","a7ce824e":"markdown","6fed4fdc":"markdown","4cc62869":"markdown","5e723fd5":"markdown","c177122d":"markdown","66159686":"markdown","2569337f":"markdown","51b0dc64":"markdown","7a991b2c":"markdown","1dfc137c":"markdown"},"source":{"021e2de0":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# !pip install chart_studio\n# !pip install textstat\n\nimport numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n# import chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# File system manangement\nimport os\n\n# Pytorch\nimport torch\n\n#Transformers\nfrom transformers import BertTokenizer\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n","6d0dc9c6":"# Training Data\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n\nprint(\"Training data shape.. \",train.shape)\nprint(\"Testing data shape.. \",test.shape)\n\n# First few rows of the training dataset\ntrain.head()\n\n# First few rows of the testing dataset\ntest.head()","454f3249":"#Missing values in training set\ntrain.isnull().sum()\n#Missing values in test set\ntest.isnull().sum()","a8636805":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","a7089969":"# Applying the cleaning function to both test and training datasets\ntrain['excerpt_clean'] = train['excerpt'].apply(str).apply(lambda x: text_preprocessing(x))\ntest['excerpt_clean'] = test['excerpt'].apply(str).apply(lambda x: text_preprocessing(x))","d7f7618f":"train['excerpt_len'] = train['excerpt_clean'].astype(str).apply(len)\ntrain['excerpt_count'] = train['excerpt_clean'].apply(lambda x: len(str(x).split()))","138728c6":"train['Character Count'] = train['excerpt'].apply(lambda x: len(str(x)))","0076d592":"train.head()","1d8cc6b7":"train['excerpt_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Excerpt text Length Distribution')","44b1c852":"trace0 = go.Box(\n    y=train['excerpt_len'],\n    name = 'Text',\n    marker = dict(\n        color = 'red',\n    )\n)","3da84d59":"data = [trace0]\nlayout = go.Layout(\n    title = \"Length of the text\"\n)\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Length of the text of different polarities\")","593d1848":"train['excerpt_count'].iplot(\nkind='hist',\nbins=50,\nxTitle='text_length',\nlinecolor='black',\ncolor='green',\nyTitle='count',\ntitle='Excerpt text word count')","80c680e3":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","eb11d379":"unigrams=get_top_n_words(train['excerpt_clean'],20)\ndf1 = pd.DataFrame(unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Unigrams in excerpt text',orientation='h')","fde5c00d":"#Distribution of top Bigrams\ndef get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","84d2d4af":"bigrams = get_top_n_gram(train['excerpt_clean'],(2,2),20)","98de002c":"for word,freq in bigrams:\n    print(word,freq)","d8a90a50":"#for word, freq in top_bigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='blue', title='Top 20 Bigrams in excerpt text',orientation='h')","9514969d":"trigrams = get_top_n_gram(train['excerpt_clean'],(3,3),20)\ndf2 = pd.DataFrame(trigrams,columns=['Text','count'])\n\ndf2.groupby(\"Text\").sum()['count'].sort_values(ascending=True).iplot(\nkind='bar',yTitle='Count',linecolor='black',color='orange',title='Top 20 Trigrams Text',orientation='h')","dfc541f8":"tc = train['excerpt_clean']","bb3c7ecc":"from wordcloud import WordCloud\nfig, (ax1) = plt.subplots(1, 1, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(tc))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('excerpt clean text',fontsize=40);\n","9a2e0679":"train['target'].iplot(kind='hist',xTitle='Target',yTitle='Density',linecolor='black',color='blue')","cfce2832":"train['standard_error'].iplot(kind='hist',xTitle='standard_error',yTitle='Density',linecolor='black',color='blue')","26fbc572":"vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(1,6))\nX = vectorizer.fit_transform(train.excerpt_clean,)\nX.shape","15ec0783":"%%time\nmodel = LinearSVR(random_state=42)\nscores = cross_val_score(model, X, train.target, cv=5,scoring='neg_root_mean_squared_error')\nscores *=-1\nscores.mean()","8073f4c0":"%%time\nmodel.fit(X,train.target)","5890994d":"# sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv',index_col='id')\n# x = vectorizer.transform(sub.excerpt)\n# p = model.predict(x)\n# sub['target'] = p\n# sub[['target']].to_csv('submission.csv')","c3901894":"from time import time\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","48c5030c":"tfidf = TfidfVectorizer(sublinear_tf=True, norm='l2',ngram_range=(1,1))\nfeatures = tfidf.fit_transform(train.excerpt_clean).toarray()\nfeatures.shape","587f271c":"features_test = tfidf.transform(test.excerpt_clean).toarray()","09ffaf48":"params = {'metric': 'rmse','random_state': 48,'n_estimators': 20000,'reg_alpha': 0.0010819683712588644,\n          'reg_lambda': 0.004760428916800031, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.01,\n          'max_depth': 100, 'num_leaves': 39, 'min_child_samples': 12, 'cat_smooth': 67}\npreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(features,train['target']):\n    X_tr,X_val=features[trn_idx],features[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = LGBMRegressor(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(features_test)\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1","037de44e":"import seaborn as sns\n# Prediction distibution\nplt.figure(figsize=(10,4))\nsns.kdeplot(preds,shade=True)\nplt.show()","869fbc7c":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv',index_col='id')\nsub['target']=preds\nsub[['target']].to_csv('submission.csv')","9c7e3272":"# pip install sentence-transformers","cbf4eaf0":"# loading model using sentence transformers\n\n# import sentence_transformers\n# from sentence_transformers import SentenceTransformer, models","81ef6ad6":"# setting model path for fine-tuned roberta weights\n\n# model_path = '..\/input\/finetuned-model1\/checkpoint-568'\n# word_embedding_model = models.Transformer(model_path, max_seq_length=275)\n# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])","8154b3c2":"# test.head()","90e42106":"# encoding train and test strings\n\n# X_train = model.encode(train.excerpt, device='cuda')\n# X_test = model.encode(test.excerpt, device='cuda')","b6699426":"# from sklearn.model_selection import StratifiedKFold\n# from datetime import datetime\n# from sklearn.metrics import mean_squared_error\n# from sklearn.linear_model import BayesianRidge\n\n# preds = []\n# train_scores = []\n\n# df_oof=train.copy()\n# df_oof['oof'] = 0\n\n# skf = StratifiedKFold(10, shuffle=True, random_state=42)\n\n# splits = list(skf.split(X=X_train, y=train['Character Count']))\n\n# # predicting out of fold scores for each fold and doing predictions for each training set\n\n# for i, (train_idx, val_idx) in enumerate(splits):\n#     print(f'\\n------------- Training Fold {i + 1} \/ {10}')\n#     print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n\n#     clf = BayesianRidge(n_iter=300, verbose=True)\n#     clf.fit(X_train[train_idx],train.target[train_idx])\n#     train_score=mean_squared_error(train.target[train_idx], clf.predict(X_train[train_idx]), squared=False)\n#     train_scores.append(train_score)\n#     print(f\"Fold {i} train RMSE: {train_score}\")\n    \n    \n#     preds.append(clf.predict(X_test))\n#     x=clf.predict(X_train[val_idx])\n#     df_oof['oof'].iloc[val_idx]+= x\n\n# print(f'Training score: {np.mean(train_scores)}, Training STD: {np.std(train_scores)}')\n# print(f'OOF score across folds: {mean_squared_error(df_oof.target, df_oof.oof, squared=False)}')","a0257d7a":"# getting mean prediction across 5 folds\n# y_pred = np.mean(preds,0)\n# y_pred.shape","bd3c4a74":"# sub = test[[\"id\"]].copy()\n# sub[\"target\"] = y_pred\n# sub.to_csv('submission.csv', index = False)","9d3de390":"# checking submission file\n\nsub.head()","7ee732b2":"## Checking the missing value","d8493e89":"# Unigram","d3782589":"# Distribution of Excerpt length","76358eca":"# Trigrams","a7ce824e":"# Building the baseline model","6fed4fdc":"## Utility function","4cc62869":"# Bigrams","5e723fd5":"# Now we will use LightGBM","c177122d":"# **Loading the Data**","66159686":"##   List the top n words in a vocabulary according to occurrence in a text corpus","2569337f":"# Distribution of the target variable","51b0dc64":"# Distribution of Standard error","7a991b2c":"### Text preprocessing using the utility function","1dfc137c":"## Distribution of excerpt count"}}