{"cell_type":{"3dd0a085":"code","c0185dad":"code","746962a4":"code","054292c5":"code","2930a5d6":"code","ee47e7df":"code","f5ad2c6a":"code","957f6d22":"code","e47383fc":"code","55396fbf":"code","d09e6abb":"code","d8c12dce":"code","b8332d52":"markdown","4678f714":"markdown"},"source":{"3dd0a085":"# %%writefile DataGenerator.py\n# learned from: https:\/\/github.com\/allenday\/contextual-bandit\/blob\/master\/contextual_bandit_sim.ipynb\nimport numpy as np\nclass DataGenerator():\n    \"\"\"\n    Generate bandit data.\n    Defaults:\n    K = 2 arms\n    D = 2 features\/arm\n    only K arms with fixed features of dimension D too.\n    And if you select an arm => it is put into the sample => so why there are many samples when we only have K fixed arms? => what will be put in the sampled results if user select an arm? => isn't it the arm itself and corresponding rewards?\n    \n    \"\"\"\n    def __init__(self, K = 2, D = 2):\n        self.D = D # dimension of the feature vector\n        self.K = K # number of bandits\n        self.means = np.random.normal(size=self.K)\n        self.stds = 1 + 2*np.random.rand(self.K)\n        # generate the weight vectors. Initialioze estimate of feature importance for each arm's features\n        self.generate_weight_vectors()\n    def generate_weight_vectors(self, loc=0.0, scale=1.0):\n        self.W = np.random.normal(loc=loc, scale=scale, size=(self.K, self.D))\n    \n    def generate_samples(self, n = 1000):\n        X = np.random.randint(0, 5, size=(n, self.D))\n\n        # The rewards are functions of the inner products of the feature vectors with current weight estimates\n        IP = np.dot(X, self.W.T)\n        \n        # now get the rewards\n        R = np.abs(np.random.normal(self.means + IP, self.stds))\n        \n        return X, R\n    \n    # Thompson Smapling\n    # basic idea: samples from distribution and compares those values for the arms instead\n    def thompson_sampling(self, observed_data):\n        return np.argmax(np.random.beta(observed_data[:, 0], observed_data[:, 1]))","c0185dad":"# %%writefile OnlineVariance.py\n#learned from: https:\/\/github.com\/allenday\/contextual-bandit\/\n#http:\/\/stackoverflow.com\/questions\/5543651\/computing-standard-deviation-in-a-stream\nimport numpy as np\nclass OnlineVariance(object):\n    \"\"\"\n    Welford's algorithm computes the sample variance incrementally\n    ddof: delta degree of freedom (used in the divisor: N - ddof) e.g., for the whole population ddof = 0; for sample of elements ddof = 1;\n    \"\"\"\n    def __init__(self, iterable=None, ddof = 1):\n        self.ddof, self.n, self.mean, self.M2, self.variance = ddof, 0, 0.0, 0.0, 0.0\n        if iterable is not None:\n            for datum in iterable:\n                self.include(datum)\n    def include(self, datum):\n        self.n += 1\n        self.delta = datum - self.mean\n        self.mean += self.delta\/self.n\n        self.M2 += self.delta*(datum - self.mean)\n        self.variance = self.M2\/(self.n - self.ddof)\n    @property\n    def std(self):\n        return np.sqrt(self.variance)","746962a4":"# %%writefile PositiveStrategy.py\n# learned from: https:\/\/github.com\/allenday\/contextual-bandit\/\nimport numpy as np\n# from OnlineVariance import OnlineVariance\nclass PositiveStrategy(object):\n    \"\"\"\n    Positive strategy selector.\n    Defaults:\n    K = 2 arms\n    D = 2 features\/arm\n    epsilon = 0.05 (learning rate\/exploration in this case)\n    \"\"\"\n    def __init__(self, K = 2, D = 2, epsilon = 0.05):\n        self.K = K\n        self.D = D\n        self.epsilon = epsilon\n        \n        self.stats = np.empty((K, D), dtype = object)\n        \n        for k in range(0, K):\n            for d in range(0, D):\n                self.stats[k, d] = OnlineVariance(ddof = 0)\n                \n    def mu(self):\n        result = np.zeros((self.K, self.D))\n        for k in range(0, self.K):\n            for d in range(0, self.D):\n                result[k, d] = self.stats[k, d].mean\n        return result\n    \n    def sigma(self):\n        result = np.zeros((self.K, self.D))\n        for k in range(0, self.K):\n            for d in range(0, self.D):\n                result[k, d] = self.stats[k, d].std\n        return result\n    \n    def include(self, arm, features, value):\n        for d in range(0, self.D):\n            if features[d] > 0:\n                self.stats[arm, d].include(value)\n                \n    def estimate(self, arm, features):\n        # why estimate what is the purpose, this return sum([1, 2, 3]*[1, 3, 2]) = sum([1, 6, 6]) = 13 => so the estimate is the dot product of the features and the arm feature.\n        # So there is a context feature and an arm feature\n        # OK, sample feature is there, how about arm feature? Where do you get it? And if it is selected => the sample should include the arm feature too?\n        # how about time t => at that round => what do you present to the agent?\n        return np.sum(features * [val for val in map(lambda x: np.random.normal(x.mean, x.std if x.std > 0 else 1), self.stats[arm])])\n    \n    def rmse(self, weights):\n        # it is the root means squared error of two matrices of size (K, D), between the estimated W and the actual weights.\n        return np.sqrt(np.mean((weights-self.mu())**2)\/self.K)","054292c5":"import numpy as np\nclass Simulator(object):\n    \"\"\"\n    Simulate model\n    epsilon=0.05 learning rate, this is the exploration rate.\n    \"\"\"\n    def __init__(self, model, epsilon=0.05):\n        self.model = model\n        self.K = model.K\n        self.D = model.D\n        self.epsilon = epsilon\n\n    def simulate(self,features,rewards,weights):\n        N = int(rewards.size\/self.K)\n\n        regret = np.zeros((N,1))\n        rmse = np.zeros((N,1))\n\n        for i in range(0,N):\n            F = features[i]\n            R = rewards[i]\n            \n            #known reward and correct choice\n            armOptimal = np.argmax(R)\n            #estimate the values of the arms and select the armChoice\n            armChoice = np.argmax([self.model.estimate(k, F) for k in range(self.K)])\n            \n            #learn from an arm other than best estimate with p=epsilon\n            learn = np.random.uniform() <= self.epsilon\n            if learn:\n                armAlt = armChoice\n                while (armAlt == armChoice):\n                    armAlt = int(np.random.uniform() * self.K)\n                armChoice = armAlt\n\n            #calculate reward and regret for chosen arm\n            armReward = R[armChoice]\n            armMaxReward = R[armOptimal]\n            armRegret = armMaxReward - armReward\n            regret[i] = armRegret\n            rmse[i]   = self.model.rmse(weights)\n\n            #reward\/penalize accordingly\n            if armRegret == 0:\n                self.model.include(armChoice, F, armReward)\n            else:\n                self.model.include(armChoice, F, -1 * armRegret)\n            \n        return regret, rmse","2930a5d6":"%matplotlib inline\nimport numpy as np\nfrom matplotlib import pylab as plt","ee47e7df":"num_arms = 3\nnum_features = 5\n\n# define number of samples and number of choices\nnum_samples = 1000\nnum_batches = 100\nnum_experiments = 10\ndg = DataGenerator(num_arms, num_features)","f5ad2c6a":"total_regret = []\ntotal_rmse = []\nfor e in range(0, num_experiments):\n    print(\"experiment: %d\" % e)\n    positiveStrategy = PositiveStrategy(num_arms, num_features)\n    simulator = Simulator(positiveStrategy)\n    \n    previous_rmse = 0.\n    for b in range(0, num_batches):\n        (sample_features, sample_rewards) = dg.generate_samples(num_samples)\n        regret, rmse = simulator.simulate(sample_features, sample_rewards, dg.W)\n        \n        if previous_rmse == 0:\n            initial_rmse = rmse[0][-1]\n            previous_rmse = rmse[0][-1]\n        if(len(total_rmse) == 0):\n            total_rmse = rmse\n            total_regret = regret\n        else:\n            total_rmse += rmse\n            total_regret += regret\n        \nmean_regret = total_regret\/num_experiments\nmean_rmse = total_rmse\/num_experiments","957f6d22":"print(dg.W)","e47383fc":"print(positiveStrategy.mu())","55396fbf":"print(len(sample_rewards))","d09e6abb":"plt.semilogy(np.cumsum(mean_regret)\/num_experiments)\nplt.title('Simulated Bandit Performance for K = ' + str(num_arms))\nplt.ylabel('Cumulative Expected Regret')\nplt.xlabel('Round Index')","d8c12dce":"plt.semilogx(mean_rmse\/num_experiments)\nplt.title('Simulated Bandit Performance for K = ' +str(num_arms))\nplt.ylabel('RMSE')\nplt.xlabel('Round Index')\nplt.show()","b8332d52":"# TEST","4678f714":"learned from: https:\/\/github.com\/allenday\/contextual-bandit\/"}}