{"cell_type":{"74e848b7":"code","8f14bbfa":"code","0dd46c4c":"code","b01d0f15":"code","8a53de90":"code","7fe848db":"code","21d3abaf":"code","504a8a85":"code","f8ad26f4":"code","d8348201":"code","93ad0657":"code","87c38eee":"code","618242ae":"code","2ffacd2d":"code","90d44bdf":"code","80e13070":"code","2c3b0322":"code","65955f8d":"code","4e2ed9af":"code","ec42c0fd":"code","591db6c0":"code","2d7a3574":"code","a8dcff42":"code","92ff5d5f":"code","6acf698b":"code","96a913fb":"code","79a6a766":"code","d11678e3":"code","d28dcd4a":"code","d016a3cf":"code","1897b83d":"code","7698f2b6":"code","97b2aa53":"code","4cec6415":"code","cd47b065":"code","5fb86dfa":"code","8c88dbb1":"code","01d3f813":"code","927684e1":"code","73fde91e":"code","6d9c4a1e":"code","a7039504":"code","1b375fb7":"markdown","c0580f08":"markdown","6f4d11b0":"markdown","e02fb7de":"markdown"},"source":{"74e848b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f14bbfa":"my_train_df = pd.read_csv(\"\/kaggle\/input\/learn-ai-bbc\/BBC News Train.csv\")\nmy_test_df = pd.read_csv(\"\/kaggle\/input\/learn-ai-bbc\/BBC News Test.csv\")","0dd46c4c":"#Categorize sport articas 1 and non-sport as\nmy_train_df['Category_id'] = np.where(my_train_df['Category'] == 'sport', 1, 0)\nlen(my_train_df)","b01d0f15":"my_train_df.drop_duplicates(subset=['Category', 'Text'], inplace=True)\nlen(my_train_df)","8a53de90":"#separating sport and non sport distribution to do correct class distribution\nsport_articles_df = my_train_df[my_train_df['Category_id'] == 1]\nnon_sport_articles_df = my_train_df[my_train_df['Category_id'] != 1]","7fe848db":"len(sport_articles_df),len(non_sport_articles_df)","21d3abaf":"# Addressing Class imbalance.\n# Performing Oversampling\n\n#non_sport_articles_df = non_sport_articles_df.sample(n=len(sport_articles_df) , ignore_index=True)\nsport_articles_df = sport_articles_df.sample(n=len(non_sport_articles_df),replace=True,ignore_index=True)","504a8a85":"len(sport_articles_df),len(non_sport_articles_df)","f8ad26f4":"#now we need to merge the two dataframe.\n\ntotal_df = pd.concat([sport_articles_df,non_sport_articles_df], axis=0)\nlen(total_df)","d8348201":"#now we need to suffle the df\ntotal_df = total_df.sample(frac=1, ignore_index=True)","93ad0657":"#lowercase\ntotal_df['Text'] = total_df.Text.apply(lambda x: x.lower())","87c38eee":"#remove punctuations\nimport string\ntable = str.maketrans(\"\", \"\", string.punctuation)\ndef remove_punc(text):\n    return text.translate(table)","618242ae":"total_df['Text'] = total_df.Text.apply(lambda x: remove_punc(x))\ntotal_df.head(100)","2ffacd2d":"#Remove stopwords\n\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words(\"english\"))\n\ndef rem_stop(text):\n    word_list = [word for word in text.split() if word not in stop]\n    return \" \".join(word_list)","90d44bdf":"total_df['Text'] = total_df.Text.apply(lambda x: rem_stop(x))\ntotal_df.head(30)","80e13070":"# Lemmatization process\n# import nltk\n# dler = nltk.downloader.Downloader()\n# dler.download('wordnet')\n# import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef tokenize_and_lemmatize(text):\n    # tokenization to ensure that punctuation is caught as its own token\n#     tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n#     filtered_tokens = []\n    \n#     for token in tokens:\n#         if re.search('[a-zA-Z]', token):\n#             filtered_tokens.append(token)\n    lem = [lemmatizer.lemmatize(stemmer.stem(t)) for t in text.split()]\n    return \" \".join(lem)\n","2c3b0322":"total_df['Text'] = total_df.Text.apply(lambda x: tokenize_and_lemmatize(x))\ntotal_df.head(30)","65955f8d":"#Create corpus for glove\n\nfrom nltk.tokenize import word_tokenize\n\ndef create_corpus(df):\n    corpus = []\n    for text in df['Text']:\n        words = [word for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus","4e2ed9af":"corpus = create_corpus(total_df)","ec42c0fd":"corpus_len = len(corpus)\nprint(corpus_len)","591db6c0":"#getting max length for padding\nmax_length = max([len(text.split()) for text in total_df['Text']])","2d7a3574":"#train, val split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(\n    total_df['Text'], total_df['Category_id'], test_size=0.20, random_state=1)","a8dcff42":"x_train.shape, x_val.shape, y_train.shape, y_val.shape","92ff5d5f":"#install tensorflow\n!pip install tensorflow","6acf698b":"# #Tokenizer\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","96a913fb":"# #tokenize texts\ntokenizer = Tokenizer(num_words=corpus_len)","79a6a766":"# # text word sequences for train\ntokenizer.fit_on_texts(x_train)\ntrain_sequence = tokenizer.texts_to_sequences(x_train)","d11678e3":"# # text word sequences for val\ntokenizer.fit_on_texts(x_val)\nval_sequence = tokenizer.texts_to_sequences(x_val)","d28dcd4a":"# #Padding Train and Val. Using max_length in total corpus of train and val\ntrain_padded = pad_sequences(train_sequence, maxlen=max_length, truncating=\"post\", padding=\"post\")\nval_padded = pad_sequences(val_sequence, maxlen=max_length, truncating=\"post\", padding=\"post\")","d016a3cf":"#get the word index in dictionary \nword_index = tokenizer.word_index","1897b83d":"#len(word_index)","7698f2b6":"# Glove embedding\nembedding={}\nwith open(\"\/kaggle\/input\/glove-embeddings\/glove.6B.300d.txt\",\"r\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding[word] = vectors\nf.close()","97b2aa53":"# #Embedding contains in matrix\n# num_words = len(word_index) +1\n# embedding_matrix = np.zeros((num_words, 300)) #each word will be a 1:100 dim vector\n\n# for word, i in word_index.items():\n#     if i < num_words:\n#         embd_vec = embedding.get(word)\n#         if embd_vec is not None:\n#             embedding_matrix[i] = embd_vec","4cec6415":"# sentance vectorization\n\ndef sent_vectorize(sentance):\n    sent_vec = []\n    count = 0\n    for w in sentance:\n        try:\n            if count == 0:\n                sent_vec = embedding.get(w) #get the embedding of the word using the position index\n            else:\n                sent_vec = np.add(sent_vec, embedding.get(w)) #adding all the word embeddings into one embedding for a sentance\n            count += 1\n        except:\n            pass\n    \n    return np.asarray(sent_vec) \/ count #average out the sentace embedding","cd47b065":"train_data = []\nfor sentance in x_train:\n    train_data.append(sent_vectorize(sentance))","5fb86dfa":"val_data = []\nfor sentance in x_val:\n    val_data.append(sent_vectorize(sentance))","8c88dbb1":"#import\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import plot_model","01d3f813":"model = Sequential()\nmodel.add(Dense(100, input_dim = 300, activation=\"relu\"))\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dropout(0.2))\n# model.add(Dense(50, activation=\"relu\"))\n# model.add(Dropout(0.2))\nmodel.add(Dense(1, activation=\"sigmoid\"))","927684e1":"plot_model(model, show_shapes=True, show_layer_names=True)","73fde91e":"#optimizer\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=['accuracy'])","6d9c4a1e":"#training\ntrain_data = tf.stack(train_data)\ny_train = tf.stack(y_train)\ntf.compat.v1.reset_default_graph()\nmodel.fit(train_data, y_train, epochs=100, batch_size=10, verbose=0)","a7039504":"#evaluation\nval_data = tf.stack(val_data)\ny_val = tf.stack(y_val)\nloss, accuracy = model.evaluate(val_data,y_val, verbose=0)\nprint(\"Model loss: %2f, Accuracy: %2f\" %((loss*100),(accuracy*100)))","1b375fb7":"# Glove pretrained embedding : Downloaded from (https:\/\/nlp.stanford.edu\/data\/glove.6B.zip)","c0580f08":"# Glove Embedding","6f4d11b0":"# Processing: Punctuation removal, stop words removal ..","e02fb7de":"# Model Creation"}}