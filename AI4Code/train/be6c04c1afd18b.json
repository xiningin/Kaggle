{"cell_type":{"da2d01c4":"code","59caa3d3":"code","2bd9328d":"code","bbce3bc1":"code","4998b0c1":"code","db311614":"code","9e8567a7":"code","ee5104b6":"code","27f87113":"code","89419f6f":"code","f2279d25":"code","530e6085":"code","e20204f9":"markdown","33093e60":"markdown","0a088d3f":"markdown","7b8e736a":"markdown","f0bf099c":"markdown","7a4db67f":"markdown","82462d01":"markdown","ba6a49ca":"markdown","9d16aa0d":"markdown","1405fc66":"markdown","76634679":"markdown","02af40ac":"markdown","40510228":"markdown","b28f0f60":"markdown"},"source":{"da2d01c4":"# Import libraries.\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import metrics","59caa3d3":"data=pd.read_csv(\"..\/input\/standardized-dataset\/cluster_data.csv\")\ndata.head()","2bd9328d":"for columns in data:\n    print(columns,\"--> # of missing value\", data[columns].isna().sum() )","bbce3bc1":"# Apply Principle Component Anaylsis for visualizing the data in 2D space.\n\npca = PCA()\npca_data = pca.fit_transform(data)\npca_data = pd.DataFrame(pca_data, columns=[\"pc\"+str(i+1) for i in range(len(data.columns))])\nprint(\"pca.explained variance ratio:\\n \", \" \".join(map(\"{:.3f}\".format, pca.explained_variance_ratio_)))","4998b0c1":"plt.plot(np.cumsum(pca.explained_variance_ratio_));","db311614":"# Plot within-cluster sum of square.\n\nwss = []\n\nK = range(2,11)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=123)\n    kmeans = kmeans.fit(data)\n    wss.append(kmeans.inertia_)\nplt.plot(K, wss, \"b*-\")\nplt.xlabel(\"Number of clusters k\")\nplt.ylabel(\"Total Within Sum of Squares\")\nplt.title(\"Optimal number of clusters\")\nplt.show()\n    \n","9e8567a7":"def find_optimal_cluster_number_kmeans(data, lower_bound, upper_bound, random_state):\n    \n    \"Find optimal number of cluster according to silhouette score.\"\n    \n    silhouette_average = []\n    K = range(lower_bound, upper_bound)\n\n    for k in K:\n        kmeans = KMeans(n_clusters=k, random_state=random_state)\n        cluster_labels=kmeans.fit_predict(data) \n        silhouette_score = metrics.silhouette_score(data, cluster_labels)\n        silhouette_average.append([k, silhouette_score])\n        \n    silhouette_average = np.array(silhouette_average)\n    print(\"n_clusters =\", int(silhouette_average[np.argmax(silhouette_average[:,1:2])][0]),\n          \"The average silhouette_score is : %.4f\" % silhouette_average[np.argmax(silhouette_average[:,1:2])][1])","ee5104b6":"find_optimal_cluster_number_kmeans(data, 2, 11, random_state=123)","27f87113":"pca_data1 = pca_data[[\"pc1\",\"pc2\"]].copy()\ndata1 = data.copy() # data1 is created, we do not want to change original data as adding the cluster column.\n\nkmeans = KMeans(n_clusters=4, random_state=2464063)\ndata1[\"clusters\"] = kmeans.fit_predict(data1)\n\nplt.scatter(pca_data1[\"pc1\"], pca_data1[\"pc2\"], c=data1.clusters)\nplt.title(\"The visualization of the clustered data\")\nplt.xlabel(\"pc1:\" + \"{:.2f}\".format(pca.explained_variance_ratio_[0] * 100) + \" %\")\nplt.ylabel(\"pc2:\" + \"{:.2f}\".format(pca.explained_variance_ratio_[1] * 100) + \" %\")\nplt.show()","89419f6f":"epsilon = [1,1.25,1.5,1.75, 2,2.25,2.5,2.75, 3,3.25,3.5,3.75, 4]\nmin_samples = [10,15,20,25]\n\n\nsil_avg = []\nmax_value = [0,0,0,0]\n\nfor i in range(len(epsilon)):\n    for j in range(len(min_samples)):\n\n        db = DBSCAN(min_samples = min_samples[j], eps =epsilon[i]).fit(data)\n        #cluster_labels=dbscan.fit_predict(data) \n        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n        core_samples_mask[db.core_sample_indices_] = True\n        labels = db.labels_\n\n        # Number of clusters in labels, ignoring noise if present.\n        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n        n_noise_ = list(labels).count(-1)\n\n\n        silhouette_avg = metrics.silhouette_score(data, labels)\n        if silhouette_avg > max_value[3]:\n            max_value=(epsilon[i], min_samples[j], n_clusters_, silhouette_avg)\n        sil_avg.append(silhouette_avg)\n\nprint(\"epsilon=\", max_value[0], \n      \"\\nmin_sample=\", max_value[1],\n      \"\\nnumber of clusters=\", max_value[2],\n      \"\\naverage silhouette score= %.4f\" % max_value[3])","f2279d25":"# Apply DBSCAN\n\ndb = DBSCAN(eps=3.5, min_samples=20).fit(data)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(data, labels))","530e6085":"# Plot result\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = pca_data1[class_member_mask & core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = pca_data1[class_member_mask & ~core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('The visualization of the clustered data; estimated number of clusters: %d' % n_clusters_)\nplt.xlabel(\"pc1:\"+\"{:.2f}\".format(pca.explained_variance_ratio_[0]*100)+\" %\")\nplt.ylabel(\"pc2:\"+\"{:.2f}\".format(pca.explained_variance_ratio_[1]*100)+\" %\")\nplt.show()","e20204f9":"### Find optimal parameter setting for DBSCAN.","33093e60":"- The model (epsilon = 3.5 and min_sample = 20) which has 0.6695 maximum average silhouette score has the optimal number of cluster; 4.","0a088d3f":"## K-means Clustering","7b8e736a":"### Visualize K-means","f0bf099c":"- 4 clusters are identified by DBSCAN algorithm. The result is plotted in terms of pc1 and pc2. Black points in the graph represent the outliers.","7a4db67f":"## Comparison of K-Means and DBSCAN","82462d01":"## DBSCAN","ba6a49ca":"- Both K-means and DBSCAN(with optimal epsilon and min_sample parameter) algorithms give the k = 4 as optimal number of cluster.\n- Quality of clustering measured by silhouette score approach which determine how well each object lies within its cluster, high average silhouette width implies good clustering. K-means algorithm has a 0.6718 silhouette score and DBSCAN algorithm has 0.6695 silhouette score.They are nearly same but K-means perform better. \n\n- The cluster which is in the middle of the graph seem more connected than the others. Other cluster has varying density. Since there are local outliers and there is a slight variation in terms of density on the clusters, with epsilon =3.5 and min_samples =20 DBSCAN can identify 4 clusters but K-means outperform.","9d16aa0d":"# Application of K-Means Clustering and DBSCAN \n\nThis case study is compare two popular clustering algorithms, K-means and DBSCAN. \n\nPrinciple Component Analysis is applied to visualize the data.\n\nThe clustering results are evaluated, the algorithms are optimized and the clusters are visualized. \n\n**Note:** The dataset is already in standardized form.","1405fc66":"- Compactness of clustering is measured by total within-cluster sum of square, it should be as small as possible.\n\n- From the within-cluster sum of square (WSS) graph, it can be said that the optimal number of clusters is 4 because the bend in the knee occurs there. ","76634679":"* PCA is used to visualize the data.\n* From the Explanied Variance Ratio graph, the first two principle component can explain 97.5% of the all variance. Also, there is an elbow after the second principle component, so pc1 and pc2 are enough for representing this data. ","02af40ac":"- Quality of clustering measured by silhouette score approach which determine how well each object lies within its cluster, high average silhouette width implies good clustering.\n\nWhen number of cluster is 4, average silhouette score is the highest, 0.67. From the both within-cluster sum of sqaure graph and average silhoutte approach, it can be said that optimal number of cluster is 4.  ","40510228":"* The dataset does not contain missing values.","b28f0f60":"- 4 clusters are identified by K-means algorithm. The result is plotted in terms of pc1 and pc2."}}