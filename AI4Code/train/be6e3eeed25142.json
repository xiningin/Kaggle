{"cell_type":{"492df2c0":"code","4d12f672":"code","d013103d":"code","4bd2ac5b":"code","b665b00d":"code","3d42712d":"code","db626100":"code","574b1e63":"code","e4589704":"code","34867bb0":"code","df112353":"code","207dbc0e":"code","8f875af5":"code","f9d8f295":"code","60cd9fb4":"code","0ec784e1":"code","bb0d0c82":"markdown","3ecea9ad":"markdown","c0d63fd4":"markdown","c5bf0d34":"markdown","e86d6c7e":"markdown","7f729798":"markdown","235b5f54":"markdown","84c4c0f3":"markdown","05cae8b3":"markdown","09db369a":"markdown","e1059fb3":"markdown","bd2979bb":"markdown","75f91e05":"markdown"},"source":{"492df2c0":"import os\nfrom functools import partial\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","4d12f672":"data = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ndata.head()","d013103d":"data.isnull().sum()","4bd2ac5b":"data.dropna(inplace=True)\ndata.isnull().sum()","b665b00d":"real_news = data[data[\"label\"]==0].values\nfake_news = data[data[\"label\"]==1].values\nplt.bar(0,height=len(real_news))\nplt.bar(1,height=len(fake_news))\nplt.xticks([0,1],[\"Reliable News\",\"Unreliable News\"])\nplt.ylabel(\"Counts\")\nplt.show()","3d42712d":"class MyDataset(Dataset):\n    def __init__(self, ds):\n        super(MyDataset,self).__init__()\n        self.labels = ds[\"label\"].values\n        self.data = ds[\"text\"].values\n\n    def __getitem__(self, idx):\n        # load data and labels\n        labels = self.labels[idx]\n        data = self.data[idx]\n        return labels, data\n\n    def __len__(self):\n        return len(self.data)","db626100":"ds = MyDataset(data)\nlabel, text = next(iter(ds))\nlabel, text[:50]","574b1e63":"tokenizer = get_tokenizer('basic_english')\ndef yield_tokens(data_iter,tokenizer):\n    for _,text in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(ds,tokenizer))","e4589704":"def text_pipeline(text,vocab,tokenizer):\n    tokens = tokenizer(text)\n    vocab_list = []\n    for t in tokens:\n        vocab_list.append(vocab[t])\n    return vocab_list\n\ntext_pipeline = partial(text_pipeline,vocab=vocab,tokenizer=tokenizer)","34867bb0":"_, text = next(iter(ds))\ntext_pipeline(text)[:5]","df112353":"ds_iter = iter(ds)\nall_ds_lengths = []\nfor d in ds_iter:\n    _, text = d\n    all_ds_lengths.append(len(text_pipeline(text)))\n\nplt.hist(all_ds_lengths,100)\nplt.xlabel(\"Number of Words in Sequence\")\nplt.ylabel(\"Counts\")\nplt.show()","207dbc0e":"def collate_batch(batch,text_pipeline,max_seq_len,device):\n    data = torch.zeros((len(batch),max_seq_len),dtype=torch.int64)\n    labels = torch.zeros((len(batch)),dtype=torch.int64)\n    for index,data_tuple in enumerate(batch):\n        processed_text = torch.tensor(text_pipeline(data_tuple[1]), dtype=torch.int64)\n        data[index,:len(processed_text)] = processed_text[:max_seq_len]\n        labels[index] = data_tuple[0]\n    return labels.to(device), data.to(device)\n\nmax_seq_len = 1000\ncol_fn = partial(collate_batch,text_pipeline=text_pipeline,max_seq_len=max_seq_len,device=device)","8f875af5":"class NewsPrediction(torch.nn.Module):\n    def __init__(self,vocab_length,embedding_dim,seq_len,num_layers,num_hidden):\n        super(NewsPrediction,self).__init__()\n        self.embedding = nn.Embedding(vocab_length,embedding_dim,max_norm=True)\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=num_hidden,num_layers=num_layers,batch_first=True)\n        self.fc = nn.Linear(seq_len*num_hidden,2)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        x = torch.reshape(x, (x.size(0),-1,))\n        x = self.fc(x)\n        return F.log_softmax(x,dim=-1)","f9d8f295":"batch_size = 512\nmodel = NewsPrediction(len(vocab),embedding_dim=32,seq_len=max_seq_len,num_layers=1,num_hidden=32)\nmodel.to(device)\ndl = DataLoader(ds,batch_size=batch_size,shuffle=True,num_workers=0,collate_fn=col_fn)","60cd9fb4":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(params=model.parameters())\n\nnum_train = int(len(dl)*0.8)\nnum_valid = len(dl) - num_train\ntraining_data, validation_data = random_split(dl, [num_train,num_valid])\ndatasets = {\"Training\":training_data.dataset, \"Validation\":validation_data.dataset}","0ec784e1":"num_epoch = 10\nweights_path = r\"..\/input\/fake-news\/model_weights.pt\"\nval_losses = [np.inf]\nno_improvement = 0\nfor epoch in range(num_epoch):\n    for d in datasets:\n        if d == \"Training\":\n            model.train(True)\n        else:\n            model.train(False)\n        dataset = datasets[d]\n        total_pts = 0\n        running_loss, running_acc = 0.0, 0.0\n        for i, sample in enumerate(dataset):\n            labels,data = sample\n            optimizer.zero_grad()\n            out = model(data)\n            _, pred = torch.max(out, 1)\n            num_correct = (pred == labels).sum()\n            loss = criterion(out,labels)\n            if d == \"Training\":\n                loss.backward()\n                optimizer.step()\n            running_loss += loss.item()\n            running_acc  += num_correct.data.item()\n            total_pts += len(sample[0])\n\n        print(\"Epoch {}, {} Loss: {}, Accuracy: {}\".format(epoch + 1, d, running_loss \/ i, running_acc \/ total_pts * 100))\n        if d == \"Validation\":\n            val_loss = running_loss \/ i\n            if all(val_losses < np.array(val_loss)):\n                torch.save(model.state_dict(),weights_path)\n                no_improvement = 0\n            else:\n                no_improvement += 1\n            val_losses.append(val_loss)\n            if no_improvement == 3:\n                break","bb0d0c82":"##### Set the optimization algorithm, loss function, and split the dataset into training and validation","3ecea9ad":"##### First, let's view the data","c0d63fd4":"## Fake News Detection\n\nThis kernel will walk through basic usage of an LSTM neural network in order to perform fake news detection.","c5bf0d34":"##### The data should be checked for a distribution of the sequence lengths","e86d6c7e":"### The maximum sequence length could be set at 2,500 words without affecting too much of the data.\n\nAn argument could also be given about how the gist of an article could be resolved in fewer than 2,500 words. A transformation function needs to be written to implement this.","7f729798":"##### Build the vocabulary list.","235b5f54":"##### Initialize the model and create an iterable dataset","84c4c0f3":"##### Print the label and the first 50 words for the first entry in the dataset.","05cae8b3":"#### Test the text pipeline\nA list\/array of words can be fed into the text pipeline to return the index of that word in the vocab list. Words not contained in the vocabulary list return an index of 0.","09db369a":"##### Drop the missing values and check the dataframe again.","e1059fb3":"### Build a Pytorch Dataset.\n\nThe data has several fields that could be used for prediction. However, since titles often exaggerate the truth to catch the reader's attention, only the text of the news article will be utilized. ","bd2979bb":"##### The data needs to be checked for missing values.","75f91e05":"#### Create the LSTM model\nThe embedding layer turns the vectorized text into dense vectors where related words will have a higher similarity to each other. The similarity between words becomes a useful feature vector, and helps the neural network make predictions.\n\nThe LSTM (Long Short-Term Memory) is a special type of RNN (Recurrent Neural Network) that circumvents the \"vanishing\" or \"exploding\" gradient problem seen by RNNs.\n\nFinally the fully-connected layer outputs values which are used to predict the truth of the text read by the neural network."}}