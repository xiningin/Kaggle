{"cell_type":{"35215133":"code","8ad6dbac":"code","2f48ebb2":"code","da37c16d":"code","8cfddd23":"code","0c578cac":"code","f56074cd":"code","3fead137":"code","7bd579e9":"code","bba464bb":"code","f0f7e5c5":"markdown","ab590d79":"markdown","5b297d97":"markdown","bcc38442":"markdown","05adb616":"markdown","b7db902e":"markdown"},"source":{"35215133":"import os\nimport tensorflow as tf\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nimport matplotlib.pyplot as plt","8ad6dbac":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n\nprint(\"train.shape:\",train.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity\n\nX_train = train.copy()\ndel train\n\n#Fill NaNs\nX_train.fillna(-999, inplace=True)\n\n# Label Encoding\nprint('label encoding')\nfor f in X_train.columns:\n    if X_train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        \n# Normalize\ndef apply_norm(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    np_scaled = min_max_scaler.fit_transform(df)\n    df_norm = pd.DataFrame(np_scaled, columns=df.columns)\n    return df_norm\n    \nprint('normalizing')\nX_train_norm = apply_norm(X_train)\n# Get a validation set\nX_train_norm, X_val_norm = train_test_split(X_train_norm, test_size=0.1)\ndel X_train\n\n# Drop target\nX_train_norm = X_train_norm.drop('isFraud', axis=1)\n\nprint(\"last shapes\")\nprint(\"X_train_norm.shape:\",X_train_norm.shape)\nprint(\"X_val_norm.shape:\",X_val_norm.shape)","2f48ebb2":"print(\"isFraud==0 in validation set:\",len(X_val_norm[X_val_norm.isFraud == 0]))\nprint(\"isFraud==1 in validation set:\",len(X_val_norm[X_val_norm.isFraud == 1]))","da37c16d":"X_val_norm.describe()","8cfddd23":"# Xavier initializer\ndef xavier_init(fan_in, fan_out, constant=1): \n    \"\"\" Xavier initialization of network weights\"\"\"\n    # https:\/\/stackoverflow.com\/questions\/33640581\/how-to-do-xavier-initialization-on-tensorflow\n    low = -constant*np.sqrt(6.0\/(fan_in + fan_out)) \n    high = constant*np.sqrt(6.0\/(fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out), \n                             minval=low, maxval=high, \n                             dtype=tf.float32)\n\n### Definitions and things\nclass VariationalAutoencoder(object):\n    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n    \n    This implementation uses probabilistic encoders and decoders using Gaussian \n    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n    end-to-end.\n    \n    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n    \"\"\"\n    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n                 learning_rate=0.001, batch_size=100):\n        self.network_architecture = network_architecture\n        self.transfer_fct = transfer_fct\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        \n        # tf Graph input\n        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n        \n        # Create autoencoder network\n        self._create_network()\n        # Define loss function based variational upper-bound and \n        # corresponding optimizer\n        self._create_loss_optimizer()\n        \n        # Initializing the tensor flow variables\n        init = tf.global_variables_initializer()\n\n        # Launch the session\n        self.sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n        self.sess.run(init)\n    \n    def _create_network(self):\n        # Initialize autoencode network weights and biases\n        network_weights = self._initialize_weights(**self.network_architecture)\n\n        # Use recognition network to determine mean and \n        # (log) variance of Gaussian distribution in latent\n        # space\n        self.z_mean, self.z_log_sigma_sq = \\\n            self._recognition_network(network_weights[\"weights_recog\"], \n                                      network_weights[\"biases_recog\"])\n\n        # Draw one sample z from Gaussian distribution\n        n_z = self.network_architecture[\"n_z\"]\n        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n                               dtype=tf.float32)\n        # z = mu + sigma*epsilon\n        self.z = tf.add(self.z_mean, \n                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n\n        # Use generator to determine mean of\n        # Bernoulli distribution of reconstructed input\n        self.x_reconstr_mean = \\\n            self._generator_network(network_weights[\"weights_gener\"],\n                                    network_weights[\"biases_gener\"])\n            \n    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n                            n_hidden_gener_1,  n_hidden_gener_2, \n                            n_input, n_z):\n        all_weights = dict()\n        all_weights['weights_recog'] = {\n            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n        all_weights['biases_recog'] = {\n            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n        all_weights['weights_gener'] = {\n            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n        all_weights['biases_gener'] = {\n            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n        return all_weights\n            \n    def _recognition_network(self, weights, biases):\n        # Generate probabilistic encoder (recognition network), which\n        # maps inputs onto a normal distribution in latent space.\n        # The transformation is parametrized and can be learned.\n        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n                                           biases['b1'])) \n        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n                                           biases['b2'])) \n        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n                        biases['out_mean'])\n        z_log_sigma_sq = \\\n            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n                   biases['out_log_sigma'])\n        return (z_mean, z_log_sigma_sq)\n\n    def _generator_network(self, weights, biases):\n        # Generate probabilistic decoder (decoder network), which\n        # maps points in latent space onto a Bernoulli distribution in data space.\n        # The transformation is parametrized and can be learned.\n        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n                                           biases['b1'])) \n        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n                                           biases['b2'])) \n        x_reconstr_mean = \\\n            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n                                 biases['out_mean']))\n        return x_reconstr_mean\n            \n    def _create_loss_optimizer(self):\n        # The loss is composed of two terms:\n        # 1.) The reconstruction loss (the negative log probability\n        #     of the input under the reconstructed Bernoulli distribution \n        #     induced by the decoder in the data space).\n        #     This can be interpreted as the number of \"nats\" required\n        #     for reconstructing the input when the activation in latent\n        #     is given.\n        # Adding 1e-10 to avoid evaluation of log(0.0)\n        reconstr_loss = \\\n            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n                           1)\n        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n        ##    between the distribution in latent space induced by the encoder on \n        #     the data and some prior. This acts as a kind of regularizer.\n        #     This can be interpreted as the number of \"nats\" required\n        #     for transmitting the the latent space distribution given\n        #     the prior.\n        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n                                           - tf.square(self.z_mean) \n                                           - tf.exp(self.z_log_sigma_sq), 1)\n        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n        # Use ADAM optimizer\n        self.optimizer = \\\n            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n        \n    def partial_fit(self, X):\n        \"\"\"Train model based on mini-batch of input data.\n        \n        Return cost of mini-batch.\n        \"\"\"\n        opt, cost = self.sess.run((self.optimizer, self.cost), \n                                  feed_dict={self.x: X})\n        return cost\n    \n    def transform(self, X):\n        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n        # Note: This maps to mean of distribution, we could alternatively\n        # sample from Gaussian distribution\n        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n    \n    def generate(self, z_mu=None):\n        \"\"\" Generate data by sampling from latent space.\n        \n        If z_mu is not None, data for this point in latent space is\n        generated. Otherwise, z_mu is drawn from prior in latent \n        space.        \n        \"\"\"\n        if z_mu is None:\n            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n        # Note: This maps to mean of distribution, we could alternatively\n        # sample from Gaussian distribution\n        return self.sess.run(self.x_reconstr_mean, \n                             feed_dict={self.z: z_mu})\n    \n    def reconstruct(self, X):\n        \"\"\" Use VAE to reconstruct given data. \"\"\"\n        return self.sess.run(self.x_reconstr_mean, \n                             feed_dict={self.x: X})\n    \n### Training function\ndef train(dataset, network_architecture, learning_rate=0.001,\n          batch_size=1000, training_epochs=10, display_step=5):\n    vae = VariationalAutoencoder(network_architecture, \n                                 learning_rate=learning_rate, \n                                 batch_size=batch_size)\n    n_samples = dataset.train_num_examples() \n    \n    # Training cycle\n    for epoch in range(training_epochs):\n        time_start = time.time()\n        \n        avg_cost = 0.\n        total_batch = int(n_samples \/ batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs = dataset.train_next_batch(batch_size) \n            \n            # Fit training using batch data\n            cost = vae.partial_fit(batch_xs)\n            # Compute average loss\n            avg_cost += cost \/ n_samples * batch_size\n            \n        # Display logs per epoch step\n        time_end = time.time()\n        \n        if epoch % display_step == 0:\n            print(\"Epoch:\", '%04d ends -- ' % (epoch+1), \n                  \"avg_cost=\", \"{:.9f}\".format(avg_cost), \" time:{:.2f} s\".format(time_end-time_start))\n    return vae\n\n\n### Dataset wrapper\nclass Dataset:\n    def __init__(self, X_train, X_test, random_state=None):\n        self.x_train = X_train\n        self.x_test = X_test\n        self.random_state = random_state\n      \n    def train_num_examples(self):\n        return len(self.x_train)\n\n    def train_next_batch(self,batch_size):\n        batch = self.x_train.sample(n=batch_size, replace=False, random_state = self.random_state)\n        return batch\n    \n    def test_next_batch(self,batch_size):\n        batch = self.x_test.sample(n=batch_size, replace=False, random_state = self.random_state)\n        return batch","0c578cac":"dataset = Dataset(X_train_norm, X_val_norm, random_state=None)\n\nnetwork_architecture = \\\n    dict(n_input=432,  # input dimension\n         n_hidden_recog_1=216, # 1st layer encoder neurons\n         n_hidden_recog_2=108, # 2nd layer encoder neurons\n         n_z=54, # dimensionality of latent space\n         n_hidden_gener_1=108, # 1st layer decoder neurons\n         n_hidden_gener_2=216, # 2nd layer decoder neurons\n        )  \n\nvae = train(dataset, network_architecture, \n            training_epochs=70, learning_rate=5*10e-8,\n            batch_size=1000,\n            display_step=1)","f56074cd":"x_val_notFraud = X_val_norm[X_val_norm.isFraud == 0]\nx_val_fraud = X_val_norm[X_val_norm.isFraud == 1]\n\n# Drop the label column\nx_val_notFraud = x_val_notFraud.drop('isFraud', axis=1)\nx_val_fraud = x_val_fraud.drop('isFraud', axis=1)\n\nprint(len(x_val_notFraud))\nprint(len(x_val_fraud))","3fead137":"def vis_reconstruct(df):\n    np_arr = df.values\n    x_reconstruct = vae.reconstruct(np_arr)\n\n    plt.figure(figsize=(8, 12))\n    for i in range(5):\n        plt.subplot(5, 2, 2*i + 1)\n        plt.imshow(np_arr[i].reshape(18, 24), cmap=\"viridis\")\n        plt.title(\"Test input\")\n        plt.colorbar()\n        plt.subplot(5, 2, 2*i + 2)\n        plt.imshow(x_reconstruct[i].reshape(18, 24), cmap=\"viridis\")\n        plt.title(\"Reconstruction\")\n        plt.colorbar()\n    plt.tight_layout()","7bd579e9":"## Reconstruct isFraud == 0\nvis_reconstruct(x_val_notFraud.head(1000))\n","bba464bb":"## Reconstruct isFraud == 1\nvis_reconstruct(x_val_fraud.head(1000))\n","f0f7e5c5":"### Try to reconstruct data","ab590d79":"# Preprocessing\n\nAdditional to xhulu's work, I've added the min-max normalization. It is required for the cost function of the VAE. Also, I am using the train data, with a training-validation split.","5b297d97":"# About \n\n### -- press f to pay respects --\nForked from the kernel: https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s\n\nVAE code is from here: https:\/\/jmetzen.github.io\/2015-11-27\/vae.html\n","bcc38442":"# Variational autoencoder\n- Code below is taken from J.H. Metzen (and modified a little bit)\n\n\n","05adb616":"### Initialize and run\n","b7db902e":"### some notes:\n- when avg_cost gets as low as ~80, I get nan loss error"}}