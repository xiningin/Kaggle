{"cell_type":{"52e92834":"code","f0b33494":"code","4dca17df":"code","d0ff8182":"code","3030a982":"code","953c2094":"code","ddb153cc":"code","9a373003":"code","00558929":"code","2dd7ec13":"code","c78590bc":"code","eda85ea6":"code","9b9075b8":"code","28967f3b":"code","54312dd3":"code","fc779900":"code","abde1f26":"code","208faee6":"code","5dbd47b5":"code","c2153551":"code","8871d26a":"code","a00952ba":"code","7e60ae58":"code","d26c41ea":"code","e5f71381":"code","1cd4aa4c":"code","7c224fb5":"code","f3057b04":"code","3a41842d":"code","35f7e07d":"code","7aa3d8ce":"code","ac94c1ae":"code","11200ab1":"code","885d7592":"code","3b384c64":"code","8bd39231":"code","91e2a3c1":"code","62e019e5":"code","aa9f045f":"code","a6df0f1a":"code","a05a02d6":"code","f4867957":"code","90705425":"code","e8fd18df":"code","4ca40768":"code","575b6172":"code","df7f1d40":"code","37a5c2dd":"code","5553e82c":"code","fe861074":"code","48275a39":"code","558a0c3d":"code","eb5f44c2":"code","79e22456":"code","f7ee76b1":"markdown","0c32b203":"markdown","2fddbcae":"markdown","2099ba56":"markdown","7ec15256":"markdown","95ff8701":"markdown","2a62ec82":"markdown","6ff93b2a":"markdown","c30be9e2":"markdown","09201be1":"markdown","a3fbe966":"markdown","ab171094":"markdown","40a48ea0":"markdown","f88c2ab5":"markdown","887059b3":"markdown","641b6ee3":"markdown","c63229d6":"markdown","202634e5":"markdown","fda6613e":"markdown","d201ad00":"markdown","444bcbc5":"markdown","93de869f":"markdown","42eeb2de":"markdown","6681a4c6":"markdown","365b997c":"markdown","f43538c5":"markdown","b0069df2":"markdown","31e2c83c":"markdown","632778cc":"markdown","1c1be17b":"markdown","3b96a3c7":"markdown","a1f456a8":"markdown","e4fd32b7":"markdown","5e4f6e5c":"markdown","7845ac2e":"markdown","0e28d3e9":"markdown","3a92af77":"markdown","9fc41eed":"markdown","7f6476e8":"markdown","fe3eacce":"markdown","2b953c61":"markdown","cfc6b7f1":"markdown","e039cf52":"markdown","7a86f632":"markdown","84e04987":"markdown","c3a7e5fb":"markdown"},"source":{"52e92834":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","f0b33494":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/careercon\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","4dca17df":"%%time\nX_train = pd.read_csv(os.path.join(PATH, 'X_train.csv'))\nX_test = pd.read_csv(os.path.join(PATH, 'X_test.csv'))\ny_train = pd.read_csv(os.path.join(PATH, 'y_train.csv'))","d0ff8182":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","3030a982":"X_train.head()","953c2094":"y_train.head()","ddb153cc":"X_test.head()","9a373003":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","00558929":"missing_data(X_train)","2dd7ec13":"missing_data(X_test)","c78590bc":"missing_data(y_train)","eda85ea6":"X_train.describe()","9b9075b8":"X_test.describe()","28967f3b":"y_train.describe()","54312dd3":"f, ax = plt.subplots(1,1, figsize=(16,4))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['surface'], order = y_train['surface'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of labels for each class\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(100*height\/total),\n            ha=\"center\") \nplt.show()    ","fc779900":"f, ax = plt.subplots(1,1, figsize=(18,8))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['group_id'], order = y_train['group_id'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of group_id\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format(100*height\/total),\n            ha=\"center\", rotation='90') \nplt.show()    ","abde1f26":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.distplot(df1[feature], hist=False, label=label1)\n        sns.distplot(df2[feature], hist=False, label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","208faee6":"features = X_train.columns.values[3:13]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","5dbd47b5":"def plot_feature_class_distribution(classes,tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.distplot(ttc[feature], hist=False,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","c2153551":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","8871d26a":"fig, ax = plt.subplots(1,1,figsize=(24,6))\ntmp = pd.DataFrame(y_train.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\nm = tmp.pivot(index='surface', columns='group_id', values='series_id')\ns = sns.heatmap(m, linewidths=.1, linecolor='black', annot=True, cmap=\"YlGnBu\")\ns.set_title('Number of surface category per group_id', size=16)\nplt.show()","a00952ba":"f,ax = plt.subplots(figsize=(6,6))\nm = X_train.iloc[:,3:].corr()\nsns.heatmap(m, annot=True, linecolor='darkblue', linewidths=.1, cmap=\"YlGnBu\", fmt= '.1f',ax=ax)","7e60ae58":"f,ax = plt.subplots(figsize=(6,6))\nm = X_test.iloc[:,3:].corr()\nsns.heatmap(m, annot=True, linecolor='darkblue', linewidths=.1, cmap=\"YlGnBu\", fmt= '.1f',ax=ax)","d26c41ea":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","e5f71381":"def perform_euler_factors_calculation(df):\n    df['total_angular_velocity'] = np.sqrt(np.square(df['angular_velocity_X']) + np.square(df['angular_velocity_Y']) + np.square(df['angular_velocity_Z']))\n    df['total_linear_acceleration'] = np.sqrt(np.square(df['linear_acceleration_X']) + np.square(df['linear_acceleration_Y']) + np.square(df['linear_acceleration_Z']))\n    df['total_xyz'] = np.sqrt(np.square(df['orientation_X']) + np.square(df['orientation_Y']) +\n                              np.square(df['orientation_Z']))\n    df['acc_vs_vel'] = df['total_linear_acceleration'] \/ df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angle'] = np.sqrt(np.square(df['euler_x']) + np.square(df['euler_y']) + np.square(df['euler_z']))\n    df['angle_vs_acc'] = df['total_angle'] \/ df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] \/ df['total_angular_velocity']\n    return df","1cd4aa4c":"def perform_feature_engineering(df):\n    df_out = pd.DataFrame()\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    def mean_abs_change(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in df.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        df_out[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        df_out[col + '_min'] = df.groupby(['series_id'])[col].min()\n        df_out[col + '_max'] = df.groupby(['series_id'])[col].max()\n        df_out[col + '_std'] = df.groupby(['series_id'])[col].std()\n        df_out[col + '_mad'] = df.groupby(['series_id'])[col].mad()\n        df_out[col + '_med'] = df.groupby(['series_id'])[col].median()\n        df_out[col + '_skew'] = df.groupby(['series_id'])[col].skew()\n        df_out[col + '_range'] = df_out[col + '_max'] - df_out[col + '_min']\n        df_out[col + '_max_to_min'] = df_out[col + '_max'] \/ df_out[col + '_min']\n        df_out[col + '_mean_abs_change'] = df.groupby('series_id')[col].apply(mean_abs_change)\n        df_out[col + '_mean_change_of_abs_change'] = df.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df_out[col + '_abs_max'] = df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        df_out[col + '_abs_min'] = df.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        df_out[col + '_abs_mean'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(x)))\n        df_out[col + '_abs_std'] = df.groupby('series_id')[col].apply(lambda x: np.std(np.abs(x)))\n        df_out[col + '_abs_avg'] = (df_out[col + '_abs_min'] + df_out[col + '_abs_max'])\/2\n        df_out[col + '_abs_range'] = df_out[col + '_abs_max'] - df_out[col + '_abs_min']\n\n    return df_out","7c224fb5":"%%time\nX_train = perform_euler_factors_calculation(X_train)","f3057b04":"%%time\nX_test = perform_euler_factors_calculation(X_test)","3a41842d":"X_train.shape, X_test.shape","35f7e07d":"features = X_train.columns.values[13:23]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)","7aa3d8ce":"classes = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","ac94c1ae":"USE_ALL_FEATURES = False\nif(USE_ALL_FEATURES):\n    features = X_train.columns.values\nelse:\n    features = X_train.columns.values[:13]","11200ab1":"%%time\nX_train = perform_feature_engineering(X_train[features])","885d7592":"%%time\nX_test = perform_feature_engineering(X_test[features])","3b384c64":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","8bd39231":"X_train.head()","91e2a3c1":"X_test.head()","62e019e5":"%%time\ncorrelations = X_train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]","aa9f045f":"correlations.head(10)","a6df0f1a":"correlations.tail(10)","a05a02d6":"n_top_corr = correlations[correlations[0]==1.0].shape[0]\nprint(\"There are {} different features pairs with correlation factor 1.0.\".format(n_top_corr))","f4867957":"drop_features = list(correlations.head(n_top_corr)['level_0'].unique())\nX_train = X_train.drop(drop_features,axis=1)\nX_test = X_test.drop(drop_features,axis=1)","90705425":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","e8fd18df":"corr = X_train.corr()\nfig, ax = plt.subplots(1,1,figsize=(16,16))\nsns.heatmap(corr,  xticklabels=False, yticklabels=False)\nplt.show()","4ca40768":"le = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","575b6172":"X_train.fillna(0, inplace = True)\nX_train.replace(-np.inf, 0, inplace = True)\nX_train.replace(np.inf, 0, inplace = True)\nX_test.fillna(0, inplace = True)\nX_test.replace(-np.inf, 0, inplace = True)\nX_test.replace(np.inf, 0, inplace = True)","df7f1d40":"scaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))\nprint (\"Scaled !\")","37a5c2dd":"folds = StratifiedKFold(n_splits=49, shuffle=True, random_state=2018)","5553e82c":"sub_preds_rf = np.zeros((X_test_scaled.shape[0], 9))\noof_preds_rf = np.zeros((X_train_scaled.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled, y_train['surface'])):\n    clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf.fit(X_train_scaled.iloc[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_rf[val_idx] = clf.predict(X_train_scaled.iloc[val_idx])\n    sub_preds_rf += clf.predict_proba(X_test_scaled) \/ folds.n_splits\n    score += clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_,clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","fe861074":"def plot_confusion_matrix(actual, predicted, classes, title='Confusion Matrix'):\n    conf_matrix = confusion_matrix(actual, predicted)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Greens)\n    plt.title(title, size=12)\n    plt.colorbar(fraction=0.05, pad=0.05)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    thresh = conf_matrix.max() \/ 2.\n    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n        horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","48275a39":"plot_confusion_matrix(y_train['surface'], oof_preds_rf, le.classes_, title='Confusion Matrix')","558a0c3d":"USE_LGB = True\nif(USE_LGB):\n    sub_preds_lgb = np.zeros((X_test.shape[0], 9))\n    oof_preds_lgb = np.zeros((X_train.shape[0]))\n    score = 0\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train['surface'])):\n        train_x, train_y = X_train.iloc[trn_idx], y_train['surface'][trn_idx]\n        valid_x, valid_y = X_train.iloc[val_idx], y_train['surface'][val_idx]\n        clf =  LGBMClassifier(\n                      nthread=-1,\n                      n_estimators=2000,\n                      learning_rate=0.01,\n                      boosting_type='gbdt',\n                      is_unbalance=True,\n                      objective='multiclass',\n                      numclass=9,\n                      silent=-1,\n                      verbose=-1,\n                      feval=None)\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                     verbose= 1000, early_stopping_rounds= 200)\n\n        oof_preds_lgb[val_idx] = clf.predict(valid_x)\n        sub_preds_lgb += clf.predict_proba(X_test) \/ folds.n_splits\n        score += clf.score(valid_x, valid_y)\n        print('Fold: {} score: {}'.format(fold_,clf.score(valid_x, valid_y)))\n    print('Avg Accuracy', score \/ folds.n_splits)","eb5f44c2":"submission = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))\nsubmission['surface'] = le.inverse_transform(sub_preds_rf.argmax(axis=1))\nsubmission.to_csv('submission_rf.csv', index=False)\nsubmission.head(10)","79e22456":"USE_LGB = True\nif(USE_LGB):\n    submission['surface'] = le.inverse_transform(sub_preds_lgb.argmax(axis=1))\n    submission.to_csv('submission_lgb.csv', index=False)\n    submission.head(10)","f7ee76b1":"\n## LightGBM Classifier\n\nWe also use a LightGBM Classifier model.","0c32b203":"## Random Forest classifier\n\nWe use first a Random Forest Classifier model.","2fddbcae":"![](https:\/\/upload.wikimedia.org\/wikipedia\/en\/3\/39\/R2-D2_Droid.png)","2099ba56":"We calculate euler factors and several addtional features starting from the original features.","7ec15256":"We eliminate the features that have a correlation factor 1.0 with other features.   \n","95ff8701":"# <a id='3'>Data exploration<\/a>  \n\n## <a id='31'>Check the data<\/a>  \n\nLet's check the train and test set.\n\nWe start with the train.","2a62ec82":"And let's see now the most correlated features. We show only the first 10, then we print the total number of them.","6ff93b2a":"## <a id='34'>Density plots of features<\/a>  \n\nLet's show now the density plot of variables in train and test dataset. \n\nWe represent with different colors the distribution for values with different values of **surface**.\n\nWe introduce two utility functions for plotting.","c30be9e2":"# <a id='5'>Model<\/a>  \n\nWe use LabelEncoder for the target feature.","09201be1":"We define the routine for feature engineering.","a3fbe966":"# <a id='7'>References<\/a>    \n\n[1] https:\/\/www.kaggle.com\/vanshjatana\/help-humanity-by-helping-robots-4e306b  \n[2] https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\n[3] https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive  \n[4] https:\/\/www.kaggle.com\/hsinwenchang\/randomforestclassifier  \n[5] https:\/\/en.wikipedia.org\/wiki\/Quaternion\n","ab171094":"# <a id='6'>Submission<\/a>  \n\nWe submit the solution for both the RF and LGB.","40a48ea0":"Very strong correlation (1.0) is between **orientation_X** and **orientation_W** and between **orientation_Z** and **orientation_Y**.   \nThere is a strong inverse correlation (-0.8) between **angular_velocity_Z** and **angular_velocity_Y**.    \nAlso, there is a medium positive correlation (0.4) between **linear_acceleration_Y** and **linear_acceleration_Z**.  \n\nLet's also check the features correlation for test set.\n","f88c2ab5":"There are no missing values in train and test data.  \nLet's check also train labels.","887059b3":"## <a id='35'>Target feature - surface and group_id distribution<\/a>  \n\nLet's show now the distribution of target feature - surface and group_id.","641b6ee3":"## Prepare for cross-validation","c63229d6":"We follow with the test.","202634e5":"We scale the train and test data.","fda6613e":"Let's check the confusion matrix.\n\nWe will use a simplifed version of the plot function defined here: https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive","d201ad00":"\nLet's show again the shape of train and test.","444bcbc5":"Also, train labels has no missing data.\n\nLet's check now the data distribution using *describe*.","93de869f":"# <a id='4'>Features engineering<\/a>  \n","42eeb2de":"This section is heavily borrowing from: https:\/\/www.kaggle.com\/vanshjatana\/help-humanity-by-helping-robots Kernel. \nThe quaternion_to_euler transformation procedure is also credited in the original Kernel, and I kept this reference as well.\nI also corrected few issues and added some more engineered features. Thanks for @timmmmmms for pointing them out.","6681a4c6":"Let's load the data.","365b997c":"Missing data in the train set.","f43538c5":"Very strong correlation (1.0) is between **orientation_X** and **orientation_W** and between **orientation_Z** and **orientation_Y**.   \nThere is a strong inverse correlation (-0.8) between **angular_velocity_Z** and **angular_velocity_Y**.    \nAlso, there is a medium positive correlation (0.4) between **linear_acceleration_Y** and **linear_acceleration_Z**.  ","b0069df2":"## Load data   \n\nLet's check what data files are available.","31e2c83c":"![](https:\/\/upload.wikimedia.org\/wikipedia\/en\/3\/39\/BB-8%2C_Star_Wars_The_Force_Awakens.jpg)","632778cc":"## Features correlation\n\n\nLet's look now to the new features correlation for train set.","1c1be17b":"Missing data in the test set.","3b96a3c7":"# <a id='36'>Features correlation<\/a>  \n\nLet's check the features correlation for train set.","a1f456a8":"X_train and X_test datasets have the following entries:  \n\n* series and measurements identifiers: **row_id**, **series_id**, **measurement_number**: these identify uniquely a series and measurement; there are 3809 series, each with max 127 measurements;  \n* measurement orientations: **orientation_X**, **orientation_Y**, **orientation_Z**, **orientation_W**;   \n* angular velocities: **angular_velocity_X**, **angular_velocity_Y**, **angular_velocity_Z**;\n* linear accelerations: **linear_acceleration_X**, **linear_acceleration_Y**, **linear_acceleration_Z**.\n\ny_train has the following columns:  \n\n* **series_id** - this corresponds to the series in train data;  \n* **group_id**;  \n* **surface** - this is the surface type that need to be predicted.  \n\n\nLet's check now for missing data.\n\n","e4fd32b7":"## Euler factors and additional features\n\nWe calculate the Euler factors and few additional features. First we calculate for train set.","5e4f6e5c":"We visualize the correlation matrix.","7845ac2e":"## Aggregated feature engineering\n\nWe apply now the feature engineering procedure for train and test. \nThe resulted features are calculated by aggregation of original features (and the features calculated in the previous step - optionally). ","0e28d3e9":"After feature engineering, the new shapes are:","3a92af77":"Then we calculate the same factors for test set.","9fc41eed":"We replace with 0 NAs and $\\infty$.","7f6476e8":"# <a id='2'>Prepare for data analysis<\/a>  \n\n\n## Load packages\n","fe3eacce":"<h1><center><font size=\"6\">Robots need help!<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/df\/RobotsMODO.jpg\" width=\"400\"><\/img>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check the data<\/a>   \n - <a href='#32'>Distribution of target feature - surface<\/a>   \n - <a href='#33'>Distribution of group_id<\/a>    \n - <a href='#34'>Density plots of features<\/a>   \n - <a href='#35'>Target feature - surface and group_id distribution<\/a>   \n - <a href='#36'>Features correlation<\/a>   \n- <a href='#4'>Feature engineering<\/a>\n- <a href='#5'>Model<\/a>\n- <a href='#6'>Submission<\/a>  \n- <a href='#7'>References<\/a>","2b953c61":"## <a id='33'>Distribution of group_id<\/a>  ","cfc6b7f1":"We can observe that train data and labels have different number of rows.","e039cf52":"## <a id='32'>Distribution of target feature - surface<\/a> \n","7a86f632":"# <a id='1'>Introduction<\/a>  \n\n## Competition\nIn this competition, we willl help robots recognize the floor surface they\u2019re standing on. The floor could be of various types, like carpet, tiles, concrete.\n\n## Data\nThe data provided by the organizers  is collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises.  \n\n## Kernel\nIn this Kernel we perform EDA on the data, explore with feature engineering and build two predictive models.\n\n","84e04987":"There is the same number of series in X_train and y_train, numbered from 0 to 3809 (total 3810). Each series have 128 measurements.   \nEach series in train dataset is part of a group (numbered from 0 to 72).  \nThe number of rows in X_train and X_test differs with 6 x 128, 128 being the number of measurements for each group.  ","c3a7e5fb":"Let's see the least correlated features."}}