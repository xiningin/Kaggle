{"cell_type":{"e21b9919":"code","2bf6f0f6":"code","579678ef":"code","c6affdf6":"code","febc0719":"code","d575bfad":"code","bdd5023e":"code","f4bf998f":"code","3efb2939":"code","11a97f58":"code","13d667dd":"code","f2e200f0":"code","f8a8318a":"code","ddc30c91":"code","af517793":"code","1489b379":"code","0c16e33e":"code","a68b9c31":"code","2fe56454":"code","978b881b":"code","b64ea148":"code","37826113":"code","5920b7d6":"code","66b4ba4e":"code","435d4df9":"code","328793c3":"code","0d605dda":"code","1bf59e14":"code","a8d50097":"code","d461d430":"code","5b521b88":"code","02e7a167":"code","93e4dacd":"code","4049d02f":"markdown","f4e8d991":"markdown","9d4b0cb1":"markdown","7214c98e":"markdown","8c533a9b":"markdown","b4311e5f":"markdown","48686a5d":"markdown","26e95647":"markdown","695dae30":"markdown","d6e193d4":"markdown","ca05fc6e":"markdown","f178e809":"markdown"},"source":{"e21b9919":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\n\n# Standard plotly imports\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\nimport os\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2bf6f0f6":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","579678ef":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb","c6affdf6":"print(\"XGBoost version:\", xgb.__version__)","febc0719":"print('# File sizes')\ntotal_size = 0\nstart_path = '..\/input\/jane-street-market-prediction'  # To get size of current directory\nfor path, dirs, files in os.walk(start_path):\n    for f in files:\n        fp = os.path.join(path, f)\n        total_size += os.path.getsize(fp)\nprint(\"Directory size: \" + str(round(total_size\/ 1000000, 2)) + 'MB')","d575bfad":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\nexample_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')\nprint (\"Data is loaded!\")","bdd5023e":"print('train shape is {}'.format(train.shape))\nprint('features shape is {}'.format(features.shape))\nprint('example_test shape is {}'.format(example_test.shape))\nprint('sample_prediction_df shape is {}'.format(sample_prediction_df.shape))","f4bf998f":"train.head()","3efb2939":"def reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df","11a97f58":"train = reduce_memory_usage(train)","13d667dd":"missing_values_count = train.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(train.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","f2e200f0":"# I have taked this cell from https:\/\/www.kaggle.com\/jazivxt\/the-market-is-reactive\n# And https:\/\/www.kaggle.com\/drcapa\/jane-street-market-prediction-starter-xgb\n\ntrain = train[train['weight'] != 0]\n\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\n\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = train.loc[:, 'action']","f8a8318a":"X_train = X_train.fillna(-999)","ddc30c91":"del train","af517793":"# The training part taked from here https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=10,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.7,\n    missing=-999,\n    random_state=2020,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)","1489b379":"%time clf.fit(X_train, y_train)","0c16e33e":"import shap\n\nshap.initjs()","a68b9c31":"%%time\n# compute the SHAP values for every prediction in the validation dataset\nexplainer = shap.TreeExplainer(clf)","2fe56454":"%%time\nX_sample = X_train.sample(10000)\nshap_values = explainer.shap_values(X_sample)","978b881b":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_sample.iloc[0,:])","b64ea148":"shap.summary_plot(shap_values, X_sample)","37826113":"# sort the features indexes by their importance in the model\n# (sum of SHAP value magnitudes over the validation dataset)\ntop_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n\n# make SHAP plots of the three most important features\nfor i in range(20):\n    shap.dependence_plot(top_inds[i], shap_values, X_sample)","5920b7d6":"features = features.set_index('feature')\nfeatures = features.T * 1\nfeatures.columns","66b4ba4e":"%%time\nimport lightgbm as lgb\nfrom sklearn import *\n\nk = cluster.KMeans(n_clusters=29, random_state=0).fit(features[['feature_' + str(i) for i in range(130)]])\nn = preprocessing.Normalizer()\nX_train['k'] = k.predict(n.fit_transform(X_train[['feature_' + str(i) for i in range(130)]].fillna(-999)))","435d4df9":"col = [c for c in X_train.columns if c not in ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id', 'date', 'action']]","328793c3":"params = {'objective':'binary', 'boosting': 'gbdt', 'learning_rate': 0.2, 'max_depth': -1, 'random_state': 20, 'device':'gpu'}\nx1, x2, y1, y2 = model_selection.train_test_split(X_train[col], y_train, test_size=0.3, random_state=20)","0d605dda":"del X_train, y_train","1bf59e14":"model = lgb.train(params, lgb.Dataset(x1, y1), 450,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=100)","a8d50097":"%%time\n# compute the SHAP values for every prediction in the validation dataset\nexplainer = shap.TreeExplainer(model)","d461d430":"%%time\nX_sample = x2.sample(10000)\nshap_values = explainer.shap_values(X_sample)","5b521b88":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:])","02e7a167":"shap.summary_plot(shap_values, X_sample)","93e4dacd":"# sort the features indexes by their importance in the model\n# (sum of SHAP value magnitudes over the validation dataset)\ntop_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n\n# make SHAP plots of the three most important features\nfor i in range(20):\n    shap.dependence_plot(top_inds[0][i], shap_values[0], X_sample)","4049d02f":"# Pre-processing before training","f4e8d991":"## Create Environment","9d4b0cb1":"### Missing Values Count","7214c98e":"Since computing the shap values for the entire set takes an *inordinately* long time, we will use only a small sample(of around ~10k).   ","8c533a9b":"## The next few blocks of code will take some time to run!","b4311e5f":"## Training\n##### To activate GPU usage, simply use tree_method='gpu_hist' (took me an hour to figure out, I wish XGBoost documentation was clearer about that).","48686a5d":"# Reducting memory usage by 75% \n\nSource: https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75","26e95647":"# LGB model","695dae30":"![](https:\/\/commons.wikimedia.org\/wiki\/File:Anne-nygard-lOcP_QZzitI-unsplash.jpg)","d6e193d4":"**Credits to the following notebooks**\n\n[Reduce Memory Usage by 75%](https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75)\n\n[Market Prediction: XGBoost with GPU (Fit in 1min)](https:\/\/www.kaggle.com\/hamditarek\/market-prediction-xgboost-with-gpu-fit-in-1min)\n\n[The market is reactive](https:\/\/www.kaggle.com\/jazivxt\/the-market-is-reactive)","ca05fc6e":"# Loading Shap library & **initjs()**","f178e809":"# SHAP values & model interpretability \n\nSHAP values explain the change in the expected model prediction based on the feature values. \n\n![](https:\/\/unsplash.com\/photos\/lOcP_QZzitI)\n\n> \n> SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations.\n> \n"}}