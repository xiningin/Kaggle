{"cell_type":{"eda442a6":"code","50fd6a88":"code","122a050d":"code","11b82c35":"code","0951b2c9":"code","7d72128e":"code","909da354":"code","721b2ef9":"code","af3a533b":"code","51fde111":"code","061bc34f":"code","104277f9":"code","45a05f03":"code","ef90d685":"code","0f0c05c0":"code","919b0bda":"code","51e7ff04":"code","5df75afd":"code","a15ee2c2":"code","90ef9404":"code","e25d6b6e":"code","803a4202":"markdown","990d545b":"markdown","f180d26d":"markdown","7a9859e8":"markdown","57a5be90":"markdown","92bd4a66":"markdown","7d52f0dc":"markdown","053bdfc4":"markdown","822ba6ba":"markdown","ef2b4142":"markdown"},"source":{"eda442a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Declaring a random state so the execution doesn't change\nrandom_state = 0","50fd6a88":"# Loading the data\nraw_data = pd.read_csv('\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\nraw_data.head()","122a050d":"# Statistics about the dataset\nraw_data.describe()","11b82c35":"# Let's check how much data is null\nraw_data.isnull().sum()","0951b2c9":"# Checking the unique values of the columns\nfor column in raw_data.columns:\n    print(column)\n    print(raw_data[column].unique())\n    print('\\n')","7d72128e":"# Get dummies\ndf = pd.get_dummies(raw_data, columns=['cp', 'restecg', 'slope', 'thal'])\ndf.head()","909da354":"# Splitting inputs and targets\nX_encoded = df.drop(['target'], axis=1)\ny = df['target']","721b2ef9":"# Splitting the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=random_state)","af3a533b":"# Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\n\n# Cross validation\ndt_accuracy = np.mean(cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy'))\nprint(\"Mean accuracy: \", dt_accuracy)\n\n# Plotting the tree\nplt.figure(figsize=(28, 18))\nplot_tree(dt, filled=True, rounded=True, class_names=['No HD', 'HD'], feature_names=X_encoded.columns);","51fde111":"# Confusion matrix\nplot_confusion_matrix(dt, X_test, y_test, display_labels=['Does not have HD', 'Has HD'])\nplt.grid(False)","061bc34f":"# Overall accuracy of the pruned tree\ndt.score(X_test, y_test)","104277f9":"# Determine the values for alpha\npath = dt.cost_complexity_pruning_path(X_train, y_train)\n# Extract the different values for alpha\nccp_alphas = path.ccp_alphas\n# Exclude the maximum value for alpha, as this value would produce a tree with only one leaf\nccp_alphas = ccp_alphas[:-1]\n\n# Let's create an array to hold our decision trees\ndts = []\n\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    dt.fit(X_train, y_train)\n    dts.append(dt)","45a05f03":"train_scores = [dt.score(X_train, y_train) for dt in dts]\ntest_scores = [dt.score(X_test, y_test) for dt in dts]\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle='steps-post')\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle='steps-post')\nax.legend()\nplt.show()","ef90d685":"# Creating an array to store the results of cross validation\nalpha_loop_values = []\n\n# Cross validation\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    scores = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \n# Let's visualize the candidate alphas\nalpha_results = pd.DataFrame(alpha_loop_values, columns=['alpha', 'mean_accuracy', 'std'])\n\nalpha_results.plot(x='alpha', y='mean_accuracy', yerr='std', marker='o', linestyle='--')","0f0c05c0":"# Let's get the best value\nideal_ccp_alpha = alpha_results['alpha'][alpha_results['mean_accuracy'].idxmax]\nideal_ccp_alpha","919b0bda":"# Pruned Decision Tree\ndt_pruned = DecisionTreeClassifier(ccp_alpha=ideal_ccp_alpha)\ndt_pruned.fit(X_train, y_train)\n\n# Plotting the tree\nplt.figure(figsize=(24, 18))\nplot_tree(dt_pruned, filled=True, rounded=True, class_names=['No HD', 'HD'], feature_names=X_encoded.columns);","51e7ff04":"# Confusion matrix\nplot_confusion_matrix(dt_pruned, X_test, y_test, display_labels=['Does not have HD', 'Has HD'])\nplt.grid(False)","5df75afd":"# Overall accuracy of the pruned tree\ndt_pruned.score(X_test, y_test)","a15ee2c2":"# Random Forest\nrf = RandomForestClassifier(n_estimators=50)\nrf.fit(X_train, y_train)\n\n# Cross validation\nrf_accuracy = np.mean(cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy'))\nprint('Mean accuracy: ', rf_accuracy)","90ef9404":"# Accuracy\nrf.score(X_test, y_test)","e25d6b6e":"error_rate = []\nn_est = []\n\nfor n in range(20, 200):\n    rf.set_params(n_estimators=n, oob_score=True, random_state=random_state)\n    rf.fit(X_train, y_train)\n    \n    # Record the OOB error for each `n_estimators=i` setting.\n    oob_error = 1 - rf.oob_score_\n    n_est.append(n)\n    error_rate.append(oob_error)\n    \nplt.plot(n_est, error_rate)\nplt.legend()\nplt.show()","803a4202":"# Data preprocessing\n\nAs some of the columns are categorical data, we need to create dummies for them. We'll apply the get dummies method to the cp, restecg, slope and thal columns.","990d545b":"# Model building\n\nLet's try to fit a Decision Tree and a Random Forest to compare each method. We can apply some techniques to improve the models as well.","f180d26d":"# The columns are:\n\n* age\n* sex: 1 = male, 0 = female\n* cp: chest pain type\n    * 1: typical angina\n    * 2: atypical angina\n    * 3: non-anginal pain\n    * 4: asymptomatic\n* trestbps: resting blood pressure\n* chol: serum cholestoral in mg\/dl\n* fbs: fasting blood sugar 0 = >=120 mg\/dl, 1 = <120 mg\/dl\n* restecg: resting electrocardiographic results \n    * 0: normal\n    * 1: having ST-T wave abnormality\n    * 2: showing probable or definite left ventricular hypertrophy\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina 0 = no, 1 = yes\n* oldpeak: oldpeak = ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n    * 1: upsloping\n    * 2: flat\n    * 3: downsloping\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n* target: 0 = less chance of heart attack 1 = more chance of heart attack","7a9859e8":"## Let's tune the model\n\n* n_estimators: Cross-validation to find the best value for n_estimators","57a5be90":"# Cost complexity pruning: Cross validation for finding the best alpha\n\nA second method that we can apply is cross validation for finding the best alpha. We will run a 5-fold cross validation for each candidate alpha and plot the results.","92bd4a66":"### **There's a tiny improvement over the original tree, and in this case (for this random state) it seems like we lost time, but after all, we got a higher accuracy with a much smaller tree, and that's good.**\n\n# Random Forest\n\nLet's try now a much better approach for classifying: Random Forest.","7d52f0dc":"# Let's check the accuracy of it's predictions","053bdfc4":"We can try to improve the model by pruning. Let's visualize the optimal alpha for our tree.\n\n# Cost complexity pruning: visualizing alpha\n\n## **This part was taken from this [webinar](https:\/\/https:\/\/www.youtube.com\/watch?v=q90UDEgYqeI)**","822ba6ba":"Now let's plot the accuracy of the trees using the Training Dataset and the Testing Dataset as a function of alpha","ef2b4142":"The value for n_estimators seems to be stable after n=50 (using a random_state)"}}