{"cell_type":{"a5b20a9e":"code","e883d3ac":"code","30b87f8c":"code","d9a14069":"code","e2b2f4bf":"code","63733e89":"code","feaed82e":"code","58e2b706":"code","27815081":"code","73ebb6dc":"markdown"},"source":{"a5b20a9e":"import os, sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skimage.io\nfrom skimage.transform import resize\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport PIL\nfrom PIL import Image\nimport cv2\nfrom sklearn.utils import class_weight, shuffle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSIZE = 512","e883d3ac":"# Load dataset info\npath_to_train = '..\/input\/human-protein-atlas-image-classification\/train\/'\ndata = pd.read_csv('..\/input\/human-protein-atlas-image-classification\/train.csv')\n\ntrain_dataset_info = []\nfor name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n    train_dataset_info.append({\n        'path':os.path.join(path_to_train, name),\n        'labels':np.array([int(label) for label in labels])})\ntrain_dataset_info = np.array(train_dataset_info)","30b87f8c":"class data_generator:\n    \n    def create_train(dataset_info, batch_size, shape, augument=True):\n        assert shape[2] == 3\n        while True:\n            dataset_info = shuffle(dataset_info)\n            for start in range(0, len(dataset_info), batch_size):\n                end = min(start + batch_size, len(dataset_info))\n                batch_images = []\n                X_train_batch = dataset_info[start:end]\n                batch_labels = np.zeros((len(X_train_batch), 28))\n                for i in range(len(X_train_batch)):\n                    image = data_generator.load_image(\n                        X_train_batch[i]['path'], shape)   \n                    if augument:\n                        if not np.all(X_train_batch[i]['labels']==(0,25)):\n                            #to balance the training data I don\u00b4t augument the labels 0 and 25\n                            image = data_generator.augment(image)\n                    batch_images.append(image\/255.)\n                    batch_labels[i][X_train_batch[i]['labels']] = 1\n                yield np.array(batch_images, np.float32), batch_labels\n\n    def load_image(path, shape):\n        image_red_ch = Image.open(path+'_red.png')\n        image_yellow_ch = Image.open(path+'_yellow.png')\n        image_green_ch = Image.open(path+'_green.png')\n        image_blue_ch = Image.open(path+'_blue.png')\n        image_red_yellow_ch = PIL.ImageChops.blend(image_red_ch,image_yellow_ch,0.5)\n        image = np.stack((\n        np.array(image_green_ch), \n        np.array(image_red_yellow_ch), \n        np.array(image_blue_ch)), -1)\n        image = cv2.resize(image, (shape[0], shape[1]),interpolation = cv2.INTER_CUBIC)\n        return image\n\n    def augment(image):\n        augment_img = iaa.Sequential([\n            iaa.OneOf([\n                iaa.Affine(rotate=0),\n                iaa.Affine(rotate=90),\n                iaa.Affine(rotate=180),\n                iaa.Affine(rotate=270),\n                iaa.Fliplr(0.5),\n                iaa.Flipud(0.5),\n            ])], random_order=True)\n\n        image_aug = augment_img.augment_image(image)\n        return image_aug","d9a14069":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D,BatchNormalization, Input, Conv2D\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.xception import Xception\nfrom keras.applications.nasnet import NASNetMobile, NASNetLarge\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam, Adagrad, Adamax \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\n\n    \ndef create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    base_model = NASNetMobile(include_top=False,\n                   weights='imagenet',\n                   input_shape=input_shape) \n    bn = BatchNormalization()(input_tensor)\n    x = base_model(bn)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1056, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, output)\n    \n    return model","e2b2f4bf":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nepochs = 10; batch_size = 16\ncheckpoint = ModelCheckpoint('..\/working\/InceptionV3.h5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=6)\ncallbacks_list = [checkpoint]\n\n# split data into train, valid\nindexes = np.arange(train_dataset_info.shape[0])\nnp.random.shuffle(indexes)\ntrain_indexes, valid_indexes = train_test_split(indexes, test_size=0.19, random_state=8)\n\n# create train and valid datagens\ntrain_generator = data_generator.create_train(\n    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\nvalidation_generator = data_generator.create_train(\n    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n","63733e89":"model = create_model(\n    input_shape=(SIZE,SIZE,3), \n    n_out=28)","feaed82e":"\n\n\nmodel.compile(loss='binary_crossentropy',\n            optimizer=Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n            metrics=['accuracy'])\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=np.ceil(float(len(train_indexes)) \/ float(batch_size)),\n    validation_data=validation_generator,\n    validation_steps=np.ceil(float(len(valid_indexes)) \/ float(batch_size)),\n    epochs=5, \n    verbose=1,\n    callbacks=callbacks_list)","58e2b706":"#model.compile(loss='binary_crossentropy',\n#            optimizer=Adamax(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001),\n#            metrics=['accuracy'])\n#model.fit_generator(\n#    train_generator,\n#    steps_per_epoch=np.ceil(float(len(train_indexes)) \/ float(batch_size)),\n#    validation_data=validation_generator,\n#    validation_steps=np.ceil(float(len(valid_indexes)) \/ float(batch_size)),\n#    epochs=, \n#    verbose=1,\n#    callbacks=callbacks_list)","27815081":"submit = pd.read_csv('..\/input\/human-protein-atlas-image-classification\/sample_submission.csv')\npredicted = []\ndraw_predict = []\nmodel.load_weights('..\/working\/InceptionV3.h5')\nfor name in tqdm(submit['Id']):\n    path = os.path.join('..\/input\/human-protein-atlas-image-classification\/test\/', name)\n    image = data_generator.load_image(path, (SIZE,SIZE,3))\/255.\n    score_predict = model.predict(image[np.newaxis])[0]\n    draw_predict.append(score_predict)\n    label_predict = np.arange(28)[score_predict>=0.2]\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    predicted.append(str_predict_label)\n\nsubmit['Predicted'] = predicted\nnp.save('draw_predict_InceptionV3.npy', score_predict)\nsubmit.to_csv('submit20.csv', index=False)","73ebb6dc":"## Thanks to  \nNguyen Tang Tri DucInceptionV3 baseline (LB: 0.379)\nhttps:\/\/www.kaggle.com\/mathormad\/inceptionv3-baseline-lb-0-379"}}