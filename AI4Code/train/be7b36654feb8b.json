{"cell_type":{"39e598c1":"code","6f4f6dc0":"code","9fce9977":"code","0e800971":"code","af947869":"code","030919c9":"code","6c227ce9":"code","babebc5b":"code","54917708":"code","bab7e55e":"code","bd824179":"code","5e2bc3d8":"code","0b51749c":"code","50e37a3b":"code","bb77f2ea":"code","9425865b":"code","c2d132b3":"code","610374a4":"code","375f7010":"code","a267369d":"code","44b73fa7":"code","c5897fff":"code","3feb4802":"code","437f1a71":"code","b8c1ae37":"code","ce7fd4c8":"code","dec69584":"code","c52b8b57":"code","c1898d09":"code","a95e7501":"code","38a77512":"code","03cee567":"code","460aa452":"markdown","8bacc70a":"markdown","e1355cc8":"markdown","6202179f":"markdown","021a0819":"markdown","57ec9b5d":"markdown","cc97d851":"markdown","2c257205":"markdown","c8c2c0c8":"markdown","902f72ef":"markdown","6968107d":"markdown","b23794e5":"markdown","b2711679":"markdown","970966cd":"markdown","d2f6054a":"markdown","ce91f54e":"markdown","6e3b708f":"markdown","ce11a5f3":"markdown","8bc9ea72":"markdown","9c1806e0":"markdown","e84738fa":"markdown"},"source":{"39e598c1":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport soundfile as sf","6f4f6dc0":"import matplotlib.image as mpimg\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')","9fce9977":"train_meta=pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv')\ntrain_sound=pd.read_csv('..\/input\/birdclef-2021\/train_soundscape_labels.csv')\ntest_df=pd.read_csv('..\/input\/birdclef-2021\/test.csv')\nsample_submission=pd.read_csv('..\/input\/birdclef-2021\/sample_submission.csv')","0e800971":"train_meta.head(10)","af947869":"train_sound.head()","030919c9":"path = '\/kaggle\/input\/birdclef-2021\/'\nos.listdir(path)","6c227ce9":"test_df.head()","babebc5b":"train_meta.shape","54917708":"train_meta.info()","bab7e55e":"train_meta.describe()","bd824179":"train_meta.isnull()","5e2bc3d8":"train_meta.tail()","0b51749c":"row = 0\ntrain_meta.iloc[row]","50e37a3b":"train_meta.columns ","bb77f2ea":"species = train_meta['primary_label'].value_counts()\nfig = go.Figure(data=[go.Bar(y=species.values, x=species.index)],\n                layout=go.Layout(margin=go.layout.Margin(l=0, r=1, b=5, t=10)))\n\nfig.update_layout(title='Number of traning samples per species')\nfig.show()","9425865b":"train_meta['year'] = train_meta['date'].apply(lambda x: x.split('-')[0])\ntrain_meta['month'] = train_meta['date'].apply(lambda x: x.split('-')[1])\ntrain_meta['day_of_month'] = train_meta['date'].apply(lambda x: x.split('-')[2])","c2d132b3":"label = train_meta.loc[row, 'primary_label']\nfilename = train_meta.loc[row, 'filename']\n\n# Check if the file is in the folder\nfilename in os.listdir(path+'train_short_audio\/'+label)\ndata, samplerate = sf.read(path+'train_short_audio\/'+label+'\/'+filename)\nprint(data[:8])\nprint(samplerate)","610374a4":"def read_ogg_file(path, file):\n    \"\"\" Read ogg audio file and return numpay array and samplerate\"\"\"\n    \n    data, samplerate = sf.read(path+file)\n    return data, samplerate\n\n\ndef plot_audio_file(data, samplerate):\n    \"\"\" Plot the audio data\"\"\"\n    \n    sr = samplerate\n    fig = plt.figure(figsize=(8, 4))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    plt.plot(x, y, color='red')\n    plt.legend(loc='upper center')\n    plt.grid()\n    \n    \ndef plot_spectrogram(data, samplerate):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    \n    sr = samplerate\n    spectrogram = librosa.feature.melspectrogram(data, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","375f7010":"plot_audio_file(data, samplerate)","a267369d":"plot_spectrogram(data, samplerate)","44b73fa7":"train_sound['audio_id'].unique()","c5897fff":"train_sound.groupby(by=['audio_id']).count()['birds'][:4]","3feb4802":"labels = []\nfor row in train_sound.index:\n    labels.extend(train_sound.loc[row, 'birds'].split(' '))\nlabels = list(set(labels))\n\nprint('Number of unique bird labels:', len(labels))","437f1a71":"df_labels_train = pd.DataFrame(index=train_sound.index, columns=labels)\nfor row in train_sound.index:\n    birds = train_sound.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_train.loc[row, bird] = 1\ndf_labels_train.fillna(0, inplace=True)\n\n# We set a dummy value for the target label in the test data because we will need for the Data Generator\ntest_df['birds'] = 'nocall'\n\ndf_labels_test = pd.DataFrame(index=test_df.index, columns=labels)\nfor row in test_df.index:\n    birds = test_df.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_test.loc[row, bird] = 1\ndf_labels_test.fillna(0, inplace=True)\ndf_labels_train.sum().sort_values(ascending=False)[:20]","b8c1ae37":"train_sound = pd.concat([train_sound, df_labels_train], axis=1)\ntest_df = pd.concat([test_df, df_labels_test], axis=1)\nprint(train_sound)","ce7fd4c8":"print(test_df)","dec69584":"file = os.listdir(path+'train_soundscapes')[0]\nfile\ndata, samplerate = read_ogg_file(path+'train_soundscapes\/', file)\n\nsub_data = data[int(455\/5)*160000:int(460\/5)*160000]\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(sub_data, sr=samplerate)\nplt.grid()\nplt.show()","c52b8b57":"import librosa\naudio_data = '..\/input\/birdclef-2021\/train_short_audio\/acafly\/XC109605.ogg'\nx , sr = librosa.load(audio_data)\nprint(type(x), type(sr))\nprint(x.shape, sr)","c1898d09":"import IPython.display as ipd\nipd.Audio(audio_data)","a95e7501":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)","38a77512":"df=sample_submission\ndf.to_csv('submission.csv', index=False)","03cee567":"df","460aa452":"# Here we created or called this function to make and help with the sound to get the graph out","8bacc70a":"# describe data","e1355cc8":"# Now showing the tail of data","6202179f":"# Do some analyzes for the data","021a0819":"# showing columns of data","57ec9b5d":"# import dataset ","cc97d851":"# read dataset","2c257205":"# showing some of information about data","c8c2c0c8":"# Now we're going to do a simple merge between data train_sound and test_df","902f72ef":"# Because we specify the time for the sound to use","6968107d":"# Now we can hear some voices","b23794e5":"# Show chart\n","b2711679":"**Recent advances in robotic listening have improved audio data collection. However, generating and retrieving analysis outputs with high accuracy remains a challenge. The majority of the data has not been examined due to the lack of effective tools to efficiently and reliably extract signals of interest (for example, bird calls).\nThe Hua Laboratory is the most popular, best and much interested laboratory, dedicated to enhancing the understanding and protection of birds and the natural world. The lab joins people from all walks of life to make new scientific discoveries, share ideas, and stimulate conservation actions. In this competition, they collaborate with Google Research, LifeCLEF, and Xeno-canto.**","970966cd":" # <p style=\"color:blue\"><center> Introduction<\/center><\/p>\n**We all know that birds are the creatures that are most vulnerable to risks and that they face various types of these many dangers, so we, as data scientists, must provide the best we have to help the organizations that are in the eyes of the eye specializing in helping birds and the organizations responsible for that and provide the best analyzes for them.**","d2f6054a":"# **Import some used Libraries**","ce91f54e":"**Here we will show you some of the audio and improve it for use**","6e3b708f":"# Now we're going to plot this matrix to see what the result is","ce11a5f3":"# ploting spectrogram to showing result ","8bc9ea72":"![Birdes](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQM_tI5u29WHhcyBilT5vMfjmywWqtS-9Iscw&usqp=CAU)\n![Birdes](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT8-TegC4ENTy0ywDXXLGWZkitqi_7s32gEmA&usqp=CAU)","9c1806e0":"# This is a drawing of the recorded external sound","e84738fa":"# Clarify if there are empty cells in the data or not"}}