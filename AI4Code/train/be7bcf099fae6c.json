{"cell_type":{"9494a07a":"code","525761c1":"code","54377d70":"code","a3a09c55":"code","e0e6a9a0":"code","cf1575f1":"code","5f1f8e5b":"code","c5a72aab":"code","af79debb":"code","82b86ec9":"code","97bae50e":"code","9b7a5f0a":"code","9b89e25e":"code","42cf40dd":"code","7cede596":"code","9c1e0841":"code","011942e0":"code","71f8a071":"code","01607f61":"code","efb9a0b0":"code","89249518":"code","d7db9f84":"code","80cea5a0":"code","c2f31254":"code","f3f74333":"code","380b3e51":"code","9e66a0ed":"code","8251cd0c":"code","340ad112":"code","af9bce9e":"code","f3b7ac41":"code","426fe13d":"code","60468c14":"code","41e5b849":"code","afadffe3":"code","d7454f3f":"code","bf830a82":"code","0aa97805":"code","799075ba":"code","787b94e6":"code","4dd080b1":"code","04764761":"code","373596b3":"code","0241825c":"code","3956b74f":"code","205e856a":"code","90c16d40":"code","3ea0b91d":"code","e839a74a":"code","5208d770":"code","5ed4b8a6":"code","42d8edc0":"code","61bb1ff4":"code","4c363185":"code","93804183":"code","85f2064f":"code","a6d0b3aa":"code","20bf8035":"code","b1e6c104":"code","429f9a02":"code","cf36eba3":"code","bff3d67b":"code","d068a298":"code","3b7f8619":"code","41794f8f":"code","12cf387c":"code","8d04cce2":"code","995f63dc":"code","c7501968":"code","2d08581f":"code","4bf58eab":"code","2048bcfe":"code","1a7aaffb":"code","d03026d5":"code","7cb73e97":"code","cb56a6e5":"code","45b026e6":"code","cc4b68a3":"code","67e67a8f":"code","208bd4c7":"code","0f801b76":"markdown","0cfe062c":"markdown","7b992834":"markdown","46b3fda6":"markdown","308d6aea":"markdown","2104b7d7":"markdown","fbb44588":"markdown","d2f76b6a":"markdown","3b7ed7eb":"markdown","4f9910f0":"markdown","66780399":"markdown","c132f341":"markdown","51470a7a":"markdown","5c01c06b":"markdown","db3adf34":"markdown"},"source":{"9494a07a":"#importing libraries\nimport pandas as pd\ntry:\n    import yfinance as yf\nexcept:\n    !pip install yfinance\n    import yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly as py\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom datetime import date\n","525761c1":"tickerSymbol='^GSPC'\ntickerData=yf.Ticker(tickerSymbol)\nmultiday_data = yf.download(tickers=tickerSymbol,\n                            period=\"30y\",\n                            interval=\"1d\",\n\n                            auto_adjust=True)\nmultiday_data.describe()\nmultiday_data","54377d70":"\nmultiday_data_filtered=multiday_data\nmultiday_data_filtered['True_Range']=multiday_data_filtered['High']-multiday_data_filtered['Low']\nmultiday_data_filtered['Volatility']=((multiday_data_filtered['High']-multiday_data_filtered['Low'])\/multiday_data_filtered['Low'])*100\n\n#the EMA and MA functions\ndef EMA_Calculator(days,smoothing,list_):\n    EMA_yesterday=list_[0]\n    EMA_list=[]\n    for Value_Today in list_:\n        EMA_today=(Value_Today*(smoothing\/(days+1)))+(EMA_yesterday*(1-(smoothing\/(days+1))))\n        EMA_list.append(round(EMA_today,2))\n        EMA_yesterday=EMA_today\n    return(EMA_list)\ndef simple_moving_average(days,list_):\n    list_ = list_.reset_index(drop=True)\n    i=days\n    o=0\n    f=0\n    MA_list=[]\n    while f<days:\n        MA_list.append(0)\n        f=f+1\n    while i<len(list_):\n        MA=round(np.average(list_[o:i]),2)\n        MA_list.append(MA)\n        i=i+1\n        o=o+1\n    return(MA_list)\n        \n        \n#adjust price to inflation\navg_inflation=0.97\/(multiday_data_filtered.index[-1]-multiday_data_filtered.index[0]).days\n\nmultiday_data_filtered['Inflation']=[ ((multiday_data_filtered.index[-1]-x).days)*avg_inflation for x in multiday_data_filtered.index]\nmultiday_data_filtered['Inflation']=multiday_data_filtered['Inflation']\n\nmultiday_data_filtered['True_Range_Inflation_adj']=(multiday_data_filtered['True_Range']*multiday_data_filtered['Inflation'])+multiday_data_filtered['True_Range']\nmultiday_data_filtered['Volume_Inflation_adj']=(multiday_data_filtered['Volume']*multiday_data_filtered['Inflation'])+multiday_data_filtered['Volume']\n     \n\n\n#Implementing the values in to the DataFrame\nmultiday_data_filtered['True_Range_Inflation_adj_EMA50']=EMA_Calculator(50,2,multiday_data_filtered['True_Range_Inflation_adj'])\nmultiday_data_filtered['True_Range_EMA50']=EMA_Calculator(50,2,multiday_data_filtered['True_Range'])\nmultiday_data_filtered['True_Range_Inflation_adj_MA365']=simple_moving_average(365,multiday_data_filtered['True_Range_Inflation_adj'])\nmultiday_data_filtered['Volatility_EMA50']=EMA_Calculator(50,2,multiday_data_filtered['Volatility'])\nmultiday_data_filtered['Volatility_MA365']=simple_moving_average(365,multiday_data_filtered['Volatility'])\nmultiday_data_filtered['Volume_Inflation_adj_EMA50']=EMA_Calculator(50,2,multiday_data_filtered['Volume_Inflation_adj'])\nmultiday_data_filtered['Volume_Inflation_adj_MA365']=simple_moving_average(365,multiday_data_filtered['Volume_Inflation_adj'])\nmultiday_data_filtered['Volume_EMA50']=EMA_Calculator(50,2,multiday_data_filtered['Volume'])\n\nmultiday_data_filtered","a3a09c55":"\n\ndef Graph(column):\n    layout = go.Layout(\n    title='Visualization of the '+column,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\n    fig = go.Figure(data=go.Scatter(x=multiday_data_filtered.index, y=multiday_data_filtered[column+'_EMA50'],mode='lines',name='EMA50', line_width=2),layout=layout)\n    try:\n        if column.replace('_Inflation_adj','')!=column:\n            \n            fig.add_trace(go.Scatter(x=multiday_data_filtered.index, y=multiday_data_filtered[column.replace('_Inflation_adj','')+'_EMA50'], name=\"EMA50 not inflation adjusted\",line_shape='linear',line_width=1,line_color='Green'))\n        pass\n    except:\n        pass\n    fig.add_trace(go.Scatter(x=multiday_data_filtered.index, y=multiday_data_filtered[column+'_MA365'], name=\"MA365\",line_shape='linear',line_width=4,line_color='Red'))\n\n\n\n    fig.add_shape(type='line',\n                    x0='1991-09-23',\n                    y0=np.average(multiday_data_filtered[column]),\n                    x1='2021-09-23',\n                    y1=np.average(multiday_data_filtered[column]),\n                    line=dict(color='Green',),\n                    xref='x',\n                    yref='y'\n    )\n    fig.show()\nGraph('True_Range_Inflation_adj')\nGraph('Volatility')\nGraph('Volume_Inflation_adj')","e0e6a9a0":"\nlayout = go.Layout(\n    title='Volatility',\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\nfig = px.violin(multiday_data_filtered, x=\"Volatility\",points=False,box=True)\nfig.update_layout(layout)\n\nfig.show()","cf1575f1":"months={'01':[],'02':[],'03':[],'04':[],'05':[],'06':[],'07':[],'08':[],'09':[],'10':[],'11':[],'12':[]}\nlenght=[]\ni=0\nfor x in multiday_data_filtered.index:\n    string=x.strftime(\"%m\") \n    months[string].append(multiday_data_filtered['Volatility'][x])\nfor y,x in months.items():\n    lenght.append(len(x))\n\nfor y,x in months.items():\n    if len(x)>min(lenght):\n        list_=months[y]\n        list_=list_[0:min(lenght)]\n        months[y]=list_\naverages=[]\n\n\nMonthly_volatility=pd.DataFrame.from_dict(months)\nMonthly_volatility        \nfor x in Monthly_volatility:\n    averages.append(np.average(Monthly_volatility[x]))\naverages","5f1f8e5b":"\nfig = px.box(Monthly_volatility,points=False)\nfig.add_trace(go.Scatter(x=Monthly_volatility.columns,y=averages, name=\"Average\",line_shape='linear',line_width=2,line_color='Red'))\n\nlayout = go.Layout(\n    title='Monthly Volatility',\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\nfig.update_layout(layout)\n\nfig.show()","c5a72aab":"days_volatility=dict()\nfor x in list(range(1,32)):\n    days_volatility[x]=[]\nfor x in multiday_data_filtered.index:\n    string=x.strftime(\"%d\") \n    days_volatility[int(string)].append(multiday_data_filtered['Volatility'][x])\ndf_days_volatility=pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in days_volatility.items() ]))\naverages=[]\nfor x in df_days_volatility:\n    averages.append(np.average(df_days_volatility[x]))\naverages\ndf_days_volatility","af79debb":"fig = px.box(df_days_volatility,points=False)\n\n\nfig.update_layout(layout)\n\nfig.show()","82b86ec9":"annual_volatility=dict()\nfor x in list(range(1991,2022)):\n    annual_volatility[x]=[]\nfor x in multiday_data_filtered.index:\n    string=x.strftime(\"%Y\") \n    annual_volatility[int(string)].append(multiday_data_filtered['Volatility'][x])\ndf_annual_volatility=pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in annual_volatility.items() ]))\ndf_annual_volatility","97bae50e":"fig = px.box(df_annual_volatility,points=False)\nlayout = go.Layout(\n    title='Annual Volatility',\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\nfig.update_layout(layout)\n\nfig.show()","9b7a5f0a":"layout = go.Layout(\n    title='Gain\/Loss%',\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\nmultiday_data_price=multiday_data.drop(['Low','High'],axis=1)\nmultiday_data_price['Gain\/Loss']=multiday_data_price['Open']-multiday_data_price['Close']\nmultiday_data_price['Gain\/Loss%']=((multiday_data_price['Open']-multiday_data_price['Close'])\/multiday_data_price['Open'])*100\n\n","9b89e25e":"layout = go.Layout(\n    title='Gain%',\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title_x=0.5\n    )\nvolatility_positive=[]\nvolatility_negative=[]\nfor x in multiday_data_price.index:\n    if multiday_data_price['Gain\/Loss%'][x]>0:\n        volatility_positive.append(multiday_data_filtered['Volatility'][x])\n    else:\n        volatility_negative.append(multiday_data_filtered['Volatility'][x])\n_={'pos':volatility_positive,'neg':volatility_negative}\ndf_posneg=pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in _.items() ])) #B (Better)\nprint(np.average(volatility_positive))\nprint(np.average(volatility_negative))\nfig=px.scatter(df_posneg)\nfig.update_layout(layout)\nfig.update_traces(marker=dict(size=3.5))\nfig.show()    \n        \n        ","42cf40dd":"train_df=multiday_data_filtered\n\nmedians={'day':[],'month':[],'year':[]}\ndf_days_volatility.fillna(df_days_volatility.median(),inplace=True )\ndf_annual_volatility.fillna(df_annual_volatility.median(),inplace=True )\nMonthly_volatility.fillna(Monthly_volatility.median(),inplace=True )\nfor x in train_df.index:\n    medians['day'].append(np.median(df_days_volatility[int(x.strftime(\"%d\"))]))\n    medians['year'].append(np.median(df_annual_volatility[int(x.strftime(\"%Y\"))]))\n    medians['month'].append(np.median(Monthly_volatility[x.strftime(\"%m\")]))\n\ntrain_df['median_day']=medians['day']\ntrain_df['median_month']=medians['month']\ntrain_df['median_year']=medians['year']","7cede596":"window_length = 14\ndelta = train_df['Close'].diff()\n# Get rid of the first row, which is NaN since it did not have a previous \n# row to calculate the differences\ndelta = delta[1:] \n\n# Make the positive gains (up) and negative gains (down) Series\nup, down = delta.clip(lower=0), delta.clip(upper=0)\n\n# Calculate the SMA\nroll_up2 = up.rolling(window_length).mean()\nroll_down2 = down.abs().rolling(window_length).mean()\n\n# Calculate the RSI based on SMA\nRS2 = roll_up2 \/ roll_down2\nRSI2 = 100.0 - (100.0 \/ (1.0 + RS2))\n\ntrain_df['RSI']=RSI2\ntrain_df['RSI'] = train_df['RSI'].fillna(40)\n","9c1e0841":"\ntrain_df.reset_index(inplace=True,drop=False)\ndate_time = pd.to_datetime(train_df.pop('Date'), format='%d.%m.%Y %H:%M:%S')\n","011942e0":"\ntimestamp_s = date_time.map(pd.Timestamp.timestamp)\nyear = (365.2425)*24*60*60\n\n\ntrain_df['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ntrain_df['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))","71f8a071":"plt.plot(np.array(train_df['Year sin'])[:365])\nplt.plot(np.array(train_df['Year cos'])[:365])\nplt.xlabel('Time [d]')\nplt.title('Time of year signal')","01607f61":"train_df","efb9a0b0":"import tensorflow as tf\nimport seaborn as sns\n","89249518":"fft = tf.signal.rfft(train_df['True_Range_Inflation_adj'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_d = len(train_df['True_Range_Inflation_adj'])\nhours_per_year = 24*365.2524\nyears_per_dataset = n_samples_d\/(365.2524)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 400000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1\/Year', '1\/day'])\n_ = plt.xlabel('Frequency (log scale)')","d7db9f84":"column_indices = {name: i for i, name in enumerate(train_df.columns)}\ntrain_df_out=train_df\nn = len(train_df_out)\ntrain_df = train_df_out[0:int(n*0.7)]\nval_df = train_df_out[int(n*0.7):int(n*0.9)]\ntest_df = train_df_out[int(n*0.9):]\n\nnum_features = train_df.shape[1]\n","80cea5a0":"\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) \/ train_std\nval_df = (val_df - train_mean) \/ train_std\ntest_df = (test_df - train_mean) \/ train_std\n\n\ndf_std = (train_df - train_mean) \/ train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(train_df_out.keys(), rotation=90)\n","c2f31254":"class WindowGenerator():\n      def __init__(self, input_width, label_width, shift,\n                   train_df=train_df, val_df=val_df, test_df=test_df,\n                   label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df        \n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in\n                                        enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in\n                               enumerate(train_df.columns)}        \n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift        \n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n    \n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n      def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Label column name(s): {self.label_columns}'])\n    ","f3f74333":"w1 = WindowGenerator(input_width=30, label_width=1, shift=1,\n                     label_columns=['Close'])\nw1","380b3e51":"def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n          axis=-1)\n\n    # Slicing doesn't preserve static shape information, so set the shapes\n    # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n\n    return inputs, labels\n\nWindowGenerator.split_window = split_window\n# Stack three slices, the length of the total window.\nexample_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n                           np.array(train_df[100:100+w1.total_window_size]),\n                           np.array(train_df[200:200+w1.total_window_size])])\n\nexample_inputs, example_labels = w1.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')","9e66a0ed":"w1.example = example_inputs, example_labels\n\ndef plot(self, model=None, plot_col='Close', max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(12, 8))\n    plot_col_index = self.column_indices[plot_col]\n    max_n = min(max_subplots, len(inputs))\n    for n in range(max_n):\n        plt.subplot(max_n, 1, n+1)\n        plt.ylabel(f'{plot_col} [not normed]')\n        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n               label='Inputs', marker='.', zorder=-10)\n\n        if self.label_columns:\n            label_col_index = self.label_columns_indices.get(plot_col, None)\n        else:\n            label_col_index = plot_col_index\n\n        if label_col_index is None:\n            continue\n\n        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                    marker='X', edgecolors='k', label='Predictions',\n                    c='#ff7f0e', s=64)\n\n        if n == 0:\n            plt.legend()\n\n    plt.xlabel('Time [d]')\n\nWindowGenerator.plot = plot\n","8251cd0c":"w1.plot(plot_col='Close')\n","340ad112":"w1.plot(plot_col='Low')\n#no labels","af9bce9e":"def make_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=self.total_window_size,\n        sequence_stride=1,\n        shuffle=True,\n        batch_size=32,)\n  \n    ds = ds.map(self.split_window)\n  \n    return ds\n  \nWindowGenerator.make_dataset = make_dataset","f3b7ac41":"@property\ndef train(self):\n    return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n    return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n    return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n    result = getattr(self, '_example', None)\n    if result is None:\n      # No example batch was found, so get one from the `.train` dataset\n      result = next(iter(self.train))\n      # And cache it for next time\n      self._example = result\n    return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","426fe13d":"for example_inputs, example_labels in w1.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')\n\n","60468c14":"single_step_window = WindowGenerator(\n    input_width=1, label_width=1, shift=1,\n    label_columns=['Close'])\nsingle_step_window","41e5b849":"for example_inputs, example_labels in single_step_window.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')","afadffe3":"class Baseline(tf.keras.Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        result = inputs[:, :, self.label_index]\n        return result[:, :, tf.newaxis]\n    ","d7454f3f":"wide_window = WindowGenerator(\n    input_width=60, label_width=60, shift=1,\n    label_columns=['Close'])\n\nwide_window","bf830a82":"baseline = Baseline(label_index=column_indices['Close'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(wide_window.val)\nperformance['Baseline'] = baseline.evaluate(wide_window.test, verbose=0)","0aa97805":"wide_window.plot(baseline)\n","799075ba":"linear = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1)\n])\n\nprint('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', linear(wide_window.example[0]).shape)","787b94e6":"MAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, patience=2):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                      patience=patience,\n                                                      mode='min')   \n    model.compile(loss=tf.losses.MeanSquaredError(),\n                  optimizer=tf.optimizers.Adam(),\n                  metrics=[tf.metrics.MeanAbsoluteError()])   \n    history = model.fit(window.train, epochs=MAX_EPOCHS,\n                        validation_data=window.val,\n                        callbacks=[early_stopping])\n    return history","4dd080b1":"history = compile_and_fit(linear, wide_window)\n\nval_performance['Linear'] = linear.evaluate(wide_window.val)\nperformance['Linear'] = linear.evaluate(wide_window.test, verbose=0)","04764761":"print('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', baseline(wide_window.example[0]).shape)\n","373596b3":"wide_window.plot(linear)\n","0241825c":"plt.bar(x = range(len(train_df.columns)),\n        height=linear.layers[0].kernel[:,0].numpy())\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","3956b74f":"dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(dense, wide_window)\n\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)","205e856a":"wide_window.plot(dense)","90c16d40":"plt.bar(x = range(len(train_df.columns)),\n        height=dense.layers[0].kernel[:,0].numpy())\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","3ea0b91d":"CONV_WIDTH = 30\nconv_window = WindowGenerator(\n    input_width=CONV_WIDTH,\n    label_width=1,\n    shift=1,\n    label_columns=['Close'])\n\nconv_window","e839a74a":"conv_window.plot()","5208d770":"multi_step_dense = tf.keras.Sequential([\n    # Shape: (time, features) => (time*features)\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n    # Add back the time dimension.\n    # Shape: (outputs) => (1, outputs)\n    tf.keras.layers.Reshape([1, -1]),\n])\n\nprint('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', multi_step_dense(conv_window.example[0]).shape)","5ed4b8a6":"history = compile_and_fit(multi_step_dense, conv_window)\n\nval_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\nperformance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)","42d8edc0":"conv_window.plot(multi_step_dense)\n","61bb1ff4":"conv_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32,\n                           kernel_size=(CONV_WIDTH,),\n                           activation='relu'),\n    tf.keras.layers.Dense(units=32, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n])\nprint(\"Conv model on `conv_window`\")\nprint('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', conv_model(conv_window.example[0]).shape)","4c363185":"history = compile_and_fit(conv_model, conv_window)\n\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)","93804183":"conv_window.plot(conv_model)\n","85f2064f":"MAX_EPOCHS=20\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])","a6d0b3aa":"print('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', lstm_model(wide_window.example[0]).shape)","20bf8035":"history = compile_and_fit(lstm_model, wide_window)\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)","b1e6c104":"wide_window.plot(lstm_model)\n","429f9a02":"x = np.arange(len(performance))\nwidth = 0.3\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.ylabel('mean_absolute_error [Close, normalized]')\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\n_ = plt.legend()","cf36eba3":"for name, value in performance.items():\n    print(f'{name:12s}: {value[1]:0.4f}')","bff3d67b":"single_step_window = WindowGenerator(\n    # `WindowGenerator` returns all features as labels if you \n    # don't set the `label_columns` argument.\n    input_width=1, label_width=1, shift=1)\n\nwide_window = WindowGenerator(\n    input_width=60, label_width=60, shift=1)\n\nfor example_inputs, example_labels in wide_window.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')","d068a298":"baseline = Baseline()\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(wide_window.val)\nperformance['Baseline'] = baseline.evaluate(wide_window.test, verbose=0)","3b7f8619":"dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=num_features)\n])\nhistory = compile_and_fit(dense, single_step_window)\n\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)","41794f8f":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=num_features)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window)\n\nval_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n\nprint()","12cf387c":"class ResidualWrapper(tf.keras.Model):\n        def __init__(self, model):\n            super().__init__()\n            self.model = model\n        \n        def call(self, inputs, *args, **kwargs):\n            delta = self.model(inputs, *args, **kwargs)\n        \n          # The prediction for each time step is the input\n          # from the previous time step plus the delta\n          # calculated by the model.\n            return inputs + delta","8d04cce2":"%%time\nresidual_lstm = ResidualWrapper(\n    tf.keras.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(\n        num_features,\n        # The predicted deltas should start small.\n        # Therefore, initialize the output layer with zeros.\n        kernel_initializer=tf.initializers.zeros())\n]))\n\nhistory = compile_and_fit(residual_lstm, wide_window)\n\nval_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)\nperformance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0)\nprint()","995f63dc":"x = np.arange(len(performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\nplt.ylabel('MAE (average over all outputs)')\n_ = plt.legend\nfor name, value in performance.items():\n    print(f'{name:15s}: {value[1]:0.4f}')","c7501968":"OUT_STEPS = 24\nmulti_window = WindowGenerator(input_width=24,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window","2d08581f":"class MultiStepLastBaseline(tf.keras.Model):\n    def call(self, inputs):\n        return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.losses.MeanSquaredError(),\n                      metrics=[tf.metrics.MeanAbsoluteError()])\n\nmulti_val_performance = {}\nmulti_performance = {}\n\nmulti_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\nmulti_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(last_baseline)","4bf58eab":"multi_linear_model = tf.keras.Sequential([\n    # Take the last time-step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_linear_model, multi_window)\n\nmulti_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\nmulti_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_linear_model)\n\n","2048bcfe":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\n\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_dense_model)","1a7aaffb":"CONV_WIDTH = 3\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\n\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_conv_model)","d03026d5":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)","7cb73e97":"class FeedBack(tf.keras.Model):\n      def __init__(self, units, out_steps):\n        super().__init__()\n        self.out_steps = out_steps\n        self.units = units\n        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n        self.dense = tf.keras.layers.Dense(num_features)\n        \n        \nfeedback_model = FeedBack(units=60, out_steps=OUT_STEPS)\n\n    ","cb56a6e5":"def warmup(self, inputs):\n    # inputs.shape => (batch, time, features)\n    # x.shape => (batch, lstm_units)\n    x, *state = self.lstm_rnn(inputs)\n  \n    # predictions.shape => (batch, features)\n    prediction = self.dense(x)\n    return prediction, state\n  \nFeedBack.warmup = warmup","45b026e6":"prediction, state = feedback_model.warmup(multi_window.example[0])\nprediction.shape","cc4b68a3":"def call(self, inputs, training=None):\n    # Use a TensorArray to capture dynamically unrolled outputs.\n    predictions = []\n    # Initialize the LSTM state.\n    prediction, state = self.warmup(inputs)\n  \n    # Insert the first prediction.\n    predictions.append(prediction)\n  \n    # Run the rest of the prediction steps.\n    for n in range(1, self.out_steps):\n      # Use the last prediction as input.\n      x = prediction\n      # Execute one lstm step.\n      x, state = self.lstm_cell(x, states=state,\n                                training=training)\n      # Convert the lstm output to a prediction.\n      prediction = self.dense(x)\n      # Add the prediction to the output.\n      predictions.append(prediction)\n  \n    # predictions.shape => (time, batch, features)\n    predictions = tf.stack(predictions)\n    # predictions.shape => (batch, time, features)\n    predictions = tf.transpose(predictions, [1, 0, 2])\n    return predictions\n\nFeedBack.call = call\nprint('Output shape (batch, time, features): ', feedback_model(multi_window.example[0]).shape)\n","67e67a8f":"history = compile_and_fit(feedback_model, multi_window)\n\n\nmulti_val_performance['AR LSTM'] = feedback_model.evaluate(multi_window.val)\nmulti_performance['AR LSTM'] = feedback_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(feedback_model)","208bd4c7":"x = np.arange(len(multi_performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in multi_val_performance.values()]\ntest_mae = [v[metric_index] for v in multi_performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=multi_performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()","0f801b76":"#### This project is divided into several parts. Each part will analyze the volatility of the S&P500 on a diffent time frame; the first part will analyze the index in a long term view, the second part (WIP)","0cfe062c":"#### The DataFrame above has 5 Columns \n>* Open: the opening price (first)\n>\n>* Close: the closing price (last)\n>\n>* High :the maximum price in that session\n>\n>* Low : the minimum price in that session\n##### The aim of this project is to evaluate the importance of the price volatility so the Open and Close columns for moment are not necessary","7b992834":"In the data frame above, volatility is sorted by month. In the process of converting the values some data got lost, but, since there is a great amount of it, this is not a significant problem.","46b3fda6":"# Multi-output models","308d6aea":" #### Considerations\n >* there is a colleration between time and Volume. This is easy to understand because as the Index price rises, so does the Volume, so the more time passes, the more the index value rises, and, consequently, the Volume increases. \n >\n >* there is not a clear correlation between time and Volatility\/True Range\n >\n >","2104b7d7":"# Advanced: Autoregressive model","fbb44588":"# Forecasting","d2f76b6a":"There is not a big variation of the volatility  \n## WORK IN PROGRESS","3b7ed7eb":"## Part 1: A general overview","4f9910f0":"#### Now let's explain what variables I added\n>* The MA and EMA acronyms refer to the simple moving average and the exponential moving average respectively, and they are some useful tools to use to show trends and visualize the data in a continuative way.\n>\n>* True range is important to measure the volatility of the price in a single day\n>\n>* The Volatility is the true range in percentage and it's normalized to prioritize the variation of the prices to the absolute price.","66780399":"The goal of this first part is to find any correlation in the data we have, find some interesting insights and elaborate new values to use in the following parts. The state-of-the-art of Data exploration is Data visualization because it helps us to identify hidden patterns and create a structured summary of the data that we have to work with.\n","c132f341":"# Single window generator","51470a7a":"## WORK IN PROGRESS","5c01c06b":"# General volatility of ETFs","db3adf34":"##### The first thing to do is to have a look at the data structure, to familiarize with the values that ve have to manage"}}