{"cell_type":{"43565afe":"code","b3d9d749":"code","0c6a2e96":"code","8cb9331c":"code","f4e5645f":"code","fc6fe2f3":"code","96512757":"code","9579d504":"code","09d2f8ca":"code","f66cfd91":"code","121bbd01":"code","dfcc0b86":"code","3e308a96":"code","dd8ca977":"code","4cb7cc99":"code","a124d32b":"markdown","1444e906":"markdown","51d5c073":"markdown","8a972e52":"markdown","e88968c0":"markdown","d30b88db":"markdown","f8059ca2":"markdown","53faf7d0":"markdown","6f7cfe41":"markdown"},"source":{"43565afe":"image_size = 512\nbatch_size = 32\nnum_workers = 4\nn_batch = 10 # to avoid oom, split 70000+ images into 10 batches\nsim_thresh = 0.8","b3d9d749":"import pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') ","0c6a2e96":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","8cb9331c":"transforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])","f4e5645f":"class SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.astype(np.float32)\n        img = img.transpose(2,0,1)\n        \n        if self.mode == 'test':\n            return torch.tensor(img).float()\n        else:\n            return torch.tensor(img).float(), torch.tensor(row.label_group).float()","fc6fe2f3":"class ArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s=10, m=0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs\n\n","96512757":"class SHOPEEDenseNet(nn.Module):\n\n    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=False):\n        super(SHOPEEDenseNet, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.channel_size = channel_size\n        self.out_feature = out_feature\n        self.in_features = self.backbone.classifier.in_features\n        self.margin = ArcModule(in_features=self.channel_size, out_features = self.out_feature)\n        self.bn1 = nn.BatchNorm2d(self.in_features)\n        self.dropout = nn.Dropout2d(dropout, inplace=True)\n        self.fc1 = nn.Linear(self.in_features * 16 * 16 , self.channel_size)\n        self.bn2 = nn.BatchNorm1d(self.channel_size)\n        \n    def forward(self, x, labels=None):\n        features = self.backbone.features(x)\n        features = self.bn1(features)\n        features = self.dropout(features)\n        features = features.view(features.size(0), -1)\n        features = self.fc1(features)\n        features = self.bn2(features)\n        features = F.normalize(features)\n        if labels is not None:\n            return self.margin(features, labels)\n        return features\n","9579d504":"data_dir = '..\/input\/shopee-product-matching\/train_images'\ndf_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(data_dir, x))","09d2f8ca":"model = SHOPEEDenseNet(512, 11014)\nmodel.load_state_dict(torch.load('..\/input\/margin-05-cv731\/baseline_fold0_densenet_512_epoch30.pth', map_location='cuda:0'))\nmodel.to(device);","f66cfd91":"test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\ntest['file_path'] = test.image.apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/test_images',x))\ndataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","121bbd01":"def generate_test_features(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images) in enumerate(bar):\n\n            images = images.to(device)\n\n            features = model(images)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","dfcc0b86":"FEAS = generate_test_features(test_loader)","3e308a96":"n, _ = FEAS.shape\nbs = n \/\/ 10 ","dd8ca977":"del model\ngc.collect()\ntorch.cuda.empty_cache()","4cb7cc99":"if n != 3:\n    FEAS = torch.tensor(FEAS).cuda()\n    batches = []\n    for i in range(n_batch):\n        left = bs * i\n        right = bs * (i+1)\n        if i == n_batch - 1:\n            right = n\n        batches.append(FEAS[left:right,:])\n    \n    matches = []\n    for batch in tqdm(batches):\n        batch = batch.cuda()\n        selection = ((batch@FEAS.T) > sim_thresh).cpu().numpy()\n        for row in selection:\n            matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n    submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n    submission['matches'] = matches\n    submission.to_csv('submission.csv', index=False)\n    \n## don't do anything during commit\nelse:\n    submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n    submission.to_csv('submission.csv', index=False)","a124d32b":"## Read in test data","1444e906":"## Inference by batches","51d5c073":"## Dataset","8a972e52":"## Model","e88968c0":"## Transforms","d30b88db":"## Configuration","f8059ca2":"## Imports","53faf7d0":"## Generate Features","6f7cfe41":"## Summary\nThis is the inference notebook for https:\/\/www.kaggle.com\/underwearfitting\/pytorch-densenet-arcface-validation-training. In this notebook, I submitted a single fold trained Arcface Densenet121 with a CV 0.731. I computed the cosine similarities between the feature vectors. To make it faster, I put this process on GPU and compute by batches to avoid OOM issue. Don't hesitate if you have any questions; answering your questions can help me learn as well. "}}