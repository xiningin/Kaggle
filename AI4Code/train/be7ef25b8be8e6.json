{"cell_type":{"a96e3af6":"code","329b8964":"code","6ec18d28":"code","048ab1ee":"code","4db7ac0b":"code","21274834":"code","d64bf00e":"code","85ff9068":"code","834c1e38":"code","b31d04c7":"code","4cc08172":"code","bd460e53":"code","8b0e043e":"code","b562c616":"code","bb51753e":"code","171683eb":"code","74127ca9":"code","b247fa0d":"code","0dab005b":"code","c7bc67ae":"code","3f0f5c14":"code","38dc566e":"code","0f4dd0ec":"code","fd5edaa7":"code","cb9f98a5":"code","19cf52ca":"code","4ca3e0ce":"code","7a20fdd2":"code","de2b358c":"code","f40354fc":"code","dd9deaad":"code","6cec0ead":"code","5fba10f9":"code","1319b2bd":"code","0001f301":"code","98d02269":"code","03100a91":"code","21bf59a4":"code","e6dcc636":"code","6385c529":"code","f8dffd81":"markdown","1a61d778":"markdown","5414d116":"markdown","0cdd221b":"markdown","24cfdaea":"markdown","be3fdb18":"markdown","350e5dec":"markdown","6606e9aa":"markdown","5e06d08d":"markdown","0271cc5f":"markdown","7ebda160":"markdown","123484e2":"markdown","84bed18d":"markdown","39f7ae26":"markdown","cc7122f8":"markdown","f6a4eed5":"markdown","da9df4ab":"markdown","fef77541":"markdown","43b09858":"markdown","0b77d459":"markdown","9e49548b":"markdown","92ef2710":"markdown","6fbc8a7c":"markdown","fdf9d3cc":"markdown","1cd99e60":"markdown"},"source":{"a96e3af6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","329b8964":"#import Library \nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6ec18d28":"#Read data from csv file\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","048ab1ee":"print('The shape of our training set: ',df_train.shape[0], 'houses', 'and', df_train.shape[1], 'features')\nprint('The shape of our testing set: ',df_test.shape[0], 'houses', 'and', df_test.shape[1], 'features')\nprint('The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  ')","4db7ac0b":"df_train.head()","21274834":"df_test.head()","d64bf00e":"df_train.describe()","85ff9068":"df_test.describe()","834c1e38":"#Header name Columns \ndf_train.columns","b31d04c7":"df_test.columns","4cc08172":"numeric = df_train.select_dtypes(exclude='object')\ncategorical = df_train.select_dtypes(include='object')","bd460e53":"print(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","8b0e043e":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])","b562c616":"# Isolate the numeric features and check his relevance\n\nnum_corr = numeric.corr()\ntable = num_corr['SalePrice'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb","bb51753e":"\nf, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(df_train.corr(),annot=True, linewidths=.1, fmt= '.1f',ax=ax, cmap=\"YlGnBu\")","171683eb":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = df_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","74127ca9":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","b247fa0d":"#missing data in Traing examples\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(20)","0dab005b":"na = df_train.shape[0] #na is the number of rows of the original training set\nnb = df_test.shape[0]  #nb is the number of rows of the original test set\ny_train = df_train['SalePrice'].to_frame()\n#Combine train and test sets\nc1 = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)\n#Drop the target \"SalePrice\" and Id columns\nc1.drop(['SalePrice'], axis=1, inplace=True)\nc1.drop(['Id'], axis=1, inplace=True)\nprint(\"Total size for train and test sets is :\",c1.shape)","c7bc67ae":"##msv1 method to visualize missing values per columns\ndef msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3): \n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","3f0f5c14":"msv1(c1, 20, color=('silver', 'gold', 'lightgreen', 'skyblue', 'lightpink'))","38dc566e":"# drop columns (features ) with > 80% missing vales\nc=c1.dropna(thresh=len(c1)*0.8, axis=1)\nprint('We dropped ',c1.shape[1]-c.shape[1], ' features in the combined set')","0f4dd0ec":"print('The shape of the combined dataset after dropping features with more than 80% M.V.', c.shape)","fd5edaa7":"allna = (c.isnull().sum() \/ len(c))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\n\ndef msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(width, height))\n\n    allna = (data.isnull().sum() \/ len(data))*100\n    tightout= 0.008*max(allna)\n    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n    ax.set_xlabel('Percentage', weight='bold', size=15)\n    ax.set_ylabel('Features with missing values', weight='bold')\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    for i in ax.patches:\n        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n            fontsize=10, fontweight='bold', color='grey')\n    return plt.show()","cb9f98a5":"msv2(c)","19cf52ca":"NA=c[allna.index.to_list()]","4ca3e0ce":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint('We have :',NAcat.shape[1],'categorical features with missing values')\nprint('We have :',NAnum.shape[1],'numerical features with missing values')","7a20fdd2":"NAnum.head()","de2b358c":"NANUM= NAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNANUM = NANUM.style.background_gradient(cmap=cm)\nNANUM","f40354fc":"#MasVnrArea: Masonry veneer area in square feet, the missing data means no veneer so we fill with 0\nc['MasVnrArea']=c.MasVnrArea.fillna(0)\n#LotFrontage has 16% missing values. We fill with the median\nc['LotFrontage']=c.LotFrontage.fillna(c.LotFrontage.median())\n#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\nc['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)\n#For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns:\n#We will fill with 0s because they just mean that the hosue doesn't have a basement, bathrooms or a garage","dd9deaad":"bb=c[allna.index.to_list()]\nnan=bb.select_dtypes(exclude='object')\nN= nan.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nN= N.style.background_gradient(cmap=cm)\nN","6cec0ead":"NAcat.head()","5fba10f9":"NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNAcat1 = NAcat1.style.background_gradient(cmap=cm)\nNAcat1","1319b2bd":"fill_cols = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\n\nfor col in c[fill_cols]:\n    c[col] = c[col].fillna(method='ffill')","0001f301":"dd=c[allna.index.to_list()]\nw=dd.select_dtypes(include='object')\na= w.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\na= a.style.background_gradient(cmap=cm)\na","98d02269":"#we will just 'None' in categorical features\n#Categorical missing values\nNAcols=c.columns\nfor col in NAcols:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")\n","03100a91":"#we will just fill 0s in the numerical features \n#Numerical missing values\nfor col in NAcols:\n    if c[col].dtype != \"object\":\n        c[col]= c[col].fillna(0)","21bf59a4":"c.isnull().sum().sort_values(ascending=False).head()","e6dcc636":"\n\n\nFillNA=c[allna.index.to_list()]\n\n\n\nFillNAcat=FillNA.select_dtypes(include='object')\n\nFC= FillNAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFC= FC.style.background_gradient(cmap=cm)\nFC\n\n\n","6385c529":"FillNAnum=FillNA.select_dtypes(exclude='object')\n\nFM= FillNAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFM= FM.style.background_gradient(cmap=cm)\nFM\n\n","f8dffd81":"# scatter plot","1a61d778":" We isolate the missing values from the rest of the dataset to have a good idea of how to treat them ","5414d116":"# 2- Data cleaning","0cdd221b":"Clean and Edit Dataframes\nWe must combine train and test datasets. Because This processes are must be carried out together","24cfdaea":"But before going any further, we start by cleaning the data from missing values. I set the threshold to 80% (red line), all columns with more than 80% missing values will be dropped.","be3fdb18":"# Numerical features:","350e5dec":"And we have 18 Categorical features with missing values:\nSome features have just 1 or 2 missing values, so we will just use the forward fill method because they are obviously values that can't be filled with 'None's Features with many missing values are mostly basement and garage related (same as in numerical features) so as we did with numerical features (filling them with 0s), we will fill the categorical missing values with \"None\"s assuming that the houses lack basements and garages","6606e9aa":"First thing to do is get rid of the features with more than 80% missing values (figure above). \nFor example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 80%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 80% missing values.","5e06d08d":"The table above helps us to locate the categorical features with few missing values.\n\nWe start our cleaning with the features having just few missing value (1 to 4): We fill the gap with forward fill method:","0271cc5f":"#  Now what do we do in combine data that contains less than 80% missing values ","7ebda160":"# split training data into numeric and categorical data","123484e2":"#  Categorical features:","84bed18d":"# Before  compelete cleaning the data, we zoom at the features with missing values, those missing values won't be treated equally. Some features have barely 1 or 2 missing values, we will use the forward fill method to fill them.","39f7ae26":"We dealt already with small missing values or values that can't be filled with \"0\" such as Garage year built.\nThe rest of the features are mostly basement and garage related with 100s of missing values, \nwe will just fill 0s in the numerical features and 'None' in categorical features, assuming that the houses don't have basements, full bathrooms or garage","cc7122f8":"# Missing values percentage per column with less than 80 % ","f6a4eed5":"## numerical features correlation","da9df4ab":"Number of missing values per column in  Categorical features after the drop missing values with > 80%","fef77541":"### before the cleaning data we combine training and test data in order to remain keep the same structure","43b09858":"So, 18 categorical features and 10 numerical features to clean.\n\nWe start with the numerical features, first thing to do is have a look at them to learn more about their distribution and decide how to clean them:\nMost of the features are going to be filled with 0s because we assume that they don't exist, for example GarageArea, GarageCars with missing values are simply because the house lacks a garage.\nGarageYrBlt: Year garage was built can't be filled with 0s, so we fill with the median (1980).","0b77d459":"correlation_train=train.corr()\nsb.set(font_scale=2)\nplt.figure(figsize = (50,35))\nax = sb.heatmap(correlation_train, annot=True,annot_kws={\"size\": 25},fmt='.1f',cmap='PiYG', linewidths=.5)","9e49548b":"# **1- Exploratory data analysis**\n","92ef2710":" # Descriptive statistics ","6fbc8a7c":"We split them to:\n\n* Categorical features\n* Numerical features","fdf9d3cc":" Features with >80% missing values , we will drop ","1cd99e60":"# Correlation Matrix"}}