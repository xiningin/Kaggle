{"cell_type":{"f1f62ff1":"code","d52ba83f":"code","7f8eb94f":"code","e82249e9":"code","977c54cd":"code","45360a3b":"code","a2712d25":"code","43a27f04":"code","cec2b924":"code","20452dfc":"code","917a5f38":"code","4a441794":"code","3d623880":"code","fb284ae3":"code","21c93df6":"code","abf33124":"code","10a258f6":"code","a0f72c19":"code","a6914033":"code","d290a67c":"code","301c3df6":"code","d3a2b45e":"code","fe6d0e31":"code","c559facc":"code","3728cdba":"code","0c7bd847":"markdown","c86baf81":"markdown","bff8c823":"markdown","c621252a":"markdown","1617ce19":"markdown","2f9517fa":"markdown","86e47938":"markdown","3cf11f30":"markdown","acb0a758":"markdown","691bb931":"markdown","ff6c4ab3":"markdown","27ac23bf":"markdown","f2589fdb":"markdown","c76b0f94":"markdown","af1ce006":"markdown","3177612a":"markdown","441cf203":"markdown","02d00044":"markdown","b40cfaec":"markdown","c6191785":"markdown","c831df77":"markdown"},"source":{"f1f62ff1":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport math\nimport seaborn as sn\nimport albumentations as A\nimport tensorflow as tf\nfrom tensorflow.keras.applications import mobilenet_v2 as tf_mobilenet_v2\nfrom tensorflow.keras import layers as tf_layers\nfrom tensorflow.keras import models as tf_models\nfrom tensorflow.keras import callbacks as tf_callbacks\nfrom sklearn import metrics as sk_metrics\nfrom IPython.display import YouTubeVideo","d52ba83f":"YouTubeVideo('7vtpUklKlsk', width=800, height=450)","7f8eb94f":"DATASET_PATH = '..\/input\/lego-minifigures-classification'\n\ndf_index = pd.read_csv(os.path.join(DATASET_PATH, 'index.csv'), index_col=0)\ndf_metadata = pd.read_csv(os.path.join(DATASET_PATH, 'metadata.csv'), index_col=0)\ndf_index = pd.merge(df_index, df_metadata[['class_id', 'minifigure_name']], on='class_id')\n\ndf_index","e82249e9":"YouTubeVideo('iedmZlFxjfA', width=800, height=450)","977c54cd":"ax = df_index['minifigure_name'].value_counts().plot(\n    kind='bar',\n    figsize=(14,8),\n    title=\"Count of each mini-figure\",\n)\n\nax.set_xlabel(\"Mini-figure\")\nax.set_ylabel(\"Count\")\nplt.show()","45360a3b":"YouTubeVideo('Ql8QPcp8818', width=800, height=450)","a2712d25":"plt.figure(figsize=(16, 10))\nfor ind, el in enumerate(df_index.sample(15).iterrows(), 1):\n    plt.subplot(3, 5, ind)\n    image = cv2.imread(os.path.join(DATASET_PATH, el[1]['path']))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.title(f\"{el[1]['class_id']}: {el[1]['minifigure_name']}\")\n    plt.xticks([])\n    plt.yticks([])","43a27f04":"plt.figure(figsize=(16, 10))\nfor ind, el in enumerate(df_index[df_index['minifigure_name']=='SPIDER-MAN'].sample(15).iterrows(), 1):\n    plt.subplot(3, 5, ind)\n    image = cv2.imread(os.path.join(DATASET_PATH, el[1]['path']))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.xticks([])\n    plt.yticks([])","cec2b924":"YouTubeVideo('oy5EeamF_M8', width=800, height=450)","20452dfc":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(\n        self, \n        paths, \n        targets, \n        image_size=(224, 224), \n        batch_size=64, \n        shuffle=True, \n        transforms=None\n    ):\n        # the list of paths to files\n        self.paths = paths\n        # the list with the true labels of each file\n        self.targets = targets\n        # images size\n        self.image_size = image_size\n        # batch size (the number of images)\n        self.batch_size = batch_size\n        # if we need to shuffle order of files\n        # for validation we don't need to shuffle, for training - do\n        self.shuffle = shuffle\n        # Augmentations for our images. It is implemented with albumentations library\n        self.transforms = transforms\n        \n        # Call function to create and shuffle (if needed) indices of files\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        # This function is called at the end of each epoch while training\n        \n        # Create as many indices as many files we have\n        self.indexes = np.arange(len(self.paths))\n        # Shuffle them if needed\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        # We need that this function returns the number of steps in one epoch\n        \n        # How many batches we have\n        return len(self.paths) \/\/ self.batch_size\n    \n    \n    def __getitem__(self, index):\n        # This function returns batch of pictures with their labels\n        \n        # Take in order as many indices as our batch size is\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        # Take image file paths that are included in that batch\n        batch_paths = [self.paths[k] for k in indexes]\n        # Take labels for each image\n        batch_y = [self.targets[k] - 1 for k in indexes]\n        batch_X = []\n        for i in range(self.batch_size):\n            # Read the image\n            img = cv2.imread(batch_paths[i])\n            # Resize it to needed shape\n            img = cv2.resize(img, self.image_size)\n            # Convert image colors from BGR to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            # Normalize image\n            img = img \/ 255.\n            # Apply transforms (see albumentations library)\n            if self.transforms:\n                img = self.transforms(image=img)['image']\n            \n            batch_X.append(img)\n            \n        return np.array(batch_X), np.array(batch_y)","917a5f38":"YouTubeVideo('hxLU32zhze0', width=800, height=450)","4a441794":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.Rotate(limit=30, border_mode=cv2.BORDER_REPLICATE, p=0.5),\n            A.Cutout(num_holes=8, max_h_size=20, max_w_size=20, fill_value=0, p=0.5),\n            A.Cutout(num_holes=8, max_h_size=20, max_w_size=20, fill_value=1, p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.RandomContrast(p=0.5),\n            A.Blur(p=0.5),\n        ], \n        p=1.0\n    )","3d623880":"BASE_DIR = '..\/input\/lego-minifigures-classification\/'\n\n# Read information about dataset\ndf = pd.read_csv('..\/input\/lego-minifigures-classification\/index.csv', index_col=0)\n\n# Get only train rows\ntmp_train = df[df['train-valid'] == 'train']\n# Get train file paths\ntrain_paths = tmp_train['path'].values\n# Get train labels\ntrain_targets = tmp_train['class_id'].values\n# Create full train paths (base dir + concrete file)\ntrain_paths = list(map(lambda x: os.path.join(BASE_DIR, x), train_paths))\n\n# Get only valid rows\ntmp_valid = df[df['train-valid'] == 'valid']\n# Get valid file paths\nvalid_paths = tmp_valid['path'].values\n# Get valid labels\nvalid_targets = tmp_valid['class_id'].values\n# Create full valid paths (base dir + concrete file)\nvalid_paths = list(map(lambda x: os.path.join(BASE_DIR, x), valid_paths))","fb284ae3":"IMAGE_SIZE = (512, 512)\n\nTRAIN_BATCH_SIZE = 4\n\nVALID_BATCH_SIZE = 1 \n\n# Initialize the train data generator\ntrain_generator = DataGenerator(\n    train_paths, \n    train_targets, \n    batch_size=TRAIN_BATCH_SIZE, \n    image_size=IMAGE_SIZE,\n    shuffle=True, \n    transforms=get_train_transforms()\n)\n\n# Initialize the valid data generator\nvalid_generator = DataGenerator(\n    valid_paths, \n    valid_targets, \n    image_size=IMAGE_SIZE,\n    batch_size=VALID_BATCH_SIZE, \n    shuffle=False,\n)","21c93df6":"YouTubeVideo('OO4HD-1wRN8', width=800, height=450)","abf33124":"# We define the number of classes\nN_CLASSES = 22\n\n# We take pretrained MobileNetV2 (see Keras docs)\nbase_model = tf_mobilenet_v2.MobileNetV2()\n# Take penultimate layer of the MobileNetV2 model and connect this layer with Dropout\nx = tf_layers.Dropout(.5)(base_model.layers[-2].output)\n# Add additional Dense layer, with number of neurons as number of our classes\n# Use softmax activation because we have one class classification problem\noutputs = tf_layers.Dense(N_CLASSES, activation='softmax')(x)\n# Create model using MobileNetV2 input and our created output\nmodel = tf_models.Model(base_model.inputs, outputs)\n\n\n# Compile model using Adam optimizer and categorical crossentropy loss\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.0001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","10a258f6":"# checkpoint to saving the best model by validation loss\ncallback_save = tf_callbacks.ModelCheckpoint(\n    'best.hdf5',\n    monitor=\"val_loss\",\n    save_best_only=True,\n    mode=\"min\",\n)\n\n# checkpoint to stop training if model didn't improve valid loss for 3 epochs\ncallback_early_stopping = tf_callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=3,\n)","a0f72c19":"EPOCHS = 25\n\n# Train model using data generators\nhistory = model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=EPOCHS,\n    callbacks=[\n        callback_save, \n        callback_early_stopping\n    ],\n)","a6914033":"YouTubeVideo('apmNSYWEEnw', width=800, height=450)","d290a67c":"plt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='valid loss')\nplt.grid()\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='train acc')\nplt.plot(history.history['val_accuracy'], label='valid acc')\nplt.grid()\nplt.legend();","301c3df6":"model = tf_models.load_model('best.hdf5')\n\ny_pred = []\ny_valid = []\nfor _X_valid, _y_valid in valid_generator:\n    y_pred.extend(model.predict(_X_valid).argmax(axis=-1))\n    y_valid.extend(_y_valid)\n\nprint(f'Accuracy score on validation data:  {sk_metrics.accuracy_score(y_valid, y_pred)}')\nprint(f'Macro F1 score on validation data:  {sk_metrics.f1_score(y_valid, y_pred, average=\"macro\")}')","d3a2b45e":"YouTubeVideo('Kdsp6soqA7o', width=800, height=450)","fe6d0e31":"df_metadata = pd.read_csv('..\/input\/lego-minifigures-classification\/metadata.csv')\nlabels = df_metadata['minifigure_name'].tolist()\n\nconfusion_matrix = sk_metrics.confusion_matrix(y_valid, y_pred)\ndf_confusion_matrix = pd.DataFrame(confusion_matrix, index=labels, columns=labels)\nplt.figure(figsize=(12, 12))\nsn.heatmap(df_confusion_matrix, annot=True, cbar=False);","c559facc":"true_images = []\ntrue_label = []\ntrue_pred = []\n\nfor _X_valid, _y_valid in valid_generator:\n    pred = model.predict(_X_valid).argmax(axis=-1)\n    if pred[0] == _y_valid:\n        true_images.extend(_X_valid)\n        true_label.extend(_y_valid)\n        true_pred.extend(pred)\n\ntrue_images = true_images[:4]\n\nfor ind, image in enumerate(true_images):\n    plt.subplot(math.ceil(len(true_images) \/ int(len(true_images) ** 0.5)), int(len(true_images) ** 0.5), ind + 1)\n    plt.imshow(image)\n    plt.title(f'Predicted: {labels[true_pred[ind]]} | Real: {labels[true_label[ind]]}')\n    plt.axis('off')","3728cdba":"error_images = []\nerror_label = []\nerror_pred = []\n\nfor _X_valid, _y_valid in valid_generator:\n    pred = model.predict(_X_valid).argmax(axis=-1)\n    if pred[0] != _y_valid:\n        error_images.extend(_X_valid)\n        error_label.extend(_y_valid)\n        error_pred.extend(pred)\n\nerror_images = error_images[:4]\n\nfor ind, image in enumerate(error_images):\n    plt.subplot(math.ceil(len(error_images) \/ int(len(error_images) ** 0.5)), int(len(error_images) ** 0.5), ind + 1)\n    plt.imshow(image)\n    plt.title(f'Predicted: {labels[error_pred[ind]]} | Real: {labels[error_label[ind]]}')\n    plt.axis('off')","0c7bd847":"# Training the model","c86baf81":"# Final Validation","bff8c823":"# Confusion Matrix","c621252a":"# Accuracy \/ Loss visualization","1617ce19":"# Exploratory data analysis (EDA)","2f9517fa":"# Data Augmentation","86e47938":"**In this guide, I will provide a full tutorial on how to do EDA \/ apply MobilNetV2 model to classify 22 Lego mini-figure classes, please put an upvote if you like this notebook**","3cf11f30":"# Model Definition (MobilNetV2)","acb0a758":"# Importing libraries","691bb931":"I was highly inspired by the following notebooks by @ihelon\n1. [LEGO Minifigures - EDA](https:\/\/www.kaggle.com\/ihelon\/lego-minifigures-eda\/notebook)\n2. [LEGO Minifigures - TF Modeling](https:\/\/www.kaggle.com\/ihelon\/lego-minifigures-tf-modeling)","ff6c4ab3":"# Data Generator Class","27ac23bf":"Visualizing number of images per unique minifigure_name","f2589fdb":"# Visualizing wrong predictions","c76b0f94":"Visualizing image examples for class \"Spider-Man'","af1ce006":"# Reference","3177612a":"# *** LEGO Minifigures EDA + Classification (97% Accuracy) [Beginner-Friendly] ***\n# ![LEGO](https:\/\/www.kindpng.com\/picc\/m\/115-1158040_lego-figures-png-transparent-lego-characters-png-png.png)","441cf203":"# Visualizing good predictions","02d00044":"# Importing Dataset","b40cfaec":"> 1. Importing libraries\n> 2. Importing Dataset\n> 3. Exploratory Data Analysis (EDA)\n> 4. Data Generator Class\n> 5. Data Augmentation\n> 6. Train\/Test splitting \/ Generators\n> 7. Model Definition (MobilNetV2)\n> 8. Training the model\n> 9. Accuracy \/ Loss visualization\n> 10. Final validation\n> 11. Confusion Matrix\n> 12. Visualizing good predictions\n> 13. Visualizing wrong predictions\n> 14. Reference","c6191785":"Visualizing image examples","c831df77":"# Train-Test Splitting \/ Generators"}}