{"cell_type":{"0a1aaf07":"code","7f7da752":"code","76431cb2":"code","3ad06b11":"code","0adee0fb":"code","aecdb87d":"code","6cecfe71":"code","edf26ee1":"code","308f4f55":"code","175cb6ae":"code","9e60364f":"code","ad11e360":"code","624372c8":"code","d391bd79":"code","f93ffdfa":"code","7e1a261c":"code","37d1bbf9":"code","b36b30d5":"code","25bf073f":"code","38828e5c":"code","389f4abd":"code","f9555080":"code","792a0bd6":"code","8f3a4fe2":"code","c82aec27":"code","982e0bf7":"code","20b8e86d":"markdown","2e093c08":"markdown","ca19e3ed":"markdown","dfc32c4d":"markdown","b499417e":"markdown","fe344e52":"markdown","f0870505":"markdown","0d9d32ce":"markdown","22308d60":"markdown","51949135":"markdown","b63d466a":"markdown","4c08c2dc":"markdown","2e2d6626":"markdown","d7b9782c":"markdown","a8b31c9b":"markdown"},"source":{"0a1aaf07":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport albumentations # Library to do augmentation on images\nimport time\nimport os\nimport PIL\nfrom PIL import Image\n\n# Graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Sk Learn\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n#SciKit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics, model_selection\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error","7f7da752":"df = pd.read_csv(\"..\/input\/bostonhoustingmlnd\/housing.csv\")\ndf.head()","76431cb2":"df.info()","3ad06b11":"y = df[\"MEDV\"]\ndf.drop(columns=\"MEDV\", axis=1, inplace=True)\nX = df\n\nX.shape, y.shape","0adee0fb":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    shuffle=True\n)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","aecdb87d":"# Standard version\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Min Max version\nscaler2 = MinMaxScaler()\nscaler2.fit(X_train)\nXm_train = scaler2.transform(X_train)\nXm_test = scaler2.transform(X_test)","6cecfe71":"X_train, Xm_train","edf26ee1":"params = [{\n    \"hidden_layer_sizes\": [(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (10,), (12,), (50,), (100,)],\n    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n}]","308f4f55":"model = GridSearchCV(\n    MLPRegressor(\n        solver='lbfgs', \n        alpha=0.0001, \n        max_iter=10000, \n        random_state=0, \n        max_fun=15000\n    ),\n    params,\n    n_jobs=-1,\n    verbose=8\n)\n\nmodel.fit(X_train, y_train)","175cb6ae":"model.best_params_","9e60364f":"model.best_score_","ad11e360":"params_m = [{\n    \"hidden_layer_sizes\": [(1,), (4,), (5,), (7,), (10,), (59,), (60,), (61,), (62,), (63,), (64,), (65,), (32, 16), (64, 32)],\n    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n}]","624372c8":"model_m = GridSearchCV(\n    MLPRegressor(\n        solver='lbfgs', \n        alpha=0.0001, \n        max_iter=10000, \n        random_state=0, \n        max_fun=15000\n    ),\n    params_m,\n    n_jobs=-1,\n    verbose=8\n)\n\nmodel_m.fit(Xm_train, y_train)","d391bd79":"model_m.best_params_","f93ffdfa":"model_m.best_score_","7e1a261c":"y_train_pred = model.predict(X_train)","37d1bbf9":"RMSE_train = mean_squared_error(y_train, y_train_pred, squared=False)\nRMSE_train","b36b30d5":"[a, b] = np.polyfit(y_train, y_train_pred, 1)\nmodel_lin_train = np.polyfit(y_train, y_train_pred, 1)\npredict_train = np.poly1d(model_lin_train)\ny_lin_train = predict_train(y_train)\n\ncorr_train = np.corrcoef(y_train, y_train_pred)[0,1]\nR2_train = corr_train ** 2\nR2_train","25bf073f":"plt.figure(figsize=(16, 9))\n\nplt.scatter(y_train, y_train_pred)\nplt.plot(y_train, y_lin_train, 'orange', label='y = {:.2}x + {:.6}'.format(a, b))\nplt.plot(y_train, y_train, color='red')\n\nplt.title('Boston Housing Training set, R$^2$ = {:.3}'.format(R2_train), size=25)\nplt.xlabel('y_train', size=20)\nplt.ylabel('y_pred_train', size=20)\n\nplt.legend()\nplt.show()","38828e5c":"ym_train_pred = model_m.predict(Xm_train)\n\nRMSE_m_train = mean_squared_error(y_train, ym_train_pred, squared=False)\nRMSE_m_train","389f4abd":"[c, d] = np.polyfit(y_train, ym_train_pred, 1)\nmodel_m_lin_train = np.polyfit(y_train, ym_train_pred, 1)\npredict_train_m = np.poly1d(model_m_lin_train)\nym_lin_train = predict_train_m(y_train)\n\ncorr_train_m = np.corrcoef(y_train, ym_train_pred)[0,1]\nR2_train_m = corr_train_m ** 2\nR2_train_m","f9555080":"plt.figure(figsize=(16, 9))\n\nplt.scatter(y_train, ym_train_pred)\nplt.plot(y_train, ym_lin_train, 'orange', label='y = {:.2}x + {:.6}'.format(c, d))\nplt.plot(y_train, y_train, color='red')\n\nplt.title('Boston Housing Training set, R$^2$ = {:.3}'.format(R2_train_m), size=25)\nplt.xlabel('y_train', size=20)\nplt.ylabel('y_pred_train', size=20)\n\nplt.legend()\nplt.show()","792a0bd6":"y_test_pred = model.predict(X_test)\n\nRMSE_test = mean_squared_error(y_test, y_test_pred, squared=False)\nRMSE_test","8f3a4fe2":"[e, f] = np.polyfit(y_test, y_test_pred, 1)\nmodel_lin_test = np.polyfit(y_test, y_test_pred, 1)\npredict_test = np.poly1d(model_lin_test)\ny_lin_test = predict_test(y_test)\n\ncorr_test = np.corrcoef(y_test, y_test_pred)[0,1]\nR2_test = corr_test ** 2\n\n\nplt.figure(figsize=(16, 9))\nplt.scatter(y_test, y_test_pred)\nplt.plot(y_test, y_lin_test, 'orange', label='y = {:.2}x + {:.6}'.format(e, f))\nplt.plot(y_test, y_test, color='red')\n\nplt.title('Boston Housing Test set, R$^2$ = {:.4}'.format(R2_test), size=25)\nplt.xlabel('y_test', size=20)\nplt.ylabel('y_pred_test', size=20)\n\nplt.legend()\nplt.show()","c82aec27":"ym_test_pred = model_m.predict(Xm_test)\n\nRMSE_m_test = mean_squared_error(y_test, ym_test_pred, squared=False)\nRMSE_m_test","982e0bf7":"[g, h] = np.polyfit(y_test, ym_test_pred, 1)\nmodel_lin_m_test = np.polyfit(y_test, ym_test_pred, 1)\npredict_m_test = np.poly1d(model_lin_m_test)\nym_lin_test = predict_m_test(y_test)\n\ncorr_m_test = np.corrcoef(y_test, ym_test_pred)[0,1]\nR2_m_test = corr_m_test ** 2\n\n\nplt.figure(figsize=(16, 9))\nplt.scatter(y_test, ym_test_pred)\nplt.plot(y_test, ym_lin_test, 'orange', label='y = {:.2}x + {:.6}'.format(g, h))\nplt.plot(y_test, y_test, color='red')\n\nplt.title('Boston Housing Test set, R$^2$ = {:.4}'.format(R2_m_test), size=25)\nplt.xlabel('y_test', size=20)\nplt.ylabel('y_pred_test', size=20)\n\nplt.legend()\nplt.show()","20b8e86d":"---\n\n## Performance on the Training Set","2e093c08":"**Notes**\n* 1st Version:\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 50 --> 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 50, 100\n* 2nd Version:\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 60\n* 3rd Version:\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 62  --> 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, (32, 16), (64, 32)","ca19e3ed":"**Notes**\n* 1st Version:\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 10 --> 1, 2, 3, 5, 10\n* 2nd Version:\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 10 --> 5, 10, 40, 80, 100\n* 3rd Version with R^2 score!\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 5  --> 1, 2, 3, 5, 8, 10, 12, 15, 20\n* 4th Version with R^2 score!\n    * 'activation': 'relu', \n    * 'hidden_layer_sizes': 5 --> 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 50, 100\n* 5th Version:\n    * I took out the scoring R^2 of the GridSearch to verify which score it takes by default\n    * Kept the same parameters as 4th Version.\n    * Kept the same `best_score_`","dfc32c4d":"---\n\n## Loading Dataset","b499417e":"### Min Max Scaled Version","fe344e52":"---\n\n## Conclusion\n\n* Our best model shows a RMSE of 62k on the Test Set\n* The best model looks to be the one scaled using MinMaxScaler","f0870505":"### Graph","0d9d32ce":"---\n\n## Import Librairies","22308d60":"---\n\n## Training","51949135":"---\n\n## Preprocessing","b63d466a":"---\n\n## Performance on the Test Set\n\n### Standard Scaled Version","4c08c2dc":"---\n\n## Split the Dataset","2e2d6626":"### Training the MinMax Version","d7b9782c":"### Standard Scaler","a8b31c9b":"### Min Max Scaler"}}