{"cell_type":{"4cdfda34":"code","69217976":"code","0db33a43":"code","d1117b25":"code","96f772b8":"code","e8067850":"code","97dbc2f5":"code","1b389345":"code","2656060f":"code","8c1c02b8":"code","e8b094f5":"code","279ab3cc":"code","ae2e6b48":"code","eca8f9b8":"code","cc074045":"code","79e73f8a":"code","05a93f1c":"code","e7991efc":"code","296e2b2e":"code","49c7052c":"code","c589837c":"code","4d4b26f8":"code","971b3ce1":"code","8656591d":"code","3d52fa0a":"code","6a388787":"code","5890ff2d":"code","2dfcbf98":"code","84e94f47":"code","53a545ba":"code","3be92566":"code","295a366b":"code","01c5e379":"markdown","d37d9692":"markdown","f70e1286":"markdown","449a6353":"markdown","605e39af":"markdown","b1ab66f5":"markdown","c7f4de19":"markdown","6b3570c1":"markdown","fdf08d3f":"markdown","7fa1a2aa":"markdown"},"source":{"4cdfda34":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","69217976":"BASE_PATH = \"\/kaggle\/input\/ham-and-spam-dataset\/\"\nHAM_DIR = os.path.join(BASE_PATH, \"ham\")\nSPAM_DIR = os.path.join(BASE_PATH, \"spam\")\nham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]","0db33a43":"len(ham_filenames)","d1117b25":"len(spam_filenames)","96f772b8":"import email\nimport email.policy","e8067850":"def load_email(is_spam, filename, spam_path=BASE_PATH):\n    directory = \"spam\" if is_spam else \"ham\"\n    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)","97dbc2f5":"ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]","1b389345":"# ham\nprint(ham_emails[1].get_content().strip())","2656060f":"# spam \nprint(spam_emails[6].get_content().strip())","8c1c02b8":"def get_email_structure(email):\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ]))\n    else:\n        return email.get_content_type()","e8b094f5":"from collections import Counter\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures","279ab3cc":"# Most common HAM email structure\nstructures_counter(ham_emails).most_common()","ae2e6b48":"# Most common SPAM email structure\nstructures_counter(spam_emails).most_common()","eca8f9b8":"# SPAM email header example\nspam_emails[4][\"Subject\"]","cc074045":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","79e73f8a":"import re\nfrom html import unescape\n\n# Helper to convert HTML to plain text\n\ndef html_to_plain_text(html):\n    text = re.sub('<head.*?>.*?<\/head>', '', html, flags=re.M | re.S | re.I)\n    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n    return unescape(text)","05a93f1c":"# Before converting to plain text\nhtml_spam_emails = [email for email in X_train[y_train==1]\n                    if get_email_structure(email) == \"text\/html\"]\nsample_html_spam = html_spam_emails[7]\nprint(sample_html_spam.get_content().strip()[:1000], \"...\")","e7991efc":"# After converting to plain text\nprint(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")","296e2b2e":"# Helper to convert email to plain text\n\ndef email_to_text(email):\n    html = None\n    for part in email.walk():\n        ctype = part.get_content_type()\n        if not ctype in (\"text\/plain\", \"text\/html\"):\n            continue\n        try:\n            content = part.get_content()\n        except: # in case of encoding issues\n            content = str(part.get_payload())\n        if ctype == \"text\/plain\":\n            return content\n        else:\n            html = content\n    if html:\n        return html_to_plain_text(html)","49c7052c":"# Test\nprint(email_to_text(sample_html_spam)[:100], \"...\")","c589837c":"try:\n    import nltk\n\n    stemmer = nltk.PorterStemmer()\n    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n        print(word, \"=>\", stemmer.stem(word))\nexcept ImportError:\n    print(\"Error: stemming requires the NLTK module.\")\n    stemmer = None","4d4b26f8":"# we just pip install urlextract\ntry:\n    !pip install -q -U urlextract\nexcept ImportError:\n    pass ","971b3ce1":"try:\n    import urlextract # may require an Internet connection to download root domain names\n    \n    url_extractor = urlextract.URLExtract()\n    print(url_extractor.find_urls(\"Will it detect github.com and https:\/\/youtu.be\/7Pq-S557XQU?t=3m32s\"))\nexcept ImportError:\n    print(\"Error: replacing URLs requires the urlextract module.\")\n    url_extractor = None","8656591d":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n                 replace_urls=True, replace_numbers=True, stemming=True):\n        self.strip_headers = strip_headers\n        self.lower_case = lower_case\n        self.remove_punctuation = remove_punctuation\n        self.replace_urls = replace_urls\n        self.replace_numbers = replace_numbers\n        self.stemming = stemming\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_transformed = []\n        for email in X:\n            text = email_to_text(email) or \"\"\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_urls and url_extractor is not None:\n                urls = list(set(url_extractor.find_urls(text)))\n                urls.sort(key=lambda url: len(url), reverse=True)\n                for url in urls:\n                    text = text.replace(url, \" URL \")\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            word_counts = Counter(text.split())\n            if self.stemming and stemmer is not None:\n                stemmed_word_counts = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_counts[stemmed_word] += count\n                word_counts = stemmed_word_counts\n            X_transformed.append(word_counts)\n        return np.array(X_transformed)","3d52fa0a":"# Test on a few emails\nX_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\nX_few_wordcounts","6a388787":"from scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10)\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","5890ff2d":"vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\nX_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\nX_few_vectors","2dfcbf98":"X_few_vectors.toarray()","84e94f47":"vocab_transformer.vocabulary_","53a545ba":"from sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\n\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)","3be92566":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nscore.mean()","295a366b":"from sklearn.metrics import precision_score, recall_score\n\nX_test_transformed = preprocess_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nlog_clf.fit(X_train_transformed, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))","01c5e379":"#### Split Data into Training and Test Sets","d37d9692":"#### Build Pipeline","f70e1286":"In the array above, the second row represents the second email and the first column has the value 116 which shows there are 116 words that are not part of the vocabulary. The 9 implies that the first word in the vocabulary is present 9 times and so on","449a6353":"We get a **98.9%** accuracy on classifying SPAM :D","605e39af":"#### Create Transformer\n","b1ab66f5":"Let's use the `email` module to parse the emails and get info like headers, encoding, etc","c7f4de19":"#### Deciphering Email Structure and Content\nSometimes, emails are multipart and have images and attachments and so on","6b3570c1":"Here's an example of a **ham** email and a **spam** email","fdf08d3f":"## Disclaimer & Credits\n\nIn this kernel, I have attempted to re-implement the code for the third chapter of **Aur\u00e9lien G\u00e9ron's** amazing book [Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow](https:\/\/github.com\/ageron\/handson-ml2). You can find his detailed jupyter notebooks for each chapter in the link mentioned before. This notebook is primarily a way for me to internalize the content shared in each chapter of the book, and I hope it is useful to you. \n\n\n**Note:** _The code and content here is contained in the notebooks linked above. I have done my best not to include anything present in his book but not present in the notebooks._","7fa1a2aa":"### 4. Build a SPAM Classifier"}}