{"cell_type":{"1c9bc90a":"code","e1f9f9d3":"code","03664cca":"code","793e3424":"code","c23c2335":"code","9456a9e0":"code","83ec9d85":"code","237a2c0e":"code","6f10d8ea":"code","ff538a6a":"code","a2b4dd0e":"code","8545ddbf":"code","0c3c7609":"code","eca2f323":"code","cfaa2d69":"code","46f76a18":"code","f5c2dab7":"code","172c05b4":"code","47d7dffa":"code","89ffba8a":"code","72bb280d":"code","69ed51bd":"code","66c446fb":"code","f98664e3":"code","adac6e10":"code","e2db83b7":"code","9996983c":"code","e0308732":"code","1c9418bc":"code","4d987a15":"code","47ec7e97":"markdown","e3f7a886":"markdown","f13ca69d":"markdown","1ca46095":"markdown","18036abc":"markdown","dd2521eb":"markdown","02adf345":"markdown","929bb54c":"markdown","3a8289d5":"markdown","d0c8fe63":"markdown","713f1be8":"markdown","ac851ee8":"markdown","e76235ea":"markdown","aa01c293":"markdown","9d509163":"markdown","67d3511c":"markdown","29391e0b":"markdown","698bca9e":"markdown","9231f1cb":"markdown","2e816817":"markdown","0216bd29":"markdown","f76de371":"markdown","716f1cdc":"markdown","85e411bf":"markdown","2311dea7":"markdown","facd4905":"markdown","8493ffb4":"markdown","d25ea43e":"markdown","a793c1fa":"markdown","5a9de62f":"markdown","5d742097":"markdown","909f21a2":"markdown","7a70118d":"markdown","a49ad536":"markdown","9fa53c20":"markdown","297c081e":"markdown","0fc46e17":"markdown","190e9ea3":"markdown"},"source":{"1c9bc90a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sn\n%matplotlib inline\npalette = sn.color_palette('Set3')\nsn.set(style='white', context='notebook')","e1f9f9d3":"srandom = 42","03664cca":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","793e3424":"train.head(3)","c23c2335":"X_train = train.drop(['label'], axis=1)\nY_train = train['label']\n\nsn.countplot(Y_train, palette=palette);","9456a9e0":"print('There are %d missing values.' % X_train.isna().sum().sum())","83ec9d85":"X_train = X_train.values.reshape(-1,28, 28,1)\ntest = test.values.reshape(-1,28, 28,1)","237a2c0e":"fig, ax = plt.subplots(4,4,figsize=(6, 6))\nfor i in range(4):\n    for j in range(4):\n        ax[i,j].imshow(X_train[np.random.randint(len(X_train))][:,:,0], cmap='gray_r')\n        ax[i,j].set_axis_off()","6f10d8ea":"X_train = X_train.astype('float32')\ntest = test.astype('float32')\n\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","ff538a6a":"from keras.utils.np_utils import to_categorical;","a2b4dd0e":"Y_train = to_categorical(Y_train, num_classes = 10)","8545ddbf":"from sklearn.model_selection import train_test_split","0c3c7609":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size= 0.1, random_state=srandom)","eca2f323":"from keras.layers import Conv2D, MaxPool2D,Flatten, Dense, Dropout\nfrom keras.models import Sequential","cfaa2d69":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu'))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\n    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation = 'relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(10, activation = 'softmax'))\n    return model","46f76a18":"model = create_model()","f5c2dab7":"from keras.optimizers import RMSprop","172c05b4":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","47d7dffa":"model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])","89ffba8a":"history = model.fit(X_train, Y_train, epochs=30, batch_size=63, validation_data = (X_val,Y_val), verbose=2)","72bb280d":"print('History object contains: %s' % history.history.keys())","69ed51bd":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\nax1.plot(history.history['acc'])\nax1.plot(history.history['val_acc'])\nax1.legend(['train', 'validation'], loc='best')\nax1.set(xlabel='Epoch', ylabel='Accuracy')\nax1.set_title('Model accuracy');\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nax2.legend(['train', 'validation'], loc='best')\nax2.set(xlabel='Epoch', ylabel='Loss')\nax2.set_title('Model loss');\n\n","66c446fb":"Y_predict = model.predict(test)","f98664e3":"plt.imshow(test[0][:,:,0], cmap='gray_r');","adac6e10":"Y_predict[0]","e2db83b7":"print('The first sample in the test set is: %d' % Y_predict[0].argmax())","9996983c":"Y_predict_class = np.argmax(Y_predict, axis=1)","e0308732":"np.arange(0,len(Y_predict_class)).shape","1c9418bc":"predictions = pd.DataFrame({'ImageId' : np.arange(1,len(Y_predict_class)+1), 'Label' : Y_predict_class})","4d987a15":"predictions.to_csv('mnist_cnn_keras.csv', index=False)","47ec7e97":"MNIST Handwritten Digits dataset is considered a \"Hello World!\" example in computer vision.\nThis kernel goes through creating a Convolutional Neural Network to classify handwritten digits based on MNIST dataset using Keras API.","e3f7a886":"Before going on with creating and training the model, we need to split our dataset into training and validation subsets in order to evaluate our model's performance:","f13ca69d":"## 3.2. Structure of CNN\nA Convolutional Neural Network model has two distinct parts:\n* Feature Extraction\n* Classification\nThe Feature Extraction is done in the convolutional (hence the name) and pooling layers.\nThe Classification part is made of densly connected layers (your simple usual neural networks).","1ca46095":"# 6. Training and evaluating the model","18036abc":"## 6.2. History object\nThe `fit()` method returns `History` object. The `History.history` attribute is a dictionary recording training and validation accuracy and loss values at successive epochs. We can what data this object holds by:","dd2521eb":"The counts of different labels are quite similar, so we don't need to worry about the problems skewed classes usually cause.","02adf345":"The output layer in our neural network as we will see later, has 10 nodes, with each node represents the probability of the input image representing any digit in the range `0\u20139`, in order to train our neural network we need to convert the labels in `Y_train` into 10-dimensional vector or what is called One Hot Encoding. For example a `4` will become:\n\n$4 = \\left[\n\\begin{array}{cccccccccc}\n0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\n\\end{array}\n\\right]$\n\nTo do so, we can use `to_categorical()` function in Keras:","929bb54c":"# 2. Loading, exploring and preparing data","3a8289d5":"The value at position `2` is the highest, obviously the input image represents number `2`.\nWe can use the function `argmax()` in `NumPy` package to find the position (hence, the class) of the maximum value directly:","d0c8fe63":"# 2.8. Label encoding","713f1be8":"## 2.3. Seperating images from labels\nFirst, we want to seperate the images from their corresponding labels in the training dataset, the images are stored in a variable called `X_train` and the target labels (the first column) are stored in `Y_train` then we plot the count of the different labels (how many images represent different digits from `0` to `9`) using Seaborn's function `countplot()`:","ac851ee8":"Let's look how a sample of this dataset looks like as an image of size `28\u00d728`:\n![](https:\/\/i.imgur.com\/sRtRGhL.png)","e76235ea":"We will use `argmax()` to find the classes of all our predicted values:","aa01c293":"The dataset contains grayscale images of handwritten digits, each row of the dataset represents an image (as a vector of `784 pixels` each pixel has a value between `0` and `255`) which we will reshape into 2D matrices to represent images of `28\u00d728`.\n\nEach row also contains the true label of the number represented in the image as a simple integer value from `0` to `9`.\n\nLet's see how the first 3 samples look like:","9d509163":"## 6.1. Fitting the model","67d3511c":"We will use Keras API (with TensorFlow backend) to create the model. First, we import the necessary modules:\n+ **Conv2D**: Creates a convolutional layer\n+ **MaxPool2D**: Creates a pooling layer\n+ **Flatten**: Converts the output of convolutional layer into a column vector to serve as an input for the classifier\n+ **Dense**: Creates fully-connected neural network (that is, the usual neural network structure used for classification)\n+ **Dropout**: Applies dropout to the input, dropout helps preventing the model from overfitting\n+ **Sequential**: This model enables us to create a linear stack of layers that we imported so far ","29391e0b":"## 2.5. Reshaping\nAs we said, the images in the dataset are rows\/vectors of numerical values that represent pixels.\nBefore continuing, we need to reshape every image as 2D matrix, or a grid of numerical values representing pixels (eventually, that's how images are supposed to be represented).\nIn order to do so we can use NumPy's function `reshape()` as follows:","698bca9e":"## 2.6. Peeking at the data\nNow let's have a look at some random samples:","9231f1cb":"## 3.1. What are Convolutional Neural Networks\nConvolutional Neural Networks are a special type neural networks which contains one or more convolutional layers (more on that in a moment).\nCNN proved their reliability when it comes to computer vision problems, especially image classification.","2e816817":"# 1. Introduction","0216bd29":"To finally train our model we call the function `fit()` and pass it the training images and corresponding true labels (`X_train` and `Y_train`).\n\n`epochs` specifies how many times the model goes through the whole dataset during the training process.\n\n`batch_size` specifies how many samples are used in each cycle of the training process.\n\n`validation_data` we pass the validation split we created earlier to validate our model. The training algorithm DOESN'T USE these samples for training.","f76de371":"## 3.4. Pooling layers\nPooling operations are another important part of Convolutional Neural Networks.\nJust like the convolution, pooling operation works by sliding a window through our input and performs one of different operations such as taking the average or maximum value of that window, what pooling does actually, is summarizing regions of our data to reduce its complexity.\nThe goal of pooling is to reduce the number of parameters and the amount of computation required.\n\nPooling operation is visualized as:\n\n<center>![pooling](https:\/\/i.imgur.com\/5eCRAwz.png)<\/center>\n<center>Figure 03: Computing the output values of 3\u00d73 max pooling operation on 5\u00d75 input ([source](https:\/\/arxiv.org\/abs\/1603.07285))<\/center>","716f1cdc":"Now we need to compile the sequential model we created, specifying the optimizer we chose and a loss function. Since this is a multiclass classification problem, the appropriate loss function is *categorical cross entropy*:","85e411bf":"The data is clean without any missing values","2311dea7":"# 4. Splitting the data","facd4905":"# 7. Making predictions\nThe last step would be, of course, to use our model to make predictions on the test data provided by Kaggle.\nFor that we use the `predict()` function and input the variable `test` holding our test data:","8493ffb4":"The predictions of our model on this exact sample is:","d25ea43e":"We need an optimizer to train our network, we will use RMSprop for that with learning rate of `0.001`:","a793c1fa":"## 2.7. Normalization\nNormalizing your data is an important step in all machine learning applications, it helps your optimization algorithm converge faster. For images we can simply divide every pixel by `255.0` (maximum value a pixel can have) which will bring every pixel value to the range `0.0\u20131.0`:","5a9de62f":"## 3.3. The convolution layers\nConvolutions are mathematical operations that are applied to images (which are stored in multidimensional arrays) in order to extract some features, thus convolutions preserve the relationship among adjacent pixels and extracts the features those pixels represent in the original image (e.g. horizontal lines, vertical lines, edges, \u2026) which wouldn't be the case if we used standard neural networks where such topological information aren't taken into account.\n\nHere is a visual example of a convolution operation:\n<center>\n    ![convolution](https:\/\/i.imgur.com\/hfuEZ8h.png)\n    <\/center>\n    <center>\n    Figure 01: Computing the output values of a discrete convolution ([source](https:\/\/arxiv.org\/abs\/1603.07285))\n    <\/center>\n<br\/>\nThe light blue grid is called the input feature map. The kernel (shaded area) slides across the feature map. At each location, the product between each element of the kernel and the input element it overlaps is computed and the results are summed up to obtain the output of the current location.\n<br\/>\n<br\/>\nHere is the animation of the convolution:\n<center>\n![convolution-animation](https:\/\/raw.githubusercontent.com\/vdumoulin\/conv_arithmetic\/master\/gif\/no_padding_no_strides.gif)\n<\/center>\n    <center>\n    Figure 02: Animated convolution ([source](https:\/\/github.com\/vdumoulin\/conv_arithmetic))\n    <\/center>","5d742097":"## 2.1. Loading the data\nWe start by loading the data using Pandas and `read_csv()` function","909f21a2":"# 5. Creating the model","7a70118d":"And if the arguments we passed to `reshape()` look unintuitive to you, the `-1` argument is shortcut\/equivalent to specifying the size of our dataset, or how many images we have and it can be replaced with `X_train.shape[0]` (returns the number of rows). `28, 28` means we want the `784` pixels to be represented instead as a grid of `28\u00d728` and the last argument `1` is the number of channels we have and since all images are grayscale representations we specify just one channel.","a49ad536":"## 6.3. Plotting accuracy and loss over epochs\nTo see how our model has improved over over time (or epochs), we can plot the different values of model accuracy and model loss for both training and validation data as a function of number of epochs:","9fa53c20":"## 2.4. Missing values\nWe check if there are any missing values in the training set","297c081e":"## 2.2. Describing the dataset","0fc46e17":"# 3. Convolutional Neural Networks","190e9ea3":"The output of the function `predict()` is a 10-dimensional vector (the number of our classes) holding the probability of the input being one of the output classes, think of it as: how much does the model think that the input image is one of the numbers from 0 to 9 (e.g. if the input is an image of `4` and our model predicted it right, the output vector holds 10 values with the value at position 4 is the highest).\n\n Let's see how the first sample in the test set looks like:"}}