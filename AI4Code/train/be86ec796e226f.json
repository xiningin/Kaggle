{"cell_type":{"b0cfeb98":"code","55b32636":"code","54560766":"code","818109ca":"code","3bf5a58f":"code","e9052fe1":"code","fa82adce":"code","efe33ea9":"code","fd150014":"code","93820d93":"code","8d9e91b0":"code","43df9d69":"code","a739c124":"code","6ae999ad":"code","23fe347b":"code","8f370ea9":"code","768ffab9":"code","64e0401a":"code","04029d6e":"code","fcb65c58":"code","9e5c7d79":"markdown","6f851b23":"markdown","a9953804":"markdown","1d7e59ad":"markdown","491e8c83":"markdown","95c2f6d4":"markdown","04239212":"markdown","4b69cf38":"markdown","660ee415":"markdown"},"source":{"b0cfeb98":"import os","55b32636":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54560766":"pip install nlpaug","818109ca":"pip install sklearn","3bf5a58f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as naf\n\nfrom tqdm import tqdm\n\nfrom nlpaug.util import Action\nfrom sklearn.model_selection import train_test_split","e9052fe1":"tweet= pd.read_csv('..\/input\/nlpgettingstarted\/train.csv')\ntest=pd.read_csv('..\/input\/nlpgettingstarted\/test.csv')","fa82adce":"tweet","efe33ea9":"test","fd150014":"tweet.shape","93820d93":"tweet=tweet.drop(['keyword','location'],axis=1)\nprint(tweet)","8d9e91b0":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","43df9d69":"# model_type: word2vec, glove or fasttext\naug_w2v = naw.WordEmbsAug(\n#     model_type='word2vec', model_path='..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin',\n    model_type='glove', model_path='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt',\n    action=\"substitute\")","a739c124":"text = tweet.iloc[0]['text']\ntext","6ae999ad":"aug_w2v.aug_p=0.2\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_w2v.augment(text)\n    print(augmented_text)","23fe347b":"train,valid=train_test_split(tweet,test_size=0.15)\nprint('Shape of train',train.shape)\nprint(\"Shape of Validation \",valid.shape)","8f370ea9":"from sklearn.utils import shuffle\ndef augment_text(df,samples=300,pr=0.2):\n    aug_w2v.aug_p=pr\n    new_text=[]\n    \n    ##dropping samples from validation\n    df_n=df[df.target==1].reset_index(drop=True)\n\n    ## data augmentation loop\n    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n        \n            text = df_n.iloc[i]['text']\n            augmented_text = aug_w2v.augment(text)\n            new_text.append(augmented_text)\n    \n    \n    ## dataframe\n    new=pd.DataFrame({'text':new_text,'target':1})\n    df=shuffle(df.append(new).reset_index(drop=True))\n    return df","768ffab9":"train = augment_text(train,samples=400)   ## change samples to 0 for no augmentation\ntweet = train.append(valid).reset_index(drop=True)","64e0401a":"df=pd.concat([tweet,test])","04029d6e":"df","fcb65c58":"df.shape","9e5c7d79":"# Predict which Tweets are about real disasters and which ones are not","6f851b23":"**This python library helps you with augmenting nlp for your machine learning projects.**\n*content source: https:\/\/neptune.ai\/blog\/data-augmentation-in-python*\n\n**What is Data Augmentation?**\nData Augmentation is a technique that can be used to artificially expand the size of a training set by creating modified data from the existing one. It is a good practice to use DA if you want to prevent overfitting, or the initial dataset is too small to train on, or even if you want to squeeze better performance from your model.\n\nLet\u2019s make this clear, Data Augmentation is not only used to prevent overfitting. In general, having a large dataset is crucial for the performance of both ML and Deep Learning (DL) models. However, we can improve the performance of the model by augmenting the data we already have. It means that Data Augmentation is also good for enhancing the model\u2019s performance.\n\nIn general, DA is frequently used when building a DL model. That is why throughout this article we will mostly talk about performing Data Augmentation with various DL frameworks. Still, you should keep in mind that you can augment the data for the ML problems as well.\n\nYou can augment:\n\n1.Audio\n2.Text\n3.Images\n4.Any other types of data\n\nWe will focus on image augmentations as those are the most popular ones. Nevertheless, augmenting other types of data is as efficient and easy. That is why it\u2019s good to remember some common techniques which can be performed to augment the data.\n\n**Data Augmentation techniques**\n\nWe can apply various changes to the initial data. For example, for **images** we can use:\n\n1. Geometric transformations \u2013 you can randomly flip, crop, rotate or translate images, and that is just the tip of the iceberg\n2. Color space transformations \u2013 change RGB color channels, intensify any color\n3. Kernel filters \u2013 sharpen or blur an image \n4. Random Erasing \u2013 delete a part of the initial image\n5. Mixing images \u2013 basically, mix images with one another. Might be counterintuitive but it works\n\n\n**For text there are:**\n\n1. Word\/sentence shuffling\n2. Word replacement \u2013 replace words with synonyms\n3. Syntax-tree manipulation \u2013 paraphrase the sentence to be grammatically correct using the same words\n   Other described in the article about Data Augmentation in NLP (https:\/\/neptune.ai\/blog\/data-augmentation-nlp)\n\nFor audio augmentation you can use:\n\n1. Noise injection\n2. Shifting\n3. Changing the speed of the tape\n4. And many more\n\nMoreover, the greatest advantage of the augmentation techniques is that you may use all of them at once. Thus, you may get plenty of unique samples of data from the initial one.","a9953804":"## Class distribution","1d7e59ad":"From the barplot we can see that there is class distribution!!! Tweets with no disaster(0) is more than tweets with diaster(1).","491e8c83":"## Loading and showing initial data","95c2f6d4":"**The OS module in Python provides functions for interacting with the operating system. OS comes under Python\u2019s standard utility modules. This module provides a portable way of using operating system dependent functionality. The *os* and *os.path* modules include many functions to interact with the file system.**","04239212":"Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\nSource: Wikipedia","4b69cf38":"Let's check class distribution.","660ee415":"## Data Augmentation"}}