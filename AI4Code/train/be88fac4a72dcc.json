{"cell_type":{"b55bfe1b":"code","77352eb4":"code","953c91d8":"code","f506b785":"code","9ed7b2ed":"code","4c3f9a0c":"code","ad1271a8":"code","cd81774b":"code","9798d545":"code","e9165e46":"code","0a02befd":"code","d52dd793":"code","c14c38c4":"code","eb264ca4":"code","fe06038d":"code","4835ef6a":"code","79e80d54":"code","d9d9757f":"code","d3fd859e":"code","eab8c317":"code","4fed7c2f":"code","788447ca":"code","13ae14a8":"code","47c24b2b":"code","8e16561c":"code","8fb040b7":"code","5948b8a9":"code","8e2cf3cb":"code","ee24fdfb":"code","8f9ceab1":"code","be5a229e":"code","bf024be7":"code","3b66fae6":"code","1811da48":"code","9e94900d":"code","e1dd450f":"code","49192098":"code","f74d67eb":"code","e25efb1e":"code","999df939":"code","bdeae2ef":"code","a914bde1":"code","0ad66fba":"code","d2ccc0e9":"code","97518605":"code","4c8c2c30":"code","4f82e9ab":"code","50d6c970":"code","e8bff132":"code","3c48528a":"code","1cb7bd61":"code","84307f43":"code","f4403752":"code","61c80811":"code","0aab52be":"code","47766fda":"code","98b2eda5":"code","6e9c5312":"code","1070f67f":"code","f7042468":"code","b6cbf0e8":"code","c162d3f7":"markdown","bc8cda21":"markdown","7e06fce3":"markdown","e66f17e5":"markdown","fc79b5bb":"markdown","4aa00719":"markdown","a79a0438":"markdown","f6704bcb":"markdown","103ae6ed":"markdown","414d93a6":"markdown"},"source":{"b55bfe1b":"import numpy as np \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n#Scikit-learn models\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \n# Scikit-learn metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics","77352eb4":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')","953c91d8":"df.head()","f506b785":"df.tail()","9ed7b2ed":"df.describe()","4c3f9a0c":"df.corr()","ad1271a8":"df.hist(figsize=(12,8));","cd81774b":"missing={\"missing\":df.isnull().sum(),\" % of missing\":round(((df.isnull().sum()\/df.shape[0])*100),2)}\npd.DataFrame(missing)","9798d545":"pH_nan_1 = df.query('Potability == 1')['ph'][df['ph'].isna()].index\n\ndf.loc[pH_nan_1,'ph'] =df.query('Potability == 1')['ph'][df['ph'].notna()].mean()\n\npH_nan_0 = df.query('Potability == 0')['ph'][df['ph'].isna()].index\ndf.loc[pH_nan_0,'ph'] = df.query('Potability == 0')['ph'][df['ph'].notna()].mean()","e9165e46":"Sulfate_nan_1 = df.query('Potability == 1')['Sulfate'][df['Sulfate'].isna()].index\ndf.loc[Sulfate_nan_1,'Sulfate'] =df.query('Potability == 1')['Sulfate'][df['Sulfate'].notna()].mean()\n\nSulfate_nan_0 = df.query('Potability == 0')['Sulfate'][df['Sulfate'].isna()].index\ndf.loc[Sulfate_nan_0,'Sulfate'] = df.query('Potability == 0')['Sulfate'][df['Sulfate'].notna()].mean()","0a02befd":"df=df.dropna(subset=[\"Trihalomethanes\"])","d52dd793":"df.isnull().sum()","c14c38c4":"Potability=df[\"Potability\"].value_counts()\nPotability","eb264ca4":"plt.pie(Potability,labels=[\"Non-potable\",\"potable\"],startangle=90,explode=[0.3,0])\nplt.show()","fe06038d":"# Feature correlation heat map\n# Get Pearson correlation values\ndata = df.corr()    # Pairwise correlation with a null value is ignored\n# Generate heat map using seaborn\nfig, ax = plt.subplots(figsize=(12,8))                          # Create grid of empty subplots using matplotlib library                      \nmask = np.triu(np.ones_like(data, dtype=bool))                   # Mask correlation matrix along its line of symmetry to remove redencency and correlation of a feature with itself\nsns.heatmap(data, cmap='seismic', annot=True, mask=mask, ax=ax, vmin=-0.2, vmax=0.2)    # Create heat map useing seaborn library\nfig.text(0.5, 1.05, 'Correlation Heat Map', horizontalalignment='center', verticalalignment='center', fontsize=14, fontweight='bold', transform=ax.transAxes)   # Add title\nsns.set_style('white')        # Remove tick marks","4835ef6a":"# Box Plots\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))  # Create empty grid of subplots\nfig.subplots_adjust(hspace=.5)                       # Adjust vertical\/height spacing \n\n# Fill each subplot with the distribution of a feature separated by potability\na=0                               # Increment subplot coordinates\nfor feature in df.drop('Potability', axis=1):    # Iterate through features ('Potability' is a label) \n  df.boxplot(by='Potability', column=[feature], ax=axes[ a%2, a%5 ], grid=False)    # Create boxplots for each feature grouped by potable or not (df.boxplot() auto handles nan correctly). Subplot coordinates [a%2, a%5] start top left and vertically zig zag moving right.\n  a+=1\n\naxes[1,4].remove()        # Remove unnecessary subplot from 2x5 grid\nplt.show()","79e80d54":"df_copy=df\nx=df.drop([\"Potability\"],axis=1)\ny=df.Potability\n","d9d9757f":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)","d3fd859e":"Accuracy={}","eab8c317":"logistic_regression= LogisticRegression()\nlogistic_regression.fit(X_train,y_train)\ny_pred=logistic_regression.predict(X_test)","4fed7c2f":"confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True,fmt='g')","788447ca":"Accuracy_Logistic_regresion=round((metrics.accuracy_score(y_test, y_pred)*100),2)\nprint('Accuracy Logistic regresion: ',Accuracy_Logistic_regresion,\"%\")\nAccuracy[\"Logisticregresion\"]=Accuracy_Logistic_regresion","13ae14a8":"from sklearn import svm","47c24b2b":"svm_class= svm.SVC()\nsvm_class.fit(X_train,y_train)\ny_pred=svm_class.predict(X_test)","8e16561c":"confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')","8fb040b7":"Accuracy_Svm=round((metrics.accuracy_score(y_test, y_pred)*100),2)\nprint('Accuracy: ',Accuracy_Svm,\"%\")\nAccuracy[\"SVC\"]=Accuracy_Svm","5948b8a9":"from sklearn import tree","8e2cf3cb":"clf=tree.DecisionTreeClassifier()\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)","ee24fdfb":"confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')","8f9ceab1":"Accuracy_Decision_Tree=round((metrics.accuracy_score(y_test, y_pred)*100),2)\nprint('Accuracy: ',Accuracy_Decision_Tree,\"%\")\nAccuracy[\"DecisionTreeClassifier\"]=Accuracy_Decision_Tree","be5a229e":"from sklearn.ensemble import RandomForestClassifier","bf024be7":"clf= RandomForestClassifier()\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)","3b66fae6":"confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')","1811da48":"Accuracy_RandomForestClassifier=round((accuracy_score(y_test,y_pred)*100),2)\nprint(\"Accuracy_RandomForestClassifier : \",Accuracy_RandomForestClassifier,\"%\")\nAccuracy[\"RandomForestClassifier\"]=Accuracy_RandomForestClassifier","9e94900d":"clf=KNeighborsClassifier()\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)","e1dd450f":"confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\n","49192098":"Accuracy_KNeighborsClassifier=round((metrics.accuracy_score(y_test,y_pred)*100),2)\nprint(\"Accuracy_KNeighborsClassifier : \",Accuracy_KNeighborsClassifier,\"%\")\nAccuracy[\"Accuracy_KNeighborsClassifier\"]=Accuracy_KNeighborsClassifier","f74d67eb":"import xgboost as xgb","e25efb1e":"xg_reg = xgb.XGBClassifier(eval_metric = 'logloss', use_label_encoder=False)","999df939":"xg_reg.fit(X_train,y_train)\ny_pred = xg_reg.predict(X_test)\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')","bdeae2ef":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","a914bde1":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","0ad66fba":"models = [  \n  LogisticRegression(),\n  SVC(),\n  DecisionTreeClassifier(),\n  XGBClassifier(eval_metric = 'logloss', use_label_encoder=False),\n  AdaBoostClassifier(),\n  RandomForestClassifier(),\n  AdaBoostClassifier(),\n  GaussianNB(),\n  KNeighborsClassifier()]\n\nmodel_name=[\"LogisticRegression\",\"SVC\",\"DecisionTreeClassifier\",'XGBClassifier','AdaBoostClassifier',\"RandomForestClassifier\",\"AdaBoostClassifier\",\"GaussianNB\",\"KNeighborsClassifier\"]\n\nprints={}\nfor mod,name in zip(models,model_name):\n  model=mod\n  model.fit(X_train,y_train)\n  y_pred=model.predict(X_test)\n  acc=round((accuracy_score(y_test,y_pred)*100),2)\n  prints[name]=acc","d2ccc0e9":"accuracy_all=pd.DataFrame(prints,index=[1])\naccuracy_all","97518605":"from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV","4c8c2c30":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor","4f82e9ab":"# Create the random grid\nrandom_grid = {\"n_neighbors\":[3,4,5,6,7,8,10]\n               }\n\nrf = KNeighborsClassifier()\n\nfrom pprint import pprint\npprint(random_grid)\nrf = KNeighborsClassifier()\ngrid_cv_dtm = GridSearchCV(rf, random_grid)","50d6c970":"grid_cv_dtm.fit(X_train,y_train)","e8bff132":"df = pd.DataFrame(data=grid_cv_dtm.cv_results_)\ndf.head(7)","3c48528a":"from sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, SGD\nfrom tensorflow.keras.layers import Activation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation","1cb7bd61":"x = df_copy.drop(['Potability'], axis = 1)\ny = df_copy['Potability']","84307f43":"st = StandardScaler()\nx_columns= x.columns\nx[x_columns] = st.fit_transform(x[x_columns])","f4403752":"x.head()","61c80811":"x.describe()","0aab52be":"X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.25, random_state = 0)","47766fda":"num_classes =2\nY_train = keras.utils.to_categorical(Y_train, num_classes)\nY_test = keras.utils.to_categorical(Y_test, num_classes)","98b2eda5":"model = models.Sequential()\n\nmodel.add(layers.Dense(100, input_shape=(9,)))\nmodel.add(Activation(\"relu\"))\n\n\nmodel.add(layers.Dense(100))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.25))\n\nmodel.add(layers.Dense(50))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.25))\n\nmodel.add(layers.Dense(2))\nmodel.add(Activation(\"sigmoid\"))","6e9c5312":"model.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.85,beta_2=0.9),\n              metrics=['accuracy'])","1070f67f":"model.summary()","f7042468":"tf.random.set_seed(0)\nhistory = model.fit(X_train, Y_train,\n          batch_size=32, epochs=200,\n          verbose=2,\n          validation_data=(X_test, Y_test))\n","b6cbf0e8":"# plotting the metrics\nfig = plt.figure()\nplt.subplot(2,1,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\n\nplt.subplot(2,1,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\n\nplt.tight_layout()","c162d3f7":"# **Logistic regresion**","bc8cda21":"# **SVM**","7e06fce3":"# **All Model Building**","e66f17e5":"# **train_test_split**","fc79b5bb":"# **XGBClassifier**","4aa00719":"# **RandomForestClassifier**","a79a0438":"# **Missing Values Analysis**","f6704bcb":"# **KNeighborsClassifier**","103ae6ed":"# **Decision Tree Classifier**","414d93a6":"# **Deep Learning**"}}