{"cell_type":{"d9659227":"code","82826552":"code","3659dcd4":"code","fddfb60f":"code","f86a65ae":"code","b93a5b75":"code","99f98da1":"code","b1d1b5ce":"code","94a45c43":"code","b0ec3b90":"code","5bbbdc42":"code","6a746b02":"code","6c830025":"code","01299bfe":"code","70f87ca2":"code","31414fd7":"code","5c9bb4a0":"code","3b76cf40":"code","d4729ae8":"code","5a2e3827":"code","668de907":"code","5bd7a1a1":"code","dde1feec":"code","1adf06c7":"code","62754c5d":"code","85dc5fd6":"code","8407ca56":"code","6614ace7":"code","f1f56f7b":"code","83d326c9":"code","eb87e98f":"code","58af7131":"code","d71b5063":"code","d482339d":"code","d81ee67f":"code","2d24edad":"markdown","5abc7dc9":"markdown","ca4b84b3":"markdown","7e3d701c":"markdown","4a07a9eb":"markdown","9003a940":"markdown","cfcb8f9e":"markdown","c6fcec32":"markdown","9b8d34f9":"markdown","b174c276":"markdown"},"source":{"d9659227":"import pandas as pd\nimport numpy as np\nimport seaborn as sbn\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nfrom matplotlib.ticker import AutoMinorLocator\nimport os","82826552":"from sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier","3659dcd4":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve","fddfb60f":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nData = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\npd.set_option(\"display.max_columns\", 7)\nData.head()","f86a65ae":"# \"class\" to Binary data (\"Abnormal\" = 1, \"Normal\" = 0)\nData[\"class\"] = np.where(Data[\"class\"] == \"Abnormal\", 1, 0)","b93a5b75":"Data.head()","99f98da1":"Missing_values_percent = 100*(Data.isnull().sum()\/len(Data[\"class\"]))\nprint(Missing_values_percent)","b1d1b5ce":"Data.dtypes","94a45c43":"sbn.pairplot(Data, hue = \"class\")\nplt.show()","b0ec3b90":"plt.figure(figsize = (14, 8))\nsbn.heatmap(Data.corr(), annot = True, vmin = -1, cmap = \"coolwarm\")\nplt.show()","5bbbdc42":"triu = np.triu(Data.corr())\nplt.figure(figsize = (14, 8))\nsbn.heatmap(Data.corr(), annot = True, vmin = -1, cmap = \"coolwarm\", mask = triu)\nplt.show()","6a746b02":"tril = np.tril(Data.corr())\nplt.figure(figsize = (14, 8))\nsbn.heatmap(Data.corr(), annot = True, vmin = -1, cmap = \"coolwarm\", mask = tril)\nplt.show()","6c830025":"Font1 = {\"family\": \"serif\", \"weight\": \"bold\", \"color\": \"darkred\", \"size\": 16}","01299bfe":"Sacral_slope = np.array(Data[\"sacral_slope\"], dtype = float)\nPelvic_incidence = np.array(Data[\"pelvic_incidence\"], dtype = float)\nlumbar_lordosis_angle = np.array(Data[\"lumbar_lordosis_angle\"], dtype = float)\nSacral_slope = Sacral_slope.reshape(-1, 1)\nLinearModel = LinearRegression()\nLinearModel2 = LinearRegression()\nLinearModel.fit(Sacral_slope, Pelvic_incidence)\nLinearModel2.fit(Sacral_slope, lumbar_lordosis_angle)","70f87ca2":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize = (15, 7))\naxs[0].scatter(Sacral_slope, Pelvic_incidence, marker = \"o\", color = \"darkblue\")\naxs[0].plot(Sacral_slope, Sacral_slope*LinearModel.coef_ + LinearModel.intercept_, color = \"darkred\", linewidth = 2.6)\naxs[0].set_xlabel(\"Sacral slope\", fontdict = Font1)\naxs[0].set_ylabel(\"Pelvic incidence\", fontdict = Font1)\n# minor_locator - Major_locator\naxs[0].xaxis.set_minor_locator(AutoMinorLocator())\naxs[0].yaxis.set_minor_locator(AutoMinorLocator())\naxs[0].tick_params(axis = \"both\", direction = \"in\", labelcolor='black', labelsize=16, top = True, right = True)\naxs[0].tick_params(which = \"major\", direction = \"in\", color='black', length=7, width = 3)\naxs[0].tick_params(which = \"minor\", direction = \"in\", color='black', length=4, width = 2, top = True, right = True)\nfor axis in [\"left\", \"bottom\", \"right\", \"top\"]:\n    axs[0].spines[axis].set_linewidth(1.4)\naxs[1].scatter(Sacral_slope, lumbar_lordosis_angle, marker = \"o\", color = \"darkblue\")\naxs[1].plot(Sacral_slope, Sacral_slope*LinearModel2.coef_ + LinearModel2.intercept_, color = \"darkred\", linewidth = 2.6)\naxs[1].set_xlabel(\"Sacral slope\", fontdict = Font1)\naxs[1].set_ylabel(\"Lumbar lordosis angle\", fontdict = Font1)\n# minor_locator - Major_locator\naxs[1].xaxis.set_minor_locator(AutoMinorLocator())\naxs[1].yaxis.set_minor_locator(AutoMinorLocator())\naxs[1].tick_params(axis = \"both\", direction = \"in\", labelcolor='black', labelsize=16, top = True, right = True)\naxs[1].tick_params(which = \"major\", direction = \"in\", color='black', length=7, width = 3)\naxs[1].tick_params(which = \"minor\", direction = \"in\", color='black', length=4, width = 2, top = True, right = True)\nfor axis in [\"left\", \"bottom\", \"right\", \"top\"]:\n    axs[1].spines[axis].set_linewidth(1.4)\nfig.patch.set_facecolor(\"white\")  \nplt.show()","31414fd7":"x = Data.drop([\"class\"], axis = 1)\ny = Data[\"class\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 9)","5c9bb4a0":"Logistic_Regression = LogisticRegression()","3b76cf40":"penalty = [\"l1\", \"l2\", \"elasticnet\"]\nC = np.array([0.0007, 0.001, 0.005, 0.009, 0.01])\nparam_grid = {\"C\": C, \"penalty\": penalty}\nGrid_Logistic_Regression = GridSearchCV(estimator = Logistic_Regression, param_grid = param_grid, cv = 5, n_jobs = -1)\nGrid_Logistic_Regression.fit(x_train, y_train)\nprint(f\"C: {Grid_Logistic_Regression.best_estimator_.C} \/\/ penalty: {Grid_Logistic_Regression.best_estimator_.penalty}\")","d4729ae8":"Logistic_Regression = LogisticRegression(C =  0.001, penalty = \"l2\")\nLogistic_Regression.fit(x_train, y_train)","5a2e3827":"Bagging_Logistic_Regression = BaggingClassifier(base_estimator=Logistic_Regression, n_estimators=100,max_samples=0.5, n_jobs=-1)\nBagging_Logistic_Regression.fit(x_train, y_train)","668de907":"GaussianNB = GaussianNB()\nBagging_GaussianNB = BaggingClassifier(base_estimator=GaussianNB, n_estimators=100, max_samples = 0.5, n_jobs=-1)\nBagging_GaussianNB.fit(x_train, y_train)","5bd7a1a1":"KNN = KNeighborsClassifier()\nn_neighbors = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10])\np = np.array([2, 3 ,4 ,5 , 6, 7, 8, 9, 10])\nmetric = [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\nparam_grid = {\"n_neighbors\": n_neighbors, \"p\": p, \"metric\": metric}\nGridKNN = GridSearchCV(estimator = KNN, param_grid = param_grid, cv = 5, n_jobs=-1)\nGridKNN.fit(x_train, y_train)\nprint(f\"n_neighbors: {GridKNN.best_estimator_.n_neighbors} \/\/ p: {GridKNN.best_estimator_.p} \/\/ metric: {GridKNN.best_estimator_.metric}\")","dde1feec":"KNN = KNeighborsClassifier(n_neighbors = 9,  p = 2, metric = \"euclidean\")\nKNN.fit(x_train, y_train)","1adf06c7":"Bagging_KNN = BaggingClassifier(base_estimator=KNN, n_estimators=100, max_samples=0.5, n_jobs=-1)\nBagging_KNN.fit(x_train, y_train)","62754c5d":"%%time\nRandom_Forest = RandomForestClassifier()\nExtra_Trees = ExtraTreesClassifier()\nDecision_Tree = DecisionTreeClassifier()\nmax_depth = np.array([9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\nmin_samples_split = np.array([2, 3, 4, 5, 6, 7, 8, 9])\ncriterion = [\"gini\", \"entropy\"]\nparam_grid = {\"max_depth\":max_depth, \"min_samples_split\":min_samples_split}\nparam_grid2 = {\"max_depth\":max_depth, \"min_samples_split\":min_samples_split, \"criterion\": criterion}\nGridRandom_Forest = GridSearchCV(estimator = Random_Forest, param_grid = param_grid, cv = 5, n_jobs = -1)\nGridExtra_Trees = GridSearchCV(estimator = Extra_Trees, param_grid = param_grid, cv =  5, n_jobs = -1)\nGridDecision_Tree = GridSearchCV(estimator = Decision_Tree, param_grid = param_grid2, cv =  5, n_jobs = -1)\nGridRandom_Forest.fit(x_train, y_train)\nGridExtra_Trees.fit(x_train, y_train)\nGridDecision_Tree.fit(x_train, y_train)\nprint(f\"Random Forest: max_depth = {GridRandom_Forest.best_estimator_.max_depth} \/\/ min_samples_split = {GridRandom_Forest.best_estimator_.min_samples_split}\")\nprint(f\"Extra Trees: max_depth = {GridExtra_Trees.best_estimator_.max_depth} \/\/ min_samples_split = {GridExtra_Trees.best_estimator_.min_samples_split}\")\nprint(f\"Decision Tree: max_depth = {GridDecision_Tree.best_estimator_.max_depth} \/\/ min_samples_split = {GridDecision_Tree.best_estimator_.min_samples_split}\")","85dc5fd6":"Random_Forest = RandomForestClassifier(max_depth = 17, min_samples_split = 2)\nExtra_Trees = ExtraTreesClassifier(max_depth = 14, min_samples_split = 3)\nDecision_Tree = DecisionTreeClassifier(max_depth = 11, min_samples_split = 2)\nRandom_Forest.fit(x_train, y_train)\nExtra_Trees.fit(x_train, y_train)\nDecision_Tree.fit(x_train, y_train)","8407ca56":"Bagging_Random_Forest = BaggingClassifier(base_estimator=Random_Forest, n_estimators=100, max_samples=0.5, n_jobs=-1)\nBagging_Extra_Trees = BaggingClassifier(base_estimator=Extra_Trees, n_estimators=100, max_samples=0.5, n_jobs=-1)\nBagging_Decision_Tree = BaggingClassifier(base_estimator=Decision_Tree, n_estimators=100, max_samples=0.5, n_jobs=-1)\nBagging_Random_Forest.fit(x_train, y_train)\nBagging_Extra_Trees.fit(x_train, y_train)\nBagging_Decision_Tree.fit(x_train, y_train)","6614ace7":"%%time\nAdaBoost = AdaBoostClassifier(n_estimators = 500)\nGradientBoosting = GradientBoostingClassifier(n_estimators = 200)\nlearning_rate = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\nmin_samples_split = np.array([2, 3, 4, 5, 6])\nmin_samples_leaf = np.array([2 , 3 ,4 ,5 ,6])\nmax_depth = np.array([2 , 3 ,4 ,5 ,6 ,7])\nparam_grid_Adaboost = {\"learning_rate\": learning_rate}\nparam_grid_GradientBoosting = {\"learning_rate\": learning_rate, \"min_samples_split\": min_samples_split, \n                              \"min_samples_leaf\": min_samples_leaf, \"max_depth\" : max_depth}\nGrid_Adaboost = GridSearchCV(estimator = AdaBoost, param_grid = param_grid_Adaboost, cv = 5, n_jobs=-1)\nGrid_GradientBoosting = GridSearchCV(estimator = GradientBoosting, param_grid = param_grid_GradientBoosting, cv = 5, n_jobs=-1)\nGrid_Adaboost.fit(x_train, y_train)\nGrid_GradientBoosting.fit(x_train, y_train)\nprint(f\"AdaBoost: learning_rate = {Grid_Adaboost.best_estimator_.learning_rate}\")\nprint(f\"GradientBoosting: learning_rate  = {Grid_GradientBoosting.best_estimator_.learning_rate} \/\/ min_samples_split = {Grid_GradientBoosting.best_estimator_.min_samples_split} \/\/ min_samples_leaf = {Grid_GradientBoosting.best_estimator_.min_samples_leaf}\")","f1f56f7b":"AdaBoost = AdaBoostClassifier(n_estimators = 500, learning_rate = 0.1)\nGradientBoosting = GradientBoostingClassifier(n_estimators = 200, learning_rate  = 0.8, min_samples_split = 2, min_samples_leaf = 5)\nAdaBoost.fit(x_train, y_train)\nGradientBoosting.fit(x_train, y_train)","83d326c9":"Bagging_Adaboost = BaggingClassifier(base_estimator=AdaBoost, n_estimators=100,max_samples=0.5, n_jobs=-1)\nBagging_GradientBoosting = BaggingClassifier(base_estimator=GradientBoosting, n_estimators=100,max_samples=0.5, n_jobs=-1)","eb87e98f":"Bagging_Adaboost.fit(x_train, y_train)\nBagging_GradientBoosting.fit(x_train, y_train)","58af7131":"GaussianNB.fit(x_train, y_train)\ny_proba_Logistic_Regression = Logistic_Regression.predict_proba(x_test)\ny_proba_Logistic_Regression = y_proba_Logistic_Regression[:, 1]\ny_proba_GaussianNB = GaussianNB.predict_proba(x_test)\ny_proba_GaussianNB = y_proba_GaussianNB[:, 1]\ny_proba_KNN = KNN.predict_proba(x_test)\ny_proba_KNN = y_proba_KNN[:, 1]\ny_proba_Random_Forest = Random_Forest.predict_proba(x_test)\ny_proba_Random_Forest = y_proba_Random_Forest[:, 1]\ny_proba_ExtraTrees = Extra_Trees.predict_proba(x_test)\ny_proba_ExtraTrees = y_proba_ExtraTrees[:, 1]\ny_prova_Decision_Trees = Decision_Tree.predict_proba(x_test)\ny_prova_Decision_Trees = y_prova_Decision_Trees[:, 1]\ny_proba_Adaboost = AdaBoost.predict_proba(x_test)\ny_proba_Adaboost = y_proba_Adaboost[:, 1]\ny_proba_GradientBoosting = GradientBoosting.predict_proba(x_test)\ny_proba_GradientBoosting = y_proba_GradientBoosting[:, 1]\ny_proba_BagLogistic_Regression = Bagging_Logistic_Regression.predict_proba(x_test)\ny_proba_BagLogistic_Regression = y_proba_BagLogistic_Regression[:, 1]\ny_proba_BagGaussianNB = Bagging_GaussianNB.predict_proba(x_test)\ny_proba_BagGaussianNB = y_proba_BagGaussianNB[:, 1]\ny_proba_BagKNN = Bagging_KNN.predict_proba(x_test)\ny_proba_BagKNN = y_proba_BagKNN[:, 1]\ny_proba_BagRandom_Forest = Bagging_Random_Forest.predict_proba(x_test)\ny_proba_BagRandom_Forest = y_proba_BagRandom_Forest[:, 1]\ny_proba_BagExtraTrees = Bagging_Extra_Trees.predict_proba(x_test)\ny_proba_BagExtraTrees = y_proba_BagExtraTrees[:, 1]\ny_prova_BagDecision_Trees = Bagging_Decision_Tree.predict_proba(x_test)\ny_prova_BagDecision_Trees = y_prova_BagDecision_Trees[:, 1]\ny_proba_BagAdaboost = Bagging_Adaboost.predict_proba(x_test)\ny_proba_BagAdaboost = y_proba_BagAdaboost[:, 1] \ny_proba_BagGradientBoosting = Bagging_GradientBoosting.predict_proba(x_test)\ny_proba_BagGradientBoosting = y_proba_BagGradientBoosting[:, 1]","d71b5063":"fpr_Logistic_Regression, tpr_Logistic_Regression, thresholds_LR = roc_curve(y_test, y_proba_Logistic_Regression)\nfpr_GaussianNB, tpr_GaussianNB, thresholds_NB = roc_curve(y_test, y_proba_GaussianNB)\nfpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_test, y_proba_KNN)\nfpr_Random_Forest, tpr_Random_Forest, thresholds_Random_Forest = roc_curve(y_test, y_proba_Random_Forest)\nfpr_ExtraTrees, tpr_ExtraTrees, thresholds_ExtraTrees = roc_curve(y_test, y_proba_ExtraTrees)\nfpr_DecisionTree, tpr_DecisionTree, thresholds_DecisionTree = roc_curve(y_test, y_prova_Decision_Trees)\nfpr_Adaboost, tpr_Adaboost, thresholds_Adaboost = roc_curve(y_test, y_proba_Adaboost)\nfpr_GradientBoosting, tpr_GradientBoosting, thresholds_GradientBoosting = roc_curve(y_test, y_proba_GradientBoosting)","d482339d":"fpr_BagLogistic_Regression, tpr_BagLogistic_Regression, thresholds_BagLR = roc_curve(y_test, y_proba_BagLogistic_Regression)\nfpr_BagGaussianNB, tpr_BagGaussianNB, thresholds_BagNB = roc_curve(y_test, y_proba_BagGaussianNB)\nfpr_BagKNN, tpr_BagKNN, thresholds_BagKNN = roc_curve(y_test, y_proba_BagKNN)\nfpr_BagRandom_Forest, tpr_BagRandom_Forest, thresholds_BagRandom_Forest = roc_curve(y_test, y_proba_BagRandom_Forest)\nfpr_BagExtraTrees, tpr_BagExtraTrees, thresholds_BagExtraTrees = roc_curve(y_test, y_proba_BagExtraTrees)\nfpr_BagDecisionTree, tpr_BagDecisionTree, thresholds_BagDecisionTree = roc_curve(y_test, y_prova_BagDecision_Trees)\nfpr_BagAdaboost, tpr_BagAdaboost, thresholds_BagAdaboost = roc_curve(y_test, y_proba_BagAdaboost)\nfpr_BagGradientBoosting, tpr_BagGradientBoosting, thresholds_BagGradientBoosting = roc_curve(y_test, y_proba_BagGradientBoosting)","d81ee67f":"fig, ax = plt.subplots(figsize = (12, 9))\nFonte1 = {\"family\": \"serif\", \"weight\": \"bold\", \"color\": \"black\", \"size\": 15}\nFonte2 = {\"family\": \"serif\", \"weight\": \"bold\", \"color\": \"black\", \"size\": 15}\nFonte3 = FontProperties(family=\"serif\",\n                                   weight='bold',\n                                   style='normal', \n                                     size=15)\nax.plot(fpr_Logistic_Regression, tpr_Logistic_Regression, label = \"Logistic Regression\", color = \"darkblue\", linewidth = 2)\nax.plot(fpr_GaussianNB, tpr_GaussianNB, label = \"Naive Bayes\", color = \"blue\", linewidth = 2)\nax.plot(fpr_KNN, tpr_KNN, label = \"KNN\", color = \"darkred\", linewidth = 2)\nax.plot(fpr_Random_Forest, tpr_Random_Forest, label = \"Random Forest\", color = \"red\", linewidth = 2)\nax.plot(fpr_ExtraTrees, tpr_ExtraTrees, label = \"Extra Trees\", color = \"green\", linewidth = 2)\nax.plot(fpr_DecisionTree, tpr_DecisionTree, label = \"Decision Tree\", color = \"yellow\", linewidth = 2)\nax.plot(fpr_Adaboost, tpr_Adaboost, label = \"AdaBoost\", color = \"cyan\", linewidth = 2)\nax.plot(fpr_GradientBoosting, tpr_GradientBoosting, label = \"Gradient Boosting\", color = \"purple\", linewidth = 2)\nax.plot(fpr_BagLogistic_Regression, tpr_BagLogistic_Regression, label = \"Bagging(Logistic Regression)\", color = \"brown\", linewidth = 2)\nax.plot(fpr_BagGaussianNB, tpr_BagGaussianNB, label = \"Bagging(GaussianNB)\", color = \"black\", linewidth = 2)\nax.plot(fpr_BagKNN, tpr_BagKNN, label =  \"Bagging(KNN)\", color = \"pink\", linewidth = 2)\nax.plot(fpr_BagRandom_Forest, tpr_BagRandom_Forest, label = \"Bagging(Random Forest)\", color = \"gray\", linewidth = 2)\nax.plot(fpr_BagExtraTrees, tpr_BagExtraTrees, label = \"Bagging(Extra Trees)\", color = \"orange\", linewidth = 2)\nax.plot(fpr_BagDecisionTree, tpr_BagDecisionTree, label = \"Bagging(Decision Tree)\", color = \"darkorange\", linewidth = 2)\nax.plot(fpr_BagAdaboost, tpr_BagAdaboost, label = \"Bagging(AdaBoost)\", color = \"yellow\", linewidth = 2)\nax.plot(fpr_BagGradientBoosting, tpr_BagGradientBoosting, label = \"Bagging(Gradient Boosting)\", color = \"cyan\", linewidth = 2)\nax.lines[15].set_linestyle(\"--\")\nax.lines[13].set_linestyle(\":\")\nax.lines[5].set_linestyle(\":\")\nax.xaxis.set_minor_locator(AutoMinorLocator())\nax.yaxis.set_minor_locator(AutoMinorLocator())\nax.tick_params(axis = \"both\", direction = \"in\", labelcolor='black', labelsize=15, bottom = True, left = True, top = True, right = True)\nax.tick_params(which='major', direction = \"in\", color='black', length=7, width = 3)\nax.tick_params(which='minor', direction = \"in\", length=4, color='black', width = 3, top = True, right = True, bottom = True, left = True)\nfor axis in ['top','bottom','left','right']:\n    ax.spines[axis].set_linewidth(3) \nplt.legend(frameon = False, prop = Fonte3)\nax.set_xlabel(\"Taxa de falsos positivos\", fontdict = Fonte1)\nax.set_ylabel(\"Taxa de verdadeiros positivos\", fontdict = Fonte1)\nplt.title(\"Curvas roc dos classificadores para a predi\u00e7\u00e3o de doen\u00e7as ortop\u00e9dicas\", fontdict = Fonte2)\nfig.patch.set_facecolor(\"white\")\nplt.legend(frameon = False, prop = Fonte3)\nplt.show()","2d24edad":"# 1. Importing Libraries","5abc7dc9":"${\\color{green}{\\textbf{3.1 Missing Values Treatment}}}$","ca4b84b3":"${\\color{green}{\\textbf{3.3 Correlation}}}$","7e3d701c":"${\\color{green}{\\textbf{3.2 Dtypes}}}$","4a07a9eb":"# 4. Models","9003a940":"${\\color{green}{\\textbf{There are NO missing values in the dataset!!}}}$","cfcb8f9e":"# 3. Data Preprocessing","c6fcec32":"${\\color{green}{\\textbf{Pelvic incidence x Sacral slope and lumbar lordosis angle x Sacral slope}}}$","9b8d34f9":"# 2. Reading the data","b174c276":"${\\color{green}{\\textbf{3.4 Data split}}}$"}}