{"cell_type":{"d9350406":"code","d45f7a0f":"code","fb775f77":"code","cad88021":"code","3040f6de":"code","9c5a5706":"code","97a0b886":"code","d79d32a0":"code","6ad61129":"code","0eeda2f8":"code","9645b671":"code","8d74315c":"code","7cf8fc9d":"code","4cd93312":"code","1bdcf321":"code","884e142e":"code","c35f85fd":"code","cc182637":"code","c9f61625":"code","c42fc0c7":"code","81017d9b":"code","fe4e4454":"code","49a2c6b5":"code","c98ca418":"code","b9761a28":"code","4061731a":"code","0b65a06a":"code","cc9bcc6a":"code","9681ab90":"code","e6674082":"code","566fa842":"code","7f25948e":"code","9c6c647c":"code","925a99d2":"code","b407cda2":"code","9b1bf00d":"code","95ddcac5":"code","d31557ef":"code","98cebfd5":"code","7596668c":"code","832b3b9d":"code","a534af23":"code","d2f533e6":"code","c6bbed7d":"code","b165ed8e":"code","f516f9a0":"code","2f829a2e":"code","071aad94":"code","98be362d":"code","d5debe66":"code","ec5da6ec":"code","94417d54":"code","eecd083b":"code","1a2ef04d":"code","4773cdaa":"code","b909610c":"code","a2cde98d":"code","af3aed89":"markdown","c7e34d29":"markdown","cb77cf35":"markdown","2c61c7bd":"markdown","bf1f4a21":"markdown","74cbf1d8":"markdown","d88f1ce7":"markdown","ee6be84b":"markdown","add6e1f6":"markdown","16b8c98a":"markdown","89f5e822":"markdown","61bd07ae":"markdown","d9835d10":"markdown","d682466d":"markdown","5f67a8d8":"markdown","a48358ab":"markdown","42944161":"markdown","cf6c07b8":"markdown","727c73e9":"markdown","435b9f26":"markdown","57ff13cb":"markdown","68a0aa08":"markdown","fe6aa46a":"markdown","f0599e13":"markdown","adf4766a":"markdown","345643c2":"markdown","467cb8da":"markdown","0933cbbb":"markdown","845ec060":"markdown","f77ec48c":"markdown","4151bb55":"markdown","ff29f591":"markdown","5f98f1be":"markdown","abd5cee9":"markdown","db76386f":"markdown","435cfd83":"markdown","bff3bd2d":"markdown","0f113e6c":"markdown","ea97e3b8":"markdown","7154d9c8":"markdown","cce40d14":"markdown","f5d11572":"markdown","326b70d2":"markdown","88eada58":"markdown","399f40fc":"markdown","ef73cb38":"markdown","14f33610":"markdown"},"source":{"d9350406":"# Importing Libraries\n\n# Used for Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Used for Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-bright')\n\n# Used for NLP\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Used for ML\/Deep Learning Algorithms\nfrom tensorflow.keras.layers import Embedding,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM,Bidirectional,GRU,MaxPooling1D,Conv1D\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","d45f7a0f":"# Reading data from csv\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain.head()","fb775f77":"test.head()","cad88021":"# Displaying rows and columns in dataset\n\nprint(\"There are {} rows and {} columns in training data\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} rows and {} columns in training data\".format(test.shape[0],test.shape[1]))","3040f6de":"# Visualizing the target classes\nplt.figure(figsize=(8,5))\nplt.title(\"Count of Target Classes\")\nsns.countplot(y=train[\"target\"],linewidth=2,\n                   edgecolor='black')\n\nplt.show()","9c5a5706":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.len()\nax1.hist(char_len_dis,color='red',edgecolor='black', linewidth=1.2)\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.len()\nax2.hist(char_len_ndis,color='blue',edgecolor='black', linewidth=1.2)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Length of Characters in text\")\nplt.tight_layout()\nplt.show()\n","97a0b886":"# Analysing number of words in text.\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(char_len_dis,color='red',edgecolor='black', linewidth=1.2)\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(char_len_ndis,color='blue',edgecolor='black', linewidth=1.2)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Length of words in text\")\nplt.tight_layout()\nplt.show()","d79d32a0":"# Analysing average word length in text.\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(char_len_dis.map(lambda x: np.mean(x)),ax=ax1,color='darkblue')\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(char_len_ndis.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Average Word Length in text\")\nplt.tight_layout()\nplt.show()","6ad61129":"# Creating sample corpus for further analysis.\ndef create_corpus(target):\n    corpus = []\n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n    ","0eeda2f8":"# Analysing top stop words in text.\nfrom collections import defaultdict\n\ndef analyze_stopwords(data,func,target):\n    values_list = []\n    for labels in range(0,len(target)):\n        dic = defaultdict(int)\n        corpus = func(target[labels])\n        for word in corpus:\n            dic[word]+=1\n        top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n        x_items,y_values = zip(*top)\n        values_list.append(x_items)\n        values_list.append(y_values)\n    \n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.barh(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Non-Disaster Tweets\")\n    \n    ax2.barh(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Disaster Tweets\")\n            \n    plt.suptitle(\"Top Stop words in text\")\n    plt.show()\n\nanalyze_stopwords(train,create_corpus,[0,1])","9645b671":"# Anaysing Punctuations\nimport string\n\ndef analyze_punctuation(data,func,target):\n    values_list = []\n    special = string.punctuation\n    for labels in range(0,len(target)):\n        dic = defaultdict(int)\n        corpus = func(target[labels])\n        for i in corpus:\n            if i in special:\n                dic[i]+=1\n        x_items,y_values = zip(*dic.items())\n        values_list.append(x_items)\n        values_list.append(y_values)\n    \n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Non-Disaster Tweets\")\n    \n    ax2.bar(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Disaster Tweets\")\n            \n    plt.suptitle(\"Punctuations in text\")\n    plt.show()\n\nanalyze_punctuation(train,create_corpus,[0,1])","8d74315c":"# Checking Null values\nmissing_train = train.isnull().sum()  \nmissing_test = test.isnull().sum()  \nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\nmissing_train = missing_train[missing_train>0].sort_values()\nax1.pie(missing_train,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['yellow','cyan'])\nax1.set_title(\"Null values present in Train Dataset\")\n\nmissing_test = missing_test[missing_test>0].sort_values()\nax2.pie(missing_test,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['yellow','#66ff00'])\nax2.set_title(\"Null values present in Test Dataset\")\nplt.suptitle(\"Distribution of Null Values in Dataset\")\nplt.tight_layout()\nplt.show()","7cf8fc9d":"# Analysing Top 20  disastrous KeyWords in text .\nplt.figure(figsize=(10,7))\ntrain[train['target']==1]['keyword'].value_counts()[:20].plot(kind='barh', fontsize=12,title='Top 20 Disastrous Keywords in Text', color='#0096FF',edgecolor='black', linewidth=1.2)\nplt.show()","4cd93312":"# Analysing Top 20 disastrous Locations in text.\nplt.figure(figsize=(10,7))\ntrain[train[\"target\"]==1][\"location\"].value_counts()[:20].plot(kind='barh',fontsize=12, title='Top 20 Disastrous Locations in Text', color='#66ff00',edgecolor='black', linewidth=1.2)\nplt.show()","1bdcf321":"# Seperating independent and dependent features\nX = train.drop(columns=[\"target\"],axis=1)\ny = train[\"target\"]","884e142e":"# Perfoming data cleaning\n\nmessages_train = X.copy()\nmessages_test  = test.copy()\n\nps = PorterStemmer()\nwl = WordNetLemmatizer()\ndef preprocess_data(data):\n    '''\n    Input: Data to be cleaned.\n    Output: Cleaned Data.\n    \n    '''\n    review =re.sub(r'https?:\/\/\\S+|www\\.\\S+|http?:\/\/\\S+',' ',data) #removal of url\n    review =re.sub(r'<.*?>',' ',review) #removal of html tags\n    review = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',review)\n    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.\n    review = review.lower() # Lowering all the words in text\n    review = review.split()\n    review = [ps.stem(words) for words in review if words not in stopwords.words('english')] #Stemming\n    review = [i for i in review if len(i)>2] # Removal of words with length<2\n    review = ' '.join(review)\n    return review\n\ntrain[\"Cleaned_text\"] = train[\"text\"].apply(preprocess_data)\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)","c35f85fd":"train.head()","cc182637":"# Analysing common words using WordCloud \n\nwc = WordCloud(background_color='black')\nwc.generate(' '.join(train['Cleaned_text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","c9f61625":"# Analysing top 50 words in training data\n\ndisaster_tweet = train[train.target==1][\"Cleaned_text\"]\nnon_disaster_tweet = train[train.target==0][\"Cleaned_text\"]\n\ncolor = ['Paired','Accent']\nsplitedData = [disaster_tweet,non_disaster_tweet]\ntitle = [\"Disaster Tweets\", \"Non-Disaster Tweets\"]\nfor item in range(2):\n    plt.figure(figsize=(20,8))\n    plt.title(title[item],fontsize=12)\n    pd.Series(' '.join([i for i in splitedData[item]]).split()).value_counts().head(50).plot(kind='bar',fontsize=10,colormap=color[item],edgecolor='black', linewidth=1.2)\n    plt.show()","c42fc0c7":"common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ntrain[\"Cleaned_text\"] = train[\"Cleaned_text\"].apply(text_cleaning)\ntest[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)","81017d9b":"train.head(1) # Much more cleaner","fe4e4454":"# Creating function for analysing top n grams\n\ndef top_ngrams(data,n,grams):\n    '''\n    Input:- Data: Input Data\n            n   : Number of top n-words\n            grams:Type of N-grams. 1-> Unigram  2-> Bigram  3->Trigram\n            \n    Output: Word Frequency of top  n words\n    \n    '''\n    if grams == 1:\n        count_vec = CountVectorizer(ngram_range=(1,1)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    elif grams == 2:\n        count_vec = CountVectorizer(ngram_range=(2,2)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    elif grams == 3:\n        count_vec = CountVectorizer(ngram_range=(3,3)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n\n    return word_freq[:n]","49a2c6b5":"common_words_uni = top_ngrams(train[\"Cleaned_text\"],20,1)\ncommon_words_bi = top_ngrams(train[\"Cleaned_text\"],20,2)\ncommon_words_tri = top_ngrams(train[\"Cleaned_text\"],20,3)\ncommon_words_uni_df = pd.DataFrame(common_words_uni,columns=['word','freq'])\ncommon_words_bi_df = pd.DataFrame(common_words_bi,columns=['word','freq'])\ncommon_words_tri_df = pd.DataFrame(common_words_tri,columns=['word','freq'])\nfig,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(15,20))\nax1.bar(common_words_uni_df[\"word\"],common_words_uni_df[\"freq\"],color=\"#FFCCCB\",edgecolor='black', linewidth=1.2)\nax1.set_title(\"Top 20 Unigrams in Text.\")\nax1.set_xlabel(\"Words\")\nax1.set_ylabel(\"Frequency\")\nax1.set_xticklabels(rotation=90,labels=common_words_uni_df[\"word\"],fontsize=10)    \n\nax2.bar(common_words_bi_df[\"word\"],common_words_bi_df[\"freq\"],color=\"lightblue\",edgecolor='black', linewidth=1.2)\nax2.set_title(\"Top 20 Bigrams in Text.\")\nax2.set_xlabel(\"Words\")\nax2.set_ylabel(\"Frequency\")\nax2.set_xticklabels(rotation=90,labels=common_words_bi_df[\"word\"],fontsize=10)    \n\nax3.bar(common_words_tri_df[\"word\"],common_words_tri_df[\"freq\"] ,color=\"lightgreen\",edgecolor='black', linewidth=1.2)\nax3.set_title(\"Top 20 Trigrams in Text.\")\nax3.set_xlabel(\"Words\")\nax3.set_ylabel(\"Frequency\")\nax3.set_xticklabels(rotation=90,labels=common_words_tri_df[\"word\"],fontsize=10) \nplt.suptitle(\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",fontsize=\"15\")\nplt.tight_layout(pad=1.85)\nplt.show()\n","c98ca418":"# Creating functions for using BOW,TF-IDF \n\ndef encoding(train_data,test_data,bow,tf_idf):\n    '''\n    Input : Data to be encoded and choice of encoding.\n    Output : Desired Encoding.\n    \n    '''\n    if bow==True: \n        cv = CountVectorizer(ngram_range=(1, 1))\n        cv_df_train = cv.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(cv_df_train,columns=cv.get_feature_names())\n        cv_df_test = cv.transform(test_data).toarray()\n        test_df = pd.DataFrame(cv_df_test,columns=cv.get_feature_names())\n        \n    elif tf_idf==True:\n        \n        tfidf = TfidfVectorizer(\n            ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1)    \n        tf_df_train = tfidf.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names())\n        tf_df_test = tfidf.transform(test_data).toarray()\n        test_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names())\n        \n    return train_df,test_df\n\n\nx_final,x_test_final = encoding(train[\"Cleaned_text\"],test[\"Cleaned_text\"],bow=True,tf_idf=False)\ny_final = np.array(y) # Converting y to array","b9761a28":"# Checking dimensions of training and testing data\nx_final.shape,y_final.shape,x_test_final.shape","4061731a":"#Converting to list\ntext = train[\"Cleaned_text\"].tolist()\ntext_test = test[\"Cleaned_text\"].tolist()\ntext[:3] # Analysing first 3 sentence in train data.","0b65a06a":"from tensorflow.keras.preprocessing.text import Tokenizer\ntoken = Tokenizer()\ntoken.fit_on_texts(text)","cc9bcc6a":"# Finding the vocab size\nvocab_size = len(token.word_index)+1\nprint(\"The vocabulary size is : {}\".format(vocab_size))","9681ab90":"# Positions of tokens arranged by tokenizer in vovabulary.\n\n#print(token.index_word)  #==> O\/p: 1:'like',2:'fire' etc.","e6674082":"# Encoding tokens in words to numerical formats\nencoded_text = token.texts_to_sequences(text)\nencoded_text_test = token.texts_to_sequences(text_test)\nencoded_text[:2]","566fa842":"#Considering 120 words\nmax_length = 120 # Considering top 120 tokens.\nX = pad_sequences(encoded_text,maxlen=max_length,padding='post') # This is done to make the sequence of same length.\nX_test = pad_sequences(encoded_text_test,maxlen=max_length,padding='post')\nX","7f25948e":"# Using Glove Vector representations:\n# you -0.11076 0.30786 -0.5198 0.035138 0.10368 -0.052505...... -0.35471 0.2331 -0.0067546 -0.18892 0.27837 -0.38501 -0.11408 0.28191 -0.30946 -0.21878 -0.059105 0.47604 0.05661\n\n# The first word is key and rest is their vector reprr.","9c6c647c":"%%time\n#declaring dict to store all the words as keys in the dictionary and their vector representations as values\nglove_vectors = dict()\n\n# Now, we will convert the words in glove vectors into key value pairs. We have used glove representation of 200D. \n\n\nfile = open('..\/input\/glove6b200d\/glove.6B.200d.txt',encoding='utf-8')\n\nfor line in file:\n    values = line.split()  # contains list of keys and their vectors\n    word = values[0] # contains words\n    vectors = np.asarray(values[1:]) # storing vectors\n    glove_vectors[word] = vectors #storing the vector representation of the respective word in the dictionary\nfile.close()","925a99d2":"# Checking length of glove vectors\nprint(\"The maximum size of global vectors is : {}\".format(len(glove_vectors)))","b407cda2":"# Checking dimensions of Glove Vectors.\nglove_vectors.get('you').shape","9b1bf00d":"# Now we are creating a matrix for the tokens which we are having in our dataset and then storing their vector representation values in the matrix if it matches with glove_vectors words else print the misspelled words or words which are not present.\n\nword_vector_matrix = np.zeros((vocab_size,200))  # size of the word matrix\nfor word,index in token.word_index.items():\n    vector = glove_vectors.get(word)\n    if vector is not None:\n        word_vector_matrix[index] = vector\n    #else:\n        #print(word)","95ddcac5":"print(\"The Size of Word Matrix is :{}\".format(word_vector_matrix.shape))","d31557ef":"# Dividing the data into training, validation and testing\nfrom sklearn.model_selection import train_test_split\n# for bow and tf-idf\n#x_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)\n#X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n#x_test_final = x_test_final\n\n#  for Word Embeddings\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify = y)\nX_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n#x_test_final = x_test_final","98cebfd5":"model_1 = LogisticRegression(C=1.0)\nmodel_1.fit(X_train,Y_train)\npred_1 = model_1.predict(x_test)\ncr1    = classification_report(y_test,pred_1)\nprint(cr1)","7596668c":"model_2 = MultinomialNB(alpha=0.7)\nmodel_2.fit(X_train,Y_train)\npred_2 = model_2.predict(x_test)\ncr2    = classification_report(y_test,pred_2)\nprint(cr2)","832b3b9d":"model_3 = DecisionTreeClassifier()\nmodel_3.fit(X_train,Y_train)\npred_3 = model_3.predict(x_test)\ncr3    = classification_report(y_test,pred_3)\nprint(cr3)","a534af23":"model_4 = RandomForestClassifier()\nmodel_4.fit(X_train,Y_train)\npred_4 = model_4.predict(x_test)\ncr4    = classification_report(y_test,pred_4)\nprint(cr4)","d2f533e6":"model_5 = XGBClassifier()\nmodel_5.fit(X_train,Y_train)\npred_5 = model_5.predict(x_test)\ncr5    = classification_report(y_test,pred_5)\nprint(cr5)","c6bbed7d":"model_6 = CatBoostClassifier(iterations=100)\nmodel_6.fit(X_train,Y_train)\npred_6 = model_6.predict(x_test)\ncr6    = classification_report(y_test,pred_6)\nprint(cr6)","b165ed8e":"from sklearn.linear_model import PassiveAggressiveClassifier\nmodel_7 = PassiveAggressiveClassifier()\nmodel_7.fit(X_train,Y_train)\npred_7 = model_7.predict(x_test)\ncr7    = classification_report(y_test,pred_7)\nprint(cr7)","f516f9a0":"from sklearn.ensemble import VotingClassifier\n\nestimators = []\nestimators.append(('LR', \n                  LogisticRegression(C=1.0)))\nestimators.append(('NB', MultinomialNB(alpha=0.7)))\nestimators.append(('XBG', XGBClassifier()))\n\n\nvc = VotingClassifier(estimators=estimators,voting='soft')\nvc.fit(X_train,Y_train)\npred_vc = vc.predict(x_test)\ncr_vc    = classification_report(y_test,pred_vc)\nprint(cr_vc)","2f829a2e":"# Finding the max. length of sentences \n\n'''\ncurr_length = 0\nprev_length = 0\n\ndef calc_len(data,curr_length,prev_length):\n    \n    \n    Input: Data for which length is to be calculated.\n    Output: Calculated length.\n    \n    \n    result = 0\n    for i in range(0,len(data)):\n        curr_length = len(data[i])\n        if curr_length > prev_length:\n            prev_length = curr_length\n\n        else:\n            i+=1\n\n        curr_length = 0\n        \n    result = prev_length\n    return result\n\ntrain_length = calc_len(one_hot_train,curr_length,prev_length)\ntest_length  = calc_len(one_hot_test,curr_length,prev_length)\nprint(\"The length of training data: {} and testing data {}.\".format(train_length,test_length))\n'''","071aad94":"# Correcting spellings\n#!pip install pyspellchecker\n'''\nfrom spellchecker import SpellChecker\nfrom tqdm import tqdm\nspell = SpellChecker()\ndef correct_spell(text):\n    corrected_text = []\n    mispelled_words = spell.unknown(text.split())\n    for word in tqdm(text.split()):\n        if word in mispelled_words:\n            corrected_text.append(spell.correction(text))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ntrain[\"Cleaned_text\"] = train[\"Cleaned_text\"].apply(lambda x: correct_spell(x))\n'''","98be362d":"from keras.optimizers import Adam,SGD\nfrom tensorflow.keras import regularizers\n\nembedding_feature_vector = 200 # Since we used glove vector embedding of dim 200.\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_feature_vector,input_length=max_length,weights = [word_vector_matrix], trainable = False))\nmodel.add(Dropout(0.35))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())","d5debe66":"from tensorflow.keras.callbacks import *\nn_epoch = 30\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, \n                           mode='min', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='min')","ec5da6ec":"# Training the model\nhistory = model.fit(X_train,Y_train,validation_data=(x_valid,y_valid),callbacks=[reduce_lr,early_stop],epochs=n_epoch,batch_size= 64)","94417d54":"predictions = model.predict_classes(x_test)\ncr = classification_report(y_test,predictions)\nprint(cr)","eecd083b":"#weights_82 = model.save_weights('weights_82_81(82).h5')\n#model_82 = model.to_json()","1a2ef04d":"# Verifying the results.\nfrom tensorflow.keras.models import model_from_json\nmodel = model_from_json(model_82)\nmodel.load_weights('.\/weights_82_81(82).h5')\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())\nmodel.evaluate(x_test, y_test, batch_size=64, verbose=2)","4773cdaa":"acc_1 = 0.78084\nacc_2 = 0.78740\nacc_3 = 0.76378\nacc_4 = 0.78609\nacc_5 = 0.80709\nacc_6 = 0.79659\nacc_7 = 0.75590\nacc_8 = 0.80052\nacc_9 = 0.82021\nresults = pd.DataFrame([[\"Logistic Regression\",acc_1],[\"Naive Bayes\",acc_2],[\"Decision Trees\",acc_3],\n                       [\"Random Forest\",acc_4],[\"XGBoost\",acc_5],[\"CatBoost\",acc_6],\n                       [\"Passive Aggressor\",acc_7],[\"Voting Ensemble(LR+NB+XGB)\",acc_8],\n                       [\"LSTM\",acc_9]],columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\nresults.style.background_gradient(cmap='Blues')","b909610c":"# Making Predictions on test data\npredictions_test = pd.DataFrame(model.predict_classes(X_test))\ntest_id = pd.DataFrame(test[\"id\"])\nsubmission = pd.concat([test_id,predictions_test],axis=1)\nsubmission.columns = [\"id\",\"target\"]\nsubmission.to_csv(\"Submission.csv\",index=False)","a2cde98d":"submission.head()","af3aed89":"<h4>Let's further analyse top 50 words of disaster\/non-disastrous in training data:-<\/h4>\n","c7e34d29":"<h4>Inference:-<\/h4>\n\n\n*From the above distributions, it can be observed that the average word count for disaster tweets are found to be in the range(7-7.5) while for non-disaster tweets are in the range of (4.5-5).*","cb77cf35":"<h4>Below cell loads the training and testing data into variables train and test resp. using pandas.<\/h4>","2c61c7bd":"**3. Decision Trees**","bf1f4a21":"**8. Voting Ensemble**","74cbf1d8":"<strong> Let's check one example whether there are any change occured or not<\/strong>","d88f1ce7":"<h4> Let's first start by analysing top N-grams using Bag Of Words.<\/h4>\n","ee6be84b":"**Let's perform data encoding using bow and tf-idf.** \n\nThe function accepts cleaned training and testing data, boolean flags for selecting type of encoding as input and outputs cleaned text.","add6e1f6":"<h4>Inference:-<\/h4>\n\n*The above Bar Chart displays the top 20 Locations in tweets. From the chart, it is observed that <strong>the most <\/strong> occuring\/referred location is <strong>USA\/United States (68)<\/strong> while <strong>the least occuring <\/strong> are <strong>Los Angeles and California (5)<\/strong>.*","16b8c98a":"## If you like my work\/notebook, please upvote it!!!!","89f5e822":"<h4> Conclusion <\/h4>\n\nFrom the above table, it appears that <strong>LSTM model <\/strong>performs the best by achieving an overall accuracy of about <strong>82.02% <\/strong> while <strong>Passive Aggresive Model <\/strong> performs less accurately by outputing an accuracy score of about <strong>75.59% <\/strong>. \n\n<h5> Since, the LSTM Model shows robust performance, therefore it is selected as the final prediction model. <\/h4>","61bd07ae":"<h4> Let's explore the Target Variable <\/h4>","d9835d10":"<h4> Let's explore the columns in the dataset <\/h4>\n\n*  id      : A unique identifier for each tweet.   \n*  keyword : A particular keyword from the tweet (may be blank).\n*  location: The location the tweet was sent from (may be blank).\n*  text    : The text of the tweet.\n*  target  : This denotes whether a tweet is about a real disaster (1) or not (0). ","d682466d":"# Chapter 1: Exploring the Data","5f67a8d8":"**2. Naive Bayes**","a48358ab":"<h4> Plotting top 20 N-grams <\/h4>\n","42944161":"<h4>Let's  analyze the keywords column:-<\/h4>\n\n","cf6c07b8":"**6. CatBoost (Untuned)**","727c73e9":"<h4>Using Word Embedding using Glove Vectors for Encoding:-<\/h4>","435b9f26":"<h4>Inference:-<\/h4>\n\n*The above Bar Chart represents the top 20 disastrous keywords in text. From the bar chart, it is observed that <strong>the most <\/strong> occuring keywords are <strong>derailment,wreckage,outbreak (35+)<\/strong> while <strong>the least occuring <\/strong>are <strong>sandstorm and evacuation (28)<\/strong>.*","57ff13cb":"**1. Logistic Regression**","68a0aa08":"<h4>Inference:-<\/h4>\n\n*The above Bar Charts displays the top 50 Words post cleaning in text. From the chart, it is observed that <strong>the most <\/strong> occuring word for disaster tweets is: <strong> fire (250+)<\/strong> and for non-disaster is :<strong> like (250+) <\/strong>while <strong>the least occuring <\/strong> are <strong> collapse, atomic (80)<\/strong> for disaster and <strong> feel, really etc. (80) <\/strong> for non-disastrous.*","fe6aa46a":"# Chapter 3: Getting Ready!!!! \n\n<h4> Let's transform the data in numerical format that is suitable for prediction by models. \n     We will use following techniques to transform the data.<\/h4>\n\n* BOW.\n* TF-IDF.\n* Word Embbedding using Glove Vectors.","f0599e13":"<h4> Let's analyze common words after cleaning of text using Word Cloud<\/h4>","adf4766a":"# About the Notebook\n\n<h2> This is a step by step guide on Natural Language Processing with Disaster Tweets. <\/h2>\n\n<h3>The aim of this challenge is to build a text classifier to predict disaster and non-disastrous tweets in text.<\/h3>\n\n<h3>This notebook is divided into following 4 sections:<\/h3>\n\n* **Exploring the Data** : In this section, we will explore the data using various visualization plots to gain an insight on our data.\n* **Removing the Garbage**: In this section, we will clean the data and remove the noise in the data.\n* **Getting Ready**: In this section, we will transform the data in a suitable format for prediction.\n* **The Final Stage**: In this section, we will implement various ML\/DL models to classify textual data.","345643c2":"<h4>Let's start by analysing total number of characters in text.<\/h4>","467cb8da":"<h4>Inference:-<\/h4>\n\n\n*From the above histograms, it can be observed that the words count for disaster and non-disaster tweets are in the range of (15-20).*","0933cbbb":"# Chapter 4: The Final Stage \n\n<h4> Let's create some Models !!! <\/h4>\n","845ec060":"# Chapter 2: Removing the Garbage\n\n<h4> Let's Clean the Data. Following operations are carried out on text column for performing Data Cleaning <\/h4>\n\n* Removal of URL's.\n* Removal of HTMl tags.\n* Removal of Emoji's.\n* Filtering out miscellaneous text.\n* Lowering the text.\n* Performing Stemming (in case of bag of words(bow) and tf-idf) and lemmatization for (LSTM).\n* Discarding words of length < 2.\n\n\nNote: The Stemming process is performed for bow and tf-idf because there is no need of meaningful words while lemmatization is performed for LSTM because we do require meaningful words (discussed in subsequent sections).\n","f77ec48c":"**5. XGBOOST (Untuned)**","4151bb55":"<h4>Inference:-<\/h4>\n\n*The above Bar Charts displays the top 20 N-Grams. From the chart, it is observed that <strong>the most <\/strong> occuring unigram in text is: <strong> fire (350+)<\/strong>, for bigram, it is :<strong> suicide bomber (60) <\/strong> and for trigram, it is <strong> liked youtube video <\/strong> while <strong> the least <\/strong> is <strong> burning (100) <\/strong> for unigram, it is <strong> home-razed (29)<\/strong> for bigram and it is <strong>wreckage conclusively confirmed (24)<\/strong> for trigrams.*","ff29f591":"<h4>Now, Let's Compare the performance of various ML\/DL Models<\/h4>","5f98f1be":"<h4>Inference:-<\/h4>\n\n*The above displays top common words post the cleaning of text in training data. The most occuring words are: <strong>amp, new, one, people, time etc. <\/strong>.*","abd5cee9":"**Let's initialize a Tokenizer to read all the words in the text.**","db76386f":"<h4>Inference:-<\/h4>\n\n*The above Bar Charts displays the top 10 punctuations in tweets. From the bar chart, it is observed that <strong>the most <\/strong> occuring punctuation in both disaster\/non-disaster tweets is <strong>\"-\"(350+)<\/strong> while <strong>the least occuring <\/strong>for non-disaster are <strong>\"%\",\"\/:\",\"$\",\"_\"<\/strong> and for disaster tweets is <strong>\"=>\", \")\"<\/strong>.*","435cfd83":"**In the above code, we have selected n-grams to be (1,1) as starting point, but feel free to experiment with other options.**","bff3bd2d":"**7. Passive Agressive Classifier**","0f113e6c":"<h4>Let's analyze Locations column:-<\/h4>","ea97e3b8":"<h4>Inference:-<\/h4>\n\n*The above pictorial representation displays the missing values in dataset. From the distribution, it is observed that columns <strong>Keyword and Location<\/strong> contains  missing values. For training data, the % of missing values  are <strong>97.6 for \"location\" and 24 for \"keywords\"<\/strong> while for testing data, it is  <strong>97.7% for \"location\" and 23% for keywords<\/strong>. Also, the column having <strong>maximum missing values is: location<\/strong> while <strong>Keywords column has the minimum<\/strong> count of missing values for both sets of data.*","7154d9c8":"<h4>Inference:-<\/h4>\n\n*The above Bar Charts displays the top 10 stop words in tweets. From the bar chart, it is observed that <strong>the most frequently<\/strong> occuring stopword in both disaster\/non-disaster tweets is <strong>\"the\"(1000+)<\/strong> while <strong>the least occuring <\/strong>for non-disaster is <strong>\"for\"(400)<\/strong> and for disaster tweets is <strong>\"is\"(300)<\/strong>.*","cce40d14":"<h4> Let's take a look at Cleaned Data <\/h4>","f5d11572":"<h4>Inference:-<\/h4>\n\n*From the Countplot, it can be observed that the total number of samples in Target Class 1 is around 3200 while in Target Class 0, it is about 4500. Also, the classes seems to be in balanced state.*","326b70d2":"<h4>Inference:-<\/h4>\n\n*From the above histograms, it can be observed that the characters count for disaster and non-disaster tweets are in the range of (120,140).*","88eada58":"<h3>Let's explore the data further in depth:-<\/h3>\n\n<h4>Below sections perform following analysis:<\/h4>\n\n* Stop Words Analysis\n* Punctuations Analysis\n* Analysis of Missing words.","399f40fc":"**4. Random Forest (Untuned)**","ef73cb38":"**9. Long Short Tem Memory (LSTM)**","14f33610":"<strong> From above chart, it appears that our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model, resulting in false prediction. Now, we will further remove some confusing words from text based on above charts.<\/strong>"}}