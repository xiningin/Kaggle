{"cell_type":{"a0929266":"code","77842f31":"code","468fcc8b":"code","25ef10b8":"code","3eb2872b":"code","8091518c":"code","0a6e3134":"code","f31c52b3":"code","be280a7d":"code","6c31b2ac":"code","fef3f3f3":"code","dde4af2e":"code","8d89de02":"code","08da3e2a":"code","f012038e":"code","ae9d1a07":"code","47a2fcf1":"code","5cdad453":"code","be389f0b":"code","952e91e2":"code","11b9f13d":"code","fbc3c940":"code","d97d6d26":"code","7d471474":"code","b79671d6":"code","36322612":"code","5af7ee6e":"code","242f8489":"code","4cb7545e":"code","1d96bf28":"code","463f2ead":"code","11417e93":"code","fba2198e":"code","f5582031":"code","47f1ebdd":"code","9735c01d":"code","98795cd0":"code","686b2b81":"code","f9b742ec":"code","b8e3b021":"code","cadafa98":"code","11f8b116":"code","21b21a0b":"code","c8d8d3ed":"code","c08a5f5b":"code","e6624ecd":"markdown","5e32742d":"markdown","04e7b947":"markdown","4019f6fa":"markdown","a170d7fa":"markdown","07c7790f":"markdown","eb61c0cc":"markdown","34198518":"markdown"},"source":{"a0929266":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.neighbors import DistanceMetric\nfrom math import radians\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_moons, make_blobs\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import make_scorer, classification_report, make_scorer, recall_score, f1_score\nfrom sklearn.metrics import roc_curve,auc","77842f31":"df = pd.read_csv(\"\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\naus_town_gps = pd.read_csv(\"..\/input\/aus-town-gps\/aus_town_gps.csv\",sep=\",\")\nclimatsaus = pd.read_csv(\"..\/input\/climatsaus-v2\/climatsAUS_v2.csv\",sep=\";\")","468fcc8b":"df.info()","25ef10b8":"df.describe(include='object')","3eb2872b":"#Attribution de la classe de climat (classification de K\u00f6ppen) pour chaque ville\nclimatsaus.head()","8091518c":"# Pour simplifier, on regroupe les climats en 4 cat\u00e9gories : chaud_humide, temp\u00e9r\u00e9_froid, sec et m\u00e9diterran\u00e9en. On pourra ainsi faire des visualisations plus facilement.\n\nclimats_type = {'Am':'chaud_humide',\n                'Aw':'chaud_humide',\n                'Cfa':'chaud_humide',\n                'Cfb':'temp\u00e9r\u00e9_froid', \n                'Cfc':'temp\u00e9r\u00e9_froid', \n                'BSh':'sec',\n                'BSk':'sec',\n                'Bsk':'sec', \n                'Bwh':'sec',\n                'Csa':'m\u00e9diterran\u00e9en',\n                'Csb':'m\u00e9diterran\u00e9en'              \n               }\n\nclimatsaus['Clim_type']=climatsaus['Climat_Koppen'].map(climats_type)","0a6e3134":"#Fusion des dataframes\n\ndf = pd.merge(df, aus_town_gps, how='left', left_on=\"Location\",right_on=\"Location\")\ndf = pd.merge(df, climatsaus, how='left', left_on=\"Location\",right_on=\"Location\")\ndf.head(10)","f31c52b3":"df.info()","be280a7d":"#cr\u00e9ation de quelques variables de date et conversion de raintoday et raintomorrow en num\u00e9riques\ndf['RainToday_Num'] = (df['RainToday'] ==  'Yes')*1\ndf['RainTomorrow_Num'] = (df['RainTomorrow'] ==  'Yes')*1\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Mois'] = df['Date'].dt.month\ndf['Trimestre'] = df['Date'].dt.quarter\ndf['Annee'] = df['Date'].dt.year","6c31b2ac":"#cr\u00e9ation d'un dictionnaire associant la direction du vent \u00e0 l'angle correspondant (en degr\u00e9s) sur le cercle trigonom\u00e9trique (ie. E=0\u00b0 et rotation dans le sens direct)\nangles = {'E':0, \n          'ENE':22.5, \n          'NE':45, \n          'NNE':67.5, \n          'N':90, \n          'NNW':112.5, \n          'NW':135, \n          'WNW':157.5, \n          'W':180, \n          'WSW':202.5, \n          'SW':225, \n          'SSW':247.5, \n          'S':270, \n          'SSE':292.5, \n          'SE':315, \n          'ESE':337.5}\n\n#ajout des variables indiquant l'angle du vent au DF\ndf['WindGust_Ang']=df['WindGustDir'].map(angles)\ndf['Wind9am_Ang'] = df['WindDir9am'].map(angles)\ndf['Wind3pm_Ang'] = df['WindDir3pm'].map(angles)\n\n#ajout de variables correspondant au cosinos de l'angle (abscisse des coordonn\u00e9es trigo). Un cosinus n\u00e9gatif correspond \u00e0 un vent d'ouest, un cosinus positif \u00e0 un vent d'est.\ndf['WindGust_cos'] = np.cos(np.radians(df['WindGust_Ang']))\ndf['Wind9am_cos'] = np.cos(np.radians(df['Wind9am_Ang']))\ndf['Wind3pm_cos'] = np.cos(np.radians(df['Wind3pm_Ang']))\n\n#ajout de variables correspondant au sinus de l'angle (ordonn\u00e9e des coordonn\u00e9es trigo). Un sinus n\u00e9gatif correspond \u00e0 un vent de sud, un sinus positif \u00e0 un vent de nord.\ndf['WindGust_sin'] = np.sin(np.radians(df['WindGust_Ang']))\ndf['Wind9am_sin'] = np.sin(np.radians(df['Wind9am_Ang']))\ndf['Wind3pm_sin'] = np.sin(np.radians(df['Wind3pm_Ang']))","fef3f3f3":"df[\"LogRainfall\"] = np.log(df[\"Rainfall\"])\ndf[\"LogEvaporation\"] = np.log(df[\"Evaporation\"])\n\ndf= df.sort_values([\"Location\",\"Date\"])\ndf[\"Rain_J-1\"] = df[\"RainToday_Num\"].shift(1)\ndf[\"Rain_J-2\"] = df[\"RainToday_Num\"].shift(2)\ndf[\"Rain_J+2\"] = df[\"RainToday_Num\"].shift(-2)\ndf[\"Rain_J+3\"] = df[\"RainToday_Num\"].shift(-3)\n","dde4af2e":"df.describe()","8d89de02":"from sklearn.model_selection import train_test_split \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.preprocessing import LabelEncoder","08da3e2a":"df1 = df.dropna()\n\nlabelencoder = LabelEncoder()\ndf1['Clim_type1'] = labelencoder.fit_transform(df1['Clim_type'])\n\nfeatures = [\"RainToday_Num\",\"Rain_J-1\",\"MinTemp\",\"Mois\",\"Sunshine\",\"Humidity3pm\",\"Humidity9am\",\"Cloud3pm\",\"Cloud9am\", \n            \"WindGust_cos\",\"WindGust_sin\", \"Clim_type1\"]\n#features = [\"RainToday_Num\",\"MinTemp\",\"Mois\",\"Sunshine\"]\ntarget = df1[\"RainTomorrow_Num\"]\n#target = df1[\"Rain_J+3\"]\ndata = df1[features]","f012038e":"X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=123)\n\ndt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=3,random_state=123,min_samples_split=300,min_samples_leaf=100 \n                                    ,max_leaf_nodes=8\n                                    ,min_impurity_decrease=0.00001)\ndt_clf.fit(X_train , y_train)","ae9d1a07":"# Ins\u00e9rez votre code ici\n\ny_pred = dt_clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n## M\u00e9thode 2 : \u00e0 l'aide de pandas\ncm = pd.crosstab(y_test, y_pred, rownames=['Classe r\u00e9elle'], colnames=['Classe pr\u00e9dite'])\ncm","47a2fcf1":"feats = {}\nfor feature, importance in zip(data.columns, dt_clf.feature_importances_):\n    feats[feature] = importance \n    \nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\nimportances.sort_values(by='Importance', ascending=False).head(8)","5cdad453":"import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(20,15))\nplot_tree(dt_clf,feature_names = list(features),filled=True,proportion=True)\nplt.show()","be389f0b":"probs_dt = dt_clf.predict_proba(X_test)\n\nfpr_dt, tpr_dt, seuils_dt = roc_curve(y_test,probs_dt[:,1],pos_label=1)\n#fpr1, tpr1, seuils1 = det_curve(y_test,probs[:,1],pos_label=1)\nroc_auc_dt = auc(fpr_dt, tpr_dt)\n\nplt.figure(figsize=(20,6))\nplt.subplot(121)\nplt.plot(fpr_dt, tpr_dt, color='purple', lw=2, label='Decision Tree (auc = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Al\u00e9atoire (auc = 0.5)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taux faux positifs')\nplt.ylabel('Taux vrais positifs')\nplt.title('Courbe ROC')\nplt.legend(loc=\"lower right\")","952e91e2":"data.info()","11b9f13d":"df1 = df.dropna()\n\nlabelencoder = LabelEncoder()\ndf1['Clim_type1'] = labelencoder.fit_transform(df1['Clim_type'])\ndf1['Clim_type1'] = labelencoder.fit_transform(df1['Clim_type'])\ndf1['Cloud3pm_Num'] = df1['Cloud3pm']*1\ndf1['Cloud9am_Num'] = df1['Cloud9am']*1\n\n\n#features = [\"RainToday_Num\",\"MinTemp\",\"Mois\",\"Sunshine\",\"Humidity3pm\",\"Humidity9am\",\"Cloud3pm\",\"Cloud9am\", \"Clim_type1\"]\nfeatures = [\"RainToday_Num\",\"MinTemp\",\"Mois\",\"Sunshine\",\"Humidity3pm\",\"Humidity9am\",\"Clim_type1\"]\ntarget = df1[\"RainTomorrow_Num\"]\n#target = df1[\"Rain_J+3\"]\ndata = df1[features]\n\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=123)","fbc3c940":"std_scal = StandardScaler()\nX_train_std = std_scal.fit_transform(X_train)\nX_test_std = std_scal.transform(X_test)","d97d6d26":"from sklearn.linear_model import LogisticRegression\nLogistic = LogisticRegression(C =1.0)\nLogistic.fit(X_train , y_train)","7d471474":"# The estimated coefficients will all be around 1:\nprint(Logistic.coef_)","b79671d6":"# get importance\nimportance = abs(Logistic.coef_[0])\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,5))\nplt.barh(features, importance)\n\nplt.show()","36322612":"y_pred = Logistic.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n## M\u00e9thode 2 : \u00e0 l'aide de pandas\ncm = pd.crosstab(y_test, y_pred, rownames=['Classe r\u00e9elle'], colnames=['Classe pr\u00e9dite'])\ncm\n","5af7ee6e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","242f8489":"probs_Log = Logistic.predict_proba(X_test)\nfpr_Log, tpr_Log, seuils_Log = roc_curve(y_test,probs_Log[:,1],pos_label=1)\n#fpr1, tpr1, seuils1 = det_curve(y_test,probs[:,1],pos_label=1)\nroc_auc_Log = auc(fpr_Log, tpr_Log)\n","4cb7545e":"stats = pd.DataFrame()\n\nfor i in (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9):\n    y_preds = np.where(probs_Log[:,1]>i,1,0)\n    temp = pd.crosstab(y_test, y_preds, rownames=['Classe r\u00e9elle'], colnames=['Classe pr\u00e9dite'])\n    temp[\"Seuil\"] = i\n    stats = pd.concat([stats, temp])\n    cm = confusion_matrix(y_test, y_preds)\n    cm_display = ConfusionMatrixDisplay(cm).plot()\n    plt.title(\"Seuil :\")","1d96bf28":"i = 0.5\ny_preds = np.where(probs_Log[:,1]>i,1,0)\ncm = confusion_matrix(y_test, y_preds)\nTN = cm[0][0]\nFN = cm[0][1]\nFP = cm[1][0]\nTP = cm[1][1]\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP\/(TP+FN)\n# Specificity or true negative rate\nTNR = TN\/(TN+FP) \n# Precision or positive predictive value\nPPV = TP\/(TP+FP)\n# Negative predictive value\nNPV = TN\/(TN+FN)\n# Fall out or false positive rate\nFPR = FP\/(FP+TN)\n# False negative rate\nFNR = FN\/(TP+FN)\n# False discovery rate\nFDR = FP\/(TP+FP)\n\n# Overall accuracy\nACC = (TP+TN)\/(TP+FP+FN+TN)","463f2ead":"TNR","11417e93":"probs_tr = pd.qcut(probs_Log[:,1], 20, labels=False)\nstats1 = pd.crosstab(y_test, probs_tr, rownames=['Classe r\u00e9elle'], colnames=['Classe pr\u00e9dite'],normalize=\"columns\")\n\nstats = pd.DataFrame(list(zip(y_test,probs_Log[:,1],probs_tr)), columns = ['y_test','probs','probs_tr'])\n#stats = pd.concat([X_test,stats],axis=1)\nAd_Modele = stats.groupby(['probs_tr']).mean().reset_index()\nplt.plot(\"probs_tr\", \"probs\",data=Ad_Modele)\nplt.plot(\"probs_tr\", \"y_test\",data=Ad_Modele)","fba2198e":"from sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_test, probs_Log[:,1],pos_label=1)\nplt.figure(figsize=(20,6))\n#plt.plot(thresholds, recall, color='orange', lw=2,label='Tx vrais positifs')\n#plt.plot(thresholds, precision, color='blue', lw=2,label='Tx faux positifs')\nplt.plot(recall, precision, color='orange', lw=2, label='Mod\u00e8le Logistic')","f5582031":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nplt.plot(fpr_Log, tpr_Log, color='orange', lw=2, label='Mod\u00e8le Logistic (auc = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Al\u00e9atoire (auc = 0.5)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taux faux positifs')\nplt.ylabel('Taux vrais positifs')\nplt.title('Courbe ROC')\nplt.legend(loc=\"lower right\")\n\nplt.subplot(122)\nplt.plot(seuils_Log, tpr_Log, color='green', lw=2,label='Tx vrais positifs')\nplt.plot(seuils_Log, fpr_Log, color='red', lw=2,label='Tx faux positifs')\nplt.title('Taux de vrais et faux positifs en fonction du seuil choisi')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1])\nplt.xlabel('Seuils')\nplt.legend(loc=\"upper right\")\nplt.show();\n","47f1ebdd":"df1[\"Class\"] = df1[\"RainTomorrow_Num\"]\ndf1['Class'].replace({1: -1}, inplace=True)\ndf1['Class'].replace({0: 1}, inplace=True)","9735c01d":"target = df1['Class']\nsns.countplot(target);","98795cd0":"frac = df1[df1['Class']==-1]['Class'].count()\/df1['Class'].count()\nfrac","686b2b81":"target = df1['Class']\nfeatures = [\"RainToday_Num\",\"MinTemp\",\"Mois\",\"Sunshine\",\"Humidity3pm\",\"Humidity9am\",\"Clim_type1\"]\nfeats = df1[features]\n\nX_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.20, random_state=66) ","f9b742ec":"from sklearn.ensemble import IsolationForest\nIsoF = IsolationForest(n_estimators=100,contamination=0.2)\nIsoF.fit(X_train)","b8e3b021":"y_pred = IsoF.predict(X_test)\n\npd.crosstab(y_test, y_pred, rownames=['Classes r\u00e9elles'], colnames=['Classes pr\u00e9dites'])","cadafa98":"print(classification_report(y_test, y_pred))","11f8b116":"# On d\u00e9coupe manuellement nos jeu de donn\u00e9es de validation crois\u00e9e au sein de l'\u00e9chantillon d'apprentissage\nskf = StratifiedKFold(n_splits=3) \nfolds = list(skf.split(X_train, y_train))\nforest = IsolationForest()\n\n# Dans la situation o\u00f9 on ne connait pas \u00e0 priori le param\u00e8tre de contamination,\n# on ajoutera la contamination dans la grille de recherche\n\n#resc = make_scorer(recall_score,pos_label=-1)\nresc = make_scorer(f1_score,pos_label=-1)\n\nparams = {'contamination': np.linspace(0.1, 0.8, 20), 'n_estimators': [200]}\n\nsearch = GridSearchCV(estimator=forest, param_grid=params, scoring=resc, cv=folds, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\n# predict\noptimal_forest = search.best_estimator_\ny_pred = optimal_forest.predict(X_test)\n\n\npd.crosstab(y_test, y_pred, rownames=['Classes r\u00e9elles'], colnames=['Classes pr\u00e9dites'])","21b21a0b":"print(classification_report(y_test, y_pred))","c8d8d3ed":"probs_IsoForest = optimal_forest.score_samples(X_test)\nfpr_IsoForest, tpr_IsoForest, seuils_IsoForest = roc_curve(y_test,probs_IsoForest)\nroc_auc_IsoForest = auc(fpr_IsoForest, tpr_IsoForest)","c08a5f5b":"plt.figure(figsize=(20,6))\nplt.plot(fpr_Log, tpr_Log, color='red', lw=2, label='Mod\u00e8le Logistic (auc = %0.2f)' % roc_auc_Log)\nplt.plot(fpr_dt, tpr_dt, color='purple', lw=2, label='Decision Tree (auc = %0.2f)' % roc_auc_dt)\nplt.plot(fpr_IsoForest , tpr_IsoForest , color='green', lw=2, label='Isolation Forest (auc = %0.2f)' % roc_auc_IsoForest)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Al\u00e9atoire (auc = 0.5)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taux faux positifs')\nplt.ylabel('Taux vrais positifs')\nplt.title('Courbe ROC')\nplt.legend(loc=\"lower right\")\n","e6624ecd":"# Algorithmes de d\u00e9tections d'anomalies","5e32742d":"**Chargements des jeux de donn\u00e9es :**\n - df : donn\u00e9es m\u00e9t\u00e9o en australie sur 10 ans\n - aus_town_gps : localisation des stations m\u00e9t\u00e9o (x,y) => ce jeu de donn\u00e9es va nous permettre de repr\u00e9senter les indicateurs sur une carte et de calculer des distances entre stations m\u00e9t\u00e9o\n - climatsaus : climat des stations m\u00e9teo ","04e7b947":"# Arbre de d\u00e9cision","4019f6fa":"## Import des fichiers de travail","a170d7fa":"# Regression logistique","07c7790f":"## Num\u00e9risation des vents","eb61c0cc":"Le jeu de donn\u00e9es poss\u00e8de 145 460 entr\u00e9es et 23 colonnes dont :\n- La date de l'observation.\n- La ville dans laquelle se situe la station m\u00e9t\u00e9o.\n- 20 variables d\u00e9crivant les conditions atmosph\u00e9riques du jour de l\u2019observation. *\n- La variable cible RainTomorrow dont la valeur (Yes ou No) indique s'il a plu le lendemain de l'observation.\n\n*Le jeu de donn\u00e9es contient un m\u00e9lange de variables explicatives cat\u00e9gorielles (type object) et de variables explicatives num\u00e9riques (type float64) :\n- 14 variables continues : MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Temp9am, Temp3pm\n- 2 variables discr\u00e8tes (Nombre d'octas, de 0 \u00e0 9) : Cloud9am, Cloud3pm\n- 4 variables cat\u00e9gorielles non-num\u00e9riques : WinGustDir, WindDir3am, WindDir3pm, RainToday.\n- Les valeurs de la variable RainToday (Yes, No) sont d\u00e9finies par la variable Rainfall (Yes si pr\u00e9cipitations > 1mm)*\n\nPlusieurs variables poss\u00e8dent de nombreuses valeurs manquantes qu'il faudra g\u00e9rer.\n\nLe jeu de donn\u00e9es comporte 3436 journ\u00e9es d'observations m\u00e9t\u00e9orologiques (entre d\u00e9cembre 2008 et juin 2017) r\u00e9alis\u00e9es par 49 stations m\u00e9t\u00e9o (Location).","34198518":"# Decouverte du dataframe - Pr\u00e9processing"}}