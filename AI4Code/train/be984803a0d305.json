{"cell_type":{"54d7f023":"code","e8c46067":"code","ef086fde":"code","05ce6f3a":"code","057bb7ff":"code","012e57c9":"code","61fa8794":"code","517f5b19":"code","af309d1d":"code","89c23b0b":"code","aac5efa0":"code","7cdd5dff":"code","da742f16":"code","18b6982a":"code","363a40e4":"code","4f33d7fa":"code","d516218b":"code","9763201f":"code","f844da01":"markdown","0e64843d":"markdown","80337903":"markdown","c5e1a56e":"markdown"},"source":{"54d7f023":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom wordcloud import WordCloud\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n","e8c46067":"data = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'latin')\ndata.columns = ['Sentiment','ID','Date','Query','User_ID','Text']\nprint('Length of Data',len(data))\nprint('Shape of the data',data.shape)\ndata.head()","ef086fde":"data = data.drop(['ID','Date','Query','User_ID'],axis=1)\ndata['Sentiment'] = data['Sentiment'].map({0:'Negative',4:'Postive'})\ndata.head()","05ce6f3a":"data['Sentiment'].value_counts().plot(kind='bar')","057bb7ff":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(text, stem=False):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n\ndata['C_Text'] = data['Text'].apply(preprocess)","012e57c9":"data.head()","61fa8794":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.Sentiment == 'Negative'].C_Text))\nplt.imshow(wc , interpolation = 'bilinear')","517f5b19":"\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.Sentiment == 'Postive'].C_Text))\nplt.imshow(wc , interpolation = 'bilinear')","af309d1d":"X = data['C_Text']\nY = data['Sentiment']\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n\nprint(x_train.shape)\nprint(x_test.shape)","89c23b0b":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index # Contains the index of each word\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size', vocab_size)","aac5efa0":"max_seq_len = 30\nman_nb_words = 100000\nx_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_seq_len)\nx_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_seq_len)\n\nprint(x_train.shape)\nprint(x_test.shape)","7cdd5dff":"encoder = LabelEncoder()\n\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(y_test)\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(y_train.shape)\nprint(y_test.shape)","da742f16":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","18b6982a":"glove_emb = '\/kaggle\/working\/glove.6B.300d.txt'\nembedding_dim = 300\nlr = 1e-3\nbatch_size = 1024\nepochs = 15","363a40e4":"import tensorflow as tf\n\nembedding_index = {}\n\nf = open(glove_emb)\nfor line in f:\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()\n\nprint('Found word vectors', len(embedding_index))\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix],\n                                           input_length=max_seq_len, trainable=False)","4f33d7fa":"sequence_input = Input(shape=(max_seq_len,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","d516218b":"from tensorflow.keras.optimizers import Adam\nmodel.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, batch_size=1000, epochs=epochs,\n                    validation_data=(x_test, y_test))","9763201f":"Score = model.evaluate(x_test,y_test)\nprint(' Test Accuracy ', Score[1])","f844da01":"### Making the data ready for model\n\n- Train test split\n- Tokenization ( tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. The process is called Tokenization )\n- Sequence model ( We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from pad_sequence to do our job. It will make all the sequence in one constant length MAX_SEQUENCE_LENGTH)\n- Word Emdedding (Word Embedding is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Word Embedding is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.)\n\n(We could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use Transfer Learning. We download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like GloVe & Word2Vec gives more insights for a word which can be used for classification.)","0e64843d":"### Text Preprocessing\n\n- Stemming\/Lematization (Cutting the word \/ reducing the word to root word)\n- Removal of Hyperlinks and Mentions\n- Removal of stopwords & Puncations","80337903":"### Visualization","c5e1a56e":"# Twitter Sentiment Analysis using NLP & LSTM\n\n- By Sachin Saj"}}