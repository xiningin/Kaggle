{"cell_type":{"d1f1dd86":"code","1871457a":"code","2997517a":"code","8c9b49f6":"code","9def28c5":"code","5f457a0b":"code","bcf3284e":"code","85ca0298":"code","21585582":"code","75cef000":"code","c54199df":"code","f5b74c97":"code","172a60b3":"code","71189cd0":"code","1402670d":"code","f9482998":"code","61dfacc2":"code","f3a068bf":"code","55088477":"code","2c276768":"code","5e879bca":"code","75b377c4":"code","16d40711":"code","f174079a":"code","51b8e80f":"code","d3a1ade8":"code","00ce2bdd":"code","ea06144f":"markdown","3b6d2947":"markdown","2c4edcb4":"markdown","589c4fdc":"markdown","8b3aaca8":"markdown","afe7a5c5":"markdown","6ae31211":"markdown","65cd28c2":"markdown","92d38996":"markdown","a5cd0747":"markdown","a303ddd8":"markdown","fd8beb1c":"markdown","58da2703":"markdown","f91d96ff":"markdown"},"source":{"d1f1dd86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1871457a":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers","2997517a":"import warnings\nwarnings.filterwarnings('ignore')","8c9b49f6":"import pandas as pd\nimport seaborn as sns\ntrainDF = pd.read_csv('..\/input\/train_lyrics_1000.csv', usecols=range(7))\nvalidDF =pd.read_csv('..\/input\/valid_lyrics_200.csv')\nprint(\"Validation data with lyrics and labels\",validDF.head())\n#print(\"Trainning Data with lyrics,title,singer,genre and mood label\")\n# trainDF['lyrics'] = lyrics\n# trainDF['mood'] = mood\ntrainDF.head()","9def28c5":"train_x =trainDF['lyrics']\ntrain_y =trainDF['mood']\nvalid_x =validDF['lyrics']\nvalid_y =validDF['mood']","5f457a0b":"# label encode the target variable \nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)","bcf3284e":"import nltk\nimport string\nimport re\n\nporter_stemmer = nltk.stem.porter.PorterStemmer()\n\ndef porter_tokenizer(text, stemmer=porter_stemmer):\n    \"\"\"\n    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n    and applies the porter stemming algorithm to each of the obtained token. \n    Tokens that are only consisting of punctuation characters are removed as well.\n    Only tokens that consist of more than one letter are being kept.\n    \n    Parameters\n    ----------\n        \n    text : `str`. \n      A sentence that is to split into words.\n        \n    Returns\n    ----------\n    \n    no_punct : `str`. \n      A list of tokens after stemming and removing Sentence punctuation patterns.\n    \n    \"\"\"\n    lower_txt = text.lower()\n    tokens = nltk.wordpunct_tokenize(lower_txt)\n    stems = [porter_stemmer.stem(t) for t in tokens]\n    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n    return no_punct\n","85ca0298":"# Commented out to prevent overwriting files:\n#\n# stp = nltk.corpus.stopwords.words('english')\n# with open('.\/stopwords_eng.txt', 'w') as outfile:\n#    outfile.write('\\n'.join(stp))\n    \n    \nwith open('..\/input\/stopwords_eng.txt', 'r') as infile:\n    stop_words = infile.read().splitlines()\nprint('stop words %s ...' %stop_words[:5])","21585582":"# create a count vectorizer object \n#analyzer='word',\n                      \n                      \n                       \n                     \ncount_vect = CountVectorizer(analyzer='word',preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),stop_words=stop_words,\n            tokenizer=porter_tokenizer,ngram_range=(1,1))\ncount_vect.fit(trainDF['lyrics'])\n\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(train_x)\nxvalid_count =  count_vect.transform(valid_x)","75cef000":"# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), max_features=5000)\ntfidf_vect.fit(trainDF['lyrics'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\n\n# ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), ngram_range=(2,2), max_features=5000)\ntfidf_vect_ngram.fit(trainDF['lyrics'])\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), ngram_range=(1,1), max_features=5000)\ntfidf_vect_ngram_chars.fit(trainDF['lyrics'])\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) ","c54199df":"#sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve,f1_score\nfrom sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#load package\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from math import sqrt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","f5b74c97":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model. RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n   #tree.ExtraTreeClassifier(),\n    \n    ]","172a60b3":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf.toarray(), train_y).predict(xvalid_tfidf.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(xtrain_tfidf.toarray(), train_y), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(xvalid_tfidf.toarray(), valid_y), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n    MLA_compare.loc[row_index, 'MLA F1 Score'] = f1_score(valid_y, predicted)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","71189cd0":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf.toarray(), train_y).predict(xvalid_tfidf.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","1402670d":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_count.toarray(), train_y).predict(xvalid_count.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(xtrain_count.toarray(), train_y), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(xvalid_count.toarray(), valid_y), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n    MLA_compare.loc[row_index, 'MLA F1 Score'] = f1_score(valid_y, predicted)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","f9482998":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_count.toarray(), train_y).predict(xvalid_count.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","61dfacc2":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf_ngram.toarray(), train_y).predict(xvalid_tfidf_ngram.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(xtrain_tfidf_ngram.toarray(), train_y), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(xvalid_tfidf_ngram.toarray(), valid_y), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n    MLA_compare.loc[row_index, 'MLA F1 Score'] = f1_score(valid_y, predicted)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","f3a068bf":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf_ngram.toarray(), train_y).predict(xvalid_tfidf_ngram.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","55088477":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf_ngram_chars.toarray(), train_y).predict(xvalid_tfidf_ngram_chars.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(xtrain_tfidf_ngram_chars.toarray(), train_y), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(xvalid_tfidf_ngram_chars.toarray(), valid_y), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(valid_y, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n    MLA_compare.loc[row_index, 'MLA F1 Score'] = f1_score(valid_y, predicted)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","2c276768":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(xtrain_tfidf_ngram_chars.toarray(), train_y).predict(xvalid_tfidf_ngram_chars.toarray())\n    fp, tp, th = roc_curve(valid_y, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","5e879bca":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, valid_y)","75b377c4":"# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\naccuracy1 = train_model(naive_bayes.BernoulliNB(), xtrain_count, train_y, xvalid_count)\nprint (\"MultinomialNB, Count Vectors: \", accuracy)\nprint (\"BernoulliNB, Count Vectors: \", accuracy1)\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\naccuracy1 = train_model(naive_bayes.BernoulliNB(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"MultinomialNB, WordLevel TF-IDF: \", accuracy)\nprint (\"BernoulliNB, WordLevel TF-IDF: \", accuracy1)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\naccuracy1 = train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint (\"MulitnomialNB, N-Gram Vectors: \", accuracy)\nprint (\"BernoulliNB, N-Gram Vectors: \", accuracy1)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\naccuracy1 = train_model(naive_bayes.BernoulliNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\nprint (\"MultinomialNB, CharLevel Vectors: \", accuracy)\nprint (\"BernoulliNB, CharLevel Vectors: \", accuracy1)","16d40711":"accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\nprint (\"LR, Count Vectors: \", accuracy)\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"LR, WordLevel TF-IDF: \", accuracy)\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint (\"LR, N-Gram Vectors: \", accuracy)\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\nprint (\"LR, CharLevel Vectors: \", accuracy)","f174079a":"accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"SVM, WordLevel TF-IDF: \", accuracy)\n\naccuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\nprint (\"SVM, Count Vectors: \", accuracy)\n\n# SVM on Ngram Level TF IDF Vectors\naccuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\nprint (\"SVM, N-Gram Vectors: \", accuracy)","51b8e80f":"accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\nprint (\"RF, Count Vectors: \", accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint (\"RF, WordLevel TF-IDF: \", accuracy)","d3a1ade8":"# Extereme Gradient Boosting on Count Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\nprint (\"Xgb, Count Vectors: \", accuracy)\n\n# Extereme Gradient Boosting on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\nprint (\"Xgb, WordLevel TF-IDF: \", accuracy)\n\n# Extereme Gradient Boosting on Character Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\nprint (\"Xgb, CharLevel Vectors: \", accuracy)","00ce2bdd":"def create_model_architecture(input_size):\n    # create input layer \n    input_layer = layers.Input((input_size, ), sparse=True)\n    \n    # create hidden layer\n    hidden_layer = layers.Dense(1000, activation=\"relu\")(input_layer)\n    \n    # create output layer\n    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n\n    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    return classifier \n\nclassifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\naccuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\nprint (\"NN, Ngram Level TF IDF Vectors\",  accuracy)\n","ea06144f":"**1. Dataset preparation**\nFor the purpose of this project, I am the using dataset of music lyrics which can be obtained from sebastain rashka github. The dataset consists of 100k text lyrics and their labels as happy and sad, we will use whole data. To prepare the dataset, load the downloaded data into a pandas dataframe containing two columns \u2013 text and label. ","3b6d2947":"Lets implement these models and understand their details. The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed.","2c4edcb4":"3.4 Bagging Model\nImplementing a Random Forest Model\n\nRandom Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family.","589c4fdc":"**3. Model Building**\nThe final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n\nNaive Bayes Classifier\nLinear Classifier\nSupport Vector Machine\nBagging Models\nBoosting Models\nShallow Neural Networks","8b3aaca8":"Implementing a naive bayes model using sklearn implementation with different features\n\nNaive Bayes is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature ","afe7a5c5":"**2. Feature Engineering**","6ae31211":"**3.2 Linear Classifier**\nImplementing a Linear Classifier (Logistic Regression)\n\nLogistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic\/sigmoid function. One can read more about logistic regression here","65cd28c2":"**2.1 Count Vectors as features**\nCount Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document.","92d38996":"3.5 Boosting Model\nImplementing Xtereme Gradient Boosting Model\n\nBoosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing)","a5cd0747":"**3.6 Shallow Neural Networks**\nA neural network is a mathematical model that is designed to behave similar to biological neurons and nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled data. A shallow neural network contains mainly three types of layers \u2013 input layer, hidden layer, and output layer. ","a303ddd8":"The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n\n2.1 Count Vectors as features\n2.2 TF-IDF Vectors as features\n\nWord level\nN-Gram level\nCharacter level\n2.3 Word Embeddings as features\n2.4 Text \/ NLP based features\n2.5 Topic Models as features\n\nLets look at the implementation of these ideas in detail.","fd8beb1c":"**2.2 TF-IDF Vectors as features**\nTF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n\nTF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\nIDF(t) = log_e(Total number of documents \/ Number of documents with term t in it)\n\nTF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n\na. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\nb. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\nc. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus","58da2703":"libraries for dataset preparation, feature engineering, model training ","f91d96ff":"3.3 Implementing a SVM Model\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane \/ line that segregates the two classes"}}