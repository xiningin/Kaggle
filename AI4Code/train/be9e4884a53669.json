{"cell_type":{"6508bae4":"code","f1ae7c44":"code","23709a5e":"code","e67d2003":"code","54cae9c3":"code","e5bed3c3":"code","4b479897":"code","6a630e9c":"code","0bfbd51e":"code","dd4eaeff":"code","13afa3c8":"code","014aa134":"code","da5eeb4d":"code","1f741a45":"code","0ddf20a2":"code","4b463c90":"code","72630305":"code","da4ce854":"code","4b034ab8":"code","d5199052":"code","a325e1e2":"code","0bb9d276":"code","c1ca027c":"code","6f08d1ac":"code","0c5e2e8b":"code","9cea38d1":"code","2e8f2711":"code","87cfb096":"code","55cf48bb":"code","0e548d7b":"code","7c48635a":"code","827126be":"code","7edb605d":"code","9e488b35":"code","fb35a56b":"code","f30024da":"code","78a4e613":"code","089804c2":"code","45ebe1c1":"code","35e4dd81":"code","788ebab5":"code","8d4b4bc1":"code","522cd8cd":"code","42e0171a":"code","3f62e9e1":"code","866e0b6a":"code","acf0c3a7":"markdown","c2735deb":"markdown","2f58f48b":"markdown","0d1fd688":"markdown","8a343998":"markdown","b24c4a3d":"markdown","69118fd8":"markdown","2bdbf385":"markdown","bbe5c3ed":"markdown","22de5e73":"markdown","6ae0fd30":"markdown","0605089c":"markdown","3935bbaa":"markdown","54661072":"markdown","e53cbf07":"markdown","e5d3ba24":"markdown","915011fb":"markdown","54f38c4d":"markdown","50876ebd":"markdown","0bf04100":"markdown","a33686ef":"markdown","8c148582":"markdown","eefda68b":"markdown","e055ea9b":"markdown","069e8de7":"markdown","9c49d539":"markdown","6d498559":"markdown","512cc4cb":"markdown","0ed9e1be":"markdown"},"source":{"6508bae4":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set_style(style='darkgrid')\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import plot_tree \nfrom sklearn import svm\nfrom sklearn.metrics import log_loss\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import plot_confusion_matrix \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.model_selection import GridSearchCV\nfrom math import sqrt","f1ae7c44":"heart = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","23709a5e":"heart.head()","e67d2003":"heart.shape # To see now of rows and columns","54cae9c3":"heart.describe() # StatisticalSummary","e5bed3c3":"heart.isnull().sum() # Checking Missing Values","4b479897":"plt.figure(dpi=80)\nsns.heatmap(heart.isnull())\nplt.show()","6a630e9c":"heart.dtypes","0bfbd51e":"#Age\nplt.figure(dpi=125)\nsns.distplot(a=heart['age'],kde=False,bins=20)\nplt.axvline(x=np.mean(heart['age']),c='green',label='Mean Age of all People')\nplt.legend()\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of Age')\nplt.show()","dd4eaeff":"plt.figure(dpi=125)\nmale =len(heart[heart['sex'] == 1])\nfemale = len(heart[heart['sex']== 0])\nsns.countplot('sex',data = heart,)\nplt.xlabel('Sex Female-0, Male-1')\nplt.ylabel('Count')\nplt.title('Count of Sex')\nMale, Female =heart.sex.value_counts()\nprint('Female -',Female)\nprint('Male -',Male)\nplt.show()","13afa3c8":"plt.figure(dpi=125)\nsns.countplot('cp',data = heart,)\nplt.xlabel('Chest Pain - 0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic')\nplt.ylabel('Count')\nplt.title('Count of Chest Pain')\nA,B,C,D =heart.cp.value_counts()\n\nprint('Typical Angina -',A)\nprint('Atypical Angina -',C)\nprint('Non-Anginal Pain -',B)\nprint('Asymptomatic -',D)\n \nplt.show()","014aa134":"plt.figure(dpi=125)\nsns.countplot('fbs',data = heart,)\nplt.xlabel('Fasting Blood Pressure -  0 = >=120 mg\/dl,1 = <120 mg\/dl')\nplt.ylabel('Count')\nplt.title('Count of Fasting Blood Pressure')\nA,B =heart.fbs.value_counts()\n\nprint('Greater than 120 mg\/dl -',A)\nprint('Less than 120 mg\/dl-',B)\n\n \nplt.show()","da5eeb4d":"plt.figure(dpi=125)\nsns.countplot('exang',data = heart,)\nplt.xlabel(' Exercise Induced Angina - 0 = no, 1 = yes')\nplt.ylabel('Count ')\nplt.title('Count of Exercise Induced Angina')\nA,B =heart.exang.value_counts()\n\nprint('No -',A)\nprint('Yes -',B)\n\n \nplt.show()","1f741a45":"plt.figure(figsize=(14,7),dpi=100)\nsns.heatmap(np.round(heart.corr(),2), annot = True)\nplt.show()","0ddf20a2":"plt.figure(figsize=(12,6),dpi=100)\nsns.regplot(x='age',y='chol',data=heart,color='Green')\nplt.xlabel('Age')\nplt.ylabel('Cholesterol in mg\/dl')\nplt.title('Age vs Cholesterol')\nplt.show()","4b463c90":"plt.figure(figsize=(12,6),dpi=100)\nsns.regplot(x='age',y='thalach',data=heart,color='Blue')\nplt.xlabel('Age')\nplt.ylabel('Maximum heart rate achieved')\nplt.title('Age vs Max Heart Rate')\nplt.show()","72630305":"plt.figure(figsize=(12,6),dpi=100)\nsns.regplot(x='age',y='trestbps',data=heart,color='Red')\nplt.xlabel('Age')\nplt.ylabel('Resting blood pressure (in mm Hg)')\nplt.title('Age vs Resting Blood Pressure')\nplt.show()","da4ce854":"plt.figure(figsize=(12,6),dpi=100)\nsns.scatterplot(x='chol',y='thalach',data=heart,hue='target')\nplt.xlabel('Cholesterol (in mg\/dl)')\nplt.ylabel('Resting blood pressure (in mm Hg)')\nplt.title('Scatter Plot for Cholesterol and Resting blood pressure')\nplt.show()","4b034ab8":"plt.figure(figsize=(12,6),dpi=100)\nsns.swarmplot(x='sex',y='chol',data=heart,hue='target')\nplt.xlabel('Sex Female-0, Male-1')\nplt.ylabel('Cholesterol in mg\/dl')\nplt.show()","d5199052":"plt.figure(figsize=(12,6),dpi=100)\nsns.swarmplot(x='sex',y='thalach',data=heart,hue='target',dodge=False)\nplt.xlabel('Sex Female-0, Male-1')\nplt.ylabel('Maximum heart rate achieved')\nplt.show()","a325e1e2":"plt.figure(figsize=(12,6),dpi=100)\nsns.swarmplot(x='sex',y='trestbps',data=heart,hue='target',dodge=False)\nplt.xlabel('Sex Female-0, Male-1')\nplt.ylabel('Resting blood pressure (in mm Hg)')\nplt.show()","0bb9d276":"X= heart.drop('target',axis=1)\ny= heart['target']","c1ca027c":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nclf_dt = DecisionTreeClassifier(random_state=42)\nclf_dt = clf_dt.fit(X_train, y_train)","6f08d1ac":"plot_confusion_matrix(clf_dt, X_test, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])","0c5e2e8b":"path = clf_dt.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas = ccp_alphas[:-1]\n\nclf_dts = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf_dt.fit(X_train, y_train)\n    clf_dts.append(clf_dt)","9cea38d1":"train_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\ntest_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","2e8f2711":"clf_dt_pruned = DecisionTreeClassifier(random_state=42, \n                                       ccp_alpha=0.02)\nclf_dt_pruned = clf_dt_pruned.fit(X_train, y_train) ","87cfb096":"plot_confusion_matrix(clf_dt_pruned, \n                      X_test, \n                      y_test, \n                      display_labels=[\"Does not have HD\", \"Has HD\"])","55cf48bb":"DT_score = clf_dt_pruned.score(X_test, y_test)\nprint(\"Decision Tree Accuracy:\" , DT_score)","0e548d7b":"plt.figure(figsize=(15,7.5))\nplot_tree(clf_dt_pruned, \n          filled=True, \n          rounded=True, \n          class_names=[\"No HD\", \"Yes HD\"], \n          feature_names=X.columns)","7c48635a":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","827126be":"LR_score = LR.score(X_test, y_test)\nprint(\"Logistic Regression Accuracy:\" ,LR_score)","7edb605d":"ylr = LR.predict(X_test)","9e488b35":"jaccard_score(y_test, ylr)","fb35a56b":"plot_confusion_matrix(LR, \n                      X_test, \n                      y_test, \n                      display_labels=[\"Does not have HD\", \"Has HD\"])","f30024da":"print (classification_report(y_test, ylr))","78a4e613":"rfc =  RandomForestClassifier(random_state=42).fit(X_train, y_train)","089804c2":"print(\"Random Forest Accuracy: \", rfc.score(X_test,y_test))","45ebe1c1":"print('Parameters currently in use')\nprint(rfc.get_params())","35e4dd81":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","788ebab5":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrfc = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train,y_train)","8d4b4bc1":"rf_random.best_params_","522cd8cd":"rfc_n =  RandomForestClassifier(n_estimators= 1000,min_samples_split = 5,min_samples_leaf = 2,max_depth = 20,bootstrap =True,random_state=42).fit(X_train, y_train)","42e0171a":"RF_score = rfc_n.score(X_test,y_test)\nprint(\"Random Forest Accuracy: \", RF_score)","3f62e9e1":"data = [['Decision Tree', DT_score], ['Logistic Regression', LR_score], ['Random Forest', RF_score]] \naccuracy = pd.DataFrame(data,columns = ['Model', 'Accuracy',])\naccuracy.head()","866e0b6a":"fig= plt.figure(dpi=100)\nsns.barplot(x=accuracy['Model'],y=accuracy['Accuracy'])\nplt.title('Model Accuracy Comparison')\nplt.show()","acf0c3a7":"<h2 style='color:red'><center>Let's do Univariate analysis to understand our columns better<\/center><h2>\n","c2735deb":"**The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models.**\n\n**C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. \nNow lets fit our model with train set.**","2f58f48b":"<h1 style='background:brown; border:0; color:white'><center>HEART DISEASE : ANALYSIS AND PREDICTION<\/center><h1>\n","0d1fd688":"**So, There were 303 People out of which Mean Age is approx 54**\n\n**Now, lets find the count of Sex distribution**","8a343998":"Now let's discuss how to interpret the tree.\nIn each node, we have:\n- The variable (column name) and the threshold for splitting the observations. For example, in the tree's root, we use **ca** to split the observations. \n- **gini** is the gini index or score for that node\n- **samples** tell us how many samples are in that node\n- **value** tells us how many samples in the node are in each category. In this example, we have two categories, **No** and **Yes**, referring to whether or not a patient has heart disease. The number of patients with **No** comes first because the categories are in alphabetical order. Thus, in the root, 93 patients have **No** and 134 patients have **Yes**.\n- **class** tells us whichever category is represented most in the node. \n\nThe leaves are just like the nodes, except that they do not contain a variable and threshold for splitting the observations.\n\nThe nodes and leaves are colored by the **class**. In this case **No** is different shades of orange-ish and **Yes** is different shades of blue. \n\nThe the darker the shade, the lower the **gini** score, and that tells us how much the node or leaf is skewed towards one class.","b24c4a3d":"**Understanding the Confusion Matrix**\n\n**Out of those 76 cases, the classifier predicted 'Has HD' 40 times, and 'Doesn't have' 36 times.**\n\n**In reality, 41 patients in the sample have the Heart disease, and 35 patients do not.**","69118fd8":"**Now, let's try Random Forest and compare it with Logistic Regression in this case.**","2bdbf385":"<h2 style='background:black; border:0; color:white'><center>THANK YOU !<\/center><h2>","bbe5c3ed":"![download.jpg](data:image\/jpeg;base64,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)","22de5e73":"**Lets try jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by the size of the union of two label sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.**","6ae0fd30":"<h1 style='background:green; border:0; color:white'><center>DECISION TREE<\/center><h1>","0605089c":"**Understanding all the columns**\n- **age**,\n- **sex**, 0 = female, 1 = male\n- **cp**, chest pain, 0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic\n- **trestbps**, resting blood pressure (in mm Hg)\n- **chol**, serum cholesterol in mg\/dl\n- **fbs**, fasting blood sugar 0 = >=120 mg\/dl , 1= <120 mg\/dl \n- **restecg**, resting electrocardiographic results 1 = normal, 2 = having ST-T wave abnormality, 3 = showing probable or definite left ventricular hypertrophy\n- **thalach**,  maximum heart rate achieved\n- **exang**, exercise induced angina, 0 = no, 1 = yes\n- **oldpeak**, ST depression induced by exercise relative to rest 1 = upsloping, 2 = flat, 3 = downsloping\n- **slope**, the slope of the peak exercise ST segment,1 = upsloping, 2 = flat, 3 = downsloping\n- **ca**, number of major vessels (0-3) colored by fluoroscopy\n- **thal**, this is short of thalium heart scan, 3 = normal (no cold spots), 6 = fixed defect (cold spots during rest and exercise), 7 = reversible defect (when cold spots only appear during exercise)\n- **target**, diagnosis of heart disease, the predicted attribute","3935bbaa":"**Since, it is a classification problem we need to choose a Classification algorithm.**\n\n**Let's start with Decision Trees then we will move on to Logistic Regression and Random Forest.**","54661072":"Trying to improve accuracy with Hyperparameter Tuning\n\nLearned about the Hyperparamter Tuning of the Random Forest -\n\nhttps:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","e53cbf07":"<h1 style='background:green; border:0; color:white'><center>RANDOM FOREST<\/center><h1>","e5d3ba24":"**Let's analyse the relationship of Age with Cholesterol, Max Heart Rate and Resting Blood Pressure.**","915011fb":"# **BIVARIATE ANALYSIS**","54f38c4d":"# **UNIVARIATE ANALYSIS**","50876ebd":"# **MODEL BUILDING**","0bf04100":"Based on the count of each section, we can calculate precision and recall of each label:\n\n\n- __Precision__ is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n- __Recall__ is true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\n    \nSo, we can calculate precision and recall of each class.\n\n__F1 score:__\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n\n**The F1score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.**\n\n\nAnd finally, we can tell the average accuracy for this classifier is the average of the f1-score for both labels, which is 0.87 in our case.","a33686ef":"<h2 style='color:red'><center>Let's do Bivariate analysis to understand the relationships between columns better<\/center><h2>","8c148582":"**Clearly Here, Logistic Regression performed well even we did a lot of tuning in Random Forest.**","eefda68b":"**Understanding the Confusion Matrix**\n\n**Out of those 76 cases, the classifier predicted 'Has HD' 43 times, and 'Doesn't have' 33 times.**\n\n**In reality, 41 patients in the sample have the Heart disease, and 35 patients do not.**","e055ea9b":"**Understanding the Confusion Matrix**\n\n**Out of those 76 cases, the classifier predicted 'Has HD' 43 times, and 'Doesn't have' 33 times.**\n\n**In reality, 41 patients in the sample have the Heart disease, and 35 patients do not.**","069e8de7":"**Now, Let's analyse the relationship of Sex with Cholesterol, Max Heart Rate and Resting Blood Pressure with respect to the Target**","9c49d539":"<h1 style='background:green; border:0; color:white'><center>LOGISTIC REGRESSION<\/center><h1>","6d498559":"# **MODEL ACCURACY COMPARISON**","512cc4cb":"**The term \"heart disease\" is often used interchangeably with the term \"cardiovascular disease.\" Cardiovascular disease generally refers to conditions that involve narrowed or blocked blood vessels that can lead to a heart attack, chest pain (angina) or stroke. Other heart conditions, such as those that affect your heart's muscle, valves or rhythm, also are considered forms of heart disease.**","0ed9e1be":"**Decision Trees are notorious for being overfit to the Training Dataset, so let's prune this tree in hopes that we can improve the accuracy with the Testing Dataset.**\n\n**Pruning a decision tree is all about finding the right value for the pruning parameter, alpha, which controls how little or how much pruning happens. One way to find the optimal value for alpha is to plot the accuracy of the tree as a function of different values for alpha. We'll do this for both the Training Dataset and the Testing Dataset.**"}}