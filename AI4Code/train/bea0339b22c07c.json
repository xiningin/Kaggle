{"cell_type":{"6414b8b7":"code","ecce6264":"code","b8414380":"code","0f197d29":"code","e1d75750":"code","b1b3da97":"code","6a039fcc":"code","936f46fb":"code","0c2a7651":"code","a1e66981":"code","28f5add1":"code","8ead82be":"code","0b3b87bf":"code","bca976c6":"code","2f43173e":"code","85b472ec":"code","71b2f4d7":"code","0bb3e3b7":"code","055f65e7":"code","a9cf8341":"code","e25f1e53":"code","55b1b6f5":"code","f7e9da70":"code","a161e8ac":"code","57b70eb3":"code","f6dc41bc":"code","f74a9029":"code","16f1c36c":"code","5fe1c698":"code","401723ed":"code","7a6a6b97":"code","46d1ec76":"code","0b9a3e1a":"code","0a936557":"code","538826f4":"code","b06162ac":"code","2f9eb5ba":"code","3e0247b7":"code","029e87a3":"code","41e8dbc4":"code","eef7f409":"code","2c0622f1":"code","2c8c3b38":"code","1b0314a1":"code","d493ae98":"code","f7acd098":"code","d0375077":"code","904431df":"code","49fc0e51":"code","1f0c3d5f":"code","1429cd2b":"code","7c27ea5f":"code","bb75d99f":"code","6474839b":"code","c073430c":"code","16a2d0b6":"code","fd43439e":"code","157ec013":"code","bdf32309":"code","9d5e3b04":"code","522483fb":"code","095902fa":"code","f61c0721":"code","74de87c4":"code","2cafea29":"code","30f1ae09":"code","8f1893c3":"code","01e3bf9e":"code","55078e00":"code","1b291a92":"code","98f9b4b4":"code","bd103a75":"code","a775e3a0":"code","18cf58a4":"code","da40ee88":"code","41b64075":"code","b7cd8939":"code","d3b35f4e":"code","63768481":"code","0192fc8b":"code","4364b77a":"code","a464fbc1":"code","a549d0f6":"code","1f1feb38":"code","9d4e330e":"code","41eb0964":"code","236db204":"code","c4380209":"code","2340d869":"code","63224c44":"code","729ee9d1":"code","dc650600":"code","87eff632":"code","68fd37c7":"code","553f5ac6":"code","e1f0477d":"code","514af2ce":"code","33381a1c":"code","f4811523":"code","99a76d41":"code","dd21962f":"code","4ed835d5":"code","aebeb5a0":"code","14275e89":"code","40b165a8":"code","1c443315":"code","c25e4701":"code","e3a45b61":"markdown","d01c4c6c":"markdown","3d51b67f":"markdown","5bdcfdb9":"markdown","fe383676":"markdown","d6bfea9e":"markdown","9decc92d":"markdown","209025a2":"markdown","f4c86d84":"markdown","02f8ff6e":"markdown","ce843164":"markdown","6fd12620":"markdown","1f32f6dd":"markdown","d1df7816":"markdown","c0240814":"markdown","e36c03ef":"markdown","915a82cb":"markdown","139f89a2":"markdown","2d355e30":"markdown","e0fbb04c":"markdown","decfc37f":"markdown","7a0b24f8":"markdown","6179d2e8":"markdown","227c0943":"markdown","cae81089":"markdown","beb78a8d":"markdown","8583e700":"markdown","23dd8267":"markdown","46d7984f":"markdown","dd26c791":"markdown","f301571c":"markdown","0246a31c":"markdown","ac88e84b":"markdown","ce04fe83":"markdown","14b0a407":"markdown","bc2e6162":"markdown","bfd77db2":"markdown","ea20f93c":"markdown","2c9d1195":"markdown","3780d1aa":"markdown","bd2b1109":"markdown","dc436644":"markdown","0103526d":"markdown","df873900":"markdown","28e92fe1":"markdown","104e7649":"markdown","4f9cb626":"markdown","1dccee55":"markdown","0a0b1215":"markdown","ceb7a1d6":"markdown","e87b91cd":"markdown","0d7ae8fb":"markdown","cc040f97":"markdown","5aa0eccd":"markdown","7eb318a4":"markdown","a49cc407":"markdown","17ad15ae":"markdown","31357c4a":"markdown","a0e73b63":"markdown","ce6de9e0":"markdown","55d26b79":"markdown","6319b579":"markdown","9055b098":"markdown","e441dede":"markdown","7c581dd2":"markdown","e64e3f49":"markdown","e0ff1177":"markdown"},"source":{"6414b8b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecce6264":"# import libaries \nimport pandas as pd\nimport seaborn as sns\nimport numpy as np # linear algebra\n\n# plot figures\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\n#from fast_ml.outlier_treatment import OutlierTreatment \nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n#scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n#models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Perceptron\nfrom matplotlib.colors import ListedColormap\n\n\n#import kmeans\nfrom sklearn.cluster import KMeans\npd.options.mode.chained_assignment = None\nfrom sklearn.decomposition import PCA\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.neural_network import MLPClassifier\n\n#decision Tree\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, precision_recall_fscore_support\npd.options.mode.chained_assignment = None\n\n# ROC \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nimport warnings\nwarnings.filterwarnings('ignore')","b8414380":"#load the data\n#df=pd.read_csv(\"Data.csv\")\n#Number of entries and attributes\ndf = pd.read_csv(\"..\/input\/customer-analytics\/Train.csv\")\ndf.shape","0f197d29":"df.head()","e1d75750":"# more info about the data such as number of Attributes (columns), null \/ non null data type\ndf.info()","b1b3da97":"# Checking the null values\n#df.isna().values.any() --> this is other way to check the null values if any\ndf.isnull().sum()","6a039fcc":"# copy df to data for cleaning operations\ndata=df.copy()","936f46fb":"# drop ID since it is not important\ndata.drop('ID', axis = 1, inplace = True)","0c2a7651":"data.head()","a1e66981":"data.describe()","28f5add1":"# Plot a histogram for each numerical attribute\ndata.hist(bins=50, figsize=(20,15))\nplt.show()","8ead82be":"# Check the corrleation between attributes\nscatter_matrix(data, marker='o', figsize=(15, 15), hist_kwds={'bins': 20}, s=60, alpha=.8)\nplt.show()","0b3b87bf":"# Showing the counts with categories for the below coulmns\ncolumns = [\"Warehouse_block\", \"Mode_of_Shipment\", \"Customer_care_calls\", \n\"Customer_rating\", \"Prior_purchases\", \"Product_importance\", \"Gender\", \"Reached.on.Time_Y.N\", \"Discount_offered\" ]\nplt.figure(figsize = (15, 20))\nplotnumber = 1\nfor i in range(len(columns)):\n    if plotnumber <= 10:\n        ax = plt.subplot(5, 2, plotnumber)\n        sns.countplot(x = columns[i],ax=ax, data = data)\n    plotnumber += 1\n    plt.tight_layout()\nplt.show()","bca976c6":"# Explore the relation between Reached on time and other attributes\ncolumns = [\"Warehouse_block\", \"Mode_of_Shipment\", \"Customer_care_calls\", \n\"Customer_rating\", \"Prior_purchases\", \"Product_importance\", \"Gender\"]\nplt.figure(figsize = (15, 20))\nplotnumber = 1\nfor i in range(len(columns)):\n    if plotnumber <= 9:\n        ax = plt.subplot(5, 2, plotnumber)\n        sns.countplot( columns[i], hue ='Reached.on.Time_Y.N',  data = data, ax=ax)\n    plotnumber += 1\nplt.show()","2f43173e":"# Other way of visulization the correeltation with Reached on time to consider also Dicsount_Offered and Weighs \nsns.pairplot(data=data, hue = \"Reached.on.Time_Y.N\", palette = \"RdBu\")","85b472ec":"#Detecting outliers if any for the numerical data (not the cateogries)\nnumericData= data.select_dtypes(include=['number'])\nplt.figure(figsize=(25,5))\nfor i,colour in zip(range(0, len(numericData.columns)),['blue', 'green', 'red', 'yellow','lightblue','gray']):\n    plt.subplot(1,len(numericData.columns),i+1)\n    sns.boxplot(numericData[numericData.columns[i]], color=colour)\n    plt.tight_layout()","71b2f4d7":"# Outliers Treatment using fast_ml.outlier_treatment\n#ots = OutlierTreatment() \n#ots.fit(data,[\"Discount_offered\"])  \n#ots.fit(data,[\"Prior_purchases\"])\n#data = ots.transform(data)\n#data","0bb3e3b7":"#Normalization: convert categorical variable into numerical\/indicator variables\nencoder = LabelEncoder()\ndata['Mode_of_Shipment'] = encoder.fit_transform(data['Mode_of_Shipment'])\ndata['Product_importance'] = encoder.fit_transform(data['Product_importance'])\ndata['Warehouse_block'] = encoder.fit_transform(data['Warehouse_block'])\ndata['Gender'] = encoder.fit_transform(data['Gender'])\ndata","055f65e7":"# present the correlation in clear way by showing the positive or negative correlation between attributes\nplt.figure(figsize = (12, 8))\nsns.heatmap(data.corr(),  annot = True)","a9cf8341":"# Scaling each feature to a given range\n# MinMaxScaler subtracts the minimum value in the feature and then divides by the range.\n# The range is the difference between the original maximum and original minimum.\nscaler = MinMaxScaler()\nscaler.fit(data.drop('Reached.on.Time_Y.N',axis=1))\nscaled_features = scaler.transform(data.drop('Reached.on.Time_Y.N',axis=1))\ndataScale = pd.DataFrame(scaled_features,columns=data.columns[:-1])\ndataScale.head()","e25f1e53":" def PlotRoc():\n    plt.figure()\n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0,1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.plot([0,1], [0,1], 'k--' )\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","55b1b6f5":"# dataset for DecsisionTree analysis\ndataDT=data #dataset for decision Tree\ndataR=data #dataset for Research questions\ndataN=data #dataset for Neural Network\ndataC=data #dataset for Clustering","f7e9da70":"# run the naive Bayes on data to predict the reached on time\n#x= data.drop('Reached.on.Time_Y.N', axis=1)\nx=dataScale\ny = data['Reached.on.Time_Y.N']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nnb = GaussianNB()\nnb = nb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","a161e8ac":"# Recording the major metrics\nprint(classification_report(y_test,y_pred))\n#print(metrics.confusion_matrix(y_test, y_pred))\nconfusion_matrix = plot_confusion_matrix(nb, X_test, y_test)\n#cm = metrics.confusion_matrix(y_test, y_pred)\n#print('\\nTrue Negatives(TP) = ', cm[0,0])\n#print('\\nTrue Positives(TN) = ', cm[1,1])\n#print('\\nFalse Positives(FP) = ', cm[0,1])\n#print('\\nFalse Negatives(FN) = ', cm[1,0])","57b70eb3":"#ROC Curve\nPlotRoc()","f6dc41bc":"# DataSet 1 with 10 attributes\n\ndataSet1=dataScale[[\"Warehouse_block\", \"Mode_of_Shipment\", \"Customer_care_calls\", \n\"Customer_rating\", \"Cost_of_the_Product\", \"Prior_purchases\", \"Product_importance\", \"Gender\", \"Discount_offered\", \"Weight_in_gms\"]]\nx=dataSet1\ny = data['Reached.on.Time_Y.N']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nnb = GaussianNB()\nnb = nb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(nb, X_test, y_test)\n#ROC Curve\nPlotRoc()","f74a9029":"# DataSet 2 with 5 attributes\n\n#dataSet2=dataScale[[\"Warehouse_block\", \"Mode_of_Shipment\", \"Customer_rating\",\"Gender\",\"Discount_offered\"]]\ndataSet2=dataScale[[\"Warehouse_block\", \"Mode_of_Shipment\", \"Discount_offered\", \"Customer_care_calls\", \"Product_importance\"]]\n\nx=dataSet2\ny = data['Reached.on.Time_Y.N']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nnb = GaussianNB()\nnb = nb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n#ROC Curve\nPlotRoc()","16f1c36c":"# DataSet3 with 2 attributes\n\n#dataSet3=dataScale[[ \"Customer_rating\", \"Discount_offered\"]]\n#\"Warehouse_block\", \"Mode_of_Shipment\", \"Discount_offered\", \"Customer_care_calls\", \"Product_importance\"\ndataSet3=dataScale[[ \"Warehouse_block\", \"Discount_offered\"]]\nx=dataSet3\ny = data['Reached.on.Time_Y.N']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nnb = GaussianNB()\nnb = nb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(nb, X_test, y_test)\n\n#ROC Curve\nPlotRoc()","5fe1c698":"# with train size 75%\nx=dataScale\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nModels = {\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Logistic Regression\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n    \"Support Vector Machine (RBF Kernel)\": SVC(),\n    \"Neural Network\": MLPClassifier()\n}\nfor name, model in Models.items():\n    model.fit(X_train, y_train)\n    print(name + \": {:1.2f}%\".format(model.score(X_test, y_test) * 100))","401723ed":"x=dataScale\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\n\nmodel = GradientBoostingClassifier()\nmodel = model.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(model, X_test, y_test)\n\nPlotRoc()","7a6a6b97":"#Visulaize the 2 Features\nplt.scatter(df[\"Cost_of_the_Product\"],df['Discount_offered'])\nplt.xlabel('Cost_of_the_Product')\nplt.ylabel('Discount_offered')","46d1ec76":"#Preprocessing using min max scaler\nscaler = MinMaxScaler()\nscaler.fit(df[['Discount_offered']])\ndf['Discount_offered'] = scaler.transform(df[['Discount_offered']])\nscaler.fit(df[['Cost_of_the_Product']])\ndf['Cost_of_the_Product'] = scaler.transform(df[['Cost_of_the_Product']])\ndf[['Cost_of_the_Product','Discount_offered']].head()","0b9a3e1a":"km = KMeans(n_clusters=2,random_state=42) #n_initint, default=10\ny_predicted = km.fit_predict(df[['Cost_of_the_Product','Discount_offered']])\ny_predicted","0a936557":"km.cluster_centers_","538826f4":"df['cluster']=y_predicted\ndf[['Cost_of_the_Product','Discount_offered','cluster']].head()","b06162ac":"#Visualize Kmeans with 2 clusters\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\nplt.scatter(df1['Cost_of_the_Product'],df1['Discount_offered'],color='green')\nplt.scatter(df2['Cost_of_the_Product'],df2['Discount_offered'],color='red')\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\nplt.xlabel('Cost_of_the_Product')\nplt.ylabel('Discount_offered')\nplt.legend()\ndf[\"Cost_of_the_Product\"],df['Discount_offered']\nprint(\"Inertia of Kmeans with 2 Clusters: \", km.inertia_)","2f9eb5ba":"#Visualize the Kmeans 2 clusters with n_init =1\n#n_initint\n#Number of time the k-means algorithm will be run with different centroid seeds. \n#The final results will be the best output of n_init consecutive runs in terms of inertia.\nkm2 = KMeans(n_clusters=2,random_state=42, n_init=1) \ny_predicted = km2.fit_predict(df[['Cost_of_the_Product','Discount_offered']])\ny_predicted\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\nplt.scatter(df1['Cost_of_the_Product'],df1['Discount_offered'],color='green')\nplt.scatter(df2['Cost_of_the_Product'],df2['Discount_offered'],color='red')\nplt.scatter(km2.cluster_centers_[:,0],km2.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\nplt.xlabel('Cost_of_the_Product')\nplt.ylabel('Discount_offered')\nplt.legend()\ndf[\"Cost_of_the_Product\"],df['Discount_offered']\nprint(\"Inertia of Kmeans with 2 Clusters: \", km2.inertia_)","3e0247b7":"#Elbow Method\n\n#Elbow method gives us an idea on what a good k number of clusters would be based on the sum of squared distance (SSE)\n#between data points and their assigned clusters\u2019 centroids. \nsse = []\nk_rng = range(1,10)\nfor k in k_rng:\n    km2 = KMeans(n_clusters=k)\n    km2.fit(df[['Cost_of_the_Product','Discount_offered']])\n    sse.append(km2.inertia_)\nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(k_rng,sse)","029e87a3":"#Kmeans with K= 3 \nkm3 = KMeans(n_clusters=3,random_state=42)\ny_predicted = km3.fit_predict(df[['Cost_of_the_Product','Discount_offered']])\ndf['cluster']=y_predicted\n\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\ndf3 = df[df.cluster==2]\nplt.scatter(df1['Cost_of_the_Product'],df1['Discount_offered'],color='green')\nplt.scatter(df2['Cost_of_the_Product'],df2['Discount_offered'],color='red')\nplt.scatter(df3['Cost_of_the_Product'],df3['Discount_offered'],color='Blue')\nplt.scatter(km3.cluster_centers_[:,0],km3.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\nplt.xlabel('Cost_of_the_Product')\nplt.ylabel('Discount_offered')\nplt.legend()\ndf[\"Cost_of_the_Product\"],df['Discount_offered']\nprint(\"Inertia of Kmeans with 3 Clusters: \", km3.inertia_)","41e8dbc4":"#Kmeans with K= 4\nkm4 = KMeans(n_clusters=4,random_state=42)\ny_predicted = km4.fit_predict(df[['Cost_of_the_Product','Discount_offered']])\ndf['cluster']=y_predicted\n\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\ndf3 = df[df.cluster==2]\ndf4 = df[df.cluster==3]\nplt.scatter(df1['Cost_of_the_Product'],df1['Discount_offered'],color='green')\nplt.scatter(df2['Cost_of_the_Product'],df2['Discount_offered'],color='red')\nplt.scatter(df3['Cost_of_the_Product'],df3['Discount_offered'],color='Blue')\nplt.scatter(df4['Cost_of_the_Product'],df4['Discount_offered'],color='gray')\n\nplt.scatter(km4.cluster_centers_[:,0],km4.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\nplt.xlabel('Cost_of_the_Product')\nplt.ylabel('Discount_offered')\nplt.legend()\ndf[\"Cost_of_the_Product\"],df['Discount_offered']\nprint(\"Inertia of Kmeans with 3 Clusters: \", km4.inertia_)","eef7f409":"cluster_data=df[['Cost_of_the_Product','Discount_offered']]\nprint(\"Inertia of Kmeans with 2 Clusters: \", km2.inertia_)\nprint(\"Inertia of Kmeans with 3 Clusters: \", km3.inertia_)\nprint(\"Score of Kmeans with k=2 is \", km2.score(cluster_data))\nprint(\"Score of Kmeans with k=3 is \", km3.score(cluster_data))","2c0622f1":"# Silhouette_score\nsil_score_max = -1\nbest_n_clusters = 0\ncluster_data=df[['Cost_of_the_Product','Discount_offered']]\nfor k in range(2,11):\n  km = KMeans(n_clusters = k)\n  labels = km.fit_predict(cluster_data)\n  sil_score = silhouette_score(cluster_data, labels)\n  print(f\"The mean value of the silhouette score for {k} clusters is {sil_score}\")\n  if sil_score > sil_score_max:\n    sil_score_max = sil_score\n    best_n_clusters = k\n    \nprint(f\"The optimal number of clusters is: {best_n_clusters}\")","2c8c3b38":"for i, k in enumerate([2, 3, 4]):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    \n    # Run the Kmeans algorithm\n    km = KMeans(n_clusters=k)\n    labels = km.fit_predict(cluster_data)\n    centroids = km.cluster_centers_\n\n    # Get silhouette samples\n    silhouette_vals = silhouette_samples(cluster_data, labels)\n\n    # Silhouette plot\n    y_ticks = []\n    y_lower, y_upper = 0, 0\n    for i, cluster in enumerate(np.unique(labels)):\n        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n        ax1.text(-0.03, (y_lower + y_upper) \/ 2, str(i + 1))\n        y_lower += len(cluster_silhouette_vals)\n\n    # Get the average silhouette score and plot it\n    avg_score = np.mean(silhouette_vals)\n    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n    ax1.set_yticks([])\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_xlabel('Silhouette coefficient values')\n    ax1.set_ylabel('Cluster labels')\n    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n     # Scatter plot of data colored with labels\n    ax2.scatter(cluster_data['Cost_of_the_Product'], cluster_data['Discount_offered'], c=labels)\n    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlabel('Eruption time in mins')\n    ax2.set_ylabel('Waiting time to next eruption')\n    ax2.set_title('Visualization of clustered data', y=1.02)\n    ax2.set_aspect('equal')\n    plt.tight_layout()\n    plt.suptitle(f'Silhouette analysis using k = {k}',\n                 fontsize=16, fontweight='semibold', y=1.05);","1b0314a1":"cluster_data = data[[\"Weight_in_gms\", \"Gender\", \"Cost_of_the_Product\"]]\ncluster_model = KMeans(n_clusters = best_n_clusters)\n# training the model\nresult = cluster_model.fit_predict(cluster_data)\nlabels = cluster_model.labels_\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","d493ae98":"sns.lmplot(\"Weight_in_gms\",\"Cost_of_the_Product\",data=result_data,hue = 'Gender',col ='labels',fit_reg = False)\nresults_0.describe()","f7acd098":"results_1.describe()","d0375077":"#GM with n_clusters = 2\ncluster_data=df[['Cost_of_the_Product','Discount_offered']]\n\nn_clusters = 2\ngmm_model = GaussianMixture(n_components=n_clusters)\ngmm_model.fit(cluster_data)\ncluster_labels = gmm_model.predict(cluster_data)\ndf = pd.DataFrame(df)\ndf['cluster'] = cluster_labels\ncolor=['Green','Red','Blue']\nfor k in range(0,n_clusters):\n    data = df[df[\"cluster\"]==k]\n    plt.scatter(data[\"Cost_of_the_Product\"],data[\"Discount_offered\"],c=color[k])\n\nplt.title(\"Clusters Identified by Guassian Mixture Model\")    \nplt.ylabel(\"Discount_offered\")\nplt.xlabel(\"Cost_of_the_Product\")\nplt.show()","904431df":"#GM with n_clusters = 3\nn_clusters = 3\ngmm_model = GaussianMixture(n_components=n_clusters)\ngmm_model.fit(cluster_data)\ncluster_labels = gmm_model.predict(cluster_data)\ndf = pd.DataFrame(df)\ndf['cluster'] = cluster_labels\ncolor=['Green','Red','Blue']\nfor k in range(0,n_clusters):\n    dataC = df[df[\"cluster\"]==k]\n    plt.scatter(dataC[\"Cost_of_the_Product\"],dataC[\"Discount_offered\"],c=color[k])\n\nplt.title(\"Clusters Identified by Guassian Mixture Model\")    \nplt.ylabel(\"Discount_offered\")\nplt.xlabel(\"Cost_of_the_Product\")\nplt.show()","49fc0e51":"#GM with k=4\nn_clusters = 4\ngmm_model = GaussianMixture(n_components=n_clusters)\ngmm_model.fit(cluster_data)\ncluster_labels = gmm_model.predict(cluster_data)\ndf = pd.DataFrame(df)\ndf['cluster'] = cluster_labels\ncolor=['Green','Red','Blue','Gray']\nfor k in range(0,n_clusters):\n    dataC = df[df[\"cluster\"]==k]\n    plt.scatter(dataC[\"Cost_of_the_Product\"],dataC[\"Discount_offered\"],c=color[k])\n\nplt.title(\"Clusters Identified by Guassian Mixture Model\")    \nplt.ylabel(\"Discount_offered\")\nplt.xlabel(\"Cost_of_the_Product\")\nplt.show()","1f0c3d5f":"dataDT.head()","1429cd2b":"#1 run Tree model \ntree_model = DecisionTreeClassifier(random_state=42)\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, shuffle=True, random_state=42)\ntree_model.fit(X_train, y_train)\nprint(\"Accuracy on Training data is \", tree_model.score(X_train, y_train) * 100)","7c27ea5f":"#10 Fold Cross Validation\naccuracies = cross_val_score(estimator = tree_model, X=X_train, y=y_train, cv=10)\naccuracies\nprint(\"10 Fold Cross Validation with Tree Model:\",accuracies)\nprint(\"Mean Accuracy:\",np.mean(accuracies))","bb75d99f":"#2 Checking the model with training and testing dataset\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, shuffle=True, random_state=42)\ny_pred = tree_model.predict(X_test)\nprint(\"Accuracy on Training data:\", tree_model.score(X_train, y_train) * 100)\nprint(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(tree_model, X_test, y_test)\nPlotRoc()","6474839b":"#3 Experiment with various decision tree parameters that control the size of the tree\n#Regularization Hyperparameters\n\n# Max Depth 3\ntree_model = DecisionTreeClassifier(max_depth=3,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data   :\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model: {100*acc:.2f}%\")","c073430c":"plt.figure(figsize=(35,30))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On time\",\"Not on time\"])\nplt.show()\n#A node\u2019s gini attribute measures its impurity: a node is \u201cpure\u201d (gini=0) if all training instances \n#it applies to belong to the same class. ","16a2d0b6":"# Max Depth 4\ntree_model = DecisionTreeClassifier(max_depth=4,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data   :\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model for the testing data: {100*acc:.2f}%\")","fd43439e":"# min_samples_leaf\n#checking the optimal accuracy of teting with min samples leaf\n\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nX_train\nlooprange= [200,225]\n\nfor f in range(200, 225):\n    tree_model = DecisionTreeClassifier(min_samples_leaf=f, random_state=42)\n    tree_model.fit(X_train, y_train)\n    y_pred = tree_model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    #print(f\"The accuracy of the DT model with Min Samples Leaf=\", f + \" is {100*acc:.2f}%\")\n    #print(\"Min Samples Leaf=\", f)\n    #print(f\"The accuracy of the DT model is {100*acc:.2f}%\")\n    print(\"The accuracy of the DT model with Min Samples Leaf=\",  str(f) , f\"is {100*acc:.2f}%\")","157ec013":"# min_samples_leaf of 205 is good number to get high accuracy as the above figures\ntree_model = DecisionTreeClassifier(min_samples_leaf=205,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data is\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model for the testing data is {100*acc:.2f}%\")","bdf32309":"plt.figure(figsize=(35,30))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On time\",\"Not on time\"])\nplt.show()","9d5e3b04":"# Checking different parameters toegther \ntree_model = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=3, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=205,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=40, splitter='best')\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data is\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model for the testing data is {100*acc:.2f}%\")\nplt.figure(figsize=(35,30))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On time\",\"Not on time\"])\nplt.show()","522483fb":"#trying other parameters toegther including the Entropy\n## Entropy is zero when it contains instances of only one class (pure split)- maximum entropy\ntree_model = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=4, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=205,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=40, splitter='best')\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data is\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model for the testing data is {100*acc:.2f}%\")\nplt.figure(figsize=(35,30))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On time\",\"Not on time\"])\nplt.show()","095902fa":"#Checking the tree model for best accuracy\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\ntree_model = DecisionTreeClassifier(max_depth=3,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nprint(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)","f61c0721":"# Moving 30% of the instances from the original training\n\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.49, random_state=42)\nX_train\ntree_model = DecisionTreeClassifier(random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc_Train = tree_model.score(X_train, y_train)\nprint(f\"The accuracy of the DT model with Training dataset is {100*acc_Train:.2f}%\")\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy of the DT model with testing dataset is {100*acc:.2f}%\")","74de87c4":"#5 Moving 60% of the instances from the original training\n\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\n#x = dataDT[[\"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.28, random_state=42)\nX_train\ntree_model = DecisionTreeClassifier(random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc_Train = tree_model.score(X_train, y_train)\nprint(f\"The accuracy of the DT model with Training dataset is {100*acc_Train:.2f}%\")\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy of the DT model with testing dataset is {100*acc:.2f}%\")","2cafea29":"# Moving 30% of the instances from the original training with Max Depth=4\n\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.49, random_state=42)\nX_train\ntree_model = DecisionTreeClassifier(max_depth=4, random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc_Train = tree_model.score(X_train, y_train)\nprint(f\"The accuracy of the DT model with Training dataset is {100*acc_Train:.2f}%\")\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy of the DT model with testing dataset is {100*acc:.2f}%\")","30f1ae09":"#5 Moving 60% of the instances from the original training with Max Depth=4\n\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\n#x = dataDT[[\"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.28, random_state=42)\nX_train\ntree_model = DecisionTreeClassifier(max_depth=4,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc_Train = tree_model.score(X_train, y_train)\nprint(f\"The accuracy of the DT model with Training dataset is {100*acc_Train:.2f}%\")\nacc = accuracy_score(y_test, y_pred)\nprint(f\"The accuracy of the DT model with testing dataset is {100*acc:.2f}%\")","8f1893c3":"# Check the best accuracy without overfitting\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\n#x = dataDT[[\"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nX_train\n\ntree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy on Training data   :\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model with testing dataset is: {100*acc:.2f}%\")","01e3bf9e":"#6 Random Forest\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nRF_model = RandomForestClassifier( random_state=42, n_estimators=20)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\nacc_Train = RF_model.score(X_train, y_train)\nprint(f\"The accuracy of the RF model with Training dataset is {100*acc_Train:.2f}%\")\n\nprint(\"Accuracy of the the RF model with testing data:\",metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(tree_model, X_test, y_test)\n\n#ROC Curve\nPlotRoc()","55078e00":"# examine different hyper parameters\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nRF_model = RandomForestClassifier( max_depth=20, min_samples_leaf=5,n_estimators=200)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\nacc_Train = RF_model.score(X_train, y_train)\nprint(f\"The accuracy of the RF model with Training dataset is {100*acc_Train:.2f}%\")\nprint(\"Accuracy of Testing data:\",metrics.accuracy_score(y_test, y_pred))","1b291a92":"# examine different hyper parameters\nx = dataDT[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataDT[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nRF_model = RandomForestClassifier( n_jobs=-1, max_depth=5,\n                                       n_estimators=100, oob_score=True, random_state=42, min_samples_split=210)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\nacc_Train = RF_model.score(X_train, y_train)\nprint(f\"The accuracy of the RF model with Training dataset is {100*acc_Train:.2f}%\")\nprint(\"Accuracy of Testing data:\",metrics.accuracy_score(y_test, y_pred))","98f9b4b4":"dataN.shape","bd103a75":"dataN.head()","a775e3a0":"# 1  Linear classifier with training dataset\nx = dataScale[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataN[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)\nPer_model = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\nPer_model.fit(X_train, y_train)\nacc_Train = Per_model.score(X_train, y_train)\nprint(f\"The accuracy of the Perceptron model with Training dataset is {100*acc_Train:.2f}%\")","18cf58a4":"#10 Fold Cross Validation\naccuracies = cross_val_score(estimator = Per_model, X=X_train, y=y_train, cv=10)\naccuracies\nprint(\"10 Fold Cross Validation with Tree Model:\",accuracies)\nprint(\"Mean Accuracy:\",np.mean(accuracies))","da40ee88":"#2 Checking the model with training and testing dataset\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\ny_pred = Per_model.predict(X_test)\nprint(\"Accuracy on Training data:\", Per_model.score(X_train, y_train) * 100)\nprint(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)\nprint(classification_report(y_test,y_pred))\nconfusion_matrix = plot_confusion_matrix(Per_model, X_test, y_test)\nPlotRoc()","41b64075":"#2 Run the Multilayer Perceptron\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier().fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy on Training data:\", mlp.score(X_train, y_train) * 100)\nprint(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)","b7cd8939":"# MLP with Cross validation\nmlpc = MLPClassifier().fit(X_train,y_train)\npredict = mlpc.predict(X_test)\nR2CV = cross_val_score(mlpc,X_test,y_test,cv=10).mean()\nprint(\"Cross Validation Score: \",R2CV)\nerror = -cross_val_score(mlpc,X_test,y_test,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(\"Mean Squared Error: \",np.sqrt(error))","d3b35f4e":"#Experiment with various Neural Network parameters with Multilayer Perceptron \n    \nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(alpha=1, max_iter=1000)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\n#print(\"Accuracy on Training data:\", mlp.score(X_train, y_train) * 100)\nprint(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)\nprint(classification_report(y_test,y_pred))","63768481":"#Initializing the MLPClassifier : hidden_layer_sizes=(150,100,50)\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=42)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","0192fc8b":"#Experiment different parameters together \nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(100), activation='relu', solver='adam', alpha=0.0001,\n batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, \n max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, \n momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9,\n beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","4364b77a":"#Experiment with various Neural Network parameters with Multilayer Perceptron \nclf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=200, alpha=0.0001,\n                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001, momentum=0.9)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","a464fbc1":"print(\"Accuracy of the testing data:\",metrics.accuracy_score(y_test, y_pred)* 100)\n#Loss is the quantitative measure of deviation or difference between the predicted output\n#and the actual output in anticipation. It gives us the measure of mistakes made by the network in predicting the output.\n\n#loss decreases as the iteration increases but it keep the same with around 0.53 from 127 iteration to 200","a549d0f6":"# Activation function =  identity\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,150,100), max_iter=1000,\n                    activation = 'identity',solver='adam',random_state=42, alpha=1)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","1f1feb38":"# Activation function =  tanh\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,150,100), max_iter=1000,\n                    activation = 'tanh',solver='adam',random_state=42, alpha=1)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","9d4e330e":"# Activation function =  relu\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,150,100), max_iter=1000,\n                    activation = 'relu',solver='adam',random_state=42, alpha=1)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","41eb0964":"# Activation function =  logistic\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,150,100), max_iter=1000,\n                    activation = 'logistic',solver='adam',random_state=42, alpha=1)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","236db204":"# Learning rate = adaptive or  invscaling\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,150,100), max_iter=1000,\n                    activation = 'logistic',solver='adam',random_state=42, alpha=1, learning_rate='adaptive')\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","c4380209":"# Tuning the model for best accuracy: \nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=42)\nmlp = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=200,activation = 'relu',solver='adam',random_state=42)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\nprint(\"Accuracy of the MLPClassifier:\",metrics.accuracy_score(y_test, y_pred)* 100)","2340d869":"#3 Convolutional Neural networks Keras Sequential\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=1)\n\nmodel = keras.models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dense(2, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nmodel.build()\nmodel.summary()","63224c44":"history = model.fit(X_train, y_train, validation_split=0.2, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('the cost of the model')\nplt.ylabel('Loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","729ee9d1":"model.evaluate(X_test, y_test)","dc650600":"#check with sigmoid\n\nmodel = keras.models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dense(2, activation=\"sigmoid\")\n])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nmodel.build()\nmodel.summary()","87eff632":"history = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('the cost of the model')\nplt.ylabel('Loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","68fd37c7":"model.evaluate(X_test, y_test)","553f5ac6":"dataR.head()","e1f0477d":"#1 check the relation between customer care calls, customer ratings and gender\n\nplt.figure(figsize = (16, 8))\nsns.lineplot(x = 'Customer_care_calls', y = 'Customer_rating', hue = 'Gender', data = dataR, ci = 1)\nplt.title('Relation between Customer Care Calls and Customer Rating of Males and Females\\n', fontsize = 17)\nplt.show()","514af2ce":"# checking the relation between customer care calls, customer ratings and reached on time.\n\nplt.figure(figsize = (16, 7))\nsns.barplot(x = 'Customer_care_calls', y = 'Customer_rating', hue = 'Reached.on.Time_Y.N', data = dataR)\nplt.ylim(0, 5)\nplt.show()","33381a1c":"#2 What is The Number of Times The Products Stored In Each Warehouse Arrive on Time?\nplt.figure(figsize=(16,5))\n\n# plot\ntotal = float(len(dataR)) \n\nax = sns.countplot(x=\"Warehouse_block\", data=dataR, hue='Reached.on.Time_Y.N');\nsns.despine(top=True, right=True, left=False, bottom=False);\nax.spines['left'].set_color('lightblue');\nax.spines['bottom'].set_color('lightgray');\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 100,\n            '{0:.1%}'.format(height\/total),\n            ha=\"center\", fontsize=10)\n    \nax.set_ylabel('Frequency');\n","f4811523":"#3 What are the factors or attributes that affect the customer rating for the products?\n\n#Customer Rating Correlation:\nprint(\"Customer Rating Correlation with the following attributes:\\n\")\n\nfeatures=['Warehouse_block', 'Mode_of_Shipment', 'Customer_care_calls',\n          'Cost_of_the_Product', 'Prior_purchases','Product_importance', \n          'Gender','Reached.on.Time_Y.N']\nfor f in features:\n    related = dataR['Customer_rating'].corr(dataR[f])\n    print(\"%s: %f\" % (f,related))","99a76d41":"#visulize the figures\ncorrelations = [dataR['Customer_rating'].corr(dataR[f]) for f in features ]\nfeatures_df = pd.DataFrame({'attributes': features, 'correlation': correlations}) \nfeatures_df.plot(x ='attributes', y='correlation', kind = 'line',\n                    figsize=(16,4), title='Customer Rating Correlation with other features',\\\n                    linewidth=3.3, marker='.', markersize=17)                \n\nplt.xlabel('Features')\nplt.ylabel('Customer Ratings')","dd21962f":"features=[\"Customer_care_calls\",\"Reached.on.Time_Y.N\"]\nfor f in features:\n    related = dataR['Customer_rating'].corr(dataR[f])","4ed835d5":"correlations = [dataR['Customer_rating'].corr(dataR[f]) for f in features ]\nfeatures_df = pd.DataFrame({'attributes': features, 'correlation': correlations}) \nfeatures_df.plot(x ='attributes', y='correlation', figsize=(16,4), \n                 title='Customer Rating Correlation with other features', kind = 'bar')                \nplt.xlabel('')\nplt.ylabel('Customer Ratings')","aebeb5a0":"#4 Was the Customer query being answered?\nplt.figure(figsize= (14,8))\nsns.countplot(data=dataR,x='Customer_care_calls',hue='Customer_rating')","14275e89":"#5 Does more Discount offered leads to increasing the customer rating?\n\nqueryDiscount=dataR[dataR['Discount_offered'] >= 10.5]\nsns.countplot(data=dataR,x='Customer_rating')","40b165a8":"print(\"Customer Rating with discount >= 10.5\")\nsns.countplot(data=queryDiscount, x='Customer_rating')","1c443315":"#decision tree  to check the discount offered \nx = dataR[[\"Discount_offered\", \"Weight_in_gms\", \"Product_importance\"]]\ny = dataR[\"Reached.on.Time_Y.N\"]\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\ntree_model = DecisionTreeClassifier(max_depth=3,random_state=42)\ntree_model.fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\n#print(\"Accuracy on Training data   :\", tree_model.score(X_train, y_train) * 100)\nprint(f\"The accuracy of the DT model: {100*acc:.2f}%\")\nplt.figure(figsize=(35,30))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"On time\",\"Not on time\"])\nplt.show()\n#A node\u2019s gini attribute measures its impurity: a node is \u201cpure\u201d (gini=0) if all training instances \n#it applies to belong to the same class. ","c25e4701":"#6 Which Products, Customer service have to pay more attention to?\n\nfig = px.density_heatmap(pd.read_csv(\"..\/input\/customer-analytics\/Train.csv\"), x=\"Customer_care_calls\", y=\"Cost_of_the_Product\", \n                         nbinsx=10, nbinsy=10, color_continuous_scale=\"Blues\")\nfig.show()","e3a45b61":"Conclusion:\n\nOptimal number of clusters to choose is 2 with Silhoutte score 0.57\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) \/ max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1.\n\nThe silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping.","d01c4c6c":"# Forming 3 Data Sets","3d51b67f":"Observations:\n1- More Items not reached on time with discount_offered higher than 30 and weight more than 4000 gms.\n2- Check the possible outliers with Weights and Dicount offered since it shows skewed distribution. ","5bdcfdb9":"Conclusion: \n1- Products delievered on time with ship more than Road or flight\n2- Products delievered on time with the product imprtance of (low and medium) are more than High importance.","fe383676":"Kmeans Inertia and Score:\n\nTo select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets. But at least we can measure the distance between each instance and its centroid. This is the idea behind the inertia metric.\n\nThe score() method returns the negative inertia. Why negative? Well, it is because a predictor's score() method must always respect the \"greater is better\" rule.\nAs you can easily verify, inertia is the sum of the squared distances between each training instance and its closest centroid:","d6bfea9e":"Shipments with more than 10.5 discount did not arrive on time. This could be because the company was giving discounts on delayed products. The greater the delay, the greater the discount.\n\nHowever, Higher Discount offered leads to slightly higher Customer Rating of 5. Hence delayed Shipment with heigher discount will still can get higher rate.","9decc92d":"The network was built with the term Adam optimizer, which implements an exponential moving average of the gradients to scale the training rate. Maintains an exponentially declining average of previous gradients. Adam is computationally efficient and has very low memory requirements.\n\nAdam Optimizer is one of the most popular gradient descent optimization algorithms. In addition to the optimizer, sparse_categorical_crossentropy was selected as a cost function, and the accuracy parameter was chosen for network quality measurements.","209025a2":"Run Random Forest","f4c86d84":"Conclusion of influence on the classifier's performance with overfitting: \n\n1- Max Depth with 3 is good number to get high accuracy for the testing with with highe accuracy around 68% and in the same time with avoiding overfitting.\n\n2- With Max depth of 4, it slightly decreases the accuracy\n\n3-A node\u2019s gini attribute measures its impurity:A Gini Index of 0.481 denotes equally distributed elements into both classes.\n\n4- Shipments with more than 10.5 discount did not arrive on time. This could be because the company was giving discounts on delayed products. The greater the delay, the greater the discount.","02f8ff6e":"From the above chart, it shows that the arrival of shipments does not depend on the warehouse where it is stored.","ce843164":"Due to the uniform distribution of the attributes, the cluster study does not provide additional information because the Kmeans divide these sets into similar frames with a similar appearance. There is some slight differences for cost procust and weights between both clusters. Looking at the gender of consumers or the cost of products, it does not affect product groups.","6fd12620":"Use Decision trees on a training set to predict the shipment being Reached on Time (Yes or No)","1f32f6dd":"The following Features will be used to perform the clustering:\n\nDiscount_offered and Cost_of_the_Product\n\nThe purpose of the analysis is to check if we are able to identify any specific groups of products purchased by Customers.","d1df7816":"# Part 1. Data Analysis and Bayes Nets.\n","c0240814":"# Part 3. Supervised Learning: Generalisation & Overfitting; Decision trees.\n","e36c03ef":"low inertia is better than greater inertia so this shows that 3 cluster is better. However, we will use Silhouette score to further evaluate the quality of clusters created. \n\nSilhouette analysis can be used to determine the degree of separation between clusters. For each sample:\n\nThe coefficient can take values in the interval [-1, 1].\nIf it is 0 \u2013> the sample is very close to the neighboring clusters.\nIt it is 1 \u2013> the sample is far away from the neighboring clusters.\nIt it is -1 \u2013> the sample is assigned to the wrong clusters.\n\nTherefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters","915a82cb":"1- Kmeans Clustering with K=2","139f89a2":"Conclusions:\n\n1- Dataset1 with all attributes (10 attributes) gives better accuracy of 65% which is better than other 2 datasets with 5 and 2 attributes which both gave accuracy around 64%. \n2- The AUC is better also for the first dataset with 0.71 which shows that the model is capable of distinguishing between classes(Reached on time or not) rather than the other 2 datasets with 0.70\n3- F1 Score (model\u2019s precision and recall) is better also for the first data set with the Reached on time (yes) class with 0.59 rather than 0.58 with the other 2 data sets.\n4- Adding more features can enhance the model performance, such as customer address or location and courier.","2d355e30":"Observations:\nThe discount offered and Prior Purchases have outliers","e0fbb04c":"# Cleaning the data","decfc37f":"Optional: Summary for the other ML Models:\nGradiant Boosting is the best accuracy with 68.5% with 0.71 AUC, with the higher F1 score with 0.68 for Reached on time(yes) and less than the NB Gaussian with only 0.01 for reached on time (No) Class.","7a0b24f8":"Random Forest Conclusion: We avoided the overfitting with Random Forest and in the same time there is no much differnce between the accuracy of Tree model and Random Forest.","6179d2e8":"# Pre-processing","227c0943":"#Experiment with various Neural Network parameters\n\nHidden_layer_sizes : This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple. Thus the length of tuple denotes the total number of hidden layers in the network.\n\nMax_iter: It denotes the number of epochs.\n\nActivation: The activation function for the hidden layers.\n\nSolver: This parameter specifies the algorithm for weight optimization across the nodes.\n\nRandom_state: The parameter allows to set a seed for reproducing the same results\n\nMomentum: for gradient descent update\n\nLearning Rate: schedule for weight updates.\n\nAn epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very large). Some people use the term iteration loosely and refer to putting one batch through the model as an iteration. \n\nAn activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. ","cae81089":"# Features Correlation (Positive or Negative)","beb78a8d":"# Run other ML Classifier","8583e700":"# Initial Data Exploration","23dd8267":"Conculsion: No diference on the accuracy with different learning rates types.\n\nHence, the best accuracy is 67.95% as the above analysis with experminent different parameters.","46d7984f":"# E-Commerce Shipping Data\n\nFull end-to-end Machine Learning and Data Mining solution to provide analysis and to predict if the shipments were delivered on time or not using Supervised and unsupervised learning, Neural Networks, and Convolutional Neural Networks.\n","dd26c791":"# Part 2: Clustering","f301571c":"# Part 5. Research Question (Finding interesting Properities)\n","0246a31c":"#Try to identify gender groups from Weight_in_gms and Cost_of_the_Product using Kmeans","ac88e84b":"# # Part 5. Research Question (Finding Interesting Properties)","ce04fe83":"Conclusion: \n1- Customers give 5 rate with 2 calls rather than calling more than 2 times\n2- When the customers call more than 3 times the rating of only 3 is get high","14b0a407":"Conclusion:\n\nSince the accuracy of the test data is almost the same like acuuracy iof the training data, we can say that the linear classifier does genrealize to the new data.\n\nThe lower accuracy of the training dataset show that there is no overfiting.\n\nThe lower accuracy of the testing dataset shows that this is a strong evidence that this data is not linearly separable with a high confidence.","bc2e6162":"#4 Make new training and testing sets, by moving 30% of the instances from the original training\nset into the testing set.","bfd77db2":"Conclusion: we got a good accuracy for the testing with max depth =3 and without overfitting. let us now check the other hyperparameters such as min samples leaf ","ea20f93c":"The mlp generealize to the new data since the accuracy of tratining and testing data are without much difference.\n\nThe lower accuracy of the testing dataset shows that this is a strong evidence that this data is not linearly separable with a high confidence.","2c9d1195":"The graph above shows that k=3 is not a bad choice. Sometimes it\u2019s still hard to figure out a good number of clusters to use because the curve is monotonically decreasing and may not show any elbow or has an obvious point where the curve starts flattening out. \n\nThat is why we will try K=3 in the coming analysis and check the results.","3780d1aa":"# Part 2. Clustering","bd2b1109":"Finding interesting Marketing\/Business to get he most out of the data with insights to help the product reach on time and to get higher customer ratings:\n\n\n1- Does the number of customer calls affect on the shipment being reached on time? \n\n2- What is The Number of Times The Products Stored In Each Warehouse Arrive on Time?\n\n3- What are the factors or attributes that affect the customer rating for the products?\n\n4- Does more Discount offered leads to increasing the customer rating?\n\n5- Which Products, Customer service have to pay more attention to?\n\n6- Was the Customer query being answered?\n","dc436644":"Conclusion: Since the Accuracy of training is larger than the cross Validation accuracies and the mean accuracy, this means that there is an overfitting. ","0103526d":"Conclusion: The prices of the most ordered products are between 250 and 300 with 6 or 7 Customer calls.\n\nFor 2 or 4 calls, the price drops to between 150-200. In this case, as the price of the product gets more expensive, people tend to call customer service more. \n\nSolution: The customer service has to pay more attention for prodcuts prices more than 250 $. Hence, this will increase the customer ratings as the previous analysis above.\n\nWith the above insights for Part6 we got many interesting info and connected relationship from the data that can be used by decision makers to enhance the customer rating, shipment of the products, customer care service and discount offered.","df873900":"# Features Scaling ","28e92fe1":"2- Clustering using Gaussian Mixture.\n\nGM implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.\nGaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters.","104e7649":"We avoided the overfitting now as the above figures in both datasets with pruning the tree depth. The first dataset accuracy is slightly better with the testing dataset.","4f9cb626":"The MLP model accuracy is slightly better than Sequnetial but still the tree model achieved slightley better than MLP and Sequential.\n\nThis means that complex solutions are not always good for every solution. In a situation where there is little correlation of the variables, and the distributions are not normal, non-parametric models may work better than parametric models. However, the hypothesis can be confirmed using the neural network model with the above accuracies.","1dccee55":"Conclusion: the best accuracy is with activation funcation = relu as the above analysis.\nAlso form the iterations and the accuracy, we can realize that there is no overfitting and the model can genrealize to new data.","0a0b1215":"# Part 4. Neural Networks and Convolutional Neural Networks.","ceb7a1d6":"Observations:\n1- Males count are almost the same like females\n2- Number of items reached on time is bigger\n3- Number of product importance Low and Medium are bigger than high\n4- Warehouse F is the highest\n5- Movement by ship is the highest\n6- Discount_offered and Weight_in_gms shows skewed distribution ","e87b91cd":"Observations:\n\n1- There is psoitive relation between Discount offered and Reached on time\n2- There is slight positive relation between Reached on time and Gender, warehouse and customer rating\n3- There is psoitive relation between Cost of product and Customer care Calls\n4- There is a slight positive relation between prior purchase and customer care calls\n5- There is a slight positive relation between Customer rating and customer care calls\n6- There is negative relation between weights in gms and customer care calls\n7- There is negative relation between weights in gms and discount offered.","0d7ae8fb":"There is an overfitting in the data as the the above figures, Hence we will prune the hyper Parameters","cc040f97":"Conclusion: \nWhen comparing the results between the Kmeans Cluster Analysis and the Gaussian Mixture, slight differences can be seen. The Kmeans analysis divided the data set mainly due to the means to update the centroid while GMM takes into account the mean as well as the variance of the data. \n\nHence, the Gaussian is more realistic in this case as per the distribution of the data (Cost of the product and discount offered).\n\nIn Part 1 of Classification, we had to classes (Reached on time (Yes or No)) and predict this using calssifiers with accuracy reached to 68%. Difference from classification is that after cluster analysis we got 2 clusters that can be used to define, for example, (Shipment reached on time or not) or Gender groups of people (males or females) from the products purchased features (Cost of the product and Discount offered). From using Kmeans with Weight of the products and Cost of the product, it showed that we can not predict Gender from those two features. However, we can use the clusters that we created for other sales\/marketing purposes. ","5aa0eccd":"# Run Na\u00efve Bayes Classifier ","7eb318a4":"# Visualizing the data","a49cc407":"Observations:\nThere are 10999 entries and 12 Columns\/attributes with no null data.\nThere are object as type data for cateogories attributes and numeric values for other attributes as integer64. \nID is not important and can be dropped.","17ad15ae":"3-What are the factors or attributes that affect the customer rating for the products?\n\nThe chart above clarifies that there is a high correlation between the Customer Rating and (Reach on Time, Prior Purchase, Customer Care Calls, cost of Products and Warehouse). \n\nConclusion: customers who are satisfied in these areas (Cost of the product, Custmer care calls, Prior Purchase and being the product reahced on time) are likely to give the product better ratings.The most 2 factor that affect on the ratings are Reached on time and Customer care calls.\n\nAnd also from the first question above, it showed that customer care calls playing important role in reaching the shipment to the cusomters on time.\n\nSolution : Marketing team have to pay more attention to those 2 factors to improve the customer ratings further more.","31357c4a":"Conclusion:\nBy analyzing the results of the decision tree prediction, we are able to confirm the hypothesis that whether Shipments were delivered on time depends on the features selected above.\n\nThe accuracy of the model was determined at the level of about 67% which is a very good result looking at the very weak relationships between the feaures. Taking into account the data provided, the results are satisfactory.\n\nDecision trees are prone to overfit the data. Therefore, ensemble techniques such as random forest can be used because it have the capability to reduce variance without increasing the bias.","a0e73b63":"Conclusion: \nThe accuracy for the training set still the same for the above 2 training dataset, but the accuracy for the test decreased for both datasets which means there is an overfitting noticed. \n\nHence we will use the max depth to avoid the overfitting with those 2 datasets as we did on the above analysis with Regularization Hyperparameters.","ce6de9e0":"# Checking the correlation between features","55d26b79":"Conclusion: \nThe accuracy with the testing data set is 65% which means that there is overfitting with the testing data which is less than tha ccuracy of the training set.\n\nTo avoid overfitting the training data, we need to restrict the Decision Tree\u2019s freedom during training. As you know by now, this is called regularization.\n\nRegularization hyperparameters for example can restrict the maximum depth of the Decision Tree. This is controlled by the max_depth hyperparameter (the default value is None, which means unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n\nIncreasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model; resulting in a smaller tree & reduced overfitting\n\nOther parameters that restrict the shape of the Decision Tree:\n\nmin_samples_split (the minimum number of sam\u2010 ples a node must have before it can be split)\nmin_samples_leaf (the minimum num\u2010 ber of samples a leaf node must have)\nmin_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total number of weighted instances)\nmax_leaf_nodes (maximum number of leaf nodes)\nmax_features (maximum number of features that are evaluated for splitting at each node).","6319b579":"                                   # The Importance of the E-commerce Shipping Dataset #\n  \nReasons for choosing this data set:\n\nE-Commerce plays a significant role in our world nowadays. Companies like Amazon, Noon, and Food deliveries such as Talabat and Delivero face different challenges every day to provide the best customer service and to deliver the shipments and items on time and improve the customer rating at the same time. \n\nSome Questions we need to answer like the following:\nWas the shipment delivered on time or not?\nIf Product importance or the number of customer care calls are high, the item will be delivered on time?\nWhat is the Customer rating? \n\nOther interesting findings will be discovered with the below analysis and exploration of the data.\n\nSource of dataset : https:\/\/www.kaggle.com\/prachi13\/customer-analytics\n","9055b098":"The epoches show that there is no overfitting and the model can genrealize to new data with the above accuracy.","e441dede":"# Part 4. Neural Networks and Convolutional Neural Networks.","7c581dd2":"1-Does the number of customer calls affect on the shipment being reached on time? \n\nYes, When Customers calls increase (>= 4), the number of shipments reached on time would become higher than shipments that were not reached on time.\n\nThis means that customer care calls is playing important role for reaching the shipment to the cusomters on time.","e64e3f49":"Observations:\nDiscount offered and Weights in grams show skewed distribution","e0ff1177":"# Part 3: Decision Tree"}}