{"cell_type":{"39649e52":"code","54c3519d":"code","8de1b0ef":"code","8f19b76a":"code","ed9e1f68":"code","62b9d30d":"code","29686c2f":"code","60e29695":"code","53744327":"code","2a383560":"code","a440f8f2":"code","5048e889":"code","a73e1d50":"code","f8472098":"code","4d88b16f":"code","6dbcfc00":"code","fee4c9cf":"code","8b1f6234":"code","0a95788c":"code","ffea0919":"code","70fef26c":"code","52fff009":"code","4b46f104":"code","c254a880":"code","0d1a9929":"code","c5675145":"code","7fd6c494":"markdown","d720744b":"markdown","54994f9d":"markdown"},"source":{"39649e52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54c3519d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 20)\n\nPATH = \"..\/input\/demand-forecasting-kernels-only\"\ntrain = pd.read_csv(f\"{PATH}\/train.csv\", low_memory=False, parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(f\"{PATH}\/test.csv\", low_memory=False, parse_dates=['date'], index_col=['date'])\nsample_sub = pd.read_csv(f\"{PATH}\/sample_submission.csv\")","8de1b0ef":"display(train)\ndisplay(test)","8f19b76a":"# Expand with more date columns\ndef expand_df(df, istest = True):\n    data = df.copy()\n    data['day'] = data.index.day\n    data['month'] = data.index.month\n    data['year'] = data.index.year\n    data['dayofweek'] = data.index.dayofweek\n    if istest:\n        data = data[['store', 'item', 'day', 'month', 'year', 'dayofweek', 'sales']]\n    else:\n        data = data[['id', 'store', 'item', 'day', 'month', 'year', 'dayofweek']]\n    return data","ed9e1f68":"train_data = expand_df(train)\ndisplay(train_data)\n\ngrand_avg = train_data.sales.mean()\nprint(f\"The grand average of sales in this dataset is {grand_avg:.4f}\")","62b9d30d":"test_data = expand_df(test, istest=False)\ndisplay(test_data)","29686c2f":"agg_year_item = pd.pivot_table(train_data, index='year', columns='item', values='sales', aggfunc=np.mean).values\nagg_year_store = pd.pivot_table(train_data, index='year', columns='store', values='sales', aggfunc=np.mean).values","60e29695":"agg_year_item.shape, agg_year_store.shape","53744327":"display(pd.pivot_table(train_data, index='year', columns='store', values='sales', aggfunc=np.mean))","2a383560":"plt.figure(figsize=(12, 7))\nplt.subplot(121)\nplt.plot(agg_year_item) #\/ agg_year_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_year_store) #\/ agg_year_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","a440f8f2":"TRAIN_SPLIT = 40000\ndataset = train_data.values\ndataset = dataset[-50000:]\ndata_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\ndata_std = dataset[:TRAIN_SPLIT].std(axis=0)","5048e889":"# data normalization\ndataset = (dataset-data_mean)\/data_std","a73e1d50":"def multivariate_data(dataset, target, start_index, end_index, history_size,\n                      target_size, step, single_step=False):\n  data = []\n  labels = []\n\n  start_index = start_index + history_size\n  if end_index is None:\n    end_index = len(dataset) - target_size\n\n  for i in range(start_index, end_index):\n    indices = range(i-history_size, i, step)\n    data.append(dataset[indices])\n\n    if single_step:\n      labels.append(target[i+target_size])\n    else:\n      labels.append(target[i:i+target_size])\n\n  return np.array(data), np.array(labels)","f8472098":"past_history = 120 # history length\nfuture_target = 90  # three future months as required to predict\nSTEP = 1 # every day\n\nx_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 6], 0, TRAIN_SPLIT,\n                                                 past_history,\n                                                 future_target, STEP)\nx_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 6], TRAIN_SPLIT, None,\n                                             past_history,\n                                             future_target, STEP)","4d88b16f":"print ('Single window of past history : {}'.format(x_train_multi[0].shape))\nprint ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))","6dbcfc00":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\n\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","fee4c9cf":"BATCH_SIZE = 256\nBUFFER_SIZE = 10000\n\ntrain_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\ntrain_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\nval_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()","8b1f6234":"def create_time_steps(length):\n  return list(range(-length, 0))","0a95788c":"def plot_train_history(history, title):\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs = range(len(loss))\n\n  plt.figure()\n\n  plt.plot(epochs, loss, 'b', label='Training loss')\n  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n  plt.title(title)\n  plt.legend()\n\n  plt.show()","ffea0919":"def multi_step_plot(history, true_future, prediction):\n  plt.figure(figsize=(12, 6))\n  num_in = create_time_steps(len(history))\n  num_out = len(true_future)\n\n  plt.plot(num_in, np.array(history[:, 6]), label='History')\n  plt.plot(np.arange(num_out)\/STEP, np.array(true_future), 'b-',\n           label='True Future')\n  if prediction.any():\n    plt.plot(np.arange(num_out)\/STEP, np.array(prediction), 'r-',\n             label='Predicted Future')\n  plt.legend(loc='upper left')\n  plt.show()","70fef26c":"for x, y in train_data_multi.take(3):\n  multi_step_plot(x[0], y[0], np.array([0]))","52fff009":"multi_step_model = tf.keras.models.Sequential()\nmulti_step_model.add(tf.keras.layers.LSTM(32,\n                                          return_sequences=True,\n                                          input_shape=x_train_multi.shape[-2:]))\nmulti_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\nmulti_step_model.add(tf.keras.layers.Dense(y_train_multi.shape[-1]))\n\n# multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\nmulti_step_model.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mse')\nmulti_step_model.summary()","4b46f104":"# sanity\nfor x, y in val_data_multi.take(1):\n  print (x.shape, multi_step_model.predict(x).shape)","c254a880":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, cooldown=4, patience=3, min_lr=0.00001, verbose=1)\nearly_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmulti_step_history = multi_step_model.fit(train_data_multi, epochs=25,\n                                          steps_per_epoch=400,\n                                          validation_data=val_data_multi,\n                                          validation_steps=50, callbacks=[reduce_lr, early_stop]) ","0d1a9929":"plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')","c5675145":"for x, y in val_data_multi.take(3):\n  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])","7fd6c494":"## Summary:\n- Deep learning is used where RNN (LSTM) layers are applied to capture\/learn the time sequences \n- Multivariate and multi-step future prediction are used\n    - Multivariate to learn based on more than one feature per step\n    - multi-step future prediction to predict multi days in the future\n- Input shape is (num of samples, num of history steps = 120 days for example, input features = 7 'date details and sales')\n- Output shape is (num of samples, num of future steps = 90 days, output features = 1 'sales')\n- To construct more important features, dates are expanded to separate columns, day, month, year, day of the week, ...\n\nThis code is inspired by the tensorflow [time series tutorial](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series)   \nNice data analysis can be found [here](https:\/\/www.kaggle.com\/thexyzt\/keeping-it-simple-by-xyzt) \n\n## Notes:\n- We have 50 items and 10 stores\n- We should have 500 models!\n- However, we can train one model for all items and stores (One global model)\n- At test time, we give the model the most recent historical data for a given item and store, and then ask the trained model to predict the next future steps\n\n\n*This code is for demonstration purposes only*","d720744b":"## What is Next:\n- Prepare the whole data where the sequences are separated at item\/store boundaries (Where no sequences containing different item\/store)\n- Use one-hot-encoding for items and stores  \n- Hyper-parameter optimization for the model\n- Train, evaluate and then test the model on the required test set\n- Submit the results :)   ","54994f9d":"Do not worry about the overfitting, we used **EarlyStopping** and **restore_best_weights**"}}