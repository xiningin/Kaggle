{"cell_type":{"42194a27":"code","6cbbe875":"code","148ebb44":"code","4f4090d8":"code","615c0020":"code","d69cca6d":"code","18c4786e":"code","639f6087":"code","021fcab1":"code","2ee18a54":"code","61c3bb02":"code","520ace78":"code","f0eeeda4":"code","dbc0f847":"code","72baab29":"code","3d1b0648":"code","639d4a16":"code","ce936e1b":"code","cf5c8df8":"code","138ee4ef":"code","df53d2bb":"code","30122310":"code","484312c1":"code","1ce91f29":"code","0ada2ead":"code","8a1a1318":"code","6a6c3e17":"code","0ebb8889":"code","116ba33c":"code","f5727410":"markdown","55045b8e":"markdown","895ba856":"markdown","b43345fd":"markdown","5011a72f":"markdown","a49edb70":"markdown","3ae72f90":"markdown","f4e9422d":"markdown","60d304c4":"markdown","f3d4112e":"markdown","881615c5":"markdown","f7803e34":"markdown","b08ece79":"markdown","57eebba7":"markdown","c27c0270":"markdown","7cbf5cfc":"markdown","04955d4e":"markdown","7fc7c4e8":"markdown","82a40370":"markdown","24831baf":"markdown","e46a2e04":"markdown","d19a73cd":"markdown","916e9e67":"markdown"},"source":{"42194a27":"import numpy as np\nimport pandas as pd\nimport json\nimport pandas_profiling as pdp\nimport missingno\nfrom tqdm import tqdm\nfrom IPython.core.display import HTML, Image\nimport os\n\n# graphs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ML\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report","6cbbe875":"# Variables\npandas_profiling_executor = False # Change it for True if you want to update the latest report\npandas_profiling_file = \"covid-19_br_einsteindata4u.html\"\npandas_profiling_link = \"https:\/\/github.com\/DougTrajano\/ds_covid-19_brazil\/blob\/master\/covid-19_br_einsteindata4u.html\"\ntarget_col = \"SARS-Cov-2 exam result\"\nfile_path = \"\/kaggle\/input\/covid19\/dataset.xlsx\"","148ebb44":"print(file_path)\ndf = pd.read_excel(file_path)\n\nprint(df.shape)\ndf.head()","4f4090d8":"if pandas_profiling_executor:\n    profile = pdp.ProfileReport(df, title='COVID-19 - Hospital Albert Einstein BR')\n    profile.to_file(output_file=pandas_profiling_file)\n    pandas_profiling_link = pandas_profiling_file\n    print(\"pandas_profiling_link changed to {}\".format(pandas_profiling_file))\n    \nHTML('<a href=\"{href}\" target=\"_blank\">{link_name}<\/a>'.format(href=pandas_profiling_link,\n                                                               link_name=pandas_profiling_file))","615c0020":"cols_convert_to_binary = [\"SARS-Cov-2 exam result\", \"Respiratory Syncytial Virus\", \"Influenza A\", \"Influenza B\",\n                         \"Parainfluenza 1\", \"CoronavirusNL63\", \"Rhinovirus\/Enterovirus\", \"Coronavirus HKU1\",\n                         \"Parainfluenza 3\", \"Chlamydophila pneumoniae\", \"Adenovirus\", \"Parainfluenza 4\",\n                         \"Coronavirus229E\", \"CoronavirusOC43\", \"Inf A H1N1 2009\", \"Bordetella pertussis\", \n                         \"Metapneumovirus\", \"Parainfluenza 2\", \"Influenza B, rapid test\", \"Influenza A, rapid test\",\n                         \"Strepto A\", \"Urine - Esterase\", \"Urine - Hemoglobin\", \"Urine - Bile pigments\",\n                         \"Urine - Ketone Bodies\", \"Urine - Urobilinogen\", \"Urine - Protein\", \"Urine - Hyaline cylinders\",\n                         \"Urine - Granular cylinders\", \"Urine - Yeasts\", ]\n\ncols_categorical = [\"Urine - Aspect\", \"Urine - Crystals\", \"Urine - Leukocytes\", \"Urine - Color\"]\n\ncols_invalid = [\"Patient ID\", \"Mycoplasma pneumoniae\", \"Fio2 (venous blood gas analysis)\", \"Urine - pH\",\n                \"Urine - Nitrite\", \"Urine - Sugar\", \"Partial thromboplastin time\\xa0(PTT)\\xa0\", \"Vitamin B12\"]","d69cca6d":"missingno.matrix(df);","18c4786e":"df = df[df[\"Hemoglobin\"].notnull()]\ndf.reset_index(drop=True, inplace=True)\nprint(df.shape)","639f6087":"df_missing = []\nfor col in df.columns:\n    missing = (len(df[df[col].notnull()]) \/ len(df))*100\n    missing = round(abs(missing - 100),2)\n    df_missing.append({\"column\": col, \"missing percentage\": missing})\n    if missing == 0:\n        print(\"Column {col} has no missing values.\".format(col=col))\n    else:\n        print(\"Column {col} has {missing}% of missing values.\".format(col=col, missing=missing))\n\ndf_missing = pd.DataFrame(df_missing)","021fcab1":"missingno.matrix(df);","2ee18a54":"def _convert_to_binary(value):\n    positive_lst = [1, \"positive\", \"detected\", \"present\", \"normal\"]\n    negative_lst = [0, \"negative\", \"not_detected\", \"absent\", \"not_done\"]\n    if value in positive_lst:\n        return 1\n    elif value in negative_lst:\n        return 0\n    else:\n        return value\n    \n    \ndef processing(dataset, cols_invalid=None, cols_categorical=None, cols_convert_to_binary=None):\n    \"\"\"\n    docstring\n    \"\"\"\n    temp = dataset.to_dict(orient=\"records\")    \n    df_processed = []\n    \n    # processing each record\n    with tqdm(total=len(temp)) as pbar:\n        for values in temp:\n            if isinstance(cols_convert_to_binary, list):\n                for col in cols_convert_to_binary:\n                    values[col] = _convert_to_binary(values[col])\n\n            if isinstance(cols_invalid, list):    \n                for col in cols_invalid:\n                    del values[col]\n\n            if isinstance(cols_categorical, list):\n                for col in cols_categorical:\n                    values = _encoder(values)\n        \n            # add processed record\n            df_processed.append(values)\n            pbar.update(1)\n        \n    df_processed = pd.DataFrame(df_processed)\n    return df_processed\n\n\ndef _encoder(value):\n    filename = \"cat_features_encoding.json\"\n    with open(filename, 'r') as filename:\n        encoding = json.load(filename)\n        \n    for col in encoding.keys():        \n        for i in encoding[col]:\n            if value[col] == i:\n                value[col] = encoding[col][i]             \n        \n    return value\n\n\ndef create_encoder(dataset, cat_features):\n    \"\"\"\n    This function can create a Label Encoder for categorical features.\n    \n    A json file called \"cat_features_encoding.json\" will be saved on folder's script. This file can be used on _encoder function.\n    Input\n    - dataset (DataFrame, required): The DataFrame loaded from listings.csv.\n    - cat_features (list, required): The categorical features list that you want to convert in numeric values.\n    Output\n    - encoding (dict): A dictionary with the encoder created. The same content of json file.\n    \"\"\"\n    encoding = {}\n    filename = \"cat_features_encoding.json\"\n    for col in cat_features:\n        temp = {}\n        n_values = dataset[col].unique()\n        i = 0\n        for n in n_values:\n            temp[str(n)] = i\n            i += 1\n        encoding[col] = temp\n    \n    # save json file encoder\n    with open(filename, 'w') as filename:\n        json.dump(encoding, filename)\n        \n    return encoding","61c3bb02":"for col in cols_categorical:\n    print(col, len(df[col].unique()))\n    print(df[col].unique())\n    print()","520ace78":"create_encoder(df, cols_categorical)","f0eeeda4":"df_processed = processing(df, cols_invalid=cols_invalid, cols_categorical=cols_categorical,\n                         cols_convert_to_binary=cols_convert_to_binary)\n\nprint(df_processed.shape)\ndf_processed.head()","dbc0f847":"df_processed.info()","72baab29":"# Correlation features\ncor = df_processed.corr()\ncor_target = abs(cor[target_col])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.05].sort_values(ascending=False)\n\ndf_cols_stats = pd.DataFrame(relevant_features[1:])\ndf_cols_stats.reset_index(inplace=True)\ndf_cols_stats.columns = [\"column\", \"correlation with exam result\"]\ndf_cols_stats = pd.merge(df_cols_stats, df_missing, on=\"column\")\ndf_cols_stats.head(20)","3d1b0648":"cols_pairplot_filter = df_cols_stats[df_cols_stats[\"missing percentage\"] < 90][\"column\"].tolist()\ncols_pairplot_filter.append(target_col) # add target_col to the columns\n\ndf_cols_stats[df_cols_stats[\"missing percentage\"] < 90]","639d4a16":"df_processed[cols_pairplot_filter].groupby(by=target_col).count().transpose()","ce936e1b":"%%time\npairplot_file = \"pairplot_1.png\"\ntemp = cols_pairplot_filter[:20]\nif target_col not in temp:\n    temp.append(target_col)\n\nprint(\"Pairplot - 1\")\nsns.pairplot(df_processed[temp], hue=target_col, diag_kind=\"hist\", dropna=True)\nplt.savefig(pairplot_file)\n\nplt.clf() # Clean parirplot figure from sns\n\nImage(filename=pairplot_file) # Show pairplot as image","cf5c8df8":"# Download image\nHTML('<a href=\"{href}\" target=\"_blank\">Download image (1)<\/a>'.format(href=pairplot_file))","138ee4ef":"%%time\npairplot_file = \"pairplot_2.png\"\ntemp = cols_pairplot_filter[20:]\nif target_col not in temp:\n    temp.append(target_col)\n    \nprint(\"Pairplot - 2\")\nsns.pairplot(df_processed[temp], hue=target_col, diag_kind=\"hist\", dropna=True)\nplt.savefig(pairplot_file)\n\nplt.clf() # Clean parirplot figure from sns\n\nImage(filename=pairplot_file) # Show pairplot as image","df53d2bb":"# Download image\nHTML('<a href=\"{href}\" target=\"_blank\">Download image (2)<\/a>'.format(href=pairplot_file))","30122310":"df_processed[cols_pairplot_filter].info()","484312c1":"X = df_processed[cols_pairplot_filter]\n\nimputer = KNNImputer(n_neighbors=4)\nX = imputer.fit_transform(X.values)\n\nX = pd.DataFrame(X, columns=cols_pairplot_filter)\nX.head()","1ce91f29":"y = X[target_col].values\nX.drop(columns=[target_col], inplace=True)","0ada2ead":"temp = pd.DataFrame(df_processed[target_col].value_counts())\n\nfor each in range(len(temp)):\n    percentage = temp.values[each]\/sum(temp.values)\n    percentage = round(percentage[0]*100, 2)\n    print(\"Class {ix}: {qty} - ({percentage}%)\".format(ix=temp.index[each], qty=temp.values[each], percentage=percentage))\n\ndf_processed[\"SARS-Cov-2 exam result\"].value_counts().plot(kind=\"bar\", title=\"Classes distribution\", legend=True, rot=1);","8a1a1318":"class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n\nclass_weights = {\n    0: class_weights[0],\n    1: class_weights[1]\n}\n\nprint(class_weights)","6a6c3e17":"# Split the dataset in train and test.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint(\"train size:\", len(X_train))\nprint(\"test size:\", len(X_test))","0ebb8889":"%%time\nclf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0, \n                             min_samples_leaf=5, min_samples_split=15,\n                             class_weight=class_weights, n_jobs=-1)\n\nclf.fit(X_train, y_train)\n\nscores = cross_val_score(clf, X, y, cv=5)\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy (cross-validation): %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\nprint()\nprint(classification_report(y_test, y_pred, digits=4))","116ba33c":"%%time\n\nparameters = {'n_estimators':[25, 50, 100, 200, 300],\n             \"min_samples_leaf\": [5, 10, 15, 20],\n             \"min_samples_split\": [5, 10, 15, 20],\n             \"criterion\": [\"gini\", \"entropy\"]}\n\nmodel = RandomForestClassifier(random_state=0, class_weight=class_weights, n_jobs=-1)\nclf = GridSearchCV(model, parameters, cv=5, verbose=2, n_jobs=-1)\nclf.fit(X_train, y_train)\nclf = clf.best_estimator_\npreds = clf.predict(X_test)\nprint(classification_report(y_test, preds, digits=4))\nclf","f5727410":"## \/ Filling missing values\n\nFilling **missing values** in this dataset will be a challenge because the percentage of missing values it's very high.\n\nWe'll get the relevant features that has more than 10% of the values, for these columns we'll use KNNImputer to complete missing values.","55045b8e":"## \/ Split in X and Y\n\nIn this section we'll split the features `X` and the answer `Y`.","895ba856":"Almost all of relevant features has a high number of missing values.\n\nI'll trying to get a mix of correlation and missing percentage that I think that it's good enough.","b43345fd":"## \/ GridSearchCV","5011a72f":"---\n## \/ Pairplots\n\nThe pairplot created by Seaborn it's very useful to understand the behavior of a specific column between others columns of the dataset.\n\nWe'll check how the **relevant features** are distributed considering `SARS-Cov-2 exam result`.","a49edb70":"It's better, but we lost a lot of data and actually we have missig values yet. We'll works on this later.","3ae72f90":"I'll cut the dataset by `Hemoglobin` because when this feature is present, the missing values percentage it's smaller.","f4e9422d":"## \/ Load file","60d304c4":"## \/ Check missing Values\n\nUnfortunally, almost all columns have more than 90% of missing data.","f3d4112e":"## \/ Unbalanced classes\n\nHouston. We have a problem! o\/\n\nWe have **unbalanced classes**. See below the numbers of each classes.","881615c5":"---\n## \/ Resume about pandas Profiling\n\nWe have a lot of features that has missing values,\n\nTo processind the dataset, we'll divide the columns in three categories:\n\n- `cols_convert_to_binary`: Columns to be replaced by boolean values (0, 1).\n- `cols_categorical`:  Columns to be replaced by numeric values in encoder.\n- `cols_invalid`: Columns to be removed because it's ID or have no data available.","f7803e34":"---\n## \/ LabelEncoder for categorical features","b08ece79":"---\n# \/ Data Transformation and Exploratory analysis\n\nI decided to transform the data and explore it in the same time.","57eebba7":"# \/ Conclusions\n\nWe have few samples for **positive cases** in `SARS-Cov-2 exam result` feature and for this small dataset of positive cases, we have a lot of missing values that doesn't provide any trust that this model will works in the real world. We filled a lot of missing values in the dataset and that it's not good.\n\nIn the meantime, I think that this study can help advance knowledge about **COVID-19**.\n\nI hope that more data about clinical spectrum can be provided. In the future, we can provide better models or understand the impacts of the coronav\u00edrus on our bodies.","c27c0270":"# \/ Introduction\n\nThis notebook and others files are available in this [repository](https:\/\/github.com\/DougTrajano\/ds_covid-19_brazil) on GitHub.\n\n## Objective\n\nThe objective of this analysis and modeling is trying to answer the question below.\n\n> [Predict confirmed COVID-19 cases among suspected cases](https:\/\/www.kaggle.com\/einsteindata4u\/covid19\/tasks?taskId=645)\n\n## Data\n\nThe data used was provided by Albert Einstein Hospital from Brazil.\n\nThe target that we will trying to predict is `SARS-Cov-2 exam result`.\n\n> If value is `1` the exam to SARS-Cov-2 was **positive**.\n\n> If value is `0` the exam to SARS-Cov-2 was **negative**.\n\n## Alerts\n\n- This dataset doesn't has a metadata available, but I'll describe some variables based on their names.\n- I don't have access to a doctor to provide health knowledge for this study. Then, the results from this analysis is invalid to use as real conclusions.","7cbf5cfc":"## \/ Random Forest","04955d4e":"---\n\n# \/ Modeling","7fc7c4e8":"We can see that these selected features has a good distribution between values of the target_col `SARS-Cov-2 exam result`.\n\nI don't know if this distribution make sense in the health sector, but seeing only in the data for this small dataset, we can see that.","82a40370":"## \/ Class weight\n\nTo solve the **unbalanced classes** we will compute class weight and use it on the algorithms.","24831baf":"---\n## \/ Relevant features\n\nUsing correlation we can filter relevant features on this dataset. The correlation will be applied in target_col `SARS-Cov-2 exam result`.","e46a2e04":"---\n## \/ Pandas Profiling\n\nPandas Profile it's an amazing lib to get a quickly exploratory analysis.","d19a73cd":"## \/ Imports, variables and configs setting.","916e9e67":"## \/ Split in train and test\n\nHere we'll split the datasets (`X`, `Y`) again, but now, in **train** and **test**."}}