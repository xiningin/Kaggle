{"cell_type":{"6e39dc5f":"code","398a5d6c":"code","662a1bba":"code","be9990fe":"code","88629d8d":"code","5e27dcbf":"code","aab3dc81":"code","cb16f004":"code","b24f3d51":"code","94bfca85":"code","f73661cb":"code","51b0e32d":"code","e5c234fc":"code","f8742664":"code","16761496":"code","c8e8fac7":"code","26b3c3b5":"code","446fd45d":"code","8877f642":"code","758c9f49":"code","3a5b4faf":"code","e2d113f6":"code","e1a5cff7":"code","8e10bb3c":"code","c8c0ea14":"code","557d2ba9":"code","f5c1ee3c":"code","bb178cff":"code","e31b6159":"code","a3b55571":"code","14fa7dff":"markdown","37c1e6d1":"markdown","ce4e56dc":"markdown","2f4ebbb1":"markdown","768f8931":"markdown","27340103":"markdown","d650fca9":"markdown","f227e598":"markdown","cc2a5de7":"markdown","6c20b25c":"markdown","01f3bde7":"markdown","2c67878c":"markdown","c1695e8d":"markdown","4476c9f7":"markdown","c2494010":"markdown","8c3497f8":"markdown"},"source":{"6e39dc5f":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import cross_validation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import log_loss\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport datetime\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, log_loss\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom statsmodels.tools import eval_measures\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\nfrom matplotlib import  pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nfrom statsmodels.base.model import GenericLikelihoodModel\nfrom sklearn.svm import SVR\nimport time as time\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nX_train = pd.read_csv('..\/input\/dengue_features_train.csv')\nX_test = pd.read_csv('..\/input\/dengue_features_test.csv')\ny_train = pd.read_csv('..\/input\/dengue_labels_train.csv')\n\n\nprint(X_train.head(10))\nprint(X_train.describe())\nprint(X_train.info())\n\n# Let's check the percentage of missing values in our dataset\n\ntotal = X_train.isnull().sum().sort_values(ascending=False)\npercent = (X_train.isnull().sum()\/X_train['city'].count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n","398a5d6c":"ax = sns.countplot(X_train['city'],label=\"Count\")       # M = 212, B = 357\nSanJuan, Iquitos = X_train['city'].value_counts()\nprint('San Juan: ', SanJuan)\nprint('Iquitos : ', Iquitos)\n","662a1bba":"data_dia = X_train.copy()\ny = X_train['city']\ndel data_dia['city']\ndel data_dia['year']\ndel data_dia['week_start_date']\n\n\n\ndel data_dia['reanalysis_sat_precip_amt_mm']\ndel data_dia['precipitation_amt_mm']\ndel data_dia['reanalysis_avg_temp_k']\ndel data_dia['reanalysis_air_temp_k']\ndel data_dia['reanalysis_dew_point_temp_k']\ndel data_dia['reanalysis_max_air_temp_k']\ndel data_dia['reanalysis_min_air_temp_k']\ndel data_dia['reanalysis_precip_amt_kg_per_m2']\ndel data_dia['reanalysis_relative_humidity_percent']\ndel data_dia['reanalysis_specific_humidity_g_per_kg']\ndel data_dia['reanalysis_tdtr_k']\n\ndata_n_2 = (data_dia - data_dia.mean()) \/ (data_dia.std())              # standardization\ndata = pd.concat([y,data_n_2],axis=1)\ndata = pd.melt(data,id_vars=\"city\",\n                    var_name=\"features\",\n                    value_name='value')\npd.to_numeric(data['value'], downcast='float')\nplt.figure(figsize=(10,10))\nax = sns.violinplot(x=\"features\", y=\"value\", hue=\"city\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","be9990fe":"print(X_train['station_precip_mm'].describe())","88629d8d":"data_dia2 = X_train.copy()\ndel data_dia2['city']\ndel data_dia2['year']\ndel data_dia2['week_start_date']\n\n\ndel data_dia2['station_precip_mm']\ndel data_dia2['reanalysis_sat_precip_amt_mm']\ndel data_dia2['precipitation_amt_mm']\ndel data_dia2['reanalysis_avg_temp_k']\ndel data_dia2['reanalysis_air_temp_k']\ndel data_dia2['reanalysis_dew_point_temp_k']\ndel data_dia2['reanalysis_max_air_temp_k']\ndel data_dia2['reanalysis_min_air_temp_k']\ndel data_dia2['reanalysis_precip_amt_kg_per_m2']\ndel data_dia2['reanalysis_relative_humidity_percent']\ndel data_dia2['reanalysis_specific_humidity_g_per_kg']\ndel data_dia2['reanalysis_tdtr_k']\n\ndata_n_22 = (data_dia2 - data_dia2.mean()) \/ (data_dia2.std())              # standardization\n\ndata2 = pd.concat([y,data_n_22],axis=1)\n\ndata2 = pd.melt(data2,id_vars=\"city\",\n                    var_name=\"features\",\n                    value_name='value')\npd.to_numeric(data2['value'], downcast='float')\nplt.figure(figsize=(10,10))\nax = sns.violinplot(x=\"features\", y=\"value\", hue=\"city\", data=data2,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\n\n","5e27dcbf":"data_dia2 = X_train.copy()\ndel data_dia2['city']\ndel data_dia2['year']\ndel data_dia2['week_start_date']\ndel data_dia2['ndvi_ne']\ndel data_dia2['ndvi_nw']\ndel data_dia2['station_diur_temp_rng_c']\ndel data_dia2['station_avg_temp_c']\ndel data_dia2['station_precip_mm']\ndel data_dia2['ndvi_se']\ndel data_dia2['ndvi_sw']\ndel data_dia2['station_max_temp_c']\ndel data_dia2['station_min_temp_c']\n\n\ndata_n_22 = (data_dia2 - data_dia2.mean()) \/ (data_dia2.std())              # standardization\n\ndata2 = pd.concat([y,data_n_22],axis=1)\n\ndata2 = pd.melt(data2,id_vars=\"city\",\n                    var_name=\"features\",\n                    value_name='value')\npd.to_numeric(data2['value'], downcast='float')\nplt.figure(figsize=(10,10))\nax = sns.violinplot(x=\"features\", y=\"value\", hue=\"city\", data=data2,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\n","aab3dc81":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X_train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n#empt = ['ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','precipitation_amt_mm','reanalysis_air_temp_k','reanalysis_avg_temp_k','reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k','reanalysis_min_air_temp_k','reanalysis_precip_amt_kg_per_m2','reanalysis_relative_humidity_percent','reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg','reanalysis_tdtr_k','station_avg_temp_c','station_diur_temp_rng_c','station_max_temp_c','station_min_temp_c','station_precip_mm']\n#","cb16f004":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia2 = X_train.copy()\ndel data_dia2['city']\ndel data_dia2['year']\ndel data_dia2['week_start_date']\n\n\ndel data_dia2['station_precip_mm']\ndel data_dia2['reanalysis_sat_precip_amt_mm']\ndel data_dia2['precipitation_amt_mm']\ndel data_dia2['reanalysis_avg_temp_k']\ndel data_dia2['reanalysis_air_temp_k']\ndel data_dia2['reanalysis_dew_point_temp_k']\ndel data_dia2['reanalysis_max_air_temp_k']\ndel data_dia2['reanalysis_min_air_temp_k']\ndel data_dia2['reanalysis_precip_amt_kg_per_m2']\ndel data_dia2['reanalysis_relative_humidity_percent']\ndel data_dia2['reanalysis_specific_humidity_g_per_kg']\ndel data_dia2['reanalysis_tdtr_k']\n\ndata_n_22 = (data_dia2 - data_dia2.mean()) \/ (data_dia2.std())              # standardization\n\ndata2 = pd.concat([y,data_n_22],axis=1)\n\ndata2 = pd.melt(data2,id_vars=\"city\",\n                    var_name=\"features\",\n                    value_name='value')\npd.to_numeric(data2['value'], downcast='float')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"city\", data=data2)\nplt.xticks(rotation=90)","b24f3d51":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia2 = X_train.copy()\ndel data_dia2['city']\ndel data_dia2['year']\ndel data_dia2['week_start_date']\ndel data_dia2['ndvi_ne']\ndel data_dia2['ndvi_nw']\ndel data_dia2['station_diur_temp_rng_c']\ndel data_dia2['station_avg_temp_c']\ndel data_dia2['station_precip_mm']\ndel data_dia2['ndvi_se']\ndel data_dia2['ndvi_sw']\ndel data_dia2['station_max_temp_c']\ndel data_dia2['station_min_temp_c']\n\n\ndata_n_22 = (data_dia2 - data_dia2.mean()) \/ (data_dia2.std())              # standardization\n\ndata2 = pd.concat([y,data_n_22],axis=1)\n\ndata2 = pd.melt(data2,id_vars=\"city\",\n                    var_name=\"features\",\n                    value_name='value')\npd.to_numeric(data2['value'], downcast='float')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"city\", data=data2)\nplt.xticks(rotation=90)","94bfca85":"\n\nnew_xtrain = X_train.copy()\nnew_xtrain['total_cases'] = y_train['total_cases']\n\nplt.plot_date(new_xtrain['week_start_date'], new_xtrain['total_cases'], color='green', label='sj')\nplt.plot_date(new_xtrain['week_start_date'], new_xtrain['total_cases'], color='blue', label='iq')\nplt.xlabel('Year')\nplt.ylabel('Total cases')\nplt.legend(loc='upper right')\nplt.show()","f73661cb":"delete = ['week_start_date','reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg']\nfor z in delete:\n    del X_train[z]\n    del X_test[z]","51b0e32d":"X_trainc = pd.read_csv('..\/input\/dengue_features_train.csv')\nX_testc = pd.read_csv('..\/input\/dengue_features_test.csv')\ny_trainc = pd.read_csv('..\/input\/dengue_labels_train.csv')\n\n\nX_train_sj = X_trainc.loc[X_train.city=='sj']\ny_train_sj = y_trainc.loc[y_train.city=='sj']\n\nX_train_iq = X_trainc.loc[X_train.city=='iq']\ny_train_iq = y_trainc.loc[y_train.city=='iq']\n\nprint('features: ', X_train_sj.shape)\nprint('labels  : ', y_train_sj.shape)\n\nprint('\\nIquitos')\nprint('features: ', X_train_iq.shape)\nprint('labels  : ', y_train_iq.shape)\n","e5c234fc":"(X_train_sj.ndvi_ne.plot.line(lw=0.8))\nplt.title('Vegetation Index over Time')\nplt.xlabel('Time')","f8742664":"print('San Juan')\nprint('mean: ', y_train_sj.mean()[2])\nprint('var :', y_train_sj.var()[2])\n\nprint('\\nIquitos')\nprint('mean: ', y_train_iq.mean()[2])\nprint('var :', y_train_iq.var()[2])","16761496":"X_train_sj['total_cases'] = y_train_sj['total_cases']\nX_train_iq['total_cases'] = y_train_iq['total_cases']\n\n\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X_train_sj.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nax.set_title('Correlations for San Juan')\nplt.show()\n\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X_train_iq.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nax.set_title('Correlations for Iquitos')\nplt.show()","c8e8fac7":"sj_correlations = X_train_sj.corr()\niq_correlations = X_train_iq.corr()\n(sj_correlations['total_cases'].drop('total_cases').sort_values(ascending=False).plot.barh())\nplt.title('Correlations for San Juan')","26b3c3b5":"(iq_correlations['total_cases'].drop('total_cases').sort_values(ascending=False).plot.barh())\nplt.title('Correlations for Iquitos')","446fd45d":"for i in set(y_train_sj['year']):\n    df = y_train_sj[y_train_sj['year'] == i]\n    df.set_index('weekofyear', drop = True, inplace = True)\n    plt.plot(df['total_cases'], alpha = .3)\n    \ny_train_sj.groupby('weekofyear')['total_cases'].mean().plot(c = 'k', figsize = (10,4))\nplt.legend(set(y_train_sj['year']), loc='center left', bbox_to_anchor=(1, .5))\n\nplt.title('Number of Cases per Week in San Juan, Puerto Rico')\nplt.xlabel('Week of the Year')\nplt.ylabel('Number of Cases')","8877f642":"for i in set(y_train_iq['year']):\n    df = y_train_iq[y_train_iq['year'] == i]\n    df.set_index('weekofyear', drop = True, inplace = True)\n    plt.plot(df['total_cases'], alpha = .3)\n\ny_train_iq.groupby('weekofyear')['total_cases'].mean().plot(c = 'k', figsize = (10,4))\nplt.legend(set(y_train_iq['year']), loc='center left', bbox_to_anchor=(1, .5))\n\nplt.title('Number of Cases per Week in Iquitos, Peru')\nplt.xlabel('Week of the Year')\nplt.ylabel('Number of Cases')","758c9f49":"X_test_sj = X_testc.loc[X_testc.city=='sj']\nX_test_iq = X_testc.loc[X_testc.city=='iq']\n\n\nX_train_sj=X_train_sj.join(X_train_sj.groupby(['city','weekofyear'])['total_cases'].mean(), on=['city','weekofyear'], rsuffix='_avg')\nX_test_sj=X_test_sj.join(X_train_sj.groupby(['city','weekofyear'])['total_cases'].mean(), on=['city','weekofyear'], rsuffix='_avg')\nX_train_iq=X_train_iq.join(X_train_iq.groupby(['city','weekofyear'])['total_cases'].mean(), on=['city','weekofyear'], rsuffix='_avg')\nX_test_iq=X_test_iq.join(X_train_iq.groupby(['city','weekofyear'])['total_cases'].mean(), on=['city','weekofyear'], rsuffix='_avg')\n\nfeatures2=['total_cases','total_cases', 'reanalysis_specific_humidity_g_per_kg','station_avg_temp_c','reanalysis_dew_point_temp_k','station_min_temp_c','station_max_temp_c','reanalysis_min_air_temp_k','reanalysis_max_air_temp_k','reanalysis_air_temp_k','reanalysis_avg_temp_k','reanalysis_specific_humidity_g_per_kg','reanalysis_dew_point_temp_k','reanalysis_min_air_temp_k','station_min_temp_c']      \n\n#TRAIN\nX_sj= X_train_sj[features2]\nY_sj = X_train_sj['total_cases']\n\nX_iq= X_train_iq[features2]\nY_iq = X_train_iq['total_cases']\n\n#TEST\nX_sj_t= X_test_sj[features2]\nX_iq_t= X_test_iq[features2]\n\nX_sj.fillna(method='bfill', inplace=True)\nX_iq.fillna(method='bfill', inplace=True)\n\nX_sj_t.fillna(method='bfill', inplace=True)\nX_iq_t.fillna(method='bfill', inplace=True)\n","3a5b4faf":"##SAN JUAN\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X_sj,Y_sj)\nsvr_fit = time.time() - t0\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n      % svr_fit)\nmodel_sj=svr.best_estimator_\nprint(model_sj)","e2d113f6":"##IQUITOS\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X_iq,Y_iq)\nsvr_fit = time.time() - t0\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n      % svr_fit)\nmodel_iq=svr.best_estimator_\nprint(model_iq)","e1a5cff7":"#KNN \nk_range = list(range(1, 50))\nknnsj = KNeighborsClassifier(n_neighbors = 30)\nknnsj.fit(X_sj,Y_sj)\n\n\nknniq = KNeighborsClassifier(n_neighbors = 30)\nknniq.fit(X_iq,Y_iq)\n\n\n# ----------------\n# LogisticRegression\n# ----------------\nlogregsj = LogisticRegression()\nlogregsj.fit(X_sj, Y_sj)\n\nlogregiq = LogisticRegression()\nlogregiq.fit(X_iq, Y_iq)","8e10bb3c":"map_city = {'sj':0, 'iq':1}\n\ndatasets = [X_train_sj, X_train_iq, y_train_sj ,y_train_iq,X_test_sj, X_test_iq ]\n\nfor df in datasets:\n    df['city'] = df['city'].map(map_city)\n\n#sj_train_subtrain = X_train_sj.head(700)\n#sj_train_subtest = X_train_sj.tail(X_train_sj.shape[0] - 700)\nsj_train_subtrain = X_train_sj.sample(frac=0.7)\nsj_train_subtest = X_train_sj.loc[~X_train_sj.index.isin(sj_train_subtrain.index)]\n\n\n#iq_train_subtrain = X_train_iq.head(350)\n#iq_train_subtest = X_train_iq.tail(X_train_iq.shape[0] - 350)\niq_train_subtrain = X_train_iq.sample(frac=0.7)\niq_train_subtest = X_train_iq.loc[~X_train_iq.index.isin(iq_train_subtrain.index)]\n\n\n\niq_train_subtrain.fillna(method='bfill', inplace=True)\niq_train_subtest.fillna(method='bfill', inplace=True)\nsj_train_subtrain.fillna(method='bfill', inplace=True)\nsj_train_subtest.fillna(method='bfill', inplace=True)\n","c8c0ea14":"#create preds\npreds_sj= model_sj.predict(sj_train_subtest[features2]).astype(int)\npreds_iq=model_iq.predict(iq_train_subtest[features2]).astype(int)\n#add to the dataframes\nsj_train_subtest['fitted'] = preds_sj\niq_train_subtest['fitted'] = preds_iq\n### reset axis\nsj_train_subtest.index = sj_train_subtest['week_start_date']\niq_train_subtest.index = iq_train_subtest['week_start_date']\n## make plot\n\n\n# KNN\npreds_sj2 = knnsj.predict(sj_train_subtest[features2]).astype(int)\npreds_iq2 = knniq.predict(iq_train_subtest[features2]).astype(int)\n\nsj_train_subtest['fitted2'] = preds_sj2\niq_train_subtest['fitted2'] = preds_iq2\n\nsj_train_subtest.index = sj_train_subtest['week_start_date']\niq_train_subtest.index = iq_train_subtest['week_start_date']\n\n# Logistic regression\n\npreds_sj3 = logregsj.predict(sj_train_subtest[features2]).astype(int)\npreds_iq3 = logregiq.predict(iq_train_subtest[features2]).astype(int)\n\nsj_train_subtest['fitted3'] = preds_sj3\niq_train_subtest['fitted3'] = preds_iq3\n\nsj_train_subtest.index = sj_train_subtest['week_start_date']\niq_train_subtest.index = iq_train_subtest['week_start_date']\n\n\n\"\"\"\nplt.figure(figsize=(15,10))\nsj_train_subtest.total_cases.plot(label=\"Actual\")\nsj_train_subtest.fitted.plot( label=\"Predictions\")\nplt.title(\"Dengue Predicted Cases vs. Actual Cases in San Juan\")\nplt.legend()\n\n\"\"\"","557d2ba9":"\n## make plot\nplt.figure(figsize=(15,10))\nsj_train_subtest.total_cases.plot(label=\"Actual\")\nsj_train_subtest.fitted.plot( label=\"Predictions SVM\")\nsj_train_subtest.fitted2.plot( label=\"Predictions KNN\")\nsj_train_subtest.fitted3.plot( label=\"Predictions logreg\")\nplt.title(\"Dengue Predicted Cases vs. Actual Cases in San Juan\")\nplt.legend()\n","f5c1ee3c":"plt.figure(figsize=(15,10))\niq_train_subtest.total_cases.plot(label=\"Actual\")\niq_train_subtest.fitted.plot(label=\"Predictions SVM\")\niq_train_subtest.fitted2.plot( label=\"Predictions KNN\")\niq_train_subtest.fitted3.plot( label=\"Predictions logreg\")\nplt.title(\"Dengue Predicted Cases vs. Actual Cases in Iquitos\")\nplt.legend()","bb178cff":"\nscores = cross_val_score(knnsj, X_sj,Y_sj, cv=5, scoring = \"accuracy\")\nprint(\"Scores for San Juan using SVM:\", scores)\nprint(\"Mean for San Juan using SVM:\", scores.mean())\n\nscores2 = cross_val_score(knniq, X_iq,Y_iq, cv=5, scoring = \"accuracy\")\nprint(\"Scores for San Juan using SVM:\", scores2)\nprint(\"Mean for San Juan using SVM:\", scores2.mean())","e31b6159":"knnsj.fit(X_sj,Y_sj)\nknniq.fit(X_iq,Y_iq)\n\nsj_predictions = knnsj.predict(X_sj_t).astype(int)\niq_predictions = knniq.predict(X_iq_t).astype(int)\n\nprint(sj_predictions)\n\nsubmission = pd.read_csv(\"..\/input\/submission_format.csv\",\n                         index_col=[0, 1, 2])\n\nsubmission.total_cases = np.concatenate([sj_predictions, iq_predictions])\n\nsubmission.to_csv(\"knn.csv\")","a3b55571":"\n\nmodel_sj.fit(X_sj,Y_sj)\nmodel_iq.fit(X_iq,Y_iq)\n\nsj_predictions2 = model_sj.predict(X_sj_t).astype(int)\niq_predictions2 = model_iq.predict(X_iq_t).astype(int)\n\nprint(sj_predictions2)\n\nsubmission2 = pd.read_csv(\"..\/input\/submission_format.csv\",\n                         index_col=[0, 1, 2])\n\nsubmission2.total_cases = np.concatenate([sj_predictions2, iq_predictions2])\n\nsubmission2.to_csv(\"svm.csv\")","14fa7dff":"# Feature Selection\n\nOk then, so we are now ready for selecting the features we will use for classification. For the moment, I will drop the following features: **reanalysis_sat_precip_amt_mm**, **reanalysis_specific_humidity_g_per_kg **. I won't do the same with the ones that have correlation value of 0.9 at this time.\n","37c1e6d1":"There's something strange going on there, right? We see that, after normalizing, the feature \"station_precip_mm\" alters the scale of our graph. Let's print the description of that feature.","ce4e56dc":"There are a few things to notice here. \n\nFirstly, from the description at first sight, from the description we cannot infer too much information. We have numbers that don't mean much to us right now. They seem well, but they may not be correct, so we will deal with them later on.\n\nSecondly, good news. As we can see, there are not to many missing values. We will fill them with the mean later on aswell. Now we will check if there are outliers in our dataset.","2f4ebbb1":"Our target variable, total_cases is a non-negative integer, which means we're looking to make some count predictions. Standard regression techniques for this type of prediction include:\n\n1. Poisson regression\n2. Negative binomial regression\n\nWhich technique will perform better depends on many things, but the choice between Poisson regression and negative binomial regression is pretty straightforward. Poisson regression fits according to the assumption that the mean and variance of the population distribution are equal. When they aren't, specifically when the variance is much larger than the mean, the negative binomial approach is better. Why? It isn't magic. The negative binomial regression simply lifts the assumption that the population mean and variance are equal, allowing for a larger class of possible models. In fact, from this perspective, the Poisson distribution is but a special case of the negative binomial distribution.","768f8931":"1. # **DengAI**\n\n**This code is based on previous works and it's still under construction**\n\n\n## **Table of Contents:**\n* Introduction\n* Exploratory data analysis and Data Preprocessing\n    - Converting Features\n    - Creating new Features\n* Building Machine Learning Models","27340103":"What information does this give us? Well, we can see that there are some features in which the two cities are very separated, and this will be good for classification (the most clear example being **reanalysis_tdtr_k**. On the other hand, there are features in which the information is very mixed, and those will not be that good for classification.\n","d650fca9":"Again, we see that there are different features which have extreme values. Exploring the dataset, it seems that they are not outleirs nor errors, so we cannot drop them, and we will have to take them into account. Those values are about precipitation, and since they are rain values, it is reasonable to assume that depending on the areas weather can change drastically. \n\n\nBut what else can we extract from these plots? For example, in **ndvi_ne** feature, median of the *San Juan* and *Iquitos* looks like separated so it can be good for classification. However, in **reanalysis_avg_temp_k** feature,  median of the *San Juan* and *Iquitos* does not looks like separated so it does not gives good information for classification. This happens with many other features.\n\nAlso, note that the features **reanalysis_avg_temp_k** and **reanalysis_specific_humidity_g_per_kg** have a very similar shape,  but how can we decide whether they are correlated with each other or not? Correlation is the answer.","f227e598":"But there's much more we can do with this dataset! For example, we can create a new dataframe which will be X_train plus the total_cases column of y_train, so that we can cretre new and interesting plots.","cc2a5de7":"**BOOM**. There you have it. It has an extremely large variance. Looking at its mean, it has an average of around 40mm, while the maximum is 543 (after normalizing). Let's drop it for a moment, just to have a clearer view of the plots.","6c20b25c":"If we tried to do machine learning up to this point, we would notice that the accuracy is really small. This is because the total cases vary a lot, from 0 to 400+. So what can we do to improve that? \n\nThe first thing we'll try is to divide our dataset in two, one for San Juan and another one for Iquitos. Since we've modified our dataframes, we'll import them again and play with them.","01f3bde7":"# Building Machine Learning models\n\nNow that we have a more clearer understanding of our dataset, we will proceed to build our ML models.\n\nFirst of all, we will also do the same as we did before, splitting now the test data into San Juan and Iquitos, and do the same pre-processing for both.","2c67878c":"variance >> mean suggests total_cases can be described by a negative binomial distribution, so we'll use a negative binomial regression below. We will now check the correlations to see if there is any difference between the cities. To gain more insight, we will add the colum of total cases to see if there is any correlation with other variables.","c1695e8d":"Checking this last graph, we see that some features have perfect correlation (value 1) and nearly perfect correlation (0.9). These are:\n\n**reanalysis_sat_precip_amt_mm** and** precipitation_amt_mm **  with perfect correlation.\n**reanalysis_specific_humidity_g_per_kg** and** reanalysis_dew_point_temp_k ** with perfect correlation.\n**ndvi_nw** and **ndvi_ne** having correltion value 0.9.\n**reanalysis_avg_temp_k **and **reanalysis_air_temp_k ** having correltion value 0.9.\n**reanalysis_tdtr_k ** and **reanalysis_max_air_temp_k** having correltion value 0.9.\n**station_diur_temp_rng_c ** and **reanalysis_tdtr_k** having correltion value 0.9.\n\n\nBefore making some actions, we will explore a little bit more our dataset.","4476c9f7":"So we have some valuable information here. For both cities,**reanalysis_specific_humidity_g_per_kg**, **reanalysis_dew_point_temp_k** and  **reanalysis_min_air_temp_k** are the ones that are most strongly correlated with the total cases. Does this make sense? Of course, we know that mosquitoes tend to live in places with high humidity. Also, temperature is highly related with the spread of mosquitoes, so it makes sense it is related with the total cases. Surprisingly, in the city of San Juan, the week of year is highly correlated as well, so we'll keep an eye on that. ","c2494010":"If we plot the amount of cases as a function of the week of year, we see that there are outbreaks in both Iquitos and San Juan toward the ends of each year. The increases in cases and outbreaks tend to happen in weeks 35 to 45 in San Juan and weeks 45 to 50 in Iquitos.","8c3497f8":"## Introduction\n\nWe begin by importing all the necessary libraries and the dataset. "}}