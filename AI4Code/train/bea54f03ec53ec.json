{"cell_type":{"4071afd9":"code","1c8b128f":"code","975afaee":"code","3cc0319a":"code","d5689207":"code","770756bf":"code","ce5fa0a2":"code","18666d92":"code","1a88ce89":"code","6ecb5818":"code","1fe5db4c":"code","1116af7a":"code","afef4623":"code","f723d8ed":"code","42a4dbb2":"code","0cf3340c":"code","b679af29":"code","98a30806":"code","c4bf6818":"code","7c5a8edd":"code","659ff86a":"markdown","fede1239":"markdown","83c52c3a":"markdown","d9d5a6ed":"markdown","a6a8b530":"markdown","9dc7bf64":"markdown","9ed88291":"markdown","65e58be1":"markdown","4441e96f":"markdown","e872a05e":"markdown","9ce968b0":"markdown"},"source":{"4071afd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c8b128f":"path = '..\/input\/state-of-the-union-corpus-1989-2017\/sotu\/'\ndirs = os.listdir(path)\n\ndf = pd.DataFrame(columns=['year', 'president', 'text', 'party'])\n\n\nfor i in range(len(dirs)):\n    components = dirs[i].split('_')\n    name = components[0]\n    year = components[1].split('.')[0]\n    df.loc[i,'year'] = year\n    df.loc[i,'president'] = name   \n    \n    filename = os.path.join(path, dirs[i])\n    text_file = open(filename, \"r\")\n    \n    lines = text_file.read()\n    df.loc[i, 'text'] = lines.replace('\\n', ' ')\n    \ndf.year = df.year.astype(int) \ndf.president = df.president.astype(str)\ndf.text = df.text.astype(str)","975afaee":"df.info()","3cc0319a":"# need to distinuish between Theodore Roosevelt and Franklin D. Roosevelt\n#thank you to kaggle user mjmurphy28 for this code\nindices = df.query(\"president =='Roosevelt' & year <= 1909\").index\ndf.loc[indices,'president'] = 'Theodore Roosevelt'\n\nindices = df.query(\"president == 'Roosevelt'\").index\ndf.loc[indices,'president'] = 'Franklin D. Roosevelt'\n\nindices = df.query(\"president =='Bush' & year <= 1992\").index\ndf.loc[indices,'president'] = 'George H. W. Bush'\n\nindices = df.query(\"president == 'Bush'\").index\ndf.loc[indices,'president'] = 'George W. Bush'\n\nindices = df.query(\"president =='Johnson' & year <= 1869\").index\ndf.loc[indices,'president'] = 'Andrew Johnson'\n\nindices = df.query(\"president == 'Johnson'\").index\ndf.loc[indices,'president'] = 'Lyndon B. Johnson'\n\nindices = df.query(\"president =='Adams' & year <= 1801\").index\ndf.loc[indices,'president'] = 'John Adams'\n\nindices = df.query(\"president == 'Adams'\").index\ndf.loc[indices,'president'] = 'John Quincy Adams'\n\nindices = df.query(\"president =='Harrison' & year <= 1841\").index\ndf.loc[indices,'president'] = 'William Henry Harrison'\n\nindices = df.query(\"president == 'Harrison'\").index\ndf.loc[indices,'president'] = 'Benjamin Harrison'\n\n#thank you to kaggle user mjmurphy28 for this code","d5689207":"#add party name to each year\n#thank you to kaggle user mjmurphy28 for this code\ndef pres_to_party(name):\n    republican = ['Lincoln', 'Grant', 'Hayes', 'Garfield', 'Arthur', \n                  'Benjamin Harrison', 'McKinley', 'Theodore Roosevelt', \n                  'Taft', 'Harding', 'Coolidge', 'Hoover', 'Eisenhower', \n                  'Nixon', 'Ford', 'Reagan', 'George H. W. Bush', \n                  'George W. Bush', 'Trump']\n    if name in republican:\n        return 'Republican'\n    \n    democratic = ['Jackson', 'Buren', 'Polk', 'Pierce', \n                  'Buchanan', 'Cleveland', 'Wilson', 'Franklin D. Roosevelt', \n                  'Truman', 'Kennedy', 'Lyndon B. Johnson', 'Carter', 'Clinton', 'Obama']\n    if name in democratic:\n        return 'Democratic'\n    \n    whig = ['William Henry Harrison', 'Taylor', 'Fillmore']\n    if name in whig:\n        return 'Whig'\n    \n    national_union = ['Andrew Johnson']\n    if name in national_union:\n        return 'National Union'\n    \n    unaffiliated = ['Washington', 'Tyler']\n    if name in unaffiliated:\n        return 'Unaffiliated'\n    \n    federalist = ['John Adams']\n    if name in federalist:\n        return 'Federalist'\n    \n    democratic_republican = ['Jefferson', 'Madison', 'Monroe', 'John Quincy Adams']\n    if name in democratic_republican:\n        return 'Democratic-Republican'\n    \ndf.party = df.president.apply(pres_to_party)","770756bf":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download stopwords list\nnltk.download('punkt')\nstop_words = set(stopwords.words('english')) \n\n# Interface lemma tokenizer from nltk with sklearn\n# Thank you to kaggle user mjmurphy28 for this code\nclass LemmaTokenizer:\n    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '--', '-','...', 'american', 'america', 'world']\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n\n# Lemmatize the stop words\ntokenizer=LemmaTokenizer()\ntoken_stop = tokenizer(' '.join(stop_words))\ndocuments = df['text']\n","ce5fa0a2":"from nltk.corpus import wordnet\n\n#Return pos tag in wordnetlemmatizer format\n\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\n        'J': wordnet.ADJ,\n        'N': wordnet.NOUN,\n        'V': wordnet.VERB,\n        'R': wordnet.ADV\n    }\n    return tag_dict.get(tag, wordnet.NOUN)\n\n#Create the lemmatoken\n# Interface lemma tokenizer from nltk with sklearn\nclass POSLemmaTokenizer:\n    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '--', '-', '...', 'american', 'america', 'world']\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t, get_wordnet_pos(t)) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n\n# Lemmatize the stop words\npos_tokenizer=POSLemmaTokenizer()\npos_token_stop=pos_tokenizer(' '.join(stop_words))","18666d92":"# Create TF-idf model\nvectorizer = TfidfVectorizer(stop_words=token_stop, \n                              tokenizer=tokenizer)\n#Fit transform current document\ntfidf_doc = vectorizer.fit_transform(documents)\n\ntfidf_array = tfidf_doc.toarray()\ntfidf_df = pd.DataFrame(tfidf_array, columns = vectorizer.get_feature_names())\ntfidf_df.info()","1a88ce89":"# Create TF-idf model\npos_vectorizer = TfidfVectorizer(stop_words=pos_token_stop, \n                              tokenizer=pos_tokenizer)\n#Fit transform current document\npos_tfidf_doc = pos_vectorizer.fit_transform(documents)\n\npos_tfidf_array = pos_tfidf_doc.toarray()\npos_tfidf_df = pd.DataFrame(pos_tfidf_array, columns = pos_vectorizer.get_feature_names())\npos_tfidf_df.info()","6ecb5818":"df['topic'] = np.arange(0,228)\n\n#Print out the topic-representing words in each year SOTU speech: \nfor i in range(0, 228):\n    topc = []\n    topic_words = tfidf_df.iloc[i, :].sort_values().tail(5).reset_index()\n    for word in topic_words['index']:\n        topc.append(word)\n    df['topic'][i] = topc","1fe5db4c":"df['pos_topic'] = np.arange(0,228)\n\n#Print out the topic-representing words in each year SOTU speech: \nfor i in range(0, 228):\n    topcs = []\n    topic_words = pos_tfidf_df.iloc[i, :].sort_values().tail(5).reset_index()\n    for word in topic_words['index']:\n        topcs.append(word)\n    df['pos_topic'][i] = topcs","1116af7a":"df.drop(['text'],axis=1).to_csv('SOTU topics.csv', index=False)","afef4623":"from pprint import pprint\nfrom gensim.models.ldamulticore import LdaMulticore","f723d8ed":"import re\nfrom gensim import models, corpora\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nNUM_TOPICS = 5\nSTOPWORDS = stopwords.words('english')\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n\ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]\n\ndef clean_text(text):\n    tokenized_text = word_tokenize(text.lower())\n    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n    return lemmatize_sent(' '.join(cleaned_text))","42a4dbb2":"df['tokens'] = df.text.apply(clean_text)\ndf.head()","0cf3340c":"# Build a Dictionary - association word to numeric id\ndictionary = corpora.Dictionary(df.tokens)\n#dictionary.filter_extremes(no_below=3, no_above=.03)\n\n# Transform the collection of texts to a numerical form\ncorpus = [dictionary.doc2bow(text) for text in df.tokens]\n\n# Build the LDA model\nlda_model = models.LdaModel(corpus=corpus,  \n                            num_topics=20, \n                            id2word=dictionary)\n\nprint(\"LDA Model:\")\n \nfor idx in range(20):\n    # Print the first 10 most representative topics\n    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 20))","b679af29":"df['lda_topic'] = df['tokens']\n#def get_most_popular_topic(index)\nfor j in np.arange(0, 228):\n    df['lda_topic'][j] = [i[0] for i in lda_model.get_document_topics(dictionary.doc2bow(df.tokens[j]), minimum_probability=0.2)]","98a30806":"df.head(20)","c4bf6818":"def sotu_topic_finder(year):\n    \"\"\"\n    Find SOTU topics using LDA. The LDA model is only trained on the text of that year topic\n    Input: index i of the speech\n    Output: list 5 topics found by the model\n    \"\"\"\n    # Clean the text\n    sent_text = sent_tokenize(df.text[year - 1979])\n    token_list = []\n    for sent in sent_text:\n        cleaned_sent = clean_text(sent)\n        token_list.append(cleaned_sent)\n\n    # Prepare the dictionary and corpus\n    dictionary = corpora.Dictionary(token_list)\n    corpus = [dictionary.doc2bow(text) for text in token_list]\n\n    # Build the LDA model\n    lda_model = models.LdaModel(corpus=corpus,  \n                                num_topics=5, \n                                id2word=dictionary)\n\n    #Output model\n    print(\"LDA Model of %i:\" % year)\n    for idx in range(5):\n        # Print the first 10 most representative topics\n        print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 5))","7c5a8edd":"for year in range(1990, 1994):\n    sotu_topic_finder(year)","659ff86a":"1.1.1 Lemmatize without POS tagging","fede1239":"# 0. Read","83c52c3a":"2.2 EDA on POS-tagged docs","d9d5a6ed":"1.1.2 Lemmatize with POS tagging","a6a8b530":"# 3. Latent Dirichlet Allocation","9dc7bf64":"1.2.2 TFIDF with POS tagged document","9ed88291":"# 2. Finding Year-specific topic using TFIDF model","65e58be1":"1.2.1 Tfidf on no-POS tagged documents","4441e96f":"# 1. Tokenizer, Lemmatize, Tfidf","e872a05e":"2.1 EDA on no-POS taggged docs","9ce968b0":"3.1 Corpus using no pos-tagged text"}}