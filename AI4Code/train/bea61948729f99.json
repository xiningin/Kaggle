{"cell_type":{"d79c8f8e":"code","fb64e0d4":"code","634fd97f":"code","80224304":"code","f20fa41d":"code","5cbdedf5":"code","a6bf79c1":"code","bad35b2b":"code","be72afdb":"code","03840c3b":"code","fcd14d8d":"code","a7cbd12a":"code","d9a73374":"code","1cc33dc8":"code","7475074d":"code","cf4a1778":"code","49fb8851":"code","216f9231":"code","55bbe6a7":"code","e0260bd7":"code","ddc4c91b":"code","922f934a":"code","ddd2cd7d":"code","1597f98d":"code","ae6ecfdc":"code","19f8fae1":"code","20605bf2":"code","98a0440c":"code","6a6fa4bb":"code","66f5b76b":"code","e71a9f2f":"code","8696f0f9":"code","050288c8":"code","d76f7665":"code","7f14bcc6":"code","cb01118a":"code","24dc63d0":"code","14537bf7":"code","99f8932b":"code","8215562c":"code","c4ae6f49":"code","dc7b3e08":"code","71e23d00":"code","d7d2acd7":"code","834a9a4f":"code","2b49ca33":"code","ba9209e8":"code","b7b9fb7f":"code","281d1b7d":"code","ce42bfa0":"code","4495582b":"code","7ef0b6d5":"code","7bfaadaa":"code","d0d72c4b":"code","a74c90dd":"code","c28e8a00":"code","d4bcb299":"code","1e674f03":"code","3b91da38":"code","4fdb7d52":"code","8cf7482f":"code","59e43790":"code","49e3ae8e":"code","bf20cf5d":"code","bed116d8":"markdown","99d17c96":"markdown","a85513fa":"markdown","648a12c0":"markdown","c1483743":"markdown","065f38d5":"markdown","96b9c2d8":"markdown","ff32599d":"markdown","29024b0f":"markdown","32d15de7":"markdown","2c5dd1a2":"markdown","e1af093b":"markdown","27f8acc1":"markdown","2870e3de":"markdown","5d5bc11b":"markdown","ea45331b":"markdown","01f105e2":"markdown","a9f80274":"markdown","847b75ca":"markdown","429899df":"markdown","977baad5":"markdown","fa56da7f":"markdown","88b3ba74":"markdown","ffe458ed":"markdown","e942e207":"markdown","0be155f7":"markdown","ecd98b14":"markdown","0171e5d7":"markdown","8439ee78":"markdown","b9960ce0":"markdown","f2ffb5dd":"markdown","c14a0862":"markdown","495ac09b":"markdown","53030d91":"markdown","4b9208b0":"markdown","feba699b":"markdown","9b6b1272":"markdown","f31a675d":"markdown","31c03e1f":"markdown","f71ff43d":"markdown","6c76f9e9":"markdown","6a50c716":"markdown","f4be8373":"markdown","25d3213f":"markdown","90f80259":"markdown","ca1a1c7f":"markdown","414119b1":"markdown","cf977e85":"markdown","9dbb50b1":"markdown","0b2348c5":"markdown","c47906a6":"markdown","29de364a":"markdown","76a698fa":"markdown","bcc0579b":"markdown"},"source":{"d79c8f8e":"import numpy as np\nimport pandas as pd\n\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n%matplotlib inline\nrcParams['figure.figsize'] = 10, 8\nsns.set_style('whitegrid')\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn import metrics \nfrom sklearn.metrics import log_loss, classification_report, make_scorer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\n\nimport nltk\nfrom nltk import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer\n\nimport string\n\nfrom datetime import datetime\n\nfrom sklearn.model_selection import GridSearchCV","fb64e0d4":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge. Increase the number of iterations.\")\nwarnings.filterwarnings(\"ignore\", message=\"The max_iter was reached which means the coef_ did not converge\")","634fd97f":"def replacePattern(ps, pattern='url', replace='', regex=True):\n    \n    if type(ps) != pd.core.series.Series:\n        raise ValueError('\"ps\" parameter must be a Pandas Series object type')\n    \n    if pattern == 'url':\n        pattern = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    \n    ps.replace(to_replace = pattern, value = replace, regex = regex, inplace=True)\n    return ps","80224304":"def removePunctuation(post):\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])","f20fa41d":"# Basic preprocessing on the text body to eliminate delimiters and URLs\ndef preProcess(ps):\n    ps = ps.str.lower()\n    ps = replacePattern(ps, 'url', 'url-web') \n    ps = replacePattern(ps, '\\.\\.\\.\\|\\|\\|', ' ') # Longs posts end with \"...|||\"\n    ps = replacePattern(ps, '\\|\\|\\|', ' ') # Posts are separated with \"|||\"\n    return ps","5cbdedf5":"def getData():\n    \n#   Import\n    mbti_train = pd.read_csv('..\/input\/train.csv')\n    mbti_test = pd.read_csv('..\/input\/test.csv')\n    mbti = pd.concat([mbti_train, mbti_test], sort=False)\n    \n#   Basic preprocessing\n    mbti['posts'] = preProcess(mbti['posts'])\n    \n#   Create target categories\n    mbti['mind_i'] = mbti_train['type'].apply(lambda x: 1 if x[0] == 'I' else 0)\n    mbti['energy_s'] = mbti_train['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\n    mbti['nature_t'] = mbti_train['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n    mbti['tactics_j'] = mbti_train['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n\n    return (mbti, mbti_train, mbti_test)","a6bf79c1":"# Returns the number of occurances of a string in a body of text\ndef getStringCount(ps, s):\n    sumCount = 0\n    if type(s) == list:\n        for item in s:\n            sumCount = sumCount + ps.apply(lambda x: len(re.findall(item, x))).sum()\n    else:\n        sumCount = sumCount + ps.apply(lambda x: len(re.findall(s, x))).sum()\n    return sumCount\n\n# Returns a dataframe of word occurances, normalised according to frequency\ndef getCommonWordCloud(mbti, th=100, ngrams=None):\n    \n#   Remove punctuation and get word vector\n    mbti = mbti.copy()\n    posts_no_punct = mbti['posts'].apply(removePunctuation)\n    \n    if ngrams != None:\n        mbti_vect = CountVectorizer(lowercase=True, max_features=None, min_df=th, ngram_range=ngrams)\n    else:\n        mbti_vect = CountVectorizer(lowercase=True, max_features=None, min_df=th)\n        \n    X = mbti_vect.fit_transform(posts_no_punct)\n\n#   Summarise vector for each personality type as new dataframe\n    dfX = pd.DataFrame(X.A)\n    dfX.columns = mbti_vect.get_feature_names()\n\n    df = pd.DataFrame(columns=dfX.columns)\n\n    df = df.append(dfX[list(mbti['mind_i'] == 1)].sum(), ignore_index=True)\n    df = df.append(dfX[list(mbti['mind_i'] == 0)].sum(), ignore_index=True)\n\n    df = df.append(dfX[list(mbti['energy_s'] == 1)].sum(), ignore_index=True)\n    df = df.append(dfX[list(mbti['energy_s'] == 0)].sum(), ignore_index=True)\n\n    df = df.append(dfX[list(mbti['nature_t'] == 1)].sum(), ignore_index=True)\n    df = df.append(dfX[list(mbti['nature_t'] == 0)].sum(), ignore_index=True)\n\n    df = df.append(dfX[list(mbti['tactics_j'] == 1)].sum(), ignore_index=True)\n    df = df.append(dfX[list(mbti['tactics_j'] == 0)].sum(), ignore_index=True)\n\n    df = df.append(dfX.sum(), ignore_index=True)\n\n    df.index = ['i', 'e', 's', 'n', 't', 'f', 'j', 'p', 'total']\n\n#   Adjust each row according to the word count for that personality type\n#   And by total word frequency\n    for k in ['i', 'e', 's', 'n', 't', 'f', 'j', 'p']:\n        df.loc[k] = (df.loc[k] \/ max(1,df.loc[k].sum())) \/ df.loc['total']\n\n#   Normalise rows\n    df.iloc[0:8] = df.iloc[0:8]\/df.iloc[0:8].mean().mean()\n\n#   Calculate the standard deviation of each word\n    df.loc['std_dev'] = df.drop('total').std(axis=0)\n    \n    return df.transpose()","bad35b2b":"mbti_base, mbti_train, mbti_test = getData()\nmbti = mbti_base.copy()","be72afdb":"df = getCommonWordCloud(mbti, th=100)","03840c3b":"df.head()","fcd14d8d":"df.shape","a7cbd12a":"len(df[(df['i'] == 0) | (df['e'] == 0) | (df['s'] == 0) | (df['n'] == 0) | (df['t'] == 0) | (df['f'] == 0) | (df['j'] == 0) | (df['p'] == 0)])","d9a73374":"sns.distplot(list(df['std_dev']))","1cc33dc8":"np.array(df['std_dev']).mean()","7475074d":"df[df['total'] > 2000].shape","cf4a1778":"sns.distplot(list(df['total']))","49fb8851":"sns.distplot(list(df[df['total'] > 2000]['std_dev']))","216f9231":"df[df['total'] > 2000]['std_dev'].mean()","55bbe6a7":"sns.distplot(list(df[df['total'] < 2000]['std_dev']))","e0260bd7":"df[df['total'] < 2000]['std_dev'].mean()","ddc4c91b":"df = df.sort_values(by='std_dev', axis=0, ascending=False)\ndf[['std_dev','total']][0:10]","922f934a":"df = df.sort_values(by='total', axis=0, ascending=False)\ndf[['std_dev','total']][0:10]","ddd2cd7d":"mbti_emoticons = mbti_base.copy()\n\n# Here we define what emoticons we are searching for\nemoticons_list = {\n    'smile': [':\u2011\\)',':\\)',':-\\]',':\\]',':-3',':3',':->',':>','8-\\)','8\\)',':-\\}',':\\}',':o\\)',':c\\)',':\\^\\)','=\\]','=\\)'],\n    'laugh': [':\u2011d',':d','8\u2011d','8d','x\u2011d','xd','x\u2011d','=d','=3','b\\^d'],\n    'unhappy': [':\u2011\\(', ':\\(', ':\u2011c', ':c', ':\u2011< ', ':<', ':\u2011\\[', ':\\[', ':-\\|\\|', ':\\{', ':@'],\n    'surprise': [':\u2011o', ':o', ':-0', '8\u20110'],\n    'wink': [';\\)'],\n}\n\nemoticons_count = {}\n\nfor k,v in emoticons_list.items():\n    emoticons_count[f'emoticon{k}'] = getStringCount(mbti_emoticons['posts'], emoticons_list[k])","1597f98d":"# Graph the frequency of emoticons\nyRange = np.arange(len(emoticons_count))\nplt.barh(yRange, list(emoticons_count.values()))\nplt.yticks(yRange, list(emoticons_count.keys()))\nplt.show()","ae6ecfdc":"def replaceEmoji(ps):\n    ps = ps.apply(lambda x:  re.sub(r\":([A-Za-z]+):\", r'emoji\\1', x))\n    return ps\n\ndef replaceEmoticons(ps, emoticons_list):\n    for k,v in emoticons_list.items():\n        for item in v:\n            ps = replacePattern(ps, item, f'emoticon{k}')\n    return ps","19f8fae1":"mbti_emoticons = mbti_base.copy()\ncorpus = ' '.join(list(mbti_emoticons['posts']))\nemojiSet = set(re.findall(r\":([A-Za-z]+):\", corpus))","20605bf2":"len(list(emojiSet))","98a0440c":"list(emojiSet)[0:8]","6a6fa4bb":"mbti_emoticons['posts'] = replaceEmoji(mbti_emoticons['posts'])\nmbti_emoticons['posts'] = replaceEmoticons(mbti_emoticons['posts'], emoticons_list)\nmbti_emoticons['posts'] = mbti_emoticons['posts'].apply(removePunctuation)\n\ndf = getCommonWordCloud(mbti_emoticons)","66f5b76b":"df.loc[list(emoticons_count.keys())][['total', 'std_dev']]","e71a9f2f":"matchingList = []\nfor i in emojiSet:\n    if 'emoji'+i in list(df.index):\n        matchingList.append('emoji' + i)\n\ndf.loc[matchingList].sort_values(['std_dev'], ascending=False)[['total', 'std_dev']][0:8]","8696f0f9":"def mbti_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words]\n\ndef mbti_lemma(words, lemmatizer):\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef replaceWords(words, vocab_dict):\n    return [vocab_dict.get(word, word) for word in words]\n\ndef reduceUncommonWords(mbti, th=100, uncommon_th=1000):\n    mbti_vect = CountVectorizer(lowercase=True, max_features=None, min_df=th)\n    X_count = mbti_vect.fit_transform(mbti['posts'])\n\n    X = X_count.A\n    X = X.sum(axis=0)\n\n    df_vocab = pd.DataFrame(mbti_vect.vocabulary_, index=['index']).transpose()\n    df_vocab = df_vocab.sort_values(['index'], ascending=True)\n\n    df_vocab['count'] = X\n    df_vocab = df_vocab.reset_index()\n    df_vocab.rename(columns={'level_0': 'word'}, inplace=True)\n    df_vocab.drop(['index'], axis=1, inplace=True)\n\n    stemmer = SnowballStemmer('english')\n    lemmatizer = WordNetLemmatizer()\n\n    # Get low count words\n    s_vocab_low = df_vocab[df_vocab['count'] < uncommon_th]['word'].apply(str)\n    df_vocab_low = pd.DataFrame(s_vocab_low)\n\n    # Only keep english words\n    s_isEnglish = df_vocab_low['word'].apply(lambda x: len(wordnet.synsets(x)) > 0)\n    df_vocab_low = df_vocab_low[s_isEnglish]\n\n    # Get lemmas & stems\n    df_vocab_low['stems'] = mbti_stemmer(mbti_lemma(df_vocab_low['word'], lemmatizer), stemmer)\n\n    df_vocab_conver = df_vocab_low[df_vocab_low['word'] != df_vocab_low['stems']][['word', 'stems']]\n    df_vocab_conver_dict = dict(zip(df_vocab_conver['word'], df_vocab_conver['stems']))\n\n    tokeniser = TreebankWordTokenizer()\n    \n    mbti['tokens'] = mbti['posts'].apply(tokeniser.tokenize)\n    mbti['posts'] = mbti['tokens'].apply(lambda x: ' '.join(x))\n    mbti['tokens_replaced'] = mbti['tokens'].apply(lambda x: replaceWords(x, df_vocab_conver_dict))\n    mbti['post_replaced'] = mbti['tokens_replaced'].apply(lambda x: ' '.join(x))\n    mbti['posts'] = mbti['post_replaced']\n    \n    mbti.drop(['tokens', 'tokens_replaced', 'post_replaced'], axis=1, inplace=True)\n    return mbti","050288c8":"mbti = mbti_base.copy()\nmbti['posts'] = mbti['posts'].apply(removePunctuation)\nmbti_reduced = reduceUncommonWords(mbti, th=10, uncommon_th=100)\n\ndf = getCommonWordCloud(mbti_reduced)\n\ndf['std_dev'].mean()","d76f7665":"mbti_devisive = mbti_base.copy()\nmbti_devisive['posts'] = mbti_devisive['posts'].apply(removePunctuation)\ndf = getCommonWordCloud(mbti_devisive, th=5, ngrams=None)","7f14bcc6":"df.shape","cb01118a":"df['std_dev'].mean()","24dc63d0":"df = df[(df['i'] != 0) & (df['e'] != 0) & (df['s'] != 0) & (df['n'] != 0) & (df['t'] != 0) & (df['f'] != 0) & (df['j'] != 0) & (df['p'] != 0)]","14537bf7":"df['std_dev'].mean()","99f8932b":"devisive_vocab = list(df[df['std_dev'] > 0.15].index)","8215562c":"# Runs for a long while\nmbti_combined = mbti_base.copy()\nmbti_combined['posts'] = replaceEmoji(mbti_emoticons['posts'])\nmbti_combined['posts'] = replaceEmoticons(mbti_combined['posts'], emoticons_list)\nmbti_combined['posts'] = mbti_combined['posts'].apply(removePunctuation)\ndf = getCommonWordCloud(mbti_combined, th=100, ngrams=(1,2))","c4ae6f49":"df = df[(df['i'] != 0) & (df['e'] != 0) & (df['s'] != 0) & (df['n'] != 0) & (df['t'] != 0) & (df['f'] != 0) & (df['j'] != 0) & (df['p'] != 0)]","dc7b3e08":"df = df.sort_values(by='std_dev', axis=0, ascending=False)\ndf[['std_dev','total']][0:10]","71e23d00":"df['std_dev'].mean()","d7d2acd7":"df[df['std_dev'] > 0.1]['std_dev'].mean()","834a9a4f":"combined_vocab = list(df[df['std_dev'] > 0.1].index)\n\nvect = CountVectorizer(lowercase=True, max_features=None, min_df=100, ngram_range=(1,2))\nngramX = vect.fit_transform(mbti_combined['posts'])\n\ndfX = pd.DataFrame(ngramX.A)\n\ndfX.columns = vect.get_feature_names()\n\ndfX_reduced = dfX[combined_vocab]","2b49ca33":"dfX_reduced.shape","ba9209e8":"# Returns CountVectorized X_train and X_test\ndef getX(mbti, splitAt, min_df=100, vocab=None, ngram_range=None):\n    if vocab != None:\n        mbti_vect = CountVectorizer(lowercase=True, vocabulary=vocab)\n    elif ngram_range != None:\n        mbti_vect = CountVectorizer(lowercase=True, max_features=None, min_df=min_df, ngram_range=ngram_range)\n    else:\n        mbti_vect = CountVectorizer(lowercase=True, max_features=None, min_df=min_df)\n        \n    X = mbti_vect.fit_transform(mbti['posts'])\n    X_train = X[:splitAt]\n    X_test = X[splitAt:]\n    return (X_train, X_test)\n\ndef logLossScorer(y_true, y_pred):\n    return log_loss(y_true, y_pred)\n\n# Returns mean cross-validated score\ndef getCrossValScore(X, y, scorer, custom_regressor=None):\n    logreg = LogisticRegression(C=np.inf, solver='lbfgs')\n    \n    if custom_regressor != None:\n        return cross_val_score(custom_regressor, X, y, cv=2, scoring=scorer).mean()\n    return cross_val_score(logreg, X, y, cv=2, scoring=scorer).mean()\n\n# Makes a custom logLoss scorer and return the cross-validated score\ndef getScore(X, targets, custom_regressor=None):\n    scores = []\n    scorer = make_scorer(logLossScorer)\n    for target in targets:\n        scores.append(getCrossValScore(X, target, scorer, custom_regressor))\n    return round(np.array(scores).mean(), 12)\n\n# Creates a regressor then fits and predicts\ndef predictColumn(X, y, X_test, logreg=None):\n    if logreg == None:\n        logreg = LogisticRegression(C=np.inf, solver='lbfgs')\n    logreg.fit(X, y)\n    return logreg.predict_proba(X_test)\n\n# Return the target values as boolean arrays\ndef getTargets(train):\n    y_ie = train['type'].apply(lambda x: 1 if x[0] == 'I' else 0)\n    y_sn = train['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\n    y_tf = train['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n    y_jp = train['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n    return (y_ie, y_sn, y_tf, y_jp)\n\n# A GridSearchCV method for finding the best paramaters for a regressor\ndef tune_model(X, y): \n    parameters = {'C':(0.0001, 0.001, 0.01, 1, 10, 100),\n                 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n    clf = GridSearchCV(LogisticRegression(), parameters, cv=3, iid=True)\n    clf.fit(X,y)\n    return clf","b7b9fb7f":"# Get universal targets\ny_ie, y_sn, y_tf, y_jp = getTargets(mbti_train)","281d1b7d":"mbti_basic = mbti_base.copy()\nmbti_basic['posts'] = mbti_basic['posts'].apply(removePunctuation)\n\nX, X_test = getX(mbti_basic, len(mbti_train), 100)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","ce42bfa0":"X, X_test = getX(mbti_basic, len(mbti_train), 100)\nlogreg = LogisticRegression(C=np.inf, solver='lbfgs', class_weight='balanced')\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp], logreg)\nprint(f'Score {score}')","4495582b":"X, X_test = getX(mbti_basic, len(mbti_train), 5)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","7ef0b6d5":"X, X_test = getX(mbti_basic, len(mbti_train), 100, ngram_range=(1,2))\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","7bfaadaa":"X, X_test = getX(mbti_basic, len(mbti_train), 5, ngram_range=(1,2))\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","d0d72c4b":"mbti_punct = mbti_base.copy()\nX, X_test = getX(mbti_punct, len(mbti_train), 100)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","a74c90dd":"(X, X_test) = getX(mbti_reduced, len(mbti_train), 100)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","c28e8a00":"(X, X_test) = getX(mbti_emoticons, len(mbti_train), 100)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","d4bcb299":"X, X_test = getX(mbti_devisive, len(mbti_train), vocab=devisive_vocab)\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","1e674f03":"X = dfX_reduced[:len(mbti_train)]\nscore = getScore(X, [y_ie, y_sn, y_tf, y_jp])\nprint(f'Score {score}')","3b91da38":"X, X_test = getX(mbti_basic, len(mbti_train), 100)\ny = mbti_train['type']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlogreg = LogisticRegression(C=np.inf, solver='lbfgs', multi_class='auto')\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict_proba(X_test)\ndf_y_pred = pd.DataFrame(y_pred)\n\npred_labels = df_y_pred.apply(lambda x: logreg.classes_[x.idxmax()], axis=1)\n\npred_labels = pred_labels.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n(pred_labels == y_test).mean()*100","4fdb7d52":"# use the best scoring preprocessed text body\nX_test = dfX_reduced[len(mbti_train):]\n\n# Set our targets\ny_ie = mbti_train['type'].apply(lambda x: 0 if x[0] == 'I' else 1)\ny_sn = mbti_train['type'].apply(lambda x: 0 if x[1] == 'S' else 1)\ny_tf = mbti_train['type'].apply(lambda x: 0 if x[2] == 'F' else 1)\ny_jp = mbti_train['type'].apply(lambda x: 0 if x[3] == 'P' else 1)\n\n# Train test split\nX_train_ie, X_test_ie, y_train_ie, y_test_ie = train_test_split(X, y_ie, test_size=0.2, random_state=42)\nX_train_sn, X_test_sn, y_train_sn, y_test_sn = train_test_split(X, y_sn, test_size=0.2, random_state=42)\nX_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X, y_tf, test_size=0.2, random_state=42)\nX_train_jp, X_test_jp, y_train_jp, y_test_jp = train_test_split(X, y_jp, test_size=0.2, random_state=42)","8cf7482f":"# Create the regressors with custom parameters\nlogreg_ie = LogisticRegression(C=0.01, solver='saga')\nlogreg_sn = LogisticRegression(C=1, solver='saga')\nlogreg_tf = LogisticRegression(C=0.001, solver='liblinear')\nlogreg_jp = LogisticRegression(C=0.001, solver='lbfgs')","59e43790":"# Predict the respective target values\npredict_ie = pd.Series(predictColumn(X_train_ie, y_train_ie, X_test_ie, logreg_ie)[:,1])\npredict_sn = pd.Series(predictColumn(X_train_sn, y_train_sn, X_test_sn, logreg_sn)[:,1])\npredict_tf = pd.Series(predictColumn(X_train_tf, y_train_tf, X_test_tf, logreg_tf)[:,1])\npredict_jp = pd.Series(predictColumn(X_train_jp, y_train_jp, X_test_jp, logreg_jp)[:,1])","49e3ae8e":"## Scoring\npredict_ie_r = predict_ie.apply(lambda x: 1 if x > 0.4 else 0)\nscore_ie = logLossScorer(y_test_ie, predict_ie_r)\n\npredict_sn_r = predict_sn.apply(lambda x: 1 if x > 0.6 else 0)\nscore_sn = logLossScorer(y_test_sn, predict_sn_r)\n\npredict_tf_r = predict_tf.apply(lambda x: 1 if x > 0.5 else 0)\nscore_tf = logLossScorer(y_test_tf, predict_tf_r)\n\npredict_jp_r = predict_jp.apply(lambda x: 1 if x > 0.45 else 0)\nscore_jp = logLossScorer(y_test_jp, predict_jp_r)\n\nnp.array([score_ie, score_sn, score_tf, score_jp]).mean()","bf20cf5d":"X = dfX_reduced[:len(mbti_train)]\nX_test = dfX_reduced[len(mbti_train):]\n\n# IE and SN targets are flipped for the submission\ny_ei = mbti_train['type'].apply(lambda x: 0 if x[0] == 'I' else 1)\ny_ns = mbti_train['type'].apply(lambda x: 0 if x[1] == 'S' else 1)\ny_tf = mbti_train['type'].apply(lambda x: 0 if x[2] == 'F' else 1)\ny_jp = mbti_train['type'].apply(lambda x: 0 if x[3] == 'P' else 1)\n\npredict_ie = predictColumn(X, y_ei, X_test, logreg_ie)\npredict_sn = predictColumn(X, y_ns, X_test, logreg_sn)\npredict_tf = predictColumn(X, y_tf, X_test, logreg_tf)\npredict_jp = predictColumn(X, y_jp, X_test, logreg_jp)\n\ndf_pred = pd.DataFrame(data={'id':mbti_test['id'], 'mind': predict_ie[:,1], 'energy': predict_sn[:,1], 'nature': predict_tf[:,1], 'tactics': predict_jp[:,1]})\n\ndf_pred['mind'] = df_pred['mind'].apply(lambda x: 1 if x > 0.4 else 0)\ndf_pred['energy'] = df_pred['energy'].apply(lambda x: 1 if x > 0.6 else 0)\ndf_pred['nature'] = df_pred['nature'].apply(lambda x: 1 if x > 0.5 else 0)\ndf_pred['tactics'] = df_pred['tactics'].apply(lambda x: 1 if x > 0.55 else 0)\n\ndf_pred.to_csv('.\/submission.csv', index=False)","bed116d8":"## Import Data","99d17c96":"Here we see that frequently used words tend to be used by everyone in roughly the same ratio. We could consider these low impact words.","a85513fa":"```\ny_ie: C=0.01,  solver='saga'\ny_sn: C=1,     solver='saga'\ny_tf: C=0.001, solver='liblinear'\ny_jp: C=0.001, solver='lbfgs'\n```","648a12c0":"With the mean pegged at `0.11`, we know that values larger than that indicates influential words.","c1483743":"## Data exploration","065f38d5":"## Preprocessing","96b9c2d8":"# Helper functions","ff32599d":"Looks like it worked. Let's replace all occurances of emoji and emoticons in the text.","29024b0f":"### GridSearchCV auto tuning\n\nHere we use GridSearchCV to search across different solvers and regularization strength to find the best parameters.\n\n\nThe following range is used for GridSearchCV:\n\n```\n{'C':(0.0001, 0.001, 0.01, 1, 10, 100),\n                 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n```\n\nSince these run for quite a while, the output has been saved as text.","32d15de7":"There's a significant amount of usage of emoticons here.","2c5dd1a2":"### Only use devisive words\nHere we only use the vocabulary where each word frequency had a high standard deviation across personality types","e1af093b":"# EDA","27f8acc1":"The first few sections is just boilerplate code. Feel free to skip ahead.","2870e3de":"The above calculation shows that each word is used by each personality type at least once. If it was otherwise we might want to consider removing that word before running our regressor.","5d5bc11b":"First, import the data and save to a base DataFrame","ea45331b":"Each cell is calculated as:\n\nNumber of times the words appears in all posts of that personality type\n\n**1)** Divided by the amount of words used in total by that personality type\n\n**2)** Divided by how many times that word is used in the entire text body\n\n**3)** Divided by the mean value across all cells.\n\n**Step 1.** accounts for cases where some personality types have more posts or have larger text bodies.\n\n**Step 2.** accounts common use words versus low use ones. We're only interested in the relative difference for a word use across two personality types.\n\n**Step 3.** shifts the mean value to be around 1 for better reference.","01f105e2":"### Baseline\nHere we establish a baseline score, with no significant processing or adjustments.","a9f80274":"### bi-Grams\nHere we introduce bi-grams to capture meaningful expressions in the text","847b75ca":"## Import Libraries","429899df":"### Convert emoticons and emoji to text\nHere we preserve emoticons & emoji as text before removing punctuation","977baad5":"### Stemmed and Lemmed uncommon words\nWhat if uncommon words (frequency between 10 and 100) are changed to their root word before using the CountVectorizer?","fa56da7f":"Understandably low impact words are stop words.","88b3ba74":"# Logistic Regression on the MBTI dataset","ffe458ed":"### Balanced classes\nHere we introduce balanced classes, to account for the I vs E, S vs N, and so on, imbalance.","e942e207":"## Only use devisive keywords\nThe logistic regressor allows us to provide a vocabulary instead of building one up itself. We can pick words with high standard deviation values to help guide the regressor.","0be155f7":"### Do not remove punctuation\nWhat if there is hidden meaning in the punctuation used in the text?","ecd98b14":"## Combined method\nHere we combine a number of preprocessing methods, namely: nGrams, emoticons and devisive words.","0171e5d7":"## Emoticons and Emoji\nLet's see what the presence of emoticons & emoji are in the text","8439ee78":"### Use a multiclass logistic regressor\nWhat if we attempt to classify each compound personality type separately?","b9960ce0":"We suppress some warnings for readability","f2ffb5dd":"Looks like emoticons and emoji can have a significant impact factor.","c14a0862":"### Increase vocabulary size\nHere we decrease the minimum word frequency size to 5 to capture a much larger dictionary, and thus word matrix.","495ac09b":"The mean standard devation is slightly higher than the original body of text, so it might perform better during regression.","53030d91":"Let's look at the emoticons:","4b9208b0":"Then we execute the `getCommonWordCloud` function on the text body. This function creates a matrix of word frequencies, summarises the frequencies according to the personalities and then normalises it.","feba699b":"Words that tend to be favoured by some personalities more than others appear to be MBTI labels themselves. Probably due to this data being taken from a social media site that focused on MBTI.","9b6b1272":"## Reduce uncommon words to stems\nThe following code finds words between two frequency thresholds and finds their lemmas and stems. It only consider words recognised in the english vocabulary. It then maps this back to the original body of text and maps those words.","f31a675d":"# Predict","31c03e1f":"### Increase vocabulary size and use bi-Grams\nThe combination of the two","f71ff43d":"# Predicting","6c76f9e9":"And at the emoji:","6a50c716":"## Testing\nWe will run through different model settings and preprocessing methods to find the best possible solution.\n\nIn most cases we use cross validation with only 2 folds in interest of performance.","f4be8373":"We can see that most words are used rather seldom, with others having a very frequent usage.","25d3213f":"## Fine tuning\nHere we attempt to find the best possible classifications by using the optimal regressor parameters and adjust where the we separate the classifications between personality types.","90f80259":"The above score is the percentage of correct estimations, which is very poor at only 61%","ca1a1c7f":"Here we see the distribution of the standard deviation of word frequencies across personality types. Most are around the 0~2 range with a tail running to the right.","414119b1":"Still a promising score. Let's save all words that have a score over `0.15`","cf977e85":"### Only use devisive words with bi-grams","9dbb50b1":"An improved score over our best. We'll use these parameters for the submission.","0b2348c5":"We captured 4794 words that appear more than 100 times","c47906a6":"A good score, but let's ensure that all words are used by each personality type is used at least once.","29de364a":"Let's find emoji:","76a698fa":"Less commonly used words tend to follow the general trend.","bcc0579b":"## Function definitions"}}