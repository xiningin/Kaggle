{"cell_type":{"38446042":"code","1bcdaa73":"code","6b13978c":"code","cd287ddf":"code","0de1c578":"code","8b992918":"code","00b976e4":"code","b986a097":"markdown","074b61f5":"markdown","68b9bf3e":"markdown","01e69ed8":"markdown","4c934357":"markdown","ca572e10":"markdown","eab141c9":"markdown","db674956":"markdown","d7932066":"markdown","7f7aff5c":"markdown","40629f4f":"markdown","18ce8b4c":"markdown"},"source":{"38446042":"# Importing required Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n%matplotlib inline","1bcdaa73":"data_df = pd.read_csv('\/kaggle\/input\/diabetes-dataset\/diabetes.csv')\ndata_df.head()","6b13978c":"y = data_df[\"Outcome\"].values\nx = data_df.drop([\"Outcome\"],axis=1)","cd287ddf":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndata_df = ss.fit_transform(data_df)\n\n#Divide into training and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) # 70% training and 30% test","0de1c578":"train_score = []\ntest_score = []\nk_vals = []\n\nfor k in range(1, 51):\n    k_vals.append(k)\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    \n    tr_score = knn.score(X_train, y_train)\n    train_score.append(tr_score)\n    \n    te_score = knn.score(X_test, y_test)\n    test_score.append(te_score)","8b992918":"plt.figure(figsize=(10,5))\nplt.xlabel('Different Values of K')\nplt.ylabel('Model score')\nplt.plot(k_vals, train_score, color = 'r', label = \"training score\")\nplt.plot(k_vals, test_score, color = 'b', label = 'test score')\nplt.legend(bbox_to_anchor=(1, 1),\n           bbox_transform=plt.gcf().transFigure)\nplt.show()","00b976e4":"\nknn = KNeighborsClassifier(n_neighbors = 30)\n\n#Fit the model\nknn.fit(X_train,y_train)\n\n#get the score\nknn.score(X_test,y_test)","b986a097":"![image.png](attachment:image.png)","074b61f5":"# Every machine learning algorithm has three prediction error which can be broken down into three parts:\n\n1. Bias Error\n1. Variance Error\n1. Irreducible Error","68b9bf3e":"# What is Variance?\n\n**Variance is the amount that the estimate of the target function will change if different training data was used.\n\n**the model predicts very complex relationships between the outcome and the input features when a quadratic equation would have sufficed. This is how a classification model would look like when there is a high variance error\/when there is overfitting:**\n\n![image.png](attachment:image.png)\n\n\nLow Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\nHigh Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n\n\n**Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.**\n\n**Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.**\n\n\n# To summarise,\n\n# A model with a high bias error underfits data and makes very simplistic assumptions on it\n# A model with a high variance error overfits the data and learns too much from it.\n# A good model is where both Bias and Variance errors are balanced.\n\n\n\n**To achieve a balance between the Bias error and the Variance error, we need a value of k such that the model neither learns from the noise (overfit on data) nor makes sweeping assumptions on the data(underfit on data). To keep it simpler, a balanced model would look like this:\n\n\n\n\n","01e69ed8":"**we will use the simplest K-nearest neighbor classifier(Knn) to classify whether the patient has diabetes or not.\n\nHowever, how do we decide the value of \u2018k\u2019?\n\n**Let us take a few possible values of k and fit the model on the training data for all those values. We will also compute the training score and testing score for all those values.**","4c934357":"![image.png](attachment:image.png)","ca572e10":"**In the simplest terms, Bias is the difference between the Predicted Value and the Expected Value. To explain further, the model makes certain assumptions when it trains on the data provided. When it is introduced to the testing\/validation data, these assumptions may not always be correct.**\n\n![image.png](attachment:image.png)\n\n\n#  when there is a high bias error, it results in a very simplistic model that does not consider the variations very well. Since it does not learn the training data very well, it is called Underfitting.\n\n* Low Bias: Suggests less assumptions about the form of the target function.\n* High-Bias: Suggests more assumptions about the form of the target function.\n\n**Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.**\n\n**Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression**\n","eab141c9":"**The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.**","db674956":"![image.png](attachment:image.png)","d7932066":"#  we will focus on the two parts\n# # The bias error and the variance error.","7f7aff5c":"# Thanks.....!!!!","40629f4f":"# So What is Bias?","18ce8b4c":"* For low values of k, the training score is high, while the testing score is low\n* As the value of k increases, the testing score starts to increase and the training score starts to decrease.\n* However, at some value of k, both the training score and the testing score are close to each other."}}