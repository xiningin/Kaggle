{"cell_type":{"d917d3f9":"code","09a1e26d":"code","0b954a89":"code","9146378f":"code","b3c4eaa6":"code","c1853996":"code","15bde409":"code","649debdf":"code","d41c6dd4":"code","fb301a07":"code","97783bcf":"code","f50c27f0":"code","db8cb531":"code","6b7490eb":"code","405c10c9":"code","c333bb3a":"code","0983fecb":"code","9e751140":"code","022ffe67":"code","7afb1192":"code","54e3df5b":"code","eb7df771":"code","f83b7a8b":"code","8720fd93":"code","561ff91f":"code","0dc09ad8":"code","1c2d2949":"code","853e4bb0":"code","4cde61eb":"code","bc73f8eb":"code","0196a60b":"code","0b16d931":"code","043a141e":"code","fafb1a76":"code","4523c704":"code","a6610a18":"code","a5b35ff3":"code","c90366a0":"code","2acb1176":"code","fb7e4206":"code","abe1c77a":"code","ee8517b8":"code","36d15132":"code","0f4c8762":"code","0694d0ed":"code","c39d37f4":"code","c64981f2":"code","7b3e3936":"code","5a29dbd7":"code","c6b86935":"code","bccd2cab":"code","1dfc1956":"code","8d87f8cb":"code","e6c3806d":"code","42e29c69":"code","e8a85944":"code","961b4382":"code","926b0e57":"code","960092c2":"markdown","ced0faa5":"markdown","cc3ac471":"markdown","f1ae787c":"markdown","e858e925":"markdown","79fb47c7":"markdown","fa569256":"markdown","dde9ace1":"markdown","0226f47b":"markdown","3114eb63":"markdown","1a724071":"markdown","ffd3091b":"markdown","49df8701":"markdown","d34711ce":"markdown","e3ed0366":"markdown","f92d9bd7":"markdown","8e165f3c":"markdown","b13008c5":"markdown","a276fb9c":"markdown","0279583c":"markdown","2f0a1690":"markdown","13b605ec":"markdown","7cdf1ed9":"markdown","9835c23e":"markdown","5cefb57a":"markdown","085b3a75":"markdown","21f6f7bc":"markdown","24d3bc40":"markdown","a4a31258":"markdown","631eccff":"markdown","10f551ed":"markdown","ca456a07":"markdown"},"source":{"d917d3f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09a1e26d":"try:\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\nprint(tf.__version__)","0b954a89":"from numpy import array\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\nfrom tensorflow.compat.v1.keras.backend import set_session\nimport keras\nimport sys, time, os, warnings \nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport numpy as np\nimport pandas as pd \nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.utils import shuffle\nfrom keras.applications.xception import Xception, preprocess_input\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n# small library for seeing the progress of loops.\nfrom tqdm.notebook import tqdm\ntqdm().pandas()","9146378f":"from os import listdir\n## The location of the Flickr8K_ photos\nimage_dir = '..\/input\/flickr8k\/images'\nimages = listdir(image_dir)\n\n## The location of the caption file\ndescriptions_dir = '..\/input\/flickr8k\/captions.txt'\n\nprint(\"The number of jpg flies in Flicker8k: {}\".format(len(images)))","b3c4eaa6":"import string \n# loading the file and reading a content as a string\ndef load_document(filename):\n    file=open(filename,'r') #open file in read only mode\n    text=file.read()        #reading the file content\n    file.close()\n    return text           # return the file content as a string\n\n# to remove punctuation\ndef remove_punctuation(text_orignal):\n    cleaned_punctuation=text_orignal.translate(string.punctuation) # string.punctuation contain all set of strings\n    return (cleaned_punctuation)\n\n# to remove single characters\ndef remove_single_character(text):\n    text_len_more_then1=\"\";\n    for word in text.split():\n        if len(word)>1:\n            text_len_more_then1+=\" \"+ word\n    return (text_len_more_then1)\n\n#TO remove numeric values\ndef remove_numeric(text,printTF=False):\n    text_without_numeric=\"\"\n    for word in text.split():\n        isalpha=word.isalpha()  # if word is a alphabet\n        if printTF:\n            print(\"    {:10} : {:}\".format(word,isalpha))\n        if isalpha:\n            text_without_numeric+=\" \"+ word\n    return (text_without_numeric)\n            \n\n","c1853996":"\n## read in the Flickr caption data\ntext = load_document(descriptions_dir)\n#len of string\nprint(len(text))\nprint(text[:325])","15bde409":"text[39:230]","649debdf":"# remove first line\n\ntext=text.lower()\nprint(text[:325])","d41c6dd4":"text=text[39:]\n\ndef make_dataset(text):\n    df=[]\n    splited=[]\n    for sentence in text.split('\\n'):\n        split=sentence.split('|')\n    \n        df.append(split)\n    return df\n","fb301a07":"df=make_dataset(text)","97783bcf":"df[:10]","f50c27f0":"len(df)\n    ","db8cb531":"data = pd.DataFrame(df,columns=[\"filename\",\"index\",\"caption\"])\n# Reordering columns for better readability\ndata = data.reindex(columns =['index','filename','caption'])\n","6b7490eb":"x=data['index']>='5'\nx\n    \n    ","405c10c9":"print(x[:][1]==False)","c333bb3a":"# 2nd last index is 5 correct it Nd remove last row\ndata.drop(data.tail(1).index,inplace=True) # drop last 1 rows","0983fecb":"data","9e751140":"#corrected last column\ndata.tail(1)['index']='4'","022ffe67":"data","7afb1192":"# If any filename dosn't have .jpg extension at last then mark it as Invalid filename\ndef invalid_filename_check(data):\n  for filenames in data[\"filename\"]:\n    found = re.search(\"(.(jpg)$)\", filenames)\n    if (found):\n        pass\n    else:\n        print(\"Error file: {}\".format(filenames))","54e3df5b":"data.filename.values        ","eb7df771":"# Exploring the data frame\ndef utility_counter(data):\n    unique_filenames=np.unique(data.filename.values)\n    print(\"The number of unique file names : {}\".format(len(unique_filenames)))\n\n    ct_dict = Counter(data.filename.values)\n    print(\"We can see that all the keys are having values count = 5\")\n    # print(ct_dict)\n    \n    print(\"The distribution of the number of captions for each image:\")\n    ct = Counter(Counter(data.filename.values).values())\n    print(ct)\n    return unique_filenames","f83b7a8b":"unique_filenames = utility_counter(data)","8720fd93":"# sample images and caption\ndef sample_image_desc(data):\n    no_of_img=5\n    no_pix=224\n    target_img_size=(no_pix,no_pix,3)\n    \n    cnt=1\n    fig=plt.figure(figsize=(10,20))\n    for imgpath in unique_filenames[31:36]:\n        filename=image_dir+'\/'+imgpath\n        captions=list(data[\"caption\"].loc[data[\"filename\"]==imgpath].values)\n        image_load=load_img(filename,target_size=target_img_size)\n        \n        ax = fig.add_subplot(no_of_img,2,cnt,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        cnt += 1\n        \n        ax=fig.add_subplot(no_of_img,2,cnt)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(captions))\n        for i,caption in enumerate(captions):\n                ax.text(0,i,caption,fontsize=20)\n        cnt+=1\n    plt.show()\n        ","561ff91f":"sample_image_desc(data)","0dc09ad8":"def create_vocab(data):\n    vocab=[]\n    for captions in data.caption.values:\n        vocab.extend(captions.split())  #extend the one list in other list\n    print(\"Vocabulary Size : {}\".format(len(set(vocab))))\n    return vocab\n        ","1c2d2949":"vocabulary=create_vocab(data)","853e4bb0":"def df_word_count(data,vocabulary):\n    ct = Counter(vocabulary)  # Dict subclass for counting hashable items.  Sometimes called a bag or multiset.\n    appen_1 = []\n    appen_2 = []\n    for i in ct.keys():\n        appen_1.append(i)\n    for j in ct.values():\n        appen_2.append(j)\n    data = {\"word\":appen_1 , \"count\":appen_2}\n    dfword = pd.DataFrame(data)\n    dfword = dfword.sort_values(by='count', ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)","4cde61eb":"#vocabulary o funique words\ndfwordcount=df_word_count(data,vocabulary)","bc73f8eb":"# dfwordcount[:10]  both are same \ndfwordcount.iloc[:10,:]\n","0196a60b":"def text_clean(text_orignal):\n    text=remove_punctuation(text_orignal)\n    text=remove_single_character(text)\n    text=remove_numeric(text)\n    return (text)\n\nfor i ,caption in enumerate(data.caption.values):\n    newcaption=text_clean(caption)\n    data['caption'][i]=newcaption","0b16d931":"clean_vocabulary = create_vocab(data)      ","043a141e":"data","fafb1a76":"def path_of_each_image(data):\n    all_img_path=[]\n    \n    for filenames in data[\"filename\"]:\n        full_image_path=image_dir+'\/'+filenames\n        all_img_path.append(full_image_path)\n    return all_img_path\n\nall_img_path=path_of_each_image(data)\nall_img_path[:10]","4523c704":"# Adding the <start> and  <end> token in each caption\ndef adding_token(data):\n    total_caption=[]\n    \n    for caption in data[\"caption\"].astype(str):\n        total_caption.append('<start>'+caption+ '<end>')\n    return total_caption\n\ntotal_caption= adding_token(data)\ntotal_caption[:10]","a6610a18":"### viewing some of the example\nten_images=all_img_path[:50]    # Each image repeats 5 times in dataset\nunique_img=np.unique(ten_images)\nprint(unique_img)","a5b35ff3":"from matplotlib.pyplot import figure, imshow, axis\nfrom matplotlib.image import imread\n\ndef showImagesHorizontally(ten_images):\n    fig=figure(figsize=(10,200))\n    number_of_files=len(ten_images)\n    for i in range(number_of_files):\n        a=fig.add_subplot(1,number_of_files,i+1)\n        image=imread(ten_images[i])\n        imshow(image)\n        axis('off')\n        ","c90366a0":"showImagesHorizontally(unique_img)","2acb1176":"print(\"Total Images : \" + str(len(all_img_path)))\nprint(\"Total Captions : \" + str(len(total_caption)))","fb7e4206":"def data_limiter(num,total_captions,all_img_path):\n    # Shuffle captions and image_names together\n    train_captions, img_name_vector = shuffle(total_captions,all_img_path,random_state=1)\n    train_captions = train_captions[:num]\n    img_name_vector = img_name_vector[:num]\n    return train_captions,img_name_vector\n    ","abe1c77a":"train_captions,img_name_vector = data_limiter(40000,total_caption,all_img_path)","ee8517b8":"print(\"Total Captions = {0} , Total images = {1}\".format(len(train_captions),len(img_name_vector)))","36d15132":"from pickle import dump\n\n# save to file\ndump(train_captions, open('captions.pkl', 'wb'))\ndump(img_name_vector, open('img_names.pkl', 'wb'))","0f4c8762":"train_captions = load(open('captions.pkl', 'rb'))\nimg_name_vector = load(open('img_names.pkl', 'rb'))","0694d0ed":"print(train_captions[:10])\nprint(img_name_vector[:10])","c39d37f4":"# all of the given images have different shape\n\n# To know the shape of images\ndef img_shape_finder(image):\n  img= plt.imread(image)\n\n  print(\"Shape of the image ==> {0} is ==> {1}\".format(image.split('\/')[1],img.shape))","c64981f2":"img_list=[]\nfor i in range(10):\n  img_list.append(img_name_vector[i])","7b3e3936":"for j in img_list:\n  img_shape_finder(j)","5a29dbd7":"# function which shows the sample image and its shape\ndef sample_img_shape(image_path):\n    img=plt.imread(image_path)    #read the image\n    plt.imshow(img)\n    print(\"Shape of the image:\",img.shape)\n    \nsample_img_shape( '..\/input\/flickr8k\/images\/1003163366_44323f5815.jpg'\n)","c6b86935":"import imageio\ndef image_flipper(image):\n    original_img=imageio.imread(image)\n    \n    plt.figure(1)\n    \n# Orignal image\n\n    plt.subplot(221)\n    plt.imshow(original_img)\n    \n#left-right flip Image\n\n    flipped_img_tensor=tf.image.flip_left_right(original_img)\n    flipped_img=flipped_img_tensor.numpy()\n    plt.subplot(222)\n    plt.imshow(flipped_img)\n    \n#Up-Down flip Image\n\n    upside_down_flip_tensor = tf.image.flip_up_down(original_img)\n    upside_down_flip= upside_down_flip_tensor.numpy()\n    plt.subplot(223)\n    plt.imshow(upside_down_flip)\n\n#Gray scale Image\n\n    gray_tensor = tf.image.rgb_to_grayscale(original_img)\n    grayimg= gray_tensor.numpy()\n    plt.subplot(224)\n    plt.imshow(tf.squeeze(grayimg))\n\n  #plt.imsave('doggo.jpg',flipped_img)\n\nimage_flipper('..\/input\/flickr8k\/images\/1001773457_577c3a7d70.jpg')\n    ","bccd2cab":"def resize_img(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (229, 229))\n    img = preprocess_input(img) \n    return img, image_path\n    \n    \nimg1,img1_path=resize_img('..\/input\/flickr8k\/images\/1001773457_577c3a7d70.jpg')\nprint(\"Shape after resize :\", img1.shape)\nplt.imshow(img1)","1dfc1956":"#Pre-trained Xception Model\nXceptionModel=Xception(include_top=False,pooling='avg')\n\nnew_input=XceptionModel.input   ## Any arbitrary shapes with 3 channels\nhidden_layer=XceptionModel.layers[-1].output\n\nimage_features_extract_model=Model(new_input,hidden_layer)  #`Model` groups layers into an object with training and inference features.\n","8d87f8cb":"image_features_extract_model.summary()","e6c3806d":"# Get unique images\nencode_train=sorted(set(img_name_vector))  #Return a new list containing all items from the iterable in ascending order.\nprint(encode_train[:10])\nprint('length of encode_train',len(encode_train))\nprint(type(encode_train[0]))","42e29c69":"#Creates a `Dataset` whose elements are slices of the given tensors.\n\n# The given tensors are sliced along their first dimension. This operation\n# preserves the structure of the input tensors, removing the first dimension\n# of each tensor and using it as the dataset dimension. All input tensors\n# must have the same size in their first dimensions.\nimage_dataset=tf.data.Dataset.from_tensor_slices(encode_train)\nprint(type(image_dataset))\n\n# for files in image_dataset: \n#     print(files.numpy())","e8a85944":"# Maps `map_func` across the elements of this dataset.\n\n# This transformation applies `map_func` to each element of this dataset, and\n# returns a new dataset containing the transformed elements, in the same\n# order as they appeared in the input. `map_func` can be used to change both\n# the values and the structure of a dataset's elements. For example, adding 1\n# to each element, or projecting a subset of element components.\n\n# resizing all images and making  a batch of 64\nimage_dataset=image_dataset.map(resize_img,num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","961b4382":"# contain images and path of image resize to (229,229,3)\nprint(image_dataset)\nprint('length of image_dataset:',len(image_dataset))","926b0e57":"for img,path in tqdm(image_dataset):\n    batch_features=image_features_extract_model(img)  #extracting the batch feature of image\n    print(batch_features.shape)\n#     batch_features=tf.reshape(batch_features,(batch_features.shape[0],-1,batch_features.shape[3]))\n#     print(batch_features.shape)\n    for bf,p in zip(batch_features,path):\n        print(p)\n        path_of_feature=p.numpy().decode(\"utf-8\")\n        print(path_of_feature)\n        print(bf.numpy().shape)\n        dump( , open(\"features.p\",\"wb\"))      # write in binary mode\n#         np.save(path_of_feature,bf.numpy())\n    \n    ","960092c2":"# AUDIO-DESCRIPTION-OF-IMAGE-FOR-VISUALLY-IMAPIRED","ced0faa5":"![image.png](attachment:9f81dffa-e103-4b0d-b79f-434df3478c43.png)","cc3ac471":"# Cleaning the caption","f1ae787c":"* Here we are setting the path for each image so that we can load the images at once using the path set","e858e925":"Now , we will use Transfer Learning for determining the feature Vector from Pre-trained Model\n\nWe are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.\n\nXception Model is proposed by Francois Chollet. Xception is an extension of the inception Architecture which replaces the standard Inception modules with depthwise Separable Convolutions.","79fb47c7":"   # Map each image name to the function to load the image","fa569256":"* An NPY file is a NumPy array file created by the Python software package with the NumPy library installed. It contains an array saved in the NumPy (NPY) file format. NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information.","dde9ace1":"# Preprocessing the images\u00b6","0226f47b":"## Concept of Attention Mechanism\u00b6","3114eb63":"Normally, the CNN's last layer is the softmax layer, which assigns the probability that each object might be in the image. But, if we remove that softmax layer from CNN, we can feed the CNN's rich encoding of the image into the DECODER(language generation of RNN) designed to produce phrases. We can then train the whole system directly on images and their captions, so it maximizes the likelihood that the descriptions it produces best match the training descriptions for each image","1a724071":"# Exploratory Data Analysis","ffd3091b":"# Pre-Trained Image Model (Xception Model)","49df8701":"# Resizing the images","d34711ce":"* The most common words are articles such as \"a\", or \"the\", or punctuations.\n* These words do not have much infomation about the data.\n* Using the utility functions (define in above cells) to clean the captions and restoring the cleaned captions to the dataset","e3ed0366":"Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it \u201cserializes\u201d the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script.","f92d9bd7":"## captions","8e165f3c":"# After this step all the images have been resized to (229,229,3)","b13008c5":"# Some Data cleaning :\n - All the filenames should end with '.jpg' extension","a276fb9c":"# Data Acquisition","0279583c":"# Features extraction from Xception","2f0a1690":"Let's start the Project\n\nThe Idea Behind the Project:\nImage ---> Caption ---> Audio\n\nNow, For image-captioning we will use the Encoder-Decoder network(CNN-LSTM) with attention model to generate the caption of the given image then we will convert the caption to AUDIO.\n`Attention-based model`, help us to see what parts of the image the model focuses on as it generates a caption.","13b605ec":"But for the purpose of this case study, I have used the Flickr 8k dataset which you can download from here. Also training a model with large number of images may not be feasible on a system which is not a very high end PC\/Laptop.\nThis dataset contains 8000 images each with 5 captions (as we have already seen in the Introduction section that an image can have multiple captions, all being relevant simultaneously).\nThese images are bifurcated as follows: Training Set \u2014 6000 images Dev Set \u2014 1000 images Test Set \u2014 1000 images","7cdf1ed9":"## Need to bring every image of shape (229,229,3) as we have used Xception Model","9835c23e":"# Image Augmentation","5cefb57a":"not found","085b3a75":"* With an Attention mechanism, the image is first divided into n parts, and we compute with a Convolutional Neural Network (CNN) representations of each part h1,\u2026, hn. When the RNN is generating a new word, the attention mechanism is focusing on the relevant part of the image, so the decoder only uses specific parts of the image.\n-We can recognize the figure of the \u201cclassic\u201d model for image captioning, but with a new layer of attention model. What is happening when we want to predict the new word of the caption? If we have predicted i words, the hidden state of the LSTM is hi. We select the \u00ab relevant \u00bb part of the image by using hi as the context. Then, the output of the attention model zi, which is the representation of the image filtered such that only the relevant parts of the image remains, is used as an input for the LSTM. Then, the LSTM predicts a new word and returns a new hidden state hi+1.","21f6f7bc":"# Functions to load and clean data","24d3bc40":"# Making a dataframe out of raw text","a4a31258":"# Loading all the Dependencies","631eccff":"## Save variables in a pickle file and restore them to use it again","10f551ed":"## Taking only 40,000 images and captions\nSo that we can select batch size properly i.e. 625 batches if batch_size=64","ca456a07":"# Preprocessing"}}