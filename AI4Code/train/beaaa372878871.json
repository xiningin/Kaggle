{"cell_type":{"6d064bb7":"code","be7cb392":"code","8b16a9f3":"code","4795c271":"code","3536cb9e":"code","48127398":"code","5e51d4fa":"code","a409aa3b":"code","ba80e5ec":"code","af73e6cd":"code","5ad7c716":"code","ed9968a6":"code","887ebad2":"code","3c6c1e88":"code","2387817d":"code","57bb8fb3":"code","f825d244":"code","1fcc06f7":"code","cc09a2fc":"code","a3223db3":"code","fb0d33df":"code","7d3e2538":"code","8e09734d":"code","4a3cfcef":"code","b95c79ee":"code","9b3ce257":"code","a0ff43b2":"code","458a44ff":"code","1543c8e5":"code","c291d320":"code","14b0359a":"code","4051b57d":"code","e9309110":"code","9b397a8f":"code","38e9580e":"code","e2e29ee2":"code","e727f0fe":"code","21bc4881":"code","4959f2ef":"code","4ea4f508":"code","36de6b76":"code","d6cd2557":"code","526d02c7":"code","54b95124":"code","a8357895":"code","b8ab095b":"code","6612d018":"code","94358190":"code","0426c234":"code","28a29a75":"code","92a6c564":"code","c3db66d2":"code","9926f45f":"code","5f269595":"markdown","439441e7":"markdown","1df8b262":"markdown","79e54cf3":"markdown","44c465a5":"markdown","3460b316":"markdown","7e1ea1a9":"markdown","fb063313":"markdown","5f087226":"markdown","ca090569":"markdown","dd38f4ae":"markdown","18623a38":"markdown","6a90b619":"markdown","4168cd81":"markdown","a993c80d":"markdown","f69138d8":"markdown","fa8b797b":"markdown","582cc1aa":"markdown"},"source":{"6d064bb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be7cb392":"df = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/5802.csv\", low_memory=False)\nprint(df.shape)\ndf.head()","8b16a9f3":"df.isnull().sum()","4795c271":"#Handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","3536cb9e":"df[numerical_nan].isna().sum()","48127398":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","5e51d4fa":"#https:\/\/stackoverflow.com\/questions\/62759022\/typeerror-invalid-type-promotion-while-fitting-a-logistic-regression-model-in-s\n#Answered by Sal_H\n\ndf['time'] = pd.to_datetime(df['time'],infer_datetime_format=True)","a409aa3b":"#https:\/\/stackoverflow.com\/questions\/62759022\/typeerror-invalid-type-promotion-while-fitting-a-logistic-regression-model-in-s\n#Answered by Sal_H\n\ndf['time']=df['time'].apply(lambda x: x.toordinal())","ba80e5ec":"x=df.copy()\nx.drop('engagement_index',axis=1,inplace=True)\ny=df['engagement_index']","af73e6cd":"cat=[i for i in x.columns if x[i].dtypes=='O']","5ad7c716":"from sklearn.preprocessing import OrdinalEncoder,LabelEncoder\nencode=OrdinalEncoder()\nlabe=LabelEncoder()","ed9968a6":"#Code by Ayush Yadav https:\/\/www.kaggle.com\/smokingkrils\/indian-e-commerce-analysis-and-customer-retention\/notebook\n\n#using ordinal encoder for independent features\nfor i in cat:\n    x[i]=encode.fit_transform(x[i].values.reshape(-1,1))\n\n#Using label encoder for Label Column\ny=labe.fit_transform(y)","887ebad2":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","3c6c1e88":"xd=scaler.fit_transform(x)\nx=pd.DataFrame(xd,columns=x.columns)","2387817d":"from sklearn.ensemble import RandomForestRegressor\nm=RandomForestRegressor()\nm.fit(x,y)","57bb8fb3":"#Code by Ayush Yadav https:\/\/www.kaggle.com\/smokingkrils\/indian-e-commerce-analysis-and-customer-retention\/notebook\n\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(m.feature_importances_, index=x.columns)\nplt.figure(figsize=(10,8))\nfeat_importances.nlargest(10).plot(kind='barh', color = 'green')\nplt.show()","f825d244":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","1fcc06f7":"selection = SelectKBest(score_func=chi2, k='all')\nfit = selection.fit(x,y)","cc09a2fc":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns","a3223db3":"print(featureScores.nlargest(10,'Score'))  #print10??? best features I have only 4!\nfeat=list(featureScores.nlargest(10,'Score')['Features'])","fb0d33df":"#Code by Ayush Yadav https:\/\/www.kaggle.com\/smokingkrils\/indian-e-commerce-analysis-and-customer-retention\/notebook\n\nfrom sklearn.decomposition import PCA\npca = PCA().fit(x)","7d3e2538":"#Code by Ayush Yadav https:\/\/www.kaggle.com\/smokingkrils\/indian-e-commerce-analysis-and-customer-retention\/notebook\n\nfig, ax = plt.subplots(figsize=(20,10))\nxi = np.arange(1, 4, step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 3, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=1, color='r', linestyle='-')\nplt.text(0.5, 0.85, '100% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","8e09734d":"#Code by Ayush Yadav https:\/\/www.kaggle.com\/smokingkrils\/indian-e-commerce-analysis-and-customer-retention\/notebook\n\npca=PCA(n_components=3)\nx=pca.fit_transform(x)\nx=pd.DataFrame(x)\nx.head()","4a3cfcef":"#Code by saiabhiteja Chepuri  https:\/\/www.kaggle.com\/saiabhitejachepuri\/advance-regression-05\/notebook\n\n#assiging X values\nX=df.drop([\"engagement_index\",'lp_id'],axis=1)\nX = X.values\nX.shape","b95c79ee":"y = df['engagement_index']\ny = y.values\ny.shape","9b3ce257":"#assiging xtest values\nxtest=df.drop(['lp_id'],axis=1)\nxtest = xtest.values\nxtest.shape","a0ff43b2":"#train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1\/3, random_state = 69)","458a44ff":"from sklearn.linear_model import LinearRegression,RidgeCV,LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor,XGBRFRegressor","1543c8e5":"lr = LinearRegression()\nlr.fit(X_train,y_train)","c291d320":"pred = lr.predict(X_test)","14b0359a":"#Code by saiabhiteja Chepuri  https:\/\/www.kaggle.com\/saiabhitejachepuri\/advance-regression-05\/notebook\n\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","4051b57d":"#visuvalising the errors\nsns.set(style='darkgrid', rc = {'figure.figsize':(8,6)})\nsns.distplot(pred-y_test,kde=False, color='red');","e9309110":"rc = RidgeCV(cv = 20)\nrc.fit(X_train,y_train)","9b397a8f":"pred = rc.predict(X_test)","38e9580e":"#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","e2e29ee2":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(8,6)})\nsns.distplot(pred-y_test,kde=False);","e727f0fe":"ls = LassoCV(cv = 20)\nls.fit(X_train,y_train)","21bc4881":"pred = ls.predict(X_test)\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","4959f2ef":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(10,8)})\nsns.distplot(pred-y_test,kde=False);","4ea4f508":"rf = RandomForestRegressor(max_depth=4,n_estimators = 500)\nrf.fit(X_train,y_train)","36de6b76":"pred = rf.predict(X_test)","d6cd2557":"pred = rf.predict(X_test)\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","526d02c7":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(10,8)})\nsns.distplot(pred-y_test,kde=False, color='red');","54b95124":"xgb = XGBRegressor()\nxgb.fit(X_train,y_train)","a8357895":"pred = xgb.predict(X_test)\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","b8ab095b":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(10,8)})\nsns.distplot(pred-y_test,kde=False, color='red');","6612d018":"xgb = XGBRFRegressor()\nxgb.fit(X_train,y_train)","94358190":"pred = xgb.predict(X_test)\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","0426c234":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(10,8)})\nsns.distplot(pred-y_test,kde=False, color='green');","28a29a75":"from sklearn.ensemble import GradientBoostingRegressor","92a6c564":"gd = GradientBoostingRegressor(n_estimators=300,max_depth=6)\ngd.fit(X_train,y_train)","c3db66d2":"pred = gd.predict(X_test)\n#evaluvating the regressor performance (near to 1 is good)\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nprint(r2_score(y_test,pred)*100)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, pred))\nprint('Mean Squred Error:', mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))","9926f45f":"#visuvalising the errors\nsns.set(style='darkgrid',rc = {'figure.figsize':(8,6)})\nsns.distplot(pred-y_test,kde=False, color='green');","5f269595":"#Linear Regression","439441e7":"#Encoding Categorical Features","1df8b262":"That's all for now with PCA, Scaling and Regressors","79e54cf3":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #3CB371;\"><b style=\"color:black;\">Principal Component Analysis (PCA).<\/b><\/h1><\/center>","44c465a5":"#Using Feature importance of Random Forest","3460b316":"On the plotted chart above, we see what number of principal components we need.\nhttps:\/\/www.mikulskibartosz.name\/pca-how-to-choose-the-number-of-components\/\n\nNumber of components I wrote one less than the 4 features.\n\nOn the original code was 29 (features choosen).\n\n#Don\u2019t choose the number of components manually. (I did because I\u00b4m still learning)\n\nInstead of that, use the option that allows you to set the variance of the input that is supposed to be explained by the generated components.\n","7e1ea1a9":"#Random Forest","fb063313":"![](https:\/\/image.slidesharecdn.com\/pca-lda-ppt-190704130157\/95\/principal-component-analysis-pca-and-lda-ppt-slides-1-638.jpg?cb=1562246425)slideshare.net","5f087226":"#We don\u00b4t have test in this Dataset. Save the snippet for the next time.\n\nI replaced dftest to df, since we don't have test.","ca090569":"#Run pd.to_datetime. That will Date \"become\" int64 instead of datetime.\n\nhttps:\/\/stackoverflow.com\/questions\/62759022\/typeerror-invalid-type-promotion-while-fitting-a-logistic-regression-model-in-s\nAnswered by Sal_H","dd38f4ae":"#Selecting Models","18623a38":"#Lasso","6a90b619":"#Using chi2 test","4168cd81":"#Scale the data to the range between 0 and 1 before using PCA!","a993c80d":"#XGBoost","f69138d8":"#Attention xi=np.arange (1,4, step=1) I wrote 4 because there are 4 features.","fa8b797b":"#PCA","582cc1aa":"#Ridge Regressor"}}