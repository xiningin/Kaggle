{"cell_type":{"57904a8f":"code","d5b0c944":"code","06363d38":"code","992fc1cb":"code","8033ec31":"code","082be1fd":"code","bc9873b6":"code","fd20c20e":"code","3229e478":"code","7124d83f":"markdown","4daf8902":"markdown"},"source":{"57904a8f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_circles\nfrom sklearn import preprocessing\n%matplotlib inline","d5b0c944":"x, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1, random_state=14)\nx = preprocessing.scale(x)\n\nx_test=x[:500]\ny_test=y[:500]\nx=x[500:]\ny=y[500:]\n\ny=np.where(y==0,-1,1)\ny_test=np.where(y_test==0,-1,1)\n\nsns.scatterplot(x[:,0],x[:,1],hue=y.reshape(-1))","06363d38":"# Here we solve the primal form of problem but generaly dual form of svm problem is used while using kernel trick \n# as it is more efficient to compute predictions as alpha is non zero only for support vectors unlike weights in\n# primal form\nclass support_vector_machine:\n    def __init__(self,C=10,features=2,sigma_sq=0.1,kernel=\"None\"):\n        self.C=C\n        self.features=features\n        self.sigma_sq=sigma_sq\n        self.kernel=kernel\n        self.weights=np.zeros(features)\n        self.bias=0.\n        \n    def __similarity(self,x,l):\n        return np.exp(-sum((x-l)**2)\/(2*self.sigma_sq))\n\n    def gaussian_kernel(self,x1,x):\n        m=x.shape[0]\n        n=x1.shape[0]\n        op=[[self.__similarity(x1[x_index],x[l_index]) for l_index in range(m)] for x_index in range(n)]\n        return np.array(op)\n\n    def loss_function(self,y,y_hat):\n        sum_terms=1-y*y_hat\n        sum_terms=np.where(sum_terms<0,0,sum_terms)\n        return (self.C*np.sum(sum_terms)\/len(y)+sum(self.weights**2)\/2)\n\n    def fit(self,x_train,y_train,epochs=1000,print_every_nth_epoch=100,learning_rate=0.01):\n        y=y_train.copy()\n        x=x_train.copy()\n        self.initial=x.copy()\n        \n        assert x.shape[0]==y.shape[0] , \"Samples of x and y don't match.\"\n        assert x.shape[1]==self.features , \"Number of Features don't match\"\n        \n        if(self.kernel==\"gaussian\"):\n            x=self.gaussian_kernel(x,x)\n            m=x.shape[0]\n            self.weights=np.zeros(m)\n\n        n=x.shape[0]\n        \n        for epoch in range(epochs):\n            y_hat=np.dot(x,self.weights)+self.bias\n            grad_weights=(-self.C*np.multiply(y,x.T).T+self.weights).T\n            \n            for weight in range(self.weights.shape[0]):\n                grad_weights[weight]=np.where(1-y_hat<=0,self.weights[weight],grad_weights[weight])\n            \n            grad_weights=np.sum(grad_weights,axis=1)\n            self.weights-=learning_rate*grad_weights\/n\n            grad_bias=-y*self.bias\n            grad_bias=np.where(1-y_hat<=0,0,grad_bias)\n            grad_bias=sum(grad_bias)\n            self.bias-=grad_bias*learning_rate\/n\n            if((epoch+1)%print_every_nth_epoch==0):\n                print(\"--------------- Epoch {} --> Loss = {} ---------------\".format(epoch+1, self.loss_function(y,y_hat)))\n    \n    def evaluate(self,x,y):\n        pred=self.predict(x)\n        pred=np.where(pred==-1,0,1)\n        diff=np.abs(np.where(y==-1,0,1)-pred)\n        return((len(diff)-sum(diff))\/len(diff))\n\n    def predict(self,x):\n        if(self.kernel==\"gaussian\"):\n            x=self.gaussian_kernel(x,self.initial)\n        return np.where(np.dot(x,self.weights)+self.bias>0,1,-1)\n        \n            ","992fc1cb":"def visualize(model,title):\n    print(\"Test Accuracy = {}\".format(model.evaluate(x_test,y_test)))\n    x1=np.arange(-5,6,0.3)\n    x2=np.arange(-5,4,0.3)\n    for i in range(len(x1)):\n        for j in range(len(x2)):\n            pred=model.predict(np.array([np.array(np.array([x1[i],x2[j]]))]))[0]\n            if(pred>0.5):\n                plt.scatter(x1[i],x2[j],c=\"r\")\n            else:\n                plt.scatter(x1[i],x2[j],c=\"b\")\n    plt.title(title)\n    plt.show()","8033ec31":"model=support_vector_machine(C=20,sigma_sq=0.01)\nmodel.fit(x,y,epochs=20,print_every_nth_epoch=2,learning_rate=0.01)\nprint(\"Training Accuracy = {}\".format(model.evaluate(x,y)))\n","082be1fd":"visualize(model,\"Vanilla SVM\")","bc9873b6":"x, y = make_circles(n_samples=1000, noise=0.09)\nx = preprocessing.scale(x)\n\nx_test=x[:500]\ny_test=y[:500]\nx=x[500:]\ny=y[500:]\n\ny=np.where(y==0,-1,1)\ny_test=np.where(y_test==0,-1,1)\n\nsns.scatterplot(x[:,0],x[:,1],hue=y.reshape(-1))","fd20c20e":"model=support_vector_machine(C=10,kernel=\"gaussian\",sigma_sq=0.01)\nmodel.fit(x,y,epochs=20,print_every_nth_epoch=2,learning_rate=0.01)\nprint(\"Training Accuracy = {}\".format(model.evaluate(x,y)))","3229e478":"visualize(model,\"SVM with Gaussian Kernel\")","7124d83f":"### Support Vector Machine\n- Similar to logistic regression without kernel trick\n- Many kernels can be used to classify non linearly seperable data\n- Allows use of kernels with good performance in dual form.","4daf8902":"### Using The Kernel Trick\n\n- Here we will use gaussian Kernel\n- It use transforms a feature vector to infinite dimensions and returns dot product\n- The similarity function gives value between 0 to 1 depending upon how similar two vectors are\n- Sigma square dictates how rapidly the value will fall as we move away from a vectors\n- This can be used to create non linear classification boundry"}}