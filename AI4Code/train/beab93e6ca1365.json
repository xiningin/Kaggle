{"cell_type":{"e93990f0":"code","97445f43":"code","95cb9fa3":"code","85cd2393":"code","fed29c57":"code","2465629c":"code","c7253997":"code","455500f3":"code","5015e4b9":"code","aa9ce73b":"code","f2e64741":"code","b978b237":"code","26c5fbda":"code","b9a01a59":"code","1463de56":"code","7446cf84":"code","46b2c6ee":"code","193df81a":"code","3eb66bcb":"code","3033c61d":"code","6dd1f980":"code","eba67100":"code","ea38277a":"code","52c94ec1":"code","1ea462eb":"code","eadd713c":"code","4dd99d58":"code","68de98b0":"code","a7b9849b":"code","44331d45":"code","7c442186":"code","cdd36726":"code","cb4db5e6":"code","aaeaa377":"code","85a6c004":"code","f01c4ac5":"code","5c6f9bd9":"code","7ea356aa":"code","c5fa5ad9":"code","901f9a13":"code","aa858e44":"code","c1c33d17":"code","fdef52d1":"code","15ad8964":"code","3ddad542":"code","a12d9404":"code","74b03401":"code","92d45429":"code","f4acee4f":"code","925af208":"code","8e3918e8":"code","7f90ec67":"code","a201cce1":"code","d30c72e1":"code","71743084":"code","7a8cd84b":"code","de84e3f6":"code","0c041c3d":"code","78b04e39":"code","3d4794be":"code","26171383":"code","e3f44f6c":"code","6ffa1440":"code","feb10c2f":"code","3914626e":"code","19853638":"code","1366bcf5":"code","9746949a":"code","944c8360":"code","9fa3197a":"code","2cf55592":"code","72bcd6a8":"code","2af905fa":"code","9659a833":"code","1e5360b1":"code","77823041":"code","fbb8fa19":"code","4bf9f2ce":"code","c6b8b37d":"code","b31baa30":"code","b0853293":"code","a6c6dfd5":"code","f97de75b":"code","09d4d847":"code","21900722":"code","55af4f21":"code","bcb1ed19":"markdown","2f408f52":"markdown","d411cea7":"markdown","8f0f1782":"markdown","5195a16d":"markdown","2cdf92b2":"markdown","b0973f02":"markdown","d6ce9e48":"markdown","a473c2db":"markdown","53dfd887":"markdown","89249553":"markdown","2e237fcc":"markdown","585c636b":"markdown","f91e2ddc":"markdown","f8ece093":"markdown","69be57d5":"markdown","6334d470":"markdown","25537d43":"markdown","296852ba":"markdown","bff81cfc":"markdown","4667bb17":"markdown","f87647d2":"markdown","d898112e":"markdown","bca969f1":"markdown","0ce8aa7b":"markdown","ef4f3a95":"markdown","aebdafe6":"markdown","51874824":"markdown","5be70efb":"markdown","ac792405":"markdown","9ce8a7b7":"markdown","5756bf03":"markdown","dc7ebc60":"markdown","9569d947":"markdown","4e01ff6c":"markdown","63d42672":"markdown","6a22cb42":"markdown","cb3af21e":"markdown","1ac0e8ae":"markdown","c09d158b":"markdown","be76fdb6":"markdown","7024d071":"markdown","0a270f1e":"markdown","6767f407":"markdown","de7dfddb":"markdown","a10f20c8":"markdown","3912a716":"markdown","10689352":"markdown","668fef94":"markdown","fe5ac05e":"markdown","1d5afed8":"markdown","e46693b8":"markdown","bc709519":"markdown"},"source":{"e93990f0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport copy","97445f43":"data_raw = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","95cb9fa3":"data_raw.dtypes","85cd2393":"data_raw.shape","fed29c57":"data_raw.sample(5)","2465629c":"data_raw.info()","c7253997":"data_raw.describe()","455500f3":"data_raw.boxplot(figsize=(10,10), rot=90)","5015e4b9":"data_raw.hist(figsize=(15,20), )","aa9ce73b":"not_allowed_zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndata = copy.deepcopy(data_raw)","f2e64741":"data[not_allowed_zero_cols] = data[not_allowed_zero_cols].replace(0, np.NaN)","b978b237":"data.isnull().sum()","26c5fbda":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,20))\nsns.distplot(data.Glucose, ax=ax[0][0])\nsns.distplot(data.BloodPressure, ax=ax[0][1])\nsns.distplot(data.Insulin, ax=ax[1][0])\nsns.distplot(data.SkinThickness, ax=ax[1][1])\nsns.distplot(data.BMI, ax=ax[2][0])","b9a01a59":"data['Glucose'].fillna(data.Glucose.mean(), inplace=True)\ndata['BloodPressure'].fillna(data.BloodPressure.mean(), inplace=True)\ndata['BMI'].fillna(data.BMI.mean(), inplace=True)\ndata['SkinThickness'].fillna(data.SkinThickness.mean(), inplace=True)\n\ndata['Insulin'].fillna(data.Insulin.median(), inplace=True)","1463de56":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,20))\nsns.distplot(data.Glucose, ax=ax[0][0])\nsns.distplot(data.BloodPressure, ax=ax[0][1])\nsns.distplot(data.Insulin, ax=ax[1][0])\nsns.distplot(data.SkinThickness, ax=ax[1][1])\nsns.distplot(data.BMI, ax=ax[2][0])","7446cf84":"data.dtypes.value_counts().plot(kind='bar')","46b2c6ee":"sns.countplot(data.Outcome, ).set(title=\"Data Imbalance Check\")","193df81a":"sns.pairplot(data, hue='Outcome')","3eb66bcb":"cor = data.corr()\nmask = np.triu(np.ones_like(cor, dtype=np.bool))","3033c61d":"plt.figure(figsize=(10,10))\nsns.heatmap(cor, mask=mask, center=0,\n            square=True, linewidths=.5, annot=True)","6dd1f980":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, roc_curve, recall_score, precision_score, f1_score \nfrom sklearn.preprocessing import StandardScaler","eba67100":"X_scaled = StandardScaler().fit_transform(data.drop(['Outcome'], axis='columns'))","ea38277a":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, data.Outcome, random_state=123, test_size=.2)","52c94ec1":"from sklearn.linear_model import LogisticRegression","1ea462eb":"lr_clf= LogisticRegression(class_weight='balanced', random_state=123, max_iter=500)","eadd713c":"lr_clf.fit(X_train, y_train)","4dd99d58":"lr_pred = lr_clf.predict(X_test)","68de98b0":"lr_model_vals = dict(accuracy=accuracy_score(y_test, lr_pred),\n                    auc=roc_auc_score(y_test, lr_pred),\n                    recall=recall_score(y_test, lr_pred),\n                    precision=precision_score(y_test, lr_pred),\n                    f1_score = f1_score(y_test, lr_pred),\n                    )","a7b9849b":"y_pred_prob_lr = lr_clf.predict_proba(X_test)[:, 1]\nfpr_lr, tpr_lr , th_lr = roc_curve(y_test, y_pred_prob_lr)\ngmean_lr = np.sqrt(tpr_lr * (1-fpr_lr))\nix_lr = np.argmax(gmean_lr)","44331d45":"th_lr[np.argmax(gmean_lr)]","7c442186":"y_roc_pred_lr = [0 if pred < th_lr[ix_lr] else 1 for pred in y_pred_prob_lr ]","cdd36726":"print(\"Test classification Report With  tuned threshold\")\nprint(classification_report(y_test, y_roc_pred_lr)  )\n\nprint(\"Test classification Report Without  tuned threshold\")\nprint(classification_report(y_test, lr_pred) )","cb4db5e6":"fpr_tlr, tpr_tlr , th_tlr = roc_curve(y_test, y_roc_pred_lr)\ngmean_tlr = np.sqrt(tpr_tlr * (1-fpr_tlr))\nix_tlr = np.argmax(gmean_tlr)","aaeaa377":"tlr_model_vals = dict(accuracy=accuracy_score(y_test, y_roc_pred_lr),\n                    auc=roc_auc_score(y_test, y_roc_pred_lr),\n                    recall=recall_score(y_test, y_roc_pred_lr),\n                    precision=precision_score(y_test, y_roc_pred_lr),\n                    f1_score = f1_score(y_test, y_roc_pred_lr),\n                    )","85a6c004":"from sklearn.naive_bayes import GaussianNB","f01c4ac5":"gnb_clf = GaussianNB()","5c6f9bd9":"gnb_clf.fit(X_train, y_train)","7ea356aa":"gnb_pred = gnb_clf.predict(X_test)","c5fa5ad9":"gnb_model_vals = dict(accuracy=accuracy_score(y_test, gnb_pred),\n                    auc=roc_auc_score(y_test, gnb_pred),\n                    recall=recall_score(y_test, gnb_pred),\n                    precision=precision_score(y_test, gnb_pred),\n                    f1_score = f1_score(y_test, gnb_pred),\n                    )","901f9a13":"y_pred_prob_gnb = gnb_clf.predict_proba(X_test)[:, 1]\nfpr_nb, tpr_nb , th_nb = roc_curve(y_test, y_pred_prob_gnb)\ngmean_nb = np.sqrt(tpr_nb * (1-fpr_nb))\nix_nb = np.argmax(gmean_nb)","aa858e44":"print(\"Train Classification Report\")\nprint(classification_report(y_train, gnb_clf.predict(X_train))  )","c1c33d17":"print(\"Test Classification Report\")\nprint(classification_report(y_test, gnb_clf.predict(X_test))  )","fdef52d1":"from sklearn.ensemble import RandomForestClassifier","15ad8964":"rf_clf = RandomForestClassifier()","3ddad542":"rf_clf.fit(X_train, y_train)","a12d9404":"rf_pred = rf_clf.predict(X_test)","74b03401":"rf_model_vals = dict(accuracy=accuracy_score(y_test, rf_pred),\n                    auc=roc_auc_score(y_test, rf_pred),\n                    recall=recall_score(y_test, rf_pred),\n                    precision=precision_score(y_test, rf_pred),\n                    f1_score = f1_score(y_test, rf_pred),\n                    )","92d45429":"y_pred_prob_rf = rf_clf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf , th_rf = roc_curve(y_test, y_pred_prob_rf)\ngmean_rf = np.sqrt(tpr_rf * (1-fpr_rf))\nix_rf = np.argmax(gmean_rf)","f4acee4f":"print(\"\\t\\tTrain Classification Report\\n\")\nprint(classification_report(y_train, rf_clf.predict(X_train))  )","925af208":"print(\"\\t\\tTest Classification Report\\n\")\nprint(classification_report(y_test, rf_clf.predict(X_test))  )","8e3918e8":"alphas=[]\ntest=[]\ntrain=[]\nfor alpha in np.linspace(.03, .05, 10):\n    rf = RandomForestClassifier(ccp_alpha=alpha, random_state=123)\n    rf.fit(X_train, y_train)\n    y_train_predicted = rf.predict(X_train)\n    y_test_predicted = rf.predict(X_test)\n    mse_train = mean_squared_error(y_train, y_train_predicted)\n    mse_test = mean_squared_error(y_test, y_test_predicted)\n    alphas.append(alpha)\n    test.append(mse_test)\n    train.append(mse_train)\n    print(\"Alpha: {} Train mse: {} Test mse: {}\".format(alpha, mse_train, mse_test))\n    \nscore=pd.DataFrame({'alpha': alphas, 'test':test, 'train': train})","7f90ec67":"plt.plot(score.alpha, score.test)\nplt.plot(score.alpha, score.train)\nplt.legend(['Test Error', 'Train Error'])\nplt.xlabel('Alpha')\nplt.ylabel('Error')","a201cce1":"from sklearn.model_selection import RandomizedSearchCV","d30c72e1":"# Create the random grid\nrandom_grid = { 'ccp_alpha': np.linspace(.03, .05, 10),\n                'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [int(x) for x in np.linspace(5, 55, num = 10)], \n               'min_samples_split': [5, 10, 12], \n               'min_samples_leaf': [3,5,7,10],\n               }","71743084":"rf_clf_cv = RandomForestClassifier(class_weight=\"balanced\", random_state=123)\nrscv = RandomizedSearchCV(estimator=rf_clf_cv, param_distributions=random_grid, cv=3, scoring='f1_weighted')","7a8cd84b":"rscv.fit(X_train, y_train)","de84e3f6":"rscv.best_estimator_","0c041c3d":"print(\"\\t\\tTest Classification Report\\n\")\nprint(classification_report(y_test, rscv.predict(X_test))  )","78b04e39":"print(\"\\t\\tTrain Classification Report\\n\")\nprint(classification_report(y_train, rscv.predict(X_train))  )","3d4794be":"tuned_rf_model_vals = dict(accuracy=accuracy_score(y_test, rscv.predict(X_test)),\n                    auc=roc_auc_score(y_test, rscv.predict(X_test)),\n                    recall=recall_score(y_test, rscv.predict(X_test)),\n                    precision=precision_score(y_test, rscv.predict(X_test)),\n                    f1_score = f1_score(y_test, rscv.predict(X_test)),\n                    )","26171383":"y_pred_prob_trf = rscv.predict_proba(X_test)[:, 1]\nfpr_trf, tpr_trf , th_trf = roc_curve(y_test, y_pred_prob_trf)\ngmean_trf = np.sqrt(tpr_trf * (1-fpr_trf))\nix_trf = np.argmax(gmean_trf)","e3f44f6c":"import sklearn.metrics\nsorted(sklearn.metrics.SCORERS.keys())","6ffa1440":"from sklearn.neighbors import KNeighborsClassifier","feb10c2f":"nbr = []\nerror_rmse = []\nerror_rate = []\naccuracy = []\nfor n in range(2, 50):\n    knn_clf = KNeighborsClassifier(n_neighbors=n, weights='distance')\n    knn_clf.fit(X_train, y_train)\n    pred = knn_clf.predict(X_test)\n    \n    nbr.append(n)\n    error_rmse.append(mean_squared_error(y_test, pred, squared=False))\n    error_rate.append(np.mean(y_test != pred))\n    accuracy.append(accuracy_score(y_test, pred))\n    \nknn_stats = pd.DataFrame({'neighbour': nbr, 'rmse': error_rmse, 'error_rate': error_rate, 'accuracy': accuracy})      ","3914626e":"sns.lineplot(x='neighbour', y='error_rate', data=knn_stats)","19853638":"sns.lineplot(x='neighbour', y='rmse', data=knn_stats)","1366bcf5":"sns.lineplot(x='neighbour', y='accuracy', data=knn_stats)","9746949a":"knn_stats.neighbour[knn_stats.rmse.argmin()]","944c8360":"knn_stats.neighbour[[5,19,9,10]]","9fa3197a":"knn_clf = KNeighborsClassifier(n_neighbors=7, weights='distance')\nknn_clf.fit(X_train, y_train)\nknn_pred = knn_clf.predict(X_test)","2cf55592":"print(classification_report(y_test, pred))","72bcd6a8":"knn_model_vals = dict(accuracy=accuracy_score(y_test, knn_pred),\n                    auc=roc_auc_score(y_test, knn_pred),\n                    recall=recall_score(y_test, knn_pred),\n                    precision=precision_score(y_test, knn_pred),\n                    f1_score = f1_score(y_test, knn_pred),\n                    )","2af905fa":"y_pred_prob_knn = knn_clf.predict_proba(X_test)[:, 1]\nfpr_knn, tpr_knn , th_knn = roc_curve(y_test, y_pred_prob_knn)\ngmean_knn = np.sqrt(tpr_knn * (1-fpr_knn))\nix_knn = np.argmax(gmean_knn)","9659a833":"knn_param_grid = {'n_neighbors' : [7, 11, 12, 21],\n                      'weights': ['distance', 'uniform'],\n                      'algorithm' : ['ball_tree', 'kd_tree'],\n                     'leaf_size' :[30,40,50],                 \n                 }","1e5360b1":"knn_rscv = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions=knn_param_grid, cv=3, scoring='f1_weighted')","77823041":"knn_rscv.fit(X_train, y_train)","fbb8fa19":"knn_rscv.best_params_","4bf9f2ce":"tknn_pred = knn_rscv.predict(X_test)","c6b8b37d":"y_pred_prob_knn_cv = knn_rscv.predict_proba(X_test)[:, 1]\nfpr_tknn, tpr_tknn , th_tknn = roc_curve(y_test, y_pred_prob_knn_cv)\ngmean_tknn = np.sqrt(tpr_tknn * (1-fpr_tknn))\nix_tknn = np.argmax(gmean_tknn)","b31baa30":"accuracy_score(y_test, tknn_pred)","b0853293":"tknn_model_vals = dict(accuracy=accuracy_score(y_test, tknn_pred),\n                    auc=roc_auc_score(y_test, tknn_pred),\n                    recall=recall_score(y_test, tknn_pred),\n                    precision=precision_score(y_test, tknn_pred),\n                    f1_score = f1_score(y_test, tknn_pred),\n                    )","a6c6dfd5":"plt.subplots(figsize=(12,9))\nplt.plot(fpr_knn, tpr_knn, marker='o', markevery=[ix_knn])\nplt.plot(fpr_tknn, tpr_tknn, marker='o', markevery=[ix_tknn])\nplt.plot(fpr_rf, tpr_rf, marker='o', markevery=[ix_rf])\nplt.plot(fpr_trf, tpr_trf, marker='o', markevery=[ix_trf])\nplt.plot(fpr_nb, tpr_nb, marker='o', markevery=[ix_nb])\nplt.plot(fpr_lr, tpr_lr, marker='o', markevery=[ix_lr])\nplt.plot(fpr_tlr, tpr_tlr, marker='o', markevery=[ix_tlr])\nplt.plot([0,1], [0,1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC ROC Curve for all models')\nplt.legend(['KNN', 'TunedKNN', 'Random Forest', 'Tuned Random Forest', 'Naive Bayes', 'Logistic Regression',  'Tuned Logistic Regression','Dumb Classifier'])\nplt.show()","f97de75b":"model_stats = pd.DataFrame(data=[lr_model_vals, tlr_model_vals, gnb_model_vals, rf_model_vals, \n                                 tuned_rf_model_vals, knn_model_vals, tknn_model_vals ], \n                           index=['LogReg', 'Tuned LogReg auc_roc', 'naive_bayes', 'random_forest', \n                                  'tuned_random_forest ', 'knn', 'tuned knn'])","09d4d847":"model_stats.T.plot(kind='line', figsize=(12,9))","21900722":"final_models = pd.DataFrame(data=[ tlr_model_vals, tuned_rf_model_vals,  tknn_model_vals ], \n                           index=['Logistic Regression',  'Random Forest', 'KNN'])","55af4f21":"final_models.T.plot(kind='line', figsize=(12,9), table=True)","bcb1ed19":"### Tuning KNN","2f408f52":"<a id=problem><\/a>\n\n\n_Notebook Overview:_ This notebook creates 4 different basic models with basic feature tuning for PIMA diabetes dataset. We get logistic regression at 93% Recall\/Sensitivity. We also get KNN at 83% accuracy. \n\n## DESCRIPTION\n\n### Problem Statement\n- NIDDK (National Institute of Diabetes and Digestive and Kidney Diseases) research creates knowledge about and treatments for the most chronic, costly, and consequential diseases.\n- The dataset used in this project is originally from NIDDK. The objective is to predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.\n- Build a model to accurately predict whether the patients in the dataset have diabetes or not.\n\n### Dataset Description\nThe datasets consists of several medical predictor variables and one target variable (Outcome). Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and more.\n\n|Variables | Description|\n|----------|-------------|\n|Pregnancies|Number of times pregnant|\n|Glucose|\tPlasma glucose concentration in an oral glucose tolerance test|\n|BloodPressure |\tDiastolic blood pressure (mm Hg)|\n|SkinThickness |\tTriceps skinfold thickness (mm)|\n|Insulin |\tTwo hour serum insulin|\n|BMI |\tBody Mass Index|\n|DiabetesPedigreeFunction|\tDiabetes pedigree function|\n|Age|\tAge in years|\n|Outcome|\tClass variable (either 0 or 1). 268 of 768 values are 1, and the others are 0|","d411cea7":"---","8f0f1782":"<a id=roc_auc><\/a>\n\n#### AUC ROC Curve of all the models put together","5195a16d":"<a id=tune_lr><\/a>\n\n### Tunuing by AUC_ROC Threshold","2cdf92b2":"---","b0973f02":"The geometric mean between TPR and FPR is an optimum value which is max for any give tpr, fpr. If our focus is to create a model that predicts both sides, then this threshold value could be choosen to be optimum threshold. \nThe optimam threshold to classify True or False, we get at .364 ","d6ce9e48":"<a id=eda><\/a>\n\n## Exploratory Data Analysis:\n\n1. Check the balance of the data by plotting the count of outcomes by their value. Describe your findings and plan future course of action.\n\n2. Create scatter charts between the pair of variables to understand the relationships. Describe your findings.\n\n3. Perform correlation analysis. Visually explore it using a heat map.","a473c2db":"From the looks of training data, we can say that Random forest has overfitted. Due to overfitting, it may show very good responses but ultimately it is not a good model. We will tune the parmas for this. We will tune on Cost parameter and see what cost function makes the training and test data accuracy comparable.\n\nIn below code, we see only one iteration, but before coming to below values, I have done several iterations and compared trin and test errors to arrive at optimum cost value. The below iteration is to arrive at more precise cost value. ","53dfd887":"### Plot data types ","89249553":"## Observation","2e237fcc":"<a id=pre_process><\/a>\n\n## Scaling and splitting the data","585c636b":"<a id=compare><\/a>\n\n### Comparing model parameters","f91e2ddc":"[Go to Index](#index)    \n[Model Building](#mb)","f8ece093":"## Observations  \n \n   This is a binary data classification problem where depending on the all the features, the model has to predict\nwhether a person have diabetes or not. We have several ways to build the model for binary\/multi-class classification. \nFew of them are listed below:\n\n 1. Logistics Regression \n 2. Naive Bayes classification \n 3. Stochastic Gradient Descent\n 4. K-Nearest Neighbours\n 5. Decision Tree\n 6. Random Forest\n 7. Support Vector Machine\n\n    We are going to build four models and compare their performance on test and train dataset. We will tune the models if there is need to tune. \n    We will use K-Fold Cross Validation to validate the models. We will plot all the models stats together and compare their performance.\n    Of all the tuned models, we will pick up the best model. The step by step procedure can be followed below: \n\n     1. [Common Terminalogy](#common_terms)\n     2. [Data Scaling & Splitting](#pre_process)\n     3. [Logistic Regression](#logreg)\n     4. [Naive Bayes Classification](#nb)\n     5. [Random Forest](#rf)\n     6. [K Nearest Neighbours](#knn)\n     7. [Putting it all together](#summary)\n       * [ROC AUC Curves](#roc_auc)\n       * [Model Comparison](#compare)\n\n   From Model Comparison, we find that KNN is the most stable classifier. All the parameters are quite good. \n   It has best accuracy, auc, precision and f1_score of all the models. \n\n   If we are looking for a highly sensitive model, we can take logistic regression model, which has the highest recall. \n","69be57d5":"### Pair plot  analysis","6334d470":"#### Plots after filling the NaN values. ","25537d43":"### Observations\n\n It is an imbalanced dataset where positive outcomes are almost half of the negative outcomes. While creating model, \n we need to balance the outcomes either by oversampling the minority class or undersampling of majority class. Other \n workaround could be to do a weighted computation while training the model.\n\n**Pair plot analysis**\n\n* BMI and Skinthickness have a positive correlation \n* Insulin and Glucose have a positive correlation.\n* Rest other fields are uncorrelated or very weakly correlated.\n    \n**Correlation Analysis**\n\n* There is no strong correlation between any two fields\n* The BMI-Skinthickness and Insulin-Glucose are the highest correlated in the set but they are moderately correlated\n* Outcome is moderately correlated to Glucose","296852ba":"**Observation:** ccp_alpha is 0.037 ","bff81cfc":"<a id=common_terms><\/a>\n\n## Classificaiton Terminalogy \n\n**Precision**:  What proportion of positive identifications were actually positive?\nPrecision is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.\n    \nPrecision = \n$$\n\\frac{True Positives} {True Positives + False Positives} \\\\\n$$\n\n\nWhat proportion of actual positives were identified correctly is called **Recall\/Sensitivity\/True Positive Rate(TPR)**\n\nIn medical terms, **Sensitivity** measures how often a test correctly generates a positive result for people who have the condition that\u2019s being tested for. A test that\u2019s highly sensitive will flag almost everyone who has the disease and not generate many false-negative results. (Example: a test with 90% sensitivity will correctly return a positive result for 90% of people who have the disease, but will return a negative result \u2014 a false-negative \u2014 for 10% of the people who have the disease and should have tested positive.)\n\n**Recall\/Sensitivity\/True Positive Rate(TPR)** = \n$$\n\\frac{True Positives} {True Positives + False Negatives} \\\\\n$$\n\n\n**Specificity\/True Negative Rate** measures a test\u2019s ability to correctly generate a negative result for people who don\u2019t have the condition that\u2019s being tested for. A high-specificity test will correctly rule out almost everyone who doesn\u2019t have the disease and won\u2019t generate many false-positive results. (Example: a test with 90% specificity will correctly return a negative result for 90% of people who don\u2019t have the disease, but will return a positive result \u2014 a false-positive \u2014 for 10% of the people who don\u2019t have the disease and should have tested negative.)\n\n**Specificity\/True Negative Rate =**\n\n$$\n\\frac{True Negatives} {True Negatives + False Positives} \\\\\n$$\n\n$$\nFalse Positive Rate = {1 - Specificity} \\\\\n$$\n\n**Inverted specificity = false alarm rate = False Positive Rate =**\n\n$$\n\\frac{False Positives} {False Positives + True Negatives} \\\\\n$$\n\n\nFor any test, there is usually a trade-off between TPR and FPR.\n\n### ROC-AUC Curve\nIt is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate.\n\n### Precision - Recall Curve\nReviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n\nThe reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.\n\nKey to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1.\n\nA precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve.\nIn terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the area under curve summarize the skill of a model across thresholds, like ROC AUC.\n\n**F1 Score=**\n\n$$\n\\frac{2 * precision * recall} {precision + recall}\n$$\n","4667bb17":"---","f87647d2":"### AUC ROC for Naive Bayes classifier ","d898112e":"We have seen that 4 values of Nearest Neighbours yeilded the same error in our earlier plot. We will tune the model with all those given\nvalues and pickup the best.","bca969f1":"---","0ce8aa7b":"<a id=nb><\/a>\n\n## Naive Bayes Classificaiton ","ef4f3a95":"### Correlation analysis\n","aebdafe6":"## [Index:](#index)\n\n\n* [Problem Description](#problem)\n* [Data Cleaning](#dc)\n* [EDA](#eda)\n* [Model Building](#mb)\n     1. [Common Terminalogy](#common_terms)\n     2. [Data Scaling & Splitting](#pre_process)\n     3. [Logistic Regression](#logreg)\n     4. [Naive Bayes Classification](#nb)\n     5. [Random Forest](#rf)\n     6. [K Nearest Neighbours](#knn)\n     7. [Putting it all together](#summary)\n       * [ROC AUC Curves](#roc_auc)\n       * [Model Comparison](#compare)\n* [Model Comparison](#mc)","51874824":"<a id=mc><\/a>\n\n## Model Comparison\n\n**Data Modeling:**\n\nCreate a classification report by analyzing sensitivity, specificity, AUC (ROC curve), etc. Please be descriptive to explain what values of these parameter you have used.","5be70efb":"#### AUC ROC Curve parmas computation for Logistic Regression","ac792405":"We are capturing rmse, error_rate and accuracy for a range of nearest neighbours. We will plot all of them to observer nearest \nneighbours. We observe that error_rate and rmse give same plot while accuracy gives a mirror image of other two.","9ce8a7b7":"[Go to Index](#index)    \n[Go to Week 4](#week_4)","5756bf03":"---","dc7ebc60":"### Observations\n**Exploratory Data Analysis:**\n\n1. \n * Age, Insulin, DiabetesPedigreeFunction and Pregnancies are right skewed.\n * Zero values in blood pressure, BMI, Insulin and Glocuse clearly stands out in the plot\n * After removing zeros for non-zero expected columns, we see that except Insulin which \n   is highly right skewed, all other are near to gaussian distribution. \n2. \n * Except for Insulin, for rest of other non-zero columns, we can take mean value. \n * For Insulin, we took median value to fill\n3. \n * In Data type count plot, we can see that there are 2 int type columns and 7 float types","9569d947":"---","4e01ff6c":"## Please comment and upvote if you liked it :) ","63d42672":"---","6a22cb42":"<a id=final_model><\/a>\n\n### Final Model: \n\nOf all the models tuned and plotted above, I am picking up best 3 models and we will compare their values. From observing the plot, we can say that \nknn model is best in 4 out of 5 parameters hence that can be termed as best models. KNN has best accuracy, auc, precision and f1_score. The logistic Regression has best Recall\/Sensitivity at 93%\n","cb3af21e":"### Checking Data balance","1ac0e8ae":"<a id=rf><\/a>\n\n## Random Forest","c09d158b":"---","be76fdb6":"<a id=logreg><\/a>\n\n## Logistic Regression","7024d071":"<a id=knn><\/a>\n\n## K-Nearest Neighbour ","0a270f1e":"[Tableau Public Link](https:\/\/public.tableau.com\/profile\/awadhesh2246#!\/vizhome\/PGP-DSFinalProject\/scatter_bubble?publish=yes)","6767f407":"<a id=dc><\/a>\n\n## Data Cleaning:\n\n1. Perform descriptive analysis. Understand the variables and their corresponding values. On the columns below, a value of zero does not make sense and thus indicates missing value:\n\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\n\n2. Visually explore these variables using histograms. Treat the missing values accordingly.\n\n3. There are integer and float data type variables in this dataset. Create a count (frequency) plot describing the data types and the count of variables. ","de7dfddb":"#### With weights of distance we have  7 as the nearest neighbour counnt. We will create our model with 7 NN","a10f20c8":"<a id=mb><\/a>\n\n### Creating, Tuning and Comparing Models\n\n1. Devise strategies for model building. It is important to decide the right validation framework. Express your thought process.\n2. Apply an appropriate classification algorithm to build a model. Compare various models with the results from KNN algorithm.","3912a716":"### Treating Zero valued columns ","10689352":"### Performing Exploratory Data Analysis","668fef94":"[Go to Index](#index)    \n[EDA](#eda)","fe5ac05e":"---","1d5afed8":"### Observations\n\nI have plotted auc_roc curve of all the models together for comparison purpose. \nAlso, I have plotted all the model stats together, tuned and non-tuned version. \nWe can compare the performance by looking at the plots. Of that, I have selected \n3 best tuned models and plotted again to show what model is best of the lot.","e46693b8":"---","bc709519":"### Observations\n\nI have plotted different parameters of all the models above. By a simple look, we know that recall and precision are in opposite direction for all the models. The overfitted Random Forest had similar charaterstics as KNN but once that was tuned, its recall has gone up and precision came down.\n\nConsidering overall parameter values, KNN is best predictor of all the models. I have tuned KNN as well. With tuning the model performance has increased on all the parameters.\n\nI have tuned Logistic regression as well. After tuning, TPR has gone up while precision has gone down.\n\nIf we are looking for a model with high Sensitivity, we can pick up Logistic Regression model. For over-all better performance, we can choose KNN. "}}