{"cell_type":{"3b3f19f2":"code","59872d9c":"code","a73dae52":"code","2e8f05f0":"code","b7668e2f":"code","298b4748":"code","6dc0f49a":"code","9086d812":"code","635338ed":"code","4d84cec8":"code","a6b7d41a":"code","640fa22f":"code","63ed0f0f":"code","8b526cb0":"code","acd6c380":"code","37cf3146":"code","a0ddf223":"code","15840114":"code","61cb9c3f":"code","0ae7cad7":"code","afc35cf5":"code","a956380b":"code","66a3ddee":"code","6a0bc7a5":"code","b91fa267":"code","68aa9517":"code","87526105":"code","38096061":"code","117341ed":"code","20f1339e":"code","f29d0a29":"code","cc589938":"code","3c010431":"code","a1c6c2de":"code","8074e1da":"code","7dc011ff":"code","07609c33":"code","dbf2b091":"code","f16a04d6":"code","303547c4":"code","5c8d8f46":"code","29becdfd":"code","b5ad5c8c":"code","18c9ead9":"code","2e657aec":"code","34f2e11c":"code","7affa5ce":"code","33f69a5f":"code","6c8b88a0":"code","93c6bb3c":"code","733ff6d7":"code","3028b955":"code","071fc1c8":"code","0956a0d3":"markdown","bf81f868":"markdown","b1f0f455":"markdown","5d6381b8":"markdown","47e8d592":"markdown","bd47b6c6":"markdown","eea7b109":"markdown","4a3a56af":"markdown","b76cb0be":"markdown","7870a06b":"markdown","c98b77e3":"markdown","5d9f77ab":"markdown","2568dbde":"markdown","54d414ac":"markdown","604b00cb":"markdown","d27b1e4b":"markdown","159bb3f4":"markdown","73cad823":"markdown","762ed6af":"markdown"},"source":{"3b3f19f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59872d9c":"#Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\npd.set_option(\"display.max_columns\", None)\nmpl.rcParams['figure.figsize'] = (8,6)\nmpl.rcParams['axes.grid'] = False","a73dae52":"#Read the data\ndf = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","2e8f05f0":"df.head()","b7668e2f":"print(\"Rows Count     :: \",df.shape[0])\nprint(\"\\nColumns Count  :: \",df.shape[1])\nprint(\"\\nFeatures       :: \",df.columns.tolist())\nprint(\"\\nMissing Values :: \\n\",df.isnull().any())\nprint(\"\\nUnique Values  :: \\n\",df.nunique())","298b4748":"df.describe()","6dc0f49a":"df.info()","9086d812":"df['Date'] = pd.to_datetime(df['Date'])","635338ed":"df['RainTomorrow'].isnull().sum()","4d84cec8":"df = df.dropna(subset=['RainTomorrow'])","a6b7d41a":"sns.heatmap(df.isnull(),cmap='viridis',cbar=False)","640fa22f":"axis = sns.countplot(x='RainTomorrow', data=df)\naxis.set_title('Class Distribution for Target Feature', size=18)\n\nfor patch in axis.patches:\n    axis.text(x = patch.get_x() + patch.get_width()\/2, y = patch.get_height()\/2,\n             s = f\"{np.round(patch.get_height()\/len(df)*100, 1)}%\",\n             ha = 'center', size = 40, rotation = 0, weight = 'bold',\n             color = 'white')\naxis.set_xlabel('Rain Tomorrow', size=14)\naxis.set_ylabel('Count', size=14)\nplt.show()","63ed0f0f":"def encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\ndf['month'] = df['Date'].dt.month\ndf = encode(df,'month',12)\n\ndf['day'] = df['Date'].dt.day\ndf = encode(df,'day',31)","8b526cb0":"from sklearn.model_selection import train_test_split","acd6c380":"df_train, df_test = train_test_split(df, train_size = 0.8, random_state=101, stratify = df['RainTomorrow'])","37cf3146":"cat_feature = [feature for feature in df_train.columns if df[feature].dtype=='object']\ndf_train[cat_feature].isnull().mean()*100","a0ddf223":"for feature in cat_feature:\n    df_train[feature].fillna(df_train[feature].mode()[0], inplace = True)\n    df_test[feature].fillna(df_test[feature].mode()[0], inplace = True)","15840114":"num_feature = df.describe().columns.to_list()\ndf_train[num_feature].isnull().mean()*100","61cb9c3f":"cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm']\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(4,2, figsize = (12,8), constrained_layout = True)\n\nfor i, num_var in enumerate(cols): \n    sns.kdeplot(data = df_train, x = num_var, ax = ax[i][0],\n                fill = True, alpha = 0.6, linewidth = 1.5)\n    ax[i][0].set_ylabel(num_var)\n    ax[i][0].set_xlabel(None)\n    \n    sns.histplot(data = df_train, x = num_var, ax = ax[i][1], color='red')\n    ax[i][1].set_ylabel(None)\n    ax[i][1].set_xlabel(None)\n    \nfig.suptitle('Features having high missing values (>35%)', size = 16);","0ae7cad7":"for dataframe in [df_train, df_test]:\n    for feature in ['Sunshine','Cloud9am','Cloud3pm']:\n        dataframe[feature].fillna(dataframe[feature].median(), inplace=True)\n        \n    dataframe['Evaporation'].fillna(dataframe['Evaporation'].mean(), inplace=True)","afc35cf5":"df_train = df_train.dropna()\ndf_test = df_test.dropna()","a956380b":"numeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm',\n              'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\nplt.figure(figsize=(16,12))\naxis=sns.heatmap(df[numeric_col].corr(), cmap='coolwarm',annot=True, linewidths=3, square=True, fmt='.0%')\n\naxis.set_title('Corelation Between the features', size=16)\naxis.set_xticklabels(numeric_col, fontsize=12)\naxis.set_yticklabels(numeric_col, fontsize=12, rotation=0);","66a3ddee":"# Droping the columns\nfor dataframe in [df_train, df_test]:\n    dataframe.drop(['Temp3pm', 'Pressure3pm', 'Temp9am'], axis = 1, inplace = True)","6a0bc7a5":"numeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\n\nplt.style.use('seaborn')\nfig, axis = plt.subplots(13, 2, figsize = (12, 24))\nfor i, num_var in enumerate(numeric_col):\n    \n    # Checking for the outliers using boxplot\n    sns.boxplot(y = num_var, data = df_train, ax = axis[i][0], color = 'skyblue')\n    \n    # Checking for the distribution using kdeplot\n    sns.kdeplot(x = num_var, data = df_train, ax = axis[i][1], color = 'skyblue',\n               fill = True, alpha = 0.6, linewidth = 1.5)\n    \n    axis[i][0].set_ylabel(f\"{num_var}\", fontsize = 12)\n    axis[i][0].set_xlabel(None)\n    axis[i][1].set_xlabel(None)\n    axis[i][1].set_ylabel(None)\n\nfig.suptitle('Analysing Numeric Features', fontsize = 16, y = 1)\nplt.tight_layout()","b91fa267":"threshold = 0.05\nfor col in numeric_col:\n    \n    # Lower and upper threshold\n    lower_threshold = df_train[col].quantile(threshold)\n    upper_threshold = df_train[col].quantile(1-threshold)\n    \n    # Dropping the values below lower threshold and beyond upper threshold\n    df_train = df_train[(df_train[col]>=lower_threshold) & (df_train[col]<=upper_threshold)]\n    df_test = df_test[(df_test[col]>=lower_threshold) & (df_test[col]<=upper_threshold)]","68aa9517":"df_train.head(3)","87526105":"#Converting 'Yes' to 1 and 'No' to 0 in our target column\ndf_train['RainTomorrow'] = df_train['RainTomorrow'].map(dict({'Yes':1, 'No':0}))\ndf_test['RainTomorrow'] = df_test['RainTomorrow'].map(dict({'Yes':1, 'No':0}))","38096061":"# Dropping the features not required for model\ndf_train.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)\ndf_test.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)","117341ed":"# Splitting the data into y and X\ny_train = df_train.pop('RainTomorrow')\nX_train = df_train\n\ny_test = df_test.pop('RainTomorrow')\nX_test = df_test","20f1339e":"# Now the data is ready for preprocessing, let's convert categorical variables into one hot encoding\nX_train = pd.get_dummies(X_train, drop_first = True).reset_index(drop = True)\nX_test = pd.get_dummies(X_test, drop_first = True).reset_index(drop = True)","f29d0a29":"# Getting the categorical columns\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n               'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n               'Humidity3pm', 'Pressure9am', 'Cloud9am', 'Cloud3pm',\n               'month_sin', 'month_cos', 'day_sin', 'day_cos']\n\ncategorical_col = [i for i in X_train.columns if i not in numeric_col]","cc589938":"#Lets Scale our data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train_scale = pd.DataFrame(sc.fit_transform(X_train[numeric_col]), columns=numeric_col)\nX_test_scale = pd.DataFrame(sc.fit_transform(X_test[numeric_col]), columns=numeric_col)","3c010431":"# Creating final train and test data\nX_train_final = pd.concat([X_train_scale, X_train[categorical_col]], axis = 1)\nX_test_final = pd.concat([X_test_scale, X_test[categorical_col]], axis = 1)","a1c6c2de":"print(X_train_final.shape)\nprint(X_test_final.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8074e1da":"#ML imports\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\nimport matplotlib\n\n#ANN Imports\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout,  BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n","7dc011ff":"early_stopping = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)","07609c33":"model=Sequential()\nmodel.add(Dense(1024, kernel_initializer = 'uniform', activation='relu',input_dim = X_train_final.shape[1]))\nmodel.add(Dense(512, kernel_initializer = 'uniform', activation='relu',input_dim = X_train_final.shape[1]))\nmodel.add(Dense(256, kernel_initializer = 'uniform', activation='relu',input_dim = X_train_final.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, kernel_initializer = 'uniform', activation='relu',input_dim = X_train_final.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))","dbf2b091":"model.summary()","f16a04d6":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])","303547c4":"# Train the ANN\nepochs = 50\nbatch_size = 64\nhistory = model.fit(X_train_final, y_train, batch_size=batch_size,validation_data=(X_test_final,y_test), epochs=epochs, callbacks=[early_stopping], validation_split=0.3)","5c8d8f46":"model_loss = pd.DataFrame(model.history.history)","29becdfd":"model_loss.plot()","b5ad5c8c":"y_pred = model.predict_classes(X_test_final)\nprint(classification_report(y_test, y_pred))\nprint('Accuracy Score : ',accuracy_score(y_test,y_pred))","18c9ead9":"classifier_svc = SVC()","2e657aec":"classifier_svc.fit(X_train_final,y_train)","34f2e11c":"y_svc = classifier_svc.predict(X_test_final)\nprint(classification_report(y_test, y_svc))\nprint('Accuracy Score : ',accuracy_score(y_test,y_svc))\n","7affa5ce":"classifier_rf = RandomForestClassifier()\nclassifier_rf.fit(X_train_final,y_train)\ny_rf=classifier_rf.predict(X_test_final)","33f69a5f":"print(classification_report(y_test, y_rf))\nprint('Accuracy Score : ',accuracy_score(y_test,y_rf))","6c8b88a0":"classifier_ab = AdaBoostClassifier()\nclassifier_ab.fit(X_train_final,y_train)\ny_ab=classifier_ab.predict(X_test_final)","93c6bb3c":"print(classification_report(y_test, y_ab))\nprint('Accuracy Score : ',accuracy_score(y_test, y_ab))","733ff6d7":"classifier_gb = GradientBoostingClassifier()\nclassifier_gb.fit(X_train_final,y_train)\ny_gb=classifier_gb.predict(X_test_final)\nprint(classification_report(y_test, y_gb))\nprint('Accuracy Score : ',accuracy_score(y_test, y_gb))","3028b955":"classifier_nb = GaussianNB()\nclassifier_nb.fit(X_train_final,y_train)\ny_nb=classifier_nb.predict(X_test_final)\nprint(classification_report(y_test, y_nb))\nprint('Accuracy Score : ',accuracy_score(y_test, y_nb))","071fc1c8":"ann_df = pd.DataFrame(data=[f1_score(y_test,y_pred),accuracy_score(y_test, y_pred), recall_score(y_test, y_pred), precision_score(y_test, y_pred), roc_auc_score(y_test, y_pred)], \n             columns=['Artificial Neural Network'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nsvc_df = pd.DataFrame(data=[f1_score(y_test,y_svc),accuracy_score(y_test, y_svc), recall_score(y_test, y_svc),precision_score(y_test, y_svc), roc_auc_score(y_test, y_svc)], \n             columns=['Support Vector Classifier'],index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nrf_df = pd.DataFrame(data=[f1_score(y_test,y_rf),accuracy_score(y_test, y_rf), recall_score(y_test, y_rf), precision_score(y_test, y_rf), roc_auc_score(y_test,y_rf)], \n             columns=['Random Forest'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nab_df = pd.DataFrame(data=[f1_score(y_test,y_ab),accuracy_score(y_test, y_ab), recall_score(y_test, y_ab), precision_score(y_test, y_ab), roc_auc_score(y_test, y_ab)], \n             columns=['Adaboost'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n                      \nnb_df = pd.DataFrame(data=[f1_score(y_test,y_gb),accuracy_score(y_test, y_gb), recall_score(y_test, y_gb), precision_score(y_test, y_gb), roc_auc_score(y_test,y_gb)], \n             columns=['Naive Bayes'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n                      \ngb_df = pd.DataFrame(data=[f1_score(y_test,y_gb),accuracy_score(y_test, y_gb), recall_score(y_test, y_gb), precision_score(y_test, y_gb), roc_auc_score(y_test,y_gb)], \n             columns=['Gradient Boosting'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\ndf_models = round(pd.concat([ann_df,svc_df,rf_df,ab_df,nb_df,gb_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()","0956a0d3":"#### Imports","bf81f868":"##### *Many numeric features have data points beyond IQR. I am considering a threshold of 5 percentile, for outlier removal, i.e any point beyound 95 percentile and below 5 percentile is considerd as outlier and will be removed.*\n\n##### *The threshold of 5 percentile is choosen at random, you can very well consider other values for the threshold also.*","b1f0f455":"#### *All the missing features have null values less than >10% Lets impute them with the mode*","5d6381b8":"### *Model Building*","47e8d592":"### Random Forest","bd47b6c6":"#### *Now we have missing values less than 10%, I am going to remove those, We also replace these missing data with mean or median*","eea7b109":"### Outliers\n","4a3a56af":"### Feature Transformation","b76cb0be":"### Cleaning Numerical Features","7870a06b":"### AdaBoost","c98b77e3":"#### *Features 'Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm' have missing values more than 35% We need to fill them, Lets do some analysis to find best fit for these features*","5d9f77ab":"#### *Except Evaporation all the three features are distributed data, So I am going to impute these three features with median, and for evaporation I will use mean to fill the missing values*","2568dbde":"### Naive Bayes","54d414ac":"### Support Vector Machine","604b00cb":"### *Cleaning Categorical Feature*","d27b1e4b":"### Multicolinearty\n\n#### *Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model.*","159bb3f4":"### Gradient Boosting ","73cad823":"#### Data Preprocessing and EDA","762ed6af":"#### Strong Corelations\n\n\n##### pressure3pm and pressure9am\n\n##### temperature9am and minTemp\n\n##### temperature9am and maxTemp\n\n##### temperature3pm and maxTemp\n\n##### temperature3pm and temperature9am\n\nWe have to take only one from these\n"}}