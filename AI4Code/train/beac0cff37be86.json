{"cell_type":{"c5a1c2a5":"code","8f05b466":"code","5ec85bb3":"code","4cd50d72":"code","da22fbd0":"code","0f00b869":"code","b2b7ea9e":"code","ad92a607":"code","c90eb60a":"code","29b85525":"code","eb15f47c":"code","f97502fe":"code","1c1a807c":"code","45406d37":"code","ab82c3d6":"code","cbdbc6a0":"code","bac6761c":"code","f46f5b2d":"code","fc4ad886":"code","6ab058f5":"code","0a8aa314":"code","30a55c43":"code","77a0416e":"code","c4237a30":"code","1ff54c0d":"code","dd6fd8c7":"code","7bbbe6aa":"code","8f986cb3":"code","14e7a068":"code","69912a74":"code","df9e9dbd":"code","fa89ac21":"code","f033a319":"code","c233d296":"code","de5730ce":"code","462d92f1":"code","3301db29":"markdown","f49e56ad":"markdown","5bfdbd55":"markdown","9349162f":"markdown","e9cdb182":"markdown","d6687ca2":"markdown","3368284a":"markdown","53f7067f":"markdown","c1924009":"markdown","e4b3f9c9":"markdown","346d2174":"markdown","4059159e":"markdown","6c516172":"markdown","7adf8431":"markdown","92237b29":"markdown","883184fa":"markdown","37ad7ff0":"markdown","fb5d0894":"markdown","c141ca50":"markdown","02629b73":"markdown","1e80fa54":"markdown","2fe3f674":"markdown","ec447070":"markdown"},"source":{"c5a1c2a5":"!python --version\n!pip uninstall -y typing\n!pip install torch torchvision git+https:\/\/github.com\/BloodAxe\/catalyst git+https:\/\/github.com\/BloodAxe\/pytorch-toolbelt albumentations timm==0.2.1 iterative-stratification","8f05b466":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport gc\nimport torch\n\nimport albumentations as A\n\nfrom typing import Union, List, Optional, Dict, Callable\n\nfrom pytorch_toolbelt.utils import fs\n\nfrom pytorch_toolbelt.utils import fs\nfrom pytorch_toolbelt.utils.torch_utils import tensor_from_rgb_image, rgb_image_from_tensor, to_numpy, image_to_tensor \n\nfrom sklearn.utils import compute_sample_weight, compute_class_weight\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import OrderedDict\nfrom functools import partial\n\nfrom pytorch_toolbelt.utils import maybe_cuda, count_parameters\nfrom pytorch_toolbelt.modules import conv1x1, UnetBlock, ACT_RELU, ABN, ACT_SWISH\nfrom pytorch_toolbelt.modules import encoders as E\nfrom pytorch_toolbelt.modules.decoders import UNetDecoder\nfrom pytorch_toolbelt.modules.encoders import EncoderModule, Resnet18Encoder, Resnet34Encoder, Resnet50Encoder, Resnet101Encoder, SEResNeXt50Encoder, DenseNet121Encoder\nfrom pytorch_toolbelt.modules.encoders.timm import B0Encoder, B2Encoder, B3Encoder, B4Encoder, B6Encoder, DPN68Encoder, SKResNet18Encoder, SKResNeXt50Encoder\nfrom pytorch_toolbelt.utils.catalyst import F1ScoreCallback\n\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\nfrom torch.optim import SGD, Adam, RMSprop, AdamW\nfrom torch.utils.data import Dataset, WeightedRandomSampler, ConcatDataset, DataLoader\n\nfrom catalyst.dl import (\n    SupervisedRunner,\n    SchedulerCallback,\n    CriterionCallback\n)\nfrom catalyst.utils import set_global_seed\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","5ec85bb3":"DATA_DIR = \"..\/input\/xview2-damage-assessment\"\n\n\nos.listdir(DATA_DIR)","4cd50d72":"df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ndf","da22fbd0":"\ndf = df.drop(columns=[\"light_damaged_pixels\",\n                      \"destroyed_pixels\",\n                      \"medium_damaged_pixels\",\n                      \"non_damaged_pixels\", \n                      \"folder\"])\n\ndf[\"key\"] = df[\"event_name\"] + df[\"sample_id\"].apply(str)\ndf","0f00b869":"df_pre = df[df[\"event_type\"] == \"pre\"]\\\n    .rename(columns={\"image_fname\": \"image_fname_pre\", \"mask_fname\": \"mask_fname_pre\"})\\\n    .drop(columns=[\"event_type\", \"image_id\", \"sample_id\",\n                      \"light_damaged_buildings\",\n                      \"destroyed_buildings\",\n                      \"medium_damaged_buildings\",\n                      \"non_damaged_buildings\"])\ndf_post = df[df[\"event_type\"] == \"post\"]\\\n    .rename(columns={\"image_fname\": \"image_fname_post\", \"mask_fname\": \"mask_fname_post\"})\\\n    .drop(columns=[\"event_name\", \"image_id\", \"event_type\", \"sample_id\"])\ndf_all = df_pre.merge(df_post, on='key')\n#df_all[\"total_buildings\"] = df_all[\"destroyed_buildings\"] + df_all[\"medium_damaged_buildings\"] + df_all[\"light_damaged_buildings\"] + df_all[\"non_damaged_buildings\"]\ndf_all[\"damaged_buildings\"] = df_all[\"destroyed_buildings\"] + df_all[\"medium_damaged_buildings\"] + df_all[\"light_damaged_buildings\"]\nprint(\"Total samples\", len(df_all))\n\n","b2b7ea9e":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ndf_all = df_all[df_all[\"damaged_buildings\"] > 0].copy()\nprint(\"Samples with damaged buildings\", len(df_all))\n\n#train_df, valid_df = train_test_split(df_all, stratify=df_all[\"event_name\"], test_size=0.2, shuffle=True, random_state=42)\n\n\nmskf = MultilabelStratifiedKFold(n_splits=4, shuffle=True, random_state=0)\nstratify_label = np.stack([\n    df_all[\"non_damaged_buildings\"].values > 0,\n    df_all[\"light_damaged_buildings\"].values > 0,\n    df_all[\"medium_damaged_buildings\"].values > 0,\n    df_all[\"destroyed_buildings\"].values > 0,\n    \n], axis=-1)\n\nprint(stratify_label.shape)\n\ntrain_index, valid_index = next(mskf.split(df_all, stratify_label))\n\ntrain_df = df_all.iloc[train_index]\nvalid_df = df_all.iloc[valid_index]\n\ntrain_df = train_df.sort_values(by=\"damaged_buildings\", ascending=False)[:300]\nvalid_df = valid_df.sort_values(by=\"damaged_buildings\", ascending=False)[:60]\nholdout_df = valid_df[50:]\nvalid_df = valid_df[:50]\n\nprint(\"Train samples\", len(train_df))\nprint(\"Valid samples\", len(valid_df))\nprint(\"Holdout samples\", len(holdout_df))\n\ntrain_img_pre = train_df[\"image_fname_pre\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\ntrain_img_post = train_df[\"image_fname_post\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\ntrain_mask_post = train_df[\"mask_fname_post\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\n\nvalid_img_pre = valid_df[\"image_fname_pre\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\nvalid_img_post = valid_df[\"image_fname_post\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\nvalid_mask_post = valid_df[\"mask_fname_post\"].apply(lambda x: os.path.abspath(os.path. os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\n\nholdout_img_pre = holdout_df[\"image_fname_pre\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\nholdout_img_post = holdout_df[\"image_fname_post\"].apply(lambda x: os.path.abspath(os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()\nholdout_mask_post = holdout_df[\"mask_fname_post\"].apply(lambda x: os.path.abspath(os.path. os.path.join(DATA_DIR, x.replace(\"\\\\\", \"\/\")))).tolist()","ad92a607":"train_df","c90eb60a":"print(train_df[\"event_name\"].unique())","29b85525":"INPUT_IMAGE_KEY = \"image\"\nINPUT_INDEX_KEY = \"index\"\n\nINPUT_IMAGE_PRE_KEY = \"image_pre\"\nINPUT_IMAGE_POST_KEY = \"image_post\"\n\nINPUT_IMAGE_ID_KEY = \"image_id\"\nINPUT_BUILDINGS_MASK_KEY = \"buildings_mask\"\nINPUT_DAMAGE_MASK_KEY = \"damage_mask\"\n\n\nOUTPUT_DAMAGE_MASK_KEY = \"predicted_damage_mask\"\n\ndef read_image_rgb(fname):\n    image = cv2.imread(fname)[..., ::-1]\n    if image is None:\n        raise FileNotFoundError(fname)\n    return image\n\n\ndef read_mask(fname):\n    from PIL import Image\n    mask = np.array(Image.open(fname))  # Read using PIL since it supports palletted image\n    if len(mask.shape) == 3:\n        mask = np.squeeze(mask, axis=-1)\n\n    return mask\n    \ndef overlay_damage_mask(image, mask):\n    overlay = image.copy()\n    overlay[mask == 1] = (0,   255, 0) # Regular buildingn are green\n    overlay[mask == 2] = (255, 255, 0) # Destroyed buildings are yellow\n    overlay[mask == 3] = (255,  69, 0) # Destroyed buildings are orange\n    overlay[mask == 4] = (255,   0, 0) # Destroyed buildings are red\n    return cv2.addWeighted(overlay, 0.5, image, 0.5, 0, dtype=cv2.CV_8U)","eb15f47c":"import matplotlib.pyplot as plt\n\nf, ax = plt.subplots(1,3, figsize=(30,9))\nax[0].imshow(read_image_rgb(train_img_pre[16]))\nax[0].axis('off')\nax[1].imshow(read_image_rgb(train_img_post[16]))\nax[1].axis('off')\nax[2].imshow(overlay_damage_mask(\n    read_image_rgb(train_img_post[16]),\n    read_mask(train_mask_post[16])\n))\nax[2].axis('off')\nf.tight_layout()\nf.show()","f97502fe":"def show_image(image):\n    plt.figure(figsize=(10,10))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \nsample_image = read_image_rgb(train_img_post[16])\nsample_mask = read_mask(train_mask_post[16])\n\nshow_image(overlay_damage_mask(sample_image, sample_mask))","1c1a807c":"transform = A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, p=1)\naugmented = transform(image=sample_image, mask=sample_mask)\n\nshow_image(overlay_damage_mask(augmented[\"image\"], augmented[\"mask\"]))\n","45406d37":"sample_image = read_image_rgb(train_img_post[16])\nsample_mask = read_mask(train_mask_post[16])\n\ntransform = A.Compose([\n    A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=15, p=1, border_mode=cv2.BORDER_REFLECT),\n    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1),\n])\n\naugmented = transform(image=sample_image, mask=sample_mask)\n\nshow_image(overlay_damage_mask(augmented[\"image\"], augmented[\"mask\"]))","ab82c3d6":"color_augmentations = A.Compose([\n    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, brightness_by_max=True, p=0.5),\n    A.RandomGamma(gamma_limit=(90, 110)),\n    A.Normalize()\n])\n\n\nspatial_augmentations = A.Compose([\n    # Random crop training samples to 512x512 patches\n    A.RandomSizedCrop((512-64, 512+64), 512, 512),\n    # D4\n    A.RandomRotate90(p=1),\n    A.Transpose(p=0.5),\n    # Random rotate\n    A.ShiftScaleRotate(\n        shift_limit=0.05,\n        scale_limit=0.1,\n        rotate_limit=5,\n        border_mode=cv2.BORDER_CONSTANT,\n        value=0,\n        mask_value=0,\n    ),    \n])","cbdbc6a0":"class ImageMaskDataset(Dataset):\n    def __init__(\n        self,\n        pre_image_filenames: List[str],\n        post_image_filenames: List[str],\n        post_mask_filenames: Optional[List[str]],\n        spatial_transform: A.Compose,\n        color_transform: A.Compose = None,\n    ):\n        assert len(pre_image_filenames) == len(post_image_filenames)\n\n        self.pre_image_filenames = pre_image_filenames\n        self.post_image_filenames = post_image_filenames\n        self.post_mask_filenames = post_mask_filenames\n\n        self.spatial_transform = spatial_transform\n        self.color_transform = color_transform\n\n    def __len__(self):\n        return len(self.pre_image_filenames)\n\n    def __getitem__(self, index):\n        pre_image = read_image_rgb(self.pre_image_filenames[index]) # 1024x1024x3\n        post_image = read_image_rgb(self.post_image_filenames[index]) # 1024x1024x3\n        mask = read_mask(self.post_mask_filenames[index]) # 1024x1024\n        \n        if self.color_transform is not None:\n            pre_image = self.color_transform(image=pre_image)[\"image\"]\n            post_image = self.color_transform(image=post_image)[\"image\"]\n\n        image = np.dstack([pre_image, post_image]) # 1024x1024x6\n        \n        if self.spatial_transform is not None:\n            data = self.spatial_transform(image=image, mask=mask)\n            image = data[\"image\"]\n            mask = data[\"mask\"]\n            \n        sample = {\n            INPUT_INDEX_KEY: index,\n            INPUT_IMAGE_ID_KEY: fs.id_from_fname(self.pre_image_filenames[index]),\n            INPUT_IMAGE_KEY: image_to_tensor(image),\n            INPUT_BUILDINGS_MASK_KEY: torch.from_numpy(mask > 0).long(),\n            INPUT_DAMAGE_MASK_KEY: torch.from_numpy(mask).long()\n        }\n\n        return sample\n","bac6761c":"train_ds = ImageMaskDataset(\n    train_img_pre,\n    train_img_post,\n    train_mask_post,\n    spatial_transform=spatial_augmentations,\n    color_transform=color_augmentations,\n)\n\nvalid_ds = ImageMaskDataset(\n    valid_img_pre,\n    valid_img_post,\n    valid_mask_post,\n    color_transform=A.Normalize(),\n    spatial_transform=None,\n)\n\nprint(len(train_ds), len(valid_ds))","f46f5b2d":"def show_sample(sample: Dict[str, Tensor]):    \n    image_pre = rgb_image_from_tensor(sample[INPUT_IMAGE_KEY][0:3])\n    image_post = rgb_image_from_tensor(sample[INPUT_IMAGE_KEY][3:6])\n    damage_mask = to_numpy(sample[INPUT_DAMAGE_MASK_KEY])\n    \n    f, ax = plt.subplots(1,3, figsize=(30,9))\n    ax[0].imshow(image_pre)\n    ax[0].axis('off')\n    ax[1].imshow(image_post)\n    ax[1].axis('off')\n    ax[2].imshow(overlay_damage_mask(image_post, damage_mask))\n    ax[2].axis('off')\n    f.tight_layout()\n    f.show()","fc4ad886":"show_sample(train_ds[1])","6ab058f5":"show_sample(valid_ds[40])","0a8aa314":"from collections import OrderedDict\nfrom functools import partial\nfrom typing import Union, List, Dict\n\nfrom pytorch_toolbelt.modules import conv1x1, UnetBlock, ACT_RELU, ABN, ACT_SWISH\nfrom pytorch_toolbelt.modules import encoders as E\nfrom pytorch_toolbelt.modules.decoders import UNetDecoder\nfrom pytorch_toolbelt.modules.encoders import EncoderModule\nfrom pytorch_toolbelt.modules.encoders.timm import *\n\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\n\n\nclass UnetSegmentationModel(nn.Module):\n    def __init__(\n        self,\n        encoder: EncoderModule,\n        unet_channels: Union[int, List[int]],\n        num_classes: int = 1,\n        dropout=0.25,\n        full_size_mask=True,\n        activation=ACT_RELU,\n        upsample_block=nn.UpsamplingNearest2d,\n        need_supervision_masks=False,\n        last_upsample_block=None,\n    ):\n        super().__init__()\n        self.encoder = encoder\n\n        abn_block = partial(ABN, activation=activation)\n        self.decoder = UNetDecoder(\n            feature_maps=encoder.channels,\n            decoder_features=unet_channels,\n            unet_block=partial(UnetBlock, abn_block=abn_block),\n            upsample_block=upsample_block,\n        )\n\n        self.mask = nn.Sequential(\n            OrderedDict([(\"drop\", nn.Dropout2d(dropout)), (\"conv\", conv1x1(unet_channels[0], num_classes))])\n        )\n\n\n    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n        x_size = x.size()\n        x = self.encoder(x)\n        x = self.decoder(x)\n\n        # Decode mask\n        mask = self.mask(x[0])\n        mask = F.interpolate(mask, size=x_size[2:], mode=\"bilinear\", align_corners=False)\n\n        output = {OUTPUT_DAMAGE_MASK_KEY: mask}\n        return output\n\n    \ndef b4_unet32(input_channels=6, num_classes=5, dropout=0.1, pretrained=True):\n    # We take outputs of stages 1,2,3,4 which corresponds to strides 4,8,16,32\n    encoder = B4Encoder(pretrained=pretrained, layers=[1,2,3,4])\n    if input_channels != 3:\n        encoder.change_input_channels(input_channels)\n\n    return UnetSegmentationModel(\n        encoder, num_classes=num_classes, unet_channels=[32, 64, 128], activation=ACT_SWISH, dropout=dropout\n    )","30a55c43":"DEFAULT_LR = 1e-3\nDEFAULT_NUM_EPOCHS = 10\nDEFAULT_BATCH_SIZE = 20\nDEFAULT_NUM_WORKERS = 3","77a0416e":"def train_closure(model, batch_size:int, experiment_name:str, epochs=DEFAULT_NUM_EPOCHS, workers=DEFAULT_NUM_WORKERS):\n    set_global_seed(42)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    \n    # Define our data loaders\n    loaders = OrderedDict([\n        (\"train\", DataLoader(\n            train_ds,\n            batch_size=batch_size,\n            num_workers=workers,\n            pin_memory=True,\n            drop_last=True,\n            shuffle=True,\n        )),\n        (\"valid\", DataLoader(\n            valid_ds,\n            batch_size=batch_size \/\/ 4,\n            num_workers=workers,\n            pin_memory=True,\n            shuffle=False,\n        ))\n    ])\n    \n    print(\"Experiment\",experiment_name)\n    print(\"Model\", count_parameters(model))\n\n    runner = SupervisedRunner(input_key=INPUT_IMAGE_KEY, output_key=None)\n    runner.train(\n      fp16 = {\"amp\": True},\n      model = model,\n      criterion = {\n          \"ce\": nn.CrossEntropyLoss()\n      },\n      # Define what optimizer we will use\n      optimizer=Adam(model.parameters(), lr=DEFAULT_LR),\n      scheduler=None,\n      callbacks=[\n        CriterionCallback(\n            input_key=INPUT_DAMAGE_MASK_KEY,\n            output_key=OUTPUT_DAMAGE_MASK_KEY,\n            criterion_key=\"ce\"),\n        F1ScoreCallback(\n            prefix=\"f1\",\n            input_key=INPUT_DAMAGE_MASK_KEY,\n            output_key=OUTPUT_DAMAGE_MASK_KEY,\n            num_classes=5\n        )\n      ],\n      loaders = loaders,\n      logdir=os.path.join(\"logs\", experiment_name),\n      num_epochs=epochs,\n      verbose=True,\n      main_metric=\"f1\",\n      minimize_metric=False,\n    )\n    \n    best_f1_score = runner.best_valid_metrics[\"f1\"]\n    best_checkpoint_fname = os.path.join(runner.experiment.logdir, \"checkpoints\", \"best.pth\")\n    del runner, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return best_checkpoint_fname, best_f1_score\n","c4237a30":"best_checkpoint_fname, best_f1 = train_closure(model=b4_unet32(), batch_size=20, experiment_name=\"b4_unet32\")\nprint(best_checkpoint_fname, best_f1)","1ff54c0d":"@torch.no_grad()\ndef predict(model, image_pre, image_post):\n    if isinstance(image_pre, str):\n        image_pre = read_image_rgb(image_pre)\n    if isinstance(image_post, str):\n        image_post = read_image_rgb(image_post)\n        \n    normalize = A.Normalize()\n    image = np.dstack([\n        normalize(image=image_pre)[\"image\"],\n        normalize(image=image_post)[\"image\"],\n    ])\n    model_input = image_to_tensor(image).unsqueeze(0).cuda()\n    outputs = model.eval()(model_input)\n    \n    mask = outputs[OUTPUT_DAMAGE_MASK_KEY].argmax(dim=1)\n    return to_numpy(mask[0])\n\ndef show_predictions(image, gt_mask, pred_mask):\n    f, ax = plt.subplots(1, 3, figsize=(30,10))\n    \n    gt_overlay = overlay_damage_mask(image, gt_mask)\n    pred_overlay = overlay_damage_mask(image, pred_mask)\n    \n    ax[0].imshow(image)\n    ax[0].axis('off')\n\n    ax[1].imshow(gt_overlay)\n    ax[1].axis('off')\n\n    ax[2].imshow(pred_overlay)\n    ax[2].axis('off')\n    \n    f.tight_layout()\n    f.show()","dd6fd8c7":"# Recreate the model and load weights from the best checkpoint\nmodel = b4_unet32().cuda()\nmodel.load_state_dict(torch.load(best_checkpoint_fname)[\"model_state_dict\"])\n\n# Run the predictions\nfor i in range(len(holdout_mask_post)):\n    gt_mask = read_mask(holdout_mask_post[i])\n    image_pre = read_image_rgb(holdout_img_pre[i])\n    image_post = read_image_rgb(holdout_img_post[i])\n    pred_mask = predict(model, image_pre, image_post)\n    \n    show_predictions(image_post, gt_mask, pred_mask)\n\ndel model","7bbbe6aa":"def resnet18_unet32(input_channels=6, num_classes=5, dropout=0.1, pretrained=True):\n    # We take outputs of stages 1,2,3,4 which corresponds to strides 4,8,16,32\n    encoder = Resnet18Encoder(pretrained=pretrained, layers=[1,2,3,4])\n    if input_channels != 3:\n        encoder.change_input_channels(input_channels)\n\n    return UnetSegmentationModel(\n        encoder, num_classes=num_classes, unet_channels=[32, 64, 128], activation=ACT_RELU, dropout=dropout\n    )","8f986cb3":"best_checkpoint_fname, best_f1 = train_closure(model=resnet18_unet32(), batch_size=20, experiment_name=\"resnet18_unet32\")\nprint(best_checkpoint_fname, best_f1)","14e7a068":"labels_count = np.zeros(5)\nfor mask_fname in train_mask_post:\n    mask = read_mask(mask_fname)\n    labels_count = np.bincount(mask.flatten(), minlength=5)\n    \nprint(\"Not building\", \"No damage\", \"Light\", \"Medim\", \"Destroyed\")\nprint(labels_count)","69912a74":"# Task: Assign class weights based on labels count\nce_class_weights = np.ones_like(labels_count)\n\nprint(ce_class_weights)\n\ndef train_closure_with_weighted_ce(model, batch_size:int, experiment_name:str, epochs=DEFAULT_NUM_EPOCHS, workers=DEFAULT_NUM_WORKERS):\n    set_global_seed(42)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    \n    # Define our data loaders\n    loaders = OrderedDict([\n        (\"train\", DataLoader(\n            train_ds,\n            batch_size=batch_size,\n            num_workers=workers,\n            pin_memory=True,\n            drop_last=True,\n            shuffle=True,\n        )),\n        (\"valid\", DataLoader(\n            valid_ds,\n            batch_size=batch_size \/\/ 4,\n            num_workers=workers,\n            pin_memory=True,\n            shuffle=False,\n        ))\n    ])\n    \n    print(\"Experiment\",experiment_name)\n    print(\"Model\", count_parameters(model))\n\n    runner = SupervisedRunner(input_key=INPUT_IMAGE_KEY, output_key=None)\n    runner.train(\n      fp16 = {\"amp\": True},\n      model = model,\n      criterion = {\n          \"weighted_ce\": nn.CrossEntropyLoss(weight=torch.tensor(ce_class_weights).float().cuda())\n      },\n      # Define what optimizer we will use\n      optimizer=Adam(model.parameters(), lr=DEFAULT_LR),\n      scheduler=None,\n      callbacks=[\n        CriterionCallback(\n            input_key=INPUT_DAMAGE_MASK_KEY,\n            output_key=OUTPUT_DAMAGE_MASK_KEY,\n            criterion_key=\"weighted_ce\"),\n        F1ScoreCallback(\n            prefix=\"f1\",\n            input_key=INPUT_DAMAGE_MASK_KEY,\n            output_key=OUTPUT_DAMAGE_MASK_KEY,\n            num_classes=5\n        )\n      ],\n      loaders = loaders,\n      logdir=os.path.join(\"logs\", experiment_name),\n      num_epochs=epochs,\n      verbose=True,\n      main_metric=\"f1\",\n      minimize_metric=False,\n    )\n    \n    best_f1_score = runner.best_valid_metrics[\"f1\"]\n    best_checkpoint_fname = os.path.join(runner.experiment.logdir, \"checkpoints\", \"best.pth\")\n    del runner, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return best_checkpoint_fname, best_f1_score\n","df9e9dbd":"best_checkpoint_fname, best_f1 = train_closure_with_weighted_ce(model=b4_unet32(), batch_size=20, experiment_name=\"b4_unet32_weighted_ce\")\nprint(best_checkpoint_fname, best_f1)\n\n# Recreate the model and load weights from the best checkpoint\nmodel = b4_unet32().cuda()\nmodel.load_state_dict(torch.load(best_checkpoint_fname)[\"model_state_dict\"])\n\n# Run the predictions\nfor i in range(len(holdout_mask_post)):\n    gt_mask = read_mask(holdout_mask_post[i])\n    image_pre = read_image_rgb(holdout_img_pre[i])\n    image_post = read_image_rgb(holdout_img_post[i])\n    pred_mask = predict(model, image_pre, image_post)\n    \n    show_predictions(image_post, gt_mask, pred_mask)","fa89ac21":"class SiameseUnetSegmentationModel(nn.Module):\n    def __init__(\n        self,\n        encoder: EncoderModule,\n        num_classes: int,\n        unet_channels: List[int],\n        dropout=0.25,\n        abn_block: Union[ABN, Callable[[int], nn.Module]] = ABN,\n        full_size_mask=True,\n        activation=ACT_RELU,\n        upsample_block=nn.UpsamplingNearest2d,\n    ):\n        super().__init__()\n        self.encoder = encoder\n\n        feature_maps = [2 * fm for fm in encoder.channels]\n        \n        abn_block = partial(ABN, activation=activation)\n        self.decoder = UNetDecoder(\n            feature_maps=feature_maps,\n            decoder_features=unet_channels,\n            unet_block=partial(UnetBlock, abn_block=abn_block),\n            upsample_block=upsample_block,\n        )\n\n        self.mask = nn.Sequential(\n            OrderedDict([(\"drop\", nn.Dropout2d(dropout)), (\"conv\", conv1x1(unet_channels[0], num_classes))])\n        )        \n\n    def forward(self, image):\n        batch_size = image.size(0)\n        # Split input image batch into pre- and post- batches\n        pre, post = image[:, 0:3, ...], image[:, 3:6, ...] # [B,3,H,W], [B,3,H,W]\n        \n        # Concatenate them along batch dimension since it's faster than calling self.encoder(pre), self.encoder(post)\n        x = torch.cat([pre, post], dim=0) # [2 * B, 3, H, W]\n        \n        # Encoder\n        features = self.encoder(x) # List[Tensor] of strides 4,8,16,32\n        \n        pre_features = [f[0: batch_size] for f in features]\n        post_features = [f[batch_size: batch_size * 2] for f in features]\n        features = [torch.cat([pre, post], dim=1) for pre, post in zip(pre_features, post_features)]\n        \n        # Decoder part\n        features = self.decoder(features)\n        \n        # Decode mask\n        mask = self.mask(features[0])\n        mask = F.interpolate(mask, size=image.size()[2:], mode=\"bilinear\", align_corners=False)\n\n        output = {OUTPUT_DAMAGE_MASK_KEY: mask}\n        return output","f033a319":"def siamese_b4_unet64(input_channels=3, num_classes=5, dropout=0.2, pretrained=True):\n    encoder = B4Encoder(pretrained=pretrained)\n    if input_channels != 3:\n        encoder.change_input_channels(input_channels)\n\n    return SiameseUnetSegmentationModel(\n        encoder, num_classes=num_classes, unet_channels=[64, 128, 256], activation=ACT_SWISH, dropout=dropout\n    )","c233d296":"SIAMESE_BATCH_SIZE = 8","de5730ce":"best_checkpoint_fname, best_f1 = train_closure(\n    model=siamese_b4_unet64().cuda(),\n    batch_size=SIAMESE_BATCH_SIZE,\n    experiment_name=\"b4-siamese\")","462d92f1":"model = siamese_b4_unet64().cuda()\nmodel.load_state_dict(torch.load(best_checkpoint_fname)[\"model_state_dict\"])\n\nfor i in range(len(holdout_mask_post)):\n    gt_mask = read_mask(holdout_mask_post[i])\n    image_pre = read_image_rgb(holdout_img_pre[i])\n    image_post = read_image_rgb(holdout_img_post[i])\n    pred_mask = predict(model, image_pre, image_post)\n    \n    show_predictions(image_post, gt_mask, pred_mask)\n    \ndel model","3301db29":"# Optimizing the loss\n\nSo far we've trained our segmentation model using cross-entropy loss, which does not account balance of classes in the dataset. It is safe to assume most of the pixels in the images will belong to \"not building\" (label zero), but we don't have any clues on the distribution of the damaged class labels. Let's fix that.\n\nFirst, we compute counts of the pixels belonging to each label, and then - assign class labels in weighted cross-entropy loss.","f49e56ad":"Now it's time to write our dataset class. In PyTorch, we have a concept of the Dataset. It's contract to provide training & validation samples during the training loop. Each sample in our dataset is a pair of images (corresponding to pre- and post- event photo of the area) and corresponding damage mask. Pre- and post- images are already aligned. Typicaly, segmentation models works with only one image, but we can 'pretend' we have only one image by concatenating then via channels dimension. So instead of having two RGB images we will get one 6-channel RGBRGB image. If necessary, we can always split it back into two images inside the model.\n\n","5bfdbd55":"# Training time!\n\n![Catalyst](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/catalyst_logo.png)\n\nWe use Catalyst framework to instead of pure PyTorch training loop. Catalyst brings us numerous advantages:\n\n* Automatic support of mixed-precision training\n* Support of training on TPU\n* Tracking of metrics and automatic saving of the best model\n* Simple, clear training loop that fits into one screen\n\nThere are key componenets, that you need to pass to Catalyst runner - datasets, model, criterion and optimizer.","9349162f":"# Let's compare B4 to ResNet-18\n\nBoth B4 and ResNet-18 has similar number of parameters, so why not compare them?","e9cdb182":"Let's see how the predictions of the siamese model looks like ","d6687ca2":"## Albumentations crash course\n\nDocs: https:\/\/albumentations.ai\/docs\/\nExamples: https:\/\/github.com\/albumentations-team\/albumentations_examples\/blob\/master\/notebooks\/pytorch_semantic_segmentation.ipynb\n\nLet's augment single pair of images and see what happens:","3368284a":"**Disclaimer**: We have a little more than 9K pairs of images, which is quite a lot for training image segmentation model. However, Kaggle environment offers only dual-core CPU, which becomes a bottleneck in our training pipeline. So for the sake of staying within workshop time limit, we train our model using only a small part of the dataset - 300 train images and 50 test images.\n\nNo doubt, this small number of samples is anywhere close to enough number of training samples, however it is sufficient to evaluate different ideas in a controlled environment (With fixed random seed and determinictic GPU execution mode to ensure we're always training on exactly the same data samples). I encourage you to run these experiments on full training set and check whether our assumptions & findings holds true for larger number of samples.","53f7067f":"Awesome! Now, let's apply several augmentations sequentially. With Albumentations you can write really sophisticated augmentations. Here we use A.Compose to perform sequential composition of two augmentations: ","c1924009":"It seems, ResNet18 performs much worse, than B4. This comes with no surprise, but it is always worth checking different architectures. Maybe we long-enough training or bigger ResNet you can get similar score to the EfficientNet-s.","e4b3f9c9":"Despite vanilla U-Net is well-known and proven architecture, we can do better. With some help from pytorch-toolbelt we can do model surgery and attach pre-trained encoder like ResNet34 or EfficientNetB4 to U-Net decoder. By using pre-trained encoder we increase speed of the convergence and overall model's performance (Iglovikov et. al https:\/\/arxiv.org\/abs\/1801.05746). This can be also considered as transfer-learning appoach, where we train decoder part and fine-tuning encoder jointly. ","346d2174":"**Task**: Try adjusting parameters for `A.RandomShiftScale` and `A.RandomBrightnessContrast` and see how it affects the augmented image. Try to find boundary, where generated images are \"way too far off\" from the original. ","4059159e":"\n# Evaluation metric\n\nIn original competition target metric was weighted average of localization F1 and damage classification F1 scores. For sake of keeping the focus on modelling we will use **macro F1** on all classes (land, non-damaged building, light damage, major damage, destroyed). \n\n![F1](https:\/\/qph.fs.quoracdn.net\/main-qimg-71d99f821819c3777bfab9b461864f01)\n","6c516172":"# Visualizing predictions\n\nLet's take a look what predictions our model can make at this point. We use holdout dataset here to ensure we're not overfitting to the validation set.  ","7adf8431":"# Frameworks that we will use\n\nIn this workshop you will learn how to build advanced image segmentation model using PyTorch. To reduce the amount of boilerplate code, we will use [Catalyst](https:\/\/github.com\/catalyst-team\/catalyst) framework for quick & easy training of our models. Often, model architecture search is necessary to find the optimal model architecture, depth & width configuration of the network. To enable us to quicky change the encoder and decoder parameters of our model, we will use [pytorch-toolbelt](https:\/\/github.com\/BloodAxe\/pytorch-toolbelt) library which offers very flexible way to create building blocks of our model. And lastly, to ensure our model will not overfit, we will use [Albumentations](https:\/\/github.com\/albumentations-team\/albumentations) library to help us with image augmentations during the training.\n\n","92237b29":"Clearly, there is a strong class imbalance towards not-building class and under represented \"destroyed building\" class. Let's try to fix this by adding class weights to cross-entropy loss:","883184fa":"\n# Deep learning for satellite image processing\n\n## 9th Data Science UA Conference 2020\n### By [Eugene Khvedchenya](http:\/\/kaggle.com\/bloodaxe\/) (ekhvedchenya@gmail.com)\n\nOn this workshop you will learn how to apply deep learning to solve building damage assessment task. Given pair of images before and after natural disaster (wildfire, volcano eruption, flooding, etc.) one need to detect and classify level of damage dealt to buildings.\n\n## What you will learn\n\n*   How to build solid PyTorch training pipeline in Catalyst\n*   How to train image segmentation model\n*   How to build siamese dual-input segmentation model\n\n## What you will need \n\n* PyTorch Deep learning framework\n* Kaggle account (https:\/\/www.kaggle.com\/)\n\n## Hardward requirements\n\n* A regular laptop\/PC - all models training will happen in Kaggle Notebooks\n\n\n![front-image2.ccf9f50d.jpg](https:\/\/xview2.org\/img\/front-image2.ccf9f50d.jpg)\n\n## References\n\n* [xBD: A Dataset for Assessing Building Damage from Satellite Imagery](https:\/\/arxiv.org\/abs\/1911.09296)\n* [xView2 Data Challenge](https:\/\/xview2.org)\n* [Eugene Khvedchenya's xView2 Solution](https:\/\/github.com\/BloodAxe\/xView2-Solution)\n","37ad7ff0":"## Training augmentations\n\nPro Tip: Try playing with `A.Posterize`, `A.CLAHE`, `A.HueSaturationValue` for color augmentations and `A.MaskDropout`, `A.ElasticTransform` and `A.RandomGridShuffle` for spatial augmentations. See https:\/\/github.com\/albumentations-team\/albumentations for all showsace notebooks.","fb5d0894":"# Problem statement \n\nHigh resolution imagery is required to see the details of specific damage conditions in overhead imagery of a disaster area. However, the large areas affected by natural disasters combined with the huge numbers of pixels representing those areas make it laborious for analysts to search and evaluate specific buildings in a disaster area. The xView2 Challenge benchmarks automated computer vision capabilities for localizing and scoring the degree of damage to buildings before and after natural disasters.\n\n\n\n<table align=\"left\">\n    <tr>\n        <td>\n  <img src=\"https:\/\/xview2.org\/img\/ex-pre.1df3ea2c.jpg\" width=\"512\" \/><br><p align=\"center\">Image before the distater<\/p>\n        <\/td>\n        <td>\n  <img src=\"https:\/\/xview2.org\/img\/ex-post.fdc947c6.jpg\" width=\"512\" \/><br><p align=\"center\">Same area, after the disaster<\/p>\n        <\/td>\n    <\/tr>\n<\/table>\n\n\nOur goal is two-fold:\n* Detect buildings in the first image\n* Detect how much each building has been damaged after the disaster\n\n<img align=\"left\" src=\"https:\/\/xview2.org\/img\/ex-labeled.d0a0b19b.jpg\" \/><br\/>\n","c141ca50":"# Let's look at the data","02629b73":"# Build segmentation model\n\nThere are plenty of deep learning architectures for image segmentation. The vast majority of them follows encoder-decoder principle, in which Encoder part responsible for extracting semantic representation from the image, and task of the decoder is to reconstruct dense predictions map (mask). The most famous architectures are:\n\n* U-Net\n* Ternaus-Net\n* Link-Net\n* PSPNet\n* FPN\n* DeepLab\n* Panoptic DeepLab\n* HRNet\n* Hourglass Networks\n* U2Net\n\nEach architecture has it's own pros and cons. More specificaly - U-Net architecture tends to be quite accurate and can perform pixel-perfect predictions. PSPNet and DeepLab models shows best performance in dealing with segmentation of large number of classes. Panoptic DeepLab suitable for instance segmentation. \n\nIn this workshop we will use Encoder-Decoder segmentation model similar to U-Net. For the encoder part we will use pre-trained feature extractors from EfficientNet and ResNet families. Use of pre-trained encoders greatly improves training speed and final accuracy of the trained model.\n\nA U-NET is a convolutional neural network that was initially developed for biomedical image segmentation but has since proven its value for all image segmentation task no matter what specific topic.\n\n![U-Net](https:\/\/miro.medium.com\/max\/1838\/0*wPTXmX2_TqKwBy9i.png)\n(https:\/\/arxiv.org\/abs\/1505.04597)","1e80fa54":"# Conclusions & Home-work\n\nWe've build a proof-of-concept damage asessment model using pair of sattelite images, evaluated different models and learned how to tune our loss to account labels prevalence in a dataset. I encourage you to play with this notebook. Things that we didn't cover, but could be beneficial to try:\n\n1. Try different optimizers and learning rate schedulers\n1. Try Label smoothing with cross-entropy loss, Lovasz loss, Focal loss\n1. Try heavier encoders (B6 or even B7), different decoders (FPN, DeepLab, etc.)\n1. Try incorporating stride 2 feature map (layers=[0,1,2,3,4])\n1. Try training on all data (This may required running on your local hardware)\n\n\n# Thank You\n\nThank you for participating. Hope you enjoyed it and I hope to see you on my next workshops.\nI'd love to hear feedback from you. Don't hesitate to drop a line to ekhvedchenya@gmail.com.\n\nBest,\nEugene Khvedchenya\n\n","2fe3f674":"# Changing model architecture\n\nSo far we've been using 6-channel image as direct input to U-Net model. But there is another approach to this problem - hybrid siamese networks. In siamese networks we have two networks, that share same weights working in tanderm on two different inputs to compute comparable output vectors. We may adopt this approach for assessing damage buildings. In xView 2 Challenge I took the third place with exactly this approach. It proved to work much better than baseline.\n\nHere's an idea - we use image encoder independently for pre- and post- images and we concatenate features from the encoder path. In this case, encoder and decoder lears feature maps that are uncorrelated between pre- and post and it is the decoder part to find the differences in visual appearance of the buildings and based on this - assign damage label.\n\n![Siamese U-Net](https:\/\/github.com\/BloodAxe\/xView2-Solution\/releases\/download\/v1.0\/siamese_unet.png)","ec447070":"# Image Augmentations\n![Albumentations](https:\/\/camo.githubusercontent.com\/16b1b10ec83ffa96034bce1d8c7fc56b31c4f7a8\/68747470733a2f2f616c62756d656e746174696f6e732e72656164746865646f63732e696f2f656e2f6c61746573742f5f7374617469632f6c6f676f2e706e67)\n\nSince we work only with small portion of the training set, it's important to more diversity to our images by adding on-the-fly image augmentations. We can safely rotate our images and masks, slight change their scale, brightness and color during training. This would increase model's generalization and prevent overfitting. The only trick is not to forget augment image and corresponding mask consistently. \nThe [albumentations](https:\/\/github.com\/albumentations-team\/albumentations) library works with images and masks at the same time, which is what we need. Let's take a look at the example:\n\n"}}