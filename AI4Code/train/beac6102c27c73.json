{"cell_type":{"375074d0":"code","25017039":"code","4b215504":"code","7ee9bb20":"code","68e3c5e3":"code","3d798c60":"code","469270f4":"code","4cefce01":"code","a9d8e2f0":"code","3d0e83d8":"code","1cc5a1f7":"code","1a4e852f":"code","9b2a9210":"code","41a283a3":"code","a8c28343":"markdown","5f406b56":"markdown","5b9d4af1":"markdown","b9f12281":"markdown","ddc23ebe":"markdown","3c38c19e":"markdown","184b0c6b":"markdown","584a7e76":"markdown","7677c21e":"markdown","ea42d90f":"markdown","a5116cb3":"markdown","ea985246":"markdown","b4e278f7":"markdown","1e20843b":"markdown","daa9b3a9":"markdown","e80b9da8":"markdown","2c6ce798":"markdown","7cbdc47b":"markdown","e277081a":"markdown","fc226e8d":"markdown"},"source":{"375074d0":"#Installing the dependencies\n!pip install gym\n!apt-get install python-opengl -y\n!apt install xvfb -y","25017039":"!pip install gym[atari]\n!pip install pyvirtualdisplay\n!pip install piglet","4b215504":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","7ee9bb20":"# This code creates a virtual display to draw game images on. \n# If you are running locally, just ignore it\nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n    !bash ..\/xvfb start\n    %env DISPLAY=:1","68e3c5e3":"import gym\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\ngymlogger.set_level(40) # error only\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfrom IPython import display as ipythondisplay","3d798c60":"def show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","469270f4":"RANDOM_SEED=1\nN_EPISODES=500\n\n# random seed (reproduciblity)\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# set the env\nenv=gym.make(\"CartPole-v1\") # env to import\n#env=wrap_env(env) #To enable video, just do \"env = wrap_env(env)\"\"\nenv.seed(RANDOM_SEED)\nenv.reset() # reset to env","4cefce01":"class REINFORCE:\n\n  def __init__(self, env, path=None):\n    self.env=env #import env\n    self.state_shape=env.observation_space.shape # the state space\n    self.action_shape=env.action_space.n # the action space\n    self.gamma=0.99 # decay rate of past observations\n    self.alpha=1e-4 # learning rate in the policy gradient\n    self.learning_rate=0.01 # learning rate in deep learning\n    \n    if not path:\n      self.model=self._create_model() #build model\n    else:\n      self.model=self.load_model(path) #import model\n\n    # record observations\n    self.states=[]\n    self.gradients=[] \n    self.rewards=[]\n    self.probs=[]\n    self.discounted_rewards=[]\n    self.total_rewards=[]","a9d8e2f0":"  def remember(self, state, action, action_prob, reward):\n    '''stores observations'''\n    encoded_action=self.hot_encode_action(action)\n    self.gradients.append(encoded_action-action_prob)\n    self.states.append(state)\n    self.rewards.append(reward)\n    self.probs.append(action_prob)\n  \n  def hot_encode_action(self, action):\n    '''encoding the actions into a binary list'''\n\n    action_encoded=np.zeros(self.action_shape, np.float32)\n    action_encoded[action]=1\n\n    return action_encoded","3d0e83d8":"  def _create_model(self):\n    ''' builds the model using keras'''\n    model=Sequential()\n\n    # input shape is of observations\n    model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n    # add a relu layer \n    model.add(Dense(12, activation=\"relu\"))\n\n    # output shape is according to the number of action\n    # The softmax function outputs a probability distribution over the actions\n    model.add(Dense(self.action_shape, activation=\"softmax\")) \n    model.compile(loss=\"categorical_crossentropy\",\n            optimizer=Adam(lr=self.learning_rate))\n        \n    return model","1cc5a1f7":"  def get_action(self, state):\n      '''samples the next action based on the policy probabilty distribution \n      of the actions'''\n      # transform state \n      state=state.reshape([1, state.shape[0]])\n      # get action probably\n      action_probability_distribution=self.model.predict(state).flatten()\n      # norm action probability distribution\n      action_probability_distribution\/=np.sum(action_probability_distribution)\n    \n      # sample action\n      action=np.random.choice(self.action_shape,1,\n                            p=action_probability_distribution)[0]\n\n      return action, action_probability_distribution","1a4e852f":"  def get_discounted_rewards(self, rewards): \n\n    '''Use gamma to calculate the total reward discounting for rewards\n    Following - \\gamma ^ t * Gt'''\n    \n    discounted_rewards=[]\n    cumulative_total_return=0\n    # iterate the rewards backwards and and calc the total return \n    for reward in rewards[::-1]:      \n      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n      discounted_rewards.insert(0, cumulative_total_return)\n\n    # normalize discounted rewards\n    mean_rewards=np.mean(discounted_rewards)\n    std_rewards=np.std(discounted_rewards)\n    norm_discounted_rewards=(discounted_rewards-\n                          mean_rewards)\/(std_rewards+1e-7) # avoiding zero div\n    \n    return norm_discounted_rewards","9b2a9210":"def update_policy(self):\n    '''Updates the policy network using the NN model.\n    This function is used after the MC sampling is done - following\n    \\delta \\theta = \\alpha * gradient + log pi'''\n      \n    # get X\n    states=np.vstack(self.states)\n\n    # get Y\n    gradients=np.vstack(self.gradients)\n    rewards=np.vstack(self.rewards)\n    discounted_rewards=self.get_discounted_rewards(rewards)\n    gradients*=discounted_rewards\n    gradients=self.alpha*np.vstack([gradients])+self.probs\n\n    history=self.model.train_on_batch(states, gradients)\n    \n    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n\n    return history","41a283a3":"   def train(self, episodes, rollout_n=1, render_n=50):\n    '''train the model\n        episodes - number of training iterations \n        rollout_n- number of episodes between policy update\n        render_n - number of episodes between env rendering ''' \n    \n    env=self.env\n    total_rewards=np.zeros(episodes)\n\n    for episode in range(episodes):\n      # each episode is a new game env\n      state=env.reset()\n      done=False          \n      episode_reward=0 #record episode reward\n      \n      while not done:\n        # play an action and record the game state & reward per episode\n        action, prob=self.get_action(state)\n        next_state, reward, done, _=env.step(action)\n        self.remember(state, action, prob, reward)\n        state=next_state\n        episode_reward+=reward\n        #print(\"Episode_reward{}\".format(episode_reward))\n        '''\n        if episode%render_n==0: ## render env to visualize.\n          plt.imshow(env.render('rgb_array'))\n          display.clear_output(wait=True)\n          display.display(plt.gcf())'''\n        if done:\n          # update policy \n          if episode%rollout_n==0:\n            history=self.update_policy()\n\n      total_rewards[episode]=episode_reward\n    model=self.model\n    model.save('model.h5')\n    self.total_rewards=total_rewards\n\n\nplt.figure(figsize=(4, 3))\ndisplay.clear_output(wait=True)\nAgent=REINFORCE(env)\nAgent.train(episodes=500)","a8c28343":"#THE POLICY GRADIENT\n\nThe policy gradient is the mechanism by which action probabilities produced by the policy are changed. If the return $R_t(\u03c4)$ > 0, then the probability of the action $\u03c0_\u03b8 (a_t|s_t)$ is increased; conversely, if the return $R_t$(\u03c4) < 0, then the probability of the action $\u03c0_\u03b8 (a_t|s_t)$ is decreased. Over the course of many updates, the policy will learn to produce actions that result in high $R_t(\u03c4)$.\n\nTo maximize the objective, we perform gradient ascent on the policy parameters \u03b8.To improve on the objective1, compute the policy gradient, and use it to update the parameters as shown below:\n\n1.Compute the policy gradient\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-04%20at%2012.11.50%20AM.png?raw=true)     \n                        Equation 2.3\n\nThe term $\u25bd_\u03b8J(\u03c0_\u03b8)$ is known as the policy gradient. The term $\u03c0_\u03b8(a_t|s_t)$ is the probability of the action taken by the agent at time step t. The action is sampled from the policy, $a_t$ \u223c $\u03c0_\u03b8(s_t)$. The right-hand side of the equation states that the gradient of the log probability of the action with respect to \u03b8 is multiplied by return $R_t(\u03c4)$. Here I have skipped the proof of above equation but you can look at https:\/\/danieltakeshi.github.io\/2017\/03\/28\/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients\/\n\n2.Update the policy parameters \u03b8\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-04%20at%2012.13.44%20AM.png?raw=true)                           \n                            Equation 2.4\n\n\u03b1 is a scalar known as the learning rate; it controls the size of the parameter update","5f406b56":"![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-04%20at%204.47.48%20AM.png?raw=true)\n\nFirst, initialize the learning rate \u03b1 and construct a policy network \u03c0\u03b8 with randomly initialized weights.\nNext, iterate for multiple episodes as follows: use the policy network \u03c0\u03b8 to generate a trajectory $\u03c4 = s_0, a_0, r_0, . . . , s_T, a_T, r_T$ for an episode. Then, for each time step t in the trajectory, compute the return $R_t(\u03c4)$. Next, use $R_t(\u03c4)$ to estimate the policy gradient. Sum the policy gradients for all time steps, then use the result to update the policy network parameters \u03b8","5b9d4af1":"This is the Part-2 of Deep Reinforcement Learning Notebook series.***In this Notebook I have introduced introduces the first Policy Gradient method known as REINFORCE***.\n\n\nThe Notebook series is about Deep RL algorithms so itexcludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only.\n","b9f12281":"**Below code setups the environment required to run the game.**","ddc23ebe":"The REINFORCE algorithm involves learning a parametrized policy that produces action probabilities given states. Agents use this policy directly to act in an environment. The key idea is that during learning, actions that resulted in good outcomes should become more probable (these actions are positively reinforced). Conversely, actions that resulted in bad outcomes should become less probable. If learning is successful, over the course of many iterations action probabilities produced by the policy shift to distribution that results in a good performance in an environment. Action probabilities are changed by following the policy gradient, hence REINFORCE \nis known as a policy gradient algorithm.\nThe algorithm needs three components:\n1. a parametrized policy\n2. an objective to be maximized\n3. a method for updating the policy parameters","3c38c19e":"##Action Selection\nThe get_action method guides its action choice. It uses the neural network to generate a normalized probability distribution for a given state. Then, it samples its next action from this distribution.","184b0c6b":"**Disadvantage of Reinforce Algorithm and its few remedies**\n\nDisadvantage=>\n\nWhen using Monte Carlo sampling, the policy gradient estimate may have high variance because the returns can vary significantly from trajectory to trajectory. This is due to three factors. First, actions have some randomness because they are sampled from a probability distribution. Second, the starting state may vary per episode. Third, the environment transition function may be stochastic.\n\nRemedy=>\n\nOne way to reduce the variance of the estimate is to modify the returns by subtracting a suitable action-independent baseline as shown below(Equation 2.5)\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-04%20at%204.59.45%20AM.png?raw=true)\n\nEquation 2.5\n\nOne option for the baseline is the value function V. This choice of baseline motivates the Actor-Critic algorithm(Which we discuss later in this notebook series)\n\nAn alternative is to use the mean returns over the trajectory.Let $b=\\sum_{t=0}^T R_t(\u03c4)$\n\nTo see why this is useful, consider the case where all the rewards for an environment are negative. Without a baseline, even when an agent produces a very good action, it gets discouraged because the returns are always negative. Over time, this can still result in good policies since worse actions will get discouraged even more and thus indirectly increase the probabilities of better actions. However, it can lead to slower learning because probability adjustments can only be made in a single direction. Likewise, the converse happens in environments where all the rewards are positive. A more effective way is to both increase and decreases the action probabilities directly. This requires having both positive and negative returns.","584a7e76":"The REINFORCE algorithm numerically estimates the policy gradient using Monte Carlo sampling.\nMonte Carlo sampling refers to any method which uses random sampling to generate data which is used to approximate a function. In essence, it is just \u201capproximation with random sampling\u201d\n\nThe expectation $E_{\u03c4\u223c\u03c0\u03b8}$ implies that as more trajectories \u03c4 s are sampled using a policy $\u03c0_\u03b8$ and averaged, it approaches the actual policy gradient $\u25bd_\u03b8J(\u03c0_\u03b8)$. Instead of sampling many trajectories per policy, we can sample just one as shown in equation 2.3.This is how policy gradient is implemented \u2014 as a Monte Carlo estimate over sampled trajectories.","7677c21e":"##Creating a Neural Network Model\nI chose to use a neural network to implement this agent. The network is a simple feedforward network with a few hidden layers. The output layer incorporates a softmax activation. The softmax function takes in logit scores and outputs a vector that represents the probability distributions of a list of potential outcomes.","ea42d90f":"##Updating the Policy\nFollowing each Monte-Carlo episode, the model uses the data collected to update the policy parameters. Recall the last step shown in the pseudo-code above. Here, training the neural network updates the policy. The network fits a vector of states to a vector of the gradients multiplied by the discounted rewards and the learning rate. This step facilitates the stochastic gradient ascent optimization.\n","a5116cb3":"# Here is the implementation of Reinforce Algorithm","ea985246":"##Constructing the Reward\nThe REINFORCE model includes a discounting parameter, \ud835\udefe, that governs the long term reward calculation. Using gamma, the model discounts rewards from the early stages of the game. This calculation ensures that longer episodes would award a state-action pair greater than shorter ones. This function returns the normalized vector of discounted rewards.","b4e278f7":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n","1e20843b":"#Defining the REINFORCE Class\nAt initiation, the REINFORCE object sets a few parameters. First, is the environment in which the model learns and its properties. Second, are both the parameters of the REINFORCE algorithm \u2014 Gamma (\ud835\udefe) and alpha (\ud835\udefc). Gamma, as discussed above is the decay rate of past observations and alpha is the learning rate by which the gradients affect the policy update. Lastly, it sets the learning rate for the neural network. In addition, this snippet includes the saved space for recording the observations during the game.\n","daa9b3a9":"##Training the model\nThis method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a sequence ends, the model is using the recorded observations to update the policy.","e80b9da8":"# THE OBJECTIVE FUNCTION\nAn objective can be understood as an agent\u2019s goal, such as winning a game or getting the highest score possible.that an agent acting in an environment generates a trajectory, which contains a sequence of rewards along with the states and actions. A trajectory is denoted \u03c4 = $s_0, a_0,r_0,...,s_T,a_T,r_T$ .The return of a trajectory $R_t$(\u03c4) is defined as a discounted sum of rewards from time step t to the end of a trajectory as shown below(Equation 2.1).\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-03%20at%2011.14.35%20PM.png?raw=true)      \n                          Equation 2.1\n\nHere we can see that the sum starts from time step t, but the power that the discount factor \u03b3 is raised to starts from 0 when summing for return, hence we need to offset the power by the starting time step t using t\u2032 \u2013 t.\n\nThe natural question which might now arise is that why do we need the discounted return? Why not just simply add all returns? The Answer is the return is task-oriented i.e. in some cases you might give equal priority to every return and in the other one return is more important than others. Different values of gamma(\u03b3) have different effects on the agent. If we keep the value of gamma less than one then the agent is short-sighted i.e. it gives more priority to initial rewards than rewards obtained later in the time. And similarly, if the value of gamma if greater than one then the agent is far-sighted i.e. it gives more priority to rewards that are obtained later in time rather than initial rewards. The value of gamma depends on the tasks of the agent. We can have gamma equal to one in case we want to give equal priority to all rewards.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-04%20at%2012.00.27%20AM.png?raw=true)                         \n                          Equation 2.2\n\nHere the objective is the expected return overall complete trajectories generated by an agent. The above equation(Equation 2.2) says that the expectation is calculated over many trajectories sampled from a policy, that is, \u03c4 \u223c $\u03c0_\u03b8$. This expectation approaches the true value as more samples are gathered, and it is tied to the specific policy $\u03c0_\u03b8$ used.\n","2c6ce798":"##Defining useful utility methods\nThe REINFORCE agent object uses a couple of utility methods. The first, hot_encode_action, encodes the actions into a one-hot-encoder format (read more about what is one-hot-encoding and why is it a good idea, here(https:\/\/medium.com\/@michaeldelsolewhat-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179)). And the second, remember, records the observations of each step.","7cbdc47b":"This part ensures the reproducibility of the code below by using a random seed.\n","e277081a":"# A Parametrized Policy\nA policy \u03c0 is a function which maps states to action probabilities, which is used to sample an action a \u223c \u03c0(s). In REINFORCE, an agent learns a policy and uses this to act in an environment. A good policy is one that maximizes the cumulative discounted rewards. The key idea in the algorithm is to learn a good policy, and this means doing function approximation. Neural networks are powerful and flexible function approximators, so we can represent a policy using a deep neural network consisting of learnable parameters \u03b8. This is often referred to as a policy network $\u03c0_\u03b8$. We say that the policy is parametrized by \u03b8. Each specific set of values of the parameters of the policy network represents a particular policy. Formulated in this way, the process of learning a good policy corresponds to searching for a good set of values for \u03b8. For this reason, the policy network must be differentiable. We will see later in this notebook that the mechanism by which the policy is improved is through gradient ascent in parameter space.\n  ","fc226e8d":"# Below is the implementation in code of this algorithm on a simple example of Cart-Pole where agent tries to balance the pole on the cart"}}