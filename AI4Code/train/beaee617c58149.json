{"cell_type":{"c7254107":"code","e6e0e144":"code","7701fae0":"code","b81be84b":"code","4f557175":"code","01fe7e22":"code","6d20a2c5":"code","d31c6d07":"code","e210e605":"code","42de6922":"code","e929b2c8":"code","13602f33":"code","874cac43":"code","4e8aa1dd":"code","329e6883":"code","5d85e38a":"code","1cc2c08b":"code","06342efb":"code","1b0582f3":"code","f2562d39":"code","451fceb5":"code","f230dc59":"code","f314cf2a":"code","333a002b":"code","7f00d170":"code","43ed2bc4":"code","b804d45c":"code","d7f27031":"code","b61e06d1":"code","c0075f9b":"code","3d767456":"code","1707a137":"code","6da95bc5":"code","c787f6e1":"code","b8d2e720":"code","fc5e7b05":"code","129eb271":"code","0e853a70":"code","4d5c98cc":"code","86dd70e4":"code","4dc584df":"code","05587943":"code","98b87fb9":"code","b473310a":"code","d076fc65":"code","92e79eb0":"code","ab917a00":"code","737f4971":"markdown","46373ebf":"markdown","43378d41":"markdown","34fe6a13":"markdown","72766dbd":"markdown","84603f95":"markdown","c4b59207":"markdown","ba0364bc":"markdown","b882e644":"markdown","8775b2ae":"markdown","3e6dd356":"markdown","d100af23":"markdown","1c4412b2":"markdown","1f385caa":"markdown","a2e4448f":"markdown","58f12568":"markdown","6e0d9dce":"markdown"},"source":{"c7254107":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble  import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble  import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.linear_model import Perceptron\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.ensemble import VotingClassifier\n%matplotlib inline\nfrom sklearn import model_selection\nimport os\n\nprint(os.listdir(\"..\/input\"))","e6e0e144":"df=pd.read_csv(\"..\/input\/diabetes.csv\")","7701fae0":"df.head()","b81be84b":"df.isna().any() # No NAs\n","4f557175":"print(df.dtypes)","01fe7e22":"dataset2 = df.drop(columns = ['Outcome'])\n\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(dataset2.shape[1]):\n    plt.subplot(6, 3, i + 1)\n    f = plt.gca()\n    f.set_title(dataset2.columns.values[i])\n\n    vals = np.size(dataset2.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(dataset2.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","6d20a2c5":"# Calculate the median value for BMI\nmedian_bmi = df['BMI'].median()\n# Substitute it in the BMI column of the\n# dataset where values are 0\ndf['BMI'] = df['BMI'].replace(\n    to_replace=0, value=median_bmi)","d31c6d07":"median_bloodp = df['BloodPressure'].median()\n# Substitute it in the BloodP column of the\n# dataset where values are 0\ndf['BloodPressure'] = df['BloodPressure'].replace(\n    to_replace=0, value=median_bloodp)","e210e605":"# Calculate the median value for PlGlcConc\nmedian_plglcconc = df['Glucose'].median()\n# Substitute it in the PlGlcConc column of the\n# dataset where values are 0\ndf['Glucose'] = df['Glucose'].replace(\n    to_replace=0, value=median_plglcconc)","42de6922":"# Calculate the median value for SkinThick\nmedian_skinthick = df['SkinThickness'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['SkinThickness'] = df['SkinThickness'].replace(\n    to_replace=0, value=median_skinthick)","e929b2c8":"# Calculate the median value for SkinThick\nmedian_skinthick = df['Insulin'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['Insulin'] = df['Insulin'].replace(\n    to_replace=0, value=median_skinthick)","13602f33":"dataset2 = df.drop(columns = ['Outcome'])\n\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(dataset2.shape[1]):\n    plt.subplot(6, 3, i + 1)\n    f = plt.gca()\n    f.set_title(dataset2.columns.values[i])\n\n    vals = np.size(dataset2.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(dataset2.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","874cac43":"dataset2.corrwith(df.Outcome).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Outcome\", fontsize = 15,\n        rot = 45, grid = True)\n","4e8aa1dd":"## Correlation Matrix\n\n\n# Compute the correlation matrix\ncorr = dataset2.corr()\nsns.heatmap(corr,annot=True)\n","329e6883":"X = df.drop(['Outcome'],axis=1)\ny = df['Outcome']","5d85e38a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state = 42)","1cc2c08b":"min_train = X_train.min()\nrange_train = (X_train - min_train).max()\nX_train_scaled = (X_train - min_train)\/range_train","06342efb":"min_test = X_test.min()\nrange_test = (X_test - min_test).max()\nX_test_scaled = (X_test - min_test)\/range_test","1b0582f3":"from sklearn.linear_model import LogisticRegression\nlogi = LogisticRegression()\nlogi.fit(X_train_scaled, y_train)","f2562d39":"\ny_predict = logi.predict(X_test_scaled)\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score\nroc=roc_auc_score(y_test, y_predict)\n\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nresults = pd.DataFrame([['Logistic Regression', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","451fceb5":"from xgboost import XGBClassifier\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train_scaled, y_train)","f230dc59":"y_predict = xgb_classifier.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['XGBOOST', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","f314cf2a":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train_scaled, y_train)","333a002b":"y_predict = random_forest.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['Random Forest', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","7f00d170":"sgd = SGDClassifier(max_iter=1000)\n\nsgd.fit(X_train_scaled, y_train)\ny_predict = sgd.predict(X_test_scaled)","43ed2bc4":"roc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['SGD', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","b804d45c":"adaboost =AdaBoostClassifier()\nadaboost.fit(X_train_scaled, y_train)\ny_predict = adaboost.predict(X_test_scaled)","d7f27031":"roc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['Adaboost', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","b61e06d1":"gboost =GradientBoostingClassifier()\ngboost.fit(X_train_scaled, y_train)\ny_predict = gboost.predict(X_test_scaled)","c0075f9b":"roc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['Gboost', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","3d767456":"knn = KNeighborsClassifier(n_neighbors = 7)\nknn.fit(X_train_scaled, y_train)\ny_predict = knn.predict(X_test_scaled)","1707a137":"roc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['KNN7', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","6da95bc5":"from sklearn.svm import SVC \n\n\nsvc_model = SVC(kernel='linear')\nsvc_model.fit(X_train, y_train)\ny_predict = svc_model.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict)\nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\n\nmodel_results = pd.DataFrame([['SVC Linear', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","c787f6e1":"clf1=LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3=AdaBoostClassifier()\nclf4=XGBClassifier()\nclf5=SGDClassifier(max_iter=1000,loss='log')\nclf6=KNeighborsClassifier(n_neighbors = 7)\nclf7=GradientBoostingClassifier()\n\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('ada', clf3),('xgb',clf4),('sgd',clf5),('knn',clf6),('gboost',clf7)], voting='soft', weights=[1,1,2,2,1,3,2])\neclf1.fit(X_train_scaled,y_train)","b8d2e720":"eclf_predictions = eclf1.predict(X_test_scaled)\nacc = accuracy_score(y_test, eclf_predictions)\nprec = precision_score(y_test, eclf_predictions)\nrec = recall_score(y_test, eclf_predictions)\nf1 = f1_score(y_test, eclf_predictions)\nfrom sklearn.metrics import roc_auc_score\nroc=roc_auc_score(y_test, eclf_predictions)\nmodel_results = pd.DataFrame([['Voting Classifier ', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","fc5e7b05":"clf1=LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3=AdaBoostClassifier()\nclf4=XGBClassifier()\nclf5=SGDClassifier(max_iter=1000,loss='log')\nclf6=GradientBoostingClassifier()\nknn=KNeighborsClassifier(n_neighbors = 7)\n\n\nsclf = StackingClassifier(classifiers=[clf1,clf2, clf3, clf4,clf5,clf6], \n                          meta_classifier=knn)\n\nprint('10-fold cross validation:\\n')\n\nfor clf, label in zip([clf1,clf2, clf3, clf4,clf5,clf6, sclf], \n                      ['Logistic Regression'\n                       'Random Forest', \n                       'Adaboost',\n                          'XGB','SGD','Gradient',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X_test_scaled, y_test,\n                                              cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","129eb271":"import time\nparameters = {\n        'min_child_weight': [1, 5,7, 10],\n        'max_depth': [2,3, 5,7,10,12],\n        'n_estimators':[10,50,100,200]\n        }\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = xgb_classifier, # Make sure classifier points to the RF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 5,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search.fit(X_train_scaled, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n","0e853a70":"grid_search.best_params_","4d5c98cc":"grid_predictions = grid_search.predict(X_test_scaled)","86dd70e4":"from sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, grid_predictions)","4dc584df":"sns.heatmap(cm, annot=True)","05587943":"acc = accuracy_score(y_test, grid_predictions)\nprec = precision_score(y_test, grid_predictions)\nrec = recall_score(y_test, grid_predictions)\nf1 = f1_score(y_test, grid_predictions)\nfrom sklearn.metrics import roc_auc_score\nroc=roc_auc_score(y_test, grid_predictions)\nmodel_results = pd.DataFrame([['XGBoost Optimized', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","98b87fb9":"import time\nparameters = {\n         'C':[0.1, 1, 10, 100,1000],\n        'gamma':[1, 0.1, 0.01, 0.001,0.0001],\n    'kernel':['rbf','linear']\n        }\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = svc_model, # Make sure classifier points to the RF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 5,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search.fit(X_train_scaled, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))","b473310a":"grid_search.best_params_","d076fc65":"grid_predictions = grid_search.predict(X_test_scaled)\nacc = accuracy_score(y_test, grid_predictions)\nprec = precision_score(y_test, grid_predictions)\nrec = recall_score(y_test, grid_predictions)\nf1 = f1_score(y_test, grid_predictions)\nfrom sklearn.metrics import roc_auc_score\nroc=roc_auc_score(y_test, grid_predictions)\nmodel_results = pd.DataFrame([['SVC Optimized', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","92e79eb0":"import lightgbm\ntrain_data = lightgbm.Dataset(X_train_scaled, label=y_train)\ntest_data = lightgbm.Dataset(X_test_scaled, label=y_test)\n\n\n#\n# Train the model\n#\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'max_bin': 200,\n    'boosting': 'gbdt',\n    'num_leaves': 10,\n    'bagging_freq': 20,\n    'learning_rate': 0.003,\n    'verbose': 0\n}\n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)","ab917a00":"y_predict = model.predict(X_test_scaled)\n#convert into binary values\nfor i in range(0,154):\n    if y_predict[i]>=.5:       # setting threshold to .5\n       y_predict[i]=1\n    else:  \n       y_predict[i]=0\n    \nacc = accuracy_score(y_test, y_predict)\nprec = precision_score(y_test, y_predict)\nrec = recall_score(y_test, y_predict)\nf1 = f1_score(y_test, y_predict)\nfrom sklearn.metrics import roc_auc_score\nroc=roc_auc_score(y_test, y_predict)\nmodel_results = pd.DataFrame([['Light GBM', acc,prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","737f4971":"As we have seen in above histograms there are zero entries in features **Glucose,BloodPressure,SkinThickness,Insulin and BMI**.So, we will fill it with median so that there will be uniform distribution of data before fitting into the machine learning models.","46373ebf":"# Splitting the Dataset","43378d41":"So,I have transformed most of the columns having zero entries, except some value such as number of times pregnant can make sense to be zero .Now lets watch the distribution how it looks like now.","34fe6a13":"So, from stacking we have achieved accuracy of about 74%.","72766dbd":"Splitting the dataset is a very important step for supervised machine learning models. This step split the dataset into Train set and Test Set.We will use Train set to train the model (ignoring the column target variable), then we use the trained model to make predictions on new data (which is the test dataset, not part of the training set) and compare the predicted value with the pre assigned label.","84603f95":"The maximum accuracy we have achieved using Voting weighted Average classifier is 75.32% which is equivalent to XGBoost.","c4b59207":"As seeing from the above graph, it is inferred that Target variable which is Outcome is highly correlated with Glucose, BMI, Age and Pregnancies.\nWhile Blood pressure and Skin Thickness is not much correlated with target variable but we as of now we will not remove these features and train the model with all the features.","ba0364bc":"# Voting Classifier\n\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n\nIt works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nWe will be using weighted Voting Classifier. We will assign weights to the classifiers according to their accuracies. So the classifier with single accuracy will be assigned the highest weight and so on.\n\n<img src=\"https:\/\/image.ibb.co\/bEOhML\/majority-voting.png\">****","b882e644":"**1. Applying Machine Learning Models**","8775b2ae":"We have achieved 6% accuracy increase in SVM with parameter tuning from 64% to 72% whereas other metric parameters also achieved good result.","3e6dd356":"# Finding Correlation among feature variables","d100af23":"# Feature Scaling","1c4412b2":"# EARLY DIABETES PREDICTION : PIMA INDIAN DIABETES","1f385caa":"# Applying Machine Learning Models","a2e4448f":"As there is uneven distribution of data in the dataset, so we will perform Normalization or feature scaling so that all the features in Training and Test set have values between 0 and 1. So, that machine learning models provide greater accuracy. ","58f12568":"# Stacking\n\nStacking is a way of combining multiple models, that introduces the concept of a meta learner. It is less widely used than bagging and boosting. Unlike bagging and boosting, stacking may be (and normally is) used to combine models of different types. \n\nIn stacking, the combining mechanism is that the output of the classifiers (Level 0 classifiers) will be used as training data for another classifier (Level 1 classifier) to approximate the same target function. Basically, you let the Level 1 classifier to figure out the combining mechanism.\n\nThe point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target. This final model is said to be stacked on the top of the others, hence the name. Thus you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model.\n\n<img src=\"https:\/\/image.ibb.co\/d5ySo0\/stackingclassification-overview.png\">","6e0d9dce":"# Correlation with Target variable"}}