{"cell_type":{"74f7470d":"code","d2ace96c":"code","05714933":"code","7c29d6e6":"code","35bb193f":"code","60d256cf":"code","9bebb539":"code","524f90d2":"code","2bee10bd":"code","f0444de8":"code","aa9bdf8d":"code","552dbed1":"code","5cc47b9a":"code","52482ccc":"code","ce7a2e17":"code","ec4cb3dc":"code","3aef8746":"code","76be07f3":"code","2a97f536":"code","42ec40f9":"code","f3a02d89":"code","5653f2e4":"code","d82d4eb1":"code","1084c300":"code","34aefba8":"code","02a44f62":"code","effbc0fa":"code","0e597031":"code","d095e65b":"code","cbe5e8c6":"code","7723333b":"code","cad57b3e":"code","fdad8d25":"code","0b7ecc89":"code","2ec23079":"code","fc359290":"code","33ed289d":"code","29011aff":"code","55653421":"code","e7f90515":"code","16bc6c66":"code","efc325ad":"code","9ddaa20e":"code","0d63f105":"code","fc2ac8d7":"code","ab1396cd":"code","134dc561":"code","8ceb25ce":"code","8efda87b":"code","e6582d2a":"code","d2bba9bb":"code","1561b679":"code","97104a45":"code","2f0eecf6":"code","d6a4db30":"code","22ecfe19":"code","082f3c3b":"code","387bbd79":"code","b7f96f77":"code","65c9ebc7":"code","89c4e401":"code","98717b6c":"code","a26c891a":"code","befb1367":"code","ff0cf396":"code","5bb1233a":"code","24090d97":"code","7e514313":"code","7e0803bc":"code","cd543978":"code","1aa2583a":"code","3e6cd34c":"code","0af1aa19":"code","2de41194":"code","51b8550e":"code","22805e75":"code","1a73e2a8":"code","aad8afe8":"code","b7dc390b":"code","0f7751ce":"code","215b8d64":"code","01a7a7ed":"code","4829f51f":"code","7ba562b3":"code","c43f7568":"code","dfa73c8d":"code","22b89d0f":"code","6d0e5dbc":"code","40f2659b":"code","2858813c":"code","cccfecbf":"code","2e3ad8d3":"code","bfb23198":"code","14c28180":"code","c32eed8d":"code","4eb33c11":"code","47d0e420":"code","41f06fb2":"code","1bf6f4d6":"code","695aa6b1":"code","1c24ae7f":"code","852200d4":"code","c6cd1910":"code","d2b99caf":"code","94f2b881":"code","2c557591":"code","514fffdf":"code","59dd9c0a":"code","1f37b047":"code","1a1ebf67":"code","4d8db5ac":"code","54212d39":"code","0ed20f13":"code","c56e9123":"code","7723f23b":"code","0340a194":"code","2e5b9c83":"code","67d02979":"code","af47aed6":"code","d3592f37":"markdown","2647d062":"markdown","e4c728da":"markdown","9d95e22c":"markdown","c409ddfb":"markdown","46ce0d7b":"markdown","1bec6443":"markdown","1aa11376":"markdown","49194a4d":"markdown","0b0bb3aa":"markdown","5ce3b8de":"markdown","79d009a6":"markdown","5e1af8b4":"markdown","062f1dde":"markdown","60e1c36e":"markdown","c4623c07":"markdown","a413910a":"markdown","cb15fd0f":"markdown","ca05e998":"markdown","62438bcb":"markdown","445ee82a":"markdown","7aa56b43":"markdown","97f7fa25":"markdown","afbd0ff1":"markdown","8c23b100":"markdown","748caf82":"markdown","f09d19b0":"markdown","ee8a492a":"markdown","120dd300":"markdown","ed26c05e":"markdown","81449814":"markdown","781088b5":"markdown","6e3d80bf":"markdown","fce9fe37":"markdown","d4962d62":"markdown","cf19b024":"markdown","9d2c8614":"markdown","9548f249":"markdown","b40b25cd":"markdown","513ada56":"markdown","c9ae3ae5":"markdown"},"source":{"74f7470d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d2ace96c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","05714933":"sns.set_style('whitegrid')","7c29d6e6":"dat_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'","35bb193f":"train_raw = pd.read_csv(f'{dat_path}train.csv')\ntest_raw = pd.read_csv(f'{dat_path}test.csv')","60d256cf":"sample_sub = pd.read_csv(f'{dat_path}sample_submission.csv')","9bebb539":"train_raw.shape","524f90d2":"test_raw.shape","2bee10bd":"train_raw.dropna(axis=1)","f0444de8":"test_raw.dropna(axis=1)","aa9bdf8d":"plt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['font.size'] = 16","552dbed1":"plt.bar(np.arange(test_raw.shape[1]), test_raw.isna().sum() \/ len(test_raw))\nplt.ylabel('Fraction of Rows with NaN Value')\nplt.xlabel('Column Index')","5cc47b9a":"plt.bar(np.arange(train_raw.shape[1]), train_raw.isna().sum() \/ len(train_raw))\nplt.ylabel('Fraction of Rows with NaN Value')\nplt.xlabel('Column Index')","52482ccc":"test_raw.shape","ce7a2e17":"test = test_raw.dropna(thresh=test_raw.shape[0]*0.9, axis=1)\ntrain = train_raw.dropna(thresh=train_raw.shape[0]*0.9, axis=1)","ec4cb3dc":"test.shape","3aef8746":"train.shape","76be07f3":"train = train.dropna(axis=0)","2a97f536":"train.shape","42ec40f9":"train.columns.values","f3a02d89":"train['Exterior2nd'].dtype.name","5653f2e4":"categorical_train_cols = [col_name for col_name in train.columns.values if train[col_name].dtype.name == 'object']","d82d4eb1":"categorical_train_cols += ['MSSubClass']","1084c300":"categorical_test_cols = [col_name for col_name in test.columns.values if test[col_name].dtype.name == 'object']","34aefba8":"categorical_test_cols += ['MSSubClass']","02a44f62":"numeric_cols = [col_name for col_name in train.columns.values if col_name not in categorical_train_cols]","effbc0fa":"train_df = pd.concat((train[numeric_cols], pd.concat([\n    pd.get_dummies(train[col_name], prefix = f'{col_name}') for col_name in categorical_train_cols\n], axis = 1)), axis = 1)","0e597031":"numeric_test_cols = [col_name for col_name in test.columns.values if col_name not in categorical_test_cols]","d095e65b":"test_df = pd.concat((test[numeric_test_cols], pd.concat([\n    pd.get_dummies(test[col_name], prefix = f'{col_name}') for col_name in categorical_test_cols\n], axis = 1)), axis = 1)","cbe5e8c6":"train_df","7723333b":"test_df","cad57b3e":"extra_train_cols = set(train_df.columns.values).difference(set(test_df.columns.values))","fdad8d25":"extra_test_cols = set(test_df.columns.values).difference(set(train_df.columns.values))","0b7ecc89":"extra_train_cols.remove('SalePrice')","2ec23079":"train_df = train_df.drop(columns = extra_train_cols)\ntest_df = test_df.drop(columns = extra_test_cols)","fc359290":"train_X = train_df.copy().drop(columns = ['SalePrice', 'Id'])\ntrain_Y = train_df['SalePrice'].copy()","33ed289d":"from sklearn.model_selection import train_test_split","29011aff":"train_X, test_X_all, train_Y, test_Y_all = train_test_split(train_X, train_Y, train_size=0.7, shuffle=True, random_state = 42)","55653421":"train_X.shape","e7f90515":"test_X_all.shape","16bc6c66":"submission_test_df = test_df.copy()","efc325ad":"test_X, validation_X, test_Y, validation_Y = train_test_split(test_X_all, test_Y_all, train_size=0.6, shuffle=True)","9ddaa20e":"test_X.shape","0d63f105":"validation_X.shape","fc2ac8d7":"plt.hist(train_Y, bins = 40)\nplt.title('Distribution of Sale Prices for Train Data')","ab1396cd":"train_log_Y = np.log(train_Y)","134dc561":"plt.hist(train_log_Y, bins = 40)\nplt.title('Distribution of (Log-Scaled) Sale Prices for Train Data')","8ceb25ce":"correlations_series = train_df.corrwith(train_log_Y, method='pearson').dropna()","8efda87b":"correlations_series","e6582d2a":"sorted(correlations_series)","d2bba9bb":"plt.bar(np.arange(len(correlations_series)), sorted(correlations_series))\nplt.title('Correlation of Individual Features with Target Variable (LogSalePrice)')\nplt.ylabel('Correlation (Pearson R)')\nplt.xlabel('Feature Index Number');","1561b679":"from numpy.linalg import lstsq, norm","97104a45":"# set rcond = -1 to use higher precision than the default\nlstsq_weights, residuals, train_rank, train_sing_values = lstsq(train_X, train_log_Y, rcond= -1)","2f0eecf6":"lstsq_train_loss = norm(train_X.dot(lstsq_weights) - train_log_Y)**2 \/ len(train_X)","d6a4db30":"lstsq_train_loss","22ecfe19":"norm(np.exp(train_X.dot(lstsq_weights)) - train_Y) \/ len(train_X)","082f3c3b":"test_log_Y = np.log(test_Y)","387bbd79":"lstsq_test_loss = norm(test_X.dot(lstsq_weights) - test_log_Y) \/ len(test_log_Y) ","b7f96f77":"lstsq_test_loss","65c9ebc7":"norm(np.exp(test_X.dot(lstsq_weights)) - test_Y) \/ len(test_log_Y) ","89c4e401":"from sklearn.ensemble import RandomForestRegressor","98717b6c":"rf_regressor = RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42)","a26c891a":"rf_regressor.fit(train_X, train_log_Y)","befb1367":"rf_train_loss = norm(rf_regressor.predict(train_X) - train_log_Y) \/ len(train_X)","ff0cf396":"rf_test_loss = norm(rf_regressor.predict(test_X) - test_log_Y) \/ len(test_Y)","5bb1233a":"rf_train_loss","24090d97":"rf_test_loss","7e514313":"weak_rf_regressor = RandomForestRegressor(n_estimators = 10, max_depth = 4, random_state = 42)","7e0803bc":"weak_rf_regressor.fit(train_X, train_log_Y)","cd543978":"weak_rf_train_loss = norm(weak_rf_regressor.predict(train_X) - train_log_Y) \/ len(train_X)","1aa2583a":"weak_rf_test_loss = norm(weak_rf_regressor.predict(test_X) - test_log_Y) \/ len(test_X)","3e6cd34c":"weak_rf_train_loss","0af1aa19":"weak_rf_test_loss","2de41194":"plt.bar(np.arange(len(lstsq_weights)), sorted(np.abs(lstsq_weights)))\nplt.title('Feature Weights for Ordinary Least Squares Regression')\nplt.ylabel('Coefficient (Absolute Value)');","51b8550e":"from sklearn.linear_model import Lasso","22805e75":"lasso_model = Lasso(alpha = 1.0, normalize = True, fit_intercept = True, tol=1e-6, random_state = 42)","1a73e2a8":"lasso_model.fit(train_X, train_log_Y)","aad8afe8":"lasso_train_loss = norm(lasso_model.predict(train_X) - train_log_Y) \/ len(train_X)\nlasso_test_loss = norm(lasso_model.predict(test_X) - test_log_Y) \/ len(test_X)","b7dc390b":"lasso_train_loss","0f7751ce":"lasso_test_loss","215b8d64":"lasso_model_001 = Lasso(alpha = 0.01, normalize = True, fit_intercept = True, tol=1e-6, random_state = 42)","01a7a7ed":"lasso_model_001.fit(train_X, train_log_Y)","4829f51f":"lasso_train_loss_001 = norm(lasso_model_001.predict(train_X) - train_log_Y) \/ len(train_X)\nlasso_test_loss_001 = norm(lasso_model_001.predict(test_X) - test_log_Y) \/ len(test_X)","7ba562b3":"lasso_train_loss_001","c43f7568":"lasso_test_loss_001","dfa73c8d":"lasso_model_1e4 = Lasso(alpha = 0.0001, normalize = True, fit_intercept = True, tol=1e-6, random_state = 42)","22b89d0f":"lasso_model_1e4.fit(train_X, train_log_Y)","6d0e5dbc":"lasso_train_loss_1e4 = norm(lasso_model_1e4.predict(train_X) - train_log_Y) \/ len(train_X)\nlasso_test_loss_1e4 = norm(lasso_model_1e4.predict(test_X) - test_log_Y) \/ len(test_X)","40f2659b":"lasso_train_loss_1e4","2858813c":"lasso_test_loss_1e4","cccfecbf":"lstsq_test_loss","2e3ad8d3":"lasso_weights = lasso_model_1e4.coef_","bfb23198":"plt.scatter(np.abs(lstsq_weights), np.abs(lasso_weights), s = 10, marker='o');\nplt.xlabel('Feature Weight for Least Squares')\nplt.ylabel('Feature Weight for LASSO')\nplt.title('Feature Weights for Regression with\/without Regularization');","14c28180":"validation_log_Y = np.log(validation_Y)","c32eed8d":"lasso_validation_loss_1e4 = norm(lasso_model_1e4.predict(validation_X) - validation_log_Y) \/ len(validation_X)\nweak_rf_regressor_validation_loss = norm(weak_rf_regressor.predict(validation_X) - validation_log_Y) \/ len(validation_X)\nrf_regressor_validation_loss = norm(rf_regressor.predict(validation_X) - validation_log_Y) \/ len(validation_X)","4eb33c11":"lstsq_validation_loss = norm(validation_X.dot(lstsq_weights) - validation_log_Y) \/ len(validation_X)","47d0e420":"lasso_validation_loss_1e4","41f06fb2":"weak_rf_regressor_validation_loss","1bf6f4d6":"rf_regressor_validation_loss","695aa6b1":"lstsq_validation_loss","1c24ae7f":"plt.bar(np.arange(len(rf_regressor.feature_importances_)), rf_regressor.feature_importances_)\nplt.ylabel('Feature Importance (Sums to 1)')\nplt.title('Feature Importance for Random Forest Regressor')","852200d4":"rf_weights = rf_regressor.feature_importances_","c6cd1910":"plt.plot(np.arange(len(rf_weights)), np.cumsum(sorted(rf_weights, reverse=True)), marker='^')\nplt.title('Cumulative Feature Weight for Random Forest')\nplt.ylabel('Sum of Feature Weights')\nplt.xlabel('Index of feature weight')","d2b99caf":"top_feature_indices = np.where(rf_weights > 0.01)","94f2b881":"train_X.columns.values[top_feature_indices]","2c557591":"rf_weights[top_feature_indices]","514fffdf":"plt.bar(np.arange(len(top_feature_indices[0])), rf_weights[top_feature_indices])\nplt.xticks(np.arange(len(top_feature_indices[0])), train_X.columns.values[top_feature_indices], rotation=60);\nplt.ylabel('Feature Weight')\nplt.title('Weights for Top Features in Random Forest Regressor');","59dd9c0a":"sample_submission_df = pd.read_csv(f'{dat_path}sample_submission.csv')","1f37b047":"submission_test_df.shape","1a1ebf67":"train_X.shape","4d8db5ac":"submission_X = submission_test_df.drop(columns = ['Id'])","54212d39":"feature_means = np.mean(train_X, axis=0)","0ed20f13":"submission_X_no_nan = submission_X.fillna(value=feature_means)","c56e9123":"submission_X_no_nan.shape","7723f23b":"submission_Y_predict = np.exp(rf_regressor.predict(submission_X_no_nan))","0340a194":"submission_df_final = pd.concat((submission_test_df['Id'], pd.Series(submission_Y_predict)), axis = 1)","2e5b9c83":"submission_df_final.rename(columns = {0: 'SalePrice'}, inplace=True)","67d02979":"submission_df_final","af47aed6":"submission_df_final.to_csv('house_prices_submission.csv')","d3592f37":"Let's do the same for the training data. ","2647d062":"The high-priced outliers will skew a model that is trained on squared errors. So, we should log-normalize sale prices to account for this. ","e4c728da":"## Which Features Matter? ","9d95e22c":"# Prepare Submission","c409ddfb":"So far we have tested 4 regression models: \n\n1. Ordinary Least Squares Regression \n\n2. LASSO Regression (AKA Sparse Linear Regression) \n\n3. Random Forest Regression \n\n4. Low-Depth RF Regression (Random Forest with Stronger Sparsity Constraints)\n\nTo select the best-performing model, we need to test on our *validation data* to avoid data incest. ","46ce0d7b":"Surprisingly, the majority of the weight goes to a single feature! But beyond the top 2, it looks like there are several smaller features making a contribution.\n\nHow many features do we need to get to 80 or 90 percent weight?","1bec6443":"We need to fill in missing values in the submission DF. Let's use the mean of each feature.","1aa11376":"### Sparse Regression - Is it worth it? ","49194a4d":"# Test Models \n\n## Ordinary Least Squares\n\nThe simplest regression technique, and a good baseline for further testing. ","0b0bb3aa":"### LASSO Regression\n\nLASSO regression is a particular kind of linear regression in which the $\\ell_1$-norm of the weight vector is part of the loss function. Hence there is an incentive to assign lower weights to features, which is what we hope to achieve in order to overcome overfitting.","5ce3b8de":"**Result**: It looks like there are quite a few columns where only a small fraction of the rows are null. whereas there are some for which a substantial portion (> 15 percent) are missing. Let's drop the columns where at least 15% of values are missing, and then retain the rest of the columns, but drop the rows for which a value is missing there. ","79d009a6":"### Odds and Ends\n\n* We need to split the train data into train, test, and validation data. Our \"test_df\" as of now has no training labels, because it's used for submission - so the name 'test' is a bit of a misnomer. We'll need a 3-way split because we're going to be comparing several regression models. Since the test data will be used for hyperparameter selection at the individualized model level (e.g. the regularization weight for LASSO), we need an entirely separate validation dataset at the level of comparing models. \n\n* We need to separate the predictive features (\"X\") from the sale price feature (\"Y\")\n\n* We should eliminate the additonal columns present in the train data that are not in the test data. No point in using these features for prediction if they are not part of the test data. ","5e1af8b4":"Let's get the counts of null values by column.","062f1dde":"### How to deal with NaN values? \n\nIt looks like quite a few columns are dropped when we drop Null values. However, we don't necessarily need to drop the entire column if only a few of the values are null - we can just as well drop a few rows if those are the only ones for which the column is null. ","60e1c36e":"We can use the `get_dummies` function in Pandas to convert this categorical data into one-hot vectors. ","c4623c07":"Least squares regression uses a **Mean Squared Error** loss function. Mathematically, the vector $y$ is the target variable (log Sale Price) and the matrix $X$ is a collection of feature vectors, so that each house corresponds to a unique row. We solve for $w$ in the equation:\n\n$$y = Xw$$ \n\nThe resulting estimator is denoted $\\hat w$, and the loss is then: \n\n$$ \\mathcal{L}(\\hat w) = \\frac{1}{n} \\left\\lVert y - X \\hat w \\right\\rVert_2$$\n\nWhere $n$ is the number of data points. ","a413910a":"So an average prediction error of about $500, which isn't bad. But we could certainly do better! Finally, let's get the test loss.","cb15fd0f":"### Convert Categorical Data\n\nQuite a few of our variables are categorical (as opposed to numerical). Let's take a look at these and see if we can create one-hot vectors out of them without too much trouble.","ca05e998":"## Distribution of Target Prices","62438bcb":"# Regression Models for Housing Prices \n\nBy Akhil Jalan\n\n# Load\/Clean Data\n\nLet's take a look at our dataset, and do some simple cleaning\/reorganization for easy study later.","445ee82a":"## Feature Correlations","7aa56b43":"## Random Forest","97f7fa25":"The two strongest features are thus: \n    \n* *OverallQual: Rates the overall material and finish of the house*\n    \n* *GrLivArea: Above grade (ground) living area square feet*\n\nBoth make sense. The first feature is a sort of human-based assessment which corresponds to the quality of the house. The second is square footage, a common metric used in home sales which no doubt influences advertising and negotations. \n\nInterestingly, all of the top features seem to be numeric (as opposed to categorical) variables. Perhaps this tells us soemthing about the *psychology of home pricing* - namely, that people prefer to base prices on seemingly objective metrics like square footage, number of cars that can be fit in the garage,the year built, and so on.","afbd0ff1":"Indeed, it looks like there isn't really a bifurcation of features weights, and instead a significant portion are used with relatively large weight. Nevertheless, it looks like a significant portion of the weight is in the top 50-100 most important features.","8c23b100":"## Dealing with Overfitting\n\n### Weaker Random Forest\n\nLet's use a much weaker RF model with just 10 trees and a max depth of 4. ","748caf82":"First, let's look at what OLS ended up doing for individual feature weights. The idea behind sparse\/regularized regression is to ensure that *only a few features are used*, so we need to make sure this isn't already happening.","f09d19b0":"# Model Selection and Result Submission","ee8a492a":"Note that this is not the error in predicting housing precises, but rather the *log* of those prices. Here's the loss when we undo the logarithms. ","120dd300":"As we can see, while it only takes a few features to get to around 90% feature weight, we need to get the top 100 or so features to reach roughly 99% of the full feature weights. Hence the *long-tailed* distribution of feature weights explains *why the random forest outperforms other models*. Its additional depth and number of trees allows it to account for the fine-grained signal in these additional features.","ed26c05e":"While the train loss increases as we would expect, this doesn't help since the test loss actually increased slightly! We already have a fairly weak random forest regressor which does not solve the overfitting issue - let's try regularization for linear regression next.","81449814":"## Why is Random Forest Better?\n\nOne hypothesis for why random forest is better is that one really need a lot of features to get all the signal in the data. This is why regularization might fail. Is this true? Let's take a look at feature weights for the random forest regressor.","781088b5":" ## Compute Validation Error","6e3d80bf":"**Takeaway**: \n\n1. Most features have fairly low correlation so we can impose a fairly strong condition on the number of features used without losing much signal. For regression, this involves a sparsity constraint on the feature vector. For decision trees, this means a maximum depth constraint. \n\n2. We have an almost perfect balance between negatively and positively corelated features. This is useful for decision trees especially, as it enables us to split downwards (that is, predict a lower price) and upwards as we traverse the tree. ","fce9fe37":"Now we're getting somewhere - the train and test loss are relatively close! Unfortunately the test loss is quite high, so we probably overshot a bit. Let's try a weaker regularization (e.g. lower $\\alpha$)","d4962d62":"We get a very slight improvement, but LASSO is still not competitive with the random forest model. Let's try an extremely weak regularization of $\\alpha = 10^{-4}$ and see if we get anywhere.","cf19b024":"Great! Looks like we saw a real improvement with the lower $\\alpha$. Now LASSO outperforms OLS on the test loss, although it still is outperformed by random forest models. Let's compare feature weights for OLS and the weakest LASSO to see what the practical effect is in terms of feature importance. ","9d2c8614":"# Conclusion\/Insights: Random Forest Wins\n\nAlthough regularized models like LASSO or weaker random forest did reduce the gap between train\/test loss, it appears that a random forest model performed best. In terms of insights, what this tells us is that **model strength overcomes overfitting**. Of course, this doesn't answer the question of **how many features we really need** for this problem. To answer this, we need to look at feature weights for the best-performing random forest model.","9548f249":"Good! A test loss of $0.0104$ is a big improvement over the previous $0.0155$. However, notice that there is an even bigger difference between train\/test loss here than in the previous model of OLS, indicating some overfitting. ","b40b25cd":"# Data Exploration","513ada56":"**How to interpret this plot**: This mainly tells use that LASSO assigned much lower weights to most features. The almost-solid line at the $x$-axis tells use that many nonzero features in OLS were set to zero for LASSO. Despite this, LASSO has a better test error! Therefore we found that correcting for the overfitting was worth it. \n\n**Comparison to Random Forest**: Interestingly, while we were able to get a useful improvement for linear regression when using regularization, our weaker random forest model had worse test error. This might indicate a *double descent* type phenomenon, or just that we didn't search hyperparameter space well enough.","c9ae3ae5":"Let's gather up all the column names whose datatype is \"object\" (and so NOT numeric). "}}