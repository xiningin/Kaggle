{"cell_type":{"1cf7f304":"code","f1cf9907":"code","6b83b9f4":"code","9fd8ed71":"code","2350c9fa":"code","0815acf3":"code","606288d0":"code","e9868bee":"code","96889c1e":"code","5128cdec":"code","540dafc2":"code","34151036":"code","1fad3b9f":"code","be5a6ff4":"code","e3a9b295":"code","60626c7a":"code","a99df733":"code","f3326187":"code","29ae9364":"code","22c1e943":"markdown","42121399":"markdown","b7bb11e7":"markdown","8832379b":"markdown","216bfa39":"markdown","2e31ced4":"markdown","204d9862":"markdown","8cb99021":"markdown","393b9426":"markdown","82782bfc":"markdown","2288a459":"markdown","0113a9b5":"markdown","d65304c0":"markdown","337958fe":"markdown","50b2ff85":"markdown","e6229e84":"markdown","317533dd":"markdown"},"source":{"1cf7f304":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math # para ceil() en particular\n\nimport h5py\nimport matplotlib.pyplot as plt\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils, to_categorical\nfrom keras.layers import Dense, Dropout, Flatten, LeakyReLU, Activation\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.regularizers import l2\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f1cf9907":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","6b83b9f4":"K.set_image_data_format('channels_last')","9fd8ed71":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","2350c9fa":"train_dfX = train_df.drop(['label'], axis=1)\ntrain_dfY = train_df['label'] \n# Utilizando split de 80% test - 20% validation\n# Tambi\u00e9n se usa .values para obtener numpy arrays en vez de dataframes\ntrain_X, val_X, train_Y, val_Y = train_test_split(train_dfX.values, train_dfY.values, test_size=0.2,stratify=train_dfY)\nprint(\"Entrenamiento: \",train_X.shape)\nprint(\"Validacion:    \",val_X.shape)","0815acf3":"train_X = train_X.reshape(train_X.shape[0], 28, 28, 1)\nval_X = val_X.reshape(val_X.shape[0], 28, 28, 1)","606288d0":"unique, count= np.unique(train_Y, return_counts=True)\nprint(\"Distribucion de los labels en el train set: %s \" % dict (zip(unique, np.round(count\/train_Y.shape[0],3)) ), \"\\n\" )\n\nunique, count= np.unique(val_Y, return_counts=True)\nprint(\"Distribucion de los labels en el validation set: %s \" % dict (zip(unique, np.round(count\/val_Y.shape[0],3)) ), \"\\n\" )","e9868bee":"labels_indices = np.unique(train_Y,return_index=True)\nfor label in labels_indices[0]:\n    plt.subplot(2, 5, label + 1)\n    plt.axis('off')\n    plt.imshow(train_X[labels_indices[1][label]].squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('label: %i' % label )","96889c1e":"NUM_CLASSES = labels_indices[0].size","5128cdec":"datagen = ImageDataGenerator(\n    rescale = 1\/255, # Solo nos interesa valores entre 0 y 1\n    rotation_range = 20, # Rango de rotaci\u00f3n de 20\u00b0\n    zoom_range = 0.2 # Rango del zoom\n)\ntrain_X_Aug = np.array(train_X, copy=True)\ntrain_Y_Aug = np.array(train_Y, copy=True)\n\ndatagen.fit(train_X_Aug)\n\n# Concatenar valores originales con augmented\ntrain_X_Final = np.concatenate((train_X, train_X_Aug), axis=0)\ntrain_Y_Final = np.concatenate((train_Y, train_Y_Aug), axis=0)","540dafc2":"train_Y_Final = to_categorical(train_Y_Final, NUM_CLASSES).astype('int32')\nval_Y = to_categorical(val_Y, NUM_CLASSES).astype('int32')","34151036":"EPOCHS = 40\ndef graf_model(train_history):\n    f = plt.figure(figsize=(EPOCHS,10))\n    ax = f.add_subplot(121)\n    ax2 = f.add_subplot(122)\n    # summarize history for accuracy\n    ax.plot(train_history.history['acc'])\n    ax.plot(train_history.history['val_acc'])\n    ax.set_title('model accuracy')\n    ax.set_ylabel('accuracy')\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(train_history.history['loss'])\n    ax2.plot(train_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper left')\n    plt.show()","1fad3b9f":"LEARNING_RATE = 0.00025\n#LR_DECAY = LEARNING_RATE\/EPOCHS\nBATCH_SIZE = 32\nL2_REG = 0.0030\nLR_REDUC = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1,factor=0.5, min_lr=0.00001)","be5a6ff4":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=7, padding=\"same\", input_shape=(28, 28, 1), kernel_regularizer=l2(L2_REG)))\nmodel.add(LeakyReLU(0.1))\nmodel.add(Conv2D(filters=64, kernel_size=5, padding=\"same\", kernel_regularizer=l2(L2_REG)))\nmodel.add(LeakyReLU(0.1))\n    \nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(filters=64, kernel_size=5, padding=\"valid\", kernel_regularizer=l2(L2_REG)))\nmodel.add(LeakyReLU(0.1))\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"valid\", kernel_regularizer=l2(L2_REG)))\nmodel.add(LeakyReLU(0.1))\n    \nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\n    \nmodel.add(Flatten())\nmodel.add(Dense(256, kernel_regularizer=l2(L2_REG)))\nmodel.add(Dropout(0.30))\nmodel.add(LeakyReLU(0.1))\n\nmodel.add(Dense(128, kernel_regularizer=l2(L2_REG)))\nmodel.add(Dropout(0.75))\nmodel.add(LeakyReLU(0.1))\n    \nmodel.add(Dense(10))\nmodel.add(Activation(\"softmax\"))\n\n#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=LEARNING_RATE, decay=LR_DECAY), metrics=['accuracy'])\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=LEARNING_RATE), metrics=['accuracy'])\nmodel.summary()","e3a9b295":"# Data Augmentation ocurre en paralelo al entrenamiento\nhist = model.fit_generator(datagen.flow(train_X_Final, train_Y_Final, batch_size=BATCH_SIZE),\n                    steps_per_epoch=math.ceil(len(train_X) \/ BATCH_SIZE),\n                    epochs = EPOCHS, \n                    validation_data = (val_X, val_Y),\n                    callbacks=[LR_REDUC])\n","60626c7a":"train_acc = hist.history['acc'][-1]\nval_acc = hist.history['val_acc'][-1]\nprint('Train Acc: %.4f, Val Acc: %.4f' % (train_acc, val_acc))","a99df733":"graf_model(hist)","f3326187":"import sklearn.metrics as metrics\n\npred_Y = model.predict(val_X)  # shape=(n_samples, 12)\npred_Y = np.argmax(pred_Y, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\ntrue_Y = np.argmax(val_Y, axis=1)\nconfusion_matrix = metrics.confusion_matrix(y_true=true_Y, y_pred=pred_Y)  # shape=(12, 12)\nprint(confusion_matrix)","29ae9364":"test = (test_df.values).reshape(-1, 28, 28 , 1)\npred = model.predict(test)\npred = np.argmax(pred,axis = 1)\npred = pd.Series(pred, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1 ,pred.shape[0]+1) ,name = \"ImageId\"), pred], axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)","22c1e943":"Hiperpar\u00e1metros","42121399":"**Reshape**<p>\nEn base a la convenci\u00f3n \"channels_last\", se tendr\u00eda:<p>\n(m samples, n_H filas, n_W columnas, n_C channels)<p>\nEl dataset da 784 valores para cada imagen: 28x28 p\u00edxeles (de valores entre 0 y 255) de un solo channel (escala de grises)","b7bb11e7":"Distribuci\u00f3n de los labels en sets Train y Validation","8832379b":"Funci\u00f3n para graficar la historia de entrenamiento","216bfa39":"**Visualizaci\u00f3n de los datos**","2e31ced4":"**Otros ajustes previos**\n<p>Se usar\u00e1 categorical crossentropy para la funci\u00f3n de p\u00e9rdida, por lo que ser\u00e1 necesario hacer one-hot encoding a los sets de y (labels).","204d9862":"Visualizaci\u00f3n de las im\u00e1genes","8cb99021":"**Predicci\u00f3n y salida de los datos**","393b9426":"**Matriz de confusi\u00f3n**","82782bfc":"**Creaci\u00f3n del Modelo**","2288a459":"**Importar datos**","0113a9b5":"Aprovechando el c\u00f3digo anterior, tenemos el n\u00famero de clases en el dataset (10)","d65304c0":"**Data Augmentation**\n<p>Se har\u00e1 uso del ImageDataGenerator de Keras para estandarizar los valores, rotar las im\u00e1genes y aplicarles zoom (alterar el tama\u00f1o).","337958fe":"Modelo","50b2ff85":"**Train-Test Split**","e6229e84":"**Ajustamos convencion de Keras de channel_size al final para coincidir con Tensorflow (y que es la que hemos usado en clase)**","317533dd":"**Fijamos random seed para comparar mejor los modelos**"}}