{"cell_type":{"ce96ea22":"code","b99a715b":"code","f4df7daa":"code","8f20e111":"code","8b5f47fe":"code","68e10a8d":"code","df658a07":"code","de9d749d":"code","be2727c5":"code","21c64154":"code","5bcd1884":"code","98b6cf60":"code","0ee19321":"code","258ebcfb":"code","0ab81623":"code","89469c68":"code","cdf5b82e":"code","6216c544":"markdown","1c1f3c5a":"markdown","164761ff":"markdown","828c8366":"markdown","7e12ec63":"markdown","f1b0f257":"markdown","999a0423":"markdown","a09b61f3":"markdown","086616f3":"markdown","602a8538":"markdown","f64e3a77":"markdown","f21d5b67":"markdown","eb53dbce":"markdown","8f032795":"markdown","0fe608a0":"markdown"},"source":{"ce96ea22":"import pandas as pd\nfrom pandas import read_excel\nimport numpy as np\nimport re\nfrom re import sub\nimport multiprocessing\nfrom unidecode import unidecode\nimport os\nfrom time import time \nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling2D, Conv1D, MaxPooling1D\nfrom keras.optimizers import SGD,Adam\nfrom keras import regularizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nimport h5py\nimport csv\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","b99a715b":"def text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n                               \n                           \"]+\", flags=re.UNICODE)\n    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n    \n    text=emoji_pattern.sub(r'', text)\n    text=english_pattern.sub(r'', text)\n\n    return text\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b\u00a3|\u00a2|\u0007\u00d1+-*\/=EROero\u09f3\u09e6\u09e7\u09e8\u09e9\u09ea\u09eb\u09ec\u09ed\u09ee\u09ef012\u201334567\u202289\u0964!()-[]{};:'\"\u201c\\\u2019,<>.\/?@#$%^&*_~\u2018\u2014\u0965\u201d\u2030\u26bd\ufe0f\u270c\ufffd\ufff0\u09f7\ufff0'''\n    \n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out","f4df7daa":"df=pd.read_excel('\/kaggle\/input\/pseudolabel\/predicted_unsupervised_sentiment.xlsx')\ndisplay(df)","8f20e111":"sns.countplot(df['sentiment']);","8b5f47fe":"df['sentence'] = df.sentence.apply(lambda x: preprocessing(str(x)))","68e10a8d":"df.reset_index(drop=True, inplace=True)","df658a07":"train1, test1 = train_test_split(df,random_state=69, test_size=0.2)\ntraining_sentences = []\ntesting_sentences = []\n\n\n\ntrain_sentences=train1['sentence'].values\ntrain_labels=train1['sentiment'].values\nfor i in range(train_sentences.shape[0]): \n    #print(train_sentences[i])\n    x=str(train_sentences[i])\n    training_sentences.append(x)\n    \ntraining_sentences=np.array(training_sentences)\n\n\n\n\n\ntest_sentences=test1['sentence'].values\ntest_labels=test1['sentiment'].values\n\nfor i in range(test_sentences.shape[0]): \n    x=str(test_sentences[i])\n    testing_sentences.append(x)\n    \ntesting_sentences=np.array(testing_sentences)\n\n\ntrain_labels=keras.utils.to_categorical(train_labels)\n\n\ntest_labels=keras.utils.to_categorical(test_labels)\nprint(\"Training Set Length: \"+str(len(train1)))\nprint(\"Testing Set Length: \"+str(len(test1)))\nprint(\"training_sentences shape: \"+str(training_sentences.shape))\nprint(\"testing_sentences shape: \"+str(testing_sentences.shape))\nprint(\"train_labels shape: \"+str(train_labels.shape))\nprint(\"test_labels shape: \"+str(test_labels.shape))\n","de9d749d":"print(training_sentences[1])\nprint(train_labels[0])","be2727c5":"vocab_size = 25000\nembedding_dim = 300\nmax_length = 100\ntrunc_type='post'\noov_tok = \"<OOV>\"","21c64154":"print(training_sentences.shape)\nprint(train_labels.shape)","5bcd1884":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nprint(len(word_index))\nprint(\"Word index length:\"+str(len(tokenizer.word_index)))\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\n\ntest_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(test_sequences,maxlen=max_length)","98b6cf60":"print(\"Sentence :--> \\n\")\nprint(training_sentences[2]+\"\\n\")\nprint(\"Sentence Tokenized and Converted into Sequence :--> \\n\")\nprint(str(sequences[2])+\"\\n\")\nprint(\"After Padding the Sequence with padding length 100 :--> \\n\")\nprint(padded[2])","0ee19321":"print(\"Padded shape(training): \"+str(padded.shape))\nprint(\"Padded shape(testing): \"+str(testing_padded.shape))","258ebcfb":"with tf.device('\/gpu:0'):\n    model= Sequential()\n    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n    model.add(Conv1D(200, kernel_size=3, activation = \"relu\"))\n    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(64)))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Flatten())\n    #l2 regularizer\n    model.add(Dense(100,kernel_regularizer=regularizers.l2(0.01),activation=\"relu\"))\n    model.add(Dense(2, activation='softmax'))\n    #sgd= SGD(lr=0.0001,decay=1e-6,momentum=0.9,nesterov=True)\n    adam=Adam(learning_rate=0.0005,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)\n    model.summary()\n    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])","0ab81623":"    history=model.fit(padded,train_labels,epochs=5,batch_size=256,validation_data=(testing_padded,test_labels),use_multiprocessing=True, workers=8)","89469c68":"print(history.history.keys())\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nplt.plot(loss)\nplt.plot(val_loss)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss', 'val_loss'])\nplt.show()\n\naccuracy = history.history['accuracy']\nval_accuracy= history.history['val_accuracy']\nplt.plot(accuracy)\nplt.plot(val_accuracy)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()","cdf5b82e":"#accuracy calculation\nloss_and_metrics = model.evaluate(padded,train_labels,batch_size=256)\nprint(\"The train accuracy is: \"+str(loss_and_metrics[1]))\nloss_and_metrics = model.evaluate(testing_padded,test_labels,batch_size=256)\nprint(\"The test accuracy is: \"+str(loss_and_metrics[1]))","6216c544":"At first i will tokenize and then i will be padding the sequences.I have used tokenizer only on training dataset to see how the model performs on unseen words.","1c1f3c5a":"# Visualization","164761ff":"Now i will predefine some variables.\n\nvocab_size is the maximum vocabulary length of Tokenizer.\n\nKERAS tokenzier allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n\nI will be tokenizing my dataset with this class here.I have did this tokenization so that it can be later used to generate Embeddings.\n\nThe main advantage of word embedding is that words that share a similar context can be represented close to each other in the vector space. Thus, vectors carry a sense of semantic of a word.\n\nI have predefined the embedding dimension as 300(embedding_dim).\n\nmax_length is the sentence maximum length.\n\ntrunc_type is the truncation type.\n\noov_token is the token for the words that are not present in the corpus.oov means out of vocabulary.","828c8366":"# Library & Package Import\nI have used KERAS to implement CNN and LSTM in this dataset.","7e12ec63":"Now we will visualize the ratio of Postive and Negative sentiment.We can see that the ratio is closly 1:1.Which is considered as a good balance.","f1b0f257":"# Data Import\nThe data is in excel file.I have about 6500+ in this dataset.The data is either positive or negative.\nI have annotated \"0\" as Negative and \"1\" as Positive.","999a0423":"# Introduction\nIn this Notebook i have done binary classification of Sentiment on a dataset that contains annotated Bangla texts.I have tried to use a deep learning based hybrid network with CNN an LSTM.Through hyper parameter tuning,i have achieved an accuracy of 84% with somewhat overfitting.\n\nMy main focus of this notebook is to see the impact of hybrid CNN-BiLSTM model in bangla sentiment analysis and newbie Bangla NLP researchers like me can get a better intution.","a09b61f3":"# Predefined Functions \nI have predefined some functions for the preprocessing of my texts.The dataset contains raw text data that have many unwanted things\n(Punctuations,English words,emojis etc..).I have cleaned this things with my function.","086616f3":"# Conclusion\nAs we can see that the result is somewhat overfitting.This is because the dataset is very small to work with with deep learning.But my main focus of this notebook was to see what is the result of bangla sentiment analysis if i use a hybrid network.","602a8538":"# Model Creation\nAt first i have created embeddings from the text.\n\nIn first layer,i created an conv1D with 200 as filter for CNN.\n\nIn second & third layer,i have applied two Bi-LSTM with a dropout of .5.\n\nIn rest of the layer i have applied Dense network.\n\nI have used Adap Optimizer with fine tuned hyperparameters.\n\nI have also applied L2 regularizations to reduce overfitting as much as possible.","f64e3a77":"Now I will prepare the dataset to train in the CNN LSTM network.So i have to convert all Sentences into a numpy Array.\n\nI have divided the training and testing data into 80\/20 ratio.\n\nI have converted the sentiment values into one hot encodings for the convenient use in model training.\n","f21d5b67":"# Accuracy and Evaluation","eb53dbce":"# Data Preprocessing\nAt first we will clean the dataset with my predefined function preprocessing().","8f032795":"I trained the model for 5 epochs with batch size 256.","0fe608a0":"The Processed result is here what you see:"}}