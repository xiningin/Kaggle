{"cell_type":{"1d8c224c":"code","308b975a":"code","0fdf85f6":"code","05e12d3b":"code","96c6da05":"code","ec573e48":"code","7658e38b":"code","d4eb16b6":"code","e214485f":"code","b24bfd58":"code","92d1e339":"code","ddc18681":"code","08762d9f":"code","7cba2d53":"code","0d9c32cd":"code","b54c12f5":"code","3749ff64":"code","e388e5ef":"code","b56490c5":"code","17e45434":"code","941c570a":"code","5cebebe0":"markdown","93d7b3b4":"markdown","4fe9ba19":"markdown","9c536cad":"markdown","3e894cee":"markdown","57b1b87e":"markdown","9b5ec340":"markdown","731aa0d5":"markdown","61524ade":"markdown","6e955dae":"markdown","9ec11e2d":"markdown","156d501c":"markdown","d6abaa5a":"markdown","84f3ad23":"markdown","a8a6d2c7":"markdown","8cf4af96":"markdown","a6bf69f0":"markdown"},"source":{"1d8c224c":"!pip install -U tensorflow-addons","308b975a":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa   # for optimizer\n\nimport matplotlib.pyplot as plt\nimport os","0fdf85f6":"num_classes = 100\ninput_shape = (32,32,3)\n\n(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n\nprint(f\"X_train shape is {X_train.shape}, y_train shape is {y_train.shape}\")\nprint(f\"X_test shape is {X_test.shape}, y_train shape is {y_test.shape}\")","05e12d3b":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 10\nimage_size = 72  # resize the (32,32,3) to this\npatch_size = 6   # each patch size extracted from input images (72\/6 = 12 patches)\nnum_patches = (image_size\/\/patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4 # (4 mha block)\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim\n]  # size of the transformer layers\n\ntransformer_layers = 8  # we would stach 8 tx layers\nmlp_head_units = [2048, 1024]  # size of the dense layer of the final classifier","96c6da05":"data_augmentation = keras.Sequential([\n    layers.experimental.preprocessing.Normalization(),\n    layers.experimental.preprocessing.Resizing(image_size, image_size),\n    layers.experimental.preprocessing.RandomFlip('horizontal'),\n    layers.experimental.preprocessing.RandomRotation(factor=0.02),\n    layers.experimental.preprocessing.RandomZoom(\n        height_factor=0.2, width_factor=0.2\n    ),\n],\n    name = 'data_augmentation',\n)","ec573e48":"# let's see the layers for data aug\ndata_augmentation.layers","7658e38b":"# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(X_train)","d4eb16b6":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation = tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","e214485f":"# get to know how patches are extracted using help\n# tf.image.extract_patches?","b24bfd58":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n        #print(patch_size)\n\n    def call(self, images):  #4-D tensor (batch, row, col, channel)\n        image_shape = tf.shape(images)\n        #print(f'image shape : {image_shape}') \n        batch_size = image_shape[0]  \n        #print(f'batch shape {batch_size}')\n        \n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        \n        #print(f'patch shape before reshape: {patches.shape}')\n        patch_dims = patches.shape[-1]  # no of elements in the patch (pixels count in patch)\n        #print(f'patch dim: {patch_dims}')\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims]) # flatten the mid two dim \n        #print(f'patch shape after reshape: {patches.shape}')\n        return patches","92d1e339":"plt.figure(figsize=(4, 4))\nimage = X_train[np.random.choice(range(X_train.shape[0]))]  # pick one random image  (32, 32, 3)\n\n#print(f'image shape {image.shape}')\nplt.imshow(image.astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(image_size, image_size)   # convert to (1, 72, 72, 3)\n)\n\n#print(f'resized image shape {resized_image.shape}')\n\n# create patches from the image\npatches = Patches(patch_size)(resized_image)  # initialize with patch size and call call() method \n\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n      ","ddc18681":"n = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","08762d9f":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units = projection_dim)\n        self.position_embedding = layers.Embedding(\n              input_dim = num_patches, output_dim = projection_dim\n        )\n        \n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","7cba2d53":"encoder = PatchEncoder(num_patches, projection_dim)(patches[0])\nencoder.shape","0d9c32cd":"def create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    patches = Patches(patch_size)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","b54c12f5":"# define optimizer\noptimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n# define loss function\nloss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# define metrics\nmetrics=[\n          keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n          keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\") \n        ]","3749ff64":"# Initialize model\nmodel = create_vit_classifier()\n\n# compile model\nmodel.compile(\n        optimizer=optimizer,\n        loss = loss,\n        metrics = metrics,\n)\n\n# let's see the model arch\nmodel.summary()","e388e5ef":"if not os.path.exists(\"checkpoint_path\"):\n    os.mkdir(\"checkpoint_path\")\n    \n!ls","b56490c5":"checkpoint_filepath = \"checkpoint_path\"\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )","17e45434":"def run_experiment(model, checkpoint_callback):\n    history = model.fit(\n     x = X_train,\n     y = y_train,\n     batch_size = batch_size,\n     epochs = num_epochs,\n     validation_split=0.1,\n     callbacks = [checkpoint_callback],\n    )\n    \n    model.load_weights(checkpoint_filepath)\n    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return history","941c570a":"history = run_experiment(model, checkpoint_callback)","5cebebe0":"# Implement patch creation as layer","93d7b3b4":"# Setup","4fe9ba19":"Practicing from keras code example at https:\/\/keras.io\/examples\/vision\/image_classification_with_vision_transformer\/\n\nPaper - https:\/\/arxiv.org\/abs\/2010.11929   Video at (https:\/\/www.youtube.com\/watch?v=TrdevFK_am4&t=637s)\n\nUnderstand Transformer in general - https:\/\/www.youtube.com\/watch?v=4Bdc55j80l8&t=323s\n","9c536cad":"![image.png](attachment:image.png)\n\nimage source - https:\/\/neurohive.io\/en\/news\/vision-transformers-transformers-work-well-in-computer-vision-too\/","3e894cee":"### Normalize the input","57b1b87e":"<img src='https:\/\/github.com\/hirotomusiker\/schwert_colab_data_storage\/blob\/master\/images\/vit_demo\/vit_input.png?raw=true'>\n\nVisit https:\/\/colab.research.google.com\/github\/hirotomusiker\/schwert_colab_data_storage\/blob\/master\/notebook\/Vision_Transformer_Tutorial.ipynb#scrollTo=fZAPoK8jBtaD   for awsome notebook walkthrough for VIT in Pytorch","9b5ec340":"# Compile, train and evaluate model","731aa0d5":"### Test the encoder","61524ade":"# Data augmentation","6e955dae":"To improve the model quality without pre-training, you can try to train the model for more epochs, use a larger number of Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, but also by parameters such as the learning rate schedule, optimizer, weight decay, etc. In practice, it's recommended to fine-tune a ViT model that was pre-trained using a large, high-resolution dataset.\n\ncheck - https:\/\/keras.io\/examples\/vision\/image_classification_with_vision_transformer\/","9ec11e2d":"Let's display patches for a sample image","156d501c":"# Implement MLP","d6abaa5a":"# Prepare the data","84f3ad23":"# Implementing patch encoding layer\nThe PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector.\n","a8a6d2c7":"# Configuring hyperparameters","8cf4af96":"# Build the VIT model","a6bf69f0":"Signature: tf.image.extract_patches(images, sizes, strides, rates, padding, name=None)\nDocstring:\nExtract `patches` from `images`.\n\nThis op collects patches from the input image, as if applying a\nconvolution. All extracted patches are stacked in the depth (last) dimension\nof the output.\n\nSpecifically, the op extracts patches of shape `sizes` which are `strides`\napart in the input image. The output is subsampled using the `rates` argument,\nin the same manner as \"atrous\" or \"dilated\" convolutions.\n\nThe result is a 4D tensor which is indexed by batch, row, and column.\n`output[i, x, y]` contains a flattened patch of size `sizes[1], sizes[2]`\nwhich is taken from the input starting at\n`images[i, x*strides[1], y*strides[2]]`.\n\nEach output patch can be reshaped to `sizes[1], sizes[2], depth`, where\n`depth` is `images.shape[3]`.\n\nThe output elements are taken from the input at intervals given by the `rate`\nargument, as in dilated convolutions.\n\nThe `padding` argument has no effect on the size of each patch, it determines\nhow many patches are extracted. If `VALID`, only patches which are fully\ncontained in the input image are included. If `SAME`, all patches whose\nstarting point is inside the input are included, and areas outside the input\ndefault to zero."}}