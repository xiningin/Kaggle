{"cell_type":{"0ef69780":"code","db94cb07":"code","2ed0c029":"code","4c53993e":"code","c6347edc":"code","b6402436":"code","01268da2":"code","8f274e86":"code","b446eb0e":"code","725c6680":"code","6b63cd60":"code","34b80783":"code","e2b7e520":"code","19ea2e6a":"code","fd99eb73":"code","4c8d0a34":"code","2ef95cda":"code","48e812dd":"code","638324f6":"markdown","835d7dec":"markdown","481001ca":"markdown","8f6805a2":"markdown","d0468f66":"markdown","71a5e668":"markdown","b665dacd":"markdown","63b843d3":"markdown"},"source":{"0ef69780":"# import all the tools\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom wordcloud import WordCloud\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","db94cb07":"# import the file\ncomp_df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding='ISO-8859-1',\n                      header=None)\n\n# Change the columns' name\ncomp_df.columns = ['target','id','date','flag','user','text']\n\n# Change label '4' to '1'\ncomp_df.target = comp_df.target.map({4:1,0:0})","2ed0c029":"# show the distribution of labels\nsns.countplot(comp_df.target)\nplt.show()","4c53993e":"# Remove all the unwanted elements in text\ndef format_text(df,col):\n  #Remove @ tags\n  comp_df = df.copy()\n    \n  # remove all the punctuation\n  comp_df[col] = comp_df[col].str.replace(r'(@\\w*)','')\n\n  #Remove URL\n  comp_df[col] = comp_df[col].str.replace(r\"http\\S+\", \"\")\n\n  #Remove # tag and the following words\n  comp_df[col] = comp_df[col].str.replace(r'#\\w+',\"\")\n\n  #Remove all non-character\n  comp_df[col] = comp_df[col].str.replace(r\"[^a-zA-Z ]\",\"\")\n\n  # Remove extra space\n  comp_df[col] = comp_df[col].str.replace(r'( +)',\" \")\n  comp_df[col] = comp_df[col].str.strip()\n\n  # Change to lowercase\n  comp_df[col] = comp_df[col].str.lower()\n\n  return comp_df\n\nformated_comp_df = format_text(comp_df,'text')\n\n# Drop the columns we don't want\nformated_comp_df.drop(['id','date','flag','user'],axis=1,inplace=True)","c6347edc":"formated_comp_df.head()","b6402436":"# data size\nformated_comp_df.shape[0]","01268da2":"def show_wc(df,stopword=False):\n    if stopword:\n        stop_words = stopwords.words('english')\n        wc = WordCloud(max_words=1000, background_color='white',stopwords=stop_words,colormap='rainbow',height=1000,width=700)\n    else:\n        wc = WordCloud(max_words=1000, background_color='white',colormap='rainbow',height=1000,width=700)\n    text =df.text.values\n    wc.generate(str(text))\n    \n    fig = plt.figure()\n    plt.imshow(wc)\n    fig.set_figwidth(10)\n    fig.set_figheight(10)\n\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","8f274e86":"show_wc(formated_comp_df, stopword=True)","b446eb0e":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom nltk import TweetTokenizer","725c6680":"tweet_tokenizer= TweetTokenizer(reduce_len=3)\nformated_comp_df.text = formated_comp_df.text.apply(lambda x: tweet_tokenizer.tokenize(x))","6b63cd60":"# See the distribution of sentence length \nsen_length = [len(x) for x in formated_comp_df.text.tolist()]\nsns.boxplot(sen_length)\nplt.title('Distribution of sentence length')\nplt.show()","34b80783":"# Function for build text list\ndef word_list(dataframe,clear_stopwords=False):\n  df = dataframe.copy()\n    \n  # lemmatize the word\n  lemma = nltk.WordNetLemmatizer()\n  df.text = df.text.apply(lambda x: [lemma.lemmatize(i) for i in x])\n  if clear_stopwords:\n    stop_words = stopwords.words('english')\n    df.text = df.text.apply(lambda x: [i for i in x if not i in stop_words])\n    \n  # combined into a sentence again\n  df.text = df.text.apply(lambda x: ' '.join(x))\n\n  word_list = df.text.tolist()\n  return word_list\n\n# Tokenize and padding the text list\ndef token_padding(word_list, tokenizer,length,pad):\n  sequences = tokenizer.texts_to_sequences(word_list)\n  padding = pad_sequences(sequences, maxlen=length,padding=pad,truncating='post')\n  return padding","e2b7e520":"# tokenize and padding\nembedding_dim = 50 # since we will use 50d of Glove\nmax_length = 30\npadding = 'post'\noov_tok = '<OOV>'\nbatch_size = 256\ntraining_size = formated_comp_df.shape[0]\ntest_size = 10000 #number of samples for testing, not percentage\nstop_words_clear = False\n\ncropped_df = formated_comp_df.sample(training_size) #get the sample randomly from the original dataset\n\n# Split the data into training and validation(test) set\nx_train,x_val,y_train,y_val = train_test_split(cropped_df.drop('target',axis=1),cropped_df['target'],test_size=round(test_size\/training_size,2))\nx_train_list = word_list(x_train,clear_stopwords=stop_words_clear)\nx_val_list = word_list(x_val,clear_stopwords=stop_words_clear)\n\n# make tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train_list)\n\nx_train_pad = token_padding(x_train_list,tokenizer,max_length,padding)\nx_val_pad = token_padding(x_val_list,tokenizer,max_length,padding)\n\ntrain_set = tf.data.Dataset.from_tensor_slices((x_train_pad,np.array(y_train))).batch(batch_size)\ntest_set = tf.data.Dataset.from_tensor_slices((x_val_pad,np.array(y_val))).batch(batch_size)","19ea2e6a":"word_index = tokenizer.word_index\nvocab_size = len(word_index)+1\nembeddings_index = {};\n\n# Word embeddings with Glove6B 50D\n# Add the glove6b 50D to the input data of kaggle before use it\nwith  open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt','r') as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = vector\n        \n# Build embedding matrix \nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    vector = embeddings_index.get(word)\n    if vector is not None:\n        embeddings_matrix[i] = vector","fd99eb73":"from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nimport tensorflow.keras.regularizers as regularizers","4c8d0a34":"class myCallback(keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs=()):\n    train_loss = logs.get('loss')\n    val_loss = logs.get('val_loss')\n    if val_loss - train_loss > 0.2:\n      print('Loss on validation set is much higher than training set. Training cancelled')\n      self.model.stop_training=True\n\n\ndef build_model(vocab_size,embedding_dim,max_length):\n  model = keras.models.Sequential([\n          Embedding(input_dim=vocab_size,\n                    output_dim=embedding_dim,\n                    input_length=max_length,\n                   weights=[embeddings_matrix],\n                   trainable=False),\n          Bidirectional(LSTM(64,dropout=0.3,return_sequences=True)),\n          Dropout(0.3),\n          Bidirectional(LSTM(64,dropout=0.3)),\n          Dropout(0.3),\n          Dense(512,activation='relu'),\n          Dropout(0.3),\n          Dense(1,activation='sigmoid')])\n      \n  model.compile(optimizer=keras.optimizers.Adam(),loss='binary_crossentropy',metrics=['acc'])\n  return model","2ef95cda":"num_epochs = 40\ncallbacks = myCallback()\nmodel = build_model(vocab_size,embedding_dim,max_length)\n\n# start training\nhistory = model.fit(train_set,epochs=num_epochs,validation_data=(test_set),verbose=1,callbacks=[callbacks])","48e812dd":"fig, (ax1) = plt.subplots(figsize=(10,5))\nax2 =ax1.twinx()\n\nax1.plot(range(1,num_epochs+1),history.history['loss'])\nax1.plot(range(1,num_epochs+1),history.history['val_loss'])\n\nax2.plot(range(1,num_epochs+1),history.history['acc'],color='green')\nax2.plot(range(1,num_epochs+1),history.history['val_acc'],color='brown')\nplt.xticks(list(range(1,num_epochs+1,1)))\n\nax1.legend(['Training loss','Validation loss'])\nax2.legend(['Acc','Validation Acc'])\n\nax1.set_xlabel('epochs')\nax1.set_ylabel('loss')\nax2.set_ylabel('Accuracy')\n\nplt.show()","638324f6":"Thank you","835d7dec":"# Plotting the loss and accuracy","481001ca":"### Conlcusion\n\n- According to the graph, the model is slightly underfitting, both loss of training set and loss of test set are still slightly decreasing while the loss of test set is lower than training set.\n\n- 40 epochs seems not enough. Increase the number of epochs or use a larger learning rate would be helped.","8f6805a2":"# Date preparation\n\n\n- Split into training set and validation(test) set\n- Lemmatize the words\n- Tokenization: word -> number\n- Word padding\n- Transoform the dataset with batch size 256.","d0468f66":"## Creating word embedding matrix","71a5e668":"# Visualize the words in word cloud","b665dacd":"# Twitter sentiment classification using keras\n\n### Context\n- This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n\n### Content\nIt contains the following 6 fields:\n\n- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n\n- ids: The id of the tweet ( 2087)\n\n- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n\n- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n\n- user: the user that tweeted (robotickilldozr)\n\n- text: the text of the tweet (Lyx is cool)\n\n","63b843d3":"# Setting up model\n\n\nModel:\n- Embedding layer\n- 2 Bidirectional LSTM layers with 64 units.\n- 2 Dense layers, 1 for output\n\nCallbacks for Early stopping\n- Track the training loss vs validation loss, if training loss is far lower than validation loss, the training is cancelled."}}