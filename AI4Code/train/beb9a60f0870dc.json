{"cell_type":{"ce95955b":"code","172d3aaf":"code","262d5c03":"code","e5077ad8":"code","c4cddde8":"code","59187716":"code","432ff9bc":"code","58b93c3b":"code","afa02639":"code","bc43b266":"code","12d9ce9c":"code","dc08d83b":"code","80c70b1a":"code","390eda98":"code","08af458e":"code","16c8347a":"code","8e455d78":"code","88ce54c9":"code","5ed13b44":"code","1d5b5cd0":"code","94fadd44":"code","db365c80":"code","544a0f91":"code","2a2de8e3":"markdown","1753a1d8":"markdown","0d761011":"markdown","6851ae6c":"markdown","ea0afd89":"markdown","6b4b12b6":"markdown","c08cb42d":"markdown","af4c58fa":"markdown","5bca2950":"markdown","fd36636a":"markdown","6221d754":"markdown","2fd61441":"markdown","3e8d8589":"markdown"},"source":{"ce95955b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","172d3aaf":"from sklearn.preprocessing import LabelEncoder\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nfrom sklearn import feature_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import make_scorer, roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost\nimport lightgbm\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","262d5c03":"df = pd.read_csv('..\/input\/news-title-dataset-csv\/News Title.csv', sep = \";\")\ndf.head()","e5077ad8":"print(f\"The dataset contains { df.Category.unique() } unique categories\")","c4cddde8":"print(df.isnull().sum()) #checking for total null values","59187716":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf.Category.value_counts().plot.pie(explode=[0,0.1,0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\n\ncount_category = df.groupby('Category')['No'].nunique()\ntotal_category = [count_category.Technology, count_category.Business,count_category.Entertainment, count_category.Medical]\nlabel = df.Category.unique()\nbar = pd.DataFrame({'Category':label, 'total':total_category})\n\nbar = sns.barplot(\n    data= bar,\n    x= 'Category',\n    y= 'total')","432ff9bc":"#label encoding the categories. After this each category would be mapped to an integer.\nencoder = LabelEncoder()\ndf['categoryEncoded'] = encoder.fit_transform(df['Category'])\ndf","58b93c3b":"from wordcloud import WordCloud\n\nstop = set(stopwords.words('english'))\n\nEntertainment = df[df['categoryEncoded'] == 1]\nEntertainment = Entertainment['News Title']\nBusiness = df[df['categoryEncoded'] == 0]\nBusiness = Business['News Title']\nTechnology = df[df['categoryEncoded'] == 3]\nTechnology = Technology['News Title']\nMedical = df[df['categoryEncoded'] == 2]\nMedical = Medical['News Title']\n\n\ndef wordcloud_draw(dataset, color = 'white'):\n\n    words = ' '.join(dataset)\n\n    cleaned_word = ' '.join([word for word in words.split()\n\n    if (word != 'news' and word != 'text')])\n\n    wordcloud = WordCloud(stopwords = stop,\n\n    background_color = color,\n\n    width = 2500, height = 2500).generate(cleaned_word)\n\n    plt.figure(1, figsize = (10,7))\n\n    plt.imshow(wordcloud)\n\n    plt.axis(\"off\")\n\n    plt.show()\nprint(\"Entertaiment WordCloud\")\nwordcloud_draw(Entertainment, 'white')\nprint(\"Business WordCloud\")\nwordcloud_draw(Business, 'white')\nprint(\"Technology WordCloud\")\nwordcloud_draw(Technology, 'white')\nprint(\"Medical WordCloud\")\nwordcloud_draw(Medical, 'white')","afa02639":"df['news_title'] = df['News Title']\ndf.head()","bc43b266":"from nltk.stem import PorterStemmer\n\ndef text_pre_processing(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    # case folding and remove punctutation\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    # tokenizing\n    lst_text = text.split()\n    # remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    # Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    # Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    #back to string from list\n    text = \" \".join(lst_text)\n    return text\n\ndf['news_title_clean'] = df['news_title'].apply(lambda news_title: text_pre_processing(news_title))\ndf.head()","12d9ce9c":"# split dataset\ndf_train, df_test = model_selection.train_test_split(df, test_size=0.3)\n# get target\ny_train = df_train[\"categoryEncoded\"].values\ny_test = df_test[\"categoryEncoded\"].values","dc08d83b":"# Tf-Idf (advanced variant of BoW)\nvectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))","80c70b1a":"corpus = df_train[\"news_title_clean\"]\nvectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","390eda98":"word = \"new york\"\ndic_vocabulary[word]","08af458e":"y = df_train[\"categoryEncoded\"]\nX_names = vectorizer.get_feature_names()\np_value_limit = 0.95\ndtf_features = pd.DataFrame()\nfor cat in np.unique(y):\n    chi2, p = feature_selection.chi2(X_train, y==cat)\n    dtf_features = dtf_features.append(pd.DataFrame(\n                   {\"feature\":X_names, \"score\":1-p, \"categoryEncoded\":cat}))\n    dtf_features = dtf_features.sort_values([\"categoryEncoded\",\"score\"], \n                    ascending=[True,False])\n    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\nX_names = dtf_features[\"feature\"].unique().tolist()","16c8347a":"for cat in np.unique(y):\n   print(\"# {}:\".format(cat))\n   print(\"  . selected features:\",\n         len(dtf_features[dtf_features[\"categoryEncoded\"]==cat]))\n   print(\"  . top features:\", \",\".join(\ndtf_features[dtf_features[\"categoryEncoded\"]==cat][\"feature\"].values[:10]))\n   print(\" \")","8e455d78":"# naive bayes\nnb = naive_bayes.MultinomialNB()\n# pipeline\nmodel_nb = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", nb)])\n# train classifier\nmodel_nb[\"classifier\"].fit(X_train, y_train)\n# test\nX_test = df_test[\"news_title_clean\"].values\npredicted_nb = model_nb.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n# Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_nb)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_nb))","88ce54c9":"# logistic regression\nlr = LogisticRegression(solver='liblinear')\n# pipeline\nmodel_lr = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", lr)])\n# train classifier\nmodel_lr[\"classifier\"].fit(X_train, y_train)\n# test\nX_test = df_test[\"news_title_clean\"].values\npredicted_lr = model_lr.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n# Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_lr)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_lr))","5ed13b44":"lgb_clf = lightgbm.LGBMClassifier(max_depth=2, random_state=4)\nmodel_lgb = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", lgb_clf)])\nmodel_lgb[\"classifier\"].fit(X_train, y_train)\n# test\nX_test = df_test[\"news_title_clean\"].values\npredicted_lgb = model_lgb.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n# Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_lgb)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_lgb))","1d5b5cd0":"knn = KNeighborsClassifier(n_neighbors=6)\nmodel_knn = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", knn)])\n# train classifier\nmodel_knn[\"classifier\"].fit(X_train, y_train)\n# test\nX_test = df_test[\"news_title_clean\"].values\npredicted_knn = model_knn.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n# Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_knn)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_knn))","94fadd44":"xgb = xgboost.XGBRFClassifier(num_class=4,\n                                  learning_rate=0.1,\n                                  num_iterations=1000,\n                                  max_depth=10,\n                                  feature_fraction=0.7, \n                                  scale_pos_weight=1.5,\n                                  boosting='gbdt',\n                                  metric='multiclass',\n                                  eval_metric='mlogloss')\nmodel_xgb = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", xgb)])\n# train classifier\nmodel_xgb[\"classifier\"].fit(X_train, y_train)\n# test\nX_test = df_test[\"news_title_clean\"].values\npredicted_xgb = model_xgb.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n# Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_xgb)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_xgb))","db365c80":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\n\n## pipeline\nmodel_rf = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"rf\", rf)])\n\n## train classifier\nmodel_rf[\"rf\"].fit(X_train, y_train)\n## test\nX_test = df_test[\"news_title_clean\"].values\npredicted_rf = model_rf.predict(X_test)\n\nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n## Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted_rf)\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted_rf))","544a0f91":"model_list = ['Naive Bayes', 'Logistic Regression', 'LightGBM','KNearestNeighbours', 'RandomForest', 'XGBRF']\naccuracy_list = [(metrics.accuracy_score(y_test, predicted_nb)*100), (metrics.accuracy_score(y_test, predicted_lr)*100), \n                 (metrics.accuracy_score(y_test, predicted_lgb)*100), (metrics.accuracy_score(y_test, predicted_knn)*100),\n               (metrics.accuracy_score(y_test, predicted_rf)*100), (metrics.accuracy_score(y_test, predicted_xgb)*100)]\nplt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y=accuracy_list, palette = \"husl\", saturation =2.0)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('% of Accuracy', fontsize = 20)\nplt.title('Accuracy of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,2)}%', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","2a2de8e3":"# Conclusion\n\nAfter being trained in 6 different algorithms, the best results are shown by the logistic regression algorithm with 90% accuracy, therefore the model that will be used is then built with the fastapi, flask, or django framework.","1753a1d8":"# Exploratory Data Analysis (EDA)\n\nDo EDA to see if there are missing values, checking classes in the category column and seeing the distribution of data from each class. From the news-title-dataset, it can be seen that there are 4 categories available, namely technology, business, entertainment, and medical. After that, check for missing values \u200b\u200busing the isnull() function from pandas and get 0 null values \u200b\u200bfrom the column news title and category","0d761011":"# Data Ingestion\n\nThe next step is to transform Category data which previously used characters to distinguish each class into numbers from 0-3, to make it easier for the computer to read it and also make easier to create a word cloud. To do that,we will using the labelencoder class from the sklearn library.\n\nBussiness = 0\n\nEntertaiment = 1\n\nMedical = 2\n\nTechnology = 3","6851ae6c":"# BoW (Bag of Words)\n\nNext, change the sentence in the news title into a vector (number) to make it easier for the computer to recognize patterns using the TF-IDF algorithm, for example word = \"new york\" when vectorized using TF-IDF becomes 5945","ea0afd89":"# Visualization Data\n\nThe next step in EDA is to visualize the data in the form of circles and barcharts, to see how much data is distributed from each category and the results are: Entertainment has a total data of 36.6% of the entire dataset, the second is the business category of 27%, the third is technology with 25 .6% and the last one is medical with 10.8% data from the whole dataset.","6b4b12b6":"# Training with ML Algorithm\n\nThe next step is training using machine learning algorithms including naive bayes, k-neirest neighbord, random forest, Light GBM, eXtra Gradient Boosting Random Forest, and logistic regression, and evaluating with a confusion matrix to get accuracy","c08cb42d":"# Text Preprocessing\n\nBefore entering the Bag of Words or vectorization process using TF-IDF, text of news titles are cleaned using the case folding algorithm, remove punctuation, tokenizing, stop words, stemming, lemmitiasation using the nltk library","af4c58fa":"# Feature Engineering\n\nAfter converting news_title_clean to vector, then it is inputted into X variable as a feature which the machine will learn the pattern.","5bca2950":"# References\n\n* https:\/\/www.kaggle.com\/foolofatook\/news-classification-using-bert\n* G. A. Dalaorao, A. M. Sison and R. P. Medina, \"Integrating Collocation as TF-IDF Enhancement to Improve Classification Accuracy,\" 2019 IEEE 13th International Conference on Telecommunication Systems, Services, and Applications (TSSA), 2019, pp. 282-285, doi: 10.1109\/TSSA48701.2019.8985458.\n* M. Laya, M. K. Delimayanti, A. Mardiyono, F. Setianingrum, A. Mahmudah and D. Anggraini, \"Classification of Natural Disaster on Online News Data Using Machine Learning,\" 2021 5th International Conference on Electrical, Telecommunication and Computer Engineering (ELTICOM), 2021, pp. 42-46, doi: 10.1109\/ELTICOM53303.2021.9590125.\n* https:\/\/www.kaggle.com\/cnokello\/news-article-classification-using-naive-bayes\n* Bracewell, D. B., Yan, J., Ren, F., & Kuroiwa, S. (2009). Category Classification and Topic Discovery of Japanese and English News Articles. Electronic Notes in Theoretical Computer Science, 225(C), 51\u201365. https:\/\/doi.org\/10.1016\/J.ENTCS.2008.12.066\n* Barua, A., Sharif, O., & Hoque, M. M. (2021). Multi-class Sports News Categorization using Machine Learning Techniques: Resource Creation and Evaluation. Procedia Computer Science, 193, 112\u2013121. https:\/\/doi.org\/10.1016\/J.PROCS.2021.11.002","fd36636a":"# Visualization Data with WordCloud\n\nNext, display the data using the wordcloud library to see what words often appear in each class, such as in the entertainment class, words that often appear are video, game, get rich, justin bieber, etc.","6221d754":"# Split Dataset\n\nSplit the dataset into train and test with a comparison of 70:30, 70% of the dataset for the training process and 30% for the testing process, for variable X = news_title_clean and variable y (destination) = categoryEncoded","2fd61441":"# **Importing Necessary Libraries**\n\nImport required libraries such as pandas, numpy, matplotblib, nltk, seaborn, and sklearn.","3e8d8589":"# Read Data\n\nReads the provided dataset using the read_csv function (for read .csv files) from pandas library, after that displays the top data using df.head(), as shown below the dataset has 3 columns, namely no, news title, and category"}}