{"cell_type":{"46172fac":"code","40270ede":"code","e044bb37":"code","0db3a438":"code","cece3c8c":"code","901068a3":"code","b9f004a3":"code","01a8358c":"code","712a35a6":"code","e99d3024":"code","3356544b":"code","42772020":"code","c109294c":"code","c85b3dd0":"code","1bd4d75f":"code","6bcbaede":"code","f7acd2dc":"code","bf96daed":"code","49e38061":"code","877f8775":"code","73c40732":"code","7b2f8ac0":"code","23671107":"code","75d08d38":"code","f906aae5":"code","0187f8db":"code","7ac4da71":"code","9c3dae1f":"code","98806c5f":"code","86f5f11f":"markdown","355fb278":"markdown","bb3b505b":"markdown","b3406ef0":"markdown","808e2561":"markdown","5b5b82c7":"markdown","39d0f32a":"markdown","bcc02804":"markdown","90119508":"markdown","33b745b9":"markdown","de60d7dc":"markdown","ac37691b":"markdown","948aa2f6":"markdown","940de59a":"markdown","e09fd699":"markdown"},"source":{"46172fac":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","40270ede":"df = pd.read_csv(\"\/kaggle\/input\/advertising\/advertising.csv\")\ndf.head()","e044bb37":"df.shape","0db3a438":"df.describe()","cece3c8c":"df.info()","901068a3":"df.duplicated().sum()\n","b9f004a3":"df.isnull().sum()","01a8358c":"pd.crosstab(df['Male'],df['Clicked on Ad']).sort_values(1,ascending=False)\n##data is balanced and equally distributed between gender","712a35a6":"no_click = df[df['Clicked on Ad'] == 0]\nclick = df[df['Clicked on Ad'] == 1]","e99d3024":"\nclick['Age'].hist(bins=10,label = 'click', alpha=0.5)\nno_click['Age'].hist(bins=10,label = 'no click', alpha=0.5)\nplt.legend(loc = 'age_click')\nplt.show()","3356544b":"\nclick['Area Income'].hist(bins=10,label = 'click', alpha=0.5)\nno_click['Area Income'].hist(bins=10,label = 'no click', alpha=0.5)\nplt.legend(loc = 'income_click')\nplt.show()","42772020":"\nclick['Daily Time Spent on Site'].hist(bins=10,label = 'click', alpha=0.5)\nno_click['Daily Time Spent on Site'].hist(bins=10,label = 'Clicked on Ad', alpha=0.5)\nplt.legend(loc = 'time_click')\nplt.show()","c109294c":"\nclick['Daily Internet Usage'].hist(bins=10,label = 'click', alpha=0.5)\nno_click['Daily Internet Usage'].hist(bins=10,label = 'no click', alpha=0.5)\nplt.legend(loc = 'fulltime_click')\nplt.show()","c85b3dd0":"import datetime\n\ndf['Date'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n\ndf['Hour']=df['Date'].dt.hour\ndf['Month']=df['Date'].dt.month\ndf['Weekdays']= df['Date'].dt.weekday","1bd4d75f":"pd.crosstab(df['Month'],df['Clicked on Ad'])\n#no season","6bcbaede":"sns.countplot('Month',hue='Clicked on Ad',data= df)","f7acd2dc":"sns.countplot('Weekdays',hue='Clicked on Ad',data= df)","bf96daed":"\nsns.countplot('Hour',hue='Clicked on Ad',data= df)","49e38061":"df.corr()","877f8775":"df['Age_bins'] = pd.cut(df['Age'], bins=[0, 29, 35, 42, 70], labels=['Young','Adult','Mid', 'Elder'])\ndf['Salary_bins'] = pd.cut(df['Area Income'], bins=[0, 30000.00, 55000.00, 65000.00, 85000.00], labels=['Low Income','Below Average','Above Average', 'Wealth'])\ndf['Daily_bins'] = pd.cut(df['Daily Internet Usage'], bins=[0, 139, 183, 218, 300], labels=['Short Time','Below Average','Above Average', 'Addict'])\ndf['Website_bins'] = pd.cut(df['Daily Time Spent on Site'], bins=[0, 51, 68, 78, 100], labels=['Short time','Below Average','Above Average', 'Addict'])","73c40732":"a = df.groupby(['Age_bins', 'Salary_bins', 'Male'])['Clicked on Ad'].sum().unstack('Salary_bins')\na.fillna(0)","7b2f8ac0":"df.groupby(['Age_bins', 'Website_bins', 'Male'])['Clicked on Ad'].sum().unstack('Website_bins')","23671107":"print('The number of towns is equal to {}.'.format(df['City'].nunique()))\nprint('The number of coutnries is equal to {}.'.format(df['Country'].nunique()))","75d08d38":"X = df.drop(['Date','Timestamp','Clicked on Ad', 'Ad Topic Line', 'Age_bins','City', 'Country', 'Salary_bins', 'Daily_bins', 'Website_bins'], axis=1)\ny = df['Clicked on Ad']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","f906aae5":"from  sklearn.preprocessing  import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","0187f8db":"import  statsmodels.api  as sm\nfrom scipy import stats\n\nX2   = sm.add_constant(X_train)\nmodel  = sm.Logit(y_train, X2)\nmodel2 = model.fit()\nprint(model2.summary(xname=['Const','Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage', 'Male', 'Hour', 'Month', 'Weekdays']))","7ac4da71":"X.drop(['Male','Hour', 'Month', 'Weekdays'], axis= 1, inplace = True)","9c3dae1f":"from sklearn.linear_model import LogisticRegression                                                                  \nlr = LogisticRegression()                \nlr.fit(X_train, y_train)                                                                        \ny_pred = lr.predict(X_test)   ","98806c5f":"from sklearn import metrics\nprint (metrics.accuracy_score(y_test, y_pred))\nprint (metrics.confusion_matrix(y_test, y_pred))\nprint (metrics.classification_report(y_test, y_pred))","86f5f11f":"****How to evaluate the model?\n\nFrom a classification prediction, the model generated four possible outcomes: \n\n* True Positive (TP): the ones that model predict to click and they clicked \n* False Positive (FP): the ones that model predict to click and they did not click\n* True Negative (TN): the ones that model predict to not click and they did not click\n* False Negative (FN): the ones that model predict to not click and they clicked\n\n\nWe plot the result in a matrix NxN, where N represents the number target. In our case, it will be a matrix 2x2. We call it Confusion Matric. From it, we compute the outcome to evaluate our model. The standard metrics are accuracy, precision, recall, and F1-score.\n\nDefinition from Sklearn:\n\n* The precision is the ratio TP \/ (TP + FP) where TP is the number of true positives and FP the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n \n* The recall is the ratio TP \/ (TP + fFN) where TP is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\n* The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. \n \n* In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.","355fb278":"Estimating a probabilistic model is indeed not far from a lineal regression. It is used, anytime that you need to estimate a probability  of an event happens. For example, probability to buy an item, probability to cheat an exam, or to get one specific outcome at the casino. All that kind of events, you can do it with a Logistics regression model or a probabilistic regression model. Both are near to estimate a linear regression but instead of estimating directly the coefficients, logit and probit use a transformation with a link function to ensure the probability will be between 0 and 1 while standard OLS can easily estimate probabilities outside that range. \n\n","bb3b505b":"No Duplicate","b3406ef0":"The average income revenue are a big indicator about who wil click. usually from the lowest to the average salary.","808e2561":"The graphs of the derivated features, Month, Weekdays and hours from TimeStamps doesn't show any patterns or any cycles\/seasonalities.","5b5b82c7":"No click are skew to the left whereas click to the right. So we can assume that older people are more sensitive to click than younger.","39d0f32a":"# Exploratory Data Analysis\n","bcc02804":"# Logistic Regression","90119508":"**Conclusion**\n\nOur model is able to determine if a target client will click not in the add by 96%.\nAccording to our previous analysis, people who are related to click on the add:\n\n- tend to have lower-medium income, between 40.000-50.000\n- generally older, 40 years-old and more\n- spend too much time neither on the website nor on the internet","33b745b9":"**Interpretation of the confusion matrix:**\n\nFrom the test set, 96% of the consumer's action has been predicted.\nAccording to the precision, the model predicted that a customer would click on the add, that customer click is correct 93% of the time. \n","de60d7dc":"# INTRODUCTION\n\nA click is a marketing metric that counts the number of customers who have interacted with your adds. The add-click expresses the percentage of clicks over the total view of the add. It defines your marketing campaign's success. The higher amount of clicks is a signal that your audiences are getting the appropriate commercial\/adds and for your company higher Returns on Investment (ROI). So, the purpose of this analysis is to predict who and why a customer will click on your add.\n\nThe dataset is composed of:\n\n    -Daily Time Spent on Site: Amount of time in the website\n    -Age: Customer age\n    -Area Income: Average revenue of customer \n    -Daily Internet Usage: daily average time on internet\n    -Ad Topic Line: Text of the advertissement\n    -City: City of the customer\n    -Male: Wheter or not user is a male\n    -Country: country of the user\n    -Timestamp: Time at which consumer clicked on Ad or closed window\n    -Clicked on Ad: 0 or 1 indicated clicking on Ad","ac37691b":"Usually, Ordinary Least Square (OLS) is a method used to estimate a linear regression model's parameter. \nWe use it to describe the relationship between the independent variables and the dependent variable. In our case, we want to interpret the relation of our explained variable on the feature Click on Ad. In other words, we want to know if X affects Y (Alternative Hypothesis). To determine the significance of our variable, we look at our p-value score(p<0.05). The smaller is it, the stronger the evidence that you should reject the null hypothesis\nOn the other hand, we will drop the variable with a high p-value (p>0.05).\nAccording to the OLS Regression result, we will only keep the variable Daily Time Spent on Site, Age, Area Income, Daily  and Internet Usage.\n","948aa2f6":"# Prepare the Data","940de59a":"No missing values","e09fd699":"Both graph about, time spent on the website and total internet, are very significant"}}