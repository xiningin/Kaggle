{"cell_type":{"fe8038cf":"code","4d62e566":"code","bc3c484b":"code","93445cf8":"code","c8189104":"code","0421d135":"code","e54b0128":"code","0d933589":"code","41cb5aba":"code","7e4411e9":"code","9b2e39dd":"code","089a9100":"code","e45642a7":"code","3d345bb5":"code","38b4c5b6":"code","7eea322f":"code","cd35a0e6":"code","38215edc":"code","5785b4da":"code","3133688a":"code","1afbe736":"code","34abd776":"code","b0e63caa":"code","263b845f":"code","7c299da4":"code","54b5c80b":"code","89e542d7":"code","74232653":"code","e99c69f7":"code","6ae34e22":"code","787d4a65":"code","5164e29d":"code","e9ec3357":"code","cbb0389a":"code","8dd2e9dd":"code","f111567a":"code","c6826efc":"code","1564aed9":"code","64fdc2f0":"code","994b1a60":"code","3e4c7637":"code","068c2107":"code","beb32a71":"code","06f2b9db":"code","643db9c0":"code","b3f38d03":"code","c24ddb28":"code","d172285d":"code","5753d222":"code","440abcae":"code","9227e6bd":"code","5699e9ae":"code","86a4d84c":"code","4e273aae":"code","eed0b95f":"code","5c04a889":"code","0ee9b34c":"code","69aa9d06":"code","0d20388b":"code","22b5d8a7":"code","a4c46527":"code","bc073a02":"code","2cc6ac41":"code","744c206a":"code","f18bb168":"code","6d985b40":"markdown","6020268e":"markdown","4f1e4b33":"markdown","76c8ba19":"markdown","51ae03f6":"markdown","830e7f29":"markdown","0f124694":"markdown","f19efffa":"markdown","32541207":"markdown","991c14b3":"markdown","22097c87":"markdown","c0747fb3":"markdown","36ff5468":"markdown","952ec49d":"markdown","b48dc9b7":"markdown","23218f24":"markdown","526049ad":"markdown","147a30af":"markdown","6db48b4d":"markdown","16185526":"markdown","91885e58":"markdown","73aa57e0":"markdown","8c050a33":"markdown","82dc8006":"markdown","5dc2bf98":"markdown","b85ffe02":"markdown","000be917":"markdown","caaff8e4":"markdown","486e19f3":"markdown","af3040b4":"markdown","25753e05":"markdown","7127fa33":"markdown","11e2eab5":"markdown","967e7b6f":"markdown","d40601a6":"markdown","097edeaa":"markdown","9339750e":"markdown","bbfe0877":"markdown","cea5b75b":"markdown","d56af878":"markdown","36e58a9e":"markdown","7c53cc09":"markdown","1480cb37":"markdown","64646450":"markdown","b8badac0":"markdown","0be0f705":"markdown","b64f7af1":"markdown","1a82b39a":"markdown","c341e22d":"markdown"},"source":{"fe8038cf":"import pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nimport category_encoders as ce\nimport warnings\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\nwarnings.filterwarnings(\"ignore\")","4d62e566":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest.head()","bc3c484b":"categories = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/categories.csv')\npd.DataFrame(categories.category_name.values.reshape(-1, 4))","93445cf8":"#create broader category groupings\ncategories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\ncategories['group_name'] = categories['group_name'].str.strip()\n#label encode group names\ncategories['group_id']  = le.fit_transform(categories.group_name.values)\ncategories.sample(5)","c8189104":"#load items\nitems = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/items.csv')\n\n#clean item_name\nitems['item_name'] = items['item_name'].str.lower()\nitems['item_name'] = items['item_name'].str.replace('.', '')\nfor i in [r'[^\\w\\d\\s\\.]', r'\\bthe\\b', r'\\bin\\b', r'\\bis\\b',\n          r'\\bfor\\b', r'\\bof\\b', r'\\bon\\b', r'\\band\\b',  \n          r'\\bto\\b', r'\\bwith\\b' , r'\\byo\\b']:\n    items['item_name'] = items['item_name'].str.replace(i, ' ')\nitems['item_name'] = items['item_name'].str.replace(r'\\b.\\b', ' ')\n\n#extract first n characters of name\nitems['item_name_no_space'] = items['item_name'].str.replace(' ', '')\nitems['item_name_first4'] = [x[:4] for x in items['item_name_no_space']]\nitems['item_name_first6'] = [x[:6] for x in items['item_name_no_space']]\nitems['item_name_first11'] = [x[:11] for x in items['item_name_no_space']]\ndel items['item_name_no_space']\n                              \n#label encode these columns\nitems.item_name_first4 = le.fit_transform(items.item_name_first4.values)\nitems.item_name_first6 = le.fit_transform(items.item_name_first6.values)\nitems.item_name_first11 = le.fit_transform(items.item_name_first11.values)\n\n#join category_name, group_name and group_id to items\nitems = items.join(categories.set_index('category_id'), on='category_id')\nitems.sample(10)","0421d135":"dupes = items[(items.duplicated(subset=['item_name','category_id'],keep=False))]\ndupes['in_test'] = dupes.item_id.isin(test.item_id.unique())\ndupes = dupes.groupby('item_name').agg({'item_id':['first','last'],'in_test':['first','last']})\n\n#if both item id's are in the test set do nothing\ndupes = dupes[(dupes[('in_test', 'first')]==False) | (dupes[('in_test', 'last')]==False)]\n#if only the first id is in the test set assign this id to both\ntemp = dupes[dupes[('in_test', 'first')]==True]\nkeep_first = dict(zip(temp[('item_id', 'last')], temp[('item_id',  'first')]))\n#if neither id or only the second id is in the test set, assign the second id to both\ntemp = dupes[dupes[('in_test', 'first')]==False]\nkeep_second = dict(zip(temp[('item_id', 'first')], temp[('item_id',  'last')]))\nitem_map = {**keep_first, **keep_second}","e54b0128":"#loading sales data\nsales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsales = (sales\n    .query('0 < item_price < 50000 and 0 < item_cnt_day < 1001') #removing outliers\n    .replace({\n        'shop_id':{0:57, 1:58, 11:10}, #replacing obsolete shop id's\n        'item_id':item_map #fixing duplicate item id's  \n    })    \n)\n\n#removing shops which don't appear in the test set\nsales = sales[sales['shop_id'].isin(test.shop_id.unique())]\n\nsales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\nsales['weekday'] = sales.date.dt.dayofweek\n\n#first day the item was sold, day 0 is the first day of the training set period\nsales['first_sale_day'] = sales.date.dt.dayofyear \nsales['first_sale_day'] += 365 * (sales.date.dt.year-2013)\nsales['first_sale_day'] = sales.groupby('item_id')['first_sale_day'].transform('min').astype('int16')\n\n#revenue is needed to accurately calculate prices after grouping\nsales['revenue'] = sales['item_cnt_day']*sales['item_price']","0d933589":"temp = sales.groupby(['shop_id','weekday']).agg({'item_cnt_day':'sum'}).reset_index()\ntemp = pd.merge(temp, sales.groupby(['shop_id']).agg({'item_cnt_day':'sum'}).reset_index(), on='shop_id', how='left')\ntemp.columns = ['shop_id','weekday', 'shop_day_sales', 'shop_total_sales']\ntemp['day_quality'] = temp['shop_day_sales']\/temp['shop_total_sales']\ntemp = temp[['shop_id','weekday','day_quality']]\n\ndates = pd.DataFrame(data={'date':pd.date_range(start='2013-01-01',end='2015-11-30')})\ndates['weekday'] = dates.date.dt.dayofweek\ndates['month'] = dates.date.dt.month\ndates['year'] = dates.date.dt.year - 2013\ndates['date_block_num'] = dates['year']*12 + dates['month'] - 1\ndates['first_day_of_month'] = dates.date.dt.dayofyear\ndates['first_day_of_month'] += 365 * dates['year']\ndates = dates.join(temp.set_index('weekday'), on='weekday')\ndates = dates.groupby(['date_block_num','shop_id','month','year']).agg({'day_quality':'sum','first_day_of_month':'min'}).reset_index()\n\ndates.query('shop_id == 28').head(15)","41cb5aba":"sales = (sales\n     .groupby(['date_block_num', 'shop_id', 'item_id'])\n     .agg({\n         'item_cnt_day':'sum', \n         'revenue':'sum',\n         'first_sale_day':'first'\n     })\n     .reset_index()\n     .rename(columns={'item_cnt_day':'item_cnt'})\n)\nsales.sample(5)","7e4411e9":"df = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]]))))\n\ndf = pd.DataFrame(np.vstack(df), columns=['shop_id', 'item_id', 'date_block_num'])\ndf.head()","9b2e39dd":"#add the appropriate date_block_num value to the test set\ntest['date_block_num'] = 34\ndel test['ID']","089a9100":"#append test set to training dataframe\ndf = pd.concat([df,test]).fillna(0)\ndf = df.reset_index()\ndel df['index']","e45642a7":"#join sales and item inforamtion to the training dataframe\ndf = pd.merge(df, sales, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\ndf = pd.merge(df, dates, on=['date_block_num','shop_id'], how='left')\ndf = pd.merge(df, items.drop(columns=['item_name','group_name','category_name']), on='item_id', how='left')","3d345bb5":"#loading shops.csv\nshops = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/shops.csv')\n\n#clustering shops\nshops_cats = pd.DataFrame(\n    np.array(list(product(*[df['shop_id'].unique(), df['category_id'].unique()]))),\n    columns =['shop_id', 'category_id']\n)\ntemp = df.groupby(['category_id', 'shop_id']).agg({'item_cnt':'sum'}).reset_index()\ntemp2 = temp.groupby('shop_id').agg({'item_cnt':'sum'}).rename(columns={'item_cnt':'shop_total'})\ntemp = temp.join(temp2, on='shop_id')\ntemp['category_proportion'] = temp['item_cnt']\/temp['shop_total']\ntemp = temp[['shop_id', 'category_id', 'category_proportion']]\nshops_cats = pd.merge(shops_cats, temp, on=['shop_id','category_id'], how='left')\nshops_cats = shops_cats.fillna(0)\n\nshops_cats = shops_cats.pivot(index='shop_id', columns=['category_id'])\nkmeans = KMeans(n_clusters=7, random_state=0).fit(shops_cats)\nshops_cats['shop_cluster'] = kmeans.labels_.astype('int8')\n\n#adding these clusters to the shops dataframe\nshops = shops.join(shops_cats['shop_cluster'], on='shop_id')","38b4c5b6":"#removing unused shop ids\nshops.dropna(inplace=True)\n\n#cleaning the name column\nshops['shop_name'] = shops['shop_name'].str.lower()\nshops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\n\n#creating a column for the type of shop\nshops['shop_type'] = 'regular'\n\n#there is some overlap in tc and mall, mall is given precedence\nshops.loc[shops['shop_name'].str.contains(r'tc'), 'shop_type'] = 'tc'\nshops.loc[shops['shop_name'].str.contains(r'mall|center|mega'), 'shop_type'] = 'mall'\nshops.loc[shops['shop_id'].isin([9,20]), 'shop_type'] = 'special'\nshops.loc[shops['shop_id'].isin([12,55]), 'shop_type'] = 'online'\n\n#the first word of shop name is largely sufficient as a city feature\nshops['shop_city'] = shops['shop_name'].str.split().str[0]\nshops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'\nshops.shop_city = le.fit_transform(shops.shop_city.values)\nshops.shop_type = le.fit_transform(shops.shop_type.values)\nshops.head()","7eea322f":"#add shop information to the training dataframe\ndf = pd.merge(df, shops.drop(columns='shop_name'), on='shop_id', how='left')\ndf.head()","cd35a0e6":"df['first_sale_day'] = df.groupby('item_id')['first_sale_day'].transform('max').astype('int16')\ndf.loc[df['first_sale_day']==0, 'first_sale_day'] = 1035\ndf['prev_days_on_sale'] = [max(idx) for idx in zip(df['first_day_of_month']-df['first_sale_day'],[0]*len(df))]\ndel df['first_day_of_month']","38215edc":"#freeing RAM, removing unneeded columns and encoding object columns\ndel sales, categories, shops, shops_cats, temp, temp2, test, dupes, item_map, \ndf.head()","5785b4da":"df['item_cnt_unclipped'] = df['item_cnt']\ndf['item_cnt'] = df['item_cnt'].clip(0, 20)","3133688a":"def downcast(df):\n    #reduce size of the dataframe\n    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n    int_cols = [c for c in df if df[c].dtype in ['int64']]\n    df[float_cols] = df[float_cols].astype('float32')\n    df[int_cols] = df[int_cols].astype('int16')\n    return df\ndf = downcast(df)","1afbe736":"df['item_age'] = (df['date_block_num'] - df.groupby('item_id')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first4_age'] = (df['date_block_num'] - df.groupby('item_name_first4')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first6_age'] = (df['date_block_num'] - df.groupby('item_name_first6')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first11_age'] = (df['date_block_num'] - df.groupby('item_name_first11')['date_block_num'].transform('min')).astype('int8')\ndf['category_age'] = (df['date_block_num'] - df.groupby('category_id')['date_block_num'].transform('min')).astype('int8')\ndf['group_age'] = (df['date_block_num'] - df.groupby('group_id')['date_block_num'].transform('min')).astype('int8')\ndf['shop_age'] = (df['date_block_num'] - df.groupby('shop_id')['date_block_num'].transform('min')).astype('int8')","34abd776":"#indicates whether shops have previously sold the item\ntemp = df.query('item_cnt > 0').groupby(['item_id','shop_id']).agg({'date_block_num':'min'}).reset_index()\ntemp.columns = ['item_id', 'shop_id', 'item_shop_first_sale']\ndf = pd.merge(df, temp, on=['item_id','shop_id'], how='left')\ndf['item_shop_first_sale'] = df['item_shop_first_sale'].fillna(50)\n#item age that stays at 0 if a shop hasn't sold the item\ndf['item_age_if_shop_sale'] = (df['date_block_num'] > df['item_shop_first_sale']) * df['item_age']\n#the length of time an item has been for sale without being sold at individual shops\ndf['item_age_without_shop_sale'] = (df['date_block_num'] <= df['item_shop_first_sale']) * df['item_age']\ndel df['item_shop_first_sale']","b0e63caa":"def agg_cnt_col(df, merging_cols, new_col,aggregation):\n    temp = df.groupby(merging_cols).agg(aggregation).reset_index()\n    temp.columns = merging_cols + [new_col]\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#individual items across all shops\ndf = agg_cnt_col(df, ['date_block_num','item_id'],'item_cnt_all_shops',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'item_cnt_all_shops_median',{'item_cnt':'median'}) \n#all items in category at individual shops\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt_median',{'item_cnt':'median'}) \n#all items in category across all shops\ndf = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops_median',{'item_cnt':'median'})\n#all items in group\ndf = agg_cnt_col(df, ['date_block_num','group_id','shop_id'],'group_cnt',{'item_cnt':'mean'})\n#all items in group across all shops\ndf = agg_cnt_col(df, ['date_block_num','group_id'],'group_cnt_all_shops',{'item_cnt':'mean'})\n#all items at individual shops\ndf = agg_cnt_col(df, ['date_block_num','shop_id'],'shop_cnt',{'item_cnt':'mean'})\n#all items at all shops within the city\ndf = agg_cnt_col(df, ['date_block_num','shop_city'],'city_cnt',{'item_cnt':'mean'})","263b845f":"def new_item_sales(df, merging_cols, new_col):\n    temp = (\n        df\n        .query('item_age==0')\n        .groupby(merging_cols)['item_cnt']\n        .mean()\n        .reset_index()\n        .rename(columns={'item_cnt': new_col})\n    )\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#mean units sold of new item in category at individual shop\ndf = new_item_sales(df, ['date_block_num','category_id','shop_id'], 'new_items_in_cat')\n#mean units sold of new item in category across all shops\ndf = new_item_sales(df, ['date_block_num','category_id'], 'new_items_in_cat_all_shops')","7c299da4":"def agg_price_col(df, merging_cols, new_col):\n    temp = df.groupby(merging_cols).agg({'revenue':'sum','item_cnt_unclipped':'sum'}).reset_index()\n    temp[new_col] = temp['revenue']\/temp['item_cnt_unclipped']\n    temp = temp[merging_cols + [new_col]]\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#average item price\ndf = agg_price_col(df,['date_block_num','item_id'],'item_price')\n#average price of items in category\ndf = agg_price_col(df,['date_block_num','category_id'],'category_price')\n#average price of all items\ndf = agg_price_col(df,['date_block_num'],'block_price')","54b5c80b":"df = downcast(df)","89e542d7":"def lag_feature(df, lag, col, merge_cols):        \n    temp = df[merge_cols + [col]]\n    temp = temp.groupby(merge_cols).agg({f'{col}':'first'}).reset_index()\n    temp.columns = merge_cols + [f'{col}_lag{lag}']\n    temp['date_block_num'] += lag\n    df = pd.merge(df, temp, on=merge_cols, how='left')\n    df[f'{col}_lag{lag}'] = df[f'{col}_lag{lag}'].fillna(0).astype('float32')\n    return df","74232653":"lag12_cols = {\n    'item_cnt':['date_block_num', 'shop_id', 'item_id'],\n    'item_cnt_all_shops':['date_block_num', 'item_id'],\n    'category_cnt':['date_block_num', 'shop_id', 'category_id'],\n    'category_cnt_all_shops':['date_block_num', 'category_id'],\n    'group_cnt':['date_block_num', 'shop_id', 'group_id'],\n    'group_cnt_all_shops':['date_block_num', 'group_id'],\n    'shop_cnt':['date_block_num', 'shop_id'],\n    'city_cnt':['date_block_num', 'shop_city'],\n    'new_items_in_cat':['date_block_num', 'shop_id', 'category_id'],\n    'new_items_in_cat_all_shops':['date_block_num', 'category_id']\n}\nfor col,merge_cols in lag12_cols.items():\n    df[f'{col}_lag1to12'] = 0\n    for i in range(1,13):\n        df = lag_feature(df, i, col, merge_cols)\n        df[f'{col}_lag1to12'] += df[f'{col}_lag{i}']\n        if i > 2:\n            del df[f'{col}_lag{i}']\n    if col == 'item_cnt':\n        del df[f'{col}_lag1']\n        del df[f'{col}_lag2']        \n    else:\n        del df[col]","e99c69f7":"lag2_cols = {\n    'item_cnt_unclipped':['date_block_num', 'shop_id', 'item_id'],\n    'item_cnt_all_shops_median':['date_block_num', 'item_id'],\n    'category_cnt_median':['date_block_num', 'shop_id', 'category_id'],\n    'category_cnt_all_shops_median':['date_block_num', 'category_id']\n}\nfor col in lag2_cols:\n    df = lag_feature(df, 1, col, merge_cols)\n    df = lag_feature(df, 2, col, merge_cols)\n    if col!='item_cnt_unclipped':\n        del df[col]","6ae34e22":"df['item_cnt_diff'] = df['item_cnt_unclipped_lag1']\/df['item_cnt_lag1to12']\ndf['item_cnt_all_shops_diff'] = df['item_cnt_all_shops_lag1']\/df['item_cnt_all_shops_lag1to12']\ndf['category_cnt_diff'] = df['category_cnt_lag1']\/df['category_cnt_lag1to12']\ndf['category_cnt_all_shops_diff'] = df['category_cnt_all_shops_lag1']\/df['category_cnt_all_shops_lag1to12']","787d4a65":"df = lag_feature(df, 1, 'category_price',['date_block_num', 'category_id'])\ndf = lag_feature(df, 1, 'block_price',['date_block_num'])\ndel df['category_price'], df['block_price']","5164e29d":"df.loc[(df['item_age']>0) & (df['item_cnt_lag1to12'].isna()), 'item_cnt_lag1to12'] = 0\ndf.loc[(df['category_age']>0) & (df['category_cnt_lag1to12'].isna()), 'category_cnt_lag1to12'] = 0\ndf.loc[(df['group_age']>0) & (df['group_cnt_lag1to12'].isna()), 'group_cnt_lag1to12'] = 0","e9ec3357":"df['item_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['item_age'],df['shop_age'],[12]*len(df))]\ndf['item_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['item_age'],[12]*len(df))]\ndf['category_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\ndf['category_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]\ndf['group_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['group_age'],df['shop_age'],[12]*len(df))]\ndf['group_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['group_age'],[12]*len(df))]\ndf['city_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['date_block_num'],[12]*len(df))]\ndf['shop_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['shop_age'],[12]*len(df))]\ndf['new_items_in_cat_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\ndf['new_items_in_cat_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]","cbb0389a":"df = downcast(df)","8dd2e9dd":"def past_information(df, merging_cols, new_col, aggregation):\n    temp = []\n    for i in range(1,35):\n        block = df.query(f'date_block_num < {i}').groupby(merging_cols).agg(aggregation).reset_index()\n        block.columns = merging_cols + [new_col]\n        block['date_block_num'] = i\n        block = block[block[new_col]>0]\n        temp.append(block)\n    temp = pd.concat(temp)\n    df = pd.merge(df, temp, on=['date_block_num']+merging_cols, how='left')\n    return df\n\n#average item price in latest block item was sold\ndf = past_information(df, ['item_id'],'last_item_price',{'item_price':'last'})\n#total units of item sold at individual shop\ndf = past_information(df, ['shop_id','item_id'],'item_cnt_sum_alltime',{'item_cnt':'sum'})\n#total units of item sold at all shops\ndf = past_information(df, ['item_id'],'item_cnt_sum_alltime_allshops',{'item_cnt':'sum'})\n\n#these columns are no longer needed, and would cause data leakage if retained\ndel df['revenue'], df['item_cnt_unclipped'], df['item_price']","f111567a":"df['relative_price_item_block_lag1'] = df['last_item_price']\/df['block_price_lag1']","c6826efc":"df['item_cnt_per_day_alltime'] = (df['item_cnt_sum_alltime']\/df['prev_days_on_sale']).fillna(0)\ndf['item_cnt_per_day_alltime_allshops'] = (df['item_cnt_sum_alltime_allshops']\/df['prev_days_on_sale']).fillna(0)","1564aed9":"import gc\ngc.collect()\ndf = downcast(df)","64fdc2f0":"def matching_name_cat_age(df,n,all_shops):\n    temp_cols = [f'same_name{n}catage_cnt','date_block_num', f'item_name_first{n}','item_age','category_id']\n    if all_shops:\n        temp_cols[0] += '_all_shops'\n    else:\n        temp_cols += ['shop_id']\n    temp = []\n    for i in range(1,35):\n        block = (\n            df\n            .query(f'date_block_num < {i}')\n            .groupby(temp_cols[2:])\n            .agg({'item_cnt':'mean'})\n            .reset_index()\n            .rename(columns={'item_cnt':temp_cols[0]})\n        )\n        block = block[block[temp_cols[0]]>0]\n        block['date_block_num'] = i\n        temp.append(block)\n    temp = pd.concat(temp)\n    df = pd.merge(df, temp, on=temp_cols[1:], how='left')\n    return df\n\nfor n in [4,6,11]:\n    for all_shops in [True,False]:\n        df = matching_name_cat_age(df,n,all_shops)","994b1a60":"#assign appropriate datatypes\ndf = downcast(df)\nint8_cols = [\n    'item_cnt','month','group_id','shop_type',\n    'shop_city','shop_id','date_block_num','category_id',\n    'item_age',\n]\nint16_cols = [\n    'item_id','item_name_first4',\n    'item_name_first6','item_name_first11'\n]\nfor col in int8_cols:\n    df[col] = df[col].astype('int8')\nfor col in int16_cols:\n    df[col] = df[col].astype('int16')","3e4c7637":"def nearby_item_data(df,col):\n    if col in ['item_cnt_unclipped_lag1','item_cnt_lag1to12']:\n        cols = ['date_block_num', 'shop_id', 'item_id']\n        temp = df[cols + [col]] \n    else:\n        cols = ['date_block_num', 'item_id']\n        temp = df.groupby(cols).agg({col:'first'}).reset_index()[cols + [col]]   \n    \n    temp.columns = cols + [f'below_{col}']\n    temp['item_id'] += 1\n    df = pd.merge(df, temp, on=cols, how='left')\n    \n    temp.columns = cols + [f'above_{col}']\n    temp['item_id'] -= 2\n    df = pd.merge(df, temp, on=cols, how='left')\n    \n    return df\n\nitem_cols = ['item_cnt_unclipped_lag1','item_cnt_lag1to12',\n             'item_cnt_all_shops_lag1','item_cnt_all_shops_lag1to12']\nfor col in item_cols:\n    df = nearby_item_data(df,col)\n    \ndel temp","068c2107":"results = Counter()\nitems['item_name'].str.split().apply(results.update)\n\nwords = []\ncnts = []\nfor key, value in results.items():\n    words.append(key)\n    cnts.append(value)\n    \ncounts = pd.DataFrame({'word':words,'count':cnts})\ncommon_words = counts.query('count>200').word.to_list()\nfor word in common_words:\n    items[f'{word}_in_name'] = items['item_name'].str.contains(word).astype('int8')\ndrop_cols = [\n    'item_id','category_id','item_name','item_name_first4',\n    'item_name_first6','item_name_first11',\n    'category_name','group_name','group_id'\n]\nitems = items.drop(columns=drop_cols)","beb32a71":"#join these word vectors to the training dataframe\ndf = df.join(items, on='item_id')","06f2b9db":"def binary_encode(df, letters, cols):\n    encoder = ce.BinaryEncoder(cols=[f'item_name_first{letters}'], return_df=True)\n    temp = encoder.fit_transform(df[f'item_name_first{letters}'])\n    df = pd.concat([df,temp], axis=1)\n    del df[f'item_name_first{letters}_0']\n    name_cols = [f'item_name_first{letters}_{x}' for x in range(1,cols)]\n    df[name_cols] = df[name_cols].astype('int8')\n    return df\n\ndf = binary_encode(df, 11, 15)\n    \ndel df['item_name_first4'], df['item_name_first6']","643db9c0":"#save dataframe for later use\ndf.to_pickle('df_complete.pkl')","b3f38d03":"#Reset the kernel to clear memory.\n%reset -f","c24ddb28":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport shap\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npd.set_option('display.max_rows', 160)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 30)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d172285d":"#load the saved training dataframe\ndf = pd.read_pickle('..\/input\/files-top-scoring-notebook-output-exploration\/df_complete.pkl')\n\nX_train = df[~df.date_block_num.isin([0,1,33,34])]\ny_train = X_train['item_cnt']\ndel X_train['item_cnt']\n\nX_val = df[df['date_block_num']==33]\ny_val = X_val['item_cnt']\ndel X_val['item_cnt']\n\nX_test = df[df['date_block_num']==34].drop(columns='item_cnt')\nX_test = X_test.reset_index()\ndel X_test['index']\n\n#free memory\ndel df","5753d222":"def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n                     categorical_feature=cat_features)\n    return model","440abcae":"#skip this cell if directly loading saved model \nparams = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1023,\n    'min_data_in_leaf':10,\n    'feature_fraction':0.7,\n    'learning_rate': 0.01,\n    'num_rounds': 1000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\n#designating the categorical features which should be focused on\ncat_features = ['category_id','month','shop_id','shop_city']\n\nlgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n\n#save model for later use\nlgb_model.save_model('initial_lgb_model.txt')","9227e6bd":"#lgb_model = lgb.Booster(model_file='..\/input\/files-top-scoring-notebook-output-exploration\/initial_lgb_model.txt')#\n#lgb_model.params['objective'] = 'rmse'","5699e9ae":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsubmission['item_cnt_month'] = lgb_model.predict(X_test).clip(0,20)\nsubmission[['ID', 'item_cnt_month']].to_csv('initial_lgb_submission.csv', index=False)","86a4d84c":"#load item\/category\/shop information to make analysing the results easier\ncategories = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/categories.csv')\ncategories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\ncategories['group_name'] = categories['group_name'].str.strip()\n\nitems = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/items.csv')\nitems['item_name'] = items['item_name'].str.lower()\nfor i in [r'[^\\w\\d\\s]', r'\\bthe\\b', r'\\bin\\b', r'\\bfor\\b', r'\\bof\\b', r'\\bd\\b', r'\\bis\\b', r'\\bon\\b']:\n    items['item_name'] = items['item_name'].str.replace(i, ' ')\nitems['item_name'] = items['item_name'].str.replace(' ', '')\nitems = items.join(categories.set_index('category_id'), on='category_id')\n\nshops = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/shops.csv')\nshops['shop_name'] = shops['shop_name'].str.lower()\nshops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\nshops['shop_city'] = shops['shop_name'].str.split().str[0]\nshops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'","4e273aae":"X_train['lgb_pred'] = lgb_model.predict(X_train).clip(0,20)\nX_train['target'] = y_train\nX_train['sq_err'] = (X_train['lgb_pred']-X_train['target'])**2\n\nX_val['lgb_pred'] = lgb_model.predict(X_val).clip(0,20)\nX_val['target'] = y_val\nX_val['sq_err'] = (X_val['lgb_pred']-X_val['target'])**2\n\nX_test['lgb_pred'] = lgb_model.predict(X_test).clip(0,20)","eed0b95f":"data = X_train.groupby('date_block_num').agg({'lgb_pred':'mean','target':'mean','sq_err':'mean'}).reset_index()\ndata['new_item_rmse'] = np.sqrt(X_train.query('item_age<=1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\ndata['old_item_rmse'] = np.sqrt(X_train.query('item_age>1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\ndata = data.append([\n    {'date_block_num':33,\n     'target':X_val.target.mean(),\n     'lgb_pred':X_val.lgb_pred.mean(),\n     'sq_err':np.sqrt(X_val.sq_err.mean()),\n     'old_item_rmse':np.sqrt(X_val.query('item_age>1').sq_err.mean()),\n     'new_item_rmse':np.sqrt(X_val.query('item_age<=1').sq_err.mean())},\n    {'date_block_num':34,\n     'target':0,\n     'lgb_pred':X_test.lgb_pred.mean(),\n     'sq_err':0,\n     'old_item_rmse':0,\n     'new_item_rmse':0}\n    ],\n    ignore_index=True\n)\ndata['date'] = [x[:7] for x in pd.date_range(start='2013-03',end='2015-09',freq='MS').astype('str')]+['Validation','Test']","5c04a889":"temp = X_test.drop(columns='lgb_pred').sample(10000)\nexplainer = shap.TreeExplainer(lgb_model)\nshap_values = explainer.shap_values(temp)\nshap.summary_plot(shap_values, temp, max_display=30)","0ee9b34c":"fig = go.Figure(data=[\n    go.Bar(name='Prediction',x=data.date,y=data.lgb_pred),\n    go.Bar(name='Target',x=data.date,y=data.target)\n])\nfig.update_layout(\n    title='Mean Prediction and Target Values by Month',\n    xaxis={'title':'Month','type':'category'},\n    yaxis={'title':'Mean Value'},\n    legend={'yanchor':'top','y':1.05,'xanchor':'left','x':0.01},\n    template='plotly_dark'\n)\n\ntext = '''\n    The model may not be adequately accounting for<br>\n    the December sales spike. There is no systemic under or<br>\n    over-prediction visible for the test month, November.\n'''\nfig.add_annotation(\n    yref='paper', y=1.1,\n    xref='paper', x=0.7,\n    text=text,\n    font={'size':11},\n    showarrow=False)\n\ntext = '''\n    Prediction mean is very close to the  <br> \n    target mean in our validation set.\n'''\nfig.add_annotation(\n    xref='paper', x=0.95,\n    yref='paper', y=0.58,\n    text=text,\n    font={'size':11},\n    showarrow=True, arrowhead=1)\nfig.show()","69aa9d06":"fig = go.Figure(data=[go.Bar(\n    x=data.date[:-1],\n    y=data.sq_err[:-1],\n    marker_color=[x%12 for x in range(2,34)]\n)])\nfig.update_layout(\n    title='RMSE by Month',\n    xaxis={'title':'Month','type':'category'},\n    yaxis={'title':'RMSE'},\n    template='plotly_dark'\n)\ntext = '''\n    In the training set, RMSE is highest in December. <br>\n    \n'''\nfig.add_annotation(\n    yref='paper', y=0.9,\n    xref='paper', x=0.1,\n    text=text,\n    font={'size':11},\n    showarrow=False)\ntext = '''\n    Validation RMSE is higher than in any month of <br>\n    the training set. This is expected given the <br>\n    gap in training set and validation set RMSE we <br>\n    saw when training the model.\n'''\nfig.add_annotation(\n    yref='paper', y=1.1,\n    xref='paper', x=1,\n    text=text,\n    font={'size':11},\n    showarrow=False)\nfig.show()","0d20388b":"temp = X_val.groupby('item_age').agg({'sq_err':'mean','target':'mean'}).reset_index()\ntemp['sq_err'] = np.sqrt(temp['sq_err'])\nfig = go.Figure(data=[\n    go.Bar(name='RMSE',x=temp.item_age,y=temp.sq_err)\n])\nfig.update_layout(\n    title='Validation set RMSE by Item Age',\n    xaxis={'title':'Item Age','type':'category'},\n    yaxis={'title':'RMSE'},\n    template='plotly_dark'\n)\ntext = '''\n    RMSE is much higher for items with age 0 or 1. There is a <br>\n    sharp decrease in RMSE between ages 0 and 2, then a much <br>\n    flatter downwards trend as age increases beyond 2.\n'''\nfig.add_annotation(\n    yref='paper', y=0.8,\n    xref='paper', x=0.04,\n    text=text,\n    font={'size':11},\n    showarrow=False)\nfig.show()","22b5d8a7":"fig = go.Figure(data=[\n    go.Bar(name='Item Age <=1 RMSE',x=data.date[:-1],y=data.new_item_rmse[:-1]),\n    go.Bar(name='Item Age > 1 RMSE',x=data.date[:-1],y=data.old_item_rmse[:-1])\n])\nfig.update_layout(\n    title='RMSE by Month for New and Old Items',\n    xaxis={'title':'Month','type':'category'},\n    yaxis={'title':'Mean Value'},\n    legend={'yanchor':'top','y':1.05,'xanchor':'left','x':0.01},\n    template='plotly_dark'\n)\ntext = '''\n    For older items, performance on the validation set is no worse than <br>\n    performance on the training set. The higher overall validation set <br>\n    RMSE is caused by a significantly higher RMSE for new items.\n'''\nfig.add_annotation(yref='paper', y=1.1,\n                   xref='paper', x=0.93,\n                   text=text,\n                   font={'size':11},\n                   showarrow=False)\nfig.show()","a4c46527":"df = pd.read_pickle('..\/input\/files-top-scoring-notebook-output-exploration\/df_complete.pkl')\n(\ndf\n[df['category_id'].isin(X_test.category_id.unique())]\n.query('item_cnt>0')\n.groupby('category_id')\n.agg({\n    'category_age':'max',\n    'shop_id':['nunique','unique'],\n    'item_cnt':'sum'\n    })\n.join(categories['category_name'])\n.join(\n    X_test       \n    .groupby('category_id')\n    .agg({'item_id':'nunique'})\n    .rename(columns={'item_id':'test_set_items'})\n)\n.sort_values(('shop_id', 'nunique'))\n.head(20)\n)","bc073a02":"(\nX_val\n[X_val['category_id'].isin(X_test.category_id.unique())]\n.groupby('shop_id')\n.agg({\n    'sq_err':'mean',\n    'target':'mean',\n    'lgb_pred':'mean'\n})\n.join(\n    X_test\n    .rename(columns={'lgb_pred':'test_pred'})\n    .groupby('shop_id')\n    .agg({'test_pred':'mean'})\n)\n.join(shops['shop_name'])\n.sort_values('sq_err', ascending=False)\n.head(20)\n)","2cc6ac41":"(\nX_val\n[X_val['category_id'].isin(X_test.category_id.unique())]\n.groupby('category_id')\n.agg({\n    'sq_err':['sum','mean'],\n    'target':'mean',\n    'lgb_pred':['sum','mean'],\n    'item_id':'nunique'\n})\n.join(\n    X_test\n    .rename(columns={'lgb_pred':'test_pred','item_id':'test_items'})\n    .groupby('category_id')        \n    .agg({\n        'test_pred':['sum','mean'],\n        'test_items':'nunique'\n    }),\n    on='category_id'\n)\n.join(categories)\n.sort_values(('sq_err', 'mean'), ascending=False)\n.head(20)\n)","744c206a":"#edit this value to look through the biggest problem items in each category\nCATEGORY = 20\n(\nitems[\n    items['item_id'].isin(X_val.item_id.unique()) & \n    items['item_id'].isin(X_test.item_id.unique())\n]\n[['category_id','category_name','item_id','item_name']]\n.join(\n    X_test\n    .rename(columns={'lgb_pred':'test_pred'})\n    .groupby('item_id')        \n    .agg({'test_pred':'mean'}),\n    on='item_id'\n)\n.join(\n    X_val\n    .groupby('item_id')\n    .agg({\n        'lgb_pred':'mean',\n        'target':'mean',\n        'sq_err':'mean',\n        'same_name4catage_cnt_all_shops':'first',\n        'new_items_in_cat_all_shops_lag1to12':'first',\n        'item_cnt_all_shops_lag1':'first',\n        'category_cnt_all_shops_lag1':'first',\n        'item_cnt_sum_alltime_allshops':'first',\n        'prev_days_on_sale':'first'\n    })\n    .rename(columns={\n        'lgb_pred':'val_pred',\n        'target':'val_target',  \n    }),\n    on='item_id'\n)\n.query(f'category_id=={CATEGORY}')\n.sort_values('sq_err',ascending=False)\n.rename(columns={\n    'same_name4catage_cnt_all_shops':'name4mean',\n    'new_items_in_cat_all_shops_lag1to12':'new_in_cat_mean',\n    'item_cnt_all_shops_lag1':'item_cnt_lag1',\n    'category_cnt_all_shops_lag1':'cat_cnt_lag1',\n    'category_id':'cat',\n    'item_cnt_sum_alltime_allshops':'item_cnt_alltime'\n})\n.head(20)\n)","f18bb168":"CATEGORY = 20  \n(\nitems[items['item_id'].isin(X_test.item_id.unique())]\n[['category_id','category_name','item_id','item_name']]\n.join(\n    X_test\n    .groupby('item_id')\n    .agg({\n        'lgb_pred':'mean',\n        'same_name4catage_cnt_all_shops':'first',\n        'new_items_in_cat_all_shops_lag1to12':'first',\n        'item_cnt_all_shops_lag1':'first',\n        'category_cnt_all_shops_lag1':'first',\n        'item_cnt_sum_alltime_allshops':'first',\n        'prev_days_on_sale':'first'\n    })\n    .rename(columns={'lgb_pred':'test_pred'}),\n    on='item_id'\n)\n.query(f'category_id=={CATEGORY}')\n.sort_values('test_pred',ascending=False)\n.rename(columns={\n     'same_name4catage_cnt_all_shops':'name4mean',\n     'new_items_in_cat_all_shops_lag1to12':'new_in_cat_mean',\n     'item_cnt_all_shops_lag1':'item_cnt_lag1',\n     'category_cnt_all_shops_lag1':'cat_cnt_lag1',\n     'item_category_id':'cat',\n     'item_cnt_sum_alltime_allshops':'item_cnt_alltime'\n})\n.head(20)\n)","6d985b40":"The following view shows which shops are causing the most loss in our validation set. Shops 25, 31, 42, and 28 have the highest MSEs but also the highest target means (we would expect to see MSE scale with target mean). Shops 12 and 55 have the next highest MSEs, and do not have relatively high target means. These are the only two shops which have a category only they sell. Could they benefit from being segregated in the training stage?","6020268e":"# 4. Analysing Model Output<a id=\"40\"><\/a>\n[1. Introduction](#10)  \n[2. Preparing Dataset](#20)  \n&nbsp;&nbsp;[2.1 Preparing Item\/Category Information](#21)  \n&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n&nbsp;&nbsp;[2.5 Ages & Aggregating Sales\/Price information](#25)  \n&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n[3. Modelling](#30)  \n&nbsp;&nbsp;[3.1 Training Model](#31)  \n&nbsp;&nbsp;[3.2 Submitting](#32)  \n[4. Analysing Model Output](#40)  \n&nbsp;&nbsp;[4.1 Plots](#41)  \n&nbsp;&nbsp;[4.2 Table Views](#42)  \n\nBy looking more closely at the test set and our model output, we can make adjustments to improve on our initial model. The exploration shown below can provide insights that will allow us to make beneficial changes to the model.","4f1e4b33":"These features each have 3 lagged columns returned:\n* lag1 shows the value of the prior month\n* lag2 shows the value two months prior\n* lag1to12 is the sum of values across the previous 12 months\n\nOriginal columns will be deleted when they are no longer needed to avoid data leakage.","76c8ba19":"We create a feature showing how many days have passed between the first time an item was sold and the beginning of the current month.","51ae03f6":"We fill the missing values in the below columns with 0 because we know this is the correct value.","830e7f29":"**If you wish to explore the model without retraining, it can be directly loaded by uncommenting and running the cell below**","0f124694":"# 3. Modelling<a id=\"30\"><\/a>\n[1. Introduction](#10)  \n[2. Preparing Dataset](#20)  \n&nbsp;&nbsp;[2.1 Preparing Item\/Category Information](#21)  \n&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n&nbsp;&nbsp;[2.5 Ages & Aggregating Sales\/Price information](#25)  \n&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n[3. Modelling](#30)  \n&nbsp;&nbsp;[3.1 Training Model](#31)  \n&nbsp;&nbsp;[3.2 Submitting](#32)  \n[4. Analysing Model Output](#40)  \n&nbsp;&nbsp;[4.1 Plots](#41)  \n&nbsp;&nbsp;[4.2 Table Views](#42) ","f19efffa":"The following view shows which items have the least accurate predictions across all shops in the validation set. Changing the value of the CATEGORY variable will allow you to look through different categories of interest.","32541207":"Not all category names have a '-' between the main category and the subcategory. We create groups by extracting the part of the name prior to a non-letter character. We then create a group_id column by label encoding the group names.","991c14b3":"**2.5 Ages & Aggregating Sales\/Price information** <a id=\"25\"><\/a>","22097c87":"These features show the mean sales of items with names that have the same first n characters, in the same category and at the same item age. The hope is that this feature can catch past performance of similar items such as earlier titles in a series, particularly in their debut month.","c0747fb3":"**2.4 Adding Shop Information** <a id=\"24\"><\/a>\n\nWe cluster the shops using K-means clustering, using the proportion of their sales they make in each category as features.\n\nk=7 was selected because:\n* k=7 resulted in the highest average silhouette score aside from a choice of k=2. \n* k=2 would not provide a useful clustering because it creates a feature with value (shop_id==55)\\*1. \n* k=7 is also in an appropriate area when using the elbow method","36ff5468":"Given it's high cardinality, we use binary encoding to create a better representation of item_name_first11. The other name features are now deleted.","952ec49d":"We create the training, validation, and test sets. The first 2 months are not used for training as many feature values are likely to be misrepresentative in this period. \n\nThe month of data directly before the test period is used as our validation set. ","b48dc9b7":"**3.2 Submitting** <a id=\"32\"><\/a>\n\nWe use this model to predict on the test set, and clip the predictions into the range (0,20) before submitting. \n\nThis achieves a score of 0.85389 on the public leaderboard.","23218f24":"We take lag1 and lag2 values for these columns","526049ad":"These features show the ratio between lag1 and lag1to12 values","147a30af":"**2.1 Preparing Item\/Category Information** <a id=\"21\"><\/a>\n\nWe load categories.csv and display the category names","6db48b4d":"These features show sales data for items with item_id 1 above and 1 below the item in question.","16185526":"We calculate the proportion of weekly sales that occurred on each weekday at each shop. Using this information we can assign a measure of weeks of sales power to each month. February always has 4 exactly weeks worth of days since there are no leap years in our time range and all other months have a value >4 since they have extra days of varying sales power. \n\nMonth, year and first day of the month features are also created.","91885e58":"We now group the sales data by month, shop_id a","73aa57e0":"**2.6 Lagging Values & Features that use Prior Information** <a id=\"26\"><\/a>\n\nThe following function will be used to create lag features of varying lag periods.","8c050a33":"**3.1 Training Model** <a id=\"31\"><\/a>\n\nHyperparameter values were selected using a gridsearch with ranges of values for num_leaves, feature_fraction, bagging_fraction and min_data_in_leaf. \n\nThe combination of values which resulted in the lowest validation set RMSE was selected. \n\nLearning rate is set to 0.01 as this value results in a well-behaved learning curve. Default values for other hyperparameters were deemed appropriate.\n\nThe moderately sized gap between training set RMSE and validation set RMSE implies some overfitting may be occurring, however, no tweaking of hyperparameters to constrict model flexibility resulted in a lower validation RMSE.\n","82dc8006":"We load items.csv, clean the name column, create multiple features based on the cleaned name, and use label encoding. The categories dataframe is then joined to the items dataframe.","5dc2bf98":"The following view shows which items in each category have the highest prediction value across all shops in the test set. Again the CATEGORY variable can be changed to look through different categories of interest.","b85ffe02":"The target variable, 'item_cnt', is the monthly sale count of individual items at individual shops. We now create features showing average monthly sales based on various groupings","000be917":"Using these final two views in tandem can help identify specific problematic items in the test set. It is easier to keep track of the information when all the columns can be seen at once in the wider view available when running the notebook.","caaff8e4":"We now create features showing the mean first month sales for items in each category","486e19f3":"These features show how many months have passed since the first appearance of the item\/name\/category\/group\/shop","af3040b4":"Duplicate rows exist in the item list. The following cell creates a dictionary that will allow us to reassign item id's where appropriate.","25753e05":"**2.2 Preparing Sales Information** <a id=\"22\"><\/a>\n\nWe load sales.csv, remove the small proportion of rows without outlying values, use the dictionary we created above to reassign item id's where appropriate, then filter out sales for shops that don't exist in the test set and create features that need to be made before the data is grouped by month.","7127fa33":"We add columns showing predicted values, along with target values and sq_error for our training and validation sets","11e2eab5":"We want to clip the target value before aggregating so that mean values are not distorted due to outliers. We retain the unclipped value for use in features that do not aggregate the sales data.","967e7b6f":"We create a dataframe that will allow us to easily graph some aspects of model's performance.","d40601a6":"Currently, lag1to12 values are the sum of values over the previous 12 months. Differing ages in the dataset mean that some lag1to12 values are calculated over the previous 12 months, but others have had less time to accrue. \n\nWe divide lag1to12 values by the minimum between 12 and previous periods in the dataset. This turns lag1to12 into a monthly average that can be more accurately compared between datapoints.","097edeaa":"Feature showing item prices relative to the block price.","9339750e":"No columns with float dtype require more than float32 precision and no int dtype columns require values outside the int16 range. The following function will compress the data types of these columns.","bbfe0877":"# 1. Introduction<a id=\"10\"><\/a>\n\nThis competition requires us to predict future sales for Russian firm 1C Company. Our submission will contain next month sales predictions for 5100 items at each of 42 shops, with predictions being clipped into the range (0,20). The performance metric is RMSE. To inform these predictions we are given sales data covering the 33 months prior to the test period. For the (shop,item) pairs we are trying to predict, the following possibilities exist:\n1. This pair exists in our training set\n2. The item is in our training set, but not with the shop\n3. The item is not in our training set at all\n\nOur model needs to be capable of making predictions in all three cases.\n\n**Attached datasets**\n\nI initially used previously uploaded datasets containing english translations of item\/category\/shop names. I found that item names in those datasets had not been translated in their entirety, so I have attached an english translated dataset with complete translations. \n\nI have also attached a dataset which contains the dataframe our initial model is trained with and the initial model itself. These can be loaded to skip directly to modelling with the dataset or exploring the model's predictions.\n\n**Contents**\n\nMultiple other notebooks have already done an excellent job covering EDA, such as [this notebook](https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\/) by the holder of the current 4th place leaderboard submission.\n\n[1. Introduction](#10)  \n[2. Preparing Dataset](#20)  \n&nbsp;&nbsp;[2.1 Preparing Item\/Category Information](#21)  \n&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n&nbsp;&nbsp;[2.5 Ages & Aggregating Sales\/Price information](#25)  \n&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n[3. Modelling](#30)  \n&nbsp;&nbsp;[3.1 Training Model](#31)  \n&nbsp;&nbsp;[3.2 Submitting](#32)  \n[4. Analysing Model Output](#40)  \n&nbsp;&nbsp;[4.1 Plots](#41)  \n&nbsp;&nbsp;[4.2 Table Views](#42) \n\n**Results**\n\nThis notebook has a submission attached which achieves a leaderboard score of 0.85389. Using this submission as a base to improve on by incorporating findings gleaned in [Section 4](#40) allowed me to achieve a leaderboard score of 0.82501.\n![Leaderboard at time of posting](https:\/\/i.imgur.com\/GiMxp8O.png)\n\nLeave a comment if the code is insufficiently explained at any point or you find any errors, and good luck with your own attempts! \ud83d\udc4d","cea5b75b":"**2.3 Constructing Training Dataframe** <a id=\"23\"><\/a>\n\nThe test set consists of the cartesian product of 42 shops and 5100 items. To make a training set which approximates the test set we create a training dataframe consisting of the cartesian product (active items) x (active shops) for each month.","d56af878":"# 2. Preparing Training Dataset & Feature Engineering<a id=\"20\"><\/a>\n\n[1. Introduction](#10)  \n[2. Preparing Dataset](#20)  \n&nbsp;&nbsp;[2.1 Preparing Item\/Category Information](#21)  \n&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n&nbsp;&nbsp;[2.5 Ages & Aggregating Sales\/Price information](#25)  \n&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n[3. Modelling](#30)  \n&nbsp;&nbsp;[3.1 Training Model](#31)  \n&nbsp;&nbsp;[3.2 Submitting](#32)  \n[4. Analysing Model Output](#40)  \n&nbsp;&nbsp;[4.1 Plots](#41)  \n&nbsp;&nbsp;[4.2 Table Views](#42)  \n\nWe'll be using shortened column names in this notebook to save some keystrokes and display more compactly. The submission file must name the target value 'item_cnt_month', but all sales data we work with will be monthly unless otherwise specified, so we will use 'item_cnt' until submission. The translated files already use category_id in place of item_category_id etc. ","36e58a9e":"We load the test set now as we'll soon be making use of the shop_id and item_id values.","7c53cc09":"**4.2 Table Views** <a id=\"42\"><\/a>\n\nThere are 42 shops in our dataset. The following view shows us which shops have previously sold any items from each category. ","1480cb37":"The following view shows which categories are causing the most loss in our validation set, and how prominent these categories are within the test set. Looking at their MSE and presence in the test set shows us that the video game categories are the causing the most loss and might warrant especially close examination.","64646450":"We clean the shop names column then use the opening word to create the shop_city feature. We then create the shop_type feature based on terms that occur in the name of the shop. Both these features are then label encoded.","b8badac0":"**2.7 Encoding Name Information** <a id=\"27\"><\/a>\n\nWe add boolean features indicating whether an item's name contains any words which frequently appear in the item set.","0be0f705":"These features use past information across more than one period of the dataset. They are not suitable to be lagged in the normal manner.","b64f7af1":"These features show per day sales values since an item was first sold.","1a82b39a":"**4.1 Plots** <a id=\"41\"><\/a>\n\nA summary plot of feature importance. We can use this along with other [shap](https:\/\/shap.readthedocs.io\/en\/latest\/) plots to get an idea of what drives the models predictions, and help us find spurious features which can be filtered out.","c341e22d":"We create lag1 values for category and block prices."}}