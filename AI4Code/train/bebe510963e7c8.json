{"cell_type":{"73c68e73":"code","2a3b8129":"code","3ca61d17":"code","44ef6909":"code","a15521e9":"code","e1875428":"code","1006d0b7":"code","d110ddc7":"code","ecf0aefd":"code","139aeca3":"code","7bc8c74c":"code","de3c6d7e":"code","5b474e9c":"code","397a2858":"code","17017c56":"code","561ef409":"code","d480e8fa":"code","37f905b8":"code","3174b26a":"code","2d9ba507":"code","772bcc90":"markdown","e0b46670":"markdown","624ea469":"markdown","cfaaeefb":"markdown","6223710f":"markdown","43817f01":"markdown","3b055e43":"markdown","131ecc9a":"markdown","9bd5245d":"markdown"},"source":{"73c68e73":"# Importing the libraries\n\nimport numpy as np\nimport statistics as stat\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\n\nfrom scipy import stats\nfrom numpy import median\nfrom statsmodels.graphics.gofplots import qqplot\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_log_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, ElasticNetCV\nfrom sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor,AdaBoostRegressor","2a3b8129":"# Reading data from the local storage\n\nfilename = '..\/input\/bike_sharing_hourly.csv' \ndata = pd.read_csv(filename)\ndata.head(2)","3ca61d17":"# Checking existence of null value in the dataset\n\ndata.isnull().sum()","44ef6909":"# Changing column name for a nice precise reading\n\ndata.rename(columns={'weathersit':'weather',\n                     'mnth':'month',\n                     'hr':'hour',\n                     'yr':'year',\n                     'hum': 'humidity',\n                     'cnt':'count'},inplace=True)\ndata.head(2)","a15521e9":"# Checking data type for each column\n\ndata.dtypes","e1875428":"# Some columns need to be deleted because they are ambiguous and add no importance to the analysis\n\ndata = data.drop(['instant','dteday'], axis=1)\n\n# Some data types need to be changed from numerical to categorical. Otherwise the prediction model can interpret wrongly\n\ndata['year'] = data.year.astype('category')\ndata['season'] = data.season.astype('category')\ndata['month'] = data.month.astype('category')\ndata['hour'] = data.hour.astype('category')\ndata['holiday'] = data.holiday.astype('category')\ndata['weekday'] = data.weekday.astype('category')\ndata['workingday'] = data.workingday.astype('category')\ndata['weather'] = data.weather.astype('category')\n\n# Confirming the converted datatype\ndata.dtypes","1006d0b7":"## Exploratory Data Analysis\n\n\n# Analyzing the change in bike sharing pattern('count' variable in dataset) with categorical variables\n\nfig,[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8] = plt.subplots(nrows=8, figsize=(15,25))\nsn.barplot(x = data['weekday'], y = data['count'],ax = ax1)\nsn.barplot(x = data['season'], y = data['count'],ax = ax2)\nsn.barplot(x = data['month'], y = data['count'],ax = ax3)\nsn.barplot(x = data['holiday'], y = data['count'],ax = ax4)\nsn.barplot(x = data['hour'], y = data['count'],ax = ax5)\nsn.barplot(x = data['weather'], y = data['count'],ax = ax6)\nsn.barplot(x = data['workingday'], y = data['count'],ax = ax7)\nsn.barplot(x = data['year'], y = data['count'],ax = ax8)","d110ddc7":"# Total bike users(count) is sum of registered and casual users. Need to analyze how they vary individually with hour\n# The variation is observed in different circumstances to check how those impact the bike users\n\nfig,axes = plt.subplots(nrows = 3,ncols = 3, figsize=(25,30))\n\nsn.pointplot(x = 'hour', y = 'registered', hue = 'month',data  = data,ax = axes[0][0])\nsn.pointplot(x = 'hour', y = 'casual', hue = 'month', data  = data,ax = axes[0][1])\nsn.pointplot(x = 'hour', y = 'count', hue = 'month', size = 7, data  = data,ax = axes[0][2])\n\nsn.pointplot(x = 'hour', y = 'registered', hue = 'season',data  = data,ax = axes[1][0])\nsn.pointplot(x = 'hour', y = 'casual', hue = 'season', data  = data,ax = axes[1][1])\nsn.pointplot(x = 'hour', y = 'count', hue = 'season', size = 7, data  = data,ax = axes[1][2])\n\nsn.pointplot(x = 'hour', y = 'registered', hue = 'weather',data  = data,ax = axes[2][0])\nsn.pointplot(x = 'hour', y = 'casual', hue = 'weather', data  = data,ax = axes[2][1])\nsn.pointplot(x = 'hour', y = 'count', hue = 'weather', size = 7, data  = data,ax = axes[2][2])","ecf0aefd":"# Analyzing the change in bike sharing pattern with numerical variables\n# Regression plot is used to verify if a pattern can be observed between 'count' and numerical variables\n\nfig,[ax1,ax2,ax3] = plt.subplots(ncols = 3, figsize = (20,8))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \n\nsn.regplot(x = 'temp', y = 'count',data = data,ax = ax1)\nax1.set(title=\"Relation between temperature and count\")\nsn.regplot(x = 'humidity', y = 'count',data = data,ax = ax2)\nax2.set(title=\"Relation between humidity and total count\")\nsn.regplot(x = 'windspeed', y = 'count',data = data,ax = ax3)\nax3.set(title=\"Relation between Windspeed and total count\")","139aeca3":"# From the regression plot, it is not very clear how the data fit with the regression line\n# To understand the central tendecy, bins can be used  in regression plot\n\nax1 = sn.jointplot(x = 'temp', y = 'count',data = data,kind = 'reg', x_bins = 100,x_estimator=np.mean,size = 5)\nax1.fig.suptitle('Relation between Temperature and total count')\nax2 = sn.jointplot(x = 'humidity', y = 'count',data = data,kind = 'reg', x_bins = 100,x_estimator=np.mean,size = 5)\nax2.fig.suptitle('Relation between Humidity and total count')\nax3 = sn.jointplot(x = 'windspeed', y = 'count',data = data,kind = 'reg', x_bins = 100,x_estimator=np.mean,size = 5)\nax3.fig.suptitle('Relation between Windspeed and total count')","7bc8c74c":"# To see how variables are connected with each other, data correlation can be checked \n\ndata_corr = data[['temp', 'atemp', 'casual', 'registered', 'humidity', 'windspeed', 'count']].corr()\nmask = np.array(data_corr)\nmask[np.tril_indices_from(mask)] = False\nfig = plt.subplots(figsize=(15,10))\nsn.heatmap(data_corr, mask=mask, vmax=1, square=True, annot=True,cmap=\"YlGnBu\")","de3c6d7e":"# Checking the outliers with boxplot\n\nfig, axes = plt.subplots(nrows=2,ncols=2,figsize=(17,15))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \n\nsn.boxplot(data=data,y=\"count\",ax=axes[0][0])\nsn.boxplot(data=data,y=\"count\",x=\"season\",hue = \"workingday\",ax=axes[0][1])\nsn.boxplot(data=data,y=\"count\",x=\"month\",hue = \"workingday\",ax=axes[1][0])\nsn.boxplot(data=data,y=\"count\",x=\"hour\",ax=axes[1][1])","5b474e9c":"# To check the skewness of distribution for 'count' variable, Q-Q plot can be drawn\n# Q-Q plot can also verify whether a normal distribution is possible\n\nfig, (ax1,ax2) = plt.subplots(nrows=2, figsize=(12,12))\nsn.distplot(data['count'], ax=ax1)\nax1.set(title=\"Distribution of count\")\nqqplot(data['count'], line='s', ax=ax2)\nax2.set(title=\"Q-Q plot of Count\")","397a2858":"# 'count' variable is the decomposition of 'casual' and 'registered variable'. These 2 variables can be deleted\n# 'atemp' and 'temp' variables are similar almost. Can get rid of these\n# 'windspeed' has not much correlation with 'count'. Can skip it too\n\ndata = data.drop(['atemp', 'casual', 'registered', 'windspeed'], axis=1)","17017c56":"# Numbers in the categorical variable are non-binary which can impact wrongly the prediction model\n# One hot encoding is used to convert those variables in binary by creating dummy data\n# To avoid multicolinearity, the first variable in the dummydata is dropped\n\ndata_dummy = data\n\ndef dummify_dataset(dataframe, col):\n    dummy_column = pd.get_dummies(dataframe[col], prefix = col, drop_first = True)\n    new_data = pd.concat([dataframe,dummy_column],axis = 1)\n    new_data = new_data.drop([col], axis=1)\n    return new_data\n\ndcol = ['season', 'month', 'hour', 'holiday', 'weekday', 'workingday', 'weather','year']\n\nfor i in range(0,8):\n    data_dummy = dummify_dataset(data_dummy,dcol[i])\n    ","561ef409":"# Training and test data is created by splitting the main data. 33% of test data is considered\n\ny = data_dummy['count']\nX = data_dummy.drop(['count'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   test_size=0.33,\n                                                   random_state=42)","d480e8fa":"# Comparing performance of the regression models\n\nmodels = [LinearRegression(),\n          AdaBoostRegressor(),\n          Ridge(),\n          HuberRegressor(),\n          ElasticNetCV(),\n          DecisionTreeRegressor(), \n          ExtraTreesRegressor(),\n          GradientBoostingRegressor(),\n          RandomForestRegressor(),\n          BaggingRegressor()]\n\n# A function is wrtten to find out the cross validation score based on mean absolute error\n\ndef test_algorithms(model):\n    kfold = model_selection.KFold(n_splits=10, random_state=0)\n    mean_dev_scores = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n    r2_scores = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='r2')\n    Scores= pd.DataFrame({'Mean deviation':[np.mean(mean_dev_scores)],'R Square':[np.mean(r2_scores)]})\n    print(Scores)\n    \nfor model in models:\n    test_algorithms(model)","37f905b8":"# It is evident that the ExtraTreeRegressor() has the lowest error rating\n# To find out the best parameters from a list, a function 'GridSearchCV' from Scikit library is used \n\nparameters={'n_estimators': (50,100,500),\n        'max_features': (20,40,50),\n        'min_samples_leaf': (1,5,10),\n        'min_samples_split': (5,10,20)}\n\nclf = GridSearchCV(ExtraTreesRegressor(), parameters,scoring = 'r2')\nclf.fit(X_train,y_train)\n\nprint(clf.best_score_)\nprint(clf.best_params_)\nprint(clf.best_estimator_)","3174b26a":"## Performance of the prediction model\n\n# 'count' variables for test data is predicted with the chosen model and parameters\n\nclf = ExtraTreesRegressor(max_features= 40, min_samples_leaf= 1, min_samples_split= 5, n_estimators= 500)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\n# A regression plot can be drawn between actual and predicted data to understand the performance of the model\n# Bins are used to make the pattern more meaningful\n\nax = sn.regplot(y_test,y_pred,x_bins = 200)\nax.set(title = \"Comparison between the actual vs prediction\")","2d9ba507":"# Another regression plot to understand the error residuals from actual test data\n\ny_res = y_test - y_pred\nax = sn.regplot(y_test,y_res)\nax.set(title = \"Comparison between the actual vs residual\")","772bcc90":"It is evident that the distribution is not normal as the quantile plot does not fit with the straight line","e0b46670":"It seems that no null value is present in the dataset","624ea469":"The R-square value is very reasonable for this dataset which has such huge number of outliers","cfaaeefb":"From the plot it is evident there are strong relationship betweek temperature and humidity with 'count' variable.\nIt's also evident that windspeed has no significant effect on 'count'","6223710f":"This code is written by Shad Ahammed. Purpose for this kernel is to make exploratory data analysis and prediction modelling of Bike Sharing Dataset. The dataset was taken from UCI Machine Learning Repository. The dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.Only the hourly data is considered.\n","43817f01":"It is evident that each of the above categorical variable has impacts on 'count' variable","3b055e43":"* It is evident that temp and atemp variables are highly correlated\n* temp has positive correlation and humidity has negative correlation with 'count'\n* Again it can be observed that windspeed has little correlation with count","131ecc9a":"We can see from the patterns that number of registered users is dominant in the count of total users","9bd5245d":"From the plot it is visible that the deviations from actual test data are not very high and restricted in a limit. It is another proof for the preciseness of the prediction model"}}