{"cell_type":{"6a5a27c7":"code","2d5c9b14":"code","a2403366":"code","467a49cf":"code","74950cef":"code","dc08e546":"code","0cdf9d14":"code","cafedf6f":"code","b2c0ee0e":"code","6bcae044":"code","648c9b48":"code","e1893ec4":"code","b0530fa4":"code","1824fd4f":"code","e3f48786":"code","3bfaef86":"code","e1fea8c3":"code","88d8a8ea":"code","12ab4ee9":"code","5aa199a7":"code","cec09707":"code","1b7b02e4":"code","b121bf53":"code","e1167b3f":"code","63d1e509":"code","f8eee095":"markdown","84685988":"markdown","b20b0d7e":"markdown","7b19c654":"markdown","c71f34d2":"markdown","42877124":"markdown","59fba624":"markdown","9eb6ec64":"markdown","142b15bf":"markdown","56eb6441":"markdown","b8a083fe":"markdown","fd78435c":"markdown"},"source":{"6a5a27c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d5c9b14":"%matplotlib inline\n\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings","a2403366":"!pip install kaggler","467a49cf":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')\nprint(f'TensorFlow: {tf.__version__}')","74950cef":"warnings.simplefilter('ignore')\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)","dc08e546":"feature_name = 'ae'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('\/kaggle\/input\/tabular-playground-series-apr-2021\/')\ntrn_file = data_dir \/ 'train.csv'\ntst_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\npseudo_label_file = '\/kaggle\/input\/tps-apr-2021-label\/voting_submission_from_5_best.csv'\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'","0cdf9d14":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape)","cafedf6f":"tst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()","b2c0ee0e":"df.info()","6bcae044":"df.describe()","648c9b48":"df.nunique()","e1893ec4":"# Feature engineering code from https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\ndf['Fare'].fillna(round(df['Fare'].median()), inplace=True,)\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x])\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x])\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","b0530fa4":"df[num_cols].describe()","1824fd4f":"plt.figure(figsize=(16, 16))\nfor i, col in enumerate(num_cols):\n    ax = plt.subplot(4, 2, i + 1)\n    ax.set_title(col)\n    df[col].hist(bins=50)","e3f48786":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \ndf.describe()","3bfaef86":"scaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])","e1fea8c3":"lbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)","88d8a8ea":"encoding_dim = 64\n\ndef get_model(encoding_dim, dropout=.2):\n    num_dim = len(num_cols)\n    num_input = keras.layers.Input((num_dim,), name='num_input')\n    cat_inputs = []\n    cat_embs = []\n    emb_dims = 0\n    for col in cat_cols:\n        cat_input = keras.layers.Input((1,), name=f'{col}_input')\n        emb_dim = max(8, int(np.log2(1 + df[col].nunique()) * 4))\n        cat_emb = keras.layers.Embedding(input_dim=df[col].max() + 1, output_dim=emb_dim)(cat_input)\n        cat_emb = keras.layers.Dropout(dropout)(cat_emb)\n        cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb)\n\n        cat_inputs.append(cat_input)\n        cat_embs.append(cat_emb)\n        emb_dims += emb_dim\n\n    merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs)\n\n    encoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(merged_inputs)\n    encoded = keras.layers.Dropout(dropout)(encoded)\n    encoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n    encoded = keras.layers.Dropout(dropout)(encoded)    \n    encoded = keras.layers.Dense(encoding_dim, activation='relu')(encoded)\n    \n    decoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n    decoded = keras.layers.Dropout(dropout)(decoded)\n    decoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(decoded)\n    decoded = keras.layers.Dropout(dropout)(decoded)    \n    decoded = keras.layers.Dense(num_dim + emb_dims, activation='linear')(decoded)\n\n    encoder = keras.Model([num_input] + cat_inputs, encoded)\n    ae = keras.Model([num_input] + cat_inputs, decoded)\n    ae.add_loss(keras.losses.mean_squared_error(merged_inputs, decoded))\n    ae.compile(optimizer='adam')\n    return ae, encoder","12ab4ee9":"ae, encoder = get_model(encoding_dim)\nae.summary()","5aa199a7":"inputs = [df[num_cols].values] + [df[x].values for x in cat_cols]\nae.fit(inputs, inputs,\n      epochs=100,\n      batch_size=16384,\n      shuffle=True,\n      validation_split=.2)","cec09707":"encoding = encoder.predict(inputs)\nprint(encoding.shape)\nnp.savetxt(feature_file, encoding, fmt='%.6f', delimiter=',')","1b7b02e4":"seed = 42\nn_fold = 5\nX = pd.concat((df[feature_cols], \n               pd.DataFrame(encoding, columns=[f'enc_{x}' for x in range(encoding_dim)])), axis=1)\ny = df[target_col]\nX_tst = X.iloc[n_trn:]\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],))\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) \/ n_fold\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')\n\nnp.savetxt(predict_val_file, p, fmt='%.6f')\nnp.savetxt(predict_tst_file, p_tst, fmt='%.6f')","b121bf53":"print(f'  CV AUC: {roc_auc_score(y, p):.6f}')\nprint(f'Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}')","e1167b3f":"n_pos = int(0.34911 * tst.shape[0])\nth = sorted(p_tst, reverse=True)[n_pos]\nprint(th)\nconfusion_matrix(pseudo_label[target_col], (p_tst > th).astype(int))","63d1e509":"sub[target_col] = (p_tst > th).astype(int)\nsub.to_csv(submission_file)","f8eee095":"If you find it helpful, please upvote the notebook and give a star to [Kaggler](https:\/\/github.com\/jeongyoonlee\/Kaggler). If you have questions and\/or feature requests for Kaggler, please post them as `Issue` in the `Kaggler` GitHub repository.\n\nHappy Kaggling!","84685988":"Basic stacked autoencoder. I will add the versions with DAE and emphasized DAE later.","b20b0d7e":"## Model Training + Feature Selection + Hyperparameter Optimization","7b19c654":"## Submission File","c71f34d2":"## Load libraries and install `Kaggler`","42877124":"In this notebook, I will show how to use autoencoder, feature selection, hyperparameter optimization, and pseudo labeling using the `Keras` and `Kaggler` Python packages.\n\nThe contents of the notebook are as follows:\n1. **Package installation**: Installing latest version of `Kaggler` using `Pip`\n2. **Regular feature engineering**: [code](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model) by @udbhavpangotra\n3. **Feature transformation**: Using `kaggler.preprocessing.LabelEncoder` to impute missing values and group rare categories automatically.\n4. **Stacked AutoEncoder**: Notebooks for DAE will be shared later.\n5. **Model training**: with 5-fold CV and pseudo label from @hiro5299834's [data](https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling).\n6. **Feature selection and hyperparameter optimization**: Using `kaggler.model.AutoLGB`\n7. **Saving a submission file**","59fba624":"## Feature Engineering (ref: [code](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model) by @udbhavpangotra)","9eb6ec64":"Label-encode categorical variables using `kaggler.preprocessing.LabelEncoder`, which creates new categories for `NaN`s as well as rare categories (using the threshold of `min_obs`).","142b15bf":"## Feature Transformation","56eb6441":"Train the `LightGBM` model with pseudo label and 5-fold CV. In the first fold, perform feature selection and hyperparameter optimization using `kaggler.model.AutoLGB`.","b8a083fe":"## AutoEncoder using `Keras`","fd78435c":"Apply `log2(1 + x)` transformation followed by standardization for count variables to make them close to the normal distribution. `log2(1 + x)` has better resolution than `log1p` and it preserves the values of 0 and 1."}}