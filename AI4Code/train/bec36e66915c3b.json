{"cell_type":{"46b2152e":"code","e4d63ec0":"code","b72e8148":"code","2f606857":"code","3270b732":"code","bd24f5a9":"code","af9163c1":"code","1757a911":"code","afa15b5c":"code","2aafe7fd":"code","9c6b2287":"code","0498ad2c":"code","85794d35":"code","24f0d330":"markdown","db385c13":"markdown","5b0635fe":"markdown","2aa69a4c":"markdown","a8f8c124":"markdown","5bad9628":"markdown","83909928":"markdown","5df99c20":"markdown","96f4aae9":"markdown","e9f394ee":"markdown","3d874076":"markdown","c6559181":"markdown","6e8bf9a9":"markdown","6690cb1f":"markdown","9762a97a":"markdown","c1b13a44":"markdown"},"source":{"46b2152e":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https:\/\/github.com\/garethjns\/kaggle-football.git\n!pip install reinforcement_learning_keras==0.6.0","e4d63ec0":"import collections\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nimport seaborn as sns\nimport gym\nimport gfootball\nimport glob \nimport imageio\nimport pathlib\nimport zlib\nimport pickle\nimport tempfile\nimport os\nimport sys\nfrom IPython.display import Image, display\n\nsns.set()\n\n# In TF > 2, training keras models in a loop with eager execution on causes memory leaks and terrible performance.\ntf.compat.v1.disable_eager_execution()\n\nsys.path.append(\"\/kaggle\/working\/kaggle-football\/\")","b72e8148":"from kaggle_football.viz import generate_gif, plot_smm_obs\n\n\nsmm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nprint(smm_env.reset().shape)\n\ngenerate_gif(smm_env, n_steps=500, expected_min=0, expected_max=255)\nImage(filename='smm_env_replay.gif', format='png')","2f606857":"class SMMFrameProcessWrapper(gym.Wrapper):\n    \"\"\"\n    Wrapper for processing frames from SMM observation wrapper from football env.\n\n    Input is (72, 96, 4), where last dim is (team 1 pos, team 2 pos, ball pos, \n    active player pos). Range 0 -> 255.\n    Output is (72, 96, 4) as difference to last frame for all. Range -1 -> 1\n    \"\"\"\n\n    def __init__(self, env: gym.Env = None,\n                 obs_shape: Tuple[int, int] = (72, 96, 4)) -> None:\n        \"\"\"\n        :param env: Gym env, or None. Allowing None here is unusual,\n                    but we'll reuse the buffer functunality later in\n                    the submission, when we won't be using the gym API.\n        :param obs_shape: Expected shape of single observation.\n        \"\"\"\n        if env is not None:\n            super().__init__(env)\n        self._buffer_length = 2\n        self._obs_shape = obs_shape\n        self._prepare_obs_buffer()\n\n    @staticmethod\n    def _normalise_frame(frame: np.ndarray):\n        return frame \/ 255.0\n\n    def _prepare_obs_buffer(self) -> None:\n        \"\"\"Create buffer and preallocate with empty arrays of expected shape.\"\"\"\n\n        self._obs_buffer = collections.deque(maxlen=self._buffer_length)\n\n        for _ in range(self._buffer_length):\n            self._obs_buffer.append(np.zeros(shape=self._obs_shape))\n\n    def build_buffered_obs(self) -> np.ndarray:\n        \"\"\"\n        Iterate over the last dimenion, and take the difference between this obs \n        and the last obs for each.\n        \"\"\"\n        agg_buff = np.empty(self._obs_shape)\n        for f in range(self._obs_shape[-1]):\n            agg_buff[..., f] = self._obs_buffer[1][..., f] - self._obs_buffer[0][..., f]\n\n        return agg_buff\n\n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[Any, Any]]:\n        \"\"\"Step env, add new obs to buffer, return buffer.\"\"\"\n        obs, reward, done, info = self.env.step(action)\n\n        obs = self._normalise_frame(obs)\n        self._obs_buffer.append(obs)\n\n        return self.build_buffered_obs(), reward, done, info\n\n    def reset(self) -> np.ndarray:\n        \"\"\"Add initial obs to end of pre-allocated buffer.\n\n        :return: Buffered observation\n        \"\"\"\n        self._prepare_obs_buffer()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n\n        return self.build_buffered_obs()","3270b732":"smm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nwrapped_smm_env = SMMFrameProcessWrapper(smm_env)\nprint(wrapped_smm_env.reset().shape)","bd24f5a9":"generate_gif(wrapped_smm_env, n_steps=500, suffix=\"wrapped_smm_env_\", expected_min=-1, expected_max=1)\nImage(filename='wrapped_smm_env_replay.gif', format='png')","af9163c1":"class SplitLayer(keras.layers.Layer):\n    def __init__(self, split_dim: int = 3) -> None:\n        super().__init__()\n        self.split_dim = split_dim\n\n    def call(self, inputs) -> tf.Tensor:\n        \"\"\"Split a given dim into seperate tensors.\"\"\"\n        return [tf.expand_dims(inputs[..., i], self.split_dim) \n                for i in range(inputs.shape[self.split_dim])]","1757a911":"class SplitterConvNN:\n\n    def __init__(self, observation_shape: List[int], n_actions: int, \n                 output_activation: Union[None, str] = None,\n                 unit_scale: int = 1, learning_rate: float = 0.0001, \n                 opt: str = 'Adam') -> None:\n        \"\"\"\n        :param observation_shape: Tuple specifying input shape.\n        :param n_actions: Int specifying number of outputs\n        :param output_activation: Activation function for output. Eg. \n                                  None for value estimation (off-policy methods).\n        :param unit_scale: Multiplier for all units in FC layers in network \n                           (not used here at the moment).\n        :param opt: Keras optimiser to use. Should be string. \n                    This is to avoid storing TF\/Keras objects here.\n        :param learning_rate: Learning rate for optimiser.\n\n        \"\"\"\n        self.observation_shape = observation_shape\n        self.n_actions = n_actions\n        self.unit_scale = unit_scale\n        self.output_activation = output_activation\n        self.learning_rate = learning_rate\n        self.opt = opt\n\n    @staticmethod\n    def _build_conv_branch(frame: keras.layers.Layer, name: str) -> keras.layers.Layer:\n        conv1 = keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                    name=f'conv1_frame_{name}', padding='same', \n                                    activation='relu')(frame)\n        conv2 = keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                    name=f'conv2_frame_{name}', padding='same', \n                                    activation='relu')(conv1)\n        conv3 = keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                    name=f'conv3_frame_{name}', padding='same', \n                                    activation='relu')(conv2)\n\n        flatten = keras.layers.Flatten(name=f'flatten_{name}')(conv3)\n\n        return flatten\n\n    def _model_architecture(self) -> Tuple[keras.layers.Layer, keras.layers.Layer]:\n        n_units = 512 * self.unit_scale\n\n        frames_input = keras.layers.Input(name='input', shape=self.observation_shape)\n        frames_split = SplitLayer(split_dim=3)(frames_input)\n        conv_branches = []\n        for f, frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame, name=str(f)))\n\n        concat = keras.layers.concatenate(conv_branches)\n        fc1 = keras.layers.Dense(units=int(n_units), name='fc1', \n                                 activation='relu')(concat)\n        fc2 = keras.layers.Dense(units=int(n_units \/ 2), name='fc2', \n                                 activation='relu')(fc1)\n        action_output = keras.layers.Dense(units=self.n_actions, name='output',\n                                           activation=self.output_activation)(fc2)\n\n        return frames_input, action_output\n\n    def compile(self, model_name: str = 'model', \n                loss: Union[str, Callable] = 'mse') -> keras.Model:\n        \"\"\"\n        Compile a copy of the model using the provided loss.\n\n        :param model_name: Name of model\n        :param loss: Model loss. Default 'mse'. Can be custom callable.\n        \"\"\"\n        # Get optimiser\n        if self.opt.lower() == 'adam':\n            opt = keras.optimizers.Adam\n        elif self.opt.lower() == 'rmsprop':\n            opt = keras.optimizers.RMSprop\n        else:\n            raise ValueError(f\"Invalid optimiser {self.opt}\")\n\n        state_input, action_output = self._model_architecture()\n        model = keras.Model(inputs=[state_input], outputs=[action_output], \n                            name=model_name)\n        model.compile(optimizer=opt(learning_rate=self.learning_rate), \n                      loss=loss)\n\n        return model\n\n    def plot(self, model_name: str = 'model') -> None:\n        keras.utils.plot_model(self.compile(model_name), \n                               to_file=f\"{model_name}.png\", show_shapes=True)\n        plt.show()\n\n\nmod = SplitterConvNN(observation_shape=wrapped_smm_env.observation_space.shape, \n                     n_actions=wrapped_smm_env.action_space.n)\nmod.compile()\nmod.plot()\nImage(filename='model.png') ","afa15b5c":"from reinforcement_learning_keras.agents.components.history.training_history import TrainingHistory\nfrom reinforcement_learning_keras.agents.components.replay_buffers.continuous_buffer import ContinuousBuffer\nfrom reinforcement_learning_keras.agents.q_learning.deep_q_agent import DeepQAgent\nfrom reinforcement_learning_keras.agents.q_learning.exploration.epsilon_greedy import EpsilonGreedy\n\nagent = DeepQAgent(\n    name='deep_q',\n    model_architecture=SplitterConvNN(observation_shape=(72, 96, 4), \n                                      n_actions=19),\n    replay_buffer=ContinuousBuffer(buffer_size=300),\n    env_spec=\"GFootball-11_vs_11_kaggle-SMM-v0\",\n    env_wrappers=[SMMFrameProcessWrapper],\n    eps=EpsilonGreedy(eps_initial=0.5, \n                      decay=0.001, \n                      eps_min=0.01, \n                      decay_schedule='linear'),\n    training_history=TrainingHistory(agent_name='deep_q', \n                                     plotting_on=True, \n                                     plot_every=5, \n                                     rolling_average=5)\n)\n\nagent.train(verbose=True, render=False,\n            n_episodes=2, max_episode_steps=100, \n            update_every=10, checkpoint_every=10)","2aafe7fd":"agent._action_model.save(\"saved_model\/\")\n!ls","9c6b2287":"%%writefile main.py\n\n\nimport os\nimport collections\nimport pickle\nimport zlib\nfrom typing import Tuple, Dict, Any, Union, Callable, List\n\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom gfootball.env import observation_preprocessing\nfrom tensorflow import keras\n\n\nclass SMMFrameProcessWrapper(gym.Wrapper):\n    \"\"\"\n    Wrapper for processing frames from SMM observation wrapper from football env.\n\n    Input is (72, 96, 4), where last dim is (team 1 pos, team 2 pos, ball pos,\n    active player pos). Range 0 -> 255.\n    Output is (72, 96, 4) as difference to last frame for all. Range -1 -> 1\n    \"\"\"\n\n    def __init__(self, env: Union[None, gym.Env] = None,\n                 obs_shape: Tuple[int, int] = (72, 96, 4)) -> None:\n        \"\"\"\n        :param env: Gym env.\n        :param obs_shape: Expected shape of single observation.\n        \"\"\"\n        if env is not None:\n            super().__init__(env)\n        self._buffer_length = 2\n        self._obs_shape = obs_shape\n        self._prepare_obs_buffer()\n\n    @staticmethod\n    def _normalise_frame(frame: np.ndarray):\n        return frame \/ 255.0\n\n    def _prepare_obs_buffer(self) -> None:\n        \"\"\"Create buffer and preallocate with empty arrays of expected shape.\"\"\"\n\n        self._obs_buffer = collections.deque(maxlen=self._buffer_length)\n\n        for _ in range(self._buffer_length):\n            self._obs_buffer.append(np.zeros(shape=self._obs_shape))\n\n    def build_buffered_obs(self) -> np.ndarray:\n        \"\"\"\n        Iterate over the last dimenion, and take the difference between this obs\n        and the last obs for each.\n        \"\"\"\n        agg_buff = np.empty(self._obs_shape)\n        for f in range(self._obs_shape[-1]):\n            agg_buff[..., f] = self._obs_buffer[1][..., f] - self._obs_buffer[0][..., f]\n\n        return agg_buff\n\n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[Any, Any]]:\n        \"\"\"Step env, add new obs to buffer, return buffer.\"\"\"\n        obs, reward, done, info = self.env.step(action)\n\n        obs = self._normalise_frame(obs)\n        self._obs_buffer.append(obs)\n\n        return self.build_buffered_obs(), reward, done, info\n\n    def reset(self) -> np.ndarray:\n        \"\"\"Add initial obs to end of pre-allocated buffer.\n\n        :return: Buffered observation\n        \"\"\"\n        self._prepare_obs_buffer()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n\n        return self.build_buffered_obs()\n\n    \n    \nclass SplitLayer(keras.layers.Layer):\n    def __init__(self, split_dim: int = 3) -> None:\n        super().__init__()\n        self.split_dim = split_dim\n\n    def call(self, inputs) -> tf.Tensor:\n        \"\"\"Split a given dim into seperate tensors.\"\"\"\n        return [tf.expand_dims(inputs[..., i], self.split_dim) \n                for i in range(inputs.shape[self.split_dim])]\n\n    \nclass SplitterConvNN:\n\n    def __init__(self, observation_shape: List[int], n_actions: int, \n                 output_activation: Union[None, str] = None,\n                 unit_scale: int = 1, learning_rate: float = 0.0001, \n                 opt: str = 'Adam') -> None:\n        \"\"\"\n        :param observation_shape: Tuple specifying input shape.\n        :param n_actions: Int specifying number of outputs\n        :param output_activation: Activation function for output. Eg. \n                                  None for value estimation (off-policy methods).\n        :param unit_scale: Multiplier for all units in FC layers in network \n                           (not used here at the moment).\n        :param opt: Keras optimiser to use. Should be string. \n                    This is to avoid storing TF\/Keras objects here.\n        :param learning_rate: Learning rate for optimiser.\n\n        \"\"\"\n        self.observation_shape = observation_shape\n        self.n_actions = n_actions\n        self.unit_scale = unit_scale\n        self.output_activation = output_activation\n        self.learning_rate = learning_rate\n        self.opt = opt\n\n    @staticmethod\n    def _build_conv_branch(frame: keras.layers.Layer, name: str) -> keras.layers.Layer:\n        conv1 = keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                    name=f'conv1_frame_{name}', padding='same', \n                                    activation='relu')(frame)\n        conv2 = keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                    name=f'conv2_frame_{name}', padding='same', \n                                    activation='relu')(conv1)\n        conv3 = keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                    name=f'conv3_frame_{name}', padding='same', \n                                    activation='relu')(conv2)\n\n        flatten = keras.layers.Flatten(name=f'flatten_{name}')(conv3)\n\n        return flatten\n\n    def _model_architecture(self) -> Tuple[keras.layers.Layer, keras.layers.Layer]:\n        n_units = 512 * self.unit_scale\n\n        frames_input = keras.layers.Input(name='input', shape=self.observation_shape)\n        frames_split = SplitLayer(split_dim=3)(frames_input)\n        conv_branches = []\n        for f, frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame, name=str(f)))\n\n        concat = keras.layers.concatenate(conv_branches)\n        fc1 = keras.layers.Dense(units=int(n_units), name='fc1', \n                                 activation='relu')(concat)\n        fc2 = keras.layers.Dense(units=int(n_units \/ 2), name='fc2', \n                                 activation='relu')(fc1)\n        action_output = keras.layers.Dense(units=self.n_actions, name='output',\n                                           activation=self.output_activation)(fc2)\n\n        return frames_input, action_output\n\n    def compile(self, model_name: str = 'model', \n                loss: Union[str, Callable] = 'mse') -> keras.Model:\n        \"\"\"\n        Compile a copy of the model using the provided loss.\n\n        :param model_name: Name of model\n        :param loss: Model loss. Default 'mse'. Can be custom callable.\n        \"\"\"\n        # Get optimiser\n        if self.opt.lower() == 'adam':\n            opt = keras.optimizers.Adam\n        elif self.opt.lower() == 'rmsprop':\n            opt = keras.optimizers.RMSprop\n        else:\n            raise ValueError(f\"Invalid optimiser {self.opt}\")\n\n        state_input, action_output = self._model_architecture()\n        model = keras.Model(inputs=[state_input], outputs=[action_output], \n                            name=model_name)\n        model.compile(optimizer=opt(learning_rate=self.learning_rate), \n                      loss=loss)\n\n        return model\n\n    def plot(self, model_name: str = 'model') -> None:\n        keras.utils.plot_model(self.compile(model_name), \n                               to_file=f\"{model_name}.png\", show_shapes=True)\n        plt.show()\n    \nFN = \"saved_model\"\nKAGGLE_PATH = f\"\/kaggle_simulations\/agent\/{FN}\"\nif os.path.exists(KAGGLE_PATH):\n    # On kaggle\n    path = KAGGLE_PATH\nelse:\n    path = FN\n\ntf_mod = keras.models.load_model(path)\nobs_buffer = SMMFrameProcessWrapper()\n\n\ndef agent(obs):\n\n    # Use the existing model and obs buffer on each call to agent\n    global tf_mod\n    global obs_buffer\n\n    # Get the raw observations return by the environment\n    obs = obs['players_raw'][0]\n    # Convert these to the same output as the SMMWrapper we used in training\n    obs = observation_preprocessing.generate_smm([obs])\n\n    # Use the SMMFrameProcessWrapper to do the buffering, but not enviroment\n    # stepping or anything related to the Gym API.\n    obs_buffer._obs_buffer.append(obs)\n\n    # Predict actions from keras model\n    actions = tf_mod.predict(obs)\n    action = np.argmax(actions)\n\n    return [action]","0498ad2c":"!tar -czvf submission.tar.gz main.py saved_model","85794d35":"from typing import Tuple, Dict, List, Any\n\nfrom kaggle_environments import make\n\nenv = make(\"football\", debug=True,configuration={\"save_video\": True,\n                                                 \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"main.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"run_right\"  # eg. A built in 'AI' agent or the agent again\n\n\noutput: List[Tuple[Dict[str, Any], Dict[str, Any]]] = env.run([left_player, right_player])\n\nprint(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\nenv.render(mode=\"human\", width=800, height=600)","24f0d330":"These frames can be used by a convolutional neural network, but require pre-processing first.\n\n - Reshaping\n   - 2D convolutional layers typically expect input with the shape (None, x, y, c), where c is the colour channel and is 1 or 3. We need to split the (x, y, 4) observation array into 4 (None, 72, 96, 1) arrays.\n - Scaling \n   - The data range is 0 -> 255. This needs to be scaled to 0 -> 1.\n - Adding time\n   - Static frames don't convey direction or speed, only position. We can add temporal information to each frame in a couple of ways:\n       - Remember the past n frames, and also give those to the network in the shape (None, x, y, n_buffer). This is very effective for solving pong, however increases the dimensionality of the network.\n       - Remember the previous frame and calculate the difference between it and the latest frame. We'll this do that here.\n       \nThese 3 steps can be accomplished using another wrapper class that follows the expected Gym API:","db385c13":"# Convolutional DQN\n\n**Work in progress!** Please forgive lack of clarity, bugs, and typos.\n\nThis notebook follows on from [Deep Q-learner starter code](https:\/\/www.kaggle.com\/garethjns\/deep-q-learner-starter-code), which includes code for running a DQN in Python using Keras with a simple fully connected model. Here we add a convolutional model instead, which is slightly less straightforward than the typical Pong-solving networks. We'll also develop the wrappers required to use the observations from the SMM version of the GFootball environment.\n\nTo keep this notebook short and focused, the example training the mode use my package [reinforcement-learning-keras package](https:\/\/github.com\/garethjns\/reinforcement-learning-keras). Alternatively, code in [Deep Q-learner starter code](https:\/\/www.kaggle.com\/garethjns\/deep-q-learner-starter-code) can be used instead, without requiring any additional packages. In either case, the model needs to be saved as a keras model to be used in a submission. This conversion process is shown below, and can be run locally by following the (again, WIP) structure and examples here https:\/\/github.com\/garethjns\/kaggle-football .","5b0635fe":"## Saving model weights\n\nThe submitted agent only needs the action model from the agent trained above. This requires the model arcitecture and custom layer defined above, and the trained weights. The trained weights can be saved with:","2aa69a4c":"And the output now looks a bit different....","a8f8c124":"Note how training becomes extremely slow after the replay buffer is filled and the model training begins. This should be much faster on GPU.","5bad9628":"# Test submission\n\nThe written submission file can be tested with the following block. \n\nSee here: https:\/\/github.com\/garethjns\/kaggle-football\/blob\/main\/scripts\/debug_agent.py for a version that can be used to run locally, but also maintain debugability.","83909928":"# Set up","5df99c20":"# Conclusions and next steps\n\nThis code should be enough to get started with, but like the dense model in the previous notebook, I haven't properly tested this model yet. It should be able to learn something in this environment but will probably require a lot of tweaking and significant training time. \n\nOne potential optimisation would be to reduce the inputs for the ball and active player, treating those as whole frames is expensive, and they could as well be represented by an array of (4,) ie. [x_last, y_last, x_now, y_now].\n\nThe submission still needs work too. It appears to intermittently time out during when running in the notebook enviroment, which forfeights the game. Similarly, when submitted some games end in an [Err] rather than a [Win], [Loss] or [Tie]. The log output isn't available, but it's likely this is also caused by intermittent time outs.","96f4aae9":"This model works as a drop in replacement for the dense model shown in [Deep Q-learner starter code](https:\/\/www.kaggle.com\/garethjns\/deep-q-learner-starter-code), and no additional changes to the replay buffer or agent are required. It's possible to reuse the code there with this model, although to avoid copying and pasting that code, I'm going to import from [reinforcement-learning-keras](https:\/\/github.com\/garethjns\/reinforcement-learning-keras) instead.\n\nOne difference to bear in mind is the agent here handles building and wrapping the env, so we only need to specify the env name and the wrapper to add. To use the environment and wrapper in a training loop as show in [Deep Q-learner starter code](https:\/\/www.kaggle.com\/garethjns\/deep-q-learner-starter-code), this code should be used:\n\n```python\nsmm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nwrapped_smm_env = SMMFrameProcessWrapper(smm_env)\n```","e9f394ee":"One potential downside here is that non-moving players are invisible!","3d874076":"# Model\n\nThe below class builds a convolutional network, however there's an additional complication compared building networks to solve games like Pong. The input here is 4 independent frames, rather than a single frame. We could prehaps combined them in the wrapper, or another solution would be to build multiple inputs.\n\nHere the first layer in the nextwork is a custom layer that splits the input array (72, 96, 4) on the last dimension. ","c6559181":"After going through the split layer, each array (now (72, 96, 1) each) go through seperate convolutional branches of the model and are concatenated into a Dense layer.","6e8bf9a9":"# Environment\n\n## SMM observation space\n\nThe \"GFootball-11_vs_11_kaggle-SMM-v0\" enviroment in GFootball uses the [SMMWrapper](https:\/\/github.com\/google-research\/football\/blob\/master\/gfootball\/doc\/observation.md) observation wrapper to return frames showing team positions, ball position, and current active player position.","6690cb1f":"The enviroment is wrapped by running:","9762a97a":"## Constructing main.py\n\nWe need:\n  - The agent(obs) function defined\n  - The action model redefined\n    - Architecture (including custom layer)\n    - The weights serialised above\n  - The environment wrapper redefined - we need to buffer the observations as in training.\n\nWe **don't** need:\n  - Components of the agent only used in training\n    - The replay buffer\n    - EpisolonGreedy action selection\n  \nThe agent function is new and needs to handle:\n - Taking the raw observation and processing to the same shape that the model was trained on - ie. the SMM space with the buffering added by SMMFrameProcessWrapper.\n - Using the naked keras model for prediction\n - Applying the \"policy\" of the Q learner, which is just argmax over the action values\n - Return the action predicted to be most valuable\n    \nIt'll look something like this:\n```\ndef agent(obs):\n    \n    # Use the existing model and obs buffer on each call to agent\n    global tf_mod\n    global obs_buffer\n\n    # Get the raw observations return by the environment\n    obs = obs['players_raw'][0]\n    # Convert these to the same output as the SMMWrapper we used in training\n    obs = observation_preprocessing.generate_smm([obs])\n    \n    # Use the SMMFrameProcessWrapper to do the buffering, but not enviroment\n    # stepping or anything related to the Gym API.\n    obs_buffer._obs_buffer.append(obs)\n    \n    # Predict actions from keras model\n    actions = tf_mod.predict(obs)\n    action = np.argmax(actions)\n\n    return [action]\n```","c1b13a44":"# Creating submission\n\nThis approach saves everything required to run the agent into one python file, including the model weights."}}