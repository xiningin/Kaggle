{"cell_type":{"f386fa04":"code","3b623059":"code","460c92be":"code","cf95d1af":"code","afb2b25c":"code","eab78ac2":"code","3b88e067":"code","737b4361":"code","8db082eb":"code","d26c0391":"code","a7be963e":"markdown","b877f5e1":"markdown","90b25cb4":"markdown","09c34eea":"markdown","1897d551":"markdown","1698ad6d":"markdown","1a66eb74":"markdown","5f7ff734":"markdown","d8d6dd71":"markdown","7dcf9934":"markdown","a1e63593":"markdown","cd0f2b94":"markdown","b657478e":"markdown","88fb8c25":"markdown","ee28a89d":"markdown","adfddf46":"markdown"},"source":{"f386fa04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b623059":"import numpy as np, pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\n# Import data\ndf = pd.read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv', names=['value'], header=0)\n\n# Original Series\nfig, axes = plt.subplots(3, 2, sharex=True)\naxes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')\nplot_acf(df.value, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(df.value.diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(df.value.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(df.value.diff().diff().dropna(), ax=axes[2, 1])\n\nplt.show()","460c92be":"# MA behaviour\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.style.use('ggplot')\ndf = pd.read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv')\n\n\n\nplt.plot(df['Monthly_beer_production'], label='Observed', color='blue' )\n# 12months because of the STL decomposition\nplt.plot(df['Monthly_beer_production'].rolling(12).mean(), label='MA 12 months', color='red')\nplt.legend(loc='best')\nplt.title('Monthly beer production\\nObserved and Moving Averages')\nplt.show()","cf95d1af":"# MA parameter\nimport pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\ndf = pd.read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv', names=['value'], header=0)\n\n\nfig, axes = plt.subplots(1, 2, sharex=True)\n# axes[0].plot(df.value.diff()); axes[0].set_title('1st Differencing')\naxes[0].plot(df.value.diff().diff()); axes[0].set_title('2nd Differencing')\naxes[1].set(ylim=(0,1.2))\n# plot_acf(df.value.diff().dropna(), ax=axes[1])\nplot_acf(df.value.diff().diff().dropna(), ax=axes[1])\n#\nplt.show()","afb2b25c":"import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndf = pd.read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv')\n\n\nresult = adfuller(df.Monthly_beer_production.dropna())\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","eab78ac2":"from pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom pandas.plotting import autocorrelation_plot\n\n# import data\ndef parser(x):\n\treturn datetime.strptime(x, '%Y-%m')\nseries = read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv',\n\t\t\t\t  header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nseries.index = series.index.to_period('M')\n\nautocorrelation_plot(series)\n\npyplot.show()","3b88e067":"import warnings\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom numpy import append\n\nplt.style.use('fivethirtyeight')\n\n\n\n\n\ndata_file_production = np.loadtxt('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv', delimiter=',', skiprows=1, usecols=(1, 1))\ndata = data_file_production[:,1]\nprint(data)\ny=data\n\n# # Define the p, d and q parameters to take any value between 0 and 2\np = d = q= range(0, 3)\np1 = range(2,3)\nd1 = q1 = range(1,2)\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p1, d1, q1))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\nAICvalue=[]\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n            AICvalue.append(results.aic)\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue\nprint(AICvalue)\nminimumAIC= min(AICvalue)\nprint(\"min value element : \", minimumAIC)\n","737b4361":"# testing ARIMA accuracy\n# base of the model is from https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/\n# evaluate an ARIMA model using a walk-forward validation\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n\n# import data\ndef parser(x):\n\treturn datetime.strptime(x, '%Y-%m')\nseries = read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv',\n\t\t\t\t  header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nseries.index = series.index.to_period('M')\n\nexpected=[]\n# split into train and test sets\nX = series.values\nsize = int(len(X) * 0.66)\ntrain, test = X[0:size], X[size:len(X)]\nactual = [x for x in train]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\t# optimal\n\tmodel = ARIMA(actual, order=(2,1,1),seasonal_order=(2, 0, 1, 12))\n\t# first guess\n\t# model = ARIMA(actual, order=(1,1,2),seasonal_order=(0, 2, 2, 12))\n\n\tmodel_fit = model.fit()\n\tforecast = model_fit.forecast(15)\n\tyhat = forecast[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\tactual.append(obs)\n\texpected.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\n\n# Accuracy\ntpredictions = pd.DataFrame(predictions)\ntexpected = pd.DataFrame(expected)\n\nprint(tpredictions)\nprint(texpected)\n\nmape = np.mean(np.abs(tpredictions - texpected)\/np.abs(texpected))  # MAPE\nprint(mape)\n\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)","8db082eb":"# base of the model is from https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/\n# evaluate an ARIMA model using a walk-forward validation\nimport export as export\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# import data\ndef parser(x):\n\treturn datetime.strptime(x, '%Y-%m')\nseries = read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv',\n\t\t\t\t  header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nseries.index = series.index.to_period('M')\n\n\n# split into train and test sets\nX = series.values\nsize = int(len(X) * 0.66)\ntrain, test = X[0:size], X[size:len(X)]\n\nhistory = [x for x in train]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(2,1,1), seasonal_order=(2, 0, 1, 12))\n\t# model = ARIMA(history, order=(1,1,1))\n\tmodel_fit = model.fit()\n\toutput = model_fit.forecast(15, alpha=0.05)\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\n\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n\nprint(output)\n\n\npredictionsout=np.full(len(predictions), None)\noutput = np.append(predictionsout, output)\nprint(output)\n\npyplot.plot(test)\npyplot.plot(predictions, color='red')\n\n# pyplot.subplot(1, 2, 2)\npyplot.plot(output, color='red')\n\npyplot.show()","d26c0391":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n\ndatabase = pd.read_csv('..\/input\/d\/hulla2hop\/monthly-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv')\nmonthly = database.Monthly_beer_production\ndata = np.array(monthly)\n\nindex = pd.date_range(start='1956-01', end='1995-09', freq='M')\nbeerdata = pd.Series(data, index)\n\nfit24 = Holt(beerdata, damped_trend=True, initialization_method=\"estimated\").fit(\n    damping_trend=0.98)\nfcast24 = fit24.forecast(16)\nprint(fcast24)\n\nplt.figure(figsize=(12, 8))\nplt.plot(beerdata, marker=\"o\", color=\"black\")\n\nplt.plot(fit24.fittedvalues, color=\"green\")\n(line4,) = plt.plot(fcast24, color=\"green\")\n\nplt.show()","a7be963e":"The result ARIMA(2,1,1)x(0,2,2,12) for free selection of seasonal part will come out with 6.2859% inaccuracy which means 93.7141% accuracy of the model. \nSeems good but I wanted to see results for d=1 in seasonal part as well. It is ARIMA(2,1,1)x(2,1,1,12) with 5.9965% which is better than previous. So, computer came up with better solution. But then I found the solution here in Kaggle: ARIMA(2,1,1)x(2,0,1,12) with 5.9618% even RSME was lower. It means, we both, computer and I did not reached the optimal solution. I blame the program I choose or minAIC criteria otherwise procedure seems  reasonable. ","b877f5e1":"![STL_addititive_period3.png](attachment:2cfc4548-83b2-46bf-aece-447b3868f1aa.png)","90b25cb4":"I used ARIMA model with guessed parameters to get some results but I wanted to use some machine learning and not to do it by hand. This was partially successful. I found and applied program which makes variations over pdq parameters (p,d,q)x(p,d,q,seasonal_period) and investigate their Akaike Information Criteria (AIC). But I will provide it below with already fixed some parameters since it takes over 700 iterations to do it for values in the range (0:3). But the result was not satisfying since after investigating the selected model ARIMA(1,1,2)x(0,2,2,12) the MAPE was near to 10%.\n\nI did \u00a0with: mape = np.mean(np.abs(predictions - expected)\/np.abs(expected)) \u00a0\nAnd rsme: rmse = sqrt(mean_squared_error(test, predictions)) I made event RSME test, although, MAPE is better to understand, since it provides information about % of non-accurate predictions. RMSE and MAPE predictions behave simmilar: for better solution they are lower, for worse solution they are bigger. I acctualy did both but explained it on MAPE.\n\nTo estimate ARIMA parameters, I used Autocorrelation, Moving average and differencing for the main parts in ARIMA. Rest I left for the computer to let the machine learn at least something. I started with a difference.","09c34eea":"Here I see that this series will need at least one difference (actually, it will be 2 but the computer found this out). I will set p in range(1,3). \n\n\nI ca fix some values for ARIMA(p,d,q)x(p1,d1,q1,seasonal_period):\np = d = q= range(0, 3)\np1 = range(2,3)\nd1 = q1 = range(1,2)\nseasonal_period = 12\nand lower the number of ARIMA model variations.\n","1897d551":"**NOTE**\n* I did not run this notebook since it contains the program with multiple iterations.\n* I am aware it could have been made nicer and cleaner. \n* I left sources of the codes I used to show I did not wrote them.","1698ad6d":"In the last graph for autocorrelation of 2nd order differencing I can see that data are a little bit over differenced. From this, I know d parameter will be 1.","1a66eb74":"**optimal ARIMA(2,1,1)x(2,0,1,12) model**","5f7ff734":"**Holt's method with damped trend**","d8d6dd71":"![ARIMA optimal.png](attachment:de49e806-a7e7-46c9-92fc-df9c5cb6f36a.png)","7dcf9934":"![holts.png](attachment:60d216f4-b0b9-4bfd-af10-87df27b32574.png)","a1e63593":"Moving averages (MA\/q) part can be actually found out during the running of autoARIMA program. For p = 2 the program prints warnings about MA part (this is said to be ok) but the program also has a problem providing Likelihood optimization and that is a problem. This applies for both MA parts - the main and the one in the seasonal part. So, now I know both q and q in sesional part are in range(0:2) but when I looked into MA estimation, I knew it will be 1. It is because without MA data do not behave nicely. They fluctuate too much.\u00a0","cd0f2b94":"To see if AR therm should be more than 0 I took ADFuller test. From high p-value: 0.177621 I know p parameter will be greater than 0.","b657478e":"![ARIMAvsHolts.PNG](attachment:0f86050e-c2c6-4252-aed0-62c81c83269c.PNG)","88fb8c25":"Even if ARIMA type of methods are not recommended for non-stationary time series, it is possible to use them when the series is transformed by differencing. So, I tried it as well. I know there is a package pdmarima that can be used for an automatic choice of ARIMA parameters (p,d,q)x(p,d,q,seasonal_period) but I do not have a licence for it. From STL I know that seasonal_period is 12. If it would not be so, the decomposition would look like in the picture below. Notice how residuals do not converge.","ee28a89d":"**6. Comparism and 7.Conslusion:**\n\n\nThe semi-automatic ARIMA process led to **ARIMA(2,1,1)x(2,1,1,12)** with 94,0035% accurate prediction which is less than accuracy of the **optimal ARIMA(2,1,1)x(2,0,1,12) model**.\nBased on the KPSS and ADFuller test the **Holt's method with damped trend** should be better model for the forecast.\nEven from the table above it is obvious that Holt's forecast is a little bit higher than ARIMA forecast. This would agree with increasing tendency of original data.","adfddf46":"![holtsprediction.png](attachment:29fcd295-9494-429f-8e05-f66f56aefa8c.png)"}}