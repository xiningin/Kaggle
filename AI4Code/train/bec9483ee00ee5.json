{"cell_type":{"cb5f41dd":"code","9abc1810":"code","2c367b19":"code","985fe1e9":"code","16d20d9b":"code","be269c29":"code","062c5ea0":"code","bae63310":"code","a39c4d11":"code","f9972ec4":"code","e9ac722f":"code","f567299b":"code","5321f72e":"code","bc04d8c7":"code","9fae0ca3":"code","5c216f9a":"code","10ffe910":"code","cb234699":"code","e69c4ac2":"code","01bf7b1f":"code","63682225":"code","6bf5a6ba":"markdown","c19eb4bf":"markdown","7a7cd4f9":"markdown","fe5f539a":"markdown","21adf9c0":"markdown"},"source":{"cb5f41dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9abc1810":"\nimport keras\nfrom keras.datasets import cifar10","2c367b19":"# Lets import some usefull libraries\nimport matplotlib\nfrom keras.utils import np_utils\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport sys\nprint(\"sys version: {}\".format(sys.version))\nprint('matplotlib version: {}'.format(matplotlib.__version__))\nprint(f\"keras version : {keras.__version__}\")","985fe1e9":"#Lets get our train and test datasets\n(X_train,y_train),(X_test,y_test)=cifar10.load_data()","16d20d9b":"##now since we have the dataset, lets explore the data a little bit\n\nprint(f\"train data shape :{X_train.shape}\")\nprint(f\"test data shape: {X_test.shape}\")\nprint(X_train[0].shape)\n\n#lets plot some images. pretty small images. it is said, that human accuracy is about 90 in this dataset.\n#So, if we can achieve an accuraacy higher than that, we will do a pretty good job\n\nf=plt.figure(figsize=(5,5))\nfor i in range(0,9):\n    f.add_subplot(330+1+i)\n    img=X_test[i]\n    plt.imshow(img)","be269c29":"#The  images are not very clear and of resolution of 32 by 32 pixelas you  can see from the X_train shape\n#Lets normalize the data by dividing by 255\nseed=6\nnp.random.seed(seed)\nX_train=X_train\/255.0\nX_test=X_test\/255.0\n# X_train=X_train.astype(\"float32\")\nprint(X_train[0])","062c5ea0":"# we will have to do some pre-processing with the label data as its a nulticlass classification(10 class) problem so we are expecting a label shape of 10,1\n# we can do that by deploying one hot encoding, We have printed the shape, before and after the encoding. To carry out the encoding\n# we will use np_utils library which we have imported at the biggining of the notebook.\nprint(\"original shape of label data:\")\nprint(y_train.shape)\nprint(y_test.shape)\nprint(\"shape of label dataset after one hot encoding:\")\ny_test_cat=np_utils.to_categorical(y_test)\ny_train_cat=np_utils.to_categorical(y_train)\nprint(y_test_cat.shape)\nprint(y_train_cat.shape)\nprint(f\"no of classes: {y_train.shape[1]}\")","bae63310":"# Here are the actual classes of the dataset. I will encourage you to go through the paper for this model(ALL-CNN) link of which has been mentioned in the biggining.\nclasses=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truc']\nplt.imshow(X_train[0])\nprint(y_train[0])\nprint(f'the image is that of a : {classes[int(y_train[0])]}')","a39c4d11":"#Lets start building the model. First import keras components\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Activation,GlobalAveragePooling2D\nfrom keras.optimizers import SGD","f9972ec4":"#Lets start adding the layers as stated in the architecture stated above\n\ndef allcnn(weights=None):\n\n    model=Sequential()\n\n    model.add(Conv2D(96,(3, 3), padding='same', input_shape=(32,32,3)))\n    model.add(Activation('relu'))\n    model.add(Conv2D(96,(3,3), padding = 'same', ))\n    model.add(Activation('relu'))\n    model.add(Conv2D(96,(3,3), padding = 'same',strides = (2,2) ))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(192,(3,3), padding = 'same', ))\n    model.add(Activation('relu'))\n    model.add(Conv2D(192,(3,3), padding = 'same', ))\n    model.add(Activation('relu'))\n    model.add(Conv2D(192,(3,3), padding = 'same',strides = (2,2) ))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(192,(3,3), padding = 'same', ))\n    model.add(Activation('relu'))\n    model.add(Conv2D(192,(1,1),padding='valid'))\n    model.add(Activation('relu'))\n    model.add(Conv2D(10,(1,1),padding='valid'))\n    #addding  global averAGE pooling with softmax\n\n    model.add(GlobalAveragePooling2D())\n    model.add(Activation('softmax'))\n    #We will use weights from a pre trained model\n    if weights:\n        model.load_weights(weights)\n    return model","e9ac722f":"# In this cell, we will define some hyper-parameters, compile the model and print the architecture of the model.\n\n#hyperparameters\n\nlearning_rate=0.01\nweight_decay=1e-6\nmomentum=0.9\n\n#defining the model as the function defined in the above cell\n\nmodel=allcnn()\n\n\n#Defining the optimizer\n\nsgd=SGD(lr=learning_rate, decay = weight_decay, momentum = momentum ,    nesterov = True)\n\n#compiling the model\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics  = ['accuracy'] )\n\n# print the model architecture\n\nprint(model.summary())","f567299b":"# We are not training the model, instead we will load the model with pretrained weights as mentioned in the above cell\n#epochs=350\n#batch_size=32\n# (X_train,y_train),(X_test,y_test)\n#model.fit(X_train,y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32, verbose = 1)\n","5321f72e":"#the weights are stored in this \n# weights=\"https:\/\/github.com\/PAN001\/All-CNN\/blob\/master\/all_cnn_best_weights_2.hdf5\"\nweights='..\/input\/all-cnn-weight\/all_cnn_weights.hdf5'\n\nmodel=allcnn(weights=weights)\n\n#compiling the model\nsgd=SGD(lr=learning_rate, decay = weight_decay, momentum = momentum ,    nesterov = True)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics  = ['accuracy'] )\n\n# print the model architecture\nprint(model.summary())","bc04d8c7":"# NOW lets test the model and evaluate it on our test data\nprint(X_test.shape)\nscores=model.evaluate(X_test,y_test)\n\n\n# We can see below that with the pretrained weights we are able to get an accuracy of more than 90% on our test data 10000 objects fro 10 different classes\n","9fae0ca3":"print(scores)","5c216f9a":"# Lets prepare a dicionary with class labels and description\nclasses=range(0,10)\n\nnames=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truc']\n\nclass_labels=dict(zip(classes,names))\nprint(class_labels)","10ffe910":"batch=X_test[9900:9905]\n\n# Lets make some predictions\n\npredictions=model.predict(batch)\nprint(f\"following are the softmax output from our predictions \\t: {predictions}\")\n\n# this will actually give out the softmax output, which is the probabilities for each class\n# SO, using the argma function we wil convert the output to be the index with the  highest probability\nclass_results=np.argmax(predictions,axis=-1)\nprint(F\"following the the predictions for our batch after applying the argmax method:\\t {class_results}\")","cb234699":"# lets print the class label once again \nprint(class_labels)\n\n# as per the above predictions X_test[9900] is a class 8 object or a ship\nplt.imshow(X_test[9900])","e69c4ac2":"# Lets check one more example,\nobject =np.array([X_test[9904]]) \nprediction_x_test_9904= model.predict(object)\n\nresult = np.argmax(prediction_x_test_9904,axis=-1)\nprint()\nprint(f\"the prediction of the below image is:\\t {class_labels[int(result)]}, and the actual label is:\\t {class_labels[int(y_test[9904])]}\" )\nplt.imshow(X_test[9904])","01bf7b1f":"# Lets check one more example,\nobject =np.array([X_test[1000]]) \nprediction_x_test_1000= model.predict(object)\n\nresult = np.argmax(prediction_x_test_1000,axis=-1)\nprint()\nprint(f\"the prediction of the below image is:\\t {class_labels[int(result)]}, and the actual label is:\\t {class_labels[int(y_test[1000])]}\" )\nplt.imshow(X_test[1000])","63682225":"# The model has done a good job in predicting 10 different types of objects. This method of applying pretrained weights to a complex model is also termed as transfer learning.\n\n# Hope you guys have liked the notebook, thanks","6bf5a6ba":"\"\"\"\nas you can see its a large model with Total params: 1,369,738. It will take a long time to train the model on CPU.\nWith GPU, the time is reduced drastically, but still it will take some 10 hours or so, if we train it for an epoch of around 350.\n\nSo, long story cut short, we will use the pretrained weight that we have downloaded from the git-hub repository  \"https:\/\/github.com\/PAN001\/All-CNN\".\nI am commenting out the instructions for training, instead will load the pretrained weights and proceed with testing the model.\n\"\"\"","c19eb4bf":"Lets first load the train and test dataset. This dataset is already a part of keras dataset.Hence, all we need to do is to first import cifar10 from keras.dataset and then use the load_data() method","7a7cd4f9":"in the paper \"https:\/\/arxiv.org\/pdf\/1412.6806.pdf\" there are three models mentioned. We will use the modelC. The architecture of the model is as stated below:\n\n* 3\u00d73conv.96ReLU\n* 3\u00d73conv.96ReLU\n* 3*3 maxpooling stride 2\n* 3\u00d73conv.192ReLU\n* 3\u00d73conv.192ReLU\n* 3\u00d73max-pooling stride2\n* 3\u00d73conv.192ReLU\n* 1\u00d71conv.192ReLU\n* 1\u00d71conv.10ReLU\n* global averaging over6\u00d76spatial dimensions\n\n* 10 way softmax\n","fe5f539a":"##Lets preprocess the data. We will carry out Normalization and one hot encoding on our data, before we use the data for training the model","21adf9c0":"In CIFAR-10 dataset 60000 images are devided into 10 common classes. 50k images for train and 10k for testing the performance of the network. \n\nwe will be using the All-CNN network **published in the 2015 ICLR paper, \"Striving For Simplicity: The All Convolutional Net\"**. This paper can be found at the following link:\n\nhttps:\/\/arxiv.org\/pdf\/1412.6806.pdf \n\nIts a relatively complaex network and takes a much longer time to train on CPU. we will use a pretrained model by using it's weights and will make predictions on the test dataset. The weights of the pretained model has been derived from the git-hub repository of \"https:\/\/github.com\/PAN001\/All-CNN\".\nWe can simply import the data from keras.dataset. First import cifar10 and then run the command cifar10.load_data()\n"}}