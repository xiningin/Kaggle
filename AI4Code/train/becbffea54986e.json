{"cell_type":{"a9ef229c":"code","f860a048":"code","a15608c4":"code","5615e233":"code","1e20ad20":"code","0648d93f":"code","8271dc88":"code","4005b34c":"code","5c51bcc3":"code","0d899b75":"code","c91acee9":"code","c8d60b5f":"code","e54e9835":"code","b1333352":"code","2bb2114d":"code","cc0cf3e2":"code","d9902c12":"code","84c5d5a9":"code","9bfb9a33":"code","cbf9ecbd":"code","1586bbc1":"code","a0525fec":"code","ac612d3d":"code","177c55d0":"code","a5c0eb78":"code","83ddf795":"code","3041f14c":"code","71f84781":"code","b1860e9d":"code","8a6800e9":"code","c3d44b3d":"code","60c61225":"code","704d4850":"code","2e8a222c":"code","dc8a4445":"code","bd76bed9":"code","486894ba":"code","a59578ae":"code","b97ad63d":"code","ff7e9593":"code","edf4e98a":"code","fbf92e11":"code","137be704":"code","2aa903c8":"code","b9e19de5":"code","e4368636":"code","26a7096e":"code","9eb10240":"code","1926db44":"markdown","014166f3":"markdown","8969cc5b":"markdown","46c57c2d":"markdown","ca26588a":"markdown","62cf7a57":"markdown","e522196d":"markdown","9170a8c0":"markdown","0a282b00":"markdown","d8a13ba8":"markdown","63c97b64":"markdown","c98fa97d":"markdown","b8298362":"markdown","485d974e":"markdown","73f2ba56":"markdown","9f46632c":"markdown","8e6c180b":"markdown","b7226c4d":"markdown","f02f0ba3":"markdown","6422d399":"markdown","6b4a91ac":"markdown"},"source":{"a9ef229c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\npd.set_option('display.max_columns', 500)\nwarnings.filterwarnings(\"ignore\")","f860a048":"# Read the Data\ndata = pd.read_csv('..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')\ndata.head(3)","a15608c4":"# let's explore the shape of the data. \ndata.shape","5615e233":"# Let's Check if the data contains any missing or NaN values.\ndata.isnull().any()","1e20ad20":"data.info()","0648d93f":"data.drop(['ID', 'ZIP Code'], axis = 1, inplace = True)","8271dc88":"data.isnull().sum()","4005b34c":"# Dividing the columns in the dataset in to numeric and categorical attributes.\ncols = set(data.columns)\ncols_numeric = set(['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage'])\ncols_categorical = list(cols - cols_numeric)\ncols_categorical","5c51bcc3":"for x in cols_categorical:\n    data[x] = data[x].astype('category')\n\ndata.info()","0d899b75":"data.describe().transpose()","c91acee9":"data_num = data.select_dtypes(include='number')\ndata_cat = data.select_dtypes(include='category')\nprint(f'Numerical Attributes: {list(data_num.columns)}')\nprint(f'Categorical Attributes: {list(data_cat.columns)}')","c8d60b5f":"# Let's construct a function that shows the summary and density distribution of a numerical attribute:\ndef summary(x):\n    x_min = data[x].min()\n    x_max = data[x].max()\n    Q1 = data[x].quantile(0.25)\n    Q2 = data[x].quantile(0.50)\n    Q3 = data[x].quantile(0.75)\n    print(f'5 Point Summary of {x.capitalize()} Attribute:\\n'\n          f'{x.capitalize()}(min) : {x_min}\\n'\n          f'Q1                    : {Q1}\\n'\n          f'Q2(Median)            : {Q2}\\n'\n          f'Q3                    : {Q3}\\n'\n          f'{x.capitalize()}(max) : {x_max}')\n\n    fig = plt.figure(figsize=(16, 10))\n    plt.subplots_adjust(hspace = 0.6)\n    sns.set_palette('pastel')\n    \n    plt.subplot(221)\n    ax1 = sns.distplot(data[x], color = 'r')\n    plt.title(f'{x.capitalize()} Density Distribution')\n    \n    plt.subplot(222)\n    ax2 = sns.violinplot(x = data[x], palette = 'Accent', split = True)\n    plt.title(f'{x.capitalize()} Violinplot')\n    \n    plt.subplot(223)\n    ax2 = sns.boxplot(x=data[x], palette = 'cool', width=0.7, linewidth=0.6)\n    plt.title(f'{x.capitalize()} Boxplot')\n    \n    plt.subplot(224)\n    ax3 = sns.kdeplot(data[x], cumulative=True)\n    plt.title(f'{x.capitalize()} Cumulative Density Distribution')\n    \n    plt.show()","e54e9835":"summary('Age')","b1333352":"summary('Experience')","2bb2114d":"summary('Income')","cc0cf3e2":"summary('CCAvg')","d9902c12":"summary('Mortgage')","84c5d5a9":"# Create a function that returns a Pie chart and a Bar Graph for the categorical variables:\ndef cat_view(x = 'Education'):\n    \"\"\"\n    Function to create a Bar chart and a Pie chart for categorical variables.\n    \"\"\"\n    from matplotlib import cm\n    color1 = cm.inferno(np.linspace(.4, .8, 30))\n    color2 = cm.viridis(np.linspace(.4, .8, 30))\n    \n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    \n     \n    \"\"\"\n    Draw a Pie Chart on first subplot.\n    \"\"\"    \n    s = data.groupby(x).size()\n\n    mydata_values = s.values.tolist()\n    mydata_index = s.index.tolist()\n\n    def func(pct, allvals):\n        absolute = int(pct\/100.*np.sum(allvals))\n        return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\n\n    wedges, texts, autotexts = ax[0].pie(mydata_values, autopct=lambda pct: func(pct, mydata_values),\n                                      textprops=dict(color=\"w\"))\n\n    ax[0].legend(wedges, mydata_index,\n              title=\"Index\",\n              loc=\"center left\",\n              bbox_to_anchor=(1, 0, 0.5, 1))\n\n    plt.setp(autotexts, size=12, weight=\"bold\")\n\n    ax[0].set_title(f'{x.capitalize()} Piechart')\n    \n    \"\"\"\n    Draw a Bar Graph on second subplot.\n    \"\"\"\n    \n    df = pd.pivot_table(data, index = [x], columns = ['Personal Loan'], values = ['Income'], aggfunc = len)\n\n    labels = df.index.tolist()\n    loan_no = df.values[:, 0].tolist()\n    loan_yes = df.values[:, 1].tolist()\n    \n    l = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    rects1 = ax[1].bar(l - width\/2, loan_no, width, label='No Loan', color = color1)\n    rects2 = ax[1].bar(l + width\/2, loan_yes, width, label='Loan', color = color2)\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax[1].set_ylabel('Scores')\n    ax[1].set_title(f'{x.capitalize()} Bar Graph')\n    ax[1].set_xticks(l)\n    ax[1].set_xticklabels(labels)\n    ax[1].legend()\n    \n    def autolabel(rects):\n        \n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        \n        for rect in rects:\n            height = rect.get_height()\n            ax[1].annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        fontsize = 'large',   \n                        ha='center', va='bottom')\n\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()","9bfb9a33":"cat_view('Family')","cbf9ecbd":"cat_view('Education')","1586bbc1":"cat_view('Securities Account')","a0525fec":"cat_view('CD Account')","ac612d3d":"cat_view('Online')","177c55d0":"splot = sns.countplot(x = 'Personal Loan', data = data)\n\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')","a5c0eb78":"X = data.drop('Personal Loan', axis = 1)\nY = data[['Personal Loan']]","83ddf795":"corr = X.corr()\nplt.figure(figsize=(10, 8))\ng = sns.heatmap(corr, annot=True, cmap = 'summer_r', square=True, linewidth=1, cbar_kws={'fraction' : 0.02})\ng.set_yticklabels(g.get_yticklabels(), rotation=0, horizontalalignment='right')\nbottom, top = g.get_ylim()\ng.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","3041f14c":"# Let's plot all Dependent variables to see their inter-relations.\nsns.pairplot(X, diag_kind = 'kde', vars = list(data_num.columns))","71f84781":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1, stratify = Y)","b1860e9d":"from sklearn.feature_selection import mutual_info_classif\nmutual_information = mutual_info_classif(X_train, y_train, n_neighbors=5, copy = True)\n\nplt.subplots(1, figsize=(26, 1))\nsns.heatmap(mutual_information[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True, annot_kws={\"size\": 20})\nplt.yticks([], [])\nplt.gca().set_xticklabels(X_train.columns, rotation=45, ha='right', fontsize=16)\nplt.suptitle(\"Variable Importance (mutual_info_classif)\", fontsize=22, y=1.2)\nplt.gcf().subplots_adjust(wspace=0.2)","8a6800e9":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nrf_clf.fit(X_train, y_train)\n\nfeatures = list(X_train.columns)\nimportances = rf_clf.feature_importances_\nindices = np.argsort(importances)\n\nfig, ax = plt.subplots(figsize=(10, 7))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nax.tick_params(axis=\"x\", labelsize=12)\nax.tick_params(axis=\"y\", labelsize=14)\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance', fontsize = 18)","c3d44b3d":"# from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nX_train_num = X_train.select_dtypes(include='number')\nX_train_cat = X_train.select_dtypes(include='category')\n\nnum_attribs = list(X_train_num.columns)\ncat_attribs = list(X_train_cat.columns)\n\ntransformer = ColumnTransformer([\n        (\"num\", StandardScaler(), num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nX_train = transformer.fit_transform(X_train)\nprint(X_train.shape)\nX_train[1, :]","60c61225":"y_train = np.array(y_train)\nprint(y_train.shape)","704d4850":"def train_model(model):\n    m = model[1]\n    y_train_pred = cross_val_predict(model[1], X_train, y_train, cv=5)\n    cm = confusion_matrix(y_train, y_train_pred)\n    print('Confusion matrix: ' + model[0])\n    print(cm)\n    print()\n    accuracy = accuracy_score(y_train, y_train_pred)\n    precision = precision_score(y_train, y_train_pred)\n    recall = recall_score(y_train, y_train_pred)\n    f1 = f1_score(y_train, y_train_pred)\n    print(f'{model[0]} Accuracy: {accuracy}')\n    print(f'{model[0]} Precision: {precision}')\n    print(f'{model[0]} Recall: {recall}')\n    print(f'{model[0]} f1 - score: {f1}')","2e8a222c":"train_model(('Gaussian Naive Bayes', GaussianNB()))","dc8a4445":"train_model(('Logistic Regression', LogisticRegression(solver=\"liblinear\")))","bd76bed9":"train_model(('k Nearest Neighbor', KNeighborsClassifier(n_neighbors= 7, weights = 'distance' )))","486894ba":"train_model(('SVM', SVC(gamma='auto')))","a59578ae":"train_model(('CART', DecisionTreeClassifier()))","b97ad63d":"train_model(('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)))","ff7e9593":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestClassifier(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='f1',\n                           return_train_score=True)\ngrid_search.fit(X_train, y_train)","edf4e98a":"grid_search.best_params_","fbf92e11":"rf_clf = grid_search.best_estimator_","137be704":"X_test = transformer.fit_transform(X_test)\nprint(X_test.shape)","2aa903c8":"y_test = np.array(y_test)\nprint(y_test.shape)","b9e19de5":"rf_clf.fit(X_test, y_test)","e4368636":"y_test_predict = rf_clf.predict(X_test)","26a7096e":"rf_clf.score(X_test, y_test)","9eb10240":"print(metrics.classification_report(y_test, y_test_predict, labels=[1, 0]))","1926db44":"# 3. Data Pre-processing\nFirst, we create a train and a test set from the data.","014166f3":"### Observation:\n - We see that the income of the customers are skewed to the left with a median income of $64,000.\n - The Income attribute also has a lot of outlier values.","8969cc5b":"### Observation:\n - The distribution of 'CD Account' and 'Securities Account' are heavily imbalanced. ","46c57c2d":"### Observation:\n - We see that the age of the customers are uniformly distributed between the Min age of 23 and Max age of 67.","ca26588a":"### Observation:\n - We see that the Average Monthly Credit Card spending of the customers are skewed heavily to the left, with a lot of outlier values.","62cf7a57":"# 1. Personal Loan Modeling in Banking\n## 1.1. Introduction\nAI and Big Data are rapidly forcing their way to the top of the priority list for digital transformation in Banking. This is hardly surprising, considering that the collective potential cost savings for banks from AI applications are estimated to reach $447 billion by 2023 (Business Insider).\n\nOne of the main ways Artificial Intelligence and Machine Learning has augmented the BFSI Industry is Customer Insight, and personalized campaigns tailor-made to individuals by leveraging the huge amount of available information in terms of both structured and unstructured data that form a customer's digital footprint.\n\nThe following image illustrates the various ways Big Data Analytics powered by AI brings value to Digital Banking.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/AnirbanDatta87\/Projects\/bbcfbd258762de70aa6923d0d5a944e8656b5397\/P2_MLC\/images\/bank3.png\"\/>\n\n## Personalized Campaigns in Digital Banking\n\nAnalyzing personal and transaction data gives Banks the opportunity to understand customers\u2019 needs today and anticipate future ones. Personalization then adds the ability to deliver those insights to customers in a contextual manner. The most obvious application of these techniques is to increase sales targeting and effectiveness according to a defined business strategy.\n\nA low balance with upcoming bills might call for a personal overdraft offer, a high balance on a current account might suggest appetite for a fixed deposit, recurring visits to the mortgage loan information page might indicate plans to purchase a home, a frequent traveller may be interested in a travel insurance, a fine dining lover might appreciate discounts at a popular restaurant, etc. The opportunities to leverage customer-centric data analytics and personalization for targeted cross-selling or merchants-based campaigns are numerous.\n\nIn this project, we attempt to predict whether a personal loan offer to a liability customer for a Bank is likely to result in success.\n\n## 1.2 Context\n\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\n## 1.3 Domain: Banking\n\n## 1.4 Data Description\nThe file Bank_Personal_Loan_Modelling.csv contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\n\n## 1.5 Attribute Information\n - ID         : Customer ID\n - Age        : Customer's age in completed years\n - Experience : No. of years of professional experience\n - Income     : Annual income of the customer ($ 000)\n \n - ZIP Code   : Home Address Zip Code\n - Family     : Family size of the customer\n - CCAvg      : Avg. Spending on Credit Card per Month ($ 000)\n \n - Education  : Education Level. 1: Undergrad; 2: Graduate; 3: Advanced \/ Professional\n - Mortgage   : Value of house mortgage if any. ($000)\n - Personal Loan : Did this customer accept the personal loan offered in the last campaign?\n - Securities Account : Does the customer have a securities account with the bank?\n - CD Account : Does the customer have a certificate of deposit (CD) account with the bank?\n - Online : Does the customer use internet banking facilities?\n - Credit card : Does the customer use a credit card issued by this Bank?\n \n## 1.6 Objectives\n\n - Exploratory Data Analysis\n - Preparing the data to train a model\n - Training and making predictions using a classification model\n - Model evaluation","e522196d":"### Observation:\n - We see that the Mortgage amount of the customers are skewed heavily to the left, with a lot of outlier values.\n - The nature of distribution of numerical columns suggest that we may need to standardize the data before training our models.","9170a8c0":"# 2. Exploratory Data Analysis\n## 2.1 Loading the Libraries","0a282b00":"### Observation:\n - We see that the work experience of the customers are uniformly distributed between the Minimum of -3 and Maximum of 43.","d8a13ba8":"## 3.1 Handling Missing Values\nAs observed, there are no missing values in our dataset, so we can proceed to the next step.\n## 3.2 Feature Selection","63c97b64":"## 2.3 Data Visualization","c98fa97d":"## 2.2 Data Preparation","b8298362":"# 5. Model Selection and Tuning","485d974e":"### 2.3.1 Univariate Analysis - Numerical Attributes","73f2ba56":"# 4. Training Models","9f46632c":"### Target Variable Distribution","8e6c180b":"### 2.3.3 Multivariate Analysis","b7226c4d":" - We see that a lot of categorical columns are being treated as integer datatypes. We'd like to convert them to categorical for our analysis.\n - First, we'd like to drop 2 columns that we think are not relevant to the prediction: 'ID' and 'ZIP Code'","f02f0ba3":"### Observation:\n - We see there is a very strong positive correlation between Age and Work Experience, which is expected.\n - There is also a positive correlation between Income and Credit Card spending.","6422d399":"### 2.3.2 Univariate Analysis - Categorical Attributes","6b4a91ac":"## 3.3 Transformation Pipeline"}}