{"cell_type":{"a57bf2bb":"code","0838a923":"code","484c6717":"code","9b9c8de4":"code","4f360abc":"code","e74e3c71":"code","3e8bbe81":"code","28a376b7":"code","330539f4":"code","43fa3a18":"code","78385461":"code","a06c0a1c":"code","98b59e72":"code","1ca69125":"code","4d371c3e":"code","0cc42729":"code","cda4a3e2":"code","914dc79e":"code","f1fd3394":"code","9ce2a6c2":"code","b2aee1c2":"code","8bdc9a34":"code","a788eb94":"code","af89d7cc":"code","3d1f7728":"code","1eecfb93":"code","e7045c41":"code","dbe08621":"code","bd5d25de":"code","159d97a7":"code","54dd7151":"code","1bb0d97a":"code","da2f3876":"code","6e56760d":"code","b1adcc8b":"code","99dcadd8":"code","d8d63489":"code","42f38111":"code","9375bdfc":"code","806fa35d":"code","6b6732d8":"code","d6e24556":"code","9a4306ed":"code","f05b5386":"code","c127629a":"code","872c6827":"code","fd9d812e":"code","af7e1033":"code","dc6f1e13":"code","0968b65b":"code","2dd841ab":"code","311c66c6":"code","a64618f5":"code","4fe967bb":"code","d7122c7f":"code","c517ec62":"code","8ffc2b3a":"code","025946e8":"code","c13bfb0f":"code","7cb10b98":"code","5ae99313":"code","bb065066":"code","835c89dc":"code","ccfb7631":"code","4d7e0283":"code","8f2203f6":"code","6607ab11":"code","3f6993a6":"code","bc30a7bb":"code","4bdbc5fb":"code","9da4e1a6":"code","80efa022":"code","cfaaa5b9":"code","68f538c1":"code","8f8dade5":"code","276ca3b0":"code","4b0846a7":"code","0a811208":"code","63ad0d15":"code","329e5f92":"code","27777bc1":"code","51504c5b":"code","2a37c9d3":"code","898b363a":"code","888da79d":"markdown","b46451ad":"markdown","fbe39a08":"markdown","ea58e75a":"markdown","e2ee356c":"markdown","fce15c46":"markdown","a96658f7":"markdown","9966c850":"markdown","bb123209":"markdown","7bda7e23":"markdown","9f0ac70f":"markdown","fb495aac":"markdown","461e3fb3":"markdown","243f5ba6":"markdown","242394a6":"markdown","10c9661d":"markdown","8bee07a0":"markdown","f6375595":"markdown","03b9c098":"markdown","616f23ca":"markdown","860782f6":"markdown","02575acf":"markdown","f9af1ffc":"markdown","54c5520e":"markdown","fa3cd942":"markdown","d6559395":"markdown","65d19437":"markdown","789c5622":"markdown","24708f1d":"markdown","99844c74":"markdown","23b7d5d6":"markdown","bc1288d2":"markdown","a98eb6d9":"markdown","d65fe4d6":"markdown","0b2ee286":"markdown","6011d5a9":"markdown","f571747e":"markdown","96e0132b":"markdown","810af40b":"markdown","8deb1337":"markdown","3692a522":"markdown","b79b162e":"markdown","e0dc6d86":"markdown","5510c8c0":"markdown","8232ca40":"markdown","f467126c":"markdown","3dfa5342":"markdown","da223640":"markdown","c729a118":"markdown","3ea5efa8":"markdown","798656cc":"markdown","705d634d":"markdown","fbb83c4c":"markdown","607691f1":"markdown","ffbc334e":"markdown"},"source":{"a57bf2bb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","0838a923":"sns.__version__","484c6717":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n","9b9c8de4":"df_train.head()","4f360abc":"df_train.info()","e74e3c71":"df_test.head()","3e8bbe81":"df_test.info()","28a376b7":"df_train.describe()","330539f4":"df_test.describe()","43fa3a18":"sns.countplot(x='Survived', data=df_train);","78385461":"print(df_train.Survived.sum()\/df_train.Survived.count())","a06c0a1c":"#df_test['Survived'] = 0\n#df_test[['PassengerId', 'Survived']].to_csv('no_survivors.csv', index=False)","98b59e72":"df_train.groupby(['Survived','Sex'])['Survived'].count()","1ca69125":"sns.catplot(x='Sex', col='Survived', kind='count', data=df_train);","4d371c3e":"print(\"% of women survived: \" , df_train[df_train.Sex == 'female'].Survived.sum()\/df_train[df_train.Sex == 'female'].Survived.count())\nprint(\"% of men survived:   \" , df_train[df_train.Sex == 'male'].Survived.sum()\/df_train[df_train.Sex == 'male'].Survived.count())","0cc42729":"f,ax=plt.subplots(1,2,figsize=(16,7))\ndf_train['Survived'][df_train['Sex']=='male'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[0],shadow=True)\ndf_train['Survived'][df_train['Sex']=='female'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[1],shadow=True)\nax[0].set_title('Survived (male)')\nax[1].set_title('Survived (female)')\n\nplt.show()","cda4a3e2":"#df_test['Survived'] = df_test.Sex == 'female'\n#df_test['Survived'] = df_test.Survived.apply(lambda x: int(x))\n#df_test[['PassengerId', 'Survived']].to_csv('women_survive.csv', index=False)","914dc79e":"pd.crosstab(df_train.Pclass, df_train.Survived, margins=True).style.background_gradient(cmap='autumn_r')","f1fd3394":"print(\"% of survivals in\") \nprint(\"Pclass=1 : \", df_train.Survived[df_train.Pclass == 1].sum()\/df_train[df_train.Pclass == 1].Survived.count())\nprint(\"Pclass=2 : \", df_train.Survived[df_train.Pclass == 2].sum()\/df_train[df_train.Pclass == 2].Survived.count())\nprint(\"Pclass=3 : \", df_train.Survived[df_train.Pclass == 3].sum()\/df_train[df_train.Pclass == 3].Survived.count())","9ce2a6c2":"sns.catplot('Pclass','Survived', kind='point', data=df_train);","b2aee1c2":"pd.crosstab([df_train.Sex, df_train.Survived], df_train.Pclass, margins=True).style.background_gradient(cmap='autumn_r')","8bdc9a34":"sns.catplot('Pclass','Survived',hue='Sex', kind='point', data=df_train);","a788eb94":"sns.catplot(x='Survived', col='Embarked', kind='count', data=df_train);","af89d7cc":"sns.catplot('Embarked','Survived', kind='point', data=df_train);","3d1f7728":"sns.catplot('Embarked','Survived', hue= 'Sex', kind='point', data=df_train);","1eecfb93":"sns.catplot('Embarked','Survived', col='Pclass', hue= 'Sex', kind='point', data=df_train);","e7045c41":"pd.crosstab([df_train.Survived], [df_train.Sex, df_train.Pclass, df_train.Embarked], margins=True)","dbe08621":"# model 3\ndf_test['Survived'] = 0\n# all women survived\ndf_test.loc[ (df_test.Sex == 'female'), 'Survived'] = 1\n# except for those in Pclass 3 and embarked in S\ndf_test.loc[ (df_test.Sex == 'female') & (df_test.Pclass == 3) & (df_test.Embarked == 'S') , 'Survived'] = 0\n#df_test[['PassengerId', 'Survived']].to_csv('embarked_pclass_sex.csv', index=False)","bd5d25de":"for df in [df_train, df_test]:\n    df['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        df.loc[ df['Age'] <= i*10, 'Age_bin'] = i","159d97a7":"print(df_train[['Age' , 'Age_bin']].head(10))","54dd7151":"sns.catplot('Age_bin','Survived',hue='Sex',kind='point',data=df_train);","1bb0d97a":"sns.catplot('Age_bin','Survived', col='Pclass' , row = 'Sex', kind='point', data=df_train);","da2f3876":"pd.crosstab([df_train.Sex, df_train.Survived], [df_train.Age_bin, df_train.Pclass], margins=True).style.background_gradient(cmap='autumn_r')","6e56760d":"# in Pclass 1 and 2 all men in Age_bin = 1 survived\ndf_test.loc[ (df_test.Sex == 'male') & (df_test.Pclass == 1) & (df_test.Age_bin == 1), 'Survived'] = 1\ndf_test.loc[ (df_test.Sex == 'male') & (df_test.Pclass == 2) & (df_test.Age_bin == 1), 'Survived'] = 1","b1adcc8b":"sns.catplot('SibSp','Survived', col='Pclass' , row = 'Sex', kind='point', data=df_train);","99dcadd8":"pd.crosstab([df_train.Sex, df_train.Survived], [df_train.SibSp, df_train.Pclass], margins=True).style.background_gradient(cmap='autumn_r')","d8d63489":"# all females with SibSp > 7 died\ndf_test.loc[ (df_test.Sex == 'female') & (df_test.SibSp > 7) , 'Survived'] = 0","42f38111":"sns.catplot('Parch','Survived', col='Pclass' , row = 'Sex', kind='point', data=df_train);","9375bdfc":"pd.crosstab([df_train.Sex, df_train.Survived], [df_train.Parch, df_train.Pclass], margins=True).style.background_gradient(cmap='autumn_r')","806fa35d":"# survival rate is below 0.5 for females with Parch = 2 and Pclass = 3 \n#df_test.loc[ (df_test.Sex == 'female') & (df_test.Pclass == 3) & (df_test.Parch == 2), 'Survived'] = 0\n\n# All females with Parch = 4 and Pclass = 3 died\n##df_test.loc[ (df_test.Sex == 'female') & (df_test.Pclass == 3) & (df_test.Parch == 4), 'Survived'] = 0\n\n# all females with Parch > 4 died\n#df_test.loc[ (df_test.Sex == 'female') & (df_test.Parch > 4) , 'Survived'] = 0\n\n# For males with Parch = 2 and Pclass = 1 survival rate is above 0.5\n##df_test.loc[ (df_test.Sex == 'male') & (df_test.Pclass == 1) & (df_test.Parch == 1) , 'Survived'] = 1\n\n#df_test.head(20)","6b6732d8":"sns.distplot(df_train['Fare']);","d6e24556":"for df in [df_train, df_test]:\n    df['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        df.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i","9a4306ed":"sns.catplot('Fare_bin','Survived', col='Pclass' , row = 'Sex', kind='point', data=df_train);","f05b5386":"pd.crosstab([df_train.Sex, df_train.Survived], [df_train.Fare_bin, df_train.Pclass], margins=True).style.background_gradient(cmap='autumn_r')","c127629a":"# males in Fare_bin = 11 survived\ndf_test.loc[ (df_test.Sex == 'male') & (df_test.Fare_bin == 11), 'Survived'] = 1","872c6827":"# model 4\n# df_test[['PassengerId', 'Survived']].to_csv('model_4.csv', index=False)","fd9d812e":"df_test.drop(['Survived'],axis=1,inplace=True)","af7e1033":"df_train_ml = df_train.copy()\ndf_test_ml = df_test.copy()","dc6f1e13":"df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\ndf_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\ndf_train_ml.dropna(inplace=True)","0968b65b":"passenger_id = df_test_ml['PassengerId']\ndf_test_ml = pd.get_dummies(df_test_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\ndf_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)","2dd841ab":"df_train_ml.head(10)","311c66c6":"df_train_ml.info()","a64618f5":"df_test_ml.info()","4fe967bb":"df_test_ml.head(10)","d7122c7f":"corr = df_train_ml.corr()\n\nf,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(corr, annot = True, linewidths=1.5 , fmt = '.2f',ax=ax)\nplt.show()","c517ec62":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# for df_train_ml\nscaler.fit(df_train_ml.drop('Survived',axis=1))\nscaled_features = scaler.transform(df_train_ml.drop('Survived',axis=1))\ndf_train_ml_sc = pd.DataFrame(scaled_features, columns=df_train_ml.columns[:-1])\n\n# for df_test_ml\ndf_test_ml.fillna(df_test_ml.mean(), inplace=True)\n# scaler.fit(df_test_ml)\nscaled_features = scaler.transform(df_test_ml)\ndf_test_ml_sc = pd.DataFrame(scaled_features, columns=df_test_ml.columns)","8ffc2b3a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_train_ml.drop('Survived',axis=1), df_train_ml['Survived'], test_size=0.30, random_state=101)\nX_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(df_train_ml_sc, df_train_ml['Survived'], test_size=0.30, random_state=101)","025946e8":"# unscaled\nX_train_all = df_train_ml.drop('Survived',axis=1)\ny_train_all = df_train_ml['Survived']\nX_test_all = df_test_ml\n\n# scaled\nX_train_all_sc = df_train_ml_sc\ny_train_all_sc = df_train_ml['Survived']\nX_test_all_sc = df_test_ml_sc","c13bfb0f":"X_test_all.fillna(X_test_all.mean(), inplace=True)\nprint(\"*\")","7cb10b98":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix","5ae99313":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\npred_logreg = logreg.predict(X_test)\nprint(confusion_matrix(y_test, pred_logreg))\nprint(classification_report(y_test, pred_logreg))\nprint(accuracy_score(y_test, pred_logreg))","bb065066":"logreg.fit(X_train_all, y_train_all)\npred_all_logreg = logreg.predict(X_test_all)","835c89dc":"sub_logreg = pd.DataFrame()\nsub_logreg['PassengerId'] = df_test['PassengerId']\nsub_logreg['Survived'] = pred_all_logreg\n#sub_logmodel.to_csv('logmodel.csv',index=False)","ccfb7631":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(X_train,y_train)\npred_gnb = gnb.predict(X_test)\nprint(confusion_matrix(y_test, pred_gnb))\nprint(classification_report(y_test, pred_gnb))\nprint(accuracy_score(y_test, pred_gnb))","4d7e0283":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(X_train_sc,y_train_sc)","8f2203f6":"pred_knn = knn.predict(X_test)\nprint(confusion_matrix(y_test, pred_knn))\nprint(classification_report(y_test, pred_knn))\nprint(accuracy_score(y_test, pred_knn))","6607ab11":"knn.fit(X_train_all, y_train_all)\npred_all_knn = knn.predict(X_test_all)","3f6993a6":"sub_knn = pd.DataFrame()\nsub_knn['PassengerId'] = df_test['PassengerId']\nsub_knn['Survived'] = pred_all_knn\n#sub_knn.to_csv('knn.csv',index=False)","bc30a7bb":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","4bdbc5fb":"pred_dtree = dtree.predict(X_test)\nprint(classification_report(y_test,pred_dtree))\nprint(accuracy_score(y_test, pred_dtree))","9da4e1a6":"dtree_2 = DecisionTreeClassifier(max_features=7 , max_depth=6,  min_samples_split=8)\ndtree_2.fit(X_train,y_train)\npred_dtree_2 = dtree_2.predict(X_test)\nprint(classification_report(y_test, pred_dtree_2))\nprint(accuracy_score(y_test, pred_dtree_2))","80efa022":"dtree_2.fit(X_train_all, y_train_all)\npred_all_dtree2 = dtree_2.predict(X_test_all)","cfaaa5b9":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(max_depth=6, max_features=7)\nrfc.fit(X_train, y_train)","68f538c1":"pred_rfc = rfc.predict(X_test)\nprint(confusion_matrix(y_test, pred_rfc))\nprint(classification_report(y_test, pred_rfc))\nprint(accuracy_score(y_test, pred_rfc))","8f8dade5":"rfc.fit(X_train_all, y_train_all)\npred_all_rfc = rfc.predict(X_test_all)","276ca3b0":"sub_rfc = pd.DataFrame()\nsub_rfc['PassengerId'] = df_test['PassengerId']\nsub_rfc['Survived'] = pred_all_rfc\n#sub_rfc.to_csv('randforest.csv',index=False)","4b0846a7":"from sklearn.svm import SVC\nsvc = SVC(gamma = 0.01, C = 100)#, probability=True)\nsvc.fit(X_train_sc, y_train_sc)","0a811208":"pred_svc = svc.predict(X_test_sc)\nprint(confusion_matrix(y_test_sc, pred_svc))\nprint(classification_report(y_test_sc, pred_svc))\nprint(accuracy_score(y_test_sc, pred_svc))","63ad0d15":"svc.fit(X_train_all_sc, y_train_all_sc)\npred_all_svc = svc.predict(X_test_all_sc)\n\nsub_svc = pd.DataFrame()\nsub_svc['PassengerId'] = df_test['PassengerId']\nsub_svc['Survived'] = pred_all_svc\nsub_svc.to_csv('svc.csv',index=False)","329e5f92":"from sklearn.model_selection import cross_val_score","27777bc1":"scores_svc = cross_val_score(svc, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_svc)\nprint(scores_svc.mean())","51504c5b":"scores_rfc = cross_val_score(rfc, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())","2a37c9d3":"scores_dtree_2 = cross_val_score(dtree_2, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_dtree_2)\nprint(scores_dtree_2.mean())","898b363a":"print(\"dtree_2 : \" , scores_dtree_2.mean())\nprint(\"rfc     : \" , scores_rfc.mean())\nprint(\"svc     : \" , scores_svc.mean())","888da79d":"for Random Forest classifier","b46451ad":"**The Classifiers with best performance are Decision Tree, Random Forest and SVC**\n","fbe39a08":"Also in df_test some values for Age and many values for Cabin are missing","ea58e75a":"**Age:  continuous numerical  to  8 bins **","e2ee356c":"for DecisionTreeClassifier","fce15c46":"**Embarked, Pclass and Sex :**\n\n** Practically all women of Pclass 2 that embarked in C and Q survived, also nearly all women of Pclass 1 survived. **\n\n** All men of Pclass 1 and 2 embarked in Q died, survival rate for men in Pclass 2 and 3 is always below 0.2 **\n\n** For the remaining men in Pclass 1 that embarked in S and Q, survival rate is approx. 0.4 **","a96658f7":"**Note on scores**\n\nSome kernels for the Titanic competition calculate scores based on the training set. This is not a good indicator for the model performance, because we want to know how well the model generalizes for data that was not used for fitting the model. Therefore, scores in this and in my other kernels are always for out of sample test or validation data.","9966c850":"**Train again for all data and submit**","bb123209":"**This is my first Kaggle for the Titanic competition.**\n\nThe notebooks explores the basic use of Pandas and scikit-learn for this Classifcation problem.  \nFor more advanced approaches like using Seaborn plots, Feature Engineering, GridSearch CV  \nand ML models based on stacking and voting have a look at [my second Titanic kernel](https:\/\/www.kaggle.com\/dejavu23\/titanic-survival-my-2nd-titanic-kernel)\n\n\n\n**My goals for this notebook:**\n\n* **[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)**  \nunderstand the data by EDA and derive simple models with Pandas as baseline\n\n\n* **[Part 2: Data wrangling](#Part-2:-Data-wrangling)**  \nfill nan, convert categorical to numerical,  \ncreate train and test data for ML algorithms\n\n* **[Part 3: Scikit-learn basic ML algorithms](#Part-3:-Scikit-learn-basic-ML-algorithms-and-comparison-of-model-results)**  \nimplement different Classifiers from the sklearn library:  \n[Logistic regression](#3.1-Logistic-Regression), [Gaussian naive Bayes](#3.2-Gaussian-Naive-Bayes), [KNN](#3.3-KNN---KNeighborsClassifier), [Decision tree](#3.4-Decision-Tree-Classifier), [Random forest](#3.5-Random-Forest-Classifier), [SVM](#3.6-SVM-Classifier)\n\n\n* **[Part 3: Comparison of Model  results](#Part-3:-Scikit-learn-basic-ML-algorithms-and-comparison-of-model-results)**  \nuse metrics like confusion_matrix, classification_report, accuracy_score  \nand implement k fold cross validation for comparison of test score\n \n\n**References**  \n**This notebook has some own approaches but is also based on these tutorials, notebooks and courses:**\n* **[Datacamp: Kaggle Tutorial: EDA & Machine Learning](https:\/\/www.datacamp.com\/community\/tutorials\/kaggle-machine-learning-eda)**\n* **[Udemy: Python for Data Science and Machine Learning Bootcamp](https:\/\/www.udemy.com\/python-for-data-science-and-machine-learning-bootcamp\/)**\n* **[Data School: Machine learning in Python with scikit-learn](https:\/\/www.youtube.com\/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)**\n\n    \n","7bda7e23":"## Part 3: Scikit-learn basic ML algorithms and comparison of model results\n\n**Test simple sklearn models and compare by metrics**\n\n\n**We test the following classifiers from scikit-learn:**  \n3.1 [Logistic Regression](#3.1-Logistic-Regression)  \n3.2 [Gaussian Naive Bayes](#3.2-Gaussian-Naive-Bayes)  \n3.3 [K nearest neighbors KNN](#3.3-KNN---KNeighborsClassifier)  \n3.4 [Decision tree classifier](#3.4-Decision-Tree-Classifier)  \n3.5 [Random forest classifier](#3.5-Random-Forest-Classifier)  \n3.6 [SVM classifier](#3.6-SVM-Classifier)  \n\n\n**First we apply the data from test\/train split to get a first overview of the model performance.  \nLater we use the k fold cross validation which gives a better estimate for out of sample data.**  \n\n\n**For comparison of the results we use these metrics:**  \naccuracy_score, classification_report, confusion_matrix\n","9f0ac70f":"\n## Part 2: Data wrangling\n\nbuilding two new dataframes df_train_ml and df_test_ml  \nthese will have only ordinal features and no missing values so they can be used for ML algorithms  \nconverting categorical to numerical by pd.get_dummies  \ndropping all features that seem to be not useful for prediction  \nThen use the Standard scaler and apply train\/test split","fb495aac":"**k fold cross_validation**\n\nThis algorithm splits the data into k sets and then makes k fits using every set k-1 times as training and one time as test data  \nIt leads to a better estimate for out of sample data  than simple train test split","461e3fb3":"**accuracy score**  \nclassification accuracy = correct predictions \/ total predictions   = (TP + TN)  \/  (TP + TN + FP + FN) ","243f5ba6":"**confusion matrix** : used to evaluate the quality of the output of a classifier.  \nThe diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.  \n*(from sklearn documentation, slightly modified)*\n\nThe rows of a confusion matrix correspond to the true (actual) classes and the columns correspond to the predicted classes.  \nSo, all together the confusion matrix for a **binary classifier** consists of 4 values:\n\nTN FP  \nFN TP  \n      \nTN: True negatives (prediction: not survived, true: not survived)  \nFP: False positives (prediction: survived, true: not survived)  \nFN: False negatives (prediction: not survived, true: survived)  \nTP: True positives (prediction: survived, true: survived)\n      \n      ","242394a6":"**Imports**","10c9661d":"For passengers in Age_bin = 1 (younger than 10) : All male in Pclass 1 and 2 survived\n\nAll female in Pclass 3 and Age_bin = 5 died. \n\n(Survival rate for female in Pclass 3 and Age_bin = 4 is below 50%)\n\n(Survival rate for male in Pclass 1 and Age_bin = 4 is above 50%)\n\n","8bee07a0":"fourth model : model 3 + Age_bin, SibSp, Parch and Fare_bin  \nsubmission : 0.789 accuracy","f6375595":"**Embarked and Sex**","03b9c098":"Very similar to SibSp - 1 , but different values  ?\nFor females with Parch = 2 and Pclass = 3 survival rate is below 0.5  \nAll females with Parch = 4 and Pclass = 3 died.\nAll females with Parch > 4 died.\n(For females with Parch = 1 and Pclass = 3 survival rate is below 0.5)\nFor males,all survival rates below 0.5 for any values of Parch, except for Parch = 2 and Pclass = 1.","616f23ca":"### **3.6 SVM Classifier**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/svm.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine) ++   [towardsdatascience](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47) ++ [datacamp](https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python) ++ [medium](https:\/\/medium.com\/machine-learning-101\/chapter-2-svm-support-vector-machine-theory-f0812effc72) ++ [youtube](https:\/\/www.youtube.com\/watch?v=N1vOgolbjSc) ++ [jakevdp](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.07-support-vector-machines.html)","860782f6":"**Passenger Class and Sex :**\n\n**Almost all women in Pclass 1 and 2 survived and nearly all men in Pclass 2 and 3 died**","02575acf":"## Some Background Information\n\n\n**The sinking of the RMS Titanic in the early morning of 15 April 1912, four days into the ship's maiden voyage from Southampton to New York City, was one of the deadliest peacetime maritime disasters in history, killing more than 1,500 people. The largest passenger liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg in the North Atlantic. The ship had received six warnings of sea ice but was travelling at near maximum speed when the lookouts sighted the iceberg. Unable to turn quickly enough, the ship suffered a glancing blow that buckled the starboard (right) side and opened five of sixteen compartments to the sea. The disaster caused widespread outrage over the lack of lifeboats, lax regulations, and the unequal treatment of the three passenger classes during the evacuation. Inquiries recommended sweeping changes to maritime regulations, leading to the International Convention for the Safety of Life at Sea (1914), which continues to govern maritime safety.**  \n*from Wikipedia*","f9af1ffc":"**Uncomment  if you want to check this submission**","54c5520e":"**Fare:  continuous numerical  to  12 bins **","fa3cd942":"**all data for submission**","d6559395":"Survived and Fare positively correlated, Survived and Sex_male negatively correlated.  \nAlso, Survived and Pclass_3 negatively correlated. SibSp and Parch correlated","65d19437":"**sklearn StandardScaler**","789c5622":"For males, no survival rate above 0.5 for any values of SibSp.\nFor females, passengers with SibSp = 3 and Pclass = 3 died, also all females with SibSp > 4 died.\nFor females with SibSp = 1 and Pclass = 3 survival rate is below 0.5","24708f1d":"for SVM classifier","99844c74":"Women were more likely to survive than men \n\n74 % of women survived\nbut only 19% of men\n(in training set)\n\n-> second model :\nall women survived and all men died\n\nsubmission : 0.766 accuracy\n","23b7d5d6":"**Embarked : Survival rate lowest for S and highest for C**","bc1288d2":"more people died than survived (38% survived)\n\n-> base model : no survivors\n\nsubmission : 0.627 accuracy","a98eb6d9":"**Uncomment  if you want to check this submission**","d65fe4d6":"## Part 1: Exploratory Data Analysis","0b2ee286":"**classification_report**  \n*from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_recall_fscore_support.html* :  \n\nThe precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_true.\n\nprecision\nrecall\nf1-score\nsupport","6011d5a9":"**Correlation Matrix**","f571747e":"**Passenger Class : Survival rate decreases with Pclass**","96e0132b":"### **3.1 Logistic Regression**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression) ++ [Coursera Andrew Ng](https:\/\/www.coursera.org\/lecture\/machine-learning\/classification-wlPeP)  ++ [towardsdatascience](https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc) ++ [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/understanding-logistic-regression-python) ++ [hackernoon](https:\/\/hackernoon.com\/introduction-to-machine-learning-algorithms-logistic-regression-cbdd82d81a36) ++ [dataaspirant](http:\/\/dataaspirant.com\/2017\/03\/02\/how-logistic-regression-model-works\/)","810af40b":"**SibSp and Parch**","8deb1337":"**pandas get_dummies for categorical features**","3692a522":"**Conclusion**  \nWith this notebook we learned the basics of EDA with Pandas and Matplotlib as well as the foundations  \nfor applying the classification models of the scikit learn library.  \nBy EDA we found a strong impact of features like Sex, Age, Embarked on the target.  \nWe then built a simple baseline model with Pandas, using only these features.  \nAgain using Pandas, we also created a dataset that can be used by the sklearn Classifiers for prediction.  \nDeciding by k fold cross validation score, the best ML models for this task and set of features were:  \nDecision Tree, Random Forest and SVC  \nSubmitting their predictions gives a score of 0,78 and a place in the top 30% of the Leaderboard.  \n\nIn [my second Titanic kernel](https:\/\/www.kaggle.com\/dejavu23\/titanic-survival-my-2nd-kernel) I study how to improve this score by \n* using features built from the existing ones (Feature Engineering)\n* optimising the model hyper-parameters with GridSearchCV\n* applying techniques like boosting, stacking and voting","b79b162e":"**Of all passengers in df_train, how many survived, how many died ?** ","e0dc6d86":"df_train has 891 entries, some values for Cabin and Age are missing","5510c8c0":"**fillna: fill nan with mean values for that column** ","8232ca40":"Comparing distribution of features in df_train and df_test, Pclass and Age seem very similar, distributions for SibSo, Parch and Fare only slightly different","f467126c":"**train_test_split**  \nuse 70% of the data for training and 30% for testing","3dfa5342":"### **3.3 KNN - KNeighborsClassifier**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) ++   [Medium](https:\/\/medium.com\/@adi.bronshtein\/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7) ++ [towardsdatascience](https:\/\/towardsdatascience.com\/introduction-to-k-nearest-neighbors-3b534bb11d26) ++ [datacamp](https:\/\/www.datacamp.com\/community\/tutorials\/k-nearest-neighbor-classification-scikit-learn) ++ [analyticsvidhya](https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/) ++ [dataaspirant](http:\/\/dataaspirant.com\/2016\/12\/27\/k-nearest-neighbor-algorithm-implementaion-python-scratch\/)\n\n\n","da223640":"all data","c729a118":"### **3.5 Random Forest Classifier**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest) ++   [towardsdatascience](https:\/\/towardsdatascience.com\/the-random-forest-algorithm-d457d499ffcd) ++ [towardsdatascience](https:\/\/towardsdatascience.com\/random-forest-in-python-24d0893d51c0) ++ [datacamp](https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python) ++ [youtube](https:\/\/www.youtube.com\/watch?v=eM4uJ6XGnSM) ++ [jakevdp](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.08-random-forests.html)","3ea5efa8":"### **3.2 Gaussian Naive Bayes**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier#Gaussian_naive_Bayes) ++   [towardsdatascience](https:\/\/towardsdatascience.com\/naive-bayes-classifier-81d512f50a7c) ++ [towardsdatascience](https:\/\/towardsdatascience.com\/naive-bayes-in-machine-learning-f49cc8f831b4) ++ [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/naive-bayes-scikit-learn) ++ [dataaspirant](http:\/\/dataaspirant.com\/2017\/02\/06\/naive-bayes-classifier-machine-learning\/) \n\n","798656cc":"third model :\nbased on PClass, Sex and Embarked ,\nsubmission : 0.779 accuracy","705d634d":"**Sex: Female more likely to survive than male**","fbb83c4c":"**Train again for all data and submit**","607691f1":"### **3.4 Decision Tree Classifier**  \n[sklearn](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) ++ [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning) ++   [Medium](https:\/\/towardsdatascience.com\/decision-trees-in-machine-learning-641b9c4e8052) ++ [Medium](https:\/\/medium.com\/deep-math-machine-learning-ai\/chapter-4-decision-trees-algorithms-b93975f7a1f1) ++ [datacamp](https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python) ++ [hackernoon](https:\/\/hackernoon.com\/what-is-a-decision-tree-in-machine-learning-15ce51dc445d) ++ [hackerearth](https:\/\/www.hackerearth.com\/practice\/machine-learning\/machine-learning-algorithms\/ml-decision-tree\/tutorial\/)","ffbc334e":"another decision tree with different parameters for max_features, max_depth and min_sample_split"}}