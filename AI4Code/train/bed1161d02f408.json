{"cell_type":{"4cf10dbc":"code","86c99497":"code","a7c6accc":"code","9faccb56":"code","6c5b1595":"code","5ac82058":"code","5c897ad8":"code","ef591241":"code","dcd5ba63":"code","84e7e589":"code","5400c255":"code","922fc7df":"code","08dcdd26":"code","b59ec5e2":"code","ac54e581":"code","5c45ea8a":"code","064d427e":"code","4ede9ba2":"code","ae317b79":"markdown","e8405645":"markdown","32fde3dd":"markdown","2616bd2c":"markdown","a8efab7a":"markdown","9c9ad2d9":"markdown","7ad8b07f":"markdown","8fa81aa7":"markdown","d13dc380":"markdown","3afe3616":"markdown","85358100":"markdown","dd525ca5":"markdown","0987650a":"markdown","0eead676":"markdown","2065312a":"markdown","c912af89":"markdown"},"source":{"4cf10dbc":"import numpy as np","86c99497":"x = np.array([(62,397,103),(82,347,107),(93,288,120),\n     (94,266,128),(65,163,169),(12,102,198),\n     (48,138,180),(77,187,157),(85,209,149),(89,316,112)])","a7c6accc":"xm = np.mean(x,axis=0)\nprint(xm.shape)","9faccb56":"x = x - xm","6c5b1595":"print(x.shape)","5ac82058":"v, s, t = np.linalg.svd(x,full_matrices=True)","5c897ad8":"sigma1 = s[0]\nsigma2 = s[1]\nsigma3 = s[2]\nv1 = t[0]\nv2 = t[1]\nv3 = t[2]","ef591241":"Z = np.zeros((x.shape[0],10), np.float32)\nZ[:,0] = x[:,0]**2\nZ[:,1] = 2*x[:,0]*x[:,1]\nZ[:,2] = 2*x[:,0]*x[:,2]\nZ[:,3] = 2*x[:,0]\nZ[:,4] = x[:,1]**2\nZ[:,5] = 2*x[:,1]*x[:,2]\nZ[:,6] = 2*x[:,1]\nZ[:,7] = x[:,2]**2\nZ[:,8] = 2*x[:,2]\nZ[:,9] = 1","dcd5ba63":"v, s, t = np.linalg.svd(Z,full_matrices=True)\nsmallest_value = np.min(np.array(s))\nsmallest_index = np.argmin(np.array(s))\nT = np.array(t)\nT = T[smallest_index,:]\nS = np.zeros((4,4),np.float32)\nS[0,0] = T[0]\nS[0,1] = S[1,0] = T[1]\nS[0,2] = S[2,0] = T[2]\nS[0,3] = S[3,0] = T[3]\nS[1,1] = T[4]\nS[1,2] = S[2,1] = T[5]\nS[1,3] = S[3,1] = T[6]\nS[2,2] = T[7]\nS[2,3] = S[3,2] = T[8]\nS[3,3] = T[9]\nnorm = np.linalg.norm(np.dot(Z,T), ord=2)**2\nprint(norm)","84e7e589":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom trackml.dataset import load_event, load_dataset\nfrom trackml.score import score_event","5400c255":"# Change this according to your directory preferred setting\npath_to_train = \"..\/input\/train_1\"","922fc7df":"# This event is in Train_1\nevent_prefix = \"event000001000\"","08dcdd26":"hits, cells, particles, truth = load_event(os.path.join(path_to_train, event_prefix))","b59ec5e2":"from sklearn.preprocessing import StandardScaler\nimport hdbscan\nfrom scipy import stats\nfrom tqdm import tqdm\nfrom sklearn.cluster import DBSCAN\n\nclass Clusterer(object):\n    def __init__(self,rz_scales=[0.65, 0.965, 1.528]):                        \n        self.rz_scales=rz_scales\n    \n    def _eliminate_outliers(self,labels,M):\n        norms=np.zeros((len(labels)),np.float32)\n        indices=np.zeros((len(labels)),np.float32)\n        for i, cluster in tqdm(enumerate(labels),total=len(labels)):\n            if cluster == 0:\n                continue\n            index = np.argwhere(self.clusters==cluster)\n            index = np.reshape(index,(index.shape[0]))\n            indices[i] = len(index)\n            x = M[index]\n            norms[i] = self._test_quadric(x)\n        threshold1 = np.percentile(norms,90)*5\n        threshold2 = 25\n        threshold3 = 6\n        for i, cluster in enumerate(labels):\n            if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n                self.clusters[self.clusters==cluster]=0   \n    def _test_quadric(self,x):\n        if x.size == 0 or len(x.shape)<2:\n            return 0\n        xm = np.mean(x,axis=0)\n        x = x - xm\n        Z = np.zeros((x.shape[0],10), np.float32)\n        Z[:,0] = x[:,0]**2\n        Z[:,1] = 2*x[:,0]*x[:,1]\n        Z[:,2] = 2*x[:,0]*x[:,2]\n        Z[:,3] = 2*x[:,0]\n        Z[:,4] = x[:,1]**2\n        Z[:,5] = 2*x[:,1]*x[:,2]\n        Z[:,6] = 2*x[:,1]\n        Z[:,7] = x[:,2]**2\n        Z[:,8] = 2*x[:,2]\n        Z[:,9] = 1\n        v, s, t = np.linalg.svd(Z,full_matrices=False)        \n        smallest_index = np.argmin(np.array(s))\n        T = np.array(t)\n        T = T[smallest_index,:]        \n        norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n        return norm\n\n    def _preprocess(self, hits):\n        \n        x = hits.x.values\n        y = hits.y.values\n        z = hits.z.values\n\n        r = np.sqrt(x**2 + y**2 + z**2)\n        hits['x2'] = x\/r\n        hits['y2'] = y\/r\n\n        r = np.sqrt(x**2 + y**2)\n        hits['z2'] = z\/r\n\n        ss = StandardScaler()\n        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n        for i, rz_scale in enumerate(self.rz_scales):\n            X[:,i] = X[:,i] * rz_scale\n       \n        return X\n    def _init(self, dfh, w1, w2, w3, w4, w5, w6, w7, epsilon, Niter):\n        dfh['r'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2 + dfh['z'].values ** 2)\n        dfh['rt'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2)\n        dfh['a0'] = np.arctan2(dfh['y'].values, dfh['x'].values)\n        dfh['z1'] = dfh['z'].values \/ dfh['rt'].values\n        dfh['z2'] = dfh['z'].values \/ dfh['r'].values\n        dfh['s1'] = dfh['hit_id']\n        dfh['N1'] = 1\n        dfh['z1'] = dfh['z'].values \/ dfh['rt'].values\n        dfh['z2'] = dfh['z'].values \/ dfh['r'].values\n        dfh['x1'] = dfh['x'].values \/ dfh['y'].values\n        dfh['x2'] = dfh['x'].values \/ dfh['r'].values\n        dfh['x3'] = dfh['y'].values \/ dfh['r'].values\n        dfh['x4'] = dfh['rt'].values \/ dfh['r'].values\n        mm = 1\n        for ii in tqdm(range(int(Niter))):\n            mm = mm * (-1)\n            dfh['a1'] = dfh['a0'].values + mm * (dfh['rt'].values + 0.000005\n                                                 * dfh['rt'].values ** 2) \/ 1000 * (ii \/ 2) \/ 180 * np.pi\n            dfh['sina1'] = np.sin(dfh['a1'].values)\n            dfh['cosa1'] = np.cos(dfh['a1'].values)\n            ss = StandardScaler()\n            dfs = ss.fit_transform(dfh[['sina1', 'cosa1', 'z1', 'z2','x1','x2','x3','x4']].values)\n            cx = np.array([w1, w1, w2, w3, w4, w5, w6, w7])\n            dfs = np.multiply(dfs, cx)\n            clusters = DBSCAN(eps=epsilon, min_samples=1, metric=\"euclidean\", n_jobs=32).fit(dfs).labels_\n            dfh['s2'] = clusters\n            dfh['N2'] = dfh.groupby('s2')['s2'].transform('count')\n            maxs1 = dfh['s1'].max()\n            dfh.loc[(dfh['N2'] > dfh['N1']) & (dfh['N2'] < 20),'s1'] = dfh['s2'] + maxs1\n            dfh['N1'] = dfh.groupby('s1')['s1'].transform('count')\n        return dfh['s1'].values\n    def predict(self, hits):         \n        self.clusters = self._init(hits,2.7474448671796874,1.3649721713529086,0.7034918842926337,\n                                        0.0005549122352940002,0.023096034747190672,0.04619756315527515,\n                                        0.2437077420144654,0.009750302717746615,338)\n        X = self._preprocess(hits) \n        cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n                             metric='braycurtis',cluster_selection_method='leaf',algorithm='best', leaf_size=50)\n        labels = np.unique(self.clusters)\n        self._eliminate_outliers(labels,X)          \n        max_len = np.max(self.clusters)\n        mask = self.clusters == 0\n        self.clusters[mask] = cl.fit_predict(X[mask])+max_len\n        return self.clusters","ac54e581":"model = Clusterer()\nlabels = model.predict(hits)","5c45ea8a":"def create_one_event_submission(event_id, hits, labels):\n    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n    return submission\n","064d427e":"submission = create_one_event_submission(0, hits, labels)\nscore = score_event(truth, submission)\nprint(\"Your score: \", score)","4ede9ba2":"path_to_test = \"..\/input\/test\"\ntest_dataset_submissions = []\n\ncreate_submission = False # True for submission \nif create_submission:\n    for event_id, hits, cells in load_dataset(path_to_test, parts=['hits', 'cells']):\n\n        # Track pattern recognition \n        model = Clusterer()\n        labels = model.predict(hits)\n\n        # Prepare submission for an event\n        one_submission = create_one_event_submission(event_id, hits, labels)\n        test_dataset_submissions.append(one_submission)\n        \n        print('Event ID: ', event_id)\n\n    # Create submission file\n    submission = pd.concat(test_dataset_submissions, axis=0)\n    submission.to_csv('submission.csv', index=False)","ae317b79":"This algorithm will determine the matrix or matrices S minimizing the total least-squares objective\n$$G(S):=\\sum_{j=1}^{p}[F(S;\\vec{x_j})]^2$$\nsubject to the constraint that\n$$\\sum_{k=1}^{4}\\sum_{\\ell=k}^4|{S_{k,\\ell}}|^2=1$$\nComputationally, arrange matrix S in one-dimensional vector in lexicographic order:\n$$S=\\vec{S}:=(s_{11}, s_{12}, s_{13}, s_{14}; s_{22}, s_{23}, s_{24}; s_{33}, s_{34}; s_{44})$$\nSimilarly, for each point x, form the vector z:\n$$\\vec{z}:=(x_{1}^2,2x_{1}x_{2},2x_{1}x_{3},2x_{1};x_{2}^2,2x_{2}x_{3},2x_{2};x_{3}^2,2x_{3}; 1)$$","e8405645":"Each affine quadric surface satisfies the equation $$F(S;\\vec{x})=0$$ defined by a quadratic\nform $$F(S;\\vec{x})=(\\bar{x}^T,1).S.(\\bar{x}^T,1)^T=\\begin{bmatrix}x_1 & x_2 & x_3 & 1\\end{bmatrix}\\begin{bmatrix}\n    s_{11} & s_{12} & s_{13} & s_{1} \\\\\n    s_{12} & s_{22} & s_{23} & s_{2} \\\\\n    s_{13} & s_{23} & s_{33} & s_{3} \\\\\n    s_{1} & s_{2} & s_{3} & s_{44}\n\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ 1\\end{bmatrix}$$","32fde3dd":"##### Similarly, if\n$$\\sigma_1=\\sigma_2=\\sigma_3=0$$\nall the data coalesce at one point","2616bd2c":"#### 2.1.2 Compute the smallest singular value and the corresponding right-singular vectors of the matrix Z","a8efab7a":"#### 1.3 Compute the singular values of X\n$$\\sigma_1\\geqslant\\sigma_2\\geqslant\\sigma_3\\geqslant0$$\nand the corresponding orthonormal vectors\n$$\\vec{v_1},\\vec{v_2},\\vec{v_3}\\in\\mathbb{R}^3$$","9c9ad2d9":"### Data points\nThe data considered here consists of p points $$\\vec{x_1}, ...,\\vec{x_p}$$","7ad8b07f":"##### The norm value near zero shows that x points above are fitted in a quadric surface of a cylinder or helix.","8fa81aa7":"#### 1.2 Form the matrix\n$$X\\in\\mathbb{M}_{p\\times{3}}$$","d13dc380":"#### 1.1 Compute the average\n$$\\bar{\\vec{x}}:= (1\/p)\\sum_{j=1}^{p}\\vec{x_j}$$","3afe3616":"### 1. Estimating the degree of colinearity of data","85358100":"This python notebook inspired in **Grzegorz Sionkowsk** algorithm to initialize the clusters with DBSCAN.\nAfter the initialization there is a post processing of clusters using  some ideas of the paper **Fitting helices to data by total least squares** by **Yves Nievergelt** and thresholds of minimum and maximum clusters sizes.\n\nThe method to determine if a set of points in space fits in a helix is shown below","dd525ca5":"#####  if\n$$\\sigma_1>\\sigma_2=0=\\sigma_3$$\nthen all data lie in a straight line","0987650a":"#### 2.1.1 Form the matrix Z","0eead676":"#### 2.1 Fitting a quadric surface to the data","2065312a":"### 2. Fitting the axis and the radius of the helix","c912af89":"##### If $$\\sigma_2>\\sigma_3\\geqslant0$$ \nthe plane of total least squares satisfies the equation $$\\langle\\vec{x}-\\bar{\\vec{x}},\\vec{v_3}\\rangle=0$$\nin particular if $$\\sigma_3=0$$\nall data lie in that plane "}}