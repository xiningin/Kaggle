{"cell_type":{"475ff693":"code","dc520ee3":"code","b2ba4158":"code","c658ade1":"code","49111efd":"code","47067055":"code","85806f2c":"code","794b28bd":"code","0f8ad8cb":"code","ba3c06d8":"code","06f90dfc":"code","66f39840":"code","dadc3984":"code","e78fd527":"code","e87ba9ce":"code","afa57088":"code","b6f8ba4d":"code","954130db":"code","6177be7e":"code","8178fc68":"code","13567615":"code","0b91c126":"code","24b244dc":"code","288d15a7":"code","9e8ab5e7":"code","acdc57ea":"code","993a5585":"code","d4fdd100":"code","a8473aef":"code","7fc95edd":"code","64464aba":"code","5cf35ca0":"code","af775f5c":"code","94181897":"code","99a28f11":"code","2dabb9cf":"code","83eef0d5":"code","44ff1b65":"code","22dab778":"code","13d9d364":"code","c2f877a2":"code","485d9c68":"code","c4276693":"code","fbe70ccc":"code","a31ba51c":"code","5bdc9e18":"code","b4c6b03e":"code","04baeab0":"code","cb61f95c":"code","3098eac0":"code","36fc24da":"markdown","fb41fd87":"markdown","58d2ac7b":"markdown","a9fb98b8":"markdown","f0f156c1":"markdown","8e4cd425":"markdown","e65ae018":"markdown","2e8bac74":"markdown","ef232c0b":"markdown","ea78119a":"markdown"},"source":{"475ff693":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip uninstall tensorflow-cloud -y\n!pip install tensorflow_datasets==4.0.1\n\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n#from tensorflow import keras\nimport tensorflow_datasets as tfds\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nimport datetime\n\nimport scipy\nimport gc\n\n#try:\n#    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#    print('Device:', tpu.master())\n#    tf.config.experimental_connect_to_cluster(tpu)\n#    tf.tpu.experimental.initialize_tpu_system(tpu)\n#    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n#except:\n#    strategy = tf.distribute.get_strategy()\n#print('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)\nprint(tfds.__version__)\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc520ee3":"# Custom variables\nmanual_dir = r'\/kaggle\/input\/retinopathy-btgraham300\/tensorflow_datasets'\n\n#AUTOTUNE = tf.data.experimental.AUTOTUNE\n#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n#IMAGE_SIZE = [300, 300]\n#EPOCHS = 25","b2ba4158":"# Configure dataset\n(ds_train, ds_val, ds_test), ds_info = \\\ntfds.load('diabetic_retinopathy_detection\/btgraham-300:3.0.0',\n          split=['train', 'validation', 'test'],\n          download=False,data_dir=manual_dir, with_info=True,\n          shuffle_files=False, as_supervised=False)","c658ade1":"print(ds_info)","49111efd":"#datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)","47067055":"#from tensorflow.keras.utils import Sequence\n#from imblearn.over_sampling import RandomOverSampler\n#from imblearn.keras import balanced_batch_generator\n\n#class BalancedDataGenerator(Sequence):\n#    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n#    def __init__(self, x, y, datagen, batch_size=32):\n#        self.datagen = datagen\n#        self.batch_size = min(batch_size, x.shape[0])\n#        datagen.fit(x)\n#        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n#        self._shape = (self.steps_per_epoch * batch_size, *x.shape[1:])\n        \n#    def __len__(self):\n#        return self.steps_per_epoch\n\n#    def __getitem__(self, idx):\n#        x_batch, y_batch = self.gen.__next__()\n#        x_batch = x_batch.reshape(-1, *self._shape[1:])\n#        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()","85806f2c":"#def map_image(tensor):\n#    return tensor['label']","794b28bd":"#[i for i in ds_train.map(map_image)]","0f8ad8cb":"#balanced_gen = BalancedDataGenerator(\n#    [img for img in ds_train.map(lambda tensor:tensor['image'])],\n#    [label for label in ds_train.map(lambda tensor:tensor['label'])], \n#    datagen, batch_size=32)","ba3c06d8":"#for i in ds_train.take(1):\n#    print(i)\n#    print(\"Espacio\")\n#    print(i['image'].numpy())","06f90dfc":"vis = tfds.visualization.show_examples(ds_train, ds_info)","66f39840":"for tensor in ds_train.take(1):\n    image=tensor['image'].numpy()\n    label=tensor['label'].numpy()","dadc3984":"print(\"Etiqueta: \",label)\nplt.imshow(image);","e78fd527":"label_list = [tensor['label'] for tensor in ds_train.as_numpy_iterator()]\nunique, counts = np.unique(label_list, return_counts=True)\nplt.bar(unique, counts)\nprint(*zip(unique, counts))","e87ba9ce":"from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n \nclass_weights = compute_class_weight('balanced', np.unique(label_list), label_list)\nsample_weights = compute_sample_weight('balanced', label_list)\n \nclass_weights_dict = dict(enumerate(class_weights))","afa57088":"len(sample_weights)","b6f8ba4d":"y = np.bincount(label_list)\nii = np.nonzero(y)[0]\nlist_zip =[*zip(ii,y[ii])]\nlist_zip","954130db":"y","6177be7e":"[y[i]*class_weights[i] for i in range(4)]\n    ","8178fc68":"#def count(counts, tensor):\n#  labels = tensor['label']\n\n#  class_1 = labels == 1\n#  class_1 = tf.cast(class_1, tf.int32)\n\n#  class_0 = labels == 0\n#  class_0 = tf.cast(class_0, tf.int32)\n\n#  class_2 = labels == 2\n#  class_2 = tf.cast(class_2, tf.int32)\n    \n#  class_3 = labels == 3\n#  class_3 = tf.cast(class_3, tf.int32)\n    \n#  class_4 = labels == 4\n#  class_4 = tf.cast(class_4, tf.int32)\n\n#  counts['class_0'] += tf.reduce_sum(class_0)\n#  counts['class_1'] += tf.reduce_sum(class_1)\n#  counts['class_2'] += tf.reduce_sum(class_2)\n#  counts['class_3'] += tf.reduce_sum(class_3)\n#  counts['class_4'] += tf.reduce_sum(class_4)  \n\n#  return counts","13567615":"#counts = ds_train.take(35126).reduce(\n#    initial_state={'class_0': 0, 'class_1': 0, 'class_2': 0, 'class_3': 0, 'class_4': 0},\n#    reduce_func = count)\n#\n#counts = np.array([counts['class_0'].numpy(),\n#                   counts['class_1'].numpy(),\n#                   counts['class_2'].numpy(),\n#                   counts['class_3'].numpy(),\n#                   counts['class_4'].numpy()]).astype(np.float32)\n\n#fractions = counts\/counts.sum()\n#print(*fractions)","0b91c126":"#def class_func(tensor): #Aux func for resampling 'cause the data is imbalance\n#    return tensor['label']","24b244dc":"#resampler = tf.data.experimental.rejection_resample(\n#    class_func, target_dist=[0.2,0.2,0.2,0.2,0.2], initial_dist=fractions)","288d15a7":"#resample_ds = ds_train.apply(resampler).take(3800)","9e8ab5e7":"#balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)","acdc57ea":"#label_list_balanced = [tensor['label'].numpy() for tensor in balanced_ds.take(3800)]\n#unique, counts = np.unique(label_list_balanced, return_counts=True)\n#plt.bar(unique, counts)\n#print(*zip(unique, counts))","993a5585":"def transform_images(row, size, reescale=True):\n    x_train = tf.image.resize(row['image'], (size, size))\n    if reescale:\n        x_train = x_train  \/ 255\n    return x_train, tf.one_hot(row['label'], depth=5)\ndef transform_images_complete(row, size):\n    x_train = tf.image.resize(row['image'], (size, size))\n    x_train = x_train  \/ 255\n    return x_train, tf.one_hot(row['label'], depth=5), row['name']\n\nds_train = ds_train.map(lambda row:transform_images(row, 400))\n#ds_train = resample_ds.map(lambda _, row:transform_images(row, 400))\nds_val = ds_val.map(lambda row:transform_images(row, 400))\n#ds_test_all = ds_test.map(lambda row:transform_images_complete(row, 300))\nds_test = ds_test.map(lambda row:transform_images(row, 400, reescale=True))","d4fdd100":"print(\"Num classes: \" + str(ds_info.features['label'].num_classes))\nprint(\"Class names: \" + str(ds_info.features['label'].names))","a8473aef":"NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(ds_train).numpy()\nprint(\"Num training images: \" + str(NUM_TRAIN_IMAGES))\n\nNUM_VAL_IMAGES = tf.data.experimental.cardinality(ds_val).numpy()\nprint(\"Num validating images: \" + str(NUM_VAL_IMAGES))\n\nNUM_TEST_IMAGES = tf.data.experimental.cardinality(ds_test).numpy()\nprint(\"Num testing images: \" + str(NUM_TEST_IMAGES))","7fc95edd":"#ds_train = ds_train.cache()\nds_train = ds_train.shuffle(1000)\nds_train = ds_train.batch(32)\n#ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n\n#ds_val = ds_val.cache()\nds_val = ds_val.shuffle(1000)\nds_val = ds_val.batch(32)\n#ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n\n#ds_test = ds_test.cache()\nds_test = ds_test.batch(32)\n#ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)","64464aba":"for i in ds_train.take(1):\n    print(i)","5cf35ca0":"\n\nfrom tensorflow.keras.layers import Input # Input Layer\nfrom tensorflow.keras.applications import DenseNet121 # Keras Application\nfrom tensorflow.keras.layers import Dense # Dense Layer (Fully connected)\nfrom tensorflow.keras.models import Model # Model Structure\n\n\n\ninput_shape=(400, 400, 3)\n\nimg_input = Input(shape=input_shape)\nbase_model = DenseNet121(include_top=False, \n                         input_tensor=img_input, \n                         input_shape=input_shape, \n                         pooling=\"max\", \n                         weights='imagenet')\nbase_model.trainable = True\nx = base_model.output\npredictions = Dense(5, \n                    activation=\"softmax\", \n                    name=\"predictions\")(x)\nmodel = Model(inputs=img_input, \n              outputs=predictions)","af775f5c":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), #by default learning_rate=0.001\n    loss='categorical_crossentropy',\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"cat_acc\"),tf.keras.metrics.AUC(name='auc'),\n            tf.keras.metrics.Recall(name='recall'),tf.keras.metrics.Precision(name='precision')]\n)","94181897":"history = model.fit(\n    ds_train,\n    epochs=20,\n    #steps_per_epoch=NUM_TRAIN_IMAGES\/32, los batchs de 32 ya han sido asignados arriba\n    validation_data=ds_val,\n    #validation_steps=NUM_VAL_IMAGES\/32,\n    class_weight=class_weights_dict,\n    shuffle=True,\n    callbacks=[\n        #tf.keras.callbacks.EarlyStopping(patience=11, verbose=1),\n        tf.keras.callbacks.ReduceLROnPlateau(patience=4, verbose=1),\n        tf.keras.callbacks.ModelCheckpoint(filepath='bestmodel.h5',\n                                          verbose=1, save_best_only=True)\n    ]\n)","99a28f11":"# Se realiza un gr\u00e1fico de la precisi\u00f3n de entrenamiento y validaci\u00f3n\nimport matplotlib.pyplot as plt\nauc = history.history['auc']\nval_auc = history.history['val_auc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ncat_acc = history.history['cat_acc']\nval_cat_acc = history.history['val_cat_acc']\n\nepochs = range(len(auc))\nplt.figure(figsize=(18, 4.8))\nplt.subplot(1,3,1)\nplt.plot(epochs, auc, 'r', label='Training auc')\nplt.plot(epochs, val_auc, 'b', label='Validation auc')\nplt.ylim(0, 1)\nplt.title('Training and validation AUC')\nplt.legend(loc=0)\n\nplt.subplot(1,3,2)\nplt.plot(epochs, loss, 'y-.', label='Training loss')\nplt.plot(epochs, val_loss, 'g-.', label='Validation loss')\nplt.title('Training and validation Loss')\nplt.ylim(0, 2)\nplt.legend(loc=0)\n\nplt.subplot(1,3,3)\nplt.plot(epochs, cat_acc, 'c-.', label='Training cat_acc')\nplt.plot(epochs, val_cat_acc, 'g', label='Validation cat_acc')\nplt.title('Training and validation cat_acc')\nplt.ylim(0, 1)\nplt.legend(loc=0)\n\n\n\nplt.show();","2dabb9cf":"best_model = tf.keras.models.load_model('bestmodel.h5')","83eef0d5":"preds = best_model.predict(ds_test, verbose=1)","44ff1b65":"evaluation_model = best_model.evaluate(ds_test, verbose=1)","22dab778":"print(*zip(evaluation_model,['loss','cat_acc','auc','recall','precision']))","13d9d364":"preds[1]","c2f877a2":"preds = [np.argmax(pred) for pred in preds]","485d9c68":"len(preds)","c4276693":"ds_test = ds_test.unbatch()","fbe70ccc":"actuals = [np.argmax(row[1]) for row in ds_test.as_numpy_iterator()]","a31ba51c":"len(actuals)","5bdc9e18":"print(\"preds:\",preds[:30])\nprint(\"trues:\",actuals[:30])","b4c6b03e":"sample_weights = compute_sample_weight('balanced', actuals)","04baeab0":"m = tfa.metrics.CohenKappa(num_classes=5, sparse_labels=True, weightage=\"quadratic\")\n#m = tf.keras.metrics.Accuracy()\nm.update_state(actuals, preds, sample_weight=sample_weights)\nprint('Final result: ', m.result().numpy())","cb61f95c":"from sklearn.metrics import (mean_squared_error,confusion_matrix, plot_confusion_matrix, f1_score)\nfrom sklearn.metrics import classification_report\n\ntarget_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4']\nprint(classification_report(actuals, preds, target_names=target_names))","3098eac0":"confusion_matrix(actuals, preds)","36fc24da":"# Build Model","fb41fd87":"# Reject undersampling","58d2ac7b":"# Train Model","a9fb98b8":"# Preds and Evaluation","f0f156c1":"# Only use the best weights for the model.","8e4cd425":"# Load And Split","e65ae018":"# Class weights","2e8bac74":"# EDA","ef232c0b":"# Plot model training","ea78119a":"# Preprocesing"}}