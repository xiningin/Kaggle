{"cell_type":{"840c60ef":"code","c1c997ea":"code","eb5e7104":"code","47067d20":"code","70fe7f33":"code","eac3b413":"code","895e9d30":"code","5320245e":"code","3dcbd524":"code","54795c0f":"code","40b16353":"code","ff065fa9":"code","9d49c69b":"code","46f74c1c":"code","f9bc9ff3":"code","3204f60f":"code","5aaa7411":"code","14b431be":"markdown","de202f37":"markdown","5bdb3122":"markdown","3ee27fe8":"markdown","15e3dd02":"markdown","5709b2be":"markdown","b35d67d9":"markdown","bad389b0":"markdown","e37770d1":"markdown","340e8d1f":"markdown","88553c50":"markdown","3a564fcd":"markdown","d8ce38b6":"markdown","b12916ca":"markdown","b99ba17c":"markdown"},"source":{"840c60ef":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import ParameterGrid\n\nfrom xgboost import XGBRegressor\nimport copy\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","c1c997ea":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\n#display(train.head())","eb5e7104":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\n#display(test.head())","47067d20":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\n#display(submission.head())","70fe7f33":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n\n#display(train.head())","eac3b413":"target = train.pop('target')","895e9d30":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, train_size=0.80)","5320245e":"# Fit model with default settings\nmodel = XGBRegressor()\nmodel.fit(X_train, y_train)","3dcbd524":"# Make predictions and compute MSE on the validation set\npredictions = model.predict(X_valid)\nprint(\"MSE: \" + str(mean_squared_error(predictions, y_valid, squared=False)))","54795c0f":"# Create first submission file\nsubmission['target'] = model.predict(test)\nsubmission.to_csv('xgboost_1.csv')","40b16353":"# Add a few parameters to improve the performance of the model\nmodel = XGBRegressor(n_estimators=500, \n                     learning_rate=0.05, \n                     n_jobs=-1)\nmodel.fit(X_train, y_train, \n          early_stopping_rounds=5,\n          eval_set=[(X_valid, y_valid)],\n          verbose=False)","ff065fa9":"# Make predictions and compute MSE on the validation set\npredictions = model.predict(X_valid)\nprint(\"MSE: \" + str(mean_squared_error(predictions, y_valid, squared=False)))","9d49c69b":"# Create second submission file\nsubmission['target'] = model.predict(test)\nsubmission.to_csv('xgboost_2.csv')","46f74c1c":"# This cell takes a long time to run, so I have commented it.\n\n#model = XGBRegressor()\n\n# Create a dictionary of hyperparameters to search\n#grid = {'max_depth': [6, 7], 'n_estimators': [100, 500, 1000], 'n_jobs': [-1], 'learning_rate': [0.05, 0.10],}\n\n#model_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\n#for g in ParameterGrid(grid):\n#    model.set_params(**g) \n#    model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n#    predictions = model.predict(X_valid)\n#    model_score = mean_squared_error(y_valid, predictions, squared=False)\n#    model_scores.append(model_score)\n#    print('MSE =', f'{model_score:0.5f} ', 'Parameters:', g)\n\n# Find best hyperparameters from the validation score and print\n#best_idx = np.argmin(model_scores)\n#print()\n#print('Best score: ', model_scores[best_idx], ParameterGrid(grid)[best_idx])","f9bc9ff3":"# Fit model with the best parameters found\nmodel = XGBRegressor(n_estimators=500,\n                     learning_rate=0.05,\n                     n_jobs=-1)\nmodel.fit(X_train, y_train, \n          early_stopping_rounds=5,\n          eval_set=[(X_valid, y_valid)],\n          verbose=False)","3204f60f":"# Make predictions and compute MSE on the validation set\npredictions = model.predict(X_valid)\nprint(\"MSE: \" + str(mean_squared_error(predictions, y_valid, squared=False)))","5aaa7411":"# Create third submission file\nsubmission['target'] = model.predict(test)\nsubmission.to_csv('xgboost_3.csv')","14b431be":"### So this is what I have so far. As mentioned before, any tips on how to improve this simple model are welcome. Thank you! :D","de202f37":"With this first submission file, LB score was **0.84924**.","5bdb3122":"### Pull out the target and make a validation split","3ee27fe8":"### Load libraries","15e3dd02":"With this second submission file, LB score was **0.84586**. A little better than the first one.","5709b2be":"## Thrid model with parameter tunning after a simple grid search","b35d67d9":"### Load data","bad389b0":"## First model: XGBoost regressor with default settings","e37770d1":"### Encode categorical variables","340e8d1f":"In this notebook I used XGBoost to fit the data and did some parameter tunning. If you have any hints on how to improve it, please feel free to comment below :)\n\nThank you!","88553c50":"With this third submission file, LB score was **0.84586**, the same as the second model's.","3a564fcd":"These were the results:\n\n`MSE = 0.85165  Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 100, 'n_jobs': -1}`\n`MSE = 0.84347  Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 500, 'n_jobs': -1}`\n`MSE = 0.84347  Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 1000, 'n_jobs': -1}`\n`MSE = 0.85032  Parameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'n_jobs': -1}`\n`MSE = 0.84367  Parameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'n_jobs': -1}`\n`MSE = 0.84367  Parameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'n_jobs': -1}`\n`MSE = 0.84582  Parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'n_jobs': -1}`\n`MSE = 0.84375  Parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'n_jobs': -1}`\n`MSE = 0.84375  Parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000, 'n_jobs': -1}`\n`MSE = 0.84526  Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'n_jobs': -1}`\n`MSE = 0.84426  Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'n_jobs': -1}`\n`MSE = 0.84426  Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'n_jobs': -1}`\n\n`Best score:  0.8434681452062144 {'n_jobs': -1, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.05}`","d8ce38b6":"The best set of parameters found was ... the same I had tried before! The only difference is that `n_jobs` was set to `-1`.\n\nAnd it seems that setting `n_estimators` to more than 500 did not make a difference.\n\nIn any case, I will fit the model once again.","b12916ca":"## Second model using some parameter tunning","b99ba17c":"# Basic XGBoost model with parameter tunning"}}