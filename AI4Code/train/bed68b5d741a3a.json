{"cell_type":{"62f0ca71":"code","da0b31dc":"code","3e0ce59d":"code","a6165a6a":"code","cf3632da":"code","4d2c88ed":"code","d0f88cc7":"code","a0c1067c":"markdown","eb54180e":"markdown","f5dd879e":"markdown"},"source":{"62f0ca71":"import re\nimport sys\nimport time\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib as mpl\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nimport sklearn.model_selection as ms\nfrom sklearn import neighbors\nfrom sklearn import tree\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KDTree\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split,cross_val_score, ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold,KFold,GridSearchCV,RandomizedSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score,mean_squared_error,confusion_matrix\n\nfrom xgboost import XGBRegressor \nfrom lightgbm import LGBMRegressor \n\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ndatabase = pd.read_csv(r\"..\/input\/google-play-store-apps\/googleplaystore.csv\")# store wine type as an attribute\n\n\n\n#############################################################  \n######Data Cleaning\n\ni = database[database['Category'] == '1.9'].index\ndatabase.loc[i]\ndatabase = database.drop(i)\n\ndatabase = database[pd.notnull(database['Last Updated'])]\ndatabase = database[pd.notnull(database['Content Rating'])]\n\n\nCategoryList = database['Category'].unique().tolist() \nCategoryList = ['cat_' + word for word in CategoryList]\ndatabase = pd.concat([database, pd.get_dummies(database['Category'], prefix='cat')], axis=1)\n\n\ndatabase['Rating'] = database['Rating'].fillna(database['Rating'].median())\ndatabase['Installs'] = database['Installs'].apply(lambda x : x.strip('+').replace(',', ''))\ndatabase['Type'] = pd.get_dummies(database['Type'])\ndatabase['Price'] = database['Price'].apply(lambda x : x.strip('$'))\ndatabase['Last Updated'] = database['Last Updated'].apply(lambda x : time.mktime(datetime.datetime.strptime(x, '%B %d, %Y').timetuple()))\n\n\n#######################################################\n###### Encoding\n\nLE = preprocessing.LabelEncoder()\ndatabase['App'] = LE.fit_transform(database['App'])\ndatabase['Genres'] = LE.fit_transform(database['Genres'])\ndatabase['Content Rating'] = LE.fit_transform(database['Content Rating'])\n\n\n########################################################\n###### Size\n\n\nk_indices = database['Size'].loc[database['Size'].str.contains('k')].index.tolist()\nconverter = pd.DataFrame(database.loc[k_indices, 'Size'].apply(lambda x: x.strip('k')).astype(float).apply(lambda x: x \/ 1024).apply(lambda x: round(x, 3)).astype(str))\ndatabase.loc[k_indices,'Size'] = converter\n\ndatabase['Size'] = database['Size'].apply(lambda x: x.strip('M'))\ndatabase[database['Size'] == 'Varies with device'] = 0\ndatabase['Size'] = database['Size'].astype(float)\n","da0b31dc":"########################################################\n###### Feature Selection\n\nshuffled_database = database.reindex(np.random.permutation(database.index))\n\n\nfeatures = ['App', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated']\nshuffled_database[features]=shuffled_database[features].astype(float)\nX = shuffled_database[features]\ny = shuffled_database['Rating']\n\n\n#######################################################\n##### Train Test Split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=10)\n","3e0ce59d":"###################################################################\n### DecisionTreeRegressor\n\nDT_Regression = tree.DecisionTreeRegressor(criterion='mae', max_depth=5, min_samples_leaf=5, random_state=42)\nDT_Regression.fit(X_train,y_train)\ny_DT_pred=DT_Regression.predict(X_test)\nDT_Regression_score=DT_Regression.score(X_test,y_test)\n\n\nprint(\"with train test split_DecisionTreeRegression\", DT_Regression_score)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_DT_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_DT_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_DT_pred)), '\\n')","a6165a6a":"###################################################################\n### RandomForestRegressor\n\nRF_Regression= RandomForestRegressor(random_state=20)\nRF_Regression.fit(X_train,y_train)\ny_RF_pred=RF_Regression.predict(X_test)\nRF_Regression_score=RF_Regression.score(X_test,y_test)\n\nprint(\"with train test split_RandomForestRegression\", RF_Regression_score)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_RF_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_RF_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_RF_pred)), '\\n')\n","cf3632da":"###################################################################\n### XGBRegressor\n\n\nXGB_Regression= XGBRegressor(random_state=20)\nXGB_Regression.fit(X_train,y_train)\ny_XBG_pred=XGB_Regression.predict(X_test)\nRF_Regression_score=XGB_Regression.score(X_test,y_test)\n\nprint(\"with train test split_XGBRegression\", RF_Regression_score)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_XBG_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_XBG_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_XBG_pred)), '\\n')\n","4d2c88ed":"###################################################################\n### LightGBM\n\n\nLGBM_Regression = LGBMRegressor(random_state=20)\nLGBM_Regression.fit(X_train,y_train)\ny_LGBM_pred=LGBM_Regression.predict(X_test)\nLGBM_Regression_socre=LGBM_Regression.score(X_test,y_test)\n\nprint(\"with train test split_LGBM_Regression\", LGBM_Regression_socre)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_LGBM_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_LGBM_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_LGBM_pred)), '\\n')","d0f88cc7":"####################################################\n## Result with Cross Validation Score and Prediction\n\nscore_CV_XGB_Regression = cross_val_score(XGB_Regression,X,y,cv=5 ,scoring='r2')\nprint(\"with CV XGB_Regression\", score_CV_XGB_Regression.mean())","a0c1067c":"The cross validation is performed in order to evaluate the model with multiple train-test splits. But it is more time consuming compared to train test split","eb54180e":"The aim of the project is to identify the accuracy of R2 score for different tree-based models, covering:\n- Decision Tree\n- Random Forest\n- XG Boost\n- LightGBM\n\nMeanwhile, the cross validation will be also adopted for model evaluation\n\nThe use of data cleaning is firstly adopted for the input of regression model. \n\n","f5dd879e":"After the shuffled the database, the features of the dataset are selected and listed below:\n\n- App \n- Reviews\n- Size\n- Installs\n- Type \n- Price\n- Content Rating \n- Genres, \n- Last Updated\n\nThe output I want to evaluate is Rating "}}