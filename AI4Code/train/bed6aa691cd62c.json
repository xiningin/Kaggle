{"cell_type":{"f407b7d3":"code","052d3aa3":"code","6e449fb4":"code","13b28c6e":"code","ba796f3c":"code","eaf97301":"code","71358446":"code","e17e88cf":"code","2756ac52":"code","348677fc":"code","d09a9649":"code","92af65c2":"code","48eb7801":"code","68582c7a":"code","f1d25e05":"code","e297ab70":"code","0e887819":"code","350168af":"code","610ed02d":"markdown","13fbc0a4":"markdown","d014735f":"markdown","d3d1a3fd":"markdown","740c4c27":"markdown","97ff1a4e":"markdown","51de5bb0":"markdown","4bf7e2b8":"markdown","5d66c329":"markdown","b8642184":"markdown","a5f31f5d":"markdown","0e1af0b2":"markdown","4408b339":"markdown","fc97f3dc":"markdown","a2256eae":"markdown","14a6b813":"markdown","5a8fac27":"markdown","cb533f29":"markdown","a54f42c6":"markdown"},"source":{"f407b7d3":"import sys\n!cp ..\/input\/rapids\/rapids.0.16.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","052d3aa3":"import cudf\nimport cuml\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.metrics import log_loss\nfrom cuml.experimental.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom tqdm.notebook import tqdm","6e449fb4":"INPUT_DATA = r\"..\/input\/lish-moa\/\"","13b28c6e":"%%time\nTrain = cudf.read_csv(INPUT_DATA+\"train_features.csv\")\nTarget_scored = cudf.read_csv(INPUT_DATA+'train_targets_scored.csv')\nTrain.shape","ba796f3c":"%%time\nTrain = Train.merge(Target_scored,how='left',on=\"sig_id\")","eaf97301":"ID = ['sig_id']\nCP = ['cp_type','cp_time','cp_dose']\nG = [x for x in Train.columns if 'g-' in x]\nC = [x for x in Train.columns if 'c-' in x]\nTargets = Target_scored.columns[1:].tolist()\nlen(C),len(G),len(Targets)","71358446":"def FeaturesImportance(val):\n    target, feat = val\n    clf = LogisticRegression().fit(Train[feat].values.reshape(-1,1), Train[target])\n    prob = clf.predict_proba(Train[feat].values.reshape(-1,1))\n    loss = log_loss(Train[target],prob)\n    return cudf.DataFrame({\"Feature\":[feat],target:[loss]})","e17e88cf":"if False:\n    %%time\n    for target in tqdm(Targets):\n\n        all_feat = [(target,f) for f in C+G]\n        df = []\n\n        for i in range(len(all_feat)):\n            df.append(FeaturesImportance(all_feat[i]))\n\n        temp = cudf.concat(df).sort_values('Feature')\n\n        if target == Targets[0]:\n            output = temp.copy()\n\n        else:\n            output[target] = temp[target]\n    output = output.set_index(\"Feature\")\nelse:\n    output = cudf.read_csv('..\/input\/moa-feat-importance-rapids\/output_source.csv', index_col=0)\noutput.sample(5)","2756ac52":"# output = output.to_pandas()","348677fc":"output_ = output.copy(deep=True)","d09a9649":"output_.min().min(), output_.max().max(), output_.std().mean()","92af65c2":"output_scaled = output_.copy(deep=True)\nfor c in output_scaled.columns:\n    output_scaled[c] = (1 - MinMaxScaler().fit_transform(output_[[c]])).round(14).values\noutput_scaled.min().min(), output_scaled.max().max(), output_scaled.std().mean()","48eb7801":"thresh_mean, thresh_max = 0.35, 0.975\nfig = plt.figure(figsize=(20,10))\nax, ax2 = fig.add_subplot(2, 1, 1), fig.add_subplot(2, 1, 2)\nax.hist(output_scaled.mean(axis=1).to_pandas(), bins=100, density=True)\nax.axvline(x=thresh_mean, ls='--', c='red', linewidth=0.5)\nax.set_title('Histogram of Mean Feature Importance distribution')\nax2.hist(output_scaled.max(axis=1).to_pandas(), bins=100, density=True)\nax2.axvline(x=thresh_max, ls='--', c='red', linewidth=0.5)\nax2.set_title('Histogram of Max Feature Importance distribution')\nplt.show()","68582c7a":"output_scaled[\"MeanImp\"] = output_scaled[Targets].mean(axis=1)\noutput_scaled[\"MaxImp\"]  = output_scaled[Targets].max(axis=1)","f1d25e05":"fs_both        = output_scaled.loc[(output_scaled.MeanImp >= thresh_mean) & (output_scaled.MaxImp >= thresh_max), Targets]\nfs_any         = output_scaled.loc[(output_scaled.MeanImp >= thresh_mean) | (output_scaled.MaxImp >= thresh_max), Targets]\nfs_out_of_both = output_scaled.loc[(output_scaled.MeanImp < thresh_mean) & (output_scaled.MaxImp < thresh_max), Targets]\noutput_scaled.shape[0], fs_both.shape[0], fs_any.shape[0], fs_out_of_both.shape[0]","e297ab70":"summary_mean_importance = cudf.concat([fs_out_of_both.mean(axis=0), fs_any.mean(axis=0), fs_both.mean(axis=0)], axis=1)\nsummary_mean_importance.columns = ['Out of both', 'In Any', 'In Both']\nsummary_max_importance = cudf.concat([fs_out_of_both.max(axis=0), fs_any.max(axis=0), fs_both.max(axis=0)], axis=1)\nsummary_max_importance.columns = ['Out of both', 'In Any', 'In Both']","0e887819":"for summary, metric in zip([summary_mean_importance, summary_max_importance], ['mean', 'max']):\n    print(f\"Summary for {metric}\")\n    print(f\"Average Difference in-both \/ out-of-both : {(summary['In Both'] - summary['Out of both']).mean().round(2)}\")\n    print(f\"Average Difference in-any \/ out-of-both : {(summary['In Any'] - summary['Out of both']).mean().round(2)}\")","350168af":"fig = plt.figure(figsize=(20,5))\nax, ax2 = fig.add_subplot(1, 2, 1), fig.add_subplot(1, 2, 2)\nsns.heatmap(fs_both[Targets].to_pandas(), ax=ax)\nsns.heatmap(fs_out_of_both[Targets].to_pandas(), ax=ax2)\nplt.savefig('feature_importance.jpg')","610ed02d":"Thanks for reading me !","13fbc0a4":"The log-loss is theoretically comprised between 0 (perfect classifier) and +infinity (perfectly wrong classifier), but in our case its values seem to be rather contracted from 6.3e-4 to 0.1515. That's a very narrow range, not very easy to analyze, given also the very small standard deviation. \n\nFor comparison and ranking purposes, we need the features importances for each target to be decreasing with the log-loss (that's more intuitive. You expect feature importance to be, well, an importance, right ?). The lower the log-loss, the better the feature is at predicting the target, and the higher the feature importance.\n\nAlso, if you want to compare features in order to keep some and dismiss others, you need them to be in \"the same unit\", if that makes sense.\n\nWe will see in the following that it allows us to split our data very easily in 2 groups, the features we keep and the ones we dismiss.\n\nThis is why, for each target, we rescale its corresponding features log-losses to [0, 1] while inversing the order of values so that the interpretation as a \"feature importance\" is more intuitive.","d014735f":"Exploratory data analysis, and especially getting an idea of features importance, is a crucial step when confronted with a data science problem. However, this step can often be trying if your database is huge, with numerous interconnexions, making the analysis costly in computing time.","d3d1a3fd":"Import the rapids functions that will be necessary :","740c4c27":"Here, I will save apart separately :\n- The features which are above *both* thresholds on mean importance and max importance ;\n- The features which are above *one* of the thresholds on mean importance and max importance.","97ff1a4e":"Now, I will also save some interesting summaries of results : comparing, on features either in none of the categories or in both, the mean and max importance they have by target. You can find them in [the dataset](https:\/\/www.kaggle.com\/louise2001\/moa-feat-importance-rapids) as well.","51de5bb0":"Wow, that's pretty impressive ! Now, just as we do a to_cpu() after CUDA computing on PyTorch, we can just convert everything back to Pandas. However, I will stay with RAPIDS in the following.","4bf7e2b8":"![tyler-lastovich-NUrnXXlhPjc-unsplash.jpg](attachment:tyler-lastovich-NUrnXXlhPjc-unsplash.jpg)\n<span><center>Photo by <a href=\"https:\/\/unsplash.com\/@lastly?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Tyler Lastovich<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/rapids?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/center><\/span>","5d66c329":"Calculating a feature's average importance on all targets is important to get a global view of how crucial it can be on your overall model. \n\nHowever, I'm not sure you should rely entirely on this indicator when discriminating between features to keep and features to dismiss : indeed, a given feature could have a very high explainability power on one specific target, and be irrelevant on all others, therefore even though it would receive a low average importance score, keeping it would be decisive in final model quality for that target.","b8642184":"[*RAPIDS*](https:\/\/developer.nvidia.com\/rapids) is a suite of packages developed out of NVIDIA, that intends to execute end-to-end data science and analytics pipelines entirely on GPUs.","a5f31f5d":"To get a global overview, we can just compare heatmaps of scaled feature importance by target on the two subsets, the features that fulfill both criterias and the ones that don't fulfill any. It becomes visible immediately that the fist graph is much lighter - the importances are higher.","0e1af0b2":"At this stage, I'm reporting the log_loss, also known as binary cross-entropy loss, as an importance metric : indeed, the better a feature explains a given target (the lower the log-loss), the more important that feature will be in predicting the target. However, we will later need a bijective, strictly decreasing transformation of that log-loss to fit the intuition that feature importance should be increasing when the loss diminishes.","4408b339":"Interpretation : \n- The features that fulfill both criteria have, on average on all targets, 0.15 more average importance than the features that don't fulfill any of the criterias.\n- The features that fulfill one of the criteria have, on average on all targets, 0.19 more maximum importance than the features that don't fulfill any of the criterias.","fc97f3dc":"Out of 872 samples, we have kept 187 features that fulfill either of the two criterias of minimum average importance or maximal importance. That seems a good skimming proportion.","a2256eae":"# Getting To Know Your Features In Seconds With Rapids","14a6b813":"At the end of this notebook, you'll be able to select, among your features, those which appear to be the most crucial in predicting your targets. \n\nThat will enable you to build robust, medium-sized models with better interpretability.\n\nIf you want to skip the processing and jump to the results, you can take a look [here](https:\/\/www.kaggle.com\/louise2001\/moa-feat-importance-rapids).","5a8fac27":"The goal of this notebook is to perform univariate regressions of each target on every feature, namely *872 x 206 = 179632* logistic models to estimate separately. The good news ? **This is possible within minutes with *RAPIDS* !** \n\nYou can note that I never import neither Pandas (replaced by cuDF) nor scikit-learn (replaced by cuML). \n\nThis notebook will be a tutorial, aiming at helping you get familiar using these libraries.","cb533f29":"Univariate regressions of each target on every feature : if you don't want to run the 40 minutes processing, you can directly read the results which I have saved in this dataset : https:\/\/www.kaggle.com\/louise2001\/moa-feat-importance-rapids.","a54f42c6":"GPUs are great at improving computing time... if you can use them ! Indeed, their use is often dedicated to training neural networks, thanks to powerful frameworks (TensorFlow or PyTorch are the most famous) that have enabled millions of developers to unleash their incredible powers.\n\nHowever, if you have some dirty preprocessing to get done with Pandas or sklearn, that can be quite heavy if your data is big. These packages don\u2019t support GPUs and to sum things up, you won't be able to get the most out of your GPU at this stage."}}