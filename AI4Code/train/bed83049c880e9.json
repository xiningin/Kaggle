{"cell_type":{"78757a89":"code","be844b2a":"code","1dc13083":"code","8dce74ad":"code","5362d8c8":"code","befe8b98":"code","9c9432ba":"code","8d1df2e5":"code","96e3937d":"code","c38104ce":"code","9722283d":"code","0f259b37":"code","f339da1e":"code","89ae56c2":"code","029ab398":"code","6ca69a76":"code","b3b6c81e":"code","a66aeb41":"code","02a96629":"code","45d6d3af":"code","b7b03679":"code","d1b0b15a":"code","3c6880ae":"code","00ef320a":"code","d10a88ca":"code","a307b9b5":"code","840f6c7b":"code","12389481":"code","9a8275f0":"code","2dc23efd":"code","6b9c1b7e":"code","fa572a89":"code","08d8d6a4":"code","59dcd1d1":"code","ce92bd7f":"code","e3df0173":"code","298116a4":"code","ce31afbc":"code","55619e27":"code","5d997240":"code","5e588368":"code","04337f2f":"code","9c271cb1":"code","7d8cb84c":"code","68bcacdf":"code","d7e22587":"code","9a69c69d":"code","a926edaa":"code","e468ba57":"code","fc1827b0":"code","847292d6":"code","0a539d73":"markdown","0d411088":"markdown","bc6db1a7":"markdown","48576f29":"markdown","1886d460":"markdown","ff01546d":"markdown","d0674c2c":"markdown","f599bd91":"markdown","8df77297":"markdown","c73ab040":"markdown","1c145d45":"markdown","769c9387":"markdown","c0ef76ee":"markdown","3bd167a1":"markdown","43b08498":"markdown","e090c92b":"markdown","3df885c3":"markdown","26b4fd34":"markdown","f937d187":"markdown","63f0bc10":"markdown","8f23894d":"markdown"},"source":{"78757a89":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom gensim.models import KeyedVectors\nfrom sklearn.linear_model import LinearRegression\nfrom collections import Counter\n\nimport string\nimport re","be844b2a":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ndf_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nprint('Training shape : {}'.format(df_train.shape))\nprint('Testing shape : {}'.format(df_test.shape))","1dc13083":"df_train.head()","8dce74ad":"df_test.head()","5362d8c8":"df_submission.head()","befe8b98":"df_train.info()","9c9432ba":"df_train.describe()","8d1df2e5":"col = df_train.columns       # .columns gives columns names in data \nprint(col)","96e3937d":"df_train.isnull().sum()","c38104ce":"df_test.isnull().sum()","9722283d":"count = df_train['excerpt'].str.split().str.len()\nprint(\"Number of words in excerpts:\\n\",count)\nprint(\"Max word count from excerpt: \", max(count))","0f259b37":"results = Counter()\ndf_train['excerpt'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","f339da1e":"longest = max(str(results.keys()).split(), key=len)\nprint(longest)\nprint(len(longest))","89ae56c2":"print(\"duplicated =>\", df_train.duplicated(keep = \"first\").sum())","029ab398":"print(\"duplicated =>\", df_test.duplicated(keep = \"first\").sum())","6ca69a76":"def removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","b3b6c81e":"def removeLinks(text):\n    return re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)","a66aeb41":"def removeNumbers(text):\n    return re.sub(r'\\d+', '', text)","02a96629":"def clean(text):\n    text = text.lower() \n    text = removePunctuations(text)\n    text = removeLinks(text)\n    text = removeNumbers(text)\n    return text","45d6d3af":"df_train['excerpt_clean'] = df_train['excerpt'].apply(clean)\ndf_train.head()","b7b03679":"X = df_train['excerpt_clean'].copy()\ny = df_train['target'].copy()\n\nprint(len(X), len(y))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))","d1b0b15a":"def plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.grid(True)","3c6880ae":"def plot_rmse(history):\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label='val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('root mean squared error')\n    plt.legend()\n    plt.grid(True)","00ef320a":"def get_predict(model, X_train, y_train, X_test, y_test):\n    print(\"\\nThe model performance for training set\")\n    print(\"--------------------------------------\")\n    print(model.score(X_train, y_train))\n    print(\"\\nThe model performance for testing set\")\n    print(\"--------------------------------------\")\n    print(model.score(X_test, y_test))","d10a88ca":"#google_model = KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)","a307b9b5":"class MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y):\n        return self.transform(X)\n\n    def transform(self, X):\n        return [np.mean([self.word2vec.get_vector(w) for w in words if w in self.word2vec.index_to_key] or [np.zeros(100)], axis=0) for words in X]","840f6c7b":"#cbow_model = LinearRegression()\n\n#embedding_vectorizer = MeanEmbeddingVectorizer(google_model)\n#X_train_vectorizer = embedding_vectorizer.transform(X_train)\n#X_test_vectorizer = embedding_vectorizer.transform(X_test)\n\n#cbow_model.fit(X_train_vectorizer, y_train)","12389481":"#get_predict(cbow_model, X_train_vectorizer, y_train, X_test_vectorizer, y_test)","9a8275f0":"vectorizer = TextVectorization(max_tokens=5000, output_sequence_length=200)\nds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\nvectorizer.adapt(ds)","2dc23efd":"voc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))","6b9c1b7e":"num_tokens = len(voc) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0","fa572a89":"filepath = '..\/input\/glove6b\/glove.6B.100d.txt'\n\nembeddings_index = {}\nwith open(filepath) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Total vectors find: %i.\" % len(embeddings_index))","08d8d6a4":"embedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Word used %d and lost %d\" % (hits, misses))","59dcd1d1":"embedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","ce92bd7f":"embedding_model = tf.keras.Sequential([\n          tf.keras.Input(shape=(1,), dtype=\"string\"),  \n          vectorizer,\n          embedding_layer,\n          layers.GlobalMaxPool1D(),\n          layers.Dense(10, activation='relu'),\n          layers.Dense(1)\n])\n\nembedding_model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\n\nembedding_model.summary()","e3df0173":"history = embedding_model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_train, y_train), verbose=0)","298116a4":"plot_loss(history)","ce31afbc":"plot_rmse(history)","55619e27":"vectorizer = TextVectorization(max_tokens=5000, output_sequence_length=200)\nds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\nvectorizer.adapt(ds)","5d997240":"voc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))","5e588368":"num_tokens = len(voc) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0","04337f2f":"filepath = '..\/input\/glove6b\/glove.6B.100d.txt'\n\nembeddings_index = {}\nwith open(filepath) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Total vectors find: %i.\" % len(embeddings_index))","9c271cb1":"embedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Word used %d and lost %d\" % (hits, misses))","7d8cb84c":"embedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","68bcacdf":"lstm_model = tf.keras.Sequential([\n    tf.keras.Input(shape=(1,), dtype=\"string\"),  \n    vectorizer,\n    embedding_layer,\n    layers.LSTM(64, return_sequences=True),\n    layers.GlobalMaxPool1D(),\n    layers.Dense(10, activation='relu'),\n    layers.Dense(1)\n])\n\nlstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\nlstm_model.summary()","d7e22587":"history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test), verbose=0)","9a69c69d":"plot_loss(history)","a926edaa":"plot_rmse(history)","e468ba57":"best_model = lstm_model","fc1827b0":"def submission(submission_file_path,model,excerpt):\n    padding_type='post'\n    classes = model.predict(excerpt)\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = classes\n    sample_submission.to_csv(\".\/submission.csv\", index=False)\n    ","847292d6":"submission_file_path = '..\/input\/commonlitreadabilityprize\/sample_submission.csv'\n\nsubmission(submission_file_path, best_model, df_test['excerpt'])","0a539d73":"CommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project.\n\nThe organizers would like to thank Schmidt Futures for their advice and support for making this work possible.","0d411088":"### Acknowledgements","bc6db1a7":"## Download Models","48576f29":"* id - unique ID for excerpt\n* url_legal - URL of source - this is blank in the test set.\n* license - license of source material - this is blank in the test set.\n* excerpt - text to predict reading ease of\n* target - reading ease\n* standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","1886d460":"## Data Preprocessing","ff01546d":"## Data Analysis","d0674c2c":"## Submission","f599bd91":"## Clean Dataset","8df77297":"### Word Embeddings: Continuous Bag of Words","c73ab040":"### Description","1c145d45":"# CommonLit Readability","769c9387":"### Content","c0ef76ee":"### Long Short Term Memory (LSTM)","3bd167a1":"## Read Dataset","43b08498":"### Embeddings","e090c92b":"### Redundancy data","3df885c3":"### Missing data","26b4fd34":"## Best Model","f937d187":"## Imports","63f0bc10":"Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\n\nCurrently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nIf successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.","8f23894d":"## Model"}}