{"cell_type":{"0f1a210c":"code","bcd50357":"code","5716c8d9":"code","49857faa":"code","f5f4a147":"code","7e2ab0a0":"code","a289871f":"code","f55f948b":"code","30394b44":"code","f9e885a6":"code","e7fa13f1":"code","385f0282":"code","8a2db42a":"code","c1a026d2":"code","0459552a":"code","7897117c":"code","06a7bfe0":"code","f22dc3f0":"code","63db1311":"code","d414899f":"code","6b11ae00":"code","89fab33c":"code","12fcb911":"code","55281791":"code","2a056125":"code","ca4a578d":"code","a05eb90d":"code","9ce4f621":"code","4ef86353":"code","41b46a4d":"code","922256cc":"code","59715e11":"code","e7b38e57":"code","8408733b":"code","fb51b685":"code","142d2a9c":"code","03695b23":"code","9a7b74c9":"code","43e35420":"code","0828eb65":"code","53b1b98b":"code","2035154b":"code","038749ae":"code","726bb41f":"code","f6539d50":"code","c8debe5c":"code","4735ac21":"code","1371803b":"code","58740f0c":"code","8792fb9b":"code","41c2a514":"code","6b736845":"code","d85d1f0d":"code","832e2061":"code","86a43977":"code","20eca5cb":"code","c31f9c0a":"code","69d7e504":"code","ad26a8a4":"code","c80974b9":"code","5d00bd79":"code","3a3c3b6f":"code","80e10ed6":"code","a1e33b95":"code","ea28afcd":"markdown","9fda2509":"markdown","300bbe57":"markdown","20d12472":"markdown","52e74643":"markdown","7232fc0d":"markdown","1e7a1cae":"markdown","7ef922e6":"markdown"},"source":{"0f1a210c":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport datetime\nfrom datetime import date\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\n# import chart_studio.plotly as py\nimport cufflinks as cf\nimport plotly.express as px\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\ncf.go_offline()\n\nimport pandas_profiling\nimport plotly.graph_objects as go\n\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","bcd50357":"df = pd.read_csv('..\/input\/youtubedata\/UStrends.csv')\ndf.rename(columns = {'trend':'trending'}, inplace = True)\ndf.drop(['Unnamed: 0.1','thumbnail_link','video_id'],axis=1,inplace=True)\ndf.head(5)\n# trend of a category was calculated by summing the views, likes and comment_count minus by dislikes, sum_stats \n# column was created with result of these calculations then sum_stats mean was taken and the values greater than\n# that mean is considered trending (1) and values less than that mean are considered as non trending (0). ","5716c8d9":"px.line(df,x='trending_date',y=['views','likes','comment_count'],labels={'value':'sum of stats'})\n# plotting line graph of views, likes and comment_count over their trending date","49857faa":"px.line(df,x='publish_time',y=['views','likes','dislikes','comment_count'])\n# plotting line graph of views, likes, dislikes and comment_count over their publish time ","f5f4a147":"px.line(df,x='trending_date',y='likes')\n# plotting line graph of likes over it's trending date","7e2ab0a0":"px.line(df,x='trending_date',y='dislikes')\n# plotting line graph of dislikes over it's trending date","a289871f":"px.line(df,x='trending_date',y='comment_count')\n# plotting line graph of comment_count over it's trending date","f55f948b":"cat1=df.groupby('category')[['likes','dislikes','comment_count']].sum().sort_values(by='likes',ascending=False)\npx.bar(cat1,barmode='group')\n# likes, dislikes and comment_count for each category","30394b44":"cat2=df.groupby('category')['views'].agg(['sum']).sort_values(by='sum',ascending=False)\npx.bar(cat2,barmode='group',labels={'value':'views'})\n# views for each category","f9e885a6":"cat3=df.groupby('comments_disabled')[['views','likes','dislikes','comment_count']].sum().sort_values(by='likes',ascending=False)\npx.bar(cat3,barmode='group')\n# 'views','likes','dislikes','comment_count' for each comment disabled","e7fa13f1":"cat4=df.groupby('ratings_disabled')[['views','likes','dislikes','comment_count']].sum().sort_values(by='likes',ascending=False)\npx.bar(cat4,barmode='group')\n# 'views','likes','dislikes','comment_count' for each ratings_disabled","385f0282":"cat5=df.groupby('video_error_or_removed')[['views','likes','dislikes','comment_count']].sum().sort_values(by='likes',ascending=False)\npx.bar(cat5,barmode='group')\n# 'views','likes','dislikes','comment_count' for each video_error_or_removed","8a2db42a":"cat5=df.groupby('trending')[['views','likes','dislikes','comment_count']].sum().sort_values(by='likes',ascending=False)\npx.bar(cat5,barmode='group')\n# # 'views','likes','dislikes','comment_count' for each trending","c1a026d2":"px.bar(df.groupby('category',as_index=False)['trending'].agg(['sum']).sort_values(by='sum',ascending=False))\n# trend of cateogries","0459552a":"# set(df['category'])\ncat_t=df.groupby('category')['trending'].value_counts()\ncat_t\n# number of trend and non trend for each category ","7897117c":"df['comments_disabled']=np.where(df['comments_disabled']==False,0,1)\ndf['ratings_disabled']=np.where(df['ratings_disabled']==False ,0,1)\ndf['video_error_or_removed']=np.where(df['video_error_or_removed']==False ,0,1)\ndf","06a7bfe0":"trend_1=df[df['trending']==1]\ntrend_1=trend_1.reset_index(drop=True)\ntrend_0=df[df['trending']==0]\ntrend_0=trend_0.reset_index(drop=True)\ntrend_1.head(5)","f22dc3f0":"stopwords = set(STOPWORDS)\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200000,\n        max_font_size=40, \n        scale=3,\n        random_state=1).generate(str(data))\n    \n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n        \n    plt.imshow(wordcloud)\n    plt.show()\nprint(\"Title - Trend 1\")\nshow_wordcloud(trend_1['title'].values)\nprint(\"Title - Trend 0\")\nshow_wordcloud(trend_0['title'].values)\n# learn about the wordcloud from the internet\n# generating wordcloud of the title column for all 1 for trending and 0 for non trending","63db1311":"print(\"Tags - Trend 1\")\nshow_wordcloud(trend_1['tags'].values)\nprint(\"Tags - Trend 0\")\nshow_wordcloud(trend_0['tags'].values)\n# generating wordcloud of the tags column for all 1 for trending and 0 for non trending","d414899f":"print(\"description - Trend 1\")\nshow_wordcloud(trend_1['description'].values)\nprint(\"description - Trend 0\")\nshow_wordcloud(trend_0['description'].values)\n# generating wordcloud of the description column for all 1 for trending and 0 for non trending","6b11ae00":"fig, ax = plt.subplots()\nfig.set_size_inches(15, 10)\nsns.heatmap(df.corr(),annot=True,cmap='viridis', fmt='g')\n# sum_stats, likes and views columns are partially correlated with trending column","89fab33c":"fdf=df[['category','views','likes','dislikes','comment_count','sum_stats','trending']]\nfdf\n# getting the required columns and discarding the rest of the columns ","12fcb911":"fdf['music_f']=np.where((fdf['category']=='Music'),1,0)\n# creating feature of music category since it is the most trending category","55281791":"fdf['views_f']=np.where(fdf['views']>fdf['views'].mean(),1,0)\nfdf['likes_f']=np.where(fdf['likes']>fdf['likes'].mean(),1,0)\nfdf['dislikes_f']=np.where(fdf['dislikes']>fdf['dislikes'].mean(),1,0)\nfdf['cc_f']=np.where(fdf['comment_count']>fdf['comment_count'].mean(),1,0)\n\nfdf\n# since the videos being trending or not was decided based on the mean of sum_stats which was the sum of views \n# likes and comment_count and minus of dislikes, it is a good idea to feature engineering them the same way so that\n# their correlation with trending column can get higher.","2a056125":"fdf.drop(['category','views', 'likes' ,'dislikes' ,'comment_count' ,'sum_stats'],axis=1,inplace=True)\n# we are done feature engineering with the columns, now we can drop all of them","ca4a578d":"fig, ax = plt.subplots()\nfig.set_size_inches(10 ,7)\nsns.heatmap(fdf.corr(),annot=True,cmap='viridis', fmt='g')\n# as you can see that resulting columns correlation have gotten alot better with the target column trending.","a05eb90d":"fdf\n# our resulting data look like this, we dropped the ss_f column because the our label column was originated from\n# sum_stats so it's not right add such column to avoid overfitting since it was completely correlated ","9ce4f621":"X2=fdf[['music_f' ,'likes_f' ,'dislikes_f' ,'cc_f']]\ny2=fdf['trending']\n# , 'views_f'\n# we did not use the views_f column because it was causing the models to overfitt, we aim to create a more \n# generalized data for our models \n# separating the target or dependent column from the independent columns or features ","4ef86353":"X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.3,random_state=42)\n# splitting the data to xtrain , xtest and ytrain , ytest","41b46a4d":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n# training the Logistics regression model","922256cc":"lr_pred = logreg.predict(X_test)\naccuracy_score(lr_pred, y_test) * 100\n# predicting the trend and calculating the accuracy","59715e11":"cv_scores = cross_val_score(logreg,X2,y2,cv=5)\nnp.mean(cv_scores)*100\n# calculating cross validation score of the dataset","e7b38e57":"confusion_matrix(y_test,lr_pred)\n# calculating confusion matrix ","8408733b":"print(classification_report(y_test,lr_pred))\n# complete performance metric report of the logistic regression","fb51b685":"lr = pd.DataFrame({'Actual': y_test, 'Predicted': lr_pred})\ncategories=[]\nfor i in lr.index:\n    categories.append(df.iloc[i,15])\n\ncats=pd.DataFrame(categories)\ncats=cats.set_index(lr.index)\n\nlr['categories']=cats\n\nlr=lr.dropna(axis=0)\n\nlr_trend=lr.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n\npx.bar(lr_trend,barmode='group',labels={'value':'trend level'},title='Logistic Regression')\n# visualizing the predictions ","142d2a9c":"lr.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n# data which our above visualization is based on.","03695b23":"decisiontree = DecisionTreeClassifier(random_state=0)\ndep = np.arange(1,10)\nparam_grid = {'max_depth' : dep}\n\nclf_cv = GridSearchCV(decisiontree, param_grid=param_grid, cv=5)\n\nclf_cv.fit(X_train,y_train)","9a7b74c9":"dt_pred=clf_cv.predict(X_test)\n\nclf_cv.best_params_,clf_cv.best_score_*100\nprint('Best value of max_depth:',clf_cv.best_params_)\n# print('Accuracy:',clf_cv.best_score_*100)\naccuracy_score(dt_pred, y_test) * 100","43e35420":"cv_scores = cross_val_score(decisiontree,X2,y2,cv=5)\nnp.mean(cv_scores)*100","0828eb65":"confusion_matrix(y_test,dt_pred)","53b1b98b":"print(classification_report(y_test,dt_pred))","2035154b":"dt = pd.DataFrame({'Actual': y_test, 'Predicted': dt_pred})\n\n# categories=[]\n# for i in dt.index:\n#     categories.append(df.iloc[i,15])\n\ncats=pd.DataFrame(categories)\ncats=cats.set_index(dt.index)\n\ndt['categories']=cats\n\ndt=dt.dropna(axis=0)\n\ndt_trend=dt.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n\npx.bar(dt_trend,barmode='group',labels={'value':'trend level'},title='Decision Tree Classifier')","038749ae":"dt.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)","726bb41f":"random_forest = RandomForestClassifier()\nne = np.arange(1,20)\nparam_grid = {'n_estimators' : ne}\n\nrf_cv = GridSearchCV(random_forest, param_grid=param_grid, cv=5)\n\nrf_cv.fit(X_train,y_train)","f6539d50":"rf_pred=rf_cv.predict(X_test)\n\nrf_cv.best_params_,rf_cv.best_score_*100\nprint('Best value of max_depth:',rf_cv.best_params_)\naccuracy_score(rf_pred, y_test) * 100","c8debe5c":"cv_scores = cross_val_score(random_forest,X2,y2,cv=5)\nnp.mean(cv_scores)*100","4735ac21":"confusion_matrix(y_test,rf_pred)","1371803b":"print(classification_report(y_test,rf_pred))","58740f0c":"rf = pd.DataFrame({'Actual': y_test, 'Predicted': rf_pred})\n\ncats=pd.DataFrame(categories)\ncats=cats.set_index(rf.index)\n\nrf['categories']=cats\n\nrf=rf.dropna(axis=0)\n\nrf_trend=rf.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n\npx.bar(rf_trend,barmode='group',labels={'value':'trend level'},title='Random Forest Classifier')","8792fb9b":"rf.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)","41c2a514":"gbk = GradientBoostingClassifier()\nne = np.arange(1,20)\ndep = np.arange(1,10)\nparam_grid = {'n_estimators' : ne,'max_depth' : dep}\n\ngbk_cv = GridSearchCV(gbk, param_grid=param_grid, cv=5)\n\ngbk_cv.fit(X_train,y_train)","6b736845":"gbk_pred=rf_cv.predict(X_test)\n\nprint('Best value of parameters:',gbk_cv.best_params_)\nprint('Best score:',gbk_cv.best_score_*100)","d85d1f0d":"cv_scores = cross_val_score(gbk,X2,y2,cv=5)\nnp.mean(cv_scores)*100","832e2061":"confusion_matrix(y_test,gbk_pred)","86a43977":"print(classification_report(y_test,gbk_pred))","20eca5cb":"gbks = pd.DataFrame({'Actual': y_test, 'Predicted': gbk_pred})\n\ncats=pd.DataFrame(categories)\ncats=cats.set_index(gbks.index)\n\ngbks['categories']=cats\n\ngbks=gbks.dropna(axis=0)\n\ngbks_trend=gbks.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n\npx.bar(gbks_trend,barmode='group',labels={'value':'trend level'},title='Gradient Boosting Classifier')","c31f9c0a":"gbks.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)","69d7e504":"knn = KNeighborsClassifier( )\nk_range = list(range(1,10))\nweights_options = ['uniform','distance']\nk_grid = dict(n_neighbors=k_range, weights = weights_options)\ngrid = GridSearchCV(knn, k_grid, cv=10, scoring = 'precision')\ngrid.fit(X_train,y_train)","ad26a8a4":"knn_pred=grid.predict(X_test)\nprint('Best value of parameters:',grid.best_params_)\nprint('Best score:',grid.best_score_*100)","c80974b9":"cv_scores = cross_val_score(knn,X2,y2,cv=5)\nnp.mean(cv_scores)*100","5d00bd79":"confusion_matrix(y_test,knn_pred)","3a3c3b6f":"print(classification_report(y_test,knn_pred))","80e10ed6":"knns = pd.DataFrame({'Actual': y_test, 'Predicted': knn_pred})\n\ncats=pd.DataFrame(categories)\ncats=cats.set_index(knns.index)\n\nknns['categories']=cats\n\nknns=knns.dropna(axis=0)\n\nknns_trend=knns.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)\n\npx.bar(knns_trend,barmode='group',labels={'value':'trend level'},title='K Nearest Neighbor Classifier')","a1e33b95":"knns.groupby('categories')[['Predicted','Actual']].sum().sort_values(by='Predicted',ascending=False)","ea28afcd":"Decision Tree Classifier","9fda2509":"Gradient boosting classfier","300bbe57":"1. Music , Entertainment and comedy have the highest likes, dislikes comment_count\n\n2. Music , Entertainment and Film & Animation have the highest views.\n\n3. trending videos have the most views compared to non trending\n\n4. Music, Entertainment and Film & Animation are the most trending categories.","20d12472":"find the reason of which,how and why certain categories seems to be trending and why certain categories are not, because we want to predict the trending categories, explore everything, and create a column which defines whether a category is trending (1) or not (0) binary classification\n\n##########################\n\n'video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed',\n'description', 'category', 'sum_stats', 'trending'\n","52e74643":"Starting the feature engineering","7232fc0d":"K nearest neighbor","1e7a1cae":"random forest classifier","7ef922e6":"Logistic Regression"}}