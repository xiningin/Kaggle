{"cell_type":{"915cfe1c":"code","dc43cb87":"code","42acb1e9":"code","72a1395b":"code","13d9a66f":"code","0adba7a3":"code","97a8a8cf":"code","dbe11ca6":"code","6434705f":"code","bff7a135":"code","cca69e8e":"code","317b72bd":"code","966f091f":"code","3b945047":"code","0d06400d":"code","652314f7":"code","fa773a71":"code","bca97cc3":"code","d89c2fd7":"code","faea8c3a":"code","1a5c9f8c":"code","27088f1c":"code","a5d53405":"code","9291e19e":"code","a1a44f53":"code","4ff521ee":"code","759385da":"code","89ed1d64":"code","bf6ec318":"code","dd0e8a54":"markdown","66e0fd9e":"markdown","a26e02bf":"markdown","616f08f2":"markdown","f7d667a8":"markdown","6fea9346":"markdown","ca08fca7":"markdown","ea499f77":"markdown","55661b67":"markdown","87588697":"markdown","276553f3":"markdown","78478ebe":"markdown","9fc5f9f2":"markdown","2c2ad4b4":"markdown","e706c689":"markdown","ae7c4054":"markdown","40ac32cc":"markdown","fb80878f":"markdown","a93ccc8d":"markdown","841bb414":"markdown","774351c4":"markdown","a498b8b7":"markdown","d1571d07":"markdown","f69bb076":"markdown","ee6241f8":"markdown","dd45daae":"markdown"},"source":{"915cfe1c":"#load the libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\nfrom collections import Counter\nimport warnings; warnings.filterwarnings('ignore')","dc43cb87":"# library for SMOTE (SMOTE, Borderline-SMOTE, SMOTE SVM, ADASYN)\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler","42acb1e9":"#load the data as Pandas DataFrame\ndata_all = pd.read_csv('..\/input\/data.csv')","72a1395b":"data_all.head()","13d9a66f":"data_all.info()","0adba7a3":"# Let's also see class distribution\ndata_all['card_offer'].value_counts()\/data_all['card_offer'].shape[0]","97a8a8cf":"data_all.describe(include='all').T","dbe11ca6":"# heatMap function\ndef heatMap(df):\n    #Create Correlation df\n    corr = df.corr()\n    #Plot figsize\n    fig, ax = plt.subplots(figsize=(6, 4))\n    #Generate Color Map, red & blue\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n    #Generate Heat Map, allow annotations and place floats in map\n    sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\",square=True, \n        cbar_kws={'shrink':.9 }, linewidths=0.1,vmax=1.0, linecolor='white',annot_kws={'fontsize':10 })\n    #Apply xticks\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    #Apply yticks\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    #show plot\n    plt.show()","6434705f":"# Draw heatmap\nheatMap(data_all[['est_income', 'hold_bal', 'pref_cust_prob', 'imp_cscore', 'RiskScore','imp_crediteval', 'axio_score']])","bff7a135":"sns.pairplot(data_all, hue=\"card_offer\"\n             , vars=['est_income', 'hold_bal', 'pref_cust_prob', 'imp_cscore', 'RiskScore', 'axio_score'])","cca69e8e":"offered_card_prob = data_all.pref_cust_prob[data_all.card_offer]\nnot_offered_card_prob = data_all.pref_cust_prob[~data_all.card_offer]\n\nprint('Mean prob value for those who were offered a card is %.2f , minimum is %.2f and maximum is %.2f' \\\n      %(offered_card_prob.mean(), offered_card_prob.min(), offered_card_prob.max()))\nprint('Mean prob value for those who were not offered a card: %.2f , minimum is %.2f and maximum is %.2f' \\\n      %(not_offered_card_prob.mean(), not_offered_card_prob.min(), not_offered_card_prob.max()))","317b72bd":"sns.catplot(\"country_reg\",\"card_offer\", data=data_all, col=\"demographic_slice\", kind=\"bar\"\n            , aspect=0.7, height=4, palette=\"Set2\")","966f091f":"sns.catplot(\"ad_exp\", \"card_offer\", data=data_all, kind=\"bar\", aspect=1, height=3, palette=\"Set2\")","3b945047":"sns.catplot(\"card_offer\", \"est_income\", data=data_all, kind=\"violin\", inner=\"box\", aspect=1.2, height=4)","0d06400d":"sns.catplot(\"card_offer\", \"hold_bal\", data=data_all, kind=\"violin\", inner=\"box\", aspect=1.2, height=4)","652314f7":"sns.catplot(\"card_offer\", \"pref_cust_prob\", data=data_all, kind=\"violin\", inner=\"box\", aspect=1.2, height=4)","fa773a71":"# drop columns\ndata_all.drop(columns=['customer_id','imp_crediteval'], inplace=True)","bca97cc3":"# get dummies\ncategory_cols = ['demographic_slice', 'country_reg','ad_exp']\ndata_all = pd.get_dummies(columns=category_cols, data=data_all, prefix=category_cols, prefix_sep=\"_\",drop_first=True)","d89c2fd7":"# standardize - scale numerical variables (needed for algorithms like Logistic Regression)\nscalar = StandardScaler()\nnum_col_names = ['est_income', 'hold_bal', 'pref_cust_prob', 'imp_cscore', 'RiskScore', 'axio_score']\nnum_col_df = data_all[num_col_names].copy()\nscaled_num_col_df = scalar.fit_transform(num_col_df)\nfor i,j in enumerate(num_col_names):\n    data_all[j] = scaled_num_col_df[:,i]","faea8c3a":"# convet target column from boolean values to binary values\ndata_all.at[data_all[data_all['card_offer'] == False].index, 'card_offer'] = 0\ndata_all.at[data_all[data_all['card_offer'] == True].index, 'card_offer'] = 1","1a5c9f8c":"data_all.head()","27088f1c":"# Train and Test\nX,y = data_all.loc[:,data_all.columns != 'card_offer'], data_all.loc[:,data_all.columns == 'card_offer'].astype('int')\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42, stratify=data_all['card_offer'])","a5d53405":"y_train['card_offer'].unique(), X_train.shape","9291e19e":"classifier = LogisticRegression(random_state=42, solver='lbfgs')","a1a44f53":"classifier.fit(X_train, y_train)\ny_train_pred = classifier.predict(X_train)\ny_train_pos_prob = classifier.predict_proba(X_train)[:, 1]\ny_test_pred = classifier.predict(X_test)\ny_test_pos_prob = classifier.predict_proba(X_test)[:, 1]","4ff521ee":"print(\"Train AUC ROC   %.3f\" % roc_auc_score(y_train, y_train_pred))\nprint(\"Train Precision %.3f\" % precision_score(y_train, y_train_pred))\nprint(\"Train Recall    %.3f\" % recall_score(y_train, y_train_pred))\nprint(\"Train F1 Score  %.3f\" % f1_score(y_train, y_train_pred))\nprint(\"--\\n\")\nprint(\"Test AUC ROC   %.3f\" % roc_auc_score(y_test, y_test_pred))\nprint(\"Test Precision %.3f\" % precision_score(y_test, y_test_pred))\nprint(\"Test Recall    %.3f\" % recall_score(y_test, y_test_pred))\nprint(\"Test F1 Score  %.3f\" % f1_score(y_test, y_test_pred))","759385da":"# calculating False Positive Rate and True Positive Rate for plotting ROC Curve\nfpr_train, tpr_train, _ = roc_curve(y_train, y_train_pos_prob)\nfpr_test, tpr_test, _ = roc_curve(y_test, y_test_pos_prob)\n\n# calculating Precision and Recall values\np_train, r_train, _ = precision_recall_curve(y_train, y_train_pos_prob)\np_test, r_test, _ = precision_recall_curve(y_test, y_test_pos_prob)\n\n\nfig, axs = plt.subplots(2,2, sharex=True, figsize=(14,10))\n\naxs[0,0].plot(fpr_train, tpr_train, label=\"Model Output\")\naxs[0,0].plot([0,1], [0,1], linestyle='--', label='No learning')\naxs[0,0].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\naxs[0,0].set_title('Train ROC Curve')\n\naxs[1,0].plot(fpr_test, tpr_test, label=\"Model Output\")\naxs[1,0].plot([0,1], [0,1], linestyle='--', label='No learning')\naxs[1,0].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\naxs[1,0].set_title('Test ROC Curve')\n\naxs[0,1].plot(r_train, p_train)\naxs[0,1].set(xlabel='Precision', ylabel='Recall')\naxs[0,1].set_title('Train PR Curve')\n\naxs[1,1].plot(r_test, p_test)\naxs[1,1].set(xlabel='Precision', ylabel='Recall')\naxs[1,1].set_title('Test PR Curve')","89ed1d64":"# We are first oversampling minority (positive) classes and changing the ratio from 0.15:0.85 (or 1.176:0.1) to 0.3:1\nover = SMOTE(sampling_strategy=0.3, random_state=42)\n\n# We are now undersampling majority (negative) classes and changing the ratio from 0.3:1 to 0.5:1 i.e 1:2 ratio for minority and majority classes\nunder = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\nsteps = [('over', over), ('under', under), ('classifier', classifier)]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(X_train, y_train)\ny_train_pred = pipeline.predict(X_train)\ny_train_pos_prob = pipeline.predict_proba(X_train)[:, 1]\ny_test_pred = pipeline.predict(X_test)\ny_test_pos_prob = pipeline.predict_proba(X_test)[:, 1]","bf6ec318":"print(\"Train AUC ROC   %.3f\" % roc_auc_score(y_train, y_train_pred))\nprint(\"Train Precision %.3f\" % precision_score(y_train, y_train_pred))\nprint(\"Train Recall    %.3f\" % recall_score(y_train, y_train_pred))\nprint(\"Train F1 Score  %.3f\" % f1_score(y_train, y_train_pred))\nprint(\"\\n\")\nprint(\"Test AUC ROC   %.3f\" % roc_auc_score(y_test, y_test_pred))\nprint(\"Test Precision %.3f\" % precision_score(y_test, y_test_pred))\nprint(\"Test Recall    %.3f\" % recall_score(y_test, y_test_pred))\nprint(\"Test F1 Score  %.3f\" % f1_score(y_test, y_test_pred))","dd0e8a54":"Checking how _ad\\_exp_ affects card_offer","66e0fd9e":"Verify that all numerical columns are successfully scaled, we will take a peek into first five rows","a26e02bf":"Customer who were offered cards have high median _hold\\_bal_, but in this case both violin graphs looks similar.. in the sense that both have high number of customers with low hold_bal value","616f08f2":"## Final Comments\n* We have got around 4.3% improvement in ROC-AUC on Test set\n* 1.7% improvement in F1-Score\n* 10.9% improvement in Recall but 6.3% drop in precision score - There is always a trade-off between precision and recall and one increases at the cost of other. Overall, we have been able to improve F1-Score and our model and also able to improve Recall which is what we wanted for this model\n* Our ROC, F1 were already very high even before implementing SMOTE and so it didn't result in significant improvement over original but the goal of this kernel was to show how we can use SMOTE to handle imbalanced data and improve our predictions and I hope I was able to achieve that44\n\n<hr>","f7d667a8":"**Objective-**  \nTo run an online or offline marketing campaign offering customers to buy credit cards, it would be benefitical if we focus our efforts and money on customers who are more likely to purchase a card. In this kernel I am trying to build a model to predict if we should offer a credit card to the customer or not, in other words we are also trying to predict which customers are more likely to buy a credit card\n\n**Key Takeaways-**  Dealing with class imbalance, using SMOTE (Synthetic Minority Oversampling TEchnique) for under and over sampling the training dataset for increasing performance of our model\n\n\n<u><b>Contents:- <\/b><\/u>\n1. [Data Exploration](#Data-Exploration)\n2. [EDA](#Exploratory-Data-Analysis)\n3. [Data Preprocessing](#Data-Preprocessing)\n4. [Data Modeling](#Data-Modeling)\n5. [Applying SMOTE](#SMOTE)\n6. [Final Comments](#Final-Comments)","6fea9346":"Now, let's also look at some column statitics - mean, min, max values","ca08fca7":"## Exploratory Data Analysis","ea499f77":"**Let's now analyse categorical variables now**  \n\nPlotting country_reg, demographic_slice against card_offer","55661b67":"ad_exp independently seems to have no effect on card_offer values","87588697":"**<u>Observation:<\/u>**  \n**imp_crediteval** and **imp_cscore** are **highly correlated** (0.93)  \nSince these are highly correlated, let's drop one of these column - **dropping imp_crediteval**","276553f3":"Let's now draw a pair plot between 'est_income', 'hold_bal', 'pref_cust_prob', 'imp_cscore', 'RiskScore', 'axio_score' variables and see how card_offerroutcome varies as these varibales vary","78478ebe":"<img src=\"https:\/\/www.dnb.com\/content\/dam\/english\/image-library\/Modernization\/scenic\/hero-bullseye.jpg\" width=1200\/>\n<p style=\"text-align:center\"><i>I do not own the image, image url - https:\/\/images.app.goo.gl\/WjixS2yDZuvrHRCk9<\/i><\/p>","9fc5f9f2":"## Data Modeling","2c2ad4b4":"We do not have definitions for all the columns here but below is my understanding-\n\n**customer_id** - unique indentifier for a customer  \n**demographic_slice** - based on attributes like geogrophy, age, education, occupation, income etc. customer are segmenented into different demographies, I'll assume this field indicates the same  \n**country_reg** - region where the customer lives  \n**ad_exp** - could indicate whether the customer saw the advertisement or whether customer's advertisement experience was positive or negative  \n**est_income** - estimated income of a customer, looks like a monthly income  \n**hold_bal** - holding balance in the account? Not sure about this one as there are negative values too in the data and the maximum value is only ~81, so may be this is pre-scaled or column means entirely something else  \n**pref_cust_prob** - looks like a probability value  \n**imp_cscore** - seems like credit score values  \n**RiskScore** - risk in giving credit to a customer, evaluating whether we would get the money back on time or not. Might be calculated using attributes like income, occupation, age, education, credit score or similar customer's previous credit\/load payment history with bank  \n**imp_crediteval** - look like a scaled version of imp_cscore field  \n**axio_score** - don't know what it denotes  \n**card_offer** - whether should be offered a card or not","e706c689":"**Key Observations-**  \n\n\n*  `pref_cust_prob` when used with any varibale, seems to be segregating target classes effectively\n* `pref_cust_prob` combined with `imp_cscore`, `axio_score` or `est_income` seem to be able to separate target classes linearly\n* I am not quite sure how `pref_cust_prob` is calculated or gathered in data, it is separating target classes so well and linearly so it looks like we may have a **data leakage** (more specifically **Target leakage** or **leaky predictors**) issue (it basically means that there is a field(s) in training data which is calculated\/created after target value is realized, if those fields are included while training model, we would get surprisingly high accuracy on training dataset but would fail big on real world dataset)  \nA good refresher on Data Leakage here - https:\/\/www.kaggle.com\/alexisbcook\/data-leakage\n\nLet's check `pref_cust_prob` field a little bit more  \n\n","ae7c4054":"We will split the dataset in to train and test sets , we will follow a 7:3 split  \n**Imp:-** stratify on 'card_offer' field while splitting. Since our classes are imbalanced it is important that we have equal proportion of classes in both training and test sets","40ac32cc":"<h1 align=\"center\"> Targeted Marketing<\/h1>\n","fb80878f":"There are no NULL values in dataset","a93ccc8d":"**Some Observations**  \n* Customer_Id seems to be a unique number, identifying each customer\n* demographic_slice, country_reg, ad_exp all are categorical columns with 4,2,2 unique values\n* Remaining are numerical columns, unfortunately no data dictionary is available to check for any potential outlier values\n\n\nWe should check for correlation between variables, drop highly correlated variables","841bb414":"## Data Preprocessing","774351c4":"## SMOTE\n\nOne of the approach to address imbalanced datasets is to oversample the minority class. The simplest approach could be duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the **Synthetic Minority Oversampling Technique**, or **SMOTE** for short.\n\n\n\n> *SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.  \nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.*\n\nHeare two great links for learning more about SMOTE\n1. Link to original paper - https:\/\/arxiv.org\/abs\/1106.1813\n2. Detailed blog explaining various techniques with examples - https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","a498b8b7":"Mean is `pref_cust_prob` is higher (above 0.5) for the customers who were offered a card but is low for customer who were not offered but these seems to be an overlap too - minimum and maximum values in both cases are close  \nWe would keep this column for now, but a field like this should be checked further with people who collected\/created the data to make sure we do not have potential data leakages. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.  ","d1571d07":"Median income of customers who were offered and bought card is more than the ones who were not offered and also from violin graph shape we can get a qualitative ides of that the customers who were offerd card are generally the ones with more income and customers having less income are not offered a card","f69bb076":"## Data Exploration","ee6241f8":"Classes are **imbalanced**, so we need to be careful with what metric we choose for measuring performance  \n__Accuracy__ won't be a good choice in this case as even if we mark all offers False, we would get ~85% accuracy  \n\nWe will choose **ROC AUC** (Receiver operating characteristic - Area under curve) as our performance metric. We will also check for Precision and Recall of our model.  \n\n<u>Some definitions for revision<\/u>-  \n\n**ROC** - Typically used in binary classification to study the output of a classifier. It is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\nIt is created by plotting the **true positive rate (TPR) against the false positive rate (FPR)** at various threshold settings. The true-positive rate is also known as **sensitivity**, **recall** or **probability of detection**. The false-positive rate is also known as **probability of false alarm** and can be calculated as **(1 \u2212 specificity)**\n\n\n$$True Positive Rate = \\frac{True Positives}{True Positives + False Negatives}$$  \n\n$$False Positive Rate = \\frac{False Positives}{False Positives + True Negatives}$$  \n<br>\n\n**Precision** - _What percent of positives identifications are actually correct?_  \nIn our case it would be - What percentage of customers predicted as the ones who buys a card when offered (target customers) actually buys a card\n\n$$Precision = \\frac{True Positives}{True Positives + False Positives}$$\n\n**Recall** - _What proportion of actual positives was identified correctly?_  \nWhat proportion of our target customers are we able to predict as the ones who would buy a card?  \n\n$$Recall = \\frac{True Positives}{True Positives + False Negatives}$$\n\nSo we can see that we would want a high recall (and resonably good precision too) as we would want to identify as many potential card buyers as possible  \n\nInstead of ROC-AUC, we can choose another metric - **F1 Score** (also F-score or F-measure) too for measuring model performance. It should be preferred over ROC when the classes are highly skewed (100:1)\n\n>The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall). It is also known as the S\u00f8rensen\u2013Dice coefficient or Dice similarity coefficient (DSC). -_Wikipedia_  \n\n$$F1\\, Score = \\frac{2 \\times Precision \\times Recall}{Precision \\times Recall}$$\n\n\nHere are two great sources for more understanding-  \n1. https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall  \n\n2. https:\/\/towardsdatascience.com\/precision-vs-recall-386cf9f89488#:~:text=Precision%20and%20recall%20are%20two,correctly%20classified%20by%20your%20algorithm\n\n","dd45daae":"We can see that little overlap now clearly which we noticed before, otherwise this variable very effectively segregates target classes"}}