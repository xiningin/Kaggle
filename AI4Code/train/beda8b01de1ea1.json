{"cell_type":{"93d628ba":"code","bfea3328":"code","26de7a04":"code","ac4fe1cc":"code","1dbc9843":"code","558a2dac":"code","30ef1295":"code","1212289e":"code","01269d7a":"code","1d193a7e":"code","554b7d09":"code","c436d178":"code","b00f9294":"code","9ad70bbd":"code","c0461342":"code","adfda476":"markdown","13d36b8d":"markdown","86142163":"markdown","cabdce88":"markdown","bdc2ef16":"markdown","fa1299ad":"markdown","8ebe0ab0":"markdown","f52ea64a":"markdown","3276cbec":"markdown","e508ca17":"markdown","5bcc40cc":"markdown","3b9db467":"markdown","59973a4a":"markdown"},"source":{"93d628ba":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","bfea3328":"All_SW_Scripts = \"\"\n\ndef TextToString(txt):\n    with open (txt, \"r\") as file:\n        data=file.readlines()\n        script = \"\"\n        for x in data[1:-1]:\n            x = x.lower().replace('\"','').replace(\"\\n\",\" \\n \").split(' ')\n            x[1] += \":\"\n            script += \" \".join(x[1:-1]).replace(\"\\n\",\" \\n \")\n        return script\n    \nAll_SW_Scripts += TextToString(\"..\/input\/SW_EpisodeIV.txt\")\nAll_SW_Scripts += TextToString(\"..\/input\/SW_EpisodeV.txt\")\nAll_SW_Scripts += TextToString(\"..\/input\/SW_EpisodeVI.txt\")","26de7a04":"print(All_SW_Scripts[:1000])","ac4fe1cc":"text_file = open(\"All_SW_Scripts.txt\", \"w\")\ntext_file.write(All_SW_Scripts)\ntext_file.close()","1dbc9843":"Text_Data = All_SW_Scripts\n\ncharindex = list(set(Text_Data))\ncharindex.sort() \nprint(charindex)\n\nnp.save(\"charindex.npy\", charindex)\n\nprint(len(Text_Data))","558a2dac":"%%time\nCHARS_SIZE = len(charindex)\nSEQUENCE_LENGTH = 100\nX_train = []\nY_train = []\nfor i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): \n    X = Text_Data[i:i + SEQUENCE_LENGTH]\n    Y = Text_Data[i + SEQUENCE_LENGTH]\n    X_train.append([charindex.index(x) for x in X])\n    Y_train.append(charindex.index(Y))\n\nX_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n\nY_train = np_utils.to_categorical(Y_train)","30ef1295":"def get_model():\n    model = Sequential()\n    inp = Input(shape=(SEQUENCE_LENGTH, ))\n    x = Embedding(CHARS_SIZE, 100, trainable=False)(inp)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512,)(x)\n    x = Dense(256, activation=\"elu\")(x)\n    x = Dense(128, activation=\"elu\")(x)\n    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=0.001),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","1212289e":"filepath=\"model_checkpoint.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=1)","01269d7a":"class TextSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        pattern = X_train[700]\n        outp = []\n        seed = [charindex[x] for x in pattern]\n        sample = 'TextSample:' +''.join(seed)+'|'\n        for t in range(100):\n          x = np.reshape(pattern, (1, len(pattern)))\n          pred = self.model.predict(x)\n          result = np.argmax(pred)\n          outp.append(result)\n          pattern = np.append(pattern,result)\n          pattern = pattern[1:len(pattern)]\n        outp = [charindex[x] for x in outp]\n        outp = ''.join(outp)\n        sample += outp\n        print(sample)\n\ntextsample = TextSample()","1d193a7e":"# model = load_model(filepath)","554b7d09":"model_callbacks = [checkpoint, early, textsample]\nmodel.fit(X_train, Y_train,\n          batch_size=64,\n          epochs=40,\n          verbose=2,\n          callbacks = model_callbacks)","c436d178":"# model = load_model(filepath)\nmodel.save_weights(\"full_train_weights.hdf5\")\nmodel.save(\"full_train_model.hdf5\")","b00f9294":"%%time\nTEXT_LENGTH  = 5000\nLOOPBREAKER = 8\n\n\nx = np.random.randint(0, len(X_train)-1)\npattern = X_train[x]\noutp = []\nfor t in range(TEXT_LENGTH):\n  if t % 500 == 0:\n    print(\"%\"+str((t\/TEXT_LENGTH)*100)+\" done\")\n  \n  x = np.reshape(pattern, (1, len(pattern)))\n  pred = model.predict(x, verbose=0)\n  result = np.argmax(pred)\n  outp.append(result)\n  pattern = np.append(pattern,result)\n  pattern = pattern[1:len(pattern)]\n  ####loopbreaker####\n  if t % LOOPBREAKER == 0:\n    pattern[np.random.randint(0, len(pattern)-10)] = np.random.randint(0, len(charindex)-1)","9ad70bbd":"outp = [charindex[x] for x in outp]\noutp = ''.join(outp)\n\nprint(outp)","c0461342":"f =  open(\"SW_text_sample.txt\",\"w\")\nf.write(outp)\nf.close()","adfda476":"# Checkpoints and Custom Callback\nWe will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results.","13d36b8d":"# Prep the Text for the RNN\nNext we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. The length of the 3 movie scripts is less than I like for text generation, around ~170,000 characters. I feel like half a million to a million is ideal. However, this will probably make a quicker and easier to train model, but just one with somewhat less variety and might be prone to looping or even overfitting in the end if we are not careful.\n\nNote that if you want to replace the Star Wars scripts with some other text to duplicate, here would be the place to do it. Just replace the All_SW_Scripts with any other text file and the rest of the notebook will run the same. (the bigger the better, anything ~1MB+ is great) ","86142163":"# Save the Model\nTraining is done, save it. This is also a great place to load any pretrained models before generating new text.","cabdce88":"# Preproccessing\nThe first thing that needs to be done is to preprocess the scripts and combine the them into one long string. Removing all capitalization, quotes, and line numbering while keeping other punctuation and new lines for format.","bdc2ef16":"Now that we have our scripts, let's save it and move on to the real work.","fa1299ad":"# Save Text Output","8ebe0ab0":"\n# Train Model\nEven with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough for decent outputs. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way possible. However, it should take an unrealistically long time to get to that point (or just impossible).\n\nSince the Star Wars scripts are so much smaller (around 1\/6th) than the size of Monty Python's scripts, lets lower the batch size and set the total epochs much higher to make full use of the 6 hour time limit.\n","f52ea64a":"# Create the Model\nThe model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and\/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they can work roughly twice as fast as regular LSTMs layers if conditions are right.\n","3276cbec":"\n# Load Model\nLoad models or weights here. For following up on partially trained models saved by checkpoint.","e508ca17":"# Let's See the Results\nAs you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines seem really plausible as Star Wars dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. More AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days. This is why stuff like Shakespeare and poetry are popular by AI generation, abstract language makes imperfect text generation less detectable. \n\nSince there is less data to go through, the lines are often pretty close to just copying Star Wars lines. Star Wars uses also very direct language and is driven by plot. Adding more scripts from the prequels and sequels should allow for better variety and more unique banter.  Lowering the loopbreaker number can bandage this problem at the cost of less coherent speech and format. Still, the direct plot nature of Star Wars can only go so far with this type of text generation.\n\n","5bcc40cc":"# Create Sequences\nIn a nutshell, this model will look at the last 100 characters in the script and attempt to predict the 101st one. Our X variable will be a 100 character sequence and our Y variable will be the 101st character. This block chops the text data into such sequences of characters. \n\nNote that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is good to save a copy of the charindex with your model, just in case. We will need it to decode our predictions later.\n","3b9db467":"# Making New AI Generated Star Wars Scripts\n(This notebook is a fork of my much better named \"Pythonic Python Script for Making Monty Python Scripts\" kernal. I wanted to work with the Star Wars scripts for another code project so I'll start with the preprocessing work and use it here as well. After the Preprocessing section, this notebook is mostly the same. Check it out here: https:\/\/www.kaggle.com\/valkling\/pythonicpythonscript4makingmontypythonscripts)\n\nThis notebook is a compressed version of my text generating AI. Text generator like this one require a lot of computational power so it only became really feasible to do them on Kaggle Kernels when they upgraded to have a GPU and a 6 hour computational limit. Even so, 6 hours is still kind of lean for an LSTM text generator but we can make it work quite well anyways.\n\nThe goal of this notebook is to serve as a introduction to text generation NLPs. These LSTM text generator are actually not that difficult to make. However, most tutorials on the topic are incomplete and\/or generate poor results. I'll try to talk about every step of the process thoroughly and clearly. Other than that, this notebook is pretty easy to adapt to any text generation you might want to do. Just pop in any sizeable txt file and the model will learn to make more text in that style. Things like Shakespeare are common and work well for this type of text generation. Make sure that GPU is enabled in settings. Now lets make an AI generate something completely different.\n\n## Imports\nAs always, a block of imports.\n","59973a4a":"# Generating New Monty Python Scripts\nThis block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.\n\nPretty much this text generator *tries* to accurately duplicate the Star Wars script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Star Wars *like* scripts. We could actually interperate these prediction errors as as a form of creativity on the part of the computer, with loss values equating to the level of abstraction from the original.\n\n## The Loopbreaker\nThis is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.\n\nChanging this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.\n"}}