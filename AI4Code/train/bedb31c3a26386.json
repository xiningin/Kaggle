{"cell_type":{"8cb5840f":"code","c1cbb5ef":"code","a0317afc":"code","c90d48d6":"code","23ee783f":"code","881a6e1b":"code","e8332e15":"code","12f5b036":"code","240f7a00":"code","23f39be5":"code","39a46642":"code","3c4ab878":"code","e730d39f":"code","bc57d3d8":"markdown"},"source":{"8cb5840f":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader","c1cbb5ef":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","a0317afc":"\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","c90d48d6":"cols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\nskf = StratifiedKFold(n_splits=5, random_state=42)","23ee783f":"cols.remove('wheezy-copper-turtle-magic')\n","881a6e1b":"class IGClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #self.inference = False\n        self.activation = nn.ELU(inplace=True)\n        self.fc1 = nn.Linear(255, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 1)\n        self.drop = nn.Dropout(0.2)\n    def forward(self, x):\n        data = x\n        out = self.drop(self.bn1(self.activation(self.fc1(data))))\n        out = self.drop(self.bn2(self.activation(self.fc2(out))))\n        out = self.fc3(out)\n        return out","e8332e15":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","12f5b036":"from torch.optim.optimizer import Optimizer\n","240f7a00":"from tqdm import tqdm_notebook","23f39be5":"debug =False\n","39a46642":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\nbatch_size = 2048\nn_epochs = 50\n# BUILD 512 SEPARATE MODELS\nfor i in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n    skf = StratifiedKFold(n_splits=20, random_state=42)\n    x_test_cuda = torch.tensor(test2[cols].values, dtype=torch.float).cuda()\n    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_cuda), batch_size=batch_size, shuffle=False)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        x_train_fold = torch.tensor(train2.loc[train_index][cols].values, dtype=torch.float).cuda()\n        y_train_fold = torch.tensor(train2.loc[train_index]['target'].values[:, np.newaxis], dtype=torch.float32).cuda()\n        x_val_fold = torch.tensor(train2.loc[test_index][cols].values, dtype=torch.float).cuda()\n        y_val_fold = torch.tensor(train2.loc[test_index]['target'].values[:, np.newaxis], dtype=torch.float32).cuda()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        model = IGClassifier()\n        model.cuda()\n        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005,weight_decay=1e-5) # Using Adam optimizer\n        step_size, base_lr, max_lr = 10, 0.0005, 0.0008  \n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,step_size=step_size, mode='exp_range',gamma=0.99994)\n        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_fold, y_train_fold), batch_size=batch_size, shuffle=True)\n        valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_val_fold, y_val_fold), batch_size=batch_size, shuffle=False)\n        avg_losses_f = []\n        avg_val_losses_f = []\n        best_auc = 0\n        earlystop = 0\n        test_preds_fold = np.zeros((len(test2)))\n        for epoch in range(n_epochs):\n            model.train()\n            avg_loss = 0.\n            for i, (x_batch, y_batch) in enumerate(train_loader):\n                y_pred = model(x_batch)\n                if scheduler:\n                    scheduler.batch_step()\n                loss = loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item()\/len(train_loader)\n            model.eval()\n            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n            avg_val_loss = 0.\n            for i, (x_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(x_batch).detach()\n                #avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n                valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            auc = roc_auc_score(train2.loc[test_index]['target'],valid_preds_fold)\n            if auc >= best_auc:\n                best_auc = auc\n                torch.save(model.state_dict(), 'params.pkl')\n            else:\n                earlystop += 1\n                if debug:\n                    print(\"epoch{}, auc{}\".format(epoch,auc))\n                if earlystop >=10:\n                    break\n        avg_losses_f.append(avg_loss)\n        avg_val_losses_f.append(avg_val_loss)\n        model.load_state_dict(torch.load('params.pkl'))\n        model.eval()\n        for i, (x_batch,) in enumerate(test_loader):\n            y_pred = model(x_batch).detach()\n            test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        #print(roc_auc_score(train2.loc[test_index]['target'],valid_preds_fold))\n        #print(test_preds_fold)\n        oof[idx1[test_index]] += valid_preds_fold#clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += test_preds_fold \/ 20 #clf.predict_proba(test2[cols])[:,1] \/ 25.0        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('NN with interactions scores CV =',round(auc,5))","3c4ab878":"auc = roc_auc_score(train['target'],oof)\nprint('NN with interactions scores CV =',round(auc,5))","e730d39f":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","bc57d3d8":"# Submit Predictions"}}