{"cell_type":{"aba957ba":"code","0a77f32a":"code","3a02ba24":"code","7c99d24c":"code","c61127f8":"code","7211f7dc":"code","2633c15c":"markdown","6fc6fc78":"markdown","a51f23e4":"markdown","3b936190":"markdown","4a14b4ad":"markdown","dcbbe1dd":"markdown","71c07e9a":"markdown","30e87ab2":"markdown"},"source":{"aba957ba":"import pandas as pd\nfrom sklearn import metrics","0a77f32a":"pred = pd.read_csv(\"..\/input\/bert-baseline\/predictions.csv\")\ndf = pd.read_csv(\"..\/input\/bert-baseline\/test.csv\")\ndf['prediction'] = pred[' Toxic']\ndf['target'] = df['target'] >= 0.5\ndf['bool_pred'] = df['prediction'] >= 0.5","3a02ba24":"def auc(df):\n    y = df['target']\n    pred = df['prediction']\n    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n    return metrics.auc(fpr, tpr)\n\noverall = auc(df)\noverall","7c99d24c":"groups = ['black', 'white', 'male', 'female',\n          'christian', 'jewish', 'muslim',\n          'psychiatric_or_mental_illness',\n          'homosexual_gay_or_lesbian']\n\ncategories = pd.DataFrame(columns = ['SUB', 'BPSN', 'BNSP'], index = groups)","c61127f8":"import numpy as np\ndef Mp(data, p=-5.0):\n    return np.average(data ** p) ** (1\/p)\n\nfor group in groups:\n    df[group] = df[group] >= 0.5\n    categories.loc[group,'SUB'] = auc(df[df[group]])\n    bpsn = ((~df[group] & df['target'])    #background positive\n            | (df[group] & ~df['target'])) #subgroup negative\n    categories.loc[group,'BPSN'] = auc(df[bpsn])\n    bnsp = ((~df[group] & ~df['target'])   #background negative\n            | (df[group] & df['target']))  #subgrooup positive\n    categories.loc[group,'BNSP'] = auc(df[bnsp])\n\ncategories.loc['Mp',:] = categories.apply(Mp, axis= 0)\ncategories","7211f7dc":"leaderboard = (np.sum(categories.loc['Mp',:]) + overall) \/ 4\nleaderboard","2633c15c":"A random sample of 100k examples was taken from the training set provided for this competition. This was split into a dev set of 50k and `test.csv` in the code below. The `predictions.csv` file here is based predictions from the BERT model described above.","6fc6fc78":"Only 9 subgroups have enough examples in the competition test set to be included in the bias calculation. Other subgroups are ignored as part of the background for this competition.","a51f23e4":" This same vanilla BERT model gave me an actual score of `0.9223` on the competition leader board. Sy my randomly sampled 50k test set must have easier examples in the sub-groups than the official test set of 97k examples.","3b936190":"## Analysis of subgroup AUCs from a BERT Baseline\n\nThe results used for this analysis are based on a straightforward fine-tuning of BERT, using `run_classifier.py`, available [here](https:\/\/github.com\/google-research\/bert). I used a sequence length of 128 tokens and trained for 1 epoch.\n\nThe analysis clearly shows that I need more civil (non-toxic) examples mentioning from the groups `Muslim` and `homosexual_gay_or_lesbian` to improve training.\n","4a14b4ad":"First, a simple AUC function taking a dataframe as input and using sklearn for the calculation.","dcbbe1dd":"Finally, the final leader-board score.","71c07e9a":"\nThe Mp function below is based on the [evaluation formula](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview\/evaluation) this competition. This formula is then applied to the 9 subgroups and their BPSN and BNSP datasets.\n\n* **SUB**\n: Claculates an AUC using only examples from the sub-group.\n* **BPSN**\n: Background Positive Subgroup Negative. Claculates an AUC using a subset of toxic comments outside the sub-group (BP) and non-toxic comments in the sub-group (SN).\n* **BNSP**\n: Background Negative Subgroup Positive. Claculates an AUC using a subset of non-toxic comments outside the sub-group (BN) and toxic comments in the sub-group (SP).","30e87ab2":"Interesting results. Clearly I need to modify my input pipeline to include more subgroup examples, especially `homosexual` and `Muslim` examples from the BPSN subset. In other words, I need more civil (non-toxic) examples mentioning Muslims and homosexuals to improve the leader board score."}}