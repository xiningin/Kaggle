{"cell_type":{"fcca7e54":"code","0ae796b6":"code","059d8889":"code","5043879e":"code","05c44fd5":"code","f289ef71":"code","9a59b303":"code","ec98238f":"code","59bf63fa":"code","06dc6cf3":"code","3bf43aa6":"code","edec7a03":"code","8aac54a2":"code","692290fc":"code","7f48c50d":"code","fdc4cb19":"code","28166ea9":"code","02064230":"code","cbdcaa5f":"code","b22d08d9":"code","0ad7e827":"code","a44f7136":"code","043598c1":"code","d39e03c4":"code","1e70fbb5":"code","e348bad7":"code","035f880c":"code","a508c984":"code","c0d25eac":"code","feba8a4e":"markdown","ca8fdc55":"markdown","bd3b8e48":"markdown","ff473857":"markdown","53a686ff":"markdown","b49e4fc6":"markdown","2243cfff":"markdown","5346cc10":"markdown","f49ee790":"markdown","48fe304c":"markdown"},"source":{"fcca7e54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk #for natural language processing\nimport string\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport emoji\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport collections\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0ae796b6":"train_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")\n","059d8889":"#To check first five rows\ntrain_df.head()\ntest_df.head()","5043879e":"#To check type of columns\ntrain_df.info()","05c44fd5":"#To check size of data\nprint(train_df.shape)\nprint(\"training set has 3339 rows and 12 columns\\n \")\n#To check no. of null values\nprint(train_df.isnull().sum())\nprint(\"\\nFollowing columns have null values more than 1:-\\r\\n1. negativereason\\r\\n2. negativereason_confidence\\r\\n3. tweet_created\\r\\n4. tweet_location\\r\\n5. usertimezone \")","f289ef71":"#To count the no. of negative , positive and neutral tweets in training data\nmood_count=train_df[\"airline_sentiment\"].value_counts()\nmood_count","9a59b303":"# To plot the abouve stats\nplt.bar([\"Negative\",\"Neutral\",'Positive'],mood_count)\nplt.xlabel(\"Mood\")\nplt.ylabel(\"Mood_Count\")\nplt.xticks(rotation=45)\nplt.title(\"Count of Moods\")","ec98238f":"# To find the count of tweets for different airlines\ntrain_df[\"airline\"].value_counts()\n","59bf63fa":"# To plot the sentiment count airline wise\ndef plot_airline_wise_sentiments(Airline):\n    df=train_df[train_df[\"airline\"]==Airline]\n    count=df[\"airline_sentiment\"].value_counts()\n    plt.bar([\"Negative\",\"Neutral\",\"Positive\"],count)\n    plt.xlabel(\"Moods\")\n    plt.ylabel(\"Moood_Counts\")\n    plt.title(\"Mood counts for {}\".format(Airline))\nplt.figure(1,figsize=(20,12))\nplt.subplot(231)\nplot_airline_wise_sentiments(\"United\")\nplt.subplot(232)\nplot_airline_wise_sentiments(\"Virgin America\")\n\n","06dc6cf3":"# To get the count of negative reasons . Here dict function is used to create dictionary.\nNR_count=dict(train_df[\"negativereason\"].value_counts())\nprint(NR_count)\n","3bf43aa6":"# To get airline wise count of negative reasons\ndef NR_count(Airline):\n    if(Airline==\"All\"):\n        df=train_df\n    else:\n        df=train_df[train_df[\"airline\"]==Airline]\n    count=dict(df[\"negativereason\"].value_counts())\n    unique_reason=list(train_df[\"negativereason\"].unique())\n    unique_reason=[x for x in unique_reason if str(x)!='nan'] # To remove none values\n    print(type(unique_reason))\n    reason_frame=pd.DataFrame({'Reasons':unique_reason})\n    reason_frame['Count']=reason_frame['Reasons'].apply(lambda x:count[x])\n    return reason_frame\n    ","edec7a03":"# To plot airline wise count of negative reason\ndef plot_reason(Airline):\n    df=NR_count(Airline)\n    index=df[\"Reasons\"]\n    plt.figure(figsize=(12,12))\n    plt.bar(index,df[\"Count\"])\n    plt.xticks(rotation=45)\n    plt.tick_params(top='off', bottom='on', left='off', right='off', labelleft='on', labelbottom='on')\n    plt.xlabel(\"Negative Reasons\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Negative reason count for \"+Airline)\n\nplot_reason(\"All\")","8aac54a2":"plot_reason(\"United\")","692290fc":"plot_reason(\"Virgin America\")","7f48c50d":"# Functions to remove unnecesary words, symbols from text .\ndef remove_mentions(input_text): # To remove @....\n    return re.sub(r'@\\w+','',str(input_text))\ndef remove_urls(input_text): # To remove http.......\n    return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', str(input_text))\ndef emoji_oneword(input_text): # To remove emojis\n    return input_text.replace('_','')\ndef remove_punctuation(input_text): # To remove punctuations(, . ! ') \n    punct=string.punctuation # punct now has all the punctuations used in english \n    trantab=str.maketrans(punct,len(punct)*' ') #every punctuation in punct will be mapped to ' ' and stored in trantab in a table\n    return  input_text.translate(trantab) # Here punctuations in text will be replaced by ' ' as defined in trantab.\ndef remove_digits(input_text): # To remove digits \n     return re.sub(r'\\d+','',str(input_text))\ndef to_lower(input_text): # To convert each word in lower case\n     return input_text.lower()\ndef remove_stopWords(input_text): # To remove stop words like the, is , in ,not.\n    stopwords_list = stopwords.words('english')\n    whitelist=[\"n't\",\"no\",\"not\"] # Some words which might indicate a certain sentiment are kept via a whitelist\n    words=input_text.split() # By default it will split the words by ' '\n    clean_words=[word for word in words if(word not in stopwords_list or word in whitelist) and len(word)>1]\n    return \" \".join(clean_words)\ndef stemming(input_text): # stemming means getting word to its original form eg: Difficulty -> Difficult\n    porter=PorterStemmer()\n    words=input_text.split()\n    stemmed_words = [porter.stem(word) for word in words]\n    return \" \".join(stemmed_words)\n ","fdc4cb19":"pd.options.mode.chained_assignment = None  # default='warn' # To hide warnings\ndf=train_df[train_df[\"airline_sentiment\"]==\"negative\"]\ndf[\"text\"]=df[\"text\"].fillna(\"No Value Found\")\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_mentions(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_urls(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji_oneword(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_punctuation(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_digits(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: to_lower(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_stopWords(x))","28166ea9":"# To create Word Cloud of most frequent negative words\nwords=' '.join( x for x in str(df.text.values).split())             \nfrom wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\nd = os.path.dirname(\"..\/input\/\")\nplane_coloring = np.array(Image.open(os.path.join(d, \"Airplane_Transparent_PNG_Clipart.png\")))\n\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=3000,\n                      height=2500,\n                      colormap=\"Blues\",\n                      max_words=15,\n                      mask=plane_coloring\n                     ).generate(words)\nimage_colors = ImageColorGenerator(plane_coloring)\n","02064230":"plt.figure(1,figsize=(25,25))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n#plt.imshow(plane_coloring, cmap=plt.cm.gray, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","cbdcaa5f":"# Function to get meaningful words from training file\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet)) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))","b22d08d9":"# Function to get length of meaningful words from training file\ndef clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet))\n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words))","0ad7e827":"# Changing the sentiment column values in numerical categorical form as we will only predict whether the sentiment is positive or negative, considering neutral sentiments as positive\ntrain_df['sentiment']=train_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n#test_df['sentiment']=test_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n","a44f7136":"#train_df['clean_tweet']=train_df['text'].apply(lambda x: tweet_to_words(x))\ntrain_df[\"text\"]=train_df[\"text\"].fillna(\"No Value Found\")\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_mentions(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_urls(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_digits(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: to_lower(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntrain_df['clean_tweet']=train_df['text']\ntrain_df['Tweet_length']=train_df['text'].apply(lambda x: clean_tweet_length(x))\ntrain,test = train_test_split(train_df,test_size=0.2,random_state=42)\n# for original test data to be used later\n#test_df['clean_tweet']=test_df['text'].apply(lambda x: tweet_to_words(x))\ntest_df[\"text\"]=test_df[\"text\"].fillna(\"No Value Found\")\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_mentions(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_urls(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_digits(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: to_lower(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntest_df['clean_tweet']=test_df['text']\ntest_df['Tweet_length']=test_df['text'].apply(lambda x: clean_tweet_length(x))","043598c1":"# Creating train and test clean tweet words data\ntrain_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)\n    \n# for original train and test data to be used later\ntrain_original_clean_tweet=[]\nfor tweet in train_df['clean_tweet']:\n    train_original_clean_tweet.append(tweet)\ntest_original_clean_tweet=[]\nfor tweet in test_df['clean_tweet']:\n    test_original_clean_tweet.append(tweet)\n","d39e03c4":"from sklearn.feature_extraction.text import CountVectorizer\n#v = CountVectorizer(analyzer = \"word\")\nv = TfidfVectorizer(analyzer=\"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\n#print(train_features)\nword_freq = dict(zip(v.get_feature_names(), np.asarray(train_features.sum(axis=0)).ravel())) #zip function is used for mapping values in different lists\n#print(train_features.sum(axis=0))\n#print(word_freq)\nword_counter = collections.Counter(word_freq)\n#print(word_counter)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.xticks(rotation=45)\nplt.show();\ntest_features=v.transform(test_clean_tweet)\n\n# for original train and test data to be used later\ntrain_original_features= v.fit_transform(train_original_clean_tweet)\ntest_original_features=v.transform(test_original_clean_tweet)","1e70fbb5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score","e348bad7":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True,gamma='auto'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]","035f880c":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    \n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))","a508c984":"Index = [1,2,3,4,5,6,7]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","c0d25eac":"dense_original_features=train_original_features.toarray()\ndense_original_test= test_original_features.toarray()\nindex=Accuracy.index(max(Accuracy))\nclassifier=Classifiers[index]\ntry:\n    fit = classifier.fit(train_original_features,train_df['sentiment'])\n    pred = fit.predict(test_original_features)\nexcept Exception:\n    fit = classifier.fit(dense_original_features,train_df['sentiment'])\n    pred = fit.predict(dense_original_test)\npred=pred.astype(object)\npred[pred==0]='neutral'\npred[pred==-1]='negative'\npred[pred==1]='positive'\nd={\"tweet_id\":test_df.tweet_id,\"airline_sentiment\":pred}\nsubmission=pd.DataFrame(d)\nsubmission.to_csv('submission.csv', index=False)","feba8a4e":"# Importing Libraries","ca8fdc55":"# General Analysis","bd3b8e48":"* From Graphs , its clear that United airlines has much more negative tweets and almost same no. of neutral and positive tweets while for Virgin      America, sentiments are somewhat balanced","ff473857":" # References\n \n ## Text Analytics Reference\n* http:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/the-different-methods-deal-text-data-predictive-python\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/\n* https:\/\/www.kaggle.com\/bertcarremans\/predicting-sentiment-with-text-features\n* https:\/\/www.kaggle.com\/adamschroeder\/countvectorizer-tfidfvectorizer-predict-comments\n\n## Predictive Model Reference\n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/  (KNN Algorithm)\n* https:\/\/www.analyticsvidhya.com\/blog\/2015\/05\/boosting-algorithms-simplified\/ (AdaBoost)\n","53a686ff":"# Feature Engineering","b49e4fc6":"* The main objective here is to determine whether the  tweet is negative or positive .\n* airline_sentiment column shows the type of tweet : Negative, Positive or Neutral\n* negativereason column shows the overall reason for a negative tweet. Its not applicable for Positive or Neutral tweets\n* airline column shows the name of the airline as a particular airline may have more negative tweets than other . We will look into that.\n* text column is the actual tweet which contains words deciding whether the tweet is negative or positive.\n* Rest of the columns have not been used in this notebook as of now.\n","2243cfff":"* Most of the tweets are negative which makes sense as people tweet mostly when they had some issues with the flight.","5346cc10":"* United airlines have most no. of tweets.","f49ee790":"* Using the best model","48fe304c":"# Importing Files"}}