{"cell_type":{"57d63339":"code","7ef93340":"code","398f7dfe":"code","f631c6c9":"code","c5af4660":"code","209ac71f":"code","ef7ca02a":"code","9d1a04f2":"code","a141a24f":"code","018d4638":"code","df6d4418":"code","a0a749b6":"code","f134dd00":"code","2a365249":"code","160fcf51":"code","8a0bd9c3":"markdown","c91d1bb1":"markdown","f767b810":"markdown","b2952ae2":"markdown","c92ba007":"markdown","fbe118fe":"markdown","5fe4aee1":"markdown","2051e707":"markdown","2a23f690":"markdown","138f71b0":"markdown"},"source":{"57d63339":"import io\n\nimport pandas as pd\nimport numpy as np\nf = pd.read_csv(\"..\/input\/home-insurance\/Model_Data_kg.csv\")\nf","7ef93340":"columns = f[['P1_EMP_STATUS','P1_MAR_STATUS','P1_SEX','OCC_STATUS','P1_PT_EMP_STATUS','CLAIM3YEARS','APPR_LOCKS','PAYMENT_METHOD','HP2_ADDON_PRE_REN','HP2_ADDON_POST_REN']]\n#One-hot \nfor col in columns:\n  for t in f[col].unique():\n    f[col + \"_is_\" + t] = [int(i==t) for i in f[col]]\n  f = f.drop(col,axis=1)\n\nN = f.replace('N', '0')\nY = N.replace('Y', '1')\nX = Y.drop(columns=['Unnamed: 0',\"X\",\"QUOTE_DATE\",\"COVER_START\",\"P1_DOB\",\"MTA_FLAG\",\"MTA_DATE\",\n                     \"Police\",\"UNSPEC_HRP_PREM\",\"ROOF_CONSTRUCTION\",\"WALL_CONSTRUCTION\",\"LISTED\",\"OWNERSHIP_TYPE\",\n                     \"BEDROOMS\",\"PROP_TYPE\",\"YEARBUILT\",\"POL_STATUS\",\"Resiliated\",\"RISK_RATED_AREA_B\",\n                      \"NCD_GRANTED_YEARS_B\",\"RISK_RATED_AREA_C\",\"NCD_GRANTED_YEARS_C\",\"SPEC_ITEM_PREM\"])\n\n#\"LAST_ANN_PREM_GROSS\",\"SUM_INSURED_CONTENTS\",\"SPEC_SUM_INSURED\",\"MAX_DAYS_UNOCC\",\"MTA_FAP\",\"MTA_APRP\"\ny = f['Resiliated']\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=2)\n","398f7dfe":"X_train","f631c6c9":"#K-Nearest Neighbors Algorithm , KNN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","c5af4660":"\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as pyplot\n\n\ndef AUC_PLOT(X_test,y_test,model):\n\n    # generate a no skill prediction (majority class)\n    ns_probs = [0 for _ in range(len(y_test))]\n\n    # predict probabilities\n    lr_probs = model.predict_proba(X_test)\n    # keep probabilities for the positive outcome only\n    lr_probs = lr_probs[:, 1]\n    # calculate scores\n    ns_auc = roc_auc_score(y_test, ns_probs)\n    lr_auc = roc_auc_score(y_test, lr_probs)\n    # summarize scores\n    NSAUC='AUC=%.3f' % (ns_auc)\n    ROAUC='AUC=%.3f' % (lr_auc)\n    #print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    #print('KNN: ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n    # plot the roc curve for the model\n    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label=('No Skill:'+NSAUC))\n    modname=str(model)\n    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=(modname+':'+ROAUC))\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show()\n \n    \nAUC_PLOT(X_test,y_test,classifier)","209ac71f":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\n\ny_pred = lr.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nscale = StandardScaler()\nX = scale.fit_transform(X)\n\n\nprint('ACC LogisticRegression = ',accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nAUC_PLOT(X_test,y_test,lr)","ef7ca02a":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='poly',probability=True)\nsvclassifier.fit(X_train, y_train)\n\ny_pred = svclassifier.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nAUC_PLOT(X_test,y_test,svclassifier)","9d1a04f2":"model = SVC(kernel='rbf',probability=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nAUC_PLOT(X_test,y_test,model)","a141a24f":"model = SVC(kernel='linear',probability=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nAUC_PLOT(X_test,y_test,model)","018d4638":"model = SVC(kernel='sigmoid',probability=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nAUC_PLOT(X_test,y_test,model)","df6d4418":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)\n\n\ny_pred = clf.predict(X_test)\n\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nAUC_PLOT(X_test,y_test,clf)","a0a749b6":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100,max_depth=10)\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nAUC_PLOT(X_test,y_test,rf)","f134dd00":"#Feature importance\nimport matplotlib.pyplot as pyplot\nfig, ax = pyplot.subplots(figsize=(3,2),dpi=150)\npd.Series(rf.feature_importances_, index=X.columns)\\\n  .nlargest(10)\\\n  .plot(kind='barh',ax=ax)","2a365249":"xx_train= X_train\nxx_train = xx_train.astype(int)\nxx_test = X_test\nxx_test = xx_test.astype(int)\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom numpy import sort\n\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(xx_train,y_train)\n# make predictions for test data\ny_pred = model.predict(xx_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nAUC_PLOT(X_test,y_test,model)","160fcf51":"#Feature importance\nimport matplotlib.pyplot as pyplot\nfig, ax = pyplot.subplots(figsize=(3,2),dpi=150)\npd.Series(model.feature_importances_, index=X.columns)\\\n  .nlargest(10)\\\n  .plot(kind='barh',ax=ax)","8a0bd9c3":"* SVM (sigmoid)","c91d1bb1":"* XGBOOST","f767b810":"* SVM (poly)","b2952ae2":"* Decision Tree","c92ba007":"# Transform Data","fbe118fe":"* SVM (rbf)","5fe4aee1":"* LOGISTIC REGRESSION","2051e707":"* SVM (linear)","2a23f690":"* RANDOM FOREST","138f71b0":"# MODEL\n* KNN"}}