{"cell_type":{"2fbbd4e7":"code","bdc671ed":"code","1d4f8b5e":"code","e856471b":"code","a709c088":"code","a970e133":"code","75826d37":"code","9eae799d":"code","eafd50c9":"code","e7caff0e":"code","fc5f602d":"code","1e4d34bb":"code","55cdb6f2":"code","af7c4d8b":"code","3ab189f4":"code","6a6afa07":"code","b067588a":"code","a537bd4d":"code","c2e10040":"code","1df53ad0":"code","4fec18a6":"code","8b2efc9c":"code","d787f3f6":"code","49253070":"code","034a0cb4":"code","41030a80":"code","85c4934e":"code","d6efea49":"code","1fcff2bc":"code","c2e72320":"code","176231fe":"code","64096687":"code","94e81c9a":"code","0e84925c":"code","4cca84a6":"code","443d1b9b":"code","08f9054f":"code","9f5840e9":"code","6a13c398":"code","6bcca03b":"code","18f8674c":"code","3945c74c":"code","8f1295af":"code","269dd6e7":"code","612571d6":"code","29efef30":"code","a2ed25f0":"code","6082e6b4":"code","4e8c27c0":"code","7b6fcdb6":"code","2e9bf5e4":"code","c8473b09":"code","843fbdf2":"code","953aa754":"markdown","ed28aab2":"markdown","26ffadad":"markdown","8a0073ba":"markdown","924a0934":"markdown","ad705686":"markdown","81c2239b":"markdown","27ed0155":"markdown"},"source":{"2fbbd4e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdc671ed":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport time \n\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\nfrom collections import Counter\n\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nimport cufflinks as cf\ncf.go_offline()\n","1d4f8b5e":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","e856471b":"train.head()","a709c088":"test.head()","a970e133":"train.info()","75826d37":"test.info()","9eae799d":"train.target.value_counts().iplot(kind='bar', title='The distribution of the training data based on target: a real disaster (1) or not (0)')","eafd50c9":"train['length'] = train['text'].apply(len)","e7caff0e":"train.keyword.nunique()  # Total of 221 unique keywords","fc5f602d":"train.keyword.value_counts()[:221].iplot(kind='bar', title='The distribution of keywords', color='blue')","1e4d34bb":"test.keyword.nunique()  # Total of 221 unique keywords","55cdb6f2":"test.keyword.value_counts()[:221].iplot(kind='bar', title='The distribution of keywords', color='blue')","af7c4d8b":"train.location.nunique()","3ab189f4":"train.location.value_counts()[:20].iplot(kind='bar', title='Top 20 locations in text', color='red')","6a6afa07":"test.location.nunique()","b067588a":"test.location.value_counts()[:20].iplot(kind='bar', title='Top 20 locations in text', color='red')","a537bd4d":"train.head()","c2e10040":"# convert text to lowercase\ndef lowercase_text(text):\n    return text.lower()\n\ntrain.text=train.text.apply(lambda x: lowercase_text(x))\ntest.text=test.text.apply(lambda x: lowercase_text(x))","1df53ad0":"train.head()","4fec18a6":"test.head()","8b2efc9c":"# The number of text having http link\ntrain.loc[train['text'].str.contains('http')].target.value_counts()","d787f3f6":"pattern = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\ndef remove_html(text):\n    no_html= pattern.sub('',text)\n    return no_html","49253070":"# Remove all text having http link\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))","034a0cb4":"# lets check if this clean works\ntrain.loc[train['text'].str.contains('http')].target.value_counts()","41030a80":"train.text[15:29]","85c4934e":"# remove noise(special characters)\nimport string\ndef remove_noise(text):\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain['text']=train['text'].apply(lambda x : remove_noise(x))\ntest['text']=test['text'].apply(lambda x : remove_noise(x))","d6efea49":"train.text[15:29]","1fcff2bc":"# Find the number of unique words in this text\ndef counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","c2e72320":"text_values = train[\"text\"]\ncounter = counter_word(text_values)\nprint(f\"The len of unique words is: {len(counter)}\")\nlist(counter.items())[:10]","176231fe":"# Split data: 80% training + 20% validation data\ntraining_size = 6090\n\ntraining_sentences = train.text[0:training_size]\ntraining_labels = train.target[0:training_size]\n\nvalid_sentences = train.text[training_size:]\nvalid_labels = train.target[training_size:]\n\nprint('The Shape of training ',training_sentences.shape)\nprint('The Shape of testing',valid_sentences.shape)","64096687":"# Define parameters for vectorizing a text corpus into integers based on word count.\n\n# The maximum number of words to be used. (most frequent)\nvocab_size = len(counter)\nembedding_dim = 32\n\n# Max number of words in each complaint.\nmax_length = 20\ntrunc_type='post'\npadding_type='post'\n\n# oov_took its set for words out our word index\noov_tok = \"<XXX>\"\nseq_len = 12","94e81c9a":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)","0e84925c":"word_index = tokenizer.word_index\nprint(\"THe first word Index are: \")\nfor x in list(word_index)[0:15]:\n    print (\" {},  {} \".format(x,  word_index[x]))","4cca84a6":"# vectorizing a text corpus into integers and padding 0 in the beginning of each sequence to ensure that all sequences have the same length.\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","443d1b9b":"print(train.text[10])\nprint(training_sequences[10])","08f9054f":"# decode function to convert integer values back to text sequences to check the result\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndef decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","9f5840e9":"decode(training_sequences[10])","6a13c398":"training_padded[1628]","6bcca03b":"# Similarly, vectorizing and padding for the validation data.\nvalid_sequences = tokenizer.texts_to_sequences(valid_sentences)\nvalid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","18f8674c":"from keras.regularizers import l2","3945c74c":"# Baseline Model Definition with LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import SGD\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n#     tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(100, input_dim=2, activation='relu', kernel_regularizer=l2(0.01)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # this is a binary clasification\n])\n\n#For a binary classification problem\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# sgd = SGD(learning_rate=0.1)\n# model.compile(optimizer=sgd,\n#               loss='binary_crossentropy',\n#               metrics=['accuracy'])\n\n# model.summary()\n","8f1295af":"# start training the model\nstart_time = time.time()\n\nnum_epochs = 3\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n\nfinal_time = (time.time()- start_time)\/60\nprint(f'The time in minutos: {final_time}')","269dd6e7":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()\nmodel_loss[['accuracy','val_accuracy']].plot(ylim=[0,1]);","612571d6":"predictions = model.predict_classes(valid_padded)   # prediction for the validation data","29efef30":"# Showing Confusion Matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","a2ed25f0":"# Showing Confusion Matrix\nplot_cm(valid_labels,predictions, 'Confution matrix of Tweets', figsize=(7,7))","6082e6b4":"# Similarly, vectorizing and padding for the test data to submit the result\ntesting_sequences = tokenizer.texts_to_sequences(test.text)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","4e8c27c0":"test_predictions = model.predict(testing_padded)","7b6fcdb6":"test_predictions","2e9bf5e4":"submission['target'] = (test_predictions > 0.5).astype(int)","c8473b09":"submission","843fbdf2":"submission.to_csv(\"submission3.csv\", index=False, header=True)","953aa754":"# 3. Building the model","ed28aab2":"# 1. **Data description and visualization:**","26ffadad":"# Improved Model with pre-trained model: BERT","8a0073ba":"# 5. Predict on test data and submit the result","924a0934":"# 4. Evaluating the model","ad705686":"# 2. Preprocesing:","81c2239b":"> **This first approach gives 0.753 accuracy in test data!**","27ed0155":"# Baseline Model Definition with LSTM"}}