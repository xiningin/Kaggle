{"cell_type":{"d15f3fd4":"code","9e556765":"code","b95c10e4":"code","cbf2fd62":"code","028f7dcb":"code","3cdfb320":"code","ac0833ee":"code","b6fa630f":"code","b20a2ea2":"code","7b68f516":"markdown","b22d6838":"markdown","b9cf03f4":"markdown","7e901f23":"markdown","1fb02ed8":"markdown","c4aa0c4d":"markdown","92b2a2cc":"markdown"},"source":{"d15f3fd4":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\nimport tqdm\n","9e556765":"dir = \"..\/input\/sarcastic-comments-on-reddit\/train-balanced-sarcasm.csv\"\ndata = pd.read_csv(dir)\ndata.head(5)","b95c10e4":"comments = data['comment'].values\nlabels = data['label'].values","cbf2fd62":"text_cleaning = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nstemmer = SnowballStemmer('english', ignore_stopwords=False)","028f7dcb":"def preprocess_data(text):\n    text = re.sub(text_cleaning, ' ', str(text).lower()).strip()\n    text = stemmer.stem(str(text))\n    return text\n\nX = []\nfor i in tqdm.tqdm(range(len(comments))):\n    X.append(preprocess_data(comments[i]))","3cdfb320":"tokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(X)","ac0833ee":"sequences = tokenizer.texts_to_sequences(X)\npadded = pad_sequences(sequences, padding='post', maxlen=20)","b6fa630f":"xtrain, xtest, ytrain, ytest = train_test_split(np.array(padded), np.array(labels))","b20a2ea2":"print(f\"Actual Sentence: {comments[860]}\\nStemmed Sentence: {X[860]}\\nTokenized: {sequences[860]}\\nPadded: {padded[860]}\")","7b68f516":"# Read the csv and extract the comments and the labels","b22d6838":"# Split the data into train and test sets","b9cf03f4":"# Preprocessing Text to be used in models","7e901f23":"# Tokenize the cleaned text and pad them accordingly","1fb02ed8":"# Now the data is ready to be used as inputs in a neural network","c4aa0c4d":"# Importing the required libraries","92b2a2cc":"# Cleaning the text and removing links\/ punctuation and so on.."}}