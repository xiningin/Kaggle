{"cell_type":{"e32a9c5d":"code","8d897832":"code","8909f7b9":"code","a665f805":"code","528c9351":"code","68996f4d":"code","a390ef99":"code","22428a34":"code","f05cf3ba":"code","fe4a3dc4":"code","1a2dbc9b":"code","6e2ce3a3":"code","426ec87a":"code","b33c2ef1":"code","b6d744d6":"code","cf8e3e15":"code","44e0fada":"code","20095461":"code","2cdb1cf2":"code","746329ab":"code","5ca1fd8e":"code","70538ce5":"code","d1d2dd3b":"code","c218ba09":"code","1e1c28d6":"code","33151170":"code","983aaba7":"code","fc44456f":"code","c1231bfb":"code","7bb9c4eb":"code","c1f5e23f":"code","10be64e8":"markdown","c72b62e4":"markdown","c7ec756c":"markdown","a7069d24":"markdown","abd0129a":"markdown"},"source":{"e32a9c5d":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.pipeline import make_pipeline \nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","8d897832":"df_train=pd.read_csv('..\/input\/Train.csv')\ndf_test=pd.read_csv('..\/input\/Test.csv')\ndf_ss=pd.read_csv('..\/input\/sample_submission.csv')","8909f7b9":"df_train","a665f805":"df_test","528c9351":"data= pd.concat([df_test,df_train],axis='rows')","68996f4d":"data= data.sort_values('ID')","a390ef99":"data[0:60]","22428a34":"duplicate = data[data.duplicated('ID',keep=False)] ","f05cf3ba":"duplicate","fe4a3dc4":"duplicate.isnull().sum()","1a2dbc9b":"duplicate_list_train=[]\nduplicate_list_test=[]\nfor i in range(len(df_train)):\n    if df_train['ID'].values[i]in duplicate['ID'].values:\n        duplicate_list_train.append(1)\n    else:\n        duplicate_list_train.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['ID'].values[i]in duplicate['ID'].values:\n        duplicate_list_test.append(1)\n    else:\n        duplicate_list_test.append(0)","6e2ce3a3":"np.bincount(duplicate_list_train),np.bincount(duplicate_list_test)","426ec87a":"duplicate_rows=pd.Series(duplicate_list_train,name=\"duplicate_rows\")\nduplicate_rows","b33c2ef1":"df_train=pd.concat((df_train,pd.Series(duplicate_list_train,name=\"duplicate_rows\")),axis=\"columns\")\ndf_test=pd.concat((df_test,pd.Series(duplicate_list_test,name=\"duplicate_rows\")),axis=\"columns\")","b6d744d6":"# handling missing values and categorical data \n\n\n\n#train\ndf_train['Gender']=df_train['Gender'].map({'Male':1,'Female':0})\ndf_train['Ever_Married']=df_train['Ever_Married'].fillna(df_train['Ever_Married'].mode(),axis='rows')\ndf_train[\"Graduated\"]=df_train[\"Graduated\"].fillna(df_train[\"Graduated\"].mode(),axis='rows')\ndf_train[\"Profession\"]=df_train[\"Profession\"].fillna(df_train[\"Profession\"].mode(),axis='rows')\ndf_train['Work_Experience']=df_train['Work_Experience'].fillna(df_train['Work_Experience'].median())\ndf_train['Family_Size']=df_train['Family_Size'].fillna(df_train['Family_Size'].median())\ndf_train['Var_1']=df_train['Var_1'].fillna('cat_6')\n\ndf_train['Segmentation']=df_train['Segmentation'].map({'A':0,'B':1,'C':2,'D':3})  \n\n#test\ndf_test['Gender']=df_test['Gender'].map({'Male':1,'Female':0})\ndf_test['Ever_Married']=df_test['Ever_Married'].fillna(df_train['Ever_Married'].mode(),axis='rows')\ndf_test[\"Graduated\"]=df_test[\"Graduated\"].fillna(df_train[\"Graduated\"].mode(),axis='rows')\ndf_test[\"Profession\"]=df_test[\"Profession\"].fillna(df_train[\"Profession\"].mode(),axis='rows')\ndf_test['Work_Experience']=df_test['Work_Experience'].fillna(df_test['Work_Experience'].median())\ndf_test['Family_Size']=df_test['Family_Size'].fillna(df_test['Family_Size'].median())\ndf_test['Var_1']=df_test['Var_1'].fillna('cat_6')","cf8e3e15":"#one-hotencoding \n#train\nmarried_dummies=pd.get_dummies(df_train['Ever_Married'],drop_first=True,prefix='m')\ngrad_dummies=pd.get_dummies(df_train['Graduated'],drop_first=True,prefix='grad')\nprof_dummies=pd.get_dummies(df_train['Profession'],drop_first=True,prefix='prof')\nspend_dummies=pd.get_dummies(df_train['Spending_Score'],drop_first=True)\ncat_dummies=pd.get_dummies(df_train['Var_1'],drop_first=True)\n\ntrain_dummies=pd.concat([married_dummies,grad_dummies,prof_dummies,spend_dummies,cat_dummies],copy=False,axis='columns')\n\n#test\nmarried_dummies_test=pd.get_dummies(df_test['Ever_Married'],drop_first=True,prefix='m')\ngrad_dummies_test=pd.get_dummies(df_test['Graduated'],drop_first=True,prefix='grad')\nprof_dummies_test=pd.get_dummies(df_test['Profession'],drop_first=True,prefix='prof')\nspend_dummies_test=pd.get_dummies(df_test['Spending_Score'],drop_first=True)\ncat_dummies_test=pd.get_dummies(df_test['Var_1'],drop_first=True)\n\ntest_dummies=pd.concat([married_dummies_test,grad_dummies_test,prof_dummies_test,spend_dummies_test,cat_dummies_test],copy=False,axis='columns')","44e0fada":"df_train=pd.concat([df_train,train_dummies],axis='columns',copy=False)\ndf_test=pd.concat([df_test,test_dummies],axis='columns',copy=False)\n\ndf_train.drop(['Ever_Married','Graduated','Profession','Spending_Score','Var_1'],axis='columns',inplace=True)\ndf_test.drop(['Ever_Married','Graduated','Profession','Spending_Score','Var_1'],axis='columns',inplace=True)","20095461":"X=df_train.loc[:,df_train.columns!='Segmentation']\ny=df_train.loc[:,'Segmentation']\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1010,test_size=0.15)\nX_train.shape,X_test.shape","2cdb1cf2":"inverse_map={0:'A',1:'B',2:'C',3:'D'}","746329ab":"xgc = make_pipeline(StandardScaler(),XGBClassifier(objective=\"multi:softmax\",n_jobs=-1,num_class=4,eval_metric=\"auc\",learning_rate =0.1,\n                                                         n_estimators=1000,\n                                                         max_depth=5,\n                                                         min_child_weight=1,\n                                                         gamma=0,\n                                                         subsample=0.8,\n                                                         colsample_bytree=0.8),verbose=1)\n","5ca1fd8e":"xgc.fit(X_train,y_train)\n\ny_pred_xgc=xgc.predict(X_test)\nprint(accuracy_score(y_test,y_pred_xgc))\nprint(classification_report(y_pred_xgc,y_test))","70538ce5":"xgc.fit(X,y)\ny_sub_xgc=xgc.predict(df_test)\n\nsub1=pd.DataFrame({'ID':df_ss['ID'],'Segmentation':y_sub_xgc})\nsub1['Segmentation']=sub1['Segmentation'].map(inverse_map)\nsub1.to_csv(\"xgc_sub2.csv\",index=False)\nprint(sub1.head())","d1d2dd3b":"duplicate1= duplicate[duplicate['Segmentation'].notna()]","c218ba09":"duplicate1","1e1c28d6":"df_non_duplicate=df_test.loc[df_test['duplicate_rows']==0]\ndf_non_duplicate","33151170":"y_pred_hc=xgc.predict(df_non_duplicate)","983aaba7":"len(y_pred_hc)","fc44456f":"sub2=pd.DataFrame({'ID':df_non_duplicate['ID'],\"Segmentation\":y_pred_hc})\nsub2['Segmentation']=sub2['Segmentation'].map(inverse_map)\nsub3=pd.DataFrame({'ID':duplicate1['ID'],\"Segmentation\":duplicate1['Segmentation']})","c1231bfb":"sub2,sub3","7bb9c4eb":"final_sub=pd.concat((sub2,sub3),axis=\"rows\",copy=False)\nfinal_sub.to_csv(\"hc_sub.csv\",index=False)\nprint(final_sub.head())","c1f5e23f":"final_sub","10be64e8":"# hard coding predictions","c72b62e4":"# Imports","c7ec756c":"# JANTA HACK CUSTOMER SEGMENTATION LB SCORE- 95.42%","a7069d24":"## Approach\n* This competition was a bit similar  to the previous competitons. The approach I used could be considered as trickery, I guess the competition was designed in such a way.\n\n* Upon combining the Train and Test Dataset and sorting them in acsending order I noticed the IDs were similar for almost 88% of the data. MoreOver not only the IDs many values were also coinciding apart form few numerical data.\n\n* So you can use the same segmentation class for your predictions and copy them , for the rest 12% or so you actually have to make the actual predictions with the help of your model.","abd0129a":"# Displaying the duplicate rows in test and train"}}