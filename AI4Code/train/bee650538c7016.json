{"cell_type":{"8d2b7426":"code","e972ffd0":"code","4a8e3bed":"code","eaa76a52":"code","0717a61c":"code","a429fc23":"code","982f45d0":"code","4b760afe":"code","00e1eb2b":"code","8adbf5b4":"code","f11ba67d":"code","fef8d391":"code","2affcd4c":"code","601a109b":"code","10bb1c77":"code","4d837b2c":"code","6b8c6863":"code","8a002da8":"code","e163cb31":"code","58e1ee46":"code","f51ad7a5":"code","54fbc3eb":"code","1798f7e3":"code","43626944":"code","357fbae8":"code","4e76c098":"code","3aa761ed":"code","748a1144":"code","933873b5":"code","0fe86f28":"code","1eb74fe7":"code","d069bdb4":"code","d60fc7a5":"code","e8c4e128":"code","ad1188c0":"code","24634c14":"code","0f23a0e3":"code","bdeb9468":"code","566345ac":"code","3fc8bb1d":"code","0ae23f8b":"code","970001ea":"code","5f075f34":"code","f67599dc":"code","f6b35d77":"code","d4b56372":"code","ddc58bcc":"code","aac27b1d":"code","9be2af34":"code","f9232ebc":"code","756f5112":"code","d889623c":"code","9dcb4497":"code","86710765":"code","07318377":"code","18c013a4":"code","c9fc7042":"code","3f18c3ca":"code","2cfef0b7":"code","a9e7a5c3":"code","663e5b25":"code","de526d80":"markdown","3c9b57cc":"markdown","de608e51":"markdown","cf49493e":"markdown","837f253e":"markdown","b572f812":"markdown","4f50f2fe":"markdown","3954bd04":"markdown","f5006e22":"markdown"},"source":{"8d2b7426":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom collections import Counter, defaultdict\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import StandardScaler, Imputer, RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import mean_squared_error\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e972ffd0":"sns.set_style('darkgrid')\npd.set_option('display.max_columns', 500)","4a8e3bed":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","eaa76a52":"train.head()","0717a61c":"train.info()","a429fc23":"train.describe()","982f45d0":"# # Checking # of NA \n# tmp = pd.isnull(train).sum()\n# for i in range(len(tmp)):\n#     if tmp[i] != 0:\n#         print('%s %s %d %s (%.2f%s)'%(tmp.index[i],' '*(20-len(tmp.index[i])), \n#                                   tmp[i], ' '*(6 - len(str(tmp[i]))), \n#                                   tmp[i]\/len(train)*100, '%'))\n        \n# Checking NA's\ntmp = pd.isnull(train).sum()\ntmp = tmp.sort_values(ascending=False)\ntmp = tmp[tmp>0]\nplt.xticks(rotation=90)\nsns.barplot(x=tmp.index, y=tmp)\nplt.show()","4b760afe":"tmp = pd.isnull(test).sum()\ntmp = tmp.sort_values(ascending=False)\ntmp = tmp[tmp>0]\nplt.xticks(rotation=90)\nsns.barplot(x=tmp.index, y=tmp)\nplt.show()","00e1eb2b":"# Categorizing variables \nnum_col = []\ncat_col = []\n\nfor column in train.columns[~pd.Series(train.columns).isin(['Id', 'SalePrice'])]:\n    if type(train[column][0]) is not str:\n        num_col.append(column)\n    else:\n        cat_col.append(column)\n\ntmp = ['MSSubClass', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold'] # Handwork ! \nnum_col = list(set(num_col) - set(tmp))\ncat_col += tmp\n\nfor col in tmp:\n    train[col] = train[col].astype(str)","8adbf5b4":"# skew and log transformation for the target variable\nfig, ax= plt.subplots(1,2, figsize = (10,5))\nprint(\"Before >  Mean: %f, Standard Deviation: %f\" %norm.fit(train['SalePrice']))\nsns.distplot(train['SalePrice'], ax = ax[0])\nstats.probplot(train['SalePrice'], plot=ax[1])\nplt.show()","f11ba67d":"# after log transformation to the target var\nfig, ax= plt.subplots(1,2, figsize = (10,5))\ntmp = np.log1p(train['SalePrice']) # log(1+x)\nprint(\"After > Mean: %f, Standard Deviation: %f\" %norm.fit(tmp))\nsns.distplot(tmp, ax = ax[0])\nstats.probplot(tmp, plot=ax[1])\nplt.show()","fef8d391":"# vars' corr with the target var\ncol_corr = []\nfor col in num_col:\n    na_idx = pd.isnull(train[col])\n    corr = np.corrcoef(x= train[col][~na_idx], y=train['SalePrice'][~na_idx])[0,1]\n    col_corr.append((col, corr))\ncol_corr.sort(key=lambda x : -abs(x[1]))","2affcd4c":"col_corr","601a109b":"# Top N's scatter plots\nN=16\ntop_N_num = [set[0] for set in col_corr[:N]]\n\nfig, ax = plt.subplots(int(np.ceil(N\/2)),2, figsize=(20,N*2))\nfor i, col in enumerate(top_N_num):\n    sns.scatterplot(data=train, \n             x=col, \n             y=\"SalePrice\", \n             alpha=0.4, \n             color='red',\n             ax=ax[i\/\/2][i%2])\n    ax[i\/\/2][i%2].set_xlabel(col, fontsize=18)\n    ax[i\/\/2][i%2].set_ylabel('SalePrice', fontsize=18)\nplt.show()","10bb1c77":"# pairplot: top 5 highest correlated vars and target var\nN=5\ntop_N_num = [set[0] for set in col_corr[:N]]\ntop_N_num.append('SalePrice')\n\nplt.figure(figsize=(10,8))\nsns.pairplot(train[top_N_num])\nplt.show()","4d837b2c":"# top N's lowest P-value variables\ncol_aov = []\nfor col in cat_col:\n    result = ols('SalePrice ~ {}'.format(col), data = train).fit()\n    aov_table = sm.stats.anova_lm(result)\n    aov_pr = aov_table['PR(>F)'][0]\n    col_aov.append((col, aov_pr))\n\ncol_aov.sort(key = lambda x: x[1]) # ascending order of p-value ","6b8c6863":"col_aov","8a002da8":"# Box plot of Top N Categorical Vars\nN= 16\ntop_N_num = [set[0] for set in col_aov[:N]]\n\nfig, ax = plt.subplots(int(np.ceil(N\/2)),2, figsize=(20,N*2))\nplt.setp(ax[0][0].get_xticklabels(), rotation=45)\nfor i, col in enumerate(top_N_num):\n    sns.boxplot(x=col, y='SalePrice', data=train, ax=ax[i\/\/2][i%2])\n    ax[i\/\/2][i%2].set_xlabel(col, fontsize=18)\n    ax[i\/\/2][i%2].set_ylabel('SalePrice', fontsize=18)\nplt.show()","e163cb31":"# Correlation of each other\nplt.figure(figsize=(10,8))\ncorr_table = train[num_col].corr()\nsns.heatmap(corr_table)\nplt.show()","58e1ee46":"# N highest corr pairs\nhigh_cor_list = []\ntmp = corr_table[abs(corr_table)>0.4]\nfor col in tmp.columns:\n    for row in tmp[col][~pd.isnull(tmp[col])].index:\n        if col == row:\n            break\n        high_cor_list.append((col,row, tmp[col][row]))\nhigh_cor_list.sort(key = lambda x : -x[2])\nhigh_cor_list","f51ad7a5":"# Correlation of each other\nplt.figure(figsize=(10,8))\ncorr_table = test[num_col].corr()\nsns.heatmap(corr_table)\nplt.show()","54fbc3eb":"# N highest corr pairs\nhigh_cor_list = []\ntmp = corr_table[abs(corr_table)>0.4]\nfor col in tmp.columns:\n    for row in tmp[col][~pd.isnull(tmp[col])].index:\n        if col == row:\n            break\n        high_cor_list.append((col,row, tmp[col][row]))\nhigh_cor_list.sort(key = lambda x : -x[2])\nhigh_cor_list","1798f7e3":"# outlier found in the above graph\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice') \nplt.show() # 2 ourliers at the bottom right","43626944":"# outlier checking with univariate anlaysis\nscaler = StandardScaler()\ntmp = scaler.fit_transform(train[['SalePrice']])\ntmp_sorted = sorted(np.squeeze(tmp))\nfor a, b in zip(tmp_sorted[:10], tmp_sorted[-10:]):\n    print('{} {} {}' .format(round(a, 5), ' '*10, round(b,5)))","357fbae8":"train[tmp>7] # Be careful of these! ","4e76c098":"skews = []\nfor col in num_col:\n    skews.append((col, skew(train[col])))\nskews.sort(key=lambda x : -abs(x[1]))","3aa761ed":"skews","748a1144":"sns.distplot(train['TotalBsmtSF'])\nplt.xlim(0,10e+03)\nplt.show()","933873b5":"# After log transformation on skewed variables( 1. GrLivArea)\nfig, ax = plt.subplots(1,2, figsize=(15,5))\ndat = train.copy()\ndat['SalePrice'] = np.log1p(dat['SalePrice'])\ndat['GrLivArea'] = np.log1p(dat['GrLivArea'])\ndat['MasVnrArea'] = np.log1p(dat['MasVnrArea'])\nsns.scatterplot(data=dat, x='GrLivArea', y='SalePrice', ax=ax[0])\nsns.scatterplot(data=dat, x='GarageArea', y='SalePrice', ax=ax[1])\nplt.show()","0fe86f28":"fig, ax = plt.subplots(20,2,figsize=(20,80))\nfor i, col in enumerate(num_col):\n    row = int(i\/2)\n    sns.distplot(train[col][~pd.isnull(train[col])], ax=ax[row][0])\n    sns.distplot(test[col][~pd.isnull(test[col])], ax=ax[row][1])\nplt.show()","1eb74fe7":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d069bdb4":"# Removing Outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)].index) # Removing\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice') # After\nplt.show()","d60fc7a5":"# Combining \ntest['SalePrice'] = 0\ndata = pd.concat([train,test], axis=0)\ndata.reset_index(drop=True, inplace=True)","e8c4e128":"# NA Handling\ndata['MSZoning'].fillna(data['MSZoning'].mode()[0], inplace=True)\n\ndata['Utilities'].fillna(data['Utilities'].mode()[0], inplace=True)\ndata['Exterior1st'].fillna(data['Exterior1st'].mode()[0], inplace=True)\ndata['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0], inplace=True)\ndata['MasVnrType'].fillna('None', inplace=True)\ndata['MasVnrArea'].fillna(0, inplace=True)\ndata['BsmtQual'].fillna('None', inplace=True)\ndata['BsmtCond'].fillna('None', inplace=True)\ndata['BsmtExposure'].fillna('None', inplace=True)\ndata['BsmtFinType1'].fillna('None', inplace=True)\ndata['BsmtFinSF1'].fillna(0, inplace=True)\ndata['BsmtFinType2'].fillna('None', inplace=True)\ndata['BsmtFinSF2'].fillna(0, inplace=True)\ndata['BsmtUnfSF'].fillna(0, inplace=True)\ndata['TotalBsmtSF'].fillna(0, inplace=True)\ndata['Electrical'].fillna(data['Electrical'].mode()[0], inplace=True)\ndata['BsmtFullBath'].fillna(0, inplace=True)\ndata['BsmtHalfBath'].fillna(0, inplace=True)\ndata['KitchenQual'].fillna(data['KitchenQual'].mode()[0], inplace=True)\ndata['Functional'].fillna('None', inplace=True)\ndata['GarageType'].fillna('None', inplace=True)\ndata['GarageFinish'].fillna('None', inplace=True)\ndata['GarageYrBlt'].fillna(data['GarageYrBlt'].mode()[0], inplace=True)\ndata['GarageArea'].fillna(0, inplace=True)\ndata['GarageCars'].fillna(0, inplace=True)\ndata['GarageQual'].fillna('None', inplace=True)\ndata['SaleType'].fillna(data['SaleType'].mode()[0], inplace=True)\n\ndata['GarageCond'].fillna('None', inplace=True)\ndata['PoolQC'].fillna('None', inplace=True)\ndata['Fence'].fillna('None', inplace=True)\ndata['MiscFeature'].fillna('None', inplace=True)\ndata['LotFrontage'].fillna(0, inplace=True)\ndata['Alley'].fillna('None', inplace=True)\ndata['FireplaceQu'].fillna('None', inplace=True)","ad1188c0":"# Dropping columns with too many NAs\ncols = ['MiscFeature', 'Alley', 'PoolQC', 'Fence', 'FireplaceQu', 'GarageCond', 'LotFrontage']\ndata.drop(cols, axis=1, inplace=True)","24634c14":"# Adding\/Revising Columns\n    # Num Columns\ndata['Total_SF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['Total_Bath'] = data['BsmtFullBath'] + data['BsmtHalfBath']*0.5 + data['FullBath'] + data['HalfBath']*0.5\ndata['Total_Footage'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['Age'] = 2020 - data['YearBuilt']\ndata['Age_rmd'] = 2020 - data['YearRemodAdd']\ndata['Age_Garage'] = 3020 - data['GarageYrBlt']\ndata['qul_grliv'] = data['OverallQual'] * data['GrLivArea']\ndata['garage'] = data['GarageCars'] * data['GarageArea']\ndata['Years'] = data['YearBuilt'] + data['YearRemodAdd'] + data['GarageYrBlt']\n\ndata['Age_not_rmd'] = data['Age'] - data['Age_rmd']\ndata['Overall'] = data['OverallQual'] + data['OverallCond']\ndata['porch'] = data['OpenPorchSF'] + data['EnclosedPorch'] + data['ScreenPorch']\n\n\ndata['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    # Cat Columns\n# data['Ext_Kit'] = data['ExterQual'] + data['KitchenQual']\n# data['Bsm_Heat'] = data['BsmtQual'] + data['HeatingQC']\n# data['Gara_TP1'] = data['GarageFinish'] + data['BsmtFinType1']\ndata['Season'] = data['MoSold'].apply(lambda x: 'Spring' if x<= 3 else ('Summer' if x <=6 else ('Fall' if x <=9 else 'Winter')))\ndata.drop(['MoSold'], axis=1, inplace=True)","0f23a0e3":"# Processing Types of Variables \nto_cat = ['MSSubClass']\ndata[to_cat] = data[to_cat].astype(str)","bdeb9468":"# Dividing columsn into num and cat\nnum_col = []\ncat_col = []\nfor col in data.columns:\n    if col == \"Id\" or col == 'SalePrice':\n        continue\n    elif data[col].dtype == 'object':\n        cat_col.append(col)\n    else:\n        num_col.append(col)\n\nprint(len(num_col))\nprint(len(cat_col))\nprint(len(data.columns))","566345ac":"# Label Encoding & Dummy\n\n# labels_col = ['LotShape', 'LandContour', 'LandSlope', 'HouseStyle', 'ExterQual',\n#              'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n#              'BsmtFinType2', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional',\n#               'GarageFinish', 'GarageQual', 'PavedDrive']\n\n# for col in labels_col:\n#     data[col] = pd.factorize(data[col])[0]\n\n# data['LotShape'] = data['LotShape'].apply(lambda x: {'IR3':0, 'IR2':1, 'IR1':2, 'Reg':3}[x])\n# data['LandContour'] = data['LandContour'].apply(lambda x: {'Low':0, 'HLS':1, 'Bnk':2, 'Lvl':3}[x])\n# data['LandSlope'] = data['LandSlope'].apply(lambda x: {'Gtl':0, 'Mod':1, 'Sev':2}[x])\n# data['HouseStyle'] = data['HouseStyle'].apply(lambda x: {'1Story':0, '1.5Unf':1, '1.5Fin':2,\n#                                                         '2Story':3, '2.5Unf':4, '2.5Fin':5,\n#                                                         'SFoyer':6, 'SLvl':7}[x])\n# data['CentralAir'] = data['CentralAir'].apply(lambda x: {'N':0, 'Y':1}[x])\n# data['Functional'] = data['Functional'].apply(lambda x: {'Sal':0, 'Sev':1, 'Maj2':2, 'Maj1':3, 'Mod':4,\n#                                                         'Min2':5, 'Min1':6, 'Typ':7, 'None':-99}[x])\n# data['BsmtExposure'] = data['BsmtExposure'].apply(lambda x: {'No':0, 'Mn':1, 'Av':2, 'Gd':3, 'None':-99}[x])\n# data['GarageFinish'] = data['GarageFinish'].apply(lambda x: {'Unf':0, 'RFn':1, 'Fin':2, 'None':-99}[x])\n# data['PavedDrive'] = data['PavedDrive'].apply(lambda x: {'N':0, 'P':1, 'Y':2}[x])\n# for col in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual']:\n#     data[col] = data[col].apply(lambda x: {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4, 'None':-99}[x])\n# for col in ['BsmtFinType1', 'BsmtFinType2']:\n#     data[col] = data[col].apply(lambda x: {'Unf':0, 'LwQ':1, 'Rec':2, 'BLQ':3, 'ALQ':4, 'GLQ':5,'None':-99}[x])\n\n\nfor col in cat_col:\n# for col in list(set(cat_col) - set(labels_col)):\n    data = pd.concat([data,pd.get_dummies(data[col], prefix=(col))], axis=1)\n    data.drop([col], axis=1, inplace=True)","3fc8bb1d":"# Solving Skew Problem\nskews = []\nfor col in num_col:\n    skews.append((col, skew(data[col])))\nskews.sort(key=lambda x : -abs(x[1]))\n\nfor col, value in skews:\n    if abs(value) >0.5:\n        data[col] = boxcox1p(data[col], 0.15) # boxcox\n        \ndata['SalePrice'] = np.log1p(data['SalePrice']) # log(x+1)","0ae23f8b":"\n### Dropping columns that have high corr with other vars \ncols = ['GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd', '2ndFlrSF', 'BsmtFullBath']\ndata.drop(cols, axis=1, inplace=True)\n\nnum_col = list(set(num_col)-set(cols))","970001ea":"# Dropping Sparse Columns\ncols = []\nfor col in data.columns:\n    major_ratio = data[col].value_counts().iloc[0]\/len(data[col])\n    if major_ratio > 0.999:\n        cols.append(col)\n\ndata.drop(cols, axis=1, inplace=True)\nprint('# of columns dropped : {}'.format (len(cols)))","5f075f34":"# PCA ! \n# pca = PCA()\n# # data[num_col] = pca.fit_transform(data[num_col])\n# data[num_col[:4]] = pca.fit_transform(data[num_col])[:,:4]\n# data.drop(num_col[4:], axis=1, inplace=True)","f67599dc":"data.shape","f6b35d77":"# Dividing\ndata.reset_index(drop=True, inplace=True)\ntrain = data[data['Id']<=1460]\ntest = data[data['Id'] >1460]\n\nX = list(set(train.columns) - set(['SalePrice']))\ny = 'SalePrice'","d4b56372":"def rmse_cv(model):\n    result = cross_val_score(cv=5, estimator=model, X=train[X].values, y=train[y].values, scoring='neg_mean_squared_error')\n    result = (-result)**(1\/2)\n    return np.mean(result), np.std(result)\n\n#  Scoring Options of 'cross_val_score' function:\n# 'accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', \n# 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', \n# 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error',\n# 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2',\n# 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score'","ddc58bcc":"def rmsle(pred, actual):\n    result = 0\n    for i in range(len(pred)):\n        result += (pred[i] - actual[i])**2\n    result \/= len(pred)\n    return result**(1\/2)","aac27b1d":"EN = make_pipeline(RobustScaler(), ElasticNet(alpha=1e-3, l1_ratio=0.7, max_iter=1e+4)) # alpha\ub294 l1_ratio\uc5d0 \uacf1\ud574\uc9d0 \n# EN =  ElasticNet(alpha=1e-3, l1_ratio=0.8, max_iter=1e+04)\nrmse_cv(EN)\n\n# # Objective Function of ElasticNet\n# 1 \/ (2 * n_samples) * ||y - Xw||^2_2\n# + alpha * l1_ratio * ||w||_1\n# + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2","9be2af34":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=5e-04, max_iter=1e+04))\n# lasso =  Lasso(alpha=1e-04, max_iter=1e+04)\nrmse_cv(lasso)","f9232ebc":"KRR = KernelRidge(alpha=1e-2, kernel='polynomial', degree=1, coef0=2) # kernel: 'linear', 'laplacian', 'rbf'\nrmse_cv(KRR)","756f5112":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=30, min_samples_split=10, \n                                   loss='huber', random_state =5)\n# rmse_cv(GBoost) # corr \ub192\uc740 \ubcc0\uc218\ub4e4 \uc548\uc798\ub77c\ub0b4\ub2c8\uae4c \ud655\uc2e4\ud788 \uc131\ub2a5\ub5a8\uc5b4\uc9d0 ","d889623c":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.3, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.3, n_estimators=1024,\n                             reg_alpha=0.3, reg_lambda=0.4,\n                             subsample=0.5, silent=1,\n                             random_state =7, nthread = -1)\n# rmse_cv(model_xgb)","9dcb4497":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=4,\n                              learning_rate=0.05, n_estimators=1024,\n                              max_bin = 40, bagging_fraction = 0.9,\n                              bagging_freq = 20, feature_fraction = 0.1, #W 0.6 > 0.2 \ub85c \ub0ae\ucd94\ub2c8 \uc5c4\uccad\ub09c\ud6a8\uacfc!\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf = 2, min_sum_hessian_in_leaf = 1, random_state=5)\n# rmse_cv(model_lgb)","86710765":"def baseline_model():\n    model = Sequential()\n    model.add(Dense(64, input_dim=len(X), kernel_initializer='normal', activation='relu'))\n#     model.add(BatchNormalization())\n    model.add(Dense(8, input_dim=64, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    opt = optimizers.Adam(learning_rate = 0.005)\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    return model\n\nnn = KerasRegressor(build_fn=baseline_model, epochs=3000, batch_size=len(train), verbose=0)\n# rmse_cv(nn)","07318377":"rf = RandomForestRegressor(n_estimators=128, min_samples_split=4, min_samples_leaf = 2,\n                           random_state=42)\nrmse_cv(rf)","18c013a4":"class Meta_Regressor(BaseEstimator):\n    def __init__(self, base_models, meta_models):\n        self.base_models = base_models # self.A = B \uc5d0\uc11c A\uc640 B\uac00 \uc774\ub984\uc774 \uac19\uc544\uc57c\ud55c\ub2e4.. \ubb50\uc9c0 \n        self.meta_models = meta_models\n        \n    def fit(self, X, y):\n        self.base_models_ = [[] for _ in self.base_models]\n        self.meta_models_ = clone(self.meta_models)\n        \n        Kf = KFold(n_splits=5, shuffle=True, random_state=5)\n        out_fold_pred = np.zeros((len(X), len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_idx, val_idx in Kf.split(X):\n                model = clone(self.base_models[i])\n                model.fit(X[train_idx], y[train_idx])\n                pred = model.predict(X[val_idx])\n                out_fold_pred[val_idx, i] = pred\n                self.base_models_[i].append(model)\n                \n        self.meta_models_.fit(X=out_fold_pred, y=y)\n                  \n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in sub_models]).mean(axis=1)\n        for sub_models in self.base_models_])\n        \n        scores = self.meta_models_.predict(meta_features)\n        return scores","c9fc7042":"class Weighted_Ensemble(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    def fit(self, X, y):\n        self.models_ = [clone(model) for model in self.models]\n        for model in self.models_:\n            model.fit(X, y)\n            \n    def predict(self, x):\n        results = np.zeros(len(x))\n        scores = [model.predict(x) for model in self.models_]\n#         for i, model in enumerate(scores):\n#             results += scores[i] * self.weights[i]\n        return scores","3f18c3ca":"meta_regressor = Meta_Regressor(base_models= [lasso, KRR, GBoost, model_xgb, model_lgb, nn, rf], meta_models = EN)\nweighted_ensemble = Weighted_Ensemble(models= [lasso, KRR, GBoost, model_xgb, model_lgb, nn, rf, EN, meta_regressor])\n\ntrain_x, test_x, train_y, test_y = train_test_split(train[X].values, train[y].values, test_size=0.3, random_state=4)\nweighted_ensemble.fit(train_x, train_y)\nscores = weighted_ensemble.predict(test_x)","2cfef0b7":"weights = [0.1, 0.2, 0.2, 0.06, 0.09, 0.01, 0.04, 0.2, 0.1] # \n       # lasso, KRR, GBoost, xgb, lgb, nn,  rf    EN, meta\n    \nscore = np.zeros(len(scores[0]))\nfor i, model in enumerate(scores):\n    score += scores[i] * weights[i]\nrmsle(score, test_y)","a9e7a5c3":"# meta_ensemble\nmeta_regressor = Meta_Regressor(base_models= [lasso, KRR, GBoost, model_xgb, model_lgb, nn, rf], meta_models = EN)\nweighted_ensemble = Weighted_Ensemble(models= [KRR, GBoost, model_xgb, EN])\nweighted_ensemble.fit(train[X].values, train[y].values)\n\nweights = [0.2, 0.2, 0.05,0.55] # \nscores = weighted_ensemble.predict(test[X].values)\nscore = np.zeros(len(scores[0]))\nfor i, model in enumerate(scores):\n    score += scores[i] * weights[i]\npred = np.expm1(score)","663e5b25":"submission = pd.DataFrame({\n    'Id': range(1461, 2920),\n    'SalePrice': pred\n})\n\nsubmission.to_csv(\"submission.csv\", index=False) # 0.11528 in leader board (505\/5027 as of 2 Feb 2020)","de526d80":"### Outlier","3c9b57cc":"### Target Variable","de608e51":"### Train VS Test","cf49493e":"### Preprocessing","837f253e":"### Final Prediction","b572f812":"### Checking Homoscedascity ","4f50f2fe":"### Independant Variables","3954bd04":"\n### Weighted Ensemble with Meta Model","f5006e22":"### Baseline Models"}}