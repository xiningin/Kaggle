{"cell_type":{"044a3735":"code","3255af32":"code","6f6e437c":"code","fee95ffc":"code","ccbaf220":"code","f77d4abd":"code","b43c55ed":"code","f02e9c10":"code","a85e29a0":"code","45118053":"code","127aefbf":"code","45e07528":"code","fe8b1087":"code","d57d9965":"code","4bd3fa28":"markdown","9eb368d9":"markdown"},"source":{"044a3735":"#location of the data \ndata_location =  \"..\/input\/flickr8k\"\n!ls $data_location","3255af32":"#reading the text data \nimport pandas as pd\ncaption_file = data_location + '\/captions.txt'\ndf = pd.read_csv(caption_file)\nprint(\"There are {} image to captions\".format(len(df)))\ndf.head(7)","6f6e437c":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n#select any index from the whole dataset \n#single image has 5 captions\n#so, select indx as: 1,6,11,16...\ndata_idx = 11\n\n#eg path to be plot: ..\/input\/flickr8k\/Images\/1000268201_693b08cb0e.jpg\nimage_path = data_location+\"\/Images\/\"+df.iloc[data_idx,0]\nimg=mpimg.imread(image_path)\nplt.imshow(img)\nplt.show()\n\n#image consits of 5 captions,\n#showing all 5 captions of the image of the given idx \nfor i in range(data_idx,data_idx+5):\n    print(\"Caption:\",df.iloc[i,1])\n","fee95ffc":"#imports \nimport os\nfrom collections import Counter\nimport spacy\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\nfrom PIL import Image","ccbaf220":"#using spacy for the better text tokenization \nspacy_eng = spacy.load(\"en\")\n\n#example\ntext = \"This is a good place to find a city\"\n[token.text.lower() for token in spacy_eng.tokenizer(text)]","f77d4abd":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        #setting the pre-reserved tokens int to string tokens\n        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n        \n        #string to int tokens\n        #its reverse dict self.itos\n        self.stoi = {v:k for k,v in self.itos.items()}\n        \n        self.freq_threshold = freq_threshold\n        \n    def __len__(self): return len(self.itos)\n    \n    @staticmethod\n    def tokenize(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def build_vocab(self, sentence_list):\n        frequencies = Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                frequencies[word] += 1\n                \n                #add the word to the vocab if it reaches minum frequecy threshold\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self,text):\n        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n        tokenized_text = self.tokenize(text)\n        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]    ","b43c55ed":"#testing the vicab class \nv = Vocabulary(freq_threshold=1)\n\nv.build_vocab([\"This is a good place to find a city\"])\nprint(v.stoi)\nprint(v.numericalize(\"This is a good place to find a city here!!\"))","f02e9c10":"class FlickrDataset(Dataset):\n    \"\"\"\n    FlickrDataset\n    \"\"\"\n    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_file)\n        self.transform = transform\n        \n        #Get image and caption colum from the dataframe\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        #Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.captions.tolist())\n        \n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        caption = self.captions[idx]\n        img_name = self.imgs[idx]\n        img_location = os.path.join(self.root_dir,img_name)\n        img = Image.open(img_location).convert(\"RGB\")\n        \n        #apply the transfromation to the image\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        #numericalize the caption text\n        caption_vec = []\n        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n        caption_vec += self.vocab.numericalize(caption)\n        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n        \n        return img, torch.tensor(caption_vec)","a85e29a0":"#defing the transform to be applied\ntransforms = T.Compose([\n    T.Resize((224,224)),\n    T.ToTensor()\n])","45118053":"def show_image(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","127aefbf":"#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"\/Images\",\n    captions_file = data_location+\"\/captions.txt\",\n    transform=transforms\n)\n\n\n\nimg, caps = dataset[0]\nshow_image(img,\"Image\")\nprint(\"Token:\",caps)\nprint(\"Sentence:\")\nprint([dataset.vocab.itos[token] for token in caps.tolist()])","45e07528":"class CapsCollate:\n    \"\"\"\n    Collate to apply the padding to the captions with dataloader\n    \"\"\"\n    def __init__(self,pad_idx,batch_first=False):\n        self.pad_idx = pad_idx\n        self.batch_first = batch_first\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs,dim=0)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n        return imgs,targets","fe8b1087":"#writing the dataloader\n#setting the constants\nBATCH_SIZE = 4\nNUM_WORKER = 1\n\n#token to represent the padding\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n)","d57d9965":"#generating the iterator from the dataloader\ndataiter = iter(data_loader)\n\n#getting the next batch\nbatch = next(dataiter)\n\n#unpacking the batch\nimages, captions = batch\n\n#showing info of image in single batch\nfor i in range(BATCH_SIZE):\n    img,cap = images[i],captions[i]\n    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n    eos_index = caption_label.index('<EOS>')\n    caption_label = caption_label[1:eos_index]\n    caption_label = ' '.join(caption_label)                      \n    show_image(img,caption_label)\n    plt.show()","4bd3fa28":"<h2>2) Writing the custom dataset<\/h2>\n<p>Writing the custom torch dataset class so, that we can abastract out the dataloading steps during the training and validation process<\/p>\n<p>Here, dataloader is created which gives the batch of image and its captions with following processing done:<\/p>\n\n<li>caption word tokenized to unique numbers<\/li>\n<li>vocab instance created to store all the relivent words in the datasets<\/li>\n<li>each batch, caption padded to have same sequence length<\/li>\n<li>image resized to the desired size and converted into captions<\/li>\n\n<br><p>In this way the dataprocessing is done, and the dataloader is ready to be used with <b>Pytorch<\/b><\/p>","9eb368d9":"# <h2>1) Exploring the dataset<\/h2>\n<p>Reading the image data and their corresponding captions from the flick dataset folder. Showing the image and captions to get the insighs of the data. Dowload link for the dataset used <a href=\"https:\/\/www.kaggle.com\/adityajn105\/flickr8k\">here<\/a><\/p>"}}