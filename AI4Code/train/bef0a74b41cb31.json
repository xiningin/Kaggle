{"cell_type":{"b153031d":"code","a17f89dd":"code","334eb2f3":"code","a244d620":"code","dbe65aba":"code","c5f21579":"code","625e37ce":"code","642e1faf":"code","10341874":"code","54de73fb":"code","918834c6":"code","970f1086":"code","22d318c6":"code","20ca9614":"code","8cfbd412":"code","359c3b54":"code","21b9f8be":"code","d9d95733":"code","140a2a08":"code","57a4d4b7":"code","9e51b963":"code","71cce02b":"code","bd321ebf":"code","117e47cf":"code","f9fce2a9":"code","ddb20fa4":"code","48701629":"code","f20897dc":"code","14b747c5":"code","80780a1e":"code","48ac9f9f":"code","17042aad":"code","847b2581":"code","8feea446":"code","daae9d77":"code","4e600f4d":"code","768db30b":"code","3651f98e":"code","9ba2d352":"markdown","2a0e0673":"markdown","158c132a":"markdown","5fc8337a":"markdown","f7790b2e":"markdown","7b76a214":"markdown","2876ec17":"markdown","d24fa71f":"markdown","0f115582":"markdown","c5dc1d4e":"markdown","412c4e35":"markdown","200c665c":"markdown","0e37c9d4":"markdown","5e1a4131":"markdown","17e25388":"markdown","8f833bf2":"markdown","3a5b6c1b":"markdown","6ca67956":"markdown","92b1d8e5":"markdown","4d2c0bab":"markdown"},"source":{"b153031d":"#import necessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()","a17f89dd":"#import the read the dataset\nheart= pd.read_csv('..\/input\/heart-disease\/heart.csv')\nheart.head()","334eb2f3":"#shape of the dataset\nheart.shape","a244d620":"heart.columns","dbe65aba":"#summary of the dataset\nheart.info()","c5f21579":"#Checking missing value in dataset\nheart.isnull().sum()","625e37ce":"#Check for unique values in columns\nheart.nunique()","642e1faf":"#statistical summary of the dataset\nheart.describe()","10341874":"heart['target'].value_counts()","54de73fb":"#convert columns to category\ncolumns_to_category= ['sex', 'cp', 'fbs', 'restecg','exang', 'slope', 'ca', 'thal', 'target']\n\nheart[columns_to_category]= heart[columns_to_category].astype('category')","918834c6":"heart.describe()","970f1086":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(heart[heart[\"target\"]==1][\"age\"], shade=True, color=\"blue\", label=\"Presence of Heart Disease\", ax=ax)\nsns.kdeplot(heart[heart[\"target\"]==0][\"age\"], shade=True, color=\"green\", label=\"Absence of Heart Disease\", ax=ax)\n\nfig.suptitle(\"Presence of Heart Disease by Age\")\n\nax.legend();","22d318c6":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(heart[heart[\"target\"]==1][\"trestbps\"], shade=True, color=\"blue\", label=\"Presence of Heart Disease\", ax=ax)\nsns.kdeplot(heart[heart[\"target\"]==0][\"trestbps\"], shade=True, color=\"green\", label=\"Absence of Heart Disease\", ax=ax)\n\nfig.suptitle(\"Presence of Heart Disease by Blood Sugar\")\n\nax.legend();","20ca9614":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(heart[heart[\"target\"]==1][\"chol\"], shade=True, color=\"blue\", label=\"Presence of Heart Disease\", ax=ax)\nsns.kdeplot(heart[heart[\"target\"]==0][\"chol\"], shade=True, color=\"green\", label=\"Absence of Heart Disease\", ax=ax)\n\nfig.suptitle(\"Presence of Heart Disease based on Cholestrol\")\n\nax.legend();","8cfbd412":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(heart[heart[\"target\"]==1][\"thalach\"], shade=True, color=\"blue\", label=\"Presence of Heart Disease\", ax=ax)\nsns.kdeplot(heart[heart[\"target\"]==0][\"thalach\"], shade=True, color=\"green\", label=\"Absence of Heart Disease\", ax=ax)\n\nfig.suptitle(\"Presence of Heart Disease based on thalach\")\n\nax.legend();","359c3b54":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(heart[heart[\"target\"]==1][\"oldpeak\"], shade=True, color=\"blue\", label=\"Presence of Heart Disease\", ax=ax)\nsns.kdeplot(heart[heart[\"target\"]==0][\"oldpeak\"], shade=True, color=\"green\", label=\"Absence of Heart Disease\", ax=ax)\n\nfig.suptitle(\"Presence of Heart Disease based on OldPeak\")\n\nax.legend();","21b9f8be":"# Create the correlation matrix\ncorr = heart.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\nplt.figure(figsize=(14,10))\n# Add the mask to the heatmap\nsns.heatmap(corr, mask=mask, cmap='YlGnBu',center=0, linewidths=1, annot=True, fmt=\".2f\")\nplt.show()","d9d95733":"fig, ax = plt.subplots(5, 1, figsize=(10,20))\nsns.boxplot(x='target', y='age', data=heart, ax=ax[0])\nax[0].set_xlabel('Target', fontsize=12)\n\nsns.boxplot(x='target', y='trestbps', data=heart, ax=ax[1])\nax[1].set_xlabel('Target', fontsize=12)\n\nsns.boxplot(x='target', y='chol', data=heart, ax=ax[2])\nax[2].set_xlabel('Target', fontsize=12)\n\nsns.boxplot(x='target', y='thalach', data=heart, ax=ax[3])\nax[3].set_xlabel('Target', fontsize=12)\n\nsns.boxplot(x='target', y='oldpeak', data=heart, ax=ax[4])\nax[4].set_xlabel('Target', fontsize=12)\n\nplt.show()","140a2a08":"fig = plt.figure(figsize=(15,10))\nsns.pairplot(data=heart, vars=['age', 'trestbps', 'chol', 'thalach', 'oldpeak'], hue='target')\nplt.subplots_adjust(top=0.9)\nplt.show();","57a4d4b7":"heart.var()","9e51b963":"#split independent and dependent variable\nX= heart.iloc[:, :13]\ny= heart.iloc[:,-1]","71cce02b":"#import necessary scikit learn libraries\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","bd321ebf":"category_cols= X.columns[X.dtypes == 'category']\n\nencoded= pd.get_dummies(X[category_cols], drop_first=True)\nencoded.head()","117e47cf":"concat_X= pd.concat([X,encoded], axis=1)\nconcat_X.shape","f9fce2a9":"final_X= concat_X.drop(['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thalach'], axis=1)\nfinal_X.shape","ddb20fa4":"X_train, X_test, y_train, y_test= train_test_split(final_X, y, random_state=99)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","48701629":"scaler = MinMaxScaler()\n\nlr = LogisticRegression(solver='liblinear', random_state=99) # Other solvers have failure to converge problem\n\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)","f20897dc":"print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))","14b747c5":"print ('Test accuracy: %.4f' % pipeline.score(X_test, y_test))","80780a1e":"print ('Training accuracy score: %.4f' % accuracy_score(y_test, y_pred))","48ac9f9f":"# Confusion matrix\npd.DataFrame(confusion_matrix(y_test, y_pred))","17042aad":"# Cross validation\nfrom sklearn.metrics import make_scorer\n\nscorer= make_scorer(accuracy_score)\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=99)\ncv_score = cross_val_score(lr, X_train, y_train, cv=cv, scoring=scorer)\nprint('Cross validation accuracy score: %.3f' %np.mean(cv_score))","847b2581":"## Feature selection\nfrom sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features\/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring=scorer)\n\npipeline2 = Pipeline([('scale',scaler), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()","8feea446":"print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])","daae9d77":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nsearcher_cv = GridSearchCV(lr, grid, cv=cv, scoring = scorer)\nsearcher_cv.fit(X_train, y_train)\n\nprint(\"Best parameter: \", searcher_cv.best_params_)\nprint(\"accuracy score: %.3f\" %searcher_cv.best_score_)","4e600f4d":"from sklearn.metrics import roc_curve, auc\n\n#compute predicted probabilities: y_pred_prob\ny_pred_prob= pipeline.predict_proba(X_test)[:,1]\n\n\n#Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Calculate the AUC\n\nroc_auc = auc(fpr, tpr)\nprint ('ROC AUC: %0.3f' % roc_auc )\n\n#Plot ROC curve\nplt.figure(figsize=(10,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","768db30b":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nnb= MultinomialNB()\ndt= DecisionTreeClassifier()\nrf= RandomForestClassifier(n_estimators=100)\ngbr= GradientBoostingClassifier()\nsvm= SVC(kernel='linear')","3651f98e":"classifiers = [('Naive Bayes', nb),('Decision Tree Classifier', dt), ('RandomForest Classifier', rf), ('Gradient Boost', gbr), ('SVC', svm)]\n\n# Iterate over the pre-defined list of regressors\nfor classifier_name, classifier in classifiers:   \n    # Fit clf to the training set\n    classifier.fit(X_train, y_train)    \n    y_pred = classifier.predict(X_test) \n    \n    training_set_score = classifier.score(X_train, y_train)\n    test_set_score = classifier.score(X_test, y_test)\n    \n    \n    print('{:s} : {:.3f}'.format(classifier_name, training_set_score))\n    print('{:s} : {:.3f}'.format(classifier_name, test_set_score))","9ba2d352":"Based on kde plots, it looks like age, Thalach are good predictor for outcome variable.","2a0e0673":"\nAttribute Information:\n\n        Age: Age\n        Sex: Sex (1 = male; 0 = female)\n        ChestPain: Chest pain (typical, asymptotic, nonanginal, nontypical)\n        RestBP: Resting blood pressure\n        Chol: Serum cholestoral in mg\/dl\n        Fbs: Fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n        RestECG: Resting electrocardiographic results\n        MaxHR: Maximum heart rate achieved\n        ExAng: Exercise induced angina (1 = yes; 0 = no)\n        Oldpeak: ST depression induced by exercise relative to rest\n        Slope: Slope of the peak exercise ST segment\n        Ca: Number of major vessels colored by flourosopy (0 - 3)\n        Thal: (3 = normal; 6 = fixed defect; 7 = reversable defect)\n        target: AHD - Diagnosis of heart disease (1 = yes; 0 = no)\n","158c132a":"Few of the features have high variance are in different scales. We need to center these variances around 0","5fc8337a":"##### Initial Observation\n    There are 303 records and 14 columns are present\n    Mixture of Integer and flaot datatypes datas are present. basically Dataset consisits numerical columns\n    Surprisingly, There are no missing values present in any of the columns\n    When we looked at unique values in each columns, barring trestbps, chol, thalach, oldpeak all remaining columns needs to be converted to categorical type\n    We need to predit presence (value 1) or absence (value 0) of heart disease in the patient which are given in target column.\n    Since we know the target class, this is supervised learning, typical binary Classification modelling.","f7790b2e":"#### Model Building","7b76a214":"This forms the basis of evaluating and modifying our models in the next section.","2876ec17":"### Model Comparison","d24fa71f":"### Model Evaluation","0f115582":"Accuracy is a good measure here because data is balanced. we can also use AUC_BOC score as evaluation metric ","c5dc1d4e":"### Exploratory Data Analytics","412c4e35":"Default Parameter helps model perform Better","200c665c":"The AUC for both the test and train samples when run on my logistic regression demonstrates relatively strong power of separation between positive and negative occurences (Presence of Heart Disease - 1, Absence of Heart Disease - 0)","0e37c9d4":"Notice more outliers are present in Oldpeak attribute. for now, we will do model building including considering outlier values","5e1a4131":"### Model Validation\n\nSeveral things will be done:\n\n    Cross validation with shuffle split\n    Feature selections\n    Grid search for hyperparameters\n    Identify errors","17e25388":"Looks like SVM model will perform better than other models in this dataset","8f833bf2":"There is no significant pattern found in Target against Blood Sugar. Blood Sugar might not be good predictor of target.","3a5b6c1b":"Features used to build the model are optimal features","6ca67956":"#### Observation\n    - mean and median of age, trestbps, oldpeaks are almost similar. Outliers may have less impact\n    - mean is higher than median which tells outliers might have present in both the columns. Data Distribution is skewed towards right","92b1d8e5":"We will almost balanced target class","4d2c0bab":"#### Understanding the Dataset"}}