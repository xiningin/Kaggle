{"cell_type":{"eb289bb5":"code","0356f2df":"code","1275465d":"code","b03f093a":"code","ed16fba6":"code","b08db0a8":"code","22d93124":"code","59c0631a":"code","be9e9191":"code","c098312d":"code","a1b7b08e":"code","7f1ed1f5":"code","2b43bacd":"code","97626875":"code","54d80366":"code","6b609393":"code","3bdfbb77":"code","d04d1193":"code","c9b97251":"code","a7489054":"code","0bd60624":"code","baaf77f0":"code","9c0310ca":"code","2ada87fa":"code","b19311aa":"code","347e3024":"code","02de3870":"code","c74e5948":"code","6e59af3a":"code","d0a555de":"code","923c926e":"code","129e04eb":"code","5ebe0b84":"code","ae9993a4":"code","06b94a1b":"code","f0e1b240":"code","f78113b8":"code","23723a48":"code","c0b6af94":"code","4a621fc3":"code","3ca89a90":"code","14b616e1":"code","0debe0f2":"code","ea610cbf":"code","285befc8":"code","3f7beea0":"code","f9a7ed57":"code","c388b65f":"code","a83ea9ef":"code","69b462d2":"code","033c6b2e":"code","7c517880":"code","57308cfd":"code","4cc1e2ee":"code","5192274a":"code","98accba6":"code","1d4799ac":"code","2d621dd1":"code","524a5ab4":"markdown","6ab59570":"markdown","b986f12b":"markdown","21cfb0aa":"markdown","9480f466":"markdown","0930f87d":"markdown"},"source":{"eb289bb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0356f2df":"!pip install pyspark","1275465d":"from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nspark = SparkSession.builder.getOrCreate()","b03f093a":"spark","ed16fba6":"f= open(\"test1.csv\",\"w+\")\nfor i in range(10):\n     f.write(\"This is line %d\\r\\n\" % (i+1))\nf.close()","b08db0a8":"\n!>test1.csv\n!cat test1.csv\n!echo \"Name,Age\">>test1.csv\n!echo \"sudhanshu,31\">>test1.csv\n!echo \"sanjay,30\">>test1.csv\n!echo \"sunny,29\">>test1.csv\n!cat test1.csv","22d93124":"pd.read_csv('test1.csv')","59c0631a":"df_pyspark=spark.read.csv('test1.csv')","be9e9191":"df_pyspark=spark.read.option(\"header\",\"true\").csv('test1.csv')\n","c098312d":"type(df_pyspark)\ndf_pyspark.head(3)","a1b7b08e":"\ndf_pyspark.printSchema()","7f1ed1f5":"!>test1.csv\n!cat test1.csv\n!echo \"Name,Age,Experience\">>test1.csv\n!echo \"sudhanshu,31,10\">>test1.csv\n!echo \"sanjay,30,9\">>test1.csv\n!echo \"sunny,29,8\">>test1.csv\n!cat test1.csv","2b43bacd":"##############Read a dataset\ndf_pyspark=spark.read.option('header','true').csv('test1.csv',inferSchema=True)\ndf_pyspark.printSchema()","97626875":"df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)\n\ndf_pyspark.printSchema()","54d80366":"type(df_pyspark)\ndf_pyspark.head(3)","6b609393":"df_pyspark.select(['Name','Experience']).show()","3bdfbb77":"df_pyspark['Name']\ndf_pyspark.dtypes","d04d1193":"\ndf_pyspark.describe().show()","c9b97251":"#############adding columns and removing columns\ndf_pyspark=df_pyspark.withColumn('experience after 2 years', df_pyspark['Experience']+2)","a7489054":"df_pyspark.show()","0bd60624":"# drop the columns\ndf_pyspark=df_pyspark.drop('experience after 2 years')\ndf_pyspark.show()\n","baaf77f0":"# Rename a column \ndf_pyspark=df_pyspark.withColumnRenamed('name','New Name' )\ndf_pyspark.show()\n","9c0310ca":"!>test1.csv\n!cat test1.csv\n!echo \"Name,Age,Experience,Salary\">>test1.csv\n!echo \"sudhanshu,31,10,30000\">>test1.csv\n!echo \"sanjay,30,9,20000\">>test1.csv\n!echo \"sunny,29,8,35000\">>test1.csv\n!echo \"krish,24,10,20000\">>test1.csv\n!echo \"paula,21,9,30000\">>test1.csv\n!echo \"subham,23,8,37000\">>test1.csv\n!echo \"Mahesh,24,10,20000\">>test1.csv\n!echo \",34,10,38000\">>test1.csv\n!echo \",36,,\">>test1.csv\n!cat test1.csv","2ada87fa":"df_pyspark=spark.read.csv('test1.csv',inferSchema=True, header=True)\ndf_pyspark.show()","b19311aa":"# drop the columns\ndf_pyspark.drop('Name').show()","347e3024":"df_pyspark.na.drop().show()","02de3870":"# any,all == how \ndf_pyspark.na.drop(how=\"any\").show()","c74e5948":"# threshold\ndf_pyspark.na.drop(how=\"any\",thresh=2).show()","6e59af3a":"# subset\ndf_pyspark.na.drop(how=\"any\",subset=['Experience']).show()","d0a555de":"# fill the missing value is working without subset only\ndf_pyspark.na.fill('Missing value',subset=['Experience','Age']).show()","923c926e":"from pyspark.ml.feature import Imputer\nimputer = Imputer(\ninputCols=['Age','Experience','Salary'],\noutputCols=[\"{}_imputed\".format(c) for c in ['Age','experience','Salary']]\n).setStrategy(\"mean\")","129e04eb":"imputer.fit(df_pyspark).transform(df_pyspark).show()","5ebe0b84":"df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True).na.drop()\ndf_pyspark.show()","ae9993a4":"### salary of the people lesss than or equal to 25000\ndf_pyspark.filter(\"Salary<=25000\").show()","06b94a1b":"### salary of the people lesss than or equal to 25000\ndf_pyspark.filter(\"Salary<=25000\").select(['Name','Age']).show()","f0e1b240":"df_pyspark.filter((df_pyspark['Salary'] >= 20000) & (df_pyspark['Salary'] <= 30000)).show()","f78113b8":"df_pyspark.filter(~(df_pyspark['Salary'] <= 20000)).show()","23723a48":"!>test1.csv\n!cat test1.csv\n!echo \"Name,Departments,Salary\">>test1.csv\n!echo \"krish,IOT,20000\">>test1.csv\n!echo \"krish,Data Science,20000\">>test1.csv\n!echo \"sudhanshu,Data Science,30000\">>test1.csv\n!echo \"sanjay,IOT,20000\">>test1.csv\n!echo \"sunny,Big Data,35000\">>test1.csv\n!echo \"krish,Big Data,20000\">>test1.csv\n!echo \"paula,Data Science,30000\">>test1.csv\n!echo \"subham,Data Science,37000\">>test1.csv\n!echo \"Mahesh,IOT,20000\">>test1.csv\n!cat test1.csv","c0b6af94":"df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)\ndf_pyspark.show()","4a621fc3":"\ndf_pyspark.printSchema()","3ca89a90":"#Group by Operation\n# grouped to find the maximum salary\ndf_pyspark.groupby('Name').sum().show()","14b616e1":"#group departments which gives maximums salary\n\ndf_pyspark.groupby('Departments').sum().show()\n","0debe0f2":"\ndf_pyspark.groupby('Departments').mean().show()","ea610cbf":"\ndf_pyspark.groupby('Departments').mean().show()","285befc8":"df_pyspark.agg({'Salary':'sum'}).show()","3f7beea0":"!>test1.csv\n!cat test1.csv\n!echo \"Name,Age,Experience,Salary\">>test1.csv\n!echo \"sudhanshu,31,10,30000\">>test1.csv\n!echo \"sanjay,30,8,25000\">>test1.csv\n!echo \"sunny,29,4,20000\">>test1.csv\n!echo \"krish,24,3,20000\">>test1.csv\n!echo \"paula,21,1,15000\">>test1.csv\n!echo \"subham,23,2,18000\">>test1.csv\n!cat test1.csv","f9a7ed57":"training=spark.read.csv('test1.csv',header=True,inferSchema=True)\ntraining.columns\ntraining.printSchema()","c388b65f":"# [Age,Experience] -------> new feature ---------> independent feature","a83ea9ef":"from pyspark.ml.feature import VectorAssembler\nfeatureassembler=VectorAssembler(inputCols=[\"Age\",\"Experience\"],outputCol=\"Independent features\")","69b462d2":"output=featureassembler.transform(training)\noutput.show()","033c6b2e":"finalized_data = output.select(\"Independent features\",\"Salary\")","7c517880":"finalized_data.show()","57308cfd":"from pyspark.ml.regression import LinearRegression\ntrain_data,test_data=finalized_data.randomSplit([0.75,0.25])\nregressor=LinearRegression(featuresCol='Independent features',labelCol='Salary')\nregressor=regressor.fit(train_data)","4cc1e2ee":"# Coefficients\nregressor.coefficients","5192274a":"# Intercepts\nregressor.intercept","98accba6":"# prediction\npred_results=regressor.evaluate(test_data)","1d4799ac":"pred_results.predictions.show()","2d621dd1":"pred_results.meanAbsoluteError,pred_results.meanSquaredError","524a5ab4":"### Filter Operation","6ab59570":"**filter operation**","b986f12b":"****Part 3****","21cfb0aa":"**** Part 2 ****\n\n","9480f466":"### Group by and Aggregate","0930f87d":"### Example"}}