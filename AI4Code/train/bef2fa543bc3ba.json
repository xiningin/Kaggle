{"cell_type":{"054d357a":"code","f9fc753e":"code","6954ab8a":"code","c78970ef":"code","f64f535b":"code","e0f9807f":"code","f174c928":"code","c6b02bdc":"code","63dfab1f":"code","ce2cc28c":"code","01b60669":"code","7ba0f6f4":"code","9f188f61":"code","0a544172":"code","5a1a5448":"code","e35b28fb":"code","11d73d52":"code","0ad8960f":"code","557eabe6":"code","1d5e1a83":"code","5273f969":"code","df1b6ff0":"code","386a2a8d":"code","def341c1":"code","397502f3":"code","e556d64a":"code","09beff70":"code","b72e7b9f":"markdown","7e114178":"markdown","2cbacf97":"markdown","a1d27ba4":"markdown","296855e6":"markdown","b846728f":"markdown","cbb846e1":"markdown","6f5ff545":"markdown","258fc1ee":"markdown","6dbe20b3":"markdown","dec6fd6d":"markdown","20b47f7f":"markdown","5baa6d19":"markdown","8a0ba636":"markdown","1b6c3a27":"markdown","6c8b9f78":"markdown","6b4cb92b":"markdown","6499518b":"markdown","246546a7":"markdown","24ad40df":"markdown","556b604c":"markdown","8875ce0c":"markdown","41e86241":"markdown","505fb1fe":"markdown","7f16a44d":"markdown","5c539ce7":"markdown","b4a84b23":"markdown","5f952940":"markdown","56b11a9a":"markdown","c615b6e9":"markdown","8f135ef9":"markdown","83ba23f5":"markdown"},"source":{"054d357a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Basic Libraries\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings\nfrom collections import Counter\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.offline as pyo\nimport plotly.express as px\nimport plotly.graph_objs as go\npyo.init_notebook_mode()\nimport plotly.figure_factory as ff\nimport missingno as msno\n\n# Data Pre-processing Libraries\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom imblearn.over_sampling import SMOTE\n\n# Modelling Libraries\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier,PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier,NearestCentroid\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\nfrom sklearn.ensemble import VotingClassifier\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score, classification_report,confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold\nfrom xgboost.sklearn import XGBClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9fc753e":"data = pd.read_csv('..\/input\/water-potability\/water_potability.csv')","6954ab8a":"data.head(10)","c78970ef":"data.round()","f64f535b":"sns.pairplot(data, hue = \"Potability\")","e0f9807f":"corr = data.drop(\"Potability\", axis=1).corr()\n\ncorr","f174c928":"# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(corr, annot=True,  linewidths=.5, ax=ax)","c6b02bdc":"heatmap = sns.heatmap(data.corr()[['Potability']].sort_values(by='Potability', ascending = False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title(\"Features correlation with Potability\", fontdict = {'fontsize':20}, pad=30);\nheatmap.figure.set_size_inches(6,6)","63dfab1f":"data.isnull().sum()","ce2cc28c":"# Create mean and std\nph_mean = data['ph'].mean()\nSulfate_mean = data['Sulfate'].mean()\nTrihalomethanes_mean = data['Trihalomethanes'].mean()\n\nph_std = data['ph'].std()\nSulfate_std = data['Sulfate'].std()\nTrihalomethanes_std = data['Trihalomethanes'].std()\n\n# Apply random number that are ranged from (mean + std) to (mean - std)\nis_null_ph = data[\"ph\"].isnull().sum()\nrand_ph = np.random.randint(ph_mean - ph_std, ph_mean + ph_std, size = is_null_ph )\nph_slice = data['ph'].copy()\nph_slice[np.isnan(ph_slice)] = rand_ph\ndata['ph'] = ph_slice\n\n\nis_null_Sulfate = data[\"Sulfate\"].isnull().sum()\nrand_Sulfate = np.random.randint(Sulfate_mean - Sulfate_std, Sulfate_mean + Sulfate_std, size = is_null_Sulfate )\nSulfate_slice = data['Sulfate'].copy()\nSulfate_slice[np.isnan(Sulfate_slice)] = rand_Sulfate\ndata['Sulfate'] = Sulfate_slice\n\n\nis_null_Trihalomethanes = data[\"Trihalomethanes\"].isnull().sum()\nrand_Trihalomethanes = np.random.randint(Trihalomethanes_mean - Trihalomethanes_std, Trihalomethanes_mean + Trihalomethanes_std, size = is_null_Trihalomethanes )\nTrihalomethanes_slice = data['Trihalomethanes'].copy()\nTrihalomethanes_slice[np.isnan(Trihalomethanes_slice)] = rand_Trihalomethanes\ndata['Trihalomethanes'] = Trihalomethanes_slice","01b60669":"data.isnull().sum()","7ba0f6f4":"for col in data.columns:\n    \n    fig = plt.figure(figsize=(22,8))\n    \n    \n    hist = sns.histplot(data[col], color = \"springgreen\", kde = True, bins=50, label = col)\n\n    title = fig.suptitle(\"DISTRIBUTION OF \" + col, x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Lato', size=39)\n\n    plt.legend()\n    plt.show()\n    \n    \n    \n    ","9f188f61":"X = data.drop('Potability',axis=1).values\ny = data['Potability'].values","0a544172":"X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.3, random_state=101)","5a1a5448":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# This data is imbalanced that we have more Potability -0 than 1. We will oversample in the minority class first. \nsmt = SMOTE()\nX_train, y_train = smt.fit_resample(X_train, y_train)\n","e35b28fb":"from sklearn import metrics\n\n# Creating AUC plot\n\ndef model_graphs(model, model_name):\n    \n    y_pred_prob = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_prob)\n    auc = metrics.roc_auc_score(y_test, y_pred_prob)\n    plt.plot(fpr,tpr,label= model_name +\" auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()","11d73d52":"# Create confusion matrix to check accuracy, F1 score, and other \n\ndef confusion_matrix_graphs(y_pred):\n\n    sns.heatmap(pd.DataFrame(confusion_matrix(y_test, y_pred)), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","0ad8960f":"# 5 folds validation and check the means accuracy score \n\ndef test_val_score(model):\n    model_cross_val_score = cross_val_score(model, X_test, y_test, scoring='accuracy', cv = 5).mean()\n    \n    print(\"===========================================================\")\n\n    print(\"The 5 fold cross value score is {:.2f}\". format(model_cross_val_score))\n    \n    print(\"===========================================================\")","557eabe6":"from sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_lr_pred = lr.predict(X_test)\n\ntest_val_score(lr)\n\n\nprint(classification_report(y_lr_pred, y_test))\n\nconfusion_matrix_graphs(y_lr_pred)\nmodel_graphs(lr, \"Logistic Regression\")","1d5e1a83":"dt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)\ny_dt_pred = dt.predict(X_test)\n\ntest_val_score(dt)\n\nprint(classification_report(y_dt_pred, y_test))\nconfusion_matrix_graphs(y_dt_pred)\nmodel_graphs(dt, \"Decision Tree\")","5273f969":"rf = RandomForestClassifier()\nrf = rf.fit(X_train, y_train)\ny_rf_pred = rf.predict(X_test)\n\ntest_val_score(rf)\n\nprint(classification_report(y_rf_pred, y_test))\nconfusion_matrix_graphs(y_rf_pred)\nmodel_graphs(rf, \"RandomForest\")","df1b6ff0":"KNN = KNeighborsClassifier()\nKNN = KNN.fit(X_train, y_train)\ny_knn_pred = KNN.predict(X_test)\n\ntest_val_score(KNN)\n\nprint(classification_report(y_knn_pred, y_test))\n\nconfusion_matrix_graphs(y_knn_pred)\n\nmodel_graphs(KNN, \"KNN\")","386a2a8d":"GNB = GaussianNB()\nGNB = GNB.fit(X_train, y_train)\ny_GNB_pred = GNB.predict(X_test)\n\ntest_val_score(GNB)\n\nprint(classification_report(y_GNB_pred, y_test))\n\nconfusion_matrix_graphs(y_GNB_pred)\n\nmodel_graphs(GNB, \"GNB\")","def341c1":"ETC = ExtraTreesClassifier()\nETC = ETC.fit(X_train, y_train)\ny_ETC_pred = ETC.predict(X_test)\n\ntest_val_score(ETC)\n\nprint(classification_report(y_ETC_pred, y_test))\n\nconfusion_matrix_graphs(y_ETC_pred)\n\nmodel_graphs(ETC, \"ETC\")","397502f3":"XGB = XGBClassifier()\nXGB = XGB.fit(X_train, y_train)\ny_XGB_pred = XGB.predict(X_test)\n\ntest_val_score(XGB)\n\nprint(classification_report(y_XGB_pred, y_test))\n\nconfusion_matrix_graphs(y_XGB_pred)\n\nmodel_graphs(XGB, \"XGB\")","e556d64a":"# The combined model--logistic regression and gradient boosted trees\nestimators = [('Logistric Regression', lr), ('Random_Forest', rf), ('Naive Bayes', GNB), ('DecisionTree', dt), ('KNN', KNN), (\"ExtraTrees Classifier\", ETC), (\"Xgb Classifier\" , XGB)]\n\n# Though it wasn't done here, it is often desirable to train \n# this model using an additional hold-out data set and\/or with cross validation\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train, y_train)\ny_VC_pred = VC.predict(X_test)\n\ntest_val_score(VC)\n\nprint(classification_report(y_VC_pred, y_test))\n\nconfusion_matrix_graphs(y_VC_pred)\n\nmodel_graphs(VC, \"Voting Classifier\")","09beff70":"accuracy_list = {'LogisticRegression': accuracy_score(y_test, y_lr_pred), \n                 'DecisionTreeClassifier': accuracy_score(y_test, y_dt_pred),\n                 'RandomForestClassifier':accuracy_score(y_test, y_rf_pred), \n                 'Naive Bayes':accuracy_score(y_test, y_GNB_pred),\n                 'KNN':accuracy_score(y_test, y_knn_pred),\n                 'ExtraTrees Classifier':accuracy_score(y_test, y_ETC_pred),\n                 'Xgb Classifier':accuracy_score(y_test, y_XGB_pred),\n                 'VotingClassifier':accuracy_score(y_test, y_VC_pred)}\n                     \n        \nprint(pd.Series(data=accuracy_list, index=['LogisticRegression', 'DecisionTreeClassifier', \n                                           'RandomForestClassifier', 'Naive Bayes','KNN','ExtraTrees Classifier',\n                                           'Xgb Classifier','VotingClassifier']))","b72e7b9f":"### Pair plot\nLet's make plots to check whether there are high correlation between each features. ","7e114178":"# Modeling ","2cbacf97":"Split the data and standardizing them!","a1d27ba4":"# Xgb Classifier","296855e6":"## Missing Data","b846728f":"Let's check where there are missing value","cbb846e1":"### Heatmap\n\nLet's confirm the correlation between each features","6f5ff545":"# Decision Tree","258fc1ee":"# Conclusion\n\n1. Most data belongs to Potability-0 as shown in our pairplot. \n2. This dataset is imbalanced. May be drinkable water data is hard to collect or there are not much drinkable water in this research. \n3. The features in this dataset does not have high correlation.\n\n","6dbe20b3":"We will create functions to look at AUC graph, confusion matrix and test value score to determine whether this model is valid, ","dec6fd6d":"# KNN ","20b47f7f":"Logistric Regression score bad. It is about 0.5. This model is just lighty better than random 50\/50 guess","5baa6d19":"# Voting Classifier","8a0ba636":"Much better than Decison Tree. The model socre is around 0.67","1b6c3a27":"## Exploratory Data Analysis ","6c8b9f78":"## Visualizations","6b4cb92b":"\n![05-water.png](attachment:f214dee2-2545-4b0a-9194-ee0cd8ee966f.png)\n\n# Introduction\n\n## Context\nAccess to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions.\n\n\n**Water Quality Features**\n\n\n1. **ph**: pH of 1. water (0 to 14).\n\n2. **Hardness**: Capacity of water to precipitate soap in mg\/L.\n\n3. **Solids**: Total dissolved solids in ppm.\n\n4. **Chloramines**: Amount of Chloramines in ppm.\n\n5. **Sulfate**: Amount of Sulfates dissolved in mg\/L.\n\n6. **Conductivity**: Electrical conductivity of water in \u03bcS\/cm.\n\n7. **Organic_carbon**: Amount of organic carbon in ppm.\n\n8. **Trihalomethanes**: Amount of Trihalomethanes in \u03bcg\/L.\n\n9. **Turbidity**: Measure of light emiting property of water in NTU.\n\n10. **Potability**: Indicates if water is safe for human consumption. Potable - 1 and Not potable - 0","6499518b":"The best model so far. The score is around 0.67","246546a7":"# Naive Bayes","24ad40df":"Better than logistric regression model. But still not so well. ","556b604c":"PH, Sulfate and Trihalomethanes have missing data. \n\nWe are going to fill them with random number that are ranged from (mean + std) to (mean - std). ","8875ce0c":"Let's check out distribution of the features and draw plots!","41e86241":"Each feature (except Potability) looks like mostly equally distributed. Let's make prediction models!","505fb1fe":"We have tried 8 models. The best model so far is Extra Tree Classifier, Voting Classifier and Random Forest. They produce almost the same score. As we dicuss in correlation plot, each feature does not have enough information to distinguish the Potability. The best score we can get is around 0.67 from our models","7f16a44d":"Let's confirm the correlation between the target-\"Potability\" and each feature","5c539ce7":"We can see that cach feature dont have high correlation. Potability-\"0\" has more data than \"1\"","b4a84b23":"# Logistric Regression","5f952940":"As we can see the graph above, each feature does not produce high correlation. The highest correlation features are Solids and Organic_carbon","56b11a9a":"# Random Forest ","c615b6e9":"# ExtraTreesClassifier","8f135ef9":"### Missing data and numbers are decimals \n\nWe see that there is Nah in Ph, Sulfate and other features. We will deal with the missing data later.\n\nWe see that most numbers have many decimals. We will round them up so we they are easy to read","83ba23f5":"Let's check a number of missing data of each feature "}}