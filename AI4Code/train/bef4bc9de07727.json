{"cell_type":{"75072c4e":"code","e9beb865":"code","8687d066":"code","7b08d62d":"code","dd61a8b8":"code","350abd19":"code","331b2472":"code","753e362d":"code","eaf96152":"code","a01e359b":"code","d40f373a":"code","5e5193c3":"code","8b2589bd":"code","110feb01":"code","d5eee8ac":"code","e28d5c87":"markdown","bb0ca29c":"markdown"},"source":{"75072c4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e9beb865":"from __future__ import print_function, division, absolute_import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch.nn.functional as F\nimport os\n\n# Any results you write to the current directory are saved as output.\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm.notebook import tqdm\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nimport cv2","8687d066":"class Selayer(nn.Module):\n\n    def __init__(self, inplanes):\n        super(Selayer, self).__init__()\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(inplanes, int(inplanes \/ 16), kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(int(inplanes \/ 16), inplanes, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n\n        out = self.global_avgpool(x)\n\n        out = self.conv1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.sigmoid(out)\n\n        return x * out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n\n        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n                               padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 2)\n\n        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n\n        self.selayer = Selayer(planes * 4)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out = self.selayer(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SeResNeXt(nn.Module):\n    def __init__(self, block, layers, cardinality=32, num_classes=1000):\n        super(SeResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.cardinality))\n                             \n        # vowel_diacritic\n        self.fc1 = nn.Linear(2048,11)\n        # grapheme_root\n        self.fc2 = nn.Linear(2048,168)\n        # consonant_diacritic\n        self.fc3 = nn.Linear(2048,7)\n        return nn.Sequential(*layers)\n        \n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        \n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x3 = self.fc3(x)\n        \n        return x1,x2,x3\n\n\ndef se_resnext50(**kwargs):\n    \"\"\"Constructs a SeResNeXt-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = SeResNeXt(Bottleneck, [3, 4, 6, 3],**kwargs)\n    return model\n\n\ndef se_resnext101(**kwargs):\n    \"\"\"Constructs a SeResNeXt-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = SeResNeXt(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef se_resnext152(**kwargs):\n    \"\"\"Constructs a SeResNeXt-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = SeResNeXt(Bottleneck, [3, 8, 36, 3],**kwargs)\n    return model","7b08d62d":"test = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')","dd61a8b8":"class GraphemeDataset(Dataset):\n    def __init__(self,df,_type='train'):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        image = self.df.iloc[idx][1:].values.reshape(SIZE,SIZE).astype(float)\n        return image, self.df.iloc[idx][0]","350abd19":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = se_resnext50().to(device)\n# model.load_state_dict(torch.load('\/kaggle\/input\/se-resnext50-baseline\/se_resnext50.pth'))\nmodel.load_state_dict(torch.load('\/kaggle\/input\/testnow\/try.pth'))","331b2472":"SIZE = 128\ndef Resize(df,size=SIZE):\n    resized = {} \n    df = df.set_index('image_id')\n    for i in tqdm(range(df.shape[0])):\n        image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n        resized[df.index[i]] = image.reshape(-1)\n    resized = pd.DataFrame(resized).T.reset_index()\n    resized.columns = resized.columns.astype(str)\n    resized.rename(columns={'index':'image_id'},inplace=True)\n    return resized","753e362d":"%%time\nmodel.eval()\ntest_data = ['test_image_data_0.parquet','test_image_data_1.parquet','test_image_data_2.parquet','test_image_data_3.parquet']\npredictions1 = []\npredictions2 = []\npredictions3 = []\nrow_ids = []\nbatch_size=256\nfor fname in test_data:\n    start = time.time()\n    data = pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/{fname}')\n    data = Resize(data)\n    e_time = time.time() - start\n    print (\"e_time:{0}\".format(e_time) + \"[s]\")\n    test_image = GraphemeDataset(data)\n    test_loader = torch.utils.data.DataLoader(test_image,batch_size=batch_size,num_workers=4,shuffle=False)\n    with torch.no_grad():\n        for inputs,names in tqdm(test_loader):\n            for name in names:\n                row_ids += [f\"{name}_consonant_diacritic\",f\"{name}_grapheme_root\",f\"{name}_vowel_diacritic\"]\n            inputs.to(device)\n            \n            outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float().cuda())\n            predictions1.append(outputs3.argmax(1).cpu().detach().numpy())\n            predictions2.append(outputs2.argmax(1).cpu().detach().numpy())\n            predictions3.append(outputs1.argmax(1).cpu().detach().numpy())","eaf96152":"import gc\ndel model,data,test_image,test_loader\ngc.collect()","a01e359b":"predictions1 = np.array(predictions1)\npredictions1 = predictions1.flatten()\npredictions1 = np.hstack(predictions1)\npredictions1","d40f373a":"predictions2 = np.array(predictions2)\npredictions2 = predictions2.flatten()\npredictions2 = np.hstack(predictions2)\npredictions2","5e5193c3":"predictions3 = np.array(predictions3)\npredictions3 = predictions3.flatten()\npredictions3 = np.hstack(predictions3)\npredictions3","8b2589bd":"pred = [[predictions1[i],predictions2[i],predictions3[i]] for i in range(len(predictions1))]\npred = np.hstack(np.hstack(pred))\npred","110feb01":"# # submission = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/sample_submission.csv')\n# submission['row_id'] = row_ids\n# submission['target'] = pred\nsubmission = pd.DataFrame({'row_id':row_ids,'target':pred},columns=['row_id','target'])\nsubmission","d5eee8ac":"submission.to_csv('submission.csv',index=False)","e28d5c87":"# I solved this problem!\n# The key is num_workers\n# And actually my answers are shuffled.\n# These are my solution. guys who has same problem, you can try inference using train_data and check result.\n# predictions = predictions1 = np.array(predictions1)\n# predictions1 = predictions1.flatten()\n# predictions1 = np.hstack(predictions1)\n# Thank you all guys who help me. ","bb0ca29c":"# this kernel is based on @mobassir 's kernel https:\/\/www.kaggle.com\/mobassir\/se-resnext50-pytorch-baseline-for-bengali\/comments#742228\n# I trained model on local and I just load the weight. Why I can't submit this by this kernel?\n# can anyone help me? ;(\n# If you have question, please ask me.\n# updated: I could submit by this kernel.\n# another problem has occur.\n# my score will be 0.0614 that is sample score.\n# I'm so sad ;("}}