{"cell_type":{"0b48a2db":"code","e9355a56":"code","54cbb8e2":"code","ccd8ed4f":"code","89661575":"code","6ba362b8":"code","cb37ae7a":"code","6b5cafd1":"code","b4673aec":"code","96686578":"code","369a1cfa":"code","83615406":"code","7ba2b12f":"code","4e28de31":"code","1f84dc0d":"code","6306c9f2":"code","31fbfb35":"code","71f1518b":"code","989991ab":"code","09551ccc":"code","23d3ba87":"code","24f6ccc9":"code","8aa7ecdb":"code","3b8692f2":"code","28925742":"code","157f2eb2":"code","e796b5ff":"code","cdf8ea9a":"code","fde1a39d":"code","4c14c96d":"code","0a169802":"code","3f5789c0":"code","a2e2ae3e":"code","ccaa2082":"code","1de03555":"code","54e70fea":"code","3c1ce1a9":"code","6c3b4992":"code","3c135c11":"code","a5a292a3":"code","59b933b5":"markdown","8c7cc515":"markdown","055c2abf":"markdown","7be3a008":"markdown","8c5e5f45":"markdown","3cd04e51":"markdown","2b18dc42":"markdown","9ae441be":"markdown"},"source":{"0b48a2db":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline","e9355a56":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain = train[['text','target']]\ntrain.drop_duplicates(inplace=True)\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest = test[['id','text']]","54cbb8e2":"import string\nimport re\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef clean(text):\n    # only aplhabest\n    text =  re.sub(r'[^A-Za-z ]+', '', text) \n    # converting all the teweets to lower case\n    text = text.lower()\n    # remove urls and html tags\n    url = re.compile(r\"https?:\/\/\\s+|www\\.\\S+\")\n    text = url.sub(r\"\", text)\n    html = re.compile(r\"<.*?>\")\n    text = html.sub(r\"\",text)\n    # removing emojis\n    emoji = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji.sub(r'',text)\n    #removing punctutions\n    punctuations = str.maketrans(\"\",\"\", string.punctuation)\n    text = text.translate(punctuations)\n    #lemmatization\n    text = text.split(' ')\n    text = [lemmatizer.lemmatize(i) for i in text]\n    text = ' '.join(text) \n    return text","ccd8ed4f":"train['text'] = train.text.apply(lambda x: clean(x))","89661575":"def tweet_cleaner(tweet):\n    # Acronyms and miswritten words\n    tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"year-old\", \"years old\", tweet)\n    tweet = re.sub(r\"yearold\", \"years old\", tweet)\n    tweet = re.sub(r\"yr old\", \"years old\", tweet)\n    tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n    tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n    tweet = re.sub(r\"rly\", \"railway\", tweet)\n    tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n    tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n    tweet = re.sub(r\"pm\", \"\", tweet)\n    tweet = re.sub(r\"PM\", \"\", tweet)\n    tweet = re.sub(r\"nan\", \" \", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n    tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n    tweet = re.sub(r\"RT\", \"retweet\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n\n    # Special characters\n    tweet = re.sub(r\"%20\", \" \", tweet)\n    tweet = re.sub(r\"%\", \" \", tweet)\n    tweet = re.sub(r\"@\", \" \", tweet)\n    tweet = re.sub(r\"#\", \" \", tweet)\n    tweet = re.sub(r\"'\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00fb_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\u00f2\", \" \", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"re\\x89\u00fb_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00db\", \" \", tweet)\n    tweet = re.sub(r\"re\\x89\u00db\", \"re \", tweet)\n    tweet = re.sub(r\"re\\x89\u00fb\", \"re \", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\u00aa\", \"'\", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\u00f2\", \" \", tweet)\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n\n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"Im\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"can not\", tweet)\n    tweet = re.sub(r\"cant\", \"can not\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"dont\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)\n    return tweet","6ba362b8":"train['text'] = train.text.apply(lambda x: tweet_cleaner(x))","cb37ae7a":"from nltk.corpus import stopwords\nstop = set(stopwords.words(\"english\"))\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    return \" \".join(text)\ntrain[\"text\"] = train[\"text\"].map(remove_stopwords)","6b5cafd1":"train.text","b4673aec":"from nltk.tokenize import word_tokenize\nimport nltk\n\ndef create_corpus_tk(df):\n    corpus=[]\n    for text in train[\"text\"]:\n        words = [word.lower() for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus","96686578":"corpus = create_corpus_tk(train)","369a1cfa":"num_words = len(corpus)\nprint(num_words)","83615406":"train_size = int(train.shape[0]*0.8)\n\ntrain_sentences = train.text[:train_size]\ntrain_labels = train.target[:train_size]\n\ntest_sentences = train.text[train_size:]\ntest_labels = train.target[train_size:]","7ba2b12f":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmax_len = 50","4e28de31":"tokenizer = Tokenizer(num_words = num_words)\ntokenizer.fit_on_texts(train_sentences)","1f84dc0d":"train_sequences = tokenizer.texts_to_sequences(train_sentences)","6306c9f2":"train_sentences.head()","31fbfb35":"train_sequences[1:5]","71f1518b":"# padding our sequences to pass it in keras model (inputs should be of the same size)\ntrain_padded = pad_sequences(\n    train_sequences, maxlen=max_len, truncating=\"post\", padding=\"post\"\n)","989991ab":"train_padded","09551ccc":"# padding the test data\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(\n    test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n)","23d3ba87":"test_padded","24f6ccc9":"print(train.text[0])\nprint(train_sequences[0])","8aa7ecdb":"# creating a dictionary out of the words and their respective sequences\nword_index = tokenizer.word_index\nprint(len(word_index))","3b8692f2":"word_index[\"like\"]","28925742":"embedding_dict = {}\nwith open(\"..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.100d.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.array(values[1:],\"float32\")\n        embedding_dict[word] = vectors\nf.close()","157f2eb2":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words,100))\n\nfor word, i in word_index.items():\n    if i<num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec","e796b5ff":"embedding_matrix","cdf8ea9a":"print(train_padded.shape)\nprint(train_labels.shape)","fde1a39d":"print(test_padded.shape)\nprint(test_labels.shape)","4c14c96d":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import adam_v2\n\nmodel = Sequential()\n\nmodel.add(\n    Embedding(\n        num_words,\n        100,\n        embeddings_initializer = Constant(embedding_matrix),\n        input_length = max_len,\n        trainable = False\n    )\n)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n# model.add(LSTM(100,dropout=0.1))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\noptimizer = adam_v2.Adam(learning_rate=5e-3)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nmodel.summary() ","0a169802":"history  = model.fit(\n    train_padded,\n    train_labels,\n    epochs=15,\n    validation_data=(test_padded, test_labels),\n    verbose=1,\n    batch_size=32\n)","3f5789c0":"# test data for submission\ntest['text'] = test.text.apply(lambda x: clean(x))\ntest[\"text\"] = test[\"text\"].map(remove_stopwords)\ntest['text'] = test.text.apply(lambda x: tweet_cleaner(x))","a2e2ae3e":"sequences = tokenizer.texts_to_sequences(test.text)\npadded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")","ccaa2082":"pred = model.predict(padded)\npred_int = pred.round().astype(\"int\")","1de03555":"pred","54e70fea":"pred_int.shape","3c1ce1a9":"test.shape","6c3b4992":"test[\"target\"] = pred_int\nsubmission = test[['id','target']]","3c135c11":"submission.to_csv(\"submission.csv\", index=False)","a5a292a3":"test.head(100)","59b933b5":"### Cleaning the tweets","8c7cc515":"### Creating the Embedding dict","055c2abf":"### Train\/Test Split","7be3a008":"### Convert texts to Sequence of numbers","8c5e5f45":"### Baseline Model with GloVe","3cd04e51":"### Embeddings using GloVe","2b18dc42":"### Removing stop words","9ae441be":"### Reading the Data"}}