{"cell_type":{"c487d321":"code","50d8b02c":"code","ee8f8c5e":"code","fedbe5a9":"code","c0aa09a2":"code","dbdeecde":"code","8671d36a":"code","10053b09":"code","6e56dfe3":"code","e714c807":"code","fc647c3e":"code","d0014c56":"code","2fa4b5ff":"code","174869c8":"code","a3293d9c":"code","46bb0275":"code","db94af01":"code","1c770bb6":"code","179c192b":"code","a2a7ba05":"code","72c1500c":"code","77c61405":"code","7670546e":"code","8fc9ce19":"code","aad29b83":"code","dc68ffba":"code","bc964680":"code","95be092e":"code","b245ea17":"code","17431f74":"code","7bde04d5":"code","4dbb272a":"markdown","d6b7d20f":"markdown","38f13523":"markdown","a7704ad0":"markdown","cf7d7804":"markdown","446008aa":"markdown","575b796f":"markdown","627401a4":"markdown","13146018":"markdown","396ed740":"markdown","205b13be":"markdown","f3a66c41":"markdown","493205a5":"markdown","fbd36568":"markdown","4116d02c":"markdown","bf083334":"markdown","ffdaef33":"markdown","67c99257":"markdown","b0ec3672":"markdown","c537186b":"markdown","0cca87f8":"markdown","402e0d35":"markdown","93ebe6bf":"markdown","d92d0281":"markdown","e1125c9c":"markdown","602c2529":"markdown","9416fec9":"markdown","d99f9d8b":"markdown","954951c0":"markdown","5efa5eac":"markdown","7dbe125b":"markdown","e067181a":"markdown","31e9571e":"markdown","60bb68e1":"markdown","02251b20":"markdown","7418a1fe":"markdown","2222bdeb":"markdown","0cc289a6":"markdown","3eec01b3":"markdown","95e1399d":"markdown","8c2d633b":"markdown","e09c1887":"markdown","2fda23f8":"markdown","faf0bf7c":"markdown","2d0d675b":"markdown","b4701eba":"markdown"},"source":{"c487d321":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import datasets\ndiabetes = datasets.load_diabetes()\ndf = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\ndf.head()","50d8b02c":"X = df['bmi'].values\nY = diabetes.target\n\nplt.scatter(X, Y);\nplt.xlabel('Body mass index (BMI)');\nplt.ylabel('Disease progression');","ee8f8c5e":"import random\nrandom.seed(0)\nidx = random.sample(range(len(df)), 5)\nx1, y1 = X[idx], Y[idx]\nplt.scatter(x1, y1);","fedbe5a9":"def plot_line(w, b):\n    x_values = np.linspace(X.min(), X.max(), 100)\n    y_values = w*x_values + b\n    plt.plot(x_values, y_values, 'r-')","c0aa09a2":"w = 1300\nb = 130\nplt.scatter(x1, y1);\nplot_line(w, b);","dbdeecde":"random.seed(12)\nidx = random.sample(range(len(df)), 5)\nx2, y2 = X[idx], Y[idx]\nplt.scatter(x1, y1);\nplt.scatter(x2, y2);\nplot_line(w, b);","8671d36a":"w = 1400\nb = 140\nplt.scatter(x1, y1);\nplt.scatter(x2, y2);\nplot_line(w, b);","10053b09":"x = np.concatenate([x1, x2])\ny = np.concatenate([y1, y2])\ny_pred = w*x + b\nerror = y - y_pred\npd.DataFrame({'x': x, 'y': y, 'y_pred': y_pred, \n              'error': error})","6e56dfe3":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()","e714c807":"x = x.reshape(-1, 1)\nlin_reg.fit(x, y)","fc647c3e":"w = lin_reg.coef_[0]\nb = lin_reg.intercept_\nw, b","d0014c56":"plt.scatter(x, y);\nplot_line(w, b);","2fa4b5ff":"X = X.reshape(-1, 1)\nlin_reg.fit(X, Y)\nw = lin_reg.coef_[0]\nb = lin_reg.intercept_\nplt.scatter(X, Y);\nplot_line(w, b);","174869c8":"lin_reg.score(X, Y)","a3293d9c":"# Create dataset for classification\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, \n                           n_redundant=0, n_informative=2, \n                           n_clusters_per_class=2, \n                           random_state=1)","46bb0275":"plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n            s=25, edgecolor='k');","db94af01":"# Train a classifier\nfrom sklearn.linear_model import LogisticRegression\nLR_clf = LogisticRegression()","1c770bb6":"LR_clf.fit(X, y)","179c192b":"print('Accuracy of Logistic regression classifier: {:.2f}'\n     .format(LR_clf.score(X, y)))","a2a7ba05":"def plot_decision_boundary(model, X, y):\n    x1, x2 = X[:, 0], X[:, 1]\n    x1_min, x1_max = x1.min() - 1, x1.max() + 1\n    x2_min, x2_max = x2.min() - 1, x2.max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n                         np.arange(x2_min, x2_max, 0.1))\n\n    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n    plt.contourf(xx1, xx2, Z, alpha=0.4)\n    plt.scatter(x1, x2, c=y, marker='o',\n                s=25, edgecolor='k');","72c1500c":"plot_decision_boundary(LR_clf, X, y)","77c61405":"# Split the dataset into testing and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","7670546e":"LR_clf = LogisticRegression().fit(X_train, y_train)","8fc9ce19":"print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(LR_clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(LR_clf.score(X_valid, y_valid)))","aad29b83":"# First we define a classifier, we do not need to train it\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\n\n# cross_val_score method makes k-folds and train the \n# classifier k times and returns k scores for each run\nfrom sklearn.model_selection import cross_val_score\n# accuracy is the default scoring metric\nprint('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=5))\n# use Area Under ROC as scoring metric\nprint('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=10, scoring = 'roc_auc'))\n# use recall as scoring metric\nprint('Cross-validation (recall)', cross_val_score(clf, X, y, cv=3, scoring = 'recall'))\n# use precision as scoring metric\nprint('Cross-validation (precision)', cross_val_score(clf, X, y, cv=3, scoring = 'precision'))\n# use F1-score as scoring metric\nprint('Cross-validation (F1-score)', cross_val_score(clf, X, y, cv=3, scoring = 'f1'))","dc68ffba":"# First we create a dataset for demonstration\nfrom sklearn.datasets import make_classification\nX1, y1 = make_classification(\n    n_samples=300, n_features=2, \n    n_redundant=0, n_informative=2, \n    n_classes=2, n_clusters_per_class=1, \n    class_sep=1, weights=[0.8, 0.2],\n    flip_y=0.05, random_state=0 \n)\n\n# We fit the PCA transformer and transform our dataset\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X1)\nX_pca = pca.transform(X1)\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Before PCA\")\nplt.axis(\"equal\")\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=y1,\n            s=25, edgecolor='k');\nplt.subplot(1, 2, 2)\nplt.title(\"After PCA\")\nplt.axis(\"equal\")\nplt.scatter(X_pca[:, 0], X_pca[:, 1], marker='o', c=y1,\n            s=25, edgecolor='k');\nplt.tight_layout()","bc964680":"# Create dataset for classification\nfrom sklearn.datasets import make_classification\nX2, y2 = make_classification(\n    n_samples=400, n_features=2, \n    n_redundant=0, n_informative=2, \n    n_classes=2, n_clusters_per_class=1, \n    class_sep=1, weights=[0.9, 0.1],\n    flip_y=0.15, random_state=0 \n)\n\n# Split the dataset into testing and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X2, y2, random_state=0)\n\n# Train a classifier\nfrom sklearn.linear_model import LogisticRegression\nLR_clf = LogisticRegression().fit(X_train, y_train)\n\n# Compute confusin matrix\nfrom sklearn.metrics import confusion_matrix\ny_predicted = LR_clf.predict(X_valid)\nconfusion = confusion_matrix(y_valid, y_predicted)\nprint('Confusion Matrix\\n', confusion)","95be092e":"y_proba = LR_clf.predict_proba(X_valid)\ny_proba_list = list(zip(y_valid[0:15], y_predicted[0:15], y_proba[0:15, 1]))\nprint(\"(Actual class, Predicted class, probability that an observation belongs to class 1):\") \ny_proba_list","b245ea17":"from sklearn.metrics import precision_recall_curve\n\ny_scores = LR_clf.decision_function(X_valid)\nprecision, recall, thresholds = precision_recall_curve(y_valid, y_scores)\nclosest_zero = np.argmin(np.abs(thresholds))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure(figsize=(6, 6))\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.title('Precision-Recall Curve', fontsize=18)\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.legend(loc='lower left', fontsize=12)\nplt.axes().set_aspect('equal')\nplt.show()","17431f74":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\naccuracy = accuracy_score(y_valid, y_predicted)\nprecision = precision_score(y_valid, y_predicted)\nrecall = recall_score(y_valid, y_predicted)\nf1 = f1_score(y_valid, y_predicted)\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1:', f1)","7bde04d5":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, _ = roc_curve(y_valid, y_scores)\nroc_auc_lr = auc(fpr, tpr)\n\nplt.figure(figsize=(7, 7))\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr, tpr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve', fontsize=16)\nplt.legend(loc='lower right', fontsize=12)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","4dbb272a":"Let us see a demonstrative example of simple linear regression using the [diabetes dataset](https:\/\/scikit-learn.org\/stable\/datasets\/index.html#diabetes-dataset) from the [sklearn.datasets](https:\/\/scikit-learn.org\/stable\/datasets\/index.html). First we import python modules and the dataset:","d6b7d20f":"### Precision and Recall\nSuppose we are trying to build a search engine using a training set of a million articles. For a search keyword, our algorithm comes up with 300 results. There are two questions we could ask. \n\n1. How many of these search results are relevant to our search keyword? (Precision)\n2. Of all the pages relevant to the search keyword, how many are included in the search results? (Recall)\n\nPrecision and recall are formulated as follows.\n\n$$\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Predicted Positives}}$$\n\n$$\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Actual Positives}}$$\n\n* Precision: How useful are the results?\n* Recall: How complete are the results?\n\nThere is a trade-off between the precision and recall. If we want to recall all the relevant pages (high recall), we will retrieve many more results, more of them irrelevant and that will lower our precision. Similarly, if we want our results to be highly accurate (high precision), we will be more selective and might end up missing a large number of relevant results (low recall). We might want to choose one over the other depending on our objective. \n\nFor some situations, high recall is more desirable than high precision. For example, if you are building a machine learning model for a bank to detect fradulent transactions, you want it to catch every fradulent transaction, even if it sometimes raises suspicion for some legit transactions. Another example would be inspecting airplane safety features, you would want to target for high recall even at the cost of low precision.\n\nThe justice systems for most democratic nations aims for high precision and assume people are innocent until proven guilty. Jurisprudence prefers that some culprits may let go free (low recall) but no innocent should be punished (high precision). \n\n![](https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/images\/mlconcepts_image2.png)\n\nNote: In the above figure, the x-axis corresponds to the probability scores of the classifier and the cut-off means the decision threshold for the probability.\n\nHigher precision (or higher recall) is achieved by increasing (or decreasing) the decision threshold. \n\n* Higher precision: Raising the decision threshold for the probability (or sliding the cut-off line to the right) will eliminate more False Postives albeit at the cost of including more False Negatives. \n\n* Higher recall: Lowering the decision threshold for the probability (or sliding the cut-off line to the left) will eliminate more False Negatives albeit at the cost of including more False Positives. \n\n![](https:\/\/www.qualtrics.com\/support\/wp-content\/uploads\/2017\/07\/precision-recall-curve-for-regression-docs1.png)","38f13523":"#### Steps for model building will be as follows.\n\nSplit the dataset into training and testing sets:","a7704ad0":"## Classification: \n- Predicting a label to classify the data points.\n- Finding a decision boundary using a labeled training dataset to determine labels for unseen data.\n\nAn example of binary classification algorithm: Logistic regression - separates the classes using a linear boundary \n\n<img src=\"https:\/\/camo.githubusercontent.com\/f663cd4f29335972950dded4d422c07aeee8af55\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Logistic Regression classifier <\/p>","cf7d7804":"Now, we test the accuracy of the classifier on both training and testing dataset.","446008aa":"#### What does the machine learn?\n\nAnswer: Model parameters (or weights) specific for each classification\/regression algorithm.\n\n#### How does the machine learn the parameters (or weights)?\n\nModel parameters (or weights) are updated to keep on ***minimizing the cost function*** iteratively using the training data. The cost has distinctive mathematical formulations for various algorithms but the gist of the learning process is the same.\n","575b796f":"In a nutshell, we are trying to find the optimal value for the slope and intercept for the line and we are learning from the training examples. Our objective is to minimize the difference between the actual $y$ value and the value predicted using the line. \n\n<img src=\"http:\/\/www.ken-szulczyk.com\/misc\/statistics\/linear_regression.png\" width=\"300\" height=\"300\" \/>","627401a4":"As we add more points, the slop and intercept of the line needs to be adjusted accordingly.","13146018":"### Comparing Precision-Recall  and ROC curve:\n\nBoth the Precision-Recall  and ROC curves try to capture the trade-off between false positives and false negatives. True Positive Rate is nothing but the accuracy of the positive class whereas False Positive Rate measures the inaccuracy of the negative class. Thus, ROC curve studies the trade-off between the accuracy of two classes without weighing one class over the other. Precision-recall curve is [preferable](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/) to ROC curve in case of imbalanced classes. Precision-recall curve focuses on two aspects - how accurately and completely do we classify the positive class? Precision and\/or recall are useful when our dataset contains fewer positive examples and we are more concerned about accurately and\/or completely classifying them. \n\nIn the confusion matrix, the two horizontal ellipsis corresponds to TPR (or recall) and FPR whereas the vertical one corresponds to Precision.\n\n![](https:\/\/github.com\/AashitaK\/ML-Workshops\/blob\/master\/Session%204\/figures\/Binary_confusion_matrix.png?raw=true)\n\n* Black: Precision\n* Purple: Recall and True Positive Rate (TPR)\n* Pink: False Positive Rate (FPR)\n\nNote: The arrangement of rows and columns in this matrix is different from the one at the top. Scikit-learn follows the convention of the matrix at the top.\n\nThe formulations for quick reference:\n\n$$\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Predicted Positives}}$$\n\n$$\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Actual Positives}}$$\n\n$$\\text{True Positive Rate (TPR)} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Actual Positives}}$$\n\n$$\\text{False Positive Rate (FPR)} = \\frac{\\text{False Positives (FP)}}{\\text{False Positives (FP) + True Negatives (TN)}}\n= \\frac{\\text{False Positives (FP)}}{\\text{Total Actual Negatives}}$$","396ed740":"***When to stop the iterative learning process? Until the cost function has reached its minimum value?*** ","205b13be":"## A Hands-on Introduction to Machine Learning using Python on Kaggle \n\n### Session 2 of Workshop series at Murty Sunak Quantitative and Computing Lab, CMC\n\n#### Instructor: Aashita Kesarwani\n\n\nThis is the notebook for the second session of [A Hands-on Introduction to Machine Learning using Python on Kaggle workshop series at CMC](https:\/\/github.com\/CMC-QCL\/A-hands-on-introduction-to-Machine-Learning-using-Python-on-Kaggle\/blob\/master\/Session%201\/Session%201.ipynb).\n\n### Supervised learning algorithms:\n- Regression\n- Classification \n\n## Regression:  \n- Determining the impact of response variables on the target variable. \n- Fitting a curve using training data to estimate target variable for unseen data\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/8a\/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Regression curve <\/p> ","f3a66c41":"Test the accuracy on both training and validation set:","493205a5":"Next, we train the regressor on the entire dataset and fit a line.","fbd36568":"### Area Under ROC curve:\n\nROC (Receiver Operating Characteristic) curve tells us how good is a classifier in **predicting the difference between two classes**. The binary classifier usually outputs the probability that an observation belongs to the positive class. If the probabilities for most observations are closer to 0 or 1, it is considered good in distinguish the two classes, whereas it is said to perform poorly if most probabilities are closer to 0.5. \n\n$$\\text{True Positive Rate (TPR)} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n= \\frac{\\text{True Positives (TP)}}{\\text{Total Actual Positives}}$$\n\n$$\\text{False Positive Rate (FPR)} = \\frac{\\text{False Positives (FP)}}{\\text{False Positives (FP) + True Negatives (TN)}}\n= \\frac{\\text{False Positives (FP)}}{\\text{Total Actual Negatives}}$$\n\n* True Positive Rate (TPR) is percentage of positive observations that are classified correctly and we want to maximize it. \n* False Positive Rate (FPR) is percentage of negative observations that are classified incorrectly and we want to minimize it. \n\nNote that True Positive Rate is same as the recall above, but False Positive Rate is entirely different from the precision, as discussed below. \n\nROC curve is calculated by plotting the True Positive Rate vs False Positive Rate for various threshold values for the probability. The area under ROC curve is used as a measure of how well the classifier distinguish between positive and negative classes. The greater the area, the better is the classifier. \n\nThe lesser the overlap, the fewer the false positives and false negatives and higher the AUC ROC score. \n\n![](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*AgDJbm6d8qr8ESHNv6VvKg.png)\n![](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*KNhNw8BsjbIETPF_BH8Qpg.png)\n\nNote: In all the examples illustrated here, the positive and negative classes are more or less balanced.\n\nThe points on the ROC curve represents True Positive Rate and False Positive Rate for different values of the threshold for probability. When we decrease the threshold to correctly classify the positive observations and thereby increase True Positive Rate, we end up increasing False Positive Rate by misclassifying some negative observations as positives. The following gif illustrates how moving the decision thresholds corresponds to the points in the ROC curve. \n\n![](https:\/\/www.spectrumnews.org\/wp-content\/uploads\/2016\/08\/fig-2-1.gif)\n\nThe area under the ROC curve is inversely proportional to the overlap between the probabilities for the two classes as predicted by the classifier. The following gif shows how the curve changes with the overlap in the classes.\n\n![](https:\/\/www.spectrumnews.org\/wp-content\/uploads\/2016\/08\/fig-3-1.gif)","4116d02c":"Try a few values of weights closer to the ones above and see which seems to fit best. ","bf083334":"We define the function to plot the decision boundaries of the classifier:","ffdaef33":"The simple linear regression (linear regression with one variable) is formulated as $ y_{pred} = w * x + b $.\n\nTo find the optimal values for $w$ and $b$, we need to quantify the cost function (also known as the error function or the loss function) that we can minimize. \n\n* How do we formulate it?\n* Should we sum up the errors? If not, why?\n\nThe simple linear regression model uses the mean-squared error (MSE) as the cost function. We square the errors and then take their average.\n\n$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n\nThe [gradient descent algorithm](https:\/\/machinelearningmastery.com\/gradient-descent-for-machine-learning\/) is used to update the weights iteratively in the direction of the steepest descent of the cost function. \n\n$$ w := w - \\alpha \\nabla J $$\n\nwhere $\\nabla J$ is the gradient of the cost function $J$ and $\\alpha$ is the learning rate that determines the size of steps that we take descending on the path of gradient.\n\n<img src=\"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/general_concepts\/gradient-optimization_files\/ball.png\" width=\"350\" height=\"450\" \/>\n<p style=\"text-align: center;\"> Minimizing the cost function using gradient descent <\/p> \n\n\n\nTo summarize, we defined a cost function to quantify the error in predicting outputs and then we update the weights so as to minimize the cost in the fastest way with the help of gradient descent algorithm.\n\nThe same formulation and understanding can be extended to linear regression with more than one variable, say $x_1, x_2, \\dots, x_n$ with the equation \n\n$$ y_{pred} = b + w_1 * x_1 + w_2 * x_2 + \\cdots + w_n * x_n$$ \n\nAnd we estimate the weights $w_1, w_2, \\dots, w_n$ corresponding to each variable as well as the intercept by minimizing the cost function using gradient descent.","67c99257":"The feature BMI does not seem to possess a linear relationship with the disease progression. The coefficient of determination (denoted by $R^2$) is a statistical measure of how close the data are to the fitted regression line and it can be calculated using `score()` method.","b0ec3672":"Three classifiers A, B and C are trained on a given labeled dataset. The accuracy of the trained classifiers in predicting the labels correctly on the same dataset is as follows.\n\n|Models | Accuracy| \n|---|---|\n| Model A | 90%|\n| Model B | 80%|\n| Model C | 70%|\n\nClearly, model A is better at predicting labels for the training data than model B and C. Do you think model A will do a better job in predicting labels for yet unseen data as well?\n\nTo answer this question, let us first briefly review the learning process for logistic classifiers, which is similar in essence to the learning process of a lot of different regression and classification algorithms including neural networks.","c537186b":"For binary classification, we assign the two classes the labels 0 and 1. The class labeled 1 is also called the positive class. The classifier predicts the probability ($p$) that an observation belongs to the positive class. The probability for the class labeled $0$ (or the negative class) would be $1-p$.\n\nTo build a linear classifier, also known by its misnomer logistic regression, is same as finding a function for probability that gives a value close to 1 for points in the upper region (or the points in the positive class) and a value close to 0 for points in the lower region (or the points in the negative class). \n\nIn the context of neural networks, such a function is called an activation function. They are said to be fired or not depending on whether $f \\to 1$ or $f \\to 0$ for the input. The logistic classifiers are one of the simplest cases of the neural networks pared-down to a single layer. And they are called so, because they use one of the most commonly used activation function called logistic (or sigmoid) function. \n\n$$sig(t) = \\frac{1}{1+e^{-t}}$$\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/53\/Sigmoid-function-2.svg\" width=400 \/>\n\nThe S-shaped curve is called sigmoid because of its shape and it was widely used in population growth models in the previous century and hence, the [name logistic](https:\/\/en.wikipedia.org\/wiki\/Logistic_function).\n\nOur main objective for a classification task is to find the optimal decision boundary to separate the classes. For the logistic regression, the boundary is linear. For the case of two features, this linear boundary is simply a line in 2-dimensional plane, whereas for three features, the linear boundary would be a linear plane separating the two classes in 3-dimensional plane and in general, a $n-1$ dimensional linear hyperplane in a $n$-dimensional space.\n\n<img src=\"https:\/\/camo.githubusercontent.com\/f663cd4f29335972950dded4d422c07aeee8af55\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"250\" \/>\n<p style=\"text-align: center;\"> Logistic Regression classifier <\/p>\n\nFinding this linear decision boundary boils down to finding the weights $w_1, w_2, \\dots, w_n$ and the slope $b$. The equation for linear regression seen above was given by the line itself $$ y_{pred} = b + w_1 * x_1 + w_2 * x_2 + \\cdots + w_n * x_n$$\n\nFor the logistic classifier, we will classify the points based on which region they fall into and output the probability that they belong to the positive class. This should be reflected in the equation for the logistic classifier and we use the sigmoid (logistic) function for that purpose. \n\nTo be able to visualize and understand intuitively, we will first crack the formulation of logistic classifier in the case of two features, say $x_1$ and $x_2$, as seen in the figure above.\n\nMath question: We represent the following line using the equation $x_1-x_2-1=0$. How do we mathematically represent the two regions that are separated by this line?\n\n![](https:\/\/github.com\/AashitaK\/ML-Workshops\/blob\/master\/Session%204\/figures\/fig1.png?raw=true)\n\nThe region containing the origin is given by $x_1-x_2-1<0$ whereas the other one by $x_1-x_2-1>0$.\n\n![](https:\/\/github.com\/AashitaK\/ML-Workshops\/blob\/master\/Session%204\/figures\/fig2.png?raw=true)\n\nAs seen above, the points in one region is characterized by $w_1*x_1 + w_2*x_2+b<0$ and the other region by $w_1*x_1 + w_2*x_2+b>0$. We combine this with the logistic (sigmoid) function above to get the equation for logistic regression:\n\n$$Prob(y=1) = sig(w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b) $$ \n\nwhere $sig$ is the sigmoid logistic function defined above. \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/53\/Sigmoid-function-2.svg\" width=400 \/>\n\nObservations:\n* The output of the sigmoid lies between 0 and 1, which corresponds to the probability in our case. \n* The logistic function (and hence the probability) approximates to 1 for the large positive values, whereas it converges to 0 for large negative values. \n* The value for $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b$ is positive for points in the region on one side of the line and negative for the other. The magnitude of the values (positive or negative) is higher for points far away from the line.\n* In view of the above equation for logistic regression and the properties of sigmoid logistic function, the points farther away from the line will be classified with a high probability to one class or the other, whereas the probability will be closer to 0 for points close to the line.\n \nIn general, we set the threshold for probability to be 0.5. This means that whenever $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b \\geq 0$, it is classified to the positive class, whereas whenever $w_1*x_1 + \\cdots + w_n*x_n + b < 0$, it is classified to the negative class. The points for which the value for $w_1*x_1 + \\cdots + w_n*x_n + b$ is not large in magnitude have probabilities that are closer to 0.5. Such points needs to be classified with extra care, as we will see later on in evaluation metrics. \n\nTo learn the weights $w_1, w_2, \\dots, w_n$ and $b$, we quantify the error of misclassification using the cost function called the cross-entropy log loss defined below.\n\nFor points with label $y=1$, the cost is\n\n$$ c(y, p) = - \\log(p) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{ if }\\ \\  y = 1$$\n\nwhereas for points with label $y=0$, the cost is\n\n$$ c(y, p) = - \\log(1-p) \\ \\  \\text{ if }\\ \\  y = 0$$\n\nThe cost function takes the average over the costs for all points. The costs for the two classes $y=0$ and $y=1$ can be summed up in the following formula.\n\n$$ J = \\frac{1}{N} \\sum_{i=1}^N c(y, p) = - \\frac{1}{N} \\sum_{i=1}^N y \\log(p) + (1-y) \\log(1-p)$$\n\nwhere $p=Prob(y=1)$.\n\nThe updates to the weights are made in a similar fashion as seen earlier for linear regression by minimizing the cost function using gradient descent algorithm.","0cca87f8":"## Dimensionality reduction (Optional):\n\nMany a times, our real-world dataset contains too many features. Using all the features can lead to a very complex model that would be prone to overfitting if we do not have the sufficient number of training examples in our dataset. A simple way to address this is to discard features.\n\nQ: How do we know which features to discard? \n\nSometimes, the features that do not seem to have much impact on the target variable individually might prove to be very useful in conjunction with other features. If we do not want to risk losing information by discarding features, we use dimensionality reduction techniques. The most commonly used one is Principal Component Analysis (PCA).\n\n\n### Principal Component Analysis (PCA):\n\nIf we have 100 features, we can think of having 100 dimensional feature space with an axis for each feature. In essence, we transform our 100 old features into 100 new ones, ranked in the order of importance. To accomplish this, we rotate the axis of the features to capture the most variability of data in our first principal axis and then try to capture as much from the remaining variability as possible in the second axis and so on. We ultimately keep only as many features as we want, discarding the least important ones and thus, reducing the dimension.\n\n![](https:\/\/www.researchgate.net\/profile\/Nicholas_Czarnek\/publication\/320410861\/figure\/fig7\/AS:551041819447302@1508390015760\/Example-application-of-principal-component-analysis-to-simple-synthetic-data-The-black.png)\n\nIn the figure, we have transformed the two dimensional data using PCA. If we want to reduce it to one-dimension, we will simply discard the principal component 2, having captured most of the variability in the principal component 1.  The downside of PCA is that the new features are no longer as interpretable, and unlike the old features, we cannot explain what they exactly represent. The upside is that we have captured as much variability as possible in fewer features.\n\nPCA is useful in machine learning for purposes other than dimensionality reduction. The linear models rely on the assumption that the features are independent of each other. In general, features from real-world datasets are not entirely independent, but if we transform them using PCA, the new features become independent of each other. This makes new features much more suited to build linear models such as linear\/ridge\/lasso regression and logistic classifier. Other uses of this technique involves data compression and visualization of data (when the number of features are reduced to 2 or 3).\n\nThe steps in PCA are outlined below. It would be Okay to skip this and revisit later after learning\/revising linear algebra, especially eigen-values and eigen-vectors.\n* First create a matrix for which each row is an observation and each column is a feature (similar to our dataframe, but all columns must be numerical values). \n* Center each column around zero by substracting the respective mean from each column.  \n* Compute the covariance matrix\n* Find the eigen values and the eigen vectors for the covariance matrix. \n* Normalize each of the eigenvector to become unit vector.\n\nWe pick the largest eigen value and the corresponding eigen vector becomes our first principal component (or first new axis). The eigen vector corresponding to the second largest eigen value becomes our second component and so on.\n\nNote: We must always normalize the data before PCA. \n\nThe following is the implementation of PCA using [`sklearn.decomposition.PCA`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)","402e0d35":"#### How to address overfitting?\n- Reduce the number of features \n    - Discard some features\n    - Dimensionality reduction techniques such PCA, LDA, etc.\n- Simplify the model (by tuning hyperparameters)\n- Early termination (reducing the number of iterations for training)\n- Regularization, if applicable\n- Add more training examples, if possible  \n<img src=\"https:\/\/i.stack.imgur.com\/rpqa6.jpg\" width=\"450\" height=\"600\" \/>\n\nIn a nutshell, to reduce overfitting, reduce complexity.\nTo reduce underfitting, enhance complexity.\n\n\nThe following concepts in machine learning - k-fold Cross-validation and dimensionality reduction - are useful for building classification and regression models, no matter the choice of the algorithm. ","93ebe6bf":"How does the values for the slope and intercept compare with the ones you guessed earlier?","d92d0281":"## k-fold Cross-validation\n\n**Cross-validation**: \n\nTo address overfitting and underfitting to the curve, we hold out a validation set to evaluate our model. This valiadation set gives us an estimate for how well our model will generalize to unseen data. We similarly use this validation set to tune the hyperparameters, for example finding the optimal value for the regularization parameter alpha. This cross-validation method has a shortcoming similar to overfitting. The overfitting happens when our model captures noise and patterns present in the training dataset that are not present in the unseen data. It can also happen that our split of the training and validation is not entirely random, and that one of the subset has patterns\/characteristics not present in the entire dataset. More commonly, it happens that when we use validation set to tune our hyperparameters, they often overfit to the validation set. For example, we find an optimal value for the regularization parameter using a validation set, and it turned out to be not-so-optimal for the unseen test data. To solve this problem, it is a common practise to use k-fold cross-validation, especially for tuning hyperparameters.\n\n1. Randomly divide the dataset into k-groups\n2. For each group, we train the algorithm on the remaining k-1 groups and evaluate its performance on this group treating it as the validation set. \n\nThus, we train k different models and have performance scores for each one of them. At the end, we take the median of the scores and also make note of their standard deviation.\n![](https:\/\/i.stack.imgur.com\/nFkpn.png)\n\nScikit-learn has a built-in function [`sklearn.model_selection.cross_val_score`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html) to calculate the cross-validation scores. We can set the number of folds using the parameter `cv` and choose from various scoring methods. We initailize a classifier and use it with the entire dataset.","e1125c9c":"Train the classifier on training set:","602c2529":"To answer the question, let us consider this binary classification problem. \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/19\/Overfitting.svg\" width=\"250\" height=\"250\" \/>\n\n* Which of the two decision boundaries (black or green) will have a lower value for the cost function?\n* Which decision boundary would you prefer for classifying the unseen examples?\n\nSince the cost function is calculated solely based on the training dataset, minimizing it too much might mean that the model do not generalize well to unseen examples. This is called overfitting. ","9416fec9":"### F1 score\nF1-score is used when you want to seek a balance between precision and recall. It is the [Harmonic mean](https:\/\/en.wikipedia.org\/wiki\/Harmonic_mean) between precision and recall. \n\n$$ F1 = 2 * \\frac{1}{\\frac{1}{Precision} + \\frac{1}{Recall}} = 2 * \\frac{Precision * Recall}{Precision + Recall} $$.","d99f9d8b":"The result is the validation score for each fold.","954951c0":"So, the R-sqaured is around 34%, which is somewhat low.\n\n$R^2$ is one of the metric for evaluating regression models. Following are some data points plots for demonstration, the first two have $R^2$ equal to 1 and the last two with $R^2$ equal to 0.\n\n\n<div style=\"display:flex\">\n     <div style=\"flex:1;padding-right:5px;\">\n          <img src=\"http:\/\/www.ken-szulczyk.com\/misc\/statistics\/correlation_01.gif\" width=\"200\" height=\"200\">\n     <\/div>\n     <div style=\"flex:1;padding-left:5px;\">\n          <img src=\"http:\/\/www.ken-szulczyk.com\/misc\/statistics\/correlation_02.gif\" width=\"200\" height=\"200\">\n     <\/div>\n     <div style=\"flex:1;padding-left:5px;\">\n          <img src=\"http:\/\/www.ken-szulczyk.com\/misc\/statistics\/correlation_03.gif\" width=\"400\" height=\"400\">\n     <\/div>\n<\/div>\n","5efa5eac":"It is not always practical or feasible to use confusion matrix to compare model performance while tuning hyperparameters. It is helpful to come up with a single easy-to-compare metric to optimize our classifier. Depending on the task, we pick one of the metrics derived from confusion matrix that are given below.","7dbe125b":"Further material that would be very useful, from the [Machine Learning workshop series at HMC](http:\/\/www.aashitak.com\/ML-Workshops\/).\n* Regression:\n\n    * The [notebook on regression algorithms](https:\/\/www.kaggle.com\/aashita\/regression-algorithms) is meant to be a gentle introduction overviewing the regression algorithms - lasso and ridge - by introducing the concept of regularization. We would use [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview) dataset from a Kaggle competition to learn the algorithms. \n    * In the [exercise notebook](https:\/\/www.kaggle.com\/aashita\/exercise-3), you will tackle the [Bike Sharing Demand](https:\/\/www.kaggle.com\/c\/bike-sharing-demand\/overview) dataset from another Kaggle competition and apply the regression algorithms.\n    \n* Classification:\n\n    * The [notebook on classification algorithms](https:\/\/www.kaggle.com\/aashita\/classification-algorithms) contains a brief overview and implementation of various other classification algorithms such as Decision Trees, k-Nearest Neighbors, Support Vector Machines (SVM), Random Forest as well as the technique of Voting Classifiers. Note that the earlier part of the notebook on Logistic Classifier is already covered here.\n    \n    * The [exercise notebook](https:\/\/www.kaggle.com\/aashita\/exercise-4) for the Titanic dataset has the data preprocessed with the similar steps that we used in the Session 1 of this workshop. You are encouraged to train models using various classification algorithms and find the optimal one. \n\nAll of the above notebooks are also available in the [Session 2 folder of the Github repository](https:\/\/github.com\/CMC-QCL\/A-hands-on-introduction-to-Machine-Learning-using-Python-on-Kaggle) for the workshop.","e067181a":"Let us pick a few points from the dataset and try to fit a line.","31e9571e":"The dataset has many features (or variables), but we will pick only the BMI so as to be able to plot and visualize the relationship between input and target variables.","60bb68e1":"### Decision Threshold\n\nMost classifiers, especially the ones we are interested in, such as logistic classifier and neural networks, predicts the probabilities that an observation will belong to the different classes in a classification task rather than predicting the classes directly.\n\nFor a binary classication problem, if the probability for a class is greater than a threshold value, usually 0.5, then the observation is classified to belong to that class. For a number of reasons, we might want to change this threshold value, also called decision threshold. \n\nThe implementation for classifiers in scikit-learn has built-in function `predict_proba()` that gives us the probability that an observation belongs to class 1.","02251b20":"Next we train the classifier using the training data:","7418a1fe":"For the above classifier, the observation with index 11 that belongs to class 1 is incorrectly classified to class 0. It has a higher probability of approx. 0.3 to belong to class 1 than most other observations. So, if we lower the threshold for probability to 0.3, it will be correctly classified. \n\n![](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*qLjMtrdG3qIcuNBALvsYQA.jpeg)\n\nThe above figure represents the probability distribution for the positive and negative classes given by green and red curves as predicted by a classifier. The black vertical line for the cut-off is the decision threshold for the probability and as we move it left or right, we change the classification prediction of the affected data points. The false positives and the false negatives mostly lies around the cut-off line.  More precisiely, the points on the green curve to the left of the cut-off line are false negatives - they should be classified as positives but due to lower probability predicted by the classifier, they are falsely classified as negatives. Similarly, the points on the red curve to the right are false positives.","2222bdeb":"We will first generate a dataset suitable for demonstration and applying classification algorithms using built-in function [`make_classification`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_classification.html) in scikit-learn.","0cc289a6":"## Evaluation metrics for classification\n\nSuppose we want to use machine learning to detect patients' in risk of developing a rare type of cancer. We are given a dataset consisting of certain features derived from subjects' past medical history and information about whether they develop the cancer later on.\n\nQ: Can this be framed as a regression, classification task or neither?   \nQ: If yes, what are the features?    \nQ: What is the target variable?  \nQ: How do we know how good is our model and how do we compare different models?   \n\n\n### Classification accuracy:\n$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions made}}$$\n\nSuppose, only 1% of the subjects in our dataset are diagonized with the cancer. We trained a classifier that correctly detects almost all the subjects with cancer, but also misclassify certain subjects that do not have it. The total accuracy of our classifier is 96%. \n\nQ: If we are given another classifier that have an accuracy of 99%. Should we prefer it over ours?   \nQ: If not, why? What additional information we would like to have to decide?\n\nTo answer this, suppose I built a dummy classifier that always predict that a subject do not have cancer. It will be right 99% of the time and hence 99% accuracy. This suggests that accuracy is not a good metric when the classes are imbalanced. \n\nQ: Suppose we were able to get additional dataset and now almost half of the subjects in our dataset were diagonized with the cancer. Is accuracy now a good choice to compare two classifiers?\n\nKey takeaway: Accuracy is a good metric for evaluating classifier performance only when the classes are more or less equally distributed and classifying each class carries the same priority.  \n\n### Confusion matrix:\n![](https:\/\/www.dataschool.io\/content\/images\/2015\/01\/confusion_matrix_simple2.png)\n\n1. True Positives (TP): Actual Yes and Predicted Yes\n2. False Positives (FP): Actual No and Predicted Yes\n3. True Negatives (TN): Actual No and Predicted No\n4. False Negatives (FN): Actual Yes and Predicted No\n\nIn the context of hypothesis testing, False Positive is also known as Type I error whereas False Negative is known as Type II error.\n![](https:\/\/i.stack.imgur.com\/W7I4r.png)","3eec01b3":"We guess values for slope and intercept by trial-and-error:","95e1399d":"Scikit-learn has an implementation of the linear regression as demonstrated below. First we import the function [LinearRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) and then initialize the regressor.","8c2d633b":"***Over-fitting and under-fitting to the training set***  \nThe models can over-train on a dataset, that is they learn the dataset so well that they do not generalize well to the examples outside of that dataset. \n\nIf we try to fit too complex of a curve as the decision boundary separating the classes and we don't have enough training examples to estimate the parameters for the curve, then we suffer from over-fitting.\n\nOn the other hand, if we try separating the classes with an over-simplified curve as the decision boundary and we have enough training examples to estimate a curve that would be a better fit, then we suffer from under-fitting. \n\n<img src=\"https:\/\/vitalflux.com\/wp-content\/uploads\/2015\/02\/fittings.jpg\" width=\"600\" height=\"800\" \/>\n\nHow do we know whether our model is overfitting or underfitting to the training set?\n\nAnswer: At the beginning, we save some examples as the validation set and use it to test the performance of the model. \n\n|Models | Accuracy on the training set | Accuracy on the validation set | \n|---|---|---|\n| Model A | 90%| 70% |\n| Model B | 80%| 75% |\n| Model C | 70%| 65% |\n\n* With this additional information, can you guess which model will likely perform better for the unseen data?\n* Which of these three models would you suspect for overfitting to the training data?\n* Which of these three models would you suspect for underfitting to the training data?\n\nThe problem for over-fitting and under-fitting and the underlying reasons of model complexity is the same for regression as well.\n\n<img src=\"https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/images\/mlconcepts_image5.png\" width=\"600\" height=\"800\" \/>\n\n#### Key take-aways so far:\n- Always save some examples from the datasets for testing model performance.\n- Pay attention to the model performance on the validation set rather than solely on the training set.\n- Watch out for both under-fitting and over-fitting.","e09c1887":"### Acknowledgements:\nThe credits for the images used above are as follows.\n- Image 1: https:\/\/commons.wikimedia.org\/wiki\/File:Gaussian_kernel_regression.png\n- Image 2: http:\/\/www.ken-szulczyk.com\/misc\/statistical_lecture_10.php\n- Image 3: https:\/\/rasbt.github.io\/mlxtend\/user_guide\/general_concepts\/gradient-optimization\/\n- Images 4, 5, 6, 7: http:\/\/www.ken-szulczyk.com\/misc\/statistical_lecture_10.php\n- Image 8 and 10: https:\/\/github.com\/trekhleb\/machine-learning-octave\/tree\/master\/logistic-regression\n- Image 9 and 13: https:\/\/towardsdatascience.com\/derivative-of-the-sigmoid-function-536880cf918e\n- Image 14: https:\/\/commons.wikimedia.org\/wiki\/File:Overfitting.svg\n- Image 15: https:\/\/vitalflux.com\/wp-content\/uploads\/2015\/02\/fittings.jpg\n- Image 16: https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/model-fit-underfitting-vs-overfitting.html\n- Image 17: https:\/\/i.stack.imgur.com\/rpqa6.jpg\n- Image 18: https:\/\/tex.stackexchange.com\/questions\/434358\/draw-customized-table-with-tikz\n- Image 19: https:\/\/www.researchgate.net\/figure\/Example-application-of-principal-component-analysis-to-simple-synthetic-data-The-black_fig7_320410861\n- Image 20: https:\/\/sebastianraschka.com\/Articles\/2014_python_lda.html\n\n- Image 21: https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\n- Image 22: https:\/\/i.stack.imgur.com\/W7I4r.png\n- Image 24: https:\/\/www.qualtrics.com\/support\/stats-iq\/analyses\/regression-guides\/confusion-matrix-precision-recall-tradeoff\/\n- Image 25: https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/binary-classification.html\n- Image 23, 26 and 27: https:\/\/medium.com\/greyatom\/lets-learn-about-auc-roc-curve-4a94b4d88152\n- Image 28 and 29: https:\/\/www.spectrumnews.org\/opinion\/viewpoint\/quest-autism-biomarkers-faces-steep-statistical-challenges\/\n- Image 30: https:\/\/en.wikipedia.org\/wiki\/File:Binary_confusion_matrix.png","2fda23f8":"Another dimensionality reduction technique is [Linear Discriminant Analysis (LDA)](https:\/\/sebastianraschka.com\/Articles\/2014_python_lda.html). LDA is similar to PCA except that it tries to maximize the class separation instead of data variance.\n<img src=\"https:\/\/sebastianraschka.com\/images\/blog\/2014\/linear-discriminant-analysis\/lda_1.png\" width=\"500\" height=\"500\"\/>","faf0bf7c":"Then we train the regressor using the `fit()` method on the smaller set of data points.","2d0d675b":"The binary classification dataset containing two features (or variables) is plotted below. ","b4701eba":"We create a logistic classifier using [`sklearn.linear_model.LogisticRegression`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)"}}