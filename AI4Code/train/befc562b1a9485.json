{"cell_type":{"ebe996f0":"code","e973adcd":"code","bcf4e275":"code","1dc772ef":"code","6bc7c935":"code","bc40da6c":"code","808751ae":"code","3789c46d":"code","1f864d4e":"code","c143b0ed":"code","974f1e0c":"code","d4422cde":"code","dd2dd07f":"code","b9e90b67":"code","84dd226a":"code","7b763b82":"code","69e1483e":"code","bf742e8c":"code","a9cd1317":"code","6e73241f":"code","d2abcbd5":"code","b9d6dee3":"code","0e155917":"code","893a1a15":"markdown","09b2b06d":"markdown","a646239c":"markdown","7ab76c1a":"markdown","aaf5e9bd":"markdown","f91d364d":"markdown","4a0b9955":"markdown","6a2c74fa":"markdown","c9225206":"markdown","c5b7dc0e":"markdown","2d6f4213":"markdown"},"source":{"ebe996f0":"# IMPORT THE NECESSARY LIBRARIES\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport torch","e973adcd":"car_prices_array = [3,4,5,6,7,8,9]\ncar_price_np = np.array(car_prices_array, dtype=np.float32)\ncar_price_np = car_price_np.reshape(-1,1)\ncar_price_tensor = Variable(torch.from_numpy(car_price_np))\n\nnumber_of_car_sell_array = [7.5, 7, 6.5, 6.0, 5.5, 5.0, 4.5]\nnumber_of_car_sell_np = np.array(number_of_car_sell_array, dtype=np.float32)\nnumber_of_car_sell_np = number_of_car_sell_np.reshape(-1,1)\nnumber_of_car_sell_tensor = Variable(torch.from_numpy(number_of_car_sell_np))\n\n# visualize\nplt.scatter(car_prices_array, number_of_car_sell_array)\nplt.xlabel(\"Car Price\")\nplt.ylabel(\"Number of Car Sell\")\nplt.title(\"Car Price & Number of Car Sell\")\nplt.show()","bcf4e275":"class LinearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        return self.linear(x)\n    \n\n\n# DEFINE MODEL\ninput_dim = 1\noutput_dim = 1\nmodel = LinearRegression(input_dim, output_dim)\n\n\n# MSE\nmse = nn.MSELoss()\n\n\n# OPTIMIZATION\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n\n\nloss_list = []\niteration_number = 1000\nfor iteration in range(iteration_number):\n    \n    # OPTIMIZATION\n    optimizer.zero_grad()\n    \n    # FORWARD TO GET OUTPUT\n    results = model(car_price_tensor)\n    \n    # CALCULATE LOSS\n    loss = mse(results, number_of_car_sell_tensor)\n    \n    # BACKWARD PROPAGATION\n    loss.backward()\n    \n    # UPDATING PARAMETERS\n    optimizer.step()\n    \n    # STORE LOSS\n    loss_list.append(loss.data)\n    \n    # PRINT LOSS\n    if(iteration % 100 == 0):\n        print('epoch {}, loss {}'.format(iteration, loss.data))\n        \n        \nplt.plot(range(iteration_number),loss_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()","1dc772ef":"predicted = model(car_price_tensor).data.numpy()\nplt.scatter(car_prices_array, number_of_car_sell_array, label=\"original data\", color=\"red\")\nplt.scatter(car_prices_array, predicted, label=\"predicted  data\", color=\"blue\")\n\nplt.legend()\nplt.xlabel(\"Car Price\")\nplt.ylabel(\"Number of Car Sell\")\nplt.title(\"Original vs Predicted values\")\nplt.show()","6bc7c935":"# PREPARE DATASET\ntrain = pd.read_csv(r\"..\/input\/fashionmnist\/fashion-mnist_train.csv\", dtype=np.float32)\n\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 \n\nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\nplt.imshow(features_numpy[20].reshape(28,28),cmap=\"gray\")\nplt.axis(\"off\")\nplt.title(str(targets_numpy[20]))\nplt.savefig('graph.png')\nplt.show()","bc40da6c":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\ninput_dim = 28*28 \noutput_dim = 10 \n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\nerror = nn.CrossEntropyLoss()\n\nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","808751ae":"count = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","3789c46d":"# visualization\nplt.figure(figsize=(25,6))\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","1f864d4e":"# IMPORT LIBRARIES\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable","c143b0ed":"# CREATE ANN MODEL\nclass ANNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        # Linear Function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear Function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear Function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear Function 4 (readout) 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        \n        out = self.fc2(out)\n        out = self.tanh2(out)\n        \n        out = self.fc3(out)\n        out = self.elu3(out)\n        \n        out = self.fc4(out)\n        return out\n    \ninput_dim = 28*28\nhidden_dim = 150\noutput_dim = 10\n\n# Craete ANN Model\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss\nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)","974f1e0c":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    for i,(images, labels) in enumerate(train_loader):\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # clear gradients\n        optimizer.zero_grad()\n        \n        # forward propagation\n        outputs = model(train)\n        \n        # calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # calculating gradients\n        loss.backward()\n        \n        # update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                test = Variable(images.view(-1,28*28))\n                outputs = model(test)\n                predicted = torch.max(outputs.data, 1)[1]\n                total += len(labels)\n                correct += (predicted == labels).sum()\n                \n            accuracy = 100 * correct \/ float(total)\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","d4422cde":"# visualization loss\nplt.figure(figsize=(25,6))\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.figure(figsize=(25,6))\nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","dd2dd07f":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable","b9e90b67":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(64 * 4 * 4, 10) \n        \n    def forward(self, x):\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        out = self.maxpool1(out)\n        out = self.cnn2(out)\n        out = self.relu2(out)\n        out = self.maxpool2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        return out\n    \nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \nmodel = CNNModel()\n\nerror = nn.CrossEntropyLoss()\n\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","84dd226a":"count = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                test = Variable(images.view(100,1,28,28))\n                outputs = model(test)\n                predicted = torch.max(outputs.data, 1)[1]\n                total += len(labels)\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","7b763b82":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN: Accuracy vs Number of iteration\")\nplt.show()","69e1483e":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","bf742e8c":"class RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        \n        # Number of hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # RNN\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        \n        # Initialize hidden state with zeros\n        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n            \n        # One time step\n        out, hn = self.rnn(x, h0)\n        out = self.fc(out[:, -1, :]) \n        return out\n\nbatch_size = 100\nn_iters = 5000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n    \n# Create RNN\ninput_dim = 28    # input dimension\nhidden_dim = 100  # hidden layer dimension\nlayer_dim = 1     # number of hidden layers\noutput_dim = 10   # output dimension\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.05\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","a9cd1317":"seq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train  = Variable(images.view(-1, seq_dim, input_dim))\n        labels = Variable(labels )\n            \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                \n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits\/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct \/ total\n            \n            loss_list.append(loss.data.item())\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(count, loss.data.item(), accuracy))","6e73241f":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"RNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"RNN: Accuracy vs Number of iteration\")\nplt.show()","d2abcbd5":"class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        \n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # LSTM\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) # batch_first=True (batch_dim, seq_dim, feature_dim)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # 28 time steps\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n    \ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 1\noutput_dim = 10\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\nerror = nn.CrossEntropyLoss()\n\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) ","b9d6dee3":"seq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as a torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\/logits\n        # outputs.size 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = error(outputs, labels)\n\n        # Getting gradients\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        count += 1\n\n        if count % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                \n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits\/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct \/ total\n            \n            loss_list.append(loss.data.item())\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(count, loss.data.item(), accuracy))","0e155917":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"LSTM: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"LSTM: Accuracy vs Number of iteration\")\nplt.show()","893a1a15":"# Deep Learning with Pytorch\n- [Linear Regression](#1)\n- [Logistic Regression](#2)\n- [Artificial Neural Networks(ANN)](#3)\n- [Convolutional Neural Networks(CNN)](#4)\n- [Recurrent Neural Networks(RNN)](#5)\n- [Long-Short Term Memory(LSTM)](#6)","09b2b06d":"## Recurrent Neural Network <a id=\"5\"><\/a>\nIn neural networks other than RNN, the inputs are handled independently, and the order in which the inputs arrive in the network does not matter. That is, after the process of the input is completed on the network and the output is produced, the neural network forgets this input. For example, in the image classification problem, an image belonging to the apple is given to the network as a single whole, and the output to which this image belongs is produced as a fruit. There is no problem here, but we need to give a time series problem consisting of successive data or a sentence consisting of successive words to the network at once. In other types of neural networks, we cannot break down such data because each data is evaluated independently.\n\nHowever, in a time series problem where the weather forecast will be made, the previous data is important for predicting the next data. Or while reading a sentence, we read the words tabbed with intermittent eye movements, we keep the words in our memory and we understand the meaning at the end of the sentence. Likewise, when trying to guess the words that may come after a word, knowledge of the previous words is needed. As a solution to such problems, RNNs with memory neural networks have been produced. RNNs proceed while keeping their training on the network, keeping the information of the previous data in their memory.\n\n\n- **Steps of RNN:**\n    - Import Libraries\n    - Prepare Dataset\n    - Create RNN Model\n    - Instantiate Model\n    - Instantiate Loss\n    - Instantiate Optimizer\n    - Traning the Model\n    - Prediction","a646239c":"**Steps of CNN:**\n- Import Libraries\n- Prepare Dataset\n- Convolutional layer\n- Pooling Layer\n- Flattening\n- Fully Connected Layer\n- Instantiate Model Class\n- Instantiate Loss\n- Instantiate Optimizer\n- Traning the Model\n- Prediction\n\n\nAs a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training).\nThanks to convolutional layer, model learnt better and accuracy(almost 98%) is better than accuracy of ANN. Actually while tuning hyperparameters, increase in iteration and expanding convolutional neural network can increase accuracy but it takes too much running time that we do not want at kaggle.","7ab76c1a":"**What is this CNN ?** <br>\nCnn use unique features that make a plane a plane or a snake to distinguish the images given. In fact, this process is also unconscious in our brains.\nFor example, when we look at an airplane picture, we can identify the airplane by separating the features such as two wings, engines, windows. Cnn does the same, but previously they detect low-level properties such as curves and edges and create them up to more abstract concepts.\n\n![](https:\/\/miro.medium.com\/max\/788\/0*96btJ-ORRbeVgZQ-)\n\nCnn works with several layers you can use to get the choices you speak. Let's give an overview of these layers and their objectives:\n- **Convolutional Layer** - Used to detect properties\n- **Non-Linearity Layer** - Recognition of nonlinearity to the system\n- **Pooling (Downsampling) Layer** - Controls predominantly and fit\n- **Flattening Layer** - Prepares the catalog for the Classic Neural Network\n- **Fully-Connected Layer** - Standard Neural Network Used in Classification","aaf5e9bd":"## Artificial Neural Network (ANN) <a id=\"3\"><\/a>\n\n**What is this Neural Network?** <br>\nToday, there are many machine learning models. Neural Network is just one of them. It is a model discovered inspired by the human brain and nervous system. <br>\nNeural Network is a structure established in layers. The first layer input is called the last layer output. The layers in the middle are called 'Hidden Layers'. Each layer contains a certain number of 'Neuron'. These neurons are connected to each other by 'Synaps'. Synapses contain a coefficient. These coefficients say how important the information in the neuron to which they are connected is.\n![](https:\/\/miro.medium.com\/max\/788\/1*96T8vlIQSQuko-Sc-o-fbw.png)\nThe value of a neuron is found by multiplying the inputs to that neurons by multiplying them with coefficients. This found result is put into an activation function. According to the result of the function, it is decided whether or not that neuron will be fired.\n\n- Steps of ANN:\n    - Import Libraries\n    - Prepare Dataset\n    - Create ANN Model\n    - Instantiate Model Class\n    - Instantiate Loss\n    - Training the Model\n    - Prediction\n    \n- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training).\n- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model.","f91d364d":"- Number of iteration is 1000.\n- Loss is almost zero that you can see from plot or loss in epoch number 1000.\n- Now we have a trained model.\n- While usign trained model, lets predict car prices.","4a0b9955":"## Convolutional Neural Network (CNN) <a id=\"4\"><\/a>","6a2c74fa":"# Logistic Regression <a id=\"2\"><\/a>","c9225206":"- Now this plot is our collected data\n- We have a question that is what will be number of car sell if the car price is 100\n- In order to solve this question we need to use linear regression.\n- We need to line fit into this data. Aim is fitting line with minimum error.\n\n\n- **Steps of Linear Regression**\n    1. create LinearRegression class\n    2. define model from this LinearRegression class\n    3. MSE: Mean squared error\n    4. Optimization (SGD:stochastic gradient descent)\n    5. Backpropagation\n    6. Prediction\n\n- Lets implement it with Pytorch","c5b7dc0e":"## Linear Regression <a id=\"1\"><\/a>\n\n- y = Ax + B.\n- A = slope of curve\n- B = bias (point that intersect y-axis)\n- For example, we have car company. If the car price is low, we sell more car. If the car price is high, we sell less car. This is the fact that we know and we have data set about this fact.\n- The question is that what will be number of car sell if the car price is 100.","2d6f4213":"## Long-Short Term Memory(LSTM)\n\n**Steps of LSTM:**\n- Import Libraries\n- Prepare Dataset\n- Create LSTM Model\n- hidden layer dimension is 100\n- number of hidden layer is 1\n- Instantiate Model\n- Instantiate Loss\n- Cross entropy loss\n- It also has softmax(logistic function) in it.\n- Instantiate Optimizer\n- SGD Optimizer\n- Traning the Model\n- Prediction"}}