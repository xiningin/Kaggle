{"cell_type":{"90250439":"code","f8c43a1c":"code","6e8874bf":"code","2519fb66":"code","563f924e":"code","2f5e56bc":"code","6556bec7":"code","5209e2ee":"code","268644b0":"code","01f8dcee":"code","610370fd":"code","487ad95b":"code","e2a32d5d":"code","c7f54d10":"code","83329226":"code","501343d2":"code","fc338474":"code","1edc6d28":"code","dbfcb11a":"code","e26ea8f2":"code","5da48ba9":"code","4c8eef8e":"code","2b28eb56":"code","5f3c0dc9":"code","dc13185d":"code","6108ec24":"code","67a8e57b":"code","4393b40a":"code","bdfb2a28":"code","112666ff":"code","981cf336":"code","1f0493b8":"code","54e02ff5":"code","7ff44ede":"code","a7ee4c18":"code","335f3c42":"code","2d58ea8d":"code","a1763ddc":"code","4fdef66a":"code","d3d25cad":"code","17c9e359":"code","f2240415":"code","30319925":"code","809372b4":"code","01387107":"code","4d2687e5":"code","24f83746":"code","be493f64":"code","8c3e77ab":"code","49382ad8":"code","33b183ab":"code","7d85e81b":"code","51113abb":"code","90e97d85":"code","c3be10cb":"code","bedd42e4":"code","0b6c83db":"markdown","e4b76b44":"markdown","f721adff":"markdown","a9dbf00b":"markdown","296e5f0b":"markdown","59a7f97d":"markdown","c680b563":"markdown","035889e3":"markdown","264eb08d":"markdown","8a944715":"markdown","be1b5366":"markdown","453775ee":"markdown","1836169e":"markdown","888414ef":"markdown","31b419a6":"markdown","7e17d44e":"markdown","11688991":"markdown","4aed404f":"markdown","2513f562":"markdown","5888a873":"markdown","7ebe265f":"markdown","f9684b31":"markdown","58f29379":"markdown","a0824e36":"markdown","7565f661":"markdown","dbdaeed3":"markdown","68572351":"markdown","7351c716":"markdown","0ae0228c":"markdown","7476fca3":"markdown","fe7a8e55":"markdown","3dcc026e":"markdown","0eb466ca":"markdown","6fe14360":"markdown","a46a0153":"markdown","55b85c7f":"markdown","995117df":"markdown","6ae8e998":"markdown","e4aca4f9":"markdown","927534c7":"markdown","d56686b2":"markdown","d0d316d2":"markdown","a8cf6c9c":"markdown","e9bad7c4":"markdown","d99b7810":"markdown","156b2d48":"markdown"},"source":{"90250439":"import pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nprint('Priyatama is ready!')","f8c43a1c":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\r\ndf.head()","6e8874bf":"sns.heatmap(df.isnull())","2519fb66":"a = (df.isnull().sum())\r\nprint(a[a>0])\r\ndf.shape","563f924e":"100*201\/5110","2f5e56bc":"df.dropna(inplace=True)","6556bec7":"df.shape","5209e2ee":"df.describe()","268644b0":"df.drop('id', axis=1,inplace=True)","01f8dcee":"df.dtypes","610370fd":"df.gender.value_counts()","487ad95b":"df2 = df[df.gender != 'Other']\r\ndf2.gender.value_counts().plot(kind='pie',autopct='%1.1f%%')","e2a32d5d":"fig, ax=plt.subplots(figsize=(20,5))\r\nsns.countplot(x=df2.age, palette='viridis')\r\nplt.xticks(rotation=90)\r\nplt.xlabel('\\n Age', fontsize=10, fontweight='bold')\r\nplt.ylabel('Count of Patients', fontsize=10, fontweight='bold')\r\nplt.title('Age of Different Patients', fontweight = 'bold', fontsize='15')\r\nplt.show()","c7f54d10":"df3 = df2[~(df2['age'] <= 2)]","83329226":"fig2, ax=plt.subplots(figsize=(20,5))\r\nsns.countplot(x=df3.age, palette='viridis')\r\nplt.xticks(rotation=90)\r\nplt.xlabel('\\n Age', fontsize=10, fontweight='bold')\r\nplt.ylabel('Count of Patients', fontsize=10, fontweight='bold')\r\nplt.title('Age of Different Patients', fontweight = 'bold', fontsize='15')\r\nplt.show()  ","501343d2":"stroke_0 = df3[~(df3['stroke'] == 1)]\r\nstroke_1 = df3[~(df3['stroke'] == 0)]\r\nsns.set(style=\"darkgrid\")\r\nfig3, (ax1, ax2) = plt.subplots(2,1, figsize=(15, 9))\r\n\r\nsns.histplot(x=stroke_0['age'], kde=True, color=\"skyblue\", ax=ax1)\r\nsns.histplot(x=stroke_1['age'], kde=True, color=\"olive\", ax=ax2)","fc338474":"fig4, ax=plt.subplots(figsize=(20,5))\r\nlegend = ['No Stroke', 'Stroke']\r\nsns.set(style=\"darkgrid\")\r\nsns.histplot(x=df3['age'], hue=df3.stroke, palette='rocket')\r\nplt.xlabel('\\n Age', fontsize=10, fontweight='bold')\r\nplt.ylabel('Count of Patients', fontsize=10, fontweight='bold')\r\nplt.title('Age of Different Patients', fontweight = 'bold', fontsize='15')\r\nplt.show()  \r\n","1edc6d28":"df3.hypertension.value_counts().plot(kind='barh')","dbfcb11a":"fig5, ax=plt.subplots(figsize=(5,5))\r\nlegend1 = ['No Hypertension', 'Hypertension']\r\nsns.countplot(x=df3.gender,hue=df3.hypertension, palette='rocket')\r\nfor p in ax.patches:\r\n    ax.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nplt.xlabel('\\n Gender & Hypertension', fontsize=10, fontweight='bold')\r\nplt.ylabel('Count of Patients', fontsize=10, fontweight='bold')\r\nplt.title('Hypertension across Gender', fontweight = 'bold', fontsize='15')\r\nplt.show()  ","e26ea8f2":"fig6, ax=plt.subplots(figsize=(20,5))\r\nax.set(facecolor='Grey')\r\nsns.set(style=\"whitegrid\")\r\nsns.histplot(x=df3['age'], hue=df3.hypertension, palette='rocket')\r\nplt.xlabel('\\n Age', fontsize=10, fontweight='bold')\r\nplt.ylabel('Count of Patients', fontsize=10, fontweight='bold')\r\nplt.title('Age of Different Patients v\/s Hypertension', fontweight = 'bold', fontsize='15')\r\nplt.show()  \r\n","5da48ba9":"sns.set(style=\"darkgrid\")\r\nfig7, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(15, 7))\r\n\r\nsns.histplot(x=df3['age'], hue=df3.hypertension,kde=True, color=\"skyblue\", ax=ax1)\r\nax1.set_xticks([])\r\nax1.set_xlabel('Age')\r\nsns.histplot(x=df3['age'], hue=df3.stroke,kde=True, color=\"olive\", ax=ax2)\r\nax2.set_xticks([])\r\nax2.set_xlabel(' ')\r\nsns.histplot(x=df3['age'], hue=df3.heart_disease,kde=True, color=\"gold\", ax=ax3)\r\nax3.set_xlabel('Age')\r\nplt.title(\"Hypertension - Stroke - Heart Disease v\/s Age\", fontsize=15, fontweight='bold')","4c8eef8e":"df3.ever_married.value_counts().plot(kind='bar')","2b28eb56":"fig8, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\r\n\r\nsns.set(style=\"darkgrid\")\r\n\r\nsns.countplot(x=stroke_0.ever_married, hue=stroke_0.gender, palette='viridis', ax=ax1)\r\nax1.set_xlabel('Marriage Status-No Stroke')\r\nfor p in ax1.patches:\r\n    ax1.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax1.set_ylabel('Count of patient')\r\n\r\nsns.countplot(x=stroke_1.ever_married, hue=stroke_1.gender, palette='rocket', ax=ax2)\r\nax2.set_xlabel('Marriage Status-Stroke')\r\nax2.set_ylabel('')\r\nfor p in ax2.patches:\r\n    ax2.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax2.set_yticks([])\r\nplt.show()  \r\n\r\n","5f3c0dc9":"fig9, ax = plt.subplots(figsize=(5, 3))\r\n\r\nsns.set(style=\"darkgrid\")\r\n\r\nsns.countplot(x=df3['Residence_type'], hue=df3.stroke, palette='viridis')\r\nplt.title('Residence Type v\/s Stroke', fontsize=15, fontweight='bold')\r\nplt.xlabel('Residence Type',fontsize=10, fontweight='bold')\r\nplt.ylabel('Patients',fontsize=10, fontweight='bold')\r\nfor p in ax.patches:\r\n    ax.annotate(f'{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nplt.show()  ","dc13185d":"fig10, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\r\n\r\nsns.set(style=\"darkgrid\")\r\n\r\nsns.countplot(x=stroke_0.gender, hue=stroke_0.Residence_type, palette='viridis',ax=ax1)\r\nax1.set_xlabel('Residence Type-No Stroke',fontsize=10, fontweight='bold')\r\nax2.set_yticks([])\r\nax1.set_ylabel('Patients',fontsize=10, fontweight='bold')\r\nfor p in ax1.patches:\r\n    ax1.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax1.set_ylabel('Count of patient')\r\n\r\nsns.countplot(x=stroke_1.gender, hue=stroke_1.Residence_type, palette='rocket', ax=ax2)\r\nax2.set_xlabel('Residence Type-Stroke',fontsize=10, fontweight='bold')\r\nax2.set_ylabel('')\r\nfor p in ax2.patches:\r\n    ax2.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax2.set_yticks([])\r\nplt.show()  \r\n\r\n","6108ec24":"fig11, ax = plt.subplots(figsize=(5, 5))\r\nsns.set(style=\"darkgrid\")\r\n\r\nsns.barplot(x=df3.gender,y=df3.avg_glucose_level,hue=df3.stroke,estimator=np.average ,ci=None,palette='icefire')\r\nplt.xlabel('Gender & Stroke',fontsize=10, fontweight='bold')\r\nplt.ylabel('Average Glusoce Level',fontsize=10, fontweight='bold')\r\nplt.title('Average Glusoce Level v\/s Gender',fontsize=15, fontweight='bold')\r\nfor p in ax.patches:\r\n    ax.annotate(f'\\n{round(p.get_height())}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\n\r\nplt.show()  \r\n","67a8e57b":"fig12, ax = plt.subplots(figsize=(15, 5))\r\nsns.set(style=\"darkgrid\")\r\n\r\nsns.barplot(y=df3.smoking_status,x=df3.bmi,hue=df3.stroke,estimator=np.average ,ci=None,palette='icefire')\r\nplt.ylabel('Smoking Habit & Stroke',fontsize=10, fontweight='bold')\r\nplt.xlabel('Averge BMI',fontsize=10, fontweight='bold')\r\nplt.title('Smoking Habits v\/s BMI',fontsize=15, fontweight='bold')\r\n\r\nplt.show()  \r\n","4393b40a":"fig13, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\r\n\r\nsns.countplot(x=stroke_0.smoking_status,hue=stroke_0.gender, palette='viridis',ax=ax1)\r\nax1.set_xlabel('Smoking Habits-No Stroke',fontsize=10, fontweight='bold')\r\nax2.set_yticks([])\r\nax1.set_ylabel('Patients',fontsize=10, fontweight='bold')\r\nfor p in ax1.patches:\r\n    ax1.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax1.set_ylabel('Count of patient')\r\n\r\nsns.countplot(x=stroke_1.smoking_status,hue=stroke_1.gender, palette='rocket',ax=ax2)\r\nax2.set_xlabel('Smoking Habits-Stroke',fontsize=10, fontweight='bold')\r\nax2.set_ylabel('')\r\nfor p in ax2.patches:\r\n    ax2.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)\r\nax2.set_yticks([])\r\nplt.show()  \r\n\r\n","bdfb2a28":"fig, plt.subplots(figsize=(20, 5))\r\n\r\nsns.stripplot(y='smoking_status', x='bmi', data = df3,  hue='stroke')\r\nplt.title('BMI V\/s Smoking Status', fontsize=15, fontweight='bold')\r\nplt.xlabel('BMI', fontsize=10, fontweight='bold')\r\nplt.ylabel('Smoking Status', fontsize=10, fontweight='bold')\r\nplt.show()  ","112666ff":"df3.stroke.value_counts().plot(kind='pie',autopct='%1.1f%%',fontsize=17)","981cf336":"data = df3.copy()","1f0493b8":"object_cols = [col for col in data.columns if data[col].dtype == \"object\"]\r\nprint(*object_cols, sep=',')","54e02ff5":"object_nunique = list(map(lambda col: data[col].nunique(), object_cols))\r\nd = dict(zip(object_cols, object_nunique))\r\n\r\n# Print number of unique entries by column, in ascending order\r\nsorted(d.items(), key=lambda x: x[1])","7ff44ede":"from sklearn.preprocessing import OrdinalEncoder\r\n\r\nordinal_encoder = OrdinalEncoder()\r\ndata[object_cols] =ordinal_encoder.fit_transform(data[object_cols])","a7ee4c18":"data.head(4)","335f3c42":"fig15,ax = plt.subplots(figsize=(7, 5))\r\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","2d58ea8d":"X = data.drop('stroke',axis = 1 )\r\ny=data.stroke","a1763ddc":"from sklearn.model_selection import train_test_split\r\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.35, random_state=21)","4fdef66a":"from sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import f1_score,confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n#random forest classifier with n_estimators=10 (default)\r\nclf_rf = RandomForestClassifier(random_state=43)      \r\nclr_rf = clf_rf.fit(X_train,y_train)\r\n\r\nac = accuracy_score(y_valid,clf_rf.predict(X_valid))\r\nprint('Accuracy is: ',round(ac*100), ' %')\r\ncm = confusion_matrix(y_valid,clf_rf.predict(X_valid))\r\nsns.heatmap(cm,annot=True,fmt=\"d\")","d3d25cad":"from sklearn.feature_selection import SelectKBest\r\nfrom sklearn.feature_selection import chi2\r\n# find best scored 5 features\r\nselect_feature = SelectKBest(chi2, k=5).fit(X_train, y_train)\r\nprint('Score list:', select_feature.scores_)\r\nprint('Feature list:', X_train.columns)","17c9e359":"X_train_2 = select_feature.transform(X_train)\r\nX_valid_2 = select_feature.transform(X_valid)\r\n#random forest classifier with n_estimators=10 (default)\r\nclf_rf_2 = RandomForestClassifier()      \r\nclr_rf_2 = clf_rf_2.fit(X_train_2,y_train)\r\nac_2 = accuracy_score(y_valid,clf_rf_2.predict(X_valid_2))\r\nprint('Accuracy is: ',round(ac_2*100), ' %')\r\ncm_2 = confusion_matrix(y_valid,clf_rf_2.predict(X_valid_2))\r\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","f2240415":"from sklearn.feature_selection import RFE\r\n# Create the RFE object and rank each pixel\r\nclf_rf_3 = RandomForestClassifier()      \r\nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\r\nrfe = rfe.fit(X_train, y_train)\r\nprint('Chosen best 5 feature by rfe:',X_train.columns[rfe.support_])","30319925":"rfe_col=X_train.columns[rfe.support_]\r\nrfe_col.values.tolist()","809372b4":"X_train_3 = X_train[rfe_col]\r\nX_valid_3 = X_valid[rfe_col]\r\n#random forest classifier with n_estimators=10 (default)\r\nclf_rf_3 = RandomForestClassifier()      \r\nclr_rf_3 = clf_rf_3.fit(X_train_3,y_train)\r\nac_3 = accuracy_score(y_valid,clf_rf_3.predict(X_valid_3))\r\nprint('Accuracy is: ',round(ac_3*100), ' %')\r\ncm_3 = confusion_matrix(y_valid,clf_rf_3.predict(X_valid_3))\r\nsns.heatmap(cm_3,annot=True,fmt=\"d\")","01387107":"from sklearn.feature_selection import RFECV\r\n\r\n# The \"accuracy\" scoring is proportional to the number of correct classifications\r\nclf_rf_4 = RandomForestClassifier() \r\nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=7,scoring='accuracy')\r\nrfecv = rfecv.fit(X_train, y_train)\r\n\r\nprint('Optimal number of features :', rfecv.n_features_)\r\nprint('Best features :', X.columns[rfecv.support_])","4d2687e5":"rfecv_col=X_train.columns[rfecv.support_]\r\nrfecv_col.values.tolist()\r\n\r\nX_train_4 = X_train[rfecv_col]\r\nX_valid_4 = X_valid[rfecv_col]\r\n\r\nrfecv_1 = rfecv.fit(X_train_4, y_train)","24f83746":"ac_4 = accuracy_score(y_valid,rfecv_1.predict(X_valid_4))\r\nprint('Accuracy is: ',round(ac_4*100), ' %')\r\ncm_4 = confusion_matrix(y_valid,rfecv_1.predict(X_valid_4))\r\nsns.heatmap(cm_4,annot=True,fmt=\"d\")","be493f64":"# Plot number of features VS. cross-validation scores\r\nplt.figure()\r\nplt.xlabel(\"Number of features selected\")\r\nplt.ylabel(\"Cross validation score of number of selected features\")\r\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\r\nplt.show()","8c3e77ab":"clf_rf_5 = RandomForestClassifier()      \r\nclr_rf_5 = clf_rf_5.fit(X,y)\r\nimportances = clr_rf_5.feature_importances_\r\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\r\n             axis=0)\r\nindices = np.argsort(importances)[::-1]\r\n\r\n# Print the feature ranking\r\nprint(\"Feature ranking:\")\r\n\r\nfor f in range(X.shape[1]):\r\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\r\n\r\n# Plot the feature importances of the forest\r\n\r\nplt.figure(1, figsize=(14, 5))\r\nplt.title(\"Feature importances\")\r\nplt.bar(range(X.shape[1]), importances[indices],\r\n       color=\"g\", yerr=std[indices], align=\"center\")\r\nplt.xticks(range(X.shape[1]), X.columns[indices],rotation=90)\r\nplt.xlim([-1, X.shape[1]])\r\nplt.show()","49382ad8":"from sklearn import svm\r\nsvm = svm.SVC(gamma='auto',C=10,kernel='linear')\r\nsvm = svm.fit(X_train,y_train)\r\nac_svm = accuracy_score(y_valid,svm.predict(X_valid))\r\nprint('Accuracy is: ',round(ac_svm*100), ' %')\r\ncm_svm = confusion_matrix(y_valid,svm.predict(X_valid))\r\nsns.heatmap(cm_svm,annot=True,fmt=\"d\")","33b183ab":"from sklearn.linear_model import LogisticRegression\r\nlr=LogisticRegression(solver='liblinear',class_weight='balanced',multi_class='auto', C=80)\r\nlr = lr.fit(X_train,y_train)\r\nac_lr = accuracy_score(y_valid,lr.predict(X_valid))\r\nprint('Accuracy is: ',round(ac_lr*100), ' %')\r\ncm_lr = confusion_matrix(y_valid,lr.predict(X_valid))\r\nsns.heatmap(cm_lr,annot=True,fmt=\"d\")","7d85e81b":"from sklearn.naive_bayes import MultinomialNB\r\nmnb=MultinomialNB()\r\nmnb = mnb.fit(X_train,y_train)\r\nac_mnb = accuracy_score(y_valid,mnb.predict(X_valid))\r\nprint('Accuracy is: ',round(ac_mnb*100), ' %')\r\ncm_mnb = confusion_matrix(y_valid,mnb.predict(X_valid))\r\nsns.heatmap(cm_mnb,annot=True,fmt=\"d\")","51113abb":"from sklearn.naive_bayes import GaussianNB\r\ngnb=GaussianNB()\r\ngnb = gnb.fit(X_train,y_train)\r\nac_gnb = accuracy_score(y_valid,gnb.predict(X_valid))\r\nprint('Accuracy is: ',round(ac_gnb*100), ' %')\r\ncm_gnb = confusion_matrix(y_valid,gnb.predict(X_valid))\r\nsns.heatmap(cm_gnb,annot=True,fmt=\"d\")","90e97d85":"# As we already have found top 5 features, we will use them with Gaussian Naive Byes.\r\ngnb2 = GaussianNB()       \r\ngnb2 = gnb2.fit(X_train_2,y_train)\r\nac_gnb2 = accuracy_score(y_valid,gnb2.predict(X_valid_2))\r\nprint('Accuracy is: ',round(ac_gnb2*100), ' %')\r\ncm_gng2 = confusion_matrix(y_valid,gnb2.predict(X_valid_2))\r\nsns.heatmap(cm_gng2,annot=True,fmt=\"d\")","c3be10cb":"from sklearn.feature_selection import RFE\r\n# Create the RFE object and rank each pixel\r\ngnb3 = GaussianNB()      \r\nrfe_2 = RFE(estimator=gnb3, n_features_to_select=5, step=1)\r\nrfe = rfe.fit(X_train, y_train)\r\nprint('Chosen best 5 feature by rfe:',X_train.columns[rfe.support_])","bedd42e4":"gnb4 = GaussianNB()      \r\ngnb4 = gnb4.fit(X_train_3,y_train)\r\nac_gnb4 = accuracy_score(y_valid,gnb4.predict(X_valid_3))\r\nprint('Accuracy is: ',round(ac_gnb4*100), ' %')\r\ncm_gnb4 = confusion_matrix(y_valid,gnb4.predict(X_valid_3))\r\nsns.heatmap(cm_gnb4,annot=True,fmt=\"d\")","0b6c83db":"# 4.3. Model Selection.","e4b76b44":"## 4.2.2. Univariate feature selection and random forest classification.","f721adff":"### After ageof 40, the chances of Stroke increases significantly.","a9dbf00b":"### Smoking habits & Stroke seem to not be corelated with BMI.","296e5f0b":"## 4.2.4. Recursive feature elimination with cross validation and random forest classification","59a7f97d":"### Smoking habits seem to not be corelated with BMI.","c680b563":"## 4.3.4. Gaussian Naive Bayes.","035889e3":"## 4.4.1 Univariate feature selection and Gaussian Naive Bayes.","264eb08d":"### The dataset has only 4% of the datapoints leading to 'Stroke' as outcome.","8a944715":"### This model still gave an accuracy of 96%, and performed slightly better to detect stroke value \"1\".","be1b5366":"### All health issues (Heart Disease, Stroke and Hypertension) increases after around age of 40.","453775ee":"## Datapoints for patients younger than 2 years seem very few, we can drop it.","1836169e":"## 3.2 Check and deal with unnecessary datapoints in all columns.","888414ef":"## Since, all models are predicting the True Negatives close to zero, we will have to try other algorithms.","31b419a6":"# 5. Conclusion.","7e17d44e":"## 4.2.5. Tree based feature selection and random forest classification","11688991":"### Accuracy is still 86%, but predictions of True Negatives have improved.","4aed404f":"## Only 1 datapoint has'gender' marked as other, we can drop it.","2513f562":"### Possibility of stroke is same beetween differentresidence types.","5888a873":"### Female have a higher chance of having stroke be it married or not.","7ebe265f":"## 4.2.1 Feature selection with correlation and random forest classification","f9684b31":"### Accuracy is reduced to 94% and as it can be seen in confusion matrix, we make few wrong prediction.Although we use 5 features in selectkBest method accuracies degraded. Now lets see other feature selection methods to find better results.","58f29379":"### The features suggested for GuassianNB are same as that for Random Forrest Classifier. ","a0824e36":"## 4.1. Encoding the categorical values.","7565f661":"### Gaussian Naive Bayes returned the most correct predictions for True Negatives.\r\n### The accuracy is 86%, which can be improved with feature selection.","dbdaeed3":"# 4. Feature Engineering.","68572351":"### Women that never smoke are more likely to have stroke than those that do. This could be due to passive smoking or other reasons that are not present in the dataset here.","7351c716":"### Best 5 feature to classify are hypertension, heart disease, married status, bmi and smoking status. So lets see what happens if we use only these best scored 5 feature.","0ae0228c":"### Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","7476fca3":"## 4.3.3. Multinomial Naive Bayes.","fe7a8e55":"### Around 4% of data points for BMI are blank, we can drop it.","3dcc026e":"## 4.4.2.. Recursive feature elimination (RFE) with GaussianNB.","0eb466ca":"### The maximum categorical values in a column are 5, so we can use ordinal encoding.","6fe14360":"# 2. Reading the dataset.","a46a0153":"## 4.3.1. Support Vector Machine.","55b85c7f":"## 4.2.3. Recursive feature elimination (RFE) with random forest","995117df":"# 1. Setting up the environment.","6ae8e998":"### Average glucose level around 145 & 125 for Male and Female respectively increase chances of stroke. ","e4aca4f9":"### Range of age of data sample is vast.","927534c7":"# 4.4. Feture Selction with Gaussian Naive Bayes.","d56686b2":"## 4.3.2. Logistic Regression.","d0d316d2":"### The accuracy increased to 91%, but correct predictions of True Negatives reduced.","a8cf6c9c":"# 3. Exploratory data analysis.","e9bad7c4":"## Though Random forrest classifier gave an accuracy of more than 90%, Gaussian Naive Bayes predict most correct True Negatives, which is the main result required from the model.","d99b7810":"## 3.1 Check for and deal with NA values.","156b2d48":"### After age of 35, chances of Hypertension increases."}}