{"cell_type":{"d559ce69":"code","66f683f6":"code","08cf70ae":"code","60a61be2":"code","d6ea4bd4":"code","5ce8170f":"code","f3f87045":"code","764583b7":"code","f2cefb65":"code","eac59921":"code","ecb6a16e":"code","ba296cc3":"code","e3210fa7":"code","ed37cfbf":"code","2957829b":"code","a8a7c22b":"code","e5d62ea6":"code","5973dd99":"code","06d9bca3":"code","2a441abe":"code","75ab8e05":"markdown","5ca5d206":"markdown","4bf351de":"markdown","0f01fe23":"markdown","68c20d00":"markdown","8dcb17be":"markdown","dbf90aad":"markdown","3a0337ee":"markdown","db842689":"markdown","6edbae12":"markdown"},"source":{"d559ce69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","66f683f6":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","08cf70ae":"print(\"Train set shape = \" +str(train.shape))\nprint(\"Test set shape = \" +str(test.shape))","60a61be2":"X=train.iloc[:,1:].values \nY=train.iloc[:,0].values ","d6ea4bd4":"X = X.reshape(X.shape[0], 28, 28,1) \nprint(X.shape)\nY = keras.utils.to_categorical(Y, 10) \nprint(Y.shape)","5ce8170f":"x_test=test.iloc[:,:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","f3f87045":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.15, random_state=42) ","764583b7":"train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 10,\n                                   width_shift_range = 0.15,\n                                   height_shift_range = 0.15,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = False)","f2cefb65":"valid_datagen = ImageDataGenerator(rescale=1.\/255) ","eac59921":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Conv2D(64, (3,3), padding='same'),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),    \n    \n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])","ecb6a16e":"model.summary()","ba296cc3":"initial_learningrate=1e-3 \nbatch_size = 128\nepochs = 40\ninput_shape = (28, 28, 1)","e3210fa7":"def lr_decay(epoch):#lrv\n    return initial_learningrate * 0.9 ** epoch","ed37cfbf":"model.compile(loss=\"categorical_crossentropy\",\n              optimizer=RMSprop(lr=initial_learningrate),\n              metrics=['accuracy'])","2957829b":"history = model.fit_generator(\n      train_datagen.flow(X_train,Y_train, batch_size=batch_size),\n      steps_per_epoch=100,\n      epochs=epochs,\n      callbacks=[LearningRateScheduler(lr_decay) \n               ],\n      validation_data=valid_datagen.flow(X_valid,Y_valid),\n      validation_steps=50,  \n      verbose=2)","a8a7c22b":"accuracy = history.history['acc']\nval_accuracy = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n","e5d62ea6":"plt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","5973dd99":"predictions = model.predict_classes(x_test\/255.)","06d9bca3":"final=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})","2a441abe":"final.to_csv(\"cnn_submission.csv\",index=False)\n","75ab8e05":"# Training","5ca5d206":"We reshape the images from the training set, and convert the labels to categorical.","4bf351de":"# Preprocessing data","0f01fe23":"# Model","68c20d00":"We define a function that reduces the learning rate with the epochs.","8dcb17be":"Now we reshape the test set.","dbf90aad":"# Plot","3a0337ee":"Now we define the hyperparameters. For this notebook I set smaller batch size and epochs, and larger initial learning rate. The best results are achieved with different parameters but it takes a lot of time and resources.","db842689":"Now we split the training set into train and validation.","6edbae12":"We use Kares data augmentation to have a larger training set. We also normalize the trainig a test sets."}}