{"cell_type":{"19244b87":"code","f4a1749b":"code","2a4bc78b":"code","fd48c001":"code","8da08d03":"code","fb71e451":"code","127aa7ca":"code","6b5a068c":"code","95129f26":"code","58b99323":"code","62367b75":"code","77973ea5":"code","d8751aa1":"code","d8f9b137":"code","e68d46b8":"code","318e4aca":"code","1ddf74a2":"code","52fd146c":"code","b934c7ee":"code","a8147757":"code","502148d5":"markdown","50ff5510":"markdown","cad1992b":"markdown","f7ede6cf":"markdown","472a6b63":"markdown","f40edead":"markdown","29443ca2":"markdown","ded8ef17":"markdown","ce7ad146":"markdown"},"source":{"19244b87":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import VarianceThreshold","f4a1749b":"data = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\ndata.shape","2a4bc78b":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),  # drop the target\n    data['TARGET'],  # just the target\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","fd48c001":"sel = VarianceThreshold(threshold=0)\n\nsel.fit(X_train)  # fit finds the features with zero variance","8da08d03":"# get_support is a boolean vector that indicates which features are retained\n# if we sum over get_support, we get the number of features that are not constant\n\n# (go ahead and print the result of sel.get_support() to understand its output)\n\nsum(sel.get_support())","fb71e451":"# now let's print the number of constant feautures\n# (see how we use ~ to exclude non-constant features)\n\nconstant = X_train.columns[~sel.get_support()]\n\nlen(constant)","127aa7ca":"# let's print the constant variable names\n\nconstant","6b5a068c":"# let's visualise the values of one of the constant variables\n# as an example\n\ndata['ind_var27'].unique()","95129f26":"# we can do the same for every feature:\n\nfor col in constant:\n    print(col, data[col].unique())","58b99323":"# capture non-constant feature names\n\nfeat_names = X_train.columns[sel.get_support()]","62367b75":"\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)\n\nX_train.shape, X_test.shape","77973ea5":"\n# X_ train is a NumPy array\nX_train","d8751aa1":"# reconstitute de dataframe\nX_train = pd.DataFrame(X_train, columns=feat_names)\nX_train.head()","d8f9b137":"# separate train and test (again, as we transformed the previous ones)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","e68d46b8":"# short and easy: find constant features\n\n# in this dataset, all features are numeric,\n# so this bit of code will suffice:\n\nconstant_features = [\n    feat for feat in X_train.columns if X_train[feat].std() == 0\n]\n\nlen(constant_features)","318e4aca":"#drop these columns from the train and test sets:\n\nX_train.drop(labels=constant_features, axis=1, inplace=True)\nX_test.drop(labels=constant_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","1ddf74a2":"# separate train and test (again, as we transformed the previous ones)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","52fd146c":"# I will cast all the numeric features as object,\n# to simulate that they are categorical\n\nX_train = X_train.astype('O')\nX_train.dtypes","b934c7ee":"# to find variables that contain only 1 label\/value\n# we use the nunique() method from pandas, which returns the number\n# of different values in a variable.\n\nconstant_features = [\n    feat for feat in X_train.columns if X_train[feat].nunique() == 1\n]\n\nlen(constant_features)","a8147757":"\nX_train.drop(labels=constant_features, axis=1, inplace=True)\nX_test.drop(labels=constant_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","502148d5":"### I hope you enjoyed it and see you in the next one! Please Upvote and Share","50ff5510":"We can see that 38 columns \/ variables are constant. This means that 34 variables show the same value, just one value, for all the observations of the training set.","cad1992b":"\nWe then use the transform() method of the VarianceThreshold to reduce the training and testing sets to its non-constant features.\n\nNote that VarianceThreshold returns a NumPy array without feature names, so we need to capture the names first, and reconstitute the dataframe in a later step.","f7ede6cf":"#### Note: \nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.","472a6b63":"Same as before, we observe 34 variables that show only 1 value in all the observations of the dataset. Like this,m we can appreciate the usefulness of looking out for constant variables at the beginning of any modeling exercise.","f40edead":"### Manual code 1: only works with numerical\nIn the following cells, I will show an alternative to the VarianceThreshold transformer of sklearn, were we write the code to find out constant variables, using the standard deviation from pandas.","29443ca2":"We see how by removing constant features, we managed to reduced the feature space quite a bit.\n\nBoth the VarianceThreshold and the snippet of code I provided work with numerical variables. What can we do to find constant categorical variables?\n\nOne alternative is to encode the categories as numbers and then use the code above. But then you will put effort in pre-processing variables that are not informative.\n\nThe code below offers a better solution:\n\n### Manual Code 2 - works also with categorical variables","ded8ef17":"### Using VarianceThreshold from Scikit-learn\nThe VarianceThreshold from sklearn provides a simple baseline approach to feature selection. It removes all features which variance doesn\u2019t meet a certain threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.","ce7ad146":"# Constant features\nConstant features are those that show the same value, just one value, for all the observations of the dataset. In other words, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n\nIdentifying and removing constant features is an easy first step towards feature selection and more easily interpretable machine learning models.\n\nHere, I will demonstrate how to identify constant features using a dataset.\n\nTo identify constant features, we can use the VarianceThreshold from Scikit-learn, or we can code it ourselves. If using the VarianceThreshold, all our variables need to be numerical. If we do it manually however, we can apply the code to both numerical and categorical variables.\n\n"}}