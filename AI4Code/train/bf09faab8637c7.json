{"cell_type":{"7c9eec3b":"code","f266b660":"code","c6c49ea2":"code","9a8ed018":"code","f5ac9603":"code","4748e1c5":"code","7b82ea0d":"code","6edf1178":"code","78ea9536":"code","af28acf4":"code","c331e7b6":"code","3df50ad8":"code","1215c6a9":"code","5814cc71":"code","493433b4":"code","5b1a5d8e":"code","1a7238de":"code","726064af":"code","a4fdc918":"code","50108c6b":"code","b5e52304":"code","61059c97":"code","62031aae":"code","0c97f498":"code","0f7052c9":"code","fad8fcc8":"code","95cd8658":"code","6884959c":"code","bf9479a3":"code","ec71fd97":"code","bf713949":"code","b7543672":"code","8f8832f2":"code","02fd1f92":"code","cf21f570":"code","b4d5d060":"code","effc33be":"code","4e808c3d":"code","fafa1ddc":"code","ec035260":"code","57a0a18b":"code","35b16ee7":"code","e10398b7":"code","d6e6d063":"code","a7bdf13d":"code","1306b3da":"code","697f425b":"code","7a0a26b2":"code","65bc6466":"code","9745c15e":"code","278c08b9":"code","f27f2b6a":"code","61f031c3":"code","145a9891":"code","68d91813":"code","5d1a7002":"code","e198f7cc":"markdown","0179eaf7":"markdown","309bf8a7":"markdown","1a370329":"markdown","b6e11785":"markdown","cb5b8211":"markdown","a03ba7e1":"markdown","6e3f6593":"markdown","66294018":"markdown","c70fbe97":"markdown","4c810fac":"markdown","7c80e965":"markdown","b6f80c9b":"markdown","ed36ed2a":"markdown","30d990b2":"markdown","45e25393":"markdown","0a7088b0":"markdown","14917852":"markdown","babcd8f2":"markdown"},"source":{"7c9eec3b":"## Setting up environvenment on colabs \nimport nltk\nnltk.download('stopwords')\n\n","f266b660":"\n# For Colabs Setup\n# from google.colab import files\n# files.upload() #upload kaggle.json\n\n\n\n","c6c49ea2":"# !pip install -q kaggle\n# !mkdir -p ~\/.kaggle\n# !cp kaggle.json ~\/.kaggle\/\n# !ls ~\/.kaggle\n# !chmod 600 \/root\/.kaggle\/kaggle.json\n# !kaggle datasets download -d rtatman\/blog-authorship-corpus\n# !unzip -q blog-authorship-corpus.zip","9a8ed018":"# Imports \n\nimport gc\ngc.enable\n\nimport pandas as pd\npd.options.mode.chained_assignment = None\nimport numpy as np\nimport re\nimport nltk\nimport spacy\nimport string\nimport seaborn as sns\nfrom nltk.stem.snowball import SnowballStemmer\n# from matplotlib import pyplot as plt\n# %matplotlib inline \n\nsns.set(rc={'figure.figsize':(15, 16)})\n# sns.set_theme(style=\"darkgrid\") #\/\/ for version 11 :\/\nprint(sns.__version__)\n\n\n# # set up display area to show dataframe in jupyter qtconsole\n# pd.set_option('display.max_rows', 500)\n# pd.set_option('display.max_columns', 500)\n# pd.set_option('display.width', 1000)","f5ac9603":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n        \n\n","4748e1c5":"## load dataset and keep the orignal \n\nPATH_ON_Kaggle =\"\/kaggle\/input\/blog-authorship-corpus\/blogtext.csv\"\nPATH_On_Colable = \"blogtext.csv\"\n\n\n## just importing small chunk to start with the process\n# data_orignal = pd.read_csv('blogtext.csv', nrows = 200000,index_col=False)\ndata_orignal = pd.read_csv(PATH_ON_Kaggle, nrows = 70000,index_col=False)\n\n\n\n\n\n\n\n## use complete dataset in the end\n# data_orignal = pd.read_csv(PATH_ON_Kaggle,index_col=False)\n\n\n# droping id and date columns\ndata_orignal.drop(labels=['id','date'], axis=1,inplace=True)\n\n# the next step is to randomize the rows of the data\ndata_orignal = data_orignal.sample(frac=1).reset_index(drop=True)\n\n# gc.collect()","7b82ea0d":"data_orignal.info()","6edf1178":"data_orignal.info()","78ea9536":"## truncated dataset to start on rapid protyping to preprocessing \ndf = data_orignal[[\"text\"]]","af28acf4":"df[:5]","c331e7b6":"pd.options.mode.chained_assignment = None\ndf['text'] = df[['text']]\ndf[\"text\"] = df[\"text\"].astype(str)\n","3df50ad8":"df[\"text_lower\"] = df[\"text\"].str.lower()\ndf.head()\n\n","1215c6a9":"\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('','',PUNCT_TO_REMOVE))\n                                        \ndf[\"text_wo_punctuation\"] = df[\"text_lower\"].apply(lambda text: remove_punctuation(text))\ndf.head()","5814cc71":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\n\n# gc.collect()","493433b4":"STOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf[\"text_wo_stop\"] = df[\"text_wo_punctuation\"].apply(lambda text : remove_stopwords(text))\ndf.head()","5b1a5d8e":"# stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n# df[\"stemmed\"] = df[\"text\"].apply(lambda texts : [stemmer.stem(text) for text in texts])\n# df[\"stemmed\"]\n","1a7238de":"# pd.set_option('display.max_colwidth', None)\n# df[\"text_wo_stop\"][:5]","726064af":"## The output looks okay , some spacing adjustment would do wonders \n## no wories it can be fixed too :)\n## ``` df[col] = df[col].str.strip() ``` to the rescue \n\n\n# Pipeline the pre-processing \n\n\ndef clean_text(text):\n    text = text.strip()\n    text = remove_punctuation(text)\n    text = remove_stopwords(text)\n    text = text.lower()\n    return text\ndata_orignal[\"text\"] = data_orignal[\"text\"].map(lambda text : clean_text(text))\n# data_orignal[\"text\"] = data_orignal[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])","a4fdc918":"gc.collect()\n\ndata_orignal.head()","50108c6b":"data_orignal.info()","b5e52304":"pd.set_option('display.max_colwidth', 50)\ndata_orignal.head(10).T","61059c97":"data_orignal.isnull().any().any() \n\n# ## Good to see there is null\/missing values ","62031aae":"data_orignal.describe(include = [np.number]).T\n","0c97f498":"print(data_orignal.age.value_counts())","0f7052c9":"data_orignal.describe(include = [np.object]).T\n\n\n\n##  we can convert [\"gender\", \"topic\", \"sign\"] as catagorical features ","fad8fcc8":"## let's do some memory analysis for dataframe and try reducing whenever possible\n\norignal_mem = data_orignal.memory_usage(deep=True)","95cd8658":"orignal_mem\n","6884959c":"# catagorical_coloumns = [\"gender\", \"topic\", \"sign\"]\n\n# data_orignal[catagorical_coloumns] = data_orignal[catagorical_coloumns].astype(\"category\")","bf9479a3":"# orignal_mem1 = data_orignal.memory_usage(deep=True)\n# orignal_mem1\n\n# print(orignal_mem\/orignal_mem1)","ec71fd97":"data_orignal.gender.value_counts()","bf713949":"sns.countplot(x=\"age\", data=data_orignal)","b7543672":"sns.countplot(x=\"age\", hue=\"gender\", data=data_orignal)","8f8832f2":"sns.countplot(x=\"topic\", hue=\"gender\", data=data_orignal)","02fd1f92":"data_orignal[\"age\"] = data_orignal[\"age\"].astype(str)\ndata_orignal[\"labels\"] = data_orignal.apply(lambda col :\n                            [col[\"gender\"],col[\"age\"],col[\"topic\"],col[\"sign\"]],axis =1)","cf21f570":"data_orignal.drop(columns=[\"gender\",\"age\",\"sign\",\"topic\"],axis =1, inplace = True)\ngc.collect()","b4d5d060":"data_orignal.head(10)","effc33be":"from sklearn.model_selection import train_test_split\n\nX = data_orignal.text\ny = data_orignal.labels\n\nX_train, X_test, y_train, y_test =train_test_split(X,y, random_state=42,\n                                                   test_size = 0.1,\n                                                  shuffle = True)","4e808c3d":"print(\"shape of training set :\", X_train.shape)\nprint(\"shape of test set :\", X_test.shape)\n\n# gc.collect()","fafa1ddc":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n                      ngram_range=(1, 3), stop_words = 'english')\n\n# inspired by : https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\n# # Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n\ncorpus = list(X_train)+list(X_test)\n# gc.collect()\n\n\n\n","ec035260":"ctv.fit(corpus)","57a0a18b":"# gc.collect()\nxtrain_ctv = ctv.transform(X_train)\n","35b16ee7":"# gc.collect()\nxtest_ctv = ctv.transform(X_test)","e10398b7":"print(len(ctv.vocabulary_))","d6e6d063":"ctv.get_feature_names()[:10]","a7bdf13d":"# tfv = TfidfVectorizer(min_df=3,  max_features=None, \n#             strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n#             ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n#             stop_words = 'english')\n\n\n# # Fitting TF-IDF to both training and test sets (semi-supervised learning)\n# tfv.fit(list(X_train) + list(X_test))\n# xtrain_tfv =  tfv.transform(X_train) \n# xvalid_tfv = tfv.transform(X_test)\n\n\n# gc.collect()","1306b3da":"\n\nlabel_counts=dict()\n\nfor labels in data_orignal.labels.values:\n    for label in labels:\n        if label in label_counts:\n            label_counts[str(label)]+=1\n        else:\n            label_counts[str(label)]=1\n        \nlabel_counts\n\n\n# gc.collect()","697f425b":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Transform between iterable of iterables and a multilabel format\nbinarizer=MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n\n\ny_train = binarizer.fit_transform(y_train)\ny_test = binarizer.transform(y_test)\n\n\n","7a0a26b2":"y_train\n","65bc6466":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\n\ndef display_metrics_micro(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: Micro', f1_score(Ytest, Ypred, average='micro'))\n    print('Average precision score: Micro', average_precision_score(Ytest, Ypred, average='micro'))\n    print('Average recall score: Micro', recall_score(Ytest, Ypred, average='micro'))\n    \n    \ndef display_metrics_macro(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: Macro', f1_score(Ytest, Ypred, average='macro'))\n    print('Average recall score: MAcro', recall_score(Ytest, Ypred, average='macro'))\n    \ndef display_metrics_weighted(Ytest, Ypred):\n    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n    print('F1 score: weighted', f1_score(Ytest, Ypred, average='weighted'))\n    print('Average precision score: weighted', average_precision_score(Ytest, Ypred, average='weighted'))\n    print('Average recall score: weighted', recall_score(Ytest, Ypred, average='weighted'))","9745c15e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Using pipeline for applying logistic regression and one vs rest classifier\nLogReg_pipeline = Pipeline([\n                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'),\n                    n_jobs=-1)),])\nLogReg_pipeline.fit(xtrain_ctv, y_train)\n\nY_predicted_oneVsRest = LogReg_pipeline.predict(xtest_ctv)\n\n","278c08b9":"display_metrics_micro(y_test,Y_predicted_oneVsRest)","f27f2b6a":"display_metrics_macro(y_test,Y_predicted_oneVsRest)","61f031c3":"display_metrics_weighted(y_test,Y_predicted_oneVsRest)","145a9891":"import random \n\ndef print_predicted(y_predicted, y_test = y_test , n = 5):\n    j = []\n    for i in range(n):\n        j.append(random.randint(0, len(y_test)))\n    print(j)\n                 \n    for k in j:\n        print(binarizer.inverse_transform(y_predicted)[k])\n        print(binarizer.inverse_transform(y_test)[k])\n        print(\"=================x==================x================x==========\")\n                \n                 \n        ","68d91813":"print_predicted(y_predicted=Y_predicted_oneVsRest,y_test=y_test, n= 10)","5d1a7002":"print(\"-----------------------------EndofKernal----------------------------------------\")","e198f7cc":"####  TF-IDF (Term Frequency - Inverse Document Frequency)","0179eaf7":"## Preprocessing\n\nSome of the common text preprocessing \/ cleaning steps are:\n\n    \n    - [ ] remove unwanted space \n    - [ ] remove unwanted characters \/ remove Punctuation\n    - [ ] remove Stopwords \n    - [ ] convert text to lowercase\n    - [ ] Stemming ( Snowball )\n\n","309bf8a7":"### Let's check for any 5 inputs and there predicted labels\nPrint true label and predicted label for any five examples\n","1a370329":"### Accuracy metric \n\n\nBefore diving into preparing models , let's settle with Metrics to measure prediction score\n\n        1. Accuracy score\n        2. F1 Score\n        3. Average precision score : \n            3.1 Macro averaged precision: calculate precision for all classes\n                individually and then average them\n            3.2 Micro averaged precision: calculate class wise true positive and false\n                positive and then use that to calculate overall precision\n        4. Average recall score\n\n","b6e11785":"\n### Lower Casing\n\nLower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n\nThis is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts \/ tfidf values.\n\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\nBy default, lower casing is done my most of the modern day vecotirzers and tokenizers like [sklearn TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) and [Keras Tokenizer](https:\/\/keras.io\/preprocessing\/text\/). So we need to set them to false as needed depending on our use case.\n","cb5b8211":"Pipeliining all the above steps :","a03ba7e1":"\n### Removal of stopwords\n\nStopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n\nThese stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n","6e3f6593":"### Transform the labels\n\n\n    Transform between iterable of iterables and a multilabel format\n    \n    As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn a. Convert your train and test labels using MultiLabelBinarizer","66294018":"### Create train and test dataset\n\n    Let's create a simple train and test dataset then create a cross validation too","c70fbe97":"### Vectorizing the features \n\n    1. Bag of words \n    2. TfIDF\n    \n    \n    \n  #### Counte Vectorizer on BoW \n  ","4c810fac":"### Merge labels for multi-label classification problem \n\n       Label columns to merge : [\"gender\", \"age\", \"topic\", \"sign\"]\n       \n       \n       Reduce dataset to \n           dataset --> dataset[text, labels]\n       ","7c80e965":"\n### Removal of Punctuations\n\nOne another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n\nWe also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n\n!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_{|}~`\n\nWe can add or remove more punctuations as per our need.\n","b6f80c9b":"##### To Try :\n 1. Bianry Relevance\n 2. Classifier Chains\n 3. Label Powerset \n 4. Adapted Algorithm\n ","ed36ed2a":"### Multi-Label Classification Techniques\n\n    Most traditional learning algorithms are developed for single-label classification problems. Therefore a lot of approaches in the literature transform the multi-label problem into multiple single-label problems, so that the existing single-label algorithms can be used.","30d990b2":"### Create a dictionary to get the count of every label \n    The key will be label name and value will be the total count of the label. Check below image for reference","45e25393":"## EDA \n\n\nAfter cleaning the text , let's have some idea about the dataset ","0a7088b0":"### Loading the dataset \n\n    First truncated then replace it with comple once protyping is complete \n    \n    As the dataset is large, I am using fewer rows.","14917852":"## About the dataset \n\n#### Context:\n\n\u201cA blog (a truncation of the expression \"weblog\") is a discussion or informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (\"posts\"). Posts are typically displayed in reverse chronological order, so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject or topic.\u201d -- Wikipedia article \u201cBlog\u201d\n\n[This](https:\/\/www.kaggle.com\/rtatman\/blog-authorship-corpus) dataset contains text from blogs written on or before 2004, with each blog being the work of a single user.\nContent:\n\nThe Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the blogger\u2019s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and\/or sign is marked as unknown.)\n\nAll bloggers included in the corpus fall into one of three age groups:\n\n    8240 \"10s\" blogs (ages 13-17),\n    8086 \"20s\" blogs(ages 23-27)\n    2994 \"30s\" blogs (ages 33-47).\n\nFor each age group there are an equal number of male and female bloggers.\n\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.","babcd8f2":"### 1. OneVsRest\n\n    Traditional two-class and multi-class problems can both be cast into multi-label ones by restricting each instance to have only one label. On the other hand, the generality of multi-label problems inevitably makes it more difficult to learn. An intuitive approach to solving multi-label problem is to decompose it into multiple independent binary classification problems (one per category).\n    \n    In an \u201cone-to-rest\u201d strategy, one could build multiple independent classifiers and, for an unseen instance, choose the class for which the confidence is maximized.\n    \n    The main assumption here is that the labels are mutually exclusive. You do not consider any underlying correlation between the classes in this method.\n    \n    "}}