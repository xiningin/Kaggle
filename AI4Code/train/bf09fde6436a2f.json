{"cell_type":{"08260dbf":"code","3dfc0589":"code","ad822947":"code","8fc2829b":"code","81c7d00f":"code","3b683952":"code","c289010d":"code","107a1dae":"code","d136f291":"code","0113c756":"code","5377a5b1":"code","6103ddbf":"code","bf8f7f3c":"code","044affc7":"code","4393a72e":"code","b97846f3":"code","21393d27":"code","aed90731":"code","ed0d74b0":"code","ffc7a912":"code","5408db8f":"code","d5c7eb29":"code","df0f1e4a":"code","e2a43732":"code","521d2b36":"code","b9417df8":"code","d564b78c":"code","d6620e22":"code","5eeb6a91":"code","36b8a22c":"code","7a3154b5":"code","917056b6":"code","129128fe":"code","903afaa2":"code","a01e993b":"code","556b0c51":"code","92e96e7c":"code","95ddace4":"code","d5f755c1":"code","862b5402":"code","06bf0218":"code","55da88d6":"code","746fbc17":"code","3e7df1ac":"code","d08eaab3":"code","ea6eb398":"code","66a4299b":"code","889f4f84":"code","f1070463":"code","ddcd2488":"markdown","cc662add":"markdown","310effa8":"markdown","bd77d04d":"markdown","6d57078a":"markdown","2b710048":"markdown","81489303":"markdown","3ac113f8":"markdown","9eb4aa25":"markdown","f9cbfb3e":"markdown","35b17249":"markdown","8a598916":"markdown","632fe7d6":"markdown","5588fe01":"markdown"},"source":{"08260dbf":"#Importing all neccessary libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport missingno as msno\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n#lets print the name of all files in our folder \n%matplotlib inline\n# Any results you write to the current directory are saved as output.","3dfc0589":"%%time\nloan = pd.read_csv(\"..\/input\/lending-club-loan-data\/loan.csv\")\npop = pd.read_csv(\"..\/input\/population-by-state\/population.csv\")\n","ad822947":"loan.head()  #reading top rows of loan data","8fc2829b":"pop.head(5)  #reading top rows of population data\n#Contains statewise population","81c7d00f":"loan.shape\n#It has 2260668 rows and 145 columns","3b683952":"#Counting null values \nloan.isnull().sum()","c289010d":"print(loan.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan.isnull().any().sum()}\")","107a1dae":"total_num = loan.isnull().sum().sort_values(ascending=False)\nperc = loan.isnull().sum()\/loan.isnull().count() *100\n#perc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis = df_miss[df_miss[\"Percentage %\"]>40]\ntop_mis.reset_index(inplace=True)\ntop_mis","d136f291":"list_to_drop = top_mis['index']\nloan_copy = loan\nloan_copy = loan_copy.drop(list_to_drop,axis=1)","0113c756":"#lets verify now\nloan_copy.shape","5377a5b1":"print(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","6103ddbf":"total_num = loan_copy.isnull().sum().sort_values(ascending=False)\nperc = loan_copy.isnull().sum()\/loan_copy.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy = df_miss_copy[df_miss_copy[\"Percentage %\"]>0]\ntop_mis_copy.reset_index(inplace=True)\ntop_mis_copy","bf8f7f3c":"list_drop_rows = list(top_mis_copy['index'][49:])\nlist_drop_rows","044affc7":"loan_copy = loan_copy.dropna(axis=0,subset=list_drop_rows)","4393a72e":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","b97846f3":"%%time\nlist_object = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"object\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object.append(i)\n       \nloan_object = loan_copy[list_object]\n\n##Print the names of the columns with percentage with object datatype and having missing data\n\ntotal_num = loan_object.isnull().sum().sort_values(ascending=False)\nperc = loan_object.isnull().sum()\/loan_object.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy_object = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy_object = df_miss_copy_object[df_miss_copy_object[\"Percentage %\"]>0]\ntop_mis_copy_object.reset_index(inplace=True)\ntop_mis_copy_object","21393d27":"\nloan_copy[\"emp_title\"]= loan_copy[\"emp_title\"].fillna(loan_copy[\"emp_title\"].mode()[0])\nloan_copy[\"emp_length\"]= loan_copy[\"emp_length\"].fillna(loan_copy[\"emp_length\"].mode()[0])\nloan_copy[\"title\"]= loan_copy[\"title\"].fillna(loan_copy[\"title\"].mode()[0])\n","aed90731":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","ed0d74b0":"list_object = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"int64\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object.append(i)\nlist_object\n\nprint(loan_copy[list_object].isnull().sum())\n\n","ffc7a912":"list_object_float = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"float64\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object_float.append(i)\n       \nloan_object_float = loan_copy[list_object_float]\nprint(loan_object_float.isnull().sum())\n\n##Print the names of the columns with percentage with object datatype and having missing data\n\ntotal_num = loan_object_float.isnull().sum().sort_values(ascending=False)\nperc = loan_object_float.isnull().sum()\/loan_object_float.isnull().count() *100\n#perc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy_float = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy_float = df_miss_copy_float[df_miss[\"Percentage %\"]>0]\ntop_mis_copy_float.reset_index(inplace=True)\ntop_mis_copy_float","5408db8f":"list_fill_rows = list(top_mis_copy_float['index'][13:46])\nlist_fill_rows","d5c7eb29":"for i in list_fill_rows:\n    print(\"For column {0} mode is {1}\".format(i,loan_copy[i].mode()[0]))\n    print(\"The unique values in column {} are {}  \".format(i,loan_copy[i].value_counts()))","df0f1e4a":"list_of_median_impute = ['mths_since_recent_bc','num_rev_accts','num_op_rev_tl','num_rev_tl_bal_gt_0','num_tl_op_past_12m','num_bc_tl','tot_hi_cred_lim','num_il_tl','total_rev_hi_lim','avg_cur_bal','num_actv_bc_tl','mo_sin_rcnt_tl','mo_sin_rcnt_rev_tl_op','mo_sin_old_rev_tl_op',\n'num_actv_rev_tl','num_bc_sats','num_sats','acc_open_past_24mths','mort_acc','total_bal_ex_mort','total_bc_limit']","e2a43732":"list_of_mode_impute = set(list_fill_rows) - set(list_of_median_impute)\nlist_of_mode_impute= list(list_of_mode_impute)\nlist_of_mode_impute","521d2b36":"for i in list_of_mode_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].mode()[0])","b9417df8":"for i in list_of_median_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].median())","d564b78c":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","d6620e22":"total_num = loan_copy.isnull().sum().sort_values(ascending=False)\nperc = loan_copy.isnull().sum()\/loan_copy.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_co = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_co = df_miss_co[df_miss[\"Percentage %\"]>0]\ntop_mis_co.reset_index(inplace=True)\ntop_mis_co","5eeb6a91":"last_list_of_empty  = top_mis_co['index'][0:13]\nlast_list_of_empty","36b8a22c":"for i in last_list_of_empty:\n    print(\"For column {0} mode is {1}\".format(i,loan_copy[i].mode()[0]))\n    print(\"The unique values in column {} are {}  \".format(i,loan_copy[i].value_counts()))","7a3154b5":"last_list_of_median_impute = ['inq_fi','open_acc_6m','open_rv_24m','open_rv_12m','open_il_24m','open_act_il','total_cu_tl','open_il_12m',\n                              'inq_last_12m','all_util','mths_since_recent_inq']","917056b6":"last_set_of_mode_impute = set(last_list_of_empty) - set(last_list_of_median_impute)\nlast_list_of_mode_impute= list(last_set_of_mode_impute)\nlast_list_of_mode_impute","129128fe":"for i in last_list_of_median_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].median())\n\nfor i in last_list_of_mode_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].mode()[0])\n","903afaa2":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","a01e993b":"loan_copy.head(5)","556b0c51":"#importing visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nloan_amount = loan_copy[\"loan_amnt\"].values\nfunded_amount = loan_copy[\"loan_amnt\"].values\ninvestor_funds = loan_copy[\"funded_amnt_inv\"].values\n\n\nsns.distplot(loan_amount, ax=ax[0], color=\"#F7522F\")\nax[0].set_title(\"Loan Applied by the Borrower\", fontsize=14)\nsns.distplot(funded_amount, ax=ax[1], color=\"#2F8FF7\")\nax[1].set_title(\"Amount Funded by the Lender\", fontsize=14)\nsns.distplot(investor_funds, ax=ax[2], color=\"#2EAD46\")\nax[2].set_title(\"Total committed by Investors\", fontsize=14)","92e96e7c":"m = loan_copy['loan_status'].value_counts()\nm = pd.DataFrame(m)\nm.reset_index(level=0, inplace=True)\n\nm['index'][6] = \"DNMCP Fully Paid\"\nm['index'][7] = \"DNMCP Charged Off\"\nm\nm.columns = ['Loan Status','Count']\nplt.subplots(figsize=(20,8))\nsns.barplot(y='Count', x='Loan Status', data=m)\nplt.xlabel(\"Length\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Loan Status in our Dataset\")\nplt.show()","95ddace4":"plt.figure(figsize=(12,6))\n\nplt.subplot(121)\ng = sns.distplot(loan_copy[\"loan_amnt\"])\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Frequency Dist\", fontsize=12)\ng.set_title(\"Frequency Distribuition\", fontsize=20)\n\nplt.subplot(122)\ng1 = sns.violinplot(y=\"loan_amnt\", data=loan_copy, \n               inner=\"quartile\", palette=\"hls\")\ng1.set_xlabel(\"\", fontsize=12)\ng1.set_ylabel(\"Amount Dist\", fontsize=12)\ng1.set_title(\"Amount Distribuition\", fontsize=20)\n\nplt.show()","d5f755c1":"loan_copy['int_round'] = loan_copy['int_rate'].round(0).astype(int)\n\nplt.figure(figsize = (10,8))\n\n#Exploring the Int_rate\nplt.subplot(211)\ng = sns.distplot(np.log(loan_copy[\"int_rate\"]))\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Distribuition\", fontsize=12)\ng.set_title(\"Int Rate Log distribuition\", fontsize=20)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"int_round\",data=loan_copy, \n                   palette=\"Set1\")\ng1.set_xlabel(\"Int Rate\", fontsize=12)\ng1.set_ylabel(\"Count\", fontsize=12)\ng1.set_title(\"Int Rate Normal Distribuition\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.6,top = 0.9)\n\nplt.show()","862b5402":"plt.figure(figsize = (14,6))\n#Looking the count of defaults though the issue_d that is The month which the loan was funded\ng = sns.countplot(x='issue_d', data=loan_copy[loan_copy['loan_status'] =='Default'])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_xlabel(\"Dates\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\ng.legend(loc='upper left')\ng.set_title(\"Analysing Defaults Count by Time\", fontsize=20)\nplt.show()","06bf0218":"import plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","55da88d6":"%%time\n# create loan amount by state column\nloan_amnt_by_state = loan_copy.groupby([\"addr_state\"]).sum()[\"loan_amnt\"]\ndf_region = loan_amnt_by_state.to_frame()\ndf_region[\"loan_amnt\"] = df_region[\"loan_amnt\"].map(\"{:,.0f}\".format)\ntemp = []\nfor x in df_region[\"loan_amnt\"]:\n    a = int(x.replace(',', ''))\n    temp.append(a)\ndf_region[\"loan_amnt\"] = temp\n\n# create number of loan issued by state column\nnum_issued_loan = loan_copy.groupby([\"addr_state\"]).count()[\"loan_amnt\"]\ndf_region[\"num_issued\"] = num_issued_loan\n\n# create average loan amount column\navg_loan_amnt_by_state = []\nfor a,b in zip(df_region[\"loan_amnt\"], df_region[\"num_issued\"]):\n    temp = int(a\/b)\n    avg_loan_amnt_by_state.append(temp)\ndf_region[\"avg_loan_amnt_by_state\"] = avg_loan_amnt_by_state\n\n","746fbc17":"df_region_copy = df_region.copy()\naddr_state = df_region_copy.index\ndf_region.index = list(range(1,52))\ndf_region[\"addr_state\"] = addr_state","3e7df1ac":"# population by states from http:\/\/worldpopulationreview.com\/states\/\npop.index = list(range(1,52))\ndf_region[\"population\"] = pop[\"Population\"]","d08eaab3":"dti = loan_copy.groupby(\"addr_state\").agg([np.mean])[\"dti\"]\ndti.columns = [\"dti\"]\nlen(dti)","ea6eb398":"%%time\nd = loan_copy[loan_copy[\"loan_status\"].isin([\"Late (16-30 days)\",\"Late (31-120 days)\",\"Default\", \"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\"])].groupby(\"addr_state\").size()\nd = d.to_frame()\ne = pd.DataFrame([0],index=[\"ME\"])\nf = pd.concat([d,e])\nf_copy = f.copy()\naddr_state = f_copy.index\nf.index = list(range(1,53))\nf[\"addr_state\"] = addr_state\nf = f.sort_values(by=\"addr_state\")\nf.index = list(range(1,53))\ndf_region[\"num_default\"] = f[0]\n\n# create default_rate column\ntemp = []\nfor x, y in zip(df_region[\"num_default\"], df_region[\"num_issued\"].astype(int)):\n    if x is not 0 and y is not 0:\n        value = (x\/y)\n        value = \"{0:.2f}\".format(value)\n        value = float(value)\n        temp.append(value)\n    else:\n        temp.append(0)\ndf_region[\"default_rate\"] = temp\n\n# create average dti by the state\ndti = loan_copy.groupby(\"addr_state\").agg([np.mean])[\"dti\"]\ndti.columns = [\"dti\"]\ndti.index = list(range(1,52))\ndf_region = df_region.join(dti)\n# plotly color setting\nfor col in df_region.columns:\n    df_region[col] = df_region[col].astype(str)\n    scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\n\n# create text column\ndf_region[\"text\"] = df_region[\"addr_state\"] + '<br>' + \\\n\"Population: \" + df_region[\"population\"] + '<br>' + \\\n\"Total loan amount ($ USD): \" + df_region[\"loan_amnt\"] + \"<br>\" + \\\n\"Avg loan amount ($ USD): \" + df_region[\"avg_loan_amnt_by_state\"] + '<br>' + \\\n\"Default rate: \" + df_region[\"default_rate\"] + \"<br>\" + \\\n\"DTI: \" + df_region[\"dti\"]\n\n# setting plotly and deploy the map\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = df_region['addr_state'],\n        z = df_region['avg_loan_amnt_by_state'], \n        locationmode = 'USA-states',\n        text = df_region['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"$s USD\")\n        ) ]\n\nlayout = dict(\n        title = 'Lending Club Loan<br> Average Loan By State',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n    \nfig = dict( data=data, layout=layout )\niplot( fig, filename='d3-cloropleth-map' )\n","66a4299b":"# import numpy as np\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import MinMaxScaler","889f4f84":"# scaler = MinMaxScaler(feature_range=[0, 1])\n# data_rescaled = scaler.fit_transform(loan.iloc[1:,])","f1070463":"# pca = PCA().fit(data_rescaled)\n# #Plotting the Cumulative Summation of the Explained Variance\n# plt.figure()\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('Number of Components')\n# plt.ylabel('Variance (%)') #for each component\n# plt.title('Lending Club Dataset Explained Variance')\n# plt.show()","ddcd2488":"We can clear see that our dataset contains alot of missing value. ","cc662add":"Added Population dataset from Kaggle which will help us perform EDA more comprehensively ","310effa8":"Dropping these 45 columns from the dataset and will handle the remaining columns with missing data ","bd77d04d":"**A Brief about LendingClub: **\n\n**LendingClub** is the first US based peer to peer lending company, headquarter in SAN Francisco, California to register its offerings as securities and exchange commission. It offers loan trading on secondary market. LendingClub enables borrowers to create unsecured personal loans between 1000 and 40000 with standard loan period of 3 years. LendingClub acts like the \"bridge\" between borrowers and Investors.\n![image.png](attachment:image.png)\n   ","6d57078a":"1. Deleting the rows from columns which have less than <2500 missing data points ","2b710048":"**Why do they need this analysis?**\n\nFrom above working model, it is clear that its very important for LendingClub to know if there is any chance of their borrowers defaulting.","81489303":"**Lets start the treatment of missing values**\n\nSince we have too many columns, lets find the percentage of missing data in each column and print columns which has more that 40 percent missing data","3ac113f8":"Reference : https:\/\/medium.com\/ibm-data-science-experience\/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87\n![image.png](attachment:image.png)\n**Imputation Techniques**\n\nMean, Median and Mode Imputation\n\nImputation with Regression\n\nk-Neareast Neighbor (kNN) Imputation\n\n\nAfter observing the various unique values in the missing columns, the kNN imputation seems to be the best. \nThe imputed values are obtained by using similarity-based methods that rely on distance metrics (Euclidean distance, Jaccard similarity, Minkowski norm etc). They can be used to predict both discrete and continuous attributes. The main disadvantage of using kNN imputation is that it becomes time-consuming when analyzing large datasets because it searches for similar instances through all the dataset ","9eb4aa25":"Now dataset is ready for EDA...","f9cbfb3e":"Imputation has to be performed in different columns and the imputation will be performed in sets.\n","35b17249":"There are no missing values in these int type columns","8a598916":"**Lets now understand the data:**\n\nOur dataset contains information about all loans issued between 2007 and 2015 including current loan status(Current, late, Fully Paid etc.). It also contains credit scores, number of finance inqueries, zip code, state, collections etc. Lets get Started...  ","632fe7d6":"![image.png](attachment:image.png)","5588fe01":"We are left with 99 columns now"}}