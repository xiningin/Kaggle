{"cell_type":{"a7e74b3d":"code","87b1ec90":"code","50adbf59":"code","c669008d":"code","c6e1cf24":"code","597ee2d1":"code","76390e4a":"code","74df28bb":"code","fcd023ae":"code","da17b575":"code","b8ab351f":"code","781dc1ab":"code","176f8caf":"code","7f23d36f":"code","3ad1fac3":"code","4e9899e8":"code","72d6ea27":"code","77aadf66":"code","dbad9ca7":"code","75dd252e":"code","47f8c69b":"code","bc35c4c4":"code","57ae65e9":"code","46dd5efe":"markdown","e7ce72aa":"markdown","db0cdbb0":"markdown","ab8f6ae5":"markdown","f3c70ab4":"markdown","989beb87":"markdown","75e1f07f":"markdown","2cd77282":"markdown","a1b60ee8":"markdown","2634aea9":"markdown","5e489edc":"markdown","458857f3":"markdown","4967e448":"markdown","bf2b4108":"markdown","8ea39956":"markdown","f13f1b00":"markdown","40ee9d41":"markdown","5aa5e0f7":"markdown","a3a972ab":"markdown","f22244ca":"markdown","d23b1a3b":"markdown","d90a5840":"markdown","ca978ce5":"markdown","63df9832":"markdown","f059f296":"markdown","74e59f41":"markdown","38bd9334":"markdown"},"source":{"a7e74b3d":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom zipfile import ZipFile\n\nzip_file = ZipFile('..\/input\/the-winton-stock-market-challenge\/train.csv.zip')\ndf = pd.read_csv(zip_file.open('train.csv'))\ndf.head()","87b1ec90":"zip_file = ZipFile('..\/input\/the-winton-stock-market-challenge\/test_2.csv.zip')\nnew_df = pd.read_csv(zip_file.open('test_2.csv'))\nnew_df.head()","50adbf59":"fig, ax = plt.subplots(figsize = (15, 5))\ndf_na = (df.isnull().sum() \/ len(df))\ndf_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending = False)[: 10]\nax.bar(range(df_na.size), df_na, width = 0.5)\nplt.xticks(range(df_na.size), df_na.index, rotation = 0)\nplt.ylim([0, 1])\nplt.title('Top ten features with the most missing values')\nplt.ylabel('Missing ratio')\nplt.show()","c669008d":"ret = df.loc[:, 'Ret_2':'Ret_120'].sum(1)\nnew_ret = new_df.loc[:, 'Ret_2':'Ret_120'].sum(1)\n\nX = np.hstack((df.loc[:, 'Feature_1':'Ret_MinusOne'].values, ret.values[:, np.newaxis]))\nts = df.loc[:, 'Ret_2':'Ret_120'].values\ny = df.loc[:, 'Ret_PlusOne':'Ret_PlusTwo'].values\ny_ts = df.loc[:, 'Ret_121':'Ret_180'].values\n\nnew_X = np.hstack((new_df.loc[:, 'Feature_1':'Ret_MinusOne'].values, new_ret.values[:, np.newaxis]))\nnew_ts = new_df.loc[:, 'Ret_2':'Ret_120'].values","c6e1cf24":"from sklearn.impute import SimpleImputer\n\nimr = SimpleImputer(strategy = 'mean')\nX = imr.fit_transform(X)\nnew_X = imr.transform(new_X)\n\nts = imr.fit_transform(ts)\nnew_ts = imr.transform(new_ts)","597ee2d1":"fig, ax = plt.subplots(figsize = (10, 5))\nplt.plot(ts[0, :], label = 'Stock 1')\nplt.plot(ts[1, :], label = 'Stock 2')\nplt.plot(ts[2, :], label = 'Stock 3')\nplt.ylim([-0.004, 0.004])\nplt.xlim([0, 118])\nplt.ylabel('Stock returns')\nplt.xlabel('Minutes')\nplt.title('Examples of the stock returns in the first 120 minutes of the current day')\nplt.legend()\nplt.show()","76390e4a":"from mlxtend.plotting import heatmap\n\ncm = np.corrcoef(np.hstack([X, y]).T)\ncols = list(df.columns[1:28]) + ['Ret', 'Ret_PlusOne', 'Ret_PlusTwo']\nhm = heatmap(cm, row_names = cols, column_names = cols, figsize = (20, 20))\nplt.title('Correlations Between the Different Features of the Data', fontsize = 20)\nplt.show()","74df28bb":"plt.scatter(df['Feature_3'], df['Feature_11'], marker = 'o', s = 1) #, linewidth = 1, edgecolor = 'black')\nplt.title('Correlation between Feature_3 and Feature_11')\nplt.xlabel('Feature_3')\nplt.ylabel('Feature_11')\nplt.show()","fcd023ae":"#from sklearn.preprocessing import StandardScaler\n#from sklearn.decomposition import PCA\n\n#scaler = StandardScaler()\n#X[:, :25] = scaler.fit_transform(X[:, :25])\n#pca = PCA(n_components = 25)\n#X[:, :25] = pca.fit_transform(X[:, :25])","da17b575":"X_train = X[:30000, :]\nX_val = X[30000:35000, :]\nX_test = X[35000:, :]\ny_train = y[:30000, :]\ny_val = y[30000:35000, :]\ny_test = y[35000:, :]\nX_train_val = X[:35000, :]\ny_train_val = y[:35000, :]","b8ab351f":"y1_train = y_train[:, 0]\ny2_train = y_train[:, 1]\ny1_val = y_val[:, 0]\ny2_val = y_val[:, 1]\ny1_test = y_test[:, 0]\ny2_test = y_test[:, 1]\ny1_train_val = y_train_val[:, 0]\ny2_train_val = y_train_val[:, 1]\ny1 = y[:, 0]\ny2 = y[:, 1]","781dc1ab":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label = y1_train)\n\nparam = { 'verbosity': 0,\n          'objective': 'reg:pseudohubererror',\n          'eval_metric': 'mae',\n          'subsample': 0.8,\n          'colsample_bytree': 0.8,\n          'tree_method': 'gpu_hist',\n          'eta': 0.1,\n          'max_depth': 5,\n          'gamma': 0,\n          'min_child_weight': 1 }\n\nbst = xgb.cv(param, dtrain, nfold = 3, num_boost_round = 1000, early_stopping_rounds = 50)","176f8caf":"fig = plt.figure(figsize = (12, 4))\nfig.suptitle('The Mean Absolute Error (MAE) of the training data and validation data')\nplt.subplot(121)\nplt.plot(bst['train-mae-mean'], label = 'train')\nplt.plot(bst['test-mae-mean'], label = 'validation')\nplt.xlabel('Runs')\nplt.ylabel('MAE')\nplt.legend()\nplt.subplot(122)\nplt.plot(bst['train-mae-mean'], label = 'train')\nplt.plot(bst['test-mae-mean'], label = 'validation')\nplt.yscale('log')\nplt.xlabel('Runs')\nplt.ylabel('MAE in log scale')\nplt.legend()\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","7f23d36f":"xgb_med = xgb.train(param, dtrain, num_boost_round = bst.shape[0])\n\nfig, ax = plt.subplots(figsize = (6, 8))\nxgb.plot_importance(xgb_med, ax = ax)\nplt.show()","3ad1fac3":"fig, ax = plt.subplots()\nxgb.plot_tree(xgb_med, ax = ax)\nplt.show()","4e9899e8":"select = [1, 5, 6, 13, 14, 18, 22, 25, 26, 27]\n\nX_train = X[:30000, select]\nX_val = X[30000:35000, select]\nX_test = X[35000:, select]\nX_train_val = X[:35000, select]","72d6ea27":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ndef mysearch(X_train, y_train, X_val, y_val, param, param1, param2 = None, estimator = XGBRegressor, score = mean_absolute_error):\n    best_score = 10000000.\n    best_param = { **param }\n    para = { **param }\n    key1 = list(param1.keys())[0]\n    if param2 is not None:\n        key2 = list(param2.keys())[0]\n        for parama in param1[key1]:\n            for paramb in param2[key2]:\n                para[key1] = parama\n                para[key2] = paramb\n                est = estimator(**para)\n                est.fit(X_train, y_train)\n                y_pred = est.predict(X_val)\n                current_score = score(y_pred, y_val)\n                #print('The current score: ', current_score)\n                #print('The current parameter: {} = {}, {} = {}'.format(key1, parama, key2, paramb))\n                if (current_score < best_score):\n                    best_score = current_score\n                    best_param[key1] = parama\n                    best_param[key2] = paramb\n        #print('The best score: ', best_score)\n        #print('The best parameter: {} = {}, {} = {}'.format(key1, best_param[key1], key2, best_param[key2]))\n    else:\n        for parama in param1[key1]:\n            para[key1] = parama\n            est = estimator(**para)\n            est.fit(X_train, y_train)\n            y_pred = est.predict(X_val)\n            current_score = score(y_pred, y_val)\n            print('The current score: ', current_score)\n            print('The current parameter: {} = {}'.format(key1, parama))\n            if (current_score < best_score):\n                best_score = current_score\n                best_param[key1] = parama\n        #print('The best score: ', best_score)\n        #print('The best parameter: {} = {}'.format(key1, best_param[key1]))\n    return best_score, best_param","77aadf66":"param = {'learning_rate': 0.1,\n         'verbosity': 0,\n         'objective': 'reg:pseudohubererror',\n         'tree_method': 'gpu_hist',\n         'n_estimators': 100,\n         'n_jobs': -1,\n         'gamma': 0,\n         'subsample': 0.8,\n         'colsample_bytree': 0.8,\n         'alpha': 0}\nparam1 = { 'max_depth': [1, 3, 5] }\nparam2 = { 'min_child_weight': [1, 3, 5] }\nscore, bst_param = mysearch(X_train, y1_train, X_val, y1_val, param, param1, param2)\nprint('The best score is:', score)\nprint('The best parameter is:', bst_param)","dbad9ca7":"best_xgbr = XGBRegressor( objective = 'reg:pseudohubererror',\n                          tree_method = 'gpu_hist',\n                          max_depth = 5,\n                          min_child_weight = 3,\n                          gamma = 0,\n                          subsample = 0.9,\n                          colsample_bytree = 0.9,\n                          alpha = 0,\n                          learning_rate = 0.01,\n                          n_estimators = 700)","75dd252e":"best_xgbr.fit(X_train_val, y1_train_val)\ny1_test_pred = best_xgbr.predict(X_test)\n\nbest_xgbr.fit(X_train_val, y2_train_val)\ny2_test_pred = best_xgbr.predict(X_test)","47f8c69b":"from sklearn.metrics import mean_absolute_error\n\nbenchmark = [np.abs(y1_test).mean(), np.abs(y2_test).mean()]\nfitted = [mean_absolute_error(y1_test_pred, y_test[:, 0]), mean_absolute_error(y2_test_pred, y_test[:, 1])]\n\nindex = np.arange(2)\nbar_width = 0.35\nplt.bar(index, benchmark, bar_width, label = 'All-zero prediction')\nplt.bar(index + bar_width, fitted, bar_width, label = 'XGBoost regressor')\nplt.xticks(index + bar_width \/ 2, ['Ret_PlusOne', 'Ret_PlusTwo'])\nplt.title('Comparison of the XGBoost regressor and the all-zero prediction')\nplt.xlabel('Stock returns in the two following days')\nplt.ylabel('MAE of the pridiction')\nplt.ylim([0, 0.021])\nplt.legend()\nplt.show()","bc35c4c4":"best_xgbr.fit(X[:, select], y[:, 0])\ny1_new_pred = best_xgbr.predict(new_X[:, select])\n\nbest_xgbr.fit(X[:, select], y[:, 1])\ny2_new_pred = best_xgbr.predict(new_X[:, select])","57ae65e9":"y1_new_pred = 0.1 * y1_new_pred + 0.9 * np.median(y[:, 0])\ny2_new_pred = 0.1 * y2_new_pred + 0.9 * np.median(y[:, 1])\n\nts_new_pred = np.zeros((new_X.shape[0], 60))\n\nzip_file = ZipFile('..\/input\/the-winton-stock-market-challenge\/sample_submission_2.csv.zip')\nsub = pd.read_csv(zip_file.open('sample_submission_2.csv'))\nsub['Predicted'] = np.hstack([ts_new_pred, y1_new_pred[:, np.newaxis], np.median(y[:, 1])* np.ones((new_X.shape[0], 1))]).flatten()\n#sub.to_csv('submission.csv', index = False)","46dd5efe":"The data are split into training data, validation data, and testing data. From the discuss section of this Challenge, we know the Feature_7 is closely related to the time. Therefore, we do not use cross-validation or random splitting here. Otherwise, future information will leak into the training data, and we will get a model that performs very good on the training data but is not useful in predicting future unseen data.","e7ce72aa":"Below are three examples of the time series of the stock returns in the first 120 minutes of the current day. We see that different stock can behaves very different. Detailed analysis of the time series will give us much information on the volatilities of the stocks, which are very useful in pricing options on the stocks.\n\nHowever, the details of the time series are too noisy to be useful to predict the average stock returns. As I mentioned before, we take the sum of the returns in the time series as a single feature.","db0cdbb0":"Below is the Mean Absolute Error (MAE) of the training data and validation data during the training.","ab8f6ae5":"Since we do not know what those independent variables stand for, we just impute the missing values with the mean of that feature. If we are in a real world knowing the meaning of those independent variables, we may find better imputation methods.","f3c70ab4":"Here is an example of using the utility function to tune the maximum depth of the tree and the minimum child weight. The tuning of other parameters are omitted here for brevity.","989beb87":"We use a preliminary XGBoost regressor to do feature selections in this section.\n\nThe Numpy ndarray are transformed into a DMatrix to be compatible with the xgboost package.","75e1f07f":"# Data Visualization and Feature Analysis","2cd77282":"We draw the correlations between different features in the data. Some independent variables are strongly correlated with each other. We will discuss this in the following two cells. We also notice that the target variables are almost not correlated with the independent features. This implies that it is very hard for our model to beat some naive predictions such as the all-zero prediction and the median prediction.","a1b60ee8":"Ensembles of the XGBoost regressor and the median prediction are used for the final results. The median prediction are used here to reduce overfitting.","2634aea9":"We carefully train a XGBoost regressor in this section. Notice that we use Pseudo-Huber loss function which is a differentiable approximation of the linear loss function. It is less sensitive to outliers compared to the conventional square loss function. This is a key idea that make our model perform well.\n\nWe first define a utility function to perform grid search for parameter tuning. The utility function can tune parameters on one-dimensional and two-dimensional grids. ","5e489edc":"We final train the models on the whole data and make prediction on the unseen data.","458857f3":"The testing data are also loaded to a DataFrame in Pandas.","4967e448":"# Feature Selection with XGBoost","bf2b4108":"# Model Training with XGBoost Regressor","8ea39956":"We load and visualize the data in the section. We also analyze the correlations between the features.\n\nTo load the zipped data, we use the zipfile package. The training data are loaded into a DataFrame in Pandas.","f13f1b00":"We select the ten most important independent variables.","40ee9d41":"Here is the final parameters after all tunings.","5aa5e0f7":"The stock returns in the following two days are split to two target variables.","a3a972ab":"We evaluate the model performance on the testing data. The final models consistently perform better than the all-zero prediction.","f22244ca":"The final models are trained on the unions of the training data and the validation data. ","d23b1a3b":"We can use the Principal Component Analysis (PCA) to decouple those independent variables. Below is an example of using PCA. However, my experience shows that using PCA make our model perform worse here. I think this is because many of the independent variable are not useful in predicting future stock returns. In using PCA, we mix useful independent variables and useless independent variables, which make our model perform worse. Therefore, we will not use PCA here.","d90a5840":"We separate the time series of the stock returns in every minute from the independent variables. I tried to use the Recurrent Neural Network (RNN) to deal with the time series, but the RNN suffers from overfitting very badly. I finally decide to take the sum of all returns in the time series as a single feature representing the stock return in the first two hours in the current day. \n\nSimilarly, I tried many models to predict the time series of the intra-day stock returns. None of them beat the all-zero prediction. This is reasonable because in real life we only need to predict the stock return in the next minute. We can update our model every minute with new observations. Therefore, I will just use the all-zero prediction for the intra-day stock returns and focus on predicting the stock returns in the two following days.","ca978ce5":"Here is an example of one decision tree inside the XGBoost model. It has only one leaf.","63df9832":"Most of the columns contain missing values. Below are the top ten features with the most missing values.","f059f296":"We are going to use XGBoost regressor to predict the stock returns in the following two days. I did not see much notebook here. Let me know if it is unproper to post this note.\n\nThe key ideas in this work are  \n(1) using XGBoost to preselect the ten most important features to fight overfitting,  \n(2) using the linear loss function instead of the conventional square loss function.\n\nThe final model consistently outperforms the all-zero predict. In my test, the model also performed better than the median prediction and could rank 30th in the private leaderboard.","74e59f41":"Here is an example of a strong correlation between two independent variables.","38bd9334":"The following figure shows the importance of each independent variable in building the decision trees in the XGBoost regressor. The most important feature 'f6' is the \"Feature_7\" in the original data. The following three important features 'f25', 'f26', 'f27' are the stock returns in the previous two days and the current day. \n\nSince the independent variables are almost not correlated with the target variables, including all of them will introduce many noises in our model. We only use the ten most important independent variables in our model."}}