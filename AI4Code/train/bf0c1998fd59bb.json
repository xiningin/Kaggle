{"cell_type":{"7aa5635c":"code","3639c003":"code","d24175e7":"code","60c10e34":"code","9882f996":"code","2c6e587a":"code","ed99950f":"code","67fbc659":"code","08670409":"code","397174f8":"code","c8ead435":"code","42dc0bc7":"code","02faf7ad":"code","8500cbdd":"code","e5fd964f":"code","b0700ef5":"code","90adb96d":"code","00eadccc":"code","31659c12":"code","9ccebaa6":"code","3150d129":"code","c1aa9928":"code","0085d29c":"code","4c1d38e3":"code","65284556":"code","e053c0e2":"code","d9cc69cf":"markdown","5efea31f":"markdown","35f2ceff":"markdown","d7c6a485":"markdown","05dd4de5":"markdown","e2d8d010":"markdown","eb176e00":"markdown","78006efc":"markdown","edd322f1":"markdown","5a1dbbef":"markdown","2ec475ce":"markdown","b1b32d90":"markdown","e5eb6707":"markdown","63f9b9b3":"markdown","8fd6e973":"markdown","c483ef52":"markdown","41a87c16":"markdown","0d6dc40f":"markdown","1ae0f68a":"markdown","8877be5f":"markdown"},"source":{"7aa5635c":"# import necessary modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualisation\nimport matplotlib.pyplot as plt # plot\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","3639c003":"from sklearn.model_selection import train_test_split\n# read csv file and EDA\ntitanic_train = pd.read_csv('..\/input\/titanic\/train.csv') ; titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_train = titanic_train.reset_index(drop=True)\nNtrain = titanic_train.shape[0] ; Ntest = titanic_test.shape[0]\ntitanic_all = pd.concat([titanic_train, titanic_test], sort=True).reset_index(drop=True)","d24175e7":"titanic_train.head(5) ","60c10e34":"# clean_dataframe function   \ndef clean_dataframe(df):\n    #\n    # Age: replace NaN values by median of each class\n    df['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n    #\n    # Embarked category: Nan value embarked at Southampton according to google\n    #mf_imputer = SimpleImputer(strategy='most_frequent')\n    df['Embarked'] = df['Embarked'].fillna('S')\n    #\n    # Fare\n    df['Fare'].replace(np.nan, df.Fare.median(), inplace=True) # replace NaN values by median age\n    #\n    return df","9882f996":"# replace NaN values\n#\ntitanic_train_clean = clean_dataframe(titanic_train)\ntitanic_test_clean = clean_dataframe(titanic_test)\ntitanic_all_clean = clean_dataframe(titanic_all)\n#","2c6e587a":"titanic_all_clean.info()","ed99950f":"def feature_engineering(df):\n    #\n    # Create deck feature from the first letter of the Cabin column (M stands for Missing)\n    df['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n    idx = df[df['Deck'] == 'T'].index\n    df.loc[idx, 'Deck'] = 'A' # Passenger in the T deck is changed to A\n    df['Deck'] = df['Deck'].replace(['A', 'B', 'C'], 'ABC')\n    df['Deck'] = df['Deck'].replace(['D', 'E'], 'DE')\n    df['Deck'] = df['Deck'].replace(['F', 'G'], 'FG')\n    #\n    # Create Title and IsMarried features\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Family'] = df.Name.apply(lambda x: x.split(',')[0])\n    df['IsMarried'] = 0\n    df['IsMarried'].loc[df['Title'] == 'Mrs'] = 1\n    df['Title'] = df['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss')\n    df['Title'] = df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Rare')\n    #\n    # Create other features\n    df['FamilySize'] = df['SibSp']+df['Parch']+1\n    df['FamilySize'] = df['FamilySize'].astype('int')\n    df['IsAlone'] = 1\n    df['IsAlone'].loc[df['FamilySize'] > 1] = 0\n    df['TicketFrequency'] = df.groupby('Ticket')['Ticket'].transform('count')\n    #\n    # Age: categorize age\n    df['Age_category'] = pd.qcut(df['Age'], 10)\n    #\n    # categorize fare\n    df['Fare_category'] = pd.qcut(df['Fare'], 13)\n    #\n    # categorize integer features\n    list_int2cat = ['Pclass', 'SibSp', 'Parch']\n    df[list_int2cat] = df[list_int2cat].astype('object')\n    #\n    # simplify name\n    df.Name = df.Name.apply(lambda x: x.split(' ')[0][:-1])\n    #\n    # remove useless features\n    #df = df.drop(['Ticket', 'Cabin'], axis=1)  #, 'Embarked'\n    #\n    return df","67fbc659":"# modify and create features\n#\ntitanic_all_clean = feature_engineering(titanic_all_clean)\n#\ntitanic_train_clean = titanic_all_clean[:Ntrain]\ntitanic_train_clean['Survived'] = titanic_train['Survived']\ntitanic_test_clean = titanic_all_clean[Ntrain:]","08670409":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n#\n# Label Encoding of non-numerical features\nlist_le = ['Age_category', 'Fare_category'] #'Pclass', 'SibSp', 'Parch', 'FamilySize', 'IsAlone', 'Embarked', 'Cabin_simp', 'Title']\nle = LabelEncoder()\ntitanic_all_encoded = titanic_all_clean.copy()\nfor feature in list_le:\n    titanic_all_encoded[feature] = le.fit_transform(titanic_all_encoded[feature].astype('category'))","397174f8":"# One-Hot encoding of all features but Age_category and Fare_category that are ordinal \nlist_oh = ['Pclass', 'Sex', 'Embarked', 'Deck', 'Title']#, 'Embarked'] #['Sex', 'Embarked', 'Deck', 'Title', 'FamilySize']\ntitanicoh = pd.get_dummies(titanic_all_clean[list_oh].astype('category')) \ntitanic_all_encoded = titanic_all_encoded.join(titanicoh)","c8ead435":"titanic_train_encoded = titanic_all_encoded[:Ntrain]\ntitanic_train_encoded['Survived'] = titanic_train['Survived']\ntitanic_test_encoded = titanic_all_encoded[Ntrain:]","42dc0bc7":"titanic_train_encoded.info()","02faf7ad":"plt.figure(0,figsize=[15,12])\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nlist_features = ['Age_category', 'Fare_category', 'Deck', 'Pclass', 'FamilySize', 'Parch', 'Title']\n#\nplt.subplot(3,3,1)\nbarplot = sns.barplot(x='Sex', y=\"Survived\", data=titanic_train_clean)\nbarplot.set_xticklabels(barplot.get_xticklabels(), rotation=45)\nfor ifeat, feature in enumerate(list_features):\n    plt.subplot(3,3,ifeat+2)\n    barplot = sns.barplot(x=feature, y=\"Survived\", hue=\"Sex\", data=titanic_train_clean)\n    barplot.set_xticklabels(barplot.get_xticklabels(), rotation=45)\n","8500cbdd":"# import the necessary modules\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom lightgbm import LGBMClassifier","e5fd964f":"# name and parameters of different classifiers\nSEED = 59\n#\nmodel_models  = [# Random Forest: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n                 ('randomforest', \n                  RandomForestClassifier(criterion='gini',min_samples_split=6,min_samples_leaf=6,\n                                         max_features='auto',oob_score=True,random_state=SEED,n_jobs=-1,verbose=1)),\n                 # Adaboost: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n                 ('adaboost', \n                  AdaBoostClassifier(random_state=SEED)),\n                 # Gradient Boosting: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n                 ('gradientboosting',  \n                 GradientBoostingClassifier(random_state=SEED,verbose=1,learning_rate=0.1)),\n                 # XGBoost: http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n                 ('xgboost', \n                  xgb.XGBClassifier(objective='binary:logistic',min_child_weight= 2,eta=0.8, subsample=0.8,colsample_bytree=0.8,\n                                    scale_pos_weight=1,nthread=-1))\n                 ]\n#\nmodel_params = [# Random forest\n                {'max_depth' : np.arange(2,14,2), #[2,4,6,8,10,12], \n                 'n_estimators' : np.logspace(1,3,9).astype(int), #[10, 30, 100, 300, 1000], #\n                 #'min_samples_split' : np.arange(4,14,2), #[1,2,4,6,8,10]\n                 #'min_samples_leaf' : np.arange(4,14,2)\n                 }, \n                # Adaboost\n                {'learning_rate' : [0.1,0.2,0.5,1,1.2,1.5,2], #np.logspace(-2,1,7), \n                 'n_estimators' : np.logspace(1,3,9).astype(int)\n                }, \n                # Gradient boosting\n                {'max_depth' : [2,4,6,8,10,12], \n                 'n_estimators' : np.logspace(1,4,7).astype(int),\n                 'learning_rate' : [0.1,0.5,1,2], #np.logspace(-2,1,7)}, \n                },\n                # XGBoost\n                {'max_depth' : [2,4,6,8,10,12,14,16,18], #[4,5,6,7,8], #\n                 'n_estimators' : np.logspace(0,3,7).astype(int), #[2,3,4,5,6],#\n                # 'eta' : [0,0.2,0.4,0.6,0.8,1.], #[0.7,0.75,0.8,0.85,0.9],#\n                # 'gamma' : [0], #[0,1,10,100,1000],\n                # 'min_child_weight' : [2,4,6,8,10,12] #[4,5,6,7,8], #\n                }, \n                ]                  ","b0700ef5":"# define training and test sets\nlist_drop = ['Age', 'Fare', 'Name', 'Parch', 'PassengerId', 'Pclass', 'SibSp', 'Deck', 'Title', 'Sex', \n             #'TicketSurvivalRate', 'TicketSurvivalRateNA', 'FamilySurvivalRate', 'FamilySurvivalRateNA', \n             'Embarked', 'Ticket', 'Cabin', 'Family']\nnum_test = 0.20\n#\nX_all = StandardScaler().fit_transform(titanic_train_encoded.drop(list_drop, axis=1).drop('Survived',axis=1)) #\ny_all = titanic_train['Survived']\nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=num_test, random_state=23)\nX_test = StandardScaler().fit_transform(titanic_test_encoded.drop(list_drop, axis=1).drop('Survived',axis=1))","90adb96d":"# Run a GridSearchCV on a specified dataset with a given model and return the model object\ndef score_model(model_tupple, model_param, X, y, verbose):\n    modelname, model = model_tupple\n    steps = [('scaler', RobustScaler()), #StandardScaler()), #\n             (modelname, model)]\n    parameters = model_param \n    acc_scorer = make_scorer(accuracy_score)\n    pipeline = Pipeline(steps)\n    grid_obj = GridSearchCV(model, param_grid=parameters,cv=5,\\\n                            scoring='accuracy',\n                            return_train_score=True,verbose=verbose,n_jobs=4) #, \n    grid_obj.fit(X, y)\n    return grid_obj ","00eadccc":"# run a GridSearchCV for a list of models with a given list of parameters\ndef rungrid(list_models, list_params, X, y):\n    list_results = [] ; list_bestestimator = [] ; list_bestparam = [] ; list_bestscore = []\n    for im, model in enumerate(list_models):\n        print('Running %s model' % (model[0]))\n        model_tupple = model \n        model_param = list_params[im] \n        grid_obj = score_model(model_tupple, model_param, X, y, 1)\n        result = pd.DataFrame(grid_obj.cv_results_)\n        bestest = grid_obj.best_estimator_\n        bestparam = grid_obj.best_params_\n        bestscore = grid_obj.best_score_ \n        list_results.append(result) ; list_bestestimator.append(bestest)\n        list_bestparam.append(bestparam) ; list_bestscore.append(bestscore)\n        list_results[-1].to_csv('results_'+model_tupple[0]+'.csv')\n        print(\"%s score: %.3f obtained for parameter value(s)\" % \n              (model_tupple[0], list_bestscore[-1]))\n        print(list_bestparam[-1])\n    return list_results, list_bestestimator, list_bestparam, list_bestscore\n","31659c12":"# run the grids for the models defined above\nmodel_results = [] ; model_bestestimator = [] ; model_bestparam = [] ; model_bestscore = []\nmodel_results, model_bestestimator, model_bestparam, model_bestscore = \\\n    rungrid(model_models, model_params, X_train, y_train)","9ccebaa6":"# compute the predictions for models defined above and create summary table\nMLA_columns = ['Model', 'Parameters','Training Accuracy Mean', 'Test Accuracy Mean', 'Test Accuracy 3*STD' , 'Validation Accuracy', 'Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nfor im, model2 in enumerate(model_models):\n    namemod = model2[0]\n    model = model_bestestimator[im]\n    #\n    titanic_test['Survived_pred('+namemod+')'] = (model.predict(X_test))\n    testcsv = titanic_test.copy()\n    testcsv['Survived'] = testcsv['Survived_pred('+namemod+')']\n    testcsv[['PassengerId', 'Survived']].set_index('PassengerId').to_csv('predictions_'+str(namemod)+'.csv')\n    #\n    pred_valid = model.predict(X_valid)\n    score_valid = accuracy_score(y_valid, pred_valid)\n    #\n    results = model_results[im].sort_values(by='mean_test_score', axis=0, ascending=False)#['params'].tolist()[0]\n    MLA_compare.loc[im,'Model'] = namemod\n    MLA_compare.loc[im,'Parameters'] = str(model_bestparam[im])\n    MLA_compare.loc[im,'Training Accuracy Mean'] = float(results['mean_train_score'].tolist()[0])\n    MLA_compare.loc[im,'Test Accuracy Mean'] = results['mean_test_score'].tolist()[0]\n    MLA_compare.loc[im,'Test Accuracy 3*STD'] = 3.*results['std_train_score'].tolist()[0]\n    MLA_compare.loc[im,'Validation Accuracy'] = score_valid\n    MLA_compare.loc[im,'Time'] = results['mean_fit_time'].tolist()[0]","3150d129":"MLA_compare.sort_values(by='Validation Accuracy',ascending=False)","c1aa9928":"# plot the score of the models as function of hyperparameter\ndef plot_score_1d(plotname, plottitle, list_results):\n    plt.figure(0,figsize=[15,10])\n    plt.subplots_adjust(wspace=0.35, hspace=0.5)\n    for idf, df in enumerate(list_results):\n        Nparams = len(df.params[0])\n        param_name = list(df.params[0].keys())\n        #\n        plt.subplot(2,int(len(list_results)\/2)+1,idf+1)\n        plt.title(plottitle[idf]) #clf_name[im])\n        #plt.ylim(0,0.06)\n        plt.xscale('log')\n        plt.ylabel('Score')\n        if Nparams == 1:\n            plt.xlabel(param_name[0]) #[x for x in model_param.keys()][0])\n            param_value = ([([x for x in df['params'][i].values()][0]) for i in range(len(df['params']))])\n            plt.plot(param_value, df['mean_train_score'],ls='-',color='b',label='Train cont')\n            plt.plot(param_value, df['mean_test_score'],ls='-',color='r',label='Test cont')\n        elif Nparams == 2:\n            plt.xlabel(param_name[0]) #[x for x in model_param.keys()][0])\n            list_param2 = sorted(set([row.params[param_name[1]] for ir, row in df.iterrows()])) \n            for ip2, param2 in enumerate(list_param2):\n                list_param1 = sorted(set([row.params[param_name[0]] for ir, row in df.iterrows()])) \n                mask = [row.params[param_name[1]] == param2 for ir, row in df.iterrows()]\n                plt.plot(list_param1, df[mask]['mean_train_score'],ls='-',color='b',label='Train cont '+str(param2))\n                plt.plot(list_param1, df[mask]['mean_test_score'],ls='-',color='r',label='Test cont '+str(param2))\n        #plt.legend()\n    plt.show()\n    #plt.savefig(plotname+'.png',bbox_inches='tight',transparent=True)\n    plt.close(0)","0085d29c":"# plot the scores for the computed models\nlist_models = list(zip([model[0] for model in model_models], model_bestestimator)) ; model_score = []\nplot_score_1d('score_classifiers', [i[0] for i in list_models], model_results)","4c1d38e3":"# plot the feature importances derived by the best estimators of each model\nlist_models = list(zip([model[0] for model in model_models], model_bestestimator)) ; model_score = []\nlist_usedfeat = titanic_train_encoded.drop(list_drop, axis=1).drop('Survived',axis=1).columns.tolist()\n#\nplt.figure(0,figsize=[15,25])\nplt.subplots_adjust(hspace=0.2)\nfor im, bestest in enumerate(model_bestestimator):\n    plt.subplot(len(model_bestestimator),1,im+1)\n    importances = pd.DataFrame(bestest.feature_importances_, index=list_usedfeat, columns=['Importance']).sort_values(by='Importance',ascending=False)\n    sns.barplot(x='Importance', y=importances.index, data=importances)\n    plt.title(model_models[im][0])\nplt.show()\n#plt.savefig('feature_importance.png',bbox_inches='tight',transparent=True)\nplt.close(0)","65284556":"# create voting classifier \nnamemod = 'voting'\nlist_models = list(zip([model[0] for model in model_models], model_bestestimator)) ; model_score = []\n#\nvc = VotingClassifier(estimators=list_models, voting='soft', n_jobs=-1)\nvc.fit(X_train, y_train) \n#\ntitanic_test['Survived_pred('+namemod+')'] = (vc.predict(X_test))\n#\ntestcsv = titanic_test.copy()\ntestcsv['Survived'] = testcsv['Survived_pred('+namemod+')']\ntestcsv[['PassengerId', 'Survived']].set_index('PassengerId').to_csv('predictions_'+str(namemod)+'.csv')\n#\nMLA_compare.loc[im+1,'Model'] = namemod\n#\ny_valid_pred = vc.predict(X_valid)\nscore_valid = accuracy_score(y_valid, y_valid_pred)\nMLA_compare.loc[im+1,'Validation Accuracy'] = score_valid","e053c0e2":"# compare predictions from voting\nMLA_compare.sort_values(by='Validation Accuracy',ascending=False)","d9cc69cf":"#### Prepare the training, test, and validation sets.","5efea31f":"# 3. EDA","35f2ceff":"Looking at the barplots, it seems that the sex has the strongest influence on the survival probability, followed by age, class, ticket fare. Number of siblings and parents seems to be less important. ","d7c6a485":"#### Import the data and clean the dataframe","05dd4de5":"#### Define the classifiers and their hyperparameters.","e2d8d010":"#### Plot the model scores as function of the hyper-parameter values.","eb176e00":"#### Label-encode and OneHot-encode the features.","78006efc":"#### Give predictions on the test set by ensembling the results of the different used classifiers with a \"soft\" voting. ","edd322f1":"### Plot the survival rate for different categories","5a1dbbef":"# 2. Data cleaning and feature engineering","2ec475ce":"# 4. Evaluate performance of various classifiers","b1b32d90":"#### Add new categories from continuous variables (age, fare) or from feature extraction (cabin, name)","e5eb6707":"Four classifiers are used here: Random Forest, Adaboost, Gradient Boosting, and XGBoost. For each classifier, a (small) grid search is performed through GridSearchCV with two or three hyper-parameters to tune. It takes about 30 minutes to run the four grids with 4 CPUs. ","63f9b9b3":"#### Create a table summarising the performances of all models","8fd6e973":"The survival rate seems to depend on the following features: sex, cabin, embarkment, and class of the cabin that are categorical variables and on the age, number of siblings and spouses, number of parents and children that are continuous variables (or floats\/integers).\n\nLet's clean a bit the dataframe by removing useless features (ticket), by categorising the age into different groups, and by simplifying the name and the cabin (what's really matter is the first letter).","c483ef52":"#### This work is largely inspired by the following notebooks:\n- [Titanic - Advanced Feature Engineering Tutorial by Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)\n- [A Data Science Framework: To Achieve 99% Accuracy by LD Freeman](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)","41a87c16":"#### Plot the feature importances derived by the models.","0d6dc40f":"# 1. Data importation","1ae0f68a":"#### Show the original training dataset","8877be5f":"#### Look for hyper-parameters that give the best predictions through GridSearchCV with a cross validation. "}}