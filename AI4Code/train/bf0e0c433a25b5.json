{"cell_type":{"c94965af":"code","7564b8d0":"code","d85cb023":"code","7a1c8349":"code","316b3f8c":"code","9ca867e9":"code","ea1f8697":"code","63c4b727":"code","7e353170":"code","cdd861c1":"code","29d6a2e9":"code","8dec18b1":"code","f152ffd7":"code","bf310860":"code","62c97ef3":"code","a5cd52ae":"code","c89953ea":"code","c774774a":"code","51921adf":"code","26673bf1":"code","869bb95d":"code","8ed49dda":"code","0c8ac919":"code","7801e171":"code","65b725d3":"code","0760e263":"code","3a425253":"code","1904e258":"code","43102a6d":"code","7f9a1680":"code","0a7fd9bc":"code","15b568aa":"code","aee964b0":"code","743a515b":"code","c7f56289":"code","5ec8b939":"code","05fff7cf":"code","9619937c":"code","776a3453":"code","b4276642":"code","cbcdfa69":"markdown","386f66d8":"markdown","616ec896":"markdown","c40d6a0f":"markdown","8ca2bd32":"markdown","175fe5d6":"markdown","a41e0d9d":"markdown","c019e530":"markdown","71bf22e8":"markdown","4d774c8c":"markdown","a6875d3a":"markdown","2e61baed":"markdown","dd7c92dc":"markdown","3ee21069":"markdown","db40e1f5":"markdown","5787a93b":"markdown","0218f2f7":"markdown","4bf13312":"markdown","2c7ce9ed":"markdown","cbb3d5cc":"markdown","781fbfeb":"markdown","47c6a833":"markdown"},"source":{"c94965af":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\nimport plotly.express as px\n","7564b8d0":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","d85cb023":"print(\"There are {} features and {} rows in the train data.\".format(train.shape[1], train.shape[0]))\nprint(\"There are {} features and {} rows in the test data.\".format(test.shape[1], test.shape[0]))","7a1c8349":"train.head()","316b3f8c":"print(train.info())\nprint()\nprint(test.info())","9ca867e9":"train.describe()","ea1f8697":"test.describe()","63c4b727":"a = train['cp_type'].value_counts().reset_index()\nfig = px.pie(a, values='cp_type', names='index', title='CP Type')\nfig.show()\n\na = train['cp_dose'].value_counts().reset_index()\nfig = px.pie(a, values='cp_dose', names='index', title='CP Dose')\nfig.show()\n","7e353170":"a = test['cp_type'].value_counts().reset_index()\na['Percentage'] = a['cp_type'] \/ len(test)\n\n\nb = test['cp_dose'].value_counts().reset_index()\nb['Percentage'] = b['cp_dose'] \/ len(test)\n\na","cdd861c1":"b","29d6a2e9":"a = train.groupby('cp_time').count()\n\n\nfig = px.bar(a, x=['24', '48', '72'], y='sig_id')\nfig.show()\n\na = test.groupby('cp_time').count()\n\n\nfig = px.bar(a, x=['24', '48', '72'], y='sig_id')\nfig.show()","8dec18b1":"a = train.groupby('cp_dose').max()\na","f152ffd7":"import plotly.figure_factory as ff\n\n# Add histogram data\nx1 = train['g-0']\nx2 = train['g-10']\nx3 = test['g-0']\nx4 = test['g-10']\n\n# Group data together\nhist_data = [x1, x2, x3, x4]\n\ngroup_labels = ['Train G-0', 'Train G-10', 'Test G-0', 'Test G-10']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\nfig.show()","bf310860":"\n# Add histogram data\nx1 = train['c-0']\nx2 = train['c-10']\nx3 = test['c-0']\nx4 = test['c-10']\n\n# Group data together\nhist_data = [x1, x2, x3, x4]\n\ngroup_labels = ['Train C-0', 'Train C-10', 'Test C-0', 'Test C-10']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\nfig.show()","62c97ef3":"import sys\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n","a5cd52ae":"import cuml, cudf\nfrom cuml.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ncolsToDrop = ['sig_id', 'cp_time', 'cp_type', 'cp_dose']\ntsneData = train.drop(colsToDrop, axis=1)\n\n\ntsne = TSNE(n_components=2)\ntrain_2D = tsne.fit_transform(tsneData)\n\nplt.scatter(train_2D[:,0], train_2D[:,1], s = 0.5)\nplt.title(\"Compressed Training Data\")\nplt.show()","c89953ea":"\ncolsToDrop = ['sig_id', 'cp_time', 'cp_type', 'cp_dose']\ntsneData = test.drop(colsToDrop, axis=1)\n\n\ntsne = TSNE(n_components=2)\ntest_2D = tsne.fit_transform(tsneData)\n\nplt.scatter(test_2D[:,0], test_2D[:,1], s = 0.5)\nplt.title(\"Compressed Test Data\")\nplt.show()","c774774a":"train_target = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","51921adf":"print(\"Target data has {} prediction variables\".format(train_target.shape[1] - 1))","26673bf1":"train_target.head()","869bb95d":"train_target.describe()","8ed49dda":"a = train_target['acat_inhibitor'].value_counts().reset_index()\nfig = px.pie(a, values='acat_inhibitor', names='index', title='acat_inhibitor')\nfig.show()","0c8ac919":"train.head()","7801e171":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nobjCols = ['cp_type', 'cp_time', 'cp_dose']\nfor col in objCols:\n    le.fit(train[col])\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])\n    \ntrain.head()","65b725d3":"from sklearn.ensemble import ExtraTreesClassifier\n\nrf = ExtraTreesClassifier(n_estimators=100, criterion=\"entropy\", max_depth = 15, max_features = \"sqrt\", random_state = 1, bootstrap=True, max_samples = 1000, n_jobs=-1)","0760e263":"rf.fit(train.drop(['sig_id'], axis=1), train_target.drop(['sig_id'], axis=1))","3a425253":"preds = rf.predict(test.drop(['sig_id'], axis=1))\npreds.shape","1904e258":"sub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\nfor i in range(len(sub.columns)):\n    if i != 0:\n        col = sub.columns[i]\n        sub[col] = preds[:, i - 1]\nsub","43102a6d":"sub.to_csv('submissionExtraTrees.csv', index=False)","7f9a1680":"from cuml.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(train.drop(['sig_id'], axis=1), train_target.drop(['sig_id'], axis=1))","0a7fd9bc":"preds = knn.predict(test.drop(['sig_id'], axis=1))","15b568aa":"sub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\nfor i in range(len(sub.columns)):\n    if i != 0:\n        col = sub.columns[i]\n        sub[col] = preds[:, i - 1]\nsub","aee964b0":"sub.to_csv('submissionKNN.csv', index=False)","743a515b":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\n\nprint(\"Tensorflow Version: \" + tf.__version__)","c7f56289":"def create_model(show_summary = True):\n    X_input = Input((875,))\n    X = Dropout(0.075)(X_input)\n    X = Dense(1024, activation='relu')(X)\n    X = Dropout(0.2)(X)\n    X = Dense(2056, activation='relu')(X)\n    X = Dropout(0.3)(X)\n    X = Dense(1024, activation='relu')(X)\n    X = Dropout(0.1)(X)\n    X = Dense(600, activation='relu')(X)\n    X = Dropout(0.05)(X)\n    X = Dense(400, activation='relu')(X)\n    X = Dropout(0.01)(X_input)\n    X = Dense(206, activation='sigmoid')(X)    #We need sigmoid not Softmax because labels are mostly independent of each other.\n    \n    model = Model(inputs = X_input, outputs = X)\n    opt = Adam(lr=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer = opt, metrics = ['AUC'])   #Closest default loss function (to competition metric) is logloss or categorical cross entropy. I will also be using AUC just as a side metric for supervision, but shouldn't be taken very seriously.\n    if show_summary:\n        model.summary()\n    return model","5ec8b939":"model = create_model()","05fff7cf":"nFolds = 5\n\nkf = KFold(n_splits = nFolds)\ntrd = train.drop(['sig_id'], axis=1).values\ntargetd = train_target.drop(['sig_id'], axis=1).values\ntestd = test.drop(['sig_id'], axis=1).values\n\npreds = np.zeros((3982, 206))\n\nfold = 1\nfor train_index, test_index in kf.split(trd):\n    \n    print(\"Fold  \" + str(fold))\n    fold += 1\n    \n    x_train, x_test = trd[train_index], trd[test_index]\n    y_train, y_test = targetd[train_index], targetd[test_index]\n\n    model = create_model(False)\n    history = 0\n    history = model.fit(x_train, y_train, batch_size = 32, epochs=65, shuffle = True, validation_data = (x_test, y_test), verbose=0)\n    \n    fig = plt.figure(figsize=(9, 5))\n\n    fig.add_subplot(1, 2, 1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper right')\n    \n    fig.add_subplot(1, 2, 2)\n    plt.plot(history.history['auc'])\n    plt.plot(history.history['val_auc'])\n    plt.title('Model AUC')\n    plt.ylabel('AUC')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n\n    foldPred = model.predict(testd)\n    preds += foldPred \/ nFolds","9619937c":"preds","776a3453":"sub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\nfor i in range(len(sub.columns)):\n    if i != 0:\n        col = sub.columns[i]\n        sub[col] = preds[:, i - 1]\nsub","b4276642":"sub.to_csv('submission.csv', index = False)","cbcdfa69":"Man, looks like we have a huge class imbalance problem. Well, the features behaved too nicely, so the catch had to be here. \nWe can see that the mean is very close to 0 for most variables, and for all it looks like it is closer to 0 than 1. Also, the first, second and third quartile values are all 0, showing that more than 75% of the data has 0 as a target. This may make it especially hard to train a NN.\n\nThis dataset is not easy to do data augmentation with either (in fact, I don't even know if that's possible). The only way to get more positives would be to search for external data. I guess this shows us how hard drug discovery is, and how patient one has to be to get results.\n\nLet's also just plot the distribution curve for one targe (doesn't matter much, just to see class imbalance).","386f66d8":"# EDA Overview","616ec896":"Again, here it looks like there isn't one specific dosage timing that has been popularly used. Also, the train and test data are similar in distribution. One important thing about this feature is that it is fundamentally a categorical variable, but has a numerical value. Hence, we should probably label encode (or one-hot encode it) instead of leaving it as a continuous variable. (This step doesn't matter when using tree based models, only linear models).","c40d6a0f":"So basically all features are float features, and there may not be any interesting encoding that could be done. We will try out a few later. Looking at the data, we have almost 900 float columns, so let's see how different they really are. Now, we can't make a plot for comparing them, since there are too many, but let's look at the mean and median for about 10 of them.","8ca2bd32":"A couple of things I wanted to mention here. I did do some more analysis on the relation between the features above, and just wanted to mention it instead of making plots for it cause it isn't value additive:\n\n- CP_Type and CP_Time are completely independent (meaning that different treatment types don't have a significantly different dosage duration).\n- CP_Dose and CP_Time are also completely indpendent (its not that high dosage has a significantly higher or lower average treatment duration).\n- CP_Type and CP_Dose are also indpendent (its not that a particular treatment type has a higher or lower average dosage.\n\n\nAll in all, from the features above, the dataset seems to be very homogenous. No imbalances whatsoever. It's almost as if every treatment picked the values of the first few columns at random, creating a sort of random experiment (maybe to try out all possible treatment combinations of a drug?). This removes any value addition from features which combine the features discussed above.\n\nNow let's move onto the majority of the features. Right now, I am only going to plot two of each, because there are almost hundreds of them.","175fe5d6":"Again, the test data is on our side, and has a similar percentage distribution as the training data. This is again great for our model.\n\nLet's move onto the dosage timing, since this is the last variable which is not anonymous.","a41e0d9d":"So they are very very similar in terms of their ranges, indicating that they may be similar in nature. There might be a fixed scale along which all of these features are measured, due to which they have a similar median and min-max range. Let's see if it is the same for test.","c019e530":"These features are much more interesting. They have very clear fat tails here, and all of them surprisingly are towards -10. Given this, I might expect the data points in the hump towards the left to have slightly different properties, so a feature could be whetehr it is in this hump or not. But this feature will be less useful for now because only a handful of datapoints (in percentage) are actually in this range.\n\nAgain, a potential empirical z-score could be applied to make features.\n\n# TSNE\n\nBefore we move onto target features, lets just use TSNE to find out how the data is sort of distributed by dimensionality reduction.\nFor this I will be using Rapids, as it is really fast compared to sklearn.","71bf22e8":"Wow, that's a lot of 0s. Let's see if there is a class imbalance problem here.","4d774c8c":"Okay, so this TSNE plot will actually change every time I run it, but the general outline stays the same (sometimes it just ends up as one blob in the corner). I am not using plotly here as it makes the notebook lag. So here's a picture of what I am seeing:\n![Screenshot%202020-09-07%20at%2010.32.33%20AM.png](attachment:Screenshot%202020-09-07%20at%2010.32.33%20AM.png)\n\nIt looks like a majority data is clustered into different groups (you can even see groups in the center, but they are less clear). Then, there are a bunch of data points that are just evenly scattered across the plot. So it looks like about more than half of the data points are structured and have a cluster, and the rest are sort of evenly distributed. This is interesting, as it gives hope that an ML model may be able to predict the targets accurately, but we will have to see.\n\nLet's see for test data","a6875d3a":"Again they look very similar. Another thing is that the ranges look like they are the same in train and test data, so that's definitely going to help when modelling. We won't have to take into account feature shifts, etc, but let's see what new insight may come later.\n\nLet's start with the two categorical variables.","2e61baed":"# MoA (Mechanisms of Action)\n\nSo in this notebook, I just want to get an overview of the data, how the variables are distributed, what do we have to predict, and maybe come up with a few feature ideas. Since I have little knowledge on the subject, most of the feature ideas would be based on data distribution and not on domain knowledge. \n\nI also want this notebook to be a basic pipeline of sorts, so that one can always come back and work on just improving the models. So let's start with some libraries.","dd7c92dc":"## Neural Network\n\nHere, I will be trying a simple 3 layer Neural Network (excluding Input Layer). Right now, the Number of neurons is quite arbritary and could be tuned to increase performance. I just tried to slowly decrease the number of neurons from 875 input dimension to 200 output neurons. I have also added a small amount of dropout. I tried out a few values and I think the numbers should be somewhere in this ballpark, but can be tuned further. For epochs, I plotted the training curves for about 50 epochs locally and chose where I think the NN started overfitting on average. I will be using Keras for implementation, and a 5 K-Fold validation system (since all features look roughly evenly distributed in train and test.\n\nNote that the Neural Network can be made much better by making it bigger or tuning parameters.","3ee21069":"\n\n\n# Conclusion:\n\nSo we have the following conclusions:\n\n- Training and Testing data seem to have a similar distribution (atleast generally). Even adversarial validation shows this.\n- The non g and c features are independent of each other and there seems to be no clear relationship.\n- Huge Class Imbalance Problem - Surprisingly using log-loss as a metric.\n- Models that don't require extensive feature engineering may work better for this dataset.\n\nI will continue to add more in this notebook. Thanks a lot for reading all the way to the end. Please upvote if you found it helpful.","db40e1f5":"Let's load in only the features for now, and later we'll load in the target labels.","5787a93b":"Let's look at a summary of our data: how many float columns and object columns are there?","0218f2f7":"The graphs clearly show us that more sample (I think, correct me if I am wrong) were treated with a compound, rather than a control Perturbation. This is good, because control perturbations don't have MoAs, so it could be more complicated to train on them.\n\nSecondly, it looks like roughly half the patients have had a high dose, and the other half have had a low dose. This shows a good equal representation in the dataset, so we may not have to worry too much about imbalances. Let's make sure the distribution is the same in test data as well.","4bf13312":"## Extra Trees Classifier\n\nSo I have already submitted this model and it scores 0.11932 on the public leaderboard. There might be some variations in the score since this is a extremely random forest model. Also, I have used only 100 estimators to keep the time under 5 mins, but using more would probably give a better result. Same with teh max_depth. Increasing it would help but would take a lot more time.","2c7ce9ed":"So this data also has one or two clear clusters. The rest is not very clear, but it may be because of the low amount of data, so it is hard for us to see structure.\n\n# Target Labels\n\nNow let's move onto target variables.","cbb3d5cc":"So this just tells us an even more pessimistic story. Only 0.1% of the data is positive. Rest is all negative results. This really may make model training hard, so I think we are going to have to develop some ingenious way for predictions.\n\nOne more interesting thing is that the Metric for this competition is based on Log-Loss. But with class imbalances, I would expect there to be another metric like AUC as they are not as susceptible to class imbalances. For comparision, in this competition, submitting all 0.5 (sample submission) gets a score of 0.69314. I created a submission of only 0s as predictions, and that got a score of 0.13073. There is a huge difference in these scores even though both are equally unintelligent, which is why Log-Loss is not used in class imbalance. Anyway, let's move on now since the metric is fixed.\n\n\nSince there are too many 0s for doing a meaningful TSNE for target labels, let's move onto modelling stage.\n\n# Modeling","781fbfeb":"## K Nearest Neighbors\n\nWith 800 features and 38000 rows, it may be mad to use KNN! But rapids is surprisingly fast, in fact, the rapids KNN took less time for running than the Extra Trees Regressor Model! I think this could be a good baseline approach since the competition says \"One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithm that search for similarity to known patterns in large genomic databases\". The training data could act as our database (and later down the line, we can add external data), and KNN is fundamentally a similarity based algorithm. I have already submitted this algorithm, and it scores 0.11351, which is better than the Extra Trees Algorithm.\n\nI won't be normalizing or scaling any features since they are already in a very similar range.","47c6a833":"This histogram gives us some more insight into the G features. It appears that they all look like they have a normal distribution, and there is a small shift in the mean. But the main dividing element is the skewness. From the tick marks shown below the histogram, we can see that the red variable looks like it has a slight positive skew (towards the left), and the green looks like a slight negative skew.\n\nEven with the orange has a positive skew, but the blue has a negative skew. Based on this, maybe a feature could be made on how far away a data point is from the mean, taking into account the skewness of each chart. Since z-score directly won't be very helpful, we could empirically find the number of data points below the row of interest (by merging train and test, and then finding the number of data points less than this value)."}}