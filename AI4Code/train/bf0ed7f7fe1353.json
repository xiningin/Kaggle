{"cell_type":{"d04cfb73":"code","0fce297f":"code","6fa34f3b":"code","b42a05b2":"code","f3dc4504":"code","09f83500":"code","af54caed":"code","5dbe077c":"code","e9f7e2e9":"code","d150be7d":"code","3ae0607e":"code","d094e3b7":"code","f3564645":"code","f3075d8d":"code","201b2ab7":"code","368fb1ee":"code","3c29f53e":"code","ce8c1b61":"code","7098bc33":"code","37e930d3":"code","0c269664":"markdown","10260d66":"markdown"},"source":{"d04cfb73":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom keras.preprocessing import image\nimport keras\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa","0fce297f":"%env SM_FRAMEWORK=tf.keras\n!pip install segmentation_models\nimport segmentation_models as sm","6fa34f3b":"test_cities = ['berlin', 'bielefeld', 'bonn', 'leverkusen', 'mainz', 'munich']\ntrain_cities = ['aachen', 'bochum', 'bremen', 'cologne', 'darmstadt', 'dusseldorf', 'erfurt', 'hamburg', 'hanover', 'jena', 'krefeld', 'monchengladbach', 'strasbourg', 'stuttgart', 'tubingen', 'ulm', 'weimar', 'zurich']\nval_cities = ['frankfurt', 'lindau', 'munster']","b42a05b2":"train_img_paths = []\ntrain_ann_paths = []\n\nfor cities in train_cities:\n\n    train_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/train\/\" + cities\n    train_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/train\/\" + cities\n    \n    \n\n    train_img_paths = train_img_paths + sorted(\n        [\n            os.path.join(train_img_dir, fname)\n            for fname in os.listdir(train_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    train_ann_paths = train_ann_paths + sorted(\n        [\n            os.path.join(train_ann_dir, fname)\n            for fname in os.listdir(train_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of train images:\", len(train_img_paths))\n\nprint(\"Number of train annotations:\", len(train_ann_paths))\n\nval_img_paths = []\nval_ann_paths = []\n    \nfor cities in val_cities:\n    val_img_dir = \"..\/input\/city-scapes-images\/leftImg8bit\/val\/\" + cities\n    val_ann_dir = \"..\/input\/cityscapes-fine-annotations\/gtFine\/val\/\" + cities\n    \n\n\n    val_img_paths = val_img_paths + sorted(\n        [\n            os.path.join(val_img_dir, fname)\n            for fname in os.listdir(val_img_dir)\n            if fname.endswith(\"_leftImg8bit.png\")\n        ]\n    )\n    val_ann_paths = val_ann_paths + sorted(\n        [\n            os.path.join(val_ann_dir, fname)\n            for fname in os.listdir(val_ann_dir)\n            if fname.endswith(\"_gtFine_labelIds.png\")\n        ]\n    )\n\nprint(\"Number of val images:\", len(val_img_paths))\n\nprint(\"Number of val annotations:\", len(val_ann_paths))","f3dc4504":"img_size = (384, 384)\nnum_classes = 8\nbatch_size = 5\nimgaug_multiplier = 4\nepochs = 50","09f83500":"# Reducing 32 categories to only 8 categories\n\ncats = {\n 'void': [0, 1, 2, 3, 4, 5, 6],\n 'flat': [7, 8, 9, 10],\n 'construction': [11, 12, 13, 14, 15, 16],\n 'object': [17, 18, 19, 20],\n 'nature': [21, 22],\n 'sky': [23],\n 'human': [24, 25],\n 'vehicle': [26, 27, 28, 29, 30, 31, 32, 33, -1]}\n\ndef convertCats(x):\n    if x in cats['void']:\n        return 0\n    elif x in cats['flat']:\n        return 1\n    elif x in cats['construction']:\n        return 2\n    elif x in cats['object']:\n        return 3\n    elif x in cats['nature']:\n        return 4\n    elif x in cats['sky']:\n        return 5\n    elif x in cats['human']:\n        return 6\n    elif x in cats['vehicle']:\n        return 7\n    \nconvertCats_v = np.vectorize(convertCats)\n\ndef preprocessImg(img):\n    image_matrix = np.expand_dims(img, 2)\n    \n    converted_image = convertCats_v(image_matrix)\n    return converted_image\n","af54caed":"def generateRandomParams(seed):\n    np.random.seed(seed)\n    angle = np.random.randint(26)\n    positive = np.random.randint(2)\n    \n    if positive == 0:\n        angle = angle * -1\n        \n    crop = np.random.randint(3)\n    crop = crop \/ 10\n        \n    return angle, crop","5dbe077c":"class Image(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        \n        x = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (3,), dtype=\"float32\")        \n        for j, path in enumerate(batch_input_img_paths):\n            img = image.load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size * imgaug_multiplier,) + self.img_size + (1,), dtype=\"uint8\")\n        \n        for j, path in enumerate(batch_target_img_paths):\n            _img = image.load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = preprocessImg(_img)\n            \n            # Image AUGMENTATION         \n        for mul in range(1, imgaug_multiplier):  \n            for i in range(0, self.batch_size):\n                \n                angle, crop = generateRandomParams(i * mul)\n                \n                seq = iaa.Sequential([                        \n                    iaa.Affine(rotate=(angle)),\n                    iaa.Crop(percent=(crop)),                    \n                ])\n                \n\n                image_aug = seq(image=x[i])\n                x[batch_size * mul + i] = image_aug\n\n                image_aug = seq(image=y[i])\n                y[batch_size * mul + i] = image_aug # END Image AUGMENTATION  \n            \n            \n        return x, y","e9f7e2e9":"train_seq = Image(\n    batch_size, img_size, train_img_paths, train_ann_paths\n)\nval_seq = Image(\n    batch_size, img_size, val_img_paths, val_ann_paths\n)","d150be7d":"BACKBONE = 'resnet50'\npreprocess_input = sm.get_preprocessing(BACKBONE)","3ae0607e":"model = sm.PSPNet(BACKBONE, encoder_weights='imagenet', classes=num_classes ,  input_shape=(*img_size, 3))\nmodel.compile(\n    'Adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy', 'sparse_categorical_accuracy'],\n)","d094e3b7":"callbacks = [\n    keras.callbacks.ModelCheckpoint(\"checkpoint.h5\", save_best_only=True),\n    keras.callbacks.EarlyStopping(patience=3, verbose=1)\n]\n\nhistory = model.fit(\n    train_seq,\n    steps_per_epoch=len(train_seq),\n    epochs=epochs,\n    callbacks=callbacks,\n    validation_data=val_seq,\n    validation_steps=len(val_seq),\n)","f3564645":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.plot(history.history['sparse_categorical_accuracy'])\nplt.plot(history.history['val_sparse_categorical_accuracy'])\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\n\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"Lowest loss\")\nplt.plot( np.argmax(history.history[\"val_accuracy\"]), np.max(history.history[\"val_accuracy\"]), marker=\"x\", color=\"b\", label=\"Highest accuracy\")\nplt.plot( np.argmax(history.history[\"val_sparse_categorical_accuracy\"]), np.max(history.history[\"val_sparse_categorical_accuracy\"]), marker=\"x\", color=\"g\", label=\"Highest accuracy\")\n\nplt.show()","f3075d8d":"model.save('model.h5')","201b2ab7":"if not os.path.exists('.\/model.h5'):\n    model = keras.models.load_model('..\/input\/model-gpu-v1\/model (4).h5')","368fb1ee":"X = np.empty((1, *img_size + (3,)))\nX[0,] = val_seq[0][0][0]","3c29f53e":"res = model.predict(X)","ce8c1b61":"def combineMask(masks):\n    _output = np.empty((384,384) + (1,))\n    _x, _y = 0, 0\n    \n    for x in range(0, masks.shape[0]):\n        for y in range(0, masks.shape[1]):\n                   _target = masks[x][y]\n                   _output[x][y] = np.argmax(_target) \n    return _output\n","7098bc33":"testing = combineMask(res[0])","37e930d3":"keras.preprocessing.image.array_to_img(\n    testing, data_format=None, scale=True, dtype=None,\n    )","0c269664":"# Prepare data","10260d66":"# Prepare PSPNET"}}