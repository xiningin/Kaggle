{"cell_type":{"5908062b":"code","db778fe2":"code","44d77d89":"code","d65b7550":"code","e3573e64":"code","351f3b53":"code","2c13d522":"code","b35c46a0":"code","afc43df9":"code","8254399d":"code","300fc90a":"code","863dd8f8":"code","83d56243":"code","6d266efc":"code","64d89bc6":"code","a02c9946":"code","88174b1f":"code","14a3b54e":"code","14092d26":"code","4b9dce06":"code","1cd29ada":"code","0924df48":"code","4adf975f":"code","e01cb64b":"code","ef979beb":"code","aa4ab18b":"code","b4fda6c0":"code","4d4089de":"code","e329f512":"code","ae85e9f1":"code","b7d51334":"code","c09e160a":"code","aeba0b51":"code","9c18d4b8":"code","9d0110ec":"code","91468b50":"code","031fc3dd":"code","932638dc":"code","c102b59d":"code","a279d8c4":"code","bcdbf0d3":"code","18d69e0d":"code","5371dc2f":"code","e041d338":"code","08d25b8a":"code","9b5daf3f":"code","c4ede7fc":"code","1a55165b":"code","06745d12":"code","637f6a4e":"code","84a8b6d7":"code","8cb3b7a7":"code","cc19feec":"code","c3d7217c":"code","1e894db9":"code","acde7872":"code","d882371b":"code","2f1be734":"code","42d08afc":"code","c22db9ee":"code","2c00e780":"code","aea89322":"code","fb9e4e80":"code","040f713c":"code","743912e2":"code","b2e6fb66":"code","3b319b90":"code","6b008084":"code","c83906a0":"code","3619ff56":"code","b1cf8968":"code","f919da7c":"code","435d9269":"code","8e50e883":"code","6af83b2b":"code","3052db66":"code","12ada21c":"code","3a58a18f":"code","1742f48e":"code","2155b888":"code","0a3dd618":"code","b401e5af":"code","cc0bffb3":"code","1b0a20be":"code","dd7b010d":"code","26727119":"code","82445adc":"code","839b3c7d":"code","4ac32752":"code","fc4bf8c5":"code","8f8b08d6":"code","ca6bbd8a":"code","10d3285b":"code","22ff1dfd":"code","edb7bb88":"code","53172c57":"code","8c3f2350":"code","8772bb13":"code","0984a946":"code","90365d77":"code","6fcb1a56":"code","b3803e1e":"code","2b9f647d":"code","2df3b56a":"code","b99b6aba":"code","4ca926cd":"code","f6802f6b":"code","403734aa":"code","4e91cb4c":"code","1d9f9ff2":"code","d5c0e59a":"code","08836486":"code","c2570b9e":"code","e704daec":"code","28fea6cf":"code","03ecbb97":"code","2499c8cb":"code","eb432b62":"code","84bdacd6":"code","c04d8f5e":"code","e3c4e8c5":"code","041438a6":"code","b79b9d22":"code","4f6964f9":"code","0a716753":"code","d16242ba":"code","5f0fbbf9":"code","a024af7f":"code","dfc52ee0":"code","9f7484ec":"code","deff0242":"code","8a793ecb":"code","d5a237d2":"code","a4f5c8ee":"code","31d93e93":"code","7d9c0872":"code","cd31f8af":"code","a217a03a":"code","761db5cd":"code","cb286524":"code","57babdf4":"code","39731301":"code","9421943c":"code","426cb594":"code","2c235b05":"code","30f595fb":"code","e32c8647":"code","02f8b602":"code","b6846f95":"code","6e7fc722":"code","329e555e":"code","524c4f1a":"code","93fcfd86":"code","2d1bccb1":"code","24a4100b":"code","9acdb464":"code","0cd94f9a":"code","c6a6b383":"code","5fbaf031":"code","1130b0ed":"code","c8803717":"code","7da6c8e4":"code","56262db0":"code","79b2bbaf":"code","cb16f50d":"code","0b2cce86":"code","f6550a9c":"code","dae79531":"code","872fb93f":"code","f814b99f":"code","29acda66":"code","eb38ba2f":"code","6dda002e":"code","88aa906d":"code","b66e630d":"code","4808d15b":"code","54ba4c15":"code","1fa85814":"code","56d7058c":"code","28ddffd0":"code","d1832a86":"code","1dd1145c":"code","d018439c":"code","36187eec":"code","b1ef98fb":"code","4d2db544":"code","5c38386b":"code","c2b46998":"code","c80dba43":"code","23b0996b":"code","a3f00118":"code","fec6383b":"code","8f77c273":"code","bc92cbe8":"code","5d826fa3":"code","5186ce50":"code","95947eb0":"code","5b31b53d":"code","7d91f787":"code","e3baff67":"code","5042f9bc":"code","8e8bafc3":"code","d70e6b42":"code","ddd6d7bd":"code","9597bc30":"code","dbc80330":"code","6ee00673":"code","93653773":"code","598bb40c":"code","417c23ca":"code","3971efde":"code","ac33ef66":"code","af03e8d2":"code","537cb5ed":"code","6e851680":"code","38428b38":"code","af2c6a60":"code","e770687b":"code","6d6d9ddc":"code","505b47a4":"code","60023134":"code","bd90f477":"code","ee6835d2":"markdown","f8235e60":"markdown","45c3d2d7":"markdown","1ba9bf41":"markdown","37f82f06":"markdown","878a4000":"markdown","2f031f0a":"markdown","1f60498a":"markdown","6aa002e3":"markdown","b82a109b":"markdown","1cfafdb7":"markdown","1cb6625f":"markdown","8feb0e0e":"markdown","71fcc744":"markdown","4f7a4d3e":"markdown","8ca8cde6":"markdown","27008749":"markdown","d7a17db8":"markdown","e2bb4ad1":"markdown","fe101572":"markdown","1644d047":"markdown","9b4b3c6c":"markdown","f7b99601":"markdown","2c1f3556":"markdown","50fd477f":"markdown","77a33e13":"markdown","68a35604":"markdown","77508bd6":"markdown","732979fb":"markdown","37cfef08":"markdown","0309303b":"markdown","5227ee99":"markdown","5a3e0a9f":"markdown","e3562736":"markdown","d5621486":"markdown","0764fac3":"markdown","0d766941":"markdown","3384302a":"markdown","db146c7a":"markdown","c28d5a8d":"markdown","b8f3c57d":"markdown","ce9346a7":"markdown","ccfae53a":"markdown","1658e32d":"markdown","a610d75e":"markdown","990d6488":"markdown","8fe9f943":"markdown","dc0e4373":"markdown","dd73c376":"markdown","845e2dde":"markdown","bdfcbfcc":"markdown","bf1ed3f0":"markdown","4748732e":"markdown","041099cc":"markdown","7b8d5633":"markdown","cb51b10d":"markdown","5ecc2d6f":"markdown","82f9ecf7":"markdown","063a3e18":"markdown","d3993516":"markdown","28d18315":"markdown","371f8d69":"markdown","0bf33225":"markdown","c67c1ebb":"markdown","094f952d":"markdown","d9099f11":"markdown","8cb85e9f":"markdown","91061cb1":"markdown","d18632af":"markdown","cf867546":"markdown","52605cc4":"markdown","9fff24b1":"markdown","619a8ff0":"markdown","e5f5d158":"markdown","7479f163":"markdown","073f5fe0":"markdown","bbaa5c67":"markdown","cb65c445":"markdown","44c3ccc1":"markdown","48c42a00":"markdown","1a1262ae":"markdown","c3c157d9":"markdown","641f479a":"markdown","1240f951":"markdown","0a2a18fc":"markdown","1c64117a":"markdown","e02acf4f":"markdown","1d321e46":"markdown","5ca6ad48":"markdown","37a8a551":"markdown","a9399f7d":"markdown","e117ed87":"markdown","05666efa":"markdown","fbfcb45c":"markdown","f3086811":"markdown","ac02c36a":"markdown","8250d212":"markdown","b053e046":"markdown","6a18441a":"markdown","3b29690b":"markdown","6f309f4c":"markdown","4724f972":"markdown","40b27b6a":"markdown","2d37d2a5":"markdown"},"source":{"5908062b":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","db778fe2":"path = datasets.untar_data(datasets.URLs.IMDB)","44d77d89":"path.ls() #note it contains a training folder, and unsupervised folder and a testeing folder ","d65b7550":"#o the first thing we do is to create a datablok API list subclass called TextTest \ndef read_file(fn): \n    with open(fn, 'r', encoding = 'utf8') as f: return f.read()\n    \nclass TextList(ItemList):\n    @classmethod\n    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs): #the get_files with a .txt file \n        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n    \n    def get(self, i): #all we have to do is overwrite get function to open a text file we do that in the read_file function above \n        if isinstance(i, Path): return read_file(i)\n        return i","e3573e64":"il = TextList.from_files(path, include=['train', 'test', 'unsup']) #and now we can create an intemlist ","351f3b53":"len(il.items) #see the lenght of it","2c13d522":"\ntxt = il[0]#index into it \ntxt","b35c46a0":"sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1)) #split the data in validation and trainin","afc43df9":"\nsd","8254399d":"\n#export\nimport spacy,html","300fc90a":"#export\n#special tokens\nUNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n\ndef sub_br(t): #fx if we find 'br \/' we replace with a new line ('\\n')\n    \"Replaces the <br \/> by \\n\"\n    re_br = re.compile(r'<\\s*br\\s*\/?>', re.IGNORECASE)\n    return re_br.sub(\"\\n\", t)\n\ndef spec_add_spaces(t): \n    \"Add spaces around \/ and #\"\n    return re.sub(r'([\/#])', r' \\1 ', t)\n\ndef rm_useless_spaces(t):\n    \"Remove multiple spaces\"\n    return re.sub(' {2,}', ' ', t)\n\ndef replace_rep(t):\n    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n    def _replace_rep(m:Collection[str]) -> str:\n        c,cc = m.groups()\n        return f' {TK_REP} {len(cc)+1} {c} '\n    re_rep = re.compile(r'(\\S)(\\1{3,})')\n    return re_rep.sub(_replace_rep, t)\n    \ndef replace_wrep(t):\n    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n    def _replace_wrep(m:Collection[str]) -> str:\n        c,cc = m.groups()\n        return f' {TK_WREP} {len(cc.split())+1} {c} '\n    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n    return re_wrep.sub(_replace_wrep, t)\n\ndef fixup_text(x):\n    \"Various messy things we've seen in documents\"\n    re1 = re.compile(r'  +')\n    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x))\n    \ndefault_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br] #predefined rules these are bit of code that are run before the tokenizasion \ndefault_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ] #default for our special tokens defined in the top of this cell ","863dd8f8":"replace_rep('cccc') #fx a repetitions of 4x'c' will be this:","83d56243":"replace_wrep('word word word word word ')#fx a repetitions of 5x'word' will be this:","6d266efc":"def replace_all_caps(x):\n    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n    res = []\n    for t in x:\n        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n        else: res.append(t)\n    return res\n\ndef deal_caps(x):\n    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n    res = []\n    for t in x:\n        if t == '': continue\n        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n        res.append(t.lower())\n    return res\n\ndef add_eos_bos(x): return [BOS] + x + [EOS] #reset its state with EOS, so it knows we are starting at a new word or something\n\ndefault_post_rules = [deal_caps, replace_all_caps, add_eos_bos]","64d89bc6":"replace_all_caps(['I', 'AM', 'SHOUTING']) #put xxup before a caps word and make it lower chase so they can compare to all the other lowerchase words ","a02c9946":"deal_caps(['My', 'name', 'is', 'Jeremy']) #same thing for mix chase just here we are using xxmaj and then the word in lowerchase ","88174b1f":"from spacy.symbols import ORTH\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef parallel(func, arr, max_workers=4):\n    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n    else:\n        with ProcessPoolExecutor(max_workers=max_workers) as ex: #ProcessPoolExecutor run things in parallel so it runs faster \n            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n    if any([o is not None for o in results]): return results","14a3b54e":"class TokenizeProcessor(Processor):\n    def __init__(self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): \n        self.chunksize,self.max_workers = chunksize,max_workers\n        self.tokenizer = spacy.blank(lang).tokenizer\n        for w in default_spec_tok:\n            self.tokenizer.add_special_case(w, [{ORTH: w}])\n        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n        self.post_rules = default_post_rules if post_rules is None else post_rules\n\n    def proc_chunk(self, args): #processing one chunk \n        i,chunk = args\n        chunk = [compose(t, self.pre_rules) for t in chunk]\n        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n        docs = [compose(t, self.post_rules) for t in docs]\n        return docs\n\n    def __call__(self, items): \n        toks = []\n        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers) #so run all the chunks (pro_chunk) in parallel for all the chunks \n        return sum(toks, [])\n    \n    def proc1(self, item): return self.proc_chunk([item])[0]\n    \n    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n    def deproc1(self, tok):    return \" \".join(tok)","14092d26":"tp = TokenizeProcessor() #so we tokenize the texst","4b9dce06":"txt[:250] #eksampel of 250 words","1cd29ada":"' \u2022 '.join(tp(il[:100])[0])[:400] #and then tokenize them ","0924df48":"import collections\n\nclass NumericalizeProcessor(Processor):\n    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): \n        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n    \n    def __call__(self, items):\n        #The vocab is defined on the first use.\n        if self.vocab is None: #chech if we have a vocab yet and the vocab is a list of all the unique words \n            #if we dont have it we will create it \n            freq = Counter(p for o in items for p in o)\n            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n            for o in reversed(default_spec_tok):\n                if o in self.vocab: self.vocab.remove(o)\n                self.vocab.insert(0, o)\n        if getattr(self, 'otoi', None) is None:\n            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) \n        return [self.proc1(o) for o in items]\n    def proc1(self, item):  return [self.otoi[o] for o in item] #call object to int on each one from the dictornary \n    \n    def deprocess(self, idxs):\n        assert self.vocab is not None\n        return [self.deproc1(idx) for idx in idxs]\n    def deproc1(self, idx): return [self.vocab[i] for i in idx] #just grapping ech one from vocab ","4adf975f":"proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()","e01cb64b":"%time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])","ef979beb":"ll.train.x_obj(0)","aa4ab18b":"pickle.dump(ll, open(path\/'ld.pkl', 'wb')) #dump the labeled list ","b4fda6c0":"ll = pickle.load(open(path\/'ld.pkl', 'rb')) #so we can load it again later ","4d4089de":"# Just using those for illustration purposes, they're not used otherwise.\nfrom IPython.display import display,HTML\nimport pandas as pd","e329f512":"stream = \"\"\"\nIn this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \nFirst we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\nThen we will study how we build a language model and train it.\\n\n\"\"\"\ntokens = np.array(tp([stream])[0])","ae85e9f1":"bs,seq_len = 6,15#create a batch size of 6 so in pracise we get 6 lines of tekst \nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))","b7d51334":"bs,bptt = 6,5 \nfor k in range(3):\n    d_tokens = np.array([tokens[i*seq_len + k*bptt:i*seq_len + (k+1)*bptt] for i in range(bs)])\n    df = pd.DataFrame(d_tokens)\n    display(HTML(df.to_html(index=False,header=None)))","c09e160a":"#note the indepened veiable(X) is a word from above, and the depened(y) veriable is the word +1 so the next word in the sentence it is trying to predict \n#so lets create the depened veriable(y) here. Note the PreLoader is the same as the dataset in this chase\nclass LM_PreLoader():\n    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle\n        total_len = sum([len(t) for t in data.x])\n        self.n_batch = total_len \/\/ bs\n        self.batchify()\n    \n    def __len__(self): return ((self.n_batch-1) \/\/ self.bptt) * self.bs #a dataset is something with a lenght\n    \n    def __getitem__(self, idx): #and a getiem \n        source = self.batched_data[idx % self.bs]\n        seq_idx = (idx \/\/ self.bs) * self.bptt\n        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1] #so when you index into it it will grap the a indepened veriable(X)\n    #and a depened veriable(y) and the indepened veriable is just the seq_idx:seq_idx+self.bptt --> a word in the text.\n    #and the depened verible is just that word +1, so the word after it in a sentence so there is just a offset of 1 \n    \n    def batchify(self):\n        texts = self.data.x\n        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n        stream = torch.cat([tensor(t) for t in texts])\n        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)","aeba0b51":"dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)","9c18d4b8":"iter_dl = iter(dl)\nx1,y1 = next(iter_dl)\nx2,y2 = next(iter_dl)","9d0110ec":"x1.size(),y1.size()","91468b50":"\nvocab = proc_num.vocab","031fc3dd":"\" \".join(vocab[o] for o in x1[0]) #grap a minibatch one at the time  for the indepened veriable","932638dc":"\n\" \".join(vocab[o] for o in y1[0])#grap a minibatch one at the time  for the depened veriable and now we can see it has a offset of one ","c102b59d":"\n\" \".join(vocab[o] for o in x2[0])","a279d8c4":"#now we can refactor it into a function \ndef get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n\ndef lm_databunchify(sd, bs, bptt, **kwargs):\n    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))","bcdbf0d3":"\nbs,bptt = 64,70\ndata = lm_databunchify(ll, bs, bptt)","18d69e0d":"proc_cat = CategoryProcessor()","5371dc2f":"\nil = TextList.from_files(path, include=['train', 'test']) #create a item list\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test')) #split the data \nll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat) #and we label it note we have added to preprocessors --> proc_x = [proc_tok, proc_num]","e041d338":"\npickle.dump(ll, open(path\/'ll_clas.pkl', 'wb'))","08d25b8a":"ll = pickle.load(open(path\/'ll_clas.pkl', 'rb'))","9b5daf3f":"[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]","c4ede7fc":"\n#export\nfrom torch.utils.data import Sampler\n#for validation set\nclass SortSampler(Sampler):\n    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n    def __len__(self): return len(self.data_source)\n    def __iter__(self):\n        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True)) #goes through our data --> (data_source) looks how many dokuments there are in it --> (len(self.data_source)\n    #create the list from 0 to the number of dokuments --> list(range(len(self.data_source))) and sort them in reversed order (sorted & reverse=True) by some key (self.key and it is a lambda function defined longer data that just tkes the lenght of the dokument ) and returns that iterator(iter)","1a55165b":"#we get different bacth sizes so we have to handle that. so the trick is to sort the data first byt lenght so the first minibatch will contain the your realy long\n#dokuments and your last minibatch will contain realy short dokuments \n\n#for training set \n#so it sorts like for the validationset where all minibacth has something at simular lenght but with some randomness#note they do not have identical lenght but just simular  \nclass SortishSampler(Sampler):\n    def __init__(self, data_source, key, bs):\n        self.data_source,self.key,self.bs = data_source,key,bs\n\n    def __len__(self) -> int: return len(self.data_source)\n\n    def __iter__(self):\n        idxs = torch.randperm(len(self.data_source)) #add random permitation in the megabatches \n        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)] #mega batch is 50 times bigger then a minibatch \n        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches]) #sort those megebatches \n        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n        batch_idxs = torch.randperm(len(batches)-2)\n        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n        return iter(sorted_idx)","06745d12":"#since we have different sizes bathces we need to write a new collate function \ndef pad_collate(samples, pad_idx=1, pad_first=False):\n    max_len = max([len(s[0]) for s in samples])\n    res = torch.zeros(len(samples), max_len).long() + pad_idx #create something that can handle the longest minibatch in the dokument\n    for i,s in enumerate(samples): #go through every dokument\n        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0]) #and dump it into the big tensor eihter at the end..\n        else:         res[i, :len(s[0]) ] = LongTensor(s[0])#.. or at the start \n    return res, tensor([s[1] for s in samples])","637f6a4e":"\nbs = 64\ntrain_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\ntrain_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate) #pass the sampler and collate to our dataloader ","84a8b6d7":"\niter_dl = iter(train_dl)\nx,y = next(iter_dl)","8cb3b7a7":"lengths = []\nfor i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\nlengths[:5], lengths[-1]","cc19feec":"x,y = next(iter_dl) #grap a minibatch where we can see padding at the end \nlengths = []\nfor i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\nlengths[:5], lengths[-1]","c3d7217c":"x","1e894db9":"#noe lets refactor it into a convenience funtion that did the above \ndef get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n\ndef clas_databunchify(sd, bs, **kwargs):\n    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))","acde7872":"bs,bptt = 64,70\ndata = clas_databunchify(ll, bs)","d882371b":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","2f1be734":"path = datasets.untar_data(datasets.URLs.IMDB)","42d08afc":"il = TextList.from_files(path, include=['train', 'test', 'unsup'])\nsd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))","c22db9ee":"\nproc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()","2c00e780":"\nll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])","aea89322":"\npickle.dump(ll, open(path\/'ll_lm.pkl', 'wb'))\npickle.dump(proc_num.vocab, open(path\/'vocab_lm.pkl', 'wb'))","fb9e4e80":"ll = pickle.load(open(path\/'ll_lm.pkl', 'rb'))\nvocab = pickle.load(open(path\/'vocab_lm.pkl', 'rb'))","040f713c":"bs,bptt = 64,70\ndata = lm_databunchify(ll, bs, bptt)","743912e2":"#model is able to forget and only remember importen things \nclass LSTMCell(nn.Module):\n    def __init__(self, ni, nh):\n        super().__init__()\n        self.ih = nn.Linear(ni,4*nh) #from input to hidden layer(ih) and nh is the numbers of hiddan layers normaly \n        self.hh = nn.Linear(nh,4*nh)#from hidden to hidden layer(hh)\n\n    def forward(self, input, state):\n        h,c = state\n        #One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split it up in four chunks or in other words splits it up 4 groups of same size \n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) #3 of them goes through a sigmoid \n        cellgate = gates[3].tanh() #one of them goes throug a tanh\n\n        c = (forgetgate*c) + (ingate*cellgate) #multiply and add from picture above\n        h = outgate * c.tanh() #multiply from picture above \n        return h, (h,c)","b2e6fb66":"class LSTMLayer(nn.Module):\n    def __init__(self, cell, *cell_args):\n        super().__init__()\n        self.cell = cell(*cell_args)\n\n    def forward(self, input, state):\n        inputs = input.unbind(1)\n        outputs = []\n        for i in range(len(inputs)): #for loop from before \n            out, state = self.cell(inputs[i], state) #call on whatever cell we call for and in this chase it is a LSTMCell #take the state and update the state \n            outputs += [out]\n        return torch.stack(outputs, dim=1), state","3b319b90":"lstm = LSTMLayer(LSTMCell, 300, 300) ","6b008084":"\nx = torch.randn(64, 70, 300)\nh = (torch.zeros(64, 300),torch.zeros(64, 300))","c83906a0":"%timeit -n 10 y,h1 = lstm(x,h)","3619ff56":"lstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","b1cf8968":"def time_fn(f):\n    f()\n    torch.cuda.synchronize()","f919da7c":"\nf = partial(lstm,x,h)\ntime_fn(f)","435d9269":"%timeit -n 10 time_fn(f)","8e50e883":"lstm = nn.LSTM(300, 300, 1, batch_first=True) #better faster and great just use this LSTM instead of the self build ","6af83b2b":"x = torch.randn(64, 70, 300)\nh = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300))","3052db66":"%timeit -n 10 y,h1 = lstm(x,h)","12ada21c":"lstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","3a58a18f":"f = partial(lstm,x,h)\ntime_fn(f)","1742f48e":"%timeit -n 10 time_fn(f)","2155b888":"import torch.jit as jit\nfrom torch import Tensor","0a3dd618":"#takes the python code and compile it into C++ code so we can creae an on GPU loop (loop from code above)#note jit is hirt and doesnt realy work right now\nclass LSTMCell(jit.ScriptModule):\n    def __init__(self, ni, nh):\n        super().__init__()\n        self.ni = ni\n        self.nh = nh\n        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n\n    @jit.script_method\n    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        hx, cx = state\n        gates = (input @ self.w_ih.t() + self.bias_ih +\n                 hx @ self.w_hh.t() + self.bias_hh)\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * torch.tanh(cy)\n\n        return hy, (hy, cy)","b401e5af":"class LSTMLayer(jit.ScriptModule):\n    def __init__(self, cell, *cell_args):\n        super().__init__()\n        self.cell = cell(*cell_args)\n\n    @jit.script_method\n    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        inputs = input.unbind(1)\n        outputs = []\n        for i in range(len(inputs)):\n            out, state = self.cell(inputs[i], state)\n            outputs += [out]\n        return torch.stack(outputs, dim=1), state","cc0bffb3":"lstm = LSTMLayer(LSTMCell, 300, 300)","1b0a20be":"\nx = torch.randn(64, 70, 300)\nh = (torch.zeros(64, 300),torch.zeros(64, 300))","dd7b010d":"%timeit -n 10 y,h1 = lstm(x,h)","26727119":"\nlstm = lstm.cuda()\nx = x.cuda()\nh = (h[0].cuda(), h[1].cuda())","82445adc":"f = partial(lstm,x,h)\ntime_fn(f)","839b3c7d":"%timeit -n 10 time_fn(f)","4ac32752":"\n#export\ndef dropout_mask(x, sz, p):\n    return x.new(*sz).bernoulli_(1-p).div_(1-p) #bernoulli_ means creae 1s and 0s (1-p) and divide by 1-p ","fc4bf8c5":"x = torch.randn(10,10)\nmask = dropout_mask(x, (10,10), 0.5); mask","8f8b08d6":"(x*mask).std(),x.std()","ca6bbd8a":"\n#export\nclass RNNDropout(nn.Module):\n    def __init__(self, p=0.5):\n        super().__init__()\n        self.p=p\n\n    def forward(self, x):\n        if not self.training or self.p == 0.: return x\n        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n        return x * m","10d3285b":"dp = RNNDropout(0.3)\ntst_input = torch.randn(3,3,7)\ntst_input, dp(tst_input)","22ff1dfd":"\n#export\nimport warnings\n\nWEIGHT_HH = 'weight_hh_l0'\n\nclass WeightDropout(nn.Module): #same as dropconnect does the same it does not only dropouts on the weights but also the activations \n    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n        super().__init__()\n        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n        for layer in self.layer_names:\n            #Makes a copy of the weights of the selected layers.\n            w = getattr(self.module, layer)\n            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n\n    def _setweights(self):\n        for layer in self.layer_names:\n            raw_w = getattr(self, f'{layer}_raw')\n            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n\n    def forward(self, *args):\n        self._setweights()\n        with warnings.catch_warnings():\n            #To avoid the warning that comes because the weights aren't flattened.\n            warnings.simplefilter(\"ignore\")\n            return self.module.forward(*args)","edb7bb88":"module = nn.LSTM(5, 2)\ndp_module = WeightDropout(module, 0.4)\ngetattr(dp_module.module, WEIGHT_HH)","53172c57":"tst_input = torch.randn(4,20,5)\nh = (torch.zeros(1,20,2), torch.zeros(1,20,2))\nx,h = dp_module(tst_input,h)\ngetattr(dp_module.module, WEIGHT_HH)","8c3f2350":"#drop out an whole row so it is dropping out whole words in this chase \nclass EmbeddingDropout(nn.Module):\n    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n    def __init__(self, emb, embed_p):\n        super().__init__()\n        self.emb,self.embed_p = emb,embed_p\n        self.pad_idx = self.emb.padding_idx\n        if self.pad_idx is None: self.pad_idx = -1\n\n    def forward(self, words, scale=None):\n        if self.training and self.embed_p != 0:\n            size = (self.emb.weight.size(0),1)\n            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n            masked_embed = self.emb.weight * mask\n        else: masked_embed = self.emb.weight\n        if scale: masked_embed.mul_(scale)\n        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)","8772bb13":"enc = nn.Embedding(100, 7, padding_idx=1)\nenc_dp = EmbeddingDropout(enc, 0.5)\ntst_input = torch.randint(0,100,(8,))\nenc_dp(tst_input)","0984a946":"\n#export\ndef to_detach(h):\n    \"Detaches `h` from its history.\"\n    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)","90365d77":"#export\nclass AWD_LSTM(nn.Module):\n    \"AWD-LSTM inspired by https:\/\/arxiv.org\/abs\/1708.02182.\"\n    initrange=0.1\n\n    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n        super().__init__()\n        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1, #create a LSTM model for some layers \n                             batch_first=True) for l in range(n_layers)]\n        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns]) \n        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n        self.input_dp = RNNDropout(input_p)\n        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n\n    def forward(self, input):\n        bs,sl = input.size()\n        if bs!=self.bs:\n            self.bs=bs\n            self.reset()\n        raw_output = self.input_dp(self.emb_dp(input))\n        new_hidden,raw_outputs,outputs = [],[],[]\n        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)): #call all the different kinds of dropout in this loop \n            raw_output, new_h = rnn(raw_output, self.hidden[l])\n            new_hidden.append(new_h)\n            raw_outputs.append(raw_output)\n            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n            outputs.append(raw_output) \n        self.hidden = to_detach(new_hidden)\n        return raw_outputs, outputs\n\n    def _one_hidden(self, l):\n        \"Return one hidden state.\"\n        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n        return next(self.parameters()).new(1, self.bs, nh).zero_()\n\n    def reset(self):\n        \"Reset the hidden states.\"\n        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]","6fcb1a56":"#untop of the LSTM model we can put a linear model untop \nclass LinearDecoder(nn.Module):\n    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n        super().__init__()\n        self.output_dp = RNNDropout(output_p) #with dropout\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias) #and layers \n        if bias: self.decoder.bias.data.zero_()\n        if tie_encoder: self.decoder.weight = tie_encoder.weight\n        else: init.kaiming_uniform_(self.decoder.weight)\n\n    def forward(self, input):\n        raw_outputs, outputs = input\n        output = self.output_dp(outputs[-1]).contiguous()\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded, raw_outputs, outputs","b3803e1e":"class SequentialRNN(nn.Sequential):\n    \"A sequential module that passes the reset call to its children.\"\n    def reset(self):\n        for c in self.children():\n            if hasattr(c, 'reset'): c.reset()","2b9f647d":"#export\ndef get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n    enc = rnn_enc.emb if tie_weights else None\n    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias)) #pass the whole seuential model to the LinearDecode which is the one that are gonna predict the next word in a sentence ","2df3b56a":"tok_pad = vocab.index(PAD)","b99b6aba":"tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)\ntst_model = tst_model.cuda()","4ca926cd":"x,y = next(iter(data.train_dl))","f6802f6b":"z = tst_model(x.cuda())","403734aa":"len(z)","4e91cb4c":"\ndecoded, raw_outputs, outputs = z","1d9f9ff2":"decoded.size()","d5c0e59a":"len(raw_outputs),len(outputs)","08836486":"[o.size() for o in raw_outputs], [o.size() for o in outputs]","c2570b9e":"\n#export\nclass GradientClipping(Callback):\n    def __init__(self, clip=None): self.clip = clip\n    def after_backward(self):\n        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip) #check the gradient after a after backword pss has benn called and if the total norm of the gradient is bigger then\n            #by some number (clip) we will devide them all by some number so it is clipping those gradients \n            #so its let you trtain with a higher learning rate and avoid the gradient of overfitting ","e704daec":"\n#export\nclass RNNTrainer(Callback):\n    def __init__(self, \u03b1, \u03b2): self.\u03b1,self.\u03b2 = \u03b1,\u03b2\n    \n    def after_pred(self):\n        #Save the extra outputs for later and only returns the true output.\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.run.pred = self.pred[0]\n    \n    def after_loss(self):\n        #AR and TAR\n        if self.\u03b1 != 0.:  self.run.loss += self.\u03b1 * self.out[-1].float().pow(2).mean() # Activation Regularization #the L2'staf' is not on the weigths but on the activations this time this is gonna make sure our activations is never to high \n        if self.\u03b2 != 0.:\n            h = self.raw_out[-1]\n            if h.size(1)>1: self.run.loss += self.\u03b2 * (h[:,1:] - h[:,:-1]).float().pow(2).mean() #Temporal Activation Regularization # which check how much does each activation change by from seqence step to seqence step and take the squere og that \n                #does this becaue its not a good idea for having things taht massive change from time step to time step \n    def begin_epoch(self):\n        #Shuffle the texts at the beginning of the epoch\n        if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()","28fea6cf":"\n#export\ndef cross_entropy_flat(input, target):\n    bs,sl = target.size()\n    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n\ndef accuracy_flat(input, target):\n    bs,sl = target.size()\n    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))","03ecbb97":"emb_sz, nh, nl = 300, 300, 2\nmodel = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, \n                           embed_p=0.1, hidden_p=0.2)","2499c8cb":"cbs = [partial(AvgStatsCallback,accuracy_flat),\n       CudaCallback, Recorder,\n       partial(GradientClipping, clip=0.1),\n       partial(RNNTrainer, \u03b1=2., \u03b2=1.),\n       ProgressCallback]","eb432b62":"learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())","84bdacd6":"learn.fit(1) ","c04d8f5e":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","e3c4e8c5":"\n#path = datasets.Config().data_path()\n#version = '103' #2","041438a6":"\n#! wget https:\/\/s3.amazonaws.com\/research.metamind.io\/wikitext\/wikitext-{version}-v1.zip -P {path}\n#! unzip -q -n {path}\/wikitext-{version}-v1.zip  -d {path}\n#! mv {path}\/wikitext-{version}\/wiki.train.tokens {path}\/wikitext-{version}\/train.txt\n#! mv {path}\/wikitext-{version}\/wiki.valid.tokens {path}\/wikitext-{version}\/valid.txt\n#! mv {path}\/wikitext-{version}\/wiki.test.tokens {path}\/wikitext-{version}\/test.txt","b79b9d22":"path = datasets.Config().data_path()\/'wikitext-103'","4f6964f9":"\ndef istitle(line):\n    return len(re.findall(r'^ = [^=]* = $', line)) != 0","0a716753":"def read_wiki(filename):\n    articles = []\n    with open(filename, encoding='utf8') as f:\n        lines = f.readlines()\n    current_article = ''\n    for i,line in enumerate(lines):\n        current_article += line\n        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n            current_article = current_article.replace('<unk>', UNK)\n            articles.append(current_article)\n            current_article = ''\n    current_article = current_article.replace('<unk>', UNK)\n    articles.append(current_article)\n    return articles","d16242ba":"\ntrain = TextList(read_wiki(path\/'train.txt'), path=path) #+read_file(path\/'test.txt')\nvalid = TextList(read_wiki(path\/'valid.txt'), path=path)","5f0fbbf9":"len(train), len(valid)","a024af7f":"\nsd = SplitData(train, valid)","dfc52ee0":"\nproc_tok,proc_num = TokenizeProcessor(),NumericalizeProcessor()","9f7484ec":"ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])","deff0242":"pickle.dump(ll, open(path\/'ld.pkl', 'wb'))","8a793ecb":"\nll = pickle.load( open(path\/'ld.pkl', 'rb'))","d5a237d2":"bs,bptt = 128,70\ndata = lm_databunchify(ll, bs, bptt)","a4f5c8ee":"vocab = ll.train.proc_x[-1].vocab\nlen(vocab)","31d93e93":"dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.2\ntok_pad = vocab.index(PAD)","7d9c0872":"\nemb_sz, nh, nl = 300, 300, 2\nmodel = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)","cd31f8af":"cbs = [partial(AvgStatsCallback,accuracy_flat),\n       CudaCallback, Recorder,\n       partial(GradientClipping, clip=0.1),\n       partial(RNNTrainer, \u03b1=2., \u03b2=1.),\n       ProgressCallback]","a217a03a":"learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())","761db5cd":"\nlr = 5e-3\nsched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr\/10., lr, lr\/1e5))\nsched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))\ncbsched = [ParamScheduler('lr', sched_lr), ParamScheduler('mom', sched_mom)]","cb286524":"learn.fit(10, cbs=cbsched)","57babdf4":"torch.save(learn.model.state_dict(), path\/'pretrained.pth')\npickle.dump(vocab, open(path\/'vocab.pkl', 'wb'))","39731301":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","9421943c":"path = datasets.untar_data(datasets.URLs.IMDB)","426cb594":"\nll = pickle.load(open(path\/'ll_lm.pkl', 'rb'))","2c235b05":"bs,bptt = 128,70\ndata = lm_databunchify(ll, bs, bptt)","30f595fb":"vocab = ll.train.proc_x[1].vocab","e32c8647":"# ! wget http:\/\/files.fast.ai\/models\/wt103_tiny.tgz -P {path}\n# ! tar xf {path}\/wt103_tiny.tgz -C {path}","02f8b602":"\ndps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\ntok_pad = vocab.index(PAD)","b6846f95":"emb_sz, nh, nl = 300, 300, 2\nmodel = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)","6e7fc722":"\nold_wgts  = torch.load(path\/'pretrained'\/'pretrained.pth')\nold_vocab = pickle.load(open(path\/'pretrained'\/'vocab.pkl', 'rb'))","329e555e":"\nidx_house_new, idx_house_old = vocab.index('house'),old_vocab.index('house')","524c4f1a":"house_wgt  = old_wgts['0.emb.weight'][idx_house_old]\nhouse_bias = old_wgts['1.decoder.bias'][idx_house_old]","93fcfd86":"#so now we are gonna have different vocab since we are working on a new text that are not from the wiki dataset \ndef match_embeds(old_wgts, old_vocab, new_vocab):\n    wgts = old_wgts['0.emb.weight']\n    bias = old_wgts['1.decoder.bias']\n    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()\n    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))\n    new_bias = bias.new_zeros(len(new_vocab))\n    otoi = {v:k for k,v in enumerate(old_vocab)}\n    for i,w in enumerate(new_vocab): \n        if w in otoi:# so go through all the embedding vocabs and if it is there we are just gonna copy the embedding over and igf it is not here we are gonna take..\n            idx = otoi[w]\n            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]\n        else: new_wgts[i],new_bias[i] = wgts_m,bias_m #... the weigth and biases and create one \n    old_wgts['0.emb.weight']    = new_wgts\n    old_wgts['0.emb_dp.emb.weight'] = new_wgts\n    old_wgts['1.decoder.weight']    = new_wgts\n    old_wgts['1.decoder.bias']      = new_bias\n    return old_wgts","2d1bccb1":"wgts = match_embeds(old_wgts, old_vocab, vocab)","24a4100b":"test_near(wgts['0.emb.weight'][idx_house_new],house_wgt)\ntest_near(wgts['1.decoder.bias'][idx_house_new],house_bias)","9acdb464":"model.load_state_dict(wgts)","0cd94f9a":"model","c6a6b383":"#normal slitter for the data \ndef lm_splitter(m):\n    groups = []\n    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]\n    return [list(o.parameters()) for o in groups]","5fbaf031":"for rnn in model[0].rnns:\n    for p in rnn.parameters(): p.requires_grad_(False)","1130b0ed":"#setup the calllback \ncbs = [partial(AvgStatsCallback,accuracy_flat),\n       CudaCallback, Recorder,\n       partial(GradientClipping, clip=0.1),\n       partial(RNNTrainer, \u03b1=2., \u03b2=1.),\n       ProgressCallback]","c8803717":"learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(), #setup our leaner \n                cb_funcs=cbs, splitter=lm_splitter)","7da6c8e4":"lr = 2e-2\ncbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)","56262db0":"learn.fit(1, cbs=cbsched)","79b2bbaf":"\nfor rnn in model[0].rnns:\n    for p in rnn.parameters(): p.requires_grad_(True)","cb16f50d":"lr = 2e-3\ncbsched = sched_1cycle([lr\/2., lr\/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)","0b2cce86":"learn.fit(10, cbs=cbsched)","f6550a9c":"torch.save(learn.model[0].state_dict(), path\/'finetuned_enc.pth')","dae79531":"\npickle.dump(vocab, open(path\/'vocab_lm.pkl', 'wb'))","872fb93f":"\ntorch.save(learn.model.state_dict(), path\/'finetuned.pth')","f814b99f":"vocab = pickle.load(open(path\/'vocab_lm.pkl', 'rb'))\nproc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor()","29acda66":"il = TextList.from_files(path, include=['train', 'test']) #load up our classifer data bunch \nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\nll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)","eb38ba2f":"pickle.dump(ll, open(path\/'ll_clas.pkl', 'wb'))","6dda002e":"\nll = pickle.load(open(path\/'ll_clas.pkl', 'rb'))\nvocab = pickle.load(open(path\/'vocab_lm.pkl', 'rb'))","88aa906d":"bs,bptt = 64,70\ndata = clas_databunchify(ll, bs)","b66e630d":"#export\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence","4808d15b":"\nx,y = next(iter(data.train_dl))","54ba4c15":"\nx.size()","1fa85814":"lengths = x.size(1) - (x == 1).sum(1)\nlengths[:5]","56d7058c":"tst_emb = nn.Embedding(len(vocab), 300)","28ddffd0":"tst_emb(x).shape","d1832a86":"128*70","1dd1145c":"packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True)","d018439c":"packed","36187eec":"packed.data.shape","b1ef98fb":"len(packed.batch_sizes)","4d2db544":"8960\/\/70","5c38386b":"tst = nn.LSTM(300, 300, 2)","c2b46998":"y,h = tst(packed)","c80dba43":"\nunpack = pad_packed_sequence(y, batch_first=True)","23b0996b":"\nunpack[0].shape","a3f00118":"unpack[1]","fec6383b":"#export\nclass AWD_LSTM1(nn.Module):\n    \"AWD-LSTM inspired by https:\/\/arxiv.org\/abs\/1708.02182.\"\n    initrange=0.1\n\n    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n        super().__init__()\n        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token\n        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n                             batch_first=True) for l in range(n_layers)]\n        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n        self.input_dp = RNNDropout(input_p)\n        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n\n    def forward(self, input):\n        bs,sl = input.size()\n        mask = (input == self.pad_token)\n        lengths = sl - mask.long().sum(1)\n        n_empty = (lengths == 0).sum()\n        if n_empty > 0:\n            input = input[:-n_empty]\n            lengths = lengths[:-n_empty]\n            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]\n        raw_output = self.input_dp(self.emb_dp(input))\n        new_hidden,raw_outputs,outputs = [],[],[]\n        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)#take data of different lenght and call on pack_padded_sequence...\n            raw_output, new_h = rnn(raw_output, self.hidden[l]) #... and pass that to a rnn ...\n            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0] #... and call on pad_packed_sequence and it basicaly takes thing of different lenght and opponialy handles them in a rnn \n            raw_outputs.append(raw_output)\n            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n            outputs.append(raw_output)\n            new_hidden.append(new_h)\n        self.hidden = to_detach(new_hidden)\n        return raw_outputs, outputs, mask\n\n    def _one_hidden(self, l):\n        \"Return one hidden state.\"\n        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n        return next(self.parameters()).new(1, self.bs, nh).zero_()\n\n    def reset(self):\n        \"Reset the hidden states.\"\n        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]","8f77c273":"#finds which bit of state we wonna use for cassifcation \nclass Pooling(nn.Module):\n    def forward(self, input):\n        raw_outputs,outputs,mask = input\n        output = outputs[-1]\n        lengths = output.size(1) - mask.long().sum(dim=1)\n        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) #do an averrge pool \n        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0] #and a max pool \n        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #and use the final state output[torch.arange(0, output.size(0)),lengths-1]\n        # then we Concat pooling tehm all togehter \n        return output,x","bc92cbe8":"emb_sz, nh, nl = 300, 300, 2\ntok_pad = vocab.index(PAD)","5d826fa3":"enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad)\npool = Pooling()\nenc.bs = bs\nenc.reset()","5186ce50":"x,y = next(iter(data.train_dl))\noutput,c = pool(enc(x))","95947eb0":"x","5b31b53d":"\ntest_near((output.sum(dim=2) == 0).float(), (x==tok_pad).float())","7d91f787":"for i in range(bs):\n    length = x.size(1) - (x[i]==1).long().sum()\n    out_unpad = output[i,:length]\n    test_near(out_unpad[-1], c[i,:300])\n    test_near(out_unpad.max(0)[0], c[i,300:600])\n    test_near(out_unpad.mean(0), c[i,600:])","e3baff67":"def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n    layers = [nn.BatchNorm1d(n_in)] if bn else []\n    if p != 0: layers.append(nn.Dropout(p))\n    layers.append(nn.Linear(n_in, n_out))\n    if actn is not None: layers.append(actn)\n    return layers","5042f9bc":"class PoolingLinearClassifier(nn.Module):\n    \"Create a linear classifier with pooling.\"\n\n    def __init__(self, layers, drops):\n        super().__init__()\n        mod_layers = []\n        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn) #just a list batch norm dropout linear layers \n        self.layers = nn.Sequential(*mod_layers)\n\n    def forward(self, input):\n        raw_outputs,outputs,mask = input\n        output = outputs[-1]\n        lengths = output.size(1) - mask.long().sum(dim=1)\n        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n        x = self.layers(x)\n        return x","8e8bafc3":"def pad_tensor(t, bs, val=0.):\n    if t.size(0) < bs:\n        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])\n    return t","d70e6b42":"class SentenceEncoder(nn.Module):\n    def __init__(self, module, bptt, pad_idx=1):\n        super().__init__()\n        self.bptt,self.module,self.pad_idx = bptt,module,pad_idx\n\n    def concat(self, arrs, bs):\n        return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]\n    \n    def forward(self, input):\n        bs,sl = input.size()\n        self.module.bs = bs\n        self.module.reset()\n        raw_outputs,outputs,masks = [],[],[]\n        for i in range(0, sl, self.bptt): #go through our sentence one bptt at the time \n            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)]) #keep calling that thing \n            masks.append(pad_tensor(m, bs, 1)) #and appending the result \n            raw_outputs.append(r)\n            outputs.append(o)\n        return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)","ddd6d7bd":"def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, \n                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):\n    \"To create a full AWD-LSTM\"\n    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n    enc = SentenceEncoder(rnn_enc, bptt)\n    if layers is None: layers = [50]\n    if drops is None:  drops = [0.1] * len(layers)\n    layers = [3 * emb_sz] + layers + [n_out] \n    drops = [output_p] + drops\n    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))","9597bc30":"\nemb_sz, nh, nl = 300, 300, 2\ndps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\nmodel = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps)","dbc80330":"\ndef class_splitter(m):\n    enc = m[0].module\n    groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)]\n    for i in range(len(enc.rnns)): groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i]))\n    groups.append(m[1])\n    return [list(o.parameters()) for o in groups]","6ee00673":"\nfor p in model[0].parameters(): p.requires_grad_(False)","93653773":"\ncbs = [partial(AvgStatsCallback,accuracy),\n       CudaCallback, Recorder,\n       partial(GradientClipping, clip=0.1),\n       ProgressCallback]","598bb40c":"model[0].module.load_state_dict(torch.load(path\/'finetuned_enc.pth')) #load our finturned encoder ","417c23ca":"learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter)","3971efde":"\nlr = 1e-2\ncbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)","ac33ef66":"learn.fit(1, cbs=cbsched)","af03e8d2":"\nfor p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True)","537cb5ed":"\nlr = 5e-3\ncbsched = sched_1cycle([lr\/2., lr\/2., lr\/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)","6e851680":"learn.fit(1, cbs=cbsched)","38428b38":"for p in model[0].parameters(): p.requires_grad_(True)","af2c6a60":"lr = 1e-3\ncbsched = sched_1cycle([lr\/8., lr\/4., lr\/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)","e770687b":"learn.fit(2, cbs=cbsched)","6d6d9ddc":"\nx,y = next(iter(data.valid_dl))","505b47a4":"pred_batch = learn.model.eval()(x.cuda())","60023134":"pred_ind = []\nfor inp in x:\n    length = x.size(1) - (inp == 1).long().sum()\n    inp = inp[:length]\n    pred_ind.append(learn.model.eval()(inp[None].cuda()))","bd90f477":"assert near(pred_batch, torch.cat(pred_ind))","ee6835d2":"\n## Finetuning the LM\nBefore tackling the classification task, we have to finetune our language model to the IMDB corpus.\n\nWe have pretrained a small model on wikitext 103 that you can download by uncommenting the following cell.","f8235e60":"\nAnd now we stack all of this together!","45c3d2d7":"# Part 3 12b_lm_pretrain","1ba9bf41":"# Preprocess text","37f82f06":"\nLastly we write a flattened version of the cross entropy loss and the accuracy metric.","878a4000":"The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames.","2f031f0a":"\nThis object can be passed to any RNN directly while retaining the speed of CuDNN.","1f60498a":"We somehow need to match our pretrained weights to the new vocabulary. This is done on the embeddings and the decoder (since the weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order.\n\nIt may also happen that we have words that aren't in the pretrained vocab, in this case, we put the mean of the pretrained embedding weights\/decoder bias.","6aa002e3":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","b82a109b":"Split the articles: WT103 is given as one big text file and we need to chunk it in different articles if we want to be able to shuffle them at the beginning of each epoch.","1cfafdb7":"GPU","1cb6625f":"Since tokenizing and applying those rules takes a bit of time, we'll parallelize it using ProcessPoolExecutor to go faster.","8feb0e0e":"\n# Batching for classification\nWhen we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use Sampler to sort (with a bit of randomness for the training set) our samples by length.\n\nFirst the data block API calls shold look familiar.","71fcc744":"\nThen an LSTM layer just applies the cell on all the time steps in order.","4f7a4d3e":"We can load the pretrained weights in our model before beginning training.","8ca8cde6":"\nOnce the items have been processed they will become list of numbers, we can still access the underlying raw data in x_obj (or y_obj for the targets, but we don't have any here).","27008749":"\nPadding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use PyTorch convenience functions that will let us ignore that padding (see 12c).","d7a17db8":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","e2bb4ad1":"![image.png](attachment:image.png)","fe101572":"## Data\n\nWe will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones.","1644d047":"And we add a convenience function:","9b4b3c6c":"\nWe should expect a total of 100,000 texts.","f7b99601":"\n## Concat pooling\nWe will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last element\/average\/maximum.","2c1f3556":"# Part 4 12c_ulmfit ","50fd477f":"\nThe decoded tensor is flattened to bs * seq_len by len(vocab):","77a33e13":"## Data\nWe load the data from 12a, instructions to create that file are there if you don't have it yet so go ahead and see.","68a35604":"\n## AWD-LSTM\nBefore explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on this website.\n\n\n\n## LSTM from scratch\nWe need to implement those equations (where $\\sigma$ stands for sigmoid):","77508bd6":"Then we can unpad it with the following function for other modules:","732979fb":"We only need to save the encoder (first part of the model) for the classification, as well as the vocabulary used (we will need to use the same in the classification task).","37cfef08":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","0309303b":"\nWe can check we have padding with 1s at the end of each text (except the first which is the longest).","5227ee99":"\n## Main model\nThe main model is a regular LSTM with several layers, but using all those kinds of dropouts.","5a3e0a9f":"Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size. Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension.","e3562736":"Let's try it!","d5621486":"\n## Builtin version\nLet's compare with PyTorch!","0764fac3":"\nLet's check the labels seem consistent with the texts.","0d766941":"# Part 2 12a_awd_lstm","3384302a":"\nCPU","db146c7a":"\n# Tokenizing\nWe need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the spacy library.","c28d5a8d":"\nPredicting on the padded batch or on the individual unpadded samples give the same results.","b8f3c57d":"Let's see how this works: first we grab a batch of the training set.","ce9346a7":"## Model","ccfae53a":"## Data\nOne time download","1658e32d":"\nSo the last hidden state isn't the last element of output. Let's check we got everything right.","a610d75e":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","990d6488":"\nBefore even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens.","8fe9f943":"So our version is running at almost the same speed on the CPU. However, on the GPU, PyTorch uses CuDNN behind the scenes that optimizes greatly the for loop.\n\n## Jit version","dc0e4373":"\nLet's check it all works ok: x1, y1, x2 and y2 should all be of size bs by bptt. The texts in each row of x1 should continue in x2. y1 and y2 should have the same texts as their x counterpart, shifted of one position to the right.","dd73c376":"\nThese rules are applies after the tokenization on the list of tokens.","845e2dde":"If we want to take advantage of our GPU, it's better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use nn.Linear to do it.\n\nWe need two linear layers: one for the input and one for the hidden state.","bdfcbfcc":"\nThen if we split it in 6 batches it would give something like this:","bf1ed3f0":"\n## Training\nWe load our pretrained encoder and freeze it.","4748732e":"\nSince the preprocessing tajes time, we save the intermediate result using pickle. Don't use any lambda functions in your processors or they won't be able to pickle.","041099cc":"\nraw_outputs and outputs each contain the results of the intermediary layers:","7b8d5633":"lets test it","cb51b10d":"\nOur pooling layer properly ignored the padding, so now let's group it with a classifier.","5ecc2d6f":"WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass.","82f9ecf7":"\n# Numericalizing\nOnce we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the CategoryProcessor).","063a3e18":"\nLet's say our stream is:","d3993516":"We have to preprocess the data again to pickle it because if we try to load the previous SplitLabeledData with pickle, it will complain some of the functions aren't in main.","28d18315":"\nThen if we have a bptt of 5, we would go over those three batches.","371f8d69":"We can see the padding at the end:","0bf33225":"remember that sigmoid is between 0 and 1 and and tanh is -1 to 1 ","c67c1ebb":"\n## Ignore padding\nWe will those two utility functions from PyTorch to ignore the padding in the inputs.","094f952d":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","d9099f11":"## Classifier\nWe have to process the data again otherwise pickle will complain. We also have to use the same vocab as the language model.","8cb85e9f":"\nFirst we train with the RNNs freezed.","91061cb1":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","d18632af":"CPU","cf867546":"\nThen we just have to feed our texts to those two blocks, (but we can't give them all at once to the AWD_LSTM or we might get OOM error: we'll go for chunks of bptt length to regularly detach the history of our hidden states.)","52605cc4":"# Batching\nWe have a bit of work to convert our LabelList in a DataBunch as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called LM_PreLoader. At the beginning of each epoch, it'll shuffle the articles (if shuffle=True) and create a big stream by concatenating all of them. We divide this big stream in bs smaller streams. That we will read in chunks of bptt length.","9fff24b1":"\nIt's at the beginning of a forward pass that the dropout is applied to the weights.","619a8ff0":"For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside.","e5f5d158":"Just in case there are some text log files, we restrict the ones we take to the training, test, and unsupervised folders.","7479f163":"## Data","073f5fe0":"If we want to apply discriminative learning rates, we need to split our model in different layer groups. Let's have a look at our model.","bbaa5c67":"\nWe need to change our model a little bit to use this.","cb65c445":"\nWith jit, we almost get to the CuDNN speed!\n\n## Dropout\nWe want to use the AWD-LSTM from Stephen Merity et al.. First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor 1\/(1-p) (think of what happens to the activations if you want to figure out why!)\n\nWe usually apply dropout by drawing a mask that tells us which elements to nullify or not:","44c3ccc1":"\nNow let's try it out and see how fast we are. We only measure the forward pass.","48c42a00":"\nThen we add an RNNTrainer that will do four things:\n\n* change the output to make it contain only the decoded tensor (for the loss function) and store the raw_outputs and outputs\n* apply Activation Regularization (AR): we add to the loss an L2 penalty on the last activations of the AWD LSTM (with dropout applied)\n* apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on the difference between two consecutive (in terms of words) raw outputs\n* trigger the shuffle of the LMDataset at the beginning of each epoch","1a1262ae":"CUDA","c3c157d9":"PyTorch puts 0s everywhere we had padding in the output when unpacking.","641f479a":"so this model toke 5 hours to train and below you can see how to download it so you dont have to train it also","1240f951":"## AWD-LSTM","0a2a18fc":"\n# Callbacks to train the model\nWe need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a higher learning rate by putting a maximum value on the norm of the gradients.","1c64117a":"Now let's check that the word \"house\" was properly converted.","e02acf4f":"\nThen the whole model with discriminative learning rates.","1d321e46":"\nWe create a PackedSequence object that contains all of our unpadded sequences","5ca6ad48":"\nWe have to write everything from scratch to be a bit faster, so we don't use the linear layers here.","37a8a551":"Once with have a dropout mask mask, applying the dropout to x is simply done by x = x * mask. We create our own dropout mask and don't rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension.","a9399f7d":"\nFor the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size 50 * bs. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n\nThen we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size.","e117ed87":"\nEmbeddingDropout applies dropout to full rows of the embedding matrix.","05666efa":"## ULMFit","fbfcb45c":"We need to pass to the utility functions the lengths of our sentences because it's applied after the embedding, so we can't see the padding anymore.","f3086811":"\nAnd let's prepare some convenience function to do this quickly.","ac02c36a":"\nThen we split by doing two groups for each rnn\/corresponding dropout, then one last group that contains the embeddings\/decoder. This is the one that needs to be trained the most as we may have new embeddings vectors.","8250d212":"Here is the first one as an example.","b053e046":"On top of this, we will apply a linear decoder. It's often best to use the same matrix as the one for the embeddings in the weights of the decoder.","6a18441a":"\nNow we can test this all works without throwing a bug.","3b29690b":"\nWhen we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones.","6f309f4c":"We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first).","4724f972":"\nIn our current vocabulary, it is very unlikely that the ids correspond to what is in the vocabulary used to train the pretrain model. The tokens are sorted by frequency (apart from the special tokens that are all first) so that order is specific to the corpus used. For instance, the word 'house' has different ids in the our current vocab and the pretrained one.","40b27b6a":"\nWe define a subclass of ItemList that will read the texts in the corresponding filenames.","2d37d2a5":"We return three things to help with regularization: the true output (probabilities for each word), but also the activations of the encoder, with or without dropouts."}}