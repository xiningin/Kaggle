{"cell_type":{"985aa524":"code","a8e3d999":"code","ebd731f3":"code","b8d7a201":"code","d02e2355":"code","8bd1b295":"code","8118d085":"code","452c663f":"code","88860f93":"code","dc8abff6":"code","b42e9df0":"code","7c171b52":"code","b9fdd015":"code","e8787d03":"code","7493a028":"code","5bc5d35d":"code","99d0844c":"code","d12d3709":"code","4ada1838":"code","99df15ae":"code","dea419b3":"code","246d808d":"code","287d220b":"code","2379d7b8":"code","caea81a6":"code","be646ffd":"code","83c24a58":"code","b62a3ade":"code","790163e2":"code","646e0e7f":"code","d888ee7d":"code","01e131af":"markdown","3fb496fa":"markdown","00f63bd7":"markdown","9cbc2fef":"markdown","6de063e7":"markdown","332f382c":"markdown","50b110c9":"markdown","0248fbfd":"markdown","998af227":"markdown","894afdd6":"markdown","44baaf52":"markdown","5ac607fe":"markdown","383199ba":"markdown","1ff02f3f":"markdown","6b38be41":"markdown","025d53d4":"markdown","dfe70bb0":"markdown","1581b14b":"markdown"},"source":{"985aa524":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer","a8e3d999":"df = pd.read_csv('vehicle.csv')\ndf.head()","ebd731f3":"df.shape","b8d7a201":"df.info()","d02e2355":"df.isna().sum()","8bd1b295":"col = df.columns[df.isnull().any()]\ndf.describe().T","8118d085":"sns.countplot(df['class'])","452c663f":"fig, ax = plt.subplots(figsize = [20, 8])\ncorr = df.corr() #Finding correlation of all the features\nsns.heatmap(corr, annot = True)","88860f93":"fig, ax = plt.subplots(figsize = [15, 6])\ncorr_pos = corr.abs() # Making all the values postive\nmask = corr_pos < 0.8 #Mask the correlation less than 0.8\nsns.heatmap(corr, annot = True, mask = mask)","dc8abff6":"selected_columns = [ 'radius_ratio','pr.axis_aspect_ratio', 'max.length_aspect_ratio', \n                    'scatter_ratio','skewness_about', 'skewness_about.1',\n                    'hollows_ratio']","b42e9df0":"i_median = SimpleImputer(strategy = 'median')\ndf_median = df.copy() #Making a copy to impute the dataset\ndf_median[col] = i_median.fit_transform(df[col]) # Imputing with median for missing values\ncorr_median = df_median.corr() # finding the correlation for imputed dataframe\ndiff = corr - corr_median\nprint(diff.max()) #printing only the maximum correlation of each features","7c171b52":"fix, ax = plt.subplots(nrows = 2, ncols = 4, figsize = [20, 7])\n\nfor col, axes in zip(selected_columns, ax.flatten()):\n  sns.distplot(df[col], ax = axes)\n  mean = df[col].mean()\n  median = df[col].median()\n  axes.axvline(mean, color = 'r', linestyle = '--') # Vertical line along axis to indicate the mean\n  axes.axvline(median, color = 'b', linestyle = '--') # Vertical line to indicate the median\n  axes.legend({'Mean': mean, 'Median': median})","b9fdd015":"fig, ax = plt.subplots(nrows = 2, ncols = 4, figsize = [20, 7])\n\nfor col, axes in zip(selected_columns, ax.flatten()):\n  sns.boxplot(df[col], ax = axes)","e8787d03":"fig, ax = plt.subplots(nrows = 2, ncols = 4, figsize = [20, 7])\n\nfor col, axes in zip(selected_columns, ax.flatten()):\n  sns.boxplot(x = col, y = 'class', data = df, ax = axes)","7493a028":"def outlier_removal(ar): # Function to replace outliers with 1.5*IQR in both lower and higher side\n  for i in range(7):\n    p = np.percentile(ar[:, i], [25, 75])\n    iqr = p[1] - p[0]\n    q1 = p[0]- 1.5*iqr\n    q3 = p[1]+ 1.5*iqr\n    ar[:, i][ar[:, i]<q1] = q1\n    ar[:, i][ar[:, i]>q3] = q3\n  return ar","5bc5d35d":"seed = 3\npipeline = Pipeline(\n    [('impute', SimpleImputer(strategy = 'median')),\n    ('outlier', FunctionTransformer(outlier_removal)),\n    ('scale', StandardScaler()),\n    ('model', SVC(random_state = seed))\n       ])","99d0844c":"x = df[selected_columns]\ny = df['class']\nx_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.3, random_state = seed)","d12d3709":"performance = pd.DataFrame(columns = ['Type', 'Train Accuracy', 'Test Accuracy', 'Mean cross validation score', 'Deviation'])\nnormal_model = pipeline.fit(x_train, y_train)\ncv = StratifiedKFold()\nnormal_score = cross_val_score( normal_model, X=x, y=y, scoring = 'accuracy', n_jobs = -1, cv = cv)\nperformance = performance.append({'Type': 'Model without PCA and tuning',\n                                 'Train Accuracy': normal_model.score(x_train, y_train)*100,\n                                 'Test Accuracy': normal_model.score(x_test, y_test)*100,\n                                 'Mean cross validation score': normal_score.mean()*100,\n                                 'Deviation': 2*100*normal_score.std()}, ignore_index = True)\n\nprint(\"--------------------------Model without PCA and tuning-----------------------------------\\n\")\nprint(\"Train accuracy : {:2.2f}%\".format(normal_model.score(x_train, y_train)*100))\nprint(\"Test accuracy : {:2.2f}%\".format(normal_model.score(x_test, y_test)*100))\nprint(\"Cross validation mean score: {:2.2f}% with deviation (+\/-{:2.2f}%)\" .format(normal_score.mean()*100, 2*100*normal_score.std()))","4ada1838":"params_svm = {\n    'model__C': [i for i in range(1, 15, 1)],\n    #'model__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n    'model__kernel' : ['rbf'], # Checked with all kernaels but RBF performed well, so using RBF in all the upcoming iterations\n    'model__gamma' : np.logspace(-5, -1, 30)\n            }\n\n\nmodel_tuning = GridSearchCV(pipeline, params_svm, scoring = 'accuracy')","99df15ae":"model_tuning.fit(x_train, y_train)\ntuned_score = cross_val_score( model_tuning.best_estimator_, X=x, y=y, scoring = 'accuracy', n_jobs = -1, cv = cv)\nperformance = performance.append({'Type': 'Model without PCA and with tuning',\n                                 'Train Accuracy': model_tuning.score(x_train, y_train)*100,\n                                 'Test Accuracy': model_tuning.score(x_test, y_test)*100,\n                                 'Mean cross validation score': tuned_score.mean()*100,\n                                 'Deviation': 2*100*tuned_score.std()}, ignore_index = True)\nprint(\"--------------------------Model without PCA and with tuning-----------------------------------\\n\")\nprint(\"Train accuracy : {:2.2f}%\".format(model_tuning.score(x_train, y_train)*100))\nprint(\"Test accuracy : {:2.2f}%\".format(model_tuning.score(x_test, y_test)*100))\nprint(\"Cross validation mean score: {:2.2f}% with deviation (+\/-{:2.2f}%)\" .format(tuned_score.mean()*100, 2*100*tuned_score.std()))","dea419b3":"def outlier_removal_pca(ar):\n  for i in range(18):\n    p = np.percentile(ar[:, i], [25, 75])\n    iqr = p[1] - p[0]\n    q1 = p[0]- 1.5*iqr\n    q3 = p[1]+ 1.5*iqr\n    ar[:, i][ar[:, i]<q1] = q1\n    ar[:, i][ar[:, i]>q3] = q3\n  return ar","246d808d":"from sklearn.decomposition import PCA\npipeline_pca = Pipeline(\n    [('impute', SimpleImputer(strategy = 'median')),\n    ('outlier', FunctionTransformer(outlier_removal_pca)),\n    ('scale', StandardScaler()),\n    ('pca', PCA(random_state = seed)),\n    ('model', SVC( random_state = seed))\n       ])","287d220b":"x_pca = df.drop('class', axis = 1)\ny_pca = df['class']\nx_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_pca, y_pca, stratify = y_pca, test_size = 0.3, random_state = seed)","2379d7b8":"pipeline_pca.fit(x_train_pca, y_train_pca)\nnormal_score_pca = cross_val_score( pipeline_pca, X=x_pca, y=y_pca, scoring = 'accuracy', n_jobs = -1, cv = cv)\nperformance = performance.append({'Type': 'Model with PCA and without tuning',\n                                 'Train Accuracy': pipeline_pca.score(x_train_pca, y_train_pca)*100,\n                                 'Test Accuracy': pipeline_pca.score(x_test_pca, y_test_pca)*100,\n                                 'Mean cross validation score': normal_score_pca.mean()*100,\n                                 'Deviation': 2*100*normal_score_pca.std()}, ignore_index = True)\nprint(\"--------------------------Model with PCA and without tuning-----------------------------------\\n\")\nprint(\"Train accuracy : {:2.2f}%\".format(pipeline_pca.score(x_train_pca, y_train_pca)*100))\nprint(\"Test accuracy : {:2.2f}%\".format(pipeline_pca.score(x_test_pca, y_test_pca)*100))\nprint(\"Cross validation mean score: {:2.2f}% with deviation (+\/-{:2.2f}%)\" .format(normal_score_pca.mean()*100, 2*100*normal_score_pca.std()))","caea81a6":"params_svm = {\n    'model__C': [i for i in range(1, 15, 1)],\n    #'model__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n    'model__kernel' : ['rbf'],\n    'model__gamma' : np.logspace(-5, -1, 30)\n            }\nmodel_tuning_pca = GridSearchCV(pipeline_pca, params_svm, scoring = 'accuracy')","be646ffd":"model_tuning_pca.fit(x_train_pca, y_train_pca)\ntuned_score_pca = cross_val_score( model_tuning_pca.best_estimator_, X=x_pca, y=y_pca, scoring = 'accuracy', n_jobs = -1, cv = cv)\nperformance = performance.append({'Type': 'Model with PCA and tuning',\n                                 'Train Accuracy': model_tuning_pca.score(x_train_pca, y_train_pca)*100,\n                                 'Test Accuracy': model_tuning_pca.score(x_test_pca, y_test_pca)*100,\n                                 'Mean cross validation score': tuned_score_pca.mean()*100,\n                                 'Deviation': 2*100*tuned_score_pca.std()}, ignore_index = True)\nprint(\"--------------------------Model with PCA and tuning-----------------------------------\\n\")\nprint(\"Train accuracy : {:2.2f}%\".format(model_tuning_pca.score(x_train_pca, y_train_pca)*100))\nprint(\"Test accuracy : {:2.2f}%\".format(model_tuning_pca.score(x_test_pca, y_test_pca)*100))\nprint(\"Cross validation mean score: {:2.2f}% with deviation (+\/-{:2.2f}%)\" .format(tuned_score_pca.mean()*100, 2*100*tuned_score_pca.std()))","83c24a58":"model_tuning_pca.best_params_","b62a3ade":"n_features = np.arange(2, 19)\nmodel = pipeline_pca\npca_analysis = pd.DataFrame(columns = ['n_features', 'Explained_variation', 'Accuracy'])\nfor i in n_features:\n  #Using the best paramters estimated by the model in hyper parameter tuning\n  model.set_params(pca__n_components=  i, model__C = 14, model__gamma = 0.0386, model__kernel = 'rbf') \n  model.fit(x_train_pca, y_train_pca)\n  pca_analysis = pca_analysis.append({'n_features': i,\n                       'Explained_variation': sum(model.named_steps['pca'].explained_variance_ratio_)*100,\n                      'Accuracy': model.score(x_test_pca, y_test_pca)*100 }, ignore_index = True)\n  ","790163e2":"fig, ax = plt.subplots(figsize = [10,7])\nsns.lineplot(x= 'n_features', y ='value' , hue = 'variable', data = pd.melt(pca_analysis, 'n_features'))\nplt.grid()","646e0e7f":"model.set_params(pca__n_components=  8, model__C = 14, model__gamma = 0.0386, model__kernel = 'rbf') \nmodel.fit(x_train_pca, y_train_pca)\nfinal_score_pca = cross_val_score( model, X=x_pca, y=y_pca, scoring = 'accuracy', n_jobs = -1, cv = cv)\nperformance = performance.append({'Type': 'Model with PCA, tuning and dimension reduction',\n                                 'Train Accuracy': model.score(x_train_pca, y_train_pca)*100,\n                                 'Test Accuracy': model.score(x_test_pca, y_test_pca)*100,\n                                 'Mean cross validation score': final_score_pca.mean()*100,\n                                 'Deviation': 2*100*final_score_pca.std()}, ignore_index = True)\nprint(\"--------------------------Model with PCA and Dimension reduction-----------------------------------\\n\")\nprint(\"Train accuracy : {:2.2f}%\".format(model.score(x_train_pca, y_train_pca)*100))\nprint(\"Test accuracy : {:2.2f}%\".format(model.score(x_test_pca, y_test_pca)*100))\nprint(\"Cross validation mean score: {:2.2f}% with deviation (+\/-{:2.2f}%)\" .format(final_score_pca.mean()*100, 2*100*final_score_pca.std()))","d888ee7d":"performance","01e131af":"##### Test to impute the values with median\n* The no of missing rows is very few compared to dataset size.\n* Here we are replacing the missing value with median and checking whether any huge difference in correlation between the features before and after imputing.\n* We found that there is not much variation in correlation between the features, so the dataset is not much biased by this impute. ","3fb496fa":"##### Shape of Dataset\n* No of datapoints - 846\n* No of features - 19","00f63bd7":"* We could see that the PCA model with all features performed well in SVM model as the PCA made all the new features independent to each other.","9cbc2fef":"##### Analysis of PCA \n\n* The figure shows the cumulative sum of percent of variation explained by the principle components.\n* Also is shows the percentage of accuracy obtained on test data by training models with given no of principle components.\n* As per our requirment we have to use the principle components which explains about 95% of variance.\n* As per the data 95% variance will be captured when we train our model with 7 principle components.\n* From the plot we can see that there is not much raise in accuracy from training with 6 to 7 principle components but there is huge raise in accuracy from training with 7 to 8 principle components. \n* So we will train with 8 principle components in our final model.  ","6de063e7":"##### Pipeline with PCA\n\n* PCA model re-arrange the feature based on the amount of variation explained. \n* Also it removes the correlation between the features making it independent to each other which is ideally expected for all machine learning algorithms\n\n#### Planned Steps:-\n\n1. Preprocessing\n>* Giving All the features as input as we using PCA for feature selection. \n>* Imputing missing values with median\n>* Replacing the outliers with 1.5*IQR in both upper and lower limit\n\n\n2. Scaling\n>* Scaling all the input features.\n\n3. Feature selection\n>* Using PCA for feature selection.\n\n4. ML Model\n>* Fitting the data to SVM Model.","332f382c":"* From this 5 point summary, we can see all the featurea have different range of values.\n* So we have to scale it to make sure that our model gives equal importance to all the features.","50b110c9":"##### Independent features vs Target feature\n* This plot helps us to find the importance of each feature individually in predicting the target.\n* The feature which has higher variation in box amoung the classes will contribute more in prediction.\n* We could see that Scatter ratio and hollows ration will be good predictor.","0248fbfd":"##### Correlation\n* From the heatmap we can see that there are many multicorrelation features.\n* Highly correlated features provide same information for the model.\n* So we can drop the highly correlated features","998af227":"##### Importing Dataset as Pandas Dataframe","894afdd6":"* On checking there is few countable no of missing values in some columns.\n* We will analyse it further to decide how to treat these missing value","44baaf52":"##### Box Plot\n* Other than hollow ratio and scatter ratio, we could see some outliers in all other features.\n* Since most of the outliers are nearer to the 1.5*IQR bin, we can replace the outliers with lower and upper bin,\n* Not having outliers in hollow ratio and scatter ratio is good as treating outliers on it may change the distribution as it is representing many other features. ","5ac607fe":"* We can use only the below mentioned features for our model without PCA","383199ba":"* All the independent features are numeric datatype\n* Also there is some null values for few attributes","1ff02f3f":"##### Distribution of selected features\n\n* We have 3 classes in target column, so the plot represents the distribution of 3 classes.\n* It is the reason that we can see multiple peaks in most of the features.\n* Mean and median are almost equal in all distribution but having outlier in some plots is pulling the mean towards slightly right side. ","6b38be41":"##### Summary\n\n1. Model with and without PCA on reduced dimensions.\n>* Initially we build a model by dropping the multicorrelated features.\n>* Then we build a model by dropping the features using PCA\n>* Both these models are build with almost same no of  features but the model with principle components performed slightly better. \n\n2. Model with PCA on all dimensions and reduced dimensions.\n>* We build a model with PCA and taking all the principle components.\n>* Then we build a model with taking the principle components which explains about 95% of variation.\n>* The 1st one performed very well as it gave the better accuracy when compared to the 2nd. \n>* The noteworthy point here is that the 1st model trained with 18 features to give 98.3% accuracy but the dimension reduced model gave around 95.7% accuracy with just 8 features.\n>* So it reduced more than 50% of model complexity with a small compensation in model accuracy. \n>* So the model with PCA and reduced dimension can be considered as the final model. ","025d53d4":"##### Multicorrelated features\n* We filtered all highly correlated features.\n* We can see that features (compactness, circularity, distance_circularity,elongatedness, pr.axis_rectangularity, max.length_rectangularity,scaled_variance, scaled_variance.1, scaled_radius_of_gyration) are highly correlated to scatter_ratio. So scatter_ratio alone will represent all these features.\n* Similarly hollows_ratio alone represent the features scaled_radius_of_gyration.1, skewness_about.2 ","dfe70bb0":"* Distribution of each class is not even but it is fairly enough to proceed with the model. ","1581b14b":"#### Pipeline\n\n* We are using Pipeline feature available in Scikit Learn to build our model.\n* Pipelines helps to bulid a better readable organized model.\n* It helps to process the train, test dataset seperately which refrains the model from data leakage.\n\n---\n#### Planned Steps:-\n\n1. Preprocessing\n>* Selection only these features and removing all highly correlated features. \n>* Selected Features: ( radius_ratio, pr.axis_aspect_ratio, max.length_aspect_ratio scatter_ratio, skewness_about, skewness_about.1,hollows_ratio)\n>* Imputing missing values with median\n>* Replacing the outliers with 1.5*IQR in both upper and lower limit\n\n\n2. Scaling\n>* Scaling all the input features.\n\n4. ML Model\n>* Fitting the data to SVM Model."}}