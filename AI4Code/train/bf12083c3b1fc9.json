{"cell_type":{"a9b98bf3":"code","9c2bc73a":"code","bc245463":"code","fb4bf8b8":"code","f6d9e3dc":"code","e2236a91":"code","4dca9a50":"code","e34dd18b":"code","9ae511d7":"code","5b041d3e":"code","2a75c3f7":"code","0be08b42":"code","f389b64a":"code","bd5e87bf":"code","64596769":"code","4afcdd82":"code","5618aafd":"code","223fb49e":"code","5f705f62":"code","84fd9e91":"code","cd7d0e86":"code","febb0e64":"code","fd31c5bf":"code","3fc50453":"code","11363ec9":"code","e2ffd29f":"code","50fc593b":"code","f44b3233":"code","4cb55f6c":"markdown","d43f38d1":"markdown","509c3109":"markdown","73329249":"markdown","6e8cc29b":"markdown","ad14f39a":"markdown"},"source":{"a9b98bf3":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport xgboost as xgb\nimport seaborn as sns\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.signal import hann\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.svm import NuSVR, SVR\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","9c2bc73a":"percent_data = 25\ntotal_data_points = 629145480\ntrain = pd.read_csv('..\/input\/train.csv', nrows=total_data_points*percent_data\/100)","bc245463":"test = pd.read_csv('..\/input\/test\/seg_004cd2.csv')\nprint('Size of test data: ', len(test))\nprint(test.describe())\ntest.head()","fb4bf8b8":"test_files = os.listdir(\"..\/input\/test\")\nfig, ax = plt.subplots(4,1, figsize=(20,25))\n\nfor n in tqdm(range(4)):\n    seg = pd.read_csv(\"..\/input\/test\/\" + test_files[n])\n    ax[n].plot(seg.acoustic_data.values, c=\"r\")\n    ax[n].set_ylabel(\"Signal\")\n    ax[n].set_ylim([-300, 300])\n    ax[n].set_title(\"Test - {}\".format(test_files[n]));","f6d9e3dc":"print(train.dtypes)\npd.options.display.precision = 10\ntrain.head()","e2236a91":"train.describe()","4dca9a50":"train_ad_sample_df = train['acoustic_data'][::50]\ntrain_ttf_sample_df = train['time_to_failure'][::50]\n\nfig, ax1 = plt.subplots(figsize=(12,8))\nplt.plot(train_ad_sample_df, color='r')\nplt.legend(['acoustic_data'], loc=[0.01, 0.95])\nax2 = ax1.twinx()\nplt.plot(train_ttf_sample_df, color='b')\nplt.legend(['time_to_failure'], loc=[0.01, 0.9])\nplt.grid(True)\n\ndel train_ad_sample_df\ndel train_ttf_sample_df","e34dd18b":"rows = 150000\nsegments = int(np.floor(train.shape[0] \/ rows))\nprint(\"Number of segments: \", segments)","9ae511d7":"train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","5b041d3e":"def create_features(seg_id, seg, X):\n    \n    xc = pd.Series(seg['acoustic_data'].values)\n    zc = np.fft.fft(xc)\n    \n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    X.loc[seg_id, 'min'] = xc.min()\n    \n    #FFT transform values\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    \n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    \n    X.loc[seg_id, 'Imag_mean'] = imagFFT.mean()\n    X.loc[seg_id, 'Imag_std'] = imagFFT.std()\n    X.loc[seg_id, 'Imag_max'] = imagFFT.max()\n    X.loc[seg_id, 'Imag_min'] = imagFFT.min()\n    \n    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n    X.loc[seg_id, 'Rmean_first_5000'] = realFFT[:5000].mean()\n    X.loc[seg_id, 'Rstd__first_5000'] = realFFT[:5000].std()\n    X.loc[seg_id, 'Rmax_first_5000'] = realFFT[:5000].max()\n    X.loc[seg_id, 'Rmin_first_5000'] = realFFT[:5000].min()\n    \n    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n    X.loc[seg_id, 'Rmean_first_15000'] = realFFT[:15000].mean()\n    X.loc[seg_id, 'Rstd_first_15000'] = realFFT[:15000].std()\n    X.loc[seg_id, 'Rmax_first_15000'] = realFFT[:15000].max()\n    X.loc[seg_id, 'Rmin_first_15000'] = realFFT[:15000].min()\n    \n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n    X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) \/ xc[:-1]))[0])\n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    \n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    \n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n    \n    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n    \n    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n    \n    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n    \n    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n    \n    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n    \n    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n    \n    X.loc[seg_id, 'max_to_min'] = xc.max() \/ np.abs(xc.min())\n    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n    X.loc[seg_id, 'sum'] = xc.sum()\n    \n    X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) \/ xc[:50000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) \/ xc[-50000:][:-1]))[0])\n    \n    X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) \/ xc[:10000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) \/ xc[-10000:][:-1]))[0])\n    \n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n    \n    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n    \n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n    \n    X.loc[seg_id, 'mad'] = xc.mad()\n    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n    X.loc[seg_id, 'skew'] = xc.skew()\n    X.loc[seg_id, 'med'] = xc.median()\n    \n    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') \/ sum(hann(150))).mean()\n    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n    \n    ewma = pd.Series.ewm\n    X.loc[seg_id, 'exp_Moving_average_300_mean'] = ewma(xc, span=300).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n    \n    no_of_std = 2\n    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n    \n    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = xc.rolling(windows).std().dropna().values\n        x_roll_mean = xc.rolling(windows).mean().dropna().values\n        \n        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()","2a75c3f7":"for seg_id in tqdm(range(segments)):\n    seg = train.iloc[seg_id*rows:seg_id*rows+rows]\n    create_features(seg_id, seg, train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","0be08b42":"print(\"New training shape: \", train_X.shape)\ntrain_X.head(10)","f389b64a":"scaler = StandardScaler()\nscaler.fit(train_X)\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\nscaled_train_X.head(10)","bd5e87bf":"#scaler = Normalizer()\n#scaler.fit(train_X)\n#scaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n#scaled_train_X.head(10)","64596769":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\ntest_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)","4afcdd82":"for seg_id in tqdm(test_X.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    create_features(seg_id, seg, test_X)","5618aafd":"scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)","223fb49e":"Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\ngammas = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ngrid_search = GridSearchCV(SVR(kernel='rbf', tol=0.01), param_grid, cv=5)\ngrid_search.fit(scaled_train_X, train_y)\nprint('Best CV Score:', grid_search.best_score_)\nprint('Best parameters: ', grid_search.best_params_)\nprint('Best estimator: ', grid_search.best_estimator_)","5f705f62":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","84fd9e91":"param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n              \"gamma\": np.logspace(-2, 2, 5)}\ngrid_search = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), param_grid, cv=5)\ngrid_search.fit(scaled_train_X, train_y)\nprint('Best CV Score:', grid_search.best_score_)\nprint('Best parameters: ', grid_search.best_params_)\nprint('Best estimator: ', grid_search.best_estimator_)","cd7d0e86":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","febb0e64":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(scaled_train_X, train_y, train_size=0.75, random_state=42)","fd31c5bf":"model = CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\nmodel.fit(scaled_train_X, train_y, cat_features=None, eval_set=(X_validation, y_validation), plot=True)","3fc50453":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","11363ec9":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(scaled_train_X, train_y, train_size=0.75, random_state=42)","e2ffd29f":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_validation, y_validation)\n\nparams = {'boosting_type': 'gbdt',\n         'objective': 'regression',\n         'metric': {'l2', 'l1'},\n         'num_leaves': 31,\n         'learning_rate': 0.05,\n         'feature_fraction': 0.9,\n         'bagging_fraction': 0.8,\n         'bagging_freq': 5,\n         'verbose': 0}\ngbm = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=lgb_eval, early_stopping_rounds=5)","50fc593b":"predictions = gbm.predict(scaled_test_X, num_iteration=gbm.best_iteration)\nprint(len(predictions))\nprint(predictions)","f44b3233":"submission.time_to_failure = predictions\nsubmission.to_csv('submission.csv', index=True)","4cb55f6c":"SVR","d43f38d1":"**Loading 25% of training data**","509c3109":"Reference: https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction","73329249":"KernelRidge","6e8cc29b":"LightGBM","ad14f39a":"CatBoost Algorithm"}}