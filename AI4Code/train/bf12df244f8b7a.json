{"cell_type":{"01d58d24":"code","37451739":"code","c7f2cefa":"code","1a663400":"code","729ccffe":"code","0e9cfa3b":"code","dd65d205":"code","dce3b149":"code","570d839b":"code","45c82eb2":"code","75217feb":"code","e56d2754":"code","58bbcc2e":"markdown","12a84701":"markdown","9568eeab":"markdown","a19ba723":"markdown","12e605bb":"markdown","28f54ff9":"markdown","d1935dfc":"markdown","5fc098ca":"markdown"},"source":{"01d58d24":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport keras\nimport os\nfrom keras.layers import Input, Activation, add, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.regularizers import l2\nfrom keras.layers.core import Reshape\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Using a GPU for this kernel\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)\n\nfrom keras import backend as K\nK.set_image_dim_ordering('tf')","37451739":"# Load up the data\ntrain_data = np.genfromtxt('..\/input\/train.csv', delimiter=',')[1:]\ntrain_X = train_data[:, 1:]\ntrain_y_orig = train_data[:, :1]\n\n# converting to one-hot encoding \ntrain_y = np.zeros([train_y_orig.shape[0], 10])\nfor ind in range(train_y_orig.shape[0]):\n    train_y[ind][int(train_y_orig[ind][0])] = 1","c7f2cefa":"plt.hist(train_y_orig, color='firebrick', bins=10)\nplt.xticks(range(0,10))\nplt.xlabel(\"number\")\nplt.ylabel(\"count\")\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.show()","1a663400":"model = Sequential()\n\ninput_shape = (28, 28, 1)\n\nmodel.add(Conv2D(32, kernel_size=(7, 7), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(32, kernel_size=(7, 7), padding='same', activation='relu'))\n# model.add(MaxPooling2D((2, 2), strides=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=(5, 5), strides=(2,2), padding='same', activation='relu'))\nmodel.add(Conv2D(64, kernel_size=(5, 5), strides=(2,2), padding='same', activation='relu'))\n# model.add(MaxPooling2D((2, 2), strides=(1,1)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), strides=(3,3), padding='same', activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), strides=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(10, activation='softmax'))\n\nopt = RMSprop()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt, \n              metrics=['accuracy'])","729ccffe":"model.summary()","0e9cfa3b":"# traingen = ImageDataGenerator(\n#             featurewise_center=False,  # set input mean to 0 over the dataset\n#             samplewise_center=False,  # set each sample mean to 0\n#             featurewise_std_normalization=False,  # divide inputs by std of the dataset\n#             samplewise_std_normalization=False,  # divide each input by its std\n#             zca_whitening=False,  # apply ZCA whitening\n#             rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n#             zoom_range = 0.1, # Randomly zoom image \n#             width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n#             height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n#             horizontal_flip=False,  # randomly flip images\n#             vertical_flip=False)  # randomly flip images\n","dd65d205":"batch_size=2**8\nepochs = 100\n\n# reshaping data and normalize\nn = train_X.shape[0]\ntrain_X = train_X.reshape((n, 28, 28, 1)).astype('float32') \/ 255\n\n# traingen.fit(train_X)\n# history = model.fit_generator(traingen.flow(train_X,train_y, batch_size=batch_size),\n#                               epochs=epochs,\n#                               steps_per_epoch=train_X.shape[0]\/\/batch_size,\n#                              )\n\n\nhistory = model.fit(train_X, train_y, \n                    epochs=epochs,\n                    batch_size=batch_size)","dce3b149":"plt.plot(history.history['acc'], color=\"dodgerblue\")\nplt.ylim(0.995, 1)\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epoch\")\nplt.show()\n\nplt.plot(history.history['loss'], color=\"firebrick\")\nplt.ylim(0, 0.015)\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.show()","570d839b":"print(model.evaluate(train_X, train_y))","45c82eb2":"# Load the test data\ntest_data = np.genfromtxt('..\/input\/test.csv', delimiter=',')[1:]","75217feb":"predictions = model.predict(test_data.reshape((test_data.shape[0], 28, 28, 1)).astype('float32') \/ 255)\npredictions = predictions.argmax(1) # this converts the classes down into a single value","e56d2754":"sub_data = np.zeros([predictions.shape[0], 2])\ncount = 0\nfor val in predictions:\n    sub_data[count] = [count + 1, val]\n    count += 1\nsub_data = sub_data.astype(int)\nnp.savetxt(fname=\"submission.csv\",\n           X=sub_data,\n           fmt='%i',\n           delimiter=',',\n           comments='',\n           header='ImageId,Label')","58bbcc2e":"# Digit Recognition with a Neural Network\nLet's begin by loading our data and taking a look at the class balance.","12a84701":"## Predict our Test Data","9568eeab":"## Evaluation on the Training Set\nThis is only slightly indicative of the model's accuracy, since it has already seen this data, it is very likely to get it correct. This is a good sanity check though, to make sure that your model is working.\n\nThe .evaluate() method returns [loss, accuracy]","a19ba723":"## Make a Submission","12e605bb":"## Train the Network","28f54ff9":"Let's see how the accuracy went as training went on.","d1935dfc":"## Data Generator for Training\n\nA data generator will slightly modify the input images to allow the model to gain more experience on similar data, improving its robustness.","5fc098ca":"Looks balanced and not skewed in any particular direction. Should be just fine.\n\n## Building the Neural Network"}}