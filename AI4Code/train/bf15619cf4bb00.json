{"cell_type":{"149965b7":"code","85e25bc7":"code","3b70bdb9":"code","9a9b8d33":"code","969235f9":"code","f9a38750":"code","b1fe2a9a":"code","c67684ce":"code","e27fdf99":"code","727fcbe0":"code","d30f7ccd":"code","e299f32c":"code","ae4c9a18":"code","a57261eb":"code","d63356d1":"code","fc1f2f98":"code","903a5354":"code","908003fe":"code","cd5c4212":"code","f7846fd2":"code","6f973ccd":"code","ac470d20":"code","026d87e4":"code","03748098":"code","2caa7440":"code","be792100":"code","cefcd438":"code","3eb7dffd":"code","0dfc7e03":"code","47f6ee43":"code","06a64b83":"code","8fa79700":"code","08bd4cb5":"code","22322f61":"code","31d51db0":"code","11e657cb":"code","3d7ae017":"code","a4829aea":"code","a51ed56c":"code","2c1d3ef6":"code","8b71e2e3":"code","bbc48d64":"code","346777c2":"code","e6c6d1c2":"code","10d7d913":"code","f63e7433":"code","c7b57431":"code","acc104ff":"code","f2d1c6e4":"code","4c6cfa3e":"code","bc940e4c":"code","0414aa2e":"markdown","7aafd264":"markdown","f324b103":"markdown","509a6967":"markdown","2460bd6e":"markdown","a19499d8":"markdown","714e0043":"markdown","6c864135":"markdown","6cebe277":"markdown","35e43b88":"markdown","0b248f31":"markdown","84395f3f":"markdown","92a53869":"markdown","ab5327ac":"markdown","99d16f3d":"markdown","89c414e4":"markdown","8f1ffe5e":"markdown","8b31cd6c":"markdown","967ab51a":"markdown","5be9c7d8":"markdown","5c915884":"markdown","9ad8f018":"markdown","f6513958":"markdown","ebb463d3":"markdown","3bf09c56":"markdown","a8aea244":"markdown","1b73981c":"markdown","3eba856d":"markdown","c01c01e1":"markdown","5ae7a88e":"markdown","87de2286":"markdown","f20d8b49":"markdown","ad4e0c7c":"markdown","5c684945":"markdown","25bda787":"markdown","224dfc1d":"markdown","6515e9a6":"markdown","fee9b109":"markdown","e7a39ca8":"markdown","0db543b0":"markdown","9292a600":"markdown","a3e7c703":"markdown","83463da4":"markdown","9aa967e9":"markdown","e6e2d355":"markdown","742f15b8":"markdown","411ef64a":"markdown","bcccce3f":"markdown","f5219258":"markdown","5c6a8e07":"markdown","b72048ce":"markdown","4fbe1179":"markdown","5d2f4177":"markdown","b6ba1878":"markdown"},"source":{"149965b7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew","85e25bc7":"df = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')\ndf.head()","3b70bdb9":"df.info()","9a9b8d33":"df.describe()","969235f9":"df.shape","f9a38750":"df['Age'] = df['Rings']+1.5\ndf = df.drop('Rings', axis=1)\ndf.head()","b1fe2a9a":"df.describe()","c67684ce":"df.hist(figsize=(20,10), grid=False, layout=(2,4), bins=30)\nplt.show()","e27fdf99":"Numerical = df.select_dtypes(include=[np.number]).columns\nCategorical = df.select_dtypes(include=[np.object]).columns","727fcbe0":"skew_values = skew(df[Numerical], nan_policy = 'omit')\ndummy = pd.concat([pd.DataFrame(list(Numerical), columns=['Features']), \n           pd.DataFrame(list(skew_values), columns=['Skewness degree'])], axis = 1)\ndummy.sort_values(by = 'Skewness degree' , ascending = False)","d30f7ccd":"plt.figure(figsize=(10,5))\nsns.countplot(df['Sex'])\nplt.show()","e299f32c":"df.boxplot(figsize=(20,10))\nplt.show()","ae4c9a18":"plt.figure(figsize=(10,5))\nsns.distplot(df['Age'])\nplt.show()","a57261eb":"df.head()","d63356d1":"fig, axes = plt.subplots(4,2, figsize=(15,15))\naxes = axes.flatten()\n\nfor i in range(1,len(df.columns)-1):\n    sns.scatterplot(x=df.iloc[:,i], y=df['Age'], ax=axes[i])\n\nplt.show()","fc1f2f98":"plt.figure(figsize=(10,5))\nsns.boxenplot(y=df['Age'], x=df['Sex'])\nplt.grid()\nplt.show()\n\ndf.groupby('Sex')['Age'].describe()","903a5354":"plt.figure(figsize=(10,5))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","908003fe":"sns.pairplot(df)\nplt.show()","cd5c4212":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","f7846fd2":"X = df.drop(['Age'], axis=1)\ny = df['Age']\n\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\nlr = sm.OLS(y, Xc).fit()\nlr.summary()","6f973ccd":"from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n\nvif = [VIF(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame(vif, index=Xc.columns, columns=['VIF'])","ac470d20":"X2 = X.drop(['Whole weight'], axis=1)\nX2c = sm.add_constant(X2)\n\nvif = [VIF(X2c.values, i) for i in range(X2c.shape[1])]\npd.DataFrame(vif, index=X2c.columns, columns=['VIF'])","026d87e4":"X2 = X.drop(['Whole weight','Diameter'], axis=1)\nX2c = sm.add_constant(X2)\n\nvif = [VIF(X2c.values, i) for i in range(X2c.shape[1])]\npd.DataFrame(vif, index=X2c.columns, columns=['VIF'])","03748098":"X2 = X.drop(['Whole weight','Diameter','Viscera weight'], axis=1)\nX2c = sm.add_constant(X2)\n\nvif = [VIF(X2c.values, i) for i in range(X2c.shape[1])]\npd.DataFrame(vif, index=X2c.columns, columns=['VIF'])","2caa7440":"lr = sm.OLS(y, X2c).fit()\nlr.summary()","be792100":"y_pred = lr.predict(X2c)\n\nplt.figure(figsize=(10,5))\nsns.regplot(y_pred, y, lowess=True, line_kws={'color':'red'})\nplt.show()","cefcd438":"stat, pval = sm.stats.diagnostic.linear_rainbow(res=lr, frac=0.5)\nprint(pval)","3eb7dffd":"# QQ Plot\n\nfrom scipy import stats\n\nresid = lr.resid\n\nplt.figure(figsize=(10,5))\nstats.probplot(resid, plot=plt)\nplt.show()","0dfc7e03":"from scipy.stats import norm\nnorm.fit(resid)\n\nplt.figure(figsize=(10,5))\nsns.distplot(resid, fit=norm)\nplt.show()","47f6ee43":"stat, pval = stats.jarque_bera(resid)\nprint(pval)","06a64b83":"fig, axes = plt.subplots(2,2, figsize=(15,18))\naxes = axes.flatten()\n\nfor i in range(len(X2.columns)-2):\n    sns.distplot(X2.iloc[:,i], ax=axes[i])\n\nplt.show()","8fa79700":"plt.figure(figsize=(10,5))\nsns.residplot(lr.predict(),lr.resid)\nplt.show()","08bd4cb5":"import statsmodels.stats.api as sms\nsms.het_goldfeldquandt(lr.resid, X2c)","22322f61":"while len(X2.columns)>0:\n    X_c = sm.add_constant(X2)\n    mod = sm.OLS(y,X_c).fit()\n    f = mod.pvalues[1:].idxmax()\n    if mod.pvalues[1:].max()>0.05:\n        X2 = X2.drop(f, axis=1)\n    else:\n        break\n\nprint(\"The final features are:\",X2.columns)","31d51db0":"mod.summary()","11e657cb":"err = mod.resid\nmse = np.mean(err**2)\nrmse = np.sqrt(mse)\n\nprint(\"The root mean Sq error derived fro the statistical summary is:\",rmse)","3d7ae017":"df.head()","a4829aea":"X = df.drop('Age', axis=1)\ny = df['Age']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nX_trains = ss.fit_transform(X_train)\nX_tests = ss.transform(X_test)","a51ed56c":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X_trains, y_train)\npred = lr.predict(X_tests)\n\nfrom sklearn.metrics import r2_score, roc_auc_score, mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, pred))\nr2 = r2_score(y_test, pred)\n\nprint(\"The root mean Sq error calculated from the base model is:\",rmse)\nprint(\"The r2-score is:\",r2)","2c1d3ef6":"from sklearn.feature_selection import RFE\nlr = LinearRegression()\nn = [{'n_features_to_select':list(range(1,10))}]\nrfe = RFE(lr)\n\nfrom sklearn.model_selection import GridSearchCV\ngsearch = GridSearchCV(rfe, param_grid=n, cv=3)\ngsearch.fit(X, y)\n\ngsearch.best_params_","8b71e2e3":"lr = LinearRegression()\nrfe = RFE(lr, n_features_to_select=8)\nrfe.fit(X,y)\n\npd.DataFrame(rfe.ranking_, index=X.columns, columns=['Rank'])","bbc48d64":"from sklearn.linear_model import Lasso, LassoCV\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X,y)\npd.DataFrame(lasso.coef_, index=X.columns, columns=['Coefs'])","346777c2":"alphas = np.linspace(0.001, 0.1, 100)\nlassocv = LassoCV(alphas=alphas, cv=3, random_state=1, max_iter=5000)\nlassocv.fit(X,y)\nlassocv.alpha_","e6c6d1c2":"lasso = Lasso(alpha=lassocv.alpha_, max_iter=5000)\nlasso.fit(X,y)\npd.DataFrame(lasso.coef_, index=X.columns, columns=['Coefs'])","10d7d913":"from sklearn.model_selection import cross_val_score\n\nres = cross_val_score(lasso, X, y, cv=3, scoring='neg_mean_squared_error')\nrmse_lasso = np.sqrt(abs(res))\nprint(\"The RMSE for Lasso regression is:\",rmse_lasso.mean())","f63e7433":"from sklearn.linear_model import Ridge, RidgeCV\nridge = Ridge(alpha=0.5)\nridge.fit(X, y)\npd.DataFrame(ridge.coef_, index=X.columns, columns=['Coefs'])","c7b57431":"alphas = np.logspace(-3,1,1000)\ncoefs = []\nfor a in alphas:\n    model = Ridge(alpha=a)\n    model.fit(X,y)\n    coefs.append(model.coef_)\n\nplt.figure(figsize=(10,5))    \nplt.plot(alphas, coefs)\nplt.show()","acc104ff":"alphas = np.logspace(-2,0,1000)\nridgecv = RidgeCV(alphas=alphas, cv=3)\nridgecv.fit(X,y)\nridgecv.alpha_","f2d1c6e4":"ridge = Ridge(alpha=ridgecv.alpha_)\nridge.fit(X,y)\npd.DataFrame(ridge.coef_, index=X.columns, columns=['Coefs'])","4c6cfa3e":"res = cross_val_score(ridge, X, y, cv=3, scoring='neg_mean_squared_error')\nrmse_ridge = np.sqrt(abs(res))\nprint(\"The RMSE for Ridge regression is:\",rmse_ridge.mean())","bc940e4c":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.linear_model import  Ridge\nfrom sklearn.svm import SVR\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_predict\n\nmodels = [   SVR(),\n             RandomForestRegressor(),\n             GradientBoostingRegressor(),\n             KNeighborsRegressor(n_neighbors = 4)]\nresults = []\nnames = ['SVM','Random Forest','Gradient Boost','K-Nearest Neighbors']\nfor model,name in zip(models,names):\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)\n    rmse = np.sqrt(mean_squared_error(y, cross_val_predict(model, X , y, cv=3)))\n    results.append(rmse)\n    names.append(name)\n    msg = \"%s: %f\" % (name, rmse)\n    print(msg)","0414aa2e":"We can see that just removing Whole Weight was not enough, we need to iterate this process.","7aafd264":"In the summary section, Sex_I still has a high pvalue. We will deal with this later.","f324b103":"Right skewed as predicted.","509a6967":"### About the Data","2460bd6e":"#### Normality","a19499d8":"Age can be seen linear with most of the variables, but we need to confirm it in order to be sure.","714e0043":"The skewness in features are more than 0, it means that the weight of the data is more towards the right tail of the distribution. Specially Height and Age. For Normally distributed data the skewness should be about 0.","6c864135":"## Problem Description\n\nPredicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\n\nFrom the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).","6cebe277":"### Preprocessing","35e43b88":"The pvalue is 0.9980, so it can be said that the variance of error is constant across the range of data.","0b248f31":"# Thank You!","84395f3f":"The alpha 0.1 chosen is completely random. We need to tune the hyperparameters of Lassoto reach an optimal alpha.","92a53869":"Bingo, desirable features at last.","ab5327ac":"#### Dropping the features with high p-values","99d16f3d":"The Age or the target feature has a lot of outliers, so does some other features. This is the reason probable why these features are not normally distributed.","89c414e4":"#### RMSE is: 2.2333","8f1ffe5e":"### Exploratory Data Analysis\n### Univariate Analysis","8b31cd6c":"### Rsme of Other Algorithms:","967ab51a":"Checking for optimal alpha.","5be9c7d8":"#### The other models are also performing similarly, with gradient boosting giving the best results.","5c915884":"The males have the highest number followed by Infants with very close margin.","9ad8f018":"#### RMSE is: 2.238724","f6513958":"### Importing Libraries","ebb463d3":"## Preprocessing\n#### Handling Categorical features","3bf09c56":"#### Base Model","a8aea244":"Arbitrarily worse model","1b73981c":"#### Train test split and Standardization","3eba856d":"The p-value also confirms the graphical analysis.\n\nTo know for the reason of this abnormality, we see the distribution plot of all the variables.","c01c01e1":"#### Homoscedasticity","5ae7a88e":"#### RMSE is: 2.2415","87de2286":"### Bivariate Analysis","f20d8b49":"### Ridge Regression","ad4e0c7c":"### Lasso Regression","5c684945":"The graph of the predicted values and residuals shows the increment in variance.","25bda787":"## Machine Learning Approach","224dfc1d":"### Selecting Best Features through RFE","6515e9a6":"#### The RFE says that all features are important except Sex_M.","fee9b109":"## Statistical Approach","e7a39ca8":"### Multivariate Analysis","0db543b0":"#### Linearity","9292a600":"#### We will check the performance of our model by applying various advanced algorithms and see how the Superstars perform in this condition.","a3e7c703":"Put the value of this optimal alpha in the regularization to obtain the best coefficients.\n\nLasso also can be used as a feature selection method as it reduces the values of the undesirable coefficients to 0.\n- Lets check the values of the final coefficients.","83463da4":"#### Check for Multicollinearity","9aa967e9":"- Male : age majority lies in between 7.5 years to 19 years\n- Female: age majority lies in between 8 years to 19 years\n- Immature: age majority lies in between 6 years to < 10 years","e6e2d355":"Oh god..The Linear Rainbow is giving a complete judgement of what was visible.","742f15b8":"#### Inferences:\n1. There are no null values.\n2. Only height has minimum value as 0, all others are good to go.\n3. All the features have different scales for measurement.\n4. The new column Age is introduced as our target feature as mentioned in the document of the Dataset.\n5. All the features are numerical except Sex.","411ef64a":"The R squared value is low in the base model, but it should be noted that R squared value being high is not the only measure for a good model.\n\nAlso we need to check on the assumptions of Linear Regression before we check the Model's performance.","bcccce3f":"## Regularization Techniques","f5219258":"By looking at the plot the residuals do not seem to be Normal, still we will check the normality with the Jarque Bera test.","5c6a8e07":"By looking at the graph, the problem of linearity does'nt seem to be a problem. But, again the graph cannot be the only judge and it is compulsory to get the statistical analysis.","b72048ce":"- Whole Weight is almost linearly varying with all other features except age\n- Height has least linearity with remaining features\n- Age is most linearly proprtional with Shell Weight followed by Diameter and length\n- Age is least correlated with Shucked Weight","4fbe1179":"### Linear Regression Assumptions","5d2f4177":"Many features have very high VIF values. This does not seems good, for our model we have put a threshhold of 5-10.\n\nWe will remove these features one by one and then check the VIF values again in order to be sure we do not remove any important feature.","b6ba1878":"### Importing the Dataset"}}