{"cell_type":{"8953af4e":"code","0e5bb8c2":"code","50123208":"code","d23d7c02":"code","3954f0b1":"code","198946f5":"code","0b9418e9":"code","344a2a51":"code","ff11cefc":"code","6cbdbb3e":"code","0bddff00":"code","869a50fa":"code","f7a2705d":"code","b4eb11ff":"code","afb5ca62":"code","6ac49cc6":"code","23b40bec":"code","178a95d9":"code","6738e1b0":"code","e7b4fcb6":"code","faa0eab9":"code","9d06a5f7":"code","40ddaf58":"code","390aa6e5":"code","0093af9a":"code","149b48ab":"code","97fa5cfa":"code","d22d09a3":"code","24330516":"code","d9ca3750":"code","2a346962":"code","c136ac9f":"code","52daaca6":"code","a7f1268a":"code","81e9141e":"code","e6e40d36":"code","2c19914a":"code","0521e8c1":"code","8bc33fda":"code","27d94636":"code","e8901a0c":"code","8e62ed2b":"code","4774b1a5":"code","a9f7f8ba":"code","2065f87e":"code","27da97d2":"code","5839addc":"code","81adf325":"code","8cb4d208":"code","4477acd0":"code","44d72a62":"markdown","420b90ed":"markdown","fdd02419":"markdown","46cb0093":"markdown","dfe70596":"markdown","0677fde2":"markdown","f782f7aa":"markdown","a06e8139":"markdown","09a0a365":"markdown","67dd2c70":"markdown","67b8a482":"markdown","04c4c376":"markdown","ff93b2c3":"markdown","d741eb80":"markdown","6e735a14":"markdown","43b601cd":"markdown","4b4c059c":"markdown","2da29dc2":"markdown","9037b6db":"markdown","ae8b2d28":"markdown","e978b0ed":"markdown","7cf9be62":"markdown","023226b4":"markdown","897e9f17":"markdown","4d87b5f3":"markdown","a41f3965":"markdown","7ac5e816":"markdown","d3533ea4":"markdown","16286eed":"markdown","bef3384a":"markdown","ff0f7833":"markdown","b1d0d0f4":"markdown","6421f5dd":"markdown","14ab8ebd":"markdown","b91d3f51":"markdown","21595960":"markdown","05eb18bf":"markdown"},"source":{"8953af4e":"import pandas as pd                             # Importing Pandas\nimport numpy as np                              # Importing Numpy\nimport matplotlib.pyplot as plt                 # Importing Matplotlib\nimport seaborn as sns                           # Importing Seaborn","0e5bb8c2":"df = pd.read_csv('..\/input\/wheat-seeds\/wheat-seeds.csv')             # Reading the csv file\ndf.head()                                       # previewing","50123208":"df.columns = ['Area','Perimeter','Compactness','LengthOfKernel','WidthOfKernel',\n              'AsymmetryCoefficient','LengthOfKernelGroove','SeedType']  # Adding column names","d23d7c02":"df.head()                                       # previewing","3954f0b1":"nulldata=pd.DataFrame(df.isnull().sum())           # checking for null values in the data\nnulldata.columns=['Null values']                   # Remaning the column\nnulldata                                           # previewing the data","198946f5":"df.shape                                 # previewing the shape.","0b9418e9":"df.info()                               # previewing the complete info of the dataset.","344a2a51":"df['SeedType'].value_counts()        # Counting SeedTypes","ff11cefc":"df.describe()                      # Describing the dataframe","6cbdbb3e":"df['SeedType'].value_counts(normalize=True).plot.bar()      # Plotting the SeedType barplot\nplt.show()                                                  # showing the plot\nplt.rcParams[\"figure.figsize\"] = [9,5]                      # resizing the plot","0bddff00":"df.hist(figsize = (15,15))                      # Histograph of dayaframe and Resizing the plot\nplt.show()                                      # showing the plot","869a50fa":"plt.figure(figsize=(15,5))                                               # resizing the graph\nsns.histplot(data=df, x=\"Compactness\", hue=\"SeedType\",palette='Dark2')   # Plotting the histogram for Compactness wrt SeedType","f7a2705d":"Plot = sns.kdeplot(df['LengthOfKernelGroove'],shade=True, color=\"green\")   # Density plot for LengthOfKernelGroove\nsns.kdeplot(df['LengthOfKernel'],shade=True, color=\"red\")                  # Density plot for LengthOfKernel\nPlot.set_xlabel('Length Attribute')                                        # Adding X Label\nplt.legend(['Length Of KernelGroove','Length Of Kernel'])                  # Adding Legend\nplt.show()                                                                 # showing the plot","b4eb11ff":"sns.set_style('darkgrid')                                   # setting darkgrid background\nax = sns.pairplot(df, hue=\"SeedType\",palette='Dark2')       # plotting pairplot for the dataset wrt label\nplt.show()                                                  # previewing","afb5ca62":"fig, ax = plt.subplots(figsize=(10,10));             #sub-plots size\nsns.heatmap(df.corr(), annot=True, cmap = \"YlGnBu\"); # heatmap of corelation\n               # this can be used to drop or include data columns for training\nplt.title('Correlation matrix heatmap');    #plot Title","6ac49cc6":"y = df['SeedType']                     # Seperating Output.\nx = df.drop('SeedType',axis=1)         # Dropping output column temporarily.","23b40bec":"from sklearn.model_selection import train_test_split               # importing train_test_split \nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)  # splitting data into train & test dtasets.","178a95d9":"from sklearn.preprocessing import StandardScaler    # importing StandardScaler\nss = StandardScaler()                               # loading StandardScaler\nXs = ss.fit_transform(x)                            # fitting the dataset\n\nX_trains = ss.fit_transform(X_train)                # transforming the X_train data\nX_tests = ss.transform(X_test)                      # transforming the X_test data","6738e1b0":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report  # importing metrics","e7b4fcb6":"from sklearn.linear_model import LogisticRegression                                # importing LogisticRegression\n  \nlogistic_model = LogisticRegression()                                              # Using LogisticRegression\nlogistic_model.fit(X_trains,y_train)                                               # fitting the training data into model\nlogistic_pred = logistic_model.predict(X_tests)                                    # predecting the data as per our training\n                                                                                   # printing the classification report\nprint(f\"Classification Report of Logistic Regression : \\n\\n{classification_report(y_test,logistic_pred)}\")","faa0eab9":"from sklearn.svm import SVC                                             # importing SVM\nfrom sklearn.model_selection import GridSearchCV                        # import gridsearchcv for tuning\n\nsvm_model = SVC()                                                       # loading Support Vector Classification\nsvm_model.fit(X_trains, y_train)                                        # fitting the training data into model\n\nsvm_pred = svm_model.predict(X_tests)                                   # predecting the data as per our training\n                                                                        # printing the classification report\nprint(f\"Classification Report of SVM (Before Tuning) : \\n\\n{classification_report(y_test,svm_pred)}\")","9d06a5f7":"# Dictionary containing hyperparameters to be tuned and their values to be tested\ngrid_parameters = {'C': [0.001,0.1,1, 10, 100, 1000], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]} ","40ddaf58":"# SCV() is estimator\n# grid_parameters from above cell\n# verbose set to greater than 1 to get the output messages, when the model is being tuned\ngrid = GridSearchCV(SVC(),grid_parameters,verbose=1)","390aa6e5":"grid.fit(X_trains,y_train.values.ravel())  # fitting the data so that GridSeach takes place ","0093af9a":"grid_predictions = grid.predict(X_tests)                                    # predecting the data as per GridSeach training.  ","149b48ab":"# printing the classification report\nprint(f\"Classification Report of SVM (After Tuning) : \\n\\n{classification_report(y_test,grid_predictions)}\")","97fa5cfa":"svm_accuracy = accuracy_score(y_test,grid_predictions)                      # storing the Accuracy","d22d09a3":"from sklearn.tree import DecisionTreeClassifier                             # Importing DecisionTreeClassifier\n\ndtc_model = DecisionTreeClassifier()                                        # Loading DecisionTreeClassifier\ndtc_model.fit(X_trains, y_train)                                            # fitting the training data into model\n\ndtc_pred = dtc_model.predict(X_tests)                                       # predecting the data as per our training\n                                                                            # printing the classification report\nprint(\"\\nConfusion Matrix of Decision Tree Classifier: \\n\", classification_report(y_test, dtc_pred))  ","24330516":"from sklearn import tree                                                     # Importing tree\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (8,8), dpi=300)       # setting the tree parameters & tree size\ntree.plot_tree(dtc_model,filled = True,rounded=True);                        # plotting the tree","d9ca3750":"X = df.iloc[:,0:6]                                             # Splitting data for clustering","2a346962":"from sklearn.cluster import KMeans\ndisortions = []                                                \nfor k in range(1, 10):                                         # We want k values from 1 to 10\n    kmeans = KMeans(n_clusters = k)                            # initializing KMeans\n    kmeans.fit(X)                                              # clustering\n    disortions.append(kmeans.inertia_)                         # Inertia is the sum of squared error for each cluster\n    \nplt.scatter(range(1, 10), disortions ,c = 'orange', s = 200,marker = '*')  # scattering for better reference\nplt.plot(range(1, 10), disortions, c = 'g', linewidth = 3)     # plot an elbow model \nplt.grid()                                                     # Adding grid for better understanding of graph\nplt.title('Elbow Plot')                                        # giving title for the plot\nplt.xlabel('Value of K')                                       # Adding the label for x-axis\nplt.ylabel('Disortions')                                       # Adding the label for y-axis\nplt.show()                                                     # previewing the plot","c136ac9f":"kmean = KMeans(n_clusters = 3, random_state=101)                        # Number of clsuters is 5\n\nkmean_pred = kmean.fit_predict(X)                                       # Predecting the output","52daaca6":"cluster = kmean.labels_                                                 # getting the values\ncluster1 = pd.DataFrame(data = cluster, columns = ['Cluster'])          # dataframe for clusters\ncluster1_sample=cluster1.sample(frac=0.5,random_state=3)                # randomly selecting  few values\ncluster1_sample.head()                                                  # previweing ","a7f1268a":"centers = kmean.cluster_centers_                                        # getting centers of final cluster\ncenters  ","81e9141e":"df['cluster'] = cluster1                                                # Adding clusters column to the dataset\ndf_sample=df.sample(frac=0.5,random_state=3)\ndf_sample.head() ","e6e40d36":"                   #scatterplot for clusters 1, 2, 3\nplt.scatter(df.iloc[kmean_pred == 0, 0], df.iloc[kmean_pred == 0, 1], s = 10, c = 'blue', label = 'Cluster 1') \nplt.scatter(df.iloc[kmean_pred == 1, 0], df.iloc[kmean_pred == 1, 1], s = 10, c = 'orange', label = 'Cluster 2')\nplt.scatter(df.iloc[kmean_pred == 2, 0], df.iloc[kmean_pred == 2, 1], s = 10, c = 'green', label = 'Cluster 3')\nplt.scatter(kmean.cluster_centers_[:, 0], kmean.cluster_centers_[:, 1],  \n            s = 200, c = 'red', label = 'Centroids',marker = '*')                          # scatter plot for centroid\nplt.title('Clusters of seedtype')                                                          # Adding title \nplt.xlabel('Width Of Kernel')                                                              # Adding X-label\nplt.ylabel('Length Of Kernel')                                                             # Adding Y-label\nplt.legend(title = 'Seed Clusters')                                                        # Adding Legend\nplt.show()                                                                                 # previewing","2c19914a":"df.head()                                                     # previewing","0521e8c1":"df[\"cluster\"].replace({0: 1, 1: 2, 2:3}, inplace=True)        # replacing 0 with 1 , 1 with 2, 2 w1th 3\ndf[\"cluster\"].value_counts()                                  # printing the count of each clusters","8bc33fda":"df.head()                                                     # previewing","27d94636":"print(classification_report(df['cluster'],df['SeedType']))   # Printing the classification report of K Mean Cluster","e8901a0c":"kmeans_accuracy = accuracy_score(df['cluster'],df['SeedType'])     # storing Accuracy","8e62ed2b":"from sklearn.neighbors import KNeighborsClassifier                                      # importing KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)                                               # loading KNN with 5 neighbors\nknn.fit(X_trains,y_train)                                                               # fitting the data into model\n\nknn_pred = knn.predict(X_tests)                                                         # Predecting the test data output\n\nprint(f\"Classification Report of KNN : \\n\\n{classification_report(y_test,knn_pred)}\")   # Printing classification report","4774b1a5":"# preparing list of accuracies of all models\nAccuracies = [logistic_model.score(X_tests, y_test),kmeans_accuracy,\n              svm_model.score(X_tests, y_test),svm_accuracy,dtc_model.score(X_tests, y_test),knn.score(X_tests, y_test)]","a9f7f8ba":"Accuracies = np.array(Accuracies)                             # Numpy array of Accuracies\nAccuracies = np.round(Accuracies, decimals=2)                 # rounding Accuracies into 2 decimals\nAccuracies = np.multiply(Accuracies,100)                      # making them integer\nAccuracies                                                    # previewing","2065f87e":"# Making the DataFrame\nModels = ['Logistic regression','KMeans','SVM (Before Tuning)','SVM (After Tuning)','Decision Tree','KNN']\nEvaluate = pd.DataFrame()\nEvaluate['Classifier Name']= Models\nEvaluate['Accuracy'] = Accuracies\nEvaluate","27da97d2":"plt.figure(figsize=(10,6))                                                     # resizing the plot\nax = sns.barplot(x='Classifier Name',y='Accuracy',data=Evaluate)               # Bar plot for Accuracies and Model Names\nplt.ylim((0,110))                                                              # setting Y-axis limit\nfor index,data in enumerate(Accuracies):                                       # printing Accuracy on top of each respective bar\n    plt.text(x=index, y =data+1 , s=f\"{data}%\" , fontdict=dict(fontsize=10))  \nplt.tight_layout()\nlab = ax.get_xticklabels()                                                     # getting x-axis labels\nax.set_xticklabels(lab,rotation = 90)                                          # rotating them 90 degrees\nplt.show()                                                                     # showing the plot","5839addc":"print(\"\\n\\t\\t Confusion for Matrix KNN \")\nsns.heatmap(confusion_matrix(y_test, knn_pred), annot=True, cmap=\"YlGnBu\" ,fmt='g')  # confusion matrix heatmap of KNN\nplt.show()\nprint(\"\\n\\t Confusion Matrix for Logistic Regression\")\nsns.heatmap(confusion_matrix(y_test,logistic_pred),annot=True,cmap=\"YlGnBu\",fmt='g') # confusionmatrix heatmapLogisticRegression\nplt.show()\nprint(\"\\n\\t Confusion Matrix for Decesion Tree\")\nsns.heatmap(confusion_matrix(y_test, dtc_pred), annot=True, cmap=\"YlGnBu\" ,fmt='g')  # confusion matrix heatmap of Decesion Tree\nplt.show()\nprint(\"\\n\\t Confusion Matrix for K MEAN Clustering\")                                 # confusionmatrix heatmap K MEAN Clustering\nsns.heatmap(confusion_matrix(df['cluster'], df['SeedType']), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.show()","81adf325":"print(\"\\n \\tConfusion Matrix for SVM (Before Tuning)\")                            # confusionmatrix heatmap SVM (Before Tuning)\nsns.heatmap(confusion_matrix(y_test, svm_pred), annot=True, cmap=\"YlGnBu\" ,fmt='g') \nplt.show() \nprint(\"\\n \\t Confusion Matrix for SVM (After Tuning)\")                            # confusionmatrix heatmap SVM (After Tuning)\nsns.heatmap(confusion_matrix(y_test,grid_predictions), annot=True, cmap=\"YlGnBu\" ,fmt='g') \nplt.show() ","8cb4d208":"Actual = np.array(y_test)                               # Actual Output Array\n\npredicted_knn = np.array(knn_pred)                      # KNN predection Array\npredicted_logistic = np.array(logistic_pred)            # logistic Regression Predection Array\npredicted_svm = np.array(svm_pred)                      # SVM predection Array\npredicted_dtc = np.array(dtc_pred)                      # Decision tree predection Array\n\nstatus_knn = []                                         # KNN status Array\nstatus_logistic = []                                    # Logistic Regression status Array\nstatus_svm = []                                         # SVM status Array\nstatus_dtc = []                                         # Decision tree status Array\n\ni = 0\nfor i in range(len(Actual)):                            # for every predection in original output array,\n    if Actual[i] == predicted_knn[i]:                   # comparing original predection with KNN predection\n        status_knn.append(\"Matched\")                    # if matched, adding \"MATCHED\" in KNN status array\n    else:\n        status_knn.append(\"NOT MATCHED\")                # if not matched, adding \"NOT MATCHED\" in KNN status array\n        \n      \n    if Actual[i] == predicted_logistic[i]:              # comparing actual predection with logistic Regression Predection\n        status_logistic.append(\"Matched\")               # if matched, adding \"MATCHED\" in logistic Regression status array\n    else: \n        status_logistic.append(\"NOT MATCHED\")           # if not matched,adding \"NOTMATCHED\" in logistic Regression status array\n        \n   \n    if Actual[i] == predicted_svm[i]:                   # comparing original predection with SVM predection\n        status_svm.append(\"Matched\")                    # if matched, adding \"MATCHED\" in SVM status array\n    else:\n        status_svm.append(\"NOT MATCHED\")                # if not matched, adding \"NOT MATCHED\" in SVM status array\n        \n        \n    if Actual[i] == predicted_dtc[i]:                   # comparing original predection with Decesion Tree predection\n        status_dtc.append(\"Matched\")                    # if matched, adding \"MATCHED\" in Decesion Tree status array\n    else: \n        status_dtc.append(\"NOT MATCHED\")                # if not matched, adding \"NOT MATCHED\" in Decesion Tree status array\n         \nstatus_knn = np.array(status_knn)                       # KNN status Numpy Array\nstatus_logistic = np.array(status_logistic)             # Logistic Regression status Numpy Array\nstatus_svm = np.array(status_svm)                       # SVM status Numpy Array\nstatus_dtc = np.array(status_dtc)                       # Decession Tree status Numpy Array\n        \narr = np.array([Actual ,predicted_knn,status_knn, predicted_logistic,status_logistic, \n                predicted_svm,status_svm, predicted_dtc,status_dtc])     \nnew_df = pd.DataFrame(arr)                               # Making the new dataframe \nnew_df = new_df.T                                        # Transpse of the new data frame\nnew_df.columns =['Actual Label', 'KNN Predection','KNN Status','LR Predection',\n                 'Logistic Status','SVM Predection ','SVM Status','DTC Predection',\n                 'DTC Status']  \nnew_df = new_df.sample(frac=0.5,random_state=7)          # Taking a random sample for head()\nnew_df.head()                                            # previewing","4477acd0":"Evaluate.sort_values(by='Accuracy', ascending=False)","44d72a62":"BY : **<a href = \"https:\/\/tinyurl.com\/manikumaradapala\" > MANI KUMAR ADAPALA <\/a>** with <a href = \"https:\/\/archive.ics.uci.edu\/ml\/datasets\/seeds\" style=\"color: green\" > UCI source for wheat seed dataset <\/a>","420b90ed":"**Correlation Matrix**","fdd02419":"- From the above histograph, I can observe the occurrence of each parameter and therir signifiance.","46cb0093":"#### Exploratory Data Analysis","dfe70596":"# Wheat Seed Quality Detection using AIML Models","0677fde2":"- from the above density plot I can observe the density relation of LengthOfKernelGroove and LengthOfKernel. At starting they are propotional to each other. But from the they were dis-propotional.","f782f7aa":"**KNN**","a06e8139":"**Logistic Regression**","09a0a365":"**Visually compare the performance of all classifiers**","67dd2c70":"**Adding Column names**","67b8a482":"The measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and 'GRAINS' package were used to construct all seven, real-valued attributes.","04c4c376":"- From the above pairplot I can observe the characterstics of each parameter with all the other parameters in the dataframe.","ff93b2c3":"**Checking For Missing Data**","d741eb80":"**Attribute Information :**","6e735a14":"**Visual Data Analysis**","43b601cd":"- From this correlation matrix I can observe that how each parameters are related with each other parameters.","4b4c059c":"**This dataframe contains the comparision of all classifiers predection with Actual label (Output) and their status (MATCHED\/NOT MATCHED) and prints the randomly selected 5 values as head of dataframe**","2da29dc2":"**Scaling the data is important if we are handling huge data while using regression algorithms inorder to avoid the max_iter limit error\/warning**","9037b6db":"**SVM**","ae8b2d28":"Based on the above implementation and performance of various classifiers, I can say that the use of ***Logistic regression*** ,***Decision Tree*** , and ***SVM Model*** are best than that of ***K Means Clustering*** and ***KNN*** algorithms, as they have the grater accuracy than remaining, the remaining are also good in accuracy we can expect the desired output form them too. Grater the accuracy, better the model performance. The following dataframe shows the accuracies of all models used here.","e978b0ed":"The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.","7cf9be62":"**Splitting Data**","023226b4":"**Decision Tree Classifier**","897e9f17":"- scatter plot (WidthOfKernel vs LengthOfKernel) and colour code the scatter plot as per the clustered seedtypes","4d87b5f3":"- From the above histograph I can observe the relation between the compactness and different seed type. Seed type 1 has has heighest compacness and Seed type 3 has the least.","a41f3965":"**Tuning**","7ac5e816":"**Furture Scope of Improvement in the algorithms**","d3533ea4":"**K Means Clustering** <br>\n- Elbow Plot","16286eed":"**Abstract :**","bef3384a":"### The Best Fit Model","ff0f7833":"To construct the data, seven geometric parameters of wheat kernels were measured:\n\n    1. area A\n    2. perimeter P\n    3. compactness C = 4*pi*A\/P^2\n    4. length of kernel\n    5. width of kernel\n    6. asymmetry coefficient\n    7. length of kernel groove.\n    8. Seed Type\nAll of these parameters were real-valued continuous.","b1d0d0f4":"- from the above graph I can observe that seeds of type 2 are hingh in number and next, seeds of type 3 and 1 are in next places.","6421f5dd":"### Overall Inferences forom the visualized data analysis: \n-  I can observe that seeds of type 2 are hingh in number and next, seeds of type 3 and 1 are in next places.\n-  I can observe the density relation of LengthOfKernelGroove and LengthOfKernel. At starting they are propotional to each    other. But from the they were dis-propotional.\n-  I can observe the relation between the compactness and different seed type. Seed type 1 has has heighest compacness and Seed type 3 has the least.\n-  I can Observe that seeds with low area and low perimeter and low Length Of Kernel Groove are mostly of seed type 3 \n-  I can Observe that seeds with high area and high perimeter and high Length Of Kernel are mostly of seed type 2 \n-  I can Observe that seeds with average area and average perimeter are mostly of seed type 1","14ab8ebd":"Improvement in dataset, i.e, a new dataset that deals with the pictures of classification of seeds can increase the accuracy. we can further develop this by using Open CV, electronic microscope connected to computer via Deep Learning algorithms is one of the best possible ways to identify the type of seed in real time scenario.","b91d3f51":"**END**","21595960":"**Data Set Information :**","05eb18bf":"**Reading CSV**"}}