{"cell_type":{"50710183":"code","ad4df7c2":"code","4c5faeb7":"code","61ed6d03":"code","ee0a074a":"code","30e09016":"code","987e2e22":"code","c929c7c1":"code","920c1330":"code","2cbe97f7":"code","103fad63":"code","a14293e1":"code","3fad950f":"code","8ffb1902":"code","163b76b6":"code","d19a18d0":"code","eb7c4f28":"code","7bbef31d":"code","c9dd45f6":"code","475e88b5":"code","477445c6":"code","c84bf935":"code","198419d3":"code","38541614":"code","71c2566f":"code","69184362":"code","68b4f4c6":"code","e78e0d67":"code","031da33a":"code","5112ebad":"code","61017ab8":"code","5953c01e":"code","c03e67ac":"code","5e00f6e9":"code","0ce08518":"code","23607d28":"code","54a308b7":"code","826dbb9f":"markdown","400b81d4":"markdown","295a5139":"markdown","85095982":"markdown","3ad782c8":"markdown","af2eca5a":"markdown","c6d40c88":"markdown","25e12eb5":"markdown","b8f74948":"markdown","81c7d80f":"markdown","b842b521":"markdown","9ccc4757":"markdown","6329c9d1":"markdown","d055de64":"markdown","7556219d":"markdown","3a1ad22f":"markdown","dca9a226":"markdown","93651921":"markdown","0481f173":"markdown","132c909d":"markdown","f8a5f66e":"markdown","f03af9cd":"markdown","816dfd0f":"markdown","199135d2":"markdown","ad71cfe4":"markdown","b37c21e3":"markdown","c463cd65":"markdown","68b667f0":"markdown","0383fd5e":"markdown","232e213f":"markdown","fb43f1b0":"markdown","edf9cf6b":"markdown"},"source":{"50710183":"# we can start by creating a paragraph of text:\npara = \"Hello World. It's good to see you. Thanks for buying this book.\"\n\nfrom nltk.tokenize import sent_tokenize\nsent_tokenize(para)","ad4df7c2":"import nltk.data\ntokenizer = nltk.data.load('..\/tokenizers\/punkt\/english.pickle')\ntokenizer.tokenize(para)","4c5faeb7":"spanish_tokenizer = nltk.data.load('tokenizers\/punkt\/spanish.pickle')\nspanish_tokenizer.tokenize('Hola amigo. Estoy bien.')","61ed6d03":"from nltk.tokenize import word_tokenize\nword_tokenize('Hello World.')","ee0a074a":"from nltk.tokenize import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\ntokenizer.tokenize('Hello World.')","30e09016":"from nltk.tokenize import WordPunctTokenizer\ntokenizer = WordPunctTokenizer()\ntokenizer.tokenize(\"Can't is a contraction.\")","987e2e22":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[\\w']+\")\ntokenizer.tokenize(\"Can't is a contraction.\")","c929c7c1":"from nltk.tokenize import regexp_tokenize\nregexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")","920c1330":"tokenizer = RegexpTokenizer('\\s+', gaps=True)\ntokenizer.tokenize(\"Can't is a contraction.\")","2cbe97f7":"from nltk.tokenize import PunktSentenceTokenizer\nfrom nltk.corpus import webtext\ntext = webtext.raw('overheard.txt')\nsent_tokenizer = PunktSentenceTokenizer(text)","103fad63":"sents1 = sent_tokenizer.tokenize(text)\nsents1[0]","a14293e1":"from nltk.tokenize import sent_tokenize\nsents2 = sent_tokenize(text)\nsents2[0]","3fad950f":"sents1[678]","8ffb1902":"sents2[678]","163b76b6":"from nltk.corpus import stopwords\nenglish_stops = set(stopwords.words('english'))\nwords = [\"Can't\", 'is', 'a', 'contraction']\n[word for word in words if word not in english_stops]","d19a18d0":"from nltk.corpus import wordnet\nsyn = wordnet.synsets('cookbook')[0]\nsyn.name()","eb7c4f28":"syn.definition()","7bbef31d":"from nltk.corpus import wordnet\nsyn = wordnet.synsets('cookbook')[0]\nlemmas = syn.lemmas()\nlen(lemmas)","c9dd45f6":"lemmas[0].name()","475e88b5":"lemmas[1].name()","477445c6":"lemmas[0].synset() == lemmas[1].synset()","c84bf935":"[lemma.name() for lemma in syn.lemmas()]","198419d3":"synonyms = []\nfor syn in wordnet.synsets('book'):\n    for lemma in syn.lemmas():\n        synonyms.append(lemma.name())\nlen(synonyms)","38541614":"len(set(synonyms))","71c2566f":"gn2 = wordnet.synset('good.n.02')\ngn2.definition()","69184362":"evil = gn2.lemmas()[0].antonyms()[0]\nevil.name","68b4f4c6":"evil.synset().definition()","e78e0d67":"ga1 = wordnet.synset('good.a.01')\nga1.definition()","031da33a":"bad = ga1.lemmas()[0].antonyms()[0]\nbad.name()","5112ebad":"bad.synset().definition()","61017ab8":"from nltk.corpus import wordnet\ncb = wordnet.synset('cookbook.n.01')\nib = wordnet.synset('instruction_book.n.01')\ncb.wup_similarity(ib)","5953c01e":"ref = cb.hypernyms()[0]\ncb.shortest_path_distance(ref)","c03e67ac":"ib.shortest_path_distance(ref)","5e00f6e9":"cb.shortest_path_distance(ib)","0ce08518":"from nltk.corpus import webtext\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\nwords = [w.lower() for w in webtext.words('grail.txt')]\nbcf = BigramCollocationFinder.from_words(words)\nbcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)","23607d28":"from nltk.corpus import stopwords\nstopset = set(stopwords.words('english'))\nfilter_stops = lambda w: len(w) < 3 or w in stopset\nbcf.apply_word_filter(filter_stops)\nbcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)","54a308b7":"from nltk.collocations import TrigramCollocationFinder\nfrom nltk.metrics import TrigramAssocMeasures\nwords = [w.lower() for w in webtext.words('singles.txt')]\ntcf = TrigramCollocationFinder.from_words(words)\ntcf.apply_word_filter(filter_stops)\ntcf.apply_freq_filter(3)\ntcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)","826dbb9f":"Now we fnally have something that can treat contractions as whole words, instead of splitting\nthem into tokens.\n#### How it works...\nThe `RegexpTokenizer` class works by compiling your pattern, then calling `re.findall()`\non your text. You could do all this yourself using the re module, but `RegexpTokenizer`\nimplements the `TokenizerI interface`, just like all the word tokenizers from the previous\nrecipe. This means it can be used by other parts of the NLTK package, such as corpus\nreaders. Many corpus readers need a way to tokenize the text they're reading, and can take optional keyword\narguments specifying an instance of a **TokenizerI** subclass. This way, you have the ability\nto provide your own tokenizer instance if the default tokenizer is unsuitable.\n#### There's more...\n`RegexpTokenizer` can also work by matching the gaps, as opposed to the tokens. Instead\nof using `re.findall()`, the `RegexpTokenizer` class will use `re.split()`. This is how the\n`BlanklineTokenizer` class in `nltk.tokenize` is implemented.\n#### Simple whitespace tokenizer\nThe following is a simple example of using RegexpTokenizer to tokenize on whitespace:","400b81d4":"# NLP Tokenizing Text and WordNet Basics with NLTK\n\n## Introduction.\n**Natural Language ToolKit (NLTK)** is a comprehensive Python library for natural language\nprocessing and text analytics. Originally designed for teaching, it has been adopted in the\nindustry for research and development due to its usefulness and breadth of coverage. NLTK\nis often used for rapid prototyping of text processing programs and can even be used in\nproduction applications. Demos of select NLTK functionality and production-ready APIs are\navailable at http:\/\/text-processing.com.\n\nThis Notebook will cover the basics of tokenizing text and using WordNet. \n1. **Tokenization** is a method of breaking up a piece of text into many pieces, such as sentences and words, and is an essential first step for NLP project. **Tokenization** is the process of splitting a string into a list of pieces or tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph.\n2. **WordNet** is a dictionary designed for programmatic access by natural language processing systems. It has many different use cases, including:\n\n        - Looking up the defnition of a word\n        - Finding synonyms and antonyms\n        - Exploring word relations and similarity\n        - Word sense disambiguation for words that have multiple uses and defnitions\n\nNLTK includes a WordNet corpus reader, which we will use to access and explore WordNet.\n**A corpus** is just a body of text, and corpus readers are designed to make accessing a corpus\nmuch easier than direct fle access.","295a5139":"## 7. Looking up lemmas and synonyms in WordNet\nBuilding on the previous recipe, we can also look up **lemmas** in WordNet to fnd synonyms\nof a word. **A lemma (in linguistics)**, is the canonical form or morphological form of a word.\n#### How to do it...\nIn the following code, we'll fnd that there are two lemmas for the cookbook Synset using the\n`lemmas()` method:","85095982":"In this Notebook, we will cover the following recipes:\n1. Tokenizing text into sentences\n2. Tokenizing sentences into words\n3. Tokenizing sentences using regular expressions\n4. Training a sentence tokenizer\n5. Filtering stopwords in a tokenized sentence\n6. Looking up Synsets for a word in WordNet\n7. Looking up lemmas and synonyms in WordNet\n8. Calculating WordNet Synset similarity\n9. Discovering word collocations","3ad782c8":"#### Tokenizing sentences in other languages\nIf you want to tokenize sentences in languages other than English, you can load one of the\nother pickle fles in `tokenizers\/punkt\/PY3` and use it just like the English sentence\ntokenizer. Here's an example for Spanish:\n\nYou can see a list of all the available language tokenizers in `\/usr\/share\/nltk_data\/\ntokenizers\/punkt\/PY3 (or C:\\nltk_data\\tokenizers\\punkt\\PY3)` in your local machine.","af2eca5a":"#### WordPunctTokenizer\nAnother alternative word tokenizer is `WordPunctTokenizer`. It splits all punctuation into\nseparate tokens:\n","c6d40c88":"#### There's more...\nIgnoring the obviously named `WhitespaceTokenizer` and `SpaceTokenizer`, there are two other word tokenizers worth looking at: `PunktWordTokenizer` and `WordPunctTokenizer`.\nThese differ from `TreebankWordTokenizer` by how they handle punctuation and\ncontractions, but they all inherit from `TokenizerI`. The inheritance tree looks like what's\nshown in the following diagram:\n![nltk_Tok.PNG](attachment:nltk_Tok.PNG)\n","25e12eb5":"So `cookbook` and `instruction_book` must be very similar, because they are only one step\naway from the same **reference_book hypernym**, and, therefore, only two steps away from\neach other.\n\n## 9. Discovering word collocations\n**Collocations** are two or more words that tend to appear frequently together, such as United\nStates. Of course, there are many other words that can come after United, such as United\nKingdom and United Airlines. As with many aspects of NLP, context is very important. And for collocations, context is everything! \n\nIn the case of collocations, the context will be a document in the form of a list of words.\nDiscovering collocations in this list of words means that we'll fnd common phrases that occur frequently throughout the text. For fun, we'll start with the script for Monty Python\nand the Holy Grail.\n#### Getting ready\nThe script for Monty Python and the Holy Grail is found in the webtext corpus, so be sure\nthat it's unzipped at `nltk_data\/corpora\/webtext\/`.\n#### How to do it...\nWe're going to create a list of all lowercased words in the text, and then produce `BigramCollocationFinder`, which we can use to fnd **bigrams**, which are pairs of words. These bigrams are found using association measurement functions in the `nltk.metrics` package, as follows:","b8f74948":"#### How it works...\nAs you can see, `cookery_book` and `cookbook` are two distinct **lemmas** in the same **Synset**.\nIn fact, a lemma can only belong to a single Synset. In this way, a Synset represents a group\nof lemmas that all have the same meaning, while a lemma represents a distinct word form.\n#### There's more...\nSince all the lemmas in a Synset have the same meaning, they can be treated as synonyms.\nSo if you wanted to get all **synonyms** for a Synset, you could do the following:","81c7d80f":"The `antonyms()` method returns a list of **lemmas**. In the frst case, as we can see in the\nprevious code, the second Synset for good as a noun is defned as moral excellence,\nand its frst antonym is evil, defned as morally wrong. In the second case, when good is\nused as an adjective to describe positive qualities, the frst antonym is bad, which describes\nnegative qualities.","b842b521":"Much better, we can clearly see four of the most common bigrams in Monty Python and the\nHoly Grail. If you'd like to see more than four, simply increase the number to whatever you\nwant, and the collocation fnder will do its best.\n#### How it works...\n`BigramCollocationFinder` constructs two frequency distributions: one for each word,\nand another for bigrams. **A frequency distribution**, or **FreqDist** in NLTK, is basically an\nenhanced Python dictionary where the keys are what's being counted, and the values are\nthe counts. Any fltering functions that are applied reduce the size of these two FreqDists\nby eliminating any words that don't pass the flter. By using a fltering function to eliminate all\nwords that are one or two characters, and all English stopwords, we can get a much cleaner\nresult. After fltering, the collocation fnder is ready to accept a generic scoring function for\nfnding collocations.\n#### There's more...\nIn addition to `BigramCollocationFinder`, there's also `TrigramCollocationFinder`,\nwhich fnds triplets instead of pairs. This time, we'll look for trigrams in Australian singles\nadvertisements with the help of the following code:","9ccc4757":"So now we have a list of sentences that we can use for further processing.\n#### How it works...\nThe sent_tokenize function uses an instance of `PunktSentenceTokenizer` from the\n`nltk.tokenize.punkt` module. This instance has already been trained and works well for\nmany European languages. So it knows what punctuation and characters mark the end of a\nsentence and the beginning of a new sentence.\n\n#### There's more...\nThe instance used in `sent_tokenize()` is actually loaded on demand from a pickle file. So if you're going to be tokenizing a lot of sentences, it's more effcient to load the `PunktSentenceTokenizer` class once, and call its `tokenize()` method instead:","6329c9d1":"## 3. Tokenizing sentences using regular expressions\nRegular expressions can be used if you want complete control over how to tokenize text.\nAs regular expressions can get complicated very quickly, I only recommend using them if\nthe word tokenizers covered in the previous recipe are unacceptable.\n#### Getting ready\nFirst you need to decide how you want to tokenize a piece of text as this will determine how\nyou construct your regular expression. The choices are:\n* Match on the tokens\n* Match on the separators or gaps\nWe'll start with an example of the frst, matching alphanumeric tokens plus single quotes so\nthat we don't split up contractions.\n#### How to do it...\n\nWe'll create an instance of `RegexpTokenizer`, giving it a regular expression string to use for\nmatching tokens:","d055de64":"While the frst sentence is the same, you can see that the tokenizers disagree on how to\ntokenize sentence 679 (this is the frst sentence where the tokenizers diverge). The default\ntokenizer includes the next line of dialog, while our custom tokenizer correctly thinks that\nthe next line is a separate sentence. This difference is a good demonstration of why it can\nbe useful to train your own sentence tokenizer, especially when your text isn't in the typical\nparagraph-sentence structure.\n#### How it works...\nThe `PunktSentenceTokenizer` class uses an **unsupervised learning algorithm** to learn\nwhat constitutes a sentence break. It is unsupervised because you don't have to give it any\nlabeled training data, just raw text. You can read more about these kinds of algorithms at\nhttps:\/\/en.wikipedia.org\/wiki\/Unsupervised_learning. The specifc technique used in this case is called ***sentence boundary detection*** and it works by counting punctuation and tokens that commonly end a sentence, such as a period or newline, then using the resulting frequencies to decide what the sentence boundaries should actually look like.\nThis is a simplifed description of the algorithm\u2014if you'd like more details, take a look\nat the source code of the nltk.tokenize.punkt.PunktTrainer class, which can be found online at http:\/\/www.nltk.org\/_modules\/nltk\/tokenize\/punkt.html#PunktSentenceTokenizer","7556219d":"There's also a simple helper function you can use if you don't want to instantiate the class,\nas shown in the following code:","3a1ad22f":"#### How it works...\nThe `word_tokenize()` function is a wrapper function that calls `tokenize()` on an instance of the `TreebankWordTokenizer` class. It works by separating words using spaces and punctuation. And as you can see, it does not discard the punctuation, allowing you to decide what to do with it. It's equivalent to the following code:","dca9a226":"#### All possible synonyms:\n\nAs mentioned earlier, many words have multiple Synsets because the word can have different\nmeanings depending on the context. But, let's say you didn't care about the context, and\nwanted to get all the possible synonyms for a word:","93651921":"## 5. Filtering stopwords in a tokenized sentence\n**Stopwords** are common words that generally do not contribute to the meaning of a sentence,\nat least for the purposes of information retrieval and natural language processing. These are\nwords such as the and a. Most search engines will flter out stopwords from search queries and documents in order to save space in their index.\n#### Getting ready\nNLTK comes with a stopwords corpus that contains word lists for many languages. Be sure to unzip the data fle, so NLTK can fnd these word lists at `nltk_data\/corpora\/stopwords\/`\n\n#### How to do it...\nWe're going to create a set of all English stopwords, then use it to flter stopwords from a sentence with the help of the following code:","0481f173":"Let's compare the results to the default sentence tokenizer, as follows:","132c909d":"Now, we don't know whether people are looking for a long-term relationship or not, but clearly\nit's an important topic. In addition to the stopword flter, I also applied a frequency flter, which removed any trigrams that occurred less than three times. This is why only one result was\nreturned when we asked for four because there was only one result that occurred more than two times.","f8a5f66e":"## 4. Training a sentence tokenizer\nNLTK's default sentence tokenizer is general purpose, and usually works quite well. But\nsometimes it is not the best choice for your text. Perhaps your text uses nonstandard\npunctuation, or is formatted in a unique way. In such cases, training your own sentence\ntokenizer can result in much more accurate sentence tokenization.\n#### Getting ready\nFor this example, we'll be using the webtext corpus, specifcally the overheard.txt file,\nso make sure you've downloaded this corpus. The text in this file is formatted as dialog that\nlooks like this:\n\n    White guy: So, do you have any plans for this evening?\n    Asian girl: Yeah, being angry!\n    White guy: Oh, that sounds good.\n    \nAs you can see, this isn't your standard paragraph of sentences formatting, which makes\nit a perfect case for training a sentence tokenizer.\n#### How to do it...\nNLTK provides a `PunktSentenceTokenizer` class that you can train on raw text to produce\na custom sentence tokenizer. You can get raw text either by reading in a fle, or from an NLTK\ncorpus using the `raw()` method. Here's an example of training a sentence tokenizer on dialog\ntext, using `overheard.txt` from the webtext corpus:","f03af9cd":"## 1. Tokenizing text into sentences\nWe'll start with sentence tokenization, or splitting a paragraph into a list of sentences.\n","816dfd0f":"#### How it works...\nThe **stopwords corpus** is an instance of `nltk.corpus.reader.WordListCorpusReader`. As such, it has a words() method that can take a single argument for the fle ID, which in this case is 'english', referring to a fle containing a list of **English stopwords**. You could also call `stopwords.words()` with no argument to get a list of all stopwords in every language available.\n\n## 6. Looking up Synsets for a word in WordNet\n**WordNet** is a lexical database for the English language. In other words, it's a dictionary\ndesigned specifcally for natural language processing. NLTK comes with a simple interface to look up words in WordNet. What you get is a list of **Synset** instances, which are groupings of synonymous words that express the same concept. Many words have only one Synset, but some have several. In this recipe, we'll explore a single Synset, and in the next recipe, we'll look at several in more detail.\n#### Getting ready\nBe sure you've unzipped the wordnet corpus at `nltk_data\/corpora\/wordnet`. This will allow `WordNetCorpusReader to access it`.\n#### How to do it...\nNow we're going to look up the Synset for cookbook, and explore some of the properties and methods of a Synset using the following code:","199135d2":"#### Hope that this notebook help you.\n#### Thanks for your appreciations and suggestions to motivate me to doing better.","ad71cfe4":"As you can see, there appears to be 38 possible synonyms for the word 'book'. But in\nfact, some synonyms are verb forms, and many synonyms are just different usages of\n'book'. If, instead, we take the set of synonyms, there are fewer unique words, as shown\nin the following code:","b37c21e3":"#### Antonyms\nSome lemmas also have antonyms. The word good, for example, has 27 Synsets, fve\nof which have lemmas with antonyms, as shown in the following code:","c463cd65":"Well, that's not very useful! Let's refne it a bit by adding a word flter to remove punctuation\nand stopwords:","68b667f0":"So they are over 91% similar!\n#### How it works...\nThe `wup_similarity` method is short for **Wu-Palmer Similarity**, which is a scoring method\nbased on how similar the word senses are and where the Synsets occur relative to each other\nin the **hypernym tree**. One of the core metrics used to calculate similarity is the shortest path\ndistance between the two Synsets and their common hypernym:","0383fd5e":"### References:\n* **Python3 Text Processing with NLTK 3 Cookbook**, *By Jacob Perkins, Second edition: August 2014.*\n* http:\/\/www.nltk.org\/","232e213f":"#### How it works...\nYou can look up any word in WordNet using `wordnet.synsets(word)` to get a list of **Synsets**. The list may be empty if the word is not found. The list may also have quite a few elements, as some words can have many possible meanings, and, therefore, many Synsets.","fb43f1b0":"In the next recipe, we'll learn how to split sentences into individual words. After that, we'll\ncover how to use regular expressions to tokenize text. We'll cover how to train your own\nsentence tokenizer in an upcoming recipe, Training a sentence tokenizer.\n## 2. Tokenizing sentences into words\nIn this recipe, we'll split a sentence into individual words. The simple task of creating a list of\nwords from a string is an essential part of all text processing.\n#### How to do it...\nBasic word tokenization is very simple; use the `word_tokenize()` function:","edf9cf6b":"## 8. Calculating WordNet Synset similarity\nSynsets are organized in a **hypernym** tree. This tree can be used for reasoning about the similarity between the Synsets it contains. The closer the two Synsets are in the tree, the more similar they are.\n#### How to do it...\nIf you were to look at all the **hyponyms of reference_book** (which is the hypernym of cookbook), you'd see that one of them is instruction_book. This seems intuitively very similar to a cookbook, so let's see what **WordNet similarity** has to say about it with the help of the following code:"}}