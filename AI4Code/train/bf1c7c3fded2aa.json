{"cell_type":{"abd7a585":"code","1322199e":"code","37fcd3a9":"code","46416103":"code","03f97d67":"code","5ca1ad33":"code","54c70721":"code","a91d23df":"code","686f1ad1":"code","8b91d61d":"code","66923acd":"code","ee2e8817":"code","1d18056b":"code","85c99f76":"code","1215e26f":"code","dc5e5eba":"code","7d8e09d7":"code","8eb3cde4":"code","79d1caae":"code","21f4ca35":"code","aa01f921":"code","09686f8e":"code","b2f3dc9d":"code","673501d0":"code","eb8da888":"code","ec57cca4":"code","1bfe3199":"code","2b08269c":"code","9e812c56":"code","7d3b0d92":"code","18966659":"code","898c440e":"code","e9c3289f":"code","0fbccb72":"code","ed3644ca":"code","80fad8f5":"code","b8385a9e":"code","5683a4ea":"code","9d5a8ba6":"code","cedb59bb":"code","e9f97cff":"code","22f203d4":"code","f58eaf4a":"code","d38c0cf6":"code","afc56c6c":"code","888c8672":"code","58f39bf7":"code","b2f3ac95":"code","97f0a953":"code","fc16ceee":"code","f333d32c":"code","78cff195":"code","392031c2":"code","2eb5e645":"code","b57b280a":"code","85d6fa48":"code","1cdef9cd":"code","50fc1f86":"code","3b81802a":"code","fcbea816":"code","3337f080":"code","3a15083b":"code","5bfccd2a":"markdown","428c2992":"markdown","d00f27c9":"markdown","5f9fdfe5":"markdown","c6c667f8":"markdown","a4ed0f4a":"markdown","7459b54b":"markdown","6e1b3087":"markdown","062f66ed":"markdown","5760beba":"markdown","93e09292":"markdown","faf48fc9":"markdown","06bb9df2":"markdown","255bdff7":"markdown","ccc9185c":"markdown","07ca1401":"markdown","fa727323":"markdown","bd766e05":"markdown","df2b5326":"markdown","bffab765":"markdown","0accccf5":"markdown","7b9f3da6":"markdown","42686d45":"markdown","b09cabee":"markdown","4793200f":"markdown","05716880":"markdown","a2fffabf":"markdown","9062c642":"markdown","73bf44cf":"markdown","3b851094":"markdown","c2bc90c6":"markdown","c4fd27e6":"markdown","54caa7d8":"markdown","19677c3d":"markdown","a8bd6e21":"markdown","9c9a5760":"markdown","15aa36fe":"markdown"},"source":{"abd7a585":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1322199e":"#Importing relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","37fcd3a9":"IN = pd.read_csv(\"..\/input\/youtube-new\/INvideos.csv\")","46416103":"IN","03f97d67":"IN.shape","5ca1ad33":"IN.info()","54c70721":"#Converting trending_date, publish_time to Datetime format\nIN[\"trending_date\"] = pd.to_datetime(IN[\"trending_date\"],format=\"%y.%d.%m\")\nIN[\"publish_time\"] = pd.to_datetime(IN[\"publish_time\"])","a91d23df":"IN.describe().apply(lambda s: s.apply(lambda x: format(x, \"f\")))","686f1ad1":"IN.isnull().sum()","8b91d61d":"IN[IN[\"description\"].isnull()]","66923acd":"IN.loc[IN[\"description\"].isnull(),\"description\"] = \" \"","ee2e8817":"IN[\"tags\"] = IN[\"tags\"].str.replace('|',\",\").str.replace('\"',\"\")","1d18056b":"IN_Category = pd.read_json(\"..\/input\/youtube-new\/IN_category_id.json\")","85c99f76":"ID_G = []\nkeys = []\nitems = []\nfor i in list(IN_Category[\"items\"]):\n    ID = i[\"id\"]\n    genre = i[\"snippet\"][\"title\"]\n    ID_G.append((ID,genre))","1215e26f":"ID_G.append(('29','Events and Motivational'))","dc5e5eba":"IN[\"video_type\"]= [dict(ID_G)[x] for x in list(map(str,IN[\"category_id\"]))]","7d8e09d7":"#Here, we could've used video_id. But we see that video_id of more that 500 records is missing.\n#So, it would be wise to use title instead.\nIN[\"trending_since(in days)\"] = [len(IN[IN[\"title\"]==i]) for i in IN[\"title\"]]","8eb3cde4":"#Records contain everyday updates of each trending video. We will drop all the duplicate records and keep the last one since, it contains total number of likes, dislikes, views,and comm\nIN = IN.drop_duplicates(subset=\"title\",keep=\"last\")","79d1caae":"IN","21f4ca35":"IN.reset_index(drop=True,inplace=True)","aa01f921":"IN[\"text\"] = IN[\"title\"]+\" \"+IN[\"tags\"]+\" \"+IN[\"description\"]","09686f8e":"IN[\"trending_month\"] = pd.DatetimeIndex(IN['trending_date']).month","b2f3dc9d":"IN.columns","673501d0":"cols = ['trending_date','trending_since(in days)', 'trending_month', 'title',\n        'channel_title', 'video_type', 'publish_time', 'tags', 'views',\n        'likes', 'dislikes', 'comment_count', 'thumbnail_link',\n        'comments_disabled', 'ratings_disabled', 'video_error_or_removed',\n        'description', 'text']","eb8da888":"IN = IN[cols]","ec57cca4":"df = IN.copy()","1bfe3199":"df","2b08269c":"df.describe()","9e812c56":"f, axes = plt.subplots(2,2,figsize=(12,9))\nsns.set_style(\"whitegrid\")\nf.suptitle('Histograms of Views,Likes,Dislikes,Comment Count.', fontsize=30)\n\nsns.distplot(df[\"views\"], ax=axes[0, 0])\naxes[0,0].set_title('Views',size=20)\n\nsns.distplot(df[\"likes\"], ax=axes[0, 1])\naxes[0,1].set_title('Likes',size=20)\n\nsns.distplot(df[\"dislikes\"], ax=axes[1, 0])\naxes[1,1].set_title('Dislikes',size=20)\n\nsns.distplot(df[\"comment_count\"], ax=axes[1, 1])\naxes[1,0].set_title('Comment Count',size=20)\n\nplt.show()","7d3b0d92":"#Creating a dictionary to get a DataFrame of the Quantiles\nd = dict([(f\"{i}\",np.quantile([df[f\"{i}\"]],[0,0.1,0.25,0.5,0.75,0.9,0.95,1])) for i in [\"views\",\"likes\",\"dislikes\",\"comment_count\"]])\npd.DataFrame(d , index = [0,0.1,0.25,0.5,0.75,0.9,0.95,1])","18966659":"plt.figure(figsize=(32,9))\nsns.barplot(df[\"video_type\"].unique(),df[\"video_type\"].value_counts(sort=False))\nplt.show()","898c440e":"plt.figure(figsize = (10,6))\n\nsns.countplot(x='comments_disabled', data=df)\nplt.title(\"Comments Disabled\", fontsize=20)\n\nplt.show()","e9c3289f":"plt.figure(figsize = (10,6))\n\nsns.countplot(x='ratings_disabled', data=df)\nplt.title(\"Ratings Disabled\", fontsize=20)\n\nplt.show()","0fbccb72":"plt.figure(figsize = (10,6))\n\nsns.countplot(x='video_error_or_removed', data=df)\nplt.title(\"Video Error or Removed\", fontsize=20)\n\nplt.show()","ed3644ca":"sns.pairplot(df.loc[:,[\"views\",\"likes\",\"dislikes\",\"comment_count\"]],height=3).fig.suptitle(\"Pair Plot\",y = 1.05, fontsize=50)\nplt.show()","80fad8f5":"plt.figure(figsize = (16,9))\nsns.heatmap(df.corr(), annot=True, cmap=\"viridis\")\nplt.show()","b8385a9e":"#List of all unique channel title\ncats = df[\"video_type\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([len(df[df[\"video_type\"]==i]) for i in df[\"video_type\"].unique()])\n#Soring by descending order and storing their index no.\nsort = cats[np.argsort(vals)[::-1]]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\",order=sort)\nplt.xlabel(\"Number of Trending Videos on YouTube\")\nplt.ylabel(\"Category of the Video\")\nplt.title(\"WHAT CATEGORY OF VIDEOS ARE MOST TRENDING IN INDIA ?\")\nplt.show()","5683a4ea":"#List of all unique channel title\ncats = df[\"video_type\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([df.groupby(\"video_type\").get_group(i)[\"views\"].sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of views on all videos of each Category\")\nplt.ylabel(\"Category Name\")\nplt.title(\"WHICH CATEGORY DO CANADIANS VIEW THE MOST ?\")\nplt.show()","9d5a8ba6":"#List of all unique channel title\ncats = df[\"video_type\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([(df.groupby(\"video_type\").get_group(i)[\"likes\"]-df.groupby(\"video_type\").get_group(i)[\"dislikes\"]).sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of likes on all videos of each category\")\nplt.ylabel(\"Category Name\")\nplt.title(\"WHICH CATEGORY DO INDIANS LIKE THE MOST ?\")\nplt.show()","cedb59bb":"#List of all unique channel title\ncats = df[\"video_type\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([df.groupby(\"video_type\").get_group(i)[\"comment_count\"].sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of comments on all videos of each Category\")\nplt.ylabel(\"Category Name\")\nplt.title(\"ON WHICH CATEGORY DO INDIANS COMMENT THE MOST ?\")\nplt.show()","e9f97cff":"#List of all unique channel title\ncats = df[\"video_type\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([(df.groupby(\"video_type\").get_group(i)[\"dislikes\"]-df.groupby(\"video_type\").get_group(i)[\"likes\"]).sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of dislikes on all videos of each Category\")\nplt.ylabel(\"Category Name\")\nplt.title(\"WHICH CATEGORY DO INDIAND DISLIKE THE MOST ?\")\nplt.show()","22f203d4":"#List of all unique channel title\ncats = df[\"channel_title\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([len(df[df[\"channel_title\"]==i]) for i in df[\"channel_title\"].unique()])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Number of Trending Videos on YouTube\")\nplt.ylabel(\"Channel Name\")\nplt.title(\"WHICH CHANNEL IS MOST TRENDING ON YOUTUBE IN INDIA ?\")\nplt.show()","f58eaf4a":"#List of all unique channel title\ncats = df[\"channel_title\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([df.groupby(\"channel_title\").get_group(i)[\"views\"].sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of views on all videos of each Channel\")\nplt.ylabel(\"Channel Name\")\nplt.title(\"WHICH CHANNEL DO INDIANS VIEW THE MOST ?\")\nplt.show()","d38c0cf6":"#List of all unique channel title\ncats = df[\"channel_title\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([(df.groupby(\"channel_title\").get_group(i)[\"likes\"]-df.groupby(\"channel_title\").get_group(i)[\"dislikes\"]).sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of likes on all videos of each Channel\")\nplt.ylabel(\"Channel Name\")\nplt.title(\"WHICH CHANNEL DO INDIANS LIKE THE MOST ?\")\nplt.show()","afc56c6c":"#List of all unique channel title\ncats = df[\"channel_title\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([df.groupby(\"channel_title\").get_group(i)[\"comment_count\"].sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of comments on all videos of each Channel\")\nplt.ylabel(\"Channel Name\")\nplt.title(\"ON WHICH CHANNEL DO INDIANS COMMENT THE MOST ?\")\nplt.show()","888c8672":"#List of all unique channel title\ncats = df[\"channel_title\"].unique()\n#Sum of Like received on each video of various channels\nvals = np.array([(df.groupby(\"channel_title\").get_group(i)[\"dislikes\"]-df.groupby(\"channel_title\").get_group(i)[\"likes\"]).sum() for i in cats])\n#Soring by descending order and storing their index no.\nsort = np.argsort(vals)[::-1]\n#Ordering the list of cats and vals according to the indexes stored and choosing top 25 values\ncats=cats[sort][:25]\nvals=vals[sort][:25]\n#Plotting the Bar Graph\nplt.figure(figsize=(16,9))\nsns.set(style=\"whitegrid\")\nsns.barplot(vals,cats,orient='h',palette=\"cool\")\nplt.xlabel(\"Total number of dislikes on all videos of each Channel\")\nplt.ylabel(\"Channel Name\")\nplt.title(\"WHICH CHANNEL DO INDIANS DISLIKE THE MOST ?\")\nplt.show()","58f39bf7":"from wordcloud import WordCloud, STOPWORDS\ndef PlotWordCloud(data,category):\n    \n    \n\n    text_words = '' \n    stopwords = set(STOPWORDS)\n\n    #Iterate through the csv file \n    for val in data[data[\"video_type\"]==category].sort_values(by=\"views\").reset_index(drop=True).loc[:100,\"text\"]:\n        \n        #Typecaste each val to string\n        val = str(val)\n        \n        #Split the value \n        tokens = val.split()\n        \n        #Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower()\n        \n        text_words += \" \".join(tokens)+\" \"\n    \n    text_words = text_words.replace(\"bit\",\" \").replace(\"http\",\" \").replace(\"https\",\" \").replace(\"com\",\" \").replace(\"youtube\",\" \").replace(\"gmail\",\" \").replace(\"ly\",\" \").replace(\"www\",\" \").replace(\"youtu\",\" \").replace(\"be\",\" \").replace(\"goo\",\" \")\n    \n    wordcloud1 = WordCloud(width = 1600, height = 900, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(text_words)\n\n    \n    \n    \n    \n    text_words = '' \n    stopwords = set(STOPWORDS)\n\n    #Iterate through the csv file \n    for val in data[data[\"video_type\"]==category].sort_values(by=\"likes\").reset_index(drop=True).loc[:100,\"text\"]:\n        \n        #Typecaste each val to string\n        val = str(val)\n        \n        #Split the value \n        tokens = val.split()\n        \n        #Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower()\n        \n        text_words += \" \".join(tokens)+\" \"\n    \n    text_words = text_words.replace(\"bit\",\" \").replace(\"http\",\" \").replace(\"https\",\" \").replace(\"com\",\" \").replace(\"youtube\",\" \").replace(\"gmail\",\" \").replace(\"ly\",\" \").replace(\"www\",\" \").replace(\"youtu\",\" \").replace(\"be\",\" \").replace(\"goo\",\" \")\n    \n    wordcloud2 = WordCloud(width = 1600, height = 900, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(text_words)\n\n    \n    \n    \n    \n    \n    text_words = '' \n    stopwords = set(STOPWORDS)\n\n    #Iterate through the csv file \n    for val in data[data[\"video_type\"]==category].sort_values(by=\"comment_count\").reset_index(drop=True).loc[:100,\"text\"]:\n        \n        #Typecaste each val to string\n        val = str(val)\n        \n        #Split the value \n        tokens = val.split()\n        \n        #Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower()\n        \n        text_words += \" \".join(tokens)+\" \"\n    \n    text_words = text_words.replace(\"bit\",\" \").replace(\"http\",\" \").replace(\"https\",\" \").replace(\"com\",\" \").replace(\"youtube\",\" \").replace(\"gmail\",\" \").replace(\"ly\",\" \").replace(\"www\",\" \").replace(\"youtu\",\" \").replace(\"be\",\" \").replace(\"goo\",\" \")\n    \n    wordcloud3 = WordCloud(width = 1600, height = 900, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(text_words)\n    \n    #Plot the WordCloud images\n    f, axarr = plt.subplots(3,1,figsize=(16,27))\n    f.suptitle(f'{category}', fontsize=50)\n    axarr[0].imshow(wordcloud1)\n    axarr[0].axis(\"off\")\n    axarr[0].set_title('On the basis of Views',size=30)\n    axarr[1].imshow(wordcloud2)\n    axarr[1].axis(\"off\")\n    axarr[1].set_title('On the basis of Likes',size=30)\n    axarr[2].imshow(wordcloud3)\n    axarr[2].axis(\"off\")\n    axarr[2].set_title('On the basis of Comment Count',size=30)","b2f3ac95":"df[\"video_type\"].value_counts()","97f0a953":"PlotWordCloud(df,\"Entertainment\")","fc16ceee":"PlotWordCloud(df,\"News & Politics\")","f333d32c":"PlotWordCloud(df,\"People & Blogs\")","78cff195":"PlotWordCloud(df,\"Music\")","392031c2":"PlotWordCloud(df,\"Comedy\")","2eb5e645":"#Creating a new dataframe that contains the log values of Views, Likes, Comment Count, Dislikes\nlog = pd.DataFrame({\"log views\":np.log(df['views']+1),\"log likes\":np.log(df['likes']+1),\"log dislikes\":np.log(df['dislikes']+1),\"log comments\":np.log(df['comment_count']+1),\"video_type\":df[\"video_type\"]})","b57b280a":"log","85d6fa48":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(log,height=7,aspect=32\/9).map(sns.boxplot,x=log[\"video_type\"],y=log[\"log views\"],order=log[\"video_type\"].unique(),palette=\"Set1\").add_legend()\nplt.show()","1cdef9cd":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(log,height=7,aspect=32\/9).map(sns.boxplot,x=log[\"video_type\"],y=log[\"log likes\"],order=log[\"video_type\"].unique(),palette=\"Set1\").add_legend()\nplt.show()","50fc1f86":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(log,height=7,aspect=32\/9).map(sns.boxplot,x=log[\"video_type\"],y=log[\"log comments\"],order=log[\"video_type\"].unique(),palette=\"Set1\").add_legend()\nplt.show()","3b81802a":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(log,height=7,aspect=32\/9).map(sns.boxplot,x=log[\"video_type\"],y=log[\"log dislikes\"],order=log[\"video_type\"].unique(),palette=\"Set1\").add_legend()\nplt.show()","fcbea816":"sns.set_style(\"whitegrid\")\nsns.FacetGrid(df,height=7,aspect=32\/9).map(sns.boxplot,x=df[\"video_type\"],y=df[\"trending_since(in days)\"],order=log[\"video_type\"].unique(),palette=\"Set1\").add_legend()\nplt.show()","3337f080":"sns.barplot(x = np.sort(df[\"trending_month\"].unique()),y = df[\"trending_month\"].value_counts(sort=False),palette=\"cool\")\nplt.show()","3a15083b":"for i in df[\"video_type\"].unique():\n    plt.figure(figsize=(16,9))\n    sns.barplot(x=np.sort(df.groupby(\"video_type\").get_group(f\"{i}\")[\"trending_month\"].unique()),y=df.groupby(\"video_type\").get_group(f\"{i}\")[\"trending_month\"].value_counts(sort=False),palette=\"cool\")\n    plt.xlabel(\"Trending Month\")\n    plt.ylabel(f\"No. of Trending Videos of {i} category\")\n    plt.title(f\"{i}\")\n    plt.show()","5bfccd2a":"**Creating another new Column that contains the no. of Days since the video is trending.**","428c2992":"### **WHICH CATEGORY DO INDIAND DISLIKE THE MOST ???**","d00f27c9":"**Lets go through the Numerical Features first**","5f9fdfe5":"### **ON WHICH CATEGORY DO INDIANS COMMENT THE MOST ???**","c6c667f8":"## **Now, Let's look at what kind of content is getting famous in India**\nWe will use WordCloud to visualize the the content Indians that goes trending in India.","a4ed0f4a":"### **WHICH CHANNEL IS MOST TRENDING ON YOUTUBE IN INDIA ???**","7459b54b":"## Bivariate Analysis","6e1b3087":"### **WHICH CATEGORY DO INDIANS LIKE THE MOST ???**","062f66ed":"### **WHICH CHANNEL DO INDIANS DISLIKE THE MOST ???**","5760beba":"**The histogram is not very clear. Lets get the values of Quantiles these columns.**","93e09292":"### **Distribution of Likes by various categories**","faf48fc9":"**Extracting the Categories**","06bb9df2":"**In the tags column, we need to remove '|' and ' \" ' .**","255bdff7":"Creating a copy of the DDataFrane so that we can manipulate the data further without losing the original data.","ccc9185c":"### **WHAT KIND OF VIDEOS ARE TRENDING IN INDIA ???**","07ca1401":"**After going through the likns, we get to know that the Videos don't include any description.**","fa727323":"**The Categories doesn't contain the 29th Category. Using the links, I examined many videos with 29th Category and most of them were related to Events and Motivation.**","bd766e05":"### **WHICH CATEGORY DO CANADIANS VIEW THE MOST ?**","df2b5326":"## Thank You\n### Stay Tuned ! \n### I'll be updating this kernel from time to time.\n### Please leave a comment below and let me know your feedback. Your suggestions are valuable to me. \n### Please upvote this kernel if you like my work.","bffab765":"### **WHICH CHANNEL DO INDIANS LIKE THE MOST ???**","0accccf5":"**Creating a new Column that contains all the text in the Video [Title, Tags, Description].**","7b9f3da6":"**Reading the json file that contains the category of the video.**","42686d45":"## **Let us look at the distribution of Numerical Columns divided by video_type**","b09cabee":"**Rearranging the required columns**","4793200f":"# **EXPLORATORY DATA ANALYSIS**","05716880":"### **Distribution of trending_since(in days) by various categories**","a2fffabf":"### **Distribution of Comment Count by various categories**","9062c642":"**Now lets look at our Categorical Features i.e. video_type, comments_disabled, ratings_disabled and video_error_or_removed**","73bf44cf":"**Creating a new Column named video_type which includes the Category Name of the Video**","3b851094":"### **Distribution of Views by various categories**","c2bc90c6":"### **WHICH CHANNEL DO INDIANS VIEW THE MOST ???**","c4fd27e6":"## Univariate Analysis","54caa7d8":"### **Distribution of Dislikes by various categories**","19677c3d":"**Creating a new column that contains the month in which video goes trending.**","a8bd6e21":"## NOW LET'S DRAW SOME INSIGHTS FROM THE DATA","9c9a5760":"### **ON WHICH CHANNEL DO INDIANS COMMENT THE MOST ???**","15aa36fe":"# Hello Everyone,\n\nHere, we have a dataset of **Youtube Trending Videos in India**.\n\nWe will **explore** this dataset, **preprocess** it and the perform **Exploratory Data Analysis**.\n\nI'm just a **beginner** and I must've made many **rookie mistakes**. So, I'm **open to suggestions**. Please leave a **review in the comments** and let me know what you think about my work.\n\nIf you like my work, please **upvote** this kernel."}}