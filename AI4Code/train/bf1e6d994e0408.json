{"cell_type":{"b2972174":"code","ebaf4365":"code","b7cf92cc":"code","4105c34b":"code","0af3f2d2":"code","f86a9f50":"code","2e31b45e":"code","797ffa38":"code","61353d35":"code","3d723f19":"code","a5edb3c4":"code","b66f1c5d":"markdown","9d36b726":"markdown","73eef419":"markdown","466d83e1":"markdown","df348868":"markdown","15c0641e":"markdown","a5426cdd":"markdown","6a782502":"markdown"},"source":{"b2972174":"%%capture\n# Install facenet-pytorch\n!pip install \/kaggle\/input\/facenet-pytorch-vggface2\/facenet_pytorch-1.0.1-py3-none-any.whl\n\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p \/tmp\/.cache\/torch\/checkpoints\/\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-logits.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_DG3kwML46X.pt\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-features.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_G5aNV2VSMn.pt\n\n# Install ffmpeg\n! tar xvf ..\/input\/ffmpeg-static-build\/ffmpeg-git-amd64-static.tar.xz","ebaf4365":"import os\nimport glob\nimport torch\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm_notebook\n\n# See github.com\/timesler\/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","b7cf92cc":"# Load face detector\nmtcnn = MTCNN(device='cuda').eval()\n\n# Load facial recognition model\nresnet = InceptionResnetV1(pretrained='vggface2', num_classes=2, device=device).eval()","4105c34b":"# Get all test videos\nfilenames = glob.glob('\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/*.mp4')\n\n# Number of frames to sample (evenly spaced) from each video\nn_frames = 10\n\nX = []\nwith torch.no_grad():\n    for i, filename in enumerate(filenames):\n        print(f'Processing {i+1:5n} of {len(filenames):5n} videos\\r', end='')\n        \n        try:\n            # Create video reader and find length\n            v_cap = cv2.VideoCapture(filename)\n            v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            # Pick 'n_frames' evenly spaced frames to sample\n            sample = np.linspace(0, v_len - 1, n_frames).round().astype(int)\n            imgs = []\n            for j in range(v_len):\n                success = v_cap.grab()\n                if j in sample:\n                    success, vframe = v_cap.retrieve()\n                    vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2RGB)\n                    imgs.append(Image.fromarray(vframe))\n            v_cap.release()\n            \n            # Pass image batch to MTCNN as a list of PIL images\n            faces = mtcnn(imgs)\n            \n            # Filter out frames without faces\n            faces = [f for f in faces if f is not None]\n            faces = torch.stack(faces).cuda()\n            \n            # Generate facial feature vectors using a pretrained model\n            embeddings = resnet(faces).cuda()\n            \n            # Calculate centroid for video and distance of each face's feature vector from centroid\n            centroid = embeddings.mean(dim=0)\n            X.append((embeddings - centroid).norm(dim=1).cpu().numpy())\n        except KeyboardInterrupt:\n            raise Exception(\"Stopped.\")","0af3f2d2":"np.save('X.npy',X)","f86a9f50":"bias = -0.2942\nweight = 0.068235746\n\nsubmission = []\nfor filename, x_i in zip(filenames, X):\n    if x_i is not None and len(x_i) == 10:\n        prob = 1 \/ (1 + np.exp(-(bias + (weight * x_i).sum())))\n    else:\n        prob = 0.5\n    submission.append([os.path.basename(filename), prob])","2e31b45e":"submission = pd.DataFrame(submission, columns=['filename', 'label'])","797ffa38":"sub = pd.read_csv('..\/input\/deepfake-detection-challenge\/sample_submission.csv')","61353d35":"result_map=dict(zip(submission.filename,submission.label))\nsub['label']=sub['filename'].map(result_map)\n","3d723f19":"sub.label = np.where(sub['label']>0.5,1,0)","a5edb3c4":"sub.to_csv('submission.csv',index=False)","b66f1c5d":"# Baseline submission using Facenet\n\nThis notebook demonstrates how to use the `facenet-pytorch` package to build a rudimentary deepfake detector without training any models.\nThe following steps are performed:\n\n1. Create pretrained facial detection (MTCNN) and recognition (Inception Resnet) models.\n1. For each test video, calculate face feature vectors for N faces evenly spaced through each video.\n1. Calculate the distance from each face to the centroid for its video.\n1. Use these distances as your means of discrimination.\n\nFor (much) better results, finetune the resnet to the fake\/real binary classification task instead - this is just a baseline. Alternatively, I'm sure there is much more interesting things that can be done with the feature vectors.","9d36b726":"## Install dependencies","73eef419":"## Create MTCNN and Inception Resnet models\n\nBoth models are pretrained. The Inception Resnet weights will be downloaded the first time it is instantiated; after that, they will be loaded from the torch cache.","466d83e1":"## Imports","df348868":"## Build submission","15c0641e":"## Predict classes\n\nThe below weights were selected by following the same process as above for the train sample videos and then using a logistic regression model to fit to the labels. Note that, intuitively, this is not a very good approach as it does nothing to take into account the progression of feature vectors throughout a video, just combines them together using the weights below. This step is provided as a placeholder only; it should be replaced with a more thoughtful mapping from a sequence of feature vectors to a single prediction.","a5426cdd":"## Process test videos\n\nLoop through all videos and pass N frames from each through the face detector followed by facenet. Calculate the distance from the centroid to the extracted feature for each face.","6a782502":"## Save submission"}}