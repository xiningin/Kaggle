{"cell_type":{"99e0f80f":"code","c43f1815":"code","a811ef1d":"code","1d05c00d":"code","565841a3":"code","0a95e51d":"code","66c174fd":"code","c723c503":"code","b80ae340":"code","07d0eed3":"code","7a216306":"code","9ab212ef":"code","d129f96d":"code","3f0ca7f3":"code","420a0c85":"code","cb146797":"code","be306ee0":"code","2951ecf1":"code","8e8b0193":"code","98a8fe88":"code","afc245bf":"code","75131aba":"code","b1c3da46":"code","e6d0a3e2":"code","eae3bf90":"code","f1ad82f6":"code","517aee45":"code","7f1cc6ea":"code","8916ed08":"code","9b1c9106":"code","e1c20d36":"code","dc7b578c":"code","4acacbb8":"code","5118adeb":"code","0306b06e":"code","4fad3da1":"code","9726cb6e":"code","cee934db":"code","4c794ed3":"code","f63b64b5":"code","3733f8fc":"code","578c192b":"code","a79a7f3e":"code","83fd8e14":"code","8617c792":"code","a287f8d3":"code","82fe3dff":"code","6e14a5b7":"code","43788e3a":"code","0eb5709d":"code","91d647eb":"code","a2d51c6a":"code","242369b4":"code","ee2ae522":"code","74dae750":"code","6610efe9":"code","456ace64":"code","b04fd416":"code","f0d3891f":"code","c957e440":"code","20ceb6cd":"code","2cb86d21":"code","ea77fcad":"code","921fa88b":"code","1608c392":"code","12c32f7b":"code","5cfdc67d":"code","4b4fff56":"code","239882bc":"code","73ea8d6f":"code","4ee5c8fa":"code","7f395bd4":"code","1fb35460":"code","b33bae1e":"code","a8a3a994":"code","efde5bc8":"code","03c927ca":"code","4d887701":"code","599d2f28":"code","a27d63b7":"code","cc5caef8":"code","b2f680db":"code","9beac64f":"code","f8088ed1":"code","02b560e4":"code","8573eb82":"code","65371898":"code","de6d293e":"code","a757d74d":"code","a331ecd6":"code","1ec138a7":"code","3d7c77e8":"code","7b224567":"code","9301445c":"code","0b5c1c91":"code","7ec0db49":"code","aaa69832":"code","fccb3e29":"code","14bfa95f":"code","06a20f33":"code","ee56a5cc":"code","c54f4137":"code","f358e104":"code","2698f28b":"markdown","11eceb17":"markdown","7f25dfc8":"markdown","22784ca8":"markdown","2f5b2be4":"markdown","5c17dc65":"markdown","11f7ae79":"markdown","e1ed80ea":"markdown","ef1263fd":"markdown","7b59ba52":"markdown","872678e0":"markdown","bbf37327":"markdown","757b5f84":"markdown","e7b61b95":"markdown","78cb6c1a":"markdown","f3d78d9f":"markdown","4a7e911e":"markdown","6493391c":"markdown","b37c424a":"markdown","4f053d25":"markdown","5819e6ef":"markdown","69edc203":"markdown","9a4d7e45":"markdown","8189a036":"markdown","f8f81f4b":"markdown","57fc62e2":"markdown","88879b24":"markdown","485de24f":"markdown","a06dc729":"markdown","b0313896":"markdown","d9283b48":"markdown","7e1ca952":"markdown","38d5bd8d":"markdown","1f3cf286":"markdown","10afe3fa":"markdown","867a5421":"markdown","86742700":"markdown","6d44d5c7":"markdown","c2256269":"markdown","127dfa8e":"markdown","bf7afbb0":"markdown","f1806ff5":"markdown","516a4c23":"markdown","656afdc9":"markdown","9adf418b":"markdown","b7625831":"markdown","d9e69c3f":"markdown","0773671c":"markdown","8c313dfd":"markdown","aae6bfc5":"markdown","3f2c56ee":"markdown","536d57b1":"markdown","98d8c96a":"markdown","164d3f80":"markdown","16a4fe9c":"markdown","d4cb31b8":"markdown","e28a7e48":"markdown","b68b59fd":"markdown","ee197241":"markdown","2af805d9":"markdown","c240b7a3":"markdown","6708de0f":"markdown","85257488":"markdown","e0f8af5b":"markdown","c19bf1f7":"markdown","e14a680b":"markdown","d2524726":"markdown","c87567c7":"markdown"},"source":{"99e0f80f":"import requests\nfrom bs4 import BeautifulSoup\nimport re\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom datetime import datetime\nimport time\nimport csv \nimport os\nimport pickle\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import export_graphviz\nimport graphviz\nfrom sklearn.neighbors import KNeighborsRegressor\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error \nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor # There is also a KerasClassifier class\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n!pip install openpyxl\nimport openpyxl\nprint(\"Library versions: pandas\", pd.__version__,\" numpy\", np.__version__,\" seaborn\", sns.__version__)","c43f1815":"def download_data():\n    URL = 'https:\/\/fossilfreefunds.org\/how-it-works'\n    page = requests.get(URL)\n    soup = BeautifulSoup(page.content, 'html.parser')\n    \n    urls = []\n    names = []\n    for i, link in enumerate(soup.findAll('a')):\n        FULLURL = link.get('href')\n        if bool(re.search('.*results.*.xlsx', FULLURL)):\n            urls.append(FULLURL)\n            names.append(os.path.basename(soup.select('a')[i].attrs['href']))\n\n    names_urls = zip(names, urls)\n    for name, url in names_urls:\n        print(\"Download file: \"+name)\n        r = requests.get(url, verify=False,stream=True)\n        r.raw.decode_content = True\n        with open(\"\/kaggle\/working\/\" + name, 'wb') as out:\n                shutil.copyfileobj(r.raw, out)    \n\ndef merge_excel():\n    df = pd.DataFrame()\n    files=os.listdir('data') \n    files_xls = [f for f in files if f[-4:]=='xlsx']\n\n    for f in files_xls:\n        if not re.match(r\".*20210[5-9]+.*\", f):\n            print('Merging file: '+f)\n            data = pd.read_excel('data\/'+f, 'Shareclasses',engine='openpyxl')\n            df = df.append(data) \n    df.to_csv('\/kaggle\/working\/fossilfund_dataset.csv', index=False)\n    print('Export to data\/fossilfund_dataset.csv is finished')\n\n#No download ncessary for this notebook, use of Kaggle dataset directly\n#if not os.path.exists('data'):\n#    os.makedirs('data')\n#    download_data()\n#    merge_excel()","a811ef1d":"df = pd.read_csv('\/kaggle\/input\/fossil-free-funds\/fossilfund_dataset.csv')","1d05c00d":"print(\"Total no. of columns in the dataframe\", len(df.columns))\nprint(\"No. of columns containing null values\", len(df.columns[df.isna().any()]))\n\nprint(\"No. of columns not containing null values\", len(df.columns[df.notna().all()]))\nprint(\"No. of numerical columns \", len(df.select_dtypes(np.number).columns))\n\nprint(\"Total no. of rows in the dataframe\", len(df))","565841a3":"df.info(max_cols=200)","0a95e51d":"date_cols=df.filter(regex=\" date.*\",axis=1).columns\ndf[date_cols]=df[date_cols].apply(pd.to_datetime, errors='coerce')\ndf[date_cols]","66c174fd":"display(HTML(df[0:10].to_html())) ","c723c503":"key_df=pd.read_excel('\/kaggle\/input\/fossil-free-funds\/raw\/InvestYourValuesshareclassresults20200716.xlsx', 'Key',  engine='openpyxl', header=None, names=['Category', 'Feature', 'Description'])\ns = key_df.style.set_properties(**{'text-align': 'left'})\ndisplay(HTML(s.render()))\n#display(HTML(key_df.to_html(index=False))) ","b80ae340":"#create variables for each group and display information separately\n#Our target value has to be 1-y as it is the most complete => need to show fill factor\n#display(HTML(    .to_html())) \norigin_categories=[\"Fund profile\",\"Fossil Free Funds\", \"Deforestation Free Funds\", \"Gender Equality Funds\", \"Gun Free Funds\", \"Prison Free Funds\", \"Weapon Free Funds\", \"Tobacco Free Funds\", \"Financial performance\"]\nfor category in origin_categories:\n    print(\"Distribution of numerical features for category: \"+category)\n    display(HTML(df.filter(regex=category+\".*\",axis=1).describe().to_html())) \n    print(\"\\n\\n\")","07d0eed3":"columns_stats=pd.DataFrame()\ncolumns_stats['fill_percent']=df.notnull().sum(axis=0)\/len(df)*100\nfig = plt.figure(figsize=(10, 22))\ncolumns_stats['fill_percent'].sort_values().plot.barh()\nplt.title(\"Fill-factor for all features\")\nplt.xlabel(\"Percentage\")\nplt.gca().xaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()","7a216306":"columns_stats[columns_stats['fill_percent']<60]","9ab212ef":"df.drop(columns=columns_stats[columns_stats['fill_percent']<60].index.values.tolist(), axis=1, inplace=True)","d129f96d":"columns_stats['fill_percent'].filter(regex=\"Financial performance.*\")","3f0ca7f3":"perf_date=pd.DataFrame()\nfor year in [1,3,5,10]:\n    filter_df=df['Fund profile: Shareclass inception date'][( (df['Financial performance: Financial performance as-of date'] - pd.DateOffset(years=year)) > df['Fund profile: Shareclass inception date']  )]\n    perf_date[\"year\"+str(year)] =filter_df.groupby(filter_df.dt.year).count()\n\n#Normalize row perf_data by year\nperf_date_norm=perf_date.div(perf_date.sum(axis=1), axis=0)*100\nperf_date_norm.loc['2000-1-1 00:00:00':'2022-1-1 00:00:00'].plot.bar(figsize=(16, 8), stacked=True)\n\n#Plot properties\nplt.xlabel(\"Inception fund dates\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Fund inception date inception date crossed with (performance as-of date - year)\")\nplt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.22),\n          ncol=4, fancybox=True, shadow=True)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n#plt.setp(plt.gca().get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.show()","420a0c85":"columns_stats=pd.DataFrame()\ncolumns_stats['unique_values']=df.nunique()\/len(df)*100\nfig = plt.figure(figsize=(10, 22))\ncolumns_stats['unique_values'].sort_values().plot.barh()\nplt.title(\"Data uniqueness for all features\")\nplt.xlabel(\"Percentage\")\nplt.gca().xaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()","cb146797":"cols_df=pd.read_excel('\/kaggle\/input\/fossil-free-funds\/misc\/replace.xlsx', 'Cols',  engine='openpyxl')\ndf.rename(columns=dict(zip(cols_df[\"Column original\"],cols_df[\"Short column name\"])), inplace=True)","be306ee0":"def getColCategory(category):\n    return list(set(cols_df[cols_df['Category']==category]['Short column name']) & set(df.columns))\n\ndef getColType(type_col):\n    return list(set(cols_df[cols_df['Type']==type_col]['Short column name']) & set(df.columns))    \n\ndef getEncoding(shortName):\n    return cols_df[cols_df['Short column name']==shortName]['encoding'].values[0]","2951ecf1":"continuous= getColType('Continuous')\ndiscrete= getColType('Discrete')\nordinal= getColType('Ordinal') \nnominal= getColType('Nominal') \n\ndate_cols=df.filter(regex=\".*Date.*\",axis=1).columns","8e8b0193":"threshold_0_level=10\n\ndef zeros_columns(df, col_category):\n    zeros_percentage=(df[col_category]==0).sum()*100\/len(df[col_category])\n    zeros=zeros_percentage[(zeros_percentage>threshold_0_level)].index.values.tolist()\n    print(\"Columns with 0-values > \"+str(threshold_0_level)+\"% : \"+str(len(zeros))+\"\/\"+str(len(col_category)))\n    print(zeros_percentage[(zeros_percentage>threshold_0_level)].sort_values(ascending=False))\n    return zeros\ncontinuous_zeros = zeros_columns(df, continuous)\ndiscrete_zeros = zeros_columns(df, discrete)","98a8fe88":"threshold_null_level=2\n\ndef null_columns(df, col_category):\n    null_percentage=(df[col_category].isnull()).sum()*100\/len(df[col_category])\n    nulls=null_percentage[(null_percentage>threshold_null_level)].index.values.tolist()\n    print(\"Columns with null-values > \"+str(threshold_null_level)+\"% : \"+str(len(nulls))+\"\/\"+str(len(col_category)))\n    print(null_percentage[(null_percentage>threshold_null_level)].sort_values(ascending=False))\n    return nulls\n    \ndiscrete_null = null_columns(df, discrete)\ncontinuous_null = null_columns(df, continuous)\nnominal_null = null_columns(df, nominal)\nordinal_null = null_columns(df, ordinal)","afc245bf":"df.duplicated().sum()","75131aba":"df[nominal].nunique()\/len(df)*100","b1c3da46":"index_col=['FI_ShareclassName', 'FP_PerformanceAs-OfDate']\nduplicate_rows=df[df.duplicated(subset=index_col, keep=False)]\nduplicate_rows.to_csv('\/kaggle\/working\/temp.csv', sep=';')\nduplicate_rows","e6d0a3e2":"origin_len=len(df)\nduplicate_len=len(df[df.duplicated(subset=index_col, keep='last')])\ndf=df[~df.duplicated(subset=index_col, keep='last')]","eae3bf90":"print(\"Remaining rows:\",len(df),\"(\",origin_len,\"-\",duplicate_len,\")\")\ndf[df.duplicated(subset=index_col, keep=False)]","f1ad82f6":"df_tmp=df.copy()\ndf_tmp['FI_AssetManagerFirstLetter']=df_tmp['FI_AssetManager'].str[0]\ngrouped_df=df_tmp.groupby(df_tmp['FP_PerformanceAs-OfDate'])\ngrouped_df['FI_AssetManagerFirstLetter'].value_counts()","517aee45":"grouped_df['FI_AssetManagerFirstLetter'].value_counts().groupby(level=0).apply(\n    lambda x: x \n).unstack().plot.bar(figsize=(16, 8), stacked=True)\n\n#Plot properties\nplt.ylabel(\"Number of funds\")\nplt.xlabel(\"FI_AssetManagerFirstLetter\")\nplt.title(\"Number of Asset Manager by date\")\nplt.get_cmap('gist_rainbow')\nplt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.35),\n          ncol=10, fancybox=True, shadow=True)\nplt.setp(plt.gca().get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.show()","7f1cc6ea":"per_share_max_count = df.groupby(['FI_ShareclassName'])['FI_ShareclassName'].value_counts().max()\nthreshold_max_count=0.6\n\npartial_share_missing=df.copy()\npartial_share_missing=partial_share_missing.groupby(['FI_ShareclassName']).filter(lambda x: len(x) <= threshold_max_count * per_share_max_count)\npartial_share_missing.groupby(['FI_ShareclassName'])['FI_ShareclassName'].value_counts().sort_values(ascending=False)","8916ed08":"origin_len=len(df)\ndf.drop(partial_share_missing.index, inplace=True)\nprint(\"Remaining rows:\",len(df),\"(\",origin_len,\"-\",len(partial_share_missing),\")\")","9b1c9106":"for date_col in df.filter(regex=\"Date.*\",axis=1).columns:\n    print('Number of empty dates for columns',date_col,\":\",len(df[df[date_col].isnull()]))\ndf[df['FP_PerformanceAs-OfDate'].isnull()]","e1c20d36":"filter=df[df['FP_PerformanceAs-OfDate'].isnull()]\ndf.loc[filter.index-1,['FP_PerformanceAs-OfDate','FI_PortfolioHoldingsAs-OfDate']]","dc7b578c":"df.loc[filter.index,'FP_PerformanceAs-OfDate']=df.loc[filter.index-1,'FP_PerformanceAs-OfDate']\ndf[df['FP_PerformanceAs-OfDate'].isnull()]","4acacbb8":"def checkuniquevalues(df, cols):\n    #Check unique values\n    for col in cols:\n        print(col,\": Total unique:\",len(df[col].sort_values().unique()),\" - Values:\",df[col].sort_values().unique())\ncheckuniquevalues(df, ordinal+nominal)\n    #for col in cols:\n    #    values=\"\"\n    #    for elem in df[col].sort_values().unique():\n    #        values=values+\"'\"+str(elem)+\"':'\"+str(elem)+\"', \"","5118adeb":"df.drop(columns=['FI_Ticker','FI_ShareclassTickers','FI_ShareclassName', 'FI_FundName', 'FI_AssetManager'], axis=1, inplace=True)\nnominal.remove('FI_Ticker')\nnominal.remove('FI_ShareclassTickers')\nnominal.remove('FI_ShareclassName')\nnominal.remove('FI_FundName')\nnominal.remove('FI_AssetManager')","0306b06e":"#df.to_csv('\/kaggle\/input\/fossil-free-funds\/prep\/fossilfund_dataset_clean.csv', index=False)","4fad3da1":"categories=cols_df['Category'].unique()   \n\nfor category in categories:\n    #index_cols=list(set(cols_df[cols_df['Category']==category]['Short column name']) & set(df.columns))\n    index_cols=getColCategory(category)\n    length=len(df[index_cols].select_dtypes(exclude=object).columns)\n    print(\"Histogram for \"+category+\" features\")\n    df[index_cols].hist(color='g', bins=50, grid=False, figsize=(length*2,length))\n    plt.tight_layout()\n    plt.show()","9726cb6e":"# (C) Preprocessing function\ndef df_wo_zeros_null(df):\n    df = df.copy()\n\n    # Continuous            \n    # Add additional column for holding 0 values\n    # Filter-out zero values\n    for c in list(continuous_zeros)+list(discrete_zeros):\n        name = c + \"_isempty\"\n        idx= df[c]==0\n        df[name] = idx\n        #Convert bool col as int\n        df[name] = df[name].astype(int)\n        df[c] = df[~idx][c]\n\n    # Fill missing values\n    for c in list(set(continuous + discrete) & set(df.select_dtypes(np.number).columns)):\n        df[c].dropna(inplace=True)\n\n    return df","cee934db":"from sklearn.preprocessing import QuantileTransformer\n\n#continuous+discrete\ncols= list(set(continuous) & set(df.select_dtypes(np.number).columns))\ntemp_df=df_wo_zeros_null(df)\nfor category in categories:\n    #index_cols=list(set(cols_df[cols_df['Category']==category]['Short column name']) & set(df.columns))\n    index_cols=list( set(getColCategory(category)) & set(cols))\n    length=len(index_cols)\n    for d in [1, 0.5, 2, 3]:\n        test_df=temp_df[index_cols].copy()\n        for c in test_df.columns:\n            name = '{}**{}'.format(c, d)\n            test_df[name]=test_df[c]**d\n            test_df.drop(c, axis=1, inplace=True)\n        test_df.hist(color='g', bins=30, grid=False, figsize=((length*1.2)+10,length+3))\n        #test_df[test_df>0].hist(color='g', bins=30, grid=False, figsize=(length*2,length))\n\n    #quantile = QuantileTransformer(output_distribution='normal')\n    #test_df=df[index_cols]\n    #test_df[index_cols]=quantile.fit_transform(test_df[index_cols]) \n    #test_df.hist(color='g', bins=30, grid=False, figsize=((length*1.2)+10,length+3))\n\n    #log1p\n    if(category != 'Gender Equality'):\n        test_df=np.log1p(df[index_cols].copy())\n        test_df.columns = [str(col) + '_log1p' for col in test_df.columns]\n        test_df.hist(color='g', bins=30, grid=False, figsize=((length*1.2)+10,length+3))\n    plt.tight_layout()\n    plt.show()   ","4c794ed3":"from scipy.stats import boxcox\nspecial_distrib=['FI_PercentRated','F_CarbonMarketValueWeight','GE_WeightOfHoldings']\nlength=len(special_distrib)\ntest_df=df[special_distrib].copy()\nfor feature in special_distrib:\n    name = 'expm1({})'.format(feature)\n    test_df[name]=(1-test_df[feature])**0.5\n    \n    #test_df[name]=boxcox(test_df[feature], 0.3)\n    test_df.drop(feature, axis=1, inplace=True)\ntest_df.hist(color='g', bins=30, grid=False, figsize=((length*1.2)+10,length+3))    \nplt.tight_layout()\nplt.show() ","f63b64b5":"cols= list(set(continuous) & set(df.select_dtypes(np.number).columns))\npreprocess_df=df_wo_zeros_null(df)\ncontinuous_log1p=[]\ncontinuous_exp05=[]\ncontinuous_exp1_05=[]\n\nfor category in categories:\n    #index_cols=list(set(cols_df[cols_df['Category']==category]['Short column name']) & set(df.columns))\n    index_cols=list( set(getColCategory(category)) & set(cols))\n    length=len(index_cols)\n    test_df=preprocess_df[index_cols].copy()\n    for c in test_df.columns:\n        encoding=getEncoding(c)\n        if(encoding == \"log1p\"):\n            name = 'log1p({})'.format(c)\n            test_df[name]=np.log1p(test_df[c])\n            test_df.drop(c, axis=1, inplace=True)\n            continuous_log1p.append(c)\n        elif (encoding == \"^0.5\"):\n            name = '{}**{}'.format(c, '0.5')\n            test_df[name]=test_df[c]**0.5\n            test_df.drop(c, axis=1, inplace=True)\n            continuous_exp05.append(c)\n        elif (encoding == \"(1-x)^0.5\"):\n            name = '(1-{})^0.5'.format(c)\n            test_df[name]=(1-test_df[c])**0.5\n            test_df.drop(c, axis=1, inplace=True)\n            continuous_exp1_05.append(c)            \n    \n    test_df.hist(color='g', bins=30, grid=False, figsize=((length*1.2)+10,length+3))\n    plt.tight_layout()\n    plt.show()   ","3733f8fc":"outliers_threshold=3\n\n# (C) Preprocessing function\ndef preprocess_numerical(df):\n    df = df.copy()\n\n    # Continuous            \n    # Add additional column for holding 0 values\n    # Filter-out zero values\n    for c in list(continuous_zeros)+list(discrete_zeros):\n        name = c + \"_isempty\"\n        idx= df[c]==0\n        df[name] = idx\n        #Convert bool col as int\n        df[name] = df[name].astype(int)\n        df[c] = df[~idx][c]\n\n\n    # Apply feature encoding\n    df[continuous_log1p]=np.log1p(df[continuous_log1p])\n    df[continuous_exp05]=df[continuous_exp05]**0.5\n    df[continuous_exp1_05]=(1-df[continuous_exp1_05])**0.5\n  \n    # Apply z-scores for data with outliers\n    for c in continuous:\n        z_scores = (df[c] - df[c].mean()) \/ df[c].std()\n        idx = (np.abs(z_scores) > outliers_threshold)\n        df[c] = df[~idx][c]\n\n    # Fill missing values\n    for c in list(set(continuous + discrete) & set(df.select_dtypes(np.number).columns)):\n        df[c].fillna(df[c].median(), inplace=True)\n\n    #Replace dates\n    for date_col in date_cols:\n        df[date_col]=pd.to_numeric(df[date_col].apply(pd.to_datetime, errors='coerce'))\n\n    return df\n\npreprocess_df=preprocess_numerical(df)","578c192b":"cols= list(set(continuous) & set(df.select_dtypes(np.number).columns))\n\nfor category in categories:\n    index_cols=list( set(getColCategory(category)) & set(cols))\n    length=len(index_cols)\n    print(\"Histogram for \"+category+\" features\")\n    preprocess_df[index_cols].hist(color='g', bins=50, grid=False, figsize=(length*2,length))\n    plt.tight_layout()\n    plt.show()","a79a7f3e":"print(\"We had\", len(df.columns), \"features before peprocessing numerical\")\nprint(\"After processing, we have a total of\", len(preprocess_df.columns), \"features\")\nprint(\"Number of null values in the dataframe:\", preprocess_df[discrete+continuous].isnull().sum().sum())","83fd8e14":"for c in preprocess_df.columns:\n    print (c)","8617c792":"def ordinal_mapping(df, cols, dictionary):\n    for col in cols:\n        df[col]=df[col].map(dictionary)\n    return df\n\n#grade_cols=df.filter(regex=\"Grade\",axis=1).columns\n#checkuniquevalues(df, grade_cols)","a287f8d3":"def preprocessing_categorical(df):\n    df = df.copy()\n    # convert ordinal to int columns\n    ordinal_mapping(df, ordinal, {'A':6, 'B':5, 'C':4, 'D':3, 'E':2, 'F':1, np.nan:0})\n\n    # One-hot encoding\n    df = pd.get_dummies(df, columns=nominal, dummy_na=False)\n\n    return df\n\npreprocess_df=preprocessing_categorical(preprocess_df)","82fe3dff":"print(\"After processing, we have a total of\", len(preprocess_df.columns), \"features\")","6e14a5b7":"def compareAfter_preprocess(df_origin, df_new):\n    number_previous_col=0\n    for col in df_new.columns:\n        if(col in df_origin.columns ):\n            if(not col in continuous):\n                number_previous_col=number_previous_col+1\n                if(len( list(set(df_new[col].unique()) -set(df_origin[col].unique()) )) >0):\n                    print(col,\"- difference of value mapping :\",list(set(df_new[col].unique()) -set(df_origin[col].unique())))\n        else:\n            print(col,\"- new column\")    \n\ncompareAfter_preprocess(df,preprocess_df)","43788e3a":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_correlations(df, n=10):\n    au_corr = df.corr().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]","0eb5709d":"correlation_threshold=0.95\n\nfor category in ['FI','FP','F','D','GE','G','W','T','P']:\n    filter=preprocess_df.filter(regex=category+\"_.*\",axis=1).select_dtypes(np.number)\n    if(len(filter.columns)>0):\n        print(\"Top Absolute Correlations\")\n        tmp_corr=get_top_correlations(filter, 40)\n        print(tmp_corr[tmp_corr >= correlation_threshold].sort_index())","91d647eb":" def correlation(dataset, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    corr_results=[]\n\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in dataset.columns:\n                    del dataset[colname] # deleting the column from the dataset\n                    corr_results.append({\n                        'Deleted column': colname,\n                        'Correlation column': corr_matrix.columns[j],\n                        'Correlation': corr_matrix.iloc[i, j]\n                    })\n\n    return pd.DataFrame(corr_results)\n\ncorr_results=correlation(preprocess_df, correlation_threshold)\npd.set_option('display.max_rows', None)\ncorr_results","a2d51c6a":"print(\"Remaining features\",len(preprocess_df.columns))","242369b4":"#preprocess_df.to_csv('\/kaggle\/input\/fossil-free-funds\/prep\/fossilfund_dataset_prep.csv', index=False)","ee2ae522":"fig = plt.figure(figsize=(25, 20))\nsns.heatmap(preprocess_df.reindex(sorted(preprocess_df.columns), axis=1).corr(method='pearson'), \n            cmap='RdBu_r',\n            annot=False,\n            linewidth=0.5)","74dae750":"sns.clustermap(preprocess_df.corr(method='pearson'), cmap='RdBu_r', figsize=(30, 30))","6610efe9":"sns.clustermap(preprocess_df.filter(regex=\".*_a.*\",axis=1).corr(method='pearson'), cmap='RdBu_r', figsize=(10, 10))","456ace64":"sns.clustermap(preprocess_df.filter(regex=\".*_c.*\",axis=1).corr(method='pearson'), cmap='RdBu_r', figsize=(10, 10))","b04fd416":"sns.clustermap(preprocess_df.filter(regex=\".*_w.*\",axis=1).corr(method='pearson'), cmap='RdBu_r', figsize=(10, 10))","f0d3891f":"sns.clustermap(preprocess_df.filter(regex=\".*_isempty\",axis=1).corr(method='pearson'), cmap='RdBu_r', figsize=(10, 10))","c957e440":"target = cols_df[ (cols_df['Category']=='Financial performance') & (cols_df['Type']=='Continuous') ]['Short column name']\ncorrelations=preprocess_df.corr(method='pearson')[target]#.sort_values(ascending=False)\nprint('Top positive correlations with ReturnsY1')\ncorrelations['FP_ReturnsY1'].sort_values(ascending=False)[0:15]","20ceb6cd":"print('Top negative correlations with ReturnsY1')\ncorrelations['FP_ReturnsY1'].sort_values(ascending=False)[-5:]","2cb86d21":"correlations.filter(regex=\"FI_AssetManager.*\",axis=0)['FP_ReturnsY1'].sort_values(ascending=False)","ea77fcad":"target = 'FP_ReturnsY1'\nnb_samples=5000\ndef chunks(l, n):\n    n = max(1, n)\n    return (l[i:i+n] for i in range(0, len(l), n))\n\nfor category in ['FI','FP','F','D','GE','G','W','T','P']:\n    index_cols=preprocess_df.filter(regex=\"^\"+category+\"_.*\",axis=1).columns\n#for category in categories:\n#    index_cols=list(set(getColCategory(category)) & set(preprocess_df.select_dtypes(np.number).columns))\n    index_cols_numb_only=preprocess_df[index_cols].select_dtypes(exclude=object).columns\n    index_cols_list=list(chunks(index_cols_numb_only, 5))\n    number_sets=len(index_cols_list)\n    for i in range(0,len(index_cols_list)):\n        #Use sampling technique to speedup the process\n        print(\"Pairplots for \"+category+\" numeric features - set \"+str(i+1)+\"\/\"+str(number_sets))\n        plot= sns.pairplot(data=preprocess_df.sample(nb_samples), y_vars=target, x_vars=index_cols_list.pop(), height=3.5, aspect=1.1)\n        plt.tight_layout()\n        plt.show()","921fa88b":"for date_col in date_cols:\n    preprocess_df[date_col]=preprocess_df[date_col].apply(pd.to_datetime, errors='coerce')","1608c392":"preprocess_df['FP_ReturnsY1_diff']=df['FP_ReturnsY1'].diff()","12c32f7b":"# create figure and axis objects with subplots()\nfig,ax=plt.subplots(figsize=(8,5))\nax.plot(preprocess_df.groupby(['FP_PerformanceAs-OfDate'])['FP_ReturnsY1'].mean(), label='Target')\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"FP_ReturnsY1\",color='C0')\n\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.plot(preprocess_df.groupby(['FP_PerformanceAs-OfDate'])['FP_ReturnsY1_diff'].mean(), color='g', label='Differenciation')\nax2.set_ylabel(\"FP_ReturnsY1_diff\",color='g')\nplt.show()","5cfdc67d":"# create figure and axis objects with subplots()\nfig,ax=plt.subplots(figsize=(8,5))\nax.plot(preprocess_df.groupby(['FI_PortfolioHoldingsAs-OfDate'])['FP_ReturnsY1'].mean(), label='Target')\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"FP_ReturnsY1\",color='C0')\n\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.plot(preprocess_df.groupby(['FI_PortfolioHoldingsAs-OfDate'])['FP_ReturnsY1_diff'].mean(), color='g', label='Differenciation')\nax2.set_ylabel(\"FP_ReturnsY1_diff\",color='g')\nplt.show()","4b4fff56":"test_size_split=0.4\nclean_df=pd.read_csv('\/kaggle\/input\/fossil-free-funds\/prep\/fossilfund_dataset_prep.csv')","239882bc":"print(\"How many non numerical columns:\",len(clean_df.select_dtypes(['number']).columns)-len(clean_df.columns))","73ea8d6f":"#Remove FP\nclean_df.drop(columns=['FP_ReturnsY3','FP_ReturnsY5','FP_ReturnsY10'], axis=1, inplace=True)","4ee5c8fa":"#All rows contain null values\nprint(\"No. of rows containing null values\", len(clean_df.isna().sum(axis=1).eq(0) ))\nprint(\"Total no. of columns in the dataframe\", len(clean_df.columns))\nprint(\"No. of numerical columns \", len(clean_df.select_dtypes(np.number).columns))\nprint(\"No. of columns containing null values\", len(clean_df.columns[clean_df.isna().any()]))\nprint(\"Details of columns containing null values\", clean_df.columns[clean_df.isna().any()])\nprint(\"No. of columns not containing null values\", len(clean_df.columns[clean_df.notna().all()]))\n\n#clean_df[clean_df.isna().any(axis=1)]","7f395bd4":"target = 'FP_ReturnsY1'\nX = clean_df.drop(columns=target)\ny = clean_df[target]","1fb35460":"# How many features do you want to keep?\nk = 20\n\n# Create the selecter object\nskb = SelectKBest(f_regression, k=k)\n\n# Fit the selecter to your data\nX_new = skb.fit_transform(X, y)\n\n# Extract the top k features from the `pvalues_` attribute\nk_feat = np.argsort(skb.pvalues_)[:k]\n\n# Reduce the dataframe according to the selecter\ndf_reduced = clean_df[X.columns[k_feat]]","b33bae1e":"# instantiate SelectKBest to determine 20 best features\nbest_features = SelectKBest(score_func=f_regression, k=k)\nfit = best_features.fit(X,y)\ndf_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(X.columns)\n# concatenate dataframes\nfeature_scores = pd.concat([df_columns, df_scores],axis=1)\nfeature_scores.columns = ['Feature_Name','Score']  # name output columns\ntopk_features=feature_scores.nlargest(k,'Score')\nprint(topk_features)  # print top k best features","a8a3a994":"def plotPredictors(selection, features_scores):\n    subset_features=feature_scores[feature_scores['Feature_Name'].isin(selection)].sort_values(by='Score', ascending=False)\n    plt.figure(figsize=(10,5))\n    plt.bar(range(len(subset_features['Score'])), subset_features['Score'])\n    plt.xticks(np.arange(0, len(subset_features['Score'])), subset_features['Feature_Name'], rotation=\"vertical\")\n    #plt.setp(plt.gca().get_xticklabels(), rotation=30, horizontalalignment='right')\n    plt.tight_layout()","efde5bc8":"plotPredictors(topk_features['Feature_Name'],feature_scores)","03c927ca":"for category in ['FI','FP','F','D','GE','G','W','T','P']:\n    featurePerCategory=feature_scores[feature_scores['Feature_Name'].str.contains(\"^\"+category+\"_.*\")]['Feature_Name'].values\n    plotPredictors(featurePerCategory,feature_scores)","4d887701":"features_simple_model=['F_RelativeCarbonFootprint','F_RelativeCarbonIntensity','F_FossilFuelGrade']\n\nfeatures_grade_model=['F_FossilFuelGrade',\n'P_PrisonIndustrialComplexGrade',\n'D_DeforestationGrade',\n'T_TobaccoGrade',\n'W_MilitaryWeaponGrade',\n'GE_WeightOfHoldings']\n\nfeatures_intermediate_model=list(feature_scores.nlargest(k,'Score')['Feature_Name'].values)\n\nfeatures_complex_model=clean_df.drop(columns=target).columns","599d2f28":"plotPredictors(features_simple_model,feature_scores)","a27d63b7":"plotPredictors(features_grade_model,feature_scores)","cc5caef8":"plotPredictors(features_intermediate_model,feature_scores)","b2f680db":"plotPredictors(features_complex_model[0:50],feature_scores)","9beac64f":"def splitTrainTest(df,target,features):\n    #split current dataframe into train set \/ test set\n    # Create X, y\n    X = df[features]\n    y = df[target]\n    # Split into train\/test sets\n    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size_split, random_state=0)\n    # Standardize data\n    scaler = StandardScaler()\n    X_tr_rescaled = scaler.fit_transform(X_tr)\n    X_te_rescaled = scaler.transform(X_te)\n    \n    return X_tr_rescaled, X_te_rescaled, y_tr, y_te, scaler\n\n\ndef saveModelResults(model, modelName, X_te_rescaled, y_te):\n    mae = MAE(y_te, model.predict(X_te_rescaled))\n    print('MAE with best alpha: {:,.3f}%'.format(mae))\n\n    #Save result\n    details = { \n        'model' : [modelName], \n        'test_accuracy' : [mae], \n    }\n    df = pd.DataFrame(details)\n    df.to_csv('\/kaggle\/working\/results.csv', index=False, mode='a', header=False, float_format='%.3f')\n    return [mae, model, modelName]\n\ndef RidgeModelTraining(df, features, modelName):\n    X_tr_rescaled, X_te_rescaled, y_tr, y_te, scaler = splitTrainTest(df,target,features)\n\n    # Fit\/test N models for optimal regularization\n    gs_results = []\n\n    # Grid search\n    for alpha in np.logspace(-8, 4, num=200):\n        # Create and fit ridge regression\n        ridge = Ridge(alpha=alpha)\n        ridge.fit(X_tr_rescaled, y_tr)\n        \n        # Save model and its performance on train\/test sets\n        gs_results.append({\n            'model': ridge,\n            'alpha': alpha,\n            'train_mse': MSE(y_tr, ridge.predict(X_tr_rescaled)),\n            'train_mae': MAE(y_tr, ridge.predict(X_tr_rescaled)),\n            'test_mse': MSE(y_te, ridge.predict(X_te_rescaled)),\n            'test_mae': MAE(y_te, ridge.predict(X_te_rescaled)),\n        })\n\n    # Convert results to DataFrame\n    gs_results = pd.DataFrame(gs_results)\n\n    # Plot the validation curves\n    plt.plot(np.log10(gs_results['alpha']), gs_results['train_mae'], label='train curve')\n    plt.plot(np.log10(gs_results['alpha']), gs_results['test_mae'], label='test curve')\n\n    # Mark best alpha value\n    best_result = gs_results.loc[gs_results.test_mae.idxmin()]\n    plt.scatter(np.log10(best_result.alpha), best_result.test_mae, marker='x', c='red', zorder=10)\n    plt.title('Best alpha: {:.1e} - mse: {:.4f} mae: {:,.0f}%'.format(\n        best_result.alpha, best_result.test_mse, best_result.test_mae))\n\n    plt.xlabel('$log_{10}(alpha)$')\n    plt.ylabel('MAE')\n    plt.legend()\n    plt.show()\n\n    ridge = Ridge(alpha=best_result.alpha)\n    ridge.fit(X_tr_rescaled, y_tr)\n    #mae_best=MAE(y_te, ridge.predict(X_te_rescaled))\n    #print('MAE with best alpha: {:,.2f}$'.format(mae_best))\n    #return [mae_best, ridge]\n    \n    return saveModelResults(ridge, modelName, X_te_rescaled, y_te)\n     ","f8088ed1":"#MAE baseline\nX_tr_rescaled, X_te_rescaled, y_tr, y_te, scaler = splitTrainTest(clean_df,target,features_simple_model)\nmedian_predictions = np.full_like(y_te, np.median(y_tr))\nmae_baseline=MAE(y_te, median_predictions)\nprint('Median baseline: {:,.2f}%'.format(mae_baseline))","02b560e4":"ridge_simple = RidgeModelTraining(clean_df, features_simple_model, 'RidgeSimple')","8573eb82":"ridge_grade = RidgeModelTraining(clean_df, features_grade_model, 'RidgeGrade')","65371898":"ridge_intermediate = RidgeModelTraining(clean_df, features_intermediate_model, 'RidgeIntermediate')","de6d293e":"ridge_complex = RidgeModelTraining(clean_df, features_complex_model, 'RidgeComplex')","a757d74d":"# Ridge comparison\nmae_values = [mae_baseline, ridge_simple[0], ridge_grade[0] , ridge_intermediate[0], ridge_complex[0]]\ntitles = ['median', 'Simple', 'Grade','Intermediate', 'Complex']\n\nxcor = np.arange(len(mae_values))\nplt.bar(xcor, mae_values)\nplt.xticks(xcor, titles)\n\nplt.ylabel('MAE')\nplt.show()","a331ecd6":"selected_model=features_simple_model\nX_tr_rescaled, X_te_rescaled, y_tr, y_te, scaler = splitTrainTest(clean_df,target,selected_model)","1ec138a7":"n_estimators=100\nmax_depth=4\n# Random Forest Regressor Model\nrdForest = RandomForestRegressor(max_depth=max_depth,n_estimators=n_estimators, random_state=0)\nmodel = rdForest.fit(X_tr_rescaled, y_tr)\n\n#Show plot\ntree = rdForest.estimators_[n_estimators-1]\n# Export decision tree\ndot_data = export_graphviz(\n    tree, out_file=None,\n    feature_names=clean_df[selected_model].columns, \n    class_names=categories,\n    filled=True, rounded=True\n)\n\n# Display decision tree\ngraphviz.Source(dot_data)","3d7c77e8":"n_estimators=500\nmax_depth=None\nmax_features=\"log2\"\n\nrdForest = RandomForestRegressor(max_features=max_features, max_depth=max_depth,n_estimators=n_estimators,random_state=0)\nmodel = rdForest.fit(X_tr_rescaled, y_tr)\n\nrdForestResults=saveModelResults(model, \"RandomForest\", X_te_rescaled, y_te)","7b224567":"n_neighbors=100\np=2\nweights='distance'\n\nneigh = KNeighborsRegressor(n_neighbors=n_neighbors, p=p, weights=weights)\nneigh.fit(X_tr_rescaled, y_tr)\n\n\nkNNResults=saveModelResults(neigh, \"KNN\", X_te_rescaled, y_te)","9301445c":"batch_size_nb=32\nepochs=200\nnumber_of_neurons=128\nnumber_of_hidden_layers=10\n\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(number_of_neurons, kernel_initializer='normal',input_dim = X_tr_rescaled.shape[1], activation='relu'))\n\n# The Hidden Layers :\nfor i in range(0,number_of_hidden_layers):\n    NN_model.add(Dense(number_of_neurons, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","0b5c1c91":"modelfilepath='\/kaggle\/input\/fossil-free-funds\/misc\/modelNN_simple'\nif not os.path.isfile(modelfilepath):\n    #os.makedirs(modelfilepath)\n    #Define a checkpoint \n    #checkpoint_name = traindir+'\/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n    #checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n    #callbacks_list = [checkpoint]\n    \n    #Train model\n    NN_model.fit(X_tr_rescaled, y_tr, epochs=epochs, batch_size=batch_size_nb, validation_split = 0.2)#, callbacks=callbacks_list)\n    NN_model.save(modelfilepath)\nelse:\n    NN_model=tf.keras.models.load_model(modelfilepath)","7ec0db49":"NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNNResults=saveModelResults(NN_model, \"NN\", X_te_rescaled, y_te)","aaa69832":"def rand_jitter(arr, delta, feature_name):\n    if not feature_name in continuous:\n        return arr + delta \n    else:\n        return arr\n\nallModels=[kNNResults, rdForestResults, NNResults, ridge_simple ]\n\nfor i in range(0, len(selected_model)):\n    plt.figure(figsize=(12,8))\n    plt.title('ML comparison for '+selected_model[i])\n    plt.xlabel(selected_model[i])\n    plt.ylabel(target)\n    x = scaler.inverse_transform(X_te_rescaled)[:,i]\n    #plt.scatter(rand_jitter(x, 0, selected_model[i]), y_te, label=\"Test set\", alpha=.5)\n    sns.violinplot(y=rand_jitter(x, 0, selected_model[i]), x=y_te)\n#    for idx,model in enumerate(allModels):\n#        delta = (idx+1)\/(len(allModels)*4)\n#        plt.scatter(rand_jitter(x, delta, selected_model[i]), model[1].predict(X_te_rescaled), label=model[2],alpha=.5)\n        \n#        plt.legend()\n    plt.show()","fccb3e29":"# Final comparison\nmae_values = [mae_baseline, ridge_simple[0], NNResults[0], rdForestResults[0], kNNResults[0]]\ntitles = ['Median', 'Ridge','NN', 'RFR', 'kNN']\n\nxcor = np.arange(len(mae_values))\nplt.bar(xcor, mae_values)\nplt.xticks(xcor, titles)\n\nplt.ylabel('MAE')\nplt.show()","14bfa95f":"def grid_search(X_tr_rescaled, y_tr, X_te_rescaled, y_te):\n    gs_results=[]\n    \n    pipeline1 = Pipeline((\n    ('rfr', RandomForestRegressor()),\n    ))\n\n    pipeline2 = Pipeline((\n    ('knn', KNeighborsRegressor()),\n    ))\n\n    parameters1 = {\n    'rfr__n_estimators': [5, 10, 15, 20, 50, 100, 200, 500], #np.arange(5,105,5).tolist(),\n    #'rfr__criterion': ['mae'], too slow, see https:\/\/stackoverflow.com\/questions\/57243267\/why-is-training-a-random-forest-regressor-with-mae-criterion-so-slow-compared-to\n    'rfr__max_features': ['auto', 'log2', 'sqrt', None],\n    'rfr__max_depth': [5, 10, 15, 20, 50, 100, None]\n    }\n\n    parameters2 = {\n    'knn__n_neighbors': [3, 7, 10, 15, 20, 50, 100, 200, 500],\n    'knn__weights': ['uniform', 'distance'],\n    'knn__p': [1, 2]\n    }\n\n    pars = [parameters1, parameters2]#, parameters3, parameters4]\n    pips = [pipeline1, pipeline2]#, pipeline3, pipeline4]\n\n    print(\"Starting Gridsearch\")\n    for i in range(len(pars)):\n        gs = GridSearchCV(pips[i], pars[i], verbose=2, scoring='neg_mean_absolute_error', n_jobs=-1)#, return_train_score=True)\n        gs.fit(X_tr_rescaled, y_tr)\n        print (\"MAE score\", gs.best_score_ , \"with parameters\",gs.best_params_)\n        # Save model and its performance on train\/test sets\n        gs_results.append({\n            'model': gs.cv_results_,\n            'best_params': gs.best_params_,\n            'train_mae': gs.best_score_,\n            'test_mae': gs.score(X_te_rescaled, y_te),\n        })\n    return pd.DataFrame(gs_results)\ngs=grid_search(X_tr_rescaled, y_tr, X_te_rescaled, y_te)  ","06a20f33":"from tensorflow.keras import regularizers\n\ndef build_model(number_of_hidden_layers=1, number_of_neurons=2):#,l2_penalty=0):\n  model = Sequential()\n\n  # First hidden layer\n  model.add(Dense(number_of_neurons, kernel_initializer='normal', input_dim = X_tr_rescaled.shape[1], activation='relu')) #, kernel_regularizer = regularizers.l2(l2_penalty)))\n\n  # hidden layers\n  for hidden_layer_number in range(1, number_of_hidden_layers):\n    model.add(Dense(number_of_neurons, kernel_initializer='normal', activation='relu'))\n\n  # output layer\n  model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n  model.compile(optimizer='adam', loss='mean_absolute_error')\n\n  return model\n\ntuned_model = KerasRegressor(build_fn=build_model)\n\n\nparams = {\n      'number_of_hidden_layers': [2, 3, 4, 5, 10, 15, 20], #Deeper Network Topology\n      'number_of_neurons': [16, 32, 64, 128] #Wider Network Topology\n      #'l2_penalty': np.logspace(-4, -1, num=5) #array([0.0001    , 0.00056234, 0.00316228, 0.01778279, 0.1       ])\n      }\n\n# Create a randomize search cross validation object, to find the best hyperparameters it will use a KFold cross validation with 4 splits\nrandom_search = RandomizedSearchCV(tuned_model, param_distributions = params, cv = KFold(4), n_jobs=2, scoring=\"neg_mean_absolute_error\", n_iter=8)\n#random_search = GridSearchCV(tuned_model, params, cv = KFold(4))\n\n# find the best parameters!\nrandom_search.fit(X_tr_rescaled, y_tr)\n\nrandom_search.best_estimator_.get_params()","ee56a5cc":"selected_model=features_grade_model\nX_tr_rescaled, X_te_rescaled, y_tr, y_te, scaler = splitTrainTest(clean_df,target,selected_model)\n#grid_search(X_tr_rescaled, y_tr, X_te_rescaled, y_te)  ","c54f4137":"#{'knn__n_neighbors': 10, 'knn__p': 2, 'knn__weights': 'distance'}\nn_neighbors=200\np=1\nweights='distance'\n\nneigh = KNeighborsRegressor(n_neighbors=n_neighbors, p=p, weights=weights)\nneigh.fit(X_tr_rescaled, y_tr)\n\n\nkNNResults=saveModelResults(neigh, \"KNN\", X_te_rescaled, y_te)","f358e104":"allModels=[kNNResults, ridge_grade ]\n\nfor i in range(0, len(selected_model)):\n    plt.figure(figsize=(12,8))\n    plt.title('ML comparison for '+selected_model[i])\n    plt.xlabel(selected_model[i])\n    plt.ylabel(target)\n    x = scaler.inverse_transform(X_te_rescaled)[:,i]\n    plt.scatter(rand_jitter(x, 0, selected_model[i]), y_te, label=\"Test set\", alpha=.5)\n    for idx,model in enumerate(allModels):\n        delta = (idx+1)\/(len(allModels)*4)\n        plt.scatter(rand_jitter(x, delta, selected_model[i]), model[1].predict(X_te_rescaled), label=model[2],alpha=.5)\n        \n        plt.legend()\n    plt.show()","2698f28b":"### 5.5 Extra - Calculate MAE KNN with grade model\n* We calculate the MAE for the grade model (instead of the simple)\n* MAE score is worst than for the simple model which is normal has the grade model include low scoring features","11eceb17":"* Prepare X and y variables for the SelectKBest","7f25dfc8":"#### 2.b.2 Continuous features encoding exploration\n* We try different types of features encoding here\n* Based on our observations, we see that \n    * *weight* sub-features require $\\exp^{1\/2}$ transformation\n    * *asset* sub-features require $\\log(x+1)$ transformation\n    * *Financial performance* features do not require transformation as they natively display a bell curve shape","22784ca8":"* The results from this plot shows that differenciation does not help to remove trend, we will not use this strategy.","2f5b2be4":"#### 2.b.4 Remove highly correlated features\n* The goal of this step is to identify and remove high correlated features (as we introduced many new features)\t","5c17dc65":"### 5.3 ML training regressors\n* The simple model will be used for other ML training regressors for this chapter\n* The goal is to try non-linear ML regressors to validate if we can achieve better performance (lower MAE) than ridge regression\n* N.B: Hyperparameter optimization has already been ran for the ML parameters. The chapter regarding Hyperparameter will be presented later on.","11f7ae79":"#### 2.a.11 Missing data\nWith the plot below, we want to verify if there is a data gap:\n * We can identify a data gap for November 2020 \n * The number and allocation of asset managers per date is quite constant","e1ed80ea":"* We plot the top features per category\n * Grade features appear quite often at the top of each category","ef1263fd":"It was found that the best combination of hyperparameters is:\n - 'number_of_hidden_layers': 10,\n - 'number_of_neurons': 128","7b59ba52":"* To catch duplicate values, as there is no unique id available per row, we will have to use a multiindex with the following combination\n'FI_ShareclassName', 'FP_PerformanceAs-OfDate'\n* We take FI_ShareclassName for this selection as it containes the most unique dataset ","872678e0":"#### 2.a.3 Feature distribution\n* The numerical features distribution per category is shown below \n* We have features with empty data, notably on categories *Gender Equality Funds* and *Prison Free Funds*\n* Several times, we see a global feature (e.g. *Fossil fuel holdings*) being decomposed on 3 sub-features: count, weight, asset\n    * These 3 sub-features should normally be related to each other (high correlation)","bbf37327":"### 2.b Process the data\n#### 2.b.1 Histogram for numeric data\n* There is a mix of continuous, discrete values  \n* Lots of 0-values are present that we need to address\n* We observe different type of skewness\n","757b5f84":"#### 2.a.2 Feature overview\n* We can assess with the code below that we have a large number of columns (121) and rows (101407). The number of rows being significantly higher than the number of columns, that should help us avoid having too much bias in our model\n* 41 columns have null values so we will have to find a way to handle that\n* We have 101 numerical values","e7b61b95":"* Sample dataset (first 10 rows)","78cb6c1a":"* Plot features to validate visualize the effectivness of our preprocessing method","f3d78d9f":"*Additional information* :\n* As suggested, the following clustermap allows to cluster correlated features together.\n* We can observe several clusters with a mix of different features category\n* *asset* and *count* type of features are clustered together: this is certainly due to the similar data distribution\n* *weight* type of features do not show such clustering which might indicate more independances betweens these variables\n* A possible result for these findings will be to group highly correlated features together\n    * In the part 2, highly correlated features have been removed to overcome this situation\n","4a7e911e":"# Project proposal\n\nIn the financial fund industry, does green investment generate better performance ?   \n\n## 1) The problem\n* The idea of this project came from the raising interest in ESG funds to provide more sustainable investment vehicles to financial investors.\n* The aim of this project is to analyze if funds selecting underlying environmental friendly assets can generate better financial performance.\n* Finding available market data about ESG was a difficult task, as this kind of data is most of the time issued by market\/index providers (MSCI, FTSE) and not freely available to the public. Hopefully, the platform https:\/\/fossilfreefunds.org\/ offers more than 3,000 US stock funds ratings about environmental metrics with more than 10 months history.    \n\n## 2) The data\n\n### (a) Clear overview of your data\n#### 2.a.1 Data origin and gathering\n* The raw data can be acquired through download of Excel files directly from the webpage https:\/\/fossilfreefunds.org\/how-it-works\n* We can find 2 Excels files published each month. We only use the Fossil Free Funds (FFF) dataset where detailed information is contained \n* The code below allows the download and import of these Excel files into a single csv file. I will be using mainly the \"Shareclasses\" sheet as it contains all the metrics for our analysis.","6493391c":"* We can verify that we have no more null values and only 9254 have been removed","b37c424a":"#### 2.a.4.1 Remove non relevant columns\n* We drop the columns with less than 60% of data completness, mostly related to Gender equality category","4f053d25":"#### 5.4.2 Project conclusion","5819e6ef":"#### 2.b.3 Pre-processing numerical features","69edc203":"* We can already convert date columns as date type instead of generic object","9a4d7e45":"#### 3.a.1 Correlation of target with features\n* The following plot also comparison between the performance target with other features. It shows that correlation of performance variables against other features are mostly on the blue zone (negative correlation)\n* The most negative correlation is with Fosssil Fuel Holdings features","8189a036":"### 5.4 Compare ML with target and conclusion\n* In this section, we compare the different features of our simple model (F_RelativeCarbonFootprint, P_PrisonIndustrialComplexGrade, F_FossilFuelGrade) against its target for the different ML regressor seen above \n* We can visually assess the nice fitting that random forest and KNN provided in comparison to Ridge and NN","f8f81f4b":"* The results above show a clear indication that funds with a high level of invested fossil-fund stocks display lower returns\n* For the other type of features (Gender equality, Weapon, ...), the trend is similar in a lower scale.\n* The date being taken from 2020, it would require a larger timeframe to assess if this correlation has always been this way or not.\n* Fortunately, these findings serve as a great marketing tool for the investment industry to transition to more green-friendly investment vehicules which is a good thing from an economical point of view and even better for the sustainability of our planet from an environmental perspective.","57fc62e2":"#### 2.a.4 Data completness\n* It is important to check the data completness fill-factor\n* We see that *Prison free* related category lacks information as well as some *Gender equality* indicators","88879b24":"* The result of the NN is similar to the ridge one. This is explainable as the model and layers used on our model tend to reproduce a linear regression (Sequential)","485de24f":"* Perform some check after the processing, we observed a large increase of new columns","a06dc729":"#### 5.3.3 Neural networks \nWe have defined here a neural network with the following properties: \n * We have define a sequential model, with some dense layers\n * Used '**relu**' as the activation function in the hidden layers\n * Used a '**normal**' initializer as the kernal_intializer (Initializers define the way to set the initial random weights of Keras layers)\n * Mean_absolute_error is our loss function\n * We defined the output layer with only one node and used 'linear 'as the activation function for the output layer","b0313896":"#### 2.a.7 Columns renaming\n* Column names are renamed to make plots more readable by using the Excel spreadsheet previously defined","d9283b48":"## 4) Machine learning \n### (a) Phrase your project goal as a clear machine learning question\n* The outcome of this project is to predict performance of a fund based on ESG features\n* The target is the *Financial performance: Month end trailing returns, year 1* variable\n* This is a regression problem \n* Calculating a regression tree (random forest regressor) for identifing key indicators for good fund selection will also be experimented\n\n### (b) What models are you planning to use and why?\n* The principal ML training algorithm that will be used is the *Ridge regression* with :\n    * Splitting for test and training data set\n    * Standardization of data based on training set\n    * Grid search for finding the best regularization (alpha parameter)\n* The features will be select among through SelectKBest method (f_regression, mutual_info_regression)\n* Models will range from \n    * simple model: 3 features\n    * intermediate model: 10 features to 20 features\n    * complex model: all features\n* We will also experiment using other ML training regressors such as *RandomForests, kNNs and neural networks* to allow comparison with results from *Ridge regression*\n\n### (c) Please tell us your detailed machine learning strategy \n* The pre-processing part will mainly required that we have no null values in our dataset\n* Ridge regression with optimization of the regularization parameter will be used\n* Our baseline will be the *median* of performance price return\n* For assessing the accuracy of our model (cost function), we will use the MAE (Mean Absolute Error) method:\n  * MAE is easy to interpret. For instance, a score of 3 means that the predictions are, in average, above or below the observed value by a distance of 3.\n  * The MAE method is robust to outliers which is a nice statistical property but is difficult to optimize because it's not smooth.\n\n* Assess individual modeles\n* How could you maybe assess individual models in more detail than just their score? How could you identify weaknesses?\n    * Assessment will be done through EDA, that is comparing the estimator resulting to our ML training against its target. The goal is to find possible root cause for the accuracy error and potentially adding\/modifying features to reduce this error","7e1ca952":"#### 5.5.2 NN hyperparameters \n* To tune NN hyperparameters we will need a function building\/returning a model.\n* This function's parameters must be our model hyperparameters that we want to tune.\n* As you can see below - we will be tuning:\n - the number of hidden layers\n - the number of neurons in each hidden layer.","38d5bd8d":"### 5.2 To get your top feature names\n* We define our 3 models based on the previous observation \n * Simple: we only keep 3 features not related to date and not related to absence of scoring (*is_empty*). We also do not select *P_PrisonIndustrialComplexGrade* as this feature had a high number of null values (33.64 %) that have been replaced by median values\n * Grade: we keep only grade features, except for the Gender equality category where *GE_WeightOfHoldings* has better scoring than the grading feature and discard *G_CivilianFirearmGrade* feature which has a too low score to be included\n * Intermediate: all top 20 features\n * Complex: all 122 features","1f3cf286":"* Verify that we have no null values","10afe3fa":"The \"Financial performance: Month end trailing returns, year 1\" is the most complete variable as a high percentage of funds have an inception date greater than 2 years (so performance for \"year 1\" is available) but less than 10 years (so no performance data available for \"year 10\")","867a5421":"* Train model and save the results to avoid model fitting calculation afterwards","86742700":"#### 2.a.10 Duplicate values check\n* We first try to see if there is some duplicate row => the result is none","6d44d5c7":"* For this stage, we define a RandomForestRegressor with optimal parameters coming from our hyperparameter optimization\n* We achieve significantly better performance than ridge regression ! The non-linearity might explain such performance","c2256269":"* We decide to drop some columns as they do not have relevant information\n * FI_FundName and FI_ShareclassName are not highly correlated to our target\n * \\*FI_Ticker and FI_ShareclassTickers features are identical to FI_FundName and FI_ShareclassName  \n * FI_AssetManager : that might have been interesting to show if some an asset management company would be more linked to performance but an EDA with one-hot encoding of this feature did not show strong correlation results relative to our target.","127dfa8e":"#### 2.a.5 Target selection\n* Our target column will be part of the group \"Financial performance\". \n    * By the table below, we select the target (among year 1,3,5,10) with the top fill-factor, that is \"year 1\". ","bf7afbb0":"* We decide to keep only the first occurence of duplicated rows together based on 'FI_ShareclassName', 'FP_PerformanceAs-OfDate'","f1806ff5":"#### 5.3.1 RandomForests\n* We will be using the RandomForestRegressor as ML regressor\n* Explain parameter of this ML\n* We first plot a graph with a limited depth random forest to visually assess how random forest behaves","516a4c23":"# 5.Model fitting\n## 5.1 Features selection\n* Make use of SelectKBest (f_regression, mutual_info_regression) to select features of the simple model and intermediate model.\n* Check that all columns of df are numbers (no null values allowed)","656afdc9":"#### 2.a.11.1 Missing date\n* Replace missing performance date with the performance date of the previous row","9adf418b":"## 3) Exploratory data analysis (EDA)\n### (a) Preliminary EDA\n","b7625831":"* With the sample dataset above, we can observe the following group of features identified by the column names:\n* *Fund information*: general information about the fund\n    * Fund profile: Shareclass name\n    * Fund profile: Ticker\n    * Fund profile: Fund name\n    * Fund profile: Asset manager\n    * Fund profile: Shareclass type\n    * Fund profile: Shareclass inception date\n    * Fund profile: Category group\n    * Fund profile: Sustainability mandate\n    * Fund profile: US-SIF member\n    * Fund profile: Oldest shareclass inception date\n    * Fund profile: Shareclass tickers\n    * Fund profile: Portfolio holdings as-of date\n    * Fund profile: Fund net assets\n    * Fund profile: Percent rated\n* *Financial results*: our target variable is contained in that group\n    * Financial performance: Financial performance as-of date \n        * *Important date, keeps track of the date where performance of the fund is calculated*\n    * Financial performance: Month end trailing returns, year 1\n    * Financial performance: Month end trailing returns, year 3\n    * Financial performance: Month end trailing returns, year 5\n    * Financial performance: Month end trailing returns, year 10\n* *Grading metrics*: overall grading by the FFF organization\n    * Fossil Free Funds: Fossil fuel grade\n    * Deforestation Free Funds: Deforestation grade\n    * Gender Equality Funds: Gender equality grade\n    * Gun Free Funds: Civilian firearm grade\n    * Weapon Free Funds: Military weapon grade\n    * Tobacco Free Funds: Tobacco grade\n    * Prison Free Funds: Prison industrial complex grade\n* *Detailed breakdown by categories*:\n    * Fossil energy:\n        * Fossil Free Funds: Fossil fuel holdings, count\n        * Fossil Free Funds: Fossil fuel holdings, weight\n        * Fossil Free Funds: Fossil fuel holdings, asset\n        * Fossil Free Funds: Carbon Underground 200, count\n        * Fossil Free Funds: Carbon Underground 200, weight\n        * Fossil Free Funds: Carbon Underground 200, asset\n        * Fossil Free Funds: Coal industry, count\n        * Fossil Free Funds: Coal industry, weight\n        * Fossil Free Funds: Coal industry, asset\n        * Fossil Free Funds: Oil \/ gas industry, count\n        * Fossil Free Funds: Oil \/ gas industry, weight\n        * Fossil Free Funds: Oil \/ gas industry, asset\n        * Fossil Free Funds: Macroclimate 30 coal-fired utilities, count\n        * Fossil Free Funds: Macroclimate 30 coal-fired utilities, weight\n        * Fossil Free Funds: Macroclimate 30 coal-fired utilities, asset\n        * Fossil Free Funds: Fossil-fired utilities, count\n        * Fossil Free Funds: Fossil-fired utilities, weight\n        * Fossil Free Funds: Fossil-fired utilities, asset\n        * Fossil Free Funds: Relative carbon footprint (tonnes CO2 \/ $1M USD invested)\n        * Fossil Free Funds: Relative carbon intensity (tonnes CO2 \/ $1M USD revenue)\n        * Fossil Free Funds: Total financed emissions scope 1 + 2 (tCO2e)\n        * Fossil Free Funds: Total financed emissions scope 1 + 2 + 3 (tCO2e)\n        * Fossil Free Funds: Carbon footprint portfolio coverage by market value weight\n        * Fossil Free Funds: Carbon footprint portfolio coverage by number of disclosing titles\n        * Fossil Free Funds: Clean200, count\n        * Fossil Free Funds: Clean200, weight\n        * Fossil Free Funds: Clean200, asset\n    * Deforestation:\n        * Deforestation Free Funds: Deforestation-risk producer, count\n        * Deforestation Free Funds: Deforestation-risk producer, weight\n        * Deforestation Free Funds: Deforestation-risk producer, asset\n        * Deforestation Free Funds: Deforestation-risk financier, count\n        * Deforestation Free Funds: Deforestation-risk financier, weight\n        * Deforestation Free Funds: Deforestation-risk financier, asset\n        * Deforestation Free Funds: Deforestation-risk consumer brand, count\n        * Deforestation Free Funds: Deforestation-risk consumer brand, weight\n        * Deforestation Free Funds: Deforestation-risk consumer brand, asset    \n     * Gender Equality:\n        * Gender Equality Funds: Gender equality group ranking\n        * Gender Equality Funds: Gender equality score (out of 100 points)\n        * Gender Equality Funds: Gender equality score, gender balance (out of 100 points)\n        * Gender Equality Funds: Gender equality score, gender policies (out of 100 points)\n        * Gender Equality Funds: Count of holdings with Equileap gender equality scores\n        * Gender Equality Funds: Weight of holdings with Equileap gender equality scores\n        * Gender Equality Funds: Gender equality score - Overall score (out of 100 points)\n        * Gender Equality Funds: Gender equality score - Gender balance in leadership and workforce (out of 40 points)\n        * Gender Equality Funds: Gender equality score - Equal compensation and work life balance (out of 30 points)\n        * Gender Equality Funds: Gender equality score - Policies promoting gender equality (out of 20 points)\n        * Gender Equality Funds: Gender equality score - Commitment, transparency, and accountability (out of 10 points)            \n    * Gun:\n        * Gun Free Funds: Civilian firearm, count\n        * Gun Free Funds: Civilian firearm, weight\n        * Gun Free Funds: Civilian firearm, asset\n        * Gun Free Funds: Gun manufacturer, count\n        * Gun Free Funds: Gun manufacturer, weight\n        * Gun Free Funds: Gun manufacturer, asset\n        * Gun Free Funds: Gun retailer, count\n        * Gun Free Funds: Gun retailer, weight\n        * Gun Free Funds: Gun retailer, asset\n    * Weapon:            \n        * Weapon Free Funds: Military weapon, count\n        * Weapon Free Funds: Military weapon, weight\n        * Weapon Free Funds: Military weapon, asset\n        * Weapon Free Funds: Major military contractors, count\n        * Weapon Free Funds: Major military contractors, weight\n        * Weapon Free Funds: Major military contractors, asset\n        * Weapon Free Funds: Nuclear weapons, count\n        * Weapon Free Funds: Nuclear weapons, weight\n        * Weapon Free Funds: Nuclear weapons, asset\n        * Weapon Free Funds: Cluster munitions \/ landmines, count\n        * Weapon Free Funds: Cluster munitions \/ landmines, weight\n        * Weapon Free Funds: Cluster munitions \/ landmines, asset\n    * Tobacco:            \n        * Tobacco Free Funds: Tobacco producer, count\n        * Tobacco Free Funds: Tobacco producer, weight\n        * Tobacco Free Funds: Tobacco producer, asset\n        * Tobacco Free Funds: Tobacco-promoting entertainment company, count\n        * Tobacco Free Funds: Tobacco-promoting entertainment company, weight\n        * Tobacco Free Funds: Tobacco-promoting entertainment company, asset\n    * Prison:            \n        * Prison Free Funds: All flagged, count\n        * Prison Free Funds: All flagged, weight\n        * Prison Free Funds: All flagged, asset\n        * Prison Free Funds: Prison industry, count\n        * Prison Free Funds: Prison industry, weight\n        * Prison Free Funds: Prison industry, asset\n        * Prison Free Funds: Border industry, count\n        * Prison Free Funds: Border industry, weight\n        * Prison Free Funds: Border industry, asset\n        * Prison Free Funds: All flagged, higher risk, count\n        * Prison Free Funds: All flagged, higher risk, weight\n        * Prison Free Funds: All flagged, higher risk, asset\n        * Prison Free Funds: Prison industry, higher risk, count\n        * Prison Free Funds: Prison industry, higher risk, weight\n        * Prison Free Funds: Prison industry, higher risk, asset\n        * Prison Free Funds: Border industry, higher risk, count\n        * Prison Free Funds: Border industry, higher risk, weight\n        * Prison Free Funds: Border industry, higher risk, asset\n        * Prison Free Funds: Private prison operators, count\n        * Prison Free Funds: Private prison operators, weight\n        * Prison Free Funds: Private prison operators, asset\n\n* So we have a wide range of data information for each fund classification category: from overall category grades to the detailed breakdown of each category \n* This classification is being kept under an Excel spreadsheet (replace.xlsx)\n* The information contained in this spreadsheet will also be used later on for column renaming and feature aggregation (continuous, discrete, nominal, ordinal variables)","d9e69c3f":"### 2.b Plan to manage and process the data\n\n#### Data cleaning and data manipulation\n- [x] A row duplicate cleaning process needs to be performed\n- [x] Features wit nearly empty fill-factor will be removed ('Fund profile: US-SIF member') as well as non-relevant\/redundant categorical columns ('Fund profile: Ticker','Fund profile: Shareclass tickers')\n- [x] Dates column need to be checked for consistency (within our analysis range) and integrity (no missing dates) \n- [x] For the set of 'asset, count, weight' features, we need to ensure strong correlation between these variable and consistency  \n    * If one of these variable equals zero (count or weight or asset = 0), we need to make sure that remaining 2 other variables should be 0\n\n#### Feature engineering\n- [x] Grades features will need to be encoded => ordinal encoding\n* Pre-processing for numerical data:\n    - [x] Null values will be replaced with median values\n    - [x] Features with high number of 0-Values will be identified (new column _isempty will be added) \n    - [x] Outliers will also need to be removed (e.g by deleting entries ranging +-10 std of the mean, assuming a gaussian distribution)\n* Pre-processing for continuous data, it might be required to do a transformation (log, exp) to reduce the skewness\n* Pre-processing for categorical data:\n    - [x] We need to make sure date is encoded consistently, verify the number of unique values\n    - [x] n\/a values will be replaced with NoMapping value\n    - [x] categorical data will be one-hot encoded","0773671c":"We can have more information provided by FFF about each feature description in the \"Key\" Excel sheet provided by FFF:","8c313dfd":"#### We observe which funds (shareclassname) have partial information\n* We decide to remove funds with less than 60% complete of information","aae6bfc5":"#### 5.3.2 kNNs \n* We use KNeighborsRegressor as ML regressor\n* Similar to random forest, we achieve the best results due to the non-linearity of this ML ","3f2c56ee":"* We first check how many duplicate entries with these 2 features combination: #9254 rows duplicate","536d57b1":"#### 2.b.2 Best encoding assessment\n* We pre-process slightly the features here (remove 0-values and replace null values by the median) and encode the features according to our findings\n* We can verify gaussian distribution across all our numerical features","98d8c96a":"* Special feature distribution handling","164d3f80":"#### 2.a.8 0-Values  \n* It is important to keep track of feature with a high percentage of low values so we can add this information later in our analyis\n* We observe that the features displaying a higher number of 0-values than our threshold are part of the sub-features (count, weight, asset) ","16a4fe9c":"### 3.b) Discuss how the EDA informs your project plan\n- [x] For the moment, we do not see a strong relationship (> +\/-0.5) between features and our target, except for financial performance features which are closely tied together \n    * Stronger correlations with other features with Y3, Y5, Y10 than Y1\n- [x] The situation will improve with processing of 0\/null values, the grade encoding and feature engineering (log\/exp encoding)\n\n### 3.c) What further EDA do you plan for project?\n- [x] More preparation of data is required at this stage so we can ensure a good data set before our modelling for the next step","d4cb31b8":"After looking at the raw data, it appears data has been published twice in 2 datasets:\n* Invest+Your+Values+shareclass+results+20200913.xlsx\n* Invest+Your+Values+shareclass+results+20200928.xlsx","e28a7e48":"#### 2.a.12 Categorical analysis\n* We show below the unique values for all the categorical columns","b68b59fd":"#### 2.a.9 Null values\n* Lot of null values for nominal and ordinal data\n* We will replace n\/a with median values for numerical data in the feature encoding section  \n* For categorical data, n\/a values will be replaced with NoMapping value","ee197241":"* We finally compare the MAE and we can assess that the full model performs better than the other models (lowest MAE)","2af805d9":"#### 2.b.5 Pre-processing categorical\n* One-hot encode categorical features, except for grade where ordinal encoding is used","c240b7a3":"* We calculate here the MAE score for each model using Ridge regression with grid search tuning for the regularization (alpha) parameter\n\n* Note: Because in linear regression the value of the coefficients is partially determined by the scale of the feature, and in regularized models all coefficients are summed together, we must make sure to standardize the feature prior to training.","6708de0f":"* We plot here the top 20 predictors (with the highest score):\n * Performance Reporting date has the best score\n * Grade and Carbon footprint are also present in the top 5\n * Absence of information (\\_isempty) appear quite often","85257488":"#### 2.a.6 Data uniqueness\n* Performance data is the only type of information showing strong uniqueness","e0f8af5b":"#### 5.4.1 MAE final comparison","c19bf1f7":"* To get even further, we compute pairplots between our target and features to observe these correlation further\n    * Financial performance (Y3,Y5,Y10) features are strongly correlated \n    * We observe the downward regression for F_FossilFuelHoldings_w\n    * A lot of 0-values and outliers can be seen, this will need to be addressed \n    * Sampling had to be used to reduce the plot time\n","e14a680b":"### 5.5 Annex - Hyper parameter optimization\n* This section aims to discover the best hyperparameters for the different ML training models\n#### 5.5.1 Grid search for Random Forest and K-NN models\n* Negative MAE is normal https:\/\/stackoverflow.com\/questions\/21443865\/scikit-learn-cross-validation-negative-values-with-mean-squared-error","d2524726":"* Here are the top positive\/negative correlations. Fossil Fuel holdings (weight) is the most correlated with our target","c87567c7":"#### 3.a.2 Remove trend\n\n* We try here to identify if our target has some trend that we get rid of (e.g. we had a stock market overall recovery process during 2020). One way of removing the trend is through *differencing*, which consists in computing the difference between consecutive observations. \n* More information on https:\/\/machinelearningmastery.com\/remove-trends-seasonality-difference-transform-python\/"}}