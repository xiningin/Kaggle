{"cell_type":{"50aa67f8":"code","77e9de79":"code","7bd0ecdb":"code","d99787c3":"code","053c2e00":"code","cde39751":"code","a3cc3429":"code","d003d8da":"code","66964aaf":"code","daee6570":"code","7c04b3ec":"code","9487906f":"code","1810536a":"code","934829d4":"code","4ff18b17":"code","92390a31":"code","41d36447":"markdown","f78622a6":"markdown","b72de368":"markdown","4c407118":"markdown","96ce0de9":"markdown","f72943ae":"markdown","764e399f":"markdown","c469e9bc":"markdown","7736f605":"markdown","26becbf3":"markdown","488e0557":"markdown","90d59414":"markdown","92e42f24":"markdown","1304f7d9":"markdown","79f96279":"markdown","fd88b365":"markdown","c7ce2c35":"markdown","1141e44c":"markdown","7fdfc07a":"markdown","07d3cc45":"markdown","6e5151a5":"markdown","892e755e":"markdown","3ee2c556":"markdown"},"source":{"50aa67f8":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as ltb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pd.set_option('display.max_columns', None)","77e9de79":"# tweet\n\ntweets = pd.read_csv('Tweets\/train_tweets.csv')\ntweets_media = pd.read_csv('Tweets\/train_tweets_vectorized_media.csv')\ntweets_text = pd.read_csv('Tweets\/train_tweets_vectorized_text.csv')\n\n# user\nuser = pd.read_csv('Users\/users.csv')\nuser_desc = pd.read_csv('Users\/user_vectorized_descriptions.csv')\nuser_img = pd.read_csv('Users\/user_vectorized_profile_images.csv')\n\n# teste\nteste_tweets = pd.read_csv('Tweets\/test_tweets.csv')\nteste_tweets_media = pd.read_csv('Tweets\/test_tweets_vectorized_media.csv')\nteste_tweets_text = pd.read_csv('Tweets\/test_tweets_vectorized_text.csv')\n\n# solution format\nsol_format = pd.read_csv('solution_format.csv')","7bd0ecdb":"# tratando os tweets e teste_tweets\n\n# Atribuindo numero as variaveis nas colunas categoricas\/nominais selecionadas\n\ncat_vars=['tweet_attachment_class']\nlbl = preprocessing.LabelEncoder()\nfor i in cat_vars:\n    tweets[i] = lbl.fit_transform(tweets[i].astype(str))\n    teste_tweets[i] = lbl.transform(teste_tweets[i].astype(str))\n\n# tratando a coluna tweet_has_attachment \n\ntweets['tweet_has_attachment'] = tweets['tweet_has_attachment'].astype(int)\nteste_tweets['tweet_has_attachment'] = tweets['tweet_has_attachment'].astype(int)","d99787c3":"# Criando uma coluna com a contagem de midia de um tweet\n\n# treino: tweets\n\n# Criando uma c\u00f3pia de tweets e tweets_media para nao alterar o dataset original\ntw_m = tweets_media.copy()\ntw = tweets.copy()\n\n# df com a quantidade de midia de cada tweet\ntw_m['count_media'] = tw_m.groupby('tweet_id')['tweet_id'].transform('count')\ndf_count = tw_m.loc[:,['tweet_id','count_media']]\ndf_count.drop_duplicates(subset=['tweet_id'], inplace=True)\n\n# adicionando a coluna count_media ao dataset tw(tweets)\ntw = pd.merge(tw,df_count, how=\"left\", on = ['tweet_id'])\ntw.count_media.fillna(0,inplace=True)\n\n\n# teste: teste_tweets\n\n# Criando uma c\u00f3pia de teste_tweets e teste_tweets_media para nao alterar o dataset original\nteste_tw_m = teste_tweets_media.copy()\nteste_tw = teste_tweets.copy()\n\n# df com a quantidade de midia de cada tweet\nteste_tw_m['count_media'] = teste_tw_m.groupby('tweet_id')['tweet_id'].transform('count')\nteste_count = teste_tw_m.loc[:,['tweet_id','count_media']]\nteste_count.drop_duplicates(subset=['tweet_id'], inplace=True)\n\n# adicionando a coluna count_media aos dataset tw(tweets) e teste_tw(teste_tweets)\nteste_tw = pd.merge(teste_tw, teste_count, how=\"left\", on = ['tweet_id'])\nteste_tw.count_media.fillna(0,inplace=True)","053c2e00":"# Fun\u00e7ao que retorna a contagem dos ids na coluna tweet_topic_ids\n\ndef to_1D(series):\n    \n    return pd.Series([x for _list in series for x in _list])\n\n# Fun\u00e7ao que retorna os valores unicos de uma lista\n\ndef unique(lista):\n    \n    lista_unica = list()\n    \n    for i in lista:\n        if i not in lista_unica:\n            lista_unica.append(i)\n\n    return lista_unica\n\n# Fun\u00e7ao que retornar um DataFrame, onde as colunas sao compostas pelos unique_items\n\ndef boolean_df(item_lists, unique_items):\n    \n    bool_dict = {}\n\n    for i, item in enumerate(unique_items):\n        bool_dict[item] = item_lists.apply(lambda x: item in x)\n\n    return pd.DataFrame(bool_dict)","cde39751":"# Transformando os valores de tweet_topic_ids em listas\n\ntw.tweet_topic_ids.fillna('[0]',inplace=True)\ntw['tweet_topic_ids'] = tw['tweet_topic_ids'].apply(lambda x: eval(x))\n\nteste_tw.tweet_topic_ids.fillna('[0]',inplace=True)\nteste_tw['tweet_topic_ids'] = teste_tw['tweet_topic_ids'].apply(lambda x: eval(x))\n\n\n# Criando uma lista com os valores unicos dos ids do dataset de treino(tw) e teste(teste_tw)\n\nids_treino = to_1D(tw[\"tweet_topic_ids\"]).value_counts().index\nids_treino = list(map(int, ids_treino))\n\nids_teste = to_1D(teste_tw[\"tweet_topic_ids\"]).value_counts().index\nids_teste = list(map(int, ids_teste))\n\nids = unique(ids_treino+ids_teste)\n\n\n# treino: df em que os id's sao colunas e as linhas sao do dataset treino(tweets)\n\ntw_bool = boolean_df(tw[\"tweet_topic_ids\"], ids)\n\n# teste: df em que os id's sao colunas e as linhas sao do dataset teste(teste_tweets)\n\nteste_tw_bool = boolean_df(teste_tw[\"tweet_topic_ids\"], ids)\n\n\n# Renomeando as colunas (ex: renomeando 80 para topic_id_80)\n\nfor i in ids:\n    tw_bool.rename(columns={i:'topic_id_'+str(i)}, inplace=True)\n    teste_tw_bool.rename(columns={i:'topic_id_'+str(i)}, inplace=True)\n\n# Transformando (True, False) em (1, 0)\ntw_bool = tw_bool*1\n\nteste_tw_bool = teste_tw_bool*1","a3cc3429":"# Juntando os datasets tweets, e tweets_text de treino\n\ntw = pd.concat([tw, tw_bool], axis=1)\ntw = pd.merge(tw, tweets_text, how='left', on=['tweet_id'])\n\n# Juntando os datasets tweets de teste\n\nteste_tw = pd.concat([teste_tw, teste_tw_bool], axis=1)\nteste_tw = pd.merge(teste_tw, teste_tweets_text, how='left', on=['tweet_id'])","d003d8da":"# Transformando (True, False) em (1, 0) nas colunas user_has_location e user_has_url\n \nuser.user_has_location = user.user_has_location.astype(int)\nuser.user_has_url = user.user_has_url.astype(int)\n\n# Renomeando a coluna user_id para tweet_user_id para ficar igual ao dataset tweets, tweets_media e tweets_text\n\nuser.rename(columns={'user_id':'tweet_user_id'}, inplace=True)\nuser_desc.rename(columns={'user_id':'tweet_user_id'}, inplace=True)\nuser_img.rename(columns={'user_id':'tweet_user_id'}, inplace=True)\n\n# Juntando os datasets user, user_desc, user_img em um \u00fanico dataset\n\ndf_user = pd.merge(user, user_desc, how='left', on=['tweet_user_id'])\ndf_user = pd.merge(df_user, user_img, how='left', on=['tweet_user_id'])","66964aaf":"# treino\n\ntw = pd.merge(tw, df_user, how='left', on=['tweet_user_id'])\n\n# teste\n\nteste_tw = pd.merge(teste_tw, df_user, how='left', on=['tweet_user_id'])","daee6570":"X = tw.drop(columns=['tweet_topic_ids','virality'])\ny = tw.loc[:,['virality']]\n\nteste_sem_target = teste_tw.drop(columns=['tweet_topic_ids'])","7c04b3ec":"pca = PCA(n_components=70)\npca.fit(X)\nX = pca.transform(X)\nteste_sem_target = pca.transform(teste_sem_target)","9487906f":"model = ltb.LGBMClassifier()\nmodel.fit(X,y)\npred = model.predict(teste_sem_target)","1810536a":"sol_format['virality'] = pred\nsol_format","934829d4":"model_cat = CatBoostClassifier()\nmodel_cat.fit(X,y)\npred_cat = model_cat.predict(teste_sem_target)","4ff18b17":"sol_format['virality'] = pred_cat\nsol_format","92390a31":"# sol_format.to_csv('results_cat_70.csv', index=False)","41d36447":"# User","f78622a6":"Para esse dataset, estou utilizando o CatBoost.\n\nTestei outros modelos, como o XGBoost e LightGBM, mas o CatBoost teve melhor score.","b72de368":"### Contagem de midia","4c407118":"### score 68.24932","96ce0de9":"# LightGBM","f72943ae":"# Treino e teste","764e399f":"### Transformando os ids de tweet_topic_ids em colunas","c469e9bc":"### PCA","7736f605":"Agora vou criar colunas individuais para cada id de tweet_topic_ids, pois \u00e9 possivel que tweets virais tenham ids em comum.<br>\n\nVou criar uma coluna para cada id, e caso o tweet nao possua nenhum id, criarei uma coluna para os tweets sem tweet_topic_ids. Nas colunas teremos os valores 0 ou 1, onde 0 significa que o tweet nao possui o id da coluna e 1 significa que possui.","26becbf3":"Utilizei PCA para reduzir a dimensionalidade do dataset, com n_components=70.<br>\n\nTestando os valores para n_components=(50, 60, 70), 70 foi o que apresentou o melhor resultado, sendo entao escolhido para o modelo.","488e0557":"# Catboost","90d59414":"Primeiro, vou tratar a coluna tweet_attachment_class, que est\u00e1 no formato 'str', atribuindo n\u00famero a essas vari\u00e1veis. Ap\u00f3s isso, substituir True e False por 1 e 0 na coluna tweet_has_attachment.\n","92e42f24":"Separando o treino em X e y.","1304f7d9":"![image.png](attachment:image.png)","79f96279":"# Tweets","fd88b365":"### Link para meu artigo no Medium: https:\/\/medium.com\/@joaopaulochen\/%C3%A9-poss%C3%ADvel-prever-se-um-tweet-vai-viralizar-74500ee30fb5","c7ce2c35":"### Criando um DataFrame com as informa\u00e7oes dos Tweets","1141e44c":"Criando os DataFrame treino e teste para treinamento do modelo e predi\u00e7\u00e3o.","7fdfc07a":"Para o treino, estou juntado os datasets tw, tw_bool e tweets_text.<br>\n\nPara o teste, estou juntando os datasets teste_tw, teste_tw_bool, teste_tweets_text.","07d3cc45":"# Leitura dos dados","6e5151a5":"Estou criando uma coluna com a quantidade de midia de cada tweet para substituir o dataset tweets_media e, assim, reduzir a dimensionalidade do nosso dataset de treino e teste.","892e755e":"score: 67.52925","3ee2c556":"Aqui estou juntando os datasets que possuem informa\u00e7oes dos usu\u00e1rios em um unico DataFrame."}}