{"cell_type":{"62e99cc4":"code","158cf03e":"code","21a97784":"code","e098f332":"code","b8ffd3ce":"code","920441c7":"code","20ee17f7":"code","0c60c484":"code","0294465a":"code","378c3ae1":"code","8b8c59b3":"code","4765fdee":"code","df88f34f":"code","cca368d1":"code","db542073":"code","66a13c62":"code","d252120f":"code","4321aa5d":"code","844a2b5c":"code","845aeae1":"code","500efcb9":"code","bd0e49c4":"code","c7efcfe8":"code","2cec5ab0":"code","ba326678":"code","ddef6b79":"code","03d09e8f":"code","9fc02f97":"code","2a2b3e4e":"code","12f08a37":"code","ed05a7b4":"code","41dbd115":"code","3b09dacd":"code","4a9641e7":"code","648b5524":"code","51ef8926":"code","a2a3a0f5":"markdown","c136e85b":"markdown","c236ef23":"markdown","3d8e1ae8":"markdown","6f2badaf":"markdown","030033b8":"markdown","1d19e884":"markdown","89cf0d9f":"markdown","1949ac1c":"markdown","cb46c4a4":"markdown","c260498b":"markdown","520fdc90":"markdown","f9cb4488":"markdown","7837740b":"markdown","5249788d":"markdown","05c85c44":"markdown","22ae9f3b":"markdown","882d479c":"markdown","6c9120b8":"markdown","7297db11":"markdown","0081ca84":"markdown","4bdf1151":"markdown","5f476605":"markdown","cc42ce9f":"markdown","7269d09e":"markdown","eb0feb8e":"markdown","c8478a33":"markdown","afd67e16":"markdown","bd02a6a2":"markdown","b0a3cc1e":"markdown","6ce6c68e":"markdown","9e00cd3d":"markdown","ec45d446":"markdown","f3e0b096":"markdown","aee8f3d5":"markdown","3afec0c3":"markdown"},"source":{"62e99cc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","158cf03e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport warnings\nwarnings.filterwarnings('ignore')","21a97784":"df= pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\n# print(df.columns)\nprint(df.shape)\ndf.head()","e098f332":"df.drop(['Unnamed: 0'] , axis=1 , inplace=True)\ndf.head()","b8ffd3ce":"df.info()","920441c7":"df.isnull().sum()","20ee17f7":"df.describe()","0c60c484":"df.loc[(df['x']==0) | (df['y']==0) | (df['z']==0)]","0294465a":"df = df[(df[['x','y','z']] != 0).all(axis=1)]","378c3ae1":"for col in ['cut','color','clarity']:\n    print('{} : {}'.format(col,df[col].unique()))\ndf.nunique()  #To check number of unique values in each feature","8b8c59b3":"plt.figure(figsize=(10,10))\ncorr = df.corr()\nsns.heatmap(data=corr, square=True , annot=True, cbar=True,linewidth=2)","4765fdee":"plt.scatter(df.carat,df.price)","df88f34f":"sns.catplot(x='cut', data=df , kind='count',aspect=2,order=['Fair','Good','Very Good','Premium','Ideal'] )","cca368d1":"sns.catplot(x='cut', y = 'price',data=df ,aspect=2,kind='box' ,order=['Fair','Good','Very Good','Premium','Ideal'])","db542073":"sns.catplot(x='color', data=df , kind='count',aspect=2 ,order=['J','I','H','G','F','E','D'])","66a13c62":"sns.catplot(x='color', y='price', data=df ,aspect=2.5,kind='box',order=['J','I','H','G','F','E','D'] )","d252120f":"sns.catplot(x='clarity', data=df , kind='count',aspect=2 ,order=['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'])","4321aa5d":"sns.catplot(x='clarity',y='price',data=df, kind='box',aspect=2,order=['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'])","844a2b5c":"plt.hist(x='depth',data=df,bins=40)\nplt.show","845aeae1":"sns.regplot(x='depth',y='price',data=df)","500efcb9":"sns.kdeplot(df['x'] ,shade=True , color='r' )\nsns.kdeplot(df['y'] , shade=True , color='g' )\nsns.kdeplot(df['z'] , shade= True , color='b')\nplt.xlim(2,10)","bd0e49c4":"df['volume'] = df['x']*df['y']*df['z']\ndf.head()","c7efcfe8":"plt.hist( x=df['volume'] , bins=50 ,color='g')\nplt.xlabel('Volume in mm^3')\nplt.ylabel('Frequency')\nplt.xlim(0,600)\nplt.ylim(0,30000)\nplt.show()","2cec5ab0":"sns.relplot(x='volume',y='price',data=df)","ba326678":"df.drop(['x','y','z'], axis=1, inplace= True)","ddef6b79":"df.head(5)","03d09e8f":"##Using map funciton of python\nL_cut_map = {'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4}\ndf['cut'] = [L_cut_map.get(item) for item in df['cut']]\n\nL_color_map = {'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6}\ndf['color'] = [L_color_map.get(item) for item in df['color']]\n\nL_clar_map = {'I1':0,'SI2':1,'SI1':2,'VS2':3,'VS1':4,'VVS2':5,'VVS1':6,'IF':7}\ndf['clarity'] = [L_clar_map.get(item) for item in df['clarity']]","9fc02f97":"##Using LabelEncoder of scikit learn\n# label_cut = LabelEncoder()\n# label_color = LabelEncoder()\n# label_clarity = LabelEncoder()\n\n\n# df['cut'] = label_cut.fit_transform(df['cut'])\n# df['color'] = label_color.fit_transform(df['color'])\n# df['clarity'] = label_clarity.fit_transform(df['clarity'])","2a2b3e4e":"df.head()","12f08a37":"df.shape","ed05a7b4":"X = df.drop(['price'],axis=1)\nY = df['price']","41dbd115":"X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.1, random_state=40)","3b09dacd":"X_test.head()","4a9641e7":"Lreg = LinearRegression()\nLreg.fit(X_train,y_train)\nprediction = Lreg.predict(X_test)\nrmse_Lreg = np.sqrt(mean_squared_error(y_test, prediction))\nprint('RMSE value is = {}'.format(rmse_Lreg))\nr2_Lreg = r2_score(y_test, prediction)\nprint('R-squared value is {}'.format(r2_Lreg))","648b5524":"RFreg_model = RandomForestRegressor()\nRFreg_model.fit(X_train,y_train)\nprediction2 = RFreg_model.predict(X_test)\nrmse_RFreg = np.sqrt(mean_squared_error(y_test, prediction2))\nprint('RMSE value is = {}'.format(rmse_RFreg))\nr2_RFreg = r2_score(y_test, prediction2)\nprint('R-squared value is {}'.format(r2_RFreg))","51ef8926":"Result= pd.DataFrame({'Actual Price':y_test,'Predicted Price By LinReg':prediction,'Predicted Price By RandForest':prediction2})\nResult","a2a3a0f5":"Correlation coefficient between price ~ carat, price ~ dimensions are high (0.9 above). That means features carat and dimensions of the diamond highly affect the target, i.e. diamond price.\nAlso the features x,y,z also show high coorelation coefficient among each other.So they can be combined into one feature.","c136e85b":"## MODELLING\n","c236ef23":"### Correlation between features","3d8e1ae8":"* Check how the values of each feature are spread.. their min,max and count obviosly.\n* Here, the min values of x,y and z are 0. It doesnt make any sense to have the diamond dimension to be zero. May be this data is there by mistake. Lets have a look at those logic less rows.","6f2badaf":"* See now we have the different categories of each categorical column of the dataset converted to numerical values as seen in df.head()\n* For example the categories of categorical column CUT are - Ideal, Premium, Very Good , Good and Fair. After Label Encoding these categories are converted to numbers like-0,1,2,3 and 4.","030033b8":"**PROBLEM STATEMENT:-**\n\n* We are provided with a few features of a Diamond and based on that we are expected to estimate its price.","1d19e884":"### By Random Forest Regression :","89cf0d9f":"### Features\n* Carat : Carat weight of the Diamond.\n\n* Cut : Describe cut quality of the diamond.\n    Quality in increasing order Fair, Good, Very Good, Premium, Ideal .\n    \n* Color : Color of the Diamond. from J (worst) to D (best)\n\n* Clarity : Diamond Clarity refers to the absence of the Inclusions and Blemishes. In order from worst to best :-  I1,SI2, SI1, VS2, VS1, VVS2, VVS1, IF\n\n* Depth : The Height of a Diamond, measured from the Culet to the table, divided by its average Girdle Diameter.\n\n* Table : The Width of the Diamond's Table expressed as a Percentage of its Average Diameter.\n\n* X : Length of the Diamond in mm.\n\n* Y : Width of the Diamond in mm.\n\n* Z : Height of the Diamond in mm.\n\n* Price : the Price of the Diamond.\n\n* Qualitative Features (Categorical) : Cut, Color, Clarity.\n* Quantitative Features (Numerical) : Carat, Depth , Table , Price , X , Y, Z.","1949ac1c":"### FEATURE ENGINEERING","cb46c4a4":"...........................\n#### What Is R-squared?\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\nThe definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n\nR-squared = Explained variation \/ Total variation\n\nR-squared is always between 0 and 100%:\n0% indicates that the model explains none of the variability of the response data around its mean.100% indicates that the model explains all the variability of the response data around its mean.\n...........................","c260498b":"### x, y and z dimensions","520fdc90":"* So there is no null values in any column.","f9cb4488":"### **FEATURE ENODING**","7837740b":"* The **Unnamed: 0** column, which is meant to be the 'Id' can be dropped bcz we already have indices.","5249788d":"# Extract the Diamond dataset,Explore and Analyse it.","05c85c44":"**Separate the features and the target column**. Below X contains the features and Y is the price column (also called Label or Target)","22ae9f3b":"#### PRICE ~ COLOR","882d479c":"####  PRICE ~ CUT","6c9120b8":"![image.png](attachment:image.png)","7297db11":"RANDOM FOREST REGRESSION PROVIDES  FAR BETTER RESULT.","0081ca84":"### THANK YOU.","4bdf1151":"#### PRICE ~ CARAT","5f476605":"* Use scikitlearn's train_test_split to create train and test files.\n* The test size=0.2 represents that 20% of X and Y dataframes goes to the test files.","cc42ce9f":"### **Visualisation of all Features**","7269d09e":"* Then train the **X_train** and **Y_train** dataframes by **LinearRegression** model.\n* Predicting the values for X_test and finding the rmse ","eb0feb8e":"### Dealing with Categorical columns","c8478a33":"### CLARITY ~ PRICE","afd67e16":"![](http:\/\/)![image.png](attachment:image.png)","bd02a6a2":"Because the diamond dimensions - x,y and z are correlated to each other (We have also seen it in the correlation matrix) , so we can replace these 3 features with one single feature -VOLUME. So lets add volume feature and drop the x,y and z features.","b0a3cc1e":"* Cut, Color and clarity are three categorical features. Lets see the unique values in them.","6ce6c68e":"#### PRICE ~ DEPTH","9e00cd3d":"* Such cases are dealt by repalcing those partiular cells by *mean,median or mode* Or they can be dropped also, depending on the data. Here dropping them seems a better option. (Check for imputer in scikit learn to know more) ","ec45d446":"### TRAIN TEST SPLIT","f3e0b096":"Now we convert categorial columns to numerical values. This step can be done manually for small datasets,or by using scikit learn's LabelEncoder or OneHotEncoder, depending on values in that column.","aee8f3d5":"### By Linear Regression :","3afec0c3":"**Please help by upvoting this kernel if you feel useful and share your suggestions for any improvement. It will be very motivating for me.**"}}