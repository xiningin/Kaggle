{"cell_type":{"bc4a4422":"code","df78ba76":"code","4f6716d8":"code","075fecc6":"code","ed8a84eb":"code","88e4959d":"code","bcdc78b4":"code","9a8d1d1b":"code","fccc5127":"code","95c86fb3":"code","531192b0":"code","84f5a7da":"code","d402cbc3":"code","63023fcf":"code","594c601a":"code","0665b5e6":"code","2038c38c":"code","6f94b669":"code","3c31919d":"code","e94befcc":"code","d071a97f":"code","eadeca4c":"code","59f616cf":"code","9f6c2211":"code","6b99e46e":"code","448d0fb4":"code","6a1a34d0":"code","82c8550d":"code","58001b2d":"code","a5f5131c":"code","07d845a5":"code","7c687270":"code","dff2aaa1":"code","5c8f2852":"code","4bfdbc1f":"code","bdc1215b":"code","82c3d54d":"code","571f5fee":"code","4380eab2":"code","07c85d5c":"code","32e39b8d":"code","b08620de":"code","7d69d2a7":"code","2aecdf28":"code","b590e98f":"code","016b01db":"code","3bf88631":"code","41f11ab2":"code","23fa8e46":"code","6d53f366":"code","a3686bf1":"code","7fa8756f":"code","55ecd302":"code","bbfec8b3":"code","613f2de0":"code","4c36b4f9":"code","7d9133a0":"code","50305f8d":"code","8ebf58f7":"code","cd6c90a0":"code","3dd7bc51":"code","1a0bbfdd":"markdown","82a782ae":"markdown","8803946e":"markdown","4c71fb64":"markdown","eb9e5916":"markdown","77dfcf59":"markdown","32b03c58":"markdown","08382dae":"markdown","f7198f4f":"markdown","8d897aaa":"markdown","54c5f3c9":"markdown","02d87441":"markdown","ad078c40":"markdown","4102eb87":"markdown","d8949bc5":"markdown","2765147b":"markdown","c22536cf":"markdown","68afc361":"markdown","422ed89c":"markdown","1334f6bf":"markdown","f74c9b24":"markdown","7b46cb67":"markdown","0441680c":"markdown","92f04304":"markdown","1ce9e862":"markdown","05c6d4e4":"markdown","0f52c43d":"markdown","f9e5446d":"markdown","2a25823f":"markdown","561cbbc1":"markdown","fdd29cc6":"markdown","967a5cbf":"markdown","1a0c2baa":"markdown","445cdd8b":"markdown","50ce4556":"markdown","bddbebae":"markdown","9c122475":"markdown","a90f531f":"markdown","377ff122":"markdown","8091ada3":"markdown","24c866f4":"markdown","261e80a3":"markdown","1e4277be":"markdown","f7371845":"markdown","c1447684":"markdown","738abfce":"markdown","313ac03a":"markdown","1c76e739":"markdown","30527d67":"markdown","f62b135d":"markdown","329724a2":"markdown"},"source":{"bc4a4422":"<font size=\"+3\" color=red><b> <center><u>IRIS + EDA + Plotly + (25+) Models For Beginners<\/u><\/center><\/b><\/font>","df78ba76":"!pip install TPOT","4f6716d8":"!pip install pycaret","075fecc6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\n\nimport cufflinks as cf \n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import  RadiusNeighborsClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeClassifier\nfrom catboost import Pool, CatBoostClassifier, cv\nimport lightgbm as lgb\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport random\n","ed8a84eb":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","88e4959d":"train = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","bcdc78b4":"table = ff.create_table(train.head().round(3))\niplot(table,filename='jupyter-table1')","9a8d1d1b":"train.columns\n","fccc5127":"train.shape\n","95c86fb3":"iplot(ff.create_table(train.dtypes.to_frame().reset_index().round(3)),filename='jupyter-table2')","531192b0":"iplot(ff.create_table(train.describe().reset_index().round(3)),filename='jupyter-table2')","84f5a7da":"train.isnull().sum()","d402cbc3":"msno.bar(train, color = 'b', figsize = (10,8))","63023fcf":"msno.matrix(train)","594c601a":"species_count = train['Species'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3),line=dict(color='#000000', width=2))\n)]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Number of Species\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","0665b5e6":"trace = go.Pie(labels = list(train.Species.unique()), values = list(train.Species.value_counts()),\n                            hole = 0.2,\n               marker=dict(colors = random_colors(3), \n                           line=dict(color='#000000', width=2)\n                           ))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Number of Species\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","2038c38c":"data = [go.Heatmap(z = np.array(train.corr().values),\n                   x = np.array(train.corr().columns),\n                   y = np.array(train.corr().columns),\n                     colorscale='Blackbody',)\n       ]\nlayout = go.Layout(\n   {\n      \"title\":\"Heatmap of Correlation \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","6f94b669":"setosa = go.Scatter(x = train['SepalLengthCm'][train.Species =='Iris-setosa'], y = train['SepalWidthCm'][train.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = train['SepalLengthCm'][train.Species =='Iris-versicolor'], y = train['SepalWidthCm'][train.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = train['SepalLengthCm'][train.Species =='Iris-virginica'], y = train['SepalWidthCm'][train.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Sepal Length and Sepal Width VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","3c31919d":"setosa = go.Scatter(x = train['PetalLengthCm'][train.Species =='Iris-setosa'], y = train['PetalWidthCm'][train.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = train['PetalLengthCm'][train.Species =='Iris-versicolor'], y = train['PetalWidthCm'][train.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = train['PetalLengthCm'][train.Species =='Iris-virginica'], y = train['PetalWidthCm'][train.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Petal Length and Petal Width VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","e94befcc":"trace0 = go.Box(y=train['SepalWidthCm'][train['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=train['SepalWidthCm'][train['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=train['SepalWidthCm'][train['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Sepal Width VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","d071a97f":"trace0 = go.Box(y=train['SepalLengthCm'][train['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=train['SepalLengthCm'][train['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=train['SepalLengthCm'][train['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Sepal Length VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","eadeca4c":"trace0 = go.Box(y=train['PetalWidthCm'][train['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=train['PetalWidthCm'][train['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=train['PetalWidthCm'][train['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Petal Width VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","59f616cf":"trace0 = go.Box(y=train['PetalLengthCm'][train['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=train['PetalLengthCm'][train['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=train['PetalLengthCm'][train['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Petal Length VS Target Class \",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","9f6c2211":"fig = go.Figure(\n    data=[go.Histogram(x=train['SepalLengthCm'])],layout_title_text=' SepalLengthCm Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","6b99e46e":"fig = go.Figure(\n    data=[go.Histogram(x=train['SepalWidthCm'])],layout_title_text=' SepalWidthCm Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","448d0fb4":"fig = go.Figure(\n    data=[go.Histogram(x=train['PetalLengthCm'])],layout_title_text=' PetalLengthCm Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","6a1a34d0":"fig = go.Figure(\n    data=[go.Histogram(x=train['PetalWidthCm'])],layout_title_text=' PetalWidthCm Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","82c8550d":"X = train.iloc[:,:-1].values\ny = train.iloc[:,-1].values\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","58001b2d":"## Logistic Regression\n\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","a5f5131c":"# K-Nearest Neighbours\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","07d845a5":"## Decision Tree\n\nModel = DecisionTreeClassifier()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","7c687270":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","dff2aaa1":"# Linear Discriminant Analysis\n\nModel = LinearDiscriminantAnalysis()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","5c8f2852":"## Light GBM\n\nparams = {'objective':'binary', 'metric':'accuracy'}\n  \n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nModel = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=10)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred.round()))\n","4bfdbc1f":"\nModel=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)\nModel.fit(X_train,y_train,eval_set=(X_test,y_test))","bdc1215b":"## CatBoost\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","82c3d54d":"## XGBoost\n\nModel=xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","571f5fee":"## Ridge Classifier\n\nModel=RidgeClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","4380eab2":"## Quadratic Discriminant Analysis\n\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","07c85d5c":"## Bagging Classifier\n\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","32e39b8d":"## MLPClassifier\n\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","b08620de":"## Linear Support Vector Classification\n \nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","7d69d2a7":"## Nu-Support Vector Classification\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","2aecdf28":"## BernoulliNB\n\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","b590e98f":"## Passive Aggressive Classifier\n\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","016b01db":"## Radius Neighbhors Classifier\n\nModel=RadiusNeighborsClassifier(radius=8.0)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n#summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accouracy score\nprint('accuracy is ', accuracy_score(y_test,y_pred))","3bf88631":"## Gradient Boosting Machine\nModel = GradientBoostingClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","41f11ab2":"## Adaboost\n\nModel = AdaBoostClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","23fa8e46":"## Extra Trees\n\nModel = ExtraTreesClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","6d53f366":"\n## Random Forest\n\nModel = RandomForestClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","a3686bf1":"## Support Vector Machine\n\nModel = SVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","7fa8756f":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","55ecd302":"data = h2o.import_file('\/kaggle\/input\/iris\/Iris.csv')\n\n\n# Identify predictors and response\nx = data.columns\ny = \"Species\"\nx.remove(y)\n\ndata[y] = data[y].asfactor()\n\naml = H2OAutoML(max_models=20, max_runtime_secs=1500, seed=1)\naml.train(x=x, y=y, training_frame=data)","bbfec8b3":"lb = aml.leaderboard\nlb.head()","613f2de0":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor\n\ntpot = TPOTClassifier(generations=5, verbosity=2)\ntpot.fit(X_train,y_train)\n","4c36b4f9":"y_pred=tpot.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","7d9133a0":"! pip install -U pycaret # Quite large depencies to install !","50305f8d":"from pycaret.classification import *\n\nclf1 = setup(data = train, \n             target = 'Species',\n             silent = True)\n","8ebf58f7":"compare_models()\n","cd6c90a0":"lgbm  = create_model('lightgbm')      ","3dd7bc51":"plot_model(estimator = lgbm, plot = 'learning')\n","1a0bbfdd":"<a id=\"3.41\"><\/a>\n<font color=\"blue\" size=+2.5><b> Petal width Variable Analysis <\/b><\/font>","82a782ae":"<a id=\"5.19\"><\/a>\n<font color=\"blue\" size=+2.5><b> \nAdaBoost <\/b><\/font>\n\n*AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the AdaBoostClassifier class*","8803946e":"<a id=\"5.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> K-Nearest Neighbours <\/b><\/font>\n\n*The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems*","4c71fb64":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> SepalWidthCm <\/b><\/font>\n","eb9e5916":"<a id=\"3.41\"><\/a>\n<font color=\"blue\" size=+2.5><b> Sepal width Variable Analysis <\/b><\/font>","77dfcf59":"<a id=\"3.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Sepal Variable Analysis <\/b><\/font>","32b03c58":"<a id=\"5.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Naive Bayes <\/b><\/font>\n\n*Naive Bayes calculates the probability of each class and the conditional probability of each class\ngiven each input value. These probabilities are estimated for new data and multiplied together,\nassuming that they are all independent (a simple or naive assumption). When working with\nreal-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\ninput variables using the Gaussian Probability Density Function. You can construct a Naive\nBayes model using the GaussianNB class*","08382dae":"<a id=\"5.5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Discriminant Analysis <\/b><\/font>\n\n*Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\nclassification. It too assumes a Gaussian distribution for the numerical input variables. You can\nconstruct an LDA model using the LinearDiscriminantAnalysis class.*","f7198f4f":"<a id=\"5.18\"><\/a>\n<font color=\"blue\" size=+2.5><b> Stochastic Gradient Boosting <\/b><\/font>\n\n*Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class*","8d897aaa":"<a id=\"3.3\"><\/a>\n<font color=\"blue\" size=+2.5><b> Non-Target Variable Analysis <\/b><\/font>\n","54c5f3c9":"<a id=\"5.10\"><\/a>\n<font color=\"blue\" size=+2.5><b> Quadratic Discriminant Analysis <\/b><\/font>\n\n\n*A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\nThe model fits a **Gaussian** density to each class.*","02d87441":"<a id=\"5.14\"><\/a>\n<font color=\"blue\" size=+2.5><b> Nu-Support Vector Classification <\/b><\/font>\n\n*Similar to SVC but uses a parameter to control the number of support vectors.*","ad078c40":"<a id=\"2.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Libraries <\/b><\/font>\n","4102eb87":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h2>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [About Data ](#1.1)\n* [Data Dictionary ](#1.3)\n* [Data Variable](#1.4)\n    \n<font color=\"blue\" size=+1><b> Load and Check Data <\/b><\/font>\n* [Importing Library](#2.1)\n* [Load Dataset](#2.2)\n\n<font color=\"blue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [Missing Value Analysis](#3.1)\n* [Target Variable Analysis](#3.2)    \n* [Non-Target Variable Analysis](#3.3)    \n\n\n<font color=\"blue\" size=+1><b> Data Handling and Preparation <\/b><\/font>\n* [Handing Missing Data ](#4.1)\n* [Train Test Split ](#4.2)\n\n<font color=\"blue\" size=+1><b> Model Training <\/b><\/font>\n* [Logistic Regression ](#5.1)\n* [K-Nearest Neighbours ](#5.2)    \n* [Decision Tree ](#5.3)\n* [Naive Bayes ](#5.4)    \n* [Linear Discriminant Analysis ](#5.5)\n* [LightLGM ](#5.6)    \n* [CatBoost ](#5.7)\n* [XGBoost ](#5.8)    \n* [Ridge Classifier ](#5.9)\n* [Quadratic Discriminant Analysis ](#5.10)    \n* [Bagging classifier ](#5.11)\n* [MLPClassifier](#5.12)    \n* [Linear Support Vector Classification ](#5.13)\n* [Nu-Support Vector Classification ](#5.14)    \n* [BernoulliNB ](#5.15)\n* [Passive Aggressive Classifier ](#5.16)    \n* [Radius Neighbors Classifier ](#5.17)\n* [Stochastic Gradient Boosting ](#5.18)    \n* [AdaBoost ](#5.19)\n* [Extra Trees ](#5.20)    \n* [Random Forest ](#5.21)\n* [SVC ](#5.22)    \n* [H2O ](#5.23)\n* [TPOT ](#5.24)    \n* [PyCaret ](#5.25)\n","d8949bc5":"<a id=\"2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Load and Check Data  <\/center><\/h2>","2765147b":"<a id=\"5.8\"><\/a>\n<font color=\"blue\" size=+2.5><b> XGBoost <\/b><\/font>\n\n*XGBoost stands for Extreme Gradient Boosting, it is a performant machine learning library based on the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost implements a Gradient Boosting algorithm based on decision trees.*","c22536cf":"<a id=\"5.17\"><\/a>\n<font color=\"blue\" size=+2.5><b> Radius Neighbors Classifier <\/b><\/font>\n\n*Classifier implementing a **vote** among neighbors within a given **radius**\n\nIn scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.*","68afc361":"<a id=\"3.44\"><\/a>\n<font color=\"blue\" size=+2.5><b> PetalWidthCm <\/b><\/font>\n","422ed89c":"<a id=\"5.15\"><\/a>\n<font color=\"blue\" size=+2.5><b> BernoulliNB <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*","1334f6bf":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Model Training <\/b><\/font>\n","f74c9b24":"<a id=\"3.44\"><\/a>\n<font color=\"blue\" size=+2.5><b> Petal length Variable Analysis <\/b><\/font>","7b46cb67":"<a id=\"1.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> About Data<\/b><\/font>\n<br\/>\n<br\/>\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\n## The columns in this dataset are:\n\n* Id\n* SepalLengthCm\n* SepalWidthCm\n* PetalLengthCm\n* PetalWidthCm\n* Species","0441680c":"<a id=\"3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h2>","92f04304":"<a id=\"5.20\"><\/a>\n<font color=\"blue\" size=+2.5><b> Extra Trees <\/b><\/font>\n\n*Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class*","1ce9e862":"![image.png](attachment:image.png)","05c6d4e4":"\n\n<a id=\"1\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Introduction  <\/center><\/h2>","0f52c43d":"<a id=\"2.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Dataset <\/b><\/font>\n","f9e5446d":"<a id=\"4.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Data Handling and Preparation <\/b><\/font>\n","2a25823f":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target Variable Analysis <\/b><\/font>\n","561cbbc1":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> Sepal length Variable Analysis <\/b><\/font>","fdd29cc6":"<a id=\"5.12\"><\/a>\n<font color=\"blue\" size=+2.5><b> MLPClassifier  <\/b><\/font>\n\n*MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.*","967a5cbf":"<a id=\"5.22\"><\/a>\n<font color=\"blue\" size=+2.5><b> Support Vector Machine <\/b><\/font>\n\n*Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes Of particular importance is the use of different kernel functions via the kernel parameter .A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class.*","1a0c2baa":"<a id=\"5.13\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Support Vector Classification  <\/b><\/font>\n\n*Similar to **SVC** with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.*","445cdd8b":"<a id=\"3.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Missing Value Analysis <\/b><\/font>\n","50ce4556":"<a id=\"5.25\"><\/a>\n<font color=\"blue\" size=+2.5><b> Pycaret <\/b><\/font>\n","bddbebae":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> PetalLengthCm <\/b><\/font>\n","9c122475":"<a id=\"5.16\"><\/a>\n<font color=\"blue\" size=+2.5><b> Passive Aggressive Classifier <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*\nThe Passive-Aggressive algorithms are a family of Machine learning algorithms that are not very well known by beginners and even intermediate Machine Learning enthusiasts. However, they can be very useful and efficient for certain applications.","a90f531f":"<a id=\"5.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Logistic Regression <\/b><\/font>\n\n*Logistic regression assumes a Gaussian distribution for the numeric input variables and can\nmodel binary classification problems. You can construct a logistic regression model using the\nLogisticRegression class*","377ff122":"<a id=\"3.62\"><\/a>\n<font color=\"blue\" size=+2.5><b> Correlation <\/b><\/font>\n","8091ada3":"<font size=\"+2\" color=red ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","24c866f4":"<a id=\"5.23\"><\/a>\n<font color=\"blue\" size=+2.5><b> H2O <\/b><\/font>","261e80a3":"<a id=\"5.9\"><\/a>\n<font color=\"blue\" size=+2.5><b> Ridge Classifier <\/b><\/font>\n\n*Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and then treats the problem as a \nregression task (multi-output regression in the multiclass case).*","1e4277be":"<a id=\"5.6\"><\/a>\n<font color=\"blue\" size=+2.5><b> LightGBM <\/b><\/font>\n\n*LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:*\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.","f7371845":"<a id=\"5.11\"><\/a>\n<font color=\"blue\" size=+2.5><b> Bagging classifier  <\/b><\/font>\n\n*A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.*","c1447684":"<a id=\"5.21\"><\/a>\n<font color=\"blue\" size=+2.5><b> Random Forest <\/b><\/font>\n\n*Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class.*","738abfce":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>Objective  <\/center><\/h2>\n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Beginners guide on IRIS Dataset.\n- Feature Analysis\n- Modelling on 25+ Models","313ac03a":"<a id=\"5.7\"><\/a>\n<font color=\"blue\" size=+2.5><b> CatBoost <\/b><\/font>\n\n*CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone.*","1c76e739":"<a id=\"5.3\"><\/a>\n<font color=\"blue\" size=+2.5><b> Decision Tree (CART) <\/b><\/font>\n\n*Classification and Regression Trees (CART or just decision trees) construct a binary tree from\nthe training data. Split points are chosen greedily by evaluating each attribute and each value\nof each attribute in the training data in order to minimize a cost function (like the Gini index).\nYou can construct a CART model using the DecisionTreeClassifier class*","30527d67":"<a id=\"3.41\"><\/a>\n<font color=\"blue\" size=+2.5><b> SepalLengthCm <\/b><\/font>\n","f62b135d":"# Upvote The Kernel If you like my work","329724a2":"<a id=\"5.24\"><\/a>\n<font color=\"blue\" size=+2.5><b> TPOT <\/b><\/font>"}}