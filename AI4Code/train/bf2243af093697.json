{"cell_type":{"4538d620":"code","397b480f":"code","1dbfbe01":"code","f56e86b0":"code","0ffbb154":"code","b8088026":"code","66f96500":"code","ebb1d2ae":"code","a26a96e1":"code","82becf4a":"code","0eac6b34":"code","fadb5492":"code","f26830d7":"code","b548e83d":"code","25259892":"code","88507558":"code","526f5152":"code","067c84f4":"code","a646853b":"code","77095394":"code","85ea7c7f":"code","f44852a9":"code","40065eef":"code","76cb74f3":"code","e34c1b55":"code","d1d8638c":"code","68458aa8":"code","498c3bb5":"code","ab1abd17":"code","1c52e133":"code","4056df1c":"code","597b4b5c":"code","732f2945":"code","f65177ef":"code","b04f0152":"code","a984167f":"code","154dff98":"code","c276d94d":"code","57fdac41":"code","4cf4c5a7":"markdown","ab438430":"markdown","c4da2af1":"markdown","3782eaad":"markdown","8881636f":"markdown","2bccd102":"markdown","6d64f8b0":"markdown","63d7a06f":"markdown","75ff9f4f":"markdown","c01b621f":"markdown","9f91ff84":"markdown","653ee1c8":"markdown","5ac49d1b":"markdown","a4671995":"markdown","5b6e7eac":"markdown","4437e070":"markdown","ea99ac8e":"markdown","50f85be7":"markdown","023b17b4":"markdown","54a9cd5b":"markdown","c6ae384b":"markdown","3e8c858b":"markdown","a5e5b5cb":"markdown","1e52afb5":"markdown","fac3f78d":"markdown","4d80d687":"markdown","764f48e2":"markdown","453062f5":"markdown","ce8d85d0":"markdown","69422e0b":"markdown","1e9360e2":"markdown","5c67d8ad":"markdown","ee745e11":"markdown","270ef5ad":"markdown","db9631ae":"markdown","4488570a":"markdown","3113fe76":"markdown"},"source":{"4538d620":"!pip install numpy --upgrade\n!pip install python-gdcm","397b480f":"from pathlib import Path\nimport sys\nfrom ast import literal_eval\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport seaborn as sns\n\nprint(sys.version)\n\nmlp.rcParams['figure.figsize'] = (15, 7)","1dbfbe01":"is_colab = 'google.colab' in sys.modules\nif is_colab:\n    from google.colab import drive\n    drive.mount('\/content\/drive')\n    path = Path('\/content\/drive\/MyDrive\/covid19-detection\/data') \nelse:\n    path = Path('\/kaggle\/input\/siim-covid19-detection') ","f56e86b0":"image_df = pd.read_csv(path\/'train_image_level.csv', index_col='id')","0ffbb154":"image_df.head()","b8088026":"image_df = image_df.rename(columns={'StudyInstanceUID':'study_id'})","66f96500":"def extract_label(row):\n    values = row.label.split()\n    if len(values) % 6 != 0:\n        # corrupted row\n        print(f'row #{row.index}: wrong number of paramerers in label field')\n    return [dict(zip('id finding_id label confident xmin ymin xmax ymax'.split(), [row.name, i] + values[6*i:6*(i+1)])) for i in range(len(values) \/\/ 6)]","ebb1d2ae":"findings = pd.DataFrame.from_dict(image_df.apply(extract_label, axis=1).sum()).set_index('id')\nfindings.head()","a26a96e1":"print(f'The unique findings label values are {findings.label.unique()}')","82becf4a":"print(f'The unique findings confdence values are {findings.confident.unique()}')","0eac6b34":"findings = image_df.apply(lambda x: [{'id':x.name, 'finding_id': i, **box} for i, box in enumerate(literal_eval(x.boxes))] if type(x.boxes) == str else [{'id': x.name}], axis=1)\nfindings = pd.DataFrame.from_dict(findings.sum()).set_index('id')","fadb5492":"print(f'Total number of findings: {findings.shape[0]}')\nfindings.head()","f26830d7":"study_df = pd.read_csv(path\/'train_study_level.csv', index_col='id')","b548e83d":"study_df.head()","25259892":"(study_df.apply(sum, axis=1) == 1).all()","88507558":"image_df.study_id += '_study' \nstudy_labels = study_df.idxmax(axis=1).rename('study_label')\n# study_labels.index = study_labels.index.str.extract('(^[^_]*)').apply(lambda x:x[0], axis=1)\nimage_df = image_df.merge(study_labels, left_on='study_id', right_index=True)","526f5152":"label_count = study_df.sum()\nplt.figure(figsize=(8, 8))\nplt.pie(label_count, labels=label_count.index, wedgeprops={'edgecolor': 'black'}, autopct='%1.f%%', textprops={'fontsize': 16}, explode=[.01]*4, shadow=True)\nplt.title('Label Distribution', fontdict={'fontsize':22});","067c84f4":"g = sns.barplot(x=label_count.index, y=label_count)\nfor label, count in zip(range(label_count.index.shape[0]), label_count):\n    g.text(label, count, count, color='black', ha=\"center\", fontdict=dict(fontsize=14, weight='bold'))","a646853b":"findings['area'] = findings.width * findings.height\nfindings_props = findings.groupby('id').area.agg(['count','sum', 'min', 'max', 'mean', 'std'])","77095394":"# findings_props.index = findings_props.index.str.extract('(^[^_]*)').apply(lambda x:x[0], axis=1)\nimage_props = image_df.join(findings_props)\nimage_props.head()","85ea7c7f":"plt.figure(figsize=(10,6))\nplt.gca().yaxis.grid(linestyle='--')\nsns.violinplot(data=image_props, x='study_label', y='count')\nplt.title('Findings Count', fontdict={'fontsize':16})\nplt.show()","f44852a9":"study_findings_props = image_props.groupby(['study_id'])\nstudy_findings_count = study_findings_props['count'].agg('sum').to_frame().join(study_labels)\nplt.figure(figsize=(10,6))\nplt.gca().yaxis.grid(linestyle='--')\nsns.violinplot(data=study_findings_count, x='study_label', y='count')\nplt.title('Findings Count', fontdict={'fontsize':16})\nplt.show()","40065eef":"studies_to_remove = study_df[study_df.index.isin(study_findings_props['count'].sum()[study_findings_props['count'].sum() == 0].index) & \n         (study_df['Negative for Pneumonia'] == 0)]\nstudy_df['removed'] = False\nstudy_df.loc[studies_to_remove.index, 'removed'] = True\nprint(f'Total number of removed rows: {study_df.loc[studies_to_remove.index].shape[0]}')\nprint(f'\\n\\nRemoved rows by label:\\n')\nprint(study_df.loc[studies_to_remove.index].iloc[:, :-1].sum().to_string())","76cb74f3":"image_props = image_props.drop(image_props.loc[image_props.study_id.isin(studies_to_remove.index)].index)\n\nstudy_findings_props = image_props.groupby(['study_id'])\nstudy_findings_count = study_findings_props['count'].agg('sum').to_frame().join(study_labels)\nplt.figure(figsize=(10,6))\n# plt.gca().yaxis.grid(linestyle='--')\nsns.violinplot(data=study_findings_count, x='study_label', y='count')\nplt.title('Findings Count', fontdict={'fontsize':16})\nplt.show()","e34c1b55":"plt.figure(figsize=(20,10))\n\nfor i, prop in enumerate(['sum', 'min', 'max', 'mean', 'std'], start=1):\n    plt.subplot(2, 3, i)\n#     plt.gca().yaxis.grid(linestyle='--')\n    sns.violinplot(data=image_props, x='study_label', y=prop, order=study_labels.unique())\n    plt.xticks(rotation=10)\n    title = f'Findings {prop.title()}'\n    if prop.title() != 'Sum': title += ' Area'\n    plt.title(title, fontdict={'fontsize':16})\nplt.tight_layout()\nplt.show()","d1d8638c":"study_props = (image_props[['study_id']]\n               .join(findings)\n               .groupby('study_id').area.agg(['count','sum', 'min', 'max', 'mean', 'std'])\n               .merge(study_labels, left_on='study_id', right_index=True))\n\nplt.figure(figsize=(20,10))\n\nfor i, prop in enumerate(['sum', 'min', 'max', 'mean', 'std'], start=1):\n    plt.subplot(2, 3, i)\n#     plt.gca().yaxis.grid(linestyle='--')\n    sns.violinplot(data=study_props, x='study_label', y=prop, order=study_labels.unique(), pallete=['blue', 'green', 'green', 'red'])\n    plt.xticks(rotation=10)\n    title = f'Findings {prop.title()}'\n    if prop.title() != 'Sum': title += ' Area'\n    plt.title(title, fontdict={'fontsize':16})\nplt.tight_layout()\nplt.show()","68458aa8":"image_df.groupby('study_id').label.count().unique()","498c3bb5":"print(image_df.groupby('study_id').label.agg(images_count='count').value_counts().to_string())\n# g = sns.countplot(data=image_df.groupby('study_id').label.agg(images_count='count'), x='images_count')","ab1abd17":"multiple_images = image_df.groupby('study_id').filter((lambda x: x.label.count() > 1))\nmultiple_images_with_findings = multiple_images[multiple_images.boxes.notna()]\nprint('Images with finding in Study Counts')\nprint(multiple_images_with_findings.groupby('study_id').boxes.agg(count='count').value_counts().to_string())","1c52e133":"from pydicom import dcmread\n\ndef extract_id(full_id): return full_id[:full_id.index('_')]\ndef get_file_path(study_id, image_id, dataset='train'):\n    study_id, image_id = extract_id(study_id), extract_id(image_id)\n    return [*(path\/dataset\/study_id).glob(f'**\/{image_id}.dcm')][0] \n\n#     return [*(path\/dataset\/row.study_id.str.extract(\"(^[^_]*)\").values[0,0]).glob(f'**\/{row.index.str.extract(\"(^[^_]*)\").values[0,0]}.dcm')][0] \nsample = image_df.sample(random_state=14)\nfpath = get_file_path(sample.study_id.values[0], sample.index.values[0])\nds = dcmread(fpath)\n\nprint(ds)\nplt.imshow(ds.pixel_array, cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()","4056df1c":"# import gdcm\ndef show_dicoms(df, ncols=5, size=5, annotate=False):\n    n = df.shape[0]\n    nrows = int(np.ceil(n\/ncols))\n\n    fig = plt.figure(figsize=(size*ncols, size*nrows))\n    for i, row in enumerate(df.itertuples()):\n        fpath = get_file_path(row.study_id, row.Index)\n        ds = dcmread(fpath)\n        plt.subplot(nrows, ncols,i+1)\n        plt.imshow(ds.pixel_array, cmap=plt.cm.gray)\n        if annotate:\n            if isinstance(row.boxes, str):\n                for box in literal_eval(row.boxes):\n                    rect = patches.Rectangle((box['x'], box['y']), \n                                             box['width'], box['height'],\n                                            color='r', fill=False)\n                    plt.gca().add_patch(rect)\n        plt.xticks([]), plt.yticks([])\n    plt.subplots_adjust()\n    return fig\n#     plt.show()","597b4b5c":"show_dicoms(image_df.sample(25, random_state=25))\nplt.show()","732f2945":"gb = image_df.groupby('study_label')\nfor name, group in gb:\n    if not name.startswith('Neg'): group = group.loc[group.boxes.notna()]\n    fig = show_dicoms(group.sample(9, random_state=4), 3, 8, annotate=True)\n    fig.suptitle(name, fontsize=18)\n    plt.show()","f65177ef":"from tqdm.notebook import tqdm\ndata = {}\ndef append_dcm_properties(row, props):\n#     print(row)\n    try:\n        fpath = get_file_path(row.study_id, row.Index)\n#         fpath = [*(path\/'train'\/row.study_id[:row.study_id.index(\"_\")]).glob(f'**\/{row.Index[:row.Index.index(\"_\")]}.dcm')][0] \n        ds = dcmread(fpath)\n        data[row.Index] = {prop: getattr(ds, prop.replace(' ', '')) for prop in props}\n    except Exception as e:\n        print(f'**\/{row.name[:row.name.index(\"_\")]}.dcm')\n        raise\n        \nprops = ['Image Type', 'Modality','Body Part Examined', 'Photometric Interpretation',\n                                       'Patient Sex', 'Imager Pixel Spacing', 'Rows', 'Columns']\n# image_df.apply(lambda x: append_dcm_properties(x, props) , axis=1)\nfor row in tqdm(image_df.itertuples(), total=image_df.shape[0]):\n    append_dcm_properties(row, props)","b04f0152":"dm = pd.DataFrame.from_dict(data, orient='index')\ndm.to_csv('DICOM_metadata.csv')\ndm.head()","a984167f":"for column in dm[:]:\n    print(f'{column}:' )\n    print(dm[column].unique())\n    print('-'*100)","154dff98":"sns.countplot(data=dm, x='Patient Sex');","c276d94d":"plt.figure(figsize=(15,7))\nsns.countplot(data=dm.join(image_df), x='study_label', hue='Patient Sex')","57fdac41":"body_parts = dm.loc[~dm['Body Part Examined'].isin(['CHEST', 'TORAX','THORAX', 'T?RAX',\n                                                     'T\u00d2RAX','PECHO', 'Pecho'])]\ngb = body_parts.join(image_df).groupby('Body Part Examined')\nfor name, group in gb:\n    fig = show_dicoms(group.sample(6, random_state=4), 3, 8)\n    fig.suptitle(name, fontsize=18)\n    plt.show()","4cf4c5a7":"# EDA","ab438430":"Now it's much better.\n\nFor each image we are provided with image id, study id, the findings bounding boxes, and labels for each bounding box. Let's examine the label column first.\nThe content of the label column corresponds to the submission's desired format. It contains a description of an unlimited number of finding, separated by whitespace. Each of the descriptions contains 6 fields, also separated by whitespace, as follows:\n  \n`finding_label confidence xmin ymin xmax ymax  `\nThe content of this row is as this pattern, repeated as the number of this image's findings. So if we have $k$ finding for specific image, the label row will be:\n`finding_label_1 confidence_1 xmin_1 ymin_1 xmax_1 ymax_1 finding_label_2 ...  finding_label_k confidence_k xmin_k ymin_k xmax_k ymax_k `\n\nLet's extract these values.","c4da2af1":"## Enviroment Settings","3782eaad":"Let's take a taste from our data:","8881636f":"One can see that clear covid cases strongly tend to have  larger findings area. The indeterminate cases also tend to have larger findings areas than the atypical ones, but this difference is much less significant. Let's inspect these features again, but now at the study level.","2bccd102":"Now all the positive cases have findings. Let's inspect other findings properties:","6d64f8b0":"## Understanding the challenge\n\nThis challenge, as well as the dataset itself, is composed of two levels. The first is the image level which contains the chest radiographs, and above it we have the study level, which contains the general conclusion from all the patient radiographs.  \nOn the study level, each study is classified by specialists as `Negative for Pneumonia`, or as `Typical Appearance`, `Indeterminate  Appearance`, or `Atypical Appearance` to Covid-19.\nThe grading system is based on [this paper](https:\/\/journals.lww.com\/thoracicimaging\/Fulltext\/2020\/11000\/Review_of_Chest_Radiograph_Findings_of_COVID_19.4.aspx) which proposes a new reporting language for chest radiographs (CXR) findings related to COVID-19, as described in the following table (Table 1 in the paper):\n\n\n> |Radiographic Classification | CXR Findings | Suggested Reporting Language|\n|:--------------------------|:-------------|:----------------------------|\n|Typical appearance|Multifocal bilateral, peripheral opacities Opacities with rounded morphology Lower lung\u2013predominant distribution|\u201cFindings typical of COVID-19 pneumonia are present. However, these can overlap with other infections, drug reactions, and other causes of acute lung injury\u201d|\n| Indeterminate appearance | Absence of typical findings AND Unilateral, central or upper lung predominant distribution | \u201cFindings indeterminate for COVID-19 pneumonia and which can occur with a variety of infections and noninfectious conditions\u201d |\n| Atypical appearance | Pneumothorax or pleural effusion Pulmonary edema Lobar consolidation Solitary lung nodule or mass Diffuse tiny nodules Cavity\t|\u201cFindings atypical or uncommonly reported for COVID-19 pneumonia. Consider alternative diagnoses\u201d |\n|Negative for pneumonia | No lung opacities | \u201cNo findings of pneumonia. However, chest radiographic findings can be absent early in the course of COVID-19 pneumonia\u201d|\n      \nAlthough these findings refer to the CXR themselves, on this challenge we were provided with these labels only at the study level, while each study can have many images.\nOn image level, each image has a list of bounding boxes of findings. The bounding boxes can contain findings from different types, as described by the competition hosts:\n> Bounding boxes were placed on lung opacities, whether typical or indeterminate. Bounding boxes were also placed on some atypical findings including solitary lobar consolidation, nodules\/masses, and cavities. Bounding boxes were not placed on pleural effusions, or pneumothoraces. No bounding boxes were placed for the negative for pneumonia category.\n\nThe dataset doesn't distinguish between the findings type. The findings were given the label opacity, and the prediction in the submission for the findings class should be always  `opacity`.  \nThe details of the study grading method according to the findings in the images described in the table above. Even though the exact meaning of the terminology is definitely beyond my understanding, one thing we can learn from this table is that the classifying is based on the nature of findings, as well as on their region in the lungs. This is crucial for a better understanding of what our model is supposed to learn.","63d7a06f":"Before doing anything else, we'd like to change this terrible column name `StudyInstanceUID` to a more reasonable one.","75ff9f4f":"## Basic Imports","c01b621f":"## Evalutaion\n\nThe evaluation at the study level is quite simple: we will check our prediction accuracy. But at the image level, our predictions will be bounding boxes. Probably the bounding boxes will not match exactly to the labeled ones, and we do not care if there are minor differences. So how will we decide whether our predictions are consistent with the labels or not?\nCome to think of it, the most important thing here is how much our predictions area intersect with the ground truth labels. For ideal prediction, the predicted area will match the ground truth exactly. Meaning, the intersection and the area of each prediction and the ground truth are equals. In a more realistic case, our prediction isa bit smaller or larger than the ground truth, or span out in one direction and too short in another. In all these cases, the more the intersection area is large with respect to both the predicted area and the ground truth label area, the more we can regard the prediction as correct. This is the rationale behind the PASCAL VOC2010 IoU (Intersection over Union) evaluation method, which is used in this competition: a bounding box prediction is considered correct if the rate of the intersection over union of  the prediction and the ground truth is greater than $0.5$. i.e, we demand\n$$IoU = \\frac{A_y \\cap A_\\hat{y}}{A_y \\cup A_\\hat{y}} > 0.5$$  \nWhere $A_y$ is the ground truth bounding box area and $A_\\hat{y}$ is the area of the predicted box. ","9f91ff84":"Next, let's look at some properties of the findings. The number of findings varies for each image. Each of them is an opacity (or another type of the above-mentioned findings) in the CXR, and for each of them we provided the bounding box of the opacity area. Let\u2019s look at the number of findings per image and the main statistical properties of their areas: sum, mean, max, etc.","653ee1c8":"In this dataframe each study is classified into one of 4 classes: `Negative for Pneumonia`, `Typical Appearance`, `Indeterminate Appearance`, and `Atypical Appearance`. It is important to know how these classes are distributed over the dataset.  \nIn the evaluation section in the competition details in Kaggle it's said that\n> Studies in the test set may contain more than one label. They are as follows: `negative`, `typical`, `indeterminate`, `atypical`\n\nAccordingly,  this is a multilabel classification task.    \nIn contrast, in [a post](https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/240250) in the competition discussion section, the hosts indicated that\n> Per the grading schema, chest radiographs are classified into one of four categories, which are **mutually exclusive**\n\nSince the two descriptions contradict, it is worth inspecting the training set to see which labels can be assigned to an image together.","5ac49d1b":"Many of the images are cropped, rotated, and have different lummination level. Lungs are contained in all of the images, but the location of the lungs in the image is not constant, The images margin size are varying, and the images may contain other body parts - neck, stomach, hands, etc. To get a better understanding of the matter in hand, it will be helpful to see CXR from the different labels with the annotated bounding boxes drawn on the image.","a4671995":"The dataset is composed of three parts. The CRX files in DICOM format, and two metadata tables: one for the image level and another for the study level. Let's explore first the image-level metadata of the training set.","5b6e7eac":"In most cases there\u2019s one image per study. But in the cases with multiple images, what is the difference between the images? Is the prognosis based on all of the images?  \nIt is straightforward to get an answer to the second password from the data. We simply count the number of images labeled with findings.","4437e070":"In `Body Part Examined` column, we have the unique values\n>`'CHEST' 'PORT CHEST' 'TORAX' nan 'T?RAX' 'Pecho' 'THORAX' 'ABDOMEN'\n 'SKULL' '2- TORAX' 'T\u00d2RAX' 'PECHO'`\n \n'CHEST', 'THORAX' (which exists in many versions), and 'PECHO' are all the same, whether you prefer English or Spanish. Let's try to see what else we can extract from the metadata. (Why do we have SKULLs here???)","ea99ac8e":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26680\/logos\/header.png?t=2021-04-23-22-04-05)\" \/>","50f85be7":"So it is established that , there is never more than one image labeled with findings per study. Now it will interesting to see the difference between the images in one study. To do that, we have to pay attention to the third and most important part of our dataset - the DICOM files.","023b17b4":"Before further exploration of findings properties, let's explore the study-level metadata:","54a9cd5b":"The results are not clear for the non-expert eye. Although sometimes there is a kind of opacity in the boxes, in other cases there is no clear difference between the area inside and the area outside the box. This will affect the algorithm development and verification processes since I will not be able to rely on my own knowledge and intuition.","c6ae384b":"#### DICOM files\nThe data is provided in [DICOM fromat](https:\/\/en.wikipedia.org\/wiki\/DICOM), which is the standard in medical imaging information and related data. This format packs each medical image with related data, such as Patient Id, Name, Sex, etc. In our case, the data de-identified for privacy reasons, but we still may have important data in the metadata provided in the DICOM file.\nLet's pick a file and see what it looks like.","3e8c858b":"The labels are only `none` and `opacity`, and the confidence in the training set is always 1 (since this is a labeled dataset). All bounding box data is provided in the `boxes` field, this field is `Nan` when there are no findings (as we can see in the second row in the head of the Dataframe printed above). So in fact, all the data we need exists in the `boxes` field.  \nThus, we can extract the findings data directly from the boxes fields and examine some of their properties.","a5e5b5cb":"Now we can see the domains of these values","1e52afb5":"It becomes clear that the above conclusion is correct. Almost all of the clear covid-19 cases have 2 findings, and a couple of them with 3 findings. The indeterminate cases also have at least one finding each, and only the non-covid cases sometimes have no findings, even when positive to pneumonia. But according to our table, `No Findings` means `Positive to Pneumonia`, so we'll put these instances aside for now.","fac3f78d":"We have 47% certain Covid cases (typical appearance), 36% non-covid (28% negative for pneumonia and 8% atypical to covid), and 17% obscure cases. From a covid vs non-covid point of view, the dataset is quite balanced. But from the classification point of view,  almost 50% of the cases are from one class and only 8% of the cases are from another class.  \nTo better understand the distribution, let\u2019s see that classification distribution in absolute numbers:","4d80d687":"So in the training set, each of the studies has a single label attached to it, and this classification is in fact one-hot encoded classification for each study to one of those 4 classes.  \n\nSince the results of our check on the training set supports the second post, and since inherently, by their meanings, the labels seem to be mutually exclusive , we will leave it as a single-label classification task. For convenience , we will store the labels in one row rather than in one-hot encoding format and join it with the images dataframe.","764f48e2":"The findings count for each class label is:","453062f5":"Now let's take a look at the metadata provided by the DICOMs. The attributes that may interest us are the body part examined, sex, image size, pixel spacing (represent the physical size of the image), modality (the scanning method), and image type. Let's extract these features to a pandas dataframe for later use.","ce8d85d0":"We can see here the well-known fact that statistically women suffer less from Covid-19. Although our data is quite balanced with respect to sex, women suffer much less from pneumonia of any kind. In the typical covid cases (clear\/severe covid cases) there are about two-thirds cases of women than men.","69422e0b":"It is clear now that all the negative results have no findings at all, as stated in the grading method table. On the other hand, for each of the other four types it seems that there are instances with no opacity findings, contrary to these grades descriptions in the above table. But is this really the case? We saw earlier that we have more images than studied. That is, some studies have more than one image. So it seems that in some cases the prognosis is based on findings that are determined only in one of the scans. Let's verify this conclusion.","1e9360e2":"The `ABDOMEN` images seem to contain the body's lower part too. Besides that, there does not seem to be a significant difference between the images group (in particular, we have no `SKULL`s here).\n","5c67d8ad":"# Notebook initialize","ee745e11":"Next we will check how these classes are distributed over the dataset.","270ef5ad":"# Project Description","db9631ae":"Let's inspect the value ranges of our new data:","4488570a":"The first thing to inspect is the sex field.\nHow is our data splitted between the sexes? How the sex is related with the COVID19 prognoses?","3113fe76":"It seems that there is no significant difference between the image level and study level. But, this leads us to two important questions: How many images are related to one study on average? In case that a study had more than one image, how many images the prognosis is based on?"}}