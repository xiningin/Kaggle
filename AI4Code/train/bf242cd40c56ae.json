{"cell_type":{"5b6011bc":"code","eddc1f21":"code","51ed184b":"code","d74f718c":"code","5892e619":"code","a88ff964":"code","1e8ce2c7":"code","f43041b7":"code","8bdf6128":"markdown","4fe5fa73":"markdown","14b6fe5f":"markdown","15aa88fa":"markdown"},"source":{"5b6011bc":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa as lr\nimport librosa.display as lrd\nnp.warnings.filterwarnings('ignore')\n\ndef read(**kwargs):\n    return pd.read_csv('..\/input\/train.csv', **kwargs)\n\ndef spectrum(x, sr):\n    Xr, Xi = lr.stft(np.real(x).astype(float)), lr.stft(np.imag(x).astype(float))\n    Xdb = lr.amplitude_to_db(np.sqrt(abs(Xr)**2 + abs(Xi)**2))\n    plt.figure(figsize=(14, 5))\n    lrd.specshow(Xdb, sr=int(sr), x_axis='time', y_axis='hz')\n    plt.show()\n    \ndef waveplot(x, sr):\n    xr, xi = np.real(x).copy(), np.imag(x).copy()\n    plt.figure(figsize=(14, 5))\n    lrd.waveplot(xr.astype(float), sr=int(sr))\n    lrd.waveplot(xi.astype(float), sr=int(sr))\n    plt.show()\n    \nx = read(nrows=1500000).acoustic_data.values\nspectrum(x, 4e6)","eddc1f21":"from scipy.signal import firls, convolve, decimate\nfrom scipy.spatial.distance import pdist\n\nfilt = firls(2001, bands=[0,240e3,245e3,250e3,255e3,2e6], desired=[0,0,1,1,0,0], fs=4e6)\nwaveplot(filt, 4e6);\n\ndef resample(xs):\n    xs = convolve(xs.astype(float), filt, mode='valid')\n    t = 2*np.pi*250e3\/4e6*np.arange(len(xs))\n    xs = xs*(np.cos(t) + 1j*np.sin(t))\n    xs = decimate(xs, 150, ftype='fir')\n    return xs\n\nspectrum(resample(x), 4e6\/150)\nwaveplot(resample(x), 4e6\/150)","51ed184b":"from tqdm import tqdm_notebook as tqdm\nnrows = 629145481\nchunksize = 150000\n\nxs = []\nys = []\nfor df in tqdm(read(chunksize=chunksize), total=nrows\/\/chunksize):\n    xs += [resample(df.acoustic_data)]\n    ys += [df.time_to_failure.iloc[-1]]","d74f718c":"import dask\nfrom dask.diagnostics import ProgressBar\nfrom scipy.spatial.distance import pdist\n\ndef icdf(x):\n    qs = np.linspace(0,1,200)\n    return 2*np.quantile(pdist(np.column_stack([x.real,x.imag])), qs)\n\nwith ProgressBar():\n    dists = np.array(dask.compute(*(dask.delayed(icdf)(xi) for xi in xs)))","5892e619":"from sklearn.decomposition import PCA\nX_pre = dists\npca = PCA(n_components=20)\npca.fit(X_pre)\nX = pca.transform(X_pre)\n\nplt.figure(figsize=[15,15])\nfor i in range(10):\n    comp = pca.components_[i]\n    plt.plot(np.linspace(0,1,200), comp\/abs(comp).max()\/2+i)","a88ff964":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nreg = KNeighborsRegressor(n_neighbors=100, weights='distance')\nX_post = X[:,:3]\nplt.plot(np.sort(-cross_val_score(reg, X_post, ys, cv=8, n_jobs=4, scoring='neg_mean_absolute_error')));","1e8ce2c7":"reg.fit(X_post, ys)\n\n@dask.delayed\ndef predict(fname):\n    fdf = pd.read_csv(fname)\n    x = fdf.acoustic_data.values\n    x = resample(x)\n    x = icdf(x)\n    x = pca.transform(x[None,:])\n    y = reg.predict(x[:,:3])\n    return y[0]\n    \nimport glob\nfiles = pd.DataFrame(glob.glob('..\/input\/test\/*.csv'), columns=['filename'])\nfiles['seg_id'] = files.filename.str.extract('.*\/(seg_.*).csv', expand=True)\nwith ProgressBar():\n    predictions = dask.compute(*(predict(fname) for fname in files.filename))\nfiles['time_to_failure'] = predictions\nfiles[['seg_id','time_to_failure']].to_csv('submission.csv', index=False)","f43041b7":"!head submission.csv","8bdf6128":"Looks good. The signal looks a lot less \"noisy\". Now we're ready to compute some statistics on this.\n\n## Computing Pairwise Statistics\n\nThis is the most interesting part, in my opinion, as it is essentially automatic feature engineering.\nThe idea is simple: there are a class of statistical estimators called L-estimators that are weighted\ncombinations of the quantiles of a distribution. What if we try to find these automatically?\n\nMathematically, if $F(x)$ is the CDF of a distribution, an L-estimator is given by\n$$L = \\sum_{i} w_i F^{-1}(q_i)\\,,$$\nwhere $w_i$ is the weight for the $i$th quantile $q_i$. This is just the inner product of the inverse\nCDF with a vector! Clearly, then, a set of L-estimators forms a subspace of the inverse CDF space.\n\nWhy is this important, you ask? Well, if we can find the most significant subspace of the inverse CDF\nspace, we can use those as features. Furthermore, this is *exactly* the purpose of PCA (principle\ncomponent analysis). Then we can just let PCA do the work of feature engineering for us. Let's try this out:","4fe5fa73":"# LANL Earthquake Prediction &mdash; Finding L-estimators via PCA\n\n*For visitors not aware, LANL Earthquake Prediction is a Kaggle Competition put out by\nLos Alamos National Laboratory to predict the amount of time until an earthquake occurs.\nThe data is from an experiment set up in a lab to model the process of how earthquakes\noccur on a global scale. The data consists of two time series: acoustic data (sampled at\napproximately 4 MHz) and the time until the earthquake occurs. The goal is to predict the time\nto failure for each sequence in the test data, each of which consists of 150k samples.*\n\nOver the past couple weeks I've played around with this data set and read through some\npublic kernels. For the most part, these follow the same process: for each 150k sample chunk,\ncompute a bunch of manual statistics and try to build a robust estimator out of them (usually\nsome kind of gradient-boosted random forest). I had an interesting thought: is it possible to\nautomatically derive feature statistics from the data?\n\nThis notebook walks through the process of doing so. For those of you who just want to know\nthe bottom line, here it is: I performed PCA on the CDFs of pairwise distances within a sample\nand used the top three it to build a simple $k$-NN regressor. It worked fairly well for how simple\nit is, in my opinion. In more detail, the process is the following:\n\n1. Filter high-frequency noise from the signal and decimate it to a more reasonable sample rate.\n2. Estimate the CDF of the pairwise distances within the chunk.\n3. Perform PCA on the CDF's for each chunk.\n4. Create a $k$-NN regressor.\n\n## Filtering Noise from the Signal\n\nThis problem actually begins as a signal processing problem. We are given a time-series of discretized\nacoustic data, which immediately should lead to the question: \"What are the frequencies we care about?\"\nThe first step, then, is to plot the power spectrum over the signal:","14b6fe5f":"Right off the bat, we can see that most of the signal is noise above ~300 kHz. This can be\nfrom a number of sources&mdash;thermal noise, quantization error, room noise, and so on&mdash;but\nthe point is that this noise only serves to make predictions more difficult. We would like to isolate\nthe band which has the highest signal-to-noise ratio.\n\nThe other interesting thing here is that it looks like the signal is a series of pulses. I don't have\na good understanding of earthquakes, but I would guess that these are from micro-slips that occur leading\nup to the earthquake. Of course, an earthquake is just another slip as well, so I am curious whether it\nis possible to model the whole process as a series of slips of different magnitudes.\n\nAnyway, let's filter out the noise. I isolated a small band from 240 kHz to 260 kHz using a linear-phase\nFIR filter, de-modulated the signal to baseband, and decimated it by a factor of 90 (~45 kHz).","15aa88fa":"Using PCA, it seems that most of the variations occur in the upper percentiles\nof the distribution (not surprising). I think that this reinforces the idea of\nmicro-slips causing creating noise since most of the variations would occur in\nthe tail of the distribution.\n\nLet's try to use these features to build a predictor.\nI decided to use a $k$-NN regressor as it tends to be less fussy. In the above\npicture, it seems that the bottom three features are most informative. The rest\nlook like they are a product of too little data."}}