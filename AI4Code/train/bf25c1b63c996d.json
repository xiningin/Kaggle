{"cell_type":{"c52cfc6c":"code","a266fe6e":"code","bb611518":"code","60557bc9":"code","e09a566d":"code","78856c72":"code","2d0f285f":"code","94f00bc0":"code","51778ffe":"code","b94c3c88":"code","7d655d5c":"code","3dc360a1":"code","799bdca3":"code","3f6860d2":"markdown","ce40be2c":"markdown","f7e9fc81":"markdown","2099c1a4":"markdown","c1d389f0":"markdown","eb16bc87":"markdown","72f775ea":"markdown","848195d3":"markdown","ae98c81d":"markdown","be6fe779":"markdown","b074a43a":"markdown","c2176ef2":"markdown"},"source":{"c52cfc6c":"import numpy as np\nimport cv2\nimport os\nimport time\n\nimport tensorflow as tf\nfrom keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dropout,Flatten,Dense,Activation\nfrom keras.models import Sequential\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\nimport warnings as wrn\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n","a266fe6e":"train_dog_path = '..\/input\/dogs-cats-images\/dataset\/training_set\/dogs'\ntrain_cat_path = '..\/input\/dogs-cats-images\/dataset\/training_set\/cats'\n\ntest_dog_path = '..\/input\/dogs-cats-images\/dataset\/test_set\/dogs'\ntest_cat_path = '..\/input\/dogs-cats-images\/dataset\/test_set\/cats'\n","bb611518":"train_dog_paths = [os.path.join(train_dog_path,image) for image in os.listdir(train_dog_path)]\ntrain_cat_paths = [os.path.join(train_cat_path,image) for image in os.listdir(train_cat_path)]\n\ntest_dog_paths = [os.path.join(test_dog_path,image) for image in os.listdir(test_dog_path)]\ntest_cat_paths = [os.path.join(test_cat_path,image) for image in os.listdir(test_cat_path)]\n\nfor path in train_cat_paths[:4]:\n    print(path)","60557bc9":"# DOG 0\n# CAT 1 \n\nIM_SIZE = 150\n\nstart_time = time.time()\n\nx_train = []\nx_test = []\ny_train = []\ny_test = []\n\nfor image in train_dog_paths:\n    try:\n        im  = cv2.imread(image)\n        im  = cv2.resize(im,(IM_SIZE,IM_SIZE))\n        x_train.append(im)\n        y_train.append(0)\n    except:\n        print(\"Something wrong happened when reading this file {}\".format(image))\n\nfor image in train_cat_paths:\n    try:\n        im  = cv2.imread(image)\n        im  = cv2.resize(im,(IM_SIZE,IM_SIZE))\n        x_train.append(im)\n        y_train.append(1)\n    except:\n        print(\"Something wrong happened when reading this file {}\".format(image))\n\nfor image in test_dog_paths:\n    try:\n        im  = cv2.imread(image)\n        im  = cv2.resize(im,(IM_SIZE,IM_SIZE))\n        x_test.append(im)\n        y_test.append(0)\n    except:\n        print(\"Something wrong happened when reading this file {}\".format(image))\n\nfor image in test_cat_paths:\n    try:\n        im  = cv2.imread(image)\n        im  = cv2.resize(im,(IM_SIZE,IM_SIZE))\n        x_test.append(im)\n        y_test.append(1)\n    except:\n        print(\"Something wrong happened when reading this file {}\".format(image))\n\nx_train,x_test,y_train,y_test = np.array(x_train),np.array(x_test),np.array(y_train),np.array(y_test)\n        \nend_time = time.time()\nprint(\"Reading and resizing images took {} minutes. \".format(round((end_time-start_time)\/60),2)) \n","e09a566d":"x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=1)\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n\nprint(x_val.shape)\nprint(y_val.shape)","78856c72":"plt.imshow(x_train[3213])\nplt.title(y_train[3213])\nplt.axis(\"off\")\nplt.show()","2d0f285f":"plt.imshow(x_train[7103])\nplt.title(y_train[7103])\nplt.axis(\"off\")\nplt.show()","94f00bc0":"datagen = ImageDataGenerator(horizontal_flip=True)\n\ndatagen.fit(x_train)","51778ffe":"model = Sequential()\n\nmodel.add(Conv2D(128,kernel_size=(4,4),strides=1,padding=\"same\",input_shape=(IM_SIZE,IM_SIZE,3)))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128,kernel_size=(4,4),strides=1,padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding=\"same\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(256,kernel_size=(4,4),strides=1,padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding=\"same\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128,kernel_size=(4,4),strides=1,padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding=\"same\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nopt = tf.keras.optimizers.RMSprop(lr=0.01)\n\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\nmodel.summary()","b94c3c88":"BATCH_SIZE = 32\nEPOCHS = 50\n","7d655d5c":"start_time = time.time()\n\nmodel.fit_generator(datagen.flow(x_train,y_train,batch_size=32),\n                    epochs=EPOCHS,\n                    validation_data = (x_val,y_val),\n                    steps_per_epoch = x_train.shape[0] \/\/ BATCH_SIZE\n                   )\n\nend_time = time.time()\nprocess_time = round((end_time-start_time)\/60,2)\n\nprint(\"Fitting model {} epochs took {} minutes.\".format(EPOCHS,process_time))\n","3dc360a1":"print(\"Test accuracy of model is {}\".format(round(model.evaluate(x_test,y_test)[1],2)))","799bdca3":"y_preds = model.predict_classes(x_test)\ny_true = y_test\n\nconf_matrix = confusion_matrix(y_pred=y_preds,y_true=y_true)\n\nimport seaborn as sns\n\n\nplt.subplots(figsize=(6,6))\nsns.heatmap(conf_matrix,annot=True,fmt=\".1f\",linewidths=1.5,cmap=\"BrBG\")\nplt.show()","3f6860d2":"* It looks very cute!","ce40be2c":"# CNN Modeling\n\nIn this section I am going to build and fit our CNN Model using Keras.","f7e9fc81":"Now let's take a look at confusion matrix.","2099c1a4":"# Importing Necessary Libraries\nIn this section I am going to import necessary libraries.","c1d389f0":"* It looks very cute as well.","eb16bc87":"# Checking Random Samples\nIn this section I am going to check random samples from train dataset. In order to see images I will use matplotlib's imshow function.","72f775ea":"* Everything looks great, we can read images now. We will use opencv in order to read and resize images. ","848195d3":"# Conclusion\n\nThanks for your attention, I will wait for your comments,questions and upvotes.","ae98c81d":"# Evaluating Model\n\nIn this section I am going to evaluate our trained model. In order to evaluate our model, I will use our final test set.","be6fe779":"# Animal Image Classification | Dog or Cat\n\nHello people, welcome to this kernel. In this kernel I am going to classify dog and cat images using Keras library. Before starting, let's take a look at the content of this kernel.\n\n\n# Notebook Content\n1. Importing Necessary Libraries\n1. Importing The Data\n1. Checking Random Samples\n1. Data Augmentation\n1. CNN Modeling\n1. Evaluating Model\n1. Conclusion","b074a43a":"# Data Augmentation\n\nIn this section I am going to apply data augmentation techniques to my data. But before this I want to explain that why should we use data augmentation.\n\nIn deep learning, sometimes neural models memorize train dataset, so it can't be succesfull in test dataset and validation set, we call this as overfitting.\n\nIn order to avoid overfitting, we can increase the amount of data and we can use data augmentation. \n\nNow, I will create our image data generator.","c2176ef2":"# Importing The Data\n\nIn this section I am going to import the data. In order to load the data, I will follow these steps:\n\n1. Determining paths of folders\n1. Determining paths of images\n1. Reading and resizing images\n1. Train validation splitting\n1. Checking arrays"}}