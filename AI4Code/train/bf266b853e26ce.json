{"cell_type":{"53f49d26":"code","1eb01513":"code","f361c455":"code","796cb915":"code","9fcb7daf":"code","5b3da6a5":"code","e940ae45":"code","0ebf2fcc":"code","e2d8d2d7":"code","7f2ba6e0":"code","fde47077":"code","7d37ca88":"code","ae38354e":"code","68ecd2d4":"markdown","c83c789c":"markdown","ac129eb7":"markdown"},"source":{"53f49d26":"# Importing the dependencies that we need for this project\n\nimport json\nimport pandas as pd\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport re\n\nimport gensim","1eb01513":"# Download NLTK copuses\n\n#nltk.download('stopwords')","f361c455":"# Getting a list of folders present in the dataset\n\npath = \"\/kaggle\/input\/CORD-19-research-challenge\"\n\ndatasets = []\n\nfor d in listdir(path):\n    if not isfile(join(path, d)):\n        datasets.append(d)\n\nprint(datasets)","796cb915":"# Making a list of JSON files\njson_files = []\n\nfor folder in datasets:\n    files_path = join(path, folder, folder)\n    \n    files = listdir(files_path)\n    \n    for file in files:\n        json_files.append(join(files_path, file))\n        \nprint(json_files[0:10])","9fcb7daf":"def word_tokenizer(sentence):\n    return word_tokenize(sentence) \n\ndef remove_stop_words(tokenized_sentences):\n    stop_words = set(stopwords.words('english'))\n    return [t for t in tokenized_sentences if not t in stop_words] \n\ndef remove_unchars(doc):\n    doc = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', '', doc, flags=re.MULTILINE)\n    doc = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)', '', doc)\n    doc = re.sub(r'\\b[0-9]+\\b\\s*', '', doc)\n\n    return doc\n\ndef preprocessing_text(text):\n    text = text.lower()\n    \n    text = remove_unchars(text)\n    \n    words = word_tokenizer(text)\n    \n    cleaned_words = remove_stop_words(words)\n    \n    return cleaned_words","5b3da6a5":"# Reading all the json file and extracting the text of the paper\n\ntexts = []\ncleaned_texts = []\n\nfor file in tqdm(json_files):\n    data = json.load(open(file, \"rb\"))\n    \n        \n    text = []\n    for t in data[\"body_text\"]:\n        texts.append(t[\"text\"])\n        cleaned_texts.append(preprocessing_text(t[\"text\"]))","e940ae45":"len(texts)","0ebf2fcc":"# Training a Gensim Word2Vec model\n\nmodel = gensim.models.Word2Vec(sentences=cleaned_texts)","e2d8d2d7":"# Saving the model\n\nmodel.wv.save_word2vec_format(\"model.bin\", binary=True)","7f2ba6e0":"# Methods for finding same sentences based on keywords\n\ndef find_similar_words(word):\n    words = [*word]\n    \n    for word, _ in model.wv.most_similar(word):\n        if len(word) > 2:\n            words.append(word)\n        \n    return words\n\ndef find_sentences_rank(words, keywords):\n    return len(set(words).intersection(set(keywords)))\n\ndef find_similar_sentences(cleaned_sentences, original_sentences, keywords):\n    try:\n        keywords = find_similar_words(keywords)\n        \n        similar_sentences = []\n        similar_sentences_processed = []\n\n        for i in range(len(cleaned_sentences)):\n            k = find_sentences_rank(cleaned_sentences[i], keywords)\n            if k > 2:\n                similar_sentences.append(original_sentences[i])\n\n        return similar_sentences\n    except  Exception as ex:\n        print(\"ERROR: \", ex)","fde47077":"# Finding similar sentences based on the word incubation\n\nsimilar_sentences = find_similar_sentences(cleaned_texts, texts, ['incubation'])","7d37ca88":"for similar in similar_sentences:\n    print(similar)\n    print()","ae38354e":"print(len(similar_sentences))","68ecd2d4":"The data comes as json file stored into multiple folders, so our first job is to get a clean dataframe that contains only the title, abstract and text of the paper.\n\nTo do that first we need to get a list of all the json files that are in the dataset stored in multiple folders.","c83c789c":"# COVID-19 Word Embedding Approach\n\n## Answering: ~~What is known about transmission, incubation, and environmental stability?~~ Well, possibly everything.\n\n**Prepared by Abhishek Chatterjee (abhishek.chatterjee97@protonmail.com), Github: @imdeepmind**","ac129eb7":"### The Big Picture\n\nThe main idea that I have is to make this entire corpus of scholarly articles completely searchable so that anyone can search the corpus using simple keywords and find relevant sentences. \n\nTo do that, here I'm using Word Embedding models (currently using Word2Vec, experimenting with others also) to find sentences that are similar to the keywords.\n\nWord Embedding models are great as they store not only a numeric (in vectors for embedding models) representation of the words, they store the meaning of the words, and the correlations of the words with other words.  \n\nThese Vector representations are very useful as we can calculate the Cosine Similarity of two words and find matches"}}