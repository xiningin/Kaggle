{"cell_type":{"b02dd687":"code","9a5f8835":"code","a37ac359":"code","76fde3f5":"code","18137446":"code","0d4e751c":"code","340de6b0":"code","3aadae06":"code","3d5ebc1e":"code","f3efbf26":"code","13dc69c9":"code","e3727194":"code","c0f0bbe2":"code","fed84524":"code","5b177846":"code","5cc10f86":"markdown","55a10cc9":"markdown","1ff569b1":"markdown","5a76b1a6":"markdown","af524c6c":"markdown","1fba5131":"markdown","96a97113":"markdown"},"source":{"b02dd687":"import math, re, os, gc\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom keras.utils.generic_utils import get_custom_objects\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nprint(\"Tensorflow version \" + tf.__version__)","9a5f8835":"import os, gc\nimport numpy as np \nimport pandas as pd \nimport cv2\nimport glob\nimport numba\nimport pathlib\nfrom tqdm.notebook import tqdm\nimport tifffile as tiff\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\n# For reading tiff images in parts \nimport rasterio\nfrom rasterio.windows import Window\n\nimport warnings\nwarnings.filterwarnings('ignore')","a37ac359":"IMAGE_SIZE = [512, 512]\nDIM = IMAGE_SIZE[0]\nEFUN = 3\nFOLDS = 4","76fde3f5":"data_path = '\/kaggle\/input\/'\npath_submission_file = os.path.join(data_path, 'hubmap-kidney-segmentation\/sample_submission.csv')\n","18137446":"import re\nfrom collections import namedtuple\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nimport math\nimport numpy as np\n\nGlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n                                           'drop_connect_rate'])\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n\nBlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n                                     'id_skip', 'strides', 'se_ratio'])\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\nIMAGENET_WEIGHTS = {\n\n    'efficientnet-b0': {\n        'name': 'efficientnet-b0_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b0_imagenet_1000.h5',\n        'md5': 'bca04d16b1b8a7c607b1152fe9261af7',\n    },\n\n    'efficientnet-b1': {\n        'name': 'efficientnet-b1_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b1_imagenet_1000.h5',\n        'md5': 'bd4a2b82f6f6bada74fc754553c464fc',\n    },\n\n    'efficientnet-b2': {\n        'name': 'efficientnet-b2_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b2_imagenet_1000.h5',\n        'md5': '45b28b26f15958bac270ab527a376999',\n    },\n\n    'efficientnet-b3': {\n        'name': 'efficientnet-b3_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b3_imagenet_1000.h5',\n        'md5': 'decd2c8a23971734f9d3f6b4053bf424',\n    },\n\n    'efficientnet-b4': {\n        'name': 'efficientnet-b4_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b4_imagenet_1000.h5',\n        'md5': '01df77157a86609530aeb4f1f9527949',\n    },\n\n    'efficientnet-b5': {\n        'name': 'efficientnet-b5_imagenet_1000.h5',\n        'url': 'https:\/\/github.com\/qubvel\/efficientnet\/releases\/download\/v0.0.1\/efficientnet-b5_imagenet_1000.h5',\n        'md5': 'c31311a1a38b5111e14457145fccdf32',\n    }\n\n}\n\n\ndef round_filters(filters, global_params):\n    \"\"\"Round number of filters.\"\"\"\n    multiplier = global_params.width_coefficient\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    if not multiplier:\n        return filters\n\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\"Round number of repeats.\"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef get_efficientnet_params(model_name, override_params=None):\n    \"\"\"Get efficientnet params based on model name.\"\"\"\n    params_dict = {\n        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n        # Note: the resolution here is just for reference, its values won't be used.\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    }\n    if model_name not in params_dict.keys():\n        raise KeyError('There is no model named {}.'.format(model_name))\n\n    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n\n    blocks_args = [\n        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s11_e6_i192_o320_se0.25',\n    ]\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=0.2,\n        num_classes=1000,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None)\n\n    if override_params:\n        global_params = global_params._replace(**override_params)\n\n    decoder = BlockDecoder()\n    return decoder.decode(blocks_args), global_params\n\n\nclass BlockDecoder(object):\n    \"\"\"Block Decoder for readability.\"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n        assert isinstance(block_string, str)\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        if 's' not in options or len(options['s']) != 2:\n            raise ValueError('Strides options should be a pair of integers.')\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            strides=[int(options['s'][0]), int(options['s'][1])]\n        )\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    def decode(self, string_list):\n        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n        Args:\n          string_list: a list of strings, each string is a notation of block.\n        Returns:\n          A list of namedtuples to represent blocks arguments.\n        \"\"\"\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(self._decode_block_string(block_string))\n        return blocks_args\n\n    def encode(self, blocks_args):\n        \"\"\"Encodes a list of Blocks to a list of strings.\n        Args:\n          blocks_args: A list of namedtuples to represent blocks arguments.\n        Returns:\n          a list of strings, each string is a notation of block.\n        \"\"\"\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(self._encode_block_string(block))\n        return block_strings\n\n\nclass Swish(layers.Layer):\n    def __init__(self, name=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n    def call(self, inputs, **kwargs):\n        return tf.nn.swish(inputs)\n\n    def get_config(self):\n        config = super().get_config()\n        config['name'] = self.name\n        return config\n\n\ndef SEBlock(block_args, **kwargs):\n    num_reduced_filters = max(\n        1, int(block_args.input_filters * block_args.se_ratio))\n    filters = block_args.input_filters * block_args.expand_ratio\n\n    spatial_dims = [1, 2]\n\n    try:\n        block_name = kwargs['block_name']\n    except KeyError:\n        block_name = ''\n\n    def block(inputs):\n        x = inputs\n        x = layers.Lambda(lambda a: K.mean(a, axis=spatial_dims, keepdims=True))(x)\n        x = layers.Conv2D(\n            num_reduced_filters,\n            kernel_size=[1, 1],\n            strides=[1, 1],\n            kernel_initializer=conv_kernel_initializer,\n            padding='same',\n            name=block_name + 'se_reduce_conv2d',\n            use_bias=True\n        )(x)\n\n        x = Swish(name=block_name + 'se_swish')(x)\n\n        x = layers.Conv2D(\n            filters,\n            kernel_size=[1, 1],\n            strides=[1, 1],\n            kernel_initializer=conv_kernel_initializer,\n            padding='same',\n            name=block_name + 'se_expand_conv2d',\n            use_bias=True\n        )(x)\n\n        x = layers.Activation('sigmoid')(x)\n        out = layers.Multiply()([x, inputs])\n        return out\n\n    return block\n\n\nclass DropConnect(layers.Layer):\n\n    def __init__(self, drop_connect_rate, **kwargs):\n        super().__init__(**kwargs)\n        self.drop_connect_rate = drop_connect_rate\n\n    def call(self, inputs, **kwargs):\n        def drop_connect():\n            keep_prob = 1.0 - self.drop_connect_rate\n\n            # Compute drop_connect tensor\n            batch_size = tf.shape(inputs)[0]\n            random_tensor = keep_prob\n            random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n            binary_tensor = tf.floor(random_tensor)\n            output = tf.math.divide(inputs, keep_prob) * binary_tensor\n            return output\n\n        return K.in_train_phase(drop_connect(), inputs, training=None)\n\n    def get_config(self):\n        config = super().get_config()\n        config['drop_connect_rate'] = self.drop_connect_rate\n        return config\n\n\ndef conv_kernel_initializer(shape, dtype=K.floatx()):\n    \"\"\"Initialization for convolutional kernels.\n    The main difference with tf.variance_scaling_initializer is that\n    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n    standard deviation, whereas here we use a normal distribution. Similarly,\n    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n    a corrected standard deviation.\n    Args:\n        shape: shape of variable\n        dtype: dtype of variable\n    Returns:\n        an initialization for the variable\n    \"\"\"\n    kernel_height, kernel_width, _, out_filters = shape\n    fan_out = int(kernel_height * kernel_width * out_filters)\n    return tf.random.normal(\n        shape, mean=0.0, stddev=np.sqrt(2.0 \/ fan_out), dtype=dtype)\n\n\ndef dense_kernel_initializer(shape, dtype=K.floatx()):\n    init_range = 1.0 \/ np.sqrt(shape[1])\n    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n\n\ndef MBConvBlock(block_args, global_params, idx, drop_connect_rate=None):\n    filters = block_args.input_filters * block_args.expand_ratio\n    batch_norm_momentum = global_params.batch_norm_momentum\n    batch_norm_epsilon = global_params.batch_norm_epsilon\n    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n\n    block_name = 'blocks_' + str(idx) + '_'\n\n    def block(inputs):\n        x = inputs\n\n        # Expansion phase\n        if block_args.expand_ratio != 1:\n            expand_conv = layers.Conv2D(filters,\n                                        kernel_size=[1, 1],\n                                        strides=[1, 1],\n                                        kernel_initializer=conv_kernel_initializer,\n                                        padding='same',\n                                        use_bias=False,\n                                        name=block_name + 'expansion_conv2d'\n                                        )(x)\n            bn0 = layers.BatchNormalization(momentum=batch_norm_momentum,\n                                            epsilon=batch_norm_epsilon,\n                                            name=block_name + 'expansion_batch_norm')(expand_conv)\n\n            x = Swish(name=block_name + 'expansion_swish')(bn0)\n\n        # Depth-wise convolution phase\n        kernel_size = block_args.kernel_size\n        depthwise_conv = layers.DepthwiseConv2D(\n            [kernel_size, kernel_size],\n            strides=block_args.strides,\n            depthwise_initializer=conv_kernel_initializer,\n            padding='same',\n            use_bias=False,\n            name=block_name + 'depthwise_conv2d'\n        )(x)\n        bn1 = layers.BatchNormalization(momentum=batch_norm_momentum,\n                                        epsilon=batch_norm_epsilon,\n                                        name=block_name + 'depthwise_batch_norm'\n                                        )(depthwise_conv)\n        x = Swish(name=block_name + 'depthwise_swish')(bn1)\n\n        if has_se:\n            x = SEBlock(block_args, block_name=block_name)(x)\n\n        # Output phase\n        project_conv = layers.Conv2D(\n            block_args.output_filters,\n            kernel_size=[1, 1],\n            strides=[1, 1],\n            kernel_initializer=conv_kernel_initializer,\n            padding='same',\n            name=block_name + 'output_conv2d',\n            use_bias=False)(x)\n        x = layers.BatchNormalization(momentum=batch_norm_momentum,\n                                      epsilon=batch_norm_epsilon,\n                                      name=block_name + 'output_batch_norm'\n                                      )(project_conv)\n        if block_args.id_skip:\n            if all(\n                    s == 1 for s in block_args.strides\n            ) and block_args.input_filters == block_args.output_filters:\n                # only apply drop_connect if skip presents.\n                if drop_connect_rate:\n                    x = DropConnect(drop_connect_rate)(x)\n                x = layers.add([x, inputs])\n\n        return x\n\n    return block\n\n\ndef freeze_efficientunet_first_n_blocks(model, n):\n    mbblock_nr = 0\n    while True:\n        try:\n            model.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr))\n            mbblock_nr += 1\n        except ValueError:\n            break\n\n    all_block_names = ['blocks_{}_output_batch_norm'.format(i) for i in range(mbblock_nr)]\n    all_block_index = []\n    for idx, layer in enumerate(model.layers):\n        if layer.name == all_block_names[0]:\n            all_block_index.append(idx)\n            all_block_names.pop(0)\n            if len(all_block_names) == 0:\n                break\n    n_blocks = len(all_block_index)\n\n    if n <= 0:\n        print('n is less than or equal to 0, therefore no layer will be frozen.')\n        return\n    if n > n_blocks:\n        raise ValueError(\"There are {} blocks in total, n cannot be greater than {}.\".format(n_blocks, n_blocks))\n\n    idx_of_last_block_to_be_frozen = all_block_index[n - 1]\n    for layer in model.layers[:idx_of_last_block_to_be_frozen + 1]:\n        layer.trainable = False\n\n\ndef unfreeze_efficientunet(model):\n    for layer in model.layers:\n        layer.trainable = True","0d4e751c":"from tensorflow.keras import models, layers\nfrom tensorflow.keras.utils import get_file\n\n__all__ = ['get_model_by_name', 'get_efficientnet_b0_encoder', 'get_efficientnet_b1_encoder',\n           'get_efficientnet_b2_encoder', 'get_efficientnet_b3_encoder', 'get_efficientnet_b4_encoder',\n           'get_efficientnet_b5_encoder', 'get_efficientnet_b6_encoder', 'get_efficientnet_b7_encoder']\n\n\ndef _efficientnet(input_shape, blocks_args_list, global_params):\n    batch_norm_momentum = global_params.batch_norm_momentum\n    batch_norm_epsilon = global_params.batch_norm_epsilon\n\n    # Stem part\n    model_input = layers.Input(shape=input_shape)\n    x = layers.Conv2D(\n        filters=round_filters(32, global_params),\n        kernel_size=[3, 3],\n        strides=[2, 2],\n        kernel_initializer=conv_kernel_initializer,\n        padding='same',\n        use_bias=False,\n        name='stem_conv2d'\n    )(model_input)\n\n    x = layers.BatchNormalization(\n        momentum=batch_norm_momentum,\n        epsilon=batch_norm_epsilon,\n        name='stem_batch_norm'\n    )(x)\n\n    x = Swish(name='stem_swish')(x)\n\n    # Blocks part\n    idx = 0\n    drop_rate = global_params.drop_connect_rate\n    n_blocks = sum([blocks_args.num_repeat for blocks_args in blocks_args_list])\n    drop_rate_dx = drop_rate \/ n_blocks\n\n    for blocks_args in blocks_args_list:\n        assert blocks_args.num_repeat > 0\n        # Update block input and output filters based on depth multiplier.\n        blocks_args = blocks_args._replace(\n            input_filters=round_filters(blocks_args.input_filters, global_params),\n            output_filters=round_filters(blocks_args.output_filters, global_params),\n            num_repeat=round_repeats(blocks_args.num_repeat, global_params)\n        )\n\n        # The first block needs to take care of stride and filter size increase.\n        x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n        idx += 1\n\n        if blocks_args.num_repeat > 1:\n            blocks_args = blocks_args._replace(input_filters=blocks_args.output_filters, strides=[1, 1])\n\n        for _ in range(blocks_args.num_repeat - 1):\n            x = MBConvBlock(blocks_args, global_params, idx, drop_connect_rate=drop_rate_dx * idx)(x)\n            idx += 1\n\n    # Head part\n    x = layers.Conv2D(\n        filters=round_filters(1280, global_params),\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=conv_kernel_initializer,\n        padding='same',\n        use_bias=False,\n        name='head_conv2d'\n    )(x)\n\n    x = layers.BatchNormalization(\n        momentum=batch_norm_momentum,\n        epsilon=batch_norm_epsilon,\n        name='head_batch_norm'\n    )(x)\n\n    x = Swish(name='head_swish')(x)\n\n    x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n\n    if global_params.dropout_rate > 0:\n        x = layers.Dropout(global_params.dropout_rate)(x)\n\n    x = layers.Dense(\n        global_params.num_classes,\n        kernel_initializer=dense_kernel_initializer,\n        activation='softmax',\n        name='head_dense'\n    )(x)\n\n    model = models.Model(model_input, x)\n\n    return model\n\n\ndef get_model_by_name(model_name, input_shape, classes=1000, pretrained=False):\n    \"\"\"Get an EfficientNet model by its name.\n    \"\"\"\n    blocks_args, global_params = get_efficientnet_params(model_name, override_params={'num_classes': classes})\n    model = _efficientnet(input_shape, blocks_args, global_params)\n\n    try:\n        if pretrained:\n            weights = IMAGENET_WEIGHTS[model_name]\n            weights_path = get_file(\n                weights['name'],\n                weights['url'],\n                cache_subdir='models',\n                md5_hash=weights['md5'],\n            )\n            model.load_weights(weights_path)\n    except KeyError as e:\n        print(\"NOTE: Currently model {} doesn't have pretrained weights, therefore a model with randomly initialized\"\n              \" weights is returned.\".format(e))\n\n    return model\n\n\ndef _get_efficientnet_encoder(model_name, input_shape, pretrained=False):\n    model = get_model_by_name(model_name, input_shape, pretrained=pretrained)\n    encoder = models.Model(model.input, model.get_layer('global_average_pooling2d').output)\n    encoder.layers.pop()  # remove GAP layer\n    return encoder\n\n\ndef get_efficientnet_b0_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b0', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b1_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b1', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b2_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b2', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b3_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b3', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b4_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b4', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b5_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b5', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b6_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b6', input_shape, pretrained=pretrained)\n\n\ndef get_efficientnet_b7_encoder(input_shape, pretrained=False):\n    return _get_efficientnet_encoder('efficientnet-b7', input_shape, pretrained=pretrained)","340de6b0":"from tensorflow.keras.layers import *\nfrom tensorflow.keras import models\n\n\n__all__ = ['get_efficient_unet_b0', 'get_efficient_unet_b1', 'get_efficient_unet_b2', 'get_efficient_unet_b3',\n           'get_efficient_unet_b4', 'get_efficient_unet_b5', 'get_efficient_unet_b6', 'get_efficient_unet_b7',\n           'get_blocknr_of_skip_candidates']\n\n\ndef get_blocknr_of_skip_candidates(encoder, verbose=False):\n    \"\"\"\n    Get block numbers of the blocks which will be used for concatenation in the Unet.\n    :param encoder: the encoder\n    :param verbose: if set to True, the shape information of all blocks will be printed in the console\n    :return: a list of block numbers\n    \"\"\"\n    shapes = []\n    candidates = []\n    mbblock_nr = 0\n    while True:\n        try:\n            mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n            shape = int(mbblock.shape[1]), int(mbblock.shape[2])\n            if shape not in shapes:\n                shapes.append(shape)\n                candidates.append(mbblock_nr)\n            if verbose:\n                print('blocks_{}_output_shape: {}'.format(mbblock_nr, shape))\n            mbblock_nr += 1\n        except ValueError:\n            break\n    return candidates\n\n\ndef DoubleConv(filters, kernel_size, initializer='glorot_uniform'):\n\n    def layer(x):\n\n        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv2D(filters, kernel_size, padding='same', use_bias=False, kernel_initializer=initializer)(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n        return x\n\n    return layer\n\n\ndef UpSampling2D_block(filters, kernel_size=(3, 3), upsample_rate=(2, 2), interpolation='bilinear',\n                       initializer='glorot_uniform', skip=None):\n    def layer(input_tensor):\n\n        x = UpSampling2D(size=upsample_rate, interpolation=interpolation)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n\n        return x\n    return layer\n\n\ndef Conv2DTranspose_block(filters, kernel_size=(3, 3), transpose_kernel_size=(2, 2), upsample_rate=(2, 2),\n                          initializer='glorot_uniform', skip=None):\n    def layer(input_tensor):\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate, padding='same')(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = DoubleConv(filters, kernel_size, initializer=initializer)(x)\n\n        return x\n\n    return layer\n\n\n# noinspection PyTypeChecker\ndef _get_efficient_unet(encoder, out_channels=2, block_type='upsampling', concat_input=True):\n    MBConvBlocks = []\n\n    skip_candidates = get_blocknr_of_skip_candidates(encoder)\n\n    for mbblock_nr in skip_candidates:\n        mbblock = encoder.get_layer('blocks_{}_output_batch_norm'.format(mbblock_nr)).output\n        MBConvBlocks.append(mbblock)\n\n    # delete the last block since it won't be used in the process of concatenation\n    MBConvBlocks.pop()\n\n    input_ = encoder.input\n    head = encoder.get_layer('head_swish').output\n    blocks = [input_] + MBConvBlocks + [head]\n\n    if block_type == 'upsampling':\n        UpBlock = UpSampling2D_block\n    else:\n        UpBlock = Conv2DTranspose_block\n\n    o = blocks.pop()\n    o = UpBlock(512, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n    o = UpBlock(256, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n    o = UpBlock(128, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n    o = UpBlock(64, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n    if concat_input:\n        o = UpBlock(32, initializer=conv_kernel_initializer, skip=blocks.pop())(o)\n    else:\n        o = UpBlock(32, initializer=conv_kernel_initializer, skip=None)(o)\n    o = Conv2D(out_channels, (1, 1), padding='same', kernel_initializer=conv_kernel_initializer, activation=\"sigmoid\")(o)\n\n    model = models.Model(encoder.input, o)\n\n    return model\n\n\ndef get_efficient_unet_b0(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B0 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B0 model\n    \"\"\"\n    encoder = get_efficientnet_b0_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b1(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B1 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B1 model\n    \"\"\"\n    encoder = get_efficientnet_b1_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b2(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B2 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B2 model\n    \"\"\"\n    encoder = get_efficientnet_b2_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b3(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B3 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B3 model\n    \"\"\"\n    encoder = get_efficientnet_b3_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b4(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B4 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B4 model\n    \"\"\"\n    encoder = get_efficientnet_b4_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b5(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B5 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B5 model\n    \"\"\"\n    encoder = get_efficientnet_b5_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b6(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B6 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B6 model\n    \"\"\"\n    encoder = get_efficientnet_b6_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model\n\n\ndef get_efficient_unet_b7(input_shape, out_channels=2, pretrained=False, block_type='transpose', concat_input=True):\n    \"\"\"Get a Unet model with Efficient-B7 encoder\n    :param input_shape: shape of input (cannot have None element)\n    :param out_channels: the number of output channels\n    :param pretrained: True for ImageNet pretrained weights\n    :param block_type: \"upsampling\" to use UpSampling layer, otherwise use Conv2DTranspose layer\n    :param concat_input: if True, input image will be concatenated with the last conv layer\n    :return: an EfficientUnet_B7 model\n    \"\"\"\n    encoder = get_efficientnet_b7_encoder(input_shape, pretrained=pretrained)\n    model = _get_efficient_unet(encoder, out_channels, block_type=block_type, concat_input=concat_input)\n    return model","3aadae06":"EFNS = [get_efficient_unet_b0, get_efficient_unet_b1, get_efficient_unet_b2, get_efficient_unet_b3, \n        get_efficient_unet_b4,get_efficient_unet_b5, get_efficient_unet_b6, get_efficient_unet_b7]\ndef EfficientUnet(efun=0):\n    model = EFNS[efun]((512, 512, 3),out_channels=1, pretrained=False, block_type='transpose', concat_input=True)\n    return model    ","3d5ebc1e":"def dice_coef(y_true, y_pred):\n    y_true_f = tf.reshape(y_true,[-1])\n    y_pred_f = tf.reshape(y_pred,[-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)","f3efbf26":"lr = 1e-3\n\ndef get_model(efun=0):\n    model = EfficientUnet(efun)\n    opt = tf.keras.optimizers.Adam(lr)\n    metrics = [\"acc\"]\n    model.compile(loss=dice_coef_loss, optimizer=opt, metrics=metrics)\n    return model\n","13dc69c9":"modelset = []\nfor i in range(FOLDS):\n    model = get_model(EFUN)\n    model.load_weights(f\"..\/input\/hubmapeffnetb3ns\/fold-{i}.h5\")\n    modelset.append(model)\n","e3727194":"# https:\/\/www.kaggle.com\/leighplt\/pytorch-fcn-resnet50\ndef make_grid(shape, window=256, min_overlap=32):\n    \"\"\"\n        Return Array of size (N,4), where N - number of tiles,\n        2nd axis represente slices: x1,x2,y1,y2 \n    \"\"\"\n    x, y = shape\n    nx = x \/\/ (window - min_overlap) + 1\n    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n    x1[-1] = x - window\n    x2 = (x1 + window).clip(0, x)\n    ny = y \/\/ (window - min_overlap) + 1\n    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n    y1[-1] = y - window\n    y2 = (y1 + window).clip(0, y)\n    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n    \n    for i in range(nx):\n        for j in range(ny):\n            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n    return slices.reshape(nx*ny,4)\n\n@numba.njit()\ndef rle_numba(pixels):\n    size = len(pixels)\n    points = []\n    if pixels[0] == 1: points.append(0)\n    flag = True\n    for i in range(1, size):\n        if pixels[i] != pixels[i-1]:\n            if flag:\n                points.append(i+1)\n                flag = False\n            else:\n                points.append(i+1 - points[-1])\n                flag = True\n    if pixels[-1] == 1: points.append(size-points[-1]+1)    \n    return points\n\ndef rle_numba_encode(image):\n    pixels = image.flatten(order = 'F')\n    points = rle_numba(pixels)\n    return ' '.join(str(x) for x in points)","c0f0bbe2":"identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n\n# WINDOW is the size of the tile to be read by rasterio\nWINDOW = 512\n\n# Tiles will have some overlap\nMIN_OVERLAP = 32\n\n# Tiles will be resized to NEW_SIZE, which is the size of the image\n# on which, we have trained our model.\nNEW_SIZE = 512","fed84524":"p = pathlib.Path(os.path.join(data_path, 'hubmap-kidney-segmentation'))\nsubm = {}\n\nfor i, filename in tqdm(enumerate(p.glob('test\/*.tiff')), \n                        total = len(list(p.glob('test\/*.tiff')))):\n    \n    print(filename)\n    # save GPU quota\n    if len(list(p.glob('test\/*.tiff')))==5 and i==0:\n        dataset = rasterio.open(filename.as_posix(), transform = identity)\n        slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n        preds = np.zeros(dataset.shape, dtype=np.uint8)\n        subm[i] = {'id':filename.stem, 'predicted': rle_numba_encode(preds)}\n        break\n    dataset = rasterio.open(filename.as_posix(), transform = identity)\n    slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n    preds = np.zeros(dataset.shape, dtype=np.uint8)\n    \n    for (x1,x2,y1,y2) in slices:\n        image = dataset.read([1,2,3],\n                    window=Window.from_slices((x1,x2),(y1,y2)))\n        image = np.moveaxis(image, 0, -1)\n        \n        image = tf.image.convert_image_dtype(image, \n                                 tf.float32)\n        image = cv2.resize(image.numpy(), (NEW_SIZE, NEW_SIZE))\n        image = np.expand_dims(image, 0)\n        pred = 0\n        for k in range(FOLDS):\n            pred += np.squeeze(modelset[k].predict(image,verbose=0)) \/ FOLDS\n        \n        pred = cv2.resize(pred, (WINDOW, WINDOW))\n        preds[x1:x2,y1:y2] = (pred > 0.5).astype(np.uint8)\n            \n    subm[i] = {'id':filename.stem, 'predicted': rle_numba_encode(preds)}\n    del preds\n    gc.collect();","5b177846":"submission = pd.DataFrame.from_dict(subm, orient='index')\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","5cc10f86":"# EfficientUNet(using efficientnet as encoder)\n\nmost of codes from [zhoudaxia233\/EfficientUnet](https:\/\/github.com\/zhoudaxia233\/EfficientUnet).\n\nand see also [qubvel\/efficientnet](https:\/\/github.com\/qubvel\/efficientnet).","55a10cc9":"### other notebooks\n\n[Make Tfrecords of 512x512 or other tiles](https:\/\/www.kaggle.com\/itsuki9180\/make-tfrecords-of-512x512-or-other-tiles)\n\n[HUBMAP TPU Train Phase](https:\/\/www.kaggle.com\/itsuki9180\/hubmap-tpu-train-phase)\n\nHUBMAP GPU Inference Phase (This notebook)","1ff569b1":"# HUBMAP GPU Inference Phase","5a76b1a6":"some codes from \n\n[[HuBMAP] Keras-Pipeline (Training+Inference)](https:\/\/www.kaggle.com\/joshi98kishan\/hubmap-keras-pipeline-training-inference)\n\n[Pytorch FCN-Resnet50](https:\/\/www.kaggle.com\/leighplt\/pytorch-fcn-resnet50)","af524c6c":"# Inference","1fba5131":"# Build Model","96a97113":"### References\n\n\n[Getting Started: TPUs + Cassava Leaf Disease](https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease)\n\n[CutMix and MixUp on GPU\/TPU](https:\/\/www.kaggle.com\/cdeotte\/cutmix-and-mixup-on-gpu-tpu)\n\n[Getting started with 100+ flowers on TPU](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu)\n\n[Triple Stratified KFold with TFRecords](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords)"}}