{"cell_type":{"ccac924b":"code","a7fa715e":"code","cc2b23fa":"code","553f5a24":"code","ed7a6661":"code","f8a585cb":"code","67381350":"code","d05833ad":"code","a70951d8":"code","457c1cd4":"code","5060ba03":"code","cbd13b69":"code","b65d6a9f":"code","42630f2c":"code","bd089efd":"code","b491ccff":"code","c1a3bf42":"code","81c0ed10":"code","7f69f7a3":"code","1432f790":"code","1a7f928b":"code","788849ba":"code","be6c5a6b":"code","8dd5ba18":"code","f29cea87":"code","0288f90a":"code","386bf5e6":"code","9a2037a6":"code","036ef0c4":"markdown","af1a855e":"markdown","fcccae7e":"markdown","840cdb8c":"markdown","b395b7f5":"markdown","831d888f":"markdown","fc0ec337":"markdown","b0111a7b":"markdown","6e6bb7e1":"markdown","9c9d3083":"markdown","28b5919e":"markdown","ea493470":"markdown","105c065d":"markdown","52cfe3ef":"markdown","390aedd4":"markdown"},"source":{"ccac924b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nfrom PIL import Image\nfrom IPython.display import display\nimport cv2\nfrom PIL import ImageFile\nimport torchvision.transforms as transforms\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport glob\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a7fa715e":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)","cc2b23fa":"#First let's define our data augmentation rules or transforms\n\n# define transformations for train\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomHorizontalFlip(p=.30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\n# define transformations for test\n# for test we dont need much of augmentations other than converting to tensors and normalizing the pictures\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])","553f5a24":"train_data = datasets.CIFAR10('.\/cifar10\/', train=True,\n                              download=True, transform=train_transform)\ntest_data = datasets.CIFAR10('.\/cifar10\/', train=False,\n                             download=True, transform=test_transform)","ed7a6661":"# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 32\n# percentage of training set to use as validation\nvalid_size = 0.15","f8a585cb":"# Dividing the training dataset further for validation set\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]","67381350":"# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","d05833ad":"#  data loaders (combine dataset and sampler)\n#Dataloader provides an iterable over the specified dataset by combining a dataset with a sampler.\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n","a70951d8":"classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck']","457c1cd4":"#  function to un-normalize and display an image\ndef imshow(img):\n    img = img \/ 2 + 0.5  # unnormalize\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image","5060ba03":"#  one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","cbd13b69":"# let's look into details of normalized RGB\n\nrgb_img = np.squeeze(images[7])\nchannels = ['red channel', 'green channel', 'blue channel']\n\nfig = plt.figure(figsize = (36, 36)) \nfor idx in np.arange(rgb_img.shape[0]):\n    ax = fig.add_subplot(1, 3, idx + 1)\n    img = rgb_img[idx]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(channels[idx])\n    width, height = img.shape\n    thresh = img.max()\/2.5\n    for x in range(width):\n        for y in range(height):\n            val = round(img[x][y],2) if img[x][y] !=0 else 0\n            ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center', size=8,\n                    color='white' if img[x][y]<thresh else 'black')","b65d6a9f":"# check if CUDA is available\nuse_gpu = torch.cuda.is_available()\n\nif  use_gpu:\n    print('CUDA is available.  Training on CPU ...')\nelse:\n    print('CUDA is not available!  Training on GPU ...')","42630f2c":"# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # convolutional layer (sees 32x32x3 image tensor)\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        # convolutional layer (sees 16x16x16 tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # convolutional layer (sees 8x8x32 tensor)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # linear layer (64 * 4 * 4 -> 500)\n        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n        # linear layer (500 -> 10)\n        self.fc2 = nn.Linear(500, 10)\n        # dropout layer (p=0.25)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        # flatten image input\n        x = x.view(-1, 64 * 4 * 4)\n        # add dropout layer\n        x = self.dropout(x)\n        # add 1st hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add 2nd hidden layer, with relu activation function\n        x = self.fc2(x)\n        return x\n\n# create a complete CNN\nmodel = Net()\nprint(model)\n\n# move tensors to GPU if CUDA is available\nif use_gpu:\n    model.cuda()","bd089efd":"!pip install torchsummary","b491ccff":"#let's see the model\nfrom torchsummary import summary\nsummary(model, input_size=(3, 32, 32))","c1a3bf42":"#  loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","81c0ed10":"# number of epochs to train the model\nn_epochs = 30\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if use_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if use_gpu:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.sampler)\n    valid_loss = valid_loss\/len(valid_loader.sampler)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss","7f69f7a3":"model_transfer = models.googlenet(pretrained=True)","1432f790":"print(model_transfer)","1a7f928b":"for param in model_transfer.parameters():\n    param.requires_grad = False\n    \n# replace the last fully connected layer with a Linnear layer 133 output\nin_features = model_transfer.fc.in_features\nmodel_transfer.fc = nn.Linear(in_features, 10)\n\nif use_gpu:\n    model_transfer = model_transfer.cuda()","788849ba":"summary(model_transfer, input_size=(3, 32, 32))","be6c5a6b":"criterion = nn.CrossEntropyLoss()\nmodel_transfer_grad_paramaters = filter(lambda p: p.requires_grad, model_transfer.parameters())\noptimizer = torch.optim.Adam(model_transfer_grad_paramaters, lr=0.001)","8dd5ba18":"# number of epochs to train the model\nn_epochs = 10\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model_transfer.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if use_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model_transfer(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model_transfer.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if use_gpu:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model_transfer(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.sampler)\n    valid_loss = valid_loss\/len(valid_loader.sampler)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model_transfer.state_dict(), 'model_transfer_cifar.pt')\n        valid_loss_min = valid_loss","f29cea87":"# Load the saved model\nmodel.load_state_dict(torch.load('model_cifar.pt'))\n","0288f90a":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\nimages.numpy()\n\n# move model inputs to cuda, if GPU available\nif use_gpu:\n    images = images.cuda()\n\n# get sample outputs\noutput = model(images)\n# convert output probabilities to predicted class\n_, preds_tensor = torch.max(output, 1)\npreds = np.squeeze(preds_tensor.numpy()) if not use_gpu else np.squeeze(preds_tensor.cpu().numpy())\n\n# plot the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images.cpu()[idx])\n    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))","386bf5e6":"# Load the saved model\nmodel_transfer.load_state_dict(torch.load('.\/model_transfer_cifar.pt'))\n","9a2037a6":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\nimages.numpy()\n\n# move model inputs to cuda, if GPU available\nif use_gpu:\n    images = images.cuda()\n\n# get sample outputs\noutput = model_transfer(images)\n# convert output probabilities to predicted class\n_, preds_tensor = torch.max(output, 1)\npreds = np.squeeze(preds_tensor.numpy()) if not use_gpu else np.squeeze(preds_tensor.cpu().numpy())\n\n# plot the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images.cpu()[idx])\n    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))","036ef0c4":"# Step 2: Create a CNN to Classify (from Scratch)<br \/>\n","af1a855e":"TODO: \nGotta impove google net training later :3 ","fcccae7e":"as shown, we have flipped images because of the image transforms","840cdb8c":"First, let's test the CNN model from scratch","b395b7f5":"# Step 3: Create a CNN to Classify  (using Transfer Learning)<br \/>\n","831d888f":"# Step 1: Define dataloader and visualization\n\nSince the dataset is available on pytorch dataset class so we will be downloading it from there and store on our kaggle space. ","fc0ec337":"We will be using GoogLeNet for transfer learning. Read the [paper](https:\/\/arxiv.org\/abs\/1409.4842) to learn more about GoogLeNet. ","b0111a7b":"Have you wondered about why they use 42? Do you want to know about the reason behind 42? Look [Here ](https:\/\/en.wikipedia.org\/wiki\/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life.2C_the_Universe_and_Everything_.2842.29):p\n\n","6e6bb7e1":"Visualizations","9c9d3083":"Let's see the transfer learning model ","28b5919e":"# Introduction\nLet's break the notebook into separate steps. Feel free to navigate the notebook and comment if you have any suggestions.\n\nStep 0: Import Datasets <br \/>\nStep 1: Define dataloader and visualization<br \/>\nStep 2: Create a CNN to Classify (from Scratch)<br \/>\nStep 3: Create a CNN to Classify  (using Transfer Learning)<br \/>\nStep 4: Test","ea493470":"We will be using this function mostly everywhere to run our experiments deterministically. Random functions of Numpy and Pandas will behave deterministically after this. To learn more about Deterministic Neural Networks please check out [this notebook](https:\/\/www.kaggle.com\/bminixhofer\/deterministic-neural-networks-using-pytorch)\n","105c065d":"### Train the model","52cfe3ef":"# Step 0: Imports\n\nAt first we need to import the libraries. It is considered as standard imports for pytorch.\n","390aedd4":"# Step 4 Test"}}