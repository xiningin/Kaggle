{"cell_type":{"8d616449":"code","f33c07d0":"code","22507baf":"code","89f0d475":"code","701a3c27":"code","44bb2a4c":"code","9f725aec":"code","8c6f78f9":"code","6f5ee547":"code","47269baf":"code","9d7b5199":"code","c2d869b7":"code","bdb833ff":"code","49294d82":"code","a290c023":"code","a58bfb69":"code","3d1a0f72":"code","8e3cf90a":"code","e472c8db":"code","c2933cf5":"code","f83d43fa":"code","d78bd2fd":"code","66d6e578":"code","642de29c":"code","5e5a774e":"code","400ee8fc":"code","134882a8":"code","85a92033":"code","bd798ffb":"code","266e05cf":"code","a20cae19":"code","db94cdf4":"code","01039437":"code","e861eeee":"code","198a85ca":"code","d7d1f8f6":"code","44a9b48d":"code","869fe63b":"code","fcf6435b":"code","47ca3727":"code","02dc69e5":"code","c41fafa2":"code","e7454afe":"code","b4542a8b":"markdown","10bbdb97":"markdown","a6d3ee1d":"markdown","076dbf45":"markdown","6fb6a730":"markdown","cc36643b":"markdown","0dc99e59":"markdown","20cae759":"markdown","648cb8e1":"markdown","914d5815":"markdown","589c4c4d":"markdown","6b3b655f":"markdown","8eb2ffe6":"markdown","3dfdc340":"markdown","17e545e1":"markdown","345ad3bd":"markdown","325fa489":"markdown","3d96a355":"markdown","ae736bb2":"markdown","acb202ef":"markdown"},"source":{"8d616449":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f33c07d0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB as gnb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import tree\nfrom os import system","22507baf":"data = pd.read_csv('\/kaggle\/input\/parkinsons.txt')","89f0d475":"data.head()","701a3c27":"data.shape","44bb2a4c":"#Moving status column to the last\ndf1=data.pop('status') \ndata['status'] = df1","9f725aec":"\ndata.describe().transpose()","8c6f78f9":"# There are 147 people affected with PD\ndata.status.value_counts()","6f5ee547":"fig, ax = plt.subplots(1,3,figsize=(16,10)) \nsns.boxplot(x='spread1',data=data, ax=ax[0],orient='v') \nsns.boxplot(x='spread2',data=data, ax=ax[1],orient='v')\nsns.boxplot(x='PPE',data=data,ax=ax[2],orient='v')","47269baf":"# dropping name column as this column is not much significant \ndata = data.drop('name',axis=1)","9d7b5199":"fig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.distplot(data['MDVP:Flo(Hz)'],ax=ax[0]) \nsns.distplot(data['MDVP:Fo(Hz)'],ax=ax[1]) \nsns.distplot(data['MDVP:Fhi(Hz)'],ax=ax[2])","c2d869b7":"fig, ax = plt.subplots(1,2,figsize=(16,8)) \nsns.distplot(data['NHR'],ax=ax[0]) \nsns.distplot(data['HNR'],ax=ax[1])","bdb833ff":"fig, ax = plt.subplots(2,3,figsize=(16,8)) \nsns.distplot(data['MDVP:Shimmer'],ax=ax[0,0]) \nsns.distplot(data['MDVP:Shimmer(dB)'],ax=ax[0,1]) \nsns.distplot(data['Shimmer:APQ3'],ax=ax[0,2]) \nsns.distplot(data['Shimmer:APQ5'],ax=ax[1,0]) \nsns.distplot(data['MDVP:APQ'],ax=ax[1,1]) \nsns.distplot(data['Shimmer:DDA'],ax=ax[1,2])","49294d82":"sns.distplot( data[data.status == 0]['spread1'], color = 'r')\nsns.distplot( data[data.status == 1]['spread1'], color = 'g')","a290c023":"Spread1 is normally distributed between person who have PD and who is normal. People who have spread1 between - 8.5 and -7.5 are more and they are normal. People whose spread1 is between -6.5 and -5 are having PD","a58bfb69":"fig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y='NHR',data=data,ax=ax[0])\nsns.boxplot(x='status',y='HNR',data=data,ax=ax[1])","3d1a0f72":"fig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y='MDVP:Flo(Hz)',data=data,palette=\"Set1\",ax=ax[0])\nsns.boxplot(x='status',y='MDVP:Fo(Hz)',data=data,palette=\"Set1\",ax=ax[1])","8e3cf90a":"# For categorical predictors\ncols = [\"MDVP:Jitter(%)\",\"MDVP:Jitter(%)\",\"MDVP:RAP\",\"MDVP:PPQ\",\"Jitter:DDP\"]\nfig, axs = plt.subplots(ncols = 5,figsize=(16,8))\nfig.tight_layout()\nfor i in range(0,len(cols)):\n    sns.boxplot(x='status',y=cols[i],data=data, ax = axs[i])","e472c8db":"corr = data.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 3.5})\nplt.figure(figsize=(18,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","c2933cf5":"from sklearn import metrics","f83d43fa":"X = data.drop(\"status\",axis=1)\nY = data[\"status\"]","d78bd2fd":"# Splitting Data into 70% Training data and 30% Testing Data:\nX_train, X_test, y_train,  y_test = train_test_split(X, Y,train_size=0.7, test_size=0.3, random_state=42)\nprint(len(X_train)),print(len(X_test))","66d6e578":"# Applying decision tree model\ndt_model = DecisionTreeClassifier(criterion='entropy',max_depth=6,random_state=100,min_samples_leaf=5)","642de29c":"dt_model.fit(X_train, y_train)","5e5a774e":"DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=5,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=100, splitter='best')","400ee8fc":"dt_model.score(X_test , y_test) ","134882a8":"y_pred = dt_model.predict(X_test)","85a92033":"confusion_matrix(y_test,y_pred)","bd798ffb":"#Count mis-classified one\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))","266e05cf":"from IPython.display import Image\nfrom sklearn import tree\nfrom os import system\n\ntrain_char_label = ['No', 'Yes']\npd_tree_regularized = open('pd_tree_regularized.dot','w')\ndot_data = tree.export_graphviz(dt_model, out_file= pd_tree_regularized , feature_names = list(X_train), class_names = list(train_char_label))\n\npd_tree_regularized.close()\n\nprint (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))","a20cae19":"# You can also copy the script in the .dot file and paste it at http:\/\/webgraphviz.com\/ to get tree view\n# or create a .png as below\nsystem(\"dot -Tpng pd_tree_regularized.dot -o pd_tree_regularized.png\")\nImage(\"pd_tree_regularized.png\")","db94cdf4":"k_model = KNeighborsClassifier(n_neighbors=5)\nk_model.fit(X_train, y_train)\nk_model.score(X_test,y_test)","01039437":"y_pred = k_model.predict(X_test)","e861eeee":"count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples in KNN: {}'.format(count_misclassified))","198a85ca":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, y_train)\ny_pred = rfcl.predict(X_test)\nrfcl.score(X_test , y_test)","d7d1f8f6":"count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples in Random Forest: {}'.format(count_misclassified))","44a9b48d":"feature_imp = pd.Series(rfcl.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","869fe63b":"from sklearn.ensemble import BaggingClassifier\nbgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=50, max_samples=.7)\nbgcl = bgcl.fit(X_train, y_train)\ny_pred = bgcl.predict(X_test)\nbgcl.score(X_test , y_test)","fcf6435b":"count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples in Bagging: {}'.format(count_misclassified))","47ca3727":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 50)\nabcl = abcl.fit(X_train,y_train)\ny_pred = abcl.predict(X_test)\nabcl.score(X_test , y_test)","02dc69e5":"count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples in Ada Boosting: {}'.format(count_misclassified))","c41fafa2":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\ngbcl = gbcl.fit(X_train,y_train)\ny_pred = gbcl.predict(X_test)\ngbcl.score(X_test , y_test)","e7454afe":"count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples in Gradient Boosting: {}'.format(count_misclassified))","b4542a8b":"## Bi -Variate Analysis","10bbdb97":"### Random Forest Classifier","a6d3ee1d":"When we look the relationship between status and MDVP:Fo(Hz) we can see the median value is around 199 Hz for people who are normal. For people who are affected with Parkinsons the median value comes around 145 Hz","076dbf45":"## Parkinsons Disease Data Set\n### Dataset information:\nThis dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (\"name\" column). The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD.\n\nThe data is in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column.\n\n### Attribute Information:\n\nMatrix column entries (attributes): \n* name - ASCII subject name and recording number \n* MDVP:Fo(Hz) - Average vocal fundamental frequency \n* MDVP:Fhi(Hz) - Maximum vocal fundamental frequency \n* MDVP:Flo(Hz) - Minimum vocal fundamental frequency * MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency \n* MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude \n* NHR,HNR - Two measures of ratio of noise to tonal components in the voice \n* status - Health status of the subject (one) - Parkinson's, (zero) - healthy \n* RPDE,D2 - Two nonlinear dynamical complexity measures \n* DFA - Signal fractal scaling exponent * spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation","6fb6a730":"For all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed","cc36643b":"The measures of vocal fundamental frequency are shown above. There is a positive skewness for minimum vocal fundemental frequency with more high values between 75Hz and 125Hhz. The average vocal frequency is almost normally distributed with more values ranging 115Hz and 130Hz. The high vocal frequency does not have any skewness, but some range of values are at the right most tail","0dc99e59":"People who have PD(status equal to one) have higher levels of Noise to Harmonic ratio. Also, looking into the HNR ratio people who have PD have lower levels in the same.","20cae759":"The measure of tonal component of frequency is shown above. The value NHR is right skewed for there are so many observations in the area, but they seem to be with very minimal values. The maximum number of observations is between 0 and 0.04.\nThe value HNR looks like normally distributed, but in a first look there seems to be a slight negative skewness","648cb8e1":"### Bagging","914d5815":"* MDVP:Jitter(%) has a very high correlation with MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP\n* MDVP:Shimmer has a very correlation with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA this may be because they are related to each other. This may be because multi-dimensinal voice programs analysis is closely related with these variables\n* The target variable status has a weak positive corelation with spread1","589c4c4d":"### AdaBoosting","6b3b655f":"## Correlation comparision with heat map","8eb2ffe6":"People who are suffering for PD tend to have higher jitter %. It seems if the values goes above 0.15 we can confirm the patient is having PD. The variation of fundamental frequency is in a low range for people who is normal.","3dfdc340":"The above figure shows the box plot of the frequency variation. All the three variations have outliers.\nGenerally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them","17e545e1":"### K Nearest Neighbour","345ad3bd":"## Univariate Analysis","325fa489":"Of all the above ones Random Forest algorithm gave the maximum accuracy","3d96a355":"### Gradient Boosting","ae736bb2":"## Applying Models","acb202ef":"### Decision Tree\n\nDecision trees can be used to predict both continuous and discrete values i.e. they work well for both regression and classification tasks."}}