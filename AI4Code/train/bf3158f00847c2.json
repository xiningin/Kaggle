{"cell_type":{"71c38332":"code","f669e97a":"code","a6891e1c":"code","87f26119":"code","eb5c316d":"code","bcc081bf":"code","28492858":"code","d4838c32":"code","3144aaad":"code","deee0c09":"code","d64cb19c":"code","57aca7e0":"code","00ac1818":"code","ca9fa850":"code","8f0b17a9":"code","7f1de913":"code","548616ab":"code","80e4e6fa":"code","9caa173f":"code","61ccae53":"code","be3910fb":"code","b6041ab7":"code","1fc50f7f":"code","3ac84022":"code","c94dd8d4":"code","ae738968":"code","58c1fec7":"code","1970f2ab":"code","67696628":"code","add36474":"code","d4a8fe4e":"code","54ff578e":"code","b2af8c9f":"code","2daf872b":"code","43258f3c":"code","1c9d6af1":"code","c13c734d":"code","75dcc34e":"code","1c95822c":"code","da82bbac":"code","3c435a30":"code","37465a0d":"code","dd3b8a74":"code","c6354959":"code","fb9cc42a":"code","411f6306":"code","14f0926e":"code","bf0d7f1a":"code","568b6379":"code","1f30eb72":"code","21f7650a":"code","22e706cd":"code","6931c76a":"code","f2828dba":"code","78b162f8":"code","1a159008":"code","60835683":"code","c40ae4a0":"code","0f167f2a":"code","366d3f06":"markdown","5c3cae9f":"markdown","92261787":"markdown","dc4a22e2":"markdown","8d82fcef":"markdown","bfb74309":"markdown","be89f777":"markdown","d3d3293f":"markdown","4b61c5a2":"markdown","9328c25c":"markdown","59e5e65d":"markdown","a3462cfd":"markdown","b2d4998d":"markdown","603c560d":"markdown","924b9382":"markdown","dad1b852":"markdown","512dc176":"markdown","022b4308":"markdown","1810549e":"markdown","1d3888e1":"markdown","a23ebe36":"markdown","0b9d2343":"markdown","2f09b60d":"markdown","db616539":"markdown","151b55a2":"markdown","766823c4":"markdown","950310d5":"markdown","a8088b47":"markdown","59adfb0f":"markdown","b2f38c99":"markdown","302d57ad":"markdown","47037d4f":"markdown","b38fe573":"markdown","fa336a01":"markdown","ab5b32a0":"markdown","b0b1b35b":"markdown","95ad4dbe":"markdown","ae26062f":"markdown","44404875":"markdown","0bd9560a":"markdown","e9479f6b":"markdown","7f908c2f":"markdown","b3425ced":"markdown","342c504e":"markdown","6612dde6":"markdown","514f2326":"markdown","14856b81":"markdown","7aa40f6b":"markdown","14af45dd":"markdown","d09caecb":"markdown","abcc5f34":"markdown","923f02db":"markdown","41be5785":"markdown","eb066ff2":"markdown","afb5325e":"markdown","ba984ce7":"markdown"},"source":{"71c38332":"from tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)","f669e97a":"train_data[0]","a6891e1c":"train_labels[0]","87f26119":"max([max(sequence) for sequence in train_data])","eb5c316d":"word_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()])\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])","bcc081bf":"import numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","28492858":"x_train[0]","d4838c32":"y_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")","3144aaad":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])","deee0c09":"model.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])","d64cb19c":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","57aca7e0":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","00ac1818":"history_dict = history.history\nhistory_dict.keys()","ca9fa850":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","8f0b17a9":"plt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","7f1de913":"model = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)","548616ab":"results","80e4e6fa":"model.predict(x_test)","9caa173f":"from tensorflow.keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)","61ccae53":"len(train_data)","be3910fb":"len(test_data)","b6041ab7":"train_data[10]","1fc50f7f":"word_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n    train_data[0]])","3ac84022":"train_labels[10]","c94dd8d4":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","ae738968":"def to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ny_train = to_one_hot(train_labels)\ny_test = to_one_hot(test_labels)","58c1fec7":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels)","1970f2ab":"model = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])","67696628":"model.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])","add36474":"x_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:]","d4a8fe4e":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","54ff578e":"loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","b2af8c9f":"plt.clf()\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","2daf872b":"model = keras.Sequential([\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train,\n          y_train,\n          epochs=9,\n          batch_size=512)\nresults = model.evaluate(x_test, y_test)","43258f3c":"results","1c9d6af1":"import copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nhits_array.mean()","c13c734d":"predictions = model.predict(x_test)","75dcc34e":"predictions[0].shape","1c95822c":"np.sum(predictions[0])","da82bbac":"np.argmax(predictions[0])","3c435a30":"y_train = np.array(train_labels)\ny_test = np.array(test_labels)","37465a0d":"model.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])","dd3b8a74":"model = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(partial_x_train,\n          partial_y_train,\n          epochs=20,\n          batch_size=128,\n          validation_data=(x_val, y_val))","c6354959":"from tensorflow.keras.datasets import boston_housing\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()","fb9cc42a":"train_data.shape","411f6306":"test_data.shape","14f0926e":"train_targets","bf0d7f1a":"mean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data \/= std\ntest_data -= mean\ntest_data \/= std","568b6379":"def build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n    return model","1f30eb72":"k = 4\nnum_val_samples = len(train_data) \/\/ k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=16, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)","21f7650a":"all_scores","22e706cd":"np.mean(all_scores)","6931c76a":"num_epochs = 500\nall_mae_histories = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=16, verbose=0)\n    mae_history = history.history[\"val_mae\"]\n    all_mae_histories.append(mae_history)","f2828dba":"average_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","78b162f8":"plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","1a159008":"truncated_mae_history = average_mae_history[10:]\nplt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","60835683":"model = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=130, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","c40ae4a0":"test_mae_score","0f167f2a":"predictions = model.predict(test_data)\npredictions[0]","366d3f06":"**Setting aside a validation set**","5c3cae9f":"### Further experiments","92261787":"### Preparing the data","dc4a22e2":"**Training the model**","8d82fcef":"**Saving the validation logs at each fold**","bfb74309":"### Building your model","be89f777":"### Generating predictions on new data","d3d3293f":"### Validating your approach using K-fold validation","4b61c5a2":"### A different way to handle the labels and the loss","9328c25c":"### Wrapping up","59e5e65d":"**Model definition**","a3462cfd":"**Plotting the training and validation accuracy**","b2d4998d":"**Model definition**","603c560d":"# Getting started with neural networks: Classification and regression","924b9382":"**Training your model**","dad1b852":"**Plotting the training and validation accuracy**","512dc176":"**Compiling the model**","022b4308":"**Plotting the training and validation loss**","1810549e":"### The importance of having sufficiently large intermediate layers","1d3888e1":"**Normalizing the data**","a23ebe36":"**Training the final model**","0b9d2343":"### Generating predictions on new data","2f09b60d":"**Plotting the training and validation loss**","db616539":"### Validating your approach","151b55a2":"## Classifying newswires: A multiclass classification example","766823c4":"### Validating your approach","950310d5":"**Decoding reviews back to text**","a8088b47":"**Loading the Boston housing dataset**","59adfb0f":"### Preparing the data","b2f38c99":"**Loading the Reuters dataset**","302d57ad":"### Further experiments","47037d4f":"**Plotting validation scores, excluding the first 10 data points**","b38fe573":"## Classifying movie reviews: A binary classification example","fa336a01":"### Using a trained model to generate predictions on new data","ab5b32a0":"This is a companion notebook for the book [Deep Learning with Python, Second Edition](https:\/\/www.manning.com\/books\/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n\n**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n\nThis notebook was generated for TensorFlow 2.6.","b0b1b35b":"### The Reuters dataset","95ad4dbe":"**Building the history of successive mean K-fold validation scores**","ae26062f":"### The IMDB dataset","44404875":"**Encoding the labels**","0bd9560a":"**Retraining a model from scratch**","e9479f6b":"**Retraining a model from scratch**","7f908c2f":"**K-fold validation**","b3425ced":"### Building your model","342c504e":"**Plotting validation scores**","6612dde6":"**A model with an information bottleneck**","514f2326":"## Predicting house prices: A regression example","14856b81":"**Encoding the integer sequences via multi-hot encoding**","7aa40f6b":"**Compiling the model**","14af45dd":"**Decoding newswires back to text**","d09caecb":"### The Boston Housing Price dataset","abcc5f34":"**Loading the IMDB dataset**","923f02db":"**Model definition**","41be5785":"### Preparing the data","eb066ff2":"### Building your model","afb5325e":"**Encoding the input data**","ba984ce7":"**Setting aside a validation set**"}}