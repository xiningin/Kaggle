{"cell_type":{"a9551a73":"code","0b89648f":"code","8f58cfcd":"code","13287d2a":"code","712f6f0d":"code","8f5b5779":"code","43110565":"code","236e864b":"code","b6f5c468":"code","824cddd0":"code","6ad56221":"code","cb1de9c5":"code","c2bd55a6":"code","8db433c5":"code","b9cc94a2":"code","c5366958":"code","c43165b5":"code","76fa86c1":"code","1e2a6b81":"code","0850303e":"code","6f94e4df":"code","50677c84":"code","149ba019":"code","3193d376":"code","2fe526ec":"code","0da2a61b":"code","b6e3c8f7":"code","defc0572":"code","f8c87b8c":"code","ab6cc71e":"code","0485c475":"code","4c654b64":"code","b13aef74":"code","1e1887d2":"code","a761a7c0":"code","0a82f8f5":"code","2bb7b985":"code","0375afac":"code","04ab84dc":"code","30b8c29d":"code","8cfd7233":"code","32ed18df":"code","5baa985e":"code","beb52dad":"code","c908de0a":"code","c22bada6":"code","d80c3fd5":"code","81610c35":"code","00ffb0be":"code","d3623051":"code","6149c456":"code","9e0f309f":"code","899c1a10":"code","35e295fd":"code","3fe96cfd":"code","e682ff48":"code","9ace511c":"code","13d87943":"code","8fd3a516":"code","f419a26a":"code","89d94b4a":"code","a286822b":"code","f6b750f4":"markdown","954825a8":"markdown","38441f33":"markdown","28972f78":"markdown","7b53f698":"markdown","6df1640a":"markdown","303ea3a4":"markdown","6fea6ebd":"markdown","7e38242b":"markdown","b3101830":"markdown","fa2f25b8":"markdown","c3d563db":"markdown","b8316fa6":"markdown","d5f44cb5":"markdown","96232073":"markdown","802f6a61":"markdown","0c457ec5":"markdown","2f00d632":"markdown","f60a1d6a":"markdown","9113126f":"markdown","b4ac7c0d":"markdown","76ec4dbd":"markdown","9fd9b254":"markdown","e1584ca1":"markdown","50bdf2dd":"markdown","b9efed23":"markdown","d3249933":"markdown","f57e09aa":"markdown","94eb04a9":"markdown","abe0a95c":"markdown","4593ab62":"markdown"},"source":{"a9551a73":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport numpy as np\n\nimport re\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\ndf_train = pd.read_csv('..\/input\/Train_data.csv')\ndf_test = pd.read_csv('..\/input\/test_data.csv')\ndf_test = df_test.drop('Unnamed: 0',axis=1)","0b89648f":"df_train.head()","8f58cfcd":"df_test.head()","13287d2a":"X_train = df_train.drop('xAttack', axis=1)\nY_train = df_train.loc[:,['xAttack']]\nX_test = df_test.drop('xAttack', axis=1)\nY_test = df_test.loc[:,['xAttack']]","712f6f0d":"from sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder","8f5b5779":"le = preprocessing.LabelEncoder()\nenc = OneHotEncoder()\nlb = preprocessing.LabelBinarizer()","43110565":"X_train['protocol_type'] = le.fit_transform(X_train['protocol_type'])\n# enc.fit_transform(X_train['protocol_type'])\n\nX_test['protocol_type'] = le.fit_transform(X_test['protocol_type'])\n# enc.fit_transform(X_test['protocol_type'])\n\nX_train.head()","236e864b":"Y_train['xAttack'] = le.fit_transform(Y_train['xAttack'])\nlb.fit_transform(Y_train['xAttack'])\n\nY_test['xAttack'] = le.fit_transform(Y_test['xAttack'])\nlb.fit_transform(Y_test['xAttack'])\n\nY_train.describe()","b6f5c468":"#except continuous feature\ncon_list = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'su_attempted', 'is_host_login', 'is_guest_login']\ncon_train = X_train.drop(con_list, axis=1)\n\n#drop n smallest std features\nstdtrain = con_train.std(axis=0)\nstd_X_train = stdtrain.to_frame()\nstd_X_train.nsmallest(10, columns=0).head(10)","824cddd0":"X_train = X_train.drop(['num_outbound_cmds'], axis=1)\nX_test = X_test.drop(['num_outbound_cmds'], axis=1)\n\ndf_train = pd.concat([X_train, Y_train], axis=1)\ndf_train.head()\n\nX_train.head()","6ad56221":"stdrop_list = ['urgent', 'num_shells', 'root_shell',\n        'num_failed_logins', 'num_access_files', 'dst_host_srv_diff_host_rate',\n        'diff_srv_rate', 'dst_host_diff_srv_rate', 'wrong_fragment']\n\nX_test_stdrop = X_test.drop(stdrop_list, axis=1)\n\nX_train_stdrop = X_train.drop(stdrop_list, axis=1)\n\ndf_train_stdrop = pd.concat([X_train_stdrop, Y_train], axis=1)\n\ndf_train_stdrop.head()","cb1de9c5":"from sklearn import linear_model","c2bd55a6":"LR = linear_model.LinearRegression()","8db433c5":"LR.fit(X_train, Y_train)","b9cc94a2":"lr_score = LR.score(X_test, Y_test)\nprint('Linear regression processing ,,,')\nprint('Linear regression Score: %.2f %%' % lr_score)","c5366958":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier","c43165b5":"AB = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, learning_rate=1.0)\nRF = RandomForestClassifier(n_estimators=10, criterion='entropy', max_features='auto', bootstrap=True)\nET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False)\nGB = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, max_features='auto')","76fa86c1":"y_train = Y_train['xAttack'].ravel()\nx_train = X_train.values\nx_test = X_test.values","1e2a6b81":"AB.fit(X_train, Y_train)","0850303e":"AB_feature = AB.feature_importances_\nAB_feature\n\nab_score = AB.score(X_test, Y_test)\n\nprint('AdaBoostClassifier processing ,,,')\nprint('AdaBoostClassifier Score: %.3f %%' % ab_score)","6f94e4df":"RF.fit(X_train, Y_train)","50677c84":"RF_feature = RF.feature_importances_\nRF_feature\n\nrf_score = RF.score(X_test, Y_test)\n\nprint('RandomForestClassifier processing ,,,')\nprint('RandomForestClassifier Score: %.3f %%' % rf_score)","149ba019":"ET.fit(X_train, Y_train)","3193d376":"ET_feature = ET.feature_importances_\nET_feature\n\net_score = ET.score(X_test, Y_test)\n\nprint('ExtraTreesClassifier processing ,,,')\nprint('ExtraTreeClassifier: %.3f %%' % et_score)","2fe526ec":"GB.fit(X_train, Y_train)","0da2a61b":"GB_feature = GB.feature_importances_\nGB_feature\n\ngb_score = GB.score(X_test, Y_test)\n\nprint('GradientBoostingClassifier processing ,,,')\nprint('GradientBoostingClassifier Score: %.3f %%' % gb_score)","b6e3c8f7":"cols = X_train.columns.values\n\nfeature_df = pd.DataFrame({'features': cols,\n                           'AdaBoost' : AB_feature,\n                           'RandomForest' : RF_feature,\n                           'ExtraTree' : ET_feature,\n                           'GradientBoost' : GB_feature\n                          })\nfeature_df.head(8)","defc0572":"from matplotlib.ticker import MaxNLocator\nfrom collections import namedtuple\n\ngraph = feature_df.plot.bar(figsize = (18, 10), title = 'Feature distribution', grid=True, legend=True, fontsize = 15, \n                            xticks=feature_df.index)\ngraph.set_xticklabels(feature_df.features, rotation = 80)","f8c87b8c":"a_f = feature_df.nlargest(12, 'AdaBoost')\ne_f = feature_df.nlargest(12, 'ExtraTree')\ng_f = feature_df.nlargest(12, 'GradientBoost')\nr_f = feature_df.nlargest(12, 'RandomForest')","ab6cc71e":"result = pd.concat([a_f, e_f, g_f, r_f])\nresult = result.drop_duplicates() # delete duplicate feature\nresult","0485c475":"selected_features = result['features'].values.tolist()\nselected_features","4c654b64":"AB.fit(X_train_stdrop, Y_train)","b13aef74":"ab2_score = AB.score(X_test_stdrop, Y_test)\n\nprint('AdaBoostClassifier_stdrop processing ,,,')\nprint('AdaBoostClasifier Score: %.3f %%' % ab2_score)","1e1887d2":"RF.fit(X_train_stdrop, Y_train)","a761a7c0":"rf2_score = RF.score(X_test_stdrop, Y_test)\n\nprint('RandomForestClassifier_stdrop processing ,,,')\nprint('RandomForestClassifier Score: %.3f %%' % rf2_score)","0a82f8f5":"ET.fit(X_train_stdrop, Y_train)","2bb7b985":"et2_score = ET.score(X_test_stdrop, Y_test)\n\nprint('ExtraTreesClassifier_stdrop processing ,,,')\nprint('ExtraTreesClassifier Score: %.3f %%' % et2_score)","0375afac":"GB.fit(X_train_stdrop, Y_train)","04ab84dc":"gb2_score = GB.score(X_test_stdrop, Y_test)\n\nprint('GradientBoostingClassifier_stdrop processing ,,,')\nprint('GradientBoostingClassifier Score: %.2f %%' % gb2_score)","30b8c29d":"X_train_ens = X_train[selected_features]\nX_train_ens.head()\n\nX_test_ens = X_test[selected_features]\nX_test_ens.head()","8cfd7233":"sample = X_train_ens[:10000]\n\ncolormap = plt.cm.viridis\nplt.figure(figsize=(20, 20))\nsns.heatmap(sample.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, annot=True)","32ed18df":"selected2 = ['flag', 'dst_host_serror_rate', 'serror_rate']\nX_train_cordrop = X_train_ens.drop(selected2, axis=1)\nX_train_cordrop.describe()\n\nX_test_cordrop = X_test_ens.drop(selected2, axis=1)\nX_test_cordrop.describe()","5baa985e":"AB.fit(X_train_cordrop, Y_train)","beb52dad":"ab_finalscore = AB.score(X_test_cordrop, Y_test)\n\nprint('AdaBoostClassifier_final processing ,,,')\nprint('AdaBoostClassifier_final Score: %.3f %%' % ab_finalscore)","c908de0a":"RF.fit(X_train_cordrop, Y_train)","c22bada6":"rf_finalscore = RF.score(X_test_cordrop, Y_test)\n\nprint('RandomForestClassifier_final processing ,,,')\nprint('RandomForestClassifier_final Score: %.3f %%' % rf_finalscore)","d80c3fd5":"ET.fit(X_train_cordrop, Y_train)","81610c35":"et_finalscore = ET.score(X_test_cordrop, Y_test)\n\nprint('ExtraTreesClassifier_final processing ,,,')\nprint('ExtraTreesClassifier_final Score: %.3f %%' % et_finalscore)","00ffb0be":"GB.fit(X_train_cordrop, Y_train)","d3623051":"gb_finalscore = GB.score(X_test_cordrop, Y_test)\n\nprint('GradientBoostClassifier_final processing ,,,')\nprint('GradientBoostClassifier_final Score: %.3f %%' % gb_finalscore)","6149c456":"LR.fit(X_train_cordrop, Y_train)","9e0f309f":"lr_finalscore = LR.score(X_test_cordrop, Y_test)\n\nprint('LinearRegression_final processing ,,,')\nprint('LinearRegression_final Score: %.3f %%' % lr_finalscore)","899c1a10":"from sklearn.neural_network import MLPClassifier","35e295fd":"MLP = MLPClassifier(hidden_layer_sizes=(1000, 300, 300), solver='adam', shuffle=False, tol = 0.0001)","3fe96cfd":"MLP.fit(X_train_cordrop, Y_train)","e682ff48":"mlp_finalscore = MLP.score(X_test_cordrop, Y_test)\n\nprint('MLP_final processing ,,,')\nprint('MLP_final Score: %.3f %%' % mlp_finalscore)","9ace511c":"first_model = {'Model': ['Linear Regression', 'Adaboost', 'RandomForest', 'ExtraTrees', 'GradientBoost'],\n               'accuracy' : [lr_score, ab_score, rf_score, et_score, gb_score]}\n\nresult_df = pd.DataFrame(data = first_model)\nresult_df","13d87943":"r1 = result_df.plot(x='Model', y='accuracy', kind='bar', figsize=(8, 8), grid=True, title='FIRST MODEL ACCURACY', colormap=plt.cm.viridis,\n               sort_columns=True)\nr1.set_xticklabels(result_df.Model, rotation = 45)","8fd3a516":"second_model = {'Model': ['Adaboost', 'RandomForest', 'ExtraTrees', 'GradientBoost'],\n               'accuracy' : [ab2_score, rf2_score, et2_score, gb2_score]}\n\nresult_df = pd.DataFrame(data = second_model)\nresult_df","f419a26a":"r2 = result_df.plot(x='Model', y='accuracy', kind='bar', figsize=(8, 8), grid=True, title='SECOND MODEL ACCURACY', colormap=plt.cm.viridis,\n               sort_columns=True)\nr2.set_xticklabels(result_df.Model, rotation = 45)","89d94b4a":"final_model = {'Model': ['Linear Regression', 'Adaboost', 'RandomForest', 'ExtraTrees', 'GradientBoost', 'MLP'],\n               'accuracy' : [lr_finalscore, ab_finalscore, rf_finalscore, et_finalscore, gb_finalscore, mlp_finalscore]}\n\nresult_df = pd.DataFrame(data = final_model)\nresult_df","a286822b":"r3 = result_df.plot(x='Model', y='accuracy', kind='bar', figsize=(8, 8), grid=True, title='FINAL MODEL ACCURACY', colormap=plt.cm.viridis,\n               sort_columns=True)\nr3.set_xticklabels(result_df.Model, rotation = 45)","f6b750f4":"The linear regression yields only 33% probability.","954825a8":"- Linear regression","38441f33":"As a result, feature selection and extraction did not result in high probability. I have seen 1-2% increase in accuracy, but I think the feature will be reduced and it will be able to operate a little faster and will prevent overfitting when new data comes in.\n\nComparing the score of each model\n\n- first models","28972f78":"- X OneHotEncoding","7b53f698":"### 1. Standard deviation\nWe have applied a method to exclude features with small standard deviation (small deviation). However, when the feature type is discrete, the deviation is small.","6df1640a":"#### Extract twelve features from each Ensemble model","303ea3a4":"#### Delete Duplicates","6fea6ebd":"Graphs showing the influence of features","7e38242b":"#Preprocessing and one hot encoding, X is onehotencoder, Y is LabelBinarizer****","b3101830":"#### Only features obtained through ensemble","fa2f25b8":"- Y LabelBinarizer","c3d563db":"### 3. Correlation\nFeatures that have high correlation among multiple features (redundant features) \nare merged or deleted. This is because if there is a large correlation between these features, \nthere is no need to increase the number of features.","b8316fa6":"#Import training dataset, test dataset into Pandas","d5f44cb5":"#### The above graph analysis shows that the dependency is high in the following features","96232073":"#### Below are the results of training with the exception of the features with small standard deviations.","802f6a61":"## 1) Feature selection\n    If there are a large number of features, the complexity increases with the number of samples, so the probability of overfitting is high. Therefore, the Irrelevant feature and the Redundant Feature will be removed to see the difference from the original feature.","0c457ec5":"Check the feature importances to see how accurate the basic features are.","2f00d632":"## FASTEST AND ACCURATE MODEL - ExtraTrees of the final model (76.4%)\n## STRONGEST AND THE MOST ACCURATE MODEL - GradientBoost of the final model (77.1%)\n\nGradient boost has a 77 percent chance, but the speed is significantly faster with ExtraTress.","f60a1d6a":"Baseline - Learn about performance with linear regression","9113126f":"num_outbound_cmds is removed from the first because the standard deviation is zero.","b4ac7c0d":"Let's look at how the features affect each other through Ensemble","76ec4dbd":"## 2) Modeling","9fd9b254":"- final models","e1584ca1":"- second models","50bdf2dd":"Std picks the 10 low and stores the features in drop -> X_train_stdrop. (Will be used after ensemble feature selection)","b9efed23":"#### Modeling after completion of the feature selection process (elimination of low deviation, high correlation)\n#### Comparing final modeling results with features that affect ensemble modeling","d3249933":"# Introduction\nAfter feature selection, we will find a model that shows the highest accuracy through modeling techniques such as DecisionTree, MLP, and Ensemble, using the Linear regression model as the baseline.","f57e09aa":"### Ensemble Modeling results with final modeling impact","94eb04a9":"##Save training dataset and test dateaset to each df and split X and Y (xAttack, analysis features)","abe0a95c":"### 2. Ensemble feature selection\nEnsemble Modeling can see how the feature affected each model. Therefore, we tried feature selection around those features (attempt to remove Irrelevant feature).","4593ab62":"## 3) Result"}}