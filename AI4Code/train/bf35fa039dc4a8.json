{"cell_type":{"d3afff34":"code","5dcc3bc0":"code","bab15d12":"code","7664db95":"code","7275aea9":"code","403ba0cd":"code","65d5d87f":"code","5e7d9d41":"code","432b6de0":"code","38f360da":"code","4c195433":"code","f6965469":"code","2b7c5532":"code","32e77adf":"code","f3eb8a8e":"code","c8c31d89":"code","4cf13b0d":"code","f57fd66a":"code","bb219881":"code","e2be6586":"code","5daf1d11":"code","018dfedc":"code","fe2cc163":"code","bdd13b28":"code","74527639":"code","71b65c67":"code","b45811ca":"code","1963cd2f":"code","3518a6a7":"code","79e42c2e":"code","6c25f21e":"code","cf56d248":"code","71815d4b":"code","173bfed1":"code","f1e5e1d9":"code","64dfcfa7":"code","f08d1baa":"code","f084740d":"code","7874f031":"code","1b1b0470":"code","fe1c8e00":"code","4cafe063":"code","0df7eead":"code","767a68e8":"code","a36aef43":"code","eb113e2b":"code","5a361020":"code","8866ebe2":"code","41fa057c":"code","bbb8d98c":"code","cc031795":"code","ad2b931f":"code","dbaadcb8":"code","c596fa96":"code","009d6510":"code","557d9181":"code","4944974f":"code","d5073f70":"code","a1a35bc2":"code","af2f8a61":"code","44712c46":"code","a18af8fd":"code","7bbf9330":"code","043f4bc6":"code","8336c7c4":"code","0c00ee21":"code","d9e4dbc4":"code","80b11b74":"code","63496750":"code","e4ab55d6":"code","3c6a93a7":"code","54ebc115":"code","09267135":"code","74425fa2":"code","d85e2f0d":"code","32d332db":"code","918973e2":"code","5806abef":"markdown","c3a75cd9":"markdown","520155f6":"markdown","b138153a":"markdown","805dd0f9":"markdown","f5900779":"markdown","e483c2c0":"markdown","77d5477d":"markdown","b7fa76ab":"markdown","bfcc3edd":"markdown","ecc96a86":"markdown","1b84ce78":"markdown","e6ddd5ff":"markdown","a65cd0de":"markdown","3ff4df9d":"markdown","d378e518":"markdown","2716242c":"markdown","9ca27b07":"markdown","33a5c7df":"markdown","3d6e6317":"markdown","4b9f7903":"markdown","99e2c628":"markdown","eb2fe900":"markdown","d118e95d":"markdown","d76f47d3":"markdown","91280985":"markdown","eecba84c":"markdown","13b3181f":"markdown","121f2ee0":"markdown","e80cecee":"markdown","cb536fa8":"markdown","79e94693":"markdown","108d4f14":"markdown","3bb5bf87":"markdown","b7e55b54":"markdown","83b79712":"markdown","685ac91a":"markdown","e2a37eea":"markdown","61ec1383":"markdown","516fd66a":"markdown","d7ce551e":"markdown","0b1ae22d":"markdown","e3b5f6c7":"markdown","53447fc2":"markdown","c1b7237c":"markdown","855f3e24":"markdown","82729bb8":"markdown","6c8ee5e3":"markdown","45f4c858":"markdown","4e03fff4":"markdown","8e8e4509":"markdown"},"source":{"d3afff34":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense, Embedding, Flatten, LSTM, GRU, \\\n        SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras import models\nfrom keras import layers\n\nimport pickle\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss, silhouette_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, \\\n        cross_validate, cross_val_score, KFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tqdm import tqdm","5dcc3bc0":"input_dir = '\/kaggle\/input\/lish-moa'\ntrain_features = pd.read_csv(os.path.join(input_dir, 'train_features.csv'))\ntrain_targets_scored = pd.read_csv(os.path.join(input_dir, 'train_targets_scored.csv'))\ntrain_targets_nonscored = pd.read_csv(os.path.join(input_dir, 'train_targets_nonscored.csv'))\ntest_features = pd.read_csv(os.path.join(input_dir, 'test_features.csv'))\n\ntrain_features.shape, train_targets_scored.shape, train_targets_nonscored.shape, test_features.shape","bab15d12":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n\nplt.figure(figsize=(16,4))\n\nfor idx, col in enumerate(cat_cols):\n    plt.subplot(int(f'13{idx + 1}'))\n    labels = train_features[col].value_counts().index.values\n    vals = train_features[col].value_counts().values\n    sns.barplot(x=labels, y=vals)\n    plt.xlabel(f'{col}')\n    plt.ylabel('Count')\nplt.tight_layout()\nplt.show()","7664db95":"# select all indices when 'cp_type' is 'ctl_vehicle'\nctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# evaluate number of 1s we have in the total train scores when cp_type = ctl_vehicle\ntrain_targets_scored.loc[ctl_vehicle_idx].iloc[:, 1:].sum().sum()","7275aea9":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()","403ba0cd":"# drop cp_type column since we no longer need it\nX = train_features.drop(['sig_id', 'cp_type'], axis=1).copy()\nX = X.loc[~ctl_vehicle_idx].copy()\n\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~ctl_vehicle_idx].copy()\n\nX.shape, y.shape","65d5d87f":"X.head(3)","5e7d9d41":"plt.figure(figsize=(8,5))\nsns.distplot(X.iloc[:, 2:].mean())\nplt.show()","432b6de0":"plt.figure(figsize=(8,5))\nsns.distplot(y.mean())\nplt.show()","38f360da":"plt.figure(figsize=(8,5))\nsns.distplot(train_targets_nonscored.mean())\nplt.show()","4c195433":"y.sum().sort_values()[:30].plot.bar(figsize=(18,6))\nplt.show()","f6965469":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","2b7c5532":"def plot_features(X, y, selected_idx, features_type, figsize=(14,10)):\n    x_range = range(1, X.shape[1] + 1)\n    \n    fig = plt.figure(figsize=(14,10))\n    \n    for i, idx in enumerate(selected_idx):\n        ax = fig.add_subplot(selected_idx.shape[0], 1, i + 1)\n        vals = X.iloc[idx].values\n    \n        if (y.iloc[idx] == 1).sum():\n            output_labels = list(y.iloc[idx][y.iloc[idx] == 1].index.values)\n        \n            labels = \" \".join(output_labels)\n        else:\n            labels = \"None (all labels zero)\"\n        \n        sns.lineplot(x_range, vals)\n        plt.title(f\"Row {idx}, Labels: {labels}\", weight='bold')\n        plt.xlim(0.0, X.shape[1])\n        plt.grid()\n\n    plt.xlabel(f\"{features_type}\", weight='bold', size=14)\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef plot_mean_std(dataframe, feature_name, features_type, figsize=(14,6), alpha=0.3):\n    \"\"\" Plot rolling mean and standard deviation for given dataframe \"\"\"\n    \n    plt.figure(figsize=figsize)\n    \n    x_range = range(1, dataframe.shape[1] + 1)\n    \n    chosen_rows = y.loc[y[feature_name] == 1]\n    chosen_feats = dataframe.loc[y[feature_name] == 1]\n    \n    means = chosen_feats.mean()\n    stds = chosen_feats.std()\n    \n    plt.plot(x_range, means, label=feature_name)    \n    plt.fill_between(x_range, means - stds, means + stds, \n                         alpha=alpha)\n\n    plt.title(f'{features_type}: {feature_name} - Mean & Standard Deviation', weight='bold')\n    \n    plt.xlim(0.0, dataframe.shape[1])\n    \n    plt.show()","32e77adf":"# lets plot some random rows from our data\nrandom_idx = np.random.randint(X.shape[0], size=(5,))\n\nplot_features(X_cell_v, y, random_idx, features_type='Cell Features')","f3eb8a8e":"plot_features(X_gene_e, y, random_idx, features_type='Gene Features')","c8c31d89":"# select an output label to plot associated training features\nchosen_label = 'btk_inhibitor'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,), replace=False)","4cf13b0d":"plot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","f57fd66a":"plot_mean_std(X_gene_e, 'btk_inhibitor', 'Gene Features')","bb219881":"# select an output label to plot associated training features\nchosen_label = 'histamine_receptor_antagonist'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","e2be6586":"plot_mean_std(X_gene_e, 'histamine_receptor_antagonist', 'Gene Features')","5daf1d11":"# select an output label to plot associated training features\nchosen_label = 'free_radical_scavenger'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","018dfedc":"plot_mean_std(X_gene_e, 'free_radical_scavenger', 'Gene Features')","fe2cc163":"class MOAPreprocessor:\n    \"\"\" Data Preprocessing class for the MoA dataset, processing cat and num\n        features accordingly. \"\"\"\n    \n    def __init__(self, cat_features, num_features, remove_cp_type=False):\n        self.cat_features = cat_features\n        self.num_features = num_features\n        self.std_scaler = StandardScaler()\n        self.remove_cp_type = remove_cp_type\n        \n    def preprocess_data(self, X, test=False):\n        \"\"\" Preprocess categorical and numerical features \"\"\"\n        \n        # take a copy of sig ids for reference\n        sig_ids = X.loc[:, 'sig_id']\n        \n        #  remove ctl_vehicle if selected\n        if self.remove_cp_type and not test:\n            ctl_vehicle_idx = (X['cp_type'] == 'ctl_vehicle')\n            data_df = X.loc[~ctl_vehicle_idx].copy()\n        else:\n            data_df = X.copy()\n        \n        # subsets of categorical and numerical\n        X_cat = data_df.loc[:, self.cat_features].astype(object)\n        X_num = data_df.loc[:, self.num_features]\n        \n        # one-hot encode our categorical features\n        X_cat = pd.get_dummies(X_cat)\n        \n        # if training, fit our transformers\n        if not test:\n            # fit parameters of our scaler and transform train\n            X_num[self.num_features] = self.std_scaler.fit_transform(X_num)\n            \n            # add train sig ids to class instance\n            self.train_sig_ids = sig_ids.copy()\n        \n        # otherwise, simply transform our data\n        else:\n            # transform test set\n            X_num[self.num_features] = self.std_scaler.transform(X_num)\n            \n            # add test sig ids to class instance\n            self.test_sig_ids = sig_ids.copy()\n            \n        return pd.concat([X_cat, X_num], axis=1)","bdd13b28":"#cat_features = ['cp_time', 'cp_dose', 'cp_type']\ncat_features = ['cp_time', 'cp_dose']\n\n# define non-numeric cols to form list of numeric cols\nnon_num_tuple = ('cp_time', 'cp_dose', 'cp_type', 'sig_id')\nnum_features = [x for x in train_features.columns.values if not x.startswith(non_num_tuple)]\n\ndata_processor = MOAPreprocessor(cat_features, num_features)\nX_train_full = data_processor.preprocess_data(train_features)\nX_test = data_processor.preprocess_data(test_features, test=True)\n\nX_train_full.shape, X_test.shape","74527639":"# we also need to format our labels so that it only contains the output labels, and not sig id\ny = train_targets_scored.drop('sig_id', axis=1).copy()\n\n# remove \nif data_processor.remove_cp_type:\n    ctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n    y = y.loc[~ctl_vehicle_idx].copy()\n\ny.shape","71b65c67":"lowest_occurrence = y.sum().sort_values()\n\n# identify classes that occur less than 2 times in the training set\nminority_feats = lowest_occurrence[lowest_occurrence < 2].index.values\nfor label in minority_feats:\n    print(label)","b45811ca":"# get the row index vals to remove for these features\nremove_idx = np.array([], dtype=int)\nfor feature in minority_feats:\n    remove_idx = np.append(remove_idx, y.loc[y[feature]==1].index.values.astype(int))\n\nX_train_full_tmp = X_train_full.drop(index=remove_idx)\ny_tmp = y.drop(index=remove_idx)\nX_train_full.shape, X_train_full_tmp.shape, y.shape, y_tmp.shape","1963cd2f":"# get the column index vals for these features\n#col_index_map = {}\n#for feature in minority_feats:\n#    col_index_map[feature] = y.columns.get_loc(feature)\n#col_index_map\n\n# temporarily remove these columns and then split our data\n#y_tmp.drop(columns=minority_feats, inplace=True)","3518a6a7":"# choose a larger subset for this evaluation\nX_train, X_val, y_train, y_val = train_test_split(X_train_full_tmp, y_tmp, test_size=0.2, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","79e42c2e":"X_train = X_train.append(X_train_full.iloc[remove_idx], ignore_index=True)\ny_train = y_train.append(y.iloc[remove_idx], ignore_index=True)\nX_train.shape, y_train.shape","6c25f21e":"def ann_model_1(dropout=False, dropout_val=0.45, batch_norm=False, lr=1e-3):\n    \"\"\" Create a basic Deep NN for classification \"\"\"\n    model = models.Sequential()\n    \n    model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(256, activation='relu'))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(256, activation='relu'))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n        \n    # output layer\n    model.add(layers.Dense(206, activation='sigmoid'))\n        \n    model.compile(optimizer=keras.optimizers.RMSprop(lr=lr), \n                  loss='binary_crossentropy', metrics=['accuracy'])\n    return model","cf56d248":"class LearningRateComparison(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        \n        # arrays to store current rate and associated loss\n        self.lr_rates = []\n        self.losses = []\n        \n    def on_batch_end(self, batch, logs):\n        self.lr_rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)","71815d4b":"# define custom learning rate scheduler to compare loss across many learning rates\ncustom_lr = LearningRateComparison(factor=1.0025)\n\nmodel_1 = ann_model_1(dropout=True, batch_norm=True, lr=1e-3)\n\n# train model for 10 epochs\nhistory = model_1.fit(X_train, y_train, epochs=10, \n                      batch_size=64, validation_data=(X_val, y_val), \n                      callbacks=[custom_lr])","173bfed1":"plt.figure(figsize=(12,5))\nsns.lineplot(custom_lr.lr_rates, custom_lr.losses)\nplt.gca().set_xscale('log')\nplt.hlines(min(custom_lr.losses), min(custom_lr.lr_rates), max(custom_lr.lr_rates), \n           linestyle='dashed')\nplt.axis([min(custom_lr.lr_rates), 1.0, 0, custom_lr.losses[0]])\nplt.xlabel(\"Learning rate\", weight='bold', size=13)\nplt.ylabel(\"Loss\", weight='bold', size=13)\nplt.grid()\nplt.show()","f1e5e1d9":"model_1 = ann_model_1(dropout=True, batch_norm=True, lr=2e-2)\nmodel_1.summary()","64dfcfa7":"# set up a check point for our model - save only the best val performance\nsave_path =\"ann_model_1_best.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_loss', \n                                 verbose=1, save_best_only=True, mode='min')\n\nearly_stopper = keras.callbacks.EarlyStopping(patience=20)\n\ntrg_callbacks = [trg_checkpoint, early_stopper]","f08d1baa":"history = model_1.fit(X_train, y_train, epochs=50, \n                      batch_size=64, validation_data=(X_val, y_val), \n                      callbacks=trg_callbacks)","f084740d":"# save model as a HDF5 file with weights + architecture\nmodel_1.save('ann_model_1.hdf5')\n\n# save the history of training to a datafile for later retrieval\n#with open('history_model_1.pickle', 'wb') as pickle_file:\n#    pickle.dump(history.history, pickle_file)\n    \nloaded_model = False","7874f031":"# load model with best weights as found during training\nmodel_1_best = load_model('ann_model_1_best.hdf5')","1b1b0470":"# if already trained - import history file and training weights\n#model_1 = load_model('models\/ann_model_1.hdf5')\n\n# get history of trained model\n#with open('models\/history_model_1.pickle', 'rb') as handle:\n#    history = pickle.load(handle)\n    \n#loaded_model = True","fe1c8e00":"# if loaded model set history accordingly\nif loaded_model:\n    trg_hist = history\nelse:\n    trg_hist = history.history\n\ntrg_loss = trg_hist['loss']\nval_loss = trg_hist['val_loss']\n\ntrg_acc = trg_hist['accuracy']\nval_acc = trg_hist['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(16,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","4cafe063":"val_preds = model_1.predict(X_val)\nscore = keras.losses.BinaryCrossentropy()(y_val, val_preds)\nprint('Baseline score: %.4f' % score.numpy())","0df7eead":"# load model with best weights as found during training\nmodel_1_best = load_model('ann_model_1_best.hdf5')\n\nval_preds_best = model_1_best.predict(X_val)\nbest_score = keras.losses.BinaryCrossentropy()(y_val, val_preds_best)\nprint('Best Val Loss: %.4f' % best_score.numpy())","767a68e8":"# note down best parameters to use for final models\nmodel_1_bs = 1024\nmodel_1_epochs = 59","a36aef43":"def schedule_lr_rate(epoch, lr):\n    \"\"\" Use initial learning rate for 20 epochs and then\n        decrease it exponentially \"\"\"\n    if epoch < 20:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\n# create our learning rate scheduler callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule_lr_rate)\n\n# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper, lr_scheduler]","eb113e2b":"N_FOLDS = 5\nk_folds = KFold(n_splits=N_FOLDS, shuffle=True)","5a361020":"model_histories = []\nmodel_losses = []\ntest_preds = np.zeros((test_features.shape[0], \n                       train_targets_scored.shape[1] - 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X_train_full, y)):\n    train_split = X_train_full.iloc[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X_train_full.iloc[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    temp_model = ann_model_1(dropout=True, batch_norm=True, lr=2e-2)\n    \n    # train model for 100 epochs with early stopping\n    temp_history = temp_model.fit(train_split, train_labels, \n                            epochs=50, batch_size=64, verbose=0,\n                            validation_data=(val_split, val_labels), callbacks=[trg_callbacks])\n    \n    model_histories.append(temp_history)\n    \n    # find log loss for out of fold val data\n    model_val_preds = temp_model.predict(val_split)\n    model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, model_val_preds).numpy()\n    model_losses.append(model_log_loss)\n    print(f'Current Fold Validation Loss: {model_log_loss:.4f}')\n    \n    # make predictions on test set for each fold\n    temp_test_preds = temp_model.predict(X_test)\n    test_preds += (temp_test_preds \/ N_FOLDS)\n\n# convert results to np array\nmodel_losses = np.array(model_losses)","8866ebe2":"print(f\"Mean loss across all folds: {model_losses.mean():.4f} +\/- {model_losses.std():.4f}\")","41fa057c":"fold_1_df = pd.DataFrame(model_histories[0].history)\nfold_2_df = pd.DataFrame(model_histories[1].history)\nfold_3_df = pd.DataFrame(model_histories[2].history)\nfold_4_df = pd.DataFrame(model_histories[3].history)\nfold_5_df = pd.DataFrame(model_histories[4].history)\navg_fold_df = (fold_1_df + fold_2_df + fold_3_df + fold_4_df + fold_5_df) \/ 5","bbb8d98c":"avg_fold_df[['loss', 'val_loss']].plot(figsize=(12,5))\nplt.grid()\nplt.title(\"Average Training \/ Validation Loss across all Folds\", weight='bold')\nplt.ylabel(\"Loss\", weight='bold')\nplt.xlabel(\"Epochs\", weight='bold')\nplt.legend(loc='best')\nplt.xlim(0.0, avg_fold_df.shape[0])\nplt.show()","cc031795":"# take a copy of all our training sig_ids for reference\ntest_sig_ids = test_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntest_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')\n\n# change all cp_type == ctl_vehicle predictions to zero\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0","ad2b931f":"test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_preds_df = pd.DataFrame(test_preds, columns=train_targets_scored.columns[1:])\ntest_submission = pd.concat([test_submission, test_preds_df], axis=1)\ntest_submission.head(3)","dbaadcb8":"test_submission.to_csv('submission.csv', index=False)","c596fa96":"# to-do: add batch normalisation\n\ndef ann_model_2(dropout=False, dropout_val=0.45, batch_norm=False):\n    \"\"\" Create a basic Deep NN for classification \"\"\"\n    model = models.Sequential()\n    \n    model.add(layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(256, activation='relu'))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    \n    # output layer\n    model.add(layers.Dense(206, activation='sigmoid'))\n        \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","009d6510":"model_2 = ann_model_2(dropout=True, batch_norm=True)\n\n# set up a check point for our model - save only the best val performance\nsave_path =\"ann_model_2_best.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_loss', \n                                 verbose=1, save_best_only=True, mode='min')\n\ntrg_callbacks = [trg_checkpoint]\n\nhistory_2 = model_2.fit(X_train, y_train, epochs=100, \n                      batch_size=1024, validation_data=(X_val, y_val), \n                      callbacks=trg_callbacks)","557d9181":"trg_loss = history_2.history['loss']\nval_loss = history_2.history['val_loss']\n\ntrg_acc = history_2.history['accuracy']\nval_acc = history_2.history['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(16,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","4944974f":"val_preds = model_2.predict(X_val)\nscore = keras.losses.BinaryCrossentropy()(y_val, val_preds)\nprint('Baseline score: %.4f' % score.numpy())","d5073f70":"# load model with best weights as found during training\nmodel_2_best = load_model('ann_model_2_best.hdf5')\n\nval_preds_best = model_2_best.predict(X_val)\nbest_score = keras.losses.BinaryCrossentropy()(y_val, val_preds_best)\nprint('Best Val Loss: %.4f' % best_score.numpy())","a1a35bc2":"# note down best parameters to use for final models\nmodel_2_bs = 1024\nmodel_2_epochs = 97","af2f8a61":"# to-do: add batch normalisation\n\ndef ann_model_3(dropout=False, dropout_val=0.45, batch_norm=False):\n    \"\"\" Create a basic Deep NN for classification \"\"\"\n    model = models.Sequential()\n    \n    model.add(layers.Dense(256, activation='selu', input_shape=(X_train.shape[1],)))\n    if batch_norm:\n        model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    \n    # output layer\n    model.add(layers.Dense(206, activation='sigmoid'))\n        \n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","44712c46":"model_3 = ann_model_3(dropout=True, batch_norm=True)\n\n# set up a check point for our model - save only the best val performance\nsave_path =\"ann_model_3_best.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_loss', \n                                 verbose=1, save_best_only=True, mode='min')\n\ntrg_callbacks = [trg_checkpoint]\n\nhistory_3 = model_3.fit(X_train, y_train, epochs=50, \n                      batch_size=1024, validation_data=(X_val, y_val), \n                      callbacks=trg_callbacks)","a18af8fd":"trg_loss = history_3.history['loss']\nval_loss = history_3.history['val_loss']\n\ntrg_acc = history_3.history['accuracy']\nval_acc = history_3.history['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(16,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","7bbf9330":"val_preds = model_3.predict(X_val)\nscore = keras.losses.BinaryCrossentropy()(y_val, val_preds)\nprint('Baseline score: %.4f' % score.numpy())","043f4bc6":"# load model with best weights as found during training\nmodel_3_best = load_model('ann_model_3_best.hdf5')\n\nval_preds_best = model_3_best.predict(X_val)\nbest_score = keras.losses.BinaryCrossentropy()(y_val, val_preds_best)\nprint('Best Val Loss: %.4f' % best_score.numpy())","8336c7c4":"# note down best parameters to use for final models\nmodel_3_bs = 1024\nmodel_3_epochs = 45","0c00ee21":"# lets gather the top 50 under-represented classes to oversample\ntop_100_minority = y.sum().sort_values()[:100].index.values\n\n# iterate through each minority class, select random instances that contain that class and duplicate\n# repeat this for all classes until we have a chosen minimum number of class instances\nmin_class_count = 30\n\nextra_features = pd.DataFrame()\nextra_labels = pd.DataFrame()\n    \nfor column in top_100_minority:\n    class_count = y[column].sum()\n    class_count_diff = min_class_count - class_count\n    \n    if class_count_diff > 1:\n        \n        # find instance idxs where class is 1\n        positive_idxs = y[column] == 1\n        \n        for iteration in range(int(np.ceil(class_count_diff \/ class_count))):\n        \n            # get random feature and label corresponding to class\n            rand_feature = X_train_full[positive_idxs].sample(class_count)\n            rand_label = y[positive_idxs].sample(class_count)\n        \n            extra_features = extra_features.append(rand_feature, ignore_index=True)\n            extra_labels = extra_labels.append(rand_label, ignore_index=True)\n            \nextra_features.shape, extra_labels.shape\n\noversampled_X = X_train_full.append(extra_features, ignore_index=False)\noversampled_y = y.append(extra_labels, ignore_index=False)","d9e4dbc4":"# scores 0.02027 on test set\nmodel_1 = ann_model_1(dropout=True)\nhistory_1 = model_1.fit(X_train_full, y, epochs=model_1_epochs, batch_size=model_1_bs)","80b11b74":"test_preds_1 = model_1.predict(X_test)","63496750":"model_2 = ann_model_2(dropout=True)\nhistory_2 = model_2.fit(X_train_full, y, epochs=model_2_epochs, batch_size=model_2_bs)","e4ab55d6":"test_preds_2 = model_2.predict(X_test)","3c6a93a7":"model_3 = ann_model_3(dropout=True)\nhistory_3 = model_3.fit(X_train_full, y, epochs=model_3_epochs, batch_size=model_3_bs)","54ebc115":"test_preds_3 = model_3.predict(X_test)","09267135":"# combine our model predictions into an overall average\ntest_preds = (test_preds_1 + test_preds_2 + test_preds_3) \/ 3.0","74425fa2":"# take a copy of all our training sig_ids for reference\ntest_sig_ids = test_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntest_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')\n\n# find total sum of predictions for these instances in test preds\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","d85e2f0d":"# change all cp_type == ctl_vehicle predictions to zero\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n# confirm all values now sum to zero for these instances\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","32d332db":"test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_submission[train_targets_scored.columns[1:]] = test_preds\ntest_submission.head(3)","918973e2":"#test_submission.to_csv('submission.csv', index=False)","5806abef":"We have our main input features (train_features.csv), which is high-dimensional tabular data containing a mixture of categorical and numerical features. We then have our train targets, which consists of 206 different output classes for each data instance. Its important to note that these output labels are not mutually exclusive, and it is possible to get multiple outputs for each data instance. Therefore, this problem is a multi-output classification problem, and not just a multiclass classification problem. \n\nIn contrast to a normal binary classification task, this type of multi-label problem becomes much more difficult in terms of producing and fine-tuning a classification model. ","c3a75cd9":"<a id=\"EDA\"><\/a>\n## 2. Basic Exploratory Data Analysis","520155f6":"Our loss begins to increase gradually after around $ 4 \\times 10^{-2} $, so we'll try a little bit before this as a starting point.","b138153a":"Good, now we can make our final submission using this basic DNN classifier.","805dd0f9":"<a id=\"model-production\"><\/a>\n## 4. Model production and evaluation","f5900779":"Remarks on tuning performance and history:\n- Batch size increase from 256 to 1024 resulted in an improved validation loss (val loss = 0.0158).\n- Change from hidden 1=512, hidden 2=256 to hidden 1=512, hidden 2=512 resulted in slightly worse val loss (val loss = 0.0163).\n- Change from hidden 1=512, hidden 2=256 to hidden 1=256, hidden 2=256 resulted in val loss of 0.159.\n- Change from dropout of 0.45 to dropout of 0.40 resulted in slightly better performance with val loss of 0.158.\n- Change from dropout of 0.40 to dropout of 0.35 resulted in slightly worse performance with val loss of 0.160.\n- Change from hidden 1=256, hidden 2=256 to hidden 1=1024, hidden 2=256 resulted in val loss of 0.160.\n- Change from hidden 1=256, hidden 2=256 to hidden 1=2048, hidden 2=256 resulted in val loss of 0.0158 after 43 epochs.\n- Same as above, but with dropout of 0.30,which resulted in a worse validation loss of 0.0167.\n- Same as above, but with dropout returned to 0.45 and adam optimisation used instead of rmsprop, which resulted in a validation loss of 0.0161 after 97 epochs.","e483c2c0":"Lets repeat this process for some different output labels:","77d5477d":"Only a total sum of 6, so not too bad. Nevertheless, we'll still remove these from our test set predictions, since they are incorrect.","b7fa76ab":"Lets check the number of instances we incorrectly assigned probabilities to when they should in fact be zero due cp_type == ctl_vehicle:","bfcc3edd":"#### Model 1 improvements - learning Rate Scheduling and K-Folds Cross Validation","ecc96a86":"We have some noticeable peaks throughout the features for some of the above instances. It could be worth plotting a range of data instances with the same output labels against one another, and compare their peaks. If they correlate in one or more areas, this could be insightful for developing further features with our dataset.\n\nLets now repeat above, but for data instances with the same output label(s).","1b84ce78":"<a id=\"imports\"><\/a>\n## 1. Import dependencies and load data","e6ddd5ff":"Train for several epochs with our learning rate comparison:","a65cd0de":"### Model 1 - Three hidden layers","3ff4df9d":"Now lets do the same for our gene features:","d378e518":"Now lets add the under-represented instances back into our training set:","2716242c":"---","9ca27b07":"Clearly some rows vary substancially in terms of their value range, and therefore it is worth standardising this data prior to training our models.","33a5c7df":"### Model 2 - Reduced complexity with only two hidden layers","3d6e6317":"<a id=\"test-predictions\"><\/a>\n## 5. Test Set Predictions - Combining the performance of the three best models","4b9f7903":"---","99e2c628":"For 'cp_type', the 'ctl_vehicle' refers to samples treated with a control perturbation. For control perturbations, our targets are all zero, since they have no Mechanism of Action (MoA).\n\nTo deal with this, a good strategy could be to identify samples that are ctl_vehicle (through training a classification model or simply using the feature as its in the test data!), and set all of these to zero. We can then process the test set accordingly, by first setting all test instance targets to zero if its a ctl_vehicle, followed by processing all of the others normally using our trained model.","eb2fe900":"### 5.3 Final predictions and submission","d118e95d":"Many of our minority classes have an extremely low number of samples in the training set, whereby some only have 1.\n\nWe can either remove these entirely from the training set and proceed onwards, or come up with an elaborate way of sampling to overcome this issue.\n\nFor this work, we will just simply include the 1 time occurence samples within the training set, and exclude them from the validation set (since the only way to do this would be to use duplicates which is of no value). We'll simply note these rows, remove them prior to splitting into our training and validation data, and then insert them into the training data after we have formed these splits.\n\nIdeally, for such a task like this, we should perform a suitable form of multi label stratified k folds, however this will be a task for later, rather than in this notebook.","d76f47d3":"The data has already been normalised using quantile normalisation, and so is not in its natural form as we see it.","91280985":"For an initial model this is not too bad, although we could definitely benefit from a gradual reduction in our learning rate as our number of epochs increase. Therefore, we could play around and adjust a range of learning rate schedulers and likely receive a better performance than that obtained above.\n\nIn addition, we could benefit from conducting k-folds cross-validation, rather than just using one hold-out validation set.\n\nAs a basic submission with a lightly tuned model, this current model does not perform too badly on the final test set, with a score of 0.02027 on the leaderboard.\n\nFor future improvement, we'll also attempt to tackle the class imbalance problem by over-sampling instances with our most under-represented classes (Scores 0.02060 on test set).\n\nRemarks on validation performance and tuning:\n- 1. hidden 1 = 1024, hidden 2 = 512, hidden 3 = 256, batch size 1024, rmsprop op, 0.45 dropout, batch norm, 35 epochs, validation loss = 0.0158.\n- 2. Same as above but hidden 1 = 512, after 45 epochs validation loss = 0.0158.\n- 3. hidden 1 = 512, hidden 2 = 256, hidden 3 = 256, batch size 1024, rmsprop op, 0.45 dropout, batch norm, 59 epochs, validation loss = 0.0157.\n- 4. Same as above, but without Batch Normalisation - performance decreased with validation loss of 0.0167.\n- 5. hidden 1 = 512, hidden 2 = 256, hidden 3 = 128, batch size 1024, rmsprop op, 0.45 dropout, batch norm, 67 epochs, validation loss = 0.0160.\n- 6. Same as previously, but with Batch size set to 2048 - performance slightly dropped, with validation loss of 0.0165.\n","eecba84c":"Lets split our data randomly. Ideally we'd perform a multi-label stratified split here, but due to issues with limited numbers of class instances and imbalance across the dataset, we'll avoid it for now.","13b3181f":"---","121f2ee0":"The total sum is zero, which confirms the statement above on all targets being zero for cases where cp_type is ctl_vehicle. The best thing to do with this is simply fill our targets for zero when this is the case.\n\nWe could also remove all of these from the training set, however there are arguments for and against this in practice. If we remove them, we could be witholding valuable zero case data from our models, and for new data our model might struggle to predict these cases accordingly. On the other hand, it is a lot of extra data, which could just serve to unnecessarily complicate our model.","e80cecee":"### 5.2 Production of our three models and obtaining predictions for each","cb536fa8":"# Mechanisms of Action Predictions\n\nWithin this notebook some variants of basic Deep Neural Networks are produced and used to form a range of multi-output classification predictions on the test set. The approach is simple and could be easily improved upon, but nevertheless was only intended as a simple introduction to this dataset and the competition.\n\n**Table of Contents:**\n\n1. [Imports](#imports)\n2. [EDA](#EDA)\n3. [Data Preparation and Preprocessing](#data-preprocessing)\n4. [Model Production and Evaluation](#model-production)\n5. [Test Set Predictions](#test-predictions)","79e94693":"We'll now oversample the most under-represented class instances within our training data. To obtain this we'll iterate through each minority class, select random instances with that class, and simply duplicate them. This will be repeated for all classes until we have a chosen minimum number of class instances. This approach is extremely crude and feels very rough and ready, but is simply being conducted as an experiment on how our final model performs.","108d4f14":"We'll train our model on the entire training set (both normal and oversampled variants) and make a set of predictions on the test set.","3bb5bf87":"This analysis highlights the potential for performing advanced feature engineering, such as using the trends of gene and\/or cell features as additional features to our models. We could use such features to supplement the existing data in its standard form.","b7e55b54":"### 5.1 Optional exploration - oversampling of minority data prior to making predictions","83b79712":"We'll try a model with less complexity and a reduced overall number of parameters, since it appears we are overfitting the data substancially.","685ac91a":"#### Plotting all gene \/ cell features for random samples:","e2a37eea":"<a id=\"data-preprocessing\"><\/a>\n## 3. Preprocessing and Data Preparation","61ec1383":"---","516fd66a":"#### Lets first evaluate the best learning rate for this model:\n\nLets create a custom callback for exploring the best learning rates for our models:","d7ce551e":"### Model 3 - Further simplicity, with just one hidden layer","0b1ae22d":"Remarks on tuning performance and results:\n- Hidden layer size 256, batch size 1024, dropout 0.45, BatchNorm True, 43 Epochs, val loss = 0.0161\n- Same as above, but with hidden layer size 512, 30 Epochs, val loss = 0.0162.\n- Same as previous, but with hidden layer increases to 1024 and 2048 - both result in worse val losses.\n- Same as above but with Tanh activation - validation loss of 0.0163.\n- Same as above but with Selu activation - validation loss of 0.0159.\n","e3b5f6c7":"Lets also look at the mean and standard deviation of this feature:","53447fc2":"Lets visualise the average training and validation loss across all of our folds:","c1b7237c":"#### Lets combine each set of predictions from each fold into an overall average set of test predictions\n\nSince we removed all instances with cp_type == ctl_vehicle from our training data, we will need to adjust our test set predictions so that the targets are always zero for these instances.","855f3e24":"This will be relatively simple and will include:\n- Standardisation of all numerical features.\n- Creation of embeddings or encodings for our categorical variables.\n- Removal of unwanted \/ unnecessary columns.\n\nWe'll define a simple class to perform these actions for us on both the training and test data.","82729bb8":"Some output classes only have 1 instance in the entire training set. This is problematic and is no where near enough data if we expect our models to effectively make predictions across the whole range of targets. Imbalanced dataset techniques such as minority class over-sampling may have to be introduced, which may help our models generalise better to new data.","6c8ee5e3":"Lets save this and make a submission.","45f4c858":"We'll remove these features from the dataset, perform our split, and then insert them in again:","4e03fff4":"With this in the correct format, we can now save it and make a basic submission for the competition:","8e8e4509":"Lets quickly assess how our cell data looks when plotted over all features for random instances:"}}