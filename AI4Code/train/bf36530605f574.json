{"cell_type":{"d74aeee5":"code","7d00b69f":"code","a47a7ce9":"code","e53b1dd8":"code","92e52f5a":"code","2b40d08c":"code","23dfed6b":"code","c8403796":"code","419d7154":"code","6ce490e9":"code","28ce1fe5":"code","27a4de40":"code","eabf7cdf":"code","0dc5ec09":"code","909a63ca":"code","edf5e296":"code","54df8180":"code","eeae6041":"code","3b7f0bb0":"code","1c5e1433":"code","120f5529":"code","429b9f12":"code","3ece0b2c":"code","b79130f1":"code","b68eb457":"code","dc9755dd":"code","74d50f8f":"code","0481c1ef":"code","576cecc9":"code","b4f082a3":"code","80837a81":"code","1eaa93d8":"code","ef8bfb5a":"code","4cea9302":"code","ec48ea43":"markdown"},"source":{"d74aeee5":"import numpy as np \nimport pandas as pd \nimport os","7d00b69f":"import numpy as np\nimport os\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Convolution2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D,MaxPooling2D\nfrom tensorflow.keras.layers import Dense,Flatten,SpatialDropout2D\nfrom tensorflow.keras.layers import concatenate\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import History\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3 as PTModel\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\nfrom tensorflow.keras.metrics import top_k_categorical_accuracy\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import load_model\n%matplotlib inline","a47a7ce9":"base_image_dir = os.path.join('..', 'input', 'diabetic-retinopathy-resized')\nprint(base_image_dir)\nretina_df = pd.read_csv(os.path.join(base_image_dir, 'trainLabels.csv'))\n\nindexNames = retina_df[ retina_df['level'] == 3 ].index\n \n# Delete these row indexes from dataFrame\nretina_df.drop(indexNames , inplace=True)\n\nindexNames = retina_df[ retina_df['level'] == 2 ].index\nretina_df.drop(indexNames , inplace=True)\nindexNames = retina_df[ retina_df['level'] == 1 ].index\n \n# Delete these row indexes from dataFrame\nretina_df.drop(indexNames , inplace=True)\n\nretina_df['PatientId'] = retina_df['image'].map(lambda x: x.split('_')[0])\nretina_df['path'] = retina_df['image'].map(lambda x: os.path.join(base_image_dir,'resized_train','resized_train',\n                                                         '{}.jpeg'.format(x)))\nretina_df['exists'] = retina_df['path'].map(os.path.exists)\nprint(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\nretina_df['eye'] = retina_df['image'].map(lambda x: 1 if x.split('_')[-1]=='left' else 0)\nfrom keras.utils.np_utils import to_categorical\nretina_df['level'].replace(4,1,inplace=True)\nretina_df['level_cat'] = retina_df['level'].map(lambda x: to_categorical(x, 1+retina_df['level'].max()))\n\nretina_df.dropna(inplace = True)\nretina_df = retina_df[retina_df['exists']]\n\n","e53b1dd8":"retina_df[retina_df['level']==1].shape","92e52f5a":"retina_df[['level', 'eye']].hist(figsize = (10, 5))","2b40d08c":"rr_df = retina_df[['PatientId', 'level']].drop_duplicates()\ntrain_ids, valid_ids = train_test_split(rr_df['PatientId'], \n                                   test_size = 0.25, \n                                   random_state = 2018,\n                                   stratify = rr_df['level'])\nraw_train_df = retina_df[retina_df['PatientId'].isin(train_ids)]\nvalid_df = retina_df[retina_df['PatientId'].isin(valid_ids)]\nvalid_df = valid_df.groupby(['level', 'eye']).apply(lambda x: x.sample(86, replace = False)\n                                                      ).reset_index(drop = True)\nvalid_df.drop_duplicates(subset=\"image\", keep='first', inplace=True)\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])\n\n","23dfed6b":"valid_df.drop_duplicates(subset=\"image\", keep='first', inplace=True)\nretina_df[(retina_df['level']==1) &( retina_df['eye']==1)]","c8403796":"train_df = raw_train_df.groupby(['level', 'eye']).apply(lambda x: x.sample(4000, replace = True)\n                                                      ).reset_index(drop = True)\ntrain_df.drop_duplicates(subset='image', keep='first', inplace=False)\n#retina_df.drop_duplicates(subset=None, keep='first', inplace=False)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\ntrain_df[['level', 'eye']].hist(figsize = (10, 5))","419d7154":"batch_size = 200\nIMG_SIZE = (512, 512)","6ce490e9":"train_df['level'] = train_df['level'].map(lambda x: str(x))\nvalid_df['level'] = valid_df['level'].map(lambda x: str(x))\n","28ce1fe5":"train_df.head()","27a4de40":"\nimport tensorflow\ntrain_datagen = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.inception_v3.preprocess_input,\n                                   height_shift_range=0.01,\n                                   width_shift_range=0.01,\n                                   brightness_range=(0.8,1.2),\n                                   horizontal_flip=True,\n                                   shear_range=0.01,\n                                   vertical_flip=True,\n                                   rotation_range=10,\n                                   zoom_range=0.05,\n#                                     featurewise_std_normalization=True,\n#                                     featurewise_center=True\n#                                    samplewise_std_normalization=True,\n                                  )\n\ntest_datagen = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.inception_v3.preprocess_input)\n\ntrain_gen=train_datagen.flow_from_dataframe(dataframe=train_df,\n                                            x_col=\"path\",\n                                            y_col=\"level\",\n                                            batch_size=batch_size,\n                                            seed=42,\n                                            shuffle=True,\n                                            class_mode=\"categorical\",\n                                            target_size=IMG_SIZE)\nvalid_gen=test_datagen.flow_from_dataframe(dataframe=valid_df,\n                                            x_col=\"path\",\n                                            y_col=\"level\",\n                                            batch_size=batch_size,\n                                            seed=42,\n                                            shuffle=True,\n                                            class_mode=\"categorical\",\n                                            target_size=IMG_SIZE)","eabf7cdf":"t_x, t_y = next(valid_gen)\nfig, m_axs = plt.subplots(3, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n    c_ax.set_title('Retinopathy: {}'.format(np.argmax(c_y, -1)))\n    c_ax.axis('off')","0dc5ec09":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(3, 4, figsize = (16, 16))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n    c_ax.set_title('Retinopathy {}'.format(np.argmax(c_y, -1)))\n    c_ax.axis('off')","909a63ca":"in_lay = Input(t_x.shape[1:])\nbase_pretrained_model = PTModel(input_shape =  t_x.shape[1:], include_top = False, weights =  os.path.join('..', 'input', 'inceptionv3','inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'))\nfor layers in base_pretrained_model.layers[:-80]:\n    layers.trainable = False\nbase_pretrained_model.summary()","edf5e296":"# from keras.applications.vgg16 import VGG16 as PTModel\n\n\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\npt_features = base_pretrained_model(in_lay)\nbn_features = BatchNormalization()(pt_features)\n\n# here we do an attention mechanism to turn pixels in the GAP on an off\n\nattn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.6)(bn_features))\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = Conv2D(1, \n                    kernel_size = (1,1), \n                    padding = 'valid', \n                    activation = 'sigmoid')(attn_layer)\n# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n               activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)\n\nmask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.50)(gap)\ndr_steps = Dropout(0.50)(Dense(128, activation = 'relu')(gap_dr))\nout_layer = Dense(t_y.shape[-1], activation = 'softmax', name='visualized_layer')(dr_steps)\nretina_model = Model(inputs = [in_lay], outputs = [out_layer])\n\nretina_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n                           metrics = ['categorical_accuracy'])\nretina_model.summary()","54df8180":"from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_newweights.best.hdf5\".format('retina')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=7) \ncallbacks_list = [checkpoint, early, reduceLROnPlat]","eeae6041":"history=retina_model.fit(train_gen, \n                           steps_per_epoch = train_df.shape[0]\/\/batch_size,\n                           validation_data = valid_gen,\n                             validation_steps = valid_df.shape[0]\/\/batch_size,\n                              epochs =100, \n                              callbacks = callbacks_list,\n                             workers = 0, \n                             use_multiprocessing=True, \n                             max_queue_size = 0\n                            )","3b7f0bb0":"cm_batch = valid_gen\ntest_labels = cm_batch.classes","1c5e1433":"\npredictions = retina_model.predict(cm_batch, steps=len(cm_batch), verbose=0)","120f5529":"cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))","429b9f12":"cm_batch.class_indices","3ece0b2c":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b79130f1":"cm_plot_labels = ['0','1']\nimport itertools\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","b68eb457":"def plot_model_history(history):\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['categorical_accuracy'])\n    plt.plot(history.history['val_categorical_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['lr'])\n    plt.title('learning rate')\n    plt.ylabel('lr')\n    plt.xlabel('epoch')\n    plt.show()","dc9755dd":"plot_model_history(history)","74d50f8f":"retina_model.save('\/kaggle\/working\/final_model.h5')","0481c1ef":"##### create one fixed dataset for evaluating\nfrom tqdm import tqdm_notebook\n# fresh valid gen\n# print(valid_df.head(4))\n# valid_dfnew = valid_df.head(100)\n# print(valid_dfnew.shape)\nprint(valid_df.shape[0]\/\/batch_size-1)\nvbatch_count = (valid_df.shape[0]\/\/batch_size-1)\nif(vbatch_count<0):\n    vbatch_count=1\nout_size = vbatch_count*batch_size\nprint(t_x.shape[1:])\ntest_X = np.zeros((out_size,)+t_x.shape[1:], dtype = np.float32)\ntest_Y = np.zeros((out_size,)+t_y.shape[1:], dtype = np.float32)\nfor i, (c_x, c_y) in zip(tqdm_notebook(range(vbatch_count)), \n                         valid_gen):\n    j = i*batch_size\n    test_X[j:(j+c_x.shape[0])] = c_x\n    test_Y[j:(j+c_x.shape[0])] = c_y","576cecc9":"# get the attention layer since it is the only one with a single output dim\nfor attn_layer in retina_model.layers:\n    c_shape = attn_layer.get_output_shape_at(0)\n    if len(c_shape)==4:\n        if c_shape[-1]==1:\n            print(attn_layer)\n            break","b4f082a3":"from sklearn.metrics import accuracy_score, classification_report\npred_Y = retina_model.predict(test_X, batch_size = 32, verbose = True)\npred_Y_cat = np.argmax(pred_Y, -1)\ntest_Y_cat = np.argmax(test_Y, -1)\nprint('Accuracy on Test Data: %2.2f%%' % (accuracy_score(test_Y_cat, pred_Y_cat)))\nprint(classification_report(test_Y_cat, pred_Y_cat))","80837a81":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(test_Y_cat, pred_Y_cat), \n            annot=True, fmt=\"d\", cbar = False, cmap = plt.cm.Blues, vmax = test_X.shape[0]\/\/16)","1eaa93d8":"from sklearn.metrics import roc_curve, roc_auc_score\nsick_vec = test_Y_cat>0\nsick_score = np.sum(pred_Y[:,1:],1)\nfpr, tpr, _ = roc_curve(sick_vec, sick_score)\nfig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\nax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\nax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\nax1.legend()\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate');","ef8bfb5a":"fig, m_axs = plt.subplots(2, 3, figsize = (32, 20))\nfor (idx, c_ax) in enumerate(m_axs.flatten()):\n    c_ax.imshow(np.clip(test_X[idx]*127+127,0 , 255).astype(np.uint8), cmap = 'bone')\n    c_ax.set_title('Actual Severity: {}\\n{}'.format(test_Y_cat[idx], \n                                                           '\\n'.join(['Predicted %02d (%04.1f%%): %s' % (k, 100*v, '*'*int(10*v)) for k, v in sorted(enumerate(pred_Y[idx]), key = lambda x: -1*x[1])])), loc='left')\n    c_ax.axis('off')\nfig.savefig('trained_img_predictions.png', dpi = 300)","4cea9302":"retina_model.save('16kfinal_model.h5')","ec48ea43":"Only two class is considered. severity 4 and 0 is considered."}}