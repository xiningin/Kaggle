{"cell_type":{"bef2a300":"code","446e0ab7":"code","47502419":"code","4ef18ae9":"code","dd08f28f":"code","68f81b3b":"code","eb0ace67":"code","2170635d":"code","47851408":"code","20105613":"code","eff83f5d":"code","c21a797e":"code","e95a3e61":"code","94b39dd1":"code","a6bc4c3d":"code","18e3fcf6":"code","696f2b99":"code","b03cd8aa":"code","11bb9259":"code","ba38a4c3":"code","56cfb346":"code","1ccbc112":"code","bd6550b0":"code","662261fa":"code","99a0d1bc":"code","d455de1c":"code","2180fd88":"code","f234f9dc":"markdown","b0395e56":"markdown","88f20dc1":"markdown","433b4f6b":"markdown","78749236":"markdown","b442cec3":"markdown"},"source":{"bef2a300":"import pandas as pd\nimport numpy as np\nimport os\nimport sklearn\n\nimport sys\nsys.path = [\n    '..\/input\/readability-package',\n] + sys.path\nimport readability    \n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, BayesianRidge\nimport spacy\nimport pickle\nimport joblib\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Python Version: \", sys.version)\nprint(\"Spacy Version: \", spacy.__version__)\nprint(\"SkLearn Version: \", sklearn.__version__)\nprint(\"NLTK Version: \", nltk.__version__)\nprint(\"Pandas Version: \", pd.__version__)\nprint(\"Numpy Version: \", np.__version__)\n","446e0ab7":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","47502419":"train_df.head()","4ef18ae9":"test_df.head()","dd08f28f":"# source: https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/notebook\n\n\"\"\"\nThis function uses the readability library for feature engineering.\nIt includes textual statistics, readability scales and metric, and some pos stats\n\"\"\"\ndef readability_measurements(passage: str):\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","68f81b3b":"# source: https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/notebook\n\n\"\"\"\nThis function generates features using spacy en_core_wb_lg\nUseful resources:\nhttps:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\nhttps:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners\n\"\"\"\n\ndef spacy_features(df: pd.DataFrame):  \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","eb0ace67":"# source: https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/notebook\n\ndef pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","2170635d":"# source: https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/notebook\n\n\"\"\"\nThis function is where I test miscellaneous features\nThis is experimental\nCurrently checks sentence status\n\"\"\"\ndef generate_other_features(passage: str):\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words\/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","47851408":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","20105613":"'''\nSource for feature check section: \n1.) https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\/notebook\n'''\n\nclass CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","eff83f5d":"# Results for training dataset.\ndataset = CLRDataset(train_df, train=True)\ndf = dataset.get_df()\ndf.head()","c21a797e":"# Results for testing dataset.\ntest_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\ntest_df.head(2)","e95a3e61":"def set_seed(seed=42):\n    \"\"\" Sets the Seed \"\"\"\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(42)","94b39dd1":"features = [\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n            \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n            \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\", \n            \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"]\nfeatures+=get_spacy_col_names()\nfeatures+=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n            \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n            \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]","a6bc4c3d":"\"\"\" I normalize the data here, could be useful depending on your model\"\"\"\nscaler = MinMaxScaler()\ndf[features] = scaler.fit_transform(df[features])\ntest_df[features] = scaler.transform(test_df[features])","18e3fcf6":"def train_pred_one_fold(model_name: str, fold: int, df: pd.DataFrame, test_df: pd.DataFrame, features: list, rmse: list):\n    \"\"\"\n    This function trains and predicts on one fold of your selected model\n    df is the train df, test_df is the test_df\n    X features are defined in features\n    y output is target\n    oof score is printed and stored in the rmse list\n    \"\"\"\n    train = df[df.kfold == fold]\n    X_train = train[features]\n    y_train = train[\"target\"]\n \n    valid = df[df.kfold != fold]\n    X_valid = valid[features]\n    y_valid = valid[\"target\"]\n    \n    X_test = test_df[features]\n\n    # Ridge model\n    if model_name == 'ridge' or model_name == 'bayesian_ridge':\n        model.fit(X_train, y_train)\n        oof = model.predict(X_valid)\n        print(np.sqrt(mean_squared_error(y_valid, oof)))\n        rmse.append(np.sqrt(mean_squared_error(y_valid, oof)))\n        test_preds = model.predict(X_test)\n#         with open(f\"model_{fold}.pkl\", \"wb\") as file:\n#             pickle.dump(model, file)\n        if not os.path.isfile(f\"model_{fold}.joblib\"):\n            joblib.dump(model, f\"model_{fold}.joblib\")\n    \n    else:\n        test_preds = 0\n        raise Exception(\"Not Implemented\")\n        \n    return test_preds","696f2b99":"def train_pred(model_name: str, df: pd.DataFrame, test_df: pd.DataFrame, features: list):\n    \"\"\"\n    This function trains and predicts multiple fold using train_pred_one_fold\n    The average rmse is printed the the test data predictions are returned\n    The last column is the average result from all folds to be submitted\n    \"\"\"\n    global model\n    if model_name == 'ridge':\n        model = Ridge(alpha=3, max_iter=10000)\n        \n    elif model_name == 'bayesian_ridge':\n        model = BayesianRidge(n_iter=10000, tol=0.8) \n        \n    print(f\"model_name: {model_name}\")\n    all_preds = pd.DataFrame()\n    rmse = list()\n    for f in range(2):\n        all_preds[f\"{model_name}_{f}\"] = train_pred_one_fold(model_name, f, df, test_df, features, rmse)\n\n    all_preds[f\"{model_name}\"] = all_preds.mean(axis=1)\n    print(\"---------\")\n    print(f\"avg rmse: {np.mean(rmse)}\")\n    return all_preds","b03cd8aa":"def prep_sub(preds: pd.DataFrame, col_name: str):\n    \"\"\"\n    This function takes an output prediction df from train_pred\n    and sets it to a format that can be submitted to the competition\n    \"\"\"\n    sub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n    sub[\"target\"] = preds[col_name]\n    sub.to_csv(\"submission.csv\", index=False)\n    print(sub)\n\n","11bb9259":"ridge_preds = train_pred('bayesian_ridge', df, test_df, features)\nridge_preds","ba38a4c3":"prep_sub(ridge_preds, 'bayesian_ridge')\n","56cfb346":"# For web scraping, charrrrgeeeee !!!!!\n\nimport requests\nfrom bs4 import BeautifulSoup as bs # Totally not BS\nimport re\nimport pandas as pd\nimport numpy as np\nimport itertools","1ccbc112":"the_only_url_worth_scraping = \"https:\/\/www.gutenberg.org\/browse\/scores\/top#books-last1\"\nresponse = requests.get(the_only_url_worth_scraping)\nbook_soup_is_delicious = bs(response.text, 'html.parser')","bd6550b0":"# Extract book urls\nbooks_list = book_soup_is_delicious.ol.find_all('a', attrs={'class': None})\nbooks_list = [tag.attrs['href'] for tag in books_list \n              if tag.attrs['href'].startswith('\/ebooks\/')]\nbooks_lists = list(dict.fromkeys(books_list))\n\n# print(\"In total we have \" + str(len(books_list)) + \" books\") # Comment out afterwards\n# print(books_list) # Comment out afterwards","662261fa":"# Make the test dataset. This gonna take long af.\nj = 0\ntitle_dict = {}\n\ncsv_data = pd.DataFrame([], columns=['id', 'url_legal', 'license', 'excerpt'])\nfor book in books_list:\n    book_id = book[8:]\n\n    # HTTP link constructor because main site does not like being scraped so I used mirror.\n    url_html_book = \"https:\/\/gutenberg.pglaf.org\/\"\n    if len(book_id) == 1:\n        url_html_book += \"0\/\"\n    else:\n        for i in range(len(book_id) - 1):\n            url_html_book += book_id[i] + \"\/\"\n    url_html_book += book_id + \"\/\" + book_id + \"-h\/\" + book_id + \"-h.htm\"\n    \n    # Get all the paragraphs.\n    book_response = requests.get(url_html_book, headers={\"Accept\":\"text\/html;charset=utf-8\"})\n    single_book = bs(book_response.text, 'html.parser')\n    single_book_paragraphs = single_book.find_all('p', attrs={'class': None})\n    single_book_title = single_book.title.get_text().strip()\n    \n    if(single_book_title.lower().startswith(\"the project gutenberg ebook of \")):\n        title_dict.update({book_id: single_book_title[31:]})\n    else:\n        title_dict.update({book_id: single_book_title})\n\n    paragraphs = \"\"\n    \n    # Process each paragraph.\n    for paragraph in single_book_paragraphs:\n        text = paragraph.get_text().strip()\n        \n        short_text = text[:12]\n        short_text_lower = short_text.lower()\n        short_text_upper = short_text.upper()\n        \n        # Remove \"table of contents\" parts.\n        excluded_start = short_text_lower.startswith('chapter') or short_text_lower.startswith(\"drawn by\")\n        \n        # Remove couple of multiple capital letters.\n        excluded_start = excluded_start or short_text_upper.startswith(short_text)\n        \n        # Filter off licensing text.\n        excluded_start = excluded_start or short_text_lower.startswith(\"copyright\")\n        excluded_start = excluded_start or short_text_lower.startswith(\"gnu free\")\n        excluded_start = excluded_start or short_text_lower.startswith(\"note:\")\n        excluded_start = excluded_start or text.find(\"Project Gutenberg eBook\") >= 0\n        excluded_start = excluded_start or short_text_lower.startswith(\"produced by:\")\n        \n        # Ignore empty paragraphs and short ones. Also do not include chapter listings.\n        if len(text) > 50 and not excluded_start:\n\n            # Clean the paragraph.\n            text = text.replace('  ', '').replace('\\n', ' ').replace('\\r', '').strip() + \" \\n\"\n            paragraphs += text\n            \n            if(len(paragraphs.split(' ')) >= 175):\n                paragraphs = ' '.join(paragraphs.split(' ')[:175])\n                break\n    \n    if(len(paragraphs.split(' ')) >= 75):\n        csv_form = pd.DataFrame([[book_id, url_html_book, \"Public domain in the USA.\" , \n                                  paragraphs]], columns=['id', 'url_legal', 'license', 'excerpt'], index=[j])\n        csv_data = csv_data.append(csv_form)\n    j = j + 1\n    print(\"Progress: {} of {}\".format(j, len(books_list))) \n\ncsv_data.to_csv(\"guthenberg.csv\", index=True) # Raw data to be used for processing.","99a0d1bc":"# Results for guthenberg dataset.\n# But for some reason the pre-processing disliked a few of the excerpts so can result in less data processed.\ntest_dataset = CLRDataset(csv_data, train=False) \ntest_df = test_dataset.get_df()\ntest_df.fillna(0) # Ensure all NaNs or empty values are 0 since model only processes numbers.\n# test_df.to_csv(\"sample_gutenberg_dataset.csv\") \ntest_df.head()","d455de1c":"# Prepare to use model actually.\ntest_df[features] = scaler.transform(test_df[features])\nridge_preds = train_pred('bayesian_ridge', df, test_df, features)\n\n# Temp save bayesian ridge results.\n# ridge_preds.to_csv(\"model_results.csv\", columns = [\"bayesian_ridge\"], index=False)\nridge_preds","2180fd88":"# Make a list that matches submission format.\nguthenb_list = []\nridge_list = ridge_preds[\"bayesian_ridge\"].to_list()\ntest_ids = test_df[\"id\"].to_list()\nfor i in range(len(ridge_list)):\n    guthenb_list.append([test_ids[i], ridge_list[i], title_dict[test_ids[i]]])\n# print(guthenb_list)\nguthenb = pd.DataFrame(guthenb_list, columns=[\"id\", \"target\", \"title\"])\nguthenb.to_csv(\"guthenberg-results.csv\", index=False)","f234f9dc":"# Training\nDefine the model to train.","b0395e56":"# Data Preparation\nClean the data and get them ready for model training.","88f20dc1":"# Data Extraction\nGet the training and testing dataset.","433b4f6b":"# Data Scraping\nGet at least the top 100 results from a book site called Project Gutenberg.\n\nSince the site did not recommend scraping, I can do the cheeky and only get from this link: https:\/\/www.gutenberg.org\/browse\/scores\/top#books-last1\n\nSorry MSPs but I need to enable internet for this one.","78749236":"# Feature Check\nChecking for statistics for making the body of said book\/topics.","b442cec3":"# Modelling\nMake the model and train it."}}