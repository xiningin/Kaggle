{"cell_type":{"c503bb3f":"code","be37e680":"code","f762d451":"code","efd651b0":"code","8f595c1e":"code","85604c88":"code","049264b7":"code","3545f816":"code","57d61e21":"code","3f815b40":"code","b3518248":"code","016218a7":"code","bd30b431":"code","305a6b3f":"code","94c7ec8c":"code","29f483ed":"code","0eb280d1":"code","14a903f9":"code","ac571155":"code","0f998ac7":"code","682e028a":"code","cdd1f5fa":"code","372d9912":"code","a7b1ee19":"code","d8020d85":"code","53089742":"code","ce0ab808":"code","a81067f1":"code","4f955716":"code","4403cd90":"code","303b4a6e":"code","b664742f":"code","87b25f77":"code","f6ebaca9":"code","175063c3":"code","897801d4":"code","9ba69fd4":"code","595ced63":"code","3d363a45":"code","ff45ae51":"code","4150b9a5":"code","3862c4a9":"code","b06ffc3d":"code","4cc1347c":"code","ba8130f8":"code","93a32b76":"markdown"},"source":{"c503bb3f":"'''\n\nWe can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.\n\nWe can\u2019t say blindly what type of feature extraction gives better results. One more thing is building word embeddings from our dataset or corpus will give better results. But we don\u2019t always have enough size of data set so in that case we can use pre-trained models with transfer learning\n\n'''","be37e680":"# Packages installed\n\n!pip install xmltodict #Used for transforming xml to dictionary\n# !pip install pycountry-convert #Optional package(Used to map the Location with ISO coutry codes)\n\n\n!pip install pyspellchecker","f762d451":"#Libraries used\nimport os\nimport string\nimport xmltodict\nimport json #json file type analysis\nimport pandas as pd #data analysis\nimport numpy as np #data manipulation\nfrom datetime import date ## date fields transformstions\nfrom dateutil.relativedelta import relativedelta\nimport re\n\n#data visualization\nimport matplotlib\nimport matplotlib as mpl\nmpl.rcParams['agg.path.chunksize'] = 10000\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Plotly - Interactive Graphs\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\nimport nltk #text processing\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn import preprocessing\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD\n\n\n\n##tensorflow >2.0\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\n\n\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import LSTM\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport tensorflow as tf\nimport time\n\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nfrom spellchecker import SpellChecker","efd651b0":"# Input paths\nglove_50d_pretrained_dic_path = '\/kaggle\/input\/glove6b50d\/glove.6B.50d.txt'\ninput_dir_path = '\/kaggle\/input\/stackoverflowmeta'\nusers_data_file_path = os.path.join(input_dir_path, 'Users.xml')","8f595c1e":"## Function to extract data from xml and save it to a dataframe\ndef xml_to_df(xml_path,root_tag_name, row_tag_name):\n    \n    with open(xml_path) as xml_file:\n        data_dict = xmltodict.parse(xml_file.read())\n    json_data = json.dumps(data_dict)\n    json_row_data = json.loads(json_data)\n\n    return pd.DataFrame(json_row_data[root_tag_name][row_tag_name])\n    \ndata_df = xml_to_df(users_data_file_path, 'users', 'row')\ndata_df.head(5)","85604c88":"## Function to convert datatypes for the list of columns in a given dataframe\ndef convert_dtype(df,col_list,data_type):\n    for col in col_list:\n        df[col] = df[col].astype(data_type)\n    return None\n\n# Rename the columns by replacing the special charater @ with empty string\ncolumn_names = [col.replace('@','') for col in data_df.columns]\ndata_df.columns = column_names\n\n# Identifying list of different data types columns list\nnumeric_cols = ['Reputation', 'Views', 'UpVotes', 'DownVotes' ]\n\n# Apply Date datatype conversion\nconvert_dtype(data_df, ['CreationDate','LastAccessDate'], 'datetime64[ns]')\n\n# Apply Numeric datatypes conversion\nconvert_dtype(data_df, numeric_cols, 'int')","049264b7":"# Months duration between createdate and lastaccesssdate\ndata_df['MonthsDuration'] = ((data_df.LastAccessDate - data_df.CreationDate)\/np.timedelta64(1, 'M')).astype(int)\n\n# Derive years from date features\ndata_df['CreationDateYear'] = data_df['CreationDate'].dt.year\ndata_df['LastAccessDateYear'] = data_df['LastAccessDate'].dt.year\n\ndata_df.tail(2)","3545f816":"## Cleansing text field \ndef text_cleansing(text):\n\n    # Convert text content to lower case \n    text = text.lower()\n    text = text.replace('\\n','').replace('[','').replace(']','')\n\n    # Removal of URLs\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n\n    # Removal of Hash Tags\n    html_pattern = re.compile('<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    # Removal of Punctuations\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Removal of stop words\n    cached_stop_words = stopwords.words(\"english\") \n    text = ' '.join([word for word in text.split() if word not in cached_stop_words])\n\n    # Replacing unwanted text\n    text = text.replace('yeah yeah yeah','')\n\n    return text\n                          \n## Function to apply basic cleansing on the features\ndef apply_data_cleansing(df):\n    \n    # Drop the first row since it is created by bot\n    df = df.drop(df[df[\"Id\"] == '-1'].index)\n\n    # Drop the unused features \n    df = df.drop(['AccountId', 'DisplayName', 'WebsiteUrl', 'Location', 'ProfileImageUrl' ], axis=1)\n\n    # Drop the null values (this will get rid of Memory errors for the feature 'AboutMe')\n    df = df.drop(df[df['AboutMe'].isna() == True].index)\n\n    # Appy cleansing on text feature\n    df['AboutMe'] = df['AboutMe'].apply(lambda x: text_cleansing(x))\n                           \n    # New feature to get the total number of words in text feature content\n    df[\"NumWords\"] = df[\"AboutMe\"].apply(lambda x: len(str(x).split()))\n    \n    # Drop the rows whose total words are Zero \n    df = df.drop(df[df[\"NumWords\"] == 0].index)\n\n    # Also drop the words whose length is 1 \n    df = df.drop(df[df[\"NumWords\"] == 1].index)\n    \n    return df","57d61e21":"# Apply data cleansing\ntext_df = data_df.copy()\ntext_df = apply_data_cleansing(text_df)\nprint(f'after cleaning the text (AboutMe feature) - total records will be  {text_df.AboutMe.shape} out of {data_df.AboutMe.shape}');\n\ntext_df = text_df.reset_index()\ntext_df.tail(2);","3f815b40":"## Function to apply pretrained GloVe word embedding dictionary\ndef apply_glove_embedding(df):\n\n    def get_unkwn_embedding_from_sentence(sentence):\n        words=[word.lower() for word in sentence.split()]\n        unknown_embedding = []\n        for word in words:\n            emb_vec=embedding_dict.get(word)\n            if emb_vec is  None:\n                unknown_embedding.append(word)\n\n        return unknown_embedding\n    \n    def get_mean_vector_from_sentence(sentence):\n        words=[word.lower() for word in sentence.split()]\n        sent_embedding = []\n        for word in words:\n            emb_vec=embedding_dict.get(word)\n            if emb_vec is not None:\n                sent_embedding.append(emb_vec)\n\n        vec = np.array(sent_embedding)\n        result_vector = np.mean(vec, axis=0)\n        return result_vector    \n    \n    embedding_dict={}\n    with open(glove_50d_pretrained_dic_path,'r') as f:\n        for line in f:\n            values=line.split()\n            word=values[0]\n            vectors=np.asarray(values[1:],'float32')\n            embedding_dict[word]=vectors\n    f.close()\n    \n    df['Text2MeanVec'] = df.AboutMe.apply(lambda x : get_mean_vector_from_sentence(x));\n    df['UnkwnEmbedding'] = df.AboutMe.apply(lambda x : get_unkwn_embedding_from_sentence(x))\n    df['NumUnkwnEmb'] = df[\"UnkwnEmbedding\"].apply(lambda x: len(x))\n    df['AppliedEmbedding'] = df['NumWords'] - df['NumUnkwnEmb']\n    \n    \n    plt.figure(figsize=[25,10]);\n    plt.plot(df.NumWords, \"-r\", label=\"TotalNumWords\")\n    plt.plot(df.AppliedEmbedding, \"-w\", label=\"AppliedEmbedding\")\n    plt.legend(loc=\"upper left\")\n    plt.title('Below plot shows how far the applied embeddings w.r.to total words',fontsize=14)\n    plt.show();\n    \n    return df\n    \n    ","b3518248":"glove_text_df = text_df.copy()\nglove_text_df =  apply_glove_embedding(glove_text_df);","016218a7":"glove_text_df.tail(4)","bd30b431":"## Function to split mean vectors into features\ndef get_text_vec_as_features(df, n_dim):\n\n    # filter with non na vectors\n    df = df[df['Text2MeanVec'].notna()]\n\n    t_vec_nd = np.array(df['Text2MeanVec'])\n    v_stack_vec = np.vstack(t_vec_nd)\n    v_stack_vec_df = pd.DataFrame(v_stack_vec )\n    v_stack_vec_df.columns = [f'v_{i}' for i in range(0,n_dim)]\n\n    m_df = pd.concat([v_stack_vec_df, df], axis=1, join=\"inner\")\n\n    # dropped unwanted cols \n    m_df = m_df.drop(['NumWords', 'Text2MeanVec','UnkwnEmbedding', 'AppliedEmbedding','AboutMe','index', 'Id', 'NumUnkwnEmb'], axis = 1) \n    \n    return m_df\nm_df = get_text_vec_as_features(glove_text_df, 50)","305a6b3f":"m_df.tail(4)\n","94c7ec8c":"# function to calculate & apply z-score Normalization\ndef apply_zscore(df , feature):\n    _mean = df[feature].mean()\n    _std = df[feature].std()\n    df[feature] = (df[feature] - _mean) \/ _std\n    return df[feature]\n\nstd_md_df = m_df.copy()\nstd_md_df[['CreationDateYear']] = apply_zscore(std_md_df, 'CreationDateYear')\nstd_md_df[['Reputation']] = apply_zscore(std_md_df, 'Reputation')\nstd_md_df[['Views']] = apply_zscore(std_md_df, 'Views')\nstd_md_df[['UpVotes']] = apply_zscore(std_md_df, 'UpVotes')\nstd_md_df[['DownVotes']] = apply_zscore(std_md_df, 'DownVotes')\nstd_md_df[['LastAccessDateYear']] = apply_zscore(std_md_df, 'LastAccessDateYear')\nstd_md_df.tail()","29f483ed":"# Applying  MinMax scalling technique to independent features\nmms = MinMaxScaler()\nmin_max_md_df = m_df.copy()\nmin_max_md_df[['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDateYear', 'LastAccessDateYear' ]] = mms.fit_transform(min_max_md_df[['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDateYear', 'LastAccessDateYear']])\nmin_max_md_df.tail()","0eb280d1":"## Function to split the data into train & test datasets\ndef split_features(df):\n    # independent features\n    X = df.drop(['MonthsDuration','CreationDate', 'LastAccessDate'], axis = 1) \n\n    # dependent feature\n    y = df['MonthsDuration']\n    \n    \n    # random state (seed value - it could be any integer) used to train the same order of the data points every time\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n    print(\"X Train Shape: \",X_train.shape)\n    print(\"X Test Shape: \",X_test.shape)\n    print(\"y Train Shape: \",y_train.shape)\n    print(\"y Test Shape: \",y_test.shape)\n\n    \n    # reshaping X\n    X = np.array(X_train).reshape(X_train.shape[0],X_train.shape[1],1)\n    print(f'after reshape of X : {X.shape}')\n    \n    y = y_train\n    print(f'y shape : {y.shape}')\n          \n    return X, y, X_test, y_test","14a903f9":"# split the data\nprint('z-score normalization applied train & test split')\nX_z_score, y_z_score, X_test_z_score, y_test_z_score = split_features(std_md_df)\nprint('\\nmin max normalization applied train & test split')\nX_mms, y_mms, X_test_mms, y_test_mms = split_features(min_max_md_df)","ac571155":"## Function to define model with tunable parameters\ndef stack_model(X, y, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM):\n       \n    # Defined the model layers\n    model_lstm = Sequential()\n    \n    # First LSTM layer \n    model_lstm.add(LSTM(units=50, input_shape=( X.shape[1],1 ), return_sequences=True, **LSTM_PARAM_L1))\n    \n    # Second LSTM layer\n    model_lstm.add(LSTM(units=50, return_sequences=True))\n    \n    # Third LSTM layer\n    model_lstm.add(LSTM(units=50))\n \n    # The output layer\n    model_lstm.add(Dense(units=1))\n    \n    # Compiling the RNN\n    model_lstm.compile(**LSTM_COMPILE_PARAM)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=30, shuffle=False)\n    # del X,y; gc.collect()\n    print(\"X Train Shape: \",X_train.shape)\n    print(\"X Valid Shape: \",X_valid.shape)\n    print(\"y Train Shape: \",y_train.shape)\n    print(\"y Valid Shape: \",y_valid.shape)\n\n    callbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=3,mode='auto')]\n    hist = model_lstm.fit(X_train, y_train,\n                          validation_data=(X_valid, y_valid),\n                          callbacks=callbacks_list,\n                          **LSTM_FIT_PARAM)\n\n    # Model Evaluation Parameters\n    best = np.argmin(hist.history[\"val_loss\"])\n    print(\"Optimal Epoch: {}\",best)\n    print(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))\n    \n    return model_lstm, best, hist.history[\"loss\"][best], hist.history[\"val_loss\"][best]","0f998ac7":"## Function to merge all the given dictionaries into one dictionary\ndef merge_lstm_dict(dict1, dict2, dict3):\n    return {**dict1, **dict2, **dict3}","682e028a":"print('Model Results when applying below parameters on z-score normalised data with Adam Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp1, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)","cdd1f5fa":"# Appending tunnable parameters results into a dataframe\nLSTM_TUNING_PARAMS['test_loss'] = test_loss\nLSTM_TUNING_PARAMS['val_loss'] = val_loss\nLSTM_TUNING_PARAMS['optimal_epoch'] = optimal_epoch+1\nLSTM_TUNING_PARAMS['normalization'] = 'z-score'\nhp_results_df = pd.DataFrame([LSTM_TUNING_PARAMS], index=['Exp1'])","372d9912":"hp_results_df","a7b1ee19":"## function to update the hyper parameter values\ndef update_hyper_params_in_df(hp_results_df, normalization,optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS):\n    LSTM_TUNING_PARAMS['test_loss'] = test_loss\n    LSTM_TUNING_PARAMS['val_loss'] = val_loss\n    LSTM_TUNING_PARAMS['optimal_epoch'] = optimal_epoch+1\n    LSTM_TUNING_PARAMS['normalization'] = normalization\n    hp_results_df = hp_results_df.append(LSTM_TUNING_PARAMS, ignore_index=True)\n    return hp_results_df\n\n    ","d8020d85":"print('Model Results when applying below parameters on min-max normalised data with Adam Optimizer');\n\ntrained_model_exp1_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS)","53089742":"hp_results_df\n","ce0ab808":"print('Model Results when applying below parameters on z-score normalised data with RMSprop Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'RMSprop',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp2, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on Min-Max normalised data with RMSprop Optimizer');\ntrained_model_exp2_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch,test_loss, val_loss, LSTM_TUNING_PARAMS) ","a81067f1":"print('Model Results when applying below parameters on z-score normalised data with SGD Optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'SGD',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp3, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with SGD Optimizer');\ntrained_model_exp3_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n","4f955716":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with Adam optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp4, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with Adam optimizer');\ntrained_model_exp4_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n","4403cd90":"hp_results_df\n\n\n","303b4a6e":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with RMSProp optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'RMSprop',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp5, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with RMSProp optimizer');\ntrained_model_exp5_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","b664742f":"hp_results_df\n","87b25f77":"print('Model Results when applying below parameters on z-score normalised data with MAE loss function with SGD optimizer');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'normal'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'SGD',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\n\ntrained_model_exp6, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with MAE loss function with SGD optimizer');\ntrained_model_exp6_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","f6ebaca9":"hp_results_df","175063c3":"print('Model Results when applying below parameters on z-score normalised data with uniform weights initialization');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'uniform'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp7, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with uniform weights initialization');\ntrained_model_exp7_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n","897801d4":"hp_results_df","9ba69fd4":"print('Model Results when applying below parameters on z-score normalised data with uniform weights initialization with MAE metric');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'uniform'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mae',\n              \"metrics\":[tf.keras.metrics.MeanAbsoluteError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp8, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n\nprint('Model Results when applying below parameters on min-max normalised data with uniform weights initialization with MAE metric');\ntrained_model_exp8_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) \n","595ced63":"hp_results_df","3d363a45":"print('Model Results when applying below parameters on z-score normalised data with uniform weights initialization with MSE metric');\nLSTM_PARAM_L1 = {\"activation\":\"relu\",\n              \"dropout\":0.2,\n              \"kernel_initializer\":'uniform'}\n\nLSTM_COMPILE_PARAM = {\"optimizer\":'Adam',\n              \"loss\": 'mse',\n              \"metrics\":[tf.keras.metrics.RootMeanSquaredError()]}\n\nLSTM_FIT_PARAM = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":30}\nLSTM_TUNING_PARAMS = merge_lstm_dict(LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\n\nprint(LSTM_TUNING_PARAMS);\ntrained_model_exp9, optimal_epoch, test_loss, val_loss = stack_model(X_z_score, y_z_score, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'z-score', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS)\n\nprint('Model Results when applying below parameters on min-max normalised data with uniform weights initialization with MSE metric');\ntrained_model_exp9_, optimal_epoch, test_loss, val_loss = stack_model(X_mms, y_mms, LSTM_PARAM_L1, LSTM_COMPILE_PARAM, LSTM_FIT_PARAM)\nhp_results_df=update_hyper_params_in_df(hp_results_df, 'min-max', optimal_epoch, test_loss, val_loss, LSTM_TUNING_PARAMS) ","ff45ae51":"hp_results_df\n","4150b9a5":"def plot_y_pred(title, trained_model, X_test, y_test):\n    # predicting target values by passing test data\n\n    y_pred = trained_model.predict(np.array(X_test).reshape(X_test.shape[0],X_test.shape[1],1))\n    plt.plot(y_test.values)\n    plt.plot(y_pred)\n    plt.title(title)\n    plt.savefig(f'{title}.png')\n    plt.show();\n    return y_pred","3862c4a9":"y_pred = plot_y_pred('Independent Features STD Dist(Z-Score) Scaling without dropout ratio 64 batch size ', trained_model_exp3, X_test_z_score, y_test_z_score)\n\ny_pred, y_test_z_score.values","b06ffc3d":"pred_df = std_md_df.loc[X_test_z_score.index][['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDate', 'LastAccessDate','MonthsDuration']]\n\npred_df['CreationDateYear'] = pred_df['CreationDate'].dt.year\npred_df['LastAccessDateYear'] = pred_df['LastAccessDate'].dt.year\n\npred_df['PredMonthsDuration'] = y_pred.astype('int')\n\npred_df['Diff'] = pred_df['PredMonthsDuration'] - pred_df['MonthsDuration']\nresult_df = pred_df.groupby('LastAccessDateYear').sum()\nresult_df['Year'] = result_df.index\nr_df = result_df[['MonthsDuration','PredMonthsDuration']]\nr_df.columns = ['GroundTruth', 'Predicted']\n\n","4cc1347c":"matplotlib.style.use('fivethirtyeight') \nr_df.plot(kind=\"bar\")\nplt.title(\"Model Results when Z-Score Normalization is applied\", loc ='center', fontsize=16)\nplt.yticks([])\nplt.xticks(fontsize=10, rotation=0) \nplt.xlabel('Users Last Access Date grouped by year', fontsize=10)\nplt.savefig(f'Users Last Access Date grouped by year.png')\nplt.show();","ba8130f8":"frhg\n","93a32b76":"# To DO\n* Hyper Parameter tuning\n* Compare the MinMaxScalar & Z-score results and choose the best scaling\n* Convert predicted months into date format \n* Think about the solution for unknown words which are not embedding from Glove\n* Combine all the EDA & model together\n"}}