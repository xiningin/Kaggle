{"cell_type":{"a0ad5748":"code","5a248365":"code","dd60f888":"code","7bebf790":"code","79c872c4":"code","74c96f90":"code","7467b7d0":"code","246c0996":"code","1782b28f":"code","a0a80253":"code","fd4c30b6":"code","06ad72a7":"code","87b3f8fb":"code","1131438d":"code","9b829daa":"code","51eb1c47":"code","b1917c5c":"code","274fe59f":"code","3ba5a1f2":"code","f6b75684":"code","ef7d8ea6":"code","ddd338d4":"code","300ab06a":"code","32188899":"code","464d716b":"code","c014ef28":"code","48f90bd7":"code","6ae977fa":"code","c6735365":"code","50a0606d":"code","e05c28fc":"code","88e9f29f":"code","3cf13d79":"code","dc50b50a":"code","b52e77ca":"code","7b074588":"code","e3a23335":"code","d9d0b1c7":"code","63ec7275":"code","08fc2e95":"code","ffe10543":"code","2a193d74":"code","2681acea":"code","e63d3f2a":"code","0e28e8ba":"code","d9d4cea4":"code","ee9ce554":"code","b1897116":"code","64a070bc":"code","17a29c1c":"code","9904516b":"code","e96f3fac":"markdown","41e4c442":"markdown","21a11cf3":"markdown","90a59b86":"markdown","631bef95":"markdown","39e370a7":"markdown","e5058f44":"markdown","b9e562ea":"markdown","3e0ee824":"markdown","c42a623d":"markdown","78bbb12c":"markdown","e4b884fe":"markdown","4e1b5a61":"markdown","63b21aeb":"markdown","c8ef42f2":"markdown","2036d2be":"markdown","951acae2":"markdown","589a796a":"markdown","7e1a362b":"markdown","33398947":"markdown","970b197c":"markdown","9687ad05":"markdown","d0c1d283":"markdown","dad19099":"markdown","911e7a4c":"markdown","9db98961":"markdown","5ad571bd":"markdown","4a3257b7":"markdown","e63555c3":"markdown","58843c6b":"markdown","6e217a8e":"markdown","edf78a5c":"markdown","ca8756a8":"markdown","2641c69a":"markdown","54d96f1d":"markdown","55af762e":"markdown","3d4cefb0":"markdown","eed1f35a":"markdown","64299eb2":"markdown","3887287c":"markdown","0cca985a":"markdown","b4eee06c":"markdown","eb3f3b64":"markdown","73cf5717":"markdown","b0d80079":"markdown","00225020":"markdown","7723d8bf":"markdown","4d1d30a1":"markdown","0a07ebfa":"markdown","7870d893":"markdown","216dec5d":"markdown","f0b5fa4f":"markdown","656515bf":"markdown"},"source":{"a0ad5748":"Image_Size = 200 # Reduce image to lightned memory \nNb_Channels = 3\nImage_size = (Image_Size, Image_Size, Nb_Channels)\n\n# Training constants\nBATCH_SIZE = 32  # Nb images used at each train step - Good value to avoid gradient descent noise and memory saving\nEPOCHS = 10      # Nb of cycling training process using full training dataset\nNb_Classes = 133\n\n#working_Path contains directory struture of train\/dev\/test dataset \nWorking_Path = \"\/kaggle\/input\/\"\n#working_Out is a directory contains deep learning models computted\nWorking_Out = \"\/kaggle\/working\/\"","5a248365":"#for plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nimport keras\n\n# supprimer librairies inutiles\nfrom keras import backend as K\n#from keras import kernel\nfrom keras import regularizers\n#from keras import regularizers\n\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Flatten, Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization, Input\nfrom keras.layers import Dropout, GlobalAveragePooling2D\n\nfrom keras.callbacks import Callback, EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import ModelCheckpoint\n\nimport shutil\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.models import load_model\n\nfrom keras.applications import inception_v3\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor","dd60f888":"from sklearn.datasets import load_files \nfrom keras.utils import np_utils\nimport numpy as np\nfrom glob import glob #Rechercce de chemin de fichier \n\ndef load_dataset(path):\n    data = load_files(path)\n    dog_files = np.array(data['filenames'])\n    dog_targets = np_utils.to_categorical(np.array(data['target']), 133) # Convert array of classes (0..132) to 1-hot encoding (133 columns)\n    return dog_files, dog_targets","7bebf790":"train_files, train_targets = load_dataset(Working_Path +'train')","79c872c4":"valid_files, valid_targets = load_dataset(Working_Path +'valid')\ntest_files, test_targets  =  load_dataset(Working_Path +'test')","74c96f90":"#Search all directories in train \/DeepDogBreed\/train\/ \ndog_names = [item[20:-1] for item in sorted(glob(Working_Path + \"train\/*\/\"))] \n\n\n# Let's check the dataset\nprint('There are %d total dog categories.' % len(dog_names))\nprint('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\nprint('There are %d training dog images.' % len(train_files))\nprint('There are %d validation dog images.' % len(valid_files))\nprint('There are %d test dog images.'% len(test_files))","7467b7d0":"train_targets","246c0996":"NbFilesPerClass = np.sum(train_targets,axis=0)","1782b28f":"# Creation of dictionnary of weigth relative to class population \n \nClassWeigth = { i : NbFilesPerClass[i] for i in range(0, len(NbFilesPerClass) ) }\n\nClassWeigth","a0a80253":"from keras.preprocessing import image                  \nfrom tqdm import tqdm # tqdm display progress meter\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(Image_Size, Image_Size))\n    # convert PIL.Image.Image type to 3D tensor with shape ( 200, 200, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 200, 200, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths): #\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]  # tqdm display progress meter\n    return np.vstack(list_of_tensors)","fd4c30b6":"from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')\/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')\/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')\/255","06ad72a7":"train_tensors.shape","87b3f8fb":"X = train_tensors\n#X = test_tensors\n#X = valid_tensors\nX = X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3]) # flattening image dataset\n#X_norm = X #\/ 255. ## Areay done during tensor computatons\n#X = X - X.mean(axis=0)","1131438d":"sigma = np.dot(X, X.T)\/X.shape[1] #Correlation matrix","9b829daa":"U,S,V = np.linalg.svd(sigma) #Singular Value Decomposition","51eb1c47":"epsilon = 0.1 #Whitening constant, it prevents division by zero\n# ZCA Whitening matrix: U * Lambda * U' of size M x M\nZCAMatrix = U.dot(np.diag(1.0\/np.sqrt(S + epsilon))).dot(U.T).dot(X)\n","b1917c5c":"xZCAMatrix = np.dot(ZCAMatrix, X) # project X onto the ZCAMatrix","274fe59f":"def zca_matrix(inputs):\n    sigma = np.dot(inputs, inputs.T)\/inputs.shape[1] #Correlation matrix\n    U,S,V = np.linalg.svd(sigma) #Singular Value Decomposition\n    epsilon = 0.1                #Whitening constant, it prevents division by zero\n    \n    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0\/np.sqrt(S + epsilon)), U.T)) # [M x M]\n    return   ZCAMatrix\n    \n  #  ZCAMatrix = np.dot(np.dot(U, np.diag(1.0\/np.sqrt(S + epsilon))), U.T),\n#ZCAMatrix = np.dot(U,np.dot(np.diag(1.0\/np.sqrt(S + epsilon)), U.T)) # [M x M]\n\n\n    \n    #ZCA Whitening matrix\n    #eturn np.dot(ZCAMatrix, inputs)   #Data whitening","3ba5a1f2":"def zca_whitening_matrix(X):\n    \"\"\"\n    Function to compute ZCA whitening matrix (aka Mahalanobis whitening).\n    INPUT:  X: [M x N] matrix.\n        Rows: Variables\n        Columns: Observations\n    OUTPUT: ZCAMatrix: [M x M] matrix\n    \"\"\"\n    # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) \/ N\n    sigma = np.cov(X, rowvar=True) # [M x M]\n    #sigma = np.dot(X, X.T)\/X.shape[1] #Correlation matrix\n    # Singular Value Decomposition. X = U * np.diag(S) * V\n    U,S,V = np.linalg.svd(sigma)\n        # U: [M x M] eigenvectors of sigma.\n        # S: [M x 1] eigenvalues of sigma.\n        # V: [M x M] transpose of U\n    # Whitening constant: prevents division by zero\n    epsilon = 0,1\n    # ZCA Whitening matrix: U * Lambda * U'\n    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0\/np.sqrt(S + epsilon)), U.T)) # [M x M]\n\n    return ZCAMatrix","f6b75684":"X = np.array([[0, 2, 2], [1, 1, 0], [2, 0, 1], [1, 3, 5], [10, 10, 10] ]) # Input: X [5 x 3] matrix\nZCAMatrix = zca_whitening_matrix(X) # get ZCAMatrix\nZCAMatrix # [5 x 5] matrix\nxZCAMatrix = np.dot(ZCAMatrix, X) # project X onto the ZCAMatrix\nxZCAMatrix # [5 x 3] matrix","ef7d8ea6":"#X = np.array([[0, 2, 2], [1, 1, 0], [2, 0, 1], [1, 3, 5], [10, 10, 10] ]) Input: X [5 x 3] matrix\n#X = X_norm\n\n#ZCAMatrix = zca_whitening_matrix(X) # get ZCAMatrix\nZCAMatrix = zca_matrix(X)# [5 x 5] matrix\n#X_ZCA = np.dot(ZCAMatrix, X) # project X onto the ZCAMatrix\n#X_ZCA # [5 x 3] matrix","ddd338d4":"X_ZCA = np.dot(ZCAMatrix, X) # project X onto the ZCAMatrix\nX_ZCA # [5 x 3] matrix","300ab06a":"X_train_ZCA = xZCAMatrix","32188899":"X = valid_tensors\nX_valid = X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3]) # flattening image dataset\nX_valid_ZCA = np.dot(ZCAMatrix, X_valid) \n\nX = test_tensors\nX_test = X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3]) # flattening image dataset\nX_valid_ZCA = np.dot(ZCAMatrix, X_test) ","464d716b":"#tensor_zca = X_ZCA.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\ntrain_tensors_zca = X_ZCA.reshape(train_tensors.shape[0], Image_Size,Image_Size,3)\n#valid_tensors_zca = X_ZCA.reshape(valid_tensors.shape[0], Image_Size,Image_Size,3)\n#test_tensors_zca = X_ZCA.reshape(.shape[0], Image_Size,Image_Size,3)","c014ef28":"train_tensors = train_tensors_zca \n#valid_tensors = valid_tensors_zca\n#test_tensors = test_tensors_zca","48f90bd7":"def history_display(history):\n    # Retrieve a list of accuracy results on training and validation data\n    # sets for each training epoch\n    # accuracy and val_accuracy depends on compile option of the model\n    train_acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy'] \n\n    # Retrieve a list of list results on training and validation data\n    # sets for each training epoch\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    # Get number of epochs\n    epochs = range(len(train_acc))\n\n    # Plot training and\n    # validation accuracy per epoch\n\n    return train_acc, val_acc, train_loss, val_loss, epochs","6ae977fa":"def Test_eval(path_to_model):\n\n    model.load_weights(path_to_model)\n\n    # get index of predicted dog breed for each image in test set\n    dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n    \n    # report test accuracy\n    test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))\/len(dog_breed_predictions)\n\n    return test_accuracy\n","c6735365":"model = Sequential()\n\nmodel.add(Conv2D(16, (3, 3), padding='same', use_bias=False, input_shape=(Image_Size, Image_Size, 3)))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(32, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(128, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\n# Flatten feature map to a 1-dim tensor so we can add fully connected layers\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\n\n# Create a fully connected layer with ReLU activation and 512 hidden units\nmodel.add(Dense(512, activation='relu'))\n\n# Create output classiffication layer with sigmoid activation\nmodel.add(Dense(Nb_Classes, activation='softmax'))\n\n\nmodel.summary()","50a0606d":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau , EarlyStopping\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n#early_st = EarlyStopping(monitor='val_loss', patience=8) \n\n#Save best model\nModel_Path = Working_Out + '\/best_model_from_scratch_CNN.hdf5'\ncheckpointer = ModelCheckpoint(filepath= Model_Path,\n                               verbose=1, save_best_only=True)\nhistory_scratch = model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets),\n                            verbose = 1, epochs=EPOCHS ,batch_size=32, callbacks=[checkpointer])","e05c28fc":"train_acc, val_acc, train_loss, val_loss, epochs = history_display(history_scratch)\n\nplt.figure()\n\n# Plot training and validation accuracy per epoch\nplt.subplot(1, 2, 1)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.plot(epochs, train_acc, label='train')\nplt.plot(epochs, val_acc,  label='val')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Train + val accuracy')\n\n# Plot training and validation loss per epoch\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss ,  label='train')\nplt.plot(epochs, val_loss, label='val')\nplt.title('Train + val loss')\nplt.legend(loc='upper left', frameon=False)","88e9f29f":"accuracy = Test_eval(Model_Path)\n\nprint('Test precision: %.4f%%' % accuracy)","3cf13d79":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras import backend as K\n\nmodel = Sequential()\n\n# load model without classifier layers\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(Image_Size, Image_Size, 3))\n\nmodel.add(base_model)\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\n# Add anti-overfitting layer(s)\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(Nb_Classes, activation='softmax'))\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in base_model.layers[:251]:\n   layer.trainable = False\n#for layer in base_model.layers[250:]:\n #  layer.trainable = True\n\nmodel.summary()","dc50b50a":"# Use accuracy training to enable accuracy historic display)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=[\"accuracy\"])","b52e77ca":"#Save best model\nModel_Path = Working_Out + '\/weights_best_inception3_fc.hdf5'\ncheckpointer = ModelCheckpoint(filepath = Model_Path, verbose=1, save_best_only=True)\n\n\n# train the model on the new data for a few epochs\nhistory_inception3_fc =  model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets), verbose = 1,\n                            epochs=EPOCHS ,batch_size=8, callbacks=[checkpointer])","7b074588":"train_acc, val_acc, train_loss, val_loss, epochs = history_display(history_inception3_fc)\n\nplt.figure()\n\n# Plot training and validation accuracy per epoch\nplt.subplot(1, 2, 1)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.plot(epochs, train_acc, label='train')\nplt.plot(epochs, val_acc,  label='val')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Train + val accuracy')\n\n# Plot training and validation loss per epoch\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss ,  label='train')\nplt.plot(epochs, val_loss, label='val')\nplt.title('Train + val loss')\nplt.legend(loc='upper left', frameon=False)","e3a23335":"accuracy = Test_eval(Model_Path)\n\nprint('Test accuracy: %.4f% %' % accuracy)","d9d0b1c7":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\nmodel = Sequential()\n\n# create the base pre-trained model without importing last classificatiion layer\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(Image_Size, Image_Size, 3))\n\n# add a global spatial average pooling layer\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\n\n# let's add a fully-connected layer\nmodel.add(Dense(256, activation='relu'))\n\n# and a logistic layer -- let's say we have Nb_Classezs classes\nmodel.add(Dense(Nb_Classes, activation='softmax'))\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in base_model.layers[:249]:\n   layer.trainable = False\nfor layer in base_model.layers[249:]:\n   layer.trainable = True\n\nmodel.summary()","63ec7275":"# Use accuracy training to enable accuracy historic display)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=[\"accuracy\"])","08fc2e95":"#Save best weight model\nModelPath = Working_Out + '\/weights_best_inception3_pool.hdf5'\ncheckpointer = ModelCheckpoint(filepath = ModelPath, verbose=1, save_best_only=True)\n\n# train the model on the new data for a few epochs\nhistory_inception3_pool =  model.fit(train_tensors, train_targets,\n                                     validation_data=(valid_tensors, valid_targets), \n                                     verbose = 1,\n                                     epochs=EPOCHS ,\n                                     batch_size=BATCH_SIZE, \n                                     class_weight = ClassWeigth,\n                                     callbacks=[checkpointer])","ffe10543":"train_acc, val_acc, train_loss, val_loss, epochs = history_display(history_inception3_pool)\n\nplt.figure()\n\n# Plot training and validation accuracy per epoch\nplt.subplot(1, 2, 1)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.plot(epochs, train_acc, label='train')\nplt.plot(epochs, val_acc,  label='val')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Train + val accuracy')\n\n# Plot training and validation loss per epoch\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss ,  label='train')\nplt.plot(epochs, val_loss, label='val')\nplt.title('Train + val loss')\nplt.legend(loc='upper left', frameon=False)","2a193d74":"accuracy = Test_eval( ModelPath )\n\nprint('Test precision: %.4f%% ' % accuracy)","2681acea":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\nmodel = Sequential()\n\n# create the base pre-trained model without importing last classificatiion layer\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(Image_Size, Image_Size, 3))\n\n# add a global spatial average pooling layer\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\n\n# let's add a fully-connected layer\nmodel.add(Dense(256, activation='relu'))\n\n# and a logistic layer -- let's say we have Nb_Classezs classes\nmodel.add(Dense(Nb_Classes, activation='softmax'))\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in base_model.layers[:249]:\n   layer.trainable = False\nfor layer in base_model.layers[249:]:\n   layer.trainable = True\n\n#model.summary()","e63d3f2a":"# Use accuracy training to enable accuracy historic display)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=[\"accuracy\"])","0e28e8ba":"datagen = ImageDataGenerator( # Number of cumulated transformation applied to image during training\n   #No need to normaoize features as it has already been normalized betwenn 0..1\n # featurewise_center=True, # Set input mean to 0 over the dataset, feature-wise\n   # featurewise_std_normalization=True,  # Divide inputs by std of the dataset, feature-wise.\n    \n    # randomly rotate images (degrees, 0 to 30)\n    rotation_range=30,\n    zoom_range = 0.4, #for scalling\n  \t# (fraction of total width)\n    width_shift_range=0.3,#translation\n    height_shift_range=0.3,#translation\n    # randomly flip images horizontally\n    horizontal_flip= True, #90 degree rotation\n    vertical_flip = False #Not realistic flipping\n)","d9d4cea4":"#Save best weight model\nModelPath = Working_Out + '\/weights_best_inception3_pool_over2..hdf5'\ncheckpointer = ModelCheckpoint(filepath = ModelPath, verbose=1, save_best_only=True)\n\nCALLBACKS = [checkpointer  ]\n\n# Image generated = EPOCHS * STEPS_PER_EPOCH\nEPOCHS = 10\nSTEPS_PER_EPOCH = 2* len(train_tensors) \/ 32\n\n# fits the model on batches with real-time data augmentation:\nhistory_inception3_over = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=32),\n                                         validation_data=(valid_tensors, valid_targets),\n                                         steps_per_epoch=STEPS_PER_EPOCH,\n                                         epochs=EPOCHS ,\n                                         class_weight = ClassWeigth,\n                                         callbacks=CALLBACKS,\n                                         verbose = 1) ","ee9ce554":"# Drop-Based Learning Rate Decay\nimport math\nfrom keras.callbacks import LearningRateScheduler\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.0001 \n    drop = 0.5\n    epochs_drop = 1.0\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    return lrate\n\nlrate = LearningRateScheduler(step_decay,verbose=1)","b1897116":"def time_decay(epoch):\n    initial_lrate = 0.001\n    decay_rate = 0.01\n    new_lrate = initial_lrate\/(1+decay_rate*epoch)\n    return new_lrate\n\nlrate = LearningRateScheduler(time_decay,verbose=1)","64a070bc":"#Save best weight model\nModelPath = Working_Out + '\/weights_best_inception3_pool_over2..hdf5'\ncheckpointer = ModelCheckpoint(filepath = ModelPath, verbose=1, save_best_only=True)\n\n#Early stopping stop training when val accuracy doesn't increase for 6 epochs\nearly_st_accu = EarlyStopping(monitor='val_accuracy', patience=8) #7)\n\n# divide by 10 learning rate when accuracy stop increasing of 1e-4 for 4 epochs\n#reduce_lr_acc = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=5,  min_delta=1e-4, mode='max')\n#reduce_lr_acc = ReduceLROnPlateau(monitor='val_loos', factor=0.1, patience=7, min_lr=0.001)\n#reduce_lr_acc = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, min_lr=0.001,min_delta=1e-4, mode='max' )\n\nCALLBACKS = [checkpointer ,early_st_accu, lrate ]\n\n# Image generated = EPOCHS * STEPS_PER_EPOCH\nEPOCHS = 50\nSTEPS_PER_EPOCH = 2* len(train_tensors) \/ 32\n\n# fits the model on batches with real-time data augmentation:\nhistory_inception3_over = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=32),\n                                         validation_data=(valid_tensors, valid_targets),\n                                         steps_per_epoch=STEPS_PER_EPOCH,\n                                         epochs=EPOCHS ,\n                                   #      class_weight = ClassWeigth,\n                                         callbacks=CALLBACKS,\n                                         verbose = 1) ","17a29c1c":"train_acc, val_acc, train_loss, val_loss, epochs = history_display(history_inception3_over)\n\nplt.figure()\n\n# Plot training and validation accuracy per epoch\nplt.subplot(1, 2, 1)\nplt.subplots_adjust(hspace=0.4, wspace=0.4)\nplt.plot(epochs, train_acc, label='train')\nplt.plot(epochs, val_acc,  label='val')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Train + val accuracy')\n\n# Plot training and validation loss per epoch\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss ,  label='train')\nplt.plot(epochs, val_loss, label='val')\nplt.title('Train + val loss')\nplt.legend(loc='upper left', frameon=False)","9904516b":"accuracy = Test_eval( ModelPath) \nprint('Test accuracy: %.4f%%' % accuracy)","e96f3fac":"**Train \/ validation history Inception3 + Global average Pooling**","41e4c442":"* Test 1 : Inception pool + epoch = 10 , step train = 208, batch = 32 sans wal datagen + center + norm = **64,1 %** \n\n* Test 2 : Inception pool + epoch = 10 , step train = 208, batch = 32 sans wal datagen No center No norm = **76,9& %**\n\n* Test 3 : Inception pool + epoch = 10 , step train = 416, batch = 32 sans wal datagen No center No norm = **78,9 %**\n\n* Test 4 : Inception pool + epoch = 10 , step train = 416, batch = 32 sans wal datagen No center No norm + **class weighted**= **79,45 %**\n\n\n* Test 5 : Inception pool + epoch = 10 , step train = 816, batch = 32 sans wal datagen No center No norm = **80,1 %**\n","21a11cf3":"**Accuracy score (Nb layers unfrozen)Inception 3 + fully connected**\n\n*  **Top-2 layers** = **7,13 %**\n\n* **Top-1 layers** = 2,7 %**\n\n* **No layer**  = 2,7%**\n\nWe migth still increse by reducing overfitting and increasing number of epochs.","90a59b86":"\u00cfn Pooling model tranfer lerning model is followed by pooling layer\n\nIn the example below, the last 4 layers are unfrozen but it hasn't been optimised.","631bef95":"**Accuracy evalation of CNN model on test corpora**","39e370a7":"**Dataset sphape (Nb Images, size Image, Size Image, Nb colors)**","e5058f44":"## Pre-process the Data\n\nWhen using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n\n$$\n(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n$$\n\nwhere `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n\nThe `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $Image size \\times Image size$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n\n$$\n(1, 224, 224, 3).\n$$\n\nThe `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n\n$$\n(\\text{nb_samples}, 224, 224, 3).\n$$\n\nHere, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!","b9e562ea":"In a fully connected layer CNN transfer learning technique, toppest layers of model have to be unfrozen one by one until best performance reach.\n\nIn the example below, **fine-tuning** is done by **unfreezing the last 2** layers are unfrozen.","3e0ee824":"**Test evaluation Inception 3 + fully connected**","c42a623d":"**Accuracy score (Nb layers unfrozen)Inception 3 + Global pooling**\n\n* No layer  : **59,17 %**\n\n* Top-1     : **59,73%** %\n\n* Top 2     : **60,82%**\n\n* Top 2 + class weighted    : **65,48 %**\n\n\n","78bbb12c":"**Test acccuracy results with data augment new**","e4b884fe":"## Create a CNN to Classify Dog Breeds (from Scratch)\n\nThe images that will go into our convnet are 200 x 200 color images.\n\n**Architecture**. A stack 4 modules {convolution + batch normalisation (to avoid overfitting) + relu + maxpooling + dropout} modules. Convolutions operate on 3x3 windows and our maxpooling layers operate on 2x2 windows. Our first convolution extracts 16 filters, the following ones extract 32, 64 and 128 filters. MaxPooling is applied on 2 x 2 area.\nDropOut select 20 % of nodes of each layer for each training sample (to avoid overfitting).\n\n**NOTE**: This is a configuration that is widely used and known to work well for image classification. Also, since we have relatively few training examples for 130 classes, using just 4 convolutional modules keeps the model small, which lowers the risk of overfitting.","4e1b5a61":"**Data ponderation**\n\nAs training dataset is unbalased accross classes, During training, class must be weigthed relative to their different populations which is computed here.","63b21aeb":"Load image files with categories as subfolder names => return dictionnary values 'filenames' and 'target'   ","c8ef42f2":"**Transfer learning approach** always outperform **from scrath models**  ecxept you are a deep Learning expert.\n\n**Transfer learning approach** for image classification based on pre-trained **convolutional neural networks** are usually composed of 2 parts:\n\n1.  **Convolutional base**, which performs feature extraction\n2.  **Classifier**, which classifies the input image based on the features extracted by the convolutional base.\n\n---\n Some of the most popular are to build cassifier are:\n\n1.   **Fully-connected layers**. Standard approach is to stack to pre-trained model fully-connected layer(s) followed by a softmax activated layer which outputs the probability distribution over each possible class label.\n    \n2.   **Global average pooling**. Consist in adding to pre-trained model a global average pooling layer and feed its output directly into the softmax activated layer. \n\nwhatever you choose for transfer learning, **fine tuning** consist in unfreezing n-toppest layers one by one.","2036d2be":"## Why is the dataset interesting?\n\nThe task of assigning breed to dogs from images is considered exceptionally challenging. To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n\nBrittany | Welsh Springer Spaniel\n- | - \n<img src=\"http:\/\/machinememos.com\/assets\/images\/Brittany_02625.jpg\" width=\"100\"> | <img src=\"http:\/\/machinememos.com\/assets\/images\/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n\nIt was not difficult to find other dog breed pairs with only few inter-class variations (for instance, Curly-Coated Retrievers and American Water Spaniels).\n\nLikewise, labradors come in yellow, chocolate, and black. A vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed. \n\nWhen predicting between 133 breeds, a random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imbalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%. Hence, even an accuracy of 2-3% would be considered reasonable.\n\n","951acae2":"**Data augmentation** is a popular way in image classification to prevent overfitting. The concept is to simply apply slight transformations on the input images (shift, scale\u2026) to artificially increase the number of images.\n\n**Increasing number of epochs** is a natural way to increase performance and must be used with early stopping \n\n**Early stopping**: Early Stopping is a way to stop the learning process when you notice that a given criterion does not change over a series of epochs to save **computer power**. \n\n**Reduce learning rate during training**\nThis technique is quite interesting and can help your network. Even if you have an \u201cAdam\u201d or \u201cRMSProp\u201d optimizer, your network might get stuck at some point on a plateau. In such a case, it might be useful to reduce the learning rate and try to get those extra percent of accuracy. Indeed, a learning rate a bit too large might simply mean that you overshoot the minimum and are kind of stuck close to a minimal point.","589a796a":"**Test accurycy evaluation**\n**10,13 %** is not a bad performance, better than random guessing.","7e1a362b":"**Constant definitions**","33398947":" **ImageDataGenerator**\n\n* it applies a random transformation on the images you have and use the transformed images in real time during training. \n\n* So image generated will be small variations of original ones.\n\n* In each epoch it will provide slightly altered images (depending on your configuration). It will always generate new images, no matter how many epochs you have.\n\n* So in each epoch model will train on different images, but not too different. This should prevent overfitting and in some way simulates online learning.\n\n* So total of image generated = steps_per_epoch * epochs\n\n* Finally, during training  fit_generator will never never use original datataset just the transformed ones (step per epoch)*epoch\n\n* In the** next epxperiment** we **generate images images** for **10 Epochs** with **steps_per_epochs = 2* len(train_tensors) \/ 32** onbest model get so far(global average pooling + 2 unfrozen layers)","970b197c":"**Inception v3 + Fully connected**","9687ad05":"**Ways to normalize\/preprocess images for CNNs**\n\n1. Substraxct average of each input variables over training set be close to \n2. Standardized (sustract mean and divide by standard deviation) of each variable\n3. Whitten variables to uncorrelated them to improve training of adjacent pixels.\n   A[ scientific paper Pal & Sudeep (2016) ](https:\/\/ieeexplore.ieee.org\/document\/7808140\/)) shown that **Zero Component Analysis( ZCA) whittenig outperforms other technics**\n    \nAll those normalisations are done pixelwise over all the dataset for each channel.\n   ","d0c1d283":"* Covariance coputation","dad19099":"1. # **Data augmentation**","911e7a4c":"Dataset has to be vectorized in order to normalize data accros all images and colors","9db98961":"# Dog Breed Classification with Keras","5ad571bd":"We rescale the images by dividing every pixel in every image by 255.","4a3257b7":"## Dog Breed Dataset\nThe data consists of 8351 dog images. The images are sorted into 133 directories, each directory contains only images of a single dog breed. Hopefully, the dataset will stay here: https:\/\/s3-us-west-1.amazonaws.com\/udacity-aind\/dog-project\/dogImages.zip If the url is not available, feel free to contact me. Ok, let's load the dataset.","e63555c3":"**Best model available ever from begining of notebook**","58843c6b":"Reshapping from ftatten images to tensors ","6e217a8e":"The number of breeds (**133**) is too high compared to the number of images we have in training set (**6680**) . So, the number of images par label is too small (**roughly 80)**. Thus, it's not enough to train a neural network. It will overfit the images and so the error on the test set will be high. We might employ different strategies to deal with this problem:\n\n*    **Use a pre-trained neural net** and modify it to fit in our label problem.\n*  **Label dimension reduction**: we reduce the number of breeds by grouping different labels under the same name. This could work, however, I am not an expert in dogs, and so I can't figure out the good clusters of breeds.\n*   **Use other database to get more data** (example: ImageNet): problem we are not sure these images are already coming from ImageNet, so we won't add any new data.\n*  **Data augmentation of the training set**: we apply several transformation in the training set images in order to create new images with the same label. It will make the CNN more robust to any small variation in the images.  \n\n*   Use **antioverfitting technics** (like** or **reduce learning rate**w during training)\n*   **Use a combination of them**","edf78a5c":"**Inception v3 + Pooling layer model + unfrozen layers**","ca8756a8":"**Handling unbalance dataset**","2641c69a":"**Learning rate has to be smaller with transfer rather than from model from scratch**","54d96f1d":"* **Libraries import**","55af762e":"* **Test 1** :  Inception pool + epoch = 50 + early=6 , step = 208, batch = 32 no norm + no center datagen = **78,32%** \n\n* **Test 2**  Inception pool + epoch = 20 + early=4 , step = 416, batch = 32 no norm + no center datagen = **80,82%** \n\n* **Test 3:**  Inception pool + epoch = 50 + early=3 , step train = 416, batch = 32 no norm + no center datagen + norm = **79,76 %**\n* It is probably close to the results of expe same settings + epoch 10 because not enough data generated (416) between epochs\n\n* **Test 4:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen + Reduce(monitor='val_acc', factor=0.1, patience=5,  min_delta=1e-4, mode='max') = **77,26**\n \n* **Test 5:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen + Reduce(monitor='val_acc', factor=0.2, patience=5, min_lr=0.001) = **75,3**\n\n* **Test 6:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen + Reduce(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) = **76,71**\n\n* **Test 7:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen + Reduce(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) +early =7= **75,3**\n\n* **Test 8:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen + Reduce(monitor='val_acc', factor=0.1,  patience=10, min_lr=0.001) +early =7= **78,35**\n\n* **Test 9:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay(epoch): initial_lrate = 0.001 drop = 0.5 - epochs_drop = 10.0 - lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop)) **80,25**\n\n* **Test 10:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen time_decay: initial_lrate = 0.001 ; decay_rate = 0.01: new_lrate = initial_lrate\/(1+decay_rate*epoch)\n**76,98**\n\n* **Test 11:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay(epoch): initial_lrate = 0.001 drop = 0.5 - epochs_drop = 5.0 - **80,6**\n\n* **Test 12:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay: initial_lrate = 0.0001 drop = 0.5 - epochs_drop = 5.0 - **81,97**\n\n* **Test 12:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay: initial_lrate = 0.0001 drop = 0.5 - epochs_drop = 5.0 - **class weighted** - **82,47**\n\n* **Test 13:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay: initial_lrate = 0.0001 drop = 0.5  epochs_drop - 2.0 **81,47**\n\n* **Test 14:**  Inception pool + epoch = 50 + early = 3 , step = 416, batch = 32 no norm + no center datagen +step_decay: initial_lrate = 0.0001, drop = 0.5 epochs_drop = 1.0 - **76,14 %**","3d4cefb0":"# **CNN models building**\n","eed1f35a":"## **Improving perforformance by transfert Learning of Inception V3 model + fine tuning**","64299eb2":"**Training\/validation history display function**","3887287c":"    # Overcome overfitting with transfer learning, data augmentation, increase number of epochs while dereasing learning rate ","0cca985a":"**Test evalution of Inception3 with regulation technics**","b4eee06c":"2. **Increasing number of epochs (to 50)** might be a natural way to improve results, but  we need to **stop** **training** to **avoid** **overftting** and **waste of computation time**, when it doesn't improve any more.\n\n    In the next experiment we we increse number of epochs to 50 by introcing **\"early stopping**\" to stop training when **accuracy** doesn't improvemet for **6 epochs** keeping same settings of previous (1.).\n\n3. **Reduce learning rate during training**: Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. \n\n  Following numbers of experiments, step_decay with configuration beow appeared to get achieved bestt result **81,97 %**","eb3f3b64":"The dataset is already split into train, validation and test parts. As the training set consists of 6680 images, there are only 50 dogs per breed on average. That is really a rather small dataset and an ambitious task to do. The [cifar 10 dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html) for example contains 60000 images and only 10 categories. The categories are airplane, automobile, bird, cat, etc. Thus, objects to be classified are very different and therefore easier to classify. ","73cf5717":"During CNN training, images transmorned naturally are considered as new images.It is** one of the best technic against ovefitting.**\n\nSo, we can operate several operations to augment the number of images that make \n\n*   Scaling\n*   Translation : dog is not well in the middle of the picture\n*   Rotation(90 degree): we do it to traduce the fact the customer won't be perfectly vertical when taking the picture\n*   Adding Salt and Pepper noise:  add to mimic the unperfectness of a user's camera. \n*   Lighting condition: really important because the lightning changes a lot following the picture conditions. \n\nWith those kinds of transformations, CNNs take those transformed images as new images and we can augment drastically dataset size.","b0d80079":"# **Prerequirement to notebook usage**","00225020":"As already elaborated, designing a CNN architecture that achieves even 9,3% accuracy is not an easy task. The first thing you notice is that increasing the filters depth leads to better results, yet slower training.[Batch Normalization](https:\/\/arxiv.org\/abs\/1502.03167) seems not only to lead to faster training, but also to better results. I used the [source code of InceptionV3](https:\/\/github.com\/fchollet\/deep-learning-model_bases\/blob\/master\/inception_v3.py) as an example when configuring the batch normalization layers. As batch normalization allowed for the model to learn much faster and I added a fourth convolutional layer and further increased the filter depth. Then, I altered the max pooling layer to shrink the layers by a factor of x4 instead of x2. This drastically decreased the number of trainable params and increased the speed by which the model is learning. At the end I added Dropout, to decrease overfitting, as the network started to overfit after the 4th epoch.","7723d8bf":"# **Further improvment by reducting overfitting and other tuning techniques**","4d1d30a1":"******test evaluation of CNN from scrach**","0a07ebfa":"[](http:\/\/)\nThis notebook is as tutorial on Convolutional Neural Network transfer learning and anti-overfitting technics based on a Kaggle challenge: [Dog breed identification.](https:\/\/www.kaggle.com\/c\/dog-breed-identification) which consist of identifying 133 dog breeds with  6680 training data.\n\n\nSo I explore 4 modern technics of image classification to compare their different advantages.\n\n1.  I train a convolutional neural network from scratch and got pretty bab performance 9 %\n\n2.  I train with transfer learning and fine tuning using an Inception v3 model and pretty good improvement 60,81 %\n\n3.  Then, I train the model with data augmentation to cope with the small dataset 78,9 %\n4.  Finaly I use a regualrisation technic that dimminish learning rate to get further imrpovement 81,97 %\n","7870d893":"**History Inception 3 pooling + Regularisation**","216dec5d":"**Test Evaluation Inception3 + Global average ooling**","f0b5fa4f":"**Train \/ validation historic Incerption 3 + fully connected**","656515bf":"**Training history display of scratch CNN model**"}}