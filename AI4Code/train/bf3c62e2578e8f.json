{"cell_type":{"af3b982f":"code","acb7152e":"code","ab4983f6":"code","2a56cb5a":"code","c0b2b2a0":"code","f90f9cb1":"code","9a5e19b8":"code","5a14de8f":"code","ef66ee4f":"code","d4c30124":"code","f44a182e":"code","b4bf6504":"code","ab632503":"code","cdf929d7":"code","f8e259f2":"code","a61461c5":"code","33b92abb":"code","b49450ce":"code","f625896a":"code","cf536bee":"code","537e521b":"code","6f132b14":"code","2c393f9c":"code","db02ed5c":"code","aa3ca4c5":"code","47c9ba68":"code","224d065a":"code","4ec55e7d":"code","f4e49f6a":"code","eb06b12d":"code","09d8ae80":"code","bb525949":"code","c66879a4":"code","5e1081c2":"code","e0064449":"code","61f5f04d":"code","30d0d346":"code","a979901e":"code","b6dce417":"code","8e5b1082":"code","6bbe38f2":"code","95def363":"code","bd3644e7":"code","7ec91ebf":"code","c225da7c":"code","726ebb55":"code","7e5601b1":"code","59933d69":"code","0a69fa76":"code","e598ba22":"code","6a996618":"code","1196d8ae":"code","dc0b53ff":"code","ab29e86a":"code","6eb09bbc":"code","6fa6551c":"code","0eb3cf9d":"code","2ecf691a":"markdown","8e1cef6c":"markdown"},"source":{"af3b982f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score,log_loss\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acb7152e":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","ab4983f6":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","2a56cb5a":"df_sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","c0b2b2a0":"df_sample.head()","f90f9cb1":"df.head()","9a5e19b8":"y = df.target.values","5a14de8f":"random_seed=22","ef66ee4f":"xtrain, xvalid, ytrain, yvalid = train_test_split(df.text.values, y, \n                                                  stratify=y, \n                                                  random_state=random_seed, \n                                                  test_size=0.1, shuffle=True)","d4c30124":"tfidf = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfidf.fit(list(xtrain) + list(xvalid))\nxtrain_tfidf =  tfidf.transform(xtrain) \nxvalid_tfidf = tfidf.transform(xvalid)","f44a182e":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfidf, ytrain)\npredictions = clf.predict(xvalid_tfidf)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","b4bf6504":"df_test.shape","ab632503":"xtest_tfidf = tfidf.transform(list(df_test.text.values))\ntest_predictions = clf.predict(xtest_tfidf)\n# intialise data of lists. \ndata = {'id':df_test.id, \n        'target':test_predictions} \n# Create DataFrame \ndf_submit = pd.DataFrame(data)\ndf_submit.to_csv('submit_logistic.csv',index=False)","cdf929d7":"df_submit.head()","f8e259f2":"count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\ncount_vect.fit(list(xtrain) + list(xvalid))\nxtrain_cv =  count_vect.transform(xtrain) \nxvalid_cv = count_vect.transform(xvalid)","a61461c5":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_cv, ytrain)\npredictions = clf.predict(xvalid_cv)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","33b92abb":"clf = MultinomialNB()\nclf.fit(xtrain_tfidf, ytrain)\npredictions = clf.predict(xvalid_tfidf)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","b49450ce":"clf = MultinomialNB()\nclf.fit(xtrain_cv, ytrain)\npredictions = clf.predict(xvalid_cv)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","f625896a":"svd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfidf)\nxtrain_svd = svd.transform(xtrain_tfidf)\nxvalid_svd = svd.transform(xvalid_tfidf)\n\n# Scale the data obtained from SVD\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","cf536bee":"clf = SVC(C=1.0) # since we need  labels\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict(xvalid_svd_scl)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","537e521b":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfidf.tocsc(), ytrain)\npredictions = clf.predict(xvalid_tfidf.tocsc())\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","6f132b14":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_cv.tocsc(), ytrain)\npredictions = clf.predict(xvalid_cv.tocsc())\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","2c393f9c":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","db02ed5c":"clf = GradientBoostingClassifier(random_state=random_seed)\nclf.fit(xtrain_tfidf.tocsc(), ytrain)\npredictions = clf.predict(xvalid_tfidf.tocsc())\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","aa3ca4c5":"clf = GradientBoostingClassifier(random_state=random_seed)\nclf.fit(xtrain_cv.tocsc(), ytrain)\npredictions = clf.predict(xvalid_cv.tocsc())\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","47c9ba68":"clf = GradientBoostingClassifier(random_state=random_seed)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","224d065a":"\nlgb_param_grid = {\"max_depth\": [5,10,20],\n              \"learning_rate\": [0.01,0.1,0.5,1.],\n              \"gamma\":[0.1,0.5,1.0],\n              \"n_estimators\":[50,100,150]\n              }\n\nlgb_grid = GridSearchCV(estimator = LGBMClassifier(objective=\"binary\", \n                                                       boosting_type='gbdt',\n                                                            n_jobs=-1, eval_metric=\"auc\",\n                                                            silent=1,random_state=random_seed), \n                           param_grid = lgb_param_grid, \n                           scoring = \"roc_auc\",\n                           cv = 3, n_jobs = -1, verbose=2)\n\nlgb_grid.fit(xtrain_tfidf.tocsc(), ytrain)\n\npredictions = lgb_grid.predict(xvalid_tfidf.tocsc())\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","4ec55e7d":"\nlgb_param_grid = {\"max_depth\": [5,10,20],\n              \"learning_rate\": [0.01,0.1,0.5,1.],\n              \"gamma\":[0.1,0.5,1.0],\n              \"n_estimators\":[50,100,150]\n              }\n\nlgb_grid = GridSearchCV(estimator = LGBMClassifier(objective=\"binary\", \n                                                       boosting_type='gbdt',\n                                                            n_jobs=-1, eval_metric=\"auc\",\n                                                            silent=1,random_state=random_seed), \n                           param_grid = lgb_param_grid, \n                           scoring = \"roc_auc\",\n                           cv = 3, n_jobs = -1, verbose=2)\n\nlgb_grid.fit(xtrain_svd, ytrain)\n\npredictions = lgb_grid.predict(xvalid_svd)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","f4e49f6a":"mll_scorer = metrics.make_scorer(log_loss, greater_is_better=False, needs_proba=False)","eb06b12d":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","09d8ae80":"param_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': [ 'l1','l2']}","bb525949":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='neg_log_loss',verbose=10, n_jobs=-1, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfidf, ytrain)  # we can use the full data here but im only using xtrain\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","c66879a4":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='neg_log_loss',\n                                 verbose=10, n_jobs=-1, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfidf, ytrain)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","5e1081c2":"embeddings_index = {}\nf = open('..\/input\/glove-vectors\/glove.840B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    except:\n        print(word)\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","e0064449":"def sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())\n\n","61f5f04d":"for x in tqdm(xtrain):\n    print(x)\n    break","30d0d346":"xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]","a979901e":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","b6dce417":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict(xvalid_glove)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","8e5b1082":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict(xvalid_glove)\n\nprint (\"accuracy: %0.3f \" % accuracy_score(yvalid, predictions))","6bbe38f2":"scl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","95def363":"ytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","bd3644e7":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","7ec91ebf":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=10, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","c225da7c":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 80\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","726ebb55":"xtest_seq = token.texts_to_sequences(df_test.text.values)\nxtest_pad = sequence.pad_sequences(xtest_seq,maxlen=max_len)","7e5601b1":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","59933d69":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","0a69fa76":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=150, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","e598ba22":"test_predictions = model.predict(xtest_pad)","6a996618":"test_predictions = test_predictions.argmax(axis=-1)\ndata = {'id':df_test.id, \n        'target':test_predictions} \n# Create DataFrame \ndf_submit = pd.DataFrame(data)\ndf_submit.to_csv('submit_deep_learning.csv',index=False)","1196d8ae":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","dc0b53ff":"test_predictions = model.predict(xtest_pad)\ntest_predictions = test_predictions.argmax(axis=-1)\ndata = {'id':df_test.id, \n        'target':test_predictions} \n# Create DataFrame \ndf_submit = pd.DataFrame(data)\ndf_submit.to_csv('submit_LSTM.csv',index=False)","ab29e86a":"# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(Bidirectional(LSTM(300, dropout=0.5, recurrent_dropout=0.5)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","6eb09bbc":"test_predictions = model.predict(xtest_pad)\ntest_predictions = test_predictions.argmax(axis=-1)\ndata = {'id':df_test.id, \n        'target':test_predictions} \n# Create DataFrame \ndf_submit = pd.DataFrame(data)\ndf_submit.to_csv('submit_BILSTM.csv',index=False)","6fa6551c":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(GRU(300, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\nmodel.add(GRU(300, dropout=0.5, recurrent_dropout=0.5))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=200, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","0eb3cf9d":"test_predictions = model.predict(xtest_pad)\ntest_predictions = test_predictions.argmax(axis=-1)\ndata = {'id':df_test.id, \n        'target':test_predictions} \n# Create DataFrame \ndf_submit = pd.DataFrame(data)\ndf_submit.to_csv('submit_GRU.csv',index=False)","2ecf691a":"## Deep Learning","8e1cef6c":"## Word Vectors"}}