{"cell_type":{"d07800b8":"code","a15fca60":"code","2b329805":"code","1ff15564":"code","5deae486":"code","8097b1ea":"code","b6eeac0d":"code","42bedf65":"code","3003c170":"code","bcec1459":"code","30a271d5":"code","5b8e78ac":"code","223440e8":"code","3e29db72":"code","95b52b69":"code","ad89be5a":"code","19b10b0a":"code","731b1aac":"code","55250198":"code","1365b949":"code","643f5b43":"code","549f2482":"code","c2495b13":"code","9cba8e22":"code","8fa6e530":"code","74701187":"code","d9f4f9d8":"code","180c466d":"code","0a5075b7":"code","7cd27cd7":"code","eff47ef7":"code","9dce0b74":"code","92040b8d":"code","eb18baa2":"markdown","b1ebcc3b":"markdown","ec18a572":"markdown","8317a203":"markdown","7ee7e993":"markdown","18bb515d":"markdown","7c73ebf7":"markdown","fc1e3ae7":"markdown","e1fdaebd":"markdown","ddfe801f":"markdown","12a5edb2":"markdown","be1a7e94":"markdown","3db922ff":"markdown","1ddf10e2":"markdown","ba0eff29":"markdown","ce6969e1":"markdown","14d1e096":"markdown","ff26665d":"markdown","d4361e28":"markdown","b6f552a5":"markdown","f211a810":"markdown","49054982":"markdown","cb5a9aa0":"markdown","b1bb1e79":"markdown","55609c9f":"markdown","810eb9f8":"markdown","a813563f":"markdown","36ced729":"markdown","768a58cc":"markdown","4ed9ce05":"markdown","435ea1ea":"markdown","8e12d9e9":"markdown","45ed5fc2":"markdown"},"source":{"d07800b8":"# Import various libraries\/packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a15fca60":"titanic_train = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\ntitanic_train.head()","2b329805":"titanic_train.describe()","1ff15564":"titanic_test.describe()","5deae486":"fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20,14))\nsns.set_theme()\nsns.color_palette()\n\nsns.histplot(titanic_train['Age'], ax=axes[0,0], color='green')\nsns.histplot(titanic_train['Fare'], ax=axes[0,1])\nsns.histplot(titanic_train['SibSp'], ax=axes[1,0], color='red')\nsns.histplot(titanic_train['Parch'], ax=axes[1,1], color='orange')\nsns.histplot(titanic_train['Pclass'], ax=axes[2,0], color='purple')\nsns.histplot(titanic_train['Sex'], ax=axes[2,1], color='gray')","8097b1ea":"titanic_train['Survived'].value_counts().plot.bar()","b6eeac0d":"print(\"Train:\")\nprint(titanic_train.isnull().sum())\nprint()\nprint(\"Test:\")\nprint(titanic_test.isnull().sum())","42bedf65":"titanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].mean())\ntitanic_test['Age'] = titanic_test['Age'].fillna(titanic_test['Age'].mean())\n\ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna(titanic_train['Embarked'].mode()[0])\ntitanic_test['Fare'] = titanic_test['Fare'].fillna(titanic_test['Fare'].mean())\n\ntitanic_train.drop(columns=['Cabin'], inplace=True)\ntitanic_test.drop(columns=['Cabin'], inplace=True)","3003c170":"print(\"Train:\")\nprint(titanic_train.isnull().sum())\nprint()\nprint(\"Test:\")\nprint(titanic_test.isnull().sum())","bcec1459":"sns.displot(data=titanic_train, x='Age', hue='Survived')","30a271d5":"# Create age categories\nlabel_names = ['Infant', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\ncut_points =  [0, 5, 12, 18, 35, 60, 100]\n\ntitanic_train['Age_category'] = pd.cut(titanic_train['Age'], cut_points, labels=label_names)\ntitanic_test['Age_category'] = pd.cut(titanic_test['Age'], cut_points, labels=label_names)","5b8e78ac":"age_cats = titanic_train.pivot_table(index='Age_category', values='Survived')\nage_cats.plot.bar()","223440e8":"# Create dummy columns\ndef create_dummies(df, column_name):\n    dummies = pd.get_dummies(df[column_name], prefix = column_name)\n    df = pd.concat([df, dummies], axis=1)\n    return df\n    \ntitanic_train = create_dummies(titanic_train, 'Sex')\ntitanic_test = create_dummies(titanic_test, 'Sex')\n\ntitanic_train = create_dummies(titanic_train, 'Age_category')\ntitanic_test = create_dummies(titanic_test, 'Age_category')\n\ntitanic_train = create_dummies(titanic_train, 'Pclass')\ntitanic_test = create_dummies(titanic_test, 'Pclass')","3e29db72":"titanic_train.iloc[:, 11:].head()","95b52b69":"from sklearn.preprocessing import minmax_scale\n\nfor col in ['SibSp', 'Parch', 'Fare']:\n    titanic_train[col + '_scaled'] = minmax_scale(titanic_train[col])\n    titanic_test[col + '_scaled'] = minmax_scale(titanic_test[col])","ad89be5a":"titanic_train.iloc[:, 11:].head()","19b10b0a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression(random_state=1)\n\ncolumns = ['Age_category_Infant',\n       'Age_category_Child', 'Age_category_Teenager',\n       'Age_category_Young Adult', 'Age_category_Adult',\n       'Age_category_Senior', 'Fare_scaled', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male','SibSp_scaled', 'Parch_scaled']\n\nscores = cross_val_score(lr, titanic_train[columns], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","731b1aac":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=1)\n\nscores = cross_val_score(rf, titanic_train[columns], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","55250198":"import xgboost as xgb\n\nxgboost = xgb.XGBClassifier(random_state=1, verbosity=0, use_label_encoder=False)\n\nscores = cross_val_score(xgboost, titanic_train[columns], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","1365b949":"columns = ['Age_category_Infant',\n       'Age_category_Child', 'Age_category_Teenager',\n       'Age_category_Young Adult', 'Age_category_Adult',\n       'Age_category_Senior', 'Fare_scaled', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male','SibSp_scaled', 'Parch_scaled']\n\nlr = LogisticRegression()\nlr.fit(titanic_train[columns], titanic_train['Survived'])\ncoefficients = lr.coef_\nfeature_importance = pd.Series(coefficients[0], index=columns)\nfeature_importance.plot.barh()","643f5b43":"correlations = titanic_train[columns].corr()\nsns.set(style='white')\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.zeros_like(correlations, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11,9))\nsns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","549f2482":"def process_isalone(df):\n    df['family_size'] = df[['SibSp', 'Parch']].sum(axis=1)\n    df['isalone'] = 0\n    df.loc[(df['family_size'] == 0), 'isalone'] = 1\n    df = df.drop('family_size', axis=1)\n    return df\n\ntitanic_train = process_isalone(titanic_train)\ntitanic_test = process_isalone(titanic_test)","c2495b13":"columns.append('isalone')\n\nscores = cross_val_score(xgboost, titanic_train[columns], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","9cba8e22":"from sklearn.feature_selection import RFECV\n\ndef select_features(df, clf):\n    df = df.select_dtypes([np.number])\n    all_x = df.drop(['PassengerId', 'Survived'], axis=1)\n    all_y = df['Survived']\n    \n    selector = RFECV(clf, min_features_to_select=5, cv=10, scoring='accuracy')\n    selector.fit(all_x, all_y)\n    \n    best_columns = list(all_x.columns[selector.support_])\n    return best_columns\n\nselect_features(titanic_train.drop(columns=['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']), xgboost)","8fa6e530":"new_cols = ['Sex_female',\n 'Age_category_Infant',\n 'Age_category_Child',\n 'Age_category_Senior',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'SibSp_scaled',\n 'Fare_scaled']\n\nscores = cross_val_score(xgboost, titanic_train[new_cols], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","74701187":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(titanic_train[new_cols], \n                                                      titanic_train['Survived'], \n                                                      test_size=.20, \n                                                      random_state=1)\n\nxgboost = xgb.XGBClassifier(random_state=1, \n                            n_estimators = 1000, \n                            learning_rate = .05,\n                            max_depth = 4,\n                            verbosity = 0, \n                            use_label_encoder = False)\n\nxgboost.fit(x_train, y_train, \n            early_stopping_rounds = 5, \n            eval_metric = 'error', \n            eval_set = [[x_valid, y_valid]], verbose=0)\n\nscores = cross_val_score(xgboost, titanic_train[new_cols], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","d9f4f9d8":"from sklearn.model_selection import GridSearchCV\n\nclf = xgb.XGBClassifier(random_state=1, verbosity=0, use_label_encoder=False)\nhyperparameters = {\n    'n_estimators': [1000],\n    'learning_rate': [.25, .05, .1],\n    'max_depth': [3, 4, 5, 10]\n                   }\n\nfit_params={'early_stopping_rounds':15, \n            'eval_metric': 'error', \n            'eval_set':[[x_valid, y_valid]]}\n\ngs = GridSearchCV(clf, param_grid=hyperparameters, verbose=0, cv=10, scoring='accuracy')\ngs.fit(x_train, y_train, **fit_params, verbose=0)\n\nbest_params = gs.best_params_\nbest_score = gs.best_score_\n\nprint(best_params) \nprint(best_score)","180c466d":"xgboost = xgb.XGBClassifier(random_state=1, \n                            n_estimators = 1000, \n                            learning_rate = .25,\n                            max_depth = 4,\n                            verbosity = 0, \n                            use_label_encoder = False)\n\nxgboost.fit(x_train, y_train, \n            early_stopping_rounds = 5, \n            eval_metric = 'error', \n            eval_set = [[x_valid, y_valid]], verbose=0)\n\n\ndef save_submission(model, cols, filename='titanic_sub.csv'):\n    test_data = titanic_test[cols]\n    predictions = model.predict(test_data)\n    p_ids = titanic_test['PassengerId']\n    submission_df = {\"PassengerId\": p_ids,\n                    'Survived': predictions}\n    submission = pd.DataFrame(submission_df)\n    submission.to_csv(filename, index=False)\n    \nsave_submission(xgboost, new_cols)","0a5075b7":"sample = titanic_train.copy()\n\ndef create_features(df):\n    df['Father'] = 0\n    df.loc[(df['Sex_male'] == 1) & (df['Age'] >= 18) & (df['Parch'] >=1), 'Father'] = 1\n    \n    df['Mother'] = 0\n    df.loc[(df['Sex_female'] == 1) & (df['Age'] >= 18) & (df['Parch'] >=1), 'Mother'] = 1\n    \n    df['single_father'] = 0\n    df.loc[(df['Father'] == 1) & (df['SibSp'] == 0), 'single_father'] = 1\n    \n    df['single_mother'] = 0\n    df.loc[(df['Mother'] == 1) & (df['SibSp'] == 0), 'single_mother'] = 1\n    \n    df['has_SibSp'] = 0\n    df.loc[(df['SibSp'] >= 1), 'has_SibSp'] = 1\n    \n    df['parentless_child'] = 0\n    df.loc[(df['Age'] < 18) & (df['Parch'] == 0), 'parentless_child'] = 1\n    \n    df['childless_man'] = 0\n    df.loc[(df['Sex_male'] == 1) & (df['Age'] >= 18) & (df['Parch'] == 0), 'childless_man'] = 1\n    \n    df['childless_woman'] = 0\n    df.loc[(df['Sex_female'] == 1) & (df['Age'] >= 18) & (df['Parch'] == 0), 'childless_woman'] = 1\n    \n    df['family_size'] = df['SibSp'] + df['Parch']\n    df['family_size' + '_scaled'] = minmax_scale(df['family_size'])\n    \n    df['small_family'] = 0\n    df.loc[df['family_size'] <= 2, 'small_family'] = 1\n    \n    df['average_family'] = 0\n    df.loc[(df['family_size'] > 3) & (df['family_size'] < 7), 'average_family'] = 1\n    \n    df['large_family'] = 0\n    df.loc[df['family_size'] >= 7, 'large_family'] = 1\n    \n    df['Boy'] = 0\n    df.loc[(df['Sex_male'] == 1) & (df['Age'] < 18), 'Boy'] = 1\n    \n    df['Girl'] = 0\n    df.loc[(df['Sex_female'] == 1) & (df['Age'] < 18), 'Girl'] = 1\n    \n    df['Single_man'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_male'] == 1) & (df['family_size'] == 0), 'Single_man'] = 1\n    \n    df['Single_woman'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_female'] == 1) & (df['family_size'] == 0), 'Single_woman'] = 1\n    \n    df['Rich_man'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_male'] == 1) & (df['Pclass_1'] == 1), 'Rich_man'] = 1\n    \n    df['Middle-class_man'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_male'] == 1) & (df['Pclass_2'] == 1), 'Middle-class_man'] = 1\n    \n    df['Poor_man'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_male'] == 1) & (df['Pclass_3'] == 1), 'Poor_man'] = 1\n    \n    df['Rich_woman'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_female'] == 1) & (df['Pclass_1'] == 1), 'Rich_woman'] = 1\n    \n    df['Middle-class_woman'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_female'] == 1) & (df['Pclass_2'] == 1), 'Middle-class_woman'] = 1\n    \n    df['Poor_woman'] = 0\n    df.loc[(df['Age'] >= 18) & (df['Sex_female'] == 1) & (df['Pclass_3'] == 1), 'Poor_woman'] = 1\n    \n    df['Rich_boy'] = 0\n    df.loc[(df['Boy'] == 1) & (df['Pclass_1'] == 1), 'Rich_boy'] = 1\n    \n    df['Middle-class_boy'] = 0\n    df.loc[(df['Boy'] == 1) & (df['Pclass_2'] == 1), 'Middle-class_boy'] = 1\n    \n    df['Poor_boy'] = 0\n    df.loc[(df['Boy'] == 1) & (df['Pclass_3'] == 1), 'Poor_boy'] = 1\n    \n    df['Rich_girl'] = 0\n    df.loc[(df['Girl'] == 1) & (df['Pclass_1'] == 1), 'Rich_girl'] = 1\n    \n    df['Middle-class_girl'] = 0\n    df.loc[(df['Girl'] == 1) & (df['Pclass_2'] == 1), 'Middle-class_girl'] = 1\n    \n    df['Poor_girl'] = 0\n    df.loc[(df['Girl'] == 1) & (df['Pclass_3'] == 1), 'Poor_girl'] = 1\n    \n    df['paid_fare'] = 0\n    df.loc[df['Fare'] > 0, 'paid_fare'] = 1\n    \n    df['low_fare'] = 0\n    df.loc[df['Fare'] <= 8, 'low_fare'] = 1\n    \n    df['average_fare'] = 0\n    df.loc[(df['Fare'] > 8) & (df['Fare'] < 31), 'average_fare'] = 1\n    \n    df['high_fare'] = 0\n    df.loc[df['Fare'] >= 31, 'high_fare'] = 1\n    \n    return df\n    \ncreate_features(sample)\nsample.head()","7cd27cd7":"titanic_train = create_features(titanic_train)\ntitanic_test = create_features(titanic_test)\n\nselect_features(titanic_train.drop(columns=['Pclass', 'Age', 'Fare', 'SibSp', 'Parch', 'family_size']), xgboost)","eff47ef7":"new_cols = select_features(titanic_train.drop(columns=['Pclass', 'Age', 'Fare', 'SibSp', 'Parch', 'family_size']), xgboost)\nx_train, x_valid, y_train, y_valid = train_test_split(titanic_train[new_cols], \n                                                      titanic_train['Survived'], \n                                                      test_size=.2, \n                                                      random_state=1)\n\nclf = xgb.XGBClassifier(random_state=1, verbosity=0, use_label_encoder=False)\nhyperparameters = {\n    'n_estimators': [1000],\n    'learning_rate': [.1],\n    'max_depth': [2, 3, 4, 8]\n                   }\n\nfit_params={'early_stopping_rounds':15, \n            'eval_metric': 'error', \n            'eval_set':[[x_valid, y_valid]]}\n\ngs = GridSearchCV(clf, param_grid=hyperparameters, verbose=0, cv=10)\ngs.fit(x_train, y_train, **fit_params, verbose=0)\n\nbest_params = gs.best_params_\nbest_score = gs.best_score_\n\nprint(best_params) \nprint(best_score)","9dce0b74":"xgboost = xgb.XGBClassifier(random_state = 1, \n                            n_estimators = 1000, \n                            learning_rate = .1,\n                            verbosity = 0, \n                            use_label_encoder = False,\n                            objective = 'binary:logistic',\n                            max_depth = 3)\n\n\nscores = cross_val_score(xgboost, titanic_train[new_cols], titanic_train['Survived'], cv=10, scoring='accuracy')\nprint(np.mean(scores))","92040b8d":"xgboost.fit(x_train, y_train, \n            early_stopping_rounds = 5, \n            eval_metric = 'error', \n            eval_set = [[x_valid, y_valid]], verbose = 0)\n\n    \nsave_submission(xgboost, new_cols)","eb18baa2":"### 1\uff0eDealing with Missing Values\nMachine learning models are unable to use data with missing values, so if our data contains any we will have to deal with them.\n\nFirst, let's see if our data has any missing values.","b1ebcc3b":"![image.png](attachment:2c2b0bb8-5fe2-43b8-b4d3-6407baf8106c.png)","ec18a572":"As suspected, it looks like SipSp and Parch, as well asnFare_scaled and Pclass_1 are positively correlated.\n\nWhen we looked at the coefficients from the logistic regression model, variables related to family size had relatively large coefficients, but what about passengers who were alone?\n\nLet's add a variable to our data for passengers who were alone, and see how this affects our accuracy score.","8317a203":"After looking at the coefficients, it seems that being male, the number of family members onboard, low passenger class, and being an adult are associated with not surviving, while being female, high passenger class, high ticket fare, and being a child are associated with survival.\n\nLooking closely at the results, it seems that some phenomena are being explained by the same variable. SipSp and Parch seem to represent family size, and Fare_scaled and Pclass_1 seem to be explaining the socioeconomic status of the passenger.\n\nLet's check the correlation of the variables to see if this is happening.","7ee7e993":"## II. Model Creation and Validation\nWhen creating and validating a model, there are 3 main steps\uff1a\n1. Create an initial model and calculate a baseline score\n2. Perform feature engineering as necessary\n3. Tune model hyperparameters\n\nHowever, it is important to note that this is an iterative process, and it is necessary to re-calculate the model's score at various stages to compare with the baseline.\n\nWe will be using accuracy as a validation metric, which is the percent of values that were predicted correctly by the model.\n\nWe will also use a method called cross-validation to help us test our model.\nWhen validating a model, we first use one part of the data to train the model, and then we use the rest of the data to test the model. However, it's possible that the selection of data used for either training or testing contains sample bias, which would lead to us creating a sub-optimal model.\n\nTo prevent this, we can perform validation several times, each time using a different portion of the data for training and testing, and take an average of the scores to get a better idea of how our model is actually performing. This method is known as cross-validation, and can help us to create better models.","18bb515d":"We were able to raise our model's score with the addition of isalone.\n\nWe can continue to manually add and remove variables, but this isn't very efficient. Scikit-learn's feature_selection module includes recursive feature elimination, a method that iteratively removes the variables with the least explanatory power until an optimal score is reached.\n\nLet's apply this to our data and see if it improves our score.","7c73ebf7":"![image.png](attachment:2d927344-83ff-4aae-91d3-1e3339a8456b.png)","fc1e3ae7":"### 2\uff0eProcessing Categorical Variables\nFor people, when we see words like \"Male\", we are able to use our general knowledge and experience to infer the meaning. For a machine learning model however, it is unable to infer meaning or assign value to categorical variables like male, so it is necessary to convert it to a format that it can interpret.\nTwo frequently used methods for this are:\n\n1. One-hot encoding\n2. Ordinal encoding\n\nOne-hot encoding creates a new column for each unique value in the target column, with the values of the new column being either 0 or 1. For example, using one-hot encoding on the \"sex\" column would give us two new columns, sex_male and sex_female, with female passengers having a value of 1 in the sex_female column and a 0 in the sex_male column.\n\nOrdinal encoding is a method useful when the values in a column have an order or rank. This method doesn't increase the number of columns, but replaces the values of the target column with ordered values. We could use this method on the \"Pclass\" column, since we can say Pclass 1 is higher or \"better\" than Pclass 2 and 3. We could convert the values to Pclass 3 = 0, Pclass 2 = 1, Pclass 1 = 2, to indicate their order.","e1fdaebd":"Let's use these hyperparameters and make our first submission.","ddfe801f":"## I. Preprocessing\nBefore using our data in a machine learning model, we have to process it into a format that can be understood by the model.\nIn this analysis this will mainly consist of 3 steps:\n\n1. Dealing with missing values\n2. Dealing with categorical values\n3. Standardizing\/Normalizing the data","12a5edb2":"80%! Not bad for our first try!\n\nHowever, similar to linear regression, logistic regression is a very accurate model when the predictor variables and the target variable have a linear relationship, but the model's accuracy suffers when this is not the case.\n\nLet's try the tree-based Random Forest Classifier and Xgboost Classifier and see how they compare.","be1a7e94":"We were able to increase the score by 1.6%! The isalone column that we previously added isn't included, but for now we'll continue with the new columns.","3db922ff":"## 3. Hyperparameter Tuning\nUp until now we have mostly used feature selection and feature engineering to improve our model's score, but we can also achieve this by changing the settings (hyperparameters) of our model.\n\nHyperparameters vary depending on the model being used, and our current model, Xgboost Classifier, has quite a few hyperparameters that can be adjusted. We will focus on adjusting n_estimators, learning_rate, max_depth for simplicity.\n\n* n_estimators: The number of Decision Trees to be used in our model\n* learning_rate: A weighting factor for the corrections by new trees when added to the model\n* max_depth: The maximum depth for each Decision Tree\n\nAlso, to avoid overfitting our model to the training data, we will employ early stopping. Early stopping is a method that stops the training of the model when the accuracy score begins to drop in succession.","1ddf10e2":"After using recursive feature elimination, it seems that the combination of the above columns resulted in the best scoring model.\n\nLet's confirm this with cross-validation.","ba0eff29":"### 2. Feature Engineering\nWe've created our initial model and calculated a baseline accuracy score.\nBut if we want to create an even better model, we need to reduce our variables to the ones that best predict survival, and reduce the noise in our model.\n\nIn order to do this, first let's take a look at the coefficients from the logistic regression model we used previously to get an idea of the predictive power of each variable.","ce6969e1":"Categorical variables are not limited to just text data though; we can also convert numerical data to categorical data.\n\nFor our current data, a good example of where this might be useful is the \"Age\" column. It's easy to imagine how passengers might be treated differently based on their age category (i.e. child) rather than the actual number of their age, so this seems like a good candidate for conversion to a categorical variable.\n\nLet's plot the relationship between survival and age to check this theory.","14d1e096":"Based on the graph above, it seems that children were much more likely to survive than other age groups.\nNow that we have confirmed this relationship between survival and age, it seems that age will better explain survival as a categorical variable instead of a numeric one.\n\nLet's create a new column with 6 age categories.","ff26665d":"First, let's read in our data and see what we'll be working with.","d4361e28":"When I checked the data dictionary on the competition page, I found the following column definitions: \n* survival\uff1aWhether or not the passenger survived, 0 = No, 1 = Yes\n* pclass\uff1aPassenger class, with higher passenger classes representing higher socioeconomic status, 1 = 1st, 2 = 2nd, 3 = 3rd\n* sex\uff1aGender\n* Age\uff1aAge\n* sibsp\uff1aNumber of siblings\/spouses onboard the Titanic\n* parch\uff1aNumber of parents\/children onboard the Titanic\n* ticket\uff1aTicket number\n* fare\uff1aTicket fare\n* cabin\uff1aRoom number\n* embarked\uff1aPort of departure, C = Cherbourg, Q = Queenstown, S = Southampton\n\nNext, let's check the distribution of each variable.","b6f552a5":"### 3\uff0eStandardizing\/Normalizing Data\nOur final step in preprocessing is to standardize\/normalize our numerical data.\n\nWhen we look at numerical data, we have a rough understanding of how high or low a value is based on our knowledge of the units the data is in. For example, we know that 20 years old is not a particularly high value, but 20 siblings would be considered very high. For a machine learning model to be able to interpret this and not overemphasize high values, it is necessary to scale our data. Two frequently used mehtods for this are Standardization and Normalization.\n\nTo normalizing data, we convert the data to a scale where the maximum value is 1 and the minimum value is 0.\nTo standardize data, we transform the data to have a distribution with a mean of 0 and a standard deviation of 1.\n\nDepending on several factors, such as the distribution of your data and the models being used, the appropriate method to use can vary.\n\nIn many cases it is approptiate to use standardization, but for this analysis I decided to use normalization since the data contains several variables that do not follow a normal distribution.","f211a810":"It looks like we got rid of all the missing values. Now let's move on to processing the categorical variables.","49054982":"Now let's create dummy columns for our categorical data with one-hot encoding.","cb5a9aa0":"It looks like Xgboost Classifier has the best score out of all of the models.\n\nWe'll use Xgboost going forward, and consider our baseline to be .8137.","b1bb1e79":"Out of 50,255 entries, we ranked at 43,240. I think we can improve this, so let's continue.\n\nWe know that age, gender, and passenger class are good predictors of survival, but currently these variables are all separate columns. What if we combined some of these columns into features like Rich Man or Single Mother?\n\nLet's try and create variables that combine various columns and see if it raises the predictive ability of our model.","55609c9f":"# Titanic Passenger Survival Analysis\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we'll try to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (i.e. name, age, gender, socio-economic class, etc). \n\nCompetition Page:\nhttps:\/\/www.kaggle.com\/c\/titanic","810eb9f8":"### 1. Initial Model Creation\/Baseline Accuracy Calculation","a813563f":"In this analysis our goal is to predict whether a passenger survived or not, so it is a classification problem.\n\nThere are many models we can use for classification, but first let's calculate a baseline score using a logistic regression model.","36ced729":"We will take advantage of Grid Search for our hyperparameter tuning.\n\nGrid Search takes a model and several values for hyperparameters of our choosing, and iteratively searches for the combination of hyperparameters that produces the highest score.\n\nLike recursive feature elimination, this is much more efficient that adjusting the hyperparameters manually.","768a58cc":"Out of 50,255 entries, we placed at 3,604. This puts our model in the top 8%, not a bad score!\n\nIt seems that separating the passengers into smaller groups allowed us to better predict survival with our model.\n\nThanks for reading!","4ed9ce05":"It seems that most passengers were between 20-40 years old, and that there were about twice as many men as women. We can also see there were a few passengers with a very high ticket fare.","435ea1ea":"It looks like more passengers died than survived.","8e12d9e9":"When we run recursive feature elimination again we now have some new columns included.\n\nLet's tune our hyperparameters again with these new columns and make another submission.","45ed5fc2":"It looks like both Train and Test have quite a few missing values for the Age and Cabin columns. We can also see that Train's Embarked column has 2 missing values, and Test's Fare column has 1 missing value.\n\nThere are several ways to deal with missing values, but often the 2 methods below are used:\n1. Remove columns\/rows with missing values\n2. Replace missing values with a substitute value\n\nIf we remove rows\/columns that contain missing values, we also lose all the other data contained in that column\/row, so it is usually preferable to impute a replacement value instead of removing the row\/column entirely.\n\nThere are many options for replacement values, including the mean, mode, median, and even values predicted from a model.\n\nFor this analysis I decided to drop the Cabin column, since over 75% of the values are missing. For categorical variables I chose to impute the mode for the column, and for numerical columns the mean."}}