{"cell_type":{"8cab5885":"code","411f38c8":"code","7dad7c50":"code","3b2eda10":"code","5df3e871":"code","520a9ef9":"code","889ce9df":"code","f9678f90":"code","01d22f05":"code","4ea23dcb":"code","9c8cd85c":"code","104bc4a1":"code","52bf537a":"code","6cadc32b":"code","7e11f2a3":"code","39dbdd84":"code","3a4637a3":"code","d05df7e6":"code","3c5a2b1c":"code","d4804620":"code","345bc65b":"code","771098c3":"code","64224730":"code","23f1f985":"code","69a29cac":"code","c621ca63":"code","6c1dfda1":"code","c213a0da":"code","68934113":"code","47c785a1":"code","132d0dee":"code","a70ea1dd":"code","41d66dba":"code","1f8d612f":"code","0ba87a61":"code","2e6705c1":"code","c7dc3396":"code","bd0b8639":"code","e477b4d9":"code","32250c64":"code","b393d83e":"code","9c3b77e9":"code","c0376f6c":"code","339d0cfb":"markdown","6fc18966":"markdown","3e913a09":"markdown","6323b0fb":"markdown","2e1716a4":"markdown","9c8a74a9":"markdown","ab3e3a4f":"markdown","06561cdb":"markdown","00b6a557":"markdown","d89429b0":"markdown","b819e4ab":"markdown","71518e70":"markdown","4c62093d":"markdown","b2b1df32":"markdown","62f51b6a":"markdown","d72d8eba":"markdown","132b7558":"markdown","d700f318":"markdown","47a532c2":"markdown","5a47f28d":"markdown","1c3f4db1":"markdown","8227715b":"markdown","6539f641":"markdown","579e6ad3":"markdown","c1e2beeb":"markdown","869011a2":"markdown","47d84a65":"markdown","7f9270f8":"markdown","f20386f8":"markdown","86603b6e":"markdown","9cf02e48":"markdown","9cc40cb3":"markdown"},"source":{"8cab5885":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","411f38c8":"from collections import Counter\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBClassifier\nfrom imblearn.combine import SMOTETomek\nimport optuna","7dad7c50":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","3b2eda10":"train.shape, test.shape","5df3e871":"train.head()","520a9ef9":"test.head()","889ce9df":"train.info()","f9678f90":"# number of unique values\nfor col in train.columns:\n    print(f\"{col}: {train[col].nunique()}\")","01d22f05":"# numerical features\nnum_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n            'instrumentalness', 'liveness', 'loudness', 'speechiness', \n            'tempo', 'audio_valence',]","4ea23dcb":"def detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns list of indices containing more than n outliers\n    according to Tukey's rule.\n    Parameters:\n        df: Dataframe\n        n: min number of outliers a column should have\n        features: the columns of the dataframe to consider\n        \n    Returns:\n        the row indices containing outliers\n    \"\"\"\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3-Q1\n        \n        outlier_list = df[(df[col]<(Q1-(1.5*IQR))) | (df[col]>(Q3+(1.5*IQR)))].index\n        outlier_indices.extend(outlier_list)\n        \n    outlier_indices = Counter(outlier_indices)\n    outliers = (k for k, v in outlier_indices.items() if v>n)\n    return outliers","9c8cd85c":"to_drop = detect_outliers(train, 2, num_cols)","104bc4a1":"len(train.loc[to_drop])","52bf537a":"train.isnull().sum()","6cadc32b":"feat = num_cols + ['song_popularity']\ncorr = train[feat].corr()\ncorr.style.background_gradient(cmap='coolwarm')","7e11f2a3":"fig, ax = plt.subplots(2, 5, figsize=(16,8))\nsns.kdeplot(train['song_duration_ms'], color='blue',shade=True, ax = ax[0,0])\nax[0,0].set_title(\"Song Duration\")\nax[0,0].grid()\nsns.kdeplot(train['acousticness'], color='orange', shade=True, ax = ax[0,1])\nax[0,1].set_title(\"Acousticness\")\nax[0,1].grid()\nsns.kdeplot(train['danceability'], color='red', shade=True, ax = ax[0,2])\nax[0,2].set_title(\"Danceability\")\nax[0,2].grid()\nsns.kdeplot(train['energy'], color='yellow', shade=True, ax = ax[0,3])\nax[0,3].set_title(\"Energy\")\nax[0,3].grid()\nsns.kdeplot(train['instrumentalness'], color='green', shade=True, ax = ax[0,4])\nax[0,4].set_title(\"Instrumentalness\")\nax[0,4].grid()\nsns.kdeplot(train['liveness'], color='yellow', shade=True, ax = ax[1,0])\nax[1,0].set_title(\"Liveness\")\nax[1,0].grid()\nsns.kdeplot(train['loudness'], color='red', shade=True, ax = ax[1,1])\nax[1,1].set_title(\"Loudness\")\nax[1,1].grid()\nsns.kdeplot(train['speechiness'], color='blue', shade=True, ax = ax[1,2])\nax[1,2].set_title(\"Speechiness\")\nax[1,2].grid()\nsns.kdeplot(train['tempo'], color='orange', shade=True, ax = ax[1,3])\nax[1,3].set_title(\"Tempo\")\nax[1,3].grid()\nsns.kdeplot(train['audio_valence'], color='purple', shade=True, ax = ax[1,4])\nax[1,4].set_title(\"Audio valence\")\nax[1,4].grid()\nfig.tight_layout()\nplt.show()","39dbdd84":"sns.barplot(x=train['audio_mode'], y=train['song_popularity'])","3a4637a3":"sns.barplot(x=train['time_signature'], y=train['song_popularity'])","d05df7e6":"sns.barplot(x=train['key'], y=train['song_popularity'])","3c5a2b1c":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\nsns.countplot(train['time_signature'], ax = ax[0])\nax[0].set_title('Time signature')\nsns.countplot(train['audio_mode'], ax = ax[1])\nax[1].set_title('Audio mode')\nsns.countplot(train['key'], ax = ax[2])\nax[2].set_title('Key')\nfig.tight_layout()\nplt.show()","d4804620":"sns.kdeplot(train.song_duration_ms[train['song_popularity'] == 0], color=\"red\", shade=True)\nsns.kdeplot(train.song_duration_ms[train['song_popularity'] == 1], color=\"blue\", shade=True)\nplt.grid()\nplt.legend(['Unpopular','Popular'])\nplt.show()","345bc65b":"sns.kdeplot(train.danceability[train['song_popularity'] == 0], color=\"red\", shade=True)\nsns.kdeplot(train.danceability[train['song_popularity'] == 1], color=\"blue\", shade=True)\nplt.grid()\nplt.legend(['False','True'])\nplt.show()","771098c3":"sns.scatterplot(x='energy', y='loudness', data=train)","64224730":"sns.countplot(train['song_popularity'])\nplt.show()","23f1f985":"len(train[train[\"song_popularity\"]==0])","69a29cac":"len(train[train[\"song_popularity\"]==1])","c621ca63":"cat_cols = ['key', 'audio_mode', 'time_signature']\nnum_cols =  ['song_duration_ms', 'acousticness', 'danceability', \n             'energy','instrumentalness', 'liveness', 'loudness',\n             'speechiness', 'tempo', 'audio_valence']\n\nnum_pipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\ncat_pipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('encode', OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n])\n\ndata_pipeline = ColumnTransformer([\n    ('numerical', num_pipeline, num_cols),\n    ('categorical', cat_pipeline, cat_cols),\n])","6c1dfda1":"feat = [col for col in train.columns if col not in (\"id\", \"song_popularity\")]\n# train_data = pd.DataFrame(data_pipeline.fit_transform(train[feat]))","c213a0da":"!pip install scikit-learn-intelex -q --progress-bar off","68934113":"from sklearnex import patch_sklearn\npatch_sklearn()","47c785a1":"smt = SMOTETomek(random_state=42)\n\n# X = train_data\n# y = train['song_popularity']\n\n# X, y = smt.fit_resample(X, y)","132d0dee":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []","a70ea1dd":"# xgb_params = {\n#     \"objective\":\"binary:logistic\",\n#     \"eval_metric\": \"auc\",\n#     \"random_state\": 42,\n# }","41d66dba":"# for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n#     X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n#     y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n#     model = XGBClassifier(**xgb_params)\n#     model.fit(X_train, y_train)\n    \n#     valid_preds = model.predict_proba(X_valid)[:,1]\n    \n#     auc = roc_auc_score(y_valid, valid_preds)\n#     print(f\"Fold: {fold}, AUC: {auc}\")\n#     scores.append(auc)","1f8d612f":"# print(\"Mean AUC :\", np.mean(scores))","0ba87a61":"X = train[feat]\ny = train['song_popularity']","2e6705c1":"# creating a function to train the hyperparameter to minimize the \"auc score\" \ndef run(trial, data=X, target=y):\n    # splitting the data into train and test\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42) \n    \n    # replacing the null values and scaling the data\n    x_train = pd.DataFrame(data=data_pipeline.fit_transform(x_train))\n    x_test = pd.DataFrame(data=data_pipeline.transform(x_test))\n    \n    # balancing the training data using SMOTETomek\n    x_train,y_train = smt.fit_resample(x_train,y_train)\n    \n    # parameters to be tuned to reduce the \"auc score\"\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 20000, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'objective': 'binary:logistic',\n         }\n    \n                         \n    model = XGBClassifier(**params, booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            random_state=9, \n                            gpu_id = 0,\n                            use_label_encoder=False)\n    \n    model.fit(x_train,y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=300, verbose=False)\n    \n    preds = model.predict_proba(x_test)[:,1]\n    fpr, tpr, _ = roc_curve(y_test, preds)\n    score = auc(fpr, tpr)\n    \n    return score","c7dc3396":"study = optuna.create_study(direction='maximize')\nstudy.optimize(run, n_trials=100)","bd0b8639":"study.best_params","e477b4d9":"# We get best parameters as:\n\n# {'max_depth': 6,\n#  'n_estimators': 18600,\n#  'learning_rate': 0.055685714344158255,\n#  'subsample': 0.30000000000000004,\n#  'colsample_bytree': 0.4,\n#  'colsample_bylevel': 0.4,\n#  'min_child_weight': 0.0016897154148421698,\n#  'reg_lambda': 885.4433721141279,\n#  'reg_alpha': 0.05850000482472221,\n#  'gamma': 0.002475379404766672}","32250c64":"train_data = pd.DataFrame(data_pipeline.fit_transform(train[feat]))\n\nX = train_data\ny = train['song_popularity']\n\nX_test = pd.DataFrame(data_pipeline.transform(test[feat]))","b393d83e":"xgb_params = study.best_params\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    X_train, y_train = smt.fit_resample(X_train,y_train) \n    \n    model = XGBClassifier(**xgb_params, booster= 'gbtree',\n                        eval_metric = 'auc',\n                        tree_method= 'gpu_hist',\n                        predictor=\"gpu_predictor\",\n                        random_state=fold, \n                        use_label_encoder=False)\n    \n    model.fit(X_train, y_train, eval_set=[(X_valid,y_valid)],\n            early_stopping_rounds=300,\n            verbose=False)\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    fpr, tpr, _ = roc_curve(y_valid, valid_preds)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","9c3b77e9":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = test['id']\nsub['song_popularity'] = preds\nsub","c0376f6c":"sub.to_csv(\"submission.csv\", index=False)","339d0cfb":"1. 'energy' has a high correlation with 'loudness', and is a bit correlated with 'audio_valence' as well.\n2. 'audio_valence' is correlated with 'danceability' along with 'loudness' and 'energy'.\n3.  No feature has a very high correlation with 'song_popularity'. However subpopulations of these features may be correlated to the target, so we need to explore a bit in detail","6fc18966":"### Missing values","3e913a09":"### Feature Analysis\n\n['id', 'song_duration_ms', 'acousticness', 'danceability', 'energy',\n       'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode',\n       'speechiness', 'tempo', 'time_signature', 'audio_valence',\n       'song_popularity']","6323b0fb":"skewness observed in acousticness, instrumentalness, liveness, loudness, speechiness, we need to apply transformation","2e1716a4":"## Outliers and missing values","9c8a74a9":"### sklearnx speedup","ab3e3a4f":"### XGBoost","06561cdb":"## Load data","00b6a557":"### Detecting outliers\n\nWe use Tukey's rule to detect outliers, i.e. values > (1.5 * IQR) from the quartiles are termed as outliers, where IQR is interquartile range\n\nCredits: https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#2.2-Outlier-detection","d89429b0":"#### time signature - 4 unique values","b819e4ab":"#### audio mode - binary","71518e70":"### Categorical variables","4c62093d":"### Finetuning using Optuna","b2b1df32":"## Creating a Pipeline\nTo encode, impute and scale variables","62f51b6a":"### Energy vs loudness","d72d8eba":"#### Feature distribution","132b7558":"### Correlation plot","d700f318":"## EDA","47a532c2":"1. Distribution of labels in non-uniform, so it is best to use AUC score.\n2. Since it is binary classification, we can use StratifiedKFold","5a47f28d":"#### song_duration_ms","1c3f4db1":"XGBoost handles missing values by itself","8227715b":"#### danceability","6539f641":"## Modelling","579e6ad3":"Refer: [this notebook](https:\/\/www.kaggle.com\/ankurlimbashia\/xgboost-optuna-without-any-eda\/notebook)","c1e2beeb":"Release notes:\n* V1: EDA+ XGBoost\n* V2: created a pipeline\n* V3: Used SMOTE-Tomek links method for resampling\n* V4: Finetuning using optuna","869011a2":"### Decide the metric","47d84a65":"#### key - 12 unique values","7f9270f8":"## Final prediction and submission","f20386f8":"## Resampling using SMOTE-Tomek method\n\nRefer: https:\/\/towardsdatascience.com\/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc","86603b6e":"### Target overlap","9cf02e48":"We don't have any rows with more than 2 outliers","9cc40cb3":"\"key\", \"audio_mode\", and \"time_signature\" are probably categorical variables"}}