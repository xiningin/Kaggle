{"cell_type":{"58d1d7cd":"code","4f521fd5":"code","3f96c45d":"code","55c54b6e":"code","06a3d954":"code","b477e143":"code","9b496bc6":"code","c63bbc05":"code","8f3bcf0d":"code","b675aa91":"code","5abd89d7":"code","ec568b99":"code","38c9aaa4":"code","6158e65a":"code","0c59f22c":"code","d3b5d90b":"code","6d26df03":"code","62111500":"code","19bbeb14":"code","9c6e0cf4":"code","83aafbfc":"code","ff297895":"code","68990a5d":"code","e2a8a285":"code","bdd70ebb":"code","1c0f91e2":"code","b3f81f35":"code","017e67f5":"code","68892465":"code","a0601531":"code","c7e78ad2":"code","b5500db2":"code","3eec59e2":"code","9fdbbc04":"code","0f8c873c":"markdown","40743b57":"markdown","12a150f4":"markdown","1ba1361d":"markdown","66600d3c":"markdown","ee933f87":"markdown","7475304d":"markdown","1adb14b6":"markdown","80605937":"markdown","f2a3c5b6":"markdown","ca88e8ae":"markdown","52f985e4":"markdown","ea5a13ac":"markdown","847baf48":"markdown","1ec12c11":"markdown","2b9befa7":"markdown","c8fa5b74":"markdown","35e42104":"markdown"},"source":{"58d1d7cd":"import torch","4f521fd5":"# Construct a 5x3 matrix, uninitialized\nx = torch.empty(5, 3)\nprint(x)","3f96c45d":"# Construct a randomly initialized matrix\nx = torch.rand(5, 3)\nprint(x)","55c54b6e":"# Construct a matrix of zeros and datatype long\nx = torch.zeros(5, 3, dtype=torch.long)\nprint(x)","06a3d954":"# Construct a tensor directly from data\nx = torch.tensor([1.1, 2.0, 3])\nprint(x)","b477e143":"# Create a tensor based on existing tensor. These methods will reuse the properties\n# of input tensor. e.g. dtype, unless new values are provided by user\nx = x.new_ones(5, 3, dtype=torch.double)\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)\nprint(x)","9b496bc6":"# Get size of a tensor\nprint(x.size())","c63bbc05":"# torch.Size is in fact a tuple. So it supports all tuple operations","8f3bcf0d":"# Addition: Syntax 1\ny = torch.rand(5, 3)\nprint(x + y)","b675aa91":"# Addition: Syntax 2\nprint(torch.add(x, y))","5abd89d7":"# Addition: Providing an output tensor as argument\nresult = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)","ec568b99":"# Addition: in-place\ny.add_(x)\nprint(y)","38c9aaa4":"print(x[0:3])","6158e65a":"print(x[0:1])","0c59f22c":"print(x[0:1,0:1])","d3b5d90b":"x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)\nprint(f\"x: {x.size()} \\ny: {y.size()} \\nz: {z.size()}\")","6d26df03":"x = torch.randn(1)\nprint(x)\nprint(x.item())","62111500":"# Torch Tensor\na = torch.ones(5)\nprint(a)","19bbeb14":"b = a.numpy()\nprint(b)","9c6e0cf4":"# See how changing a (Torch Tensor) changes b (NumPy Array)\na.add_(1)\nprint(a)\nprint(b)","83aafbfc":"import numpy as np\na = np.ones(5)\nprint(a)","ff297895":"b = torch.from_numpy(a)\nprint(b)","68990a5d":"# See how changing NumPy Array changes Torch Tensor\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)","e2a8a285":"# let's run this cell only if CUDA is available\n# We will use `torch.device` objects to move tensors in and out of the GPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")           # a CUDA device object\n    y = torch.ones_like(x, device=device)   # directly create a tensor on GPU\n    x = x.to(device)                        # or just use `.to('cuda')`\n    z = x + y\n    print(z)\n    print(z.to('cpu', torch.double))        # `.to` can also change dtype together","bdd70ebb":"import torch","1c0f91e2":"# Create a Tensor and set `requires_grad=True` to track computation with it\nx = torch.ones(2, 2, requires_grad=True)\nprint(x)","b3f81f35":"# Do a tensor operation\ny = x + 2\nprint(y)","017e67f5":"# y was created as a result of an operation, so it has a `grad_fn`.\nprint(y.grad_fn)","68892465":"# Do more operations on y\nz = y *  y * 3\nout = z.mean()\n\nprint(z, out)","a0601531":"a = torch.randn(2, 2)\na = ((a * 3) \/ (a - 1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb = (a * a).sum()\nprint(b.grad_fn)","c7e78ad2":"out.backward()","b5500db2":"print(x.grad)","3eec59e2":"x = torch.randn(3, requires_grad=True)\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\nprint(y)","9fdbbc04":"print(x.requires_grad)\nprint((x * 2).requires_grad)\n\nwith torch.no_grad():\n    print((x * 2).requires_grad)","0f8c873c":"All the tensors on CPU except a CharTensor support converting to numpy and back","40743b57":"# **[Autograd: Automatic Differentiation](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html#autograd-automatic-differentiation)**\nThe `autograd` provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n## Tensor\n`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n\nThere\u2019s one more class which is very important for autograd implementation - a `Function`.\n\n`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the `Tensor` (except for `Tensors` created by the user - their `grad_fn` `is` `None`).\n\nIf you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar (i.e. it holds a one element data), you don\u2019t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape.","12a150f4":"We can use standard NumPy-like indexing","1ba1361d":"To know more about tensor operations [click here](https:\/\/pytorch.org\/docs\/stable\/torch.html)","66600d3c":"Any operation that mutates a tensor in-place is post fixed with an `_`.\nFor example `x.copy_()`,  `x.t_()` will change x","ee933f87":"`.requires_grad_( ... )` changes an existing Tensor\u2019s `requires_grad` flag in-place. The input flag defaults to `False` if not given.","7475304d":"Resizing: If we want to resize\/reshape tensor, we can use `torch.view`","1adb14b6":"If we a one element tensor, we can use `.item()`  to get the value as a python number","80605937":"# **Operations**\n\nThere are multiple syntax for operations. In the following example, we will take a look at addition operation.","f2a3c5b6":"You can also stop `autograd` from tracking history on Tensors with `.requires_grad=True` by wrapping the code block in `with torch.no_grad()`:\n\n","ca88e8ae":"# **CUDA Tensors**\n\nTensors can be moved to any device using the `.to` method","52f985e4":"## Converting Torch Tensor to NumPy Array","ea5a13ac":"## Converting Numpy Array to Torch Tensor","847baf48":"## Gradients\nLet\u2019s backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`.","1ec12c11":"Pending","2b9befa7":"Documentaion of [autograd](https:\/\/pytorch.org\/docs\/autograd)","c8fa5b74":"# **Numpy Bridge**\n\nConverting a Torch Tensor to NumPy array and vice-versa is very easy.\n\nThe Torch Tensor and NumPy Array will share their underlying memory locations, and changing one will change the other.","35e42104":"# **[Tensors](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)**\nFrom: [Deep Learning with PyTorch: A 60 Minute Blitz](https:\/\/pytorch.org\/tutorials\/beginner\/deep_learning_60min_blitz.html)"}}