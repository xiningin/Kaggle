{"cell_type":{"7f98a53b":"code","21c7d69b":"code","f590416f":"code","35d18866":"code","d7886c40":"code","9bd087f7":"code","7faa4085":"code","6516481b":"code","5ce0d133":"code","3563a3ff":"code","97ca1fbd":"code","9a5c4220":"code","13a8174c":"code","24b50fdb":"code","ff42ef92":"code","50826c2a":"code","a2c115c9":"code","1b2cb3de":"code","17e77041":"code","bf02b2d1":"code","65535664":"code","a015b640":"code","b1445262":"code","ab7c0bc2":"code","fccc3aa4":"code","a33de979":"code","6fee99b3":"code","6a2c19a5":"code","146aa55f":"code","bcd91106":"code","46cc33cc":"code","3509ae94":"code","806fa1a1":"markdown","678bc8e6":"markdown","7a785c00":"markdown","4159bec2":"markdown","5cdc187c":"markdown","21a03537":"markdown","bdddb4f1":"markdown","eed3e021":"markdown","80bedc71":"markdown","263a0d69":"markdown","c44a9baa":"markdown","9e190f67":"markdown","4c4b1bf4":"markdown","a0ac95c2":"markdown","90a6da0a":"markdown"},"source":{"7f98a53b":"import scipy\nimport re\nimport string\nimport nltk\nimport random\nimport os\nimport tensorflow                                           as tf\nimport numpy                                                as np \nimport pandas                                               as pd \nimport matplotlib.pyplot                                    as plt\nimport matplotlib.cm                                        as cm\nimport seaborn                                              as sns\nimport plotly.express                                       as ex\nimport plotly.graph_objs                                    as go\nimport plotly.offline                                       as pyo\nimport spacy                                                as sp\nfrom plotly.subplots                                        import make_subplots\nfrom sklearn.decomposition                                  import TruncatedSVD,PCA\nfrom sklearn.manifold                                       import Isomap\nfrom sklearn.feature_extraction.text                        import CountVectorizer,TfidfVectorizer\nfrom sklearn.cluster                                        import KMeans\nfrom tqdm.notebook                                          import tqdm\nfrom keras                                                  import backend as K\nfrom keras                                                  import Sequential\nfrom keras.layers                                           import Dense,LSTM,Input,Dropout,SimpleRNN\nfrom sklearn.model_selection                                import train_test_split\nfrom scipy.spatial                                          import distance_matrix\nfrom sklearn.metrics.pairwise                               import cosine_similarity\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \ntqdm.pandas()\nsns.set_style('darkgrid')\npyo.init_notebook_mode()\nplt.rc('figure',figsize=(16,8))\nsns.set_context('paper',font_scale=1.5)\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","21c7d69b":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nclit_df = pd.concat([train,test])[['excerpt']]\ncs_df = pd.read_csv('\/kaggle\/input\/highly-rated-children-books-and-stories\/children_books.csv',encoding='ISO-8859-1')","f590416f":"# Remove all the special characters\nclit_df.excerpt              = clit_df.excerpt.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x)))\ncs_df.Desc                   = cs_df.Desc.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \n# Substituting multiple spaces with single space \nclit_df.excerpt              = clit_df.excerpt.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\ncs_df.Desc                   = cs_df.Desc.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\n# Converting to Lowercase \nclit_df.excerpt              = clit_df.excerpt.str.lower() \ncs_df.Desc                   = cs_df.Desc.str.lower() \n","35d18866":"cs_df.head(5)","d7886c40":"def min_age(sir):\n    if sir.find('+')!=-1:\n        return np.int(sir[:sir.find('+')])\n    elif sir.find('-')!=-1:\n        return np.int(sir.split('-')[0])\n    elif bool(re.match(r'[0-9]+',sir)):\n        return np.int(sir)\n    else:\n        return 'else'\ndef max_age(sir):\n    if sir.find('+')!=-1:\n        return 99\n    elif sir.find('-')!=-1:\n        return np.int(sir.split('-')[1])\n    elif bool(re.match(r'[0-9]+',sir)):\n        return np.int(sir)\n    else:\n        return 'else'\n    \ncs_df['Min_Age'] = cs_df.Reading_age.apply(min_age)\ncs_df['Max_Age'] = cs_df.Reading_age.apply(max_age)","9bd087f7":"all_text = pd.concat([cs_df.Desc,clit_df.excerpt]).values\nVCT = TfidfVectorizer(stop_words='english')\n\nVCT.fit(all_text)\nISOMAP = Isomap(n_components =2)\nISOMAP.fit(VCT.transform(clit_df.excerpt))\ndict_size = len(VCT.vocabulary_)\ncs_tf = ISOMAP.transform(VCT.transform(cs_df.Desc))\nclit_tf = ISOMAP.transform(VCT.transform(clit_df.excerpt))\nCS = cosine_similarity(clit_tf,cs_tf)","7faa4085":"sns.clustermap(CS)\nplt.show()","6516481b":"fig = go.Figure()\nscatter_cs = go.Scatter(x=cs_tf[:,0],y=cs_tf[:,1],mode='markers',name='Children Books')\nscatter_clit = go.Scatter(x=clit_tf[:,0],y=clit_tf[:,1],mode='markers',name='CommoinLit')\nfig.add_trace(scatter_clit)\nfig.add_trace(scatter_cs)\nfig.show()","5ce0d133":"all_text = pd.concat([cs_df.Desc,clit_df.excerpt]).values\nVCT = TfidfVectorizer(stop_words='english')\n\nVCT.fit(all_text)\n\ndict_size = len(VCT.vocabulary_)\n\ncs_tf = VCT.transform(cs_df.Desc)\nclit_tf = VCT.transform(clit_df.excerpt)\n\ntrain_x_min,test_x_min,train_y_min,test_y_min = train_test_split(cs_tf.todense(),cs_df.Min_Age.astype(np.float32))","3563a3ff":"min_age_model = Sequential()\nmin_age_model.add(Input(train_x_min.shape))\nmin_age_model.add(Dense(500,activation='linear'))\nmin_age_model.add(Dense(100,activation='linear'))\nmin_age_model.add(Dense(50,activation='linear'))\nmin_age_model.add(Dense(20,activation='linear'))\nmin_age_model.add(Dense(1,activation='linear'))\n\nmin_age_model.compile(optimizer='adam',loss=root_mean_squared_error)\nmin_age_model.summary()","97ca1fbd":"history = min_age_model.fit(train_x_min,train_y_min,epochs=200,batch_size=128,verbose=0)\nplt.plot(history.history['loss'])\nplt.title('min age model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","9a5c4220":"pred=  pd.DataFrame({'Pred':np.round(min_age_model.predict(test_x_min).squeeze(),0),'True':test_y_min})\npred.head(10)","13a8174c":"train_x_max,test_x_max,train_y_max,test_y_max = train_test_split(cs_tf.todense(),cs_df.Max_Age.astype(np.float32))","24b50fdb":"max_age_model = Sequential()\nmax_age_model.add(Input(train_x_max.shape))\nmax_age_model.add(Dense(500,activation='linear'))\nmax_age_model.add(Dense(100,activation='linear'))\nmax_age_model.add(Dense(50,activation='linear'))\nmax_age_model.add(Dense(20,activation='linear'))\nmax_age_model.add(Dense(1,activation='linear'))\n\nmax_age_model.compile(optimizer='adam',loss=root_mean_squared_error)\nmax_age_model.summary()","ff42ef92":"history = max_age_model.fit(train_x_max,train_y_max,epochs=200,batch_size=128,verbose=0)\nplt.plot(history.history['loss'])\nplt.title('max age model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","50826c2a":"pred=  pd.DataFrame({'Pred_Max_Age':np.round(max_age_model.predict(test_x_max).squeeze(),0),'True_Max_Age':test_y_max})\npred.head(10)","a2c115c9":"hist_min = min_age_model.fit(cs_tf.todense(),cs_df.Min_Age.astype(np.float32),epochs=200,batch_size=128,verbose=0)\nhist_max = max_age_model.fit(cs_tf.todense(),cs_df.Max_Age.astype(np.float32),epochs=200,batch_size=128,verbose=0)","1b2cb3de":"min_ages = np.round(min_age_model.predict(clit_tf.todense()).squeeze(),0)\nmax_ages = np.round(max_age_model.predict(clit_tf.todense()).squeeze(),0)","17e77041":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntrain['Min_Age'] =min_ages[:-7]\ntrain['Max_Age'] =max_ages[:-7]\ntest['Min_Age'] =min_ages[-7:]\ntest['Max_Age'] =max_ages[-7:]","bf02b2d1":"#LIBS\nfrom transformers import TFAutoModel\nfrom transformers import BertTokenizer\n\n#VARS\nSEQ_LENGTH = 512\nN_SAMPLES  = cs_df.shape[0]\nBATCH_SIZE = 8\nSPLIT = 0.8\nSIZE  = int((N_SAMPLES\/BATCH_SIZE)*SPLIT)\n\n\n\n#FUNCS\ndef map_function(input_ids,mask,labels=None):\n    if labels != None:\n        return {'input_ids':input_ids,'attention_mask':mask},labels\n    else:\n        return {'input_ids':input_ids,'attention_mask':mask}\n\n#IMPLEMENTATION\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\ndef create_bert_dataset(corpus,SEQ_LENGTH,BATCH_SIZE,labels=None):\n    N_SAMPLES = len(corpus)\n    XIDS = np.zeros((N_SAMPLES,SEQ_LENGTH))\n    XMASK= np.zeros((N_SAMPLES,SEQ_LENGTH))\n    for aux,desc in tqdm(enumerate(corpus),leave=False):\n        tokens = tokenizer.encode_plus(desc,max_length=SEQ_LENGTH,truncation=True,\n                                       padding='max_length',\n                                       add_special_tokens=True,\n                                       return_tensors='tf')\n        XIDS[aux,:]  = tokens['input_ids']\n        XMASK[aux,:] = tokens['attention_mask']\n\n\n    if type(labels) != None:\n        dataset = tf.data.Dataset.from_tensor_slices((XIDS,XMASK,labels))\n        dataset = dataset.map(map_function)\n        dataset = dataset.shuffle(10000).batch(BATCH_SIZE,drop_remainder=True)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((XIDS,XMASK))\n        dataset = dataset.map(map_function)\n        dataset = dataset.shuffle(10000).batch(BATCH_SIZE,drop_remainder=True)\n    return dataset","65535664":"def convert_data(corpus):\n        tokens = tokenizer.encode_plus(corpus,max_length=SEQ_LENGTH,truncation=True,\n                                   padding='max_length',\n                                   add_special_tokens=True,\n                                   return_tensors='tf')\n        return {'input_ids':tf.cast(tokens['input_ids'],np.float64),'attention_mask':tf.cast(tokens['attention_mask'],np.float64)}","a015b640":"#Min Age\nlabels = np.zeros((cs_df.shape[0],100))\nlabels[np.arange(cs_df.shape[0]),cs_df.Min_Age] = 1\nmin_dataset = create_bert_dataset(cs_df.Desc,SEQ_LENGTH,BATCH_SIZE,labels)\n\ntrain_min_ds = min_dataset.take(SIZE)\nval_min_ds   = min_dataset.skip(SIZE)\n\n#Max Age\nlabels = np.zeros((cs_df.shape[0],100))\nlabels[np.arange(cs_df.shape[0]),cs_df.Max_Age] = 1\nmax_dataset = create_bert_dataset(cs_df.Desc,SEQ_LENGTH,BATCH_SIZE,labels)\n\ntrain_max_ds = max_dataset.take(SIZE)\nval_max_ds   = max_dataset.skip(SIZE)","b1445262":"BERT = TFAutoModel.from_pretrained('bert-base-cased')\n\ninput_ids = tf.keras.layers.Input(shape=(SEQ_LENGTH,),name='input_ids',dtype='int32')\nmasks     = tf.keras.layers.Input(shape=(SEQ_LENGTH,),name='attention_mask',dtype='int32')\n\nEMBDS = BERT.bert(input_ids,attention_mask=masks)[1]\n\nx = tf.keras.layers.Dense(1440,activation='relu')(EMBDS)\ny = tf.keras.layers.Dense(100,activation='softmax',name='outputs')(x)\n\nbert_min_age_model = tf.keras.Model(inputs=[input_ids,masks],outputs=y)\nbert_max_age_model = tf.keras.models.clone_model(bert_min_age_model)\nbert_min_age_model.summary()","ab7c0bc2":"optimizer = tf.keras.optimizers.Adam(lr=1e-5,decay=1e-6)\nloss      = tf.keras.losses.CategoricalCrossentropy()\naccuracy  = tf.keras.metrics.CategoricalAccuracy('accuracy')\nbert_min_age_model.compile(optimizer=optimizer,loss=loss,metrics=[accuracy])\nbert_max_age_model.compile(optimizer=optimizer,loss=loss,metrics=[accuracy])","fccc3aa4":"from tqdm.keras import TqdmCallback\nhistory_min = bert_min_age_model.fit(\n    train_min_ds,\n    validation_data = val_min_ds,\n    epochs=3, verbose=0, callbacks=[TqdmCallback(verbose=1)]\n)","a33de979":"history_max = bert_max_age_model.fit(\n    train_max_ds,\n    validation_data = val_max_ds,\n    epochs=3, verbose=0, callbacks=[TqdmCallback(verbose=1)]\n)","6fee99b3":"plt.title('Min Age Bert Model Loss')\nplt.plot(history_min.history['loss'],label='Min Age')\nplt.plot(history_max.history['loss'],label='Max Age')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","6a2c19a5":"#bert_min_age_model.save('Min_Age_BERT.h5')\n#bert_max_age_model.save('Max_Age_BERT.h5')","146aa55f":"plt.title('Min Age Connection with Target')\nsns.boxplot(y=train['target'],x=train['Min_Age'])\nplt.show()","bcd91106":"plt.title('Max Age Connection with Target')\nax = sns.boxplot(y=train['target'],x=train['Max_Age'])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","46cc33cc":"plt.title('Pearson Correlation Between Features')\nsns.heatmap(train.corr(),annot=True,cmap='mako')","3509ae94":"train.to_csv('train_with_minmax_ages.csv')\ntest.to_csv('test_with_minmax_ages.csv')\nmin_age_model.save('Min_Age_Model.mdl')\nmax_age_model.save('Max_Age_Model.mdl')","806fa1a1":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Loading and Preprocessing<\/h1>\n\n","678bc8e6":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Conclusion<\/h1>\n\n","7a785c00":"Greetings fellow Kagglers,\nMany different approaches are taken in the pursuit of the perfect model to solve the CommonLit reading ease problem.\nThe following notebook suggests yet another approach that can potentially increase the performance of already stable models,\nThe following notebook will use data collected from a book recommendation website that targets children and teens and provides the target reading age for each book in its database.\nWe will use the books' synopsis from that website to train a dense neural network that will predict the minimum recommended reading age and the maximum recommended reading age.\nThe hypothesis is that knowing the reading age bound of a text is a strong indicator of the hardness of a given text. \n\n\n","4159bec2":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Introduction<\/h1>\n\n","5cdc187c":"**Note:** After extracting bounds from the recommended reading age, we can advance and create models to predict each side of the interval.","21a03537":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Fitting and Evaluation<\/h1>\n\n","bdddb4f1":"**Note:** Above is the data collected from the book recommendation website as seen in the following [dataset]( https:\/\/www.kaggle.com\/thomaskonstantin\/highly-rated-children-books-and-stories)","eed3e021":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Baseline Architecture<\/h1>\n\n","80bedc71":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">BERT Architecture<\/h1>\n\n","263a0d69":"**Example of The Data we will train our models on as stated in the dataset mentioned below**\n\n![](https:\/\/i.ibb.co\/SyLjSvc\/h15.png)","c44a9baa":"After training a fairly basic NN model to predict the minimum and maximum recommended reading ages of a given text, we saw a fair connection between the reading ease metric and the minimum and maximum reading age.\nAlthough by themselves, those two features may not be strong predictors of the reading ease metric but we do see a confirmation of our initial hypothesis, which states a connection between the target reading age interval and the reading ease metric,\nIt may be beneficial to involve these new age parameters as part of any robust model and test their contribution to the overall RMSE reduction in future works.\n","9e190f67":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h1>\n\n","4c4b1bf4":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Similarity Analysis<\/h1>\n\n","a0ac95c2":"The following short section will test the similarity between the texts in both datasets, the first test we will use is calculating the cosine similarity between the two corpora (the children's book synopsis vs. the CommonLit texts).\nWe will also cluster the resulting values to see if any visible cluster emerges; such a result will increase our confidence in the usability of the text from the children's book dataset and may reveal any underlying topics that may reside in our dataset.\n\nThe second test we will perform is using an $R^2$ representation of the Tfidf vectorized texts from both datasets, plotting them and visually inspecting how they are located in $R^2$ space; a visual confirmation of such proximity can confirm a certain degree of similarity between both corpora.","90a6da0a":"**Our Hypothesis and Intuition for Achieving Such A Feature**\n\n![](https:\/\/i.ibb.co\/4mFwSng\/h16.png)"}}