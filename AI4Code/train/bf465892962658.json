{"cell_type":{"632e6692":"code","8c325461":"code","4a0c339a":"code","1a0fa73f":"code","e828544f":"code","35d7f9b9":"code","a49f2b4c":"code","e4addf90":"code","def1adfc":"code","a4491504":"code","ea6ffcbe":"code","f8109aa7":"code","5f40685f":"code","79bbf313":"code","112eed17":"code","f7fce491":"code","68914e48":"code","2ce35580":"code","8c371228":"code","f70ab7c7":"code","5ec05f77":"code","7fd49119":"code","13185dee":"code","6a67ec7e":"code","cecd0661":"code","8e12bd5f":"code","d88f51ff":"code","644a7786":"code","a43a3499":"code","a48887db":"code","0daf3891":"code","1362617f":"code","f21171fc":"code","5dcd3874":"code","453fa8ab":"code","69a2e4a5":"code","ad60cb26":"code","b589f0bd":"code","e0101c6c":"code","fdb691e1":"code","ab131b6f":"code","eeca623e":"code","a507ac90":"code","f9c09756":"code","2a7095e6":"code","7889768e":"code","9433def8":"code","6acc86bf":"code","1569ba65":"code","3863a7c2":"code","7cb8d9b7":"code","8e1ef507":"code","6e9dae99":"code","59f411ad":"code","7b921966":"code","6076d64a":"code","8e5130bd":"code","e8803c0a":"code","a8eb7690":"code","e010a3cb":"code","68de74c3":"code","b5000aa3":"code","2457795d":"code","1bbe5771":"code","18c28364":"code","80a86358":"code","98e46d9e":"code","e7ba681e":"code","0399f4f2":"code","f97a8a2f":"code","d04ecdd1":"code","6502b8a3":"code","78ecb0c2":"markdown","b3462f51":"markdown","5c31c326":"markdown","889abf8f":"markdown","a1a5ef43":"markdown","8ae1c29d":"markdown","3c42d205":"markdown","55b7d1ec":"markdown","2c7fd556":"markdown","b0c6c638":"markdown","7ab8179c":"markdown","f9fb6196":"markdown","217d925e":"markdown","90dba635":"markdown","8a9067c4":"markdown","7a97f4c1":"markdown","bae9b05a":"markdown","17a53499":"markdown","84348773":"markdown","c0482bc1":"markdown","8fc7f011":"markdown","1fb02867":"markdown","35cb201e":"markdown","22343102":"markdown","a1278753":"markdown","f3d211f0":"markdown","7df15407":"markdown","952c42d3":"markdown","d74a177b":"markdown","4a91f0ad":"markdown","ba8197bb":"markdown","b46e579f":"markdown","b99eddd1":"markdown","3ec67800":"markdown","3d6c6930":"markdown","8da59796":"markdown","6f23c5e2":"markdown","a36a3058":"markdown","0bf05b3b":"markdown","6ee39378":"markdown","e3e9762a":"markdown","bc7ae9a2":"markdown","15c3cb70":"markdown","c4dff701":"markdown","dfa9ddf7":"markdown","4ded998b":"markdown","afdacbcc":"markdown","9a7e5f60":"markdown","e7230b63":"markdown","d8f91418":"markdown","d4eea090":"markdown","4f4d1443":"markdown","2b6065e1":"markdown","3c6bb2a6":"markdown","99fb161b":"markdown","82d7c9b7":"markdown","6a15139a":"markdown","a8262edb":"markdown","876be66e":"markdown","40fe615e":"markdown","987919e8":"markdown","ffd41f4f":"markdown","9772efa2":"markdown","cf7f607b":"markdown","f516c22f":"markdown","f715ed4c":"markdown","8058ae88":"markdown","dfb05ee3":"markdown","439cdcff":"markdown","dbd87f82":"markdown","f550e36e":"markdown","bf5d4238":"markdown","9c56ebe7":"markdown","795c780d":"markdown","6d9639d0":"markdown","952f83d7":"markdown","a657a657":"markdown","4c961e55":"markdown","93797c30":"markdown","823875aa":"markdown","b3f831b7":"markdown","eab9e632":"markdown","85e322fc":"markdown","a79d26c5":"markdown","c3678fee":"markdown","0e9aab86":"markdown","6d4afc96":"markdown","14415b6f":"markdown","6446f81c":"markdown","8bc192f5":"markdown","c7fffd02":"markdown"},"source":{"632e6692":"!pip install transformers==3.0.2","8c325461":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a0c339a":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## Para silenciar el aviso que nos dar\u00eda Colab al no darle una KEY v\u00e1lida.","1a0fa73f":"#Las relacionadas con transformers de Hugging Face.#Las relacionadas con transformers de Hugging Face.\n\nimport transformers \nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel\n\n#Las relacionadas con TensorFlow\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam, SGD, Adamax\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\n#El train-test split de Scikit-learn.\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n#Las herramientas gr\u00e1ficas necesarias.\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\n#Para acceder a las librer\u00edas de datasets de Hugging Face.\n\n!pip install nlp\nimport nlp\n\n#Resto de funciones\n\nimport datetime\nfrom tqdm.notebook import tqdm\nimport random\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport io\nimport PIL","e828544f":"transformers.__version__","35d7f9b9":"Image.open(\"..\/input\/tf-logo\/tf_logo.png\")","a49f2b4c":"Image.open(\"..\/input\/tpu-v38\/tpu--sys-arch4 1.png\")","e4addf90":"#Detectaremos el hardware requerido, y nos devolver\u00e1 la distribuci\u00f3n adecuada.\n\ntry:\n    #No es necesario introducir un valor en el \"resolver\", ya que la enviroment variable TPU_NAME ya est\u00e1 establecida en Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    \n    #Le brindamos dicha informaci\u00f3n al tf.config.experimental_connect_to_cluster, que conecta con el cluster dado    \n    tf.config.experimental_connect_to_cluster(tpu)\n    \n    #Inicializamos el TPU.\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    #Y definimos la estrategia de distribuci\u00f3n.\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy() \n    print('Number of replicas:', strategy.num_replicas_in_sync)","def1adfc":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\n\ntest = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\n\nsubmission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')","a4491504":"response = requests.get('https:\/\/miro.medium.com\/max\/700\/1*Nv2NNALuokZEcV6hYEHdGA.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","ea6ffcbe":"train.head()","f8109aa7":"train.premise.values[1]","5f40685f":"train.hypothesis.values[1]","79bbf313":"train.label.values[1]","112eed17":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","f7fce491":"train.isnull().sum()","68914e48":"#Definimos los dos modelos a estudiar\nmodel_xml_roberta = 'jplu\/tf-xlm-roberta-large'\n\nmodel_bert = 'bert-base-multilingual-cased'\n\n#Definimos el n\u00famero de epochs a iterar y el max_len\nn_epochs = 10\nmax_len = 80\n\n# El tama\u00f1o de nuestro batch_size depender\u00e1 del n\u00famero de r\u00e9plicas en nuestra estrategia\nbatch_size = 16 * strategy.num_replicas_in_sync","2ce35580":"Image.open(\"..\/input\/table-accurancy-models\/Table accurancy models.png\")","8c371228":"response = requests.get('http:\/\/jalammar.github.io\/images\/bert-base-bert-large-encoders.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","f70ab7c7":"response = requests.get('http:\/\/jalammar.github.io\/images\/bert-encoders-input.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","5ec05f77":"response = requests.get('http:\/\/jalammar.github.io\/images\/bert-output-vector.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","7fd49119":"response = requests.get('https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert_emnedding.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","13185dee":"response = requests.get('https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert-vs-openai-.jpg')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","6a67ec7e":"response = requests.get('https:\/\/miro.medium.com\/max\/2272\/1*9cRchmIyxP4LUnONXLM82g.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","cecd0661":"response = requests.get('https:\/\/images.deepai.org\/converted-papers\/1911.02116\/x7.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","8e12bd5f":"# Cargamos el tokenizador correspondiente\ntokenizer_bert = AutoTokenizer.from_pretrained(model_bert)","d88f51ff":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_bert = train[['premise', 'hypothesis']].values.tolist()\ntest_text_bert = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_bert = tokenizer_bert.batch_encode_plus(\n    train_text_bert,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_bert = tokenizer_bert.batch_encode_plus(\n    test_text_bert,\n    pad_to_max_length=True,\n    max_length=max_len\n)","644a7786":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_bert['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_bert['input_ids']","a43a3499":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","a48887db":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_bert)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso [CLS]\n    cls_token = sequence_output[:, 0, :]\n\n    # La \u00faltima capa es la que pasamos a trav\u00e9s de softmax para la aplicaci\u00f3n del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","0daf3891":"n_steps = len(x_train) \/\/ batch_size\n\ntrain_history_bert = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","1362617f":"# list all data in history\nprint(train_history_bert.history.keys())\n# summarize history for loss\nplt.plot(train_history_bert.history['loss'])\nplt.plot(train_history_bert.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_bert.history['accuracy'])\nplt.plot(train_history_bert.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","f21171fc":"# Cargamos el tokenizador correspondiente\ntokenizer_xml_roberta = AutoTokenizer.from_pretrained(model_xml_roberta)","5dcd3874":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_xml_roberta = train[['premise', 'hypothesis']].values.tolist()\ntest_text_xml_roberta = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    train_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    test_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)","453fa8ab":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_xml_roberta['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_xml_roberta['input_ids']","69a2e4a5":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","ad60cb26":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_xml_roberta)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # La \u00faltima capa es la que pasamos a trav\u00e9s de softmax para la aplicaci\u00f3n del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","b589f0bd":"n_steps = len(x_train) \/\/ batch_size\n\ntrain_history_xml_roberta = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","e0101c6c":"# list all data in history\nprint(train_history_xml_roberta.history.keys())\n# summarize history for loss\nplt.plot(train_history_xml_roberta.history['loss'])\nplt.plot(train_history_xml_roberta.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_xml_roberta.history['accuracy'])\nplt.plot(train_history_xml_roberta.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","fdb691e1":"response = requests.get('https:\/\/miro.medium.com\/max\/398\/1*BfvKeP4ykqi4J4C5g4EZzg.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","ab131b6f":"mnli = nlp.load_dataset(path='glue', name='mnli')","eeca623e":"mnli_train_df = pd.DataFrame(mnli['train'])\n\nmnli_train_df = mnli_train_df[['premise', 'hypothesis', 'label']]\n\nmnli_train_df['lang_abv'] = 'en'","a507ac90":"mnli_train_df.head(10)","f9c09756":"mnli_sample= mnli_train_df.sample(n = 40000,random_state= 2020)","2a7095e6":"mnli_sample.head(10)","7889768e":"xnli = nlp.load_dataset(path='xnli')","9433def8":"buffer = {\n    'premise': [],\n    'hypothesis': [],\n    'label': [],\n    'lang_abv': []\n}\n\n\nfor x in xnli['validation']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buffer['premise'].append(premise)\n        buffer['hypothesis'].append(hypothesis)\n        buffer['label'].append(label)\n        buffer['lang_abv'].append(lang)\n        \n# convert to a dataframe and view\nxnli_valid_df = pd.DataFrame(buffer)\nxnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","6acc86bf":"xnli_valid_df.head(10)","1569ba65":"response = requests.get('https:\/\/miro.medium.com\/max\/750\/0*cZwrV8EyfJSeunag.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","3863a7c2":"response = requests.get('https:\/\/d3i71xaburhd42.cloudfront.net\/f3ee8dcaaad5f47f347354fe5d740096097cbed5\/1-Figure1-1.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","7cb8d9b7":"!pip -q install googletrans\nimport gc\nfrom googletrans import Translator\nfrom dask import bag, diagnostics\nimport seaborn as sns","8e1ef507":"SEED = 42\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","6e9dae99":"def translate(words, dest):\n    dest_choices = ['zh-cn',\n                    'ar',\n                    'fr',\n                    'sw',\n                    'ur',\n                    'vi',\n                    'ru',\n                    'hi',\n                    'el',\n                    'th',\n                    'es',\n                    'de',\n                    'tr',\n                    'bg'\n                    ]\n    if not dest:\n        dest = np.random.choice(dest_choices)\n        \n    translator = Translator()\n    decoded = translator.translate(words, dest=dest).text\n    return decoded\n\ndef trans_parallel(df, dest):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(translate, dest)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(translate, dest)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df","59f411ad":"eng = train.loc[train.lang_abv == \"en\"].copy().pipe(trans_parallel, dest=None)\nnon_eng =  train.loc[train.lang_abv != \"en\"].copy().pipe(trans_parallel, dest='en')\ntrain_data_translate = train.append([eng, non_eng])","7b921966":"frames = [train_data_translate, xnli_valid_df, mnli_sample]\ntrain_data_def = pd.concat(frames)","6076d64a":"tf.tpu.experimental.initialize_tpu_system(tpu)","8e5130bd":"train_data_def.head()","e8803c0a":"labels, frequencies = np.unique(train_data_def.lang_abv.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","a8eb7690":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_xml_roberta = train_data_def[['premise', 'hypothesis']].values.tolist()\ntest_text_xml_roberta = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    train_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    test_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)","e010a3cb":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_xml_roberta['input_ids'], train_data_def.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_xml_roberta['input_ids']","68de74c3":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","b5000aa3":"response = requests.get('https:\/\/miro.medium.com\/max\/1100\/1*zfdW5zAyQxge85gA_mFPYg.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","2457795d":"response = requests.get('https:\/\/paperswithcode.com\/media\/methods\/Screen_Shot_2020-05-28_at_6.15.37_PM_apRrZCo.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","1bbe5771":"response = requests.get('https:\/\/www.jeremyjordan.me\/content\/images\/2018\/02\/Screen-Shot-2018-02-24-at-11.47.09-AM.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","18c28364":"fine_tuning_lr = pd.read_csv(\"..\/input\/fine-tuning\/table tuning lr.csv\", sep = ';')\n\nfine_tuning_lr","80a86358":"fine_tuning_lr = pd.read_csv(\"..\/input\/fine-tuning\/epsilon table.csv\", sep = ';')\n\nfine_tuning_lr","98e46d9e":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_xml_roberta)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # La \u00faltima capa es la que pasamos a trav\u00e9s de softmax para la aplicaci\u00f3n del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=2e-5,beta_1=0.9, beta_2=0.999, epsilon=5e-08), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","e7ba681e":"n_steps = len(x_train) \/\/ batch_size\n\ntrain_history_xml_roberta_def = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=3\n)","0399f4f2":"# list all data in history\nprint(train_history_xml_roberta_def.history.keys())\n# summarize history for loss\nplt.plot(train_history_xml_roberta_def.history['loss'])\nplt.plot(train_history_xml_roberta_def.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_xml_roberta_def.history['accuracy'])\nplt.plot(train_history_xml_roberta_def.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","f97a8a2f":"test_preds = model.predict(test_dataset, verbose=1)\nsubmission['prediction'] = test_preds.argmax(axis=1)","d04ecdd1":"submission.head()","6502b8a3":"submission.to_csv(\"submission.csv\", index = False)","78ecb0c2":"Definimos las funciones necesarias para traducir el conjunto de datos.","b3462f51":"# ix. Generating & Submitting Predictions","5c31c326":"Convertimos los diferentes conjuntos de datos en **tf.data.Dataset.**\n\nA la hora de operar con TPUs, antes que suministrar diccionarios al modelo, es mucho m\u00e1s interesante mover la informaci\u00f3n a trav\u00e9s de tf.data.Dataset, ya que se canaliza de una manera mucho m\u00e1s eficiente y \u00e1gil.\n\nLo que entre otras ventajas, recorta sensiblemente los tiempos que necesita el modelo para realizar las distintas iteraciones de la estimaci\u00f3n (epochs).\n","889abf8f":"# i. Abstract\n\nEste trabajo analiza la efectividad comparativa de los dos principales modelos utilizados en problemas de tipo NLP, NLI y NLU, que son BERT y XML-RoBERTa, as\u00ed como una propuesta de actuaci\u00f3n a la hora de abordar este tipo de problemas; incorporando una mayor cantidad de datos similares con los que mejorar la capacidad predictiva de los mismos, realizar data augmentation y, finalmente, realizar un fine tuning sobre los hiperpar\u00e1metros del modelo.\n\nRealizamos todo el entrenamiento del modelo en TPUs desarrollados por Google, y estructuramos todas las partes del modelo a trav\u00e9s de funciones de la biblioteca Keras.\n\nFinalmente, al tratarse de una competici\u00f3n de kaggle, realizaremos un submission y trataremos de alcanzar la m\u00e1xima puntuaci\u00f3n posible.","a1a5ef43":"# x. Bibliograf\u00eda","8ae1c29d":"Veamos cuantos valores son nulos:","3c42d205":"### Especificaciones y funcionamiento de BERT","55b7d1ec":"### An\u00e1lisis del conjunto final \nEn esta parte final del trabajo, unificaremos todas los dataset, y entrenaremos al modelo con ellos.","2c7fd556":"Esta es una base de datos que no nos concuerda con la proporci\u00f3n de idiomas utilizados en nuestro data set original, por lo que s\u00f3lo utilizaremos las filas necesarias para ajustar el dataset final a la proporci\u00f3n de idiomas hallada en el dataset original.","b0c6c638":"Establecemos las rutas de carga de archivos a los input del notebook.","7ab8179c":"El proceso de Data Augmentation puede actuar como un regularizador en la prevenci\u00f3n de overfitting en redes neuronales y mejorar el rendimiento en casos de problemas desbalanceados (como es nuestro caso). \n\nPero esto tiene una serie de limitaciones; al realizar cualquier proceso de Data Augmentation, corremos el riesgo de modificar el significado de las variables de estudio de tal forma que ya no correspondan a los labels asignados, por lo que debe de hacerse con extremo cuidado y siendo conocedores de c\u00f3mo afecta cada cambio introducido en el modelo a entrenar.\n\nSi bien es posible realizar este proceso en *data-space* (aumentando el numero de datos pero manteniendo las caracter\u00edsticas) o hacerlo en *feature-space* (modificando ciertas caracteristicas creando nuevos datos con estas nuevas caracter\u00edsticas), es m\u00e1s recomendable realizarlo en data-space, como indican diferentes estudios, entre ellos \"Understanding data augmentation for classification: when to warp?\" de Sebastien C. Wong, Adam Gatt,Victor Stamatescu and Mark D. McDonnell, en orden de mantener la adecuada correlaci\u00f3n de los labels.\n\nUn ejemplo de esto, ser\u00eda rotar o deformar una imagen o variar la pose del objeto en un caso de clasificaci\u00f3n de im\u00e1genes:\n\n* Mientras que rotarlo, deformarlo o transformarlo a blanco y negro ser\u00eda un cambio en **data-space**:","f9fb6196":"Revisamos un binomio de frases, empezando por la premisa","217d925e":"# iv. Evaluaci\u00f3n de modelos","90dba635":"# **TFM Izar Ortin Riquelme**\n## \u201cM\u00e1ster Big Data y Business Analytics UNED 2019\/2020 - UNED\u201d","8a9067c4":"El principal objetivo es minimizar el valor de la funci\u00f3n estoc\u00e1stica objetivo. La esperanza de dicha funci\u00f3n es el par\u00e1metro theta. Con los diferentes f sub 0, 1, 2,..., T, denotamos las realizaciones de los subsecuentes steps.\n\nLa estocasticidad puede provenir de la evaluaci\u00f3n en submuestras aleatorias (minibatches) de puntos de datos, o surgir del ruido inherente a la propia funci\u00f3n.\n\nCon gt = \u2207\u03b8ft(\u03b8) denotamos el gradiente, es decir, el vector de las derivadas parciales de f con respecto a \u03b8 evaluado a timestep t.\n\nEl algoritmo actualiza las medias m\u00f3viles exponenciales del gradiente (mt) y el cuadrado del gradiente (vt) donde los hiperparametros beta1 y beta2 controlan las tasas de ca\u00edda exponencial de estas medias m\u00f3viles, tanto gt como vt.\n\nLas medias m\u00f3viles en si mismas son estimadores del primer momento (la media) y del segundo momento (la varianza) del gradiente. Sin embargo, dichas medias m\u00f3viles, son inicializadas como vectores 0, lo que lleva a que las estimaciones est\u00e9n sesgadas hacia 0, especialmente durante los pasos iniciales, y tambi\u00e9n cuando los ratios de ca\u00edda son peque\u00f1os (es decir, cuando las betas tienden a 1). Aunque esta inicializaci\u00f3n es f\u00e1cilmente contrarrestada en el **bias-corrected** estimando los par\u00e1metros indicados en el Compute bias-corrected first moment estimate y Compute bias-corrected second raw moment estimate.\n\nEn Adam, la regla de actualizaci\u00f3n para pesos individuales es escalar sus gradientes inversamente proporcionales a un (escalado) L^2 normal de sus gradientes individuales actuales y pasados.\n\nPara conocer en m\u00e1s profundidad la **regla de actualizacion de Adam**, el **bias-corrected** o un an\u00e1lisis te\u00f3rico de la convergencia de Adam in online convex programming, consultar \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\" de Diederik P. Kingma y Jimmy Lei Ba.","7a97f4c1":"Realizar un cambio en la pose del objeto, ser\u00eda un cambio en **feature-space**:","bae9b05a":"\n### Similitudes\n\nAmbos modelos tienen unos inputs parecidos.\n\nSe necesita codificar todos nuestros pares de premisas\/hip\u00f3tesis y prepararemos los inputs necesarios para el modelo: input word IDs, input masks y  input type IDs; siendo:\n\n\u2022\t*input word IDs*: Las palabras (o segmentos de palabras) convertidos a IDs.\n\n\u2022\t*Input masks*: Los IDs que ayudan al modelo a distinguir entre los tokens relevantes de los de relleno o padding, que son a\u00f1adidos para que ambas sentencias tengan una misma longitud.\n\n\u2022\t *input type IDs*: Se le asignan ceros tanto al CLS como a la primera sentencia y unos a la segunda.\n\nLa principal diferencia en este punto es la configuraci\u00f3n de los datos de entrada en los *input word IDS*:\n\n\u2022\tBERT: Son del tipo [CLS] A [SEP] B [SEP], siendo A y B un par de sentencias.\n\n\u2022\tXML-Roberta: Son del tipo < s > A < \/s > < \/s > B < \/s >, siendo A y B un par de sentencias.","17a53499":"### Conclusiones analisis de modelos\n\nAunque ambos modelos sobreentrenan a partir de la 3\u00aa iteraci\u00f3n, (momento en el que se cruzan las funciones de p\u00e9rdida del entrenamiento y validaci\u00f3n) en dicho punto la precisi\u00f3n del XLM-Roberta es mayor que la de BERT, por lo que utilizaremos el **XML-Roberta** a partir de este punto.","84348773":"* **Masked Language Modeling**\n\nBERT es un modelo altamente bidireccional.\n\nA diferencia de modelos contexto de izquierda a derecha (o viceversa), que son entrenados para predecir la siguiente palabra de la oraci\u00f3n, y por lo tanto susceptibles a error por perdida de informaci\u00f3n, como ocurre con el modelo GPT; el modelo ELMo, trat\u00f3 de solventar dicho problema entrenando dos LSTM de izquierda a derecha y de derecha a izquierda y concatenandolos a posteriori, pero no result\u00f3 lo suficientemente preciso.\n\nA continuaci\u00f3n podemos ver el esquema de flujos de informacion de BERT, OpenAI-GPT y ELMo:","c0482bc1":"No nos excederemos m\u00e1s en este optimizador, ya que no es tan potente para el problema que estudiamos como los otros dos optimizadores.","8fc7f011":"## Descargamos dataset y EDA","1fb02867":"Ahora revisaremos el comportamiento de las funciones de p\u00e9rdida y de precisi\u00f3n, para poder resolver si el modelo esta sobreentrenado o hay que seguir iterando.","35cb201e":"En orden de aumentar el ponderamiento del conjunto de datos original, y que por lo tanto, tenga m\u00e1s peso sobre el an\u00e1lisis, vamos a realizar un data augmentation de tipo data-space:\n\n1. Traduciendo las frases no inglesas a ingl\u00e9s.\n\n2. Traduciendo frases en ingl\u00e9s a lenguas escogidas aleatoriamente.","22343102":"### XNLI","a1278753":"* **Pre-Procesamiento de texto**\n\nLos desarrolladores de BERT, han establecido una serie de reglas a la hora de introducir los imputs.\n\nCada input embedding es una combinaci\u00f3n de 3 embeddings:\n\n1. Position Embeddings: El modelo usa y conoce la posici\u00f3n de los distintos vocablos en el input introducido, lo cual lo diferencia de las Redes Neuronales Recurrentes (RNN) que no son capaces de captar informaci\u00f3n de \"orden\" o \"secuencia\".\n\n2. Segment Embeddings: Aprende una inserci\u00f3n \u00fanica para la primera y segunda oraci\u00f3n y ayuda al modelo a distinguir entre ellas. Esto tambi\u00e9n genera problemas a la hora de calcular loss functions si una misma primera oraci\u00f3n se repite con distintas segundas oraciones, lo cual ocurre en la base de datos SNLI.\n\n3. Token Embeddings: De esta \u00faltima es de donde aprenden los distintos token espec\u00edficos procedentes del WordPiece.","f3d211f0":"Importamos el resto de librer\u00edas que utilizaremos:","7df15407":"**Y su label, que como se puede observar, corresponde a una contradicci\u00f3n.","952c42d3":"### Train, Validation y Test Sets en Machine Learning\n\nA continuaci\u00f3n, explicaremos las diferencias entre estos tres conceptos.\n\n1. Training Dataset\n\n**Definici\u00f3n:** Una muestra de datos utilizada para ajustar el modelo.\n\nEn caso de una red neuronal, los weights y bias. El modelo observa y aprende de estos datos.\n\n2. Validation Dataset\n\n**Definici\u00f3n:** Es una muestra de datos utilizada para proporcionar una evaluaci\u00f3n del entrenamiento del modelo, **mientras** se ajustan los hiperparametros en el entrenamiento. La evaluaci\u00f3n se va volviendo m\u00e1s sesgada a medida que el conjunto de datos de validaci\u00f3n se va incorporando al modelo en las distintas iteraciones.\n\nSe usan los datos para ajustar los hiperpar\u00e1metros. As\u00ed, el modelo ve los datos, pero no aprende de ellos. El modelo recoge los resultados evaluados en el conjunto de validaci\u00f3n y actualiza los hiperpar\u00e1metros. \n\nSe entiende as\u00ed que el conjunto de validaci\u00f3n afecta al modelo, pero de manera indirecta, es decir: a trav\u00e9s de los resultados obtenidos al evaluar el modelo con dichos datos.\n\nTambi\u00e9n se le conoce (aunque con menos frecuencia) como Development Dataset, y tiene sentido ya que ayuda al modelo en su etapa de \"desarrollo\".\n\n3. Test Dataset\n\n**Definici\u00f3n:** Es el conjunto de datos utilizado para evaluar de manera **insesgada** el ajuste final del modelo, que ha sido ajustado con los datos de entrenamiento. Este conjunto de datos solo se utiliza cuando el modelo esta completamente entrenado, y dicho conjunto debe estar cuidadosamente muestreado con las diferentes clases al que se enfrentar\u00eda el modelo en el mundo real.\n\nLas diferencias entre el concepto de test en data science y kaggle, las veremos al final de este trabajo.","d74a177b":"Cargamos el set.","4a91f0ad":"## Caracter\u00edsticas de los modelos\n\n### Overview\n\n* BERT fue lanzado en noviembre de 2018 por Google, con modelos para Ingl\u00e9s y Chino, poco despu\u00e9s incluyeron un modelo m\u00e1s, el mBERT, que inclu\u00eda 104 lenguas diferentes.\n\n* XLM-RoBERTa fue lanzando en noviembre de 2019 por Facebook, incluyendo 100 lenguas diferentes.\n\nSi bien es cierto que ambos son modelos multilenguaje, hay un elemento que en un principio parece darle un mayor protagonismo a XLM-R sobre BERT:\n\nAnalistas de la Charles University en Praga, encontraron que mBERT internamente clusteriza lenguas en \"familias\" y representa oraciones similares de manera diferente en diferentes idiomas. Esto ayudar\u00eda a explicar que clasifique mejor en unas pocas lenguas (ingles sobre todo) frente a otras.\nEsta diferencia, tienen un efecto en la precisi\u00f3n a la hora de clasificar y pueden observarse en la siguiente tabla (obtenida del  Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov). \n","ba8197bb":"Y finalmente lo entrenaremos:","b46e579f":"### Fine Tuning","b99eddd1":"Primero comprobaremos el **learning rate**.\n\nEsta parte es muy importante, puesto que:\n\n* Si la tasa de entrenamiento es demasiado peque\u00f1a, el entrenamiento, a\u00fan siendo m\u00e1s confiable, requiere de m\u00e1s steps.\n\n* Si la tasa de entrenamiento es demasiado grande, es posible que el entrenamiento no consiga la convergencia esperada, incluso podr\u00eda divergir.\n\nSi bien en el Algorithm 1, del ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION de Diederik P. Kingma y Jimmy Lei Ba, muestra que una buena configuraci\u00f3n predeterminada para los problemas de aprendizaje autom\u00e1tico probados es lr = 0.001, \u03b21 = 0.9, \u03b22 = 0.999 and E = 10\u22128, en este problema en concreto, parece funcionar mejor un **lr = 2e-5**.\n","3ec67800":"Y reiniciearemos el TPU, ya que se encuentra saturado por los c\u00e1lculos anteriores y no ser\u00eda capaz de mover el modelo final","3d6c6930":"### MNLI","8da59796":"Estableceremos una semilla, para mantener la elecci\u00f3n de lenguas.","6f23c5e2":"Finalmente, realizamos la traducci\u00f3n:","a36a3058":"## Definici\u00f3n de las principales variables ","0bf05b3b":"### Entrenamiento del modelo definitivo","6ee39378":"* **Arquitectura del modelo**\n\nPara empezar, hemos de saber que existen dos tama\u00f1os distintos para el modelo BERT:\n\n1. BERT BASE: con 12 capas, 12 attention heads y 110 millones de par\u00e1metros.\n\n2. BERT LARGE: con 24 capas, 16 attention heads y 340 millones de par\u00e1metros.","e3e9762a":"# iii. Data preparation y carga de librerias necesarias\n\nLo primero que haremos es actualizar la librer\u00eda transformers de Hugging Face, para evitar problemas a la hora de conectar con ciertos tokenizadores, como ocurr\u00eda en versiones anteriores.","bc7ae9a2":"El esquema de entrada seria as\u00ed:","15c3cb70":"* **Tokenizaci\u00f3n multilenguaje**\n\nSe entrena el Sentence Piece Model (SPM) y se aplica directamente en los datos de texto sin procesar. No se observa p\u00e9rdida de rendimiento con respecto a modelos entrenados con preprocesamiento espec\u00edfico del lenguaje y codificaci\u00f3n tipo byte-pair, como puede observarse en la siguiente figura.","c4dff701":"Cortamos el conjunto de entrenamiento original en entrenamiento y validaci\u00f3n.","dfa9ddf7":"* **Training Data**\n\nLa base de datos usada para entrenar a XML-RoBERTa, es significativamente mayor que la usada por BERT, ya que este usa Wiki-100, una base de datos procedente de Wikipedia, mientras que XML-R usa CommonCrawl. Se utiliz\u00f3 CommonCrawl ya que aumenta considerablemente el conjunto de datos, sobre todo para idiomas de bajos recursos, como el Birmano y el Suajili. Comprobamos esto en el gr\u00e1fico mostrado a continuaci\u00f3n:","4ded998b":"**ADAM**\n\nEs un m\u00e9todo de optimizaci\u00f3n estoc\u00e1stica que solo requiere gradientes de primer orden con relativa poca memoria. \n\nCalcula las tasas de aprendizaje adaptativo individuales para los diferentes par\u00e1metros a partir de estimaciones del primer y segundo momento de los gradientes. El nombre proviene de ADAptative Moment estimation. \nEst\u00e1 pensado para combinar las ventajas de dos m\u00e9todos recientemente populares: AdaGrad que funciona bien con gradientes dispersos y RMSProp que funciona bien en configuraciones on-line y no estacionarias.\n\nAlgunas de sus ventajas son:\n\n* Las magnitudes de las actualizaciones de los par\u00e1metros son invariantes al cambio de escala del gradiente.\n\n* Los tama\u00f1os de los steps est\u00e1n aproximadamente delimitados por el hiperpar\u00e1metro stepsize.\n\n* No requiere un par\u00e1metro estacionario objetivo.\n\n* Trabaja con gradientes reducidos.\n\nSu algoritmo ser\u00eda el siguiente:","afdacbcc":"A la hora de usar datasets extra hay que tener en cuenta que si el conjunto de datos es diferente, probablemente sus caracter\u00edsticas tengan diferentes significados y tama\u00f1os.\n\nEstos datos deber\u00edan reunir el significado, la interpretaci\u00f3n y el tama\u00f1o del conjunto de entrada original a estudiar, ya que de otra forma, la interpretabilidad y la confianza del modelo ser\u00edan menos \u00fatiles que el conjunto original de datos \u00fanicamente.\n\nPor lo que en nuestro caso, lo hacemos con un gran cuidado y fijando unos par\u00e1metros:\n\n1. Tama\u00f1o: Las frases introducidas en todos los conjuntos que a\u00f1adimos al modelo, son de una longitud similar.\n\n2. Interpretaci\u00f3n: En este punto tambi\u00e9n coinciden, ya que son todas pares de sentencias de premisas e hip\u00f3tesis junto con un label de relaci\u00f3n entre ambas.\n\n3. Estructura de los datos: Si bien XNLI tiene una distribuci\u00f3n de idiomas parecida al conjunto original, no es el mismo, y una vez realicemos el data augmentation, esta proporci\u00f3n quedar\u00eda m\u00e1s desvalanceada. De esta forma,  solucionamos esta eventualidad introduciendo una muestra del MNLI (que s\u00f3lo son sentencias en ingl\u00e9s) para mantener la proporci\u00f3n de idiomas del conjunto de datos original.","9a7e5f60":"### ADAMAX\n\nEste modelo se basa en Adam, pero generalizando la regla de actualizaci\u00f3n L^2 a una L^p. Esta variante se vuelve num\u00e9ricamente inestable para grandes p, es decir cuando p->\u221e, pero en este caso, surge un algoritmo que estabilizar\u00eda la funci\u00f3n (cuya demostraci\u00f3n matem\u00e1tica no incluir\u00e9 en este trabajo. Puede consultarse f\u00e1cilmente en \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\" de Diederik P. Kingma y Jimmy Lei Ba).\n\nA continuaci\u00f3n, veremos el algoritmo completo de ADAMAX, una vez sustituido por la nueva regla de actualizaci\u00f3n, quedando as\u00ed como una variante de Adam basada en un p que tiende a infinito.\n\nAdamax es a veces superior a Adam, especialmente en modelos con embeddings.","e7230b63":"Ahora revisaremos el comportamiento de las funciones de p\u00e9rdida y de precisi\u00f3n, para poder resolver si el modelo esta sobreentrenado o hay que seguir iterando","d8f91418":"Revisamos la distribuci\u00f3n de idiomas en el training set.","d4eea090":"Convertimos a dataframe los subconjuntos, ya que en este dataset hay 3 subconjuntos de datos \"train\", \"validation_matched\" y \"validation_mismatched\"","4f4d1443":"Podemos observar que el conjunto de datos final, guarda unas proporciones de idiomas muy similar al conjunto original, as\u00ed como al conjunto de test de esta competici\u00f3n.\n\nLo cual es muy interesante de cara al \u00faltimo paso: la confecci\u00f3n del submission.","2b6065e1":"Revisamos el subconjunto","3c6bb2a6":"### Descargaremos las librer\u00edas necesarias","99fb161b":"# **ii.\tIntroducci\u00f3n y objetivos buscados**\n\nEste trabajo es parte de la evaluaci\u00f3n del \u201cM\u00e1ster Big Data y Business Analytics UNED 2019\/2020 - UNED\u201d, y en \u00e9l se desarrolla el an\u00e1lisis, dentro de la disciplina deep learning, de la competici\u00f3n de Kaggle \u201cContradictory, My Dear Watson. Detecting contradiction and entailment in multilingual text using TPUs\u201d.\n\nEl **objetivo general** de este trabajo es encontrar un modelo NLI (Natural Language Inference) que asigne correctamente etiquetas correspondientes a implicaci\u00f3n, neutralidad o contradicci\u00f3n a pares de premisas e hip\u00f3tesis, sea cual sea el idioma en el que se encuentren y mejorar su capacidad de predicci\u00f3n.\n\nLos **objetivos espec\u00edficos**, son:\n\n1.\tEncontrar el modelo de lenguaje que mejor clasifique los pares de sentencias del data-set a analizar, entre BERT y XML-Roberta.\n\n\n2.\tEntrenar el modelo escogido con bases de datos distintas a las proporcionadas inicialmente.\n\n\n3.\tRealizar un \"data augmentation\", para evitar problemas de sobreentrenamiento y mantener la estructura del data set original.\n","82d7c9b7":"Podemos comprobar que las curvas tienen un comportamiento adecuado, y obtenemos un modelo con una precisi\u00f3n en el conjunto de validaci\u00f3n cercana al 90% y con una funci\u00f3n de p\u00e9rdida muy baja, lo que pronostica un buen ajuste con los datos del test.","6a15139a":"## Test en Data Science y Test en Kaggle\n\nLlegados a este punto, se requiere aclarar la diferencia entre ambos conceptos:\n\n* Test en Data Science: Se utiliza para evaluar la capacidad de un modelo candidato a la hora de analizar datos, extraer informaci\u00f3n, sugerir conclusiones y respaldar la toma de decisiones, y los datos surgen de un segmento de datos del conjunto de datos a analizar. Dichos datos nunca son conocidos por el modelo en su fase de entrenamiento, tan solo son introducidos al final de esta para comprobar la calidad del mismo.\n\n* Test en Kaggle: Se trata de un conjunto de datos, sin los labels correspondientes, proporcionados por Kaggle para comprobar la puntuaci\u00f3n obtenida en cada competici\u00f3n ofrecida por la plataforma.\n","a8262edb":"# vii. Entrenamiento final del modelo","876be66e":"# vi. Data Augmentation","40fe615e":"Revisamos las primeras filas. Como puede observarse, se respetan tanto las tildes como los distintos grafos utilizados por las diferentes lenguas.","987919e8":"Ahora la hip\u00f3tesis.","ffd41f4f":"Convertimos a dataframe. En \u00e9ste, en cambio, s\u00f3lo hay un subconjunto de datos","9772efa2":"Comprobamos que tenemos la versi\u00f3n de transformers correcta.","cf7f607b":"Bert es b\u00e1sicamente un Transformer Encoder stack entrenado, en el que se le introducen unos inputs compuestos de tokens, los cuales son procesados y arroja unos outputs numericos.","f516c22f":"Un esquema aproximado de las proporciones de datos que debe incorporar cada conjunto de datos, ser\u00eda el siguiente:","f715ed4c":"### Especificaciones y funcionamiento de XML-RoBERTa","8058ae88":"### Overfitting y Undercomputing\n\nEl problema central en machine learning es el aprendizaje supervisado. \n\nLos algoritmos de aprendizaje esencialmente buscan un espacio de funciones (llamadas normalmente hypothesis class) para una funci\u00f3n dada que encaje en un conjunto de datos dado.\n\nEsto lo diferencia de la programaci\u00f3n tradicional a un nivel primario, as\u00ed, mientras que en la programaci\u00f3n tradicional se trata de escoger un programa o funci\u00f3n dada, introducirle datos, y nos da un output (o respuesta esperada), en machine learning, se trata de lo opuesto; se le proporciona datos y respuestas y nos tiene que devolver un programa o funci\u00f3n que encaje a ambos.","dfb05ee3":"### ADAM, ADAMAX y SGD","439cdcff":"## XML-Roberta","dbd87f82":"# viii. Conclusiones finales\n\nSe ha conseguido alcanzar tanto el objetivo general como los espec\u00edficos. \n\nAlcanzando finalmente un modelo que pronostica la relaci\u00f3n sem\u00e1ntica entre las dos frases cualesquiera que le proporcionemos en cualquier idioma, con una precisi\u00f3n de casi un noventa por ciento.\n\nY tambi\u00e9n queda evidenciado que todo esto ha sido materialmente posible gracias a la implementaci\u00f3n de unidades de procesamiento tensorial (TPUs), sin las cuales, realizar un an\u00e1lisis de este tipo requer\u00eda una cantidad de tiempo prohibitiva, mientras que utilizando estas unidades, cada c\u00e1lculo apenas ha necesitado unos pocos minutos.","f550e36e":"Ahora construiremos el modelo dentro del TPU, lo haremos a trav\u00e9s de la orden strategy.scope() de forma que todo estar\u00e1 dentro del scope de la strategy y no lo correr\u00e1 el CPU por defecto.","bf5d4238":"Debido a que el n\u00famero de hypothesis functions es exponencialmente enorme, no se puede examinar individualmente cada una de ellas. En lugar de eso, se suele formalizar definiendo una funcion objetiva (por ejemplo, el n\u00famero de puntos incorrectamente predichos) y aplicando varios algoritmos que minimicen esa funci\u00f3n objetivo (suelen ser algoritmos heur\u00edsticos como el *gradient descent*).\n\nLa clave del problema surge de la tarea asignada al proble de machine-learning. Un algoritmo es entrenado con un conjunto de datos de entrenamiento, pero una vez entrenado es aplicado para realizar predicciones de datos nuevos. El **objetivo \u00faltimo** es maximizar su precisi\u00f3n predictiva de estos nuevos datos, no la precisi\u00f3n en los datos de entrenamiento.\n\nSi trabaja excesivamente en estos datos de entrenamiento, absorbe sus peculiaridades (que no tienen por qu\u00e9 repetirse en los nuevos datos que queremos tratar), por lo tanto, genera un ruido no deseado en lugar de encontrar una regla de predicci\u00f3n general. Este fen\u00f3meno se denomina **overfitting** o sobre-entrenamiento.\n\nAs\u00ed, un algoritmo demasiado codicioso a la hora de encontrar un \u00e1rbol de decisi\u00f3n o red neuronal que minimice la funci\u00f3n objetivo al m\u00e1ximo, en teor\u00eda, la \u00f3ptima, podr\u00eda no ser v\u00e1lido para nuestro objetivo \u00faltimo, mientras que otro que encuentre un resultado sub-\u00f3ptimo durante el entrenamiento, podr\u00eda resultarnos el resultado \u00f3ptimo a la hora de predecir nuevos datos. A este concepto se le denomina *undercomputing*, y actuar\u00eda efectivamente a la hora de evitar el overfitting.","9c56ebe7":"Convertimos los diferentes conjuntos de datos en tf.data.Dataset.","795c780d":"Y finalmente lo entrenaremos:","6d9639d0":"### EDA","952f83d7":"Ahora cargaremos dos conjuntos de datos extra;\n\nEl corpus **MNLI**: Es una colecci\u00f3n de m\u00faltiples fuentes con 433.000 pares de oraciones anotadas con informaci\u00f3n sobre la vinculaci\u00f3n sem\u00e1ntica entre ellas. Todas en ingl\u00e9s.\n\nEl corpus **XNLI**: Es una colecci\u00f3n de 7500 pares de de oraciones, traducidas a 15 idiomas. Lo que da un total de 112.500 pares de oraciones.","a657a657":"Cargamos el dataset","4c961e55":"Cortamos el conjunto de entrenamiento original en entrenamiento y validaci\u00f3n","93797c30":"El set de entrenamiento contiene una premisa, una hip\u00f3tesis, un nivel dado (0= Implicaci\u00f3n, 1=neutral y 2=contradicci\u00f3n) y el lenguaje del texto. \n\nSon 12120 binomios de hip\u00f3tesis\/premisa en varios idiomas (\u00c1rabe, b\u00falgaro, chino, alem\u00e1n, griego, ingl\u00e9s, espa\u00f1ol, franc\u00e9s, hindi, ruso, swahili, tailand\u00e9s, turco, urdu y vietnamita). \n\nPuede comprobarse en: https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data","823875aa":"Las flechas representan la direcci\u00f3n de la informaci\u00f3n de una capa a otra. Se puede apreciar facilmente que BERT es bidireccional, GPT es unidireccional y ELMo es artificialmente \"bidireccional\".\n\n","b3f831b7":"Como podemos observar, no hay ning\u00fan valor null","eab9e632":"## BERT","85e322fc":"## Configuraci\u00f3n de TensorFlow\n\nTensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que les permite a los investigadores impulsar un aprendizaje autom\u00e1tico innovador y, a los desarrolladores, compilar e implementar con facilidad aplicaciones con tecnolog\u00eda de AA.\n\nConfiguraremos nuestro TPU, siendo este uno de tipo V3-8.\n\nEste tipo define tanto la versi\u00f3n de TPU (v3) como la cantidad de n\u00facleos de \"TPU-v3\" de que dispone.\n\nContamos con 8 y la cantidad de memoria de TPU que est\u00e1 disponible para la carga de trabajo de aprendizaje autom\u00e1tico; que son 128 GiB, as\u00ed como las zonas disponibles, que en este caso son us-central1-a,b y f.","a79d26c5":"* **Arquitectura del modelo**\n\nPara empezar, hemos de saber que existen dos tama\u00f1os distintos para el modelo BERT:\n\nXML-R BASE: con 12 capas, 12 attention heads y 270 millones de parametros.\n\nXML-R LARGE: con 24 capas, 16 attention heads y 550 millones de parametros.","c3678fee":"### SGD\n\nEste optimizador, realizara una actualizaci\u00f3n de par\u00e1metros para cada ejemplo de entrenamiento y su label.\n\n\u03b8=\u03b8\u2212\u03b7\u22c5\u2207\u03b8J(\u03b8;x(i);y(i))\n\nLa fluctuaci\u00f3n del SGD salta a m\u00ednimos locales nuevos y potencialmente mejores. Por otro lado, esto finalmente complica la convergencia al m\u00ednimo exacto, ya que SGD seguir\u00e1 excedi\u00e9ndose. Sin embargo, se ha demostrado que cuando disminuimos lentamente el learning rate (la tasa de aprendizaje), SGD converge a un m\u00ednimo local o global para la optimizaci\u00f3n no convexa y convexa, respectivamente.\n\nComo queda bien ilustrado en el siguiente grafico.","0e9aab86":"Ahora construiremos el modelo dentro del TPU, lo haremos a trav\u00e9s de la orden strategy.scope() de forma que todo estar\u00e1 dentro del scope de la strategy y no lo correr\u00e1 el CPU por defecto.","6d4afc96":"Y ahora ajustaremos **epsilon**, al cual definiremos en 5e-08.","14415b6f":"La estructura de estos tokens, son explicados m\u00e1s adelante en el apartado \"Similitudes\".\n\nEn cuanto al output, ser\u00eda un vector num\u00e9rico, siguiendo el siguiente esquema:","6446f81c":"Comprobaremos que mantienen tanto el formato, como la proporci\u00f3n de lenguas inglesas y no inglesas de comienzo.","8bc192f5":"# v. Dataset Extra","c7fffd02":"* Jeff Dean, Rajat Monga: TensorFlow. Disponible en:https:\/\/www.tensorflow.org\/\n\n* Towards Data Science Inc. TowardsDataScience. Disponible en:https:\/\/towardsdatascience.com\/how-to-use-dataset-in-tensorflow-c758ef9e4428\n\n* Diederik P. Kingma y Jimmy Lei Ba: \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\", *a conference paper at ICLR 2015*, disponible en: https:\/\/arxiv.org\/pdf\/1412.6980.pdf\n\n* Tarang Shah: \"About Train, Validation and Test Sets in Machine Learning\", *towards data science*, 06\/12\/2017. Disponible en: https:\/\/towardsdatascience.com\/train-validation-and-test-sets-72cb40cba9e7\n\n* Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: \"Unsupervised Cross-lingual Representation Learning at Scale\", *Cornell University*, 05\/11\/2019. Disponible en: https:\/\/arxiv.org\/abs\/1911.02116\n\n* Hugging Face, Inc.https:\/\/huggingface.co\/\n\n* Fernando P\u00e9rez: Colaboratory. Disponible en: https:\/\/colab.research.google.com\/\n\n* Nick Doiron: \"A whole world of BERT\", *Medium*, 30\/01\/2020. Disponible en: https:\/\/medium.com\/@mapmeld\/a-whole-world-of-bert-f20d6bd47b2f\n\n* Rick Merritt: \"BERT Does Europe: AI Language Model Learns German, Swedish\", *Blogs Nvidia*, 23\/12\/2019. Disponible en: https:\/\/blogs.nvidia.com\/blog\/2019\/12\/23\/bert-ai-german-swedish\/\n\n* xhlulu: \"Concise Keras XLM-R on TPU\", *kaggle public notebooks*, 01\/08\/20. Disponible en: https:\/\/www.kaggle.com\/xhlulu\/contradictory-watson-concise-keras-xlm-r-on-tpu\n\n* AdityaMishra: \"NLI - Data Translation Augmentation\", *kaggle public notebooks*, 01\/08\/20. Disponible en: https:\/\/www.kaggle.com\/aditya08\/nli-data-translation-augmentation\n\n* Yih-Dar SHIEH: \"More NLI datasets - Hugging Face nlp library\", *kaggle public notebooks*, 01\/08\/20. Disponible en: https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets-hugging-face-nlp-library\n\n* Tom Dietterich: \"Overfitting and Undercomputing in Machine Learning\", *Departament of Computer Science, Oregon State University, Corvallis*. Disponible en: https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/212094.212114?casa_token=vqzbFaLf3noAAAAA%3AN7JvZ6jkm5st8nq0j5uodFET_r6Cc7ompQBa5y6HoiiOjp0_Q5R2k8gU_5lH5K19MKZWPQP4mNOVuw\n\n* Sebastien C. Wong, Adam Gatt,Victor Stamatescu and Mark D. McDonnell: \"Understanding data augmentation for classification: when to warp?\", *2016 IEEE*. Disponible en: https:\/\/arxiv.org\/pdf\/1609.08764.pdf\n\n* Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer y Veselin Stoyanov: \"Unsupervised Cross-lingual Representation Learning at Scale\". Disponible en: https:\/\/arxiv.org\/pdf\/1911.02116.pdf\n\n* Sebastian Ruder: \"An overview of gradient descent optimization algorithms\", 19\/01\/2016. Disponible en: https:\/\/ruder.io\/optimizing-gradient-descent\/"}}