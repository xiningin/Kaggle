{"cell_type":{"a80bbe34":"code","e3ce67a9":"code","e7800908":"code","026f0a21":"code","63b838e3":"code","462c92a2":"code","a6c1fe4a":"code","6d87e36b":"code","8ed7a259":"code","601cc36f":"code","86dac7a8":"code","4d1d0b98":"code","ebdf379d":"code","4f7b2007":"code","589ab683":"code","ec90687a":"code","d15bcddd":"code","4de2c50f":"code","25b70d64":"code","13d5b0a9":"code","1c6e9233":"code","b6a235ad":"code","bc3aa1c8":"code","fe72a0ef":"code","5730debf":"code","40c19217":"code","73faa2b1":"code","b77a44e1":"code","52055519":"code","8e31c2ac":"code","4fe099c4":"code","461473fd":"code","79ee4c23":"code","ff9b6129":"code","79a0090e":"code","c2c5b930":"code","0123f8b9":"code","04d6ec9a":"code","b05dc583":"code","8c5e548e":"markdown","5d1ed9ad":"markdown","314a8913":"markdown","ebdb9ec1":"markdown","c00102ad":"markdown","a75efaa2":"markdown","5b9713bc":"markdown","fa731fcb":"markdown","aa9ea17b":"markdown","9fcb7898":"markdown","2ac18b18":"markdown","e3b1e3fc":"markdown","61ec7c31":"markdown","2a1a37d5":"markdown","4c52931c":"markdown","fc534dd3":"markdown","bd7e888c":"markdown","28795684":"markdown","c7b91441":"markdown","4de76211":"markdown","9d720f32":"markdown"},"source":{"a80bbe34":"import re\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom stop_words import get_stop_words\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","e3ce67a9":"# Get stop words\nstop_words = get_stop_words('english')","e7800908":"# Tokenise and filter \ndef tokenise(myStr):\n\n    # r is for python raw string, to avoid backslashes issues\n    cleanText = re.compile(r'[^0-9a-zA-Z:\\-|\\\/\\\\_@#]')\n    tokens = cleanText.split(myStr)\n\n    wordList = []\n\n    for t in tokens:\n        if (t not in stop_words) and (len(t)>1):\n            wordList.append(t)\n\n    return wordList","026f0a21":"# Read data, remove whitespace, and lower case\ndata = pd.read_csv('\/kaggle\/input\/tweetsentiments\/tweets_sentiment.csv', encoding = 'ISO-8859-1', usecols=[0, 1])\ndata['SentimentText'] = data['SentimentText'].str.strip()\ndata['SentimentText'] = data['SentimentText'].str.lower()","63b838e3":"# Tokenise and filter each tweet\nlistOfLists = []\nfor i in range(len(data)):\n    listOfLists.append(tokenise(data['SentimentText'][i]))\ndata['Tokens'] = pd.DataFrame({'Tokens': listOfLists})","462c92a2":"# Counts to see the dataset's ratio balance\ntotal_positive_count = (data['Sentiment']==1).sum()\ntotal_negative_count = (data['Sentiment']==0).sum()\n\ntotal_prob_positive_count = total_positive_count\/len(data)\ntotal_prob_negative_count = total_negative_count\/len(data)","a6c1fe4a":"# Total number of tweets\nlen(data)","6d87e36b":"# Positive and negative tweets count\nprint(\"Positive:\", total_positive_count, \"\\nNegative:\", total_negative_count)","8ed7a259":"# We can see that the data is balanced between the positive and negative tweets\nprint(\"Positive:\", total_prob_positive_count, \"\\nNegative:\", total_prob_negative_count)","601cc36f":"# The average length of tweets\nprint('The mean number of words in a tweet is',(data['Tokens'].str.len()).mean())","86dac7a8":"# The average length of tweets\nprint('The standard deviation of words in a tweet is',(data['Tokens'].str.len()).std())","4d1d0b98":"# Show the longest tweetsk\ndata['Tokens'].str.len().sort_values(ascending=False).head()","ebdf379d":"# This converts the list of words into space-separated strings\ndata['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))","4f7b2007":"# Transform the data into occurrences\ncount_vect = CountVectorizer()  \ncounts = count_vect.fit_transform(data['Tokens'])  ","589ab683":"# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)","ec90687a":"counts.shape","d15bcddd":"# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n","4de2c50f":"class_names = ['Negative', 'Positive']","25b70d64":"# Plot confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap='coolwarm'):\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    plt.tight_layout()","13d5b0a9":"def make_model(model, name):\n\n    # Train the model using the training sets \n    model.fit(train_x, train_y)\n    \n    # Training score output\n    print('Training score:', model.score(train_x, train_y))\n\n    # Predict Output \n    predicted = model.predict(test_x)\n\n    # Testing score output\n    print('Testing score:', model.score(test_x, test_y))\n    \n    # Mean square error\n    print('Mean-squared error:', mean_squared_error(test_y, predicted))\n    \n    # Confusion matrix\n    cnf_matrix = confusion_matrix(test_y, predicted)\n    print('Confusion Matrix without normalisation:\\n', cnf_matrix)  \n    \n    # Draw non-normalised confusion matrix\n    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False, title='Confusion matrix without normalisation')\n    \n    # Plot normalized confusion matrix\n    #plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Confusion matrix with normalisation')\n    \n    \n    # Save plot\n    plt.savefig(name+'.png', bbox_inches='tight', dpi=100)\n    \n    # Show\n    plt.show()","1c6e9233":"make_model(MultinomialNB(fit_prior=False), 'MNBNoPrior')","b6a235ad":"make_model(MultinomialNB(fit_prior=True), 'MNBWithPrior')","bc3aa1c8":"from sklearn.linear_model import SGDClassifier\n\nmake_model(SGDClassifier(loss='hinge', penalty='l2', random_state=42), 'LinearSGD')\n","fe72a0ef":"make_model(LogisticRegression(), 'LogisticalReg')","5730debf":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\nsvd.fit(train_x)  ","40c19217":"print(svd.explained_variance_ratio_)","73faa2b1":"print(svd.explained_variance_ratio_.sum())","b77a44e1":"print(svd.singular_values_)","52055519":"# This converts the list of words into space-separated strings\n# data['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,2))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","8e31c2ac":"make_model(LinearSVC(), 'SVC_1_2')","4fe099c4":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,3))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","461473fd":"make_model(LinearSVC(), 'SVC_1_3')","79ee4c23":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,4))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","ff9b6129":"make_model(LinearSVC(), 'SVC_1_4')","79a0090e":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(2,2))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_2_2')","c2c5b930":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(2,3))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_2_3')","0123f8b9":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,5))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_1_5')","04d6ec9a":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,6))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_1_6')","b05dc583":"# Data for plotting\nt = np.arange(2, 7)\ns = [0.79292, 0.79574, 0.79589, 0.79508, 0.79439]\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='Max n_gram', ylabel='Score',\n       title='Max n_gram vs score')\nax.grid()\n\nplt.savefig('Plot.png', bbox_inches='tight', dpi=100)\n\nplt.show()","8c5e548e":"# Explore Data","5d1ed9ad":"### Support Vector Machines","314a8913":"# Train","ebdb9ec1":"# Model Function","c00102ad":"### SVD \/ LSA\n\nSVD is essentially doing a variation of a PCA, and is reducing the number of columns you have (100k), to a chose n_components)","a75efaa2":"# Libraries","5b9713bc":"### 2, 2","fa731fcb":"### Naive-Bayes","aa9ea17b":"### 1,2","9fcb7898":"## Dependent Treatment","2ac18b18":"### 1,4","e3b1e3fc":"# Task 2","61ec7c31":"### 2,3","2a1a37d5":"### 1, 6","4c52931c":"### Plot ngram_max value vs score","fc534dd3":"###\u00a0Linear Classifier (Logistic Regression)","bd7e888c":"# Algorithms ","28795684":"### 1, 5","c7b91441":"## Independent Treatment","4de76211":"# Pre-processing","9d720f32":"### 1,3"}}