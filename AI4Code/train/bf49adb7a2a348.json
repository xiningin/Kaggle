{"cell_type":{"def1f5c5":"code","1a1d588a":"code","4ce5e8d5":"code","fb77f367":"code","1a68835a":"code","c4139f15":"code","8acc4efc":"code","e7387e3b":"code","d03695f8":"code","a86aa9e8":"code","299a0cbc":"code","e8b79824":"code","46eb0954":"code","3d2bd243":"code","e4df5394":"code","4218f3eb":"code","6c0d1508":"code","349e075c":"code","697de580":"code","a50b215a":"code","815f81fe":"code","e6180208":"code","4c1bcf73":"code","e0696c37":"code","c05f1f4e":"code","d0ea4f8e":"code","34f7bf6d":"code","174c49fc":"code","bdcdee7d":"code","6a94117b":"code","616c3cff":"code","c2a6e629":"code","fc640ec4":"code","28209735":"code","d5420b70":"code","0bbf9229":"code","57de0406":"code","d845d0de":"code","06165ed7":"code","d499ac95":"code","3284b6bb":"code","559dbf5a":"code","43e6f384":"code","8aef9fd5":"code","ad171bb7":"code","c15e9d69":"code","61797316":"code","68da5957":"code","1cc9e24a":"code","66640cd1":"code","fa11131f":"code","d9b1f27f":"code","ce7c2c2b":"code","74b1e727":"code","7957b37b":"code","4ead207c":"code","e877a79a":"code","c1d87469":"code","6fa91721":"code","d5383ef0":"code","6573648f":"code","da66ff55":"code","23d3b3d0":"code","e847c577":"code","e067a875":"code","9725deaa":"code","c76439c3":"code","03848ccb":"code","7e10aaf6":"code","75cc8901":"code","ee75e43e":"code","ff2a9d2c":"code","19c618da":"markdown","85ffb747":"markdown","15a686c1":"markdown","7c460a77":"markdown","fa4664f2":"markdown","fef46fbe":"markdown","d8583f66":"markdown","41659b25":"markdown","7ef3a5d1":"markdown","e81cdcf5":"markdown","0137c6a3":"markdown","92a181d7":"markdown","f43b2844":"markdown","60f54c7c":"markdown","3e80a81a":"markdown","7342850b":"markdown","74b4ad19":"markdown","ce7f8e1d":"markdown","e97433e9":"markdown","2ccdf03f":"markdown","8887678e":"markdown","a64210b6":"markdown","3270d8bc":"markdown","6c87f5f1":"markdown","c8ff3d2f":"markdown","56148f86":"markdown","247717de":"markdown","7366ca4e":"markdown","14034c9d":"markdown","731808f8":"markdown","913de280":"markdown","19ffb8fb":"markdown"},"source":{"def1f5c5":"import pandas as pd #Data Analysis\nimport numpy as np #Linear Algebra\nimport seaborn as sns #Data Visualization\nimport matplotlib.pyplot as plt #Data Visualization\\\n%matplotlib inline","1a1d588a":"import json\nimport string\nfrom pandas.io.json import json_normalize\ncolor = sns.color_palette()\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","4ce5e8d5":"import os\nprint(os.listdir(\"..\/input\"))","fb77f367":"#This is the Product_sales_train_and_test dataset but without the \"[]\" in the Customer Basket.\ndf=pd.read_csv(\"..\/input\/removed\/\/data.csv\")","1a68835a":"train=pd.read_csv(\"..\/input\/discount-prediction\/Train.csv\")","c4139f15":"test=pd.read_csv(\"..\/input\/discount-prediction\/test.csv\")","8acc4efc":"product=pd.read_csv(\"..\/input\/discount-prediction\/product_details.csv\",encoding=\"mac-roman\")","e7387e3b":"#Removing the front and trailing spaces\ndf['Customer_Basket']=df['Customer_Basket'].str.lstrip()\ndf['Customer_Basket']=df['Customer_Basket'].str.rstrip()","d03695f8":"#The count of the number of Product Id's in the Customer Basket\ndf['Length']=df['Customer_Basket'].str.split(' ').apply(len)","a86aa9e8":"df.head()","299a0cbc":"train.head()","e8b79824":"#We can see a lot of null values in the train dataset\ntrain.info()","46eb0954":"#Let us see number of null values there are\ntrain.isnull().sum()","3d2bd243":"test.isnull().sum()","e4df5394":"train[train['BillNo'].isna()].head(10)","4218f3eb":"train.dropna(axis=0,how='all',inplace=True)","6c0d1508":"train.isnull().sum()","349e075c":"train.fillna(float(0.0),inplace=True)","697de580":"train.isnull().sum()","a50b215a":"train['Customer'].value_counts().head()","815f81fe":"len(set(test['Customer']).difference(set(train['Customer'])))","e6180208":"train['Discount 5%'].value_counts()","4c1bcf73":"sns.set_style(\"whitegrid\")\nsns.countplot(x='Discount 5%',data=train)","e0696c37":"sns.set_style(\"whitegrid\")\nsns.countplot(x='Discount 12%',data=train)","c05f1f4e":"sns.set_style(\"whitegrid\")\nsns.countplot(x='Discount 18%',data=train)","d0ea4f8e":"sns.set_style(\"whitegrid\")\nsns.countplot(x='Discount 28%',data=train)","34f7bf6d":"#lol=df2[\"Customer\"].str.split(\", \",n=1,expand=True)\n#df2['CustomerName']=lol[0]\n#df2[\"Location\"]=lol[1]","174c49fc":"#lol1=df3[\"Customer\"].str.split(\", \",n=1,expand=True)\n#df3['CustomerName']=lol1[0]\n#df3[\"Location\"]=lol1[1]","bdcdee7d":"#set(df3['Location']).difference(set(df2[\"Location\"]))","6a94117b":"#df3[df3[\"Location\"]=='T.M.M. HOSPITAL, THIRUVALLA.']","616c3cff":"#sns.countplot(x=\"discount\",hue=\"Location\",data=df3)","c2a6e629":"#df2[\"discount\"].value_counts()","fc640ec4":"#len(set(trailtest['Customer']).difference(set(trailtrain['Customer'])))","28209735":"discount=[]\nfor i, row in train.iterrows():\n    if row[\"Discount 5%\"]==1.0:\n        discount.append(1)\n    elif row[\"Discount 12%\"]==1.0:\n        discount.append(2)\n    elif row[\"Discount 18%\"]==1.0:\n        discount.append(3)\n    elif row[\"Discount 28%\"]==1.0:\n        discount.append(4)\n    else:\n        discount.append(5)        ","d5420b70":"train[\"discount\"]=discount","0bbf9229":"from wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\ntrain1_df = train[train[\"discount\"]==1]\ntrain2_df = train[train[\"discount\"]==2]\ntrain3_df = train[train[\"discount\"]==3]\ntrain4_df = train[train[\"discount\"]==4]\ntrain5_df = train[train[\"discount\"]==5]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"Customer\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"Customer\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"Customer\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train4_df[\"Customer\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train5_df[\"Customer\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Discount 5%\", \n                                          \"Frequent words of Discount 12%\",\n                                         \"Frequent words of Discount 18%\",\n                                         \"Frequent words of Discount 28%\",\"Frequent words of No Discount\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()","57de0406":"train.drop(['Discount 5%','Discount 12%','Discount 18%','Discount 28%'],axis=1,inplace=True)","d845d0de":"from sklearn.feature_extraction.text import CountVectorizer\ncv1 = CountVectorizer(max_features=500)\ny = cv1.fit_transform(df[\"Customer_Basket\"]).toarray()","06165ed7":"len(cv1.vocabulary_)","d499ac95":"thirty= list(y)\nthirty1=pd.DataFrame(thirty)","3284b6bb":"final=pd.concat([df,thirty1],axis=1)","559dbf5a":"finaltrain=pd.merge(train,final,on=\"BillNo\",how=\"inner\")\nfinaltest=pd.merge(test,final,on=\"BillNo\",how=\"inner\")","43e6f384":"finaltrain.head()","8aef9fd5":"finaltest.head()","ad171bb7":"#df2=df2[df2[\"BillNo\"]!=float(0.0)]","c15e9d69":"finaltrain.drop([\"BillNo\",\"Customer_Basket\",\"Customer\",\"Date\"],axis=1,inplace=True)\nfinaltest.drop([\"BillNo\",\"Customer_Basket\",\"Customer\",\"Date\"],axis=1,inplace=True)","61797316":"X=finaltrain.drop(\"discount\",axis=1)\ny=finaltrain[\"discount\"]","68da5957":"from imblearn.over_sampling import SMOTE","1cc9e24a":"sm = SMOTE(random_state=2)","66640cd1":"X_train_res, y_train_res = sm.fit_sample(X, y.ravel())","fa11131f":"X_train=pd.DataFrame(X_train_res)","d9b1f27f":"y_train=pd.DataFrame(y_train_res)","ce7c2c2b":"X_train[\"smote\"]=y_train_res","74b1e727":"X1=X_train.drop([\"smote\"],axis=1)\ny1=X_train[\"smote\"]","7957b37b":"import lightgbm as lgb","4ead207c":"model = lgb.LGBMClassifier( class_weight = 'balanced',\n                               objective = 'multiclass', n_jobs = -1, n_estimators = 400)","e877a79a":"model.fit(X1,y1)","c1d87469":"pred_lg=model.predict(finaltest)","6fa91721":"pred_lg","d5383ef0":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","6573648f":"xgb = XGBClassifier(max_depth=5, learning_rate=0.2, n_estimators=300,\n                    objective='multi:softprob', subsample=0.6, colsample_bytree=0.6, seed=0, silent=0)                  ","da66ff55":"#xgb.fit(X1, y1)","23d3b3d0":"#pred_xg=xgb.predict(finaltest)","e847c577":"from sklearn.ensemble import RandomForestClassifier","e067a875":"rfc=RandomForestClassifier(n_estimators=500)","9725deaa":"rfc.fit(X1,y1)","c76439c3":"rfcpredict=rfc.predict(finaltest)","03848ccb":"rfcpredict","7e10aaf6":"#from sklearn.model_selection import StratifiedKFold\n#kfold = 5\n#skf = StratifiedKFold(n_splits=5)","75cc8901":"#folds = 3\n#param_comb = 5\n\n#skf = KFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n#random_search = RandomizedSearchCV(rfc, param_distributions=params, n_iter=param_comb, scoring=rmsle, n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )","ee75e43e":"#random_search.fit(X, y)","ff2a9d2c":"#print('\\n All results:')\n#print(random_search.cv_results_)\n#print('\\n Best estimator:')\n#print(random_search.best_estimator_)\n#print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n#print(random_search.best_score_ * 2 - 1)\n#print('\\n Best hyperparameters:')\n#print(random_search.best_params_)","19c618da":"It seems that there are very few 5% discounts. Now let us view it for the 12% Discount.","85ffb747":"Let us drop the Discount columns now.","15a686c1":"<h1>Cross Validation for Hyperparameter tuning<\/h1>","7c460a77":"<h1>3.Random Forest Classifier<\/h1>","fa4664f2":"No Null Values in the test dataset.","fef46fbe":"Since to differentiate the Customer Basket is an NLP Problem we will be using CountVectoriser. It converts a collection of text documents to a matrix of token counts. ","d8583f66":"<h1>Importing the datasets<\/h1>","41659b25":"![](http:\/\/)We are gonna start of by importing the generic packages that everyone uses in their kernels.","7ef3a5d1":"Again we see the same pattern as in the 5% Discount but just a little more. Let us look at the final class i.e, 28% Discount.","e81cdcf5":"<h1>Importing the Libraries<\/h1>","0137c6a3":"We can see more discounts here and therefore is the second most occuring class after 12%. Since the class labels are so imbalanced we will be using SMOTE later on.<br><br>\n\nWe initially used the Customer variable but later on in our predictions we realised that it was actually degrading our model therefore we ended up not using it.","92a181d7":"Now we will create a function that will combine all the class labels into one Target variable.","f43b2844":"From these plots we can see that not any customer was given an extra likelihood of discounts.","60f54c7c":"The objective of this \"Discount Prediction\" Competition was to build a machine learning model to Predict Medical Wholesales Discount to their customers. In this notebook, we will walk through a complete machine learning solution, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions. We would like to thank everyone for this hackathon.<br><br>\nThis notebook is majorly divided into three two parts.They are:\n<ol>\n<li>Exploratory Data Analysis and Preprocessing<\/li>\n<li>Modeling<\/li><ul>\n<li><b>LightGBM<b><\/li>\n<li>XGBOOST<\/li>\n<li>Random Forest<\/li>","3e80a81a":"Now we can see that the majority of the null values are present in the Target variables. But we can impute these values with 0.","7342850b":"From this we can tell that the majority of the Discounts were 12%. Let us look at the 18% Discount.","74b4ad19":"We commented the xgboost out becuase in the kernel it would show a long output on the kernel. The code definitely works for multiclass classification so you guys are free to run it.","ce7f8e1d":"This is the modeling section of our notebook we will be using various machine learning models to perform our predictions. We have performed the submission file creation only for one of the models but did implement it for the rest of the models.","e97433e9":"These are import statements for plotply which is also a visualization library.","2ccdf03f":"<font size=\"20\">Discount Prediction<\/font>","8887678e":"<font size=\"18\">Modeling<\/font>","a64210b6":"Let us now plot the word count of \"Customer\" for each Discount Class.","3270d8bc":"At the end we were able to discern that LightGBM gave us the best results and that was what we submitted finally.\nOnce again we would like to thank everyone for making this hackathon really enjoyable and educational !!","6c87f5f1":"<h1>Exploratory Data Analysis with Preprocessing<\/h1>","c8ff3d2f":"<h1>Result<\/h1>","56148f86":"![](http:\/\/)We commented this out as it was taking too much time to commit and we didn't want our submission to be late.","247717de":"We will be using SMOTE here to balance the classes. It achieves this by oversampling. ","7366ca4e":"<h1>1. LightGBM<\/h1>","14034c9d":"We can see that the classes are highly unbalanced. Let us visualise it for the other classes to understand this better.<br>\nFor the \"Discount 5%\" here the \"1\" represents the condition when discount is given and \"0\" is when the discount is not given.","731808f8":"<h1>2. XGBOOST<\/h1>","913de280":"From this we can tell that there are null values for the entire row present in the train dataset. The below code is to drop the entire row only when the entire row are \"NaN\" or null values.","19ffb8fb":"Finally we have no more null values.<br>"}}