{"cell_type":{"38de6e89":"code","6ad6de78":"code","a293b60d":"code","31a5ac99":"code","19032941":"code","3954a3bb":"code","e1d25ca6":"code","0b42af46":"code","659d24a1":"code","5aaa2605":"code","ac08e18a":"code","f0610d6d":"code","017250fb":"code","88f87eeb":"code","286b80ec":"code","c83c3012":"code","7ea1d65a":"code","5fef871b":"code","c098c594":"code","6b0a3949":"code","0efcd5ed":"code","fb6ab0a0":"code","9ea9612a":"code","e39c7f88":"code","9d697f7f":"code","05f2ee79":"code","a4a5d8d6":"code","8a631674":"code","afc5c55f":"code","97662584":"code","afb2fd2d":"code","8bb6745c":"code","7801b2f2":"code","ef566b69":"code","6d50d310":"code","77d21907":"code","f117875c":"code","363b8259":"code","aaaa5751":"code","e7345eca":"code","fc39238e":"code","836f8b46":"code","7a339de9":"code","5d0b81b5":"code","a3a55d6a":"code","80ec4477":"code","a7dc72ac":"code","32cd12d8":"code","e841fa3e":"code","5207b213":"code","132aed8c":"markdown","952a3b5a":"markdown","ae0c708d":"markdown","6c1efdde":"markdown","ea58b81e":"markdown","a41189f7":"markdown","821355d9":"markdown","ee8d5691":"markdown","074ec030":"markdown","fc4827b4":"markdown","1a6b7d18":"markdown","0cb1d64e":"markdown","3c92b4ad":"markdown","8c40a140":"markdown","2b7c1b59":"markdown","a697e8e6":"markdown","6b8530fe":"markdown","90b9e56b":"markdown","48201325":"markdown","acd6386b":"markdown","99276ea5":"markdown","7e6b8258":"markdown","cf3e8598":"markdown","482ebaed":"markdown","e9f87572":"markdown","77eaa1d3":"markdown","c5a196a6":"markdown","a89fc87e":"markdown"},"source":{"38de6e89":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline \n#plotting directly without requering the plot()\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") #ignoring most of warnings, cleaning up the notebook for better visualization\n\npd.set_option('display.max_columns', 500) #fixing the number of rows and columns to be displayed\npd.set_option('display.max_rows', 500)\n\nprint(os.listdir(\"..\/input\")) #showing all the files in the ..\/input directory\n\n# Any results you write to the current directory are saved as output. Kaggle message :D\n","6ad6de78":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object and col_type != '<M8[ns]':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            elif col_type != '<M8[ns]':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n","a293b60d":"sales_train = pd.read_csv(\"..\/input\/sales_train.csv\", parse_dates=True, index_col=0, infer_datetime_format=True, dayfirst=True)\ntest = pd.read_csv(\"..\/input\/test.csv\")\nshop_df = pd.read_csv(\"..\/input\/shops.csv\")\nitems = pd.read_csv('..\/input\/items.csv')\nitem_cat = pd.read_csv(\"..\/input\/item_categories.csv\") #reading the file","31a5ac99":"print(sales_train.shape, test.shape)","19032941":"print(\"The columns in the training set are: %s\" %list(sales_train.columns))\nprint(\"The columns in the testing set are: %s\" %list(test.columns))","3954a3bb":"sales_train.head() #looking at the first entries of our training set","e1d25ca6":"sales_train.isnull().sum() #checking whether we have null values or not.","0b42af46":"print('Total number of shopping(by ID): %d' %sales_train['shop_id'].max()) #number of different shop's ID\nprint('Number of months: %d' %sales_train['date_block_num'].max()) #number of months \nprint('Total number of items(by ID): %d' %sales_train['item_price'].max()) #numer of different item's ID","659d24a1":"sales_train = sales_train[sales_train['item_cnt_day'] > 0] #keeping only items with price bigger than 0","5aaa2605":"item_cat.head()","ac08e18a":"print('Number of categories: %s' %item_cat['item_category_name'].nunique()) #checking for unique names\nprint('number of categories id: %s' %str(item_cat['item_category_id'].max())) #checking for number of ids\nprint('Shape of item categories dataset: %s' %str(item_cat.shape))","f0610d6d":"items.head()","017250fb":"print('Number of items: %s' %items['item_id'].nunique()) #printing the number of unique items","88f87eeb":"shop_df.head()","286b80ec":"print('Total number of shops: %s' %shop_df['shop_id'].nunique()) \nprint('Shape: %s' %str(shop_df.shape))","c83c3012":"#### Joining first item categories, using item id\nitems = items.join(item_cat, on=\"item_category_id\", rsuffix='_')\nitems.head()","7ea1d65a":"#### now joining sales\nsales_train = sales_train.join(items, on=\"item_id\", rsuffix=\"_\")\nsales_train.head()","5fef871b":"#### Now finally, join with shopping list, then clear the memory up\nsales_train = sales_train.join(shop_df, on=\"shop_id\", rsuffix=\"_\")\ntrain = sales_train.drop([\"shop_id_\",\"item_id_\",\"item_category_id_\"], axis=1) #dropping all duplicate IDs(using the suffix \"_\")\ntrain.head()","c098c594":"# drop shops&items not in test data\nshop_id = test.shop_id.unique()\nitem_id = test.item_id.unique()\ntrain = train[train.shop_id.isin(shop_id)]\ntrain = train[train.item_id.isin(item_id)]\n","6b0a3949":"####Cleaning the memory up####\ngc.enable\ndel items, item_cat, shop_df, sales_train,shop_id,item_id\ngc.collect()","0efcd5ed":"train_by_month = train.reset_index()[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","fb6ab0a0":"train_by_month = train_by_month.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id','item_id'], as_index=False)\ntrain_by_month = train_by_month.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\ntrain_by_month.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id','item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']\ntrain_by_month.head()","9ea9612a":"shop_ids = train_by_month['shop_id'].unique()\nitem_ids = train_by_month['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])\ntrain_by_month = pd.merge(empty_df, train_by_month, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_by_month['year'] = train_by_month['date_block_num'].apply(lambda x: ((x\/\/12) + 2013)) #creating year and month using the data_block_num, a better solution would be to use datetime, but as we have created different entries above, I couldn't came up with a different solution\ntrain_by_month['month'] = train_by_month['date_block_num'].apply(lambda x: (x % 12))\ntrain_by_month.fillna(0, inplace=True) #filling with zero, information we don't have.","e39c7f88":"train_by_month = reduce_mem_usage(train_by_month) #reducing the memory usage, changing the features types, using the function implemented at the beggining","9d697f7f":"plt.style.use('fivethirtyeight')\nplt.figure(figsize = (16, 12))\n\ncnt_item_by_month = train_by_month.groupby(['month',\"shop_id\",\"item_id\"],as_index=False)['item_cnt'].sum()\ncnt_ctg_by_month = train_by_month.groupby(['month','item_category_id'], as_index=False)['item_cnt'].sum()\n\nrand_item_id = [30,30,22167,22167] ## 'randomly' picking examples\nrand_shop_id = [3,5,28,42]\nnum_plots = len(rand_item_id)\ni=0\n# iterate through the sources\nfor item, shop in zip(rand_item_id,rand_shop_id):\n    # create a new subplot for each source\n    plt.subplot(num_plots, 1, i + 1)\n    temp = cnt_item_by_month[[\"shop_id\",\"item_id\",\"month\",\"item_cnt\"]]\n    temp = cnt_item_by_month.loc[(cnt_item_by_month['item_id'] == item) & (cnt_item_by_month['shop_id'] == shop)]\n    plt.plot(temp[\"month\"],temp[\"item_cnt\"]);\n\n    plt.title('Sum of items per year_month -- item id: %s -- shop id: %s' %(item,shop)); plt.xlabel('month'); plt.ylabel('Count')\n    i+=1\nplt.tight_layout(h_pad = 1.5)\n\n#cleaning out the memory\ngc.enable\ndel rand_item_id,rand_shop_id,temp, num_plots, shop_ids,item_ids,empty_df\ngc.collect","05f2ee79":"sns.set_style('ticks')\nplt.figure(figsize = (16, 5))\nplt.subplot(2,1,1)\nsns.lineplot(x='month',y='item_cnt', data=cnt_item_by_month);plt.xticks(fontsize=10); plt.title('Item sales per month')\nplt.subplot(2,1,2)\nsns.barplot(x='item_category_id',y='item_cnt', data=cnt_ctg_by_month,errwidth=0);plt.xticks(fontsize=10); plt.title('Item count per each category')\nplt.tight_layout(h_pad = 1.5)","a4a5d8d6":"plt.figure(figsize = (16, 5))\n\nsns.barplot(x='shop_id',y='item_cnt', data=cnt_item_by_month,errwidth=0);plt.xticks(fontsize=10); plt.title('Item count per shopping id')","8a631674":"ax, fig = plt.subplots(1,1, figsize=(15,5))\nplt.subplot(2,1,1)\nsns.boxplot(x='date_block_num', y='item_price', data=train_by_month)\nplt.subplot(2,1,2)\nsns.boxplot(x='date_block_num', y='item_cnt', data=train_by_month)","afc5c55f":"train_by_month[train_by_month['item_price'] > 500000].count() #checking for the item_price higher than 500000","97662584":"train_by_month[train_by_month['item_cnt'] > 20].count() #checking for item count above 20","afb2fd2d":"train_by_month = train_by_month.query('item_price < 500000 and item_cnt >= 0 and item_cnt <= 20')","8bb6745c":"train_by_month['item_cnt_month'] = train_by_month.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)\ntrain_by_month.head()","7801b2f2":"train_by_month.tail()","ef566b69":"print(\"The shape of the training data before feature engineering: {}\".format(train_by_month.shape))","6d50d310":"# Min value\nr_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nr_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nr_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nr_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [r_min, r_max, r_mean, r_std] #list with the each function above listed that is gonna be applied taking the last 3 months\nfunction_name = ['min', 'max', 'mean', 'std'] #names of the functions\n\nfor i in range(len(function_list)):\n    train_by_month[('item_cnt_%s' % function_name[i])] = train_by_month.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_by_month['item_cnt_std'].fillna(0, inplace=True)","77d21907":"lag_list = [1, 2, 3] #creating a lag list with each month, 1, 2 and 3 months later.\n\nfor lag in lag_list: #going through the list of months\n    ft_name = ('item_cnt_%s_month_before' % lag) # lag number of months before, getting the previous item count per month\n    train_by_month[ft_name] = train_by_month.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_by_month[ft_name].fillna(0, inplace=True)","f117875c":"train_by_month['item_trend'] = train_by_month['item_cnt'] #firstly, the item trend is equal to the item count.\n\nfor lag in lag_list: #searching for each lag feature, then subtracting it from the trend value.\n    ft_name = ('item_cnt_%s_month_before' % lag)\n    train_by_month['item_trend'] -= train_by_month[ft_name]\n\ntrain_by_month['item_trend'] \/= len(lag_list) + 1 #finally, dividing it by 3 months + 1\n","363b8259":"train_by_month.head()","aaaa5751":"train_final = train_by_month.query('date_block_num >= 3 and date_block_num < 28').copy()\ntrain_validation_final = train_by_month.query('date_block_num >= 28 and date_block_num < 33').copy()\n\ntrain_final.dropna(inplace=True) #dropping any NaN values\ntrain_validation_final.dropna(inplace=True) #dropping any NaN values\n \n#dropping item category ID as we don't have it in our testing set.\ntrain_final.drop(['item_category_id'], axis=1, inplace=True) \ntrain_validation_final.drop(['item_category_id'], axis=1, inplace=True)\n\nprint(\"Training set: {}\".format(train_final.shape)+\"\\nValidation set: {}\"\n      .format(train_validation_final.shape))","e7345eca":"#grouping by shopping id, item count per month by shopping ID\ngp_shop_mean = train_final.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n#grouping by item ID, item count per month by item ID\ngp_item_mean = train_final.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n#grouping by shopping id and item ID, item count per month by shopping ID and item ID\ngp_shop_item_mean = train_final.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n#grouping by year and getting the item cnt per month mean\ngp_year_mean = train_final.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n#grouping by month and getting the item cnt per month mean\ngp_month_mean = train_final.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Merging the features created into the train final dataset\ntrain_final = pd.merge(train_final, gp_shop_mean, on=['shop_id'], how='left')\ntrain_final = pd.merge(train_final, gp_item_mean, on=['item_id'], how='left')\ntrain_final = pd.merge(train_final, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_final = pd.merge(train_final, gp_year_mean, on=['year'], how='left')\ntrain_final = pd.merge(train_final, gp_month_mean, on=['month'], how='left')\n# Merging the features created into the validation dataset\ntrain_validation_final = pd.merge(train_validation_final, gp_shop_mean, on=['shop_id'], how='left')\ntrain_validation_final = pd.merge(train_validation_final, gp_item_mean, on=['item_id'], how='left')\ntrain_validation_final = pd.merge(train_validation_final, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_validation_final = pd.merge(train_validation_final, gp_year_mean, on=['year'], how='left')\ntrain_validation_final = pd.merge(train_validation_final, gp_month_mean, on=['month'], how='left')\n\n#finally, adding those features created to our testing set as well\nadditional_features = pd.concat([train_final, train_validation_final]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\ntest_final = pd.merge(test, additional_features, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\ntest_final['year'] = 2015 #setting the month and year manually, we are predicting the next month, setting the current month to 9 then\ntest_final['month'] = 9\ntest_final = test_final[train_final.columns]#selecting only the columns present in the training set, keeping it aligned\n\nprint(\"Training set: {}\".format(train_final.shape)+\"\\nValidation set: {}\"\n      .format(train_validation_final.shape) + \"\\nTesting set: {}\".format(test_final.shape))","fc39238e":"#this pipeline is gonna be use for numerical atributes and standard scaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        #('robust_scaler', RobustScaler()),\n        ('minmax_scaler', MinMaxScaler(feature_range=(0, 1)))\n    ])","836f8b46":"train_labels = train_final['item_cnt_month'].astype(int)\ntrain_final.drop(['date_block_num','item_cnt_month'], axis=1,inplace=True)\ntrain_val_labels = train_validation_final['item_cnt_month'].astype(int)\ntrain_validation_final.drop(['date_block_num','item_cnt_month'], axis=1,inplace=True)\n\n#dropping the target in our testing set\ntest_final.drop(['date_block_num','item_cnt_month'], axis=1,inplace=True)\n\nprint(\"Training set: {}\".format(train_final.shape)+\"\\nValidation set: {}\"\n      .format(train_validation_final.shape) + \"\\nTesting set: {}\".format(test_final.shape))","7a339de9":"from sklearn.metrics import mean_squared_error\nimport time #implementing in this function the time spent on training the model\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nimport lightgbm as lgb\nimport xgboost as xgb\nimport gc\n\ncols = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_1_month_before', \n                'item_cnt_2_month_before', 'item_cnt_3_month_before', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt','transactions','year','month']\n\n#Selecting only relevant features(I ran the model before and selected the top features only), \n#ignoring item id, shopping id and using the pipeline to scale the data\n\ndef train_model(X, X_val, y, y_val, params=None, model_type='lgb', plot_feature_importance=False):\n  \n    evals_result={}\n    \n    X_train = num_pipeline.fit_transform(X.loc[:,cols])\n    x_val = num_pipeline.fit_transform(X_val.loc[:,cols])\n    \n    if model_type == 'lgb':\n        start = time.time()\n        \n        model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n        \n        model.fit(X_train, y, eval_set=[(X_train, y), (x_val, y_val)], eval_metric='rmse', early_stopping_rounds=200,\n                    verbose=50)\n            \n        y_pred_valid = model.predict(x_val, num_iteration=model.best_iteration_)\n        \n        end = time.time()\n        \n        #y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        \n        print('RMSE validation data: {}'.format(np.sqrt(mean_squared_error(y_val,y_pred_valid))))\n        \n        \n        if plot_feature_importance:\n            # feature importance\n            fig, ax = plt.subplots(figsize=(12,10))\n            lgb.plot_importance(model, max_num_features=50, height=0.8,color='c', ax=ax)\n            ax.grid(False)\n            plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n            \n        print('Total time spent: {}'.format(end-start))\n        return model\n            \n    if model_type == 'xgb':\n        start = time.time()\n        \n        model = xgb.XGBRegressor(**params, nthread = 4, n_jobs = -1)\n\n        model.fit(X_train, y, eval_metric=\"rmse\", \n                      eval_set=[(X_train, y), (x_val, y_val)],verbose=20,\n                      early_stopping_rounds=50)\n        \n        y_pred_valid = model.predict(x_val, ntree_limit=model.best_ntree_limit)\n        \n        end = time.time()\n\n        print('RMSE validation data: {}'.format(np.sqrt(mean_squared_error(y_val,y_pred_valid))))\n        \n        print('Total time spent: {}'.format(end-start))\n        return model\n            \n    if model_type == 'cat':\n        start = time.time()\n        model = CatBoostRegressor(eval_metric='RMSE', **params)\n        model.fit(X_train, y, eval_set=(x_val, y_val), \n                  cat_features=[], use_best_model=True)\n\n        y_pred_valid = model.predict(x_val)\n        \n        print('RMSE validation data: {}'.format(np.sqrt(mean_squared_error(y_val,y_pred_valid))))\n        \n        end = time.time()\n        \n        if plot_feature_importance:\n            feature_score = pd.DataFrame(list(zip(X.loc[:,cols].dtypes.index, model.get_feature_importance(Pool(X.loc[:,cols], label=y, cat_features=[])))), columns=['Feature','Score'])\n            feature_score = feature_score.sort_values(by='Score', kind='quicksort', na_position='last')\n            feature_score.plot('Feature', 'Score', kind='barh', color='c', figsize=(16,10))\n            plt.title(\"Catboost Feature Importance plot\", fontsize = 14)\n            plt.xlabel('')\n\n        print('Total time spent: {}'.format(end-start))\n        return model\n        \n    # Clean up memory\n    gc.enable()\n    del model, y_pred_valid, X_test,X_train,X_valid, y_pred, y_train, start, end,evals_result, x_val\n    gc.collect()\n","5d0b81b5":"params_cat = {\n    'iterations': 1000,\n    'max_ctr_complexity': 4,\n    'random_seed': 42,\n    'od_type': 'Iter',\n    'od_wait': 100,\n    'verbose': 50,\n    'depth': 4\n}\n\ncat_model = train_model(train_final,train_validation_final,train_labels,train_val_labels,params_cat,model_type='cat',plot_feature_importance=True)","a3a55d6a":"params_lgb = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.009,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n}\n\nlgb_model = train_model(train_final,train_validation_final,train_labels,train_val_labels,params_lgb)","80ec4477":"params_xgb = {\n    \"max_depth\": 8,\n    \"n_estimators\": 5000,\n    \"learning_rate\" : 0.05,\n    \"min_child_weight\": 1000,  \n    \"colsample_bytree\": 0.7, \n    \"subsample\": 0.7, \n    \"eta\": 0.3, \n    \"seed\": 42\n}\n\nxgb_model = train_model(train_final,train_validation_final,train_labels,train_val_labels,params_xgb,model_type=\"xgb\")","a7dc72ac":"#preparing the test dataset and passing it to each model...\ntest = num_pipeline.fit_transform(test_final.loc[:,cols])\n\nprediction_cat = cat_model.predict(test)\nprediction_lgb = lgb_model.predict(test)\nprediction_xgb = xgb_model.predict(test)\n\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nfinal_prediction = (prediction_cat+prediction_lgb+prediction_xgb)\/3\nsub['item_cnt_month'] = final_prediction.clip(0., 20.)\nsub.to_csv('mixed_sub.csv', index=False)\nsub.head()","32cd12d8":"cols = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_1_month_before',\n        'item_cnt_2_month_before', 'item_cnt_3_month_before', 'shop_mean',\n        'shop_item_mean', 'item_trend', 'mean_item_cnt','transactions','year','month']\nX_train = num_pipeline.fit_transform(train_final.loc[:,cols])\nX_val = num_pipeline.fit_transform(train_validation_final.loc[:,cols])\nX_train = np.expand_dims(X_train, axis=2)\nX_val = np.expand_dims(X_val, axis=2)\n\n#### importing relevant models ####\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Activation, ThresholdedReLU, MaxPooling2D, Embedding, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop\n\n# Defining the model layers\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(16, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(LSTM(32))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(optimizer=\"adam\", loss='mse', metrics=[\"mse\"])\nprint(model_lstm.summary())\n\n\nparams_lstm = {\"batch_size\":64,\n              \"verbose\":2,\n              \"epochs\":10}\n\ncallbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=3,mode='auto')]\nhist = model_lstm.fit(X_train, train_labels,\n                      validation_data=(X_val, train_val_labels),\n                      callbacks=callbacks_list,**params_lstm)","e841fa3e":"fig, ax = plt.subplots(2,1,figsize=(12,10))\nax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(hist.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(hist.history['mean_squared_error'], color='b', label=\"Training mean squared error\")\nax[1].plot(hist.history['val_mean_squared_error'], color='r',label=\"Validation mean squared error\")\nlegend = ax[1].legend(loc='best', shadow=True)","5207b213":"# predict results\nX_test = np.expand_dims(test, axis=2)\nresults = model_lstm.predict(X_test)\nsub['item_cnt_month'] = results.clip(0., 20.)\nsub.to_csv('lstm.csv', index=False)\nsub.head()","132aed8c":"**Noticed we have values of item count per day negative, this might have an underying reason for that, perhaps missing value, for now, I am gonna drop those**","952a3b5a":"#### Implementing a LSMT model on Keras TODO","ae0c708d":"### - Feature Engineering -","6c1efdde":"***\nNow taking a look at items categories, items and shops dataframes...","ea58b81e":"### Now that we have all information in one dataframe, gonna use only the numerical data and leave the text data we have for later, TODO","a41189f7":"* it can be seen, two categories have the largest item count, category 30 and 55","821355d9":"** Features has been created, taking a look at the dataframe **","ee8d5691":"#### Our training set has past dates and our target, which is the item_cnt_day(from which we are gonna create the count per month, item_cnt_month, which is our target), RMSE, root mean squared error is gonna be used to measure the loss.\nWe have 3 more CSV files with additional information: \n* **items.csv** - supplemental information about the items\/products.\n* **item_categories.csv**  - supplemental information about the items categories.\n* **shops.csv-** supplemental information about the shops.\n","074ec030":"* Then, we are gonna create combinations for each shopping id and item id in order to have every possible combination in our dataset, we are gonna fill it with zero","fc4827b4":"* 3 stores seems to have a significant better performance.","1a6b7d18":"* now, our target is gonna be item count per month, it's a forecasting problem, we must see one month ahead, so I am gonna use the shift method, -1, for the 34th month.","0cb1d64e":"**this wouldn't be advisable in a real application, as we would want to train a model to be able to recognize whatever item or shop id, but in this case, I am gonna drop the shopping and items id that we don't have in our testing set**","3c92b4ad":"it seems reasonable to keep only item_price < 500000 and clip the value for item count >=0 <=20","8c40a140":"* **Taking a look at the total amount of shopping, by shop_id, number of items, by item_id and months, which is given by the date_block_num**","2b7c1b59":"* Rolling 3 months back, getting the mean value across 3 months, the minimum value, the maximum and standard deviation.\n* then, using shift, and 3 months as the lag period to create the count per month for each item for the previous 3 months.","a697e8e6":"* Gonna create a column which is gonna be our target, items count per month, sorting by the month and grouping by the ids. gonna use the shift -1 function to shift the time period to one month ahead in time\n* before that, however, gonna check the item price and item count for outliers.","6b8530fe":"# Predict the future sales\n## this has been build based on this coursera project [here](https:\/\/www.coursera.org\/learn\/competitive-data-science)\n","90b9e56b":"* plotting some examples of items X shops, counts of items by month","48201325":"* OK, so, now I am gonna sort the data by date, ascending true, and groupby all other items except date \n* then after, aggregate item prices, mean and sum, item count per day, sum, mean and count\n* and finally, rename the columns ","acd6386b":"**Finally, we are gonna prepare the labels for our training and validation set**","99276ea5":"## Now, implementing a function that is gonna help us test our models\n### this function is gonna use cross fold validation","7e6b8258":"* Those plots are just examples got from the data, an overall view of the data is better\n* Gonna plot all the items sales per month,I expect to see an increase in sales by the end of the year.","cf3e8598":"* As we are gonna have different month, year, items and shop in each set, we are gonna do a mean enconding after having split the sets, this is gonna give us more features to work with. ","482ebaed":"## Loading the all dataframes","e9f87572":"** It seems that we have outliers for items prices and items count per day, isn't it? I wouldn't say that item count per day is exactly an outlier, it might be a normal value, 2000 items is not an abnormal number **","77eaa1d3":"* printing the shapes of our sales and test dataframe","c5a196a6":"#### after having created the features, we are gonna split the dataset taking into consideration that our task here is to predict one month ahead in time for our testing set. Accordingly, as the first 3 months don't have lag futures(as it was created using past data, and we don't have data before the beggining of the dataset) we are gonna use for our training set the 3th month on, until the 27th, to train the model. For the validation set, the next subsequent months until the last one, the 28th month until the 32th month. Finally, for the testing set...the 33th month.","a89fc87e":"** It's not clear whether the best approach is to merge all dataframes into the training dataset, I am gonna do so, tho, and clean the memory afterwards **"}}