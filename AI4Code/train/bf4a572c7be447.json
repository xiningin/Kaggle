{"cell_type":{"0d55828e":"code","d0418b81":"code","8f1c80a7":"code","a29ccc78":"code","92dec113":"code","49d74bb2":"code","978fba2c":"code","cc21b65a":"code","2c78f76a":"code","35429c86":"code","b1376004":"code","269fff8f":"code","8fe15895":"code","bd175bc7":"code","04d7f711":"code","cca359e3":"code","d8b397cc":"code","9f9e5405":"code","6cd127e1":"code","c1fe019c":"code","17635f50":"code","61d9e340":"code","8d4b4d71":"code","8b4c59e5":"code","dd8e0d0c":"code","34957a2b":"code","97da101f":"code","329c1559":"code","dedced5a":"code","94cfad62":"code","33c60818":"code","c9ab455f":"code","0c38a65d":"markdown","82927047":"markdown","89291a43":"markdown","5f55015f":"markdown","e1af5deb":"markdown"},"source":{"0d55828e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nprint(os.listdir(\"..\/input\"))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","d0418b81":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split","8f1c80a7":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsample = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","a29ccc78":"data = pd.concat([train, test], sort=False)","92dec113":"data['Sex'].replace(['male','female'], [0, 1], inplace=True)","49d74bb2":"data['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","978fba2c":"data['Fare'].fillna(np.mean(data['Fare']), inplace=True)\ndata['fare_value']=data['Fare']\/50","cc21b65a":"age_avg = data['Age'].mean()\nage_std = data['Age'].std()\ndata['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)\ndata['age_value']=data['Age']\/50","2c78f76a":"data['family'] = (data['SibSp'] + data['Parch'])\/5 ","35429c86":"data['isAlone'] = 0\ndata.loc[data['family'] > 0, 'isAlone'] = 1","b1376004":"delete_columns = ['Name','PassengerId','SibSp','Parch','Ticket','Cabin','Age','Fare']\ndata.drop(delete_columns, axis=1, inplace=True)","269fff8f":"train = data[:len(train)]\ntest = data[len(train):]","8fe15895":"print(train.shape)\nprint(test.shape)","bd175bc7":"trainY0 = train['Survived']\ntrainX0 = train.drop('Survived', axis = 1)\ntestX0 = test.drop('Survived', axis = 1)","04d7f711":"print(trainX0.shape)","cca359e3":"trainX1, valX1, trainY1, valY1 = train_test_split(\n            trainX0, trainY0, test_size=0.25, random_state=42)","d8b397cc":"trainY=torch.from_numpy(np.array(trainY1)).type(torch.LongTensor)\ntrainX=torch.from_numpy(np.array(trainX1)).float()\nvalY=torch.from_numpy(np.array(valY1)).type(torch.LongTensor)\nvalX=torch.from_numpy(np.array(valX1)).float()\ntestX=torch.from_numpy(np.array(testX0)).float()","9f9e5405":"batch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(trainX0) \/ batch_size)\nnum_epochs = int(num_epochs)","6cd127e1":"train_ds = torch.utils.data.TensorDataset(trainX,trainY)\nval_ds = torch.utils.data.TensorDataset(valX,valY)","c1fe019c":"batch_size = 32\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)","17635f50":"class LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\ninput_dim = 7\noutput_dim = 2\nmodel = LogisticRegressionModel(input_dim, output_dim)  \nerror = nn.CrossEntropyLoss()\nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","61d9e340":"count = 0\nloss_list = []\niteration_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        train = Variable(images.view(-1,7))\n        labels = Variable(labels)\n        optimizer.zero_grad()\n        outputs = model(train)\n        loss = error(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        count += 1\n        \n        if count % 50 == 0:      \n            correct = 0\n            total = 0\n\n            for images, labels in val_loader: \n                test = Variable(images.view(-1,7))\n                outputs = model(test)\n                predicted = torch.max(outputs.data,1)[1]\n                total += len(labels)\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            \n        if count % 5000 == 0:\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","8d4b4d71":"plt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","8b4c59e5":"model","dd8e0d0c":"test_outputs = model(testX)\ntest_predicted = torch.max(test_outputs.data,1)[1]","34957a2b":"PRED1=test_predicted.numpy()\nsubmt1=sample\nsubmt1['Survived']=PRED1\nsubmt1.to_csv('submission1.csv',index=False)\nsubmt1","97da101f":"class ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.tanh2 = nn.Tanh()\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.elu3 = nn.ELU()\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.tanh2(out)\n        out = self.fc3(out)\n        out = self.elu3(out)\n        out = self.fc4(out)\n        return out\n\ninput_dim = 7\nhidden_dim = 32\noutput_dim = 2\n\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\nerror = nn.CrossEntropyLoss()\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","329c1559":"count = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        train = Variable(images.view(-1,7))\n        labels = Variable(labels)\n        optimizer.zero_grad()\n        outputs = model(train)\n        loss = error(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        count += 1\n        \n        if count % 50 == 0:      \n            correct = 0\n            total = 0\n\n            for images, labels in val_loader:  \n                test = Variable(images.view(-1,7))\n                outputs = model(test)\n                predicted = torch.max(outputs.data, 1)[1]\n                total += len(labels)\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            \n        if count % 5000 == 0:\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","dedced5a":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","94cfad62":"model","33c60818":"test_outputs = model(testX)\ntest_predicted = torch.max(test_outputs.data,1)[1]","c9ab455f":"PRED2=test_predicted.numpy()\nsubmt2=sample\nsubmt2['Survived']=PRED2\nsubmt2.to_csv('submission2.csv',index=False)\nsubmt2","0c38a65d":"cf. https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers","82927047":"# Create Logistic Regression Model","89291a43":"# Split train data","5f55015f":"# Create ANN Model","e1af5deb":"# Data Processing"}}