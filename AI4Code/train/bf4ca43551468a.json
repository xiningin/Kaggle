{"cell_type":{"c53dba2d":"code","1ed65e83":"code","f22f4ad1":"code","5bc88aa8":"code","8d18d3c0":"code","8f448334":"code","074f283e":"code","bd6666c6":"code","a337ecc1":"code","d012d982":"code","931ef109":"code","2aed8c87":"code","8e818dd3":"code","f86ae227":"code","a4013877":"code","b20c31d1":"code","2c0877d3":"code","a5f9ae17":"code","1b3c8d34":"code","16d15451":"code","2b6f79fc":"code","1f530af8":"code","59ac3ff4":"code","633acc77":"code","e1d97640":"code","eec69d00":"code","179870f6":"code","6e96ed7f":"code","07abfc0e":"code","97a26034":"code","33055e9a":"code","a8d4d52c":"code","24ea9fe4":"code","2bee309c":"code","e8d25e3c":"code","2224d6ee":"code","e834da96":"code","5b92b77d":"code","29511cbb":"code","7ea4ffc5":"code","db62ee93":"code","9c7d9eb5":"code","98b7e32e":"code","b7a80563":"code","fcfc4eed":"code","ff182bfc":"code","45cffe98":"code","51ca3527":"markdown","34f21d5c":"markdown","c50c3496":"markdown","5bd12c44":"markdown","2ecc7769":"markdown","4e0a7dfd":"markdown","91ce50cc":"markdown","20a4baf7":"markdown","d9b9bd69":"markdown","8a25a2e2":"markdown","9c3a2764":"markdown","a6f49af1":"markdown","de7245d6":"markdown","3f13c7af":"markdown","94241aa9":"markdown","b80088ae":"markdown","38865a36":"markdown","a95dfbed":"markdown","82d19711":"markdown","2677c3cf":"markdown","8e38dd60":"markdown","31bba260":"markdown","7f5992bd":"markdown","b31ac49a":"markdown","c0990d5c":"markdown","97eb8048":"markdown","c289cf4c":"markdown","6d8254c9":"markdown","409bc31c":"markdown","0e68f393":"markdown","756d0b5d":"markdown","74eb253a":"markdown","eb28b312":"markdown","44b18cd7":"markdown","04393bee":"markdown","c8dbffa5":"markdown","3bd02d16":"markdown","aef744ea":"markdown","b3186e55":"markdown","4df8084f":"markdown","5dd18adf":"markdown","9d171e2d":"markdown","dd8ad432":"markdown","a6078e7f":"markdown","eb26444d":"markdown","9d306879":"markdown","e24a4a7f":"markdown","65b15195":"markdown","d6b9c798":"markdown","d42b7bd8":"markdown","1135eb1f":"markdown","65e3ec8f":"markdown","a49a8ec3":"markdown"},"source":{"c53dba2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1ed65e83":"#Importing Librabries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n# warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")","f22f4ad1":"# Now, we will load the data set and look at some initial rows and data types of the columns:\ndata = pd.read_csv('..\/input\/AirPassengers.csv')\nprint (data.head())\nprint ('\\n Data Types:')\nprint (data.dtypes)","5bc88aa8":"# The data contains a particular month and number of passengers travelling in that month. In order to read the data as a time series, we have to pass special arguments to the read_csv command:\ndateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')\ndata = pd.read_csv('..\/input\/AirPassengers.csv', parse_dates=['Month'], index_col='Month',date_parser=dateparse)\nprint ('\\n Parsed Data:')\nprint (data.head())\nprint (data.dtypes)","8d18d3c0":"data.index","8f448334":"ts= data['#Passengers']","074f283e":"ts[:'1949-05-01']","bd6666c6":"plt.plot(ts)","a337ecc1":"def test_stationarity(timeseries):\n    #Determing rolling statistics\n    plt.figure(figsize=(16,6))\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(timeseries.rolling(window= 12,center= False).mean(),label='Rolling Mean')\n    plt.plot(timeseries.rolling(window=12,center= False).std(),label='Rolling std')\n    plt.legend()","d012d982":"test_stationarity(ts)","931ef109":"# Stationarity tests\ndef Dickey_Fuller(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\nDickey_Fuller(ts)","2aed8c87":"ts_log = np.log(ts)\nplt.plot(ts_log)","8e818dd3":"moving_avg = ts_log.rolling(12).mean()\nplt.plot(ts_log)\nplt.plot(moving_avg, color='red')","f86ae227":"ts_log_moving_avg_diff = ts_log - moving_avg\nts_log_moving_avg_diff.head(12)","a4013877":"ts_log_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_log_moving_avg_diff)","b20c31d1":"Dickey_Fuller(ts_log_moving_avg_diff)","2c0877d3":"expwighted_avg = ts_log.ewm(halflife=12).mean()\nplt.plot(ts_log)\nplt.plot(expwighted_avg, color='red')","a5f9ae17":"ts_log_ewma_diff = ts_log - expwighted_avg","1b3c8d34":"Dickey_Fuller(ts_log_ewma_diff)","16d15451":"ts_log_diff = ts_log - ts_log.shift()\nplt.plot(ts_log_diff)","2b6f79fc":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff)\nprint(Dickey_Fuller(ts_log_diff))","1f530af8":"decomposition = sm.tsa.seasonal_decompose(ts_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid","59ac3ff4":"import statsmodels.api as sm\ntse = sm.tsa.seasonal_decompose(ts_log.values,freq=12,model='multiplicative')\ntse.plot()","633acc77":"tse = sm.tsa.seasonal_decompose(ts_log.values,freq=12,model='additive')\ntse.plot()","e1d97640":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)\nprint(Dickey_Fuller(ts_log_decompose))","eec69d00":"lag_acf = acf(ts_log_diff, nlags=20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method='ols')","179870f6":"#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","6e96ed7f":"model = ARIMA(ts_log, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))","07abfc0e":"model = ARIMA(ts_log, order=(0, 1, 2))  \nresults_MA = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))","97a26034":"model = ARIMA(ts_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))","33055e9a":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","a8d4d52c":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum.head())","24ea9fe4":"predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()","2bee309c":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)\/len(ts)))","e8d25e3c":"from fbprophet import Prophet","2224d6ee":"df = pd.read_csv('..\/input\/AirPassengers.csv')\ndf.head()\nprint(df.dtypes)","e834da96":"#converting month into date time object \ndf['Month'] = pd.DatetimeIndex(df['Month'])\ndf.dtypes","5b92b77d":"# renaming columns according to prophet model\ndf = df.rename(columns={'Month': 'ds',\n                        '#Passengers': 'y'})\ndf.head(5)","29511cbb":"ax = df.set_index('ds').plot(figsize=(12, 8))\nax.set_ylabel('Monthly Number of Airline Passengers')\nax.set_xlabel('Date')\nplt.show()","7ea4ffc5":"#set the uncertainty interval to 95% (the Prophet default is 80%)\nmy_model = Prophet(interval_width=0.95)","db62ee93":"my_model.fit(df)","9c7d9eb5":"future_dates = my_model.make_future_dataframe(periods=36, freq='MS')\nfuture_dates.tail()","98b7e32e":"forecast = my_model.predict(future_dates)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","b7a80563":"my_model.plot(forecast,\n              uncertainty=True)","fcfc4eed":"my_model.plot_components(forecast)","ff182bfc":"from fbprophet.diagnostics import cross_validation\ndf_cv = cross_validation(my_model, initial='730 days', period='180 days', horizon = '90 days')\ndf_cv.head()","45cffe98":"from fbprophet.diagnostics import performance_metrics\ndf_p = performance_metrics(df_cv)\ndf_p.head()","51ca3527":"Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:","34f21d5c":"**Forecasting usin Fbprophet**","c50c3496":"ARMA models are commonly used in time series modeling. In ARMA model, AR stands for auto-regression and MA stands for moving average. If these words sound intimidating to you, worry not \u2013 I\u2019ll simplify these concepts in next few minutes for you!\n\nWe will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series .ARIMA stands for **Auto-Regressive Integrated Moving Averages**. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\nA pure **Auto Regressive (AR only)** model is one where Yt depends only on its own lags. That is, Yt is a function of the \u2018lags of Yt\u2019.\n\n![](https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-1-min.png)\n\nwhere,  Yt\u22121  is the lag1 of the series,  \u03b21  is the coefficient of lag1 that the model estimates and  \u03b1  is the intercept term, also estimated by the model.\n\nLikewise a pure **Moving Average (MA only)** model is one where Yt depends only on the lagged forecast error.\n\n![](https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-2-min.png)\n\nAn ARIMA model is one where the time series was differenced at least once to make it stationary and you combine the AR and the MA terms. So the equation becomes:\n\n![](https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2019\/02\/Equation-4-min-1024x91.png)\n\nARIMA model in words:\n\nPredicted Yt = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n\nwhere,\n\n**1. Number of AR (Auto-Regressive) terms (p):** AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)\u2026.x(t-5).\n\n**2. Number of MA (Moving Average) terms (q):** MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)\u2026.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\n**3. Number of Differences (d):** These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results\n\nAn importance concern here is how to determine the value of \u2018p\u2019 and \u2018q\u2019. We use two plots to determine these numbers. Lets discuss them first\n\n","5bd12c44":"**AR Model **","2ecc7769":"**4. Making timeseries stationary**","4e0a7dfd":" Actually, its almost impossible to make a series perfectly stationary, but we try to take it as close as possible.\n\nLets understand what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS:\n\n**1. Trend \u2013** varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n\n**2. Seasonality** \u2013 variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.\n\nThe underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.\n\n**Difference data to make data stationary on mean (remove trend):**\n\nThe next thing to do is to make the series stationary as learned in the previous article. This to remove the upward trend through 1st order differencing the series using the following formula:\n\n\n **log transform data to make data stationary on variance**\n \nOne of the best ways to make a series stationary on variance is through transforming the original series through log transform. We will go back to our original tractor sales series and log transform it to make it stationary on variance.\n\n\n**Difference log transform data to make data stationary on both mean and variance**\n\n","91ce50cc":"Unlike numeric indexing, the end index is included here. For instance, if we index a list as a[:5] then it would return the values at indices \u2013 [0,1,2,3,4]. But here the index \u20181949-05-01\u2019 was included in the output.","20a4baf7":"**Combined Model **","d9b9bd69":"Facebook developed an open sourcing Prophet, a forecasting tool available in both Python and R. It provides intuitive parameters which are easy to tune. Even someone who lacks deep expertise in time-series forecasting models can use this to generate meaningful predictions for a variety of problems in business scenarios.\n\nImplements a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\n**a. The Prophet Forecasting Model**\n\nThe Prophet uses a decomposable time series model with three main model components: trend, seasonality, and holidays. They are combined in the following equation:\n\n**y(t)= g(t) + s(t) + h(t) + \u03b5t**\n\ng(t): piecewise linear or logistic growth curve for modeling non-periodic changes in time series\n\ns(t): periodic changes (e.g. weekly\/yearly seasonality)\n\nh(t): effects of holidays (user provided) with irregular schedules\n\n\u03b5t: error term accounts for any unusual changes not accommodated by the model.\n\nUsing time as a regressor, Prophet is trying to fit several linear and non linear functions of time as components. Modeling seasonality as an additive component is the same approach taken by exponential smoothing in Holt-Winters technique . Prophet is framing the forecasting problem as a curve-fitting exercise rather than looking explicitly at the time based dependence of each observation within a time series.\n","8a25a2e2":"This TS has even lesser variations in mean and standard deviation in magnitude. Also, the test statistic is smaller than the 1% critical value, which is better than the previous case.\n\n**Eliminating Trend and Seasonality**\n\nThe simple trend reduction techniques discussed before don\u2019t work in all cases, particularly the ones with high seasonality. Lets discuss two ways of removing trend and seasonality:\n\n**Differencing \u2013** taking the differece with a particular time lag\n\n**Decomposition \u2013 **modeling both trend and seasonality and removing them from the model.\n\n\n\n**Differencing**\n\nOne of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity. First order differencing can be done in Pandas as:\n","9c3a2764":"Prophet includes functionality for time series cross validation to measure forecast error using historical data. This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values. This figure illustrates a simulated historical forecast on the Peyton Manning dataset, where the model was fit to a initial history of 5 years, and a forecast was made on a one year horizon.","a6f49af1":"**2. Loading and Handling Time Series in Pandas**","de7245d6":"** 3.  Checking Stationarity of a Time Series?**","3f13c7af":"It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However. So, more formally, we can check stationarity using the following:\n\nmore more info go to below link:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/complete-tutorial-time-series-modeling\/\n\n**Plotting Rolling Statistics: **\n\nWe can plot the moving average or moving variance and see if it varies with time. By moving average\/variance I mean that at any instant \u2018t\u2019, we\u2019ll take the average\/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n\n**Dickey-Fuller Test: **\n\nThis is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary. Refer this article for details","94241aa9":"https:\/\/miro.medium.com\/max\/355\/1*fHlYXsqq-vxKpPN1qnrdOA.png\n\nhttps:\/\/miro.medium.com\/max\/442\/1*Xv_vjb5nvaU-BPn9wxcOcA.png\n","b80088ae":"Here we can see that the AR and MA models have almost the same RSS but combined is significantly better. Now, we are left with 1 last step, i.e. taking these values back to the original scale.\n\n**Taking it back to original scale**\n\n\nSince the combined model gave best result, lets scale it back to the original values and see how well it performs there. First step would be to store the predicted results as a separate series and observe it.\n\n","38865a36":"Stationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n\nconstant mean\n\nconstant variance\n\nan autocovariance that does not depend on time.\n\n**Stationary Time Series:**\n\ndata does not have any upward or downward trend or seasonal effects. Mean or variance are consistent over time.\n\n![](http:\/\/miro.medium.com\/max\/454\/1*jNeo1bAgBOS_FvyUhDi-0g.png)\n\n\nNon-Stationary Time Series:\n\ndata show trends, seasonal effects, and other structures depend on time. Forecasting performance is dependent on the time of observation. Mean and variance change over time and a drift in the model is captured.\n\n![](https:\/\/miro.medium.com\/max\/411\/1*-iOconMEaLW5ttxhHCdKBw.png)\n\nlets plot diagram on with TS.","a95dfbed":"link : http:\/\/sfb649.wiwi.hu-berlin.de\/fedc_homepage\/xplore\/tutorials\/xegbohtmlnode39.html","82d19711":"we assume an additive model, then we can write\n\n**yt=St+Tt+Et**\n\nwhere yt is the data at period t, St is the seasonal component at period t, Tt is the trend-cycle component at period tt and Et is the remainder (or irregular or error) component at period t Similarly for Multiplicative model,\n\n**yt=St x Tt x Et**","2677c3cf":"Note that here the parameter \u2018halflife\u2019 is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Other parameters like span and center of mass can also be used to define decay which are discussed in the link shared above. Now, let\u2019s remove this from series and check stationarity","8e38dd60":"The red line shows the rolling mean. Lets subtract this from the original series. Note that since we are taking average of last 12 values, rolling mean is not defined for first 11 values. This can be observed as:","31bba260":"**parse_dates: **This specifies the column which contains the date-time information. As we say above, the column name is \u2018Month\u2019.\n\n**index_col:** A key idea behind using Pandas for TS data is that the index has to be the variable depicting date-time information. So this argument tells pandas to use the \u2018Month\u2019 column as index.\n\n**date_parser:** This specifies a function which converts an input string into datetime variable. Be default Pandas reads data in format \u2018YYYY-MM-DD HH:MM:SS\u2019. If the data is not in this format, the format has to be manually defined. Something similar to the dataparse function defined here can be used for this purpose.","7f5992bd":"In this simpler case, it is easy to see a forward trend in the data. But its not very intuitive in presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series. There can be many ways of doing it and some of most commonly used are:\n\n1. Aggregation \u2013 taking average for a time period like monthly\/weekly averages\n2. Smoothing \u2013 taking rolling averages\n3. Polynomial Fitting \u2013 fit a regression model\n","b31ac49a":"**exponentially weighted moving**","c0990d5c":"This appears to have reduced trend considerably. Lets verify using our plots:\n\n","97eb8048":"Prophet plots the observed values of our time series (the black dots), the forecasted values (blue line) and the uncertainty intervals of our forecasts (the blue shaded regions).\n\nOne other particularly strong feature of Prophet is its ability to return the components of our forecasts. This can help reveal how daily, weekly and yearly patterns of the time series contribute to the overall forecasted values:","c289cf4c":"In the code chunk above, we instructed Prophet to generate 36 datestamps in the future.\n\nWhen working with Prophet, it is important to consider the frequency of our time series. Because we are working with monthly data, we clearly specified the desired frequency of the timestamps (in this case, MS is the start of the month). Therefore, the make_future_dataframe generated 36 monthly timestamps for us. In other words, we are looking to predict future values of our time series 3 years into the future.\n\nThe DataFrame of future dates is then used as input to the predict method of our fitted model.\n","6d8254c9":"**1. introduction to timeseries analysis**\n\nTime series forecasting is a technique for the prediction of events through a sequence of time. ... The techniques predict future events by analyzing the trends of the past, on the assumption that future trends will hold similar to historical trends","409bc31c":"** D. Seasonality, Holiday Effects, And Regressors:**\n \nSeasonal effects s(t) are approximated by the following function:\n\n\n![](https:\/\/miro.medium.com\/max\/624\/1*cnpHt71qOxrUeWkqGYDuFg.png)\n\nProphet has a built-in holiday feature which allows inputs of customized recurring events.\n","0e68f393":"An importance concern here is how to determine the value of \u2018p\u2019 and \u2018q\u2019. We use two plots to determine these numbers. Lets discuss them first.\n\n**Autocorrelation Function (ACF):** \n\nIt is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019t2\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019t2-5\u2019 (t1-5 and t2 being end points).\n\n**Partial Autocorrelation Function (PACF): **\n\nThis measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\nThe ACF and PACF plots for the TS after differencing can be plotted as:","756d0b5d":"**Estimating & Eliminating Trend**\n\n\n**Transformation**\n\ntransformation in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. These can be taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:","74eb253a":"The Dickey-Fuller test statistic is significantly lower than the 1% critical value. So this TS is very close to stationary","eb28b312":"Notice the first 11 being Nan. Lets drop these NaN values and check the plots to test stationarity.","44b18cd7":"**MA Model **","04393bee":"**5. Forecasting a Time Series**\n\n**Introduction to ARMA Time Series Modeling**","c8dbffa5":"**Moving Average **\n\nIn this approach, we take average of \u2018k\u2019 consecutive values depending on the frequency of time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics.\n","3bd02d16":"Now that our Prophet model has been initialized, we can call its fit method with our DataFrame as input. The model fitting should take no longer than a few seconds.","aef744ea":"Here the first element is base number itself and from thereon the values cumulatively added. Last step is to take the exponent and compare with the original series.\n","b3186e55":"This looks like a much better series. The rolling values appear to be varying slightly but there is no specific trend. Also, the test statistic is smaller than the 5% critical values so we can say with 95% confidence that this is a stationary series.\n\nHowever, a drawback in this particular approach is that the time-period has to be strictly defined. In this case we can take yearly averages but in complex situations like forecasting a stock price, its difficult to come up with a number. So we take a \u2018weighted moving average\u2019 where more recent values are given a higher weight. There can be many technique for assigning weights,A popular one is **exponentially weighted moving** average where weights are assigned to all the previous values with a decay factor\n","4df8084f":"To begin, we must instantiate a new Prophet object. Prophet enables us to specify a number of arguments. For example, we can specify the desired range of our uncertainty interval by setting the interval_width parameter.","5dd18adf":"**Checking Trends, seasionality and residuals**\n\nTime series datasets may contain trends and seasonality, which may need to be removed prior to modeling.\n\nTrends can result in a varying mean over time, whereas seasonality can result in a changing variance over time,\n\n**Additive or multiplicative?**\n\nIt\u2019s important to understand what the difference between a multiplicative time series and an additive one before we go any further.\n\nThere are three components to a time series:\n\n\u2013 trend how things are overall changing\n\n\u2013 seasonality how things change within a given period e.g. a year, month, week, day\n\n\u2013 error\/residual\/irregular activity not explained by the trend or the seasonal value\n\nHow these three components interact determines the difference between a multiplicative and an additive time series.\n\nIn a **multiplicative time series**, the components multiply together to make the time series. If you have an increasing trend, the amplitude of seasonal activity increases. Everything becomes more exaggerated. This is common when you\u2019re looking at web traffic.\n\nIn an **additive time series**, the components add together to make the time series. If you have an increasing trend, you still see roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the absolute value is growing but changes stay relative.\n","9d171e2d":"n this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the \u2018p\u2019 and \u2018q\u2019 values as:\n\np \u2013 The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.\n\nq \u2013 The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=2.\n\nNow, lets make 3 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.\n\nWe need to load the ARIMA model first:\n\nThe p,d,q values can be specified using the order argument of ARIMA which take a tuple (p,d,q). Let model the 3 cases:","dd8ad432":"Prophet also imposes the strict condition that the input columns be named ds (the time column) and y (the metric column), so let\u2019s rename the columns in our DataFrame:\n","a6078e7f":"We can see that the mean and std variations have small variations with time. Also, the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence. We can also take second or third order differences which might get even better results in certain applications\n\n**Decomposing**\n\n\nIn this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned. ","eb26444d":"Though the variation in standard deviation is small, mean is clearly increasing with time and this is not a stationary series. Also, the p values is greater than 0.05, so null hypothesis accepted ,means serries is not stationary ","9d306879":"Next we\u2019ve to add them to base number. For this lets create a series with all values as base number and add the differences to it. This can be done as:","e24a4a7f":"In order to obtain forecasts of our time series, we must provide Prophet with a new DataFrame containing a ds column that holds the dates for which we want predictions. Conveniently, we do not have to concern ourselves with manually creating this DataFrame, as Prophet provides the make_future_dataframe helper function:","65b15195":"**B.  Saturating growth**\n\n* Set a carrying capacity capto specify the maximum achievable point due to the business scenarios or constraints: market size, total population size, maximum budget, etc.\n\n* A saturating minimum, which is specified with a column floor in the same way as the cap column specifies the maximum.\n\n![](https:\/\/miro.medium.com\/max\/712\/1*uQcPHjTpVsvF9hTCGrtyFQ.png)\n\n**C. Trend Changepoints**\n\n* The model could be overfitting or underfitting while working with the trend component. The input of changepoints built in Prophet allowed is increased the fit becomes more flexible.\n\n* Here, you can nicely apply your business insights: big jump of sales during holidays, cost decreasing in future by purpose and etc. A user can also manually feed the changepoints with those business insights if it is required. In the below plot, the dotted lines represent the changepoints for the given time series.\n\n![](https:\/\/miro.medium.com\/max\/712\/1*JhvUB6QRzRrC1MX_FPSaow.png)\n\n\n\nAs the number of changepoints allowed is increased the fit becomes more flexible. There are basically 2 problems an analyst might face while working with the trend component:\n\nOverfitting\n\nUnderfitting\n\nA parameter called changepoint_prior_scale could be used to adjust the trend flexibility and tackle the above 2 problems. Higher value will fit a more flexible curve to the time series.\n\n\n**Trend parameters**\n\nParameter\t           Description\n\ngrowth\tlinear\u2019 or \u2018logistic\u2019 to specify a linear or logistic trend\n\nchangepoints\t: List of dates at which to include potential changepoints (automatic if not specified)\n\nn_changepoints\t: If changepoints in not supplied, you may provide the number of changepoints to be automatically included\n\nchangepoint_prior_scale\tParameter for changing flexibility of automatic changepoint selection\n \n\n**Seasonality & Holiday Parameters**\n\nParameter\t              Description\n\nyearly_seasonality\t    Fit yearly seasonality\n\nweekly_seasonality\t    Fit weekly seasonality\n\ndaily_seasonality\t    Fit daily seasonality\n\nholidays\t            Feed dataframe containing holiday name and date\n\nseasonality_prior_scale\tParameter for changing strength of seasonality model\n\nholiday_prior_scale\t    Parameter for changing strength of holiday model\n\nyearly_seasonality, weekly_seasonality & daily_seasonality can take values as True, False and no of fourier terms which was discussed in the last section. If the value is True, default number of fourier terms (10) are taken. Prior scales are defined to tell the model how strongly it needs to consider the seasonal\/holiday components while fitting and forecasting.\n","d6b9c798":"Below topics will be covered in this tutorial.\n\n1. introduction to timeseries analysis\n2. Loading and Handling Time Series in Pandas\n3. How to Check Stationarity of a Time Series?\n4. How to make a Time Series Stationary?\n5. Forecasting a Time Series","d42b7bd8":"Prophet returns a large DataFrame with many interesting columns, but we subset our output to the columns most relevant to forecasting, which are:\n\nds: the datestamp of the forecasted value\n\nyhat: the forecasted value of our metric (in Statistics, yhat is a notation traditionally used to represent the predicted values of a value y)\n\nyhat_lower: the lower bound of our forecasts\n\nyhat_upper: the upper bound of our forecasts\n\nA variation in values from the output presented above is to be expected as Prophet relies on Markov chain Monte Carlo (MCMC) methods to generate its forecasts. MCMC is a stochastic process, so values will be slightly different each time.\n\nProphet also provides a convenient function to quickly plot the results of our forecasts:\n","1135eb1f":"Notice that these start from \u20181949-02-01\u2019 and not the first month. Why? This is because we took a lag by 1 and first element doesn\u2019t have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. The cumulative sum can be found as:\n\n","65e3ec8f":"The Prophet paper gives further description of simulated historical forecasts.\n\nThis cross validation procedure can be done automatically for a range of historical cutoffs using the cross_validation function. We specify the forecast horizon (horizon), and then optionally the size of the initial training period (initial) and the spacing between cutoff dates (period). By default, the initial training period is set to three times the horizon, and cutoffs are made every half a horizon.\n\nThe output of cross_validation is a dataframe with the true values y and the out-of-sample forecast values yhat, at each simulated forecast date and for each cutoff date. In particular, a forecast is made for every observed point between cutoff and cutoff + horizon. This dataframe can then be used to compute error measures of yhat vs. y.\n\nHere we do cross-validation to assess prediction performance on a horizon of 365 days, starting with 730 days of training data in the first cutoff and then making predictions every 180 days. On this 8 year time series, this corresponds to 11 total forecasts.","a49a8ec3":"**Augmented Dickey-Fuller test (ADF)**\n\nADF tests the null hypothesis that a unit root is present in time series sample. ADF statistic is a negative number and more negative it is the stronger the rejection of the hypothesis that there is a unit root.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\n**Null Hypotehsis (H0):**\n\nIf accepted, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n\n**Alternate Hypothesis (H1):**\n\nThe null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary.\n\np-value > 0.05:\n\nFail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n\np-value <= 0.05:\n\nReject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\n**means if null hypothesis(H0) accepted, p>0.05 then series is not stationary**"}}