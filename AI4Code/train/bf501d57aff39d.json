{"cell_type":{"efeb60bc":"code","4c2f8972":"code","87d5570e":"code","7608e7ee":"code","2b5e7c9a":"code","4511c644":"code","288fac7a":"code","4ceae512":"code","4bd1916e":"code","817afecc":"code","9f6853f8":"code","de5470a8":"code","a85ffbb9":"code","74ee3023":"code","fff4959a":"code","e210c80a":"code","fe614e76":"code","d6c23da6":"code","46ca6bae":"code","b2669fb6":"code","5abd4fbc":"code","c4d947ed":"code","7fa4b963":"code","a937ab6d":"code","ea35182b":"code","7390fcb5":"code","80bb4d25":"code","3a627534":"code","ba98b79c":"code","c1e1f5bc":"code","10aeb75b":"code","7eac00dd":"code","e6b850d5":"code","97cf5d89":"code","80c7d39e":"code","dc441563":"code","69baf5be":"code","ccd545ca":"code","06484de4":"code","d1544e7d":"code","7b4e83fb":"code","62173bba":"code","30f80c0c":"code","343a9e2c":"code","49a9a969":"code","465f701e":"code","cbae0f3a":"code","e0b3454c":"code","a3de11b1":"code","d3f37428":"code","a11a44be":"code","8bc7363c":"markdown","e15177da":"markdown","038e04fd":"markdown","df16b5ac":"markdown","f811dcc7":"markdown","d0c3396d":"markdown","cc8e8ab8":"markdown","ea636ab0":"markdown","4981b169":"markdown","afb3971a":"markdown","59b24ec6":"markdown","254265ea":"markdown","ee243852":"markdown","971546b2":"markdown","16e36ddb":"markdown","00f403a3":"markdown","860b24a8":"markdown","6ea3540e":"markdown","c96ce0ff":"markdown","c7819053":"markdown","e660c174":"markdown","af74bacb":"markdown","edeea440":"markdown","a1914385":"markdown","2bcec31a":"markdown","085968a0":"markdown","5d9f9f75":"markdown","9599de8b":"markdown","61e8c404":"markdown","2fdfb8d5":"markdown","0301daaa":"markdown","2be320d2":"markdown","6b557f2c":"markdown","bb78208f":"markdown","4951ecaf":"markdown","70ec6eed":"markdown","0f0b115c":"markdown","cce826db":"markdown","e88758b3":"markdown","0472b927":"markdown","22986b6b":"markdown","42f3f6cb":"markdown","0af098d7":"markdown","b2ae4cb3":"markdown","d635034b":"markdown","f116fca9":"markdown","8fc4c32e":"markdown","712be119":"markdown","8ab666b4":"markdown","a6815527":"markdown","de00976a":"markdown","bf538a51":"markdown","0fefbce2":"markdown","259875d9":"markdown","8013cca1":"markdown","d21ce04c":"markdown","8c0974e8":"markdown","8b8adfda":"markdown","2e0562a4":"markdown","7698abd9":"markdown","79af0350":"markdown","6918619c":"markdown","ef6d786a":"markdown","f7e99a9e":"markdown","cda7b397":"markdown","04b8f2c3":"markdown","c8ecc2d2":"markdown","d0c83fbb":"markdown","5bd8b1f1":"markdown","37fcac9f":"markdown"},"source":{"efeb60bc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('notebook', font_scale=1.5)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nwine_reviews = pd.read_csv(\"..\/input\/wine-reviews\/winemag-data_first150k.csv\", index_col=0)\nprint(\"Before removing duplicates:\", len(wine_reviews))\nwine_reviews.tail()","4c2f8972":"# if we count for description, we can see there are multiple duplicate rows for each description\n# wine_reviews.description.value_counts(dropna=False)\n\n# dedup based on all columns\nwine_reviews = wine_reviews.drop_duplicates()\nprint(\"Removing duplicates based on all columns:\", len(wine_reviews))","87d5570e":"# dedup based on description\nwine_reviews_ddp = wine_reviews.drop_duplicates('description')\nprint(\"Removing duplicates based on description:\", len(wine_reviews_ddp))","7608e7ee":"# full join two dedupped data and find the rows only in the first with '_merge' flag\nwine_reviews_all = wine_reviews.merge(wine_reviews_ddp, how='outer', indicator=True)\ndup_wine_desc = wine_reviews_all[wine_reviews_all['_merge']=='left_only'].description\n\nwine_reviews_all[wine_reviews_all['description'].isin(dup_wine_desc)]","2b5e7c9a":"# just to illustrate the difference\nwine_reviews.reset_index().tail()","4511c644":"wine_reviews = wine_reviews.reset_index(drop = True)","288fac7a":"wine_reviews.describe()","4ceae512":"print('Skewness=%.3f' %wine_reviews['points'].skew())\nprint('Kurtosis=%.3f' %wine_reviews['points'].kurtosis())\nsns.distplot(wine_reviews['points'], bins=20, kde=True);","4bd1916e":"print('Skewness=%.3f' %wine_reviews['price'].skew())\nprint('Kurtosis=%.3f' %wine_reviews['price'].kurtosis())\nsns.distplot(wine_reviews['price'].dropna());","817afecc":"print('Skewness=%.3f' %np.log(wine_reviews['price']).skew())\nprint('Kurtosis=%.3f' %np.log(wine_reviews['price']).kurtosis())\nsns.distplot(np.log(wine_reviews['price']).dropna());","9f6853f8":"sns.set(style = 'whitegrid', rc = {'figure.figsize':(8,6), 'axes.labelsize':12})\nsns.scatterplot(x = 'price', y = 'points', data = wine_reviews);","de5470a8":"sns.boxplot(x = 'points', y = 'price', palette = 'Set2', data = wine_reviews, linewidth = 1.5);","a85ffbb9":"wine_reviews['points'].corr(wine_reviews['price'])","74ee3023":"wine_cat = wine_reviews.select_dtypes(include=['object']).columns\nprint('n rows: %s' %len(wine_reviews))\nfor i in range(len(wine_cat)):\n    c = wine_cat[i]\n    print(c, ': %s' %len(wine_reviews[c].unique()))","fff4959a":"wine_reviews['country'].value_counts()","e210c80a":"print(wine_reviews['region_2'].isna().sum())\nwine_reviews['region_2'].value_counts()","fe614e76":"fig, ax = plt.subplots(1, 1, figsize = (12, 7))\ncol_order = wine_reviews.groupby(['country'])['points'].aggregate(np.median).reset_index().sort_values('points')\np = sns.boxplot(x = 'country', y = 'points', palette = 'Set3', data = wine_reviews, order = col_order['country'], linewidth = 1.5)\nplt.setp(p.get_xticklabels(), rotation = 90)\nax.set_xlabel('');","d6c23da6":"fig, ax = plt.subplots(1, 1, figsize = (10, 6))\ncol_order = wine_reviews.groupby(['region_2'])['points'].aggregate(np.median).reset_index().sort_values('points')\np = sns.boxplot(x = 'region_2', y = 'points', palette = 'Set3', data = wine_reviews, order = col_order['region_2'], linewidth = 1.5)\nplt.setp(p.get_xticklabels(), rotation = 60)\nax.set_xlabel('');","46ca6bae":"wine_reviews['word_count'] = wine_reviews['description'].apply(lambda x: len(str(x).split(\" \")))\nsns.boxplot(x = 'points', y = 'word_count', palette = 'Set3', data = wine_reviews, linewidth = 1.5);","b2669fb6":"print(wine_reviews.isnull().sum())","5abd4fbc":"# calculate percentage of missing values\nwine_missing = pd.DataFrame(wine_reviews.isnull().sum()\/len(wine_reviews.index) * 100)\nwine_missing.columns = ['percent']\nwine_missing","c4d947ed":"# first, we know that region_2 has nearly 60% of missing values so drop it\nwine_reviews.drop(['region_2'], inplace = True, axis = 1, errors = 'ignore')\n\n# second, it is not sensible to replace na with most frequent category for designation and region_1, so I create a new Unkown category\nwine_reviews['designation'].fillna('Unknown', inplace = True)\nwine_reviews['region_1'].fillna('Unknown', inplace = True)\n\n# last, replace na with median for numeric variable price\nwine_reviews['price'].fillna((wine_reviews['price'].median()), inplace = True)\nwine_reviews.tail()","7fa4b963":"wine_reviews[wine_reviews['country'].isna()]","a937ab6d":"wine_reviews[wine_reviews.winery.isin(['Tsililis', 'B\u00fcy\u00fcl\u00fcba\u011f', 'Chilcas'])]","ea35182b":"wine_reviews.loc[wine_reviews.designation == 'Askitikos', 'country'] = 'Greece'\nwine_reviews.loc[wine_reviews.designation == 'Askitikos', 'province'] = 'Thessaly'\n\nwine_reviews.loc[wine_reviews.designation == 'Shah', 'country'] = 'Turkey'\nwine_reviews.loc[wine_reviews.designation == 'Shah', 'province'] = 'Marmara'\n\n# As I have said, San Rafael is located in Maule Region; for simplicity, I assign 'Maule Valley' in line with other rows\nwine_reviews.loc[wine_reviews.designation == 'Piedra Feliz', 'country'] = 'Chile'\nwine_reviews.loc[wine_reviews.designation == 'Piedra Feliz', 'province'] = 'Maule Valley'\nwine_reviews.loc[wine_reviews.designation == 'Piedra Feliz', 'region_1'] = 'San Rafael'","7390fcb5":"wine_reviews[wine_reviews.designation.isin(['Askitikos', 'Shah', 'Piedra Feliz'])]","80bb4d25":"enc_cols = wine_reviews.drop(['description', 'points', 'price', 'designation', 'winery'], axis = 1)\ndummies = pd.get_dummies(enc_cols, prefix = ['country', 'province', 'region_1', 'variety']) \n\n# combined with log transformed 'price'\nX_encoded = pd.concat([np.log(wine_reviews['price']), dummies], axis = 1)\nX_encoded.shape","3a627534":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nwine_desc = pd.DataFrame({'description': wine_reviews['description']})","ba98b79c":"wine_desc['clean_desc'] = wine_desc['description'].apply(lambda x: x.lower())\nwine_desc['clean_desc'].head()","c1e1f5bc":"wine_desc['clean_desc'] = wine_desc['clean_desc'].str.replace('[^\\w\\s]', '')\nwine_desc['clean_desc'].head()","10aeb75b":"wine_desc['clean_desc'] = wine_desc['clean_desc'].str.replace('[0-9]+', '')\nwine_desc['clean_desc'].head()","7eac00dd":"stop_words = stopwords.words('english')\nwine_desc['clean_desc'] = wine_desc['clean_desc'].apply(lambda x: ' '.join(w for w in x.split() if w not in stop_words))\nwine_desc['clean_desc'].head()","e6b850d5":"# stem words\nporter = PorterStemmer()\nwine_desc['clean_desc'][:10].apply(lambda x: ' '.join([porter.stem(w) for w in x.split()]))","97cf5d89":"# lemmatization\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nlemmatizer = WordNetLemmatizer()\n\ndef get_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatize(sentence):\n    tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n    lemmatized_sentence = []\n    for word, tag in tagged:\n        wntag = get_wordnet_tag(tag)\n        if wntag is None:\n            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n        else:\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, pos = wntag))\n    return ' '.join(lemmatized_sentence)\n\nwine_desc['clean_desc'] = wine_desc['clean_desc'].apply(lambda x: lemmatize(x))\nwine_desc['clean_desc'][:10]","80c7d39e":"pd.Series(' '.join(wine_desc['clean_desc']).split()).value_counts()[:10]","dc441563":"wine_desc.head()","69baf5be":"from sklearn.feature_extraction.text import CountVectorizer\nX_desc = wine_desc['clean_desc']\n\ncount_vectorizer = CountVectorizer(max_features = 1000)\ncount_vectorizer.fit(X_desc)\nX_count = count_vectorizer.transform(X_desc)\nprint(X_count.shape)","ccd545ca":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_features = 1000)\ntfidf_vectorizer.fit(X_desc)\nX_tfidf = tfidf_vectorizer.transform(X_desc)\nprint(X_tfidf.shape)","06484de4":"from keras.preprocessing.text import Tokenizer\n# create a tokenizer \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(wine_desc['clean_desc'].values)\nword_index = tokenizer.word_index\n\n# load the pre-trained word-embedding vectors \nembeddings_index = {}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# create token-embedding mapping\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","d1544e7d":"vocab_size = len(word_index) + 1\nnonzeros = np.count_nonzero(np.count_nonzero(embedding_matrix, axis = 1))\nnonzeros \/ vocab_size","7b4e83fb":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\ny = wine_reviews['points'].values\nX_train_count, X_test_count, y_train_count, y_test_count = train_test_split(X_count, y, test_size = 0.25, random_state = 12)\n\n# train the model\nrf = RandomForestRegressor()\nrf.fit(X_train_count, y_train_count)\n\n# test the model\ny_pred_rf_count = rf.predict(X_test_count)\n\nfrom sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test_count, y_pred_rf_count))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test_count, y_pred_rf_count))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_count, y_pred_rf_count)))","62173bba":"rf.score(X_test_count, y_test_count)","30f80c0c":"X_count_df = pd.DataFrame(X_count.toarray())\nX = pd.concat([X_encoded, X_count_df], axis = 1)\nX_train_count, X_test_count, y_train_count, y_test_count = train_test_split(X, y, test_size = 0.25, random_state = 12)\n\n# train the model\nrf = RandomForestRegressor()\nrf.fit(X_train_count, y_train_count)\n\n# test the model\ny_pred_rf_count = rf.predict(X_test_count)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test_count, y_pred_rf_count))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test_count, y_pred_rf_count))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_count, y_pred_rf_count)))","343a9e2c":"rf.score(X_test_count, y_test_count)","49a9a969":"from pprint import pprint\npprint(rf.get_params())","465f701e":"# tried to commit with following codes, but failed\n\n# from sklearn.model_selection import RandomizedSearchCV\n# params that will be sampled from\n# max_depth = [int(x) for x in np.linspace(40, 100, num = 7)]\n# max_depth.append(None)\n# random_params = {'n_estimators': [200, 400, 600, 800, 1000, 1200],\n#                 'max_depth': max_depth,\n#                 'min_samples_split': [2, 3],\n#                 'min_samples_leaf': [2, 3]}\n\n# rf = RandomForestRegressor()\n\n# to reduce the time, just sample 50 combinations\n# rf_count_rd = RandomizedSearchCV(estimator = rf, param_distributions = random_params, n_iter = 50, cv = 3, verbose = 1)\n# rf_count_rd.fit(X_train_count, y_train_count)\n# rf_count_rd.best_params_\n\n# random search with best performance parameters\n# rf_count_rd_best = rf_count_rd.best_estimator_\n# y_pred_rf_count_rd = rf_count_rd_best.predict(X_test_count)\n\n# print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_count, y_pred_rf_count_rd))  \n# print('Mean Squared Error:', metrics.mean_squared_error(y_test_count, y_pred_rf_count_rd))  \n# print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_count, y_pred_rf_count_rd)))","cbae0f3a":"X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size = 0.25, random_state = 12)\n\n# train the model\nrf = RandomForestRegressor()\nrf.fit(X_train_tfidf, y_train_tfidf)\n\n# test the model\ny_pred_rf_tfidf = rf.predict(X_test_tfidf)\nfrom sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test_tfidf, y_pred_rf_tfidf))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf)))","e0b3454c":"rf.score(X_test_tfidf, y_test_tfidf)","a3de11b1":"X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\nX = pd.concat([X_encoded, X_tfidf_df], axis = 1)\nX_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X, y, test_size = 0.25, random_state = 12)\n\n# train the model\nrf = RandomForestRegressor()\nrf.fit(X_train_tfidf, y_train_tfidf)\n\n# test the model\ny_pred_rf_tfidf = rf.predict(X_test_tfidf)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test_tfidf, y_pred_rf_tfidf))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf)))","d3f37428":"rf.score(X_test_tfidf, y_test_tfidf)","a11a44be":"# still did not work as it took too much time to run;\n# plan to run it locally using Jupyter notebook and push it to my Github\n\n# params that will be sampled from\n# max_depth = [int(x) for x in np.linspace(40, 100, num = 7)]\n# max_depth.append(None)\n# random_params = {'n_estimators': [200, 400, 600, 800, 1000, 1200],\n#                 'max_depth': max_depth,\n#                 'min_samples_split': [2, 3],\n#                 'min_samples_leaf': [2, 3]}\n\n# rf = RandomForestRegressor()\n# rf_tfidf_rd = RandomizedSearchCV(estimator = rf, param_distributions = random_params, n_iter = 50, cv = 3, verbose = 1)\n# rf_tfidf_rd.fit(X_train_tfidf, y_train_tfidf)\n# rf_tfidf_rd.best_params_\n\n# random search with best performance parameters\n# rf_tfidf_rd_best = rf_tfidf_rd.best_estimator_\n# y_pred_rf_tfidf_rd = rf_tfidf_rd_best.predict(X_test_count)\n\n# print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_tfidf, y_pred_rf_tfidf_rd))  \n# print('Mean Squared Error:', metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf_rd))  \n# print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_tfidf, y_pred_rf_tfidf_rd)))","8bc7363c":"#### 5. Lemmatization","e15177da":"Which way to use? I will find these duplicate rows and see.","038e04fd":"Looks like wines with higher points usually have longer descriptions. That is interesting. Maybe next time when I make comparisons, I can select whose labels have the most words. Easy to implement, huh?","df16b5ac":"### Drop Duplicates","f811dcc7":"It is hard to see any patterns according to the scatter plot. Let us try boxplot.","d0c3396d":"Unfortunately, there are only 51% of words that can be found in our pre-trained model. I will keep looking into this problem and see. Any suggestions are welcome!!!","cc8e8ab8":"#### 3. Remove numbers","ea636ab0":"#### 2. Correlation between price and point","4981b169":"As we can see from the plots, values of points are quite concentrated but price is highly skewed with extreme values. After log transfomation, both the skewness and kurtosis are somewhat corrected.","afb3971a":"Personally, I will go with the first approach: dedupping based on all columns. Take the first two rows for example, though the wine descriptions are the same, the wineries are different, which may somehow explain the different points and prices. If I remove one of them, we may lose some information. ","59b24ec6":"I also checked several Chilean wines on winemag.com. They usually have a appellation which format is like \"Central Valley, Chile\", rather than others like \"Paso Robles, Central Coast, California, US\". So that should be the reason why these XXX Valleys appear in the province column.","254265ea":"We only have 9 independent variables in total so the principle is keeping as most as possible. In my house price kernel, I first used 15% of missing values as the criterion to decide whether to keep a variable or not. But if we do the same thing here, we would end up with only 6 variables.","ee243852":"### Categorical Variables","971546b2":"### 2. TfidfVectorizer","16e36ddb":"First let us run our base model, - model with only description.","00f403a3":"### Numeric Variables","860b24a8":"Not so close to 1, so the positive association is not strong.","6ea3540e":"### Missing Data Imputation","c96ce0ff":"## Preliminary Analysis","c7819053":"#### 1. Check the distribution","e660c174":"*The variable description under region_1 says \"the wine growing area in a province or state (ie Napa)\" while that under region_2 says \"sometimes there are more specific regions specified within a wine growing area\". But if we take a look at both region_1 and region_2 columns, it seems that region_1 is more granular than region_2. This can also be referred from the above unique value counts, - region_1 1237 and region_2 19. *","af74bacb":"**Before we proceed with next steps, remember to reset the index.** Otherwise, you would get an error with text cleaning just like me.","edeea440":"Since country and region_1 both have 3 missing values, I am wondering if they are among the same rows. ","a1914385":"For each row, CountVectorizer returns a vector whose length equals the number of distinct words from the document, with integer counts for the number of times each word appeared in the document.","2bcec31a":"### Text Preprocessing","085968a0":"#### 2. Remove punctuation","5d9f9f75":"Model with description as well as other variables does perform better than model with only description.","9599de8b":"### 1. CountVectorizer","61e8c404":"#### 1. Word Counts with CountVectorizer","2fdfb8d5":"First try with only description. Have to say, random forest is my favorite model. :P","0301daaa":"Roadmap:\n\n1. Import data : drop duplicates\n2. Preliminary analysis\n * Numerical variables: descriptive stats, distribution, correlation analysis\n * Categorical variables: frequency table, correlation analysis\n3. Data preparation\n * Missing data imputation\n * Text preprocessing\n * Feature extraction: count vectors, tf-idf vectors, word embeddings\n4. Modeling\n * Count vectors as features\n * TF-IDF vectors as features","2be320d2":"Look at how we have cleaned the texts so far.","6b557f2c":"Word embeddings can be trained using our own texts with the gensim Python package which uses Word2Vec for calculation. Or it can be generated using pre-trained word embeddings such as GloVe, FastText, and Word2Vec. ","bb78208f":"I searched online and found the information about the wines with missing country and province.\n* [Tsililis 2015 Askitikos Assyrtiko](https:\/\/www.winemag.com\/buying-guide\/tsililis-2015-askitikos-assyrtiko)\n* [B\u00fcy\u00fcl\u00fcba\u011f 2012 Shah Red](https:\/\/www.winemag.com\/buying-guide\/buyuluba-2012-shah-red)\n* [Chilcas 2006 Piedra Feliz Pinot Noir](https:\/\/www.winemag.com\/buying-guide\/chilcas-2006-piedra-feliz-pinot-noir-san-rafael)","4951ecaf":"The points are within 80 and 100, with majority of wines have got points less than 90. There are no missing values in points. The range of price is much larger, from 4 to 2300, with a great quantity of wines are less expensive than 40. This makes sense because good wines are always precious.","70ec6eed":"Let us see the most common words in our descriptions.","0f0b115c":"*An issue not related to data...*","cce826db":"#### 3. Word Embeddings","e88758b3":"### One-Hot Encoding","0472b927":"One way to remove duplicated is based on all columns. I have also seen people removing duplicates based on the description column. The two ways yield different results as seen from below.","22986b6b":"#### 4. Remove stop words","42f3f6cb":"Now we have words of text representing discrete, categorical features of wines. In order to prepare text data for predictive modeling, we need to map these textual data to real valued vectors for use as input to a machine learning algorithm, called feature extraction.","0af098d7":"## Data Preparation","b2ae4cb3":"Then we will include 'country', 'province', 'region_1', 'variety', and 'price' in our model.","d635034b":"Further take a look at the rows whose winery are one of the above three. It seems like many unkown region_1 are located in Chile. But I am questionable about how to define province and region_1. The provinces listed are Chile\u2019s prominent wine regions, e.g. Maule Valley. Maule Valley belongs to Maule Region if you check it using Google Map. In principle, XXX Valley should be a name of a region, like Napa Valley in California. ","f116fca9":"As always, we need to check and understand our raw data first.","8fc4c32e":"See if we can improve the performance by **parameter tuning**?","712be119":"Wines produced in England far exceed any other countries though the quantity is relatively small. As a well-known wine country, France also produces many high quality wines judging from the average point. Furthermore, its variety is diverse as the points range from 80 to 100. German wines have the similar characteristics. ","8ab666b4":"Excluding the missing values in region_2, we can see that Central Coast, Sonoma, Columbia Valley, and Napa are among top wine pruduction areas.","a6815527":"I also found information about several duplicate rows online, for example, [La Mannella 2011 Brunello di Montalcino](https:\/\/www.winemag.com\/buying-guide\/la-mannella-2011-brunello-di-montalcino) and [Poggiarellino 2011 Brunello di Montalcino](https:\/\/www.winemag.com\/buying-guide\/poggiarellino-2011-brunello-di-montalcino) (first two rows), [Willow Crest 2009 Riesling](https:\/\/www.winemag.com\/buying-guide\/willow-crest-2009-riesling-columbia-valley-yakima\/) and [Winzer Krems 2009 Kremser Wachtberg Gr\u00fcner Veltliner](https:\/\/www.winemag.com\/buying-guide\/winzer-krems-2009-kremser-wachtberg-gruner-veltliner-niederosterreich\/) (first 7th and 8th rows), etc.","de00976a":"### Feature Extraction","bf538a51":"This data interested me when I searched in the Kaggle datasets. I am not a professional taster. But I remember the time I tried to create cocktails by myself during a summer holiday. It was...not good (- -|||). Recently, I started to learn about wine basics, especially about how to read a wine label, since it is always difficult to select a suitable one from so many varieties of wines when purchasing. I have stepped on several land mines so far. So hope to learn more from this dataset as well.","0fefbce2":"The median points of wines from Willamette Valley, Columbia Valley, and Napa seem to be the same. These three regions are all located in the US. Actually the regions listed in the plot are all located in the US. It seems that US wine regions are populated well while regions of other countries are not in region_2 column.","259875d9":"Again, model with text and other variables outperforms model with only text information. Remember previously when we dealt with dupcates, there are some rows with same descriptions but vary in other columns. That is probably why variables like country or price are useful in prediction.","8013cca1":"Because there are too many unique values in columns like designation, it is not very informative to make a frequency table including them all. There could be only a few counts within each category and we cannot tell any patterns. I will just pick country and region_2.","d21ce04c":"Before we process the description column, we need to encode the rest of categorical variables first. At first, I tried to encode all categorical variables including desgination and winery. However, as we have seen, the total number of distinct values in these two columns equals 30622 + 14810 = 45432, which means our dummy matrix will have more than 40,000 columns.","8c0974e8":"The higher the value of IDF, the more unique is the word. So we can think of IDF as a penalty on TF. For example, \"fruit\" is one of the commonly occurring words across wine descriptions as we have seen. However, it is not useful to distinguish between documents and so its TF-IDF score can not be high.","8b8adfda":"#### 1. Lower case","2e0562a4":"Then include the one-hot encoded categorical variables as well as price to make predictions.","7698abd9":"I have also tried stemming but sometimes stemming can transform words a lot. On the other hand, lemmatization is to obtain the root word, which is what we really want to keep in our texts.","79af0350":"#### 3. How about description?","6918619c":"Nearly half of the wines are coming from the US. Italy and France are also two main wine production countries.","ef6d786a":"## Start Training","f7e99a9e":"#### 1. Check the distribution","cda7b397":"I have updated the approach here since I found that lemmatization would treat a word as a noun by default. So we need to find out the POS tag and pass it on to the lemmatizer.","04b8f2c3":"#### 2. Correlation with point","c8ecc2d2":"#### 2. Word Frequencies with TfidfVectorizer","d0c83fbb":"## Import Data","5bd8b1f1":"The overall trend that prices go up as points increase is obvious. That is, there exists a positive correlation between these two. Of course, among the wines that have the same points, their prices can vary a lot.","37fcac9f":"- TF(Term Frequency) = (Number of Occurences of a word)\/(Total words in the document) \n- IDF(Inverse Document Frequency) = Log((Total number of documents)\/(Number of documents containing the word))  \n- TF-IDF = multiplication of the TF and IDF"}}