{"cell_type":{"2ab4854f":"code","8012e38b":"code","705a216d":"code","3276bb92":"code","454df702":"code","d2c37d82":"code","4fd63d96":"code","f559a692":"code","782852d9":"code","60ef0cd8":"code","e153c5d0":"code","401ccfe9":"code","db0776f1":"code","d04476ad":"code","6afa6186":"code","11a7d63e":"code","808ec579":"code","1bf1a0bb":"code","d43508c1":"code","7a5087c9":"code","f98b2bc9":"code","d78a98d4":"code","bb04905b":"code","19509f23":"code","564e73ee":"code","46462a65":"markdown","de6b1140":"markdown","4fe15cd9":"markdown","ecd308e9":"markdown"},"source":{"2ab4854f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8012e38b":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np","705a216d":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","3276bb92":"train.head()","454df702":"test.head()","d2c37d82":"#Distribution of Sentiment column\ntrain['sentiment'].value_counts(normalize=True)","4fd63d96":"import seaborn as sns","f559a692":"plt.figure(figsize=(8,4))\nsns.countplot(x='sentiment',data=train)","782852d9":"import string","60ef0cd8":"import re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ntrain['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","e153c5d0":"train['text_len'] = train['text'].astype(str).apply(len)\ntrain['text_word_count'] = train['text'].apply(lambda x: len(str(x).split()))","401ccfe9":"from collections import Counter","db0776f1":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","d04476ad":"from nltk.stem.snowball import SnowballStemmer\n\n# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","6afa6186":"train['text'] = train['text'].apply(stemming)\ntrain['selected_text'] = train['selected_text'].apply(stemming)\ntrain.head(10)","11a7d63e":"Neutral_train = train[train['sentiment'] == 'neutral']","808ec579":"Neutral_train.shape","1bf1a0bb":"Neutral_train.head()","d43508c1":"def jaccard(str1, str2): \n    if str1 and str2:\n        a = set(str1.strip().split()) \n        b = set(str2.strip().split())\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    else:\n        return 0.0","7a5087c9":"Neutral_train['Jaccard_score'] = train.apply(lambda x: jaccard(x.text, x.selected_text), axis=1)","f98b2bc9":"round(Neutral_train['Jaccard_score'].mean() * 100, 2)","d78a98d4":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']\n#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","bb04905b":"from plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","19509f23":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","564e73ee":"print_every = 50\ninit_size = 2000\nbatch_size = 4000\n\ndef drop_empty_rows(df):\n    nan_value = float(\"NaN\")\n    df.replace(\"\", nan_value, inplace=True)\n    \n\ndef levenshtein_distance(s, t):\n    ''' From Wikipedia article; Iterative with two matrix rows. '''\n    if s == t: return 0\n    elif len(s) == 0: return len(t)\n    elif len(t) == 0: return len(s)\n    v0 = [None] * (len(t) + 1)\n    v1 = [None] * (len(t) + 1)\n    for i in range(len(v0)):\n        v0[i] = i\n    for i in range(len(s)):\n        v1[0] = i + 1\n        for j in range(len(t)):\n            cost = 0 if s[i] == t[j] else 1\n            v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n        for j in range(len(v0)):\n            v0[j] = v1[j]\n            \n    return v1[len(t)]    \n\ndef find_optimal_clusters(data, max_k,column,init_size,batch_size):\n  iters = range(2, max_k+1, 2)    \n    \n  sse = []\n  for k in iters:\n      sse.append(MiniBatchKMeans(n_clusters=k, init_size=init_size, batch_size=batch_size, random_state=20).fit(data).inertia_)\n      if (k % print_every == 0):\n          print('Fit {} clusters for  column: {}'.format(k,column))\n  f, ax = plt.subplots(1, 1)\n  ax.plot(iters, sse, marker='o')\n  ax.set_xlabel('Cluster Centers')\n  ax.set_xticks(iters)\n  ax.set_xticklabels(iters)\n  ax.set_ylabel('SSE')\n  ax.set_title('SSE by Cluster Center Plot')\n  plt.show()\n    \ndef plot_tsne_pca(data, labels,column):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=min(3000,data.shape[0]), replace=False)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=100).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i\/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot ' + column)\n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot ' + column)\n    plt.show()\n    \ndef get_top_keywords(data, clusters, labels, n_terms,column):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {} column: {}'.format(i,column))\n        print(','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]])))\n\n            \ntfidf = TfidfVectorizer(\n    min_df = 1,\n    max_df = 0.95,\n    stop_words = 'english',    \n    max_features = 450\n)\n\nimport pandas as pd\n\ntrain[\"text\"] = train[\"text\"].astype(str)\ntrain[\"text\"] = train[\"text\"].str.lower()\ntrain[\"selected_text\"] = train[\"selected_text\"].astype(str)\ntrain[\"selected_text\"] = train[\"selected_text\"].str.lower()\ntrain[\"sentiment\"] = train[\"sentiment\"].astype(str)\ntrain[\"sentiment\"] = train[\"sentiment\"].str.lower()\ndrop_empty_rows(train)\n\ntest[\"text\"] = test[\"text\"].astype(str)\ntest[\"text\"] = test[\"text\"].str.lower()\ntest[\"sentiment\"] = test[\"sentiment\"].astype(str)\ntest[\"sentiment\"] = test[\"sentiment\"].str.lower()\ndrop_empty_rows(test)\n\ncommon_cols = list(set.intersection(*(set(df.columns) for df in [train,test])))\ncombined = pd.concat([df[common_cols] for df in [train,test]], ignore_index=True)\n# applying groupby() function to \n# group the data on team value. \ngp = combined.groupby('sentiment') \n  \n# Let's print the first entries \n# in all the groups formed. \nfor name, group in gp: \n    print(name) \n    print(group) \n    print(len(group)) \n\noptimal_clusters = 100\nfor name, group in gp: \n    tfidf.fit(group.text)\n    text = tfidf.transform(group.text)   \n    find_optimal_clusters(text, optimal_clusters,name,init_size,batch_size)\n\nkmeans_collection = {}\nn_clusters = 100           \nfor name, group in gp: \n    tfidf.fit(group.text)\n    text = tfidf.transform(group.text) \n    kmeans = MiniBatchKMeans(n_clusters=n_clusters, init_size=init_size, batch_size=batch_size, random_state=20)\n    kmeans.fit(text)\n    clusters = kmeans.predict(text) \n    plot_tsne_pca(text, clusters,name)  \n    get_top_keywords(text, clusters, tfidf.get_feature_names(), 5,name)\n    kmeans_collection[name.lower()] = kmeans\n\n\ndef get_keywords(line,data, clusters, labels, n_terms,column):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    selected_text = []\n    for i,r in df.iterrows():\n        #print('\\nCluster {} column: {}'.format(i,column))\n        #key_words = ','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]]))\n        #print(key_words)\n        n_terms = len(labels)\n        key_words = ','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]]))\n        for word in line.strip().split():\n            for kw in key_words:\n                word = word.strip()\n                kw = kw.strip()\n                ld = 1.0-levenshtein_distance(word,kw)\/max(len(word),len(kw))\n                if ld > 0.1:\n                    selected_text.append(word) \n                    break\n            #if word in key_words:\n            #    selected_text.append(word)\n    return \" \".join(selected_text)\nscores = pd.DataFrame(columns = [\"sentiment\",\"text\",\"selected_text\",\"result\",\"jaccard_score\"])\ncount = 1\nmax_count = len(train)\nprint_every = 1000\ngp = train.groupby('sentiment') \nfor name, group in gp:\n    for query,selected_text in zip(group.text,group.selected_text):\n        text = tfidf.transform([query])      \n        cluster = kmeans_collection[name.lower()].predict(text)\n        result = get_keywords(selected_text,text,cluster,tfidf.get_feature_names(), 10,name)\n        js = jaccard(selected_text,result)\n        new_row = {'sentiment':name,'text':query, 'selected_text':selected_text, 'result':result, 'jaccard_score':js}\n        scores = scores.append(new_row, ignore_index=True)\n        if (count % print_every == 0):\n            print(\"Train Processed:\",count)\n        count = count + 1\n        if max_count < count:\n            break\nplt.figure()\nscores.sort_values(by=['jaccard_score'],inplace=True,ascending=True)    \nscores[\"jaccard_score\"].plot.kde()\nplt.hist(scores[\"jaccard_score\"], color = 'blue', edgecolor = 'black')\nplt.show()\nprint(scores[\"jaccard_score\"].mean())\n\nsubmission[\"selected_text\"] = submission[\"selected_text\"].astype(str)\nfor index in range(len(test)):\n    text = tfidf.transform([test.iloc[index]['text']])      \n    cluster = kmeans_collection[test.iloc[index]['sentiment'].lower()].predict(text)\n    result = get_keywords(test.iloc[index]['text'],text,cluster,tfidf.get_feature_names(), 10,name)\n    submission.at[index,'selected_text'] = result\n    if (index % print_every == 0):\n        print(\"Result:\" ,result)\n        print(\"Test Processed:\",index)\n        \nsubmission.to_csv(\"submission.csv\",index=False)","46462a65":"**We can identify four main steps in this process:\n\n* Data gathering\n* Data preparing\n* The creation of the sentiment analysis model\n* Visualization of the results**","de6b1140":"# Sentiment Analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative or neutral. Performing Sentiment Analysis on data using machine learning can help to understand how people are talking.","4fe15cd9":"# If you like the Kernel please upvote and if any query feel free to comment..... Thank You","ecd308e9":"**The KNN Algorithm\n1. Load the data\n2. Initialize K to your chosen number of neighbors\n3. For each example in the data\n\n3.1. Calculate the distance between the query example and the current example from the data.\n\n3.2.Add the distance and the index of the example to an ordered collection\n\n3.3.Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n\n3.4.Pick the first K entries from the sorted collection\n\n3.5.Get the labels of the selected K entries\n\n3.6.If regression, return the mean of the K labels\n\n3.7.If classification, return the mode of the K labels**"}}