{"cell_type":{"9f38218e":"code","1226ef27":"code","c6a68bc6":"code","626b4131":"code","a3854651":"code","60944574":"code","f241ee5d":"code","051f4114":"code","13bcf4d9":"code","ceabd8a4":"code","70c73928":"code","793ba2ae":"code","f80f520a":"code","8bc08c67":"code","f72288d3":"markdown","c0f28b44":"markdown","f5cfca16":"markdown","11f3e10a":"markdown","962bc710":"markdown","8a52063c":"markdown","62ade223":"markdown","1a08000a":"markdown"},"source":{"9f38218e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1226ef27":"def plot_graph(func, derivative):\n    \"\"\"Function for plotting activation and its derivative\"\"\"\n    fig = plt.figure(figsize=(20,5))\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.set_title('Activation function')\n    plt.plot(func)\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.set_title('Derivative')\n    plt.plot(derivative)\n    plt.show()","c6a68bc6":"def sigmoid(scores):\n    \"\"\"sigmoid function\"\"\"\n    return (1\/(1 + np.exp(-scores)))\n\ndef sigmoid_grad(scores):\n    \"\"\"sigmoid function derivative\"\"\"\n    return sigmoid(scores)*(1-sigmoid(scores))","626b4131":"\"\"\"Plot sigmoid and its derivative\"\"\"\nscores = np.linspace(-10,10,100)\nplot_graph(sigmoid(scores), sigmoid_grad(scores))","a3854651":"def tanh(scores):\n    \"\"\"tanh function\"\"\"\n    exp_val = np.exp(-2*scores)\n    return (2\/(1 + exp_val))-1\n\ndef tanh1(scores):\n    \"\"\"alternate implementation of tanh function\"\"\"\n    return (np.exp(scores) - np.exp(-scores))\/(np.exp(scores) + np.exp(-scores))\n\ndef tanh_grad(scores):\n    \"\"\"tanh derivative function\"\"\"\n    return 1 - np.power(tanh(scores), 2)","60944574":"\"\"\"Plot tanh and its derivative\"\"\"\nscores = np.linspace(-10,10,100)\nplot_graph(tanh(scores), tanh_grad(scores))","f241ee5d":"def relu(scores):\n    \"\"\"Return 0 if scores < 0 otherwise keep scores as it is.\"\"\"\n    scores[scores<=0]=0\n    return scores\n\ndef relu_grad(scores):\n    \"\"\"Return 1 if scores > 0 otherwise return 0\"\"\"\n    scores = np.where(scores>0, 1 , 0)\n    return scores","051f4114":"\"\"\"Plot ReLU function and its derivative\"\"\"\nscores = np.linspace(-10,10,100)\nplot_graph(relu(scores), relu_grad(scores))","13bcf4d9":"def leaky_relu(scores, alpha):\n    \"\"\"Return scores if >= 0 otherwise return scores*alpha for scores < 0\"\"\"\n    scores = np.where(scores>0,scores,scores * alpha)\n    return scores\n\ndef leaky_relu_grad(scores, alpha):\n    \"\"\"Return 1 if scores > 0 else return alpha\"\"\"\n    scores = np.where(scores > 0, 1, alpha)\n    return scores","ceabd8a4":"\"\"\"Plot leakyReLU and its derivative\"\"\"\nscores = np.linspace(-20,10,100)\nalpha = 0.01\nplot_graph(leaky_relu(scores, alpha), leaky_relu_grad(scores, alpha))","70c73928":"# Softmax \ndef softmax(scores):\n    \"\"\"Softmax function\"\"\"\n    exp_scores = np.exp(scores)\n    return exp_scores\/np.sum(exp_scores)\n\n\n# Source https:\/\/eli.thegreenplace.net\/2016\/the-softmax-function-and-its-derivative\/\ndef stablesoftmax(scores):\n    \"\"\"Compute the softmax of vector scores in a numerically stable way.\"\"\"\n    shift_scores = scores - np.max(scores)\n    exps = np.exp(shift_scores)\n    return exps \/ np.sum(exps)","793ba2ae":"scores = [0.1,2.5,0.3,4.2]\ngrad = softmax(scores)\nprint(f'{grad}')","f80f520a":"# Lets run same function with some bigger values. As mentioned above output is nan.\nscores_big_values = [1000,1500,1200]\ngrad_big_values = softmax(scores_big_values)\nprint(f'{grad_big_values}')\n# Run same scores with stablesoftmax()\nprint(f'{stablesoftmax(scores_big_values)}')","8bc08c67":"# Derivative of softmax\n# Source https:\/\/medium.com\/@aerinykim\/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d\n\ndef softmax_grad(softmax):\n    \"\"\"Derivative of softmax function.\n    Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n    \"\"\"\n    s = softmax.reshape(-1,1)\n    return np.diagflat(s) - np.dot(s, s.T)\n\n# It will return matrix of same shape as shape of matrix on L-1 layer i.e just before softmax layer\/output layer.\nsoftmax_grad(softmax(scores))","f72288d3":"## Leaky ReLU\n\nLeaky ReLU fixes the problem of dying ReLU. It gives a small negative slope for values less than 0. Normaly \n$\n\\alpha\n$ is kept as 0.01.\n\n$\nf(z) = \\alpha * z\n$\n\n## Derivative of Leaky ReLU function\n$\nf'(z) = z > 0,1 ; z <=0,\\alpha\n$","c0f28b44":"### Next will consolidate the code for developing 2 layer neural network.","f5cfca16":"## Activation Functions\nActivation functions introduce non linearity in neural networks. Without activation function neural network will be linear computation through out the network. In that case neural network will not be able to learn complex data. Activation functions supress less important data points and pass on relevant information to another layer.\n\nThere are two types of Avtivation functions:\n1. Linear activation functions.\n2. Non linear activation functions\n\nIn this kernel we will talk about non linear activation functions because in practice they are more used.\n\n## Non linear activation fucntions\n\n## Sigmoid\n\nSigmoid function is used for binary classification. This function keeps output values in range between 0 and 1.\nSo we can use this function for models predicting probability.\nSigmoid function has vanishing gradient problem.\n\nExpression:\n\n$\nf(z) = \\sigma(z) = 1\/1+e ^{-z}\n$\n\n## Derivative of sigmoid function\n\n$\nf'(z) = f(z)(1-f(z))\n$","11f3e10a":" ## ReLU\n \n $\n f(z) = max(0,z)\n $\n \nReturn 0 for negative values. As all negative values considered zero so once neurons get negative value will never be recovered and it is known as **dying ReLU**. It is used in between hidden layers of the network.\n\nIt does not have vanishing gradient problem. We loose information for negative value neurons.\n\n## Derivative of ReLU function\n$\nf'(z) = z > 0, 1 ; z <= 0, 0\n$","962bc710":"## TanH\n\nTanH is also non linear function. It's range is in between -1 and 1. It's output is zero centered because values are between -1 and 1 so mean will be around zero. Gradient is stronger(look at derivative plot) so more preferred over sigmoid. It is commonly used in recurrent neural networks. Tanh also faces vanishing gradient problem.\n\nTwo ways to implement\n\n$\nf(z) = tanh(z) = (2\/1+e ^{-2z}) -1\n$\n\n$\nf(z) = tanh(z) = (e ^z - e ^{-z}) \/ (e ^z + e ^{-z})\n$\n\n## Derivative of tanh function\n$\nf'(z) = 1-(tanh(z)^2)\n$","8a52063c":"## Softmax\nSoftmax is used for multilabel classifcation. Softmax is used at the output layer of neural network. It takes N dimensional vector of real numbers and converts in N dimensional vector of real numbers in range between 0 to 1 and sum upto 1. Softmax outputs a probability distribution makes it suitable for probabilistic interpretation in classification tasks.\n\n$\np(y=j | \\theta^{(i)}) = \\frac {e^{\\theta^{(i)}}} {\\sum_{j=0}^{k}e^{\\theta_k^{(i)}}}\n$\n\n\n$\n\\theta^{(i)} = x_0w_0 + x_1w_1 + ... + x_nw_n \n$\n\n\nRunning above function with larger numbers (or large negative numbers) have a problem.\nThe numerical range of the floating-point numbers used by Numpy is limited. \nFor float64, the maximal representable number is on the order of $10^{308}$. \nExponentiation in the softmax function makes it possible to easily overshoot this number, even for fairly modest-sized inputs.\nA nice way to avoid this problem is by normalizing the inputs to be not too large or too small, \nby observing that we can use an arbitrary constant C as follows:\n\n$\np(y=j | \\theta^{(i)}) = \\frac {C e^{\\theta^{(i)}}} {\\sum_{j=0}^{k}C e^{\\theta_k^{(i)}}}\n$\n\n## Derivative for softmax function\n\n$\ni = j,\n$\n\n$\np' = p_i(1-p_i)\n$\n\n$\ni \\neq j,\n$\n\n$\np' = -p_j * p_i\n$","62ade223":"References:\n\nhttps:\/\/eli.thegreenplace.net\/2016\/the-softmax-function-and-its-derivative\/\n\nhttp:\/\/willwolf.io\/2017\/04\/19\/deriving-the-softmax-from-first-principles\/\n\nhttps:\/\/deepnotes.io\/softmax-crossentropy\n\nhttps:\/\/www.youtube.com\/watch?v=Xvg00QnyaIY - Activation functions - Andrew NG\n\nhttps:\/\/www.youtube.com\/watch?v=P7_jFxTtJEo - Derivatives of activation functions - Andrew NG\n\nhttps:\/\/www.youtube.com\/watch?v=NkOv_k7r6no - Why do you need non linear function - Andrew NG\n\nhttps:\/\/stats.stackexchange.com\/questions\/265905\/derivative-of-softmax-with-respect-to-weights","1a08000a":"## Parametric ReLU\n\nParametric ReLU is similar to Leaky ReLU. Instead of fixing value of $\\alpha$ it gives flexibility to choose what will be the best slope \nfor negative values. If $\\alpha$ value assigned to 0.01 then it will be Leaky ReLU only.\n\n$\nf(z) = \\alpha * z\n$\n\n## Derivative of Parametric ReLU function\n$\nf'(z) = z > 0,1 ; z <=0,\\alpha\n$\nImplementation is same as leaky ReLU."}}