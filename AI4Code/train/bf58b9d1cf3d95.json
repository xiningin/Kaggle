{"cell_type":{"27eee6e0":"code","20eeffd0":"code","07050451":"code","5fffafef":"code","80b7948f":"code","00311832":"code","de8d471f":"code","7752823a":"code","740518a4":"code","d25a2e32":"code","b09a2caa":"code","8f94083e":"code","5729574f":"code","f81853f1":"code","9e957259":"code","8e66bb8d":"code","e807eda2":"code","21006bb3":"code","7aa05329":"code","f9108b3a":"code","74a03498":"code","274a204f":"code","5fdc3dc6":"code","fd16da38":"code","521c2bfa":"code","357c6f49":"code","a212d920":"code","8fc26276":"code","bcd050a9":"code","313bcb9f":"code","be6381c6":"code","3b96fcd3":"code","e1d2ea6b":"code","28ec78ce":"code","68d75cd6":"code","f9610d16":"code","4752dd50":"code","83b9e1f4":"code","109dc37b":"markdown","484a3a01":"markdown","f8a2d015":"markdown","d52ea189":"markdown","458c06ee":"markdown","2eb17e5c":"markdown","018b39e0":"markdown","a2aee009":"markdown","f26ad32c":"markdown","d982f6e0":"markdown"},"source":{"27eee6e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","20eeffd0":"import warnings\n# Ignore scipy warnings as it was intended to convert to sparse matrix below\nwarnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")","07050451":"import matplotlib.pyplot as plt\nimport string\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom scipy.sparse import coo_matrix, hstack, vstack\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport hyperopt\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom lightgbm import LGBMClassifier\nimport time","5fffafef":"train = pd.read_csv(\"\/kaggle\/input\/answerclassification\/preprocessed_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/answerclassification\/preprocessed_test.csv\")","80b7948f":"import re\n# We will just lowercase everything, and sub all digits with 1 as per the results of the hyperparameter tuning notebook\ntrain[\"Processed\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x.lower()))\ntest[\"Processed\"] = test[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x.lower()))","00311832":"# Helper function to split into train and test sets\ndef get_train_test_lgbm(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.1, min_df=50):\n    \n    if type(test) != pd.core.frame.DataFrame:\n        # Just to check if test is provided, then we'll do train, test instead\n        # of train val split\n        X = train.Processed\n        y = train.Outcome\n        \n        # We split by using test_size for y_val\n        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n        \n        # We're using tfidf vectorizer for our analysis, character level model\n        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n        \n        print(\"Fitting...\")\n        start = time.time()\n        # Fit transform the training, ONLY on training\n        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(X_train) \n        # Transform the x_val\n        X_val_dtm =  tfidf_vect_ngram_chars.transform(X_val) \n        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n        print(X_train_dtm.shape, X_val_dtm.shape)\n\n        # Adding in additional variables from EDA\n        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_train_dtm = hstack((X_train_dtm, var_sparse))\n\n        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_val_dtm = hstack((X_val_dtm, var_sparse))\n        \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_val: \", X_val_dtm.shape)\n        \n        return X_train_dtm, X_val_dtm, y_train, y_val\n    else:\n        # We're using tfidf vectorizer for our analysis, character level model\n        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n\n        print(\"Fitting...\")\n        start = time.time()\n        # Fit transform the training, ONLY on training\n        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(train.Processed)\n        # Transform the test comment\n        X_test_dtm =  tfidf_vect_ngram_chars.transform(test.Processed) \n        print(f\"Operation Took {time.time()-start}s\")\n        print(X_train_dtm.shape, X_test_dtm.shape)\n\n        # Add in additional variables from EDA\n        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_train_dtm = hstack((X_train_dtm, var_sparse))\n\n        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_test_dtm = hstack((X_test_dtm, var_sparse))\n        \n        print(X_train_dtm.shape, X_test_dtm.shape)\n        \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_test: \", X_test_dtm.shape)\n        \n        return X_train_dtm, X_test_dtm, train.Outcome","de8d471f":"# Hyperparameters from bayesian optimisation\nlg_params = {'boosting_type': 'gbdt',\n 'class_weight': 'balanced',\n 'colsample_bytree': 0.6370495458782991,\n 'learning_rate': 0.1,\n 'max_depth': 200,\n 'metric': 'auc',\n 'min_child_samples': 20,\n 'n_estimators': 200,\n 'num_leaves': 25,\n 'objective': 'binary',\n 'random_state': 1234,\n 'reg_alpha': 0.0720812229772364,\n 'reg_lambda': 1.87246159415014}","7752823a":"start = time.time()\nX_train, X_val, y_train, y_val = get_train_test_lgbm(train, test = None, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df = 50)\n\nLG = LGBMClassifier(**lg_params)\n%time LG.fit(X_train, y_train)","740518a4":"from sklearn import metrics\nprint(\"Train\")\ny_pred_class = LG.predict(X_train)\n# Comparison between vanilla roc_auc using predict vs if we use predict_proba\nprint(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\nprint(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\ny_pred_class = LG.predict_proba(X_train)\nprint(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class[:, 1]))\n\nprint(\"Validation\")\ny_pred_class = LG.predict(X_val)\nprint(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\nprint(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\ny_pred_class_lgbm = LG.predict_proba(X_val)[:, 1]\nprint(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class_lgbm))\nend = time.time() - start\nprint(f\"Entire Process Took {round(end,2)}seconds\")","d25a2e32":"def get_train_test_nn(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.1, min_df=50):\n    \n    if type(test) != pd.core.frame.DataFrame:\n        \n        X = train.Processed\n        y = train.Outcome\n        \n        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n        \n        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n\n        print(\"Fitting...\")\n        start = time.time()\n        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(X_train) \n        X_val_dtm =  tfidf_vect_ngram_chars.transform(X_val) \n        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n        \n        # Neural network needs to oversample\n        from imblearn.over_sampling import RandomOverSampler\n        ros = RandomOverSampler(random_state=0)\n        X_train_dtm, y_train = ros.fit_resample(X_train_dtm, y_train)\n        \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_val: \", X_val_dtm.shape)\n        \n        return X_train_dtm, X_val_dtm, y_train, y_val\n    else:\n        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n\n        print(\"Fitting...\")\n        start = time.time()\n        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(train.Processed) \n        X_test_dtm =  tfidf_vect_ngram_chars.transform(test.Processed) \n        print(f\"Operation Took {time.time()-start}s\")\n        print(X_train_dtm.shape, X_test_dtm.shape)\n        \n        # For neural network, need to oversample\n        from imblearn.over_sampling import RandomOverSampler\n        ros = RandomOverSampler(random_state=0)\n        X_train_dtm, y_train = ros.fit_resample(X_train_dtm, train.Outcome)\n    \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_test: \", X_test_dtm.shape)\n        \n        return X_train_dtm, X_test_dtm, y_train","b09a2caa":"from __future__ import division\nimport numpy as np\ndef plot_history(history):\n    # Plot loss and accuracy \n    fig = plt.figure(figsize=(10,5))\n\n    #plt.subplot(1, 2, 1)\n    plt.plot(history.epoch, history.history['val_loss'], 'g-', label='Validation data')\n    plt.plot(history.epoch, history.history['loss'], 'r--', label='Training data')\n    plt.grid(True)\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Loss on training\/validation data')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.show()\n\n    #plt.subplot(1, 2, 2)\n    fig = plt.figure(figsize=(10,5))\n    plt.plot(history.epoch, history.history['val_accuracy'], 'g-', label='Validation data')\n    plt.plot(history.epoch, history.history['accuracy'], 'r--', label='Training data')\n    plt.grid(True)\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Accuracy on training\/validation data')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.show()","8f94083e":"start = time.time()\nX_train, X_val, y_train, y_val = get_train_test_nn(train, test = None, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df=50)","5729574f":"from numpy.random import seed\nseed(1)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nimport keras\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\n# define network\nmodel = Sequential()\nmodel.add(Dense(10, input_shape=(X_train.shape[1],), activation=\"linear\",\n                kernel_initializer=keras.initializers.he_normal(seed=1)))\nmodel.add(Activation('relu'))\n\n#model.add(LeakyReLU(alpha=.3))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=5, verbose=2, validation_data=(X_val, y_val), batch_size = 256)\nplot_history(history)","f81853f1":"print(\"Train\")\ny_pred_class = model.predict_proba(X_train)\nprint(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class.round().astype('int')))\ny_pred_class = model.predict_proba(X_train)\nprint(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n\nprint(\"Validation\")\ny_pred_class = model.predict(X_val)\nprint(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class.round().astype('int')))\ny_pred_class_nn = model.predict_proba(X_val)\nprint(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class_nn))","9e957259":"# Helper function to get train, val and test data\ndef get_train_test_cbc(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.1, min_df=50):\n    \n    if type(test) != pd.core.frame.DataFrame:\n        # To check if we want to split into train val, or train test\n        \n        # Use only the train data for train val split\n        X = train.Processed\n        y = train.Outcome\n        \n        # split into train and test set, using random_state so that it is reproducable\n        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n        \n        # We use count vect character level analyser\n        # Binary set to true\n        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n\n        print(\"Fitting...\")\n        start = time.time()\n        # Fit transform only on the train set, use it to transform the val set\n        X_train_dtm =  count_vect_ngram_chars.fit_transform(X_train) \n        X_val_dtm =  count_vect_ngram_chars.transform(X_val) \n        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n        print(X_train_dtm.shape, X_val_dtm.shape)\n\n        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            # Stacks horizontally, effectively increasing columns of features to include our EDA\n            X_train_dtm = hstack((X_train_dtm, var_sparse))\n\n        # Repeat the same for the validation set\n        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_val_dtm = hstack((X_val_dtm, var_sparse))\n        \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_val: \", X_val_dtm.shape)\n        \n        return X_train_dtm.tocsr().astype(np.int8), X_val_dtm.tocsr().astype(np.int8), y_train, y_val\n    else:\n        # We use ccount vect character level analyser\n        # Binary set to true\n        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n\n        print(\"Fitting...\")\n        start = time.time()\n        # Fit on train, transform train and test\n        X_train_dtm =  count_vect_ngram_chars.fit_transform(train.Processed) \n        X_test_dtm =  count_vect_ngram_chars.transform(test.Processed) \n        print(f\"Operation Took {time.time()-start}s\")\n        print(X_train_dtm.shape, X_test_dtm.shape)\n\n        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            # Stacks horizontally, effectively increasing columns of features to include our EDA\n            X_train_dtm = hstack((X_train_dtm, var_sparse))\n\n        # Repeat the same for the test set\n        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n        for column in add_var_df.columns:\n            var_sparse = add_var_df[column].values[:, None]\n            X_test_dtm = hstack((X_test_dtm, var_sparse))\n        \n        print(X_train_dtm.shape, X_test_dtm.shape)\n        \n        print(\"X_train: \", X_train_dtm.shape)\n        print(\"X_test: \", X_test_dtm.shape)\n        \n        return X_train_dtm.tocsr().astype(np.int8), X_test_dtm.tocsr().astype(np.int8), train.Outcome","8e66bb8d":"start = time.time()\nX_train, X_val, y_train, y_val = get_train_test_cbc(train, test = None, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df=50)\ncat_feat = X_train.shape[1] - 12","e807eda2":"y_train.value_counts().plot(kind=\"bar\")","21006bb3":"from catboost import CatBoostClassifier\nzero_weight = y_train.value_counts().loc[1]\/y_train.value_counts().loc[0]\none_weight = 1\nCBC = CatBoostClassifier(cat_features=list(range(cat_feat)), class_weights=[zero_weight, one_weight], \n                         loss_function='Logloss',eval_metric='AUC', verbose=0)\n%time CBC.fit(X_train, y_train)","7aa05329":"from sklearn import metrics\nprint(\"Train\")\ny_pred_class = CBC.predict(X_train)\n# Comparison between vanilla roc_auc using predict vs if we use predict_proba\nprint(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\nprint(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\ny_pred_class = CBC.predict_proba(X_train)\nprint(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class[:, 1]))\n\nprint(\"Validation\")\ny_pred_class = CBC.predict(X_val)\nprint(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\nprint(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\ny_pred_class_cbc = CBC.predict_proba(X_val)[:, 1]\nprint(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class_cbc))\nend = time.time() - start\nprint(f\"Entire Process Took {round(end,2)}seconds\")","f9108b3a":"y_pred_all = (y_pred_class_nn[:, 0] + y_pred_class_lgbm) \/ 2\nprint(\"Auroc ANN: \", metrics.roc_auc_score(y_val, y_pred_class_nn))\nprint(\"Auroc LGB: \", metrics.roc_auc_score(y_val, y_pred_class_lgbm))\nprint(\"Auroc Ensemble: \", metrics.roc_auc_score(y_val, y_pred_all))","74a03498":"y_pred_all = (y_pred_class_cbc + y_pred_class_lgbm) \/ 2\nprint(\"Auroc CBC: \", metrics.roc_auc_score(y_val, y_pred_class_cbc))\nprint(\"Auroc LGB: \", metrics.roc_auc_score(y_val, y_pred_class_lgbm))\nprint(\"Auroc Ensemble: \", metrics.roc_auc_score(y_val, y_pred_all))","274a204f":"y_pred_all = (y_pred_class_cbc + y_pred_class_nn[:, 0]) \/ 2\nprint(\"Auroc CBC: \", metrics.roc_auc_score(y_val, y_pred_class_cbc))\nprint(\"Auroc ANN: \", metrics.roc_auc_score(y_val, y_pred_class_nn))\nprint(\"Auroc Ensemble: \", metrics.roc_auc_score(y_val, y_pred_all))","5fdc3dc6":"y_pred_all = (y_pred_class_cbc + y_pred_class_nn[:, 0] + y_pred_class_lgbm) \/ 3\nprint(\"Auroc CBC: \", metrics.roc_auc_score(y_val, y_pred_class_cbc))\nprint(\"Auroc ANN: \", metrics.roc_auc_score(y_val, y_pred_class_nn))\nprint(\"Auroc LGB: \", metrics.roc_auc_score(y_val, y_pred_class_lgbm))\nprint(\"Auroc Ensemble: \", metrics.roc_auc_score(y_val, y_pred_all))","fd16da38":"results = pd.DataFrame([y_pred_class_cbc, y_pred_class_nn[:, 0], y_pred_class_lgbm]).T\nresults.columns = [\"CatBoost\", \"Neural Network\", \"Light Gradient Boosting\"]\nresults.head()","521c2bfa":"import seaborn as sns\nsns.heatmap(results.corr(), annot = True, cmap=sns.color_palette(\"Blues\"))\nplt.title(\"Prediction Correlations\")","357c6f49":"X_train, X_test, y_train = get_train_test_lgbm(train, test = test, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df = 50)\n\nLG = LGBMClassifier(**lg_params)\n%time LG.fit(X_train, y_train)\nfinal_pred_lgbm = LG.predict_proba(X_test)[:, 1]","a212d920":"X_train, X_test, y_train = get_train_test_nn(train, test = test, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df=50)\n\nseed(1)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nimport keras\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\n# define network\nmodel = Sequential()\nmodel.add(Dense(10, input_shape=(X_train.shape[1],), activation=\"linear\",\n                kernel_initializer=keras.initializers.he_normal(seed=1)))\nmodel.add(Activation('relu'))\n#model.add(LeakyReLU(alpha=.3))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=5, verbose=2, batch_size = 256)\nfinal_pred_nn = model.predict_proba(X_test)[:, 0] # For nn, 0 instead of 1","8fc26276":"X_train, X_test, y_train = get_train_test_cbc(train, test = test, ngram_range = (2,5), \n                    max_features=None, random_state=1, test_size=0.1, min_df=50)\ncat_feat = X_train.shape[1] - 12\nzero_weight = y_train.value_counts().loc[1]\/y_train.value_counts().loc[0]\none_weight = 1\nCBC = CatBoostClassifier(cat_features=list(range(cat_feat)), class_weights=[zero_weight, one_weight], \n                         loss_function='Logloss',eval_metric='AUC', verbose=0)\n%time CBC.fit(X_train, y_train)\nfinal_pred_cbc = CBC.predict_proba(X_test)[:, 1]","bcd050a9":"results = pd.DataFrame([final_pred_cbc, final_pred_nn, final_pred_lgbm]).T\nresults.columns = [\"CatBoost\", \"Neural Network\", \"Light Gradient Boosting\"]\nresults.head()","313bcb9f":"import seaborn as sns\nsns.heatmap(results.corr(), annot = True, cmap=sns.color_palette(\"Blues\"))\nplt.title(\"Prediction Correlations\")","be6381c6":"final_pred_all = (final_pred_cbc + final_pred_nn + final_pred_lgbm) \/ 3","3b96fcd3":"test[\"Outcome\"] = final_pred_all","e1d2ea6b":"test[[\"Id\", \"Outcome\"]].to_csv(\"submission_ensemble.csv\", index=False)","28ec78ce":"submission = test[[\"Id\", \"Outcome\"]].copy()\nsubmission.head()","68d75cd6":"plt.figure(figsize = (20,5))\ntrain = pd.read_csv(\"\/kaggle\/input\/rating-classification\/train.csv\")\nplt.plot(train.Outcome)\nplt.title(\"Natural Order of Outcome Train Set\")","f9610d16":"plt.figure(figsize=(20,5))\nplt.plot(submission.Outcome)\nplt.title(\"Predicted Probability of Outcome\")","4752dd50":"plt.figure(figsize=(20,5))\nsubmission[\"Outcome\"] = submission[[\"Outcome\"]].rolling(70).mean().fillna(0.6).Outcome\nplt.plot(submission[\"Outcome\"])\nplt.title(\"Predicted Probability of Outcome (Rolling 70 mean)\")","83b9e1f4":"submission[[\"Id\", \"Outcome\"]].to_csv(\"submission_potential_leakage.csv\", index=False)","109dc37b":"# 4. Model Correlations and Equal Weighted Ensemble","484a3a01":"# 2. Model 2: Neural Network Character Level TfidfVectorizer","f8a2d015":"### Adding all into a model yields the best results. This is because they have similar performance, while not perfectly correlated which will boost results!","d52ea189":"# 3. Model 3: Catboost Character Level CountVectorizer using Binary Categorical Variables","458c06ee":"# 1. Model 1: LightGBM Character Level TfidfVectorizer","2eb17e5c":"#### We can see that the the answers have been ordered! We can use this to our advantage.","018b39e0":"# Winning Solution for BT4222 Answer Classification\n## Ensemble of LGBM, Neural Networks, CatBoost\n\nSummary <br>\nThis competition seeks to classify answers based on quality, 0 or 1. Since answers have code, we default to a simple preprocessing of merely changing all numeric digits to 1, and lowering all texts to lowercase. If we conduct more preprocessing, we risk removing valuable information. Prior to this notebook, text preprocessing hyperparameters were found, with normalising digits found to perform the best. The ngrams used in text vectorizers were also found, and (2,5) yielded the best results.\n\n## Contents\n## Model 1: LightGBM Character Level TfidfVectorizer \nCharacter level vectorizers were far superior as compared to word level vectorizers. By using character level vectorizers, important syntax such as code can be captured effectively by the models.\n## Model 2: Neural Network Character Level TfidfVectorizer\nMerging uncorrelated but well performing models will boost performance when added into an ensemble. A shallow neural network with higher regularization is used.\n## Model 3: Catboost Character Level CountVectorizer using Binary Categorical Variables\nAnother slightly uncorrelated model using categorical variables. Instead of using numerical variables, we use categorical variables (1 if the character pattern exists, 0 if it does not)\n## Model Correlations and Equal Weighted Ensemble\nThe models had imperfect correlations while having equal predictive power. This resulted in a more robust ensemble.\n## Final Predictions (0.817 AUROC Private Leaderboard)\n## Exploiting Data Leakage: Post Data Manipulation (0.999 AUROC Private Leaderboard)\nHere, I show how in-depth exploratory data analysis led to the discovery of data leakage and how data manipulation produced a large jump in model performance.","a2aee009":"# 6. Exploiting Data Leakage: Post Data Manipulation (0.999 AUROC Private Leaderboard)","f26ad32c":"#### By applying a mean over a rolling window, the different classes will be separated nicely as shown below!","d982f6e0":"# 5. Final Predictions (0.817 AUROC)"}}