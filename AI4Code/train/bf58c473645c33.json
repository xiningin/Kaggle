{"cell_type":{"56a2bc80":"code","9e1247d8":"code","feac8add":"code","d857072a":"code","ffd26623":"code","9f56039a":"code","5a75f67b":"code","a2db8399":"code","79b9708d":"code","428d7d8f":"code","399cbd92":"code","82e93bd9":"code","5527936c":"code","ad054c4d":"code","e0d3d83a":"code","64c8f4ea":"code","469ce26b":"code","8f02d7d2":"code","79919584":"code","6d1933d3":"code","4013d501":"code","061bc04b":"code","0dec15f6":"code","9242bd93":"code","592c48af":"code","6ccee014":"code","6bf52690":"code","6c5173df":"code","41016762":"code","639efcd4":"code","ee500577":"code","3717e4b9":"code","de25f6cc":"code","294f5f6d":"code","3272c441":"markdown","89ec3fbc":"markdown","042f022a":"markdown","73eabbd0":"markdown","d0b67052":"markdown","ed3254ad":"markdown","a8ef2f5c":"markdown","2660402e":"markdown","3bd41c13":"markdown","132d56fb":"markdown","584b69ec":"markdown","902cd37c":"markdown","900b55fd":"markdown","00c44f08":"markdown","12a21f88":"markdown","2e4ce0ae":"markdown","f64b8cdc":"markdown","6833d740":"markdown","e77da982":"markdown","05abf439":"markdown","3292f435":"markdown","8ad74b01":"markdown"},"source":{"56a2bc80":"import sys\nimport os\nimport glob\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid import ImageGrid\n\nplt.style.use(\"dark_background\")","9e1247d8":"# Path to all data\nDATA_PATH = \"\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/\"\n\n# File path line length images for later sorting\nBASE_LEN = 89 # len(\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_ <-!!!43.tif)\nEND_IMG_LEN = 4 # len(\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_43 !!!->.tif)\nEND_MASK_LEN = 9 # (\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/TCGA_DU_6404_19850629_43 !!!->_mask.tif)\n\n# img size\nIMG_SIZE = 512","feac8add":"# Raw data\ndata_map = []\nfor sub_dir_path in glob.glob(DATA_PATH+\"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"\/\" + filename\n            data_map.extend([dirname, image_path])\n    else:\n        print(\"This is not a dir:\", sub_dir_path)\n        \n        \ndf = pd.DataFrame({\"dirname\" : data_map[::2],\n                  \"path\" : data_map[1::2]})\ndf.head()","d857072a":"# Masks\/Not masks\ndf_imgs = df[~df['path'].str.contains(\"mask\")]\ndf_masks = df[df['path'].str.contains(\"mask\")]\n\n# Data sorting\nimgs = sorted(df_imgs[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_IMG_LEN]))\nmasks = sorted(df_masks[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_MASK_LEN]))\n\n# Sorting check\nidx = random.randint(0, len(imgs)-1)\nprint(\"Path to the Image:\", imgs[idx], \"\\nPath to the Mask:\", masks[idx])","ffd26623":"# Final dataframe\ndf = pd.DataFrame({\"patient\": df_imgs.dirname.values,\n                       \"image_path\": imgs,\n                   \"mask_path\": masks})\n\n\n# Adding A\/B column for diagnosis\ndef positiv_negativ_diagnosis(mask_path):\n    value = np.max(cv2.imread(mask_path))\n    if value > 0 : return 1\n    else: return 0\n\ndf[\"diagnosis\"] = df[\"mask_path\"].apply(lambda m: positiv_negativ_diagnosis(m))\ndf","9f56039a":"# Plot\nax = df.diagnosis.value_counts().plot(kind='bar',\n                                      stacked=True,\n                                      figsize=(10, 6),\n                                     color=[\"violet\", \"lightseagreen\"])\n\n\nax.set_xticklabels([\"Positive\", \"Negative\"], rotation=45, fontsize=12);\nax.set_ylabel('Total Images', fontsize = 12)\nax.set_title(\"Distribution of data grouped by diagnosis\",fontsize = 18, y=1.05)\n\n# Annotate\nfor i, rows in enumerate(df.diagnosis.value_counts().values):\n    ax.annotate(int(rows), xy=(i, rows-12), \n                rotation=0, color=\"white\", \n                ha=\"center\", verticalalignment='bottom', \n                fontsize=15, fontweight=\"bold\")\n    \nax.text(1.2, 2550, f\"Total {len(df)} images\", size=15,\n        color=\"black\",\n         ha=\"center\", va=\"center\",\n         bbox=dict(boxstyle=\"round\",\n                   fc=(\"lightblue\"),\n                   ec=(\"black\"),\n                   )\n         );","5a75f67b":"# Data\npatients_by_diagnosis = df.groupby(['patient', 'diagnosis'])['diagnosis'].size().unstack().fillna(0)\npatients_by_diagnosis.columns = [\"Positive\", \"Negative\"]\n\n# Plot\nax = patients_by_diagnosis.plot(kind='bar',stacked=True,\n                                figsize=(18, 10),\n                                color=[\"mediumvioletred\", \"springgreen\"], \n                                alpha=0.9)\nax.legend(fontsize=20);\nax.set_xlabel('Patients',fontsize = 20)\nax.set_ylabel('Total Images', fontsize = 20)\nax.set_title(\"Distribution of data grouped by patient and diagnosis\",fontsize = 25, y=1.005)\n\n# Annotations\n\"\"\"for i, rows in enumerate(patients_by_diagnosis.values):\n    plt.annotate(int(rows[0]), xy=(i, rows[0]+1), rotation=90, color=\"white\")\n    plt.annotate(int(rows[1]), xy=(i, rows[1]+1), rotation=90, color=\"aqua\")\"\"\";","a2db8399":"# Data\nsample_yes_df = df[df[\"diagnosis\"] == 1].sample(5).image_path.values\nsample_no_df = df[df[\"diagnosis\"] == 0].sample(5).image_path.values\n\nsample_imgs = []\nfor i, (yes, no) in enumerate(zip(sample_yes_df, sample_no_df)):\n    yes = cv2.resize(cv2.imread(yes), (IMG_SIZE, IMG_SIZE))\n    no = cv2.resize(cv2.imread(no), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([yes, no])\n\n\nsample_yes_arr = np.vstack(np.array(sample_imgs[::2]))\nsample_no_arr = np.vstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(1, 4),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_yes_arr)\ngrid[0].set_title(\"Positive\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_no_arr)\ngrid[1].set_title(\"Negative\", fontsize=15)\ngrid[1].axis(\"off\")\n\ngrid[2].imshow(sample_yes_arr[:,:,0], cmap=\"hot\")\ngrid[2].set_title(\"Positive\", fontsize=15)\ngrid[2].axis(\"off\")\ngrid[3].imshow(sample_no_arr[:,:,0], cmap=\"hot\")\ngrid[3].set_title(\"Negative\", fontsize=15)\ngrid[3].axis(\"off\")#set_title(\"No\", fontsize=15)\n\n# annotations\nplt.figtext(0.36,0.90,\"Original\", va=\"center\", ha=\"center\", size=20)\nplt.figtext(0.66,0.90,\"With hot colormap\", va=\"center\", ha=\"center\", size=20)\nplt.suptitle(\"Brain MRI Images for Brain Tumor Detection\\nLGG Segmentation Dataset\", y=.95, fontsize=30, weight=\"bold\")\n\n# save and show\nplt.savefig(\"dataset.png\", bbox_inches='tight', pad_inches=0.2, transparent=True)\nplt.show()","79b9708d":"# Data\nsample_df = df[df[\"diagnosis\"] == 1].sample(5).values\nsample_imgs = []\nfor i, data in enumerate(sample_df):\n    #print(data)\n    img = cv2.resize(cv2.imread(data[1]), (IMG_SIZE, IMG_SIZE))\n    mask = cv2.resize(cv2.imread(data[2]), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([img, mask])\n\n\nsample_imgs_arr = np.hstack(np.array(sample_imgs[::2]))\nsample_masks_arr = np.hstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 1),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_imgs_arr)\ngrid[0].set_title(\"Images\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_masks_arr)\ngrid[1].set_title(\"Masks\", fontsize=15, y=0.9)\ngrid[1].axis(\"off\")\nplt.show()","428d7d8f":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor, ToTensorV2\n\nfrom sklearn.model_selection import train_test_split","399cbd92":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","82e93bd9":"class BrainMriDataset(Dataset):\n    def __init__(self, df, transforms):\n        \n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.df.iloc[idx, 1])\n        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n\n        augmented = self.transforms(image=image, \n                                    mask=mask)\n \n        image = augmented['image']\n        mask = augmented['mask']   \n        \n        return image, mask\n    \n        # unnormilize mask\n        #mask = torch.clamp(mask.float(), min=0, max=1)\n        #mask = torch.ceil(mask)       ","5527936c":"PATCH_SIZE = 128#256\n\nstrong_transforms = A.Compose([\n    A.RandomResizedCrop(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n    \n    # Pixels\n    A.RandomBrightnessContrast(p=0.5),\n    A.RandomGamma(p=0.25),\n    A.IAAEmboss(p=0.25),\n    A.Blur(p=0.01, blur_limit = 3),\n    \n    # Affine\n    A.OneOf([\n        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n    ], p=0.8),\n    \n    \n    A.Normalize(p=1.0),\n    #https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/pytorch.html?highlight=ToTensor#albumentations.pytorch.transforms.ToTensor\n    ToTensor(),\n])\n\n\ntransforms = A.Compose([\n    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n\n    \n    \n    A.Normalize(p=1.0),\n    ToTensor(),\n])","ad054c4d":"# Split df into train_df and val_df\ntrain_df, val_df = train_test_split(df, stratify=df.diagnosis, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\n# Split train_df into train_df and test_df\ntrain_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.15)\ntrain_df = train_df.reset_index(drop=True)\n\n#train_df = train_df[:1000]\nprint(f\"Train: {train_df.shape} \\nVal: {val_df.shape} \\nTest: {test_df.shape}\")","e0d3d83a":"# train\ntrain_dataset = BrainMriDataset(df=train_df, transforms=transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n# val\nval_dataset = BrainMriDataset(df=val_df, transforms=transforms)\nval_dataloader = DataLoader(val_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n#test\ntest_dataset = BrainMriDataset(df=test_df, transforms=transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=26, num_workers=4, shuffle=True)","64c8f4ea":"def show_aug(inputs, nrows=5, ncols=5, image=True):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0., hspace=0.)\n    i_ = 0\n    \n    if len(inputs) > 25:\n        inputs = inputs[:25]\n        \n    for idx in range(len(inputs)):\n    \n        # normalization\n        if image is True:           \n            img = inputs[idx].numpy().transpose(1,2,0)\n            mean = [0.485, 0.456, 0.406]\n            std = [0.229, 0.224, 0.225] \n            img = (img*std+mean).astype(np.float32)\n        else:\n            img = inputs[idx].numpy().astype(np.float32)\n            img = img[0,:,:]\n        \n        #plot\n        #print(img.max(), len(np.unique(img)))\n        plt.subplot(nrows, ncols, i_+1)\n        plt.imshow(img); \n        plt.axis('off')\n \n        i_ += 1\n        \n    return plt.show()\n\n    \nimages, masks = next(iter(train_dataloader))\nprint(images.shape, masks.shape)\n\nshow_aug(images)\nshow_aug(masks, image=False)","469ce26b":"def double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True))","8f02d7d2":"class UNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n                \n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n\n        self.maxpool = nn.MaxPool2d(2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n        \n        self.conv_up3 = double_conv(256 + 512, 256)\n        self.conv_up2 = double_conv(128 + 256, 128)\n        self.conv_up1 = double_conv(128 + 64, 64)\n        \n        self.last_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n        \n    def forward(self, x):\n        # Batch - 1d tensor.  N_channels - 1d tensor, IMG_SIZE - 2d tensor.\n        # Example: x.shape >>> (10, 3, 256, 256).\n        \n        conv1 = self.conv_down1(x)  # <- BATCH, 3, IMG_SIZE  -> BATCH, 64, IMG_SIZE..\n        x = self.maxpool(conv1)     # <- BATCH, 64, IMG_SIZE -> BATCH, 64, IMG_SIZE 2x down.\n        conv2 = self.conv_down2(x)  # <- BATCH, 64, IMG_SIZE -> BATCH,128, IMG_SIZE.\n        x = self.maxpool(conv2)     # <- BATCH, 128, IMG_SIZE -> BATCH, 128, IMG_SIZE 2x down.\n        conv3 = self.conv_down3(x)  # <- BATCH, 128, IMG_SIZE -> BATCH, 256, IMG_SIZE.\n        x = self.maxpool(conv3)     # <- BATCH, 256, IMG_SIZE -> BATCH, 256, IMG_SIZE 2x down.\n        x = self.conv_down4(x)      # <- BATCH, 256, IMG_SIZE -> BATCH, 512, IMG_SIZE.\n        x = self.upsample(x)        # <- BATCH, 512, IMG_SIZE -> BATCH, 512, IMG_SIZE 2x up.\n        \n        #(Below the same)                                 N this       ==        N this.  Because the first N is upsampled.\n        x = torch.cat([x, conv3], dim=1) # <- BATCH, 512, IMG_SIZE & BATCH, 256, IMG_SIZE--> BATCH, 768, IMG_SIZE.\n        \n        x = self.conv_up3(x) #  <- BATCH, 768, IMG_SIZE --> BATCH, 256, IMG_SIZE. \n        x = self.upsample(x)  #  <- BATCH, 256, IMG_SIZE -> BATCH,  256, IMG_SIZE 2x up.   \n        x = torch.cat([x, conv2], dim=1) # <- BATCH, 256,IMG_SIZE & BATCH, 128, IMG_SIZE --> BATCH, 384, IMG_SIZE.  \n\n        x = self.conv_up2(x) # <- BATCH, 384, IMG_SIZE --> BATCH, 128 IMG_SIZE. \n        x = self.upsample(x)   # <- BATCH, 128, IMG_SIZE --> BATCH, 128, IMG_SIZE 2x up.     \n        x = torch.cat([x, conv1], dim=1) # <- BATCH, 128, IMG_SIZE & BATCH, 64, IMG_SIZE --> BATCH, 192, IMG_SIZE.  \n        \n        x = self.conv_up1(x) # <- BATCH, 128, IMG_SIZE --> BATCH, 64, IMG_SIZE.\n        \n        out = self.last_conv(x) # <- BATCH, 64, IMG_SIZE --> BATCH, n_classes, IMG_SIZE.\n        out = torch.sigmoid(out)\n        \n        return out","79919584":"unet = UNet(n_classes=1).to(device)\noutput = unet(torch.randn(1,3,256,256).to(device))\nprint(\"\",output.shape)","6d1933d3":"class ConvReluUpsample(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.make_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = self.make_upsample(x)\n        return x\n\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [ConvReluUpsample(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(ConvReluUpsample(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)","4013d501":"class FPN(nn.Module):\n\n    def __init__(self, n_classes=1, \n                 pyramid_channels=256, \n                 segmentation_channels=256):\n        super().__init__()\n         \n        # Bottom-up layers\n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n        self.conv_down5 = double_conv(512, 1024)   \n        self.maxpool = nn.MaxPool2d(2)\n        \n        # Top layer\n        self.toplayer = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n        \n        # Segmentation block layers\n        self.seg_blocks = nn.ModuleList([\n            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n            for n_upsamples in [0, 1, 2, 3]\n        ])\n        \n        # Last layer\n        self.last_conv = nn.Conv2d(256, n_classes, kernel_size=1, stride=1, padding=0)\n        \n    def upsample_add(self, x, y):\n        _,_,H,W = y.size()\n        upsample = nn.Upsample(size=(H,W), mode='bilinear', align_corners=True) \n        \n        return upsample(x) + y\n    \n    def upsample(self, x, h, w):\n        sample = nn.Upsample(size=(h, w), mode='bilinear', align_corners=True)\n        return sample(x)\n        \n    def forward(self, x):\n        \n        # Bottom-up\n        c1 = self.maxpool(self.conv_down1(x))\n        c2 = self.maxpool(self.conv_down2(c1))\n        c3 = self.maxpool(self.conv_down3(c2))\n        c4 = self.maxpool(self.conv_down4(c3))\n        c5 = self.maxpool(self.conv_down5(c4)) \n        \n        # Top-down\n        p5 = self.toplayer(c5) \n        p4 = self.upsample_add(p5, self.latlayer1(c4)) \n        p3 = self.upsample_add(p4, self.latlayer2(c3))\n        p2 = self.upsample_add(p3, self.latlayer3(c2)) \n        \n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        \n        # Segmentation\n        _, _, h, w = p2.size()\n        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p2, p3, p4, p5])]\n        \n        out = self.upsample(self.last_conv(sum(feature_pyramid)), 4 * h, 4 * w)\n        \n        out = torch.sigmoid(out)\n        return out","061bc04b":"fpn = FPN().to(device)\noutput = fpn(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","0dec15f6":"from torchvision.models import resnext50_32x4d\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, padding):\n        super().__init__()\n\n        self.convrelu = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.convrelu(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = ConvRelu(in_channels, in_channels \/\/ 4, 1, 0)\n        \n        self.deconv = nn.ConvTranspose2d(in_channels \/\/ 4, in_channels \/\/ 4, kernel_size=4,\n                                          stride=2, padding=1, output_padding=0)\n        \n        self.conv2 = ConvRelu(in_channels \/\/ 4, out_channels, 1, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.deconv(x)\n        x = self.conv2(x)\n\n        return x","9242bd93":"class ResNeXtUNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.base_model = resnext50_32x4d(pretrained=True)\n        self.base_layers = list(self.base_model.children())\n        filters = [4*64, 4*128, 4*256, 4*512]\n        \n        # Down\n        self.encoder0 = nn.Sequential(*self.base_layers[:3])\n        self.encoder1 = nn.Sequential(*self.base_layers[4])\n        self.encoder2 = nn.Sequential(*self.base_layers[5])\n        self.encoder3 = nn.Sequential(*self.base_layers[6])\n        self.encoder4 = nn.Sequential(*self.base_layers[7])\n\n        # Up\n        self.decoder4 = DecoderBlock(filters[3], filters[2])\n        self.decoder3 = DecoderBlock(filters[2], filters[1])\n        self.decoder2 = DecoderBlock(filters[1], filters[0])\n        self.decoder1 = DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.last_conv0 = ConvRelu(256, 128, 3, 1)\n        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)\n                       \n        \n    def forward(self, x):\n        # Down\n        x = self.encoder0(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Up + sc\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n        #print(d1.shape)\n\n        # final classifier\n        out = self.last_conv0(d1)\n        out = self.last_conv1(out)\n        out = torch.sigmoid(out)\n        \n        return out","592c48af":"rx50 = ResNeXtUNet(n_classes=1).to(device)\noutput = rx50(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","6ccee014":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n\n    return intersection \/ union\n\n# Metric check\ndice_coef_metric(np.array([0., 0.9]), \n                 np.array([0., 1]))","6bf52690":"def dice_coef_loss(inputs, target):\n    smooth = 1.0\n    intersection = 2.0 * ((target * inputs).sum()) + smooth\n    union = target.sum() + inputs.sum() + smooth\n\n    return 1 - (intersection \/ union)\n\n\ndef bce_dice_loss(inputs, target):\n    dicescore = dice_coef_loss(inputs, target)\n    bcescore = nn.BCELoss()\n    bceloss = bcescore(inputs, target)\n\n    return bceloss + dicescore\n\n# loss check\nbce_dice_loss(torch.tensor([0.7, 1., 1.]), \n              torch.tensor([1.,1.,1.]))","6c5173df":"def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):  \n    \n    print(model_name)\n    loss_history = []\n    train_history = []\n    val_history = []\n\n    for epoch in range(num_epochs):\n        model.train() # Enter train mode\n        \n        losses = []\n        train_iou = []\n                \n        if lr_scheduler:\n            \n            warmup_factor = 1.0 \/ 100\n            warmup_iters = min(100, len(train_loader) - 1)\n            lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n        \n        \n        for i_step, (data, target) in enumerate(train_loader):\n            data = data.to(device)\n            target = target.to(device)\n                      \n            outputs = model(data)\n            \n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n            \n            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            \n            loss = train_loss(outputs, target)\n            \n            losses.append(loss.item())\n            train_iou.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            if lr_scheduler:\n                lr_scheduler.step()\n \n        #torch.save(model.state_dict(), f'{model_name}_{str(epoch)}_epoch.pt')\n        val_mean_iou = compute_iou(model, val_loader)\n        \n        loss_history.append(np.array(losses).mean())\n        train_history.append(np.array(train_iou).mean())\n        val_history.append(val_mean_iou)\n        \n        print(\"Epoch [%d]\" % (epoch))\n        print(\"Mean loss on train:\", np.array(losses).mean(), \n              \"\\nMean DICE on train:\", np.array(train_iou).mean(), \n              \"\\nMean DICE on validation:\", val_mean_iou)\n        \n    return loss_history, train_history, val_history\n\n\ndef compute_iou(model, loader, threshold=0.3):\n    \"\"\"\n    Computes accuracy on the dataset wrapped in a loader\n    \n    Returns: accuracy as a float value between 0 and 1\n    \"\"\"\n    #model.eval()\n    valloss = 0\n    \n    with torch.no_grad():\n\n        for i_step, (data, target) in enumerate(loader):\n            \n            data = data.to(device)\n            target = target.to(device)\n            #prediction = model(x_gpu)\n            \n            outputs = model(data)\n           # print(\"val_output:\", outputs.shape)\n\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            valloss += picloss\n\n        #print(\"Threshold:  \" + str(threshold) + \"  Validation DICE score:\", valloss \/ i_step)\n\n    return valloss \/ i_step","41016762":"# Optimizers\nunet_optimizer = torch.optim.Adamax(unet.parameters(), lr=1e-3)\nfpn_optimizer = torch.optim.Adamax(fpn.parameters(), lr=1e-3)\nrx50_optimizer = torch.optim.Adam(rx50.parameters(), lr=5e-4)\n\n# lr_scheduler\ndef warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) \/ warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)","639efcd4":"%%time\nnum_ep = 10                                                                                                  \n# Train UNet\n#unet_lh, unet_th, unet_vh = train_model(\"Vanila_UNet\", unet, train_dataloader, val_dataloader, bce_dice_loss, unet_optimizer, False, 20) \n\n# Train FPN\n#fpn_lh, fpn_th, fpn_vh = train_model(\"FPN\", fpn, train_dataloader, val_dataloader, bce_dice_loss, fpn_optimizer, False, 20)#\n\n# Train ResNeXt50\nrx50_lh, rx50_th, rx50_vh = train_model(\"ResNeXt50\", rx50, train_dataloader, val_dataloader, bce_dice_loss, rx50_optimizer, False, num_ep)","ee500577":"def plot_model_history(model_name,\n                        train_history, val_history, \n                        num_epochs):\n    \n    x = np.arange(num_epochs)\n\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n\n    plt.title(f\"{model_name}\", fontsize=15)\n    plt.legend(fontsize=12)\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"DICE\", fontsize=15)\n\n    fn = str(int(time.time())) + \".png\"\n    plt.show()\n    #plt.savefig(fn, bbox_inches='tight', pad_inches=0.2)\n    #plt.close()\n    ","3717e4b9":"#plot_model_history(\"Vanilla UNet\", unet_th, unet_vh, 20)\n#plot_model_history(\"FPN\", fpn_th, fpn_vh, 20)\nplot_model_history(\"UNet with ResNeXt50 backbone\", rx50_th, rx50_vh, num_ep)","de25f6cc":"#test_iou = compute_iou(unet, test_dataloader)\n#print(f\"\"\"Vanilla UNet\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\n#test_iou = compute_iou(fpn, test_dataloader)\n#print(f\"\"\"FPN\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\ntest_iou = compute_iou(rx50, test_dataloader)\nprint(f\"\"\"ResNext50\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")","294f5f6d":"# image\ntest_sample = test_df[test_df[\"diagnosis\"] == 1].sample(2).values[0] #sample(-)\nimage = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n\n#mask\nmask = cv2.resize(cv2.imread(test_sample[2]), (128, 128))\n\n# pred\npred = torch.tensor(image.astype(np.float32) \/ 255.).unsqueeze(0).permute(0,3,1,2)\npred = rx50(pred.to(device))\npred = pred.detach().cpu().numpy()[0,0,:,:]\n\n# pred with tshd\npred_t = np.copy(pred)\npred_t[np.nonzero(pred_t < 0.3)] = 0.0\npred_t[np.nonzero(pred_t >= 0.3)] = 255.#1.0\npred_t = pred_t.astype(\"uint8\")\n\n# plot\nfig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n\nax[0, 0].imshow(image)\nax[0, 0].set_title(\"image\")\nax[0, 1].imshow(mask)\nax[0, 1].set_title(\"mask\")\nax[1, 0].imshow(pred)\nax[1, 0].set_title(\"prediction\")\nax[1, 1].imshow(pred_t)\nax[1, 1].set_title(\"prediction with threshold\")\nplt.show()","3272c441":"# Test Prediction","89ec3fbc":"## Unet with ResNeXt50 backbone.","042f022a":"# Data","73eabbd0":"Samples of images and masks with a positive diagnosis","d0b67052":"## Train history ","ed3254ad":"Global Variable","a8ef2f5c":"Distribution of positive\/negative diagnosis","2660402e":"DataGenerator","3bd41c13":"transforms","132d56fb":"### Test IoU","584b69ec":"# Segmentation Quality Metric","902cd37c":"Augmentation Visualization","900b55fd":"## Data Visualization","00c44f08":"Samples of images with a positive and negative diagnosis","12a21f88":"# Train Models","2e4ce0ae":"## Data Distribution","f64b8cdc":"# DataGenerator and Data Augmentation","6833d740":"### random test sample","e77da982":"## Creating a DataFrame","05abf439":"Final dataframe","3292f435":"Distribution of positive\/negative diagnosis between each patient","8ad74b01":"Split data on train val test"}}