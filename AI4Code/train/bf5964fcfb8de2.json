{"cell_type":{"706381e9":"code","6af67f1e":"code","9805491a":"code","a521b7bd":"code","a5cb4123":"code","f018e4fe":"code","f37f1f56":"code","87f1acb2":"code","994b9dc2":"code","c5c9e601":"code","1ea1edc6":"code","1b15d471":"code","e0ed3101":"code","240666e2":"code","cebfaca5":"code","da9c20b6":"code","48479704":"code","7c1e881a":"code","2e984b38":"code","a9ff9d99":"code","5bd61ec7":"code","5d8606a0":"code","94578a5e":"code","f3c23ab3":"code","2dd16036":"code","68600753":"code","3a0e35d5":"code","fb79fb1e":"code","16501ddb":"code","217932c1":"code","26d8fc1a":"code","ae93fc57":"code","a88dfe0c":"code","f92c24bb":"code","df2d2d68":"code","678cd367":"code","48648007":"code","63d04373":"code","8eec314f":"code","ff2f20a5":"code","b9a80f78":"code","8878df21":"code","2ce56c18":"code","20915aeb":"code","af484201":"code","b6af319a":"code","80ba5678":"code","26af3c64":"code","4ef0ecec":"code","d42e3127":"code","ea297ea4":"code","17521839":"code","f721dd9d":"code","91e6956e":"code","bcdf1a29":"code","0b7caa1e":"markdown","df1f1af8":"markdown","f9c836c3":"markdown","09d92d49":"markdown","79cf3345":"markdown","3c7e012a":"markdown","d3573fa0":"markdown","a4116325":"markdown","d7e889a6":"markdown","cdb35fc8":"markdown","27f01958":"markdown","8fe36273":"markdown","64d34d78":"markdown","e6452582":"markdown","a488ee40":"markdown","367beb97":"markdown","511bf199":"markdown","92ea4008":"markdown","025786dd":"markdown","c571710f":"markdown","b21c9064":"markdown","09e21a26":"markdown","a82f980b":"markdown","3d21d1e3":"markdown","54809d84":"markdown","e4b4253e":"markdown","d2b2317c":"markdown","3761b932":"markdown","00589cbd":"markdown","8a2c37d4":"markdown","9cbb602c":"markdown","6a2c5b43":"markdown","fafe9f37":"markdown","58a5d2f9":"markdown","4783362b":"markdown","bc8c8136":"markdown","45bf4907":"markdown","9dfed514":"markdown"},"source":{"706381e9":"import numpy as np \nimport pandas as pd\nimport os\nimport re\nimport scipy.stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\nSEED = 7\n\nprint(\"Setup complete.\")","6af67f1e":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndatasets = [train, test]\n\ntrain","9805491a":"train.info()","a521b7bd":"test.info()","a5cb4123":"sns.countplot(data=train, x='Pclass', hue='Survived')\nplt.show() # To remove extra message shown on screen","f018e4fe":"def get_title(name):\n    reg_exp = re.search(' ([A-Za-z]+)\\.', name)\n    return reg_exp.group(1) if reg_exp else \"\"\n\n# Number of different titles\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n\nsns.countplot(data=train, y=train['Name'].apply(get_title), ax=ax[0])\nax[0].set(title='Train', ylabel='')\n\nsns.countplot(data=test, y=test['Name'].apply(get_title), ax=ax[1])\nax[1].set(title='Test', ylabel='')\nplt.show()","f37f1f56":"for ds in datasets:\n    # Add Title column\n    ds['Title'] = ds['Name'].apply(get_title)\n    # Replacement\n    ds['Title'] = \\\n        ds['Title'].replace(['Mlle', 'Ms'], 'Miss').replace('Mme', 'Mrs')\n    rare = ['Capt', 'Col', 'Countess', 'Don', 'Dona', 'Dr', 'Jonkheer', \n            'Lady', 'Major', 'Rev', 'Sir']\n    ds['Title'] = ds['Title'].replace(rare, 'Rare')\n\n# Number of different titles\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n\nsns.countplot(data=train, y='Title', ax=ax[0])\nax[0].set(title='Train', ylabel='')\n\nsns.countplot(data=test, y='Title', ax=ax[1])\nax[1].set(title='Test', ylabel='')\nplt.show()","87f1acb2":"sns.countplot(data=train, x='Sex', hue='Survived')\nplt.show()","994b9dc2":"for ds in datasets:\n    # Imputate (fill missing values)\n    def rand_ages():\n        np.random.seed(SEED)\n        return np.random.randint(low=ds['Age'].mean() - ds['Age'].std(),\n                                 high=ds['Age'].mean() + ds['Age'].std(),\n                                 size=ds['Age'].isnull().sum())\n    ds.loc[ds['Age'].isnull(), 'Age'] = rand_ages()\n\n    # Categorise\n    ds['Age'] = pd.cut(\n        ds['Age'], bins=[-np.inf, 14, 24, 64, np.inf], labels=range(4))\n    ds.loc[:, 'Age'] = ds['Age'].astype(int)\n    \n# Plot\nax = sns.countplot(data=train, x='Age', hue='Survived')\nax.set_xlabel('Age category')\nplt.show()","c5c9e601":"for ds in datasets:\n    # Family size is based on number of siblings\/spouses and parent\/children\n    family_size = ds['SibSp'] + ds['Parch'] + 1\n    \n    # Categorise\n    ds['FamilySize'] = pd.cut(\n        family_size, bins=[-np.inf, 1, 2, 4, np.inf], labels=range(4))\n    ds.loc[:, 'FamilySize'] = ds['FamilySize'].astype(int)\n\n# Plot\nsns.countplot(data=train, x='FamilySize', hue='Survived')\nplt.show()","1ea1edc6":"median = train['Fare'].median()\nfor ds in datasets:\n    #Impute\n    ds['Fare'] = ds['Fare'].fillna(median)\n    \n    # Categorise\n    ds['Fare'] = pd.qcut(ds['Fare'], q=4, labels=range(4))\n    ds.loc[:, 'Fare'] = ds['Fare'].astype(int)\n\n# Plot\nsns.countplot(data=train, x='Fare', hue='Survived')\nplt.show()","1b15d471":"for ds in datasets:\n    ds['HasCabin'] = ds['Cabin'].notnull().astype(int)\n\n# Plot\nsns.countplot(data=train, x='HasCabin', hue='Survived')\nplt.show()","e0ed3101":"# Count of values\nfig, ax = plt.subplots(1, 2, sharey=True)\n\nsns.countplot(data=train, x='Embarked', ax=ax[0])\nax[0].set_title('Train')\n\nsns.countplot(data=test, x='Embarked', ax=ax[1], order=['S', 'C', 'Q'])\nax[1].set_title('Test')\nplt.show()","240666e2":"# Impute\nfor ds in datasets:\n    ds['Embarked'] = ds['Embarked'].fillna('S')\n\n# Plot\nsns.countplot(data=train, x='Embarked', hue='Survived')\nplt.show()","cebfaca5":"train.head()","da9c20b6":"train.shape","48479704":"# Encoding\ndef encode_freq_sorted(feature):\n    sorted_indices = feature.value_counts().index\n    sorted_dict = dict(zip(sorted_indices, range(len(sorted_indices))))\n    return feature.map(sorted_dict).astype(int)\n    \nfor ds in datasets:\n    ds['Sex'] = encode_freq_sorted(ds['Sex']) # Sex\n    ds['Embarked'] = encode_freq_sorted(ds['Embarked']) # Embarked\n    ds['Title'] = encode_freq_sorted(ds['Title']) # Title\n\ntrain.head()","7c1e881a":"drop_features = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']","2e984b38":"correlation = train.drop(columns=drop_features).corr()\n\nplt.figure(figsize=(11, 11))\nsns.heatmap(correlation, annot=True)\nplt.show()","a9ff9d99":"drop_features.extend(['Pclass'])\n\ntrain = train.drop(columns=drop_features)\ntest = test.drop(columns=drop_features)\n\nX = train.drop(columns=['PassengerId', 'Survived'])\ny = train['Survived']\n\nX.head()","5bd61ec7":"# Split training and validation data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.75, \n                                                  random_state=SEED)\nX_test = test.drop(columns=['PassengerId']) # For consistency in naming variables","5d8606a0":"# Cross-validator\nfrom sklearn.model_selection import StratifiedKFold\n\ncross_valid = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n\n# Define randomized search as a function for later use\ndef random_search(X, y, estimator, params, score=\"accuracy\", cv=cross_valid, \n                  n_iter=100, random_state=SEED, n_jobs=-1):\n    \"\"\"\n    Randomized search of parameters, using \"cv\" fold cross validation, search \n    across \"n_iter\" different combinations, and use all available cores\n    \"\"\"\n    print(\"# Tuning hyper-parameters for {} by randomized search\".format(score))\n    \n    classifier = RandomizedSearchCV(estimator=estimator, param_distributions=params, \n                             scoring=score, cv=cv, n_iter=n_iter, n_jobs=n_jobs, \n                             random_state=random_state)\n    classifier.fit(X, y)\n    \n    print(\"Best parameters by random search:\\n\", classifier.best_params_)\n    return classifier","94578a5e":"from sklearn.ensemble import RandomForestClassifier\n\n# By setting random_state, we get the same result every time we run the command\nrandom_forest = RandomForestClassifier(random_state=SEED)\n\nrandom_forest.get_params()","f3c23ab3":"# Hyper-parameter tuning\n# Note: running this cell takes a few minutes\n\n# Create the parameter grid\n# The keys are ordered alphabetically\nparams = {\n    # Method of selecting samples for training each tree\n    'bootstrap': [True, False],\n    # Maximum number of levels in tree\n    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n    # Number of features to consider at every split\n    'max_features': ['auto', 'sqrt'],\n    # Minimum number of samples required at each leaf node\n    'min_samples_leaf': [1, 2, 4],\n    # Minimum number of samples required to split a node\n    'min_samples_split': [2, 5, 10],\n    # Number of trees in random forest\n    'n_estimators': [int(x) for x in np.linspace(200, 2000, num = 10)]\n}\n\n# Apply randomized search cross validation\nrandom_forest_tuned = random_search(\n    X_train, y_train, estimator=random_forest, params=params)","2dd16036":"# Define a function to plot learning curve for later use\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X, y, ylim=None, cv=None, n_jobs=-1,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plot the test and training learning curves.\n    \"\"\"\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, scoring='accuracy', cv=cv, n_jobs=n_jobs, \n                       train_sizes=train_sizes, return_times=True)\n    \n    # Plot learning curve\n    fig, ax = plt.subplots()\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", \n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.grid()\n    plt.legend(loc=\"best\")\n    if ylim is not None:\n        plt.ylim(*ylim)\n    ax.set(title=\"Learning curves\", \n           xlabel=\"Training examples\", ylabel=\"Score\")\n \n    return plt","68600753":"plot_learning_curve(random_forest_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","3a0e35d5":"# Prediction on validation data\ny_pred = random_forest_tuned.predict(X_val)\n\n# Accuracy of prediction\naccuracy_random_forest = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_random_forest)","fb79fb1e":"# Default params of an svm\nfrom sklearn.svm import SVC\n\nsvc = SVC(probability=True, random_state=SEED)\n\nsvc.get_params()","16501ddb":"# Hyper-parameter tuning\nparams = {\n    'C': scipy.stats.expon(scale=78),\n    'class_weight':['balanced', None],\n    'gamma': scipy.stats.expon(scale=.1),\n    'kernel':['rbf', 'linear']\n}\n\nsvc_tuned = random_search(X_train, y_train, estimator=svc, params=params)","217932c1":"# Prediction on vlaidation data\ny_pred = svc_tuned.predict(X_val)\n\n# Accuracy\naccuracy_svc = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_svc)","26d8fc1a":"plot_learning_curve(svc_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","ae93fc57":"# Default params of an svm\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state=SEED, verbosity=0)\n\nxgb.get_params()","a88dfe0c":"# Hyper-parameter tuning\nparams = {\n    'colsample_bytree': list(np.arange(0.6, 1.0, step=0.05)),\n    'gamma': list(np.arange(0.1, 15, step=0.2)),\n    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.21],\n    'max_depth': list(range(2, 12)),\n    'min_child_weight': list(range(1, 12)),\n    'n_estimators': [10, 100, 500, 1000],\n    'reg_alpha': [10**i for i in range(-5, 1)],\n    'subsample': list(np.arange(0.6, 1.0, step=0.05))\n}\n\nxgb_tuned = random_search(X_train, y_train, estimator=xgb, params=params)","f92c24bb":"# Prediction on vlaidation data\ny_pred = xgb_tuned.predict(X_val)\n\n# Accuracy\naccuracy_xgboost = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_xgboost)","df2d2d68":"plot_learning_curve(xgb_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","678cd367":"# Default params of an svm\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=SEED)\n\ndecision_tree.get_params()","48648007":"# Hyper-parameter tuning\nparams = {\n    'criterion': [\"gini\", \"entropy\"],\n    'max_depth': list(range(1, 32)),\n    'max_features': list(range(1, X_train.shape[1]+1)),\n    'min_samples_leaf': list(range(1, 9)),\n    'min_samples_split': list(np.arange(0.1, 1.1, step=0.1))\n}\n\ndecision_tree_tuned = random_search(\n    X_train, y_train, estimator=decision_tree, params=params)","63d04373":"# Prediction on vlaidation data\ny_pred = decision_tree_tuned.predict(X_val)\n\n# Accuracy\naccuracy_decision_tree = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_decision_tree)","8eec314f":"plot_learning_curve(decision_tree_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","ff2f20a5":"# Default params\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\nknn.get_params()","b9a80f78":"# Hyper-parameter tuning\n# Note: running this cell takes a few minutes\nparams = {\n    'leaf_size': list(range(20, 60)),\n    'n_neighbors': list(range(3, 30)),\n    'p': [1, 2]\n}\n\nknn_tuned = random_search(X_train, y_train, estimator=knn, params=params)","8878df21":"# Prediction on vlaidation data\ny_pred = knn_tuned.predict(X_val)\n\n# Accuracy\naccuracy_knn = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_knn)","2ce56c18":"plot_learning_curve(knn_tuned.best_estimator_, X, y, cv=cross_valid)\nplt.show()","20915aeb":"# Default params\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression(random_state=SEED)\n\nlogistic_regression.get_params()","af484201":"# Hyper-parameter tuning\nparams = {\n    'C': scipy.stats.loguniform(1e-4, 100),\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n}\n\nlogistic_regression_tuned = random_search(\n    X_train, y_train, estimator=logistic_regression, params=params)","b6af319a":"# Prediction on vlaidation data\ny_pred = logistic_regression_tuned.predict(X_val)\n\n# Accuracy\naccuracy_logistic_regression = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_logistic_regression)","80ba5678":"plot_learning_curve(logistic_regression_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","26af3c64":"# Default params\nfrom sklearn.naive_bayes import GaussianNB\n\nnaive_bayes = GaussianNB()\n\nnaive_bayes.get_params()","4ef0ecec":"# Hyper-parameter tuning\nparams = {\n    'var_smoothing': [np.exp(-i) for i in range(1, 15)]\n}\n\nnaive_bayes_tuned = random_search(\n    X_train, y_train, estimator=naive_bayes, params=params, n_iter=15-1)","d42e3127":"# Prediction on vlaidation data\ny_pred = naive_bayes_tuned.predict(X_val)\n\n# Accuracy\naccuracy_naive_bayes = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_naive_bayes)","ea297ea4":"plot_learning_curve(naive_bayes_tuned.best_estimator_, \n                    X, y, cv=cross_valid)\nplt.show()","17521839":"accuracy_df = pd.DataFrame({\n    'Model': ['Random forest', 'SVM', 'XGBoost', 'Decision Tree', 'KNN', \n              'Logistic regression', 'Naive Bayes'],\n    'Accuracy': [accuracy_random_forest, accuracy_svc, accuracy_xgboost,\n                 accuracy_decision_tree, accuracy_knn,\n                 accuracy_logistic_regression, accuracy_naive_bayes]\n})\n\n# Sort\naccuracy_df_sorted = accuracy_df.sort_values(by='Accuracy', ascending=False)\n\n# Plot\ng = sns.barplot(data=accuracy_df_sorted, y='Model', x='Accuracy')\ng.set(ylabel='', xlim=(0.7, 0.8))\nplt.show()","f721dd9d":"from sklearn.ensemble import VotingClassifier\n\nvoting = VotingClassifier(\n    estimators=[('rf', random_forest_tuned),\n                ('xgb', xgb_tuned),\n                ('knn', knn_tuned),\n                ('svc', svc_tuned),\n                ('lr', logistic_regression_tuned),\n                ('dt', decision_tree_tuned),\n                ('nb', naive_bayes_tuned)],\n    voting='soft', \n    n_jobs=-1)\n\nvoting = voting.fit(X_train, y_train)","91e6956e":"# Prediction on vlaidation data\ny_pred = voting.predict(X_val)\n\n# Accuracy\naccuracy_voting = accuracy_score(y_val, y_pred)\nprint(\"Accuracy:\", accuracy_voting)","bcdf1a29":"# The best model\nmodel = voting\n\n# Prediction\npredictions = model.predict(X_test)\n\n# Save the results\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"The results successfully saved!\")","0b7caa1e":"Here, we encode features based on frequency of occurance of their values, i.e. the most frequent value in each feature is encoded to 0, the second most frequent to 1 and so on.","df1f1af8":"Some titles can be replaced by another one, e.g. 'Mlle'=Mademoiselle (French) with 'Miss'. Also, most titles are rare, e.g. 'Col' that is used only for 2 passengers, so it'd be helpful to replace them with 'Rare'.","f9c836c3":"## Support vector machine (SVM)\nA support vector machine is a supervised machine learning model that uses classification algorithms for two-group classification problems.","09d92d49":"## Random forest\nRandom forest is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean\/average prediction of the individual trees [[link](https:\/\/en.wikipedia.org\/wiki\/Random_forest#:~:text=Random%20forests%20or%20random%20decision,average%20prediction%20(regression)%20of%20the)].\n\nFirst, we define the model in the simplest form to see what the default parameters are.\n\nThe material is inspired by [Hyperparameter Tuning the Random Forest in Python](http:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74).","79cf3345":"## K-Nearest Neighbors\nThe k-nearest neighbors algorithm (k-NN) is a non-parametric method used that assumes similar things exist in close proximity.","3c7e012a":"### 7. Parch\nThis is the number of parents\/children. We can consider it along with \"SibSp\" to make a new feature, \"FamilySize\", that can be categorized into \n 4 categories comprising 1, 2, {3, 4} and >= 5 members.","d3573fa0":"### 6. SibSp\nThis is the number of siblings\/spouses aboard the Titanic and can be considered along with \"Parch\" feature.","a4116325":"Note that if we didn't care about the frequency of values, we could use \"sklearn.preprocessing.LabelEncoder\". At the moment, that library doesn't support arbitrary ordering of values of the features.","d7e889a6":"## Naive Bayes\nNaive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features [[link](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)].","cdb35fc8":"# Results\nThe final results are saved in \"my_submission.csv\".","27f01958":"### Learning curve\n","8fe36273":"It's seen that the more the fare, the more the number of survived passengers.","64d34d78":"## Conclusion\nThe following is the result of running different models on the Titanic dataset.","e6452582":"Now that the data cleaning, feature engineering, data encoding and feature selection is done, the dataset is up and ready to be fed to machine learning models.\n\n# Classification\nHere we apply different machine learning techniques to model the training data and later predicting who survived the shipwreck.\n\nWe don't have access to the actual survived values for the test data, since this the competition's rule, until competitors don't overfit the test data. Therefore, we split the training data into training and validation, and use validation data to evaluate our model. ","a488ee40":"### 9. Fare\nTo impute, we use median of training data. Then, we categorise it into 4 categories.","367beb97":"As can be seen, there is a (rather) strong correlation between \"HasCabin\" and \"Pclass\", because the values are close to -1. We drop one of them to remove redundancy.\n\nThe values of \"HasCabin\" are closer to 0 for different features, which means it is less correlated to other features rather than \"Pclass\"; so, we drop \"Pclass\".","511bf199":"### 5. Age\nIn this column, there is plenty of missing values. To impute, we can use random numbers in the range of mean - std ($\\mu - \\sigma$) and mean + std ($\\mu + \\sigma$). Statistically speaking, 68% of data is in this range.\n\nThen, we categorise \"Age\" into 4 categories of <= 14 years (children), (14, 24] years (youth), (24, 64] years (adults) and > 64 years (seniors).","92ea4008":"# Introduction\nThis notebook explores the Titanic dataset, using Python language, with the aim of predicting which passengers survived the shipwreck.\n\nThe material is inspired by [Titanic best working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\/comments) and [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python).","025786dd":"Here, we check each and every feature.\n\nRunning the following commands, we see there are 12 columns in the training data, 3 of which having NaN -- \"Age\", \"Cabin\" and \"Embarked\". Also, the 3 columns of \"Age\", \"Fare\" and \"Cabin\" in the test dataset include missing values. We'll take care of these columns later.","c571710f":"The features are listed as above.\n\n### 0. PassengerId\nThis column includes IDs and doesn't affrect the survival.","b21c9064":"# Feature selection\nAt this step, we drop the features that wouldn't affect (much) the target.","09e21a26":"## Ensemble modelling\n","a82f980b":"### 11. Embarked\nThis column has missing values. We impute them by the most frequent value.","3d21d1e3":"# Data encoding\nLooking into the datasets, there are categorical features, that need to be converted to numberical ones in order for modelling. This is called \"encoding\".","54809d84":"Now, we try different models, for which we use randomized search to optimize hyper-parameters based on prediction accuracy. The randomized search randomly samples from a grid of paramaters and performs K-fold cross validation with each combination of values.","e4b4253e":"## Decision Tree\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes) [[link](https:\/\/en.wikipedia.org\/wiki\/Decision_tree#:~:text=A%20decision%20tree%20is%20a%20flowchart%2Dlike%20structure%20in%20which,taken%20after%20computing%20all%20attributes)].","d2b2317c":"### 3. Name\nEach name in the dataset is associated with a title. Instead of 'Name' that is observed once, probably, for each passenger, we can use a common feature, title, to group passengers.","3761b932":"## XGBoost\nXGBoost is an open-source software library that provides a gradient boosting framework.","00589cbd":"### 2. Pclass\n\nThis is the ticket class, with the numeric values 1=1st, 2=2nd, 3=3rd and no NaN.","8a2c37d4":"### 4. Sex\nThere is no NaN in this column.","9cbb602c":"### 8. Ticket\nThe ticket number doesn't affect the survival.","6a2c5b43":"We see that \"S\" is the most frequent port of embarkation.","fafe9f37":"## Logistic regression\nThe logistic model is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc [[link](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)].\n\nThe material is borrowed from [here](https:\/\/machinelearningmastery.com\/hyperparameter-optimization-with-random-search-and-grid-search\/).","58a5d2f9":"### 10. Cabin\nMost values of this column are NaN, meaning that most passengers didn't have cabins. We add a feature to show if a passenger had cabin or not.","4783362b":"Now that the best parameters are found, the model can be tested on validation data.","bc8c8136":"# Feature engineering","45bf4907":"### 1. Survived\nThis is the target column.","9dfed514":"This is because we used \n* \"Name\" -> \"Title\", \n* \"SibSp\" and \"Parch\" -> \"FamilySize\", \n* \"Ticket\" -> drop,\n* \"Cabin\" -> \"HasCabin\".\n\nTo explore the data, we can visualize the correlation between features. The correlation coefficient shows the linear relation between features. Strong correlation between two features, which is shown by values close to +1 or -1, means there is redundancy in data, therefore one of the features can be dropped."}}