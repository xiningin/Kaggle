{"cell_type":{"f9aefd11":"code","54572e25":"code","094d423e":"code","ee0a5c7e":"code","a41a5d0b":"code","1b065fde":"code","3cdb593c":"code","f5fc58bc":"code","884e5d2a":"code","280138f7":"code","e66f09e3":"code","6e3fcdae":"code","f973acef":"code","06989fc5":"code","69b5a6bd":"code","90288adf":"code","91dc7eda":"code","0d025e63":"code","d39f4a3f":"code","cf15fb31":"code","b79b4ce8":"code","f6d71fb3":"code","9933ccec":"code","222f7d23":"code","bb3a784c":"code","fd8e5fd7":"code","9a4f6dfb":"code","44bb8894":"code","0da9e7ea":"code","eda840c6":"code","9a10da32":"code","4264811d":"code","01284fa3":"markdown","5cc4cddf":"markdown"},"source":{"f9aefd11":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom matplotlib import pyplot as plt\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import FasterRCNN\nfrom albumentations.pytorch.transforms import ToTensor\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport albumentations as A\nimport torch.nn as nn","54572e25":"df = pd.read_csv('\/kaggle\/input\/face-mask-detection-dataset\/train.csv')\ndf.shape","094d423e":"len(df.classname.value_counts())","ee0a5c7e":"df.head()","a41a5d0b":"#columns are ulta-pulta :p\ndf.rename(columns = {'x2' : 'y1', 'y1' : 'x2'}, inplace = True)\ndf.head()","1b065fde":"# total 20 classes 1 to 20 labels :) ...\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nle.fit(df['classname'])\ndf['classname']=(le.transform(df['classname'])+1)\ndf.head()","3cdb593c":"df.isnull().sum()","f5fc58bc":"image_ids = df['name'].unique()\nimage_ids.sort()\nvalid_ids=image_ids[:0] \nprint(valid_ids)\n#full train :) ... \ntrain_ids=image_ids[:]","884e5d2a":"valid_df = df[df['name'].isin(valid_ids)]\ntrain_df = df[df['name'].isin(train_ids)]","280138f7":"#see what you did - no validation data - all training data\nvalid_df.shape, train_df.shape","e66f09e3":"df.classname.unique()\n#all those 20 classes ","6e3fcdae":"#https:\/\/stanford.edu\/~shervine\/blog\/pytorch-how-to-generate-data-parallel\n#refer above link to prepare dataset\nclass prepare_data(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = torch.as_tensor(records[['x1', 'y1', 'x2', 'y2']].values, dtype=torch.float32)\n        # there are 21 classes\n        labels = torch.as_tensor(records.classname.values,dtype=torch.int64)\n        \n\n        keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0]) ## To Handle NAN LOSS Cases \n        boxes = boxes[keep]\n        labels = labels[keep]\n\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        # target['area'] = area\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n","f973acef":"def get_train_transform():\n    return A.Compose([\n        ToTensor()\n    ])\n        \n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensor()\n    ])","06989fc5":"## pytorch Faster-RCNN Resnt50 Pretrained Model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","69b5a6bd":"num_classes = 21  # 20 class (masks) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","90288adf":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = prepare_data(train_df,get_train_transform())\nvalid_dataset = prepare_data(valid_df, get_valid_transform()) \n\nprint(type(train_dataset))\nprint(type(train_df))\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n#     num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=collate_fn\n)\n","91dc7eda":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","0d025e63":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","d39f4a3f":"boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()","cf15fb31":"model.to(device)\n\n#Retriving all trainable parameters from model (for optimizer)\nparams = [p for p in model.parameters() if p.requires_grad]\nprint(params)\n#Defininig Optimizer\noptimizer = torch.optim.Adam(params, lr = 0.0001)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.00017, div_factor=2 ,steps_per_epoch=len(train_data_loader), epochs=5)\n\nnum_epochs = 5","b79b4ce8":"loss_list = []\n\nfor epoch in range(num_epochs):\n    \n    z=tqdm(train_data_loader)\n    print(z)\n\n    for itr,(images, targets, image_ids) in enumerate(z):\n        torch.cuda.empty_cache()\n        \n        images = list(image.to(device).float() for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        ## Returns losses and detections \n        # refer to this link - https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html\n        loss_dict = model(images, targets)\n        print(loss_dict, end=\" \")\n        print(len(loss_dict), end=\" \")\n\n        losses = sum(loss for loss in loss_dict.values())\n        #losses.item() is basically losses[0]...\n        #refer this link \"https:\/\/github.com\/pytorch\/tnt\/issues\/108\"\n        loss_value = losses.item()\n\n        loss_list.append(loss_value)\n        z.set_description(f'Epoch {epoch+1}\/{num_epochs}, LR: %6f, Loss: %.6f'%(optimizer.state_dict()['param_groups'][0]['lr'],loss_value))\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        scheduler.step() ## Since We are using 1-Cycle LR Policy, LR update step has to be taken after every batch\n\n\n    print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n#     torch.save(model.state_dict(), f'\/content\/drive\/My Drive\/internshala round 1\/model-epoch{epoch+1}.pth') \n    print()\n    print('Saving Model.......')\n    print()","f6d71fb3":"class MaskTestDataset(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","9933ccec":"def get_test_transform():\n    return A.Compose([\n        ToTensor()\n    ])","222f7d23":"test_df=pd.read_csv('..\/input\/face-mask-detection-dataset\/train.csv')\ntest_df.head()","bb3a784c":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = MaskTestDataset(test_df[:40], get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","fd8e5fd7":"torch.cuda.empty_cache()","9a4f6dfb":"%%time\n\ndetection_threshold = 0.60\nresults = []\nmodel.eval()\nfor images, image_ids in test_data_loader:\n    torch.cuda.empty_cache()\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        labels = outputs[i]['labels'].data.cpu().numpy()\n\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        \n        result = {\n            'image_id': image_id,\n            'labels': labels,\n            'scores': scores,\n            'boxes': boxes\n        }\n\n        \n        results.append(result)","44bb8894":"## Using Dictionary is Fastest Way to Create SUBMISSION DATASET.\nnew=pd.DataFrame(columns=['image_id', 'boxes', 'label'])\nrows=[]\nfor j in range(len(results)):\n    for i in range(len(results[j]['boxes'])):\n        dict1 = {}\n        dict1={\"image_id\" : results[j]['image_id'],\n                  'x1': results[j]['boxes'][i,0],\n                  'x2': results[j]['boxes'][i,2],\n                  'y1': results[j]['boxes'][i,1],\n                  'y2': results[j]['boxes'][i,3],\n                  'classname':results[j]['labels'][i].item()}\n        rows.append(dict1)\n\n","0da9e7ea":"sub=pd.DataFrame(rows)\nsub['classname']=le.inverse_transform(sub.classname.values - 1) ## Converting Back Labels To Original Names ","eda840c6":"sub.head()","9a10da32":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\nboxes = boxes[scores >= 0.6].astype(np.int32)","4264811d":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","01284fa3":"**Adam**","5cc4cddf":"## TRAINING"}}