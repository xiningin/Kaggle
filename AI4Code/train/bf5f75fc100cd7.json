{"cell_type":{"b7b2b4c2":"code","e2fd4cd4":"code","2b3155b5":"code","8f043399":"code","01b8e47c":"code","4777ff51":"code","abb6acc7":"code","136456c5":"code","b63c37f1":"code","8ce28ec5":"code","9b78c52b":"code","a5312ba7":"code","139f3e51":"code","2e7ed386":"code","62a30c5d":"code","36ec9447":"code","8fd44352":"code","30e0ec6b":"code","84820f94":"code","d8e3a124":"code","7f378e3b":"code","f3ca931f":"code","66e7ac2f":"code","c9264579":"code","227bb6fd":"code","e1c38b15":"code","c1bc81fc":"code","4c714c72":"code","1da86111":"code","d72e0c4c":"code","51c5c387":"code","1692d2ce":"code","691c36c6":"code","1578f40d":"code","785f7a62":"code","df6f762b":"code","daeb1e6b":"code","4a729e78":"code","fd281f5f":"code","ffacc941":"code","18271eb0":"code","a1875499":"code","4f204472":"markdown","16db6764":"markdown","3ed6289c":"markdown","f06764db":"markdown","34faa7f5":"markdown","c4787f29":"markdown","7de19d4e":"markdown","90a548f9":"markdown","793e5a04":"markdown","49d58a24":"markdown","820d5946":"markdown"},"source":{"b7b2b4c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2fd4cd4":"from zipfile import ZipFile \n  \nwith ZipFile('\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r') as embd_zip: \n    print(embd_zip.namelist())\n","2b3155b5":"def DEBUG_DICTIONARY(dct, limit=10):\n    for i, key in enumerate(dct.keys()):\n        if i > limit: break\n        print(key, dct[key])","8f043399":"from sklearn.model_selection import train_test_split\n\n# conguration des donnees d entrainement et de test\ntrain_data, val_data = train_test_split(pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv'), test_size=0.2, random_state=42)\nsentences, targets = train_data['question_text'], train_data['target']\nval_sentences, val_targets = val_data['question_text'], val_data['target']","01b8e47c":"train_data.head()","4777ff51":"targets.value_counts(), val_targets.value_counts()","abb6acc7":"# Pour chaque mot - compter combien de il occure dans les phrases\ndef configure_sentences(sentences, lower = True):\n    words = {}\n    for sentence in sentences:\n        for word in sentence.split():\n            if lower: word = word.lower()\n            words[word] = words.get(word, 0) + 1\n    return words\n\nwords = configure_sentences(sentences)\n#frequence des mot dans les phrase \nDEBUG_DICTIONARY(words)","136456c5":"# coconnaitre quelle type de mot est le plus utilisee\nDEBUG_DICTIONARY({word: cnt for word, cnt in sorted(words.items(), key=lambda item: item[1], reverse=True)})","b63c37f1":"# indexage des mot\ndef configure_words(words):\n    vocabulary = {}\n    for i, word in enumerate(words.keys()):\n        vocabulary[word] = i # vocabulary[i] = word\n    return vocabulary\n\nvocabulary = configure_words(words)\nDEBUG_DICTIONARY(vocabulary)","8ce28ec5":"import matplotlib.pyplot as plt\n%matplotlib inline","9b78c52b":"# retourne le minum le maximum le moyen la longueur et trace aussi l histograme de distrubission de longueur  \ndef configure_sentence_statistic(sentences):\n    def sentence_len(s):\n        return len(s.split())\n    \n    sentences.apply(sentence_len).plot(title='Sentence Length Distribution',y='Length Frequency',kind='hist', colormap='autumn', logy=True);\n    return np.min(sentences.apply(sentence_len)), np.round(np.mean(sentences.apply(sentence_len))), np.max(sentences.apply(sentence_len))\n\nmin, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","a5312ba7":"HIDDEN_SIZE = 30","139f3e51":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\n# Affiche comment target est distrubie \ndef configure_target_statistic(targets):\n    trg_cnt = targets.value_counts()\n    labels, sizes = (np.array(trg_cnt.index)), (np.array(100*(trg_cnt\/trg_cnt.sum())))\n    py.iplot(go.Figure(data=[go.Pie(labels=labels, values=sizes)], layout=go.Layout(title='Distrubition de target',font=dict(size=15),width=500, height=500)))\n    return trg_cnt\n\nconfigure_target_statistic(targets)","2e7ed386":"# filttrer les donnees suivant les parametre fourni\ndef filter_and_display_data(sentences, targets, target=0, min_len=5, max_len=30, limit=3):\n    result = []\n    for i, sentence in enumerate(sentences):\n        sent_len = len(sentence.split(' '))\n        if min_len <= sent_len and sent_len <= max_len:\n            if targets[i] == target:\n                result.append(sentence)\n                if len(result) >= limit: break\n    \n    if(len(result) ==- 0):\n        print('no such sequencies found.')\n        return\n    \n    print('{} {} sentences with length between {}-{}:\\n'.format(limit, 'GOOD' if target == 0 else 'BAD', min_len, max_len))\n    for i, s in enumerate(result):\n        print(str(i+1)+\")\",s)","62a30c5d":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","36ec9447":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","8fd44352":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0, min_len=120, max_len=140)","30e0ec6b":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=120, max_len=140)","84820f94":"# lit et retourne un dictionnaire cle: mot;ads and returns dictionary - key: word; valeur le vecteur attachee  (vec. length=300)\ndef confnigure_embeddings(embd_path):\n    word2vecs = {}\n    with ZipFile('\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip') as embd_zip:\n        for embd in embd_zip.open(embd_path, 'r'):\n            word2vec = embd.decode().split(' ')\n            word2vecs[word2vec[0]] = np.asarray(word2vec[1:], dtype='float32')\n    return word2vecs\n            \nword2vecs = confnigure_embeddings('glove.840B.300d\/glove.840B.300d.txt')\nDEBUG_DICTIONARY(word2vecs, limit=1)","d8e3a124":"#Dans chaque phrase remplace les mots par les  in each sentence replaces words with its own embedding vectors \ndef configure_word2vecs(sentences, word2vecs):\n    def configure_sentence(sentence, len=HIDDEN_SIZE):\n        return ([word2vecs.get(word.lower(), np.zeros(300)) for word in sentence.split()] + [np.zeros(300)]*len)[:len] \n    \n    return [configure_sentence(sentence) for sentence in sentences]\n\n# embedding_sentences = configure_word2vecs(sentences, word2vecs)\n# print(embedding_sentences[:10])","7f378e3b":"import torch\nimport torch.nn as nn\n\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd","f3ca931f":"BATCH_SIZE = 256\nBATCHES = (len(sentences)+BATCH_SIZE-1)\/\/BATCH_SIZE\n\nEPOCHS = 2 # gpu :(\nEMBD_SIZE = 300","66e7ac2f":"gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu, torch.cuda.is_available()","c9264579":"#la longue courte memoire est la plus en adequation pour ce cas comme notre donnes d entrainemrnt est plus 1M et nos vecteurs attachee sont\n#de longueur 300 si nous convertons notre toute notre data dans tensor une fois on a besin de 16 gb de de ram et beauccoup plus de ressources pour entrainer cette data\n\n# Aussi nous utilisons une couche lineaire car on a pas des dependences difficile -comme on a dans tous les qui contient bad une target de 1 le plus souvent \n\n\nclass LSTM(nn.Module):\n    def __init__(self, input_dim=1, emb_dim=EMBD_SIZE, hid_dim=HIDDEN_SIZE, n_layers=1, output_dim=1, dropout=0.3):\n        super().__init__()\n        self.hid_dim, self.n_layers = hid_dim, n_layers\n        \n        # nn's\n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n        self.linear = nn.Linear(hid_dim, output_dim)\n        \n       \n        \n        \n    def forward(self, src):\n        outputs, (hidden, cell) = self.lstm(src)\n        return self.linear(hidden.reshape(-1, self.hid_dim))\n","227bb6fd":"# craete model - with lstm and linear layers\nmodel = LSTM().to(gpu)\n\n# init loss function\nloss_function = nn.BCEWithLogitsLoss().to(gpu) #nn.MSELoss()\n\n# init optimizer with learning rate 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","e1c38b15":"model","c1bc81fc":"#evalue et retourne l accuracy pour Y predit par le modele\ndef acc_function(y_pred, y_test):\n    y_pred = torch.round(torch.sigmoid(y_pred).to(gpu)).to(gpu)\n    correct = (y_pred == y_test).sum().float()\n    return torch.round(100*(correct\/y_pred.shape[0]))\n\n# genere et retoune idx-ieme batch conmme torch tensor suivant les donneees fournies\ndef get_batch(sentences, targets, idx):\n    src = configure_word2vecs(sentences[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], word2vecs)\n    trg = np.asarray(targets[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], dtype='bool')\n    return torch.FloatTensor(src).to(gpu), torch.FloatTensor(trg).to(gpu)\n\n#evalue et retourne f1 pour y predis par le model\ndef f1_score(y_pred, y_test):\n    tp = (y_test * y_pred).sum().to(torch.float32)\n    tn = ((1 - y_test) * (1 - y_pred)).sum().to(torch.float32)\n    fp = ((1 - y_test) * y_pred).sum().to(torch.float32)\n    fn = (y_test * (1 - y_pred)).sum().to(torch.float32)\n    \n    epsilon = 1e-7 #pour feviter les crash\n    precision, recall = tp \/ (tp + fp + epsilon), tp \/ (tp + fn + epsilon)\n    \n    return 2*(precision*recall)\/(precision + recall + epsilon)\n    ","4c714c72":"# pret pour l entrainement\nmodel.train()\n\nVALIDATION_BATCHES = 10\n# validation du data pour la precision pendant l entrainement -mais prenant seulement VALIDATIONBATCHES pendant que toute la data est tres grande.\nval_sents = configure_word2vecs(val_sentences[:VALIDATION_BATCHES*BATCH_SIZE], word2vecs)\nval_targs = np.asarray(val_targets[:VALIDATION_BATCHES*BATCH_SIZE], dtype='bool')\n\nval_batch = torch.FloatTensor(val_sents).to(gpu)\nval_target = torch.FloatTensor(val_targs).to(gpu)\nprint(type(val_batch), val_batch.shape, type(val_targets), val_targets.shape)","1da86111":"BATCHES, BATCH_SIZE, get_batch(sentences, targets, 0)[0].shape, get_batch(sentences, targets, 0)[1].shape","d72e0c4c":"# entrainement \nfor e in range(EPOCHS):\n    # enregistre epoch loss et accuracy\n    epoch_loss, epoch_acc = 0, 0\n    for b in range(BATCHES):\n        # prend le batch courant de la data\n        X_batch, y_batch = get_batch(sentences, targets, b)\n        \n        # met le gradientsset a zero,avant de commencer la backpropagation - evitant la msnwue de direction pour le minumum .\n        optimizer.zero_grad()\n\n        #predit targets pour le bstch courant et apprend en la comparant avec la fonction loss.\n        y_pred = model(X_batch)\n        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n        \n        #predit target pour la data de validation et evalue la precision\n        val_pred = model(val_batch)\n        acc = acc_function(val_pred, val_target.unsqueeze(1))\n\n        #grandients sont enregistree par tensors-lors de l appel de backward sur la fonction loss.\n        loss.backward()\n        \n        #met a jour les parametres du modele\n        optimizer.step()\n        \n        #ajoute bash et accuracy pour evaluer epoch loss\/acc.\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        if b == 0 or (b+1) % 100 == 0:\n            print(f'Epoch {(e+1)+0:03} | Batch {(b+1)+0:04}: | Loss: {epoch_loss\/(b+1):.5f} | Acc: {epoch_acc\/(b+1):.3f} | F1: {f1_score(val_pred, val_target.unsqueeze(1)):.3f}')\n            \n    print(f'Epoch {(e+1)+0:03}: | Epoch Loss: {epoch_loss\/BATCHES:.5f} | Epoch Acc: {epoch_acc\/BATCHES:.3f}')","51c5c387":"#initialisation du data de test\ntest_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nsentences, targets = test_data['question_text'], []\nTEST_BATCHES = (len(sentences)+BATCH_SIZE-1)\/\/BATCH_SIZE","1692d2ce":"test_data.head()","691c36c6":"min, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","1578f40d":"len(sentences), len(targets), TEST_BATCHES","785f7a62":"model.eval()\nwith torch.no_grad():\n    for b in range(TEST_BATCHES):\n        #obtenir le bash courant\n        X_batch = torch.FloatTensor(configure_word2vecs(sentences[BATCH_SIZE*b:BATCH_SIZE*(b+1)], word2vecs)).to(gpu)\n        \n        # predit bach selon notre modele entraine\n        trg = torch.round(torch.sigmoid(model(X_batch))).cpu().numpy().squeeze()\n        targets.extend(trg)\n        \n        if b == 0 or (b+1) % 100 == 0: print(f'Batch {(b+1)+0:04} predicted')","df6f762b":"# enregistre les donnees a transmetre \ntest_targets = (np.array(targets) >= 0.5).astype(np.int)\n\nsubmit = pd.DataFrame({\"qid\": test_data['qid'], \"prediction\": test_targets})\nsubmit.to_csv(\"submission.csv\", index=False)","daeb1e6b":"# Affichage des resultats\nsubmit.head()","4a729e78":"configure_target_statistic(submit['prediction'])","fd281f5f":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","ffacc941":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","18271eb0":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","a1875499":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","4f204472":"Debut de travail sur le modele ","16db6764":"DEBUT DU TRAVAIL SUR LA DATA DU TEST","3ed6289c":"..s il n ya plus de longue sequence c est ok","f06764db":"PREDIT LA DATA DU TEST SUIVANT NOTRE MODELE","34faa7f5":"1. * **DEBUT DU TRAVAIL SUR LES DONNES**","c4787f29":"il parrait que notre modele marche bien","7de19d4e":"DEBUT D ENTRAINEMET DU MODELE","90a548f9":"Affichage des expemple de nos donnees","793e5a04":"comme il parrait de notre prediction il est comme les donnees d entrainement ,c est bon","49d58a24":"Pour une longueur entre 120 et 140 il parrait que nos donnees sont difficile a determiner target meme pour un humain et pour target\n=1 on n a pas d exemples  donc il est pas bon pour nos donnnees d avoir toute les types des exemples basic mais ca ne fait rien ","820d5946":"****voyons quelque exemple de nos prediction"}}