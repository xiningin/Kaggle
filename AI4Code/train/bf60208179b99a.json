{"cell_type":{"1e56e83d":"code","6b4ca16d":"code","6333f9cb":"code","e5ba38b1":"code","12de2a8d":"code","5e9090e1":"code","9e5bbbde":"code","457c48b8":"code","981b0a6a":"code","53b7ebf2":"code","12a083f2":"code","8329378c":"code","60dd5664":"code","2b9a99ef":"code","cb7f2b96":"code","cea671a7":"code","956b4d62":"code","1e918587":"code","d9b71728":"code","4effec98":"code","b67d52b2":"code","0607910c":"code","bdc11e35":"code","ad291d94":"code","7d3fcfa7":"code","fa400fab":"code","c1fe6bc6":"code","3e3a7605":"code","df38b0f9":"code","b95aaef9":"code","fb2c9e1a":"code","168e505a":"code","b346d760":"code","1384e997":"code","d7c45ba1":"code","7e5f79e8":"code","db651a33":"code","e20d045d":"code","7d520492":"code","cb126c01":"code","e18b8b24":"code","db2a5d44":"code","d0de85e1":"code","0498e8df":"code","f242c59f":"code","c6c3c665":"code","c46c46ce":"code","00ff7826":"code","2df87602":"code","4c308282":"code","cf58bd56":"code","8209fc00":"code","61b2eb84":"code","776cf52e":"code","28f7d07d":"code","3000d731":"code","a771ed4e":"code","362e4f76":"code","9e4fe967":"code","1baa0450":"code","6aa839df":"code","465d8e1f":"code","25b540e9":"code","d0cd878f":"code","50014bed":"code","d2e2fefb":"markdown","7ca18c9c":"markdown","a2fc5889":"markdown","18dffbc1":"markdown","a6f5a233":"markdown","b830aec1":"markdown","f6d4ef62":"markdown","2708dc0e":"markdown","030857e3":"markdown","10791f79":"markdown","4fd84fde":"markdown","f71b4bc2":"markdown","644347be":"markdown","a19c0329":"markdown","b999dcf0":"markdown","bf544ffc":"markdown","77776818":"markdown","5f0f10ab":"markdown","4e5f1268":"markdown","99a95b57":"markdown","bf8deda4":"markdown","bd8f3e3f":"markdown","88792ec9":"markdown","71eda1ae":"markdown","88c9875a":"markdown","9c8e4d22":"markdown","10827ee9":"markdown","51df57b9":"markdown","1f9b9dea":"markdown","c67b102c":"markdown","98b2ac69":"markdown","4cdbcb96":"markdown","9f688aab":"markdown","23e718d4":"markdown","b01e8bd5":"markdown","56627a23":"markdown","d7770d70":"markdown","359b01f5":"markdown","751bd227":"markdown","040681e8":"markdown","0260c567":"markdown","7113b722":"markdown","95b225a2":"markdown","28bcffa6":"markdown","55a2c3ed":"markdown","63a18cd8":"markdown","d930891c":"markdown","7ba3df08":"markdown","20401f12":"markdown","eac4e62a":"markdown","507836bb":"markdown","05bc540d":"markdown","6ca0a63a":"markdown","82dca9fd":"markdown","47624f4f":"markdown","a0ca051b":"markdown","53d4e26a":"markdown","41f694b4":"markdown","4a1baef1":"markdown","b5c3977f":"markdown","884e14ac":"markdown","678f3d7e":"markdown","53317dc0":"markdown","8c18d873":"markdown","97b3e7e6":"markdown","7633087e":"markdown","0750cfaa":"markdown","055c9849":"markdown"},"source":{"1e56e83d":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","6b4ca16d":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")","6333f9cb":"train_df.info()","e5ba38b1":"train_df.describe()\n","12de2a8d":"train_df.head(15)","5e9090e1":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","9e5bbbde":"train_df.columns.values","457c48b8":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","981b0a6a":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","53b7ebf2":"sns.barplot(x='Pclass', y='Survived', data=train_df)","12a083f2":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","8329378c":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)","60dd5664":"train_df['not_alone'].value_counts()","2b9a99ef":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","cb7f2b96":"train_df = train_df.drop(['PassengerId'], axis=1)","cea671a7":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) ","956b4d62":"# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","1e918587":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)","d9b71728":"train_df[\"Age\"].isnull().sum()","4effec98":"train_df['Embarked'].describe()","b67d52b2":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","0607910c":"train_df.info()","bdc11e35":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","ad291d94":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)","7d3fcfa7":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","fa400fab":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","c1fe6bc6":"train_df['Ticket'].describe()","3e3a7605":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","df38b0f9":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","b95aaef9":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","fb2c9e1a":"# let's see how it's distributed\ntrain_df['Age'].value_counts()","168e505a":"train_df.head(10)","b346d760":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","1384e997":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","d7c45ba1":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","7e5f79e8":"# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(20)","db651a33":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","e20d045d":"# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")","7d520492":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","cb126c01":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","e18b8b24":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","db2a5d44":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","d0de85e1":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(round(acc_perceptron,2,), \"%\")","0498e8df":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")","f242c59f":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","c6c3c665":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","c46c46ce":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")","00ff7826":"print(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","2df87602":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')","4c308282":"importances.head(15)","cf58bd56":"importances.plot.bar()","8209fc00":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","61b2eb84":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","776cf52e":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","28f7d07d":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","3000d731":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","a771ed4e":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","362e4f76":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","9e4fe967":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)","1baa0450":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","6aa839df":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","465d8e1f":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)","25b540e9":"# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","d0cd878f":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","50014bed":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","d2e2fefb":"## F-Score\n\nYou can combine precision and recall into one score, which is called the F-score. The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. As a result of that, the classifier will only get a high F-score, if both recall and precision are high.","7ca18c9c":"Above you can see that 'Fare' is a float and we have to deal with 4 categorical features: Name, Sex, Ticket and Embarked. Lets investigate and transfrom one after another.","a2fc5889":"##  Which is the best Model ?","18dffbc1":"## Hyperparameter Tuning\n\nBelow you can see the code of the hyperparamter tuning for the parameters criterion, min_samples_leaf, min_samples_split and n_estimators. \n\nI put this code into a markdown cell and not into a code cell, because it takes a long time to run it. Directly underneeth it, I put a screenshot of the gridsearch's output.","a6f5a233":"Our model predicts 81% of the time, a passengers survival correctly (precision). The recall tells us that it predicted the survival of 73 % of the people who actually survived. ","b830aec1":"### Embarked:\nConvert 'Embarked' feature into numeric.","f6d4ef62":"## Converting Features:","2708dc0e":"## Precision and Recall:","030857e3":"## Missing Data:\n### Cabin:\nAs a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). \n\nFirst I thought, we have to delete the 'Cabin' variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the **letter refers to the deck**. \n\nTherefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero.\n\nIn the picture below you can see the actual decks of the titanic, ranging from A to G.\n\n![titanic decks](http:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/Titanic_cutaway_diagram.png\/687px-Titanic_cutaway_diagram.png)","10791f79":"## ROC AUC Score\nThe ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC. \n\nA classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.","4fd84fde":"**1. Age and Sex:**","f71b4bc2":"The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job. \n\nOf course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is. ","644347be":"**5.  SibSp and Parch:**\n\nSibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","a19c0329":"# **Random Forest**\n\n## What is Random Forest ?\n\nRandom Forest is a supervised learning algorithm. Like you can already see from it\u2019s name, it creates a forest and makes it somehow random. The \u201eforest\u201c it builds, is an ensemble of Decision Trees, most of the time trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n\nTo say it in simple words: Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n\nOne big advantage of random forest is, that it can be used for both classification and regression problems, which form the majority of current machine learning systems.  With a few exceptions a random-forest classifier has all the hyperparameters of a decision-tree classifier and also all the hyperparameters of a bagging classifier, to control the ensemble itself. \n\nThe random-forest algorithm brings extra randomness into the model, when it is growing the trees. Instead of searching for the best feature while splitting a node, it searches for the best feature among a random subset of features. This process creates a wide diversity, which generally results in a better model. Therefore when you are growing a tree in random forest, only a random subset of the features is considered for splitting a node. You can even make trees more random, by using random thresholds on top of it, for each feature rather than searching for the best possible thresholds (like a normal decision tree does).\n\nBelow you can see how a random forest would look like with two trees:\n\n![picture](https:\/\/img3.picload.org\/image\/dagpgdpw\/bildschirmfoto-2018-02-06-um-1.png)","b999dcf0":"**3. Embarked, Pclass  and Sex:**","bf544ffc":"As we can see, the Random Forest classifier goes on the first place. But first, let us check, how random-forest performs, when we use cross validation. ","77776818":"![Titanic](http:\/\/titanic2ship.com\/wp-content\/uploads\/2013\/10\/ColorPlans-CyrilCodus-LG.jpg)","5f0f10ab":"Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).","4e5f1268":"First, I will drop 'PassengerId' from the train set, because it does not contribute to a persons survival probability. I will not drop it from the test set, since it is required there for the submission","99a95b57":"### Name:\nWe will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","bf8deda4":"**4. Pclass:**","bd8f3e3f":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","88792ec9":"### Fare:\n\nConverting \"Fare\" from float to int64, using the \"astype()\" function pandas provides:","71eda1ae":"## K-Fold Cross Validation:\n\nK-Fold Cross Validation randomly splits the training data into **K subsets called folds**. Let's image we would split our data into 4 folds (K = 4).  Our random forest model would be trained and evaluated 4 times, using a different fold for evaluation everytime, while it would be trained on the remaining 3 folds. \n\nThe image below shows the process, using 4 folds (K = 4). Every row represents one training + evaluation process. In the first row, the model get's trained on the first, second and third subset and evaluated on the fourth. In the second row, the model get's trained on the second, third and fourth subset and evaluated on the first. K-Fold Cross Validation repeats this process till every fold acted once as an evaluation fold.\n\n\n![cross-v.](https:\/\/img3.picload.org\/image\/ddwrppcl\/bildschirmfoto2018-02-02um10.0.png)\n\nThe result of our K-Fold Cross Validation example would be an array that contains 4 different scores. We then need to compute the mean and the standard deviation for these scores. \n\nThe code below perform K-Fold Cross Validation on our random forest model, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.","88c9875a":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n              \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n              \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclf = GridSearchCV(estimator=rf, param_grid=param_grid,\n                    n_jobs=-1)\n\nclf.fit(X_train, Y_train)        \n\nclf.best_params_\n\n![GridSearch Output](https:\/\/img2.picload.org\/image\/ddwglili\/bildschirmfoto2018-02-01um15.4.png)","9c8e4d22":"# **Submission**","10827ee9":"### 2.  Fare per Person","51df57b9":"### Fare:\nFor the 'Fare' feature, we need to do the same as with the 'Age' feature.  But it isn't that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \"qcut()\" function, that we can use to see, how we can form the categories.","1f9b9dea":"## Precision Recall Curve\n\nFor each person the Random Forest algorithm has to classify, it computes a probability based on a function and it classifies the person as survived (when the score is bigger the than threshold) or as not survived (when the score is smaller than the threshold). That's why the threshold plays an important part.\n\nWe will plot the precision and recall with the threshold using matplotlib:","c67b102c":"The first row is about the not-survived-predictions: **493 passengers were correctly classified as not survived** (called true negatives)  and **56 where wrongly classified as not survived** (false positives).\n\nThe second row is about the survived-predictions: **93 passengers where wrongly classified as survived** (false negatives) and **249 where correctly classified as survived** (true positives).\n\nA confusion matrix gives you a lot of information about how well your model does, but theres a way to get even more, like computing the classifiers precision.","98b2ac69":"There we have it, a 77 % F-score. The score is not that high, because we have a recall of 73%.\n\nBut unfortunately the F-score is not perfect, because it favors classifiers that have a similar precision and recall. This is a problem, because you sometimes want a high precision and sometimes a high recall. The thing is that an increasing precision, sometimes results in an decreasing recall and vice versa (depending on the threshold). This is called the precision\/recall tradeoff. We will discuss this in the following section.\n","4cdbcb96":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the 'Age' feature, which has 177 missing values. The 'Cabin' feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","9f688aab":"\n**The training-set has 891 examples and 11 features + the target variable (survived)**. 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:\n\n    survival:\tSurvival\n    PassengerId: Unique Id of a passenger.\n    pclass:\tTicket class\t\n    sex:\tSex\t\n    Age:\tAge in years\t\n    sibsp:\t# of siblings \/ spouses aboard the Titanic\t\n    parch:\t# of parents \/ children aboard the Titanic\t\n    ticket:\tTicket number\t\n    fare:\tPassenger fare\t\n    cabin:\tCabin number\t\n    embarked:\tPort of Embarkation","23e718d4":"**Test new paramters:**","b01e8bd5":"Now we can start tuning the hyperameters of random forest. ","56627a23":"## ROC AUC Curve\n\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.","d7770d70":"From the table above, we can note a few things. First of all, that we **need to convert a lot of features into numeric** ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the **features have widely different ranges**, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n\n**Let's take a more detailed look at what data is actually missing:**","359b01f5":"Now that we have a proper model, we can start evaluating it's performace in a more accurate way. Previously we only used accuracy and the oob score, which is just another form of accuracy. The problem is just, that it's more complicated to evaluate a classification model than a regression model. We will talk about this in the following section.","751bd227":"## **Table of Contents:**\n* Introduction\n* The RMS Titanic\n* Import Libraries\n* Getting the Data\n* Data Exploration\/Analysis\n* Data Preprocessing\n    - Missing Data\n    - Converting Features\n    - Creating Categories\n    - Creating new Features\n* Building Machine Learning Models\n    - Training 8 different models\n    - Which is the best model ?\n    - K-Fold Cross Validation\n* Random Forest \n    - What is Random Forest ?\n    - Feature importance\n    - Hyperparameter Tuning   \n* Further Evaluation \n    - Confusion Matrix\n    - Precision and Recall \n    - F-Score\n    - Precision Recall Curve\n    - ROC AUC Curve\n    - ROC AUC Score\n* Submission\n* Summary","040681e8":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","0260c567":"## Creating Categories:\n\nWe will now create categories within the following features:\n\n### Age:\nNow we need to convert the 'age' feature. First we will convert it from float into integer. Then we will create the new 'AgeGroup\" variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don't want for example that 80% of your data falls into group 1.","7113b722":"# **Further Evaluation**\n\n","95b225a2":"## Feature Importance\n\nAnother great quality of random forest is that  they make it very easy to measure the relative importance of each feature. Sklearn measure a features importance by looking at how much the treee nodes, that use that feature, reduce impurity on average (across all trees in the forest). It computes this score automaticall for each feature after training and scales the results  so that the sum of all importances is equal to 1.  We will acces this below:","28bcffa6":"# **Summary**\n\nThis project deepened my machine learning knowledge significantly and I strengthened my ability to apply concepts that I learned from textbooks, blogs and various other sources, on a different type of problem. This project had a heavy focus on the data preparation part, since this is what data scientists work on most of their time. \n\nI started with the data exploration where I got a feeling for the dataset, checked about missing data and learned which features are important. During this process I used seaborn and matplotlib to do the visualizations. During the data preprocessing part, I computed missing values, converted features into numeric ones, grouped values into categories and created a few new features. Afterwards I started training 8 different machine learning models, picked one of them (random forest) and applied cross validation on it. Then I explained how random forest works, took a look at the importance it assigns to the different features and tuned it's performace through optimizing it's hyperparameter values.  Lastly I took a look at it's confusion matrix and computed the models precision, recall and f-score, before submitting my predictions on the test-set to the Kaggle leaderboard.\n\nBelow you can see a before and after picture of the train_df dataframe:\n\n![Titanic](https:\/\/img1.picload.org\/image\/dagldoor\/before_after.png)\n\n\nOf course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. Another thing that can improve the overall result on the kaggle leaderboard would be a more extensive hyperparameter tuning on several machine learning models. Of course you could also do some ensemble learning.","55a2c3ed":"Nice ! I think that score is good enough to submit the predictions for the test-set to the Kaggle leaderboard.","63a18cd8":"### Embarked:\n\nSince the Embarked feature has only 2 missing values, we will just fill these with the most common one.","d930891c":"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be **certain ages, which have increased odds of survival** and because I want every feature to be roughly on the same scale, I will create age groups later on.","7ba3df08":"# **Building Machine Learning Models**","20401f12":"Embarked seems to be correlated with survival, depending on the gender. \n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S. \n\nPclass also seems to be correlated with survival. We will generate another plot of it below.","eac4e62a":"### Sex:\n\nConvert 'Sex' feature into numeric.","507836bb":"Our random forest model predicts as good as it did before. A general rule is that,  **the more features you have, the more likely your model will suffer from overfitting** and vice versa. But I think our data looks fine for now and hasn't too much features.\n\nThere is also another way to evaluate a random-forest classifier, which is probably much more accurate than the score we used before. What I am talking about is the **out-of-bag samples** to estimate the generalization accuracy. I will not go into details here about how it works. Just note that out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.","05bc540d":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.","6ca0a63a":"# Creating new Features\n\nI will add two new features to the dataset, that I compute out of other features.\n\n### 1. Age times Class","82dca9fd":"**Conclusion:**\n\nnot_alone and Parch doesn't play a significant role in our random forest classifiers prediction process. Because of that I will drop them from the dataset and train the classifier again. We could also remove more or less features, but this would need a more detailed investigation of the features effect on our model. But I think it's just fine to remove only Alone and Parch.","47624f4f":"### Ticket:","a0ca051b":"**Training random forest again:**","53d4e26a":"# **The RMS Titanic**\n\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster.","41f694b4":"Above you can clearly see that the recall is falling of rapidly at a precision of around 85%.  Because of that you may want to select the precision\/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision\/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4.  Then you could train a model with exactly that threshold and would get the desired accuracy.\n\n\nAnother way is to plot the precision and recall against each other:","4a1baef1":"# **Data Exploration\/Analysis**","b5c3977f":"# **Introduction**\n\nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival. In this challenge, we are asked to predict whether a passenger on the titanic would have been survived or not.","884e14ac":"### Age:\n\nNow we can tackle the issue with the age features missing values. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","678f3d7e":"Above you can see the 11 features + the target variable (survived). **What features could contribute to a high survival rate ?** \n\nTo me it would make sense if everything except 'PassengerId', 'Ticket' and 'Name'  would be correlated with a high survival rate. ","53317dc0":"# **Getting the Data**","8c18d873":"This looks much more realistic than before.  Our model has a average accuracy of 82% with a standard deviation of 4 %. The standard deviation shows us, how precise the estimates are . \n\nThis means in our case that the accuracy of our model can differ **+ -** 4%.\n\nI think the accuracy is still really good and since random forest is an easy to use model, we will try to increase it's performance even further in the following section.","97b3e7e6":"## Confusion Matrix:","7633087e":"Above we can see that **38% out of the training-set survived the Titanic**. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the 'Age' feature.","0750cfaa":"# **Import Libraries**","055c9849":"# **Data Preprocessing**"}}