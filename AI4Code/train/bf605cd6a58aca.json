{"cell_type":{"5f0e7778":"code","71f63426":"code","0bc8cb4b":"code","0a94762a":"code","b8b98297":"markdown","b8a0c8a1":"markdown"},"source":{"5f0e7778":"import requests\nfrom bs4 import BeautifulSoup\nimport lxml\nimport os\nimport urllib\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport csv\nimport multiprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n##https:\/\/bulkresizephotos.com\/zh-tw <- This website can change your image to 32*32 pixels\nnew_train=pd.read_csv('..\/input\/landmark-id-from-0-to-499\/new_train_id0_499.csv')\nfilename=os.listdir(\"..\/input\/graph-id0-499\/landgraphnew_0_499\")\nfilename.sort(key=lambda x:int(x[:-4]))\nimg=[]\nfor file in filename:\n\timg.append(np.array(Image.open(\"..\/input\/graph-id0-499\/landgraphnew_0_499\/\"+file)))\nimg=np.array(img)","71f63426":"new_train.groupby(['landmark_id']).agg('count').sort_values(by='id',ascending=False).style.background_gradient(cmap='Blues')\n","0bc8cb4b":"np.random.seed(1337)\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers.normalization import BatchNormalization\nX_train,X_test,y_train,y_test=train_test_split(img,new_train['landmark_id'],test_size=0.2)\ny_train=y_train.astype(int)\ny_test=y_test.astype(int)\ny_train=np.array(y_train).reshape(-1,1)\ny_test=np.array(y_test).reshape(-1,1)\nX_train=X_train.reshape(-1,32,32,3)\/255 #Normalize\nX_test=X_test.reshape(-1,32,32,3)\/255\ny_train=np_utils.to_categorical(y_train,num_classes=500)\ny_test=np_utils.to_categorical(y_test,num_classes=500)#landmark_id is from 0 to 499","0a94762a":"model=Sequential()\nmodel.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.35))\n\nmodel.add(MaxPool2D(pool_size=(2,2),padding='same'))\n\nmodel.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.45))\n\nmodel.add(MaxPool2D(pool_size=(2,2),padding='same'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.75))\n\nmodel.add(Dense(500,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\ntrain_history=model.fit(X_train,y_train,validation_split=0.2,epochs=100,batch_size=128,verbose=1)\naccuracy=model.evaluate(X_test,y_test,verbose=1)\nprint(\"test accuracy:\",accuracy[1])#accuracy for test set\n\n\n\ndef show_train_history(train_history,train,validation):\n\tplt.plot(train_history.history[train])\n\tplt.plot(train_history.history[validation])\n\tplt.title('Train History')\n\tplt.ylabel('train')\n\tplt.xlabel('Epoch')\n\tplt.legend(['train','validation'],loc='upper left')\n\tplt.show()\n\nshow_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation.","b8b98297":"I only use the landmark_id from 0 to 499. The data size is about 30000.Because the original data size is about 1,200,000, you need much time to download these images. Some images have a problem(404 not found).We need to remove the image from the data. \"new_train_id0_499.csv\" is the data which has been processed the 404 problem.\n\nIf you like my analysis, following my Github:https:\/\/github.com\/SonyFriend\/GoogleLandmark ","b8a0c8a1":"After you have download these images,you can  go to the website:https:\/\/bulkresizephotos.com\/zh-tw  to change these images to 32*32 pixels."}}