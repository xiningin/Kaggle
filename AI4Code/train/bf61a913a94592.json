{"cell_type":{"7beec53c":"code","29a0eef5":"code","e14041eb":"code","3dd1af22":"code","dc83a06a":"code","65bad264":"code","c5b8e8c6":"code","93e214e4":"code","b6b6a1a4":"code","8b761822":"code","5dfea48c":"code","3ece1b18":"code","f02ec294":"code","6c2b0c4e":"code","9b117730":"code","a7759dfd":"code","ef85060a":"code","cddd2a4a":"code","5b5d2c44":"code","1dfed385":"code","65fb8626":"code","6f3d2582":"code","29b2a29e":"markdown","4a1a8e0a":"markdown","5d742577":"markdown"},"source":{"7beec53c":"import helper\nimport os\nimport pickle\nimport glob\nimport torch\nimport numpy as np\ndata_dir = '..\/input\/friends-tv-series-screenplay-script\/'\ntext = \"\"\nfor file in glob.glob(data_dir+\"*.txt\"):\n    f = open(file, 'r')\n    text += f.read()\n    f.close()","29a0eef5":"text[:500]","e14041eb":"text = \"\"\nfolder_name = \"..\/input\/friends-tv-series-screenplay-script\/\"\nfor f in glob.glob(folder_name + '\/*.txt'):\n    temp = open(f,'r')    \n    text += temp.read()\n    temp.close()","3dd1af22":"view_line_range = (0, 25)\nprint('The lines {} to {}:'.format(*view_line_range))\nprint('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))","dc83a06a":"print('Dataset Stats')\nunique_words = len ({word for word in text.split()})\nprint(\"Total # of Unique Words: \",unique_words)\nlines = text.split('\\n')\nprint(\"Total # of Lines: \",len(lines))\navg_words_perline = [len(line.split()) for line in lines]\nprint(\"Average number of words in each line: \", np.average(avg_words_perline))","65bad264":"from collections import Counter\nfreq_word = Counter(text)\nvocab_sorted = sorted(freq_word, key = freq_word.get, reverse = True)\nint_to_vocab = {i : word for i, word in enumerate(vocab_sorted)}\nvocab_to_int = {word : i for i, word in int_to_vocab.items()}","c5b8e8c6":"pun_dic = {\n        '.': '||period||',\n        ',': '||comma||',\n        '\"': '||quotation_mark||',\n        ';': '||semicolon||',\n        '!': '||exclamation_mark||',\n        '?': '||question_mark||',\n        '(': '||left_parentheses||',\n        ')': '||right_Parentheses||',\n        '-': '||dash||',\n        '\\n': '||return||'\n    }","93e214e4":"text = text[57:] # dropped the first two line( notice )","b6b6a1a4":"for key, token in pun_dic.items(): \n    text = text.replace(key, ' {} '.format(token))\ntext = text.lower()\ntext = text.split()","8b761822":"from collections import Counter\nSPECIAL_WORDS = {'PADDING': '<PAD>'}\nL_text = text + list(SPECIAL_WORDS.values())\n\nfreq_word = Counter(L_text)\nvocab_sorted = sorted(freq_word, key = freq_word.get, reverse = True)\nint_to_vocab = {i : word for i, word in enumerate(vocab_sorted)}\nvocab_to_int = {word : i for i, word in int_to_vocab.items()}","5dfea48c":"int_text = [vocab_to_int[word] for word in text]","3ece1b18":"train_on_gpu = torch.cuda.is_available()","f02ec294":"from torch.utils.data import TensorDataset, DataLoader\ndef get_dataloader(text, seq_length, batch_size):\n    batch_num = len(text)\/\/batch_size\n    batch_words = text[: (batch_num * batch_size)]\n    \n    feature, target = [],[]\n    target_len = len(batch_words[:-seq_length])\n    \n    for i in range(0, target_len):\n        feature.append(batch_words[i: i + seq_length])\n        target.append(batch_words[i + seq_length])\n    \n    target_tensors = torch.from_numpy(np.array(target))\n    feature_tensors = torch.from_numpy(np.array(feature))\n    \n    data = TensorDataset(feature_tensors, target_tensors)\n    \n    data_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, shuffle = True)\n    \n    return data_loader","6c2b0c4e":"import torch.nn as nn\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3):\n        super(RNN,self).__init__()\n        self.n_layers  = n_layers\n        self.output_size = output_size\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.dropout = nn.Dropout(dropout)\n        self.embedding_dim = embedding_dim\n        \n        # Model Layers\n        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout, batch_first = True)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, nn_input, hidden):\n        batch_size = nn_input.size(0)\n        nn_input = nn_input.long()\n        \n        embed_out = self.embedding(nn_input)\n        lstm_out, hidden = self.lstm(embed_out, hidden)\n        \n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        lstm_out = self.dropout(lstm_out)\n        lstm_out = self.fc(lstm_out)\n        \n        lstm_out = lstm_out.view(batch_size, -1, self.output_size)\n        lstm_output = lstm_out[:, -1]\n        \n        return lstm_output, hidden\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        if train_on_gpu:\n            hidden = (weight.new(self.n_layers, batch_size , self.hidden_dim).zero_().cuda(),\n                      weight.new(self.n_layers, batch_size , self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size , self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size , self.hidden_dim).zero_())\n        \n        return hidden\n","9b117730":"def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n    if (train_on_gpu):\n        inp, target = inp.cuda(), target.cuda()\n    \n    hidden = tuple([i.data for i in hidden])\n    \n    rnn.zero_grad()\n    out, hidden = rnn(inp, hidden)\n    \n    loss = criterion(out, target)\n    loss.backward()\n    \n    clip = 5\n    \n    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n    \n    optimizer.step()\n    \n    return loss.item(), hidden","a7759dfd":"def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n    batch_losses = []\n    rnn.train()\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        hidden = rnn.init_hidden(batch_size)\n        \n        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n            n_batches = len(train_loader.dataset)\/\/batch_size\n            if(batch_i > n_batches):\n                break\n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n            \n            batch_losses.append(loss)\n            if batch_i % show_every_n_batches == 0:\n                print('Epoch: {:>4}\/{:<4}  Loss: {}\\n'.format(\n                    epoch_i, n_epochs, np.average(batch_losses)))\n                batch_losses = []\n\n    return rnn","ef85060a":"sequence_length = 8\nbatch_size = 256\ntrain_loader = get_dataloader(int_text, sequence_length, batch_size)","cddd2a4a":"epoch = 25\nlr = 0.0003\nvocab_size = len(vocab_to_int)\noutput_size = len(vocab_to_int)\nembedding_dim = 256\nhidden_dim = 512\nn_layers = 3\nshow_every_n_batches = 500","5b5d2c44":"rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\nif train_on_gpu:\n    rnn.cuda()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\ntrained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, epoch, show_every_n_batches)\nsave_filename = os.path.splitext(os.path.basename('.\/rnn_trained'))[0] + '.pt'\ntorch.save(trained_rnn, save_filename)\nprint('Model Trained and Saved')","1dfed385":"print(rnn)","65fb8626":"import torch.nn.functional as F\n\ndef generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n    \"\"\"\n    Generate text using the neural network\n    :param decoder: The PyTorch Module that holds the trained neural network\n    :param prime_id: The word id to start the first prediction\n    :param int_to_vocab: Dict of word id keys to word values\n    :param token_dict: Dict of puncuation tokens keys to puncuation values\n    :param pad_value: The value used to pad a sequence\n    :param predict_len: The length of text to generate\n    :return: The generated text\n    \"\"\"\n    rnn.eval()\n    current_seq = np.full((1, sequence_length), pad_value)\n    current_seq[-1][-1] = prime_id\n    predicted = [int_to_vocab[prime_id]]\n    \n    for _ in range(predict_len):\n        if train_on_gpu:\n            current_seq = torch.LongTensor(current_seq).cuda()\n        else:\n            current_seq = torch.LongTensor(current_seq)\n        hidden = rnn.init_hidden(current_seq.size(0))\n        output, _ = rnn(current_seq, hidden)\n        p = F.softmax(output, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu()\n        top_k = 5\n        p, top_i = p.topk(top_k)\n        top_i = top_i.numpy().squeeze()\n        p = p.numpy().squeeze()\n        word_i = np.random.choice(top_i, p=p\/p.sum())\n        word = int_to_vocab[word_i]\n        predicted.append(word)\n        current_seq = current_seq.cpu().numpy()\n        current_seq = np.roll(current_seq, -1, 1)\n        current_seq[-1][-1] = word_i\n    \n    gen_sentences = ' '.join(predicted)\n    for key, token in token_dict.items():\n        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n    gen_sentences = gen_sentences.replace('( ', '(')\n    return gen_sentences","6f3d2582":"gen_length = 400 # modify the length to your preference\nprime_word = 'joey' # name for starting the script\npad_word = SPECIAL_WORDS['PADDING']\ngenerated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, pun_dic, vocab_to_int[pad_word], gen_length)\nprint(generated_script)","29b2a29e":"# Pre-Processing","4a1a8e0a":"# RNN, LSTM The Directors","5d742577":"# Model"}}