{"cell_type":{"0d8f50bb":"code","0abf1d25":"code","9e3ca672":"code","c804a5ad":"code","8af4704b":"code","7c853d78":"code","9f42e442":"code","96138e03":"code","3bcab620":"code","14e4616f":"code","f77e610d":"code","6726aef5":"code","3472812d":"code","f9d99775":"code","ebd71b10":"code","37082188":"code","b2c856bd":"code","a7078a9f":"code","65a648c7":"code","51656007":"code","f77c4802":"code","43e4f91d":"code","e4985bf6":"code","7d0ff913":"code","32d3edd2":"code","efd607d8":"code","1f7fb887":"code","00c940c1":"code","6f5ebfbc":"code","0522f3d9":"code","b469cd21":"code","876930ba":"code","1bb43981":"code","f5014d78":"code","ee1932ac":"code","92a9d3c1":"code","a968e76f":"code","14905ca3":"code","9827ede3":"code","e41491a7":"code","e01846b3":"code","39aed061":"code","aaf3f956":"code","aceaf984":"code","7633d0ae":"markdown","fdc33980":"markdown","ced6ef83":"markdown","2e7bfd56":"markdown","b8de49ba":"markdown","f8c17936":"markdown","77b16a50":"markdown","b858afd8":"markdown","1f286c6f":"markdown","082b4f1d":"markdown","14b4834c":"markdown","6528bc43":"markdown","4a71d523":"markdown","b635f9cd":"markdown","e036127d":"markdown","ff9211a8":"markdown","54a4714d":"markdown","5e9b3423":"markdown","8390cfe9":"markdown","a80b06ab":"markdown","cd953eb6":"markdown","58023477":"markdown"},"source":{"0d8f50bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0abf1d25":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\nimport warnings\nwarnings.filterwarnings('ignore')","9e3ca672":"pd.options.display.max_rows=None\npd.options.display.max_columns=None","c804a5ad":"df = pd.read_csv('\/kaggle\/input\/housing\/Housing_esh.csv')\ndf.head()","8af4704b":"df.info()","7c853d78":"df.describe()","9f42e442":"sns.distplot(df['price'])\nplt.show()","96138e03":"stats.probplot(df['price'], plot=plt)\nplt.show()","3bcab620":"df['lprice'] = np.log(df['price'])","14e4616f":"sns.distplot(df['lprice'])\nplt.show()","f77e610d":"stats.probplot(df['lprice'], plot=plt)\nplt.show()","6726aef5":"cat_cols = list(df.select_dtypes(include='object'))\nprint(cat_cols); print()\nfor col in cat_cols:\n    print(df[col].value_counts())","3472812d":"df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\ndf.info()","f9d99775":"y = df['lprice']\nX = df.drop(['price','lprice'], axis=1)\nXc = sm.add_constant(X)\nresult = sm.OLS(y, Xc).fit()\nresult.summary()","ebd71b10":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nvif = [vif(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame(vif, Xc.columns, columns=['vif'])","37082188":"residuals = result.resid\nsns.distplot(residuals)\nplt.show()","b2c856bd":"# Jarque-Berra Test of Normality\n# Ho: Data is normal\n# H1: Data is not normal\nprint(stats.jarque_bera(residuals))","a7078a9f":"y_pred = result.predict(Xc)\nsns.regplot(x=y_pred, y=residuals, lowess=True, line_kws={'color':'red'})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()","65a648c7":"# H0: \u03c3 is constant across the range of data. H1: \u03c3 is not constant across the range of data\nimport statsmodels.stats.api as sms\nname = ['F-statistic','p-value']\ntest = sms.het_goldfeldquandt(y=residuals, x=Xc)\ntest","51656007":"import statsmodels.tsa.api as smt\nacf = smt.graphics.plot_acf(residuals, lags=20)\nplt.show()","f77c4802":"y_pred = result.predict(Xc)\nsns.regplot(x=y_pred, y=y, lowess=True, line_kws={'color':'red'})\nplt.show()","43e4f91d":"y = df['lprice']\nX = df.drop(['lprice','price'], axis=1)\npf = PolynomialFeatures()\nXt = pf.fit_transform(X)\ncols = pf.get_feature_names(X.columns)\nXt = pd.DataFrame(Xt, columns=cols)\nXt.head().T","e4985bf6":"Xt.info(verbose=True)","7d0ff913":"model = sm.OLS(y,Xt).fit()\nmodel.summary()","32d3edd2":"cols = Xt.columns.tolist()\nwhile len(cols)>0:\n    X_1 = Xt[cols]\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values, index=cols)\n    pmax = max(p)\n    feature_max_p = p.idxmax()\n    if pmax>0.05:\n        cols.remove(feature_max_p)\n    else:\n        break","efd607d8":"print(len(cols)); print(cols)","1f7fb887":"Xn = Xt[cols]\nmodel = sm.OLS(y,Xn).fit()\nmodel.summary()","00c940c1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.30, random_state=1)","6f5ebfbc":"model = sm.OLS(y_train, X_train).fit()\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nprint('R\u00b2 of Train data:', r2_score(y_train, y_train_pred))\nprint('RMSE of Train data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint('R\u00b2 of Test data:', r2_score(y_test, y_test_pred))\nprint('RMSE of Test data:', np.sqrt(mean_squared_error(y_test, y_test_pred)))","0522f3d9":"from sklearn.linear_model import LinearRegression\ny = df['lprice']\nX = df.drop(['lprice','price'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)","b469cd21":"print('R\u00b2 of Train data:', r2_score(y_train, y_train_pred))\nprint('RMSE of Train data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint('R\u00b2 of Test data:', r2_score(y_test, y_test_pred))\nprint('RMSE of Test data:', np.sqrt(mean_squared_error(y_test, y_test_pred)))","876930ba":"from sklearn.feature_selection import RFE\nno_cols = X.shape[1]\nr2_train_score = []\nr2_test_score = []\nnvars = []\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nfor n in range(no_cols):\n    lr = LinearRegression()\n    rfe = RFE(lr, n+1)\n    \n    nvars.append(n+1)\n    rfe.fit(X_train, y_train)\n    y_train_pred = rfe.predict(X_train)\n    y_test_pred = rfe.predict(X_test)\n    \n    r2_train = r2_score(y_train, y_train_pred)\n    r2_train_score.append(r2_train)\n    r2_test = r2_score(y_test, y_test_pred)\n    r2_test_score.append(r2_test)","1bb43981":"plt.plot(nvars, r2_train_score, color='g')\nplt.plot(nvars, r2_test_score, color='r')\nplt.show()","f5014d78":"test_result = pd.DataFrame({'nvars':nvars, 'r2_test':r2_test_score})\ntest_result.sort_values(by=['r2_test'], ascending=False)","ee1932ac":"dd = np.arange(9)\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict\nkf = KFold(n_splits=3)\nfor d1, d2 in kf.split(dd):\n    print(d1, d2)","92a9d3c1":"from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\nlr = LinearRegression()\ncv_model = cross_val_score(lr, X, y, cv=3, scoring='neg_mean_squared_error')\nprint(cv_model); print(cv_model.mean())","a968e76f":"# All matrix that can be used in cross_val_score\nimport sklearn\nprint(sorted(sklearn.metrics.SCORERS.keys()))","14905ca3":"lr = LinearRegression()\nkf = KFold(n_splits=3, shuffle=True, random_state=4)\ncv_model = cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error')\nprint(cv_model); print(cv_model.mean())","9827ede3":"n_alphas = 500\nalphas = np.logspace(-10, 0, n_alphas)\nlasso_cv = LassoCV(alphas=alphas, cv=3, random_state=1)\nlasso_cv.fit(X, y)\nlasso_cv.alpha_","e41491a7":"lasso = Lasso(alpha=1e-10)\nlasso.fit(X, y)\npd.DataFrame(lasso.coef_, X.columns, columns=['coefs'])","e01846b3":"y_pred = lasso.predict(X)\nr2_score(y, y_pred)","39aed061":"n_alphas = 100\nalphas = np.logspace(-4, 3, n_alphas)\nridge_cv = RidgeCV(alphas=alphas, cv=3)\nridge_cv.fit(X, y)\nridge_cv.alpha_","aaf3f956":"ridge = Ridge(alpha=0.0001)\nridge.fit(X, y)\npd.DataFrame(ridge.coef_, X.columns, columns=['coefs'])","aceaf984":"y_pred = ridge_cv.predict(X)\nr2_score(y, y_pred)","7633d0ae":"Price seems to have outliers at the higher side. Hence, we will transform the price using log","fdc33980":"Observation: We will accept the Null Hypothesis that residual is normal.","ced6ef83":"### Train test split","2e7bfd56":"### Regularization","b8de49ba":"Observation: DW value from the model result is 1.302. The expected value for no autocorrelation is 2. Hence, there is slight indication of autocorrelation.","f8c17936":"#### Applying Polynomial Features","77b16a50":"#### Note:\nRemove const only if you are sure that it is a line passing through origin. Otherwise ignore","b858afd8":"### Illustration of Cross Validation and KFold","1f286c6f":"#### Autocorrelation","082b4f1d":"#### Homoscadesticity","14b4834c":"#### Lasso L1","6528bc43":"#### Ridge L2","4a71d523":"## Statistics","b635f9cd":"#### Applying RFE to select the optimum number of features","e036127d":"### Assumptions","ff9211a8":"## Machine Learning","54a4714d":"### Imporving model results by incorporating square terms and interaction","5e9b3423":"#### Linearity of Residuals","8390cfe9":"### Build Statistical Model","a80b06ab":"#### Normality of Residuals","cd953eb6":"#### Backward Elimination","58023477":"Observation: Since p-value is higher than \u03b1(0.05), we will accept the null hypothesis and conclude that residual variance is constant."}}