{"cell_type":{"192eaf95":"code","5e54f6bc":"code","712ab1e8":"code","2a9063da":"code","e917b595":"code","b6b2289b":"code","e3b82b7e":"code","dea19652":"code","040b36a0":"code","06e1026c":"code","ebd77363":"code","16e8b1f3":"code","e56e99c9":"code","23321513":"code","cb321fca":"code","4c6414ad":"code","1adcc359":"code","90b9f10f":"code","973ad90b":"code","051940ed":"code","0c9bd9a7":"code","743a6db1":"code","f8625779":"code","beb1801f":"code","f71203f3":"code","3b36534e":"code","2a67b62a":"code","7bba706a":"code","3427a5b5":"code","013fe0fc":"code","93713630":"code","a4e0215a":"code","0f5b997e":"code","6ce81e8a":"code","c3169196":"code","0ce3bdd3":"code","518df2ad":"code","77f7d9ff":"code","141c3893":"code","a9375a1d":"code","90c95edc":"code","e98a3816":"code","ee90f114":"code","35b9e077":"code","c8665d30":"code","73e43a58":"code","0b4d56f8":"code","77f59f3d":"code","e92113a1":"code","29116d02":"markdown","2235cc51":"markdown","729554d6":"markdown","fef3f253":"markdown","d1f924d0":"markdown","5c8700b3":"markdown","85176c07":"markdown","a285d437":"markdown","63c011a8":"markdown"},"source":{"192eaf95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e54f6bc":"from sklearn.manifold import TSNE\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","712ab1e8":"tst = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")\ntrn = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")","2a9063da":"tst.head()","e917b595":"trn.head()","b6b2289b":"sub.head()","e3b82b7e":"print(\"Train\",trn.shape)\nprint(\"Test\",tst.shape)\nprint(\"Submission\",sub.shape)","dea19652":"feat_cols = [ 'pixel'+str(i) for i in range(tst.shape[1]) ] # convert num to pixelnum for column names\nlen(feat_cols)","040b36a0":"trn.columns","06e1026c":"def subtract_lists(x,y):\n    \"\"\"Subtract Two Lists (List Difference)\"\"\"\n    return [item for item in x if item not in y]\nfeat = subtract_lists(list(trn.columns),[\"label\"])\nlen(feat)","ebd77363":"X = pd.concat([trn[feat_cols],tst],axis=0) # trn.iloc[:].iloc[1:]","16e8b1f3":"X.shape","e56e99c9":"X.head()","23321513":"df = trn\nrndperm = np.random.permutation(df.shape[0])     # random permutation to be used later for data viz\n\nplt.gray()                                       # set the colormap to \u201cgray\u201d\nfig = plt.figure( figsize=(20,9) )               # initilaize the figure with the figure size\n\nfor i in range(0,15):\n    # use subplots to get 3x5 matrix of random handwritten digit images\n    ax = fig.add_subplot(3,5,i+1, title=\"Digit: {}\".format(str(df.loc[rndperm[i],'label'])) )\n    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n    ax.set_xticks([])                             # set the xtciks and yticks as blanks\n    ax.set_yticks([]) \n\nplt.savefig(\"MINIST_DIGITS.png\",dpi=600)\nplt.show() ;                                      # display the figure","cb321fca":"data_subset = df[feat_cols].values  # get the numpy array of this dataframe and store it is subset data","4c6414ad":"plt.style.use('dark_background')","1adcc359":"univ_seed=42","90b9f10f":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = univ_seed)\ntsne_results_2D = tsne.fit_transform(X)","973ad90b":"Shape_X = trn.shape[0]","051940ed":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300, random_state = univ_seed)\ntsne_results_3D = tsne.fit_transform(X)","0c9bd9a7":"np.save(\"tsne_results_2D.npy\",tsne_results_2D)\nnp.save(\"tsne_results_3D.npy\",tsne_results_3D)","743a6db1":"y_labels = list(trn[\"label\"]) + [np.nan]*tst.shape[0]","f8625779":"reduced_df=pd.DataFrame(np.c_[y_labels ,tsne_results_2D[:,0], tsne_results_2D[:,1]], \n                        columns=['y','tsne-2d-one','tsne-2d-two' ])\nreduced_df['tsne-3d-one']=tsne_results_3D[:,0]\nreduced_df['tsne-3d-two']=tsne_results_3D[:,1]\nreduced_df['tsne-3d-three']=tsne_results_3D[:,2]\nreduced_df.head()","beb1801f":"reduced_df.tail()","f71203f3":"fig=plt.figure(figsize=(16,10))\n\n\nreduced_df_sorted=reduced_df.dropna().sort_values(by='y', ascending=True).sample(n = 10000,random_state=univ_seed)\n\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=\"y\",\n    palette=sns.color_palette(\"tab10\", 10), # hls, rocket, icefire , Spectral\n    data=reduced_df_sorted,\n    legend=\"full\",\n    alpha=1\n)\n\n\n\nplt.legend(title=\"Target Digits (y)\")\nplt.title(\"t-SNE Plot for MNIST Handwritten Digit Classification\",fontsize=20)\nplt.savefig(\"t-SNE Plot for MNIST Handwritten Digit Classification_custom1.png\",dpi=300)","3b36534e":"import plotly.express as px\n\ndf_sampled= reduced_df.dropna().sample(n = 500,random_state=univ_seed)\ndf_sampled_sorted=df_sampled.sort_values(by='y', ascending=True)\n\nfig = px.scatter_3d(df_sampled_sorted, x='tsne-3d-one', y='tsne-3d-two', z='tsne-3d-three',\n                    color='y', template=\"plotly_dark\",color_continuous_scale=px.colors.sequential.Plasma) # .Viridis\n\nfig.write_html(\"MNIST_Handwritten_Digits_Dataset_tSNE_3D_Viz.html\")\nfig.show()","2a67b62a":"# X=reduced_df[[\"tsne-2d-one\", \"tsne-2d-two\"]].values\nreduced_df_train = reduced_df.dropna()\nX_train3D=reduced_df_train[[\"tsne-3d-one\", \"tsne-3d-two\", \"tsne-3d-three\"]].values\ny_train3D=reduced_df_train[\"y\"].values\nprint(\"X_train3D Shape : \", X_train3D.shape , \"y_train3D Shape : \", y_train3D.shape)\n\nreduced_df_test = reduced_df.loc[~reduced_df.index.isin(reduced_df.dropna().index)]\nX_test3D=reduced_df_test [[\"tsne-3d-one\", \"tsne-3d-two\", \"tsne-3d-three\"]].values\ny_test3D=reduced_df_test [\"y\"].values\nprint(\"X_test3D Shape : \", X_test3D.shape , \"y_test3D Shape : \", y_test3D.shape)","7bba706a":"#train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X_train3D, y_train3D, test_size=0.2, random_state=univ_seed)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","3427a5b5":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nKs = 40+1\nmean_acc = np.zeros((Ks-1))\nmean_acc_train= np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nstd_acc_train = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat) #gets the test accuracy\n    y_pred=neigh.predict(X_train)\n    mean_acc_train[n-1] = metrics.accuracy_score(y_train,y_pred) #gets the train accuracy\n    \n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n    std_acc_train[n-1]=np.std(y_pred==y_train)\/np.sqrt(y_pred.shape[0])\nprint(\"MEAN ACCURACY\")\nlength=len(mean_acc)\nfor i in range(length):\n    test_acc='{0:.3f}'.format(round(mean_acc[i],3))\n    train_acc='{0:.3f}'.format(round(mean_acc_train[i],3))\n    \n    print(\"K=\",f\"{i+1:02d}\",\"  Avg. Test Accuracy=\",test_acc,\"  Avg. Train Accuracy=\",train_acc) ","013fe0fc":"\nprint( \"The best test accuracy was\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)\nprint( \"The corresponding training accuracy obtained was :\",mean_acc_train[mean_acc.argmax()])\n\nplt.figure(figsize=(15,7.5))\n#comment the figure sizeif you want a small figure size\nplt.plot(range(1,Ks),mean_acc_train,'r',linewidth=5)\nplt.plot(range(1,Ks),mean_acc,'g',linewidth=5)\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc_train - 1 * std_acc_train,mean_acc_train + 1 * std_acc_train, alpha=0.10)\n\n\nplt.scatter( mean_acc.argmax()+1,  mean_acc.max())\nplt.scatter( mean_acc.argmax()+1,  mean_acc_train[mean_acc.argmax()])\n#plt.annotate(\"BEST_TEST_ACC\", ( mean_acc.argmax()+1,  mean_acc.max()))\n#plt.annotate(\"CORRESPONDING_TRAIN_ACC\", ( mean_acc.argmax()+1,  mean_acc_train[mean_acc.argmax()]))\n\nplt.legend(('Train_Accuracy ','Test_Accuracy ', '+\/- 3xstd_test','+\/- 3xstd_train','BEST_TEST_ACC','CORRESPONDING_TRAIN_ACC'))\n\nplt.xticks(ticks=list(range(Ks)),labels=list(range(Ks)) )\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.title(\"Number of Neigbors Chosen vs Mean Training and Testing Accuracy Score\",fontsize=20)\nplt.tight_layout()\n\n\nplt.savefig(\"Number of Neigbors Chosen vs Mean Training and Testing Accuracy Score.png\",dpi=600)\nplt.show()\n\n#this plot clearly shows that initially the model does overfit","93713630":"#First,we keep a dictionary that measures all the losses\/scores for our model\/classifier\nTest_Scores={}\nTrain_Scores={}\n\n\n#Now evaluate the model based on metrics\n#First import scoring methods\nfrom sklearn.metrics import  accuracy_score, f1_score, confusion_matrix,precision_score, recall_score\n\nfrom sklearn.metrics import jaccard_score as jaccard_similarity_score\n#reconstruct the best model as last model is only saved. Previous models were overwritten\nbest_k=mean_acc.argmax()+1  #7\nneigh = KNeighborsClassifier(n_neighbors = best_k).fit(X_train,y_train)\nyhat=neigh.predict(X_test)\ny_pred=neigh.predict(X_train)\n\n#training scores\nTrain_Scores['KNN-jaccard']=jaccard_similarity_score(y_train, y_pred,average='weighted')\nTrain_Scores['KNN-f1-score']=f1_score(y_train, y_pred, average='weighted') \nTrain_Scores['KNN-accuracy-score']=accuracy_score(y_train, y_pred)\nTrain_Scores['KNN-precision-score']=precision_score(y_train, y_pred,average='weighted')\nTrain_Scores['KNN-recall-score']=recall_score(y_train, y_pred,average='weighted')\nprint(\"Train Scores\")\nprint(Train_Scores)\n\n#testing scores\n\nTest_Scores['KNN-jaccard']=jaccard_similarity_score(y_test, yhat,average='weighted')\nTest_Scores['KNN-f1-score']=f1_score(y_test, yhat, average='weighted')\nTest_Scores['KNN-accuracy-score']=accuracy_score(y_test, yhat) \nTest_Scores['KNN-precision-score']=precision_score(y_test, yhat, average='weighted') \nTest_Scores['KNN-recall-score']=recall_score(y_test, yhat, average='weighted') \nprint(\"Test Scores\")\nprint(Test_Scores)\n\ncm=confusion_matrix(y_test, yhat)\n\n\ncf_matrix=confusion_matrix(y_test, yhat)\n\nside_of_cm=cf_matrix.shape[0]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in\n          zip(group_counts,group_percentages)]\n\nlabels = np.asarray(labels).reshape(side_of_cm,side_of_cm)\n\nfig=plt.figure(figsize=(20,8))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='inferno')\n\n\n\nplt.xlabel(\"True Values\",fontsize=18)\nplt.ylabel(\"Predicted Values\",fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.title(\"Confusion Matrix for k-NN classifier for applied t-SNE\\nMNIST Handwritten Digit Dataset\",fontsize=20)\n\nplt.savefig(\"Confusion Matrix for k-NN classifier for applied t-SNE MNIST Handwritten Digit Dataset_1.png\",dpi=600)\n","a4e0215a":"best_k=mean_acc.argmax()+1  #7\nneigh3D = KNeighborsClassifier(n_neighbors = best_k).fit(X_train3D,y_train3D)\nyhat=neigh3D.predict(X_test3D)\ny_pred=neigh3D.predict(X_train3D)","0f5b997e":"sub.head()","6ce81e8a":"sub.shape","c3169196":"yhat.shape","0ce3bdd3":"np.unique(yhat)","518df2ad":"from copy import deepcopy\nsub3D = deepcopy(sub)\nsub3D[\"Label\"] = yhat.astype('uint8')","77f7d9ff":"sub3D.head()","141c3893":"sub3D.to_csv('sub3D.csv',index=False)","a9375a1d":"# X=reduced_df[[\"tsne-2d-one\", \"tsne-2d-two\"]].values\nreduced_df_train = reduced_df.dropna()\nX_train2D=reduced_df_train[[\"tsne-2d-one\", \"tsne-2d-two\"]].values\ny_train2D=reduced_df_train[\"y\"].values\nprint(\"X_train2D Shape : \", X_train2D.shape , \"y_train2D Shape : \", y_train2D.shape)\n\nreduced_df_test = reduced_df.loc[~reduced_df.index.isin(reduced_df.dropna().index)]\nX_test2D=reduced_df_test [[\"tsne-2d-one\", \"tsne-2d-two\"]].values\ny_test2D=reduced_df_test [\"y\"].values\nprint(\"X_test3D Shape : \", X_test2D.shape , \"y_test3D Shape : \", y_test2D.shape)","90c95edc":"#train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X_train2D, y_train2D, test_size=0.2, random_state=univ_seed)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","e98a3816":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nKs = 40+1\nmean_acc = np.zeros((Ks-1))\nmean_acc_train= np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nstd_acc_train = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat) #gets the test accuracy\n    y_pred=neigh.predict(X_train)\n    mean_acc_train[n-1] = metrics.accuracy_score(y_train,y_pred) #gets the train accuracy\n    \n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n    std_acc_train[n-1]=np.std(y_pred==y_train)\/np.sqrt(y_pred.shape[0])\nprint(\"MEAN ACCURACY\")\nlength=len(mean_acc)\nfor i in range(length):\n    test_acc='{0:.3f}'.format(round(mean_acc[i],3))\n    train_acc='{0:.3f}'.format(round(mean_acc_train[i],3))\n    \n    print(\"K=\",f\"{i+1:02d}\",\"  Avg. Test Accuracy=\",test_acc,\"  Avg. Train Accuracy=\",train_acc) ","ee90f114":"\nprint( \"The best test accuracy was\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)\nprint( \"The corresponding training accuracy obtained was :\",mean_acc_train[mean_acc.argmax()])\n\nplt.figure(figsize=(15,7.5))\n#comment the figure sizeif you want a small figure size\nplt.plot(range(1,Ks),mean_acc_train,'r',linewidth=5)\nplt.plot(range(1,Ks),mean_acc,'g',linewidth=5)\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc_train - 1 * std_acc_train,mean_acc_train + 1 * std_acc_train, alpha=0.10)\n\n\nplt.scatter( mean_acc.argmax()+1,  mean_acc.max())\nplt.scatter( mean_acc.argmax()+1,  mean_acc_train[mean_acc.argmax()])\n#plt.annotate(\"BEST_TEST_ACC\", ( mean_acc.argmax()+1,  mean_acc.max()))\n#plt.annotate(\"CORRESPONDING_TRAIN_ACC\", ( mean_acc.argmax()+1,  mean_acc_train[mean_acc.argmax()]))\n\nplt.legend(('Train_Accuracy ','Test_Accuracy ', '+\/- 3xstd_test','+\/- 3xstd_train','BEST_TEST_ACC','CORRESPONDING_TRAIN_ACC'))\n\nplt.xticks(ticks=list(range(Ks)),labels=list(range(Ks)) )\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.title(\"Number of Neigbors Chosen vs Mean Training and Testing Accuracy Score\",fontsize=20)\nplt.tight_layout()\n\n\nplt.savefig(\"Number of Neigbors Chosen vs Mean Training and Testing Accuracy Score.png\",dpi=600)\nplt.show()\n\n#this plot clearly shows that initially the model does overfit","35b9e077":"#First,we keep a dictionary that measures all the losses\/scores for our model\/classifier\nTest_Scores={}\nTrain_Scores={}\n\n\n#Now evaluate the model based on metrics\n#First import scoring methods\nfrom sklearn.metrics import  accuracy_score, f1_score, confusion_matrix,precision_score, recall_score\n\nfrom sklearn.metrics import jaccard_score as jaccard_similarity_score\n#reconstruct the best model as last model is only saved. Previous models were overwritten\nbest_k=mean_acc.argmax()+1  #7\nneigh = KNeighborsClassifier(n_neighbors = best_k).fit(X_train,y_train)\nyhat=neigh.predict(X_test)\ny_pred=neigh.predict(X_train)\n\n#training scores\nTrain_Scores['KNN-jaccard']=jaccard_similarity_score(y_train, y_pred,average='weighted')\nTrain_Scores['KNN-f1-score']=f1_score(y_train, y_pred, average='weighted') \nTrain_Scores['KNN-accuracy-score']=accuracy_score(y_train, y_pred)\nTrain_Scores['KNN-precision-score']=precision_score(y_train, y_pred,average='weighted')\nTrain_Scores['KNN-recall-score']=recall_score(y_train, y_pred,average='weighted')\nprint(\"Train Scores\")\nprint(Train_Scores)\n\n#testing scores\n\nTest_Scores['KNN-jaccard']=jaccard_similarity_score(y_test, yhat,average='weighted')\nTest_Scores['KNN-f1-score']=f1_score(y_test, yhat, average='weighted')\nTest_Scores['KNN-accuracy-score']=accuracy_score(y_test, yhat) \nTest_Scores['KNN-precision-score']=precision_score(y_test, yhat, average='weighted') \nTest_Scores['KNN-recall-score']=recall_score(y_test, yhat, average='weighted') \nprint(\"Test Scores\")\nprint(Test_Scores)\n\ncm=confusion_matrix(y_test, yhat)\n\n\ncf_matrix=confusion_matrix(y_test, yhat)\n\nside_of_cm=cf_matrix.shape[0]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in\n          zip(group_counts,group_percentages)]\n\nlabels = np.asarray(labels).reshape(side_of_cm,side_of_cm)\n\nfig=plt.figure(figsize=(20,8))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='inferno')\n\n\n\nplt.xlabel(\"True Values\",fontsize=18)\nplt.ylabel(\"Predicted Values\",fontsize=18)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.title(\"Confusion Matrix for k-NN classifier for applied t-SNE\\nMNIST Handwritten Digit Dataset\",fontsize=20)\n\nplt.savefig(\"Confusion Matrix for k-NN classifier for applied t-SNE MNIST Handwritten Digit Dataset_1.png\",dpi=600)\n","c8665d30":"best_k=mean_acc.argmax()+1  #7\nneigh2D = KNeighborsClassifier(n_neighbors = best_k).fit(X_train2D,y_train2D)\nyhat=neigh2D.predict(X_test2D)\ny_pred=neigh2D.predict(X_train2D)","73e43a58":"from copy import deepcopy\nsub2D = deepcopy(sub)\nsub2D[\"Label\"] = yhat.astype('uint8')","0b4d56f8":"sub2D.to_csv('sub2D.csv',index=False)","77f59f3d":"import pickle\n\npickle.dump( neigh2D, open( \"neigh2D.p\", \"wb\" ) )\npickle.dump( neigh3D, open( \"neigh3D.p\", \"wb\" ) )","e92113a1":"sub3D.to_csv('submission.csv',index=False)","29116d02":"### Basically does the same in a different way","2235cc51":"### Customize Matlotlib","729554d6":"### Visualize the Data","fef3f253":"### Store the Feature Column Names (except Label) in Variable","d1f924d0":"### Apply TSNE","5c8700b3":"### 70000 Number of Samples","85176c07":"### Plot the 2-Dimensional & 3-Dimensional Plots","a285d437":"# Standard Imports","63c011a8":"# Save the Files"}}