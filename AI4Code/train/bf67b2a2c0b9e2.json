{"cell_type":{"d2349429":"code","82e3b745":"code","86d4e9dc":"code","3fe64cf8":"code","ca37bbd3":"code","d4cce721":"markdown","74833226":"markdown","c5d7ba99":"markdown","7e35be05":"markdown","3c50855b":"markdown","e49351d1":"markdown","dd2b2cdb":"markdown"},"source":{"d2349429":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.reset_option('^display.', silent=True)\n\n# Load the two datasets\nX_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# Seperate independent and dependent variable\nnum_train = len(X_train)\nnum_test = len(X_test)\ny_train = X_train.SalePrice\nX_train.drop(['SalePrice'], axis=1, inplace=True)\n\n# Merge train and test data to simplify processing\ndf = pd.concat([X_train, X_test], ignore_index=True)\n\n# Rename odd-named columns\ndf = df.rename(columns={\"1stFlrSF\": \"FirstFlrSF\",\n                        \"2ndFlrSF\": \"SecondFlrSF\",\n                       \"3SsnPorch\": \"ThirdSsnPorch\"})\n\n# Shopw 5 samples\ndf.head()","82e3b745":"# Find columns with more than 1000 NaN's and drop them (see above)\ncolumns = [col for col in df.columns if df[col].isnull().sum() > 1000]\ndf = df.drop(columns, axis=1)\n\n# Fill LotFrontage with median\ndf['LotFrontage'].fillna((df['LotFrontage'].mean()), inplace=True)\n\n# No garage values means no year, area or cars\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    df[col] = df[col].fillna(0)\n    \n# No garage info means you don't have one\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    df[col] = df[col].fillna('None')\n\n# Fill no basement\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    df[col] = df[col].fillna('None')\n\n# Fill remaining categorical and numerical cols with None and 0\ncat_columns = df.select_dtypes('object').columns\nnum_columns = [i for i in list(df.columns) if i not in cat_columns]\ndf.update(df[cat_columns].fillna('None'))\ndf.update(df[num_columns].fillna(0))\n\n# Check for missing values \nprint(df.isnull().values.any())","86d4e9dc":"from sklearn.ensemble import IsolationForest\nrng = np.random.RandomState(0)\n\n# Helper function to train and predict IF model for a feature\ndef train_and_predict_if(df, feature):\n    clf = IsolationForest(max_samples=100, random_state=rng)\n    clf.fit(df[[feature]])\n    pred = clf.predict(df[[feature]])\n    scores = clf.decision_function(df[[feature]])\n    stats = pd.DataFrame()\n    stats['val'] = df[feature]\n    stats['score'] = scores\n    stats['outlier'] = pred \n    stats['min'] = df[feature].min()\n    stats['max'] = df[feature].max()\n    stats['mean'] = df[feature].mean()\n    stats['feature'] = [feature] * len(df)\n    return stats\n\n# Helper function to print outliers\ndef print_outliers(df, feature, n):\n    print(feature)\n    print(df[feature].head(n).to_string(), \"\\n\")\n\n# Run through all features and save the outlier scores for each feature\nnum_columns = [i for i in list(df.columns) if i not in list(df.select_dtypes('object').columns) and i not in ['Id']]\nresult = pd.DataFrame()\nfor feature in num_columns:\n    stats = train_and_predict_if(df, feature)\n    result = pd.concat([result, stats])\n    \n# Gather top outliers for each feature\noutliers = {team:grp.drop('feature', axis=1) \n       for team, grp in result.sort_values(by='score').groupby('feature')}\n\n# Print the top 10 outlier samples for a few selected features\nn_outliers = 10\nprint_outliers(outliers, \"LotArea\", n_outliers)\nprint_outliers(outliers, \"YearBuilt\", n_outliers)\nprint_outliers(outliers, \"BsmtUnfSF\", n_outliers)\nprint_outliers(outliers, \"GarageYrBlt\", n_outliers)","3fe64cf8":"# Use clipping to level out the observed outliers in the data\ndf.LotArea = df.LotArea.clip(1300,50000)\ndf.YearBuilt = df.YearBuilt.clip(1880,2010)\ndf.BsmtUnfSF = df.BsmtUnfSF.clip(100,1900)\ndf.GarageYrBlt = df.GarageYrBlt.clip(0,2020)","ca37bbd3":"# Train IsolationForest again, this time after with clipped outliers for 4 features\nresult = pd.DataFrame()\nfor feature in num_columns:\n    stats = train_and_predict_if(df, feature)\n    result = pd.concat([result, stats])\n\n# Gather top outliers for each feature\noutliers = {team:grp.drop('feature', axis=1) \n       for team, grp in result.sort_values(by='score').groupby('feature')}\n\n\n# Print the top 10 outlier samples for the features we chose to clip\nn_outliers = 10\nprint_outliers(outliers, \"LotArea\", n_outliers)\nprint_outliers(outliers, \"YearBuilt\", n_outliers)\nprint_outliers(outliers, \"BsmtUnfSF\", n_outliers)\nprint_outliers(outliers, \"GarageYrBlt\", n_outliers)","d4cce721":"Next step is some fairly simple pre-processing before we are able to run IsolationForest. We drop columns with an excessive number of NaN's (>1000) and fill missing values for all features. In the end we confirm that there is no longer any missing values.","74833226":"Now we re-train the IsolationForest classifier to see if clipping the values has improved the outlier scores. Notice how the outlier scores produced by IsolationForest for the example features has now gone down after we clipped them.","c5d7ba99":"# Conclusion\n\nThis nootebok demonstrated how Isolation Forest can be used to track outliers in a data set. We used the house price data set as an example. Isolation Forest thrives on sub-sampled data and does not need to build the tree from the entire data set. It works well with sub-sampled data. The algorithm executes really fast, since it does not depend on computationally expensive operations like distance or density calculation. The training stage has a linear time complexity with a low constant and hence could be used for any larger data processing scenario.","7e35be05":"# Isolation Forest\n\nThe Isolation Forest (IF) algorithm works best when the trees are not created from the entire data set, but from a sub-sampled data set. This is very different from almost all other techniques where they thrive on data and demands more of it for greater accuracy. Sub-sampling works wonder in this algorithm because normal instances can interfere with the isolation process by being a little closer to the outliers. In this example we set max_samples=100, so that Isolation Forest draws 100 samples to train the base estimator for each feature. <br>\n\nExplanation:\n\n* **clf.fit** fits the base estimator using the max_samples count for the particular feature. <br>\n* **clf.predict** returns -1 if observation is deemed an outlier, otherwise 1 <br>\n* **clf.decision_function** returns the measured outlier score based on fitted model <br>\n\nThe stats dataframe simply holds the original sample values, their scores, if IF thinks they're an outlier or not and some simple statistics for the feature like min, max and median values.","3c50855b":"Let's quickly review the results for these features:\n\n* LotArea has 4 significant outliers (-1) with an outlier score of about -0.33 and values above the 100000. These are far away from the mean value of 10168 for that feature. We see that LotArea goes between 1300 and 215245, so reducing the impact that these four observations (out of a total of has on the variance of this feature could benefit our modelling process later on. <br>\n* YearBuilt is not as diverse as LotArea, as the maximum outlier score is found to be about -0.25. The shows that values at not that far from the mean. For this feature, IF found that the lowest values (~1880) were outliers. <br>\n* BsmtUnfSF mimics YearBuilt, but has significant higher variance. <br>\n* GarageYrBlt obviously has outliers at 0 val according to IF, but these make sense, as these observations are those without any garage at all. There is a majority of houses with a GarageYrBlt and they deviate a lot from the mean.\n\nNext we use the pandas' clipping feature to trim outlier values at input thresholds. It works by assigning a minimum ands maximum value of that particular feature. All observations with a value lower that the min will be assigned the min, and all observations with a value higher than the max will be assigned the max. These are just example values, tweak as you like.","e49351d1":"# Introduction\n\nThis notebook uses the House Price dataset to find and elimentate outliers (anomalies) elegantly **without** removing data.\n\nOutlier detection means identifying something that could not be stated as \u201cnormal\u201d. The definition of \u201cnormal\u201d depends on the phenomenon that is being observed and the properties it bears. In this notebook, we dive deep into an unsupervised outlier detection algorithm called **IsolationForest** and use it on the House Price data set. This algorithm beautifully exploits the characteristics of outliers, keeping it independent of data distributions and making the approach novel.\n\nThe core of the algorithm is to \u201cisolate\u201d outliers by creating decision trees over random attributes. The random partitioning produces noticeable shorter paths for outliers since:\n\n* fewer instances (of outliers) result in smaller partitions\n* distinguishable attribute values are more likely to be separated in early partitioning\n\nHence, when a forest of random trees collectively produces shorter path lengths for some particular points, then they are highly likely to be outliers.\n\n![image.png](attachment:image.png)\n\nThe diagram above shows the number of splits required to isolate a normal point and an outlier. Splits, represented through blue lines, happens at random on a random attribute and in the process building a decision tree. The number of splits determines the level at which the isolation happened and will be used to generate the outlier score.\n\nThe process is repeated multiple times and we note the isolation level for each point\/instance. Once the iterations are over, we generate an outlier score for each point\/instance, suggesting its likeliness to be an outlier. The score is a function of the average level at which the point was isolated. The top samples gathered on the basis of the score are labeled as outliers.\n\nSources:\n\nhttps:\/\/medium.com\/@often_weird\/isolation-forest-algorithm-for-anomaly-detection-f88af2d5518d <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html <br>\n\nWe start by loading the data set and do a bit of pre-processing","dd2b2cdb":"![](https:\/\/images.unsplash.com\/photo-1536303100418-985cb308bb38?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1500&q=80)"}}