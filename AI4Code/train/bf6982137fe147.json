{"cell_type":{"99ebf328":"code","706696d7":"code","238cc689":"code","97e8e1aa":"code","d915e188":"code","4ebae993":"code","78d0d59f":"code","720087c5":"code","44cc4394":"code","592baa5b":"code","2f3afc75":"code","130698ad":"code","f1e71fcf":"code","93466f7c":"code","a322ef5a":"code","dd6c05d8":"code","83109693":"code","49efa63a":"code","f00d7f5b":"code","11676b28":"code","e9340950":"code","30c8c960":"code","94bfae97":"code","ec8c566c":"code","9c31b9a6":"code","44401857":"code","828d5892":"code","7ca75045":"code","042acc22":"code","fc608b9d":"code","86f97fa0":"code","fe2d14e4":"code","2f2f2f49":"code","0c14f140":"code","4be32ad7":"markdown","b117047e":"markdown","077ec384":"markdown","aab3d4b2":"markdown","a07855c6":"markdown","08ef6b6a":"markdown","f91c337f":"markdown","eea41e51":"markdown","2f4fdf1e":"markdown","fda1101a":"markdown","bea7d17a":"markdown","b539bc0a":"markdown","e7e736af":"markdown","5a714000":"markdown","74488cde":"markdown","14026e0d":"markdown","2c8e710d":"markdown","d1142aa7":"markdown","21c05a8d":"markdown","8ef9aec7":"markdown","97240739":"markdown","cc6e786c":"markdown","34af37a5":"markdown","d0518943":"markdown","e18b748a":"markdown","98e6a034":"markdown","61753204":"markdown","2e0ef930":"markdown"},"source":{"99ebf328":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\n\n%matplotlib inline \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nif not os.path.exists(\"..\/input\/dataset.csv\"):\n    os.symlink(\"\/kaggle\/input\/adult-census-income\/adult.csv\", \"..\/input\/dataset.csv\")\n\n    \nprint(\"Setup Complete\")","706696d7":"# Data filepath\ndata_filepath = '..\/input\/dataset.csv'\n\n# Read the data\nadult_census_data = pd.read_csv(data_filepath,\n                                header=0, \n                                sep=',', \n                                na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N\/A', 'N\/a', 'n\/a']\n                               )\n# Print the top 10 entries\nadult_census_data.head(10)","238cc689":"# Describe the data set\nadult_census_data.describe().T","97e8e1aa":"\nprint('Shape of dataset: {}'.format(adult_census_data.shape))\n\n# Data Types of all the variables\nprint('Feature Type: ')\nprint('{}'.format(adult_census_data.dtypes))\n\n# Number of Unique values present in each variable\nadult_census_data.nunique()","d915e188":"# unique values in each columns\nfor attribute in adult_census_data.columns:\n    print(\"{} \".format(attribute))\n    print(\"{}\".format(adult_census_data[attribute].unique()),\"\\n\")","4ebae993":"\n# categorising the variables in two category \" Continuos\" and \"Categorical\"\ncontinuous_attributes = [attribute for attribute in adult_census_data.columns if adult_census_data[attribute].dtypes !='object']  \ncategorical_attributes = [attribute for attribute in adult_census_data.columns if adult_census_data[attribute].dtypes =='object']\n\nprint ( continuous_attributes)\nprint ( categorical_attributes)","78d0d59f":"# Heatmap to check the missing values in the dataset\nplt.figure(figsize=(18,8))\nsns.heatmap(adult_census_data.isnull())","720087c5":"# Number of Unique values present in each variable\n# employee_data.nunique()\n\n# Make a copy of employee dataframe\ndf = adult_census_data.copy()\n\nprint(df[df.isnull().any(axis=1)])\n\n#Creating dataframe with number of missing values\nnull_data_rows = df[df.isnull().any(axis=1)]\nnull_data_columns = df.columns[df.isnull().any()]\nprint(\"number of missing data ::\", null_data_rows.count())\nprint(\"Percentage Of missing data (All tuples)\",(null_data_rows.count()\/df.count())*100 )\n\nmissing_val = pd.DataFrame(df.isnull().sum())\n\n#Reset the index to get row names as columns\nmissing_val = missing_val.reset_index()\n\n#Rename the columns\nmissing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percntage'})\nmissing_val\n\n#Calculate percentage\nmissing_val['Missing_percntage'] = (missing_val['Missing_percntage']\/len(df))*100\n\n\n#Sort the rows according to decreasing missing percentage\nmissing_val = missing_val.sort_values('Missing_percntage', ascending = False).reset_index(drop = True)\n\n#Save output to csv file\nmissing_val.to_csv(\"Missing_percntage.csv\", index = False)\n\n# Return the percentage of missing data in the original dataset\ndef PerOfMissing(d1,d2):# d1--data by droping the NAN value d2--Original data\n    percent_of_missing_data = round( 100 - ((len(d1)\/len(d2))*100), 2)\n    percent_of_missing_data = str(percent_of_missing_data) + '% of data has Missing value'\n    return percent_of_missing_data\n\n# droping all the NAN value from the data and saving the data in data_without_NAN\ndata_without_NAN = adult_census_data.dropna()\nprint (PerOfMissing(data_without_NAN,adult_census_data))\n\nprint(null_data_rows[null_data_columns].head())\nprint(null_data_columns)\nmissing_val","44cc4394":"# get names of columns with missing values\ncols_with_missing = [col for col in adult_census_data.columns\n                     if adult_census_data[col].isnull().any()] \nprint(cols_with_missing)","592baa5b":"# replacing with the MODE of the data\nfrom sklearn_pandas import CategoricalImputer\nimputer = CategoricalImputer()\n\nfor col in cols_with_missing:\n    adult_census_data[col] = imputer.fit_transform(adult_census_data[col])\n\n# get names of columns with missing values\ncols_with_missing = [col for col in adult_census_data.columns\n                     if adult_census_data[col].isnull().any()] \nprint(cols_with_missing)","2f3afc75":"print(\" CATEGORICAL ATTRIBUTES \")\nprint(categorical_attributes)","130698ad":"adult_census_data['workclass'].value_counts()","f1e71fcf":"self_employed = ['Self-emp-not-inc','Self-emp-inc']\ngovt_employees = ['Local-gov','State-gov','Federal-gov']\n\n# replace elements in list.\nadult_census_data['workclass'].replace(to_replace = self_employed ,value = 'self-employed',inplace = True)\nadult_census_data['workclass'].replace(to_replace = govt_employees,value = 'govt-employee',inplace = True)\n\nadult_census_data['workclass'].value_counts()","93466f7c":"elementary_school = ['1st-4th','5th-6th']\nhigh_school = ['7th-8th','10th','9th']\nhigher_secondary_school = ['HS-grad','11th','12th']\n\n\n# replace elements in list.\nadult_census_data['education'].replace(to_replace = elementary_school,value = 'elementary_school',inplace = True)\nadult_census_data['education'].replace(to_replace = high_school,value = 'high_school',inplace = True)\nadult_census_data['education'].replace(to_replace = higher_secondary_school,value = 'HS-grad',inplace = True)\n","a322ef5a":"married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\nseparated = ['Separated','Divorced']\n\n#replace elements in list.\nadult_census_data['marital.status'].replace(to_replace = married ,value = 'Married',inplace = True)\nadult_census_data['marital.status'].replace(to_replace = separated,value = 'Separated',inplace = True)\n\nadult_census_data['marital.status'].value_counts()","dd6c05d8":"self_employed = ['Self-emp-not-inc','Self-emp-inc']\ngovt_employees = ['Local-gov','State-gov','Federal-gov']\n\n#replace elements in list.\nadult_census_data['workclass'].replace(to_replace = self_employed ,value = 'Self_employed',inplace = True)\nadult_census_data['workclass'].replace(to_replace = govt_employees,value = 'Govt_employees',inplace = True)\n\nadult_census_data['workclass'].value_counts()","83109693":"for i, attribute in enumerate(categorical_attributes):\n    \n    # Set the width and height of the figure\n    plt.figure(figsize=(16,6))\n    plt.figure(i)\n    sns.countplot(adult_census_data[attribute])\n    plt.xticks(rotation=90)\n\nplt.show()","49efa63a":"for i, attribute in enumerate(categorical_attributes):\n    \n    if attribute == 'income':\n        continue\n    # Set the width and height of the figure\n    plt.figure(i)\n    plt.figure(figsize=(16,6))\n    table_ct = pd.crosstab(adult_census_data[attribute], adult_census_data['income'])\n    table_ct.plot.bar(stacked=False)\n    plt.legend(title='Salary')\n    plt.xlabel(attribute,fontsize = 14)\n    plt.xticks(rotation=90)\n\n\nplt.show()","f00d7f5b":"adult_census_data['education'].value_counts()\n","11676b28":"# Set the width and height of the figure\nplt.figure(figsize=(16,6))\nplt.title(\"Count of the people in different workclass\")\nsns.countplot(adult_census_data['education'])\nplt.ylabel(\"Count\")","e9340950":"table_workclass = pd.crosstab(adult_census_data['education'], adult_census_data['income'])\nfig = plt.figure(figsize = (17,6))\n\ntable_workclass.plot.bar(stacked=False)\nplt.legend(title='Salary')\nplt.xlabel(\"education\",fontsize = 14)\nplt.show()","30c8c960":"# checking the corellation between all the attributes\nplt.figure(figsize = (12,12))\ncorrelation_matrix = adult_census_data.corr().round(2)\nsns.heatmap(data=correlation_matrix, annot=True)","94bfae97":"del_cols = ['relationship','education.num']\nadult_census_data.drop(labels = del_cols,axis = 1,inplace = True)\ncontinuous_attributes = [ele for ele in continuous_attributes if ele not in del_cols] \ncategorical_attributes =  [ele for ele in categorical_attributes if ele not in del_cols] ","ec8c566c":"# Check for outliers using boxplots\n# Replace that with MEAN\n\nfor i in continuous_attributes:\n    # Getting 75 and 25 percentile of variable \"i\"\n    Q3, Q1 = np.percentile(adult_census_data[i], [75,25])\n    MEAN = adult_census_data[i].mean()\n    \n    # Calculating Interquartile range\n    IQR = Q3 - Q1\n    \n    # Calculating upper extream and lower extream\n    minimum = Q1 - (IQR*1.5)\n    maximum = Q3 + (IQR*1.5)\n    \n    # Replacing all the outliers value to Mean\n    adult_census_data.loc[adult_census_data[i]< minimum,i] = MEAN\n    adult_census_data.loc[adult_census_data[i]> maximum,i] = MEAN","9c31b9a6":"for i in continuous_attributes:\n    adult_census_data[i]=(adult_census_data[i]-min(adult_census_data[i]))\/(max(adult_census_data[i])-min(adult_census_data[i]))\n","44401857":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nscaler = MinMaxScaler()\npd.DataFrame(scaler.fit_transform(adult_census_data[continuous_attributes]),columns = continuous_attributes).head(3)\n\nclass DataFrameSelector(TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n                \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        return X[self.attribute_names]\n    \n    \nclass num_trans(TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        df = pd.DataFrame(X)\n        df.columns = continuous_attributes \n        return df\n    \npipeline = Pipeline([('selector',DataFrameSelector(continuous_attributes)),  \n                     ('scaler',MinMaxScaler()),\n                    ('transform',num_trans())])\n\nnum_df = pipeline.fit_transform(adult_census_data)\nprint(num_df.shape)\nclass dummies(TransformerMixin):\n    def __init__(self,cols):\n        self.cols = cols\n    \n    def fit(self,X,y = None):\n        return self\n    \n    def transform(self,X):\n        df = pd.get_dummies(X)\n        df_new = df[df.columns.difference(cols)] \n#difference returns the original columns, with the columns passed as argument removed.\n        return df_new\n# columns which I don't need after creating dummy variables dataframe\ncols = ['workclass_Govt_employess','education_Some-college',\n        'marital.status_Never-married','occupation_Other-service',\n        'race_Black','sex_Male','income_>50K']\npipeline_cat=Pipeline([('selector',DataFrameSelector(categorical_attributes)),\n                      ('dummies',dummies(cols))])\ncat_df = pipeline_cat.fit_transform(adult_census_data)\ncat_df.shape","828d5892":"cat_df['id'] = pd.Series(range(cat_df.shape[0]))\nnum_df['id'] = pd.Series(range(num_df.shape[0]))","7ca75045":"df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\nprint(f\"Number of observations in final dataset: {df.shape}\")","042acc22":"print(df.columns)\ndf.to_excel(\"\/adult-cencus-raw.xlsx\")","fc608b9d":"# Step 1 cleaning\n# Remove data with missing target value .. the missing y value\ndf.dropna(axis=0, subset=['income_<=50K'], inplace=True)\ny = df['income_<=50K']\ndf.drop(['income_<=50K'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size=0.8, test_size=0.2, random_state=0)","86f97fa0":"# Accuracy score and other parameters\nfrom sklearn.metrics import accuracy_score\ndef Print_Analysis(y_true, y_pred):\n    print (\"Accuracy score \", accuracy_score(y_true, y_pred))","fe2d14e4":"# MultinomialNB\n\ndef MultinomialNB_Classffier(X_train, X_valid, y_train, y_valid):\n    MultinomialNB_clf = MultinomialNB()\n    MultinomialNB_clf.fit(X_train, y_train)\n\n    y_pred = MultinomialNB_clf.predict(X_valid)\n    Print_Analysis(y_valid, y_pred)\n    confusion_matrix(y_valid, y_pred)\n\nMultinomialNB_Classffier(X_train, X_valid, y_train, y_valid)\n","2f2f2f49":"\ndef GausianNB_Classffier(X_train, X_valid, y_train, y_valid):\n    Gausian_clf = GaussianNB()\n    Gausian_clf.fit(X_train, y_train)\n\n    y_pred = Gausian_clf.predict(X_valid)\n    Print_Analysis(y_valid, y_pred)\n    confusion_matrix(y_valid, y_pred)\n\nGausianNB_Classffier(X_train, X_valid, y_train, y_valid)","0c14f140":"MultinomialNB_clf = MultinomialNB()\nMultinomialNB_clf.fit(X_train, y_train)\n\ny_pred = MultinomialNB_clf.predict(X_valid)\nPrint_Analysis(y_valid, y_pred)\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_valid, y_pred))\n\nGausian_clf = GaussianNB()\nGausian_clf.fit(X_train, y_train)\n\ny_pred = Gausian_clf.predict(X_valid)\nPrint_Analysis(y_valid, y_pred)\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_valid, y_pred))","4be32ad7":"### marital.status attribute\n\n'Married-spouse-absent','Married-civ-spouse','Married-AF-spouse' cab be merged as married \n\n'Separated','Divorced' as seperated\n","b117047e":"### Starting with categorical attributes","077ec384":"## My Models","aab3d4b2":"#### Data reduction in work classs\n'Self-emp-not-inc','Self-emp-inc' will be cmbined to self-employed\n\n'Local-gov','State-gov','Federal-gov' to be cmbined to self-employed","a07855c6":"### Check the missing data intot the dataset","08ef6b6a":"## Seperate contineous values and categorical values","f91c337f":"### Lets take care of the Missing values","eea41e51":"1. Conrinous Attributes","2f4fdf1e":"#### WORKCLASS","fda1101a":"### Introduction\n\nExtraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n\nPredict whether income exceeds $50K\/yr based on census data.\n\n#### Attribute Information:\n\n1. **age**: Describes age of the person.\n2. **workclass**: Describes the workclass of a person.Contains the following classes of workclass 'Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked'.\n3. **education**: Eduacation of the person. Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n4. **education-num**: continuous.\n5. **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n6. **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n7. **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n8. **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n9. **sex**: Female, Male.\n10. **capital-gain**: continuous.\n11. **capital-loss**: continuous.\n12. **hours-per-week**: continuous.\n13. **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n14. **fnlwgt**: Description of fnlwgt (final weight)\n    The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\n\n    A single cell estimate of the population 16+ for each state.\n\n    Controls for Hispanic Origin by age and sex.\n\n    Controls by Race, age and sex.\n\n    We use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.","bea7d17a":"## EDUCATION","b539bc0a":"### Decision Boundary","e7e736af":"### Employment status\n\nSelf employed vs govt employee","5a714000":"### Count Plots for categorical attributes","74488cde":"### Import Data into the notebook","14026e0d":"## Now Lets analise the data","2c8e710d":"## Import Packages\n\n#### Initial Setup","d1142aa7":"### Drop unnecessary attributes\n#### Feature Removal\n\nEducation num and education are giving similar information.\n\nRelationship and marital status imply similar information.Hence keeping only one of the two.","21c05a8d":"#### Missing data percentage for different attributes\n","8ef9aec7":"### Volla No Missing values now","97240739":"### Normalize the data","cc6e786c":"### Inspect the basic data statistics\n","34af37a5":"### Investigate More about the data","d0518943":"### The Education\n#### Consider Education system\n\n1. Elementary > sted 1-8th\n2. High-School > std 9-10th\n3. Higher-secondary School> 11th-12","e18b748a":"## Transform the data","98e6a034":"### CHECK OUTLINERS","61753204":"### Check corelation between the attributes","2e0ef930":"# To determine whether a person makes over $50K a year based on the census data."}}