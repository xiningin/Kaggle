{"cell_type":{"0b981ba1":"code","a6a272aa":"code","ca290da2":"code","9b963813":"code","b1eb790b":"code","cee7e4a5":"code","eff581b6":"code","ef767e68":"code","c80dcb8b":"code","6faa3ddf":"code","1b3b7972":"code","c4b0df47":"code","20d2d34a":"code","2c57d2a7":"code","31202a44":"code","52d1eee4":"code","a5df35b9":"code","7a28d1ad":"code","a3961beb":"code","c90ef499":"code","55e7ffb2":"code","29cf3cfb":"markdown","e9f77bd3":"markdown"},"source":{"0b981ba1":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import dct\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.cluster import KMeans\nimport time\nimport keras","a6a272aa":"def get_mnist_reduced(mnist, number_of_examples): #number of examples is 1000 in case of train and 100 in case of test\n    mnist_reduced = pd.DataFrame()\n    for i in range(10):\n        mnist_reduced = pd.concat([mnist_reduced, mnist[(mnist.iloc[:, 0] == i).values][0:number_of_examples]], axis = 0)\n#    return mnist_reduced.iloc[:, 1:].values.reshape(-1, 28, 28), mnist_reduced.iloc[:, 0].values\n    return mnist_reduced.iloc[:, 1:].values, mnist_reduced.iloc[:, 0].values","ca290da2":"def load_data():\n    train_mnist = pd.read_csv('..\/input\/mnist-original\/mnist_train.csv')\n    X_train, y_train = get_mnist_reduced(train_mnist, 1000)\n    test_mnist = pd.read_csv('..\/input\/mnist-original\/mnist_test.csv')\n    X_test, y_test = get_mnist_reduced(test_mnist, 100)\n    return X_train, y_train, X_test, y_test\n\nX_train, y_train, X_test, y_test = load_data()\n   #print(\"Y_train classes values are:\\n\", y_train.value_counts()) #remove .values from get_mnist_reduced to make this works\n    #print(\"Y_test classes values are:\\n\", y_test.value_counts())\nclasses=list(dict.fromkeys(y_test))\n     ","9b963813":"print(classes)","b1eb790b":"def plot_image(img, label = None):\n    plt.axis('off')\n    plt.imshow(img.reshape(28, 28), cmap = 'gray')\n    if label is not None:\n        plt.title(\"number is \" + str(label))\n    plot_image(X_train[1100], y_train[1100])","cee7e4a5":"def plot_confusion_matrix(cm, labels=[],title=\"confusion matrix\"):\n    cmap=plt.cm.Greens\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(labels))\n    plt.xticks(tick_marks, labels, rotation=45)\n    plt.yticks(tick_marks, labels)\n    plt.tight_layout()\n    plt.rcParams['axes.labelsize'] = 16\n    plt.rcParams['axes.labelweight'] = 'bold'\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","eff581b6":"def confusion_matrix(y_test,y_pred,classes,t):\n        conf = np.zeros([len(classes),len(classes)])\n        confnorm = np.zeros([len(classes),len(classes)])\n        for i in range(y_test.shape[0]):\n            j = y_test[i]\n            k = y_pred[i]\n            conf[j,k] = conf[j,k] + 1\n        for i in range(len(classes)):\n            confnorm[i,:] = conf[i,:] \/ np.sum(conf[i,:]) \n            #print(confnorm)  \n        plt.figure()\n        plot_confusion_matrix(confnorm, labels=classes,title=t)","ef767e68":"DCT_features_train = dct(X_train)[:, :120] #get DCT features\nDCT_features_test = dct(X_test)[:, :120] #get DCT features","c80dcb8b":"pca_first_120 = PCA(n_components = 120)#to calculate 120 features\npca_first_120.fit(X_train)\npca_train_120 = pca_first_120.transform(X_train)\npca_test_120 = pca_first_120.transform(X_test)","6faa3ddf":"pca_variance_bigger_than_95 = PCA(n_components = .95)\npca_variance_bigger_than_95.fit(X_train)\npca_train_variance_95 = pca_variance_bigger_than_95.transform(X_train)\npca_test_variance_95 = pca_variance_bigger_than_95.transform(X_test)","1b3b7972":"data_transformations = {'NO': [X_train, X_test],\n                            'DCT': [DCT_features_train, DCT_features_test],\n                              'PCA120': [pca_train_120, pca_test_120], \n                                'PCA95%': [pca_train_variance_95, pca_test_variance_95]}","c4b0df47":"x1, x2 = data_transformations['NO']\nprint(x1.shape, x2.shape)","20d2d34a":"#GMM \ndef GMM_Classifier(n_components,classes_n,Features_Train,class_margin):\n    G=[]\n    for i in range (classes_n):  \n        G_temp=GMM(n_components=n_components,n_init=10,max_iter=5000,covariance_type='full').\\\n        fit(Features_Train[i*class_margin:i*class_margin+class_margin-1])\n        G.append(G_temp.means_)\n    G=np.array(G)\n    return G                                            #return means\n#Predict\ndef predict_label(test_features,label_set,model):\n    Y_predict=np.zeros_like(label_set)\n    for i in range (Y_predict.shape[0]):\n        Y_predict[i]=find_class(test_features[i],model)\n    return Y_predict\n#class decision\ndef find_class(x,y):                                   #finding label\n    min_d=np.ones(y.shape[0])*100000000.0\n    for i in range(y.shape[0]):\n        for j in range(y.shape[1]):\n            temp=np.linalg.norm(x-y[i][j])\n            if temp<min_d[i]:\n                min_d[i]=temp\n    min_class_idx=np.argmin(min_d)\n    return min_class_idx\n\n\n","2c57d2a7":"n_components=[1,2,4];\nn_samples_per_class=np.count_nonzero(y_train == 1)\nn_classes=len(np.unique(y_train))\nfor key in data_transformations.keys():\n    X_train, X_test = data_transformations[key]\n    for n_component in n_components:\n        tic = time.time()\n        classifier=GMM_Classifier(n_component,n_classes,X_train,n_samples_per_class)\n        y_pred=predict_label(X_test,y_test,classifier)\n        toc = time.time()\n        confusion_matrix(y_test,y_pred,classes,\"confusion matrix Trans.:{} - n:{}\".format(key,n_component))\n        print(\"accuracy ={:.2f}% for {} Transformation no. of GMM Components={}\"\n        .format(accuracy_score(y_test,y_pred)*100,key,n_component))\n        print(\"elapsed time =\",round(toc-tic,2),\"sec\")","31202a44":"#this show for non transforming data, it will be shown also below but without linear kernel \n\"\"\"\"\nfor kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n    t0 = time.time()\n    classifier = SVC(kernel = kernel)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    confusion_matrix(y_test,y_pred,classes,\"confusion matrix Trans.:{} - Kernal:{}\".format(key,kernel))\n    print(\"after time: {}s, Accuracy score is:{}, for Kernel:{}\".format(time.time() - t0,\n                                                                accuracy_score(y_train, y_pred), kernel))\n\"\"\"","52d1eee4":"\nfor key in data_transformations.keys():\n    for kernel in ['poly', 'rbf', 'sigmoid']:\n        t0 = time.time()\n        classifier = SVC(kernel = kernel)\n        X_train, X_test = data_transformations[key]\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        confusion_matrix(y_test,y_pred,classes,\"confusion matrix Trans.:{} - Kernal:{}\".format(key,kernel))\n        print(\"after time: {}, Accuracy score is: {}, for Kernel: {} and data transformation: {}\".format(time.time() - t0,\n                                                                accuracy_score(y_test, y_pred), kernel, key))","a5df35b9":"n_digits = len(np.unique(y_test))\nprint(n_digits)\n\n# Initialize KMeans model\nkmeans = KMeans(n_clusters = n_digits)\n\n# Fit the model to the training data\nkmeans.fit(X_train)\nKMeans( init='k-means++',\n       max_iter=300, n_clusters=10,\n        n_init=10, random_state=None,tol=0.0,\n        verbose=0)","7a28d1ad":"def infer_cluster_labels(kmeans, actual_labels):\n    \"\"\"\n    Associates most probable label with each cluster in KMeans model\n    returns: dictionary of clusters assigned to each label\n    \"\"\"\n\n    inferred_labels = {}\n\n    for i in range(kmeans.n_clusters):\n\n        # find index of points in cluster\n        labels = []\n        index = np.where(kmeans.labels_ == i)\n\n        # append actual labels for each point in cluster\n        labels.append(actual_labels[index])\n\n        # determine most common label\n        if len(labels[0]) == 1:\n            counts = np.bincount(labels[0])\n        else:\n            counts = np.bincount(np.squeeze(labels))\n\n        # assign the cluster to a value in the inferred_labels dictionary\n        if np.argmax(counts) in inferred_labels:\n            # append the new number to the existing array at this slot\n            inferred_labels[np.argmax(counts)].append(i)\n        else:\n            # create a new array in this slot\n            inferred_labels[np.argmax(counts)] = [i]\n\n        #print(labels)\n        #print('Cluster: {}, label: {}'.format(i, np.argmax(counts)))\n        \n    return inferred_labels  \n\ndef infer_data_labels(X_labels, cluster_labels):\n    \"\"\"\n    Determines label for each array, depending on the cluster it has been assigned to.\n    returns: predicted labels for each array\n    \"\"\"\n    \n    # empty array of len(X)\n    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n    \n    for i, cluster in enumerate(X_labels):\n        for key, value in cluster_labels.items():\n            if cluster in value:\n                predicted_labels[i] = key\n                \n    return predicted_labels\n# test the infer_cluster_labels() and infer_data_labels() functions\ncluster_labels = infer_cluster_labels(kmeans, y_train)\nX_clusters = kmeans.predict(X_train)\npredicted_labels = infer_data_labels(X_clusters, cluster_labels)\nprint (predicted_labels)\nprint (y_train)","a3961beb":"from sklearn import metrics\ndef calculate_metrics(estimator, data, labels):\n    # Calculate and print metrics\n    print('Number of Clusters: {}'.format(estimator.n_clusters))\n    print('Inertia: {}'.format(estimator.inertia_))\n    print('Homogeneity: {}'.format(metrics.homogeneity_score(labels, estimator.labels_)))","c90ef499":"clusters = [10, 20, 40, 80, 160]\n\n# test different numbers of clusters\nfor n_clusters in clusters:\n    estimator = KMeans(n_clusters = n_clusters)\n    estimator.fit(X_train)\n    \n    # print cluster metrics\n    calculate_metrics(estimator, X_train, y_train)\n    \n    # determine predicted labels\n    cluster_labels = infer_cluster_labels(estimator, y_train)\n    predicted_Y = infer_data_labels(estimator.labels_, cluster_labels)\n    \n    # calculate and print accuracy\n    print('Accuracy: {}\\n'.format(metrics.accuracy_score(y_train, predicted_Y)))","55e7ffb2":"n_clusters=[10,20,40,80,160];\nn_samples_per_class=np.count_nonzero(y_train == 1)\nn_classes=len(np.unique(y_train))\nfor key in data_transformations.keys():\n    X_train, X_test = data_transformations[key]\n    for n_cluster in n_clusters:\n        tic = time.time()\n        estimator = KMeans(n_clusters = n_cluster)\n        estimator.fit(X_train)\n    \n    # print cluster metrics\n        #calculate_metrics(estimator, X_train, y_train)\n    \n    # determine predicted labels\n        cluster_labels = infer_cluster_labels(estimator, y_train)\n        predicted_Y = infer_data_labels(estimator.labels_, cluster_labels)\n        metrics.accuracy_score(y_train, predicted_Y)\n        toc = time.time()\n        confusion_matrix(y_test,predicted_Y,classes,\"confusion matrix Trans.:{} - n:{}\".format(key,n_cluster))\n        #print(\"accuracy ={:.2f}% for {} Transformation no. of kmean_clusters={}\"\n        #.format(accuracy_score(y_test,y_pred)*100,key,n_cluster))\n        print(\"elapsed time =\",round(toc-tic,2),\"sec\")\n        print('Accuracy: {}\\n'.format(metrics.accuracy_score(y_train, predicted_Y)))","29cf3cfb":"# **Confusion Matrix Function only call** confusion_matrix(y_test,y_pred,classes,t)","e9f77bd3":"# **GMM CLASSIFIER** **Functions**\n"}}