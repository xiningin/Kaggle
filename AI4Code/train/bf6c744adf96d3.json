{"cell_type":{"b54b94d9":"code","a02dcea8":"code","afef7526":"code","29568bea":"code","05abd7f0":"code","838e3d33":"code","b143e8ea":"code","972a4f6a":"code","d42da6f6":"code","c79c9bc0":"code","4ff224fb":"code","1afb48b7":"code","75929c5e":"code","e3e0cb5b":"code","37229d0d":"code","c7d5353a":"code","89d37f30":"code","90fee814":"code","7048fbe2":"code","6d5de9ee":"code","86366092":"code","4f32f9a1":"code","c99de6b7":"code","c166e435":"code","97e0af31":"code","a245b9a8":"code","06106859":"code","8ca55b61":"code","49db9a20":"code","1da7df52":"code","66359b21":"markdown","1fb1119b":"markdown","e30fd110":"markdown"},"source":{"b54b94d9":"pip install -U transformers","a02dcea8":"import json\nimport gc\nimport os\nimport time\nimport itertools\nfrom pathlib import Path\n\nimport optuna\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nfrom transformers import (\n    BertConfig,\n    BertModel,\n    BertTokenizer,\n    BertForPreTraining,\n    BertForMaskedLM,\n    DataCollatorForLanguageModeling\n)\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.dataloader import DataLoader","afef7526":"device = torch.device(\n    \"cuda:0\" if torch.cuda.is_available() else \"cpu\")","29568bea":"!ls ..\/input\/augmented-data-for-stanford-covid-vaccine\/48k_augment.csv","05abd7f0":"aug_df = pd.read_csv(\"..\/input\/augmented-data-for-stanford-covid-vaccine\/48k_augment.csv\")","838e3d33":"all_df = aug_df[aug_df.score > 0].reset_index(drop=True)","b143e8ea":"all_df[\"seq_length\"] = all_df[\"sequence\"].map(len)","972a4f6a":"def make_all_sequence(row):\n    length = row[\"seq_length\"]\n    a, b, c = row[\"sequence\"], row[\"structure\"], row[\"predicted_loop_type\"]\n    return [a[i] + b[i] + c[i] for i in range(length)]","d42da6f6":"all_df[\"text\"] = all_df[[\"sequence\", \"structure\", \"predicted_loop_type\", \"seq_length\"]].apply(make_all_sequence, axis=1)","c79c9bc0":"ALL_TOKENS = \"().ACGUBEHIMSX\"","4ff224fb":"tokens1 = [\"A\", \"C\", \"G\", \"U\"]\ntokens2 = [\"(\", \")\", \".\"]\ntokens3 = [\"B\", \"E\", \"H\", \"I\", \"M\", \"S\", \"X\"]","1afb48b7":"with open(\"vocab.txt\", \"w\") as f:\n    f.write(\"[PAD]\\n\")\n    f.write(\"[UNK]\\n\")\n    f.write(\"[CLS]\\n\")\n    f.write(\"[SEP]\\n\")\n    f.write(\"[MASK]\\n\")\n\n    vocab_list = []\n    ix = 0\n    for a in tokens1:\n        for b in tokens2:\n            for c in tokens3:\n                vocab_list.append(a+b+c)\n                ix += 1\n                f.write(a+b+c + \"\\n\")","75929c5e":"train_df, valid_df = train_test_split(all_df, test_size=0.2, shuffle=True, random_state=2020)","e3e0cb5b":"class EarlyStopping:\n    \"\"\"\n    ref: https:\/\/github.com\/Bjarten\/early-stopping-pytorch\n    \"\"\"\n    def __init__(self, patience=2, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.best_model_savepath = None\n\n    def __call__(self, val_loss, model, save_name):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n            self.best_model_savepath = save_name\n\n        elif score < self.best_score:\n            self.counter += 1\n            # print(f'EarlyStopping counter: {self.counter} '\n            #      'out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n            os.remove(self.best_model_savepath)\n            self.best_model_savepath = save_name\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, save_name):\n        if self.verbose:\n            print(f'Validation loss decreased ('\n                  '{self.val_loss_min:.5f} --> {val_loss:.5f}'\n                  ').  Saving model ...')\n            print(\"Save model: {}\".format(save_name))\n        torch.save(model.state_dict(), save_name)\n        self.val_loss_min = val_loss\n    \n    def get_best_filepath(self):\n        return self.best_model_savepath","37229d0d":"class NullScheduler():\n    def __init__(self, lr=0.01):\n        super(NullScheduler, self).__init__()\n        self.lr = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = \"NullScheduler\\n\" \\\n            + \"lr={0:0.5f}\".format(self.lr)\n        return string\n\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef get_learning_rate(optimizer):\n    lr = []\n    for param_group in optimizer.param_groups:\n        lr += [param_group['lr']]\n    assert(len(lr) == 1)\n    lr = lr[0]\n    return lr","c7d5353a":"class RnaDataset(Dataset):\n\n    def __init__(self, tokenizer, df, block_size=256):\n        self.examples = [tokenizer.convert_tokens_to_ids(x) for x in df[\"text\"].tolist()]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> torch.Tensor:\n        return torch.tensor(self.examples[i], dtype=torch.long)","89d37f30":"def bert_pretrain(train_df, valid_df, config):\n    ###################################\n    # Tokenizer\n    ###################################    \n    tokenizer = BertTokenizer(\n        \"vocab.txt\",\n        do_basic_tokenize=False,\n        do_lower_case=False,\n        strip_accents=False,\n        never_split=vocab_list\n    )\n\n    ###################################\n    # Bert Config\n    ###################################\n    bert_config = BertConfig(\n        vocab_size=tokenizer.vocab_size,\n        hidden_size=config[\"bert_hidden_size\"],\n        num_hidden_layers=config[\"bert_num_hidden_layers\"],\n        num_attention_heads=config[\"bert_num_attention_heads\"],\n        intermediate_size=config[\"bert_intermediate_size\"]\n    )\n\n    ###################################\n    # Model\n    ###################################\n    model = BertForMaskedLM(config=bert_config)\n    model.to(device)\n\n    ###################################\n    # Dataset\n    ###################################\n    train_dataset = RnaDataset(tokenizer, train_df)\n    valid_dataset = RnaDataset(tokenizer, valid_df)\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer\n    )\n\n    ###################################\n    # Dataloader\n    ###################################    \n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=config[\"batch_size\"],\n        collate_fn=data_collator,\n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=config[\"batch_size\"],\n        collate_fn=data_collator,\n    )\n    ##################################\n    # early stopping\n    ##################################\n    early_stopping = EarlyStopping(\n        patience=config[\"n_early_stopping_patience\"],\n        verbose=False\n    )\n    \n    \n    ##################\n    # lr scheduler\n    ##################\n    scheduler = NullScheduler(lr=config[\"learning_rate\"])\n\n    #scheduler = CosineAnnealingScheduler(\n    #    eta_min=ca_eta_min,\n    #    eta_max=ca_eta_max,\n    #    cycle=ca_cycle,\n    #    repeat=False\n    #)\n\n    ##################\n    # Optimiizer\n    ##################\n    optimizer = torch.optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=scheduler(0)\n    )\n\n    # dataloaders\n    dataloaders_dict = {\n        \"train\": train_dataloader,\n        \"valid\": valid_dataloader\n    }\n\n\n    ###############################\n    # train epoch loop\n    ###############################\n    # iteration and loss count\n    iteration = 1\n    epoch_train_loss = 0.0\n    epoch_val_loss = 0.0\n    num_epochs = config[\"n_epoch\"]\n    valid_period = 1\n\n    print(f\"Optimizer\\n  {optimizer}\")\n    print(f\"Scheduler\\n  {scheduler}\")\n    print(\"** start training here! **\")\n    print(\"                    |  val   |  train \")\n    print(\"rate    iter  epoch |  loss  |  loss  | time  \")\n    print(\"--------------------------------------------------------------------------------\")\n\n    for epoch in range(num_epochs+1):\n        t_epoch_start = time.time()\n        val_pred_list = []\n        val_true_list = []\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                lr = scheduler(epoch)\n                if lr < 0:\n                    break\n                adjust_learning_rate(optimizer, lr)\n                model.train()\n            else:  # valid\n                if((epoch+1) % valid_period == 0):\n                    model.eval()\n                else:\n                    continue\n            # get batch data loop\n            for iter_i, model_input \\\n                    in enumerate(dataloaders_dict[phase]):\n\n                for k, v in model_input.items():\n                    if isinstance(v, torch.Tensor):\n                        model_input[k] = v.to(device)\n                # zero grad\n                optimizer.zero_grad()\n                # train\n                with torch.set_grad_enabled(phase == 'train'):\n                    loss, pred = model(**model_input)\n\n                    if phase == 'train':\n                        print(f\"\\r{iter_i*config['batch_size']} \/ {len(train_dataset)}\", end='')\n                        loss.backward()  \n                        optimizer.step()\n                        epoch_train_loss += loss.item()\n                        iteration += 1\n                    elif phase == \"valid\":\n                        epoch_val_loss += loss.item()\n\n        t_epoch_finish = time.time()\n        elapsed_time = t_epoch_finish - t_epoch_start\n        lr = get_learning_rate(optimizer)\n\n        epoch_train_loss \/= len(train_dataset)\n        epoch_val_loss \/= len(valid_dataset)\n\n        print(f\"\\r\", end=\"\")\n        print(\n            \"{0:1.5f}  {1:4d}  {2:3d}  | {3:4.4f} {4:4.4f}  {5:1.5f}\"\n            .format(\n                lr,\n                iteration,\n                epoch,\n                epoch_val_loss,\n                epoch_train_loss,\n                elapsed_time),\n        )\n        t_epoch_start = time.time()\n        \n        ######################\n        # early stopping\n        ######################\n        model_save_path = f\".\/checkpoint_epoch{epoch}_val{epoch_val_loss:.4f}.pth\"\n        early_stopping(epoch_val_loss, model, model_save_path)\n        if early_stopping.early_stop:\n            print(\"******** Early stopping ********\")\n            best_score = early_stopping.best_score*(-1)\n            print(f\"Best Score: {best_score}\")\n            # load best model parameter\n            best_model_save_path = early_stopping.get_best_filepath()\n            model.load_state_dict(\n                torch.load(\n                    best_model_save_path,\n                    map_location=lambda storage,loc: storage\n                )\n            )\n            return model, bert_config, best_score\n        epoch_train_loss = 0\n        epoch_val_loss = 0\n    \n    best_score = early_stopping.best_score*(-1)\n    return model, bert_config, best_score","90fee814":"gc.collect()","7048fbe2":"# model, bert_config, best_score = bert_pretrain(train_df, valid_df, config)","6d5de9ee":"config = {\n    \"learning_rate\": 0.001,\n    \"batch_size\": 16,\n    \"n_epoch\": 200,\n    \"n_early_stopping_patience\": 20,\n    \"bert_hidden_size\": 128,\n    \"bert_num_hidden_layers\": 8,\n    \"bert_num_attention_heads\": 4,\n    \"bert_intermediate_size\": 256\n}\n\n\nmodel, bert_config, best_score = bert_pretrain(train_df, valid_df, config)","86366092":"torch.save(model.state_dict(), f\".\/bert_mlm_{best_score}.model\")","4f32f9a1":"model.save_pretrained(f\".\/bert_mlm_{best_score}\")","c99de6b7":"bert_config.to_json_file(\".\/bert_config.json\")","c166e435":"bert = model.bert","97e0af31":"tokenizer = BertTokenizer(\n    \"vocab.txt\",\n    do_basic_tokenize=False,\n    do_lower_case=False,\n    strip_accents=False,\n    never_split=vocab_list\n)\n\ntest_dataset = RnaDataset(tokenizer, valid_df)","a245b9a8":"last_hidden, pool = bert(test_dataset.__getitem__(0).view(1,-1).to(device))","06106859":"last_hidden.shape","8ca55b61":"pretrained_model = BertForMaskedLM.from_pretrained(f\".\/bert_mlm_{best_score}\")","49db9a20":"pretrained_model.to(device)","1da7df52":"pretrained_model(test_dataset.__getitem__(0).view(1,-1).to(device))","66359b21":"## Training","1fb1119b":"## Set Bert Env","e30fd110":"## Split train and test"}}