{"cell_type":{"0b3b29c3":"code","63cc2a7c":"code","515bf779":"code","5883b092":"code","94814d17":"code","eea45f1f":"code","6af59e43":"code","ac4d833f":"code","92abdd5c":"code","12c1eed1":"code","06a6f32e":"code","be21764a":"code","95165a17":"code","8bcb038e":"code","17263d5b":"code","98dfc3fa":"code","2d31feaa":"markdown","18582f1c":"markdown","7b447b6d":"markdown","e189e727":"markdown","39ab2ee8":"markdown"},"source":{"0b3b29c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nfrom imblearn.pipeline import Pipeline\nfrom tpot import TPOTClassifier\nimport os\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom tpot.builtins import ZeroCount, OneHotEncoder\nfrom xgboost import XGBClassifier\nimport catboost\n# consts \nsub_index = 68\n\ntrain_file_path = '..\/input\/saftey_efficay_myopiaTrain.csv'\ntest_file_path = '..\/input\/saftey_efficay_myopiaTest.csv'\nnot_numeric_cols = ['D_L_Sex', 'D_L_Eye','D_L_Dominant_Eye','Pre_L_Contact_Lens','T_L_Laser_Type', 'T_L_Treatment_Type', \n                    'T_L_Cust._Ablation','T_L_Micro','T_L_Head','T_L_Therapeutic_Cont._L.','T_L_Epith._Rep.']\nnot_relevant_features = ['Pre_L_Pupil_Day']\n\ngood_cols=['D_L_Sex', 'D_L_Eye', 'Pre_L_Contact_Lens','T_L_Laser_Type', 'T_L_Treatment_Type','T_L_Cust._Ablation','T_L_Micro','T_L_Head','T_L_Therapeutic_Cont._L.','T_L_Epith._Rep.']\n\n# Any results you write to the current directory are saved as output.","63cc2a7c":"# Read train and test sets\ntrain_df_copy = pd.read_csv(train_file_path)\ntest_df_copy = pd.read_csv(test_file_path)\ntrain_df_copy.replace(to_replace=[' ', 'nan'], value=np.nan, inplace=True)\ntrain_df_copy.dropna(how=\"all\", axis=0, inplace=True)","515bf779":"to_binarize = ['D_L_Eye', 'D_L_Sex', 'D_L_Dominant_Eye', 'Pre_L_Contact_Lens', 'T_L_Laser_Type', 'T_L_Treatment_Type', 'T_L_Cust._Ablation', 'T_L_Micro', 'T_L_Head','T_L_Epith._Rep.', 'T_L_Therapeutic_Cont._L.']\nunknown_types = ['Pre_L_Pupil_Day', 'Pre_L_Cycloplegia_Sph', 'Pre_L_Cycloplegia_Cyl', 'Pre_L_Cycloplegia_Axis', 'T_L_Actual_AblDepth', 'T_L_PTK_mm', 'T_L_PTK_mmm']\nlevel_2_features = ['Pre_L_Pupil_Day','Pre_L_Pupil_Night','Pre_L_Pachymetry','Pre_L_Average_K','Pre_L_Spherical_Equivalence']\n\ntrain_df_copy = pd.get_dummies(train_df_copy, columns=to_binarize)\ntest_df_copy = pd.get_dummies(test_df_copy, columns=to_binarize)\n# Edit when the means of groups are known\ntrain_df_copy['Pre_L_Average_K_new_dist'] = train_df_copy.apply (lambda row: min(\n    abs(row['Pre_L_Average_K']-42.980000), \n    abs(row['Pre_L_Average_K']-44.000000), \n    abs(row['Pre_L_Average_K']-45.050000)),axis=1)\n\ntest_df_copy['Pre_L_Average_K_new_dist'] = test_df_copy.apply (lambda row: min(\n    abs(row['Pre_L_Average_K']-42.980000), \n    abs(row['Pre_L_Average_K']-44.000000), \n    abs(row['Pre_L_Average_K']-45.050000)),axis=1)\n\ntrain_df_copy = train_df_copy.apply(lambda x : x.fillna(x.mean()), axis = 0)\ntest_df_copy = test_df_copy.apply(lambda x : x.fillna(x.mean()), axis = 0)\n\ny = train_df_copy['Class']\n\ncommon_cols = list(set(train_df_copy.columns) & set(test_df_copy.columns))\nx, test_df_copy = train_df_copy[common_cols], test_df_copy[common_cols]\n        \n# remove constant columns in the training set\n# x.drop(not_relevant_features, axis=1, inplace=True)\n# test_df_copy.drop(not_relevant_features, axis=1, inplace=True)\n\nfor m in level_2_features:\n    for n in level_2_features:\n        if n!=m:\n            x['{}_{}'.format(m,n)] = x[m]\/x[n]\n            test_df_copy['{}_{}'.format(m,n)] = test_df_copy[m]\/test_df_copy[n]\n\nx.replace(to_replace=[np.inf, -np.inf], value=np.nan, inplace=True)\ntest_df_copy.replace(to_replace=[np.inf, -np.inf], value=np.nan, inplace=True)\ntest_df_copy = test_df_copy.apply(lambda z : z.fillna(z.mean()), axis = 0)\nx = x.apply(lambda z : z.fillna(z.mean()), axis = 0)\n\n            \n            \ntest_df = test_df_copy\n\nprint(x.shape)\nprint(test_df_copy.shape)","5883b092":"# Trying stacking - Adding classsifications as features\nkf = KFold(n_splits=2)\nfor (train_index, test_index), (m,n) in zip(kf.split(x), [(catboost.CatBoostClassifier(custom_metric='AUC', iterations=100), 'cat0'),\n                                                          (catboost.CatBoostClassifier(custom_metric='AUC', iterations=100), 'rf1'),\n                                                         ]):\n    X_train = x.values[train_index]\n    y_train = y[train_index]\n    rus = RandomUnderSampler(random_state=7, ratio=1)\n    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n    m.fit(X=X_train, y=y_train)\n    x['{}_ans'.format(n)] = [a[1] for a in m.predict_proba(x)]\n    test_df_copy['{}_ans'.format(n)] = [a[1] for a in m.predict_proba(test_df_copy)]","94814d17":"#x = train_df_copy[common_cols + ['Class']]\n#info_1 = x[x['Class'] == 1]\n#info_0 = x[x['Class'] == 0]\n#\n#info_1 = info_1.describe().transpose()\n#info_0 = info_0.describe().transpose()\n#\n#info_join = info_1.join(info_0, lsuffix='_1', rsuffix='_0')\n#info_join['delta_mean'] = abs(info_join['mean_1'] - info_join['mean_0'])\n#info_join.sort_values('delta_mean', ascending=False)","eea45f1f":"params = {\n    'classification__n_estimators': [100, 200, 300,500,550,600,1000,1050, 1100, 1150, 1200],\n    'classification__criterion': ['gini', 'entropy'],\n    'classification__max_depth': [None,4,5,6],\n    'classification__min_samples_split': [0.1, 0.3, 0.5,  0.7, 0.9, 1.0 ],\n}\n\naclf = RandomForestClassifier(n_jobs=-1)\n\n","6af59e43":"params = {\n    'classification__n_estimators': [100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050],\n    'classification__base_estimator__criterion': ['gini', 'entropy'],\n    'classification__base_estimator__max_depth': [None,3,5,8],\n    'classification__base_estimator__min_samples_split': [0.1, 0.3, 0.5, 0.7, 0.9],\n    'classification__algorithm': ['SAMME', 'SAMME.R']\n}\n\naclf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n","ac4d833f":"\nparams = {\n    'classification__n_estimators': [100,150,200,300,400,500,550,600,700,750,800,850,900,1000, 1100, 1200],\n    'classification__max_depth': [3, 5, 6, 8],\n    'classification__subsample': [ 0.85, 0.9, 0.95],\n    #'classification__learning_rate': [0.1, 0.05, 0.01],\n    'classification__colsample_bytree': [0.3, 0.5, 0.7],\n    'classification__gamma': [1,5,10]\n}\n\naclf = xgb.XGBClassifier(silent=False, \n                          scale_pos_weight=1,\n                          learning_rate=0.01,  \n                          colsample_bytree = 0.4,\n                          subsample = 0.95,\n                          objective='binary:logistic', \n                          n_estimators=1000, \n                          reg_alpha = 0.3,\n                          max_depth=3, \n                          gamma=10)\n","92abdd5c":"from matplotlib.legend_handler import HandlerLine2D\n\ndef generate_graph(n_ests):\n    name, n_ests = n_ests\n    train_results = []\n    test_results = []\n    rus = RandomUnderSampler(random_state=7)\n    rus.fit(x, y)\n    X_train_res, y_train_res = rus.fit_resample(x, y)\n    \n    x_train, x_test, y_train, y_test = train_test_split(X_train_res, y_train_res, test_size=0.33)\n    for n in n_ests:\n        model =  GradientBoostingClassifier(n_estimators=n)\n        # Cross validation\n        \n        \n        model.fit(x_train, y_train)\n        train_pred = model.predict(x_train)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        train_results.append(roc_auc)\n        y_pred = model.predict(x_test)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        test_results.append(roc_auc)\n        \n    line1, = plt.plot(n_ests, train_results, 'b', label='Train AUC')\n    line2, = plt.plot(n_ests, test_results, 'r', label='Test AUC')\n    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n    plt.ylabel('AUC score')\n    plt.xlabel(name)\n    plt.show()\n#generate_graph(('n_estimators', [50*(i+1) for i in range(20)]))","12c1eed1":"\n# Does only damage for now...\ndef add_OtherAgg(train, test, features):\n    train['Mean'] = train[features].mean(axis=1)\n    train['Max'] = train[features].max(axis=1)\n    train['Var'] = train[features].var(axis=1)\n    train['Std'] = train[features].std(axis=1)\n\n    test['Mean'] = test[features].mean(axis=1)\n    test['Max'] = test[features].max(axis=1)\n    test['Var'] = test[features].var(axis=1)\n    test['Std'] = test[features].std(axis=1)\n\n    return train, test\n\n# print(x.shape)\n# x, test_df = add_OtherAgg(x, test_df, ['Pre_L_Pachymetry', 'Pre_L_Steep_Axis_max', 'Pre_L_Steep_Axis_min','Pre_L_K_Minimum','Pre_L_Average_K'])\n# print(x.shape)","06a6f32e":"#file = open('tpot_exported_pipeline.py', 'r') \n#for line in file.readlines():\n#    print(line)","be21764a":"model = xgb.XGBClassifier(silent=True, scale_pos_weight=1, learning_rate=0.01,colsample_bytree = 0.3, subsample = 0.95, objective='binary:logistic', n_estimators=850, reg_alpha = 0.5, max_depth=3, gamma=5)\n\nmodel1 = catboost.CatBoostClassifier(custom_metric='AUC',iterations=200 )\nsel_model = SelectFromModel(estimator=model1, max_features=25, threshold=-np.inf, )\n# Cross validation\nrus = RandomUnderSampler(random_state=7)\nrus.fit(x, y)\nX_train_res, y_train_res = rus.fit_resample(x, y)\n#x_train, x_test, y_train, y_test = train_test_split(X_train_res, y_train_res, test_size=0.33)\nsel_model.fit(X_train_res, y_train_res)\nX_reduced = sel_model.transform(X_train_res)\n\nscores = cross_val_score(model, X_reduced, y_train_res, cv=5)\nprint(\"{} : {}\".format(scores, sum(scores)\/len(scores)))","95165a17":"# model\n#model = xgb.XGBClassifier(silent=True, scale_pos_weight=1, learning_rate=0.01,colsample_bytree = 0.3, subsample = 0.95, objective='binary:logistic', n_estimators=850, reg_alpha = 0.5, max_depth=3, gamma=5)\nmodel = catboost.CatBoostClassifier(custom_metric='AUC', iterations=100)\n# Cross validation\nrus = RandomUnderSampler(random_state=7)\nrus.fit(x, y)\nX_train_res, y_train_res = rus.fit_resample(x, y)\nscores = cross_val_score(model, X_train_res, y_train_res, cv=5)\nprint(\"{} : {}\".format(scores, sum(scores)\/len(scores)))\n\n","8bcb038e":"#'classification__colsample_bytree': 0.3, 'classification__gamma': 10, 'classification__max_depth': 8, 'classification__n_estimators': 1100, 'classification__subsample': 0.85\n# get preds for test set\nmodel1 = xgb.XGBClassifier(silent=True, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.3,#0.4,#0.3,\n                      subsample = 0.85,#0.95,\n                      objective='binary:logistic', \n                      n_estimators=1100,#100,#850, \n                      reg_alpha = 0.3,#0.5,\n                      max_depth=8,#5,#3, \n                      gamma=10)#10)#5)\n# 'classification__criterion': 'gini', 'classification__max_depth': None, 'classification__min_samples_split': 0.1, 'classification__n_estimators': 200\nmodel2 = RandomForestClassifier(n_jobs=-1, n_estimators=200, max_depth=None, criterion='gini', min_samples_split=0.1)\n\n# 'classification__algorithm': 'SAMME', 'classification__base_estimator__criterion': 'entropy',\n#'classification__base_estimator__max_depth': 3, 'classification__base_estimator__min_samples_split': 0.9, 'classification__n_estimators': 100\nmodel3 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(min_samples_split=0.9, criterion='entropy', max_depth=3), n_estimators=100, algorithm='SAMME')\nmodel = VotingClassifier(estimators=[('xgb', model1), ('rf', model2), ('ada', model3)], voting='soft')\n\nmodel1 = xgb.XGBClassifier(silent=True, scale_pos_weight=1, learning_rate=0.01,colsample_bytree = 0.3, subsample = 0.95, objective='binary:logistic', n_estimators=850, reg_alpha = 0.5, max_depth=3, gamma=5)\n\nmodel = catboost.CatBoostClassifier(custom_metric='AUC', iterations=200)\nsel_model = SelectFromModel(estimator=model, max_features=35, threshold=-np.inf)\n\nexported_pipeline = make_pipeline(\n    OneHotEncoder(minimum_fraction=0.1, sparse=False, threshold=10),\n    XGBClassifier(learning_rate=0.001, max_depth=7, min_child_weight=15, n_estimators=100, nthread=1, subsample=0.6500000000000001)\n)\nmodel = exported_pipeline\nmodel = xgb.XGBClassifier(silent=True, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.3,#0.4,#0.3,\n                      subsample = 0.85,#0.95,\n                      objective='binary:logistic', \n                      n_estimators=1100,#100,#850, \n                      reg_alpha = 0.3,#0.5,\n                      max_depth=8,#5,#3, \n                      gamma=10)#10)#5)\n\n\nrus = RandomUnderSampler(random_state=7, ratio=1)\nrus.fit(x, y)\nX_train_res, y_train_res = rus.fit_resample(x, y)\nmodel.fit(X_train_res, y_train_res)\n###################\n\n##################\ntest_preds = model.predict_proba(test_df.values)\ntest_preds","17263d5b":"# Make submission file\nsub_name = \"submission\"+str(sub_index)\nfinal_preds = [x[1] for x in test_preds]\nfinal_id = [i for i in range(1, len(test_preds) + 1)]\n\nsub = pd.DataFrame()\nsub['id'] = final_id\nsub['class'] = final_preds\n\nsub.to_csv(sub_name+'.csv', index=False)","98dfc3fa":"# inspect data\n# for col in not_numeric_cols:\n#     print(\"{} value: {} in train set   &   {} in test set\".format(col,train_df_copy[col].unique(),test_df_copy[col].unique()))\ndeb = pd.read_csv(train_file_path)\nprint(good_cols)\ndeb","2d31feaa":"scores = ['roc_auc'] #['precision', 'recall']\n\n\nmodel = Pipeline([\n        ('sampling', RandomUnderSampler(random_state=6)),\n        ('classification', aclf)\n    ])\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(estimator=model, param_grid=params, cv=3,\n                       scoring='%s' % score,\n                      verbose=1, n_jobs=-1)\n    \n    clf.fit(x.values, y.values)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n\n","18582f1c":"# model\n\nmodel = TPOTClassifier(generations=10, population_size=50, cv=3, random_state=42, verbosity=3,n_jobs=-1, scoring='roc_auc', periodic_checkpoint_folder='.')\n\nrus = RandomUnderSampler(random_state=7)\nrus.fit(x, y)\nX_train_res, y_train_res = rus.fit_resample(x, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_res, y_train_res, test_size=0.33, random_state=7)\n\n# fit model on all training data\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\nprint(model.score(X_test, y_test))\n\nmodel.export('tpot_exported_pipeline.py')","7b447b6d":"# Read train and test sets\ntrain_df_copy = pd.read_csv(train_file_path)\ntest_df_copy = pd.read_csv(test_file_path)\n\n# Create copies for debugging n' shit\ntrain_df = train_df_copy\ntest_df = test_df_copy\n\n# clear data and one hot encoding\n    # remove rows with no info\ntrain_df.replace(to_replace=[' ', 'nan'], value=np.nan, inplace=True)\ntrain_df.dropna(how=\"all\", axis=0, inplace=True)\n\n    # remove columns with more than 0.5 nans in the 1 class or if more than 0.95 nans in class 0\ntrain_df_ones = train_df[train_df['Class'] == 1]\ntrain_df_zeros = train_df[train_df['Class'] == 0]\n\n\nfor col in train_df_ones.columns:\n    percent_missing = train_df_ones[col].isnull().sum() \/ len(train_df_ones[col])\n    if percent_missing >= 0.75:\n        not_relevant_features.append(col)\n        \nfor col in train_df_zeros.columns:\n    percent_missing = train_df_zeros[col].isnull().sum() \/ len(train_df_zeros[col])\n    if percent_missing > 0.50:\n        not_relevant_features.append(col)\n        \n\n# train_df_ones = train_df_ones.fillna(train_df_ones.mean())\n# train_df_zeros = train_df_zeros.fillna(train_df_zeros.mean())\n\n# train_df = pd.concat([train_df_ones, train_df_zeros])\n        \ntrain_df.drop(not_relevant_features, inplace=True, axis=1)\n\n    # one hot encoding\ntrain_df = pd.get_dummies(train_df)\ntrain_df = train_df.apply(lambda x : x.fillna(x.mean()), axis = 0)\n\ny = train_df['Class']\n\ntest_df = pd.get_dummies(test_df)    \ntest_df = test_df.apply(lambda x : x.fillna(x.mean()), axis = 0)\n\n# normalize cols\n# cols = train_df.columns\n# for x in cols: \n#     train_df[x]=(train_df[x]-train_df[x].min())\/(train_df[x].max()-train_df[x].min())\n# cols = test_df.columns\n# for x in cols: \n#     test_df[x]=(test_df[x]-test_df[x].min())\/(test_df[x].max()-test_df[x].min())\n\n# train_df['D_L_Age'] = pd.qcut(train_df['D_L_Age'], q=5, labels=[0,1,2,3,4])\n# test_df['D_L_Age'] = pd.qcut(test_df['D_L_Age'], q=5, labels=[0,1,2,3,4])\n\ncommon_cols = list(set(train_df.columns) & set(test_df.columns))\nx, test_df = train_df[common_cols], test_df[common_cols]\n\ncolsToRemove = []\nfor col in common_cols:\n    if x[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nx.drop(colsToRemove, axis=1, inplace=True)\ntest_df.drop(colsToRemove, axis=1, inplace=True)\n\nprint(x.shape)\nprint(test_df.shape)\nprint('ree')","e189e727":"# OLD VERSION\nto_binarize = ['D_L_Eye', 'D_L_Sex', 'D_L_Dominant_Eye', 'Pre_L_Contact_Lens', 'T_L_Laser_Type', 'T_L_Treatment_Type', 'T_L_Cust._Ablation', 'T_L_Micro', 'T_L_Head','T_L_Epith._Rep.', 'T_L_Therapeutic_Cont._L.']\nunknown_types = ['Pre_L_Pupil_Day', 'Pre_L_Cycloplegia_Sph', 'Pre_L_Cycloplegia_Cyl', 'Pre_L_Cycloplegia_Axis', 'T_L_Actual_AblDepth', 'T_L_PTK_mm', 'T_L_PTK_mmm']\ntrain_df_copy = pd.get_dummies(train_df_copy, columns=to_binarize)\ntest_df_copy = pd.get_dummies(test_df_copy, columns=to_binarize)\n# Edit when the means of groups are known\ntrain_df_copy['Pre_L_Average_K_new_dist'] = train_df_copy.apply (lambda row: min(\n    abs(row['Pre_L_Average_K']-42.980000), \n    abs(row['Pre_L_Average_K']-44.000000), \n    abs(row['Pre_L_Average_K']-45.050000)),axis=1)\n\ntest_df_copy['Pre_L_Average_K_new_dist'] = test_df_copy.apply (lambda row: min(\n    abs(row['Pre_L_Average_K']-42.980000), \n    abs(row['Pre_L_Average_K']-44.000000), \n    abs(row['Pre_L_Average_K']-45.050000)),axis=1)\n\ntrain_df_copy = train_df_copy.apply(lambda x : x.fillna(x.mean()), axis = 0)\ntest_df_copy = test_df_copy.apply(lambda x : x.fillna(x.mean()), axis = 0)\n\ny = train_df_copy['Class']\n\ncommon_cols = list(set(train_df_copy.columns) & set(test_df_copy.columns))\nx, test_df_copy = train_df_copy[common_cols], test_df_copy[common_cols]\n        \n# remove constant columns in the training set\n# x.drop(not_relevant_features, axis=1, inplace=True)\n# test_df_copy.drop(not_relevant_features, axis=1, inplace=True)\n\ntest_df = test_df_copy\n\nprint(x.shape)\nprint(test_df_copy.shape)","39ab2ee8":"# IDEAS\n* replace  missing values with mean of THE CLASS - causes overfitting\n\n### itay\n* fillna with mean is keeping the dtype of the column (ints are not becoming floats)"}}