{"cell_type":{"beb0a58f":"code","7aec4b2f":"code","c2f0a8b8":"markdown"},"source":{"beb0a58f":"# libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport numpy as np\nimport time\nimport lightgbm as lgb\nimport math\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GroupKFold, RepeatedStratifiedKFold, cross_validate, StratifiedShuffleSplit\nfrom sklearn import metrics\n\n# DATA LOADING\nfrom sklearn.neural_network import MLPClassifier\n\npath = '\/kaggle\/input\/stock-market-prediction\/'\npathOutput = '.\/'\ncompletoTrain='dataset_train_validation.csv'\ncompletoTest='dataset_test.csv'\n\n# INPUT\ntrain = pd.read_csv(os.path.join(path, completoTrain))\ntest = pd.read_csv(os.path.join(path, completoTest))\n\n# Se separan los resultados reales con los que comparar\ncolumns =  [col for col in test.columns if col not in ['company', 'age', 'market', 'TARGET']]\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n####################### ELIMINACION de filas con null en la columna TARGET\ndef filter_rows_by_values(df, col, values):\n    return df[~df[col].isin(values)]\n\ntrain = filter_rows_by_values(train, \"TARGET\", [\"null\"])\n\n########################## NUEVAS FEATURES ###########################\n######33###### RSI ###################\ndef relative_strength_idx(df, n=14):\n    close = df['close']\n    delta = close.diff()\n    delta = delta[1:]\n    pricesUp = delta.copy()\n    pricesDown = delta.copy()\n    pricesUp[pricesUp < 0] = 0\n    pricesDown[pricesDown > 0] = 0\n    rollUp = pricesUp.rolling(n).mean()\n    rollDown = pricesDown.abs().rolling(n).mean()\n    rs = rollUp \/ rollDown\n    rsi = 100.0 - (100.0 \/ (1.0 + rs))\n    return rsi\n\n################################# NUEVAS FEATURES TRAIN\ntrain['close_lag'] = train['close'].shift(1)\ntrain['RSI'] = relative_strength_idx(train).fillna(0)\ntrain = train.fillna(0)\n\n################################## NUEVAS FEATURES TEST\n# all the same for the test data\ntest['close_lag'] = test['close'].shift(1)\ntest['RSI'] = relative_strength_idx(test).fillna(0)\ntest = test.fillna(0)\n\n#########################################\n# Se fraccionan los datos de train en: train + validaci\u00f3n\nfraccion_train = 0.7  # Fracci\u00f3n de datos usada para entrenar\nfraccion_valid = 1.00 - fraccion_train\ntrain_aleatorio = train.sample(frac=1)\ntrain = train_aleatorio.iloc[:int(fraccion_train * len(train)), :]\nvalidacion = train_aleatorio.iloc[int(fraccion_train * len(train)):, :]\n\n################# Se separa en features y target\ntrain_X = train[columns]\ntrain_y = train['TARGET']\nvalid_X = validacion[columns]\nvalid_y = validacion['TARGET']\n\n\n############################## OPTUNA\n# Source: https:\/\/themachinelearners.com\/optuna-busqueda-hiperparametros\/\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.datasets import make_classification\n\nX = train_X\ny = train_y\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nbaseline2=DecisionTreeClassifier(random_state=2)\nbaseline2.fit(train_X,train_y)\nbaseline2.score(valid_X,valid_y)\n\nstart_time = time.time()\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    # Define the search space\n    \n    max_depth = trial.suggest_int('max_depth', 5, train_X.shape[1])\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n    min_impurity_decrease  = trial.suggest_float('min_impurity_decrease', 0, 1)\n    \n    model = DecisionTreeClassifier(random_state=2,\n        max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n                                   min_impurity_decrease=min_impurity_decrease\n      )\n    \n    score = cross_val_score(model, train_X, train_y, scoring='recall',cv=10).mean()\n    return score\nstudy_DT = optuna.create_study(study_name=\"dectree_artf\",\n                            direction=\"maximize\",\n                            sampler=TPESampler())\n\n# PONER UN N\u00daMERO GRANDE DE INTENTOS DE OPTIMIZACI\u00d3N\nstudy_DT.optimize(objective, n_trials=300)\nprint(\"--- {} minutos para el tuneo---\".format((time.time() - start_time)\/60))\n\n# An\u00e1lisis de resultados\nbest_params=study_DT.best_params\nprint(best_params)\n\n# Se aplican los par\u00e1metros \u00f3ptimos\nmax_depth = best_params[\"max_depth\"]\nmin_samples_split = best_params[\"min_samples_split\"]\nmin_samples_leaf = best_params[\"min_samples_leaf\"]\nmin_impurity_decrease = best_params[\"min_impurity_decrease\"]\nmodelo2=DecisionTreeClassifier(max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf\n                               ,min_impurity_decrease=min_impurity_decrease)\nmodelo2.fit(train_X, train_y)\nmodelo2.score(valid_X,valid_y)\n\n\nmodel = modelo2\n\n#####################################################\n\n##################### VALIDACI\u00d3N ###################\nprint(\"COMIENZO DE VALIDACI\u00d3N\")\nscore = metrics.mean_absolute_error(valid_y, model.predict(valid_X))\nprint('CV score: {0:.4f}.'.format(score))\nprint(\"FIN DE VALIDACI\u00d3N\")\n###############################################\n","7aec4b2f":"# PREDICCI\u00d3N independiente\n\n# Eliminaci\u00f3n de filas a null\ntest = filter_rows_by_values(test, \"TARGET\", [\"null\"])\n\nsubmission = test[columns]\nsolucion = test['TARGET']\n\n# S\u00f3lo aplicable si hay sigmoid en la \u00faltima capa\nprediccion = (model.predict(submission) > 0.5).astype(\"int32\")\nsubmission['TARGET']=prediccion\n\n# Se a\u00f1ade la empresa en la primera columna y se guarda en Excel\nsubmission=submission.join(test['company'])\nsubmission.set_index(submission.pop('company'), inplace=True)\nsubmission.reset_index(inplace=True)\nsubmission.to_csv(os.path.join(pathOutput, 'Bolsa_DL_submission.csv'), index=False)\n\n# PRECISION independiente\na=solucion\nb=prediccion\nTP=sum(1 for x,y in zip(a,b) if (x == y and y == 1))\nTPandFP=sum(b)\nprecision= TP \/ TPandFP\nprint(\"TP: \", TP)\nprint(\"TP + FP: \", TPandFP)\nprint(\"---->>>>>> PRECISION (TP\/(TP+FP)) FOR TEST DATASET: {0:.2f}% <<<<<<------\".format(precision * 100))\n\n# Tasa de desbalanceo\nift_mayoritaria = test[test.TARGET == False]\nift_minoritaria = test[test.TARGET == True]\ntasaDesbalanceo = round(ift_mayoritaria.shape[0] \/ ift_minoritaria.shape[0], 2)\nprint(\"Tasa de desbalanceo = \" + str(ift_mayoritaria.shape[0]) + \"\/\" + str(\n        ift_minoritaria.shape[0]) + \" = \" + str(tasaDesbalanceo))\n\nprint(\"Tasa de mejora de precisi\u00f3n respecto a random: \",\n              round(precision \/ (1\/(1+tasaDesbalanceo)), 2))\n\nfrom sklearn.metrics import classification_report\nprint(\"Informe de metricas:\")\nprint(classification_report(a, b))\n\nprint(\"END\")","c2f0a8b8":"\n\n# **Independent prediction:**\n\n"}}