{"cell_type":{"7ca99bd8":"code","8b14351e":"code","5a38c23a":"code","c8fc955f":"code","f96a0d29":"code","4c107faa":"code","6c81b45a":"code","8871bc20":"code","4d110d8c":"code","8edbf5f2":"code","515657db":"code","c9272450":"code","8c5ee05e":"code","325d3129":"code","9001ac4c":"markdown","5be6ab8d":"markdown","45642f72":"markdown","2583893d":"markdown","70bbed61":"markdown","8d94348e":"markdown","6170bf5f":"markdown","e9989751":"markdown","62ee47fe":"markdown","87ef8a40":"markdown","a91a704a":"markdown","714a431c":"markdown","d2e9ab3d":"markdown","3922fe43":"markdown","0ab842d6":"markdown","650872be":"markdown","ec5117f5":"markdown","406eb3be":"markdown","363eb248":"markdown","78247fc2":"markdown"},"source":{"7ca99bd8":"# Install necessary packages one by one\n!cat \/kaggle\/input\/odfkosteruw1\/koster-uw-workshop-1\/requirements.txt | xargs -n 1 pip install","8b14351e":"from pathlib import Path\nkaggle_path = Path('\/kaggle\/input\/odfkosteruw1\/koster-uw-workshop-1')","5a38c23a":"# Imports\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np","c8fc955f":"%pylab inline \nimport cv2\nfrom IPython.display import clear_output\nfrom google.colab.patches import cv2_imshow","f96a0d29":"# video_file = Path(kaggle_path, \"videos\/TjarnoROV1-990813_3-1122.mov\")\n# video = cv2.VideoCapture(video_file)\n\n# try:\n#     while True:\n#         (grabbed, frame) = video.read()\n\n#         if not grabbed:\n#             break\n\n#         # The important part - Correct BGR to RGB channel\n#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n#         axis('off')\n#         # Title of the window\n#         title(\"Input Stream\")\n#         # Display the frame\n#         imshow(frame)\n#         show()\n#         # Display the frame until new frame is available\n#         clear_output(wait=True)\n    \n#     cv2.destroyAllWindows()\n#     video.release()\n    \n# except KeyboardInterrupt: video.release()\n","4c107faa":"# video_file = Path(kaggle_path, \"videos\/TjarnoROV1-990813_3-1122.mov\")\n# video = cv2.VideoCapture(video_file)\n\n# try:\n#     while True:\n#         (grabbed, frame) = video.read()\n\n#         if not grabbed:\n#             break\n\n#         blur = cv2.GaussianBlur(frame, (21, 21), 0)\n#         hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n\n#         lower = np.array([0,120,70])\n#         upper = np.array([180,255,255])\n#         lower = np.array(lower, dtype=\"uint8\")\n#         upper = np.array(upper, dtype=\"uint8\")\n#         mask = cv2.inRange(hsv, lower, upper)\n\n#         frame = cv2.bitwise_and(frame, hsv, mask=mask)\n#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n\n#         axis('off')\n#         # Title of the window\n#         title(\"Input Stream\")\n#         # Display the frame\n#         imshow(frame)\n#         show()\n#         # Display the frame until new frame is available\n#         clear_output(wait=True)\n    \n#     cv2.destroyAllWindows()\n#     video.release()\n    \n# except KeyboardInterrupt: video.release()\n","6c81b45a":"def clearImage(image):\n    # Convert the image from BGR to gray\n    dark_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n\n    channels = cv2.split(image)\n\n    # Get the maximum value of each channel\n    # and get the dark channel of each image\n    # record the maximum value of each channel\n    a_max_dst = [ float(\"-inf\") ]*len(channels)\n    for idx in range(len(channels)):\n        a_max_dst[idx] = channels[idx].max()\n\n    dark_image = cv2.min(channels[0],cv2.min(channels[1],channels[2]))\n\n    # Gaussian filtering the dark channel\n    dark_image = cv2.GaussianBlur(dark_image,(25,25),0)\n\n    image_t = (255.-0.95*dark_image)\/255.\n    image_t = cv2.max(image_t,0.5)\n\n    # Calculate t(x) and get the clear image\n    for idx in range(len(channels)):\n        channels[idx] = cv2.max(cv2.add(cv2.subtract(channels[idx].astype(np.float32), int(a_max_dst[idx]))\/image_t,\n                                                        int(a_max_dst[idx])),0.0)\/int(a_max_dst[idx])*255\n        channels[idx] = channels[idx].astype(np.uint8)\n\n    return cv2.merge(channels)","8871bc20":"# video_file = Path(kaggle_path, \"videos\/TjarnoROV1-990813_3-1122.mov\")\n# video = cv2.VideoCapture(video_file)\n\n# try:\n#     while True:\n#         (grabbed, frame) = video.read()\n\n#         if not grabbed:\n#             break\n\n#         # The important part - Correct BGR to RGB channel\n#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#         n_frame = clearImage(frame)\n#         axis('off')\n#         # Title of the window\n#         title(\"Input Stream\")\n#         # Display the frame\n#         imshow(n_frame)\n#         show()\n#         # Display the frame until new frame is available\n#         clear_output(wait=True)\n    \n#     cv2.destroyAllWindows()\n#     video.release()\n    \n# except KeyboardInterrupt: video.release()\n","4d110d8c":"# Reference in C++: \n# https:\/\/answers.opencv.org\/question\/26280\/background-color-similar-to-object-color-how-isolate-it\/\n \n#video_file = Path(kaggle_path, \"videos\/TjarnoROV1-990813_3-1122.mov\")\n# video_file = Path(kaggle_path, \"videos\/000114 TMBL-ROV 2000 Sa\u0308ckenrevet EJ numrerade band_1440.mp4\")\n# video = cv2.VideoCapture(video_file)\n\n# blur_size = 20\n# grid_size = 500\n\n# try:\n#     while True:\n#         (grabbed, frame) = video.read()\n\n#         if frame is None: break\n\n#         # Reduce the size that we observe to reduce noise from corners of the frame\n#         origin = frame[100:500, 100:500]\n\n#         if not grabbed:\n#             break\n\n#         # Clean up our image\n#         new_img = clearImage(frame)\n\n#         new_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n#         new_img = cv2.split(frame)[2]\n\n#         # Cut to the most important segment\n#         new_img = new_img[100:500, 100:500]\n\n#         blur_size += (1 - blur_size % 2)\n\n#         blur = cv2.GaussianBlur(new_img, (blur_size, blur_size), 0)\n\n#         # equalise the histogram\n#         equal = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(5,5)).apply(blur)\n\n#         grid_size += (1 - grid_size % 2)\n\n#         # create a binary mask using an adaptive thresholding technique\n#         binimage = cv2.adaptiveThreshold(equal, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n#                                          cv2.THRESH_BINARY, grid_size, -30)\n\n#         #cv2.imshow(\"bin\", binimage)\n\n#         contours, _ = cv2.findContours(binimage.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n#         # Cycle through contours and add area to array\n#         areas = []\n#         for c in contours:\n#             areas.append(cv2.contourArea(c))\n\n#         # Sort array of areas by size\n#         try:\n#             largest = np.argmax(areas)\n#         except:\n#             largest = None\n\n#         if largest is not None:\n#             fishMask = np.zeros(new_img.shape, dtype = np.uint8)\n#             # Choose our largest contour to be the object we wish to detect\n#             fishContours = contours[largest]\n#             cv2.polylines(origin,  [fishContours],  True,  (0, 0, 255),  2)\n#             # Draw these contours we detect\n#             cv2.drawContours(fishMask, contours, -1, 255, -1);\n#             #cv2.imshow(\"fish_mask\", fishMask)\n\n            \n#         origin = cv2.cvtColor(origin, cv2.COLOR_BGR2RGB)\n        \n#         axis('off')\n#         # Title of the window\n#         title(\"Input Stream\")\n#         # Display the frame\n#         imshow(origin)\n#         show()\n#         # Display the frame until new frame is available\n#         clear_output(wait=True)\n\n#     cv2.destroyAllWindows()\n#     video.release()\n    \n# except KeyboardInterrupt: video.release()\n    \n            \n \n","8edbf5f2":"### Save frames as images\n# import cv2\n# import numpy as np\n# import scipy.io as sio\n \n# video_file = Path(kaggle_path, \"videos\/TjarnoROV1-990813_3-1122.mov\")\n# video = cv2.VideoCapture(video_file)\n\n# total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n\n# frame_id = 0\n# i = 0\n# while True:\n#     (grabbed, frame) = video.read()\n    \n#     if not grabbed:\n#         break\n \n#     new_img = clearImage(frame)\n#     new_img = cv2.resize(new_img, (416, 416))\n#     assert(new_img.shape == (416, 416, 3))\n    \n#     frame_id += 1\n    \n#     if frame_id % 100 == 0:\n#         print(\"Saved\", frame_id)\n#         cv2.imwrite(Path(kaggle_path, \"img\/odf_video_frames\/{:s}\".format(str(i)+'.jpg'), new_img))\n#         i += 1\n#     if cv2.waitKey(1) & 0xFF == ord('q'):\n#         break\n# print('Saved images')\n# cv2.destroyAllWindows()\n# video.release()","515657db":"import glob, os\n\ndataset_path = Path(kaggle_path, \"img\/odf_video_frames\")\n\n# Percentage of images to be used for the test set\npercentage_test = 10;\n\n# Create and\/or truncate train.txt and test.txt\nfile_train = open('.\/Data\/img\/train.txt', 'w')  \nfile_test = open('.\/Data\/img\/test.txt', 'w')\n\n# Populate train.txt and test.txt\ncounter = 1\nindex_test = int(percentage_test \/ 100 * len(os.listdir(dataset_path)))\nfor pathAndFilename in glob.iglob(os.path.join(dataset_path, \"*.jpg\")):  \n    title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n\n    if counter == index_test+1:\n        counter = 1\n        file_test.write(os.path.basename(title) + '.jpg' + \"\\n\")\n    else:\n        file_train.write(os.path.basename(title) + '.jpg' + \"\\n\")\n        counter = counter + 1","c9272450":"# !labelImg .\/Data\/img\/odf_video_frames\/ .\/Data\/img\/odf_classes.txt","8c5ee05e":"lines = []\nfor line in open(Path(kaggle_path, \"logs\/train_log_example.log\")):\n    if \"avg\" in line:\n        lines.append(line)\n\niterations = []\navg_loss = []\n\nprint('Retrieving data and plotting training loss graph...')\nfor i in range(len(lines)):\n    lineParts = lines[i].split(',')\n    iterations.append(int(lineParts[0].split(':')[0]))\n    avg_loss.append(float(lineParts[1].split()[0]))\n\nfig = plt.figure(figsize=(15,10))\nfor i in range(0, len(lines)):\n    plt.plot(iterations[i:i+2], avg_loss[i:i+2], 'r.-')\n\nplt.xlabel('Batch Number')\nplt.ylabel('Avg Loss')\nfig.savefig('training_loss_plot.png', dpi=1000)\n\nprint('Done! Plot saved as training_loss_plot.png')","325d3129":"## Visualize predictions using OpenCV\n\nimport argparse\nimport sys\nimport numpy as np\nimport os.path\n\n# Initialize the parameters\nconfThreshold = 0.1 #Confidence threshold\nnmsThreshold = 0.4 #Non-maximum suppression threshold\n\ninpWidth = 416  #608     #Width of network's input image\ninpHeight = 416 #608     #Height of network's input image\n        \n# Load names of classes\nclassesFile = Path(kaggle_path, \"models\/sweden_yolo\/odf_classes.names\");\n\nclasses = None\nwith open(classesFile, 'rt') as f:\n    classes = f.read().rstrip('\\n').split('\\n')\n\n# Give the configuration and weight files for the model and load the network using them.\n\nmodelConfiguration = Path(kaggle_path, \"models\/sweden_yolo\/sweden_yolo.cfg\");\nmodelWeights = Path(kaggle_path, \"models\/sweden_yolo\/sweden_yolo.backup\");\n\nnet = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\nnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n\n# Get the names of the output layers\ndef getOutputsNames(net):\n    # Get the names of all the layers in the network\n    layersNames = net.getLayerNames()\n    # Get the names of the output layers, i.e. the layers with unconnected outputs\n    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n\n# Draw the predicted bounding box\ndef drawPred(classId, conf, left, top, right, bottom):\n    # Draw a bounding box.\n    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 3)\n\n    label = '%.2f' % conf\n        \n    # Get the label for the class name and its confidence\n    if classes:\n        assert(classId < len(classes))\n        label = '%s:%s' % (classes[classId], label)\n\n    #Display the label at the top of the bounding box\n    labelSize, baseLine = cv2.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    top = max(top, labelSize[1])\n    cv2.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), \n                 (0, 0, 255), cv2.FILLED)\n    \n    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 2)\n\n# Remove the bounding boxes with low confidence using non-maxima suppression\ndef postprocess(frame, outs):\n    frameHeight = frame.shape[0]\n    frameWidth = frame.shape[1]\n\n    classIds = []\n    confidences = []\n    boxes = []\n    # Scan through all the bounding boxes output from the network and keep only the\n    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n    classIds = []\n    confidences = []\n    boxes = []\n    for out in outs:\n        print(\"out.shape : \", out.shape)\n        for detection in out:\n            #if detection[4]>0.001:\n            scores = detection[5:]\n            classId = np.argmax(scores)\n            #if scores[classId]>confThreshold:\n            confidence = scores[classId]\n            if detection[4]>confThreshold:\n                print(detection[4], \" - \", scores[classId], \" - th : \", confThreshold)\n                print(detection)\n            if confidence > confThreshold:\n                center_x = int(detection[0] * frameWidth)\n                center_y = int(detection[1] * frameHeight)\n                width = int(detection[2] * frameWidth)\n                height = int(detection[3] * frameHeight)\n                left = int(center_x - width \/ 2)\n                top = int(center_y - height \/ 2)\n                classIds.append(classId)\n                confidences.append(float(confidence))\n                boxes.append([left, top, width, height])\n\n    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n    # lower confidences.\n    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n    for i in indices:\n        i = i[0]\n        box = boxes[i]\n        left = box[0]\n        top = box[1]\n        width = box[2]\n        height = box[3]\n        drawPred(classIds[i], confidences[i], left, top, left + width, top + height)\n\n# Process inputs\nwinName = 'ODF - Sweden Demo'\ncv2.namedWindow(winName, cv2.WINDOW_NORMAL)\n\noutputFile = Path(kaggle_path, \"models\/sweden_yolo\/yolo_out_py.avi\");\n\nvideo_path = Path(kaggle_path, \"models\/sweden_yolo\/crabs.mov\")\ncap = cv2.VideoCapture(video_path)\nvid_writer = cv2.VideoWriter(outputFile, cv2.VideoWriter_fourcc('M','J','P','G'),\n                            30, (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\n\ncount = 0\n    \nwhile cv2.waitKey(1) < 0:\n    \n    # get frame from the video\n    hasFrame, frame = cap.read()\n    if frame is None: break\n    #frame = frame[100:516, 100:516]\n    frame = clearImage(frame)\n    frame = cv2.resize(frame, (inpWidth, inpHeight))\n    \n    # Stop the program if reached end of video\n    if not hasFrame:\n        print(\"Done processing !!!\")\n        print(\"Output file is stored as \", outputFile)\n        cv2.waitKey(3000)\n        break\n\n    # Create a 4D blob from a frame.\n    blob = cv2.dnn.blobFromImage(frame, 1\/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n    \n    # Sets the input to the network\n    net.setInput(blob)\n\n    # Runs the forward pass to get output of the output layers\n    outs = net.forward(getOutputsNames(net))\n\n    # Remove the bounding boxes with low confidence\n    postprocess(frame, outs)\n\n    # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)\n    t, _ = net.getPerfProfile()\n    label = 'Inference time: %.2f ms' % (t * 1000.0 \/ cv2.getTickFrequency())\n    vid_writer.write(frame.astype(np.uint8))\n    \n    count += 30 # i.e. at 30 fps, this advances one second\n    cap.set(1, count)\n    #cv2.imshow(winName, frame)\n    ","9001ac4c":"[Google Colab Workbook](https:\/\/colab.research.google.com\/drive\/1lZmojs-vsarIiSoicY1QKcpB1Bp0Co3O)","5be6ab8d":"<div class=\"alert alert-success\" role=\"alert\">\n  <h3 class=\"display-4\">OpenCV is a highly-optimised open-source computer vision library. It is built in C\/C++ with binders for Python<\/h3> \n<\/div>","45642f72":"<div class=\"alert alert-info\" role=\"alert\">\n    <h4 class=\"display-4\">First convert all the video frames into images so we can label them<\/h4>\n<\/div>","2583893d":"<div class=\"jumbotron jumbotron-fluid\">\n  <div class=\"container\">\n    <h1 class=\"display-4\">ROV underwater species detection<\/h1>\n    <p class=\"lead\">A short notebook introducing techniques and common challenges for underwater species detection\n   <\/p>\n  <\/div>\n<\/div>","70bbed61":"![](https:\/\/miro.medium.com\/max\/1200\/0*3A8U0Hm5IKmRa6hu.png)","8d94348e":"<div class=\"alert alert-warning\" role=\"alert\">\n    <h3 class=\"display-4\">Note: Visualising using OpenCV does not work in the cloud - instead you can open the output file once it has been saved<\/h3>\n<\/div>\n\n","6170bf5f":"<div class=\"alert alert-success\" role=\"alert\">\n    <h3 class=\"display-4\">Let's see what that looks like now<\/h3>\n<\/div>","e9989751":"<div class=\"alert alert-info\" role=\"alert\">\n    <h4 class=\"display-4\">Train test split<\/h4>\n<\/div>","62ee47fe":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">Detect a colour<\/h3>\n<\/div>","87ef8a40":"<div class=\"alert alert-danger\" role=\"alert\">\n    <h3 class=\"display-4\">Problem 3: A binary mask is not sufficient if we want to detect multiple objects in a frame, so what can we do?<\/h3>\n<\/div>","a91a704a":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">Problem 1: Distortion of colour between foreground and background objects<\/h3>\n<\/div>\n","714a431c":"**PLEASE NOTE: All the code blocks involving video have been commented out to speed up commits, so please use CTRL + A and then CTRL + \/ to uncomment them when inside a code block before running **","d2e9ab3d":"![](https:\/\/miro.medium.com\/max\/5856\/1*Hz6t-tokG1niaUfmcysusw.jpeg)","3922fe43":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">Play a video<\/h3>\n<\/div>","0ab842d6":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">Annotation!<\/h3>\n<\/div>\n\n<div class=\"alert alert-warning\" role=\"alert\">\n    <h3 class=\"display-4\">Note that annotation with Labelimg cannot be done in the Cloud. Please run this on your local machine if you need to annotate data.<\/h3>\n<\/div>","650872be":"<div class=\"alert alert-info\" role=\"alert\">\n    <h2 class=\"display-4\">Model Evaluation<\/h2>\n<\/div>","ec5117f5":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">YOLO V3<\/h3>\n<\/div>","406eb3be":"<div class=\"alert alert-info\" role=\"alert\">\n    <h3 class=\"display-4\">Problem 2: How do we draw contours that represent the objects we detect with a suitable mask?<\/h3>\n<\/div>","363eb248":"<div class=\"alert alert-success\" role=\"alert\">\n    <h2 class=\"display-4\">Now, to the cloud for training...<\/h2>\n<\/div>","78247fc2":"![](https:\/\/media1.tenor.com\/images\/79e142098ed62e0486d81028283660b7\/tenor.gif?itemid=12081780)"}}