{"cell_type":{"cde0497a":"code","aa324179":"code","6a60b052":"code","4badceda":"code","662d4c24":"code","cd3e1afa":"code","8231c08d":"code","9b87e227":"code","cca3330e":"code","18af3bf1":"code","31e0c0f2":"code","ed31b7f9":"code","b5ed6f03":"code","ae1aceb4":"code","f7fe73f6":"code","beefbd5c":"code","9484efb3":"code","d7afdc54":"code","d1acee1b":"code","78b40d3b":"code","8a4ce037":"code","5da1f1a0":"code","21a2f0df":"code","9baf75b1":"code","109d59b7":"code","2c7e5f67":"code","9c96753c":"code","09bc9457":"code","cd006bc0":"code","2f6cbc92":"code","0a9e6476":"code","eca4c229":"code","2a09a123":"code","f7f526f4":"code","346277a8":"code","d8957569":"code","114b4c8b":"code","72ae3bc4":"code","124104c2":"code","2a97ffa0":"code","3ba5ae29":"code","fe69351a":"code","dc239b03":"code","2016cf9b":"code","0ce4ebc7":"code","cb3d9ebb":"code","93d145e4":"code","aabb7df4":"code","fe2459e6":"code","ad863c8b":"code","3e06e4e9":"code","1b611b70":"code","b59f7d76":"code","71035f98":"code","9c33d279":"code","47ff23f6":"code","e5778d21":"code","d87a581d":"markdown","8b1454bd":"markdown","6f1f9b17":"markdown","9e48fa61":"markdown","6b709392":"markdown","6a2759dd":"markdown","f12c3fd9":"markdown","adb7e2ec":"markdown","f3cbaf1a":"markdown","9f2d143a":"markdown","067e18af":"markdown","2d447ba2":"markdown","9228db3c":"markdown","b48e8372":"markdown","c0055032":"markdown","eaa87df7":"markdown","596b45eb":"markdown","4a45d8a7":"markdown","ff55a64e":"markdown","f240a544":"markdown","239ea0ab":"markdown","c5df08c7":"markdown"},"source":{"cde0497a":"from fastai.core import *\nPath.read_csv = lambda o: pd.read_csv(o)\ninput_path = Path(\"\/kaggle\/input\/data-science-bowl-2019\")\npd.options.display.max_columns=200\npd.options.display.max_rows=200\ninput_path.ls()","aa324179":"sample_subdf = (input_path\/'sample_submission.csv').read_csv()\nspecs_df = (input_path\/\"specs.csv\").read_csv()\ntrain_df = (input_path\/\"train.csv\").read_csv()\ntrain_labels_df = (input_path\/\"train_labels.csv\").read_csv()\ntest_df = (input_path\/\"test.csv\").read_csv()","6a60b052":"train_labels_df.shape, train_df.shape, test_df.shape, specs_df.shape, sample_subdf.shape","4badceda":"# example submission\nsample_subdf.head(2)","662d4c24":"# training labels - how target: accuracy_group is created\ntrain_labels_df.head(2)","cd3e1afa":"train_labels_df[['num_correct', 'num_incorrect', 'accuracy', 'accuracy_group']].corr()","8231c08d":"train_labels_df.pivot_table(values= \"installation_id\",index=\"accuracy_group\", columns=\"accuracy\", aggfunc=np.count_nonzero)","9b87e227":"train_df.head(2)","cca3330e":"test_df.head(2)","18af3bf1":"specs_df.head(2)","31e0c0f2":"# Get last assessment start for each installation_id - it should have 'event_code' == 2000, we have exactly 1000 test samples that we need predictions of\ntest_assessments_df = test_df.sort_values(\"timestamp\").query(\"type == 'Assessment' & event_code == 2000\").groupby(\"installation_id\").tail(1).reset_index(drop=True)","ed31b7f9":"# event_data, installation_id, event_count, event_code, game_time is constant for any assessment start\n# for extarcting similar rows we can look at event_code==2000 and type==Assessment combination for each installation_id\ntest_assessments_df = test_assessments_df.drop(['event_data', 'installation_id', 'event_count', 'event_code', 'game_time'],1); test_assessments_df","b5ed6f03":"# there is unique event_id for each assesment\ntest_assessments_df.pivot_table(values=None, index=\"event_id\", columns=\"title\", aggfunc=np.count_nonzero)['game_session']","ae1aceb4":"# there are common worlds among different assessments\ntest_assessments_df.pivot_table(values=None, index=\"world\", columns=\"title\", aggfunc=np.count_nonzero)['game_session']","f7fe73f6":"test_assessments_df.describe(include='all')","beefbd5c":"def get_assessment_start_idxs(df): return listify(df.query(\"type == 'Assessment' & event_code == 2000\").index)","9484efb3":"# drop installation ids without at least 1 completed assessment\n_train_df = train_df[train_df.installation_id.isin((train_labels_df.installation_id).unique())].reset_index(drop=True)","d7afdc54":"# join training labels to game starts by game sessions  \n_trn_str_idxs = get_assessment_start_idxs(_train_df)\n_label_df = _train_df.iloc[_trn_str_idxs]\n_label_df = _label_df.merge(train_labels_df[['game_session', 'num_correct','num_incorrect','accuracy','accuracy_group']], \"left\", on=\"game_session\")\n_label_df = _label_df[[\"event_id\", \"installation_id\", 'game_session', 'num_correct','num_incorrect','accuracy','accuracy_group']]\n_label_df.head()","d1acee1b":"_label_df['accuracy_group'].value_counts(dropna=False).sort_index()","78b40d3b":"# join labels to train by event_id, game_session, installation_id\ntrain_with_labels_df = _train_df.merge(_label_df, \"left\", on=[\"event_id\", \"game_session\", \"installation_id\"])\ntrain_with_labels_df['accuracy_group'].value_counts(dropna=False).sort_index()","8a4ce037":"train_with_labels_df.shape","5da1f1a0":"# success statistics per game\n(train_with_labels_df.query(\"type == 'Assessment'\")\n                     .groupby([\"title\", \"world\"])['accuracy']\n                     .agg({np.mean, np.median, np.max, np.min})\n                     .sort_values(\"mean\"))","21a2f0df":"def count_nonnan(l): return np.sum([0 if np.isnan(o) else 1 for o in l])","9baf75b1":"# verify that all training installation ids have at least one assesment with non NaN label\nassert not any(train_with_labels_df.groupby(\"installation_id\")['accuracy'].agg(count_nonnan) == 0) ","109d59b7":"# save dataframe train with labels\ntrain_with_labels_df.to_csv(\"train_with_labels.csv\", index=False)","2c7e5f67":"# save MEM space\ndel _label_df\ndel _train_df\ngc.collect()","9c96753c":"from fastai.tabular.transform import add_datepart","09bc9457":"# set filtered and labels added df\ntrain_df = train_with_labels_df","cd006bc0":"def get_assessment_start_idxs_with_labels(df):\n    \"return indexes that will be used for supervised learning\"\n    df = df[~df.accuracy.isna()]\n    return listify(df.query(\"type == 'Assessment' & event_code == 2000\").index)","2f6cbc92":"def get_sorted_user_df(df, ins_id):\n    \"extract sorted data for a given installation id and add datetime features\"\n    _df = df[df.installation_id == ins_id].sort_values(\"timestamp\").reset_index(drop=True)\n    add_datepart(_df, \"timestamp\", time=True)\n    return _df","0a9e6476":"# pick installation_id and get data until an assessment_start\nrand_id = np.random.choice(train_df.installation_id)\nuser_df = get_sorted_user_df(train_df, rand_id)\nstart_idxs = get_assessment_start_idxs_with_labels(user_df)\nprint(f\"Assessment start idxs in user df: {start_idxs}\")","eca4c229":"# we would like to get and create features for each assessment start for supervised learning\nstr_idx = start_idxs[1]\nuser_assessment_df = user_df[:str_idx+1]; user_assessment_df","2a09a123":"from fastai.tabular import *\nimport types\n\nstats = [\"median\",\"mean\",\"sum\",\"min\",\"max\"]\n\nUNIQUE_COL_VALS = types.SimpleNamespace(\n    event_ids = np.unique(train_df.event_id),\n    media_types = np.unique(train_df.type),\n    titles = np.unique(train_df.title),\n    worlds = np.unique(train_df.world),\n    event_codes = np.unique(train_df.event_code),\n)","f7f526f4":"def array_output(f):\n    def inner(*args, **kwargs): return array(listify(f(*args, **kwargs))).flatten()\n    return inner","346277a8":"feature_funcs = []","d8957569":"@array_output\ndef time_elapsed_since_hist_begin(df):\n    \"total time passed until assessment begin\"\n    return df['timestampElapsed'].max() - df['timestampElapsed'].min()\n\nfeature_funcs.append(time_elapsed_since_hist_begin)\ntime_elapsed_since_hist_begin(user_assessment_df)","114b4c8b":"@array_output\ndef time_elapsed_since_each(df, types, dfcol):\n    \"time since last occurence of each types, if type not seen then time since history begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    last_elapsed = df['timestampElapsed'].max()\n    _d = dict(df.iloc[:-1].groupby(dfcol)['timestampElapsed'].max())\n    return [last_elapsed - _d[t] if t in _d else time_elapsed_since_hist_begin(df)[0] for t in types]","72ae3bc4":"feature_funcs.append(partial(time_elapsed_since_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_codes\", dfcol=\"event_code\"))","124104c2":"def countfreqhist(df, types, dfcol, freq=False):\n    \"count or freq of types until assessment begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _d = dict(df[dfcol].value_counts(normalize=(True if freq else False)))\n    return [_d[t] if t in _d else 0 for t in types]","2a97ffa0":"feature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=True))","3ba5ae29":"@array_output\ndef overall_event_count_stats(df):\n    \"overall event count stats until assessment begin\"\n    return df['event_count'].agg(stats)\n\nfeature_funcs.append(overall_event_count_stats)\noverall_event_count_stats(user_assessment_df)","fe69351a":"@array_output\ndef event_count_stats_each(df, types, dfcol):\n    \"event count stats per media types until assessment begin, all zeros if media type missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['event_count'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]","dc239b03":"feature_funcs.append(partial(event_count_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_codes\", dfcol=\"event_code\"))","2016cf9b":"@array_output\ndef overall_session_game_time_stats(df):\n    \"overall session game time stats until assessment begin\"\n    return df['game_time'].agg(stats)\n\nfeature_funcs.append(overall_session_game_time_stats)\noverall_session_game_time_stats(user_assessment_df)","0ce4ebc7":"@array_output\ndef session_game_time_stats_each(df, types, dfcol):\n    \"session game time stats per media types until assessment begin, all zeros if missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['game_time'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]","cb3d9ebb":"feature_funcs.append(partial(session_game_time_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_codes\", dfcol=\"event_code\"))","93d145e4":"sample_features = np.concatenate([f(user_assessment_df) for f in feature_funcs]); sample_features.shape","aabb7df4":"def get_test_assessment_start_idxs(df): \n    return list(df.sort_values(\"timestamp\")\n                  .query(\"type == 'Assessment' & event_code == 2000\")\n                  .groupby(\"installation_id\").tail(1).index)","fe2459e6":"# trn_str_idxs = get_assessment_start_idxs_with_labels(train_with_labels_df)\n# test_str_idxs = get_test_assessment_start_idxs(test_df)","ad863c8b":"# Get training features\ndef get_train_feats_row(ins_id, i):\n    \"get all assessment start features for an installation id\"\n    rows = [] # collect rows with features for each assessment start\n    user_df = get_sorted_user_df(train_with_labels_df, ins_id)\n    start_idxs = get_assessment_start_idxs_with_labels(user_df); start_idxs\n    for idx in start_idxs:\n        assessment_row = user_df.iloc[idx]\n        _df = user_df[:idx+1]\n        row_feats = np.concatenate([f(_df) for f in feature_funcs])\n        feat_row = pd.Series(row_feats, index=[f\"static_feat{i}\"for i in range(len(row_feats))])\n        row = pd.concat([assessment_row, feat_row])\n        rows.append(row)\n    return rows","3e06e4e9":"# # compute static features for train assessment start\n# installation_ids = train_with_labels_df.installation_id.unique()\n# res = parallel(get_train_feats_row, (installation_ids))\n# train_with_features_df = pd.concat([row for rows in res for row in rows],1).T","1b611b70":"# train_with_features_df.head()","b59f7d76":"# train_with_features_df.to_csv(\"train_with_features_part1.csv\")","71035f98":"def get_test_feats_row(idx, i):\n    \"get all faeatures by an installation start idx\"\n    ins_id = test_df.loc[idx, \"installation_id\"]\n    _df = get_sorted_user_df(test_df, ins_id)\n    assessment_row = _df.iloc[-1]\n    row_feats = np.concatenate([f(_df) for f in feature_funcs])\n    feat_row = pd.Series(row_feats, index=[f\"static_feat{i}\"for i in range(len(row_feats))])\n    row = pd.concat([assessment_row, feat_row])\n    return row","9c33d279":"# # compute static features for test assessment start and save \n# start_idxs = get_test_assessment_start_idxs(test_df)\n# res = parallel(get_test_feats_row, start_idxs)\n# test_with_features_df = pd.concat(res,1).T","47ff23f6":"# test_with_features_df.head()","e5778d21":"# test_with_features_df.to_csv(\"test_with_features_part1.csv\")","d87a581d":"> #### Game Time","8b1454bd":"### Compute all features for train and test\n\nThese are mostly static features per assessment start which we can compute and save for later use","6f1f9b17":"### Read data","9e48fa61":"### - Event data features\n\n- Each `event_id` have a single `event_code` \n- `event_code` can be shared across games, e.g. `event_code==2000` and there are more eventhough content is different \n- `event_id` and `event_code` has a single `title` except for `event_code==2000` which indicates start and has 20 `title`\n- There are 386 unique `event_id`\n- Same `event_id` can have different descriptions: \n\n```\n{\"description\":\"Ah! See the pans move? Now put a bowl on the other pan.\",\"identifier\":\"Cleo_SeePansMove,Cleo_PutBowlOtherPan\",\"media_type\":\"audio\",\"total_duration\":8570,\"round\":1,\"event_count\":12,\"game_time\":19061,\"event_code\":3010} 8d7e386c Happy Camel\n\n{\"description\":\"When one side tips down\u2026 that bowl is heavier. Now think\u2026 which bowl has the toy inside?\",\"identifier\":\"Cleo_TipsHeavier,Cleo_ThinkWhich\",\"media_type\":\"audio\",\"total_duration\":9192,\"round\":1,\"event_count\":16,\"game_time\":23378,\"event_code\":3010} 8d7e386c Happy Camel\n```\n\n- Same description can be seen in different `event_id`: \n\n```\n{\"description\":\"Epidermis' toy is in one of these bowls of camel food! But which one? We shall find the toys in no time thanks to my pan balance! This amazing device for measuring weight. First, move a bowl to one of the pans.\",\"identifier\":\"Cleo_EpidermisToy,Cleo_PanBalance,Cleo_MoveBowlToPan\",\"media_type\":\"audio\",\"duration\":7317,\"round\":1,\"event_count\":11,\"game_time\":19061,\"event_code\":3110} 69fdac0a Happy Camel\n\n{\"description\":\"Epidermis' toy is in one of these bowls of camel food! But which one? We shall find the toys in no time thanks to my pan balance! This amazing device for measuring weight. First, move a bowl to one of the pans.\",\"identifier\":\"Cleo_EpidermisToy,Cleo_PanBalance,Cleo_MoveBowlToPan\",\"media_type\":\"audio\",\"total_duration\":20920,\"round\":1,\"event_count\":6,\"game_time\":11744,\"event_code\":3010} 8d7e386c Happy Camel\n![](http:\/\/)```","6b709392":"### 1)  Static Until Assessment Start Features\n\nBelow we implement functions that will take `user_assessment_df` and spit out some feature vector for a unique assessment start which has unique id by combination of  <`installation_id`, `game_session`, `event_id`>.\n\n```\nValue: Time Elapsed x Count\/Freq x Event Count x Game Time \nBy: Media Type x Title x Event Id x World x Event Code\n\n```","6a2759dd":"### Feature Engineering with part 1\n\nMy intention in this notebook is to do feature engineering without looking too much at other public kernels to avoid potential biases and risk of limiting  new creative and hopefully helpful feature ideas. This is the part 1 of the feature engineering notebooks that will be implemented.\n\nThis notebook implements overall or grouped user specific statistical features based on user activity history.\n\n\n**You can also skip and go to modeling kernel:** [Part 1 Modeling Notebook](https:\/\/www.kaggle.com\/keremt\/fastai-model-part1-regression\/)","f12c3fd9":"### Feature Engineering Ideas\n\n- Generate individual features using history until an assessment start (part 1)\n- Target encoding (TODO)\n- Generate global aggregate features by groupby, independent of time (TODO)\n- Encode sequential events from `event_data` using RNNs, e.g. embeddings for different groups of `event_data` can be generated (TODO)\n- Graph based features (TODO)","adb7e2ec":"### What to predict?\n\n**What is predicted:** We predict last assessment in test set that which has start code `event_code==2000`.\n\n**Note:** During feature engineering we need to keep in mind that we can only use historical data to generate features because any data after `event_code=2000` in test set is truncated. Using global data for features won't be a problem and it will be time independent, e.g. priors, but in general for user centric features there shouldn't be any future information.\n\nFor example, we can calculate mean accuracy of a particular group or combination of groups globally without caring about time leakage since that information will also be available in test time. These type of features can be considered as priors that we feed into our model. This is sort of target encoding and even though we are allowed to use all data it should be done with caution, e.g. using out of folds, CV, etc.., to make sure that we don't overfit. See `Holdout Type` section from [h2o.ai](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-munging\/target-encoding.html#holdout-type) for more information.","f3cbaf1a":"### Next steps for part 2","9f2d143a":"We can actually see there is a direct rule for converting **accuracy** to **accuracy_group**.\n\n- `y==0 -> 0`,  `y>0 & y<0.5 -> 1`, `y==0.5 -> 2`, `y==1 -> 3`\n\nWe can tune our thresholds to based on validation set to better align conversion from **predicted accuracy** to **accuracy_group**.","067e18af":"> #### Event Count","2d447ba2":"> #### Time Elapsed","9228db3c":"**Note:**\nKernel doesn't complete within given time although it completes in my local laptop within just 1 hour. So I ran everything locally and registered necessary data as a dataset here: https:\/\/www.kaggle.com\/keremt\/dsbowl2019-feng-part1  ","b48e8372":"### - Target encoding","c0055032":"### Training data with labels\n\nLet's merge train_labels to `train_df`","eaa87df7":"### History of a user\n\nLet's check history of a single user to get a better understanding of events and possible feature engineering ideas. \n\n**Assessment Start ID:** <`installation_id`, `game_session`, `event_id`>","596b45eb":"### end","4a45d8a7":"> #### Count\/Freq","ff55a64e":"### Inspect data","f240a544":"### Imports\n\nWe will use fastai v1","239ea0ab":"### Validation: Split by `installation_id`\n\nDuring modeling it's better to craete validation sets by splitting by `installation_id` rather than random. ","c5df08c7":"### Train labels\n\nMain target **accuracy_group** is highly positively correlated with **accuracy**, so we can potentially use this as a pseudo target to convert this problem into a regression problem which will allow us to have ability to feed more granular signal during supervised learning and also it will give us the ability to post process by setting thresholds to create groups from soft prediction."}}