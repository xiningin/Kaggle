{"cell_type":{"ed30e45d":"code","b3d7ec27":"code","535c31a9":"code","3de0d7ea":"code","e157742c":"code","06055cc5":"code","c89173df":"code","2476ebee":"code","4b6d479a":"code","21f9ef72":"code","4896ca36":"code","bd18dd31":"code","55d5778f":"code","66b21a24":"code","2949f094":"code","f95f8321":"code","3d85eeee":"code","0464fb0e":"code","651c4684":"code","c68dc0e2":"code","15643fae":"code","24890363":"code","ef761677":"code","185d75c7":"code","bf48ad11":"code","e89b0220":"code","1ad05398":"code","58f65311":"code","0c36711a":"code","a1450462":"code","369279fb":"code","70de79a3":"code","7f2105f2":"code","241b9cd0":"code","38bd2b1e":"code","68ce4621":"code","eeba8930":"code","a658415f":"code","178def3d":"code","b8beb0e3":"code","d291d68f":"code","c4c9786e":"code","c5082707":"code","1e0cd596":"code","6e052b30":"code","63993dce":"code","f7804f50":"code","44118eb5":"code","cb0ef1ff":"code","224390c2":"code","94b2f4ef":"code","95a248e3":"code","d079e04e":"code","e3be0eb3":"code","4667c54a":"code","79dd1617":"code","711f735f":"code","20756052":"code","ff089f43":"code","b86d9d10":"code","b7aaaf44":"code","861e86fe":"code","6480bdfa":"code","aed59244":"code","77b134cc":"code","f6c7044c":"code","bf67f876":"code","7be71c86":"code","94e6323a":"code","2b64a77a":"code","388ead63":"code","a2c90072":"code","7c0f2f69":"code","34cba918":"code","72ed99db":"code","e582f1c3":"code","385409d5":"code","8aa8c8bf":"code","27f9f9cd":"code","21fdb48f":"code","97c06112":"code","e3b4eef1":"markdown","5143f44c":"markdown","ff8f5022":"markdown","5971f1ab":"markdown","1b8fc9c7":"markdown","a2635501":"markdown","f92ce510":"markdown","987d4bc4":"markdown","507dc7bf":"markdown","74d8f043":"markdown","5edc26bf":"markdown","caa43906":"markdown","f4216fe8":"markdown","a84a2f38":"markdown","20a8c1c7":"markdown","4525928b":"markdown","a02d37ae":"markdown","ead4eb5d":"markdown","b026bb5b":"markdown","1b23b3c9":"markdown","685d7f64":"markdown","4a92fa4a":"markdown","21d1d9c0":"markdown","0517d9c4":"markdown","d3bd46b1":"markdown","f6b2a376":"markdown","48835bff":"markdown","097c94e7":"markdown","9a8d8364":"markdown","94ae985a":"markdown","6bf15d98":"markdown","68939c35":"markdown","90a4e6a3":"markdown","47a5a911":"markdown","c21585ab":"markdown","98ea3df1":"markdown","d847db09":"markdown","8ebb09e2":"markdown","7de90f22":"markdown","90339842":"markdown","8904f7ee":"markdown","ab7bb8cd":"markdown","5db17686":"markdown","3f397cce":"markdown","95eaaade":"markdown","4c499392":"markdown","804ed8b4":"markdown","109e0454":"markdown","c05039b3":"markdown","a4e8970d":"markdown"},"source":{"ed30e45d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory|\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3d7ec27":"#importing the necessary libraries\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n\n","535c31a9":"#function for one hot encoding of the categorical columns\ndef one_hot_encoding_dataframe(df):\n    original_columns = list(df.columns)\n    cat_columns=[x for x in df.columns if df[x].dtype == 'object']\n    df=pd.get_dummies(df,columns=cat_columns,dummy_na= False)\n    new_added_columns=list(set(df.columns).difference(set(original_columns)))\n    return df,new_added_columns,df.columns","3de0d7ea":"#loading the main test and train data\nmain_df=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\ntrain_target = main_df[['TARGET', 'SK_ID_CURR']]\ntest_target = test_df['SK_ID_CURR'].copy()\n","e157742c":"def credit_card_bal(df):\n    \n    categorical_cols = ['NAME_CONTRACT_STATUS']\n    for col in categorical_cols:\n            enc = LabelEncoder()\n            df[col] = enc.fit_transform(df[col])\n            \n    #Creating new features\n    df['ratio_ab_acl'] = df['AMT_BALANCE'] \/ df['AMT_CREDIT_LIMIT_ACTUAL']\n    df['sum_draw'] = df[['AMT_DRAWINGS_ATM_CURRENT','AMT_DRAWINGS_CURRENT','AMT_DRAWINGS_OTHER_CURRENT', 'AMT_DRAWINGS_POS_CURRENT']].sum(axis=1)\n    df['ratio_tc_tr'] = df['AMT_PAYMENT_TOTAL_CURRENT'] \/ df['AMT_TOTAL_RECEIVABLE']\n    df['ratio_pc_ar'] = df['AMT_PAYMENT_CURRENT'] \/ df['AMT_RECIVABLE']\n    df['sum_cntdraw'] = df[['CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT', 'CNT_DRAWINGS_POS_CURRENT']].sum(axis=1)\n    df['diff_tr_tc'] = df['AMT_TOTAL_RECEIVABLE'] \/ df['AMT_PAYMENT_TOTAL_CURRENT']\n    df['ratio_pc_ptc'] = df['AMT_PAYMENT_CURRENT'] \/ df['AMT_PAYMENT_TOTAL_CURRENT']\n    \n    #Creating aggregates\n    aggs = {\n        'MONTHS_BALANCE': ['min', 'max', 'size'],\n        'CNT_DRAWINGS_ATM_CURRENT': ['max'], \n        'CNT_DRAWINGS_CURRENT': ['max'],\n        'CNT_DRAWINGS_POS_CURRENT': ['max'],\n        'CNT_INSTALMENT_MATURE_CUM': ['mean', 'sum'],\n        'AMT_BALANCE': ['min', 'max', 'mean', 'sum'],\n        'AMT_CREDIT_LIMIT_ACTUAL': ['max', 'mean','var'],\n        'AMT_DRAWINGS_ATM_CURRENT': ['max'],\n        'AMT_DRAWINGS_CURRENT': ['max'],\n        'AMT_DRAWINGS_POS_CURRENT': ['max'],\n        'AMT_PAYMENT_CURRENT': ['max'],\n        'AMT_PAYMENT_TOTAL_CURRENT': ['max'],\n        'AMT_RECEIVABLE_PRINCIPAL': ['mean', 'sum'],\n        'AMT_RECIVABLE': ['mean', 'sum'],\n        'AMT_TOTAL_RECEIVABLE': ['mean'],\n        \n        #New features\n        'ratio_ab_acl': ['min', 'max', 'mean'],\n        'ratio_tc_tr': ['min', 'max', 'mean'],\n        'ratio_pc_ar': ['min', 'max', 'mean'],\n        'diff_tr_tc': ['min', 'max', 'mean'],\n        'ratio_pc_ptc': ['min', 'max', 'mean']\n    }\n    #creating n from those aggregates\n    cc_aggs = df.groupby('SK_ID_CURR').agg(aggs)\n    cc_aggs.columns = pd.Index([i[0] + \"_\" + i[1].upper() + '_(CREDIT_CARD)' for i in cc_aggs.columns.tolist()])\n    cc_aggs['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n    \n    return cc_aggs","06055cc5":"credit_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/credit_card_balance.csv')\ndf_credit=credit_card_bal(credit_data)\n","c89173df":"df_credit =main_df.merge(df_credit, how='left', on='SK_ID_CURR')\ndf_credit=df_credit[df_credit['TARGET'].notnull()]\n\ny_train=df_credit['TARGET']\ntrain_column=set(df_credit.columns)-set(main_df.columns)\nX_train=df_credit[train_column]\n\ntrain_column=X_train.columns\nX_train=X_train.replace([np.inf, -np.inf],np.nan)\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer.fit_transform(X_train)\nscaler = MinMaxScaler(feature_range = (0, 1))\nX_train=scaler.fit_transform(X_train)\n","2476ebee":"import sklearn\nclasses_zero = main_df[main_df['TARGET'] == 0]\nclasses_one = main_df[main_df['TARGET'] == 1]\n\n# Convert parts into NumPy arrays for weight computation\nzero_numpy = classes_zero['TARGET'].to_numpy()\none_numpy = classes_one['TARGET'].to_numpy()\nall_together = np.concatenate((zero_numpy, one_numpy))\nunique_classes = np.unique(all_together)\n\n# Compute weights\nweights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\nweights","4b6d479a":"poswt = len(classes_zero) \/ len(classes_one)\nposwt","21f9ef72":"clf=LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005,\n    num_leaves= 30,\n    max_depth= 6,\n    subsample_for_bin= 24000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    scale_pos_weight=poswt)","4896ca36":"clf.fit(X_train,y_train,eval_metric='auc')\nprint(\"ROCAUC Score :\",roc_auc_score(y_train,clf.predict_proba(X_train)[:,1]))","bd18dd31":"cred_bal=clf.predict_proba(X_train)[:,1]","55d5778f":"df_credit2=credit_card_bal(credit_data)\ndf_credit2 =test_df.merge(df_credit2, how='left', on='SK_ID_CURR')\ntest_col=set(df_credit2.columns)-set(test_df.columns)\nX_test=df_credit2[test_col]\n","66b21a24":"df_test=X_test.replace([np.inf, -np.inf],np.nan)\ndf_test=imputer.transform(df_test)\ndf_test=scaler.transform(df_test)","2949f094":"yp=clf.predict_proba(df_test)[:,1]","f95f8321":"def install_bal(inst_df):\n          \n    inst_df['late_pay']=inst_df['DAYS_INSTALMENT']-inst_df['DAYS_ENTRY_PAYMENT']\n    inst_df['less_pay']=inst_df['AMT_INSTALMENT']-inst_df['AMT_PAYMENT']\n    inst_df['late_lp']=0.5*inst_df['late_pay']+0.5*inst_df['less_pay']\n    inst_df['ltp_flag']=((inst_df['DAYS_INSTALMENT']-inst_df['DAYS_ENTRY_PAYMENT'])>0).astype(int)\n    inst_df['lsp_flag']=((inst_df['AMT_INSTALMENT']-inst_df['AMT_PAYMENT'])>0).astype(int)\n    \n    for col in inst_df.columns:\n        if col.startswith('DAYS'):\n            inst_df[col].replace(365243, np.nan, inplace= True)\n            \n    inst_df,installments_payments_cat_columns,all_columns=one_hot_encoding_dataframe(inst_df)\n       \n\n    inst_df_agg={}\n    for col in inst_df.columns:\n        if col!='SK_ID_CURR' and col !='SK_ID_PREV':\n            inst_df_agg[col]=['mean']\n            if (col=='late_pay') |  (col=='less_pay') | (col=='NUM_INSTALMENT_VERSION') | (col=='NUM_INSTALMENT_NUMBER'):\n              inst_df_agg[col]=['mean','sum','max','min']\n    \n    inst_agg = inst_df.groupby('SK_ID_CURR').agg(inst_df_agg)\n    \n    modified_col=[]\n    for c in list(inst_agg.columns):\n        modified_col.append(\"INST_\"+c[0]+\"_\"+c[1].upper())\n    inst_agg.columns=modified_col  \n    \n    inst_agg['cnt_inst'] = inst_df.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\n\n    no = -365*3\n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >=no].copy()\n    inst_agg['3365_ltp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].mean()\n    inst_agg['3365_ltp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].min()\n    inst_agg['3365_ltp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].max()\n    inst_agg['3365_ltp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['ltp_flag'].sum() \n  \n    no = -365*2\n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >=no].copy()\n    inst_agg['2365_ltp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].mean()\n    inst_agg['2365_ltp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].max()\n    inst_agg['2365_ltp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['ltp_flag'].sum()\n    \n    no = -365 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >=no].copy()\n    inst_agg['365_ltp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].mean()\n    inst_agg['365_ltp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].max()\n    inst_agg['365_ltp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['ltp_flag'].sum()\n    \n    no = -180 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >= no].copy()\n    inst_agg['180_ltp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].mean()\n    inst_agg['180_ltp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].max()\n    inst_agg['180_ltp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['ltp_flag'].sum()\n    \n    no = -90 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >= no].copy()\n    inst_agg['90_ltp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].mean()\n    inst_agg['90_ltp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].min()\n    inst_agg['90_ltp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['late_pay'].max()\n        \n    no = -365*2\n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >=no].copy()\n    inst_agg['2365_lsp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].mean()\n    inst_agg['2365_lsp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].min()\n    inst_agg['2365_lsp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].max()\n    inst_agg['2365_lsp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['lsp_flag'].sum()\n\n    no = -365 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >=no].copy()\n    inst_agg['365_lsp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].mean()\n    inst_agg['365_lsp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].min()\n    inst_agg['365_lsp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].max()\n    inst_agg['365_lsp_flag_cnt'] = inst_agg_temp.groupby('SK_ID_CURR')['lsp_flag'].sum()\n    \n    no = -180 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >= no].copy()\n    inst_agg['180_lsp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].mean()\n    inst_agg['180_lsp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].min()\n    inst_agg['180_lsp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].max()\n    \n    no = -90 \n    inst_agg_temp = inst_df[inst_df.DAYS_ENTRY_PAYMENT >= no].copy()\n    inst_agg['90_lsp_flag_mean'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].mean()\n    inst_agg['90_lsp_flag_min'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].min()\n    inst_agg['90_lsp_flag_max'] = inst_agg_temp.groupby('SK_ID_CURR')['less_pay'].max()\n   \n    \n    return inst_agg","3d85eeee":"inst_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/installments_payments.csv')\n\ndf_inst=install_bal(inst_data)\ndf_inst =main_df.merge(df_inst, how='left', on='SK_ID_CURR')\ndf_inst=df_inst[df_inst['TARGET'].notnull()]\n\ny_train=df_inst['TARGET']\ntrain_column=set(df_inst.columns)-set(main_df.columns)\nX_train=df_inst[train_column]\n\ntrain_column=X_train.columns\nX_train=X_train.replace([np.inf, -np.inf],np.nan)\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer1.fit_transform(X_train)\nscaler = MinMaxScaler(feature_range = (0, 1))\nX_train=scaler.fit_transform(X_train)","0464fb0e":"clf2=LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005,\n    num_leaves= 30,\n    max_depth= 6,\n    subsample_for_bin= 24000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    scale_pos_weight=poswt)","651c4684":"clf2.fit(X_train,y_train,eval_metric='auc')\nprint(\"ROCAUC Score :\",roc_auc_score(y_train,clf2.predict_proba(X_train)[:,1]))","c68dc0e2":"print(\"ROCAUC Score :\",roc_auc_score(y_train,clf2.predict_proba(X_train)[:,1]))","15643fae":"inst=clf2.predict_proba(X_train)[:,1]","24890363":"df_inst2=install_bal(inst_data)\ndf_inst2 =test_df.merge(df_inst2, how='left', on='SK_ID_CURR')\ntest_col=set(df_inst2.columns)-set(test_df.columns)\nX_test=df_inst2[test_col]","ef761677":"df_test=X_test.replace([np.inf, -np.inf],np.nan)\ndf_test=imputer1.transform(df_test)\ndf_test=scaler.transform(df_test)","185d75c7":"df_test.shape","bf48ad11":"yp=clf2.predict_proba(df_test)[:,1]","e89b0220":"def pos_appl(pos_df):\n    \n    pos_df=pos_df[pos_df['NAME_CONTRACT_STATUS']!='XNA'] \n    pos_df,pos_data_cat_columns,all_columns=one_hot_encoding_dataframe(pos_df)\n     \n    pos_data_agg={}\n    for col in pos_df.columns:\n        if col!='SK_ID_CURR' and col !='SK_ID_PREV':\n            pos_data_agg[col]=['mean']\n        if col=='MONTHS_BALANCE':\n            pos_data_agg[col]=['sum','mean','max']\n            \n    \n    pos_agg = pos_df.groupby('SK_ID_CURR').agg(pos_data_agg)\n    \n    modified_col=[]\n    for col in list(pos_agg.columns):\n        modified_col.append(\"POS_\"+col[0]+\"_\"+col[1].upper())\n    pos_agg.columns=modified_col\n    pos_agg['pos_cnt'] = pos_df.groupby('SK_ID_CURR')['SK_ID_PREV'].count() \n\n    month = -24 \n    pos_temp = pos_df[pos_df.MONTHS_BALANCE >= month].copy()\n    pos_agg['24_c_inst_mean'] = pos_temp.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].mean()\n    pos_agg['24_c_inst_max'] = pos_temp.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].max()\n    \n    month = -12 \n    pos_temp = pos_df[pos_df.MONTHS_BALANCE >= month].copy()\n    pos_agg['12_c_inst_mean'] = pos_temp.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].mean()\n    pos_agg['12_c_inst_max'] = pos_temp.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].max()\n    \n    active = pos_df[pos_df['NAME_CONTRACT_STATUS_Active'] == 1]\n   \n    pos_agg['active_inst_mean'] = active.groupby('SK_ID_CURR')['CNT_INSTALMENT'].mean()\n    pos_agg['active_inst_max'] = active.groupby('SK_ID_CURR')['CNT_INSTALMENT'].max()\n  \n    pos_agg['active_dpd_mean'] = active.groupby('SK_ID_CURR')['SK_DPD'].mean()\n    pos_agg['active_dpd_max'] = active.groupby('SK_ID_CURR')['SK_DPD'].max()\n\n    pos_agg['active_inst_fut_mean'] = active.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].mean()\n    pos_agg['active_inst_fut_max'] = active.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].max()\n  \n    pos_agg['active_dpd_def_mean'] = active.groupby('SK_ID_CURR')['SK_DPD_DEF'].mean()\n    pos_agg['active_dpd_def_max'] = active.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\n  \n    completed = pos_df[pos_df['NAME_CONTRACT_STATUS_Completed'] == 1]\n    pos_agg['com_dpd_mean'] = completed.groupby('SK_ID_CURR')['SK_DPD'].mean()\n    pos_agg['com_dpd_max'] = completed.groupby('SK_ID_CURR')['SK_DPD'].max()\n\n    pos_agg['com_dpd_def_mean'] = completed.groupby('SK_ID_CURR')['SK_DPD_DEF'].mean()\n    pos_agg['com_dpd_def_max'] = completed.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\n\n    pos_agg['com_inst_fut_mean'] = completed.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].mean()\n    pos_agg['com_inst_fut_max'] = completed.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].max()\n  \n    pos_agg['com_inst_mean'] = completed.groupby('SK_ID_CURR')['CNT_INSTALMENT'].mean()\n    pos_agg['com_inst_max'] = completed.groupby('SK_ID_CURR')['CNT_INSTALMENT'].max()\n  \n    return pos_agg\n","1ad05398":"pos_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\n\ndf_pos=pos_appl(pos_data)\ndf_pos =main_df.merge(df_pos, how='left', on='SK_ID_CURR')\ndf_pos=df_pos[df_pos['TARGET'].notnull()]\n\ny_train=df_pos['TARGET']\ntrain_column=set(df_pos.columns)-set(main_df.columns)\nX_train=df_pos[train_column]\n\ntrain_column=X_train.columns\nX_train=X_train.replace([np.inf, -np.inf],np.nan)\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer1.fit_transform(X_train)\nscaler = MinMaxScaler(feature_range = (0, 1))\nX_train=scaler.fit_transform(X_train)","58f65311":"clf3=LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005,\n    num_leaves= 30,\n    max_depth= 6,\n    subsample_for_bin= 24000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    scale_pos_weight=poswt)","0c36711a":"clf3.fit(X_train,y_train,eval_metric='auc')\nprint(\"ROCAUC Score :\",roc_auc_score(y_train,clf3.predict_proba(X_train)[:,1]))","a1450462":"pos=clf3.predict_proba(X_train)[:,1]","369279fb":"df_pos2=pos_appl(pos_data)\ndf_pos2 =test_df.merge(df_pos2, how='left', on='SK_ID_CURR')\ntest_col=set(df_pos2.columns)-set(test_df.columns)\nX_test=df_pos2[test_col]","70de79a3":"df_test=X_test.replace([np.inf, -np.inf],np.nan)\ndf_test=imputer1.transform(df_test)\ndf_test=scaler.transform(df_test)","7f2105f2":"yp=clf3.predict_proba(df_test)[:,1]","241b9cd0":"def bureau_bal(bureau_balance_data,bureau_data):\n    \n    bureau_balance_data,bureau_balance_data_cat_columns,all_columns=one_hot_encoding_dataframe(bureau_balance_data)\n    #Aggregate function to be applied on numerical column \n    bureau_balance_agg = {'MONTHS_BALANCE': ['min', 'max','sum']}\n\n    #Aggregate function to be applied on cat column \n    for col in bureau_balance_data_cat_columns:\n        if (col!='SK_BUREAU_ID'):\n            bureau_balance_agg[col] = ['mean']\n\n    bal_agg = bureau_balance_data.groupby(['SK_ID_BUREAU']).agg(bureau_balance_agg)\n    \n    modified_col=[]\n    for col in list(bal_agg.columns):\n        if (col!='SK_BUREAU_ID'):\n            modified_col.append(col[0]+\"_\"+col[1].upper())\n    bal_agg.columns=modified_col\n    \n    bureau_data,bureau_data_cat_columns,all_columns=one_hot_encoding_dataframe(bureau_data)\n\n    bureau_data['SEC_LOAN_COUNT']=(bureau_data[['CREDIT_TYPE_Car loan','CREDIT_TYPE_Loan for the purchase of equipment','CREDIT_TYPE_Mortgage','CREDIT_TYPE_Real estate loan','CREDIT_TYPE_Loan for purchase of shares (margin lending)'\n                         ]]==1).sum(axis=1)\n    \n    bureau_data['UNSEC_LOAN_COUNT']=(bureau_data[[ 'CREDIT_TYPE_Another type of loan',\n       'CREDIT_TYPE_Cash loan (non-earmarked)', 'CREDIT_TYPE_Consumer credit',\n       'CREDIT_TYPE_Credit card', 'CREDIT_TYPE_Interbank credit',\n       'CREDIT_TYPE_Loan for business development',\n       'CREDIT_TYPE_Loan for working capital replenishment',\n       'CREDIT_TYPE_Microloan', 'CREDIT_TYPE_Mobile operator loan',\n       'CREDIT_TYPE_Unknown type of loan']]==1).sum(axis=1)\n    \n    bureau_data['ex_pay'] = bureau_data['AMT_ANNUITY']-bureau_data['AMT_CREDIT_SUM']\n    bureau_data['debt']=bureau_data['AMT_CREDIT_SUM_DEBT']\/bureau_data['AMT_CREDIT_SUM']\n    bureau_data['annu_per_cred']=bureau_data['AMT_ANNUITY']\/bureau_data['AMT_CREDIT_SUM']\n    \n    for col in bureau_data.columns:\n        if col.startswith('DAYS'):\n            bureau_data[col].replace(365243, np.nan, inplace= True)\n            \n    bureau_data = bureau_data.join(bal_agg, how='left', on=['SK_ID_BUREAU'])\n     \n    bureau_data_agg={}\n    for col in bureau_data.columns:\n        if (col!='SK_ID_CURR' or col!='SK_BUREAU_ID'):\n            bureau_data_agg[col]=['mean']\n            if (col=='AMT_CREDIT_SUM_DEBT') | (col=='AMT_CREDIT_SUM_OVERDUE') | (col=='UNSEC_LOAN_COUNT') |(col=='SEC_LOAN_COUNT'):\n                bureau_data_agg[col]=['sum']\n            if col=='DAYS_CREDIT':\n                bureau_data_agg[col]=['min']\n            if col=='debt':\n                bureau_data_agg[col]=['mean']\n    \n            \n    bureau_agg = bureau_data.groupby('SK_ID_CURR').agg(bureau_data_agg)\n    \n    modified_col=[]\n    for col in list(bureau_agg.columns):\n        modified_col.append(col[0]+\"_\"+col[1].upper())\n    bureau_agg.columns=modified_col\n    \n    bureau_agg['abs_cre_max']=abs(bureau_agg['DAYS_CREDIT_MIN']\/365) \n    bureau_agg['cre_max_od_max'] = bureau_data.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max()\n    bureau_agg['cnt'] = bureau_data.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count()\n    bureau_data_active = bureau_data[bureau_data['CREDIT_ACTIVE_Active'] == 1]\n    \n    bureau_agg['active_cred_mean'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean()\n    bureau_agg['active_cred_max'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].max()\n    bureau_agg['cred_od_mean'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].mean()\n    bureau_agg['cred_od_max'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max()   \n    bureau_agg['active_cred_debt_mean'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].mean()\n    bureau_agg['active_cred_debt_max'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].max()\n    bureau_agg['active_cred_limit_mean'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].mean()\n    bureau_agg['active_cred_limit_max'] = bureau_data_active.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].max()\n    bureau_agg['cred_days_mean'] = bureau_data_active.groupby('SK_ID_CURR')['DAYS_CREDIT'].mean()\n    bureau_agg['cred_days_max'] = bureau_data_active.groupby('SK_ID_CURR')['DAYS_CREDIT'].max()  \n    bureau_agg['cred_ed_mean'] = bureau_data_active.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].mean()\n    bureau_agg['cred_ed_max'] = bureau_data_active.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].max()\n    bureau_data_closed = bureau_data[bureau_data['CREDIT_ACTIVE_Closed'] == 1]\n    bureau_agg['cls_cred_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean()\n    bureau_agg['cls_cred_max'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].max()\n    bureau_agg['B_CLO_AMT_CREDIT_MAX_OVERDUE_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].mean()\n    bureau_agg['B_CLO_AMT_CREDIT_MAX_OVERDUE_max'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max()\n    bureau_agg['cls_cred_d_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].mean()\n    bureau_agg['cls_cred_d_max'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].max()\n    bureau_agg['cls_cred_d_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].mean()\n    bureau_agg['cls_cred_d_max'] = bureau_data_closed.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].max()\n    bureau_agg['cls_credd_ed_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].mean()\n    bureau_agg['cls_credd_ed_max'] = bureau_data_closed.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].max()\n    bureau_agg['cls_credd_mean'] = bureau_data_closed.groupby('SK_ID_CURR')['DAYS_CREDIT'].mean()\n    bureau_agg['cls_credd_max'] = bureau_data_closed.groupby('SK_ID_CURR')['DAYS_CREDIT'].max()\n    bureau_agg.drop(['SK_ID_CURR_MEAN'], axis=1, inplace= True)\n    bureau_agg.drop(['SK_ID_BUREAU_MEAN'], axis=1, inplace= True)\n    \n    \n    return bureau_agg","38bd2b1e":"bureau_bal_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau_balance.csv')\nbureau_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau.csv')\n\ndf_bureau=bureau_bal(bureau_bal_data,bureau_data)\ndf_bureau =main_df.merge(df_bureau, how='left', on='SK_ID_CURR')\ndf_bureau=df_bureau[df_bureau['TARGET'].notnull()]\n\ny_train=df_bureau['TARGET']\ntrain_column=set(df_bureau.columns)-set(main_df.columns)\nX_train=df_bureau[train_column]\n\n\ntrain_column=X_train.columns\nX_train=X_train.replace([np.inf, -np.inf],np.nan)\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer1.fit_transform(X_train)\nscaler = MinMaxScaler(feature_range = (0, 1))\nX_train=scaler.fit_transform(X_train)\n","68ce4621":"clf4=LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005,\n    num_leaves= 30,\n    max_depth= 6,\n    subsample_for_bin= 24000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    scale_pos_weight=poswt)","eeba8930":"clf4.fit(X_train,y_train,eval_metric='auc')\nprint(\"ROCAUC Score :\",roc_auc_score(y_train,clf4.predict_proba(X_train)[:,1]))","a658415f":"bureau=clf4.predict_proba(X_train)[:,1]","178def3d":"df_bureau2=bureau_bal(bureau_bal_data,bureau_data)\ndf_bureau2 =test_df.merge(df_bureau2, how='left', on='SK_ID_CURR')\ntest_col=set(df_bureau2.columns)-set(test_df.columns)\nX_test=df_bureau2[test_col]","b8beb0e3":"df_test=X_test.replace([np.inf, -np.inf],np.nan)\ndf_test=imputer1.transform(df_test)\ndf_test=scaler.transform(df_test)","d291d68f":"yp=clf4.predict_proba(df_test)[:,1]","c4c9786e":"def prev_appl(prev_df):\n    \n    for col in prev_df.columns:\n        if col.startswith('DAYS'):\n            prev_df[col].replace(365243, np.nan, inplace= True)\n    \n    prev_df['extra_paid'] = prev_df['CNT_PAYMENT']*prev_df['AMT_ANNUITY']-prev_df['AMT_CREDIT']\n    prev_df['to_pay'] = prev_df['CNT_PAYMENT']*prev_df['AMT_ANNUITY']-prev_df['AMT_DOWN_PAYMENT']\n    prev_df['roi'] = (1\/prev_df['CNT_PAYMENT'])*(((prev_df['CNT_PAYMENT']*prev_df['AMT_ANNUITY'])\/prev_df['AMT_CREDIT'])-1)\n    prev_df['si']= (prev_df['AMT_CREDIT']*prev_df['roi']*prev_df['CNT_PAYMENT'])\/100    \n    prev_df['xap']=((prev_df['CODE_REJECT_REASON']=='XAP')).astype(int)\n    prev_df['dp']=(prev_df['AMT_DOWN_PAYMENT']<=(0.40*prev_df['AMT_CREDIT'])).astype(int) \n    prev_df,prev_df_cat_columns,all_columns=one_hot_encoding_dataframe(prev_df)\n   \n    prev_df_agg={}\n    for col in prev_df.columns:\n        if col!='SK_ID_CURR' and col !='SK_ID_PREV':\n            prev_df_agg[col]=['mean']\n        if (col=='DAYS_TERMINATION') | (col=='DAYS_FIRST_DUE') | (col=='DAYS_LAST_DUE') | (col=='AMT_CREDIT') | (col=='AMT_ANNUITY') | (col=='AMT_DOWN_PAYMENT') | (col=='DAYS_LAST_DUE_1ST_VERSION') |(col=='HOUR_APPR_PROCESS_START') :\n            prev_df_agg[col]=['max','mean']\n            \n    prev_agg = prev_df.groupby('SK_ID_CURR').agg(prev_df_agg)\n    \n    modified_col=[]\n    for col in list(prev_agg.columns):\n        modified_col.append(\"PREV_\"+col[0]+\"_\"+col[1].upper())\n    \n    prev_agg.columns=modified_col\n    \n    ref_canc = prev_df[(prev_df['NAME_CONTRACT_STATUS_Refused'] == 1) | (prev_df['NAME_CONTRACT_STATUS_Canceled'] == 1)]\n    prev_agg['cr_cred_mean'] = ref_canc.groupby('SK_ID_CURR')['AMT_CREDIT'].mean()\n    prev_agg['cr_cred_max'] = ref_canc.groupby('SK_ID_CURR')['AMT_CREDIT'].max()\n    prev_agg['cr_int_mean'] = ref_canc.groupby('SK_ID_CURR')['roi'].mean()\n    prev_agg['cr_int_max'] = ref_canc.groupby('SK_ID_CURR')['roi'].max()\n    prev_agg['cr_annu_mean'] = ref_canc.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean()\n    prev_agg['cr_annu_max'] = ref_canc.groupby('SK_ID_CURR')['AMT_ANNUITY'].max()\n    prev_agg['cr_dp_mean'] = ref_canc.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].mean()\n    prev_agg['cr_dp_max'] = ref_canc.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].max()\n    prev_agg['cr_lp_mean'] = ref_canc.groupby('SK_ID_CURR')['to_pay'].mean()\n    prev_agg['cr_lp_max'] = ref_canc.groupby('SK_ID_CURR')['to_pay'].max()\n \n    appr = prev_df[(prev_df['NAME_CONTRACT_STATUS_Approved'] == 1)]\n    prev_agg['va_cred_mean'] = appr.groupby('SK_ID_CURR')['AMT_CREDIT'].mean()\n    prev_agg['va_cred_max'] = appr.groupby('SK_ID_CURR')['AMT_CREDIT'].max()\n    prev_agg['va_annu_mean'] = appr.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean()\n    prev_agg['va_annu_max'] = appr.groupby('SK_ID_CURR')['AMT_ANNUITY'].max()\n    prev_agg['va_dp_mean'] = appr.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].mean()\n    prev_agg['va_int_mean'] = appr.groupby('SK_ID_CURR')['roi'].mean()\n    prev_agg['va_int_max'] = appr.groupby('SK_ID_CURR')['roi'].max()\n    prev_agg['va_dp_max'] = appr.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].max()\n    prev_agg['va_lp_mean'] = appr.groupby('SK_ID_CURR')['to_pay'].mean()\n    prev_agg['va_lp_max'] = appr.groupby('SK_ID_CURR')['to_pay'].max()\n    \n    \n    return prev_agg","c5082707":"prev_data = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/previous_application.csv')\n\ndf_prevapp=prev_appl(prev_data)\ndf_prevapp =main_df.merge(df_prevapp, how='left', on='SK_ID_CURR')\ndf_prevapp=df_prevapp[df_prevapp['TARGET'].notnull()]\n\ny_train=df_prevapp['TARGET']\ntrain_column=set(df_prevapp.columns)-set(main_df.columns)\nX_train=df_prevapp[train_column]\n\n\ntrain_column=X_train.columns\nX_train=X_train.replace([np.inf, -np.inf],np.nan)\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer1.fit_transform(X_train)\nscaler = MinMaxScaler(feature_range = (0, 1))\nX_train=scaler.fit_transform(X_train)\n","1e0cd596":"clf5=LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005,\n    num_leaves= 30,\n    max_depth= 6,\n    subsample_for_bin= 24000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    scale_pos_weight=poswt)","6e052b30":"clf5.fit(X_train,y_train,eval_metric='auc')\n#yp=lgbm_clf.predict(X_train)\nprint(\"ROCAUC Score :\",roc_auc_score(y_train,clf5.predict_proba(X_train)[:,1]))","63993dce":"prevapp=clf5.predict_proba(X_train)[:,1]","f7804f50":"df_prevapp2=prev_appl(prev_data)\n\ndf_prevapp2 =test_df.merge(df_prevapp2, how='left', on='SK_ID_CURR')\ntest_col=set(df_prevapp2.columns)-set(test_df.columns)\nX_test=df_prevapp2[test_col]","44118eb5":"df_test=X_test.replace([np.inf, -np.inf],np.nan)\ndf_test=imputer1.transform(df_test)\ndf_test=scaler.transform(df_test)","cb0ef1ff":"yp=clf5.predict_proba(df_test)[:,1]","224390c2":"def appl_train_test(app_train,app_test):\n    \n    df=app_train.append(app_test).reset_index()\n        \n    #as 365243 is an outlier\n    df[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n    df['hdwmqy']=(df[['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',\n       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_YEAR']]).sum(axis=1)\n    \n    #Using domain knowledge \n    #time spent in work\n    df['days_work']=df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['days_unemp']=abs(df['DAYS_BIRTH'])-abs(df['DAYS_EMPLOYED'])\n    df['inc_per_price']=df['AMT_INCOME_TOTAL']\/df['AMT_GOODS_PRICE']\n    df['cred_per_price']=df['AMT_CREDIT']\/df['AMT_GOODS_PRICE']\n    df['ann_per_price']=df['AMT_ANNUITY']\/df['AMT_GOODS_PRICE']\n    #percentage income of person and the credit amount\n    df['inc_per'] = df['AMT_INCOME_TOTAL'] \/ (df['CNT_FAM_MEMBERS']+1)\n    #income per credit\n    df['inc_per_cred'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n    df['emp_per_cred'] = df['DAYS_EMPLOYED']\/ df['AMT_CREDIT']  \n\n    #Anually paid amount to amount credited\n    df['pay_rate'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n    df['loan_pay'] = df['AMT_INCOME_TOTAL']-df['AMT_ANNUITY']\n    df['soc_cir']=((df[['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']]).sum(axis=1))\/\/4\n    df['mean_eq']=((df[['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',\n       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']]).mean(axis=1))    \n    df['contact']=((df[['FLAG_MOBIL', 'FLAG_EMP_PHONE',\n       'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL']]).sum(axis=1))\n    \n    #Creating features from useful features\n    df['ext_mean']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3']]).mean(axis=1)   \n    df['ext_med']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3']]).median(axis=1)  \n    df['ext_min']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3']]).min(axis=1) \n    df['ext_max']=(df[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3']]).max(axis=1)\n\n    df['DOCUMNNET_COUNT']=(df[['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',\n       'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n       'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n       'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']]==1).sum(axis=1)\n    \n    df,cal_cols,acols=one_hot_encoding_dataframe(df)\n    \n    return df","94b2f4ef":"dtrain=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\ndtest=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\n\ndf=appl_train_test(dtrain,dtest)\n","95a248e3":"train_data_df=df[df['TARGET'].notnull()]\ntrain_column=set(train_data_df.columns)-set({'TARGET','index','SK_ID_CURR'})","d079e04e":"test_data_df=df[df['TARGET'].isnull()]\ntest_column=set(test_data_df.columns)-set({'SK_ID_CURR','TARGET','index'})","e3be0eb3":"#ytrain to train the model\ny_train=train_data_df['TARGET']\nlen(y_train)","4667c54a":"X_train=train_data_df[train_column]\nfrom sklearn.impute import SimpleImputer\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train=imputer1.fit_transform(X_train)\n\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n","79dd1617":"lgbm_clf = LGBMClassifier(boosting_type= 'goss',\n                          random_state=42,\n                         scale_pos_weight=poswt)\nlgbm_clf.fit(X_train,y_train)","711f735f":"print(\"ROCAUC Score :\",roc_auc_score(y_train,lgbm_clf.predict_proba(X_train)[:,1]))","20756052":"\natrain=lgbm_clf.predict_proba(X_train)[:,1]","ff089f43":"X_test=test_data_df[test_column]\nX_test=imputer1.transform(X_test)\nX_test = scaler.transform(X_test)\n","b86d9d10":"yp=lgbm_clf.predict_proba(X_test)[:,1]","b7aaaf44":"final_df = pd.DataFrame()\nfinal_df.index = main_df['SK_ID_CURR']","861e86fe":"final_df['bal']=cred_bal\nfinal_df['inst']=inst\nfinal_df['pos']=pos\nfinal_df['bureau']=bureau\nfinal_df['prevapp']=prevapp\nfinal_df['train']=atrain","6480bdfa":"dtrain=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\nls=dtrain['TARGET'].values\nfinal_df['TARGET']=ls","aed59244":"#writing it to a file for future purposes\nfinal_df.to_csv('final-ds.csv')","77b134cc":"final_df.isnull().sum()","f6c7044c":"df=pd.read_csv('..\/input\/finaldata\/final-train-ds.csv')\ny=df['TARGET']\nx=df.drop(['TARGET','SK_ID_CURR'],axis=1)","bf67f876":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf","7be71c86":"model = Sequential()\nmodel.add(Dense(6, input_dim=6, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","94e6323a":"model.compile(optimizer='sgd',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.AUC()])\n","2b64a77a":"import sklearn\nclasses_zero = df[df['TARGET'] == 0]\nclasses_one = df[df['TARGET'] == 1]\n\n# Convert parts into NumPy arrays for weight computation\nzero_numpy = classes_zero['TARGET'].to_numpy()\none_numpy = classes_one['TARGET'].to_numpy()\nall_together = np.concatenate((zero_numpy, one_numpy))\nunique_classes = np.unique(all_together)\n\n# Compute weights\nweights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)","388ead63":"weights","a2c90072":"#converting it to a dictionary\nwt={}\nwt[0]=weights[0]\nwt[1]=weights[1]","7c0f2f69":"train=model.fit(x, y, epochs=3, validation_split = 0.2,class_weight=wt,batch_size=50)","34cba918":"df1=pd.read_csv('..\/input\/finaltestds\/main-appl.csv')\ndf2=pd.read_csv('..\/input\/finaltestds\/bureau-bal.csv')\ndf3=pd.read_csv('..\/input\/finaltestds2\/cred-b.csv')\ndf4=pd.read_csv('..\/input\/finaltestds\/install.csv')\ndf5=pd.read_csv('..\/input\/finaltestds\/posapp.csv')\ndf6=pd.read_csv('..\/input\/finaltestds\/prev-appl.csv')","72ed99db":"ftest=df3.merge(df4,how='left', on='SK_ID_CURR')\nftest=ftest.merge(df5,how='left', on='SK_ID_CURR')\nftest=ftest.merge(df2,how='left', on='SK_ID_CURR')\nftest=ftest.merge(df6,how='left', on='SK_ID_CURR')\nftest=ftest.merge(df1,how='left', on='SK_ID_CURR')","e582f1c3":"X_test=ftest.drop(['SK_ID_CURR'],axis=1)","385409d5":"yp=model.predict(X_test)","8aa8c8bf":"dtest=pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')","27f9f9cd":"def create_submission(x_test, y_test, target):\n    \"\"\"\n    x_test is a dataframe\n    y_test is an array with target values\n    \"\"\"\n    submission = pd.DataFrame()\n    submission.index = dtest['SK_ID_CURR']\n    submission[target] = y_test\n    submission.to_csv('final-new.csv')\n    print(\"Finished writing to submission.csv\")\n    return pd.read_csv('.\/final-new.csv', index_col=0)\n\ncreate_submission(dtest,yp, 'TARGET')","21fdb48f":"fin = LGBMClassifier(boosting_type= 'goss',n_estimators= 10000,\n    learning_rate= 0.005134,\n    num_leaves= 54,\n    max_depth= 10,\n    subsample_for_bin= 240000,\n    reg_alpha= 0.436193,\n    reg_lambda= 0.479169,\n    colsample_bytree= 0.508716,\n    min_split_gain= 0.024766,\n    subsample= 1,\n    is_unbalance= False,)\n#fin.fit(x,y)\n#print(\"ROCAUC Score :\",roc_auc_score(y_test,fin.predict_proba(x_test)[:,1]))","97c06112":"#yp=fin.predict_proba(X_test)[:,1]","e3b4eef1":"Predictions on train data","5143f44c":"### Feature engineering","ff8f5022":" prediction on test data","5971f1ab":"After training the model, next step is to get its predictions on train data to train it on the L0 model","1b8fc9c7":"## Creating the final dataset to train the L1 model using the predictions of target on train data from each table","a2635501":"Target values for train data","f92ce510":"# Thus the final values for TARGET are obtained, and a submission is made","987d4bc4":"# 5) Previous application","507dc7bf":"**Model building**- We have used only 1 hidden layer to reduce the model complexity and reduce overfitting. The 6 features in each input will be mapped to 6 neurons in the hidden layer and weights will be assigned to them. Activation function used is relu which is a common choice for all hidden layers. The output layer has 1 neuron as we need 1 probablity value. Activation function is sigmoid as the class is binary","74d8f043":"# 6) Application train data","5edc26bf":"Importing modules for the model","caa43906":"### Normalising and training the model\nWe have used the same L0 models for all the tables using the same parameters","f4216fe8":"### Feature engineering","a84a2f38":"prediction on train data","20a8c1c7":"Here is the list of solutions which inspired our feature selection\n1. https:\/\/www.kaggle.com\/hikmetsezen\/micro-model-174-features-0-8-auc-on-home-credit\n2. https:\/\/www.kaggle.com\/yakupkaplan\/home-credit-default-risk-prediction-model\n3. https:\/\/github.com\/wins999\/Home_Credit_Loan_Prediction\n4. https:\/\/www.kaggle.com\/jamesdellinger\/home-credit-putting-all-the-steps-together\n","4525928b":"### We have choosen to use the stacking ensemble technique where an L0 model will be used to get the predictions from each table of the dataset. These predictions will then be combined as an input to the L1 model which will give the final output to be submitted","a02d37ae":"Target values for test data","ead4eb5d":"Similarly, the L0 model is used to predict vales on the test data as well. These values will be used to check the score of the inidvidual table as well as be used as input for the L1 model","b026bb5b":"### Feature engineering","1b23b3c9":"Checking the auc score with test split of the train data","685d7f64":"# 3) POS application","4a92fa4a":"Output on test data","21d1d9c0":"Target values on test data","0517d9c4":"### For the L1 model we have choosen to use vanilla artificial neural networks as we need different weights assigned to predictions from different tables to represent the weightage of that table's prediction on the final target and ANN would train do assign weights to each input","d3bd46b1":"### After getting the target values, it is written into a file to be submitted and get the table's score. \n**It needs to be noted that the next 2 cells will be used for each table as well as the final ensemble model**","f6b2a376":"Target values on train data","48835bff":"### The main table has almost 200 columns after feature engineering. Other tables has lesser features and could be trained in 20minutes. However since the training time for this table would take a lot of time we decided to use lightgbm with default parameters.","097c94e7":"Merging all predictions of traget from all files","9a8d8364":"### Feature engineering","94ae985a":"### We start by selecting and extracting features from each table, then train it on the L0 models to get the prediction\nSince we are still students with very little domain knowledge in finance, we do not have enough expertise to come up with the features on our own and the model performance is heavily dependent on the feature engineering. So the aggregates and features have been inspired from various existing solutions of the competition","6bf15d98":"# 2) Installments payments balance","68939c35":"After preprocessing the data, it is used to train the Classifier model. We have choosen LightGBM as our model as it was realised from literature review and other solutions that LightGBM trained faster and gave a better AUC score than logistic regression, decision tress or Neural netwroks","90a4e6a3":"**goss** or gradient based one sided sampling was used as the number of train instances was very high and downsampling will improve the speed of the model while keeping the accuracy good. It is usually used for ensemble models","47a5a911":"# **L0 MODELS**","c21585ab":"### Preparing the dataset for predicting on test data","98ea3df1":"### Feature engineering\nThis uses 2 tables - **Bureau balance and Bureau data** . Both the tables have been merged and then aggregated to extract features","d847db09":"**Model training** - based on brute force trial, more epoches caused the model to overfit. Upon increasing epoches more than 4 caused the auc score on train data to improve while that of validation reduced","8ebb09e2":"The features of the credit table are merged with the main table to get all the indexes on the main table and so that we can use it to train the model. However, only columns corresponding to the credit table were used to train. \nOnce the features are created, the next step is to Impute missing values and normalise the data. All missing values were imputed using median and minmax was used to normalise data","7de90f22":"Traget values on train data","90339842":"Splitting back to test and train data","8904f7ee":"### It has to be metioned that the distribution of the output classes is not uniform. Lightgbm model has the capablity to assign weights to classes by changing parameters, similarly weights for classes need to be assigned to the Neural network model too. ","ab7bb8cd":"There is more number of instances of class=0 hence it will have a lesser weight than class=1","5db17686":"Here the main application table and the test table are combined to do the feature engineering after which they are seperated back to test and train datasets. \nThen the model is trained on the train data and predicted on test data","3f397cce":"**Model compiling** - stocastic gradient descent is the optimiser used, loss is binary cross entrophy as the output class is binary and the preformance metric used is AUC score, same as the metrics used to judge the scores in the competition","95eaaade":"# Making submissions","4c499392":"# 1) Credit card balance","804ed8b4":"# 4) bureau balance data","109e0454":"Getting the target values for test data and making a submission file using it","c05039b3":"## Similarly LightGBM was also tried as the L1 model but it did not give a good score","a4e8970d":"# **Level 1 model**"}}