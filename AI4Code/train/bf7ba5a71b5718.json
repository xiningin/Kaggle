{"cell_type":{"8a20ab05":"code","718b6a2a":"code","73595bae":"code","18c1e410":"code","0ed84a99":"code","8468e474":"code","a17d545f":"code","3da6f522":"code","2c396d72":"code","415047bb":"code","b167db73":"code","d99364ff":"code","f4da3d64":"code","b1d448d0":"markdown"},"source":{"8a20ab05":"import transformers\nimport pandas as pd\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.functional import mse_loss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler#, Sampler\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\nfrom time import time\nfrom tqdm import tqdm\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error","718b6a2a":"class configuration:\n    tokenizer_path = '..\/input\/roberta-base'\n    clrp_data_path= '..\/input\/commonlitreadabilityprize'\n    pretrained_model_path = '..\/input\/clrp-trained-robertabase\/robertabase_clrp_model'\n    output_path='\/kaggle\/working\/clrp-robertabase-modelweights')\n    output_hidden_states = True\n    epochs = 3\n    evaluate_interval = 10\n    batch_size = 16\n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    wd = 0.01\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n    num_folds=5\n    base_seed=1000\n    fold_seeds=[9183,4309,4071,98,4071]\n    max_length = 300\n    train_batch_size = 8\n    val_batch_size = 32\n    num_warmup_steps=50\n   ","73595bae":"scaler = torch.cuda.amp.GradScaler() \nDEVICE = torch.device(configuration.device if torch.cuda.is_available() else 'cpu')\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n","18c1e410":"train = pd.read_csv(configuration.clrp_data_path + \"\/train.csv\")\ntest = pd.read_csv(configuration.clrp_data_path + \"\/test.csv\")","0ed84a99":"models_dir = Path(configuration.output_path)\nmodels_dir.mkdir(exist_ok=True)","8468e474":"def seed_everything(seed=configuration.base_seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_data_loaders(data, fold):\n    \n    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n    y_train = data.loc[data.fold != fold, 'target'].values\n    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n    y_val = data.loc[data.fold == fold, 'target'].values\n    \n    tokenizer = AutoTokenizer.from_pretrained(configuration.tokenizer_path)\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=configuration.max_length, \n        return_tensors='pt'\n    )\n    \n    encoded_val = tokenizer.batch_encode_plus(\n        x_val, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=configuration.max_length, \n        return_tensors='pt'\n    )\n    \n    dataset_train = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n    )\n    dataset_val = TensorDataset(\n        encoded_val['input_ids'],\n        encoded_val['attention_mask'],\n        torch.tensor(y_val)\n    )\n    \n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler = RandomSampler(dataset_train),\n        batch_size=configuration.train_batch_size\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler = SequentialSampler(dataset_val),\n        batch_size=configuration.val_batch_size\n    )\n\n    return dataloader_train, dataloader_val","a17d545f":"#create folds\nseed = 1000\nseed_everything(seed=seed)\nx=train.index.to_list()\nrand_idx=random.sample(x, len(x))\ntrain.loc[:,'fold'] = pd.cut(rand_idx, bins=configuration.num_folds,labels=False)\ntarget = train.target.to_numpy()\n","3da6f522":"def convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","2c396d72":"class AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n","415047bb":"def create_optimizer(model):\n    parameters = []\n    lr = configuration.lr\n    multiplier=.990\n    classifier_lr=lr\n    for layer in range(11,-1,-1):\n        layer_params = {\n            'params': [p for n,p in model.named_parameters() if f'encoder.layer.{layer}.' in n],\n            'lr': lr\n        }\n        parameters.append(layer_params)\n        lr *= multiplier\n    classifier_params = {\n        'params': [p for n,p in model.named_parameters() if 'layer_norm' in n or 'linear' in n \n                   or 'pooling' in n],\n        'lr': classifier_lr\n    }\n    parameters.append(classifier_params)\n    \n    return optim.AdamW(parameters)","b167db73":"   model_config = AutoConfig.from_pretrained(configuration.pretrained_model_path)\n   model_config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n   transformer = AutoModel.from_pretrained(configuration.pretrained_model_path, config=model_config) \n   model = CLRPModel(transformer, model_config)\n   model = model.to(configuration.device) ","d99364ff":"class AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss \/ self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n           \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=configuration.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=configuration.batch_size, pin_memory=True)\n    return batch_dataloader\n                   \n            \nclass CLRPTrainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device =configuration.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        self.model_num = model_num\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(configuration.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        \n        for epoch in range(configuration.epochs):\n            \n            print(f'Epoch: {epoch+1}\/{configuration.epochs}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch)\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'\\t\\t{epoch+1}#[{batch_num+1}\/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        print(f\"Val loss decreased from {best_val_loss} to {val_loss}\")\n                        torch.save(self.model, f'{configuration.output_path}\/model_{self.model_num}.bin')\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                        \n\n                step += 1\n            end_epoch_time = time()\n            print(f'The epoch took {end_epoch_time - start_epoch_time} sec..')\n\n        return record_info, best_val_loss\n            \n\n    def train(self, batch):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        train_loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n\n    return nn.functional.mse_loss(y_true,y_pred)","f4da3d64":"  seed_everything(seed)\n  for model_num in range(configuration.num_folds): \n    best_loss=999\n    seed=configuration.fold_seeds[model_num]\n    print(f'seed={seed} , Model#{model_num+1}')\n      \n    tokenizer = AutoTokenizer.from_pretrained(configuration.tokenizer_path)\n    config = AutoConfig.from_pretrained(configuration.pretrained_model_path)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n\n    train_dl = make_dataloader(train[train.fold!=model_num], tokenizer)\n    val_dl = make_dataloader(train[train.fold==model_num], tokenizer, is_train=False)\n\n    transformer = AutoModel.from_pretrained(configuration.pretrained_model_path, config=config)  \n    model = CLRPModel(transformer, config)\n    model = model.to(configuration.device)\n    optimizer = create_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps=configuration.epochs * len(train_dl),\n            num_warmup_steps=configuration.num_warmup_steps)  \n\n    criterion = mse_loss\n\n    clrp_trainer =CLRPTrainer(train_dl, val_dl, model, optimizer, scheduler, criterion, model_num)\n    \n    record_info, best_val_loss = clrp_trainer.run()\n    steps, train_losses = list(zip(*record_info['train_loss']))\n    steps, val_losses = list(zip(*record_info['val_loss']))\n\n!date '+%A %W %Y %X' > execution_time\n","b1d448d0":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertalarge-masked-lm-model\/)\n* [Fine Tune Trained Roberta-Base Model -- **This Notebook**](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\n* [https:\/\/www.kaggle.com\/chamecall\/clrp-finetune-single-roberta-base?scriptVersionId=68893027](https:\/\/www.kaggle.com\/chamecall\/clrp-finetune-single-roberta-base?scriptVersionId=68893027)\n* [https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune)"}}