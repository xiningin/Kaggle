{"cell_type":{"42655c6d":"code","b69a1941":"code","8fedb4ee":"code","f154fa78":"code","34ac5380":"code","464cdaa0":"code","0f3a6ec4":"code","b562d66a":"code","6ec29f14":"code","ab7eb66f":"code","0c0612b4":"code","698fa85e":"code","a8a31123":"code","66699948":"code","aca19dc8":"code","f3386580":"code","f34a54bf":"code","8076b8fb":"code","eda0bf96":"code","4a948bc2":"code","c9f5715f":"code","9e624fb4":"code","c161907e":"code","0b14818e":"code","66f3dedc":"code","5b1ff242":"code","3d69667b":"code","0230432a":"code","ad612c8c":"code","fbe83c66":"code","4480bcec":"code","a7579d93":"code","6d8cdbde":"code","bb631667":"code","16611dbe":"markdown","ef5535da":"markdown","ba90f377":"markdown","6bbd4144":"markdown","3e94a2e0":"markdown","54b1379e":"markdown","bf53903a":"markdown","038eeb37":"markdown","798cbe87":"markdown","4ca21b75":"markdown","7ab5ccb7":"markdown","1e9ff78f":"markdown","77541555":"markdown","94b58b64":"markdown","37b6e605":"markdown","ef7f559c":"markdown","d31b9acc":"markdown","81c625be":"markdown","c1b230fa":"markdown"},"source":{"42655c6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b69a1941":"import matplotlib.pyplot as plt\nimport seaborn as sns","8fedb4ee":"df = pd.read_csv(\"..\/input\/wine-data\/Wine.csv\")\ndf","f154fa78":"df.columns","34ac5380":"from sklearn.model_selection import train_test_split\nX=df.drop(\"Customer_Segment\",axis=1).values\ny=df[\"Customer_Segment\"].values","464cdaa0":"X_train, X_test, y_train,y_test =train_test_split(X,y, test_size=0.2,random_state=42)","0f3a6ec4":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b562d66a":"from sklearn.decomposition import PCA\npca= PCA(n_components=2)# we make an instance of PCA and decide how many components we want to have\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","6ec29f14":"print(X_train.shape) # As we can see, we have reduced feature into 2 main features\nprint(X_test.shape)","ab7eb66f":"plt.figure(figsize=(15,10))\nplt.scatter(X_train[:,0],X_train[:,1],cmap=\"plasma\")\nplt.xlabel(\"The First Principal Component\")\nplt.ylabel(\"The Second Principal Component\")\n#Here we plot all the rows of columns 1 and column 2 in a scatterplot","0c0612b4":"pca.components_","698fa85e":"df_comp=pd.DataFrame(pca.components_)\ndf_comp","a8a31123":"plt.figure(figsize=(15,10))\nsns.heatmap(df_comp,cmap=\"magma\")","66699948":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npredictions = model.predict(X_test)","aca19dc8":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n#We have %100 procent accuracy although we have just used the main components of the data","f3386580":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()\n#This is performance of the algorithm with training set","f34a54bf":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()\n#This is visualization of performance of the algorithm with test set","8076b8fb":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/captures\/Capture.PNG\"))","eda0bf96":"X_train, X_test, y_train,y_test =train_test_split(X,y, test_size=0.2,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","4a948bc2":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda =LDA(n_components=2) # we select the same number of components\nX_train = lda.fit_transform(X_train,y_train) # we have to write both X_train and y_train\nX_test = lda.transform(X_test)","c9f5715f":"print(X_train.shape)\nprint(X_test.shape)\n","9e624fb4":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npredictions = model.predict(X_test)","c161907e":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","0b14818e":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('cyan', 'purple', 'white')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('cyan', 'purple', 'white'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.legend()\nplt.show()\n","66f3dedc":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.legend()\nplt.show()","5b1ff242":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/capture-1\/Capture1.PNG\"))","3d69667b":"X_train, X_test, y_train,y_test =train_test_split(X,y, test_size=0.2,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","0230432a":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ad612c8c":"from sklearn.decomposition import KernelPCA\nkpca= KernelPCA(n_components=2,kernel = 'rbf')\nX_train = kpca.fit_transform(X_train)\nX_test = kpca.transform(X_test)","fbe83c66":"print(X_train.shape)\nprint(X_test.shape)","4480bcec":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npredictions = model.predict(X_test)","a7579d93":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","6d8cdbde":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('yellow', 'black', 'orange')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('yellow', 'black', 'orange'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","bb631667":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('yellow', 'black', 'orange')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('yellow', 'black', 'orange'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","16611dbe":"## 1. Principal Component Analysis(PCA):","ef5535da":"<font color=\"blue\">\nAll the of dimentionality analysis instruments has performed %100 accuracy. ","ba90f377":"<font color=\"blue\">\nIt can be used as a dimensionality reduction method, which can help to minimize the number of the variables (or columns of a data frame) without losing much of the original information. This is useful especially when you are building machine learning models based on the data with many variables like 100s or 1000s.\n\n\nWhile regression determines a line of best fit to a dataset, factor analysis or principal component analysis determines several orthogonal lines of best fit to the dataset.Orthogonal means at right angles. The lines are perpendicular to each other in n dimensional space where n dimensional space is the variable sample space. There as many dimensions as there are variables,i.e., a dataset with 4 variables the sample space is 4 dimensionals.\n    \n\nIf we use this technique on a dataset with large numbers of variables, we can compress the amount of explained variation to just a few components.\n\n\nPCA is just a transformation of our data and attempts to find out what features excplain the most variance in our data\n    \n\nWe try to get rid of the components that do not explain enough the variance in our data.","6bbd4144":"## 2. Linear Discriminant Analysis (LDA):","3e94a2e0":"<font color=\"blue\">\nWe get %100 accuracy when we apply  LinearDiscriminantAnalysis(LDA)","54b1379e":"<font color=\"blue\">\nIt is obvious that we can get pretty good prediction by using just the two principal components of the data instead of using all of the dataset.\n\nPCA can be very useful tool big data with many features.","bf53903a":"<font color=\"blue\">\nKernel methods seek higher dimension, while SVD seeks lower dimension.\n\nFrom the comparison in the figure we can see that KPCA gets an eigenvector with higher variance (eigenvalue) than PCA.\n\nBecause for the the largest difference of the projections of the points onto the eigenvector (new coordinates), KPCA is a circle and PCA is a straight line, so KPCA gets higher variance than PCA. So does it mean KPCA gets higher principal components than PCA.\n\nPCA (as a dimensionality reduction technique) tries to find a low-dimensional linear subspace that the data are confined to. \n\nKernel PCA can find this non-linear manifold and discover that the data are in fact nearly one-dimensional.\n\nIt does so by mapping the data into a higher-dimensional space.The data are mapped into a higher-dimensional space, but then turn out to lie on a lower dimensional subspace of it. So you increase the dimensionality in order to be able to decrease it.","038eeb37":"<font color=\"blue\">\nNow at this point we can apply PCA:","798cbe87":"<font color=\"blue\">\nAfter applying PCA, we can implement our machine learning model as follows:","4ca21b75":"<font color=\"blue\">\nBefore we use PCA in the data, we need to standartize the variables by using standart scaler of sklearn.\n\nStandart Scaler transformed our data into a numpy array and standartized all of the variables of the data","7ab5ccb7":"<font color=\"blue\">\nWe need to apply PCA before applying machine learning algorithm.\n\nWe should also apply it to the features, not to the target value\"Customer Segment\" in this dataset.","1e9ff78f":"<font color=\"blue\">\nThe goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u201ccurse of dimensionality\u201d) and also reduce computational costs.\n\nThe general LDA approach is very similar to a Principal Component Analysis (for more information about the PCA,but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes (LDA).\n\nBoth Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an \u201cunsupervised\u201d algorithm, since it \u201cignores\u201d class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is \u201csupervised\u201d and computes the directions (\u201clinear discriminants\u201d) that will represent the axes that that maximize the separation between multiple classes.","77541555":"## 3. Kernel PCA","94b58b64":"<font color=\"blue\">\nIn this heatmap above, we see the relation between the principal components and actual features\n\nThe ligh color in the heatmap shows strong correlation between the principal components and actual features while dark colors show the opposite or negative correlation.\n\nActually the principal components are the combinations of all these features of the data.\n\nAfter we get the principal components of the data, we can feed them into a machine learning algorithm because we have clear and separated components of the data instead of the complex variables.\n\nFor this data we do a logistic regression on tranformed_data instead of doing regression with the entire data.\n\nSupport vector machines can also be a good alternative for this data.","37b6e605":"<font color=\"blue\">\nIn this dataset, there are 14 dimensions or variables, thus it is difficult to visualize all of them. We can utilize PCA to learn the two most important components of the data and visualize the data in this new two dimensional space.","ef7f559c":"<font color=\"blue\">\nLDA outperforms over the PCA when it comes to the performance in the training set, but they have the accuracy in the test set.","d31b9acc":"<font color=\"blue\">\nLets apply the same machine learning model","81c625be":"<font color=\"blue\">\nEach row represents actual componnents and each column relates back original features.\n\nWe can see the relationship better via a heatmap.\n\nBut first we need to transfor it into a dataframe in order to use the visualization libraries.","c1b230fa":"<font color=\"blue\">\nLets apply KPCA:"}}