{"cell_type":{"f50278e6":"code","ded581b9":"code","1f52caac":"code","413a2945":"code","e9e6c416":"code","f19ad966":"code","47a73a9a":"code","38581df1":"code","cacce244":"code","36d9d921":"code","a07cc62d":"code","6de0e0f1":"code","7cc23956":"code","1113dd56":"code","16786de1":"code","9730cd5e":"code","18685b1b":"code","1f57b1d4":"code","da00c392":"code","8bb158bb":"code","c816cda4":"code","6d12c64f":"code","08719936":"code","ce6e95bd":"code","02646fdd":"code","9468c18e":"code","8273bc06":"code","0be32e07":"code","dc71cafd":"code","69c5e068":"code","f9e34fde":"markdown","506fda64":"markdown","8fbc8a34":"markdown","dadb0cf4":"markdown","1161e443":"markdown","ec289f0c":"markdown","9e9b3ec7":"markdown","87ef9fd4":"markdown","99276253":"markdown","1074f1df":"markdown","c4446096":"markdown","c4e4b21d":"markdown","ec305a12":"markdown","0498bad3":"markdown","7dc66df5":"markdown","694ecda9":"markdown","3d8c918d":"markdown","b7c5737a":"markdown","141fbb11":"markdown","184e5eb4":"markdown","cb2e2187":"markdown","fbce15bf":"markdown","470b599c":"markdown","0f76a6a6":"markdown","a68f1ae3":"markdown","6917bba8":"markdown"},"source":{"f50278e6":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nfrom sklearn.externals import joblib\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nfrom xgboost import XGBRegressor\n\nimport gc\nfrom itertools import product\nimport time","ded581b9":"all_data = pd.read_pickle('..\/input\/extended-train-data\/all_data_final.pkl')\nall_data.head()","1f52caac":"print('mean for whole train set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']<34, 'target'].astype(np.float32))))\nprint('mean for validation train set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']<33, 'target'].astype(np.float32))))\nprint('mean for validation test set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']==33, 'target'].astype(np.float32))))","413a2945":"test_items = all_data.loc[all_data['date_block_num']==34,'item_id'].unique()\ntrain_items = all_data.loc[all_data['date_block_num']<34,'item_id'].unique()\nitems_in_test_and_not_in_train = set(test_items).difference(set(train_items))\nprint('Items in test and not in train: {0}'.format(len(items_in_test_and_not_in_train)))\nitems_in_train_and_not_in_test = set(train_items).difference(set(test_items))\nprint('Items in train and not in test: {0}'.format(len(items_in_train_and_not_in_test)))\n\ntest_shops = all_data.loc[all_data['date_block_num']==34,'shop_id'].unique()\nprint('Number of unique shops: {0}'.format(len(test_shops)))","e9e6c416":"missing_shop_item_count = 15876 # 372*42 all missing items per month\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in all_data.loc[all_data['date_block_num']<34, 'date_block_num'].unique():\n    print(block_num)\n  \n    zero_target_df = all_data[(all_data['date_block_num'] == block_num) & (all_data['target']==0) & \n                              (all_data['item_id'].isin(items_in_train_and_not_in_test))]\n\n    idx_to_delete = zero_target_df.sample(missing_shop_item_count, random_state=block_num).index\n    all_data.drop(idx_to_delete, inplace=True)\n    temp = np.array(list(product(*[test_shops, items_in_test_and_not_in_train, [block_num]])),dtype='int32')\n    grid.append(temp)\n    \n    del zero_target_df\n    del idx_to_delete\n    del temp\n    gc.collect()\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","f19ad966":"grid['shop_id'] = grid['shop_id'].astype(np.int16)\ngrid['item_id'] = grid['item_id'].astype(np.int32)\ngrid['date_block_num'] = grid['date_block_num'].astype(np.int8)","47a73a9a":"all_data = pd.concat([all_data, grid], ignore_index=True, sort=False, keys=index_cols)\nall_data[['item_shop_last_sale', 'item_last_sale']].fillna(-1, inplace=True) #-1 is default value in this columns\nall_data.fillna(0, inplace=True)\n\ndel grid\ndel test_items\ndel test_shops\ndel train_items\ndel items_in_test_and_not_in_train\ndel items_in_train_and_not_in_test\ngc.collect()","38581df1":"all_data['is_december'] = all_data['is_december'].astype(np.int8)\nall_data['item_category_id'] = all_data['item_category_id'].astype(np.int8)\nall_data['type_code'] = all_data['type_code'].astype(np.int8)\nall_data['subtype_code'] = all_data['subtype_code'].astype(np.int8)\nall_data['city_code'] = all_data['city_code'].astype(np.int16)\n\nall_data['month'] = all_data['month'].astype(np.int8)\nall_data['days'] = all_data['days'].astype(np.int8)\nall_data['item_shop_last_sale'] = all_data['item_shop_last_sale'].astype(np.int8)\nall_data['item_last_sale'] = all_data['item_last_sale'].astype(np.int8)\nall_data['item_shop_first_sale'] = all_data['item_shop_first_sale'].astype(np.int8)\nall_data['item_first_sale'] = all_data['item_first_sale'].astype(np.int8)","cacce244":"print('mean for whole train set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']<34, 'target'].astype(np.float32))))\nprint('mean for validation train set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']<33, 'target'].astype(np.float32))))\nprint('mean for validation test set: {0}'.format(np.mean(all_data.loc[all_data['date_block_num']==33, 'target'].astype(np.float32))))","36d9d921":"# put added rows in right position\nall_test_data = all_data[all_data['date_block_num'] == 34]\nall_data = all_data[all_data['date_block_num'] < 34]\nall_data.sort_values(['date_block_num'], inplace=True)\nall_data = pd.concat([all_data, all_test_data], ignore_index=True, sort=False, keys=index_cols)\n\ndel all_test_data\ngc.collect()","a07cc62d":"dates = all_data['date_block_num']\n\nlast_block = dates.max()\nprint('Test `date_block_num` is {0}'.format(last_block))\n\nX_train = all_data.loc[dates <  last_block]\nX_test =  all_data.loc[dates == last_block]\n\ny_train = all_data.loc[dates <  last_block, 'target'].values\ny_test =  all_data.loc[dates == last_block, 'target'].values\n\nX_valid_train = all_data.loc[dates <  last_block-1]\nX_valid_test =  all_data.loc[dates == last_block-1]\n\ny_valid_train = all_data.loc[dates <  last_block-1, 'target'].values\ny_valid_test =  all_data.loc[dates == last_block-1, 'target'].values\n\nall_data.to_pickle('all_data.pkl') # will use it later. Now free RAM\n\ndel dates\ndel all_data\ngc.collect()","6de0e0f1":"columns_to_delete = ['date_block_num', 'target']\nX_valid_train = X_valid_train.drop(columns_to_delete, axis=1)\nX_valid_test = X_valid_test.drop(columns_to_delete, axis=1)\n\nX_train = X_train.drop(columns_to_delete, axis=1)\nX_test = X_test.drop(columns_to_delete, axis=1)","7cc23956":"#validation\ndef validate(estimator, X_train_, y_train_, X_val_, y_val_, grid_params):\n    keys = grid_params.keys()\n    vals = grid_params.values()\n    parameters = []\n    rmses = []\n    rmses_train = []\n    return_obj ={}\n    prods = product(*vals)\n   \n    for idx, instance in enumerate(prods):\n        print('-'*50)\n        print('model {0}:'.format(idx))\n        model_params = dict(zip(keys, instance))\n        parameters.append(model_params)\n        \n        print(model_params)\n        model = estimator(**model_params)\n        model.fit(X_train_, y_train_)\n            \n        pred_test = model.predict(X_val_)       \n        mse = mean_squared_error(y_val_, pred_test)\n        rmse = np.sqrt(mse)\n        print('RMSE: {0}'.format(rmse))\n        rmses = rmses + [rmse]\n        \n        best_rmse_so_far = np.min(rmses)\n        print('Best rmse so far: {0}'.format(best_rmse_so_far))\n        best_model_params_so_far = parameters[np.argmin(rmses)]\n        print('Best model params so far: {0}'.format(best_model_params_so_far))\n        \n        del best_rmse_so_far\n        del best_model_params_so_far\n        del pred_test\n        del model\n        gc.collect()\n    \n    rmses = np.array(rmses)\n    best_rmse = np.min(rmses)\n    print('Best rmse: {0}'.format(best_rmse))\n    best_model_params = parameters[np.argmin(rmses)]\n    print('Best model params: {0}'.format(best_model_params))\n\n    return_obj['rmses'] = rmses\n    return_obj['best_rmse'] = best_rmse\n    return_obj['best_model_params'] = best_model_params\n      \n    return return_obj","1113dd56":"alphas = [10, 100, 2000, 5000]\ngrid_params = {'alpha':alphas}\nval_res = validate(Ridge, X_valid_train, y_valid_train, \n                   X_valid_test, y_valid_test, grid_params)","16786de1":"# best_alpha=2000\n# ridge_model = Ridge(best_alpha)\n# ridge_model.fit(X_train, y_train)\n# predictions = ridge_model.predict(X_test)","9730cd5e":"best_params = {'learning_rate': 0.16, 'n_estimators': 500, \n               'max_depth': 6, 'min_child_weight': 7,\n               'subsample': 0.9, 'colsample_bytree': 0.7, 'nthread': -1, \n               'scale_pos_weight': 1, 'random_state': 42, \n               \n               #next parameters are used to enable gpu for fasting fitting\n               'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0}","18685b1b":"ts = time.time()\nmodel = XGBRegressor(**best_params)\nmodel.fit(X_valid_train.values, \n                y_valid_train,\n                eval_metric=\"rmse\", \n                eval_set=[(X_valid_test.values, y_valid_test)], \n                verbose=True, \n                early_stopping_rounds = 50)\n\n\ntime.time() - ts","1f57b1d4":"joblib.dump(model, 'xgboost_model.pkl')\ndel model\ndel X_train\ndel X_test\ndel y_train\ndel y_test\ndel X_valid_train\ndel X_valid_test\ndel y_valid_train\ndel y_valid_test\n\ngc.collect()","da00c392":"all_data = pd.read_pickle('all_data.pkl')","8bb158bb":"def compute_prediction_for_specifi_month(monthes_before, df, estimator, params, prefix):\n    predictions = pd.DataFrame(columns=['date_block_num', 'pred_'+prefix])\n    for before in monthes_before:\n        last_valid_month = np.max(df['date_block_num'])\n        print('train: 12 to {0}'.format(last_valid_month-before-1))\n        print('test: {0}'.format(last_valid_month-before))\n        \n        cur_train = df[df['date_block_num'] < last_valid_month-before]\n        cur_test = df[df['date_block_num'] == last_valid_month-before]\n        \n        cur_y_train = cur_train['target']\n        cur_train.drop('target', axis=1, inplace=True)\n        cur_test.drop('target', axis=1, inplace=True)\n        \n        model = estimator(**params)\n        model.fit(cur_train.values, cur_y_train.values)\n        pred = model.predict(cur_test.values)\n        cur_df = pd.DataFrame(columns=['date_block_num', 'pred_'+prefix])\n        \n        cur_df['pred_'+prefix] = pred\n        cur_df['date_block_num'] = (last_valid_month-before)\n       \n        predictions = pd.concat([predictions, cur_df])\n        del model\n        del pred\n        del cur_test\n        del cur_df\n        del cur_y_train\n        gc.collect()\n        \n    return predictions","c816cda4":"best_param = {\n    'learning_rate' :0.16,\n    'n_estimators':500,\n    'max_depth':6,\n    'min_child_weight':7,\n    'subsample':0.9,\n    'colsample_bytree':0.7,\n    'nthread':-1,\n    'scale_pos_weight':1,\n     #next parameters are used to enable gpu for fasting fitting\n    'random_state':42,\n    'nthread': -1,\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'gpu_id': 0,\n}\n\nts= time.time()\nmonthes_before = [6, 5, 4, 3, 2, 1, 0]\n\nstacks_xgb = compute_prediction_for_specifi_month(monthes_before, all_data, XGBRegressor, best_params,'xgb')\ntime.time()-ts","6d12c64f":"stacks_xgb['pred_xgb'] = stacks_xgb['pred_xgb'].clip(0, 20)","08719936":"ts = time.time()\nmonthes_before = [6, 5, 4, 3, 2, 1, 0]\nbest_param ={'alpha': 2000}\nstacks_lin_reg = compute_prediction_for_specifi_month(monthes_before, all_data, Ridge, best_param,'lin')\ntime.time() - ts","ce6e95bd":"stacks_lin_reg['pred_lin'] = stacks_lin_reg['pred_lin'].clip(0, 20)","02646fdd":"X_valid_train = pd.concat([stacks_xgb[stacks_xgb['date_block_num'] < 33], \n                           stacks_lin_reg[stacks_lin_reg['date_block_num'] < 33]], axis=1)\nX_valid_test = pd.concat([stacks_xgb[stacks_xgb['date_block_num'] == 33], \n                           stacks_lin_reg[stacks_lin_reg['date_block_num'] == 33]], axis=1)\ny_valid_train = all_data.loc[(all_data['date_block_num']<33) & (all_data['date_block_num']>27), 'target']\ny_valid_test = all_data.loc[all_data['date_block_num']==33, 'target']","9468c18e":"X_train = pd.concat([stacks_xgb[stacks_xgb['date_block_num'] < 34], \n                           stacks_lin_reg[stacks_lin_reg['date_block_num'] < 34]], axis=1)\nX_test = pd.concat([stacks_xgb[stacks_xgb['date_block_num'] == 34], \n                           stacks_lin_reg[stacks_lin_reg['date_block_num'] == 34]], axis=1)\ny_train = all_data.loc[(all_data['date_block_num']<34) & (all_data['date_block_num']>27), 'target']","8273bc06":"X_valid_train.drop('date_block_num', axis=1, inplace=True)\nX_valid_test.drop('date_block_num', axis=1, inplace=True)\nX_train.drop('date_block_num', axis=1, inplace=True)\nX_test.drop('date_block_num', axis=1, inplace=True)","0be32e07":"model = LinearRegression()\nmodel.fit(X_valid_train, y_valid_train)\npred = model.predict(X_valid_test).clip(0,20)\n\nrmse = np.sqrt(mean_squared_error(y_valid_test, pred))\nprint('RMSE on valid set: {0}'.format(rmse))","dc71cafd":"model = LinearRegression()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test).clip(0,20)","69c5e068":"submit = pd.DataFrame({'ID':range(len(pred)), 'item_cnt_month': pred})\nsubmit.to_csv('submit.csv', index=False)","f9e34fde":"Good, As it was before.","506fda64":"And do it for second model","8fbc8a34":"**<font size=4>Validation strategy<\/font>**","dadb0cf4":"Let's find out optimal paramter for Ridge regression","1161e443":"In the same way we can do for XGBoost. But this model has much more parameters to config. https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/ this article helps to define strategy to find them starting with some parameters that make highest impact on the model and ending with ones that give lowest.\nSo I came up with this parameters. Note that tree_method, predictor, gpu_id was fixed parameters from the begining to enable gpu. It works extremely slow without it.\n\nNote: Don't forget to enable gpu in kernel settings.","ec289f0c":"I create my own custom grid search method because I use XGBoost with GPU and I didn't find a way to make them work together. If you know how to do it please drop a comment.","9e9b3ec7":"Look how better xgboost then simple regression model.","87ef9fd4":"Our object is to predict sales for next month (i.e. November 2015, or 34 in terms of date_block_num). And we can't just randomly split our data on tran\/validation sets for parameters tuning because we need validate parameters on data in month that we didn't use in train set like in test. So,I use rows with date_block_num less than 33 as train set and rows with date_block_num equal 33 I use as validation set.\n\nTo successfully predict sales on test data you also need to have same train and test data. In other words you train and test data should have the same distribution. But stop, distribution in train data we know but how do we know distribution in test data? Such information we can get by **leaderboard probing**. For example we can easily find out mean of sales in public leader board by simlpy submiting two files with all 0's prediction and 0.5's predictions and do some math.\n\n\nAnd the mean of sales equals 0.28394. So mean of train\/validation should be close to this number in order to get good results. Let's check mean of the target in our whole train set","99276253":"In this kernel I will share how I managed to build stacking model wich consists on linear regression with regularization and xgboost. I will use alredy cleaned and prepared train data 'all_data_final.pkl' from my another kernel. https:\/\/www.kaggle.com\/emaksone\/eda-with-feature-engineering this is how obtained it.\n\nPipeline:\n\n* Define validation strategy and leaderboard probing\n* Hyperparameter tuning\n* Stacking\n","1074f1df":"Check our means","c4446096":"Clip predictions","c4e4b21d":"Now we ready to define train\/test split. And after we tune parameters we will use whole data to train a model on this parameters and can use it to predict sales on test data.","ec305a12":"And construct train\/validation\/test sets","0498bad3":"**<font size=4>Stacking<font>**","7dc66df5":"**<font size=4>Hyperparameters tuning<font>**","694ecda9":"As you see the rmse is lower then single xgboost model's rmse.\n\nLet's train a stacking model on a whole train data.","3d8c918d":"As you can see every numbers are close to mean of test sales. ","b7c5737a":"**<font size=4>Summary<\/font>**","141fbb11":"Delete columns that we can't use in test","184e5eb4":"And use simple linear regression.","cb2e2187":"One thing to notice before start building a model. Let's check if we have new items in test set","fbce15bf":"So 2000 is an optimal parameter for our model. We can use it to train model with all train data.","470b599c":"In this kernel we learned how to make proper cross validation strategy, tune parameters for single models and finnaly use one of the ensambling methods called stacking to make better predictions.","0f76a6a6":"So, we have items in the test set that do not appear in train. So our model will struggle when see unknown items when predict. To eliminate this issue I will add all missing shop\/item pair to every month with 0's.\nBut after this we decrese our target mean that will make our train\/test data distributions different. To avoid this I remove rows with target equals 0 and item that does not appear in test. At the end it doesn't change mean of targets and train data set will contain all items that exist in test.","a68f1ae3":"Clip predictions again","6917bba8":"So now after we find optimal parameters for two models we can use them for stacking. I will do it in the following way: \n\n* take data that date_block_num < 27 as train\n* train a models(Ridge and XGBoost) and predict for date_block_num == 27\n* put this prediction in two columns (xgb_prediction and ridge_prediction)\n* do it for 28, 29, 30, 31, 32, 33, 34 monthes\n* concat all this prediction.\n* fit data from 28 to 33 to Linear regression\n* use this simple model to predict data with month 34"}}