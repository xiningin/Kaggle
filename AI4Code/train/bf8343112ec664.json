{"cell_type":{"955afd82":"code","d4db8baa":"code","4510700c":"code","5995366d":"code","9a794dbd":"code","3a9d1ff4":"code","7d34e977":"code","f544ec60":"code","811980fd":"code","d6eca224":"code","9379f879":"code","ab5d2d01":"code","74237b2f":"code","cc1e4f18":"code","a8a4ada8":"code","b96e0f91":"code","5cdb1312":"code","9b93634e":"code","ecc3f63d":"code","d9cb1637":"code","638c3471":"code","6787aa42":"code","c73dafd7":"code","236537ef":"code","34cedc6d":"code","8b510809":"code","5e184f28":"markdown","121ee117":"markdown","433cd51f":"markdown","e5fcf5e5":"markdown","8170779d":"markdown"},"source":{"955afd82":"# Load required packages\nimport os\nimport math\nimport time\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport torchtext\nimport torch\nfrom torch import nn, optim\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","d4db8baa":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","4510700c":"!ls","5995366d":"# read data\ndf = pd.read_csv('..\/input\/customer-support-on-twitter\/twcs\/twcs.csv')\ndf.head()","9a794dbd":"\n# Get tweets that are first inbound tweets to companies\nfirst_inbound = df[pd.isnull(df.in_response_to_tweet_id) & df.inbound]\n\n# Get replies for first inbound tweets\ninbound_and_replies = pd.merge(first_inbound, df, left_on='tweet_id', right_on='in_response_to_tweet_id')\n\n# Remove replies that aren't from companies\ninbound_and_replies = inbound_and_replies[inbound_and_replies.inbound_y ^ True]\n\n# Replace carriage returns\ninbound_and_replies = inbound_and_replies.replace('\\r', '', regex=True)","3a9d1ff4":"# Extract inbound tweets and responses\ntweets = inbound_and_replies[['text_x', 'text_y']]\ntweets.columns = ['tweet', 'response']\ntweets.head()","7d34e977":"tweets.shape","f544ec60":"tweets.head(100_000).to_csv(\"tweets.tsv\", index=False, sep='\\t')","811980fd":"# Use same field for both columns since they have a shared vocabulary\nTWEET = torchtext.data.Field(\n    tokenize='spacy',\n    lower=True,\n    init_token='<sos>', \n    eos_token='<eos>'\n)\n\nfields = [('tweet', TWEET), ('response', TWEET)]","d6eca224":"%%time\n# Create dataset\ndataset = torchtext.data.TabularDataset(\n    path='tweets.tsv',\n    format='TSV',\n    fields=fields,\n    skip_header=True\n)","9379f879":"# Train\/val split\n(train, valid) = dataset.split(split_ratio=[0.85, 0.15])\nprint(len(train), len(valid))","ab5d2d01":"MAX_VOCAB_SIZE = 10_000\n\nTWEET.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n\nprint(f'Size of vocab: {len(TWEET.vocab)}')","74237b2f":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","cc1e4f18":"BATCH_SIZE = 128\n\ntrain_iterator, valid_iterator = torchtext.data.BucketIterator.splits(\n    (train, valid),\n    batch_size = BATCH_SIZE,\n    sort_key=lambda x: len(x.tweet),\n    device = device\n)","a8a4ada8":"# Encoder \nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n        self.rnn = nn.GRU(emb_dim, hid_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        #src = [src len, batch size]\n        \n        embedded = self.dropout(self.embedding(src))\n        #embedded = [src len, batch size, emb dim]\n        outputs, hidden = self.rnn(embedded) #no cell state!\n        #outputs = [src len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #outputs are always from the top hidden layer\n        return hidden\n\n\n# Decoder\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.output_dim = output_dim\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n        \n        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, context):\n        \n        #input = [batch size]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #context = [n layers * n directions, batch size, hid dim]\n        \n        #n layers and n directions in the decoder will both always be 1, therefore:\n        #hidden = [1, batch size, hid dim]\n        #context = [1, batch size, hid dim]\n        \n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(input))\n        \n        #embedded = [1, batch size, emb dim]\n                \n        emb_con = torch.cat((embedded, context), dim = 2)\n            \n        #emb_con = [1, batch size, emb dim + hid dim]\n            \n        output, hidden = self.rnn(emb_con, hidden)\n        \n        #output = [seq len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        \n        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n        #output = [1, batch size, hid dim]\n        #hidden = [1, batch size, hid dim]\n        \n        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n                           dim = 1)\n        \n        #output = [batch size, emb dim + hid dim * 2]\n        \n        prediction = self.fc_out(output)\n        \n        #prediction = [batch size, output dim]\n        \n        return prediction, hidden","b96e0f91":"# Seq2Seq\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            \"Hidden dimensions of encoder and decoder must be equal!\"\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src len, batch size]\n        #trg = [trg len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #last hidden state of the encoder is the context\n        context = self.encoder(src)\n        \n        #context also used as the initial hidden state of the decoder\n        hidden = context\n        \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            \n            #insert input token embedding, previous hidden state and the context state\n            #receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, hidden, context)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1) \n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = trg[t] if teacher_force else top1\n\n        return outputs","5cdb1312":"INPUT_DIM = len(TWEET.vocab)\nOUTPUT_DIM = len(TWEET.vocab)\nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nHID_DIM = 256\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Seq2Seq(enc, dec, device).to(device)","9b93634e":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.normal_(param.data, mean=0, std=0.01)\n        \nmodel.apply(init_weights)","ecc3f63d":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","d9cb1637":"optimizer = optim.Adam(model.parameters())","638c3471":"TRG_PAD_IDX = TWEET.vocab.stoi[TWEET.pad_token]\n\ncriterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)","6787aa42":"def train(model, iterator, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        \n        src = batch.tweet\n        trg = batch.response\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        \n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","c73dafd7":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n\n            src = batch.tweet\n            trg = batch.response\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            \n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","236537ef":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","34cedc6d":"N_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut2-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","8b510809":"# model.load_state_dict(torch.load('tut2-model.pt'))\n\n# test_loss = evaluate(model, test_iterator, criterion)\n\n# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')","5e184f28":"## Seq2Seq Model","121ee117":"### Data Preprocessing","433cd51f":"### Training the Seq2Seq Model","e5fcf5e5":"### Encoder, Decoder, Seq2Seq","8170779d":"### Read Dataset"}}