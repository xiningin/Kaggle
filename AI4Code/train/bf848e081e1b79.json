{"cell_type":{"e3325682":"code","2cbb72b7":"code","f02ec254":"code","89be69fc":"code","2b77a0f5":"code","412f6977":"code","87f63739":"code","e5327716":"code","55efd3fb":"code","f372fe2a":"code","bb08630c":"code","ed2431d0":"code","47a711e2":"code","a122e081":"code","63f8b655":"code","17820ca4":"code","d1cba1ef":"code","292c4fa0":"code","92f2fb8c":"code","2c1580ab":"code","8fd1922a":"code","8c24c653":"code","468eeae9":"code","ece32c4d":"code","d68a9551":"code","ccecda2b":"code","b95927c4":"code","8fc3208a":"code","60f15d85":"code","71c01653":"code","f497c3a3":"code","4ae4c90b":"code","6df31e0d":"code","3e1a624a":"code","a229f0ac":"code","138e21aa":"code","f5cb9375":"code","35ae889a":"code","b78f4390":"code","25e2340e":"code","997ed12e":"code","01aa11d7":"code","637cb6c5":"code","032853bb":"code","4f2425e6":"code","d0770824":"code","bafce294":"code","a790d07d":"code","c66a35e9":"markdown","c2c6faba":"markdown","93d46b81":"markdown","1df5d073":"markdown","4b0b566b":"markdown","db8b4bfd":"markdown","144f7e01":"markdown","f958a7ca":"markdown","b9406b9e":"markdown","40c911c0":"markdown","07dd761a":"markdown","2e6d1e16":"markdown","b022bf48":"markdown","fa11e68c":"markdown","b16ea3ae":"markdown","6dd7344e":"markdown","81a98174":"markdown","931ca921":"markdown","6736d069":"markdown","96397720":"markdown","ceff1083":"markdown","db370852":"markdown","f44fdb6a":"markdown","d82e96f2":"markdown","a16fa24c":"markdown","67b5314f":"markdown","fab9b1c8":"markdown","50f335c9":"markdown","3bc4ddb3":"markdown","3ae19656":"markdown","987f2e6c":"markdown","cbd09fbd":"markdown","c629ed64":"markdown","1bf48c35":"markdown","d541c225":"markdown","fa5dbcde":"markdown"},"source":{"e3325682":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2cbb72b7":"import pandas as pd\nimport numpy as np\nfrom textblob import TextBlob\nfrom pandas.io.json import json_normalize\nfrom wordcloud import WordCloud\nimport math\nimport re\nimport json\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","f02ec254":"data=pd.read_csv('..\/input\/facebook-outage-4th-october-tweet-dataset\/Facebook_outage_Tweet_data_4th_October.csv')","89be69fc":"data.head()","2b77a0f5":"print(f'The dataset consists of {data.shape[0]} tweets having {data.shape[1]} features')","412f6977":"data.columns","87f63739":"data.info()","e5327716":"data['language'].value_counts().head(n=20)","55efd3fb":"# Plotting a bar graph of the number of tweets in each language, for the first ten language listed\n# in the column 'language'\nlanguage_count  = data['language'].value_counts()\nlanguage_count = language_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(language_count.index, language_count.values, alpha=0.8)\nplt.title('Top 10 languages')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('language', fontsize=12)\nplt.show()","f372fe2a":"t = data[\"tweet\"].to_list()\nfor i in range(10):\n    print('Tweet Number '+str(i+1)+': '+t[i])","bb08630c":"# function to collect hashtags\ndef hashtag_extract(text_list):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in text_list:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags\n\ndef generate_hashtag_freqdist(hashtags):\n    a = nltk.FreqDist(hashtags)\n    d = pd.DataFrame({'Hashtag': list(a.keys()),\n                      'Count': list(a.values())})\n    # selecting top 15 most frequent hashtags     \n    d = d.nlargest(columns=\"Count\", n = 25)\n    plt.figure(figsize=(16,7))\n    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n    plt.xticks(rotation=80)\n    ax.set(ylabel = 'Count')\n    plt.show()","ed2431d0":"hashtags = hashtag_extract(data[\"tweet\"])\nhashtags = sum(hashtags, [])","47a711e2":"generate_hashtag_freqdist(hashtags)","a122e081":"# for all NLP related operations on text\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.classify import NaiveBayesClassifier\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\n\n# To identify the sentiment of text\nfrom textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nfrom textblob.np_extractors import ConllExtractor\n\n# ignoring all the warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# downloading stopwords corpus\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('movie_reviews')\nnltk.download('punkt')\nnltk.download('conll2000')\nnltk.download('brown')\nstopwords = set(stopwords.words(\"english\"))\n\n# for showing all the plots inline\n%matplotlib inline","63f8b655":"# 1 way\ndef fetch_sentiment_using_SIA(text):\n    sid = SentimentIntensityAnalyzer()\n    polarity_scores = sid.polarity_scores(text)\n    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'\n","17820ca4":"sentiments_using_SIA = data.tweet.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\npd.DataFrame(sentiments_using_SIA.value_counts())","d1cba1ef":"def fetch_sentiment_using_textblob(text):\n    analysis = TextBlob(text)\n    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'","292c4fa0":"sentiments_using_textblob = data.tweet.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\npd.DataFrame(sentiments_using_textblob.value_counts())","92f2fb8c":"data['sentiment'] = sentiments_using_SIA\ndata.head()","2c1580ab":"def remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    \n    return text ","8fd1922a":"# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ndata['tidy_tweets'] = np.vectorize(remove_pattern)(data['tweet'], \"@[\\w]*: | *RT*\")\ndata.head(10)","8c24c653":"cleaned_tweets = []\n\nfor index, row in data.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n\ndata['tidy_tweets'] = cleaned_tweets\ndata.head(10)","468eeae9":"data = data[data['tidy_tweets']!='']\ndata.head()","ece32c4d":"data.drop_duplicates(subset=['tidy_tweets'], keep=False)\ndata.head()","d68a9551":"data = data.reset_index(drop=True)\ndata.head()","ccecda2b":"data['absolute_tidy_tweets'] = data['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")","b95927c4":"stopwords_set = set(stopwords)\ncleaned_tweets = []\n\nfor index, row in data.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets.append(' '.join(words_without_stopwords))\n    \ndata['absolute_tidy_tweets'] = cleaned_tweets\ndata.head(10)","8fc3208a":"tokenized_tweet = data['absolute_tidy_tweets'].apply(lambda x: x.split())\ntokenized_tweet.head()","60f15d85":"word_lemmatizer = WordNetLemmatizer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\ntokenized_tweet.head()","71c01653":"for i, tokens in enumerate(tokenized_tweet):\n    tokenized_tweet[i] = ' '.join(tokens)\n\ndata['absolute_tidy_tweets'] = tokenized_tweet\ndata.head(10)","f497c3a3":"class PhraseExtractHelper(object):\n    def __init__(self):\n        self.lemmatizer = nltk.WordNetLemmatizer()\n        self.stemmer = nltk.stem.porter.PorterStemmer()\n    \n    def leaves(self, tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n\n    def normalise(self, word):\n        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n        word = word.lower()\n        # word = self.stemmer.stem_word(word) # We will loose the exact meaning of the word \n        word = self.lemmatizer.lemmatize(word)\n        return word\n\n    def acceptable_word(self, word):\n        \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n        accepted = bool(3 <= len(word) <= 40\n            and word.lower() not in stopwords\n            and 'https' not in word.lower()\n            and 'http' not in word.lower()\n            and '#' not in word.lower()\n            )\n        return accepted\n\n    def get_terms(self, tree):\n        for leaf in self.leaves(tree):\n            term = [ self.normalise(w) for w,t in leaf if self.acceptable_word(w) ]\n            yield term","4ae4c90b":"sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\ngrammar = r\"\"\"\n    NBAR:\n        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n        \n    NP:\n        {<NBAR>}\n        {<NBAR><IN><NBAR>}  # Above, connected with in\/of\/etc...\n\"\"\"\nchunker = nltk.RegexpParser(grammar)","6df31e0d":"key_phrases = []\nphrase_extract_helper = PhraseExtractHelper()\n\nfor index, row in data.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases.append(' '.join(term))\n    \n    key_phrases.append(tweet_phrases)\n    \nkey_phrases[:10]","3e1a624a":"textblob_key_phrases = []\nextractor = ConllExtractor()\n\nfor index, row in data.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases.append(list(blob.noun_phrases))\n\ntextblob_key_phrases[:10]","a229f0ac":"data['key_phrases'] = textblob_key_phrases\ndata.head(10)","138e21aa":"def generate_wordcloud(all_words):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n\n    plt.figure(figsize=(14, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","f5cb9375":"all_words = ' '.join([text for text in data['absolute_tidy_tweets'][data.sentiment == 'pos']])\ngenerate_wordcloud(all_words)","35ae889a":"all_words = ' '.join([text for text in data['absolute_tidy_tweets'][data.sentiment == 'neg']])\ngenerate_wordcloud(all_words)","b78f4390":"# For sake of consistency, we are going to discard the records which contains no phrases i.e where tweets_df['key_phrases'] contains []\ntweets_df2 = data[data['key_phrases'].str.len()>0]","25e2340e":"# BOW features\nbow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\n# bag-of-words feature matrix\nbow_word_feature = bow_word_vectorizer.fit_transform(tweets_df2['absolute_tidy_tweets'])\n\n# TF-IDF features\ntfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\n# TF-IDF feature matrix\ntfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df2['absolute_tidy_tweets'])","997ed12e":"phrase_sents = tweets_df2['key_phrases'].apply(lambda x: ' '.join(x))\n\n# BOW phrase features\nbow_phrase_vectorizer = CountVectorizer(max_df=0.90, min_df=2)\nbow_phrase_feature = bow_phrase_vectorizer.fit_transform(phrase_sents)\n\n# TF-IDF phrase feature\ntfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2)\ntfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)","01aa11d7":"target_variable = tweets_df2['sentiment'].apply(lambda x: 0 if x=='neg' else 1)","637cb6c5":"def plot_confusion_matrix(matrix):\n    plt.clf()\n    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Set2_r)\n    classNames = ['Positive', 'Negative']\n    plt.title('Confusion Matrix')\n    plt.ylabel('Predicted')\n    plt.xlabel('Actual')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TP','FP'], ['FN', 'TN']]\n\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(matrix[i][j]))\n    plt.show()","032853bb":"def naive_model(X_train, X_test, y_train, y_test):\n    naive_classifier = GaussianNB()\n    naive_classifier.fit(X_train.toarray(), y_train)\n\n    # predictions over test set\n    predictions = naive_classifier.predict(X_test.toarray())\n\n    # calculating Accuracy Score\n    print(f'Accuracy Score - {accuracy_score(y_test, predictions)}')\n    conf_matrix = confusion_matrix(y_test, predictions, labels=[True, False])\n    plot_confusion_matrix(conf_matrix)","4f2425e6":"X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=272)\nnaive_model(X_train, X_test, y_train, y_test)","d0770824":"X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=272)\nnaive_model(X_train, X_test, y_train, y_test)","bafce294":"X_train, X_test, y_train, y_test = train_test_split(bow_phrase_feature, target_variable, test_size=0.3, random_state=272)\nnaive_model(X_train, X_test, y_train, y_test)","a790d07d":"X_train, X_test, y_train, y_test = train_test_split(tfidf_phrase_feature, target_variable, test_size=0.3, random_state=272)\nnaive_model(X_train, X_test, y_train, y_test)","c66a35e9":"# Using TextBlob","c2c6faba":"# Fetch sentiments","93d46b81":"### Most common words in positive tweets\n### Answer can be best found using WordCloud","1df5d073":"**TF-IDF Phrase features**","4b0b566b":"# Removing Stop words\nWith the same reason we mentioned above, we won't perform this on 'tidy_tweets' column, because it needs to be used for key_phrases sentiment analysis.","db8b4bfd":"#  Grammatical rule to identify phrases","144f7e01":"Helper class, will help in preprocessing phrase terms","f958a7ca":"# Using NLTK's SentimentIntensityAnalyzer","b9406b9e":"**Predictions on 'key phrases' based features**","40c911c0":"# There is one other simple way to extract key phrases, by using TextBlob (ConllExtractor).","07dd761a":"# Story Generation and Visualization","2e6d1e16":"**Map target variables to {0, 1}**","b022bf48":"* Facebook,Instagram,Whatsapp have clearly been used in both positive and negative tweets, which is very obvious. However, in negative tweets we can also see negative words as well like shit, bad,etc.","fa11e68c":"# Removing links (http | https)","b16ea3ae":"# Resetting index\nIt seems that our index needs to be reset, since after removal of some rows, some index values are missing, which may cause problem in future operations.","6dd7344e":"NLTK gives us more negative sentiments, so we will prefer NLTK, since classfication seems better.","81a98174":"### Feature Extraction for 'Key Words'","931ca921":"# New feature called 'key_phrases', will contain phrases for corresponding tweet","6736d069":"# Joining all tokens into sentences","96397720":"# Removing '@names'\nHere we can see that at many places we have '@names', which is of no use, since it don't have any meaning, So needs to be removed.","ceff1083":"### Model Building: Sentiment Analysis\n","db370852":"# Converting words to Lemma","f44fdb6a":"# Feature Extraction","d82e96f2":"**BOW word features**","a16fa24c":"# Most common words in negative tweets","67b5314f":"# Tokenize *'absolute_tidy_tweets'*","fab9b1c8":"# Pre-processing 'Key Phrases'\u00b6","50f335c9":" **BOW Phrase features**","3bc4ddb3":"# Dropping duplicate rows","3ae19656":"**let's consider these key phrase only.**","987f2e6c":"### Feature Extraction for 'Key Phrases'","cbd09fbd":"# Removing Punctuations, Numbers and Special characters\nThis step should not be followed if we also want to do sentiment analysis on key phrases as well, because semantic meaning in a sentence needs to be present. So here we will create one additional column 'absolute_tidy_tweets' which will contain absolute tidy words which can be further used for sentiment analysis on key words.","c629ed64":"**TF-IDF word features**","1bf48c35":"# Removing tweets with empty text","d541c225":"We need to convert textual representation in the form on numeric features. We have two popular techniques to perform feature extraction:\n\n* Bag of words (Simple vectorization)\n* TF-IDF (Term Frequency - Inverse Document Frequency)\nWe will use extracted features from both one by one to perform sentiment analysis and will compare the result at last.","fa5dbcde":"#  Predictions on 'key words' based features"}}