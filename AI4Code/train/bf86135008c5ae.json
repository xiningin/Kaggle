{"cell_type":{"3cbfb78b":"code","d1383e73":"code","9be5a5c8":"code","d6e617b5":"code","00229198":"code","1688d4b2":"code","cde6d3be":"code","468f683c":"code","4c379216":"code","99756578":"code","7fbcaa28":"code","dfbf3c6e":"code","ab58f67a":"code","166025be":"code","967d2a82":"code","f0a975b4":"code","20b625ba":"code","fbdd3d5a":"code","2ac5af96":"code","80427d25":"code","a6754467":"code","221e847e":"code","d4f14f8c":"code","384a375f":"code","d61f250a":"code","7c39d052":"code","7f97c7e6":"code","61916444":"code","b619f9e4":"code","c2de5616":"code","0abdafea":"code","6bd55465":"code","b6fe85c1":"code","dfcea4b7":"code","24366293":"code","001ab2ff":"code","00ca638b":"code","2b9091d8":"code","2c32f4c5":"code","ee44f5f1":"code","729d5606":"code","4bfe8781":"code","57b0d189":"code","ba48adac":"code","fae51e57":"code","bbfa2665":"code","77dda1c6":"code","4bd00bf5":"code","dc82babf":"code","4190d613":"code","2eb3568c":"code","e3637b6f":"code","a061c6c7":"code","ac3fdbeb":"code","e9a81e3b":"markdown","21ee8739":"markdown","c3778469":"markdown","b59eb26f":"markdown","3340209c":"markdown","c81a0be8":"markdown","fd949946":"markdown","1a8dd3cb":"markdown","c97b58dd":"markdown","5ac7fbc8":"markdown","3abf40ea":"markdown","3d7b7d94":"markdown","07a7f863":"markdown","fa9cd356":"markdown","bcc3042e":"markdown","7809ef3b":"markdown","3f747242":"markdown","801cbdea":"markdown","b74f0a03":"markdown","4e7c198c":"markdown","b7fb1aef":"markdown","b8a7438a":"markdown","be22d6a3":"markdown","05ced891":"markdown","ef72cc91":"markdown","11d9573b":"markdown","e170e6bd":"markdown","936200c2":"markdown","58129148":"markdown"},"source":{"3cbfb78b":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","d1383e73":"train = pd.read_csv(\"..\/input\/train.csv\")#loading the training set\ntest = pd.read_csv(\"..\/input\/test.csv\")#loading the testing set","9be5a5c8":"#saving the passenger ID for later use\npassengerID = test['PassengerId'].copy()","d6e617b5":"print(train.shape,test.shape)#quite small dataset as can be seen below","00229198":"train.info()","1688d4b2":"train.describe() #using describe() to get an overall picture of the numerical values of this dataset","cde6d3be":"train[\"split\"] = \"train\"\ntest[\"split\"] = \"test\"\n\ndata = pd.concat([train , test], ignore_index=True)\ndata.set_index(\"PassengerId\")\ndata[\"Survived\"].fillna(0.0,inplace=True)\nprint(train.shape, test.shape, data.shape)","468f683c":"data.head() #taking a peek using head()","4c379216":"data.describe() #and again, describe() to see relavant statistical information about the data","99756578":"data.apply(lambda x: len(x.unique())) # let's take a look at how many unique values does it have, just to have a broad idea...","7fbcaa28":"data['FamilySize'] = data['SibSp'] + data['Parch'] + 1 #creating a feature for the number of familiars\ndata['IsAlone'] = 0  #by default everyone is 0, not alone, checking below\ndata.loc[data['FamilySize'] == 1, 'IsAlone'] = 1 #now, by using the familysize feature, determine whether or not the person was alone 0 - not alone \/ 1 - alone","dfbf3c6e":"data.groupby('Fare')['Fare'].unique().value_counts()","ab58f67a":"data['Fare'].fillna(data['Fare'].median(), inplace=True) #filling null values\ndata.loc[data['Fare'] <= 7.91, 'Fare'] = 0\ndata.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\ndata.loc[ data['Fare'] > 31, 'Fare'] = 3\ndata['Fare'] = data['Fare'].astype(int) #we don't need to have a float value here, making sure that it's an int","166025be":"data['Has_Cabin'] = data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","967d2a82":"avg = data['Age'].mean() #getting the mean\nstd = data['Age'].std() # standard deviation\nnull_count = data['Age'].isnull().sum() #sum of null values\nrandom_list = np.random.randint(avg - std, avg + std, size = null_count) #creating a list of random numbers of age\ndata['Age'][np.isnan(data['Age'])] = random_list #filling the null values in our dataset with our random numbers\ndata['Age'] = data['Age'].astype(int)","f0a975b4":"#now let's map the age into numerical values 0 - 4\ndata.loc[ data['Age'] <= 16, 'Age'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\ndata.loc[data['Age'] > 64, 'Age'] = 4 ","20b625ba":"data['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","fbdd3d5a":"#filling the null values for embarked and transforming it...\ndata['Embarked'].fillna('S', inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)#as well as other columns, mapping it and transforming into a numerical column","2ac5af96":"data['Title'] = data['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndata['Title'] = data['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndata['Title'] = data['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don'], 'Dr\/Military\/Noble')\ndata['Title'] = data['Title'].fillna('Mr').map({'Mr': 0, 'Mrs': 1, 'Miss\/Mrs\/Ms': 2, 'Dr\/Military\/Noble': 3, 'Master': 4, 'Rev': 5}).astype(int)","80427d25":"data['Title'].value_counts()","a6754467":"drop_columns = [\"PassengerId\", \"Ticket\",\"Cabin\", \"Name\", \"SibSp\"]\ndata = data.drop(drop_columns, axis=1)","221e847e":"data.isnull().sum() #counting the number of null values in the dataset.","d4f14f8c":"sns.set(rc={'figure.figsize':(5,5)}) #setting the size of the figure to make it easier to read.\nsns.countplot(y=data[\"Age\"]).set_title(\"Age distribution\", fontsize=15) #plotting it horizontally to make it easier to read","384a375f":"corr = data.drop('split',axis=1).astype(float).corr() #saving the correlation for later use\nax = sns.set(rc={'figure.figsize':(10,4)})\nsns.heatmap(corr, annot=True).set_title('Pearsons Correlation Factors Heat Map', color='black', size='25')","d61f250a":"from sklearn.model_selection import train_test_split #importing the relevant module\n\n\ntrain = data[data['split'] == 'train'].copy() #separating our data using the label as a guide\ntest = data[data['split'] == 'test'].copy()\n\ntrain_labels = train[\"Survived\"].copy() #creating our Y_train\n\n\ntrain.drop(['split','Survived'], axis=1, inplace=True)\ntest.drop(['split','Survived'], axis=1, inplace=True)","7c39d052":"train, train_val, train_labels, train_val_labels = train_test_split(train, train_labels, test_size=0.2, random_state=42)","7f97c7e6":"print(train.shape, train_val.shape, test.shape)","61916444":"#first pipeline, numerical pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n\n\n#this is a pipeline to make selection of numerical atributes elegant and simple.   \nnum_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")), #used to impute missing values in our data, we have imputed our data manually, tho, but I am keeping this here\n       # (\"Scaler\", StandardScaler())\n        (\"MinMaxScaler\", MinMaxScaler())\n    ])   ","b619f9e4":"train_prepared = num_pipeline.fit_transform(train)\ntrain_val = num_pipeline.fit_transform(train_val)","c2de5616":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\ndef report_and_confusion_matrix(label, prediction):\n    print(\"Model Report\")\n    print(classification_report(label, prediction))\n    score = accuracy_score(label, prediction)\n    print(\"Accuracy : \"+ str(score))\n    \n    ####################\n    fig, ax = plt.subplots(figsize=(8,8)) #setting the figure size and ax\n    mtx = confusion_matrix(label, prediction)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  cbar=True, ax=ax) #create a heatmap with the values of our confusion matrix\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","0abdafea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_lgre = LogisticRegression(random_state=0)\nparam_grid = {'C': [0.014,0.012], 'multi_class': ['multinomial'],  \n              'penalty': ['l1'],'solver': ['saga'], 'tol': [0.1] }\nGridCV_LR = GridSearchCV(model_lgre, param_grid, verbose=1, cv=5)\nGridCV_LR.fit(train_prepared,train_labels)\n\nscore_grid_LR = GridCV_LR.best_score_\n\nmodel_lgre = GridCV_LR.best_estimator_\nprint(score_grid_LR)\n\npredict_score_lg_clf = GridCV_LR.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_lg_clf)","6bd55465":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest_clf = RandomForestClassifier(random_state=42)\nparam_grid = {'max_depth': [15], 'min_samples_split': [5],'n_estimators' : [100] }\nGridCV_rd_clf = GridSearchCV(rand_forest_clf, param_grid, verbose=1, cv=5)\nGridCV_rd_clf.fit(train_prepared, train_labels)\nscore_grid_rd = GridCV_rd_clf.best_score_\n\nrand_forest_clf = GridCV_rd_clf.best_estimator_\n\nprint(score_grid_rd)\n\npredict_score_rd_clf = GridCV_rd_clf.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_rd_clf)","b6fe85c1":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier()","dfcea4b7":"from sklearn.ensemble import AdaBoostClassifier\n\nparams_grid_ada = {}\n\nada_model = AdaBoostClassifier(tree_clf,n_estimators=3000,\n    algorithm=\"SAMME.R\", learning_rate=0.05, random_state=42)\n\nGridCV_ada = GridSearchCV(ada_model, params_grid_ada, verbose=1, cv=5)\nGridCV_ada.fit(train_prepared, train_labels)\nscore_grid_ada = GridCV_ada.best_score_\n\nmodel_ada = GridCV_ada.best_estimator_\n\n\nprint(score_grid_ada)\n\npredict_score_ada = GridCV_ada.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_ada)","24366293":"from sklearn.ensemble import GradientBoostingClassifier\n\nparams_grid_gb = {}\n\ngbc_model = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n               learning_rate=0.05, loss='deviance', max_depth=3,\n               max_features=None, max_leaf_nodes=None,\n               min_impurity_decrease=0.0, min_impurity_split=None,\n               min_samples_leaf=1, min_samples_split=2,\n               min_weight_fraction_leaf=0.0, n_estimators=10000,\n               n_iter_no_change=None, presort='auto', random_state=None,\n               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n               verbose=0, warm_start=False)\n\nGridCV_GB = GridSearchCV(gbc_model, params_grid_gb, verbose=1, cv=5)\nGridCV_GB.fit(train_prepared, train_labels)\nscore_grid_GB = GridCV_GB.best_score_\n\nmodel_gbc = GridCV_GB.best_estimator_\n\n\nprint(score_grid_GB)\n\npredict_score_GB = GridCV_GB.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_GB)","001ab2ff":"from sklearn.neural_network import MLPClassifier\n\nmlp_clf = MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(300,), random_state=42,batch_size=1000)\nparam_grid = { 'max_iter': [1200], 'alpha': [1e-4], \n               'solver': ['sgd'], 'learning_rate_init': [0.05,0.06],'tol': [1e-4] }\n    \nGridCV_MLP = GridSearchCV(mlp_clf, param_grid, verbose=1, cv=3)\nGridCV_MLP.fit(train,train_labels)\nscore_grid_MLP = GridCV_MLP.best_score_\n\nmodel_mlp = GridCV_MLP.best_estimator_\n\n\nprint(score_grid_MLP)\n\npredict_score_MLP = GridCV_MLP.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_MLP)","00ca638b":"import lightgbm as lgb\n#turn on or off which of the parameters below, in order to test. this dict of parameters can be used for LGBM and XGBM\n\nparams_grid_search_lgb = { \n    #\"learning_rate\": [0.01, 0.1,0.005],\n    #'min_child_weight': [5,6],\n    #'max_depth': range(3,10,2),\n    #'n_estimators':[150,200,1000,5000],\n    #'scale_pos_weight':[1,2,3,4],\n    #'colsample_bytree':[0.7,0.8], \n    #'subsample':[0.7,0.8],\n    #'gamma':[0,0.2,0.4],\n}\n\nlgb_model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.7,\n        gamma=0, importance_type='split', learning_rate=0.05, max_depth=3,\n        min_child_samples=20, min_child_weight=6, min_split_gain=0.0,\n        n_estimators=20000, n_jobs=-1, nthread=4, num_leaves=31,\n        objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n        scale_pos_weight=1, seed=29, silent=True, subsample=0.7,\n        subsample_for_bin=200000, subsample_freq=0)\n\nGridCV_LGB = GridSearchCV(lgb_model, params_grid_search_lgb, verbose=1, cv=5)\nGridCV_LGB.fit(train_prepared,train_labels)\nscore_grid_LGB = GridCV_LGB.best_score_\n\nmodel_lgb = GridCV_LGB.best_estimator_\n\n\nprint(score_grid_LGB)\n\npredict_score_LGB = GridCV_LGB.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_LGB)","2b9091d8":"import xgboost as xgb\n\nparams_grid_xgb = {}\n\nxgb_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.7, gamma=0.2, learning_rate=0.009,\n       max_delta_step=0, max_depth=3, min_child_weight=6, missing=None,\n       n_estimators=10000, n_jobs=1, nthread=4, objective='binary:logistic',\n       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n       seed=29, silent=True, subsample=0.7)\n\nGridCV_XGB = GridSearchCV(xgb_model, params_grid_xgb, verbose=1, cv=5)\nGridCV_XGB.fit(train_prepared, train_labels)\nscore_grid_XGB = GridCV_XGB.best_score_\n\nmodel_xgb = GridCV_XGB.best_estimator_\n\n\nprint(score_grid_XGB)\n\npredict_score_XGB = GridCV_XGB.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_XGB)","2c32f4c5":"from sklearn.ensemble import VotingClassifier #importing the relevant module\n\nparams_grid_vclf = {}\n\nestimators = [\n    (\"Ada Boosting\", model_ada),\n    (\"Gradient Boost\", model_gbc),\n    (\"lgb_model\", model_lgb), \n    (\"XGBoost\", model_xgb),\n    (\"MLP\", model_mlp),\n    \n]\n\nvoting_clf = VotingClassifier(estimators,voting='soft') #creating our voting classifier, hard voter by default\n\nGridCV_voting_clf = GridSearchCV(voting_clf, params_grid_vclf, verbose=1, cv=5)\nGridCV_voting_clf.fit(train_prepared, train_labels)\nscore_grid_vclf = GridCV_voting_clf.best_score_\n\n\nmodel_vclf = GridCV_voting_clf.best_estimator_\n\nprint(score_grid_vclf)\n\npredict_score_vclf = GridCV_voting_clf.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, predict_score_vclf)\n","ee44f5f1":"test_prepared = num_pipeline.fit_transform(test) #preparing our test dataset to use in our models.","729d5606":"sub_voting_classifier = model_vclf.predict(test_prepared)","4bfe8781":"from mlxtend.classifier import StackingCVClassifier\nfrom sklearn import model_selection\n\n\nestimators =[model_ada,model_lgb,model_gbc, model_mlp]\nsclf = StackingCVClassifier(classifiers= estimators,\n                            #use_probas=True, #turn it on or off if you want to use the previous classifier as input for the next one\n                            meta_classifier=model_xgb)\n\nprint('3-fold cross validation:\\n')\n\n\nfor clf, label in zip([model_ada, model_lgb, model_gbc, model_mlp, sclf], \n                      ['Ada Boosting',\n                        'light gradient Boosting(LGBM)',\n                       'Gradient Boosting',\n                       \"MLP\",\n                       'StackingClassifier(Meta: XGboost)']):\n\n    scores = model_selection.cross_val_score(clf, train_prepared, train_labels, \n                                              cv=5, scoring='precision')\n    print(\"precision: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","57b0d189":"sclf.fit(train_prepared, train_labels)","ba48adac":"prediction_sclf = sclf.predict(train_val)\nreport_and_confusion_matrix(train_val_labels, prediction_sclf)","fae51e57":"prediction_stacking_clf = sclf.predict(test_prepared)","bbfa2665":"sub = pd.read_csv('..\/input\/gender_submission.csv')\nsub['Survived'] = sub_voting_classifier.astype('int64')\nsub.to_csv(\"voting_classifier.csv\", index=False)\nsub1 = pd.read_csv('..\/input\/gender_submission.csv')\nsub1['Survived'] = prediction_stacking_clf.astype('int64')\nsub.to_csv(\"stacking_clf.csv\", index=False)","77dda1c6":"## loading relevant models for this part of the notebook\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam \nfrom keras.layers import Dense, Activation, Dropout,BatchNormalization\nfrom keras.utils import to_categorical","4bd00bf5":"model = Sequential()\nmodel.add(Dense(input_dim=train.shape[1], units=128,\n                 kernel_initializer='normal', bias_initializer='zeros'))\nBatchNormalization(),\nmodel.add(Activation('relu'))\n\nfor i in range(0, 15):\n    BatchNormalization(),\n    model.add(Dense(units=100, kernel_initializer='he_normal',\n                     bias_initializer='zeros'))\n    BatchNormalization(),\n    model.add(Activation('elu'))\n    \nmodel.add(Dense(units=2))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])","dc82babf":"model.summary()","4190d613":"labels = to_categorical(train_labels)","2eb3568c":"model_history = model.fit(train, labels, epochs=200, batch_size=600,validation_split=0.2, verbose=2)","e3637b6f":"pd.DataFrame(model_history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]","a061c6c7":"test_prepared = num_pipeline.fit_transform(test)","ac3fdbeb":"submission_keras = model.predict_classes(test_prepared)\nsub['Survived'] = submission_keras.astype('int64')\nsub.to_csv(\"keras.csv\", index=False)","e9a81e3b":"* The stacking classifier is performing better than all other estimator","21ee8739":"### Implementing a function to help with this task, gonna check the precision of each model, recall and the F1 score. Finally, the cross validation score is also given","c3778469":"## Using the Voting Classifier\n### this might help increase the score a bit higher","b59eb26f":"## Ploting a heating map showing the pearson's correlation","3340209c":"## Gonna implement a stacking cross val classifier with a cross validation 3 folds","c81a0be8":"### dropping columns that won't help us in our analysis \nsince we have created the family size and isalone category, gonna drop SibSp as well","fd949946":"## now it's quite simple to test our models\n### Testing different models using our function implemented above","1a8dd3cb":"## Creating the classes and pipelines to scale our data for our models","c97b58dd":"* Let's take a look at Fare, if we scale it as it is, it's not gonna help our models, as it is a floating type with a range of different values(281 unique values)","5ac7fbc8":"### Now let's implement a grid_search together with the stacking CV classifier to see whether we can find better parameters","3abf40ea":"As can be seen, this column needs to be binned","3d7b7d94":"\n#### Next, the Light Gradient Boost, a leaf-wise algorithm,unlike other decision tree models which use level-wise growth algorithm during model training. For larger datasets, it requires less memory and it often generalizes quite well.\n\n##### creating a dict for the grid_search parameters, which is gonna be used to search for the best parameters.","07a7f863":"#### The random forest is giving us a good score so far, cross validation is telling us that this model is overfitting, tho","fa9cd356":"## Feature Engineering\n#### ideas and code taken [here](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier) by Sina\n##### Feature engineering is such an important and crucial part in order to make the models work better","bcc3042e":"***","7809ef3b":"*  notice how precision has increased. Gonna focus on recall for the next model","3f747242":"#### As can be seen, training accuracy and accuracy in the validation set is not exactly the same(it's our goal to have those values the closest) loss in the validation set started lower than in the training set(by chance) and it didn't get really lower, raising the number of epochs doesn't really help us here, we don't have a sufficient number of training samples to have a proper validation set...","801cbdea":"Creating a feature that tells whether a given passenger had a cabin or not.","b74f0a03":"### Gonna use the pipeline that has been created to transforme and impute categorical and numerical values\nPs: as we had mostly impute the columns separately, it won't be needed to use the simple imputer,however that may be, gonna use the pipeline to scale the data, thereby making training faster","4e7c198c":"* the category **is alone** is gonna be very important in order to help our model\n","b7fb1aef":"#### For smaller datasets such as this one, XGBoost tends to outpeform the LGBModel\n##### let's check it.","b8a7438a":"**Gonna fit the model and use 10% of the dataset for validation,as we have such a small dataset, it's rather difficult to separate any of our data for validation, but we need to do so**","be22d6a3":"##### Ada boosting is generalizing better than previous models","05ced891":"### Gonna put together the data, only to check for null values and a bit of data analysis, for imputing and scalling","ef72cc91":"## Now, it's time to apply Neural Networks with Keras, it's my first time implementing it...\n### as always, I am gonna try many different things","11d9573b":"* Gonna do something rather simple, gonna count the number of passangers with different titles","e170e6bd":"Running the code below, we can see each separate score and see if we see a bad score hurting the performance\n","936200c2":"### splitting our dataset into train and test again, using the split columns create previously\n#### Creating a train_labels with our target as well","58129148":" Generalizing well, it decreases precision but increased recall and cross validation."}}