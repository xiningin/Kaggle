{"cell_type":{"9b940146":"code","ba11e513":"code","504685ae":"code","03aaf43e":"code","3dcb5e69":"code","d5e828c8":"code","b9d59b4d":"code","a24a9ca3":"code","4d57485c":"code","31bbe138":"code","5f46222d":"code","589b99f9":"code","d657970f":"code","d32edc8d":"code","8b88fe82":"code","3a2e98bf":"code","7404b42e":"code","5d3fe6b7":"code","d09436c3":"code","a004fc3e":"code","a77a3037":"code","a1aca927":"code","e4df5658":"code","315e9c75":"code","a64ddf08":"code","b77a2556":"code","6fa07cb6":"code","c6cc0095":"code","5e6fd9fb":"code","79c2328d":"code","60559622":"code","60217ace":"code","ed545b98":"code","03673c49":"code","ef2bfc4c":"code","2acba4fb":"code","ba7c1b2e":"code","6d45782b":"code","67848bd8":"markdown","1216be7d":"markdown","34364361":"markdown","67f41f2d":"markdown","30676e21":"markdown","39a0543e":"markdown","67cf37ca":"markdown","e888a946":"markdown","b37d3e8f":"markdown","4cafd249":"markdown","663216b4":"markdown","082b9f97":"markdown","5f204d88":"markdown","bcbe36c5":"markdown","eeaf8c77":"markdown","2de04a6a":"markdown","22842e84":"markdown","f52c59bc":"markdown","ae04d5f8":"markdown","fc17558b":"markdown","f574240c":"markdown","76967953":"markdown","99ae4ce7":"markdown","1114a896":"markdown","e1e0cf2d":"markdown","00c7f19e":"markdown","af8f99f8":"markdown","190c4e6e":"markdown","017cbe39":"markdown","15cdfaec":"markdown","bea89e26":"markdown","06af1e07":"markdown","48f8b127":"markdown"},"source":{"9b940146":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nimport re\nimport nltk\n#from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score,hamming_loss, roc_curve, auc, confusion_matrix, classification_report\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom xgboost import XGBClassifier\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport pickle\nnltk.download('punkt')\nnltk.download(\"stopwords\")\n\nimport torch\nfrom torch.utils.data import RandomSampler, SequentialSampler, DataLoader\nfrom torch.utils.data import  Subset, TensorDataset\nfrom transformers import BertForSequenceClassification, BertModel, BertTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn import functional as F \n\nimport random\n\nfrom collections import defaultdict\nimport re \n\nimport time\n\nimport torch.nn as nn\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ba11e513":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","504685ae":"train.head()","03aaf43e":"submission.head()","3dcb5e69":"train.isna().sum()","d5e828c8":"# Target count\nplt.figure(figsize = (8,6))\nsns.countplot(x='target',data = train)\nplt.xlabel(\"Target Values\")\nplt.ylabel(\"Count Values\")\nsns.despine(left = True, bottom = True)\nplt.show()","b9d59b4d":"# Plottin the most repetitive words in \"text\" column\nstopwords = set(STOPWORDS)\ndef word_cloud(data, title = None):\n    cloud = WordCloud(background_color = \"black\",\n      stopwords = stopwords,\n      max_words=200,\n      max_font_size=40, \n      scale=3,).generate(str(data))\n    fig = plt.figure(figsize= (8, 6))\n    plt.axis(\"off\")\n    if title: \n        fig.suptitle(title, fontsize=10)\n        fig.subplots_adjust(top=0.8)\n\n    plt.imshow(cloud)\n    plt.show()","a24a9ca3":"# Most repeated words in real disaster tweets,\n#making a word cloud\nword_cloud(train[train[\"target\"] == 1][\"text\"], \"Most repeated words in real disaster tweets in train data\")","4d57485c":"# Distribution of keywords in real and fake tweets \nplt.figure(figsize = (10, 80), dpi = 100)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nsns.countplot(y = \"keyword\", hue = \"target\", data = train)\nplt.legend(loc = 1)\nplt.show()","31bbe138":"def text_preprocessing(s):\n    \"\"\"\n    - Lowercase the sentence\n    - Change \"'t\" to \"not\"\n    - Remove \"@name\"\n    - Isolate and remove punctuations except \"?\"\n    - Remove other special characters\n    - Remove stop words except \"not\" and \"can\"\n    - Remove trailing whitespace\n    \"\"\"\n    s = s.lower()\n    # Change 't to 'not'\n    s = re.sub(r\"\\'t\", \" not\", s)\n    # Remove @name\n    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n    # Isolate and remove punctuations except '?'\n    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\\/\\,])', r' \\1 ', s)\n    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n    # Remove some special characters\n    s = re.sub(r'([\\;\\:\\|\u2022\u00ab\\n])', ' ', s)\n    # Substitute some non sense words \n    s = re.sub(r\"\\x89\u00db_\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00d2\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00d3\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\",s)\n    s = re.sub(r\"\\x89\u00db\u00cf\", \"\", s)\n    s = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", s)\n    s = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\",s)\n    s = re.sub(r\"\\x89\u00db\u00f7\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00aa\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\\x9d\", \"\", s)\n    s = re.sub(r\"\u00e5_\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00a2\", \"\", s)\n    s = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", s)\n    s = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\",s)\n    s = re.sub(r\"\u00e5\u00ca\", \"\", s)\n    s = re.sub(r\"\u00e5\u00c8\", \"\", s)\n    s = re.sub(r\"Jap\u00cc_n\", \"Japan\", s)    \n    s = re.sub(r\"\u00cc\u00a9\", \"e\", s)\n    s = re.sub(r\"\u00e5\u00a8\", \"\", s)\n    s = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", s)\n    s = re.sub(r\"\u00e5\u00c7\", \"\", s)\n    s = re.sub(r\"\u00e5\u00a33million\", \"3 million\", s)\n    s = re.sub(r\"\u00e5\u00c0\", \"\", s)\n    # Remove stopwords except 'not' and 'can'\n    #s = \" \".join([word for word in s.split()\n                  #if word not in stopwords.words('english')\n                  #or word in ['not', 'can']])\n    # Remove trailing whitespace\n    #s = re.sub(r'\\s+', ' ', s).strip()\n    \n    return s","5f46222d":"# apply the function on the text feature\ntrain['text']=train['text'].apply(text_preprocessing)","589b99f9":"X = train['text'].values\ny = train['target'].values\n  \n# initializing TfidfVectorizer \nvectorizar = TfidfVectorizer(max_features=3000, max_df=0.85)\n# fitting the tf-idf on the given data\nvectorizar.fit(X)\n  \n# splitting the data to training and testing data set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n  \n# transforming the data\nX_train_tfidf = vectorizar.transform(X_train)\nX_test_tfidf = vectorizar.transform(X_test)","d657970f":"params = {'estimator__learning_rate' :[0.01, 0.1, 0.2],\n          'estimator__n_estimators':[10,100,500],\n          'estimator__subsample':(0.2,0.6,0.8),\n          'estimator__max_depth':np.arange(1,6),\n          'estimator__min_child_weight':np.arange(1,6)}\nbase_estimator = XGBClassifier()\nrsearch_cv = RandomizedSearchCV(estimator=base_estimator, param_distributions=params, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\nrsearch_cv.fit(X_train_tfidf, y_train)","d32edc8d":"# Getting the best model out of the randomized search\nmodel=rsearch_cv.best_estimator_","8b88fe82":"#re-fitting the model with the best hyper-parameter we have found yet\nmodel.fit(X_train_tfidf, y_train)","3a2e98bf":"#We first get the predictions and then we compute the accuracy score\npredictions=model.predict(X_test_tfidf)\nprint(f'accuracy score:{accuracy_score(y_test,predictions)}')\nprint('\\n')\nprint(classification_report(y_test,predictions))","7404b42e":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","5d3fe6b7":"def text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Substitute some non sense words \n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\",text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\",text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\",text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n\n    return text","d09436c3":"# Applying the cleaning functions to the train and test data\ntrain['text']=train['text'].apply(text_preprocessing)\ntest['text']=test['text'].apply(text_preprocessing)","a004fc3e":"# Setting the tokenizer parameters\nBERT_MODEL_NAME='bert-base-cased'\ntokeniser=BertTokenizer.from_pretrained(BERT_MODEL_NAME)","a77a3037":"# First we need to split the training data into train, validation datasets\ntexts, targets = train['text'].values, train['target'].values\ntexts_train, texts_test, targets_train, targets_test = train_test_split(texts, targets, test_size=0.15, random_state=42, stratify=targets)","a1aca927":"def get_input(df,labels,train=True):\n  input_ids=[]\n  attention_masks=[]\n  \n  for sentence in df:\n    \n    # we prepare the encoding for each sentence in the dataset\n    encoding=tokeniser.encode_plus(\n                    sentence,\n                    None,\n                    add_special_tokens=True,\n                    max_length= 64,\n                    padding = 'max_length',\n                    return_token_type_ids= False,\n                    return_attention_mask= True,\n                    truncation=True,\n                    return_tensors ='pt')\n    \n    # We gather the tensors in two lists\n    input_ids.append(encoding['input_ids'])\n    attention_masks.append(encoding['attention_mask'])\n\n  #convert inputs to tensors\n\n  input_ids=torch.cat(input_ids,dim=0)\n  attention_masks=torch.cat(attention_masks,dim=0)\n  labels=torch.tensor(labels)\n    \n    \n  # we wrap the inputs ensemble inside a TensorDataset\n  dataset=TensorDataset(input_ids,attention_masks,labels)\n\n  if train:\n    sampler=RandomSampler(dataset)\n  else:\n    sampler=SequentialSampler(dataset)\n  \n  dataloader=DataLoader(dataset,sampler=sampler, batch_size=16)\n\n  return dataloader","e4df5658":"# Next we set the batches for the training and validation\n#the batch size is the recommanded by the 'attention is all you need' paper\ntrain_batches=get_input(texts_train,targets_train)\nval_batches=get_input(texts_test,targets_test,train=False)","315e9c75":"class Bertclf(nn.Module):\n\n  def __init__(self):\n\n    super(Bertclf,self).__init__()\n    self.bert=BertModel.from_pretrained('bert-base-cased',return_dict=False)\n    # We are adding a dropout layer\n    self.drop=nn.Dropout(0.33)\n    # on top of our bert model we added a Linear model with two hidden layers \n    self.classifier=torch.nn.Sequential(nn.Linear(768,256),\n                                        nn.ReLU(),\n                                        nn.Linear(256,50),\n                                        nn.ReLU(),\n                                        nn.Linear(50,2))\n  # the output has two nodes for our binary classification model\n\n  def forward(self, input_ids,attention_mask):\n\n    last_hidden_state,pooled_output= self.bert(input_ids,attention_mask)\n    output=self.drop(pooled_output)\n    output=self.classifier(output)\n\n    return output","a64ddf08":"#Instantiate the BERT Model & assign it to the GPU\nmodel=Bertclf()\nmodel.to('cuda')","b77a2556":"# setting the loss function which will be the CrossEntropyLoss\nloss_fn=nn.CrossEntropyLoss().to('cuda')\n#Recommended EPOCHS for training in order not to overfit\nN_EPOCHS=4\n# optimiser will perform the weights update as its first param\n#learning rate 5e-5 \n#epsilon 1e-8\noptimiser=AdamW(model.parameters(),\n                lr=5e-5,\n                eps=1e-08)\n  \ntotal_steps = len(train_batches) * N_EPOCHS\n# the schedular updates the learning rate each step\nschedular=get_linear_schedule_with_warmup(optimiser,\n                                          num_warmup_steps=0,\n                                          num_training_steps=total_steps)","6fa07cb6":"def train_loop(train_loader,optimizer,Schedular):\n\n  #Set the model in training mode  \n  model.train()\n  \n\n  #We are storing the losses and the good predictions\n  losses=[]\n  correct_predicitons=0\n\n  for step, batch in enumerate(train_loader):\n    \n    #the model should not store the previous gradient \n    model.zero_grad()\n\n\n    #sending the batches input to the GPU\n    input,atten,label=(t.to('cuda') for t in batch)\n    logit=model(input,atten)\n    loss=loss_fn(logit,label)\n\n    #First we apply the softmax on the logits to get the probabilities distribution\n    #Next the max function give us the max value index\n    pred=torch.max((F.softmax(logit, dim=1)), dim=1)[1]\n    correct_predicitons+= torch.sum(pred==label)\n\n    losses.append(loss.item())\n\n\n    #we perform the backpropagation on the loss and store in the predicitons\n    loss.backward()\n\n    nn.utils.clip_grad_norm_(model.parameters(),1.0)\n\n    #we update the model parameters with the learning rate\n    optimizer.step()\n    Schedular.step()\n\n  return correct_predicitons\/len(texts_train) , np.mean(losses)","c6cc0095":"def validation_loop(val_loader,len_input):\n\n  #Next we switch to the evaluation mode  \n  model.eval()\n\n  losses=[]\n  correct_predicitons=0\n\n  for batch in (val_loader):\n\n      \n      input,atten,label=(t.to('cuda') for t in batch)\n    \n      #No Backpropagation is needed only the feed forward step\n      with torch.no_grad():\n        logit=model(input,atten)\n\n      loss=loss_fn(logit,label)\n\n      pred=torch.max((F.softmax(logit, dim=1)), dim=1)[1]\n      \n      correct_predicitons+= torch.sum(pred==label)\n\n      losses.append(loss.item())\n\n  return correct_predicitons\/len_input , np.mean(losses)","5e6fd9fb":"history=defaultdict(list)\n\nbest_accuracy=0\n\nfor epoch in range(N_EPOCHS):\n\n  print(f'Epoch: {epoch+1}\/{N_EPOCHS}')\n  print('-'*10)\n\n  train_acc,train_loss=train_loop(train_batches,optimiser,schedular)\n\n  print(f'Train accuracy: {train_acc} ; Train loss: {train_loss}')\n\n\n  val_acc, val_loss= validation_loop(val_batches,len(texts_test))\n  \n  print(f'val accuracy: {val_acc} ; val loss: {val_loss}')\n\n  history['train_acc'].append(train_acc)\n  history['train_loss'].append(train_loss)\n  history['val_acc'].append(val_acc)\n  history['val_loss'].append(val_loss)\n\n  if val_acc > best_accuracy:\n    torch.save(model.state_dict(), '.\/best_model_state.bin' )\n    best_accuracy = val_acc","79c2328d":"def eval_input(df):\n  input_ids=[]\n  attention_masks=[]\n  \n  for sentence in df:\n    \n    encoding=tokeniser.encode_plus(\n                    sentence,\n                    None,\n                    add_special_tokens=True,\n                    max_length= 64,\n                    padding = 'max_length',\n                    return_token_type_ids= False,\n                    return_attention_mask= True,\n                    truncation=True,\n                    return_tensors ='pt')\n    \n    input_ids.append(encoding['input_ids'])\n    attention_masks.append(encoding['attention_mask'])\n\n  #convert inputs to tensors\n\n  input_ids=torch.cat(input_ids,dim=0)\n  attention_masks=torch.cat(attention_masks,dim=0)\n\n\n  dataset=TensorDataset(input_ids,attention_masks)\n\n  dataloader=DataLoader(dataset,sampler=SequentialSampler(dataset), batch_size=16)\n\n  return dataloader","60559622":"#prepare the test batches\ntest_batches=eval_input(test['text'].values)","60217ace":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to('cuda') for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu()\n    pred=list(torch.max(probs.detach(),dim=1)[1].numpy())\n\n    return pred, probs","ed545b98":"model = Bertclf()\nmodel.load_state_dict(torch.load('.\/best_model_state.bin'))\nmodel.to('cuda')","03673c49":"predictions, probabilities=bert_predict(model,test_batches)","ef2bfc4c":"test_predictions_dt=pd.DataFrame({'id':test.index,'target':predictions})\ntest_predictions_dt.sample(10)","2acba4fb":"test_predictions_dt.to_csv('submission.csv',index=False)\n","ba7c1b2e":"predictions, probabilities=bert_predict(model,val_batches)\nprint(classification_report(list(targets_test), predictions))","6d45782b":"def show_confusion_matrix(confusion_matrix):\n\n  plt.figure(figsize=(12,10))\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment')\n  plt.show()\n\ncm = confusion_matrix(list(targets_test), predictions)\ndf_cm = pd.DataFrame(cm, index=['disaster','no_disaster'], columns=['disaster','no_disaster'])\nshow_confusion_matrix(df_cm)\n","67848bd8":"### Evaluation on the validation dataset","1216be7d":"We have used the cased version of BERT because we want to capture the meaningful UPPERCASED Words ","34364361":"## Build the BERT-Classifier","67f41f2d":"BERT (introduced in this paper) stands for Bidirectional Encoder Representations from Transformers. If you don\u2019t know what most of that means - you\u2019ve come to the right place! Let\u2019s unpack the main ideas:\n\n- Bidirectional: to understand the text you\u2019re looking you\u2019ll have to look back (at the previous words) and forward (at the next words)\n- Transformers: The Attention Is All You Need paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. his in a sentence refers to Jim).\n","30676e21":"### Helper functions for Training\/Validation","39a0543e":"for this part we will perform two steps: \n\n- First the cleaning which will be a bit harch \n- Next the fine tuning of the hyper-parameters we will define","67cf37ca":"Here it is! we have reached roughly 79% of accuracy on the test set which is a good result.\n\nLets check the transformer method and see what we will get","e888a946":"## Text Pre-Processing","b37d3e8f":"### Training parameters","4cafd249":"![image.png](attachment:b1abfadf-0078-41ac-9268-6dafa7f55d9b.png)","663216b4":"There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use BertForSequenceClassification, BertForQuestionAnswering or something else.\nWe can choose the **BertForSequenceClassification** for this task. Instead we want to try something else! We\u2019ll use the basic BertModel and build our sentiment classifier on top of it. Let\u2019s load the model:","082b9f97":"### the Training\/Validation Process","5f204d88":"Lets move on to the intresting part.\nWe are going to build two classifiers: \n\n- Ensemble Method: XGBoostClassifier from xgboost in which we are going to use TF-IDF vectorization and fine tune its hyper-parameters in order to get the best model\n\n- Transormers: Bert Method from Hugging face library and fine tune its weights with regards to our train process","bcbe36c5":"As we can see, we haven't done any improvment since the first. which is can be concluded with regards to the **val_loss** as it s increasing then stays the same","eeaf8c77":"As we can see, we have missing values for the locations and keywords.\n\nBut this will not be very important since we are focusing on the text sentiment whether it s a disaster tweet or not","2de04a6a":"## Setting the Input in BERT format ","22842e84":"## Text cleaning for BERT","f52c59bc":"Lets perform some EDA analysis!!!","ae04d5f8":"# Transformers: BERT& Pytorch","fc17558b":"this the input format for the unlabled dataset. the test dataset in our case ","f574240c":"Words like :\n- Fire \n- earthquake\n- wildfires\n- Deeds \nare references to a disaster happening ","76967953":"# Ensemble Method: XGBoost","99ae4ce7":"## Fine tuning XGboost hyper-parameters","1114a896":"## Evaluating the model","e1e0cf2d":"## Setting the data: TF-IDF Vectorizer","00c7f19e":"### Submit the output data","af8f99f8":"### Initialising the Tokenizer","190c4e6e":"Lets evaluate the model on the test set","017cbe39":"The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences and needs the whole sequence in order to capture each word meaning.","15cdfaec":"Lets check the words distribution by tweets (disaster\/No disaster)","bea89e26":"# EDA","06af1e07":"Next we are going to prepare our data in a format that BERT understands: \n- inputs_ids: the token ids which will be embedded in pytorch tensor with bert dimensions \n- attention_mask: token encoding with 1 for a token and a 0 for padding \n- Label: the target for each input\n\nAll of these parameters will be wrapped inside a Tensor dataset which will be included in a batch dalaloader for trainning and validation datasets","48f8b127":"As yo may noticed in the above sentences: \n- the word (it) can be related to the animale (sentence 1)\n- the word (it) is obviosly related to the street (sentence 2) \n\nThanks to the attention mechanism given by the equation above, the word\/token (it) won't have the same embedding since it has different meaning."}}