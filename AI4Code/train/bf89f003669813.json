{"cell_type":{"f26c8fc7":"code","c188c1b4":"code","e9f76adc":"code","0903a748":"code","4b5dc721":"code","818f8965":"code","1db7f53a":"code","e8f4370d":"code","a2e04c5a":"code","7bd2d076":"code","dc3dcbaf":"code","37e33034":"code","859a4940":"code","607b8a69":"code","d73b2067":"code","c47e976a":"code","820bd8a4":"code","b7142140":"code","d4bc8d3d":"code","a37c0d5f":"code","3cb505fa":"code","dcb3d578":"code","eb920454":"code","f762b2e3":"code","60442e61":"code","fb84b1a7":"code","35efac60":"code","73209298":"code","20d2c161":"code","9d6a0551":"code","10979fa3":"code","f008aeb8":"code","deaf50a4":"code","7a4e00a7":"code","a67975ca":"code","94439804":"code","ea0392c5":"code","aa2d3398":"code","3bb181d8":"code","ee8a621c":"code","7d57affa":"code","4eb84bf2":"code","ae6c9082":"code","ba9766d2":"code","817c8245":"code","6476f02e":"code","7e737991":"code","2ef1aac9":"code","91e9641d":"code","ec72a798":"code","86e08d47":"code","47ca2624":"code","66d05fd1":"code","dda1c1a7":"code","863c4803":"code","9d883b05":"code","d5a76b78":"code","0c52e5ab":"code","34b8983a":"code","b42b5f4c":"code","e17d5a34":"code","c0804b7c":"code","85e5b37d":"code","0825ba19":"code","60b8276c":"code","773b5d79":"code","df4d76ac":"code","2e816e42":"code","c4bd180b":"code","734a4b3b":"code","775a8b01":"code","5b9100f2":"code","98233327":"code","a16381f9":"code","a7bdbce4":"code","d0e270a6":"code","995e6c7d":"code","8bd512b8":"code","29e0771a":"code","8e99dfa0":"code","ad00ba6e":"code","da132178":"code","881fcd90":"code","c1399ebc":"code","5dddb286":"code","41e4875b":"code","537629a4":"code","25ca539c":"code","06e8cb66":"code","df3ddfa0":"code","effc763c":"code","7778cf97":"code","9947b044":"code","f34f5002":"code","7d24b1da":"code","9cc3e0fd":"code","ae027c90":"code","cac9f776":"code","d432e30c":"code","7525c509":"code","e56b10de":"markdown","169e8256":"markdown","d0c15021":"markdown","97b57d66":"markdown","57de4838":"markdown","5cd04d3c":"markdown","fc009065":"markdown","5060c7d1":"markdown","45b9e82d":"markdown","990ddfb3":"markdown","40fc35bc":"markdown","6c4ca252":"markdown","174b3088":"markdown","b54a6794":"markdown","17174088":"markdown"},"source":{"f26c8fc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c188c1b4":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport plotly.io as pio\n\n# Standard plotly imports\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\ninit_notebook_mode(connected=True)","e9f76adc":"from collections import Counter\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport operator \nfrom collections import defaultdict\nimport string\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\n","0903a748":"\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","4b5dc721":"def create_aggregrated_plots(feature_name, target, bubble_size_adj=0.1 , aggregration='mean'):\n\n    _tmp_material = train.groupby([target])[feature_name].mean()\n    fig = go.Figure()\n    color_values = list(_tmp_material.values.astype(int))\n    tmp_trace = []\n\n    fig.add_trace(go.Scatter(\n        x=_tmp_material.index, y=_tmp_material.values.astype(int),\n        marker=dict(\n            size=_tmp_material.values.astype(int)*bubble_size_adj,\n            color=color_values,\n            colorbar=dict(\n                title=\"Colorbar\"\n            ),\n            colorscale=\"Viridis\",\n        ),\n        mode=\"lines+markers\")\n    )\n\n    fig.update_layout(\n        title=go.layout.Title(\n            text=\"Average \"+feature_name + \"\/\" + target,\n        ),\n        yaxis=dict(\n                title='Average '+feature_name,\n            ),\n            xaxis=dict(\n                title=target,\n            )\n    )\n\n    fig.show()","818f8965":"def target_distribution(df , target, top_counts=None):\n    if top_counts:\n        topic_counts = df[target].astype(str).value_counts()[:top_counts]\n    else:        \n        topic_counts = df[target].astype(str).value_counts()\n    \n    fig = go.Figure([go.Bar(x=topic_counts.index, y=topic_counts.values, \n                            text=topic_counts.values,\n                            textposition='auto',\n                           marker_color='indianred')])\n    fig.update_layout(\n            title=go.layout.Title(\n                text=\"Topic Distribution\",\n            ),\n            yaxis=dict(\n                    title='Count',\n                ),\n                xaxis=dict(\n                    title=\"Topic\",\n                )\n        )\n    fig.show()\n","1db7f53a":"def create_wordcloud(df, feature_names, target, target_filter):\n    plt.figure(figsize=(25,10))\n    for ei, feature_name in enumerate(feature_names):\n        text = \" \".join(review for review in df[df[target] == target_filter][feature_name])\n        #print (\"There are {} words in the combination of all {}.\".format(len(text), feature_name))\n\n        # Create the wordcloud object\n        wordcloud = WordCloud(width=1024, height=480, margin=0).generate(text)\n\n        # Display the generated image:\n        plt.subplot(1,2,ei+1)\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.title(feature_name)\n        plt.margins(x=0, y=0)\n    plt.show()","e8f4370d":"train_raw = pd.read_csv('\/kaggle\/input\/amazon-hiring-challenge\/2901c100-b-dataset\/Dataset\/train.csv')\ntest_raw = pd.read_csv('\/kaggle\/input\/amazon-hiring-challenge\/2901c100-b-dataset\/Dataset\/test.csv')","a2e04c5a":"train = train_raw.copy()\ntest = test_raw.copy()","7bd2d076":"train.shape, test.shape","dc3dcbaf":"train.head()","37e33034":"test.head()","859a4940":"train.isnull().sum()","607b8a69":"test.isnull().sum()","d73b2067":"train['info'] = train['Review Text'] + \" \" + train['Review Title']\ntest['info'] = test['Review Text'] + \" \" + test['Review Title']\n\n\ntrain['review_length'] = train['Review Text'].apply(lambda x: len(x))\ntrain['review_word_count'] = train['Review Text'].apply(lambda x: len(x.split()))\n\ntrain['title_length'] = train['Review Title'].apply(lambda x: len(x))\ntrain['title_word_count'] = train['Review Title'].apply(lambda x: len(x.split()))\n","c47e976a":"train.head()","820bd8a4":"train.shape, test.shape","b7142140":"target_distribution(train, 'topic')","d4bc8d3d":"create_aggregrated_plots('review_length','topic',0.2)","a37c0d5f":"create_aggregrated_plots('title_length', 'topic',1)","3cb505fa":"create_aggregrated_plots('review_word_count', 'topic', 1)","dcb3d578":"create_aggregrated_plots('title_word_count', 'topic',10)","eb920454":"train.title_word_count.max(),train.title_word_count.min() , train.title_word_count.mean()","f762b2e3":"create_wordcloud(train, [\"Review Text\", \"Review Title\"], \"topic\", 'Bad Taste\/Flavor')","60442e61":"create_wordcloud(train,['Review Text', 'Review Title'],'topic', 'Packaging')\n","fb84b1a7":"create_wordcloud(train, ['Review Text', 'Review Title'],'topic', 'Expiry')","35efac60":"repeated_reviews = train['Review Text'].value_counts()","73209298":"repeated_reviews.head()","20d2c161":"train[train['Review Text'] == repeated_reviews.index[0]]","9d6a0551":"multi_label_data = defaultdict(list)\n\nfor key, grp in train.groupby(['Review Text','Review Title']):\n    multi_label_data['Review Text'].append(key[0])\n    multi_label_data['Review Title'].append(key[1])\n    multi_label_data['topic'].append(list(grp.topic.values))","10979fa3":"multi_label_df = pd.DataFrame(multi_label_data)","f008aeb8":"multi_label_df['info'] = multi_label_df['Review Text'] +\" \"+ multi_label_df['Review Title']","deaf50a4":"multi_label_df.head()","7a4e00a7":"mul_binarizer = MultiLabelBinarizer()\nmul_binarizer.fit(multi_label_df.topic)","a67975ca":"mul_binarizer.classes_","94439804":"y_trans = mul_binarizer.transform(multi_label_df.topic)","ea0392c5":"y_trans","aa2d3398":"target_distribution(multi_label_df, 'topic', 30)","3bb181d8":"GLOVE_EMBEDDING_FILE = '\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt'\nFASTTEXT_EMBEDDING_FILE = '\/kaggle\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nWIKI_EMBEDDING_FILE =     '\/kaggle\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec'","ee8a621c":"EMBEDDING_FILES = [\n    GLOVE_EMBEDDING_FILE,\n    FASTTEXT_EMBEDDING_FILE,\n    WIKI_EMBEDDING_FILE\n]","7d57affa":"glove_embeddings = load_embeddings(GLOVE_EMBEDDING_FILE)\nprint(f'loaded {len(glove_embeddings)} word vectors ')","4eb84bf2":"vocab = build_vocab(list(multi_label_df['info'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","ae6c9082":"def preprocessing(text):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', text)\n    return cleantext.split()","ba9766d2":"vocab = build_vocab(list(multi_label_df['info'].apply(preprocessing)))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","817c8245":"tokenizer = TreebankWordTokenizer()","6476f02e":"def preprocess_2(x):\n    x = ' '.join(preprocessing(x))\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x.split()","7e737991":"preprocess_2(\"I didn't know\")","2ef1aac9":"vocab = build_vocab(list(multi_label_df['info'].apply(preprocess_2)))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","91e9641d":"\"protein.\".replace(\".\",\"\")","ec72a798":"def preprocess_3(x):\n    x = ' '.join(preprocessing(x))\n    x = x.replace('.',' ')\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x.split()","86e08d47":"vocab = build_vocab(list(multi_label_df['info'].apply(preprocess_3)))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","47ca2624":"def preprocess_4(x):\n    x = ' '.join(preprocessing(x))\n    x = x.replace('.',' ')\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    x = re.sub('[+\/-]', ' ', x)\n    return x.split()","66d05fd1":"preprocess_4('pill\/capsule\/softgel\/etc')","dda1c1a7":"preprocess_4('softgel-')","863c4803":"vocab = build_vocab(list(multi_label_df['info'].apply(preprocess_4)))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","9d883b05":"white_list = string.ascii_letters + string.digits + ' '\nwhite_list += \"'\"","d5a76b78":"white_list","0c52e5ab":"glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in white_list])\nglove_symbols","34b8983a":"review_chars = build_vocab(list(multi_label_df[\"info\"]))\nreview_symbols = ''.join([c for c in review_chars if not c in white_list])\nreview_symbols","b42b5f4c":"symbols_to_delete = ''.join([c for c in review_symbols if not c in glove_symbols])\nsymbols_to_delete","e17d5a34":"def preprocess_5(x):\n    x = ' '.join(preprocessing(x))\n    x = x.replace('.',' ')\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    x = re.sub('[+\/-]', ' ', x)\n\n    x = \"\".join([ x[i] for i in range(len(x)) if x[i] not in symbols_to_delete])\n    return x","c0804b7c":"preprocess_5(\"hello i'm\")","85e5b37d":"vocab = build_vocab(list(multi_label_df['info'].apply(preprocess_5).str.split()))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","0825ba19":"unique_topics = mul_binarizer.classes_","60b8276c":"train = multi_label_df\ntest = test[~test[['Review Text','Review Title']].duplicated()]","773b5d79":"train.shape","df4d76ac":"test.shape","2e816e42":"from numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)","c4bd180b":"X_train = train['info'].apply(preprocess_5)\nX_test = test['info'].apply(preprocess_5)","734a4b3b":"X_train.head()","775a8b01":"from keras.preprocessing import text, sequence","5b9100f2":"n_unique_words = None\ntokenizer = text.Tokenizer(num_words = n_unique_words)\n","98233327":"tokenizer.fit_on_texts(list(X_train) + list(X_test))\n","a16381f9":"X_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","a7bdbce4":"X_train_len = [len(x) for x in X_train]","d0e270a6":"pd.Series(X_train_len).describe()","995e6c7d":"MAX_LEN = 100\n\nX_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\nX_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)","8bd512b8":"from keras.utils.np_utils import to_categorical","29e0771a":"y = y_trans","8e99dfa0":"y","ad00ba6e":"len(tokenizer.word_index)","da132178":"EMBEDDING_FILES","881fcd90":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES[:]], axis=-1)\n","c1399ebc":"embedding_matrix.shape","5dddb286":"from keras.models import Sequential, Input, Model\nfrom keras.layers import Dense, Flatten, Dropout, SpatialDropout1D, LSTM, GRU\nfrom keras.layers import add, concatenate, Conv1D, MaxPooling1D, merge, CuDNNLSTM, CuDNNGRU\nfrom keras.layers import Embedding \nfrom keras.callbacks import ModelCheckpoint \nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom keras.utils import plot_model\nfrom keras.initializers import Constant","41e4875b":"MAX_WORDS = len(tokenizer.word_index) + 1","537629a4":"input_layer = Input(shape=(MAX_LEN,))\nembed_layer = Embedding(MAX_WORDS, embedding_matrix.shape[1],   weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)(input_layer)\nx = Bidirectional(GRU(64, return_sequences=True, dropout=0.5,recurrent_dropout=0.5))(embed_layer)\nx = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\n\nx = concatenate([ avg_pool, max_pool])\n\npreds = Dense(len(unique_topics), activation=\"sigmoid\")(x)\n\nmodel = Model(input_layer, preds)\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n\nprint(model.summary())","25ca539c":"EPOCHS = 100","06e8cb66":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearlystop = EarlyStopping(monitor='val_loss',patience=5, min_delta=0.0001, verbose=1)\nmodel_checkpoint = ModelCheckpoint(filepath='.\/model-weights.hdf5', save_best_only=True, monitor='val_loss', verbose=1)\n\ncallbacks = [\n    earlystop, \n    model_checkpoint\n]","df3ddfa0":"history = model.fit(X_train, y, \n          batch_size=16, \n          epochs=EPOCHS, \n          verbose=1, \n          validation_split=0.2,\n          callbacks= callbacks)","effc763c":"plot_model(model, to_file='model.png')","7778cf97":"fig = go.Figure()\ntmp_trace = []\n\nfig.add_trace(go.Scatter(\n    x=history.epoch, \n    y=history.history['categorical_accuracy'],\n    name='train',\n\n    mode=\"lines+markers\")\n)\nfig.add_trace(go.Scatter(\n    x=history.epoch, \n    y=history.history['val_categorical_accuracy'],\n    name='validation',\n    mode=\"lines+markers\")\n)\n\n\nfig.update_layout(\n    title=go.layout.Title(\n        text=\"Model Accuracy\",\n    ),\n    yaxis=dict(\n            title='Categorical Accuracy',\n        ),\n        xaxis=dict(\n            title=\"Epoch\",\n        )\n)\n\nfig.show()\n","9947b044":"fig = go.Figure()\ntmp_trace = []\n\nfig.add_trace(go.Scatter(\n    x=history.epoch, \n    y=history.history['loss'],\n    name='train',\n\n    mode=\"lines+markers\")\n)\nfig.add_trace(go.Scatter(\n    x=history.epoch, \n    y=history.history['val_loss'],\n    name='validation',\n    mode=\"lines+markers\")\n)\n\n\nfig.update_layout(\n    title=go.layout.Title(\n        text=\"Model Loss\",\n    ),\n    yaxis=dict(\n            title='Loss',\n        ),\n        xaxis=dict(\n            title=\"Epoch\",\n        )\n)\n\nfig.show()\n","f34f5002":"model.load_weights('model-weights.hdf5')","7d24b1da":"predict_topics = model.predict(X_test, verbose=1)","9cc3e0fd":"output = test_raw[['Review Text', 'Review Title']].copy()\noutput['topic'] = np.nan","ae027c90":"for i in range(test.shape[0]):\n    review = test.iloc[i]['Review Text']\n    title = test.iloc[i]['Review Title']\n    output_filter = output[(output['Review Text'] == review) & (output['Review Title'] == title)]\n    test_pred = predict_topics[i]\n    test_topic = np.argsort(test_pred)[::-1]\n\n    p_topics = [ unique_topics[_] for _ in test_topic][:output_filter.shape[0]]\n    \n    output.loc[output_filter.index, 'topic'] = p_topics","cac9f776":"output.head()","d432e30c":"output.topic.value_counts()","7525c509":"output.to_csv(\".\/topic_prediction_multilabel_final_model.csv\", index=False)","e56b10de":"# Important words in Review Text, Review Title for 'Packaging'","169e8256":"# TARGET DISTRIBUTION","d0c15021":"# Word Count of Review Text , Review Title","97b57d66":"# Important words in Review Text, Review Title for 'Expiry'","57de4838":"# Make training data suitable for MULTI LABEL CLASSIFICATION problem.","5cd04d3c":"# Do pre-processing by comparing Embedding Coverage","fc009065":"# Important words in Review Text, Review Title for 'Bad Taste\/Flavor'","5060c7d1":"# Feature Engineering","45b9e82d":"# Explore data more...","990ddfb3":"# Pre-trained Embeddings","40fc35bc":"We can see html tags, let's remove those.","6c4ca252":"# Length of Review Text , Review Title","174b3088":"# Handling Missing Values:","b54a6794":"From the above output it is very clear that for same *Review Text* and *Review Title* we have different topics. So we can assume that it is a **multi label classification problem.**","17174088":"# How to handle top N ?\n> If that review occurs once in the test data, submit the most correlated prediction. If the occurrence is 2, submit the top 2 topics. And so on."}}