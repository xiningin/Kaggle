{"cell_type":{"2f467a65":"code","63bf6e1e":"code","b9052892":"code","26e28407":"code","81a676d8":"code","11d5dd4d":"code","609343c5":"code","aa9931c9":"code","345b228a":"code","29442725":"code","77f83ca2":"code","3efb6663":"code","b45c9afb":"code","763e7dc5":"code","9e8e70fe":"code","ed8a830d":"code","92ddc5c4":"code","c31390e0":"code","4ddf7280":"code","71dd13e4":"code","1a5f300d":"code","1caa648b":"markdown","0174a32b":"markdown","5ae82eda":"markdown","9260c17a":"markdown","0284bed7":"markdown","7a17222b":"markdown","296ac12a":"markdown","74577a0e":"markdown","ba14c6d4":"markdown","73a2aba8":"markdown","39fc2764":"markdown","2322f462":"markdown","5ce86f7c":"markdown","9d17a6fa":"markdown","1a5caea6":"markdown"},"source":{"2f467a65":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras","63bf6e1e":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","b9052892":"train.head()","26e28407":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/train\/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/test\/\" + identifier + \".jpg\")","81a676d8":"train.head()","11d5dd4d":"train[\"Pawpularity\"].hist()","609343c5":"tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\nimage_size = 224\nbatch_size = 128\nepochs = 100\ntrain_on_fold = None # Which Fold to train, train on all folds if None\nAUTO = tf.data.experimental.AUTOTUNE","aa9931c9":"def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n    h = tf.shape(img)[0]\n    w = tf.shape(img)[1]\n    c = tf.shape(img)[2]\n    origin_area = tf.cast(h*w, tf.float32)\n\n    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh \/ rl)), tf.int32)\n\n    e_height_h = tf.minimum(e_size_h, h)\n    e_width_h = tf.minimum(e_size_h, w)\n\n    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n    erase_area = tf.cast(erase_area, tf.uint8)\n\n    pad_h = h - erase_height\n    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n    pad_bottom = pad_h - pad_top\n\n    pad_w = w - erase_width\n    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n    pad_right = pad_w - pad_left\n\n    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    erase_mask = tf.squeeze(erase_mask, axis=0)\n    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))","345b228a":"def data_augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = random_erasing(image)\n    return image","29442725":"def preprocess_training(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = data_augment(image)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)\n\ndef preprocess_validation(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)","77f83ca2":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true -  y_pred) ** 2))","3efb6663":"def base_model():\n    efficient_net = tf.keras.applications.EfficientNetB0(\n        weights = \"..\/input\/efficientnet-b0-for-keras-no-top\/efficientnetb0_notop.h5\", \n        include_top = False, \n        input_shape = (image_size, image_size, 3)\n    )    \n    efficient_net.trainable = False\n    return efficient_net","b45c9afb":"efficient_net = base_model()","763e7dc5":"def get_tabular_model(inputs):\n    width = 32\n    depth = 3\n    activation = \"relu\"\n    kernel_regularizer = keras.regularizers.l2()\n    x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(inputs)\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.Concatenate()([x, inputs])\n    return x","9e8e70fe":"def get_model():\n    image_inputs = tf.keras.Input((image_size, image_size , 3))\n    tabular_inputs = tf.keras.Input(len(tabular_columns))\n    image_x = efficient_net(image_inputs)\n    image_x = tf.keras.layers.GlobalAveragePooling2D()(image_x)\n    #for _ in range(1):\n    #    image_x = tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2())(image_x)\n    image_x = tf.keras.layers.Dropout(0.5)(image_x)\n    tabular_x = get_tabular_model(tabular_inputs)\n    x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[image_inputs, tabular_inputs], outputs=[output])\n    return model","ed8a830d":"show_architecture = False\nif show_architecture:\n    model = get_model()\n    tf.keras.utils.plot_model(model, show_shapes=True)\n    print(model.summary())\n    image = np.random.normal(size=(1, image_size, image_size, 3))\n    tabular = np.random.normal(size=(1, len(tabular_columns)))\n    print(image.shape, tabular.shape)\n    print(model((image, tabular)).shape)","92ddc5c4":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_on_fold != None and train_on_fold != index:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    \n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    \n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess_training).shuffle(512).batch(batch_size).cache().prefetch(AUTO)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess_validation).batch(batch_size).cache().prefetch(AUTO)\n    model = get_model()\n    model.compile(loss=rmse, optimizer=optimizer, metrics=[\"mae\", \"mape\"])\n    history = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\"), (\"mape\", \"val_mape\"), [\"lr\"]]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","c31390e0":"tf.keras.utils.plot_model(model, show_shapes=True)","4ddf7280":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    # 0 won't be used in prediction, but it's needed in this senario or the tabular variable is treated as label.\n    return (image, tabular), 0","71dd13e4":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[tabular_columns])).map(preprocess_test_data).batch(batch_size).cache().prefetch(2)","1a5f300d":"total_results = []\nfor model in models:\n    total_results.append(model.predict(test_ds).reshape(-1))\nresults = np.mean(total_results, axis=0).reshape(-1)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","1caa648b":"Let's have a big picture of how this Model looks like. This Model accepts images with shape (image_size, image_size, 3) and tabular information with shape (12) as input. Since it's a Regression problem, it generate output with shape (1). ","0174a32b":"## Import datasets","5ae82eda":"# Pawpularity EfficientNet: [training]\n\n## Table of Contents\n- Summary\n- Set up\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n\n\n## Summary\nIn this Notebook, I will:\n* Use EfficientNetB0 as Image Model to fit on Image Data.\n* Use DNN as Tabular Model to fit on Tabular Data.\n* Combine the total result of Image Model and Tabular Model.\n* Use RMSE as Loss Function.\n* Use Data Augmentation and Regularization method to prevent overfiting.\n* Use K-Fold training to improve final score.\n\n## Set up","9260c17a":"### RMSE","0284bed7":"### The Tabular Prediction Model","7a17222b":"## Submission","296ac12a":"I am using RMSE as Loss Function. Overfiting it can get about 3-4, If a better way is found to solve this problem, it can possibly get a better score.","74577a0e":"## The Base Model (Efficient Net)","ba14c6d4":"## Model Development","73a2aba8":"I wrote a notebook to find optimal Tabualr Prediciton Model [here](https:\/\/www.kaggle.com\/lonnieqin\/pawpularity-model-with-dnn-on-meta-data?scriptVersionId=79014727).","39fc2764":"### Distribution of Pawpularities","2322f462":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","5ce86f7c":"### Random Blockout Augmentation","9d17a6fa":"## Data Preprocessing","1a5caea6":"### Preprocess funciton\nI am doing data augmentation on training preprocess function, so that when make evaluation on validation dataset and prediction on test dataset, data augmentation won't be triggered."}}