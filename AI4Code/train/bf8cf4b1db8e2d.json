{"cell_type":{"693028b6":"code","ca27df92":"code","948dcef0":"code","75521d48":"code","69555918":"code","b5ae345a":"code","2d94c235":"code","c1e50017":"code","b480eb3b":"code","4ec2ee27":"code","cc81b43e":"code","1b651325":"code","b7e07dd1":"code","954bbe1e":"code","3192d299":"code","8c692fe2":"code","a27c1548":"code","099db113":"code","efdb99f3":"code","69d8fac7":"code","88e6533f":"code","9314bde1":"code","44a6d4cb":"code","633ff37b":"code","0872629c":"code","be9acaaf":"code","1bc86c2c":"code","f69279b6":"code","7ca462c8":"code","6ee96d8d":"code","86bc9682":"code","f899e3ba":"code","7d6bd6a9":"code","66673116":"code","9b4639ac":"code","dd5c3919":"code","68d49255":"code","ed7c4c88":"code","9bd08758":"code","ebb35190":"code","13a8cba0":"code","42f093be":"code","b60d0ebf":"code","a5c62d42":"code","c1190e17":"code","da2f8e21":"code","e6cbae24":"code","9e9853df":"code","3adc8243":"code","696efbb3":"code","181e1d10":"code","9987f7a8":"code","91a70621":"code","44b8ef61":"code","4b50f397":"code","7cb4b6f0":"code","6f59dfe0":"markdown","b80f93e3":"markdown","47af5946":"markdown","ab2f2d02":"markdown","84dda2d7":"markdown","7f6a89ec":"markdown","6b2b893e":"markdown","50cb2428":"markdown","12c29edc":"markdown","50d089c6":"markdown","73828387":"markdown","966c4933":"markdown"},"source":{"693028b6":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as ks\nfrom tensorflow.estimator import LinearRegressor\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(tf.__version__)\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (7, 6)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nimport string\n\nimport os\n\nos.chdir(\"\/kaggle\/input\/linkedin-uk-entrylevel-data-scientist-roles-2020\/DATA\/\")","ca27df92":"# Agreed labels\ndf = pd.read_csv(\"sentence_labels_v3.csv\", usecols = [\"Text\", \"none\", \"bt\", \"tt\"])\ndf = df.drop_duplicates(\"Text\")\ndf.head()\nprint(\"Number of sentences: {}\".format(len(df)))","948dcef0":"punctuation = list(string.punctuation)\npunctuation.extend([\"\\xa0\", \"\\n\"])\npunctuation.extend(stopwords.words('english'))\n        \n\ndef clean_text(txt):\n        \"\"\"\n        Helper method to clean text of punctuation, contractions and adds sentence beginning markers at beginning of sentences.\n        Also removes hashtags and mentions.\n\n        :param txt: String raw text\n\n        :return c_txt: Nested list of sentences containing list of tokens which form the cleaned the sentences.\n        \"\"\"\n\n        #print(\"Cleaning text...\")\n        # Step 1 tokenize by sentence\n        c_txt = txt.lower()\n\n        # Step 2 remove punctuation\n        for punc in punctuation:\n            try:\n                #code.interact(local = locals(), banner = \"@, # removal\")\n                if punc == \"@\":\n                    c_txt = re.sub(r\"{0}\\w+\".format(punc), \" \", c_txt)\n                if punc in c_txt:\n                    if punc in stopwords.words('english'):\n                        c_txt = re.sub(r\" {0} \".format(punc), \" \", c_txt)\n                    else:\n                        try:\n                            if punc != \".\":\n                                c_txt = re.sub(r\"{0}\".format(punc), \" \", c_txt)\n                            else:\n                                c_txt = re.sub(r\"\\{0}\".format(punc), \" \", c_txt)\n                        except:\n                            c_txt = re.sub(r\"\\{0}\".format(punc), \"\", c_txt)\n                else:\n                    pass\n            except Exception as e:\n                pass\n            \n        # Remove non-ASCII characters\n        if c_txt.isascii():\n            pass\n        else:\n            txt = c_txt\n            c_txt = \"\"\n            for char in txt:\n                if char.isascii():\n                    c_txt = c_txt + char\n                else:\n                    pass\n                \n        #\u00a0Replace digits with DIGIT flag\n        c_txt = re.sub(r\"\\d+\".format(punc), \" DIGIT \", c_txt)\n        \n                \n        return c_txt","75521d48":"df.loc[:,\"Clean Text\"] = df[\"Text\"].apply(clean_text)","69555918":"# class distribution\ndf.loc[:, [\"none\", \"bt\", \"tt\"]].sum()\n\n# converting one hot encoding to indices to represent the classes.\n#\u00a0[none, bt, tt] == [0, 1, 2]\ndf.loc[:, \"Class\"] = df.loc[:,\"bt\"]\ndf.loc[:, \"Class\"] += 2 * df.loc[:,\"tt\"] \n\nsns.countplot(df[\"Class\"])\nplt.title(\"\"\"Class distribution over LinkedIn UK Entry-level\nData Scientist Job Listing dataset \\n (n = {})\"\"\".format(len(df)))\nplt.xticks(np.arange(3), [\"none\", \"bt\", \"tt\"])\nplt.ylabel(\"Frequency\")\nplt.show()","b5ae345a":"\"\"\"\nThe unknown tokens were identified by the following:\n\n- Tokens which have a word count of 1 in the sentences dataset.\n- Tokens which appear lower than 25th percentile = 3 of those word frequency over overall job listing dataset.\n\n\"\"\"\nunk_words = pd.read_csv(\"UNK_flag.csv\", index_col = 0)\n# Applied 50th percentile as UNK flag\nunk_words = set(list(unk_words[\"word\"].values))","2d94c235":"#\u00a0vocab size\n\nvocab = []\n\n# converting unknown flagged words with unknown flag UNK\nfor i in df[\"Clean Text\"].values:\n    for w in unk_words:\n        i = re.sub(\" {} \".format(w), \" UNK \", i)\n    vocab.extend(word_tokenize(i))\n    vocab = list(set(vocab))\n    \nvocab = sorted(vocab)\n\nprint(\"\"\"Vocabulary: {} \nUnknown Words: {}\"\"\".format(len(unk_words), len(vocab)))","c1e50017":"def encode_idxs(txt):\n    \"\"\"\n    Converts sentence of words into a sequence of indices as\n    per the vocab list.\n    \n    :param txt: (String) input sentence\n    \n    :return idxs: (List of Integers) sequence of indices\n    \"\"\"\n    idxs = []\n    for word in word_tokenize(txt):\n        if word in unk_words:\n            idxs.append(vocab.index(\"UNK\"))\n        else:\n            idxs.append(vocab.index(word))\n        \n    return idxs","b480eb3b":"seq = [encode_idxs(txt) for txt in df[\"Clean Text\"]] \nX = tf.keras.preprocessing.sequence.pad_sequences(seq)\ny = tf.keras.utils.to_categorical(df[\"Class\"], df[\"Class\"].unique().size)\n\ndata = np.hstack((X, y))\n\nprint(data.shape)","4ec2ee27":"train, test = train_test_split(data, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\n\nbatch_size = 10\n\ndef prepare_dataset(d, classes):\n    \n    X = d[:, :-classes]\n    y = d[:, -classes:]\n    \n    return X, y\n\ndef prepare_all(datasets, classes):\n    \"\"\"\n    Prepare all datasets for multi-class classification.\n    \"\"\"\n    dfs = ()\n    for d in datasets:\n        dfs = dfs + prepare_dataset(d, classes)\n        \n    return dfs","cc81b43e":"all_data = prepare_all([train, val, test], classes = 3)\n\nX_train, y_train, X_val, y_val, X_test, y_test = all_data","1b651325":"print(\"\"\"Examples:\n\nTraining Set : {}\nValidation Set : {}\nTest Set : {}\"\"\".format(X_train.shape[0], X_val.shape[0], X_test.shape[0]))","b7e07dd1":"#\u00a0Hyperparameters\n\nembed_dim = 64\nlstm_output = 15\noutput_dim = df[\"Class\"].unique().size","954bbe1e":"baseline = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.LSTM(lstm_output),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"baseline\")\n\nMETRICS = [ \n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n]\n\nbaseline.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbaseline.summary()","3192d299":"bs_history = baseline.fit(X_train, y_train, \n                       epochs=20,\n                       validation_data=(X_val, y_val)) ","8c692fe2":"def plot_loss(history, label, n):\n    # Use a log scale to show the wide range of values.\n    plt.semilogy(history.epoch,\n                 history.history['loss'],\n                 color=colors[n],\n                 label='Train '+label)\n    plt.semilogy(history.epoch,\n                 history.history['val_loss'],\n                 color=colors[n],\n                 label='Val '+label,\n                 linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n\n    plt.legend()","a27c1548":"baseline.evaluate(X_test, y_test)","099db113":"def cm(model, classes = 3, datasets = None):\n\n    f, (ax1, ax2) = plt.subplots(1,2, sharex = True, figsize = (15, 7))\n    \n    if datasets is None:\n        datasets = [(X_val, y_val), (X_test, y_test)]\n    \n    for a, ds, t in zip([ax1, ax2], datasets, [\"validation\", \"test\"]):\n        prediction = np.argmax(model.predict(ds[0]), axis = 1)\n        tst = np.vstack((np.argmax(ds[1], axis = 1), prediction)).T\n\n        cm = np.zeros((classes,classes))\n\n        for i in tst:\n            cm[i[0], i[1]] += 1\n\n        sns.heatmap(cm\/ np.atleast_2d(np.sum(cm, axis = 1)).T, annot = True, ax = a)\n        a.set_title(\"Confusion Matrix over {} set\".format(t))\n        a.set_ylabel(\"Actual\")\n        a.set_xlabel(\"Predicted\")","efdb99f3":"cm(baseline)\n","69d8fac7":"def balance_classes(data):\n    d = pd.DataFrame(data[\"Class\"].value_counts())\n    max_class = d.idxmax(axis = 0).values[0]\n    max_class_count = d.loc[max_class][0]\n\n    new_data = data[data[\"Class\"] == max_class]\n\n    for c in list(set(data[\"Class\"].unique()) - set([max_class])):\n        c_idxs = data[data[\"Class\"] == c].index.values\n        c_idxs = np.random.choice(c_idxs, max_class_count)\n        new_data = pd.concat([new_data, df.loc[c_idxs,:]], ignore_index = True)\n        \n    return new_data","88e6533f":"new_data = balance_classes(df)\n\nseq = [encode_idxs(txt) for txt in new_data[\"Clean Text\"]] \nX = tf.keras.preprocessing.sequence.pad_sequences(seq)\ny = tf.keras.utils.to_categorical(new_data[\"Class\"], new_data[\"Class\"].unique().size)\n\ndata = np.hstack((X, y))\nprint(data.shape)\n\ntrain, test = train_test_split(data, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\n\nall_data = prepare_all([train, val, test], 3)\n\nX_train, y_train, X_val, y_val, X_test, y_test = all_data","9314bde1":"#\u00a0Look at distribution of UNK tag in data by class\n\nUNK_count = np.where(X == vocab.index(\"UNK\"), 1, 0)\nUNK_count = UNK_count.sum(axis = 1)\n\nUNK_count = pd.DataFrame(np.vstack((UNK_count, new_data[\"Class\"].values))).T\nUNK_count.columns = [\"UNK\", \"Class\"]\n\nplt.figure(figsize = (5,6))\nplt.title(\"Distribution of UNK tags by class\")\nsns.boxplot(x = \"Class\", y = \"UNK\", data = UNK_count)\nplt.show()","44a6d4cb":"base_balance = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.LSTM(lstm_output),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"baseline-balanced\")\n\nbase_balance.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbase_balance.summary()","633ff37b":"bs_balanced_history = base_balance.fit(X_train, y_train, \n                       epochs=20,\n                       validation_data=(X_val, y_val))","0872629c":"cm(base_balance)","be9acaaf":"inc_embed = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim * 3),\n    layers.LSTM(lstm_output),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"increase-embedding\")\n\ninc_embed.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\ninc_embed.summary()","1bc86c2c":"inc_embed_history = inc_embed.fit(X_train, y_train, \n                       epochs=100,\n                       validation_data=(X_val, y_val))","f69279b6":"cm(inc_embed)","7ca462c8":"inc_layers = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.LSTM(lstm_output, return_sequences=True),\n    layers.LSTM(lstm_output, return_sequences=True),\n    layers.LSTM(lstm_output),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"increase-layers\")\n\ninc_layers.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\ninc_layers.summary()","6ee96d8d":"inc_layers_history = inc_layers.fit(X_train, y_train, \n                       epochs=20,\n                       validation_data=(X_val, y_val))","86bc9682":"cm(inc_layers)","f899e3ba":"bi_base = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"bidirection-baseline\")\n\nbi_base.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\nearlystop_callback = tf.keras.callbacks.EarlyStopping(\n  monitor='val_loss', min_delta=0.001, mode = \"min\",\n  patience=20, verbose=1)\n\nbi_base.summary()","7d6bd6a9":"bi_base_history = bi_base.fit(X_train, y_train, \n                              # worse performance (recall, precision) beyond 21 epochs\n                       epochs=22,\n                       validation_data=(X_val, y_val),\n                              callbacks=[earlystop_callback])","66673116":"cm(bi_base)","9b4639ac":"bi_inc_embed = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim * 3),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"bidirection-inc-embed\")\n\nbi_inc_embed.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\nbi_inc_embed.summary()","dd5c3919":"bi_inc_embed_history = bi_inc_embed.fit(X_train, y_train, \n                                        # 23 epochs was chosen as the optimal number of epochs\n                       epochs=23,\n                       validation_data=(X_val, y_val))","68d49255":"cm(bi_inc_embed)","ed7c4c88":"bi_inc_layers = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.Bidirectional(layers.LSTM(lstm_output, return_sequences = True)),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(output_dim, activation = \"softmax\")\n], name = \"bidirection-inc-layers\")\n\nbi_inc_layers.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbi_inc_layers.summary()","9bd08758":"bi_inc_layers_history = bi_inc_layers.fit(X_train, y_train, \n                       epochs=20,\n                       validation_data=(X_val, y_val))","ebb35190":"cm(bi_inc_layers)","13a8cba0":"plot_loss(bs_history, \"Baseline\", 0)\nplot_loss(inc_embed_history, \"Inc Embed\", 1)\nplot_loss(inc_layers_history, \"Inc Layers\", 2)\nplot_loss(bi_base_history, \"Bi Baseline\", 3)\nplot_loss(bi_inc_embed_history, \"Bi Inc Embed\", 4)\nplot_loss(bs_balanced_history, \"Baseline Balanced\", 1)","42f093be":"df.loc[:, \"bt\/tt\"] = (df[\"Class\"] > 0).astype(int)\n\nnone_vs_other = df.loc[:,[\"Clean Text\", \"bt\/tt\"]]\nbt_vs_tt = df[df[\"none\"] == 0].loc[:,[\"Clean Text\", \"tt\"]]","b60d0ebf":"none_vs_other = balance_classes(df)\n\nseq = [encode_idxs(txt) for txt in none_vs_other[\"Clean Text\"]] \nX = tf.keras.preprocessing.sequence.pad_sequences(seq)\ny = tf.keras.utils.to_categorical(none_vs_other[\"bt\/tt\"], none_vs_other[\"bt\/tt\"].unique().size)\n\ndata = np.hstack((X, y))\nprint(data.shape)\n\ntrain, test = train_test_split(data, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\n\nall_data = prepare_all([train, val, test], 2)\n\nnvo_X_train, nvo_y_train, nvo_X_val, nvo_y_val, nvo_X_test, nvo_y_test = all_data\n\nprint(nvo_X_train.shape)\nprint(nvo_y_train.shape)","a5c62d42":"bt_vs_tt = balance_classes(df)\n\nseq = [encode_idxs(txt) for txt in bt_vs_tt[\"Clean Text\"]] \nX = tf.keras.preprocessing.sequence.pad_sequences(seq)\ny = tf.keras.utils.to_categorical(bt_vs_tt[\"tt\"], bt_vs_tt[\"tt\"].unique().size)\n\ndata = np.hstack((X, y))\nprint(data.shape)\n\ntrain, test = train_test_split(data, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\n\nall_data = prepare_all([train, val, test], 2)\n\nbvt_X_train, bvt_y_train, bvt_X_val, bvt_y_val, bvt_X_test, bvt_y_test = all_data","c1190e17":"bi_nvo = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(2, activation = \"softmax\")\n], name = \"bidirectional-none-vs-other\")\n\nbi_nvo.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbi_nvo.summary()","da2f8e21":"bi_nvo_embed_history = bi_nvo.fit(nvo_X_train, nvo_y_train, \n                       epochs=20,\n                       validation_data=(nvo_X_val, nvo_y_val), \n                       validation_steps=30)","e6cbae24":"cm(bi_nvo, 2, [(nvo_X_test, nvo_y_test),(nvo_X_val, nvo_y_val)])","9e9853df":"bi_bvt = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(2, activation = \"softmax\")\n], name = \"bidirectional-bt-vs-tt\")\n\nbi_bvt.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbi_bvt.summary()","3adc8243":"bi_bvt_embed_history = bi_bvt.fit(bvt_X_train, bvt_y_train, \n                       epochs=20,\n                       validation_data=(bvt_X_val, bvt_y_val), \n                       validation_steps=30)","696efbb3":"cm(bi_bvt, 2, [(bvt_X_test, bvt_y_test),(bvt_X_val, bvt_y_val)])","181e1d10":"bi_nvo_inc_embed = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim * 2),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(2, activation = \"softmax\")\n], name = \"bidirectional-none-vs-other-inc-embed\")\n\nbi_nvo_inc_embed.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbi_nvo_inc_embed.summary()","9987f7a8":"bi_nvo_inc_embed_history = bi_nvo.fit(nvo_X_train, nvo_y_train, \n                       epochs=20,\n                       validation_data=(nvo_X_val, nvo_y_val), \n                       validation_steps=30)","91a70621":"cm(bi_nvo_inc_embed, 2, [(nvo_X_test, nvo_y_test),(nvo_X_val, nvo_y_val)])","44b8ef61":"bi_bvt_inc_embed = tf.keras.Sequential([\n    layers.Embedding(input_dim = len(vocab), output_dim = embed_dim * 2),\n    layers.Bidirectional(layers.LSTM(lstm_output)),\n    layers.Dense(2, activation = \"softmax\")\n], name = \"bidirectional-bt-vs-tt-inc-embed\")\n\nbi_bvt_inc_embed.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\nbi_bvt_inc_embed.summary()","4b50f397":"bi_bvt_inc_embed_history = bi_bvt_inc_embed.fit(bvt_X_train, bvt_y_train, \n                       epochs=20,\n                       validation_data=(bvt_X_val, bvt_y_val), \n                       validation_steps=30)","7cb4b6f0":"cm(bi_bvt_inc_embed, 2, [(bvt_X_test, bvt_y_test),(bvt_X_val, bvt_y_val)])","6f59dfe0":"# Try using balanced dataset, by applying minority oversampling","b80f93e3":"If we reduce the classification problem to distinguishing between two classes, will it improve precision and recall?\n\n1. Check none and other class scenario\n2. Check bt and tt classification","47af5946":"This could be due to class imbalance, there are twice as many tt examples, than both bt and none","ab2f2d02":"#\u00a0Text cleaning protocol\n\nThe below code instantiates a text cleaning protocol, which removes punctuation and stopwords (a, the, etc.) from the sentences.","84dda2d7":"#\u00a0Data\n\nThe dataset contains a set of sentences taken from LinkedIn UK Data Scientist Job Listing Dataset. The dataset so far contains 328 Data Science-related job listings. Check the following link out to find further details abotu the dataset.\n\nIn the below file, we have relation labels (bt, tt or none), which have been annotated and validated among annotators. ","7f6a89ec":"#\u00a0Model Architecture","6b2b893e":"# Data Preparation\n\nIn this stage, we are preparing the dataset for the model. Here, we add","50cb2428":"# Obversations\n\nAccording to the plots above:\n\n- Firstly, after epoch 25 approx, there is very little significant reductions in error. Error instead plateaus. This would indicate high model bias and thus we need to add further complexity to the model.\n\nSo far, I have identified two way to increase the complexity of the model. \n\n1. Increase the embedding dimension (inc_embed)\n2. Increase the number of layers (inc_layers)","12c29edc":"# Business and Non-Business Relations to Data Science Skills\n\nIn this notebook, I will be experimenting with various neural network architectures which aim to achieve generalisation performation in the following task:\nThe model will take in a sentences taken from the LinkedIn UK Entry-level Data Scientist Job Listing Dataset and will predict if the sentence:\n* Shows a business application to data science skills (bt)\n* Shows a non-business \/ technical application to data science skills (tt)\n* Shows neither application (none)\n\nThis model is used to establish whether job listings will provide entry-level data scientists with commercial experience, which is asked from many employers and recruiters. For this model, commercial experience is defined as the skill of applying data science skills to business problems, hence the bt relation. ","50d089c6":"#\u00a0Stacked \/ Ensemble Model\n\nFrom the above experiments, it appears that so far the model is relatively good at distinguishing between none and the other classes. Therefore, we are going to try an ensemble model approach, where we will train 2 separate Bi-LSTMs. \n\nOne will be trained to dinguish between none and the other classes, which will merge to form one class. \n\nThe other model will be trained to distinguish between bt and tt sentences.","73828387":"Choosing this model now for predictions, as it does show high true positive rate for all classes, particularly\nfor none and tt, which are the most important classes anyway.","966c4933":"#\u00a0Trying Bidirectional LSTM and comparing against Baseline LSTM"}}