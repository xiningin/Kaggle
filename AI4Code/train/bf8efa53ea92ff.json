{"cell_type":{"0c76c7dc":"code","350e6aa1":"code","2ba57e74":"code","3ed8cd62":"code","6e4ef5b0":"code","e004a376":"code","8699be92":"code","e01e6a99":"code","8e6c1586":"code","f95730ab":"code","28683dc7":"code","c17cfe90":"code","800b7668":"code","9444e59d":"code","bb90db52":"code","92b35c00":"code","8f0af8b9":"code","cced7197":"code","56166aac":"code","f00fc502":"code","d6bbfdf3":"code","3d22a7a9":"code","b993b371":"code","140e39cd":"code","d77c3ee3":"code","b152a733":"code","fe7b78dd":"markdown","a03f7df6":"markdown","452c33f5":"markdown","96b83aae":"markdown","243bb429":"markdown","d45e5405":"markdown","bcae10c6":"markdown","ba5f3ea3":"markdown","24d7cc91":"markdown","f65b1705":"markdown","4ce56306":"markdown","5e4f70f9":"markdown","861f61fb":"markdown","b1f23a8d":"markdown","bec56862":"markdown","c401e799":"markdown","284b14de":"markdown","dabc4345":"markdown","6fbaabc9":"markdown","f67a696a":"markdown","d62b03b0":"markdown","d85e976d":"markdown","b42c343b":"markdown","abf3195a":"markdown","ae3eeb35":"markdown","2846723c":"markdown","b662d380":"markdown","a5e12bf9":"markdown","0e53d96b":"markdown","004f743f":"markdown","86f69c81":"markdown","744336f3":"markdown","891fb2f8":"markdown"},"source":{"0c76c7dc":"# install graph drawing and interaction libraries\n# !pip uninstall networkx -y\n# !pip install networkx==1.11\n\n#pip install nxpd\n# !pip install ipywidgets\n# !apt-get install python-pydot\n\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport sympy \nfrom sympy import *\nfrom sympy import Matrix\nfrom sympy.utilities.lambdify import lambdify, implemented_function\nfrom sympy import Function\n\nimport networkx as nx\n\nimport __future__\nimport __init__\n\n%matplotlib inline\n\nimport matplotlib.pylab as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.display import Latex\n\n#If you want all graphs to be drawn inline, then you can set a global parameter.\nfrom nxpd import draw\nfrom nxpd import nxpdParams\nnxpdParams['show'] = 'ipynb'\n\nimport string\n\nfrom ipywidgets import Image\n#from IPython import display\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\nfrom IPython.display import Math, HTML, display\n\n\n\n\ndef create_Markov_matrix(height, width):\n    '''\n    randomly create a 'right' markov matrix (each row sums to 1)\n    '''\n    \n    x = np.random.random(size=[width, height])\n\n    # use float128 for precision when taking powers\n    markov_matrix = x \/ x.sum(axis=0, dtype = np.float128)  \n\n    markov_matrix = markov_matrix.transpose()\n    \n    return(markov_matrix)\n\n\ndef show_latex_matrix(matrix1, pre_latex = \"\", post_latex = \"\"):\n    '''\n    '''\n    \n    display(HTML(\"<script src='https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.3\/\"\n               \"latest.js?config=default'><\/script>\"))\n\n    return(Math(pre_latex + latex(Matrix(matrix1)) + post_latex))\n    \n    #return latex(Matrix(matrix1))\n    #return(Latex('$%s$'%latex(Matrix(matrix1))))\n\n\ndef create_Markov_graph(markov_matrix, mapping = None):\n    '''\n    inputs:\n    numpy matrix\n    a mapping dictionary from integer node ids to anything\n    \n    returns:\n    a networkx graph\n    '''\n\n    G=nx.from_numpy_matrix(markov_matrix, create_using=nx.DiGraph())\n\n    # if no mapping is passed, just keeps the node ids\n    if (mapping == None):\n        mapping = dict(zip(list(range(markov_matrix.shape[0])), list(range(markov_matrix.shape[0]))))\n        \n    G=nx.relabel_nodes(G, mapping)\n\n    nx.set_node_attributes(G, 'fontsize', 10)\n    nx.set_node_attributes(G, 'color', 'black')\n\n    nx.set_edge_attributes(G, 'fontsize', 10)\n    nx.set_edge_attributes(G, 'color', 'grey')\n\n    weights = nx.get_edge_attributes(G, 'weight')\n    weights = nx.get_edge_attributes(G, 'weight')\n    vals = weights.values()\n    vals = np.array(list(vals)).astype(float)\n    vals = np.round(vals, 2)\n    weights = dict(zip(list(weights.keys()),(vals)))\n    \n    nx.set_edge_attributes(G, name='label', values=weights)\n    \n    return(G)\n\n\ndef compute_markov_states(markov_matrix, mapping = None, num_trials = 10):\n    '''\n    inputs:\n    a numpy right markov matrix    \n    a mapping dictionary from integer node ids to anything\n    \n    returns:\n    list of all states of system\n    '''\n\n    states = []\n    \n    # if no mapping is passed, just keeps the node ids\n    if (mapping == None):\n        mapping = dict(zip(list(range(markov_matrix.shape[0])), list(range(markov_matrix.shape[0]))))\n    \n    height, width = markov_matrix.shape\n\n    # set up horizontal cumulative sums for computing next state\n    cum_sum = markov_matrix.cumsum(axis=1)\n\n    # start state\n    state_index = np.random.choice(list(range(height)))\n    state = np.zeros(height)\n    state[state_index] = 1\n\n    states.append(mapping[state_index])    \n    \n    for i in range(num_trials):\n        rand_num = np.random.rand()\n        state_index = np.where(state == 1)[0][0]\n        next_state = np.where(cum_sum[state_index,:] > rand_num)[0][0]\n        state_index = next_state\n        #print(mapping[state_index])\n        #input()  #pause at each step\n        states.append(mapping[state_index])\n    return(states)\n\n\ndef create_alphabet_mapping_dict():\n    '''\n    map the node id numbers to letters\n    '''\n    \n    # the node ids (let's use letters)\n    alphabet = list(string.ascii_uppercase)\n    keys = list(range(len(alphabet)))\n    mapping = dict(zip(keys, alphabet))\n    \n    return mapping\n\ndef coarse_grain_markov(markov_matrix, timesteps):\n    '''\n    coarsegrain the Markov Matrix\n    \n    inputs:\n    a right-markov matrix\n    number of timesteps (powers of matrix)\n    \n    returns\n    a right-markov matrix equal to the specified power of the input matrix\n    '''\n    \n    return(np.linalg.matrix_power(markov_matrix, timesteps))\n    ","350e6aa1":"height = 2\nwidth = 2\n\n# map the node id numbers to letters\nmapping = create_alphabet_mapping_dict()\n\n# create the markov matrix\nmarkov_matrix = create_Markov_matrix(height, width)\n\n# display it nicely using latex\nshow_latex_matrix(markov_matrix, \"T=\")","2ba57e74":"# check that rows sum to 1\nmarkov_matrix.sum(axis=1)","3ed8cd62":"# create networkx graph\nG = create_Markov_graph(markov_matrix, mapping)\n\n# draw networkx graph with graphviz\nim = draw(G, layout='circo')\nim","6e4ef5b0":"num_trials = 10\n\ncompute_markov_states(markov_matrix, mapping, num_trials)","e004a376":"timesteps = 2\n\ncoarse_markov_2 = coarse_grain_markov(markov_matrix, timesteps)\nshow_latex_matrix(coarse_markov_2, pre_latex = \"T^{\" + str(timesteps) + \"}=\")","8699be92":"# check that each row sums to 1\ncoarse_markov_2.sum(axis = 1)","e01e6a99":"# create networkx graph\nG = create_Markov_graph(coarse_markov_2, mapping)\n\n# draw networkx graph with graphviz\nim = draw(G, layout='circo')\nim","8e6c1586":"timesteps = 100","f95730ab":"coarse_markov = coarse_grain_markov(markov_matrix, timesteps)\nshow_latex_matrix(coarse_markov)","28683dc7":"# check that each row sums to 1\ncoarse_markov.sum(axis = 1)","c17cfe90":"# create networkx graph\nG = create_Markov_graph(coarse_markov, mapping)\n\n# draw networkx graph with graphviz\nim = draw(G, layout='circo')\nim","800b7668":"G = nx.DiGraph()\n\nG.add_edge(\"A\", \"B\", label = '1-p_a')\nG.add_edge(\"B\", \"A\", label = 'p_a')\nG.add_edge(\"A\", \"A\", label = 'p_a')\nG.add_edge(\"B\", \"B\", label = '1-p_a')\n\n\n# draw networkx graph with graphviz\ndraw(G, layout='dot')","9444e59d":"G = nx.DiGraph()\n\nG.add_edge(\"A\", \"B\", label = 'p_b = 1-p_a')\nG.add_edge(\"B\", \"A\", label = 'p_a')\nG.add_edge(\"A\", \"A\", label = 'p_a')\nG.add_edge(\"B\", \"B\", label = 'p_b = 1-p_a')\n\n# draw networkx graph with graphviz\ndraw(G, layout='dot')","bb90db52":"# get the eigenvalues and eigenvectors\nlambdas, eig_vectors = np.linalg.eig(markov_matrix.astype(np.float))\n\nprint('eigenvalues:', lambdas)\nprint()\nprint('eigenvectors:')\nprint(eig_vectors)","92b35c00":"num_coarse_grainings = 10\nlambda_values = np.zeros([num_coarse_grainings, height])\neigenvectors = []\np_A_A = [] #p(A|A)\np_B_B = [] #p(B|B)\n\nfor i in range(num_coarse_grainings):\n    coarse_markov = coarse_grain_markov(markov_matrix, i)\n    \n    lambdas, eig_vectors = np.linalg.eig(coarse_markov.astype(np.float))\n    lambda_values[i] = lambdas\n    eigenvectors.append(eig_vectors)\n    p_A_A.append(coarse_markov[0,0])\n    p_B_B.append(coarse_markov[1,1])\n    #print('$\\lambdas$:',lambdas)\n    #print('v',eig_vectors)\n    #print()","8f0af8b9":"# here plot the time evolution of the eigenvalues\n\ncol_names = []\nfor i in range(lambda_values.shape[1]):\n    col_names.append('lambda_' + str(i + 1))\n    \ndf = pd.DataFrame(lambda_values, columns=col_names)\n\ndf.plot();\nplt.xlabel('time coarse graining')\nplt.ylabel('eigenvalues');","cced7197":"# the phase diagram\n\nplt.figure()\n\n# the subsequent entries p(A|A), p(B|B) (the diagonal entries in the coarse-grained Markov matrix)\nplt.scatter(p_A_A, p_B_B);\nplt.xlabel('p(A|A)')\nplt.ylabel('p(B|B)')\n\n# the diagonal line slope 1\nplt.plot([0,1], [0,1]);\n\n# the fixed point 1-dimensional manifold\nplt.plot([0,1], [1,0]);\n\n","56166aac":"# set the transition probability\nepsilon = np.random.rand()\n\n# for display purposes, keep it to 2 decimal points\nepsilon = round(epsilon,2)\n\nG = nx.DiGraph()\n\nG.add_edge(\"A\", \"B\", label = \"1 - \u03f5\", weight = 1 - epsilon)\nG.add_edge(\"C\", \"D\", label = \"1 - \u03f5\", weight = 1 - epsilon)\nG.add_edge(\"A\", \"D\", label = '\u03f5', weight = epsilon)\nG.add_edge(\"D\", \"A\", label = '1', weight = 1)\nG.add_edge(\"C\", \"B\", label = \"\u03f5\", weight = epsilon)\nG.add_edge(\"B\", \"C\", label = \"1\", weight = 1)\n\n# draw networkx graph with graphviz\ndraw(G, layout='circo')","f00fc502":"mat = nx.adj_matrix(G)\nmarkov = mat.toarray()\nshow_latex_matrix(markov, pre_latex = \"A=\")","d6bbfdf3":"num_trials = 10\n\ncompute_markov_states(markov, mapping, num_trials)","3d22a7a9":"# set the transition probability\nepsilon = np.random.rand()\n\n# for display purposes, keep it to 2 decimal points\nepsilon = round(epsilon,2)\n\nG = nx.DiGraph()\n\nG.add_edge(\"A\", \"A\", label = \"\u03f5\", weight = epsilon)\nG.add_edge(\"C\", \"C\", label = \"\u03f5\", weight = epsilon)\nG.add_edge(\"A\", \"C\", label = \"1 - \u03f5\", weight = 1 - epsilon)\nG.add_edge(\"C\", \"A\", label = \"1 - \u03f5\", weight = 1 - epsilon)\n\nG.add_edge(\"D\", \"D\", label = \"\u03f5\", weight = epsilon)\nG.add_edge(\"B\", \"B\", label = \"\u03f5\", weight = epsilon)\nG.add_edge(\"D\", \"B\", label = \"1 - \u03f5\", weight = 1 - epsilon)\nG.add_edge(\"B\", \"D\", label = \"1 - \u03f5\", weight = 1 - epsilon)\n\n# draw networkx graph with graphviz\ndraw(G, layout='dot')","b993b371":"A_2 = coarse_grain_markov(markov, 2)\nshow_latex_matrix(A_2, pre_latex = \"A^2 =\")","140e39cd":"A_2","d77c3ee3":"G=nx.from_numpy_matrix(A_2, create_using=nx.DiGraph)\nvals = nx.get_edge_attributes(G, 'weight')\n\nnx.set_edge_attributes(G, name = 'label', values = vals)\ndraw(G, layout='dot')","b152a733":"G = nx.DiGraph()\n\nG.add_edge(\"A\", \"B\", label = \"1-\u03f5\", weight = epsilon)\nG.add_edge(\"B\", \"A\", label = \"1-\u03f5\", weight = epsilon)\n\ndraw(G, layout='dot')","fe7b78dd":"since it must be that $a = b$, from the diagonals, we get\n\n$$\na^2 + (1-a)^2 = \\epsilon\n$$\n\n$$\na^2 + 1 -2a + a^2 = \\epsilon\n$$\n\n$$\n2a^2 -2a + 1-\\epsilon = 0\n$$","a03f7df6":"In the limit this will map the probabilty matrix to the first eigenvector.\n\n$$\np \\to \\vec v_1\n$$\n\nThis is the **stationary distribution** of the original stochastic matrix.\n\nWe start out with a model that is described by many parameters (having many degrees of freedom), \n\nbut coarse-graining enough simplifies the description and reduces the description down to one parameter.","452c33f5":"$$\n= \\sqrt{-\\frac12}\n$$\n\n$$\n= 0.5 i\n$$\n\n$$\n\\implies \\frac12 \\pm \\frac 12 \\frac12 i = \\frac12 \\pm \\frac14 i\n$$\n\nThus the roots are\n\n$$\n(\\frac12 + \\frac14 i, \\frac12 - \\frac14 i)\n$$\n\nboth complex numbers.\n\nThus the $a$ (or $b$) transition probability values from the matrix $A$ must be complex for certain matrices $T$. \n\n(Here, a $T$ having $\\frac34$ diagonal values.)\n\nThis will be true for any $\\epsilon < \\frac12$.  There will be no valid probability matrix $A$ satisfying this equation.\n\n","96b83aae":"$$\n= \\sqrt{1 - 2 + \\frac32}\n$$","243bb429":"This is equivalent to coarse-graining the states:\n\n$$\nA, D \\to A\n$$\n\n$$\nB, C \\to B\n$$\n\nThis gives us:","d45e5405":"Recall the Quadratic formula:\n$$\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$ (of course, this $a$ is not the same $a$ in the equation above, but the coefficient to the quadratic term)","bcae10c6":"# Setup and Functions","ba5f3ea3":"####  this is broken in colaboratory, but should run fine offline\n\nG = nx.DiGraph()\n\nG.add_edge(\"A\", \"B\", label = r'1-\u03f5')\nG.add_edge(\"B\", \"A\", label = r'1-\u03f5')\nG.add_edge(\"A\", \"A\", label = r'\u03f5')\nG.add_edge(\"B\", \"B\", label = r'\u03f5')\n\n\n####  draw networkx graph with graphviz\n\nim = draw(G, layout='circo')\n\nim","24d7cc91":"### Plot the phase-space as we coarse grain","f65b1705":"$$\n= \\begin{pmatrix}\n    a^2 + (1-a)(1-b)       & a (1-b) + b(1-b) \\\\\n    (1-a)a + b(1-a)       & (1-a)(1-b) + b^2\n\\end{pmatrix}\n$$\n\nsetting $A^2 = T$ gives:\n\n$$\n\\begin{pmatrix}\n    a^2 + (1-a)(1-b)       & a (1-b) + b(1-b) \\\\\n    (1-a)a + b(1-a)       & (1-a)(1-b) + b^2\n\\end{pmatrix} = \n\\begin{pmatrix}\n    \\epsilon       & 1-\\epsilon \\\\\n    1-\\epsilon       & \\epsilon\n\\end{pmatrix}\n$$","4ce56306":"gives us: \n\n$$\n\\frac{2 \\pm \\sqrt{4 - 4 * 2 *(1-\\epsilon)}}{2 * 2}\n$$\n\n$$\n= \\frac12 \\pm \\frac{ 2 \\sqrt{1 - (2 - 2 \\epsilon)}}{4}\n$$\n\n$$\n= \\frac12 \\pm \\frac12 \\sqrt{1 - 2 (1-\\epsilon)}\n$$","5e4f70f9":"$$\n\\sqrt{1 - 2 (1-\\frac34)}\n$$","861f61fb":"## Fine - graining in time (instead of coarse graining)\n\nImagine a sequence of states:\n\nABABAB\n\nat a finer time scale we would have to fill in missing values\n\nA?B?A?B?A?B\n\nBut filling in either A or B in the question marks **deterministically** violates the memoryless property of Markov chains.  \n(if we put an A in the first ?, how do we know when to escape the second A and transition to B?)","b1f23a8d":"### coarse-grain by 2 timesteps: $A^2$","bec56862":"## Now, coarsegrain the Markov Matrix to arbitrary steps (set timesteps and re-run)","c401e799":"The eigenvalues of $T$ are 1 and $2\\epsilon-1$.\n\nThis lets us write the diagonalized version of of the evolution operator $\\hat{T}$\n\n$$\n\\hat{T} = \\begin{pmatrix}\n    1       & 0 \\\\\n    0       & 2\\epsilon-1\n\\end{pmatrix}\n$$\n\nWhen the transformed evolution operator (stochastic matrix) $\\hat{T}$ operates in the eigenvector space:\n\n$$\n\\begin{pmatrix}\n    1       & 0 \\\\\n    0       & 2\\epsilon-1\n\\end{pmatrix} \\begin{pmatrix}\\alpha_1 \\\\ \\alpha_2\\end{pmatrix} = \\begin{pmatrix}\\alpha_1 \\\\ (2\\epsilon-1)\\alpha_2\\end{pmatrix}\n$$\n\n\n**For a stochastic matrix, one of its eigenvalues will always have length 1, and the other eigenvalues will have length < 1.**","284b14de":"As we coarse-grain further, we get\n\n$$\nT^n = \\begin{pmatrix}\n    1       & 0 \\\\\n    0       & (2\\epsilon-1)^n\n\\end{pmatrix} \\begin{pmatrix}\\alpha_1 \\\\ \\alpha_2\\end{pmatrix} = \\begin{pmatrix}\\alpha_1 \\\\ (2\\epsilon-1)^n\\alpha_2\\end{pmatrix}\n$$","dabc4345":"# Lecture 3","6fbaabc9":"This defines the evolution operator $T$:\n$$\nT = \\begin{pmatrix}\n    \\epsilon       & 1-\\epsilon \\\\\n    1-\\epsilon       & \\epsilon\n\\end{pmatrix}\n$$\n\nCoarse-graining by 2 time-steps gives us\n\n$$\nT^2 = \\begin{pmatrix}\n    \\epsilon       & 1-\\epsilon \\\\\n    1-\\epsilon       & \\epsilon\n\\end{pmatrix}^2\n$$\n\n$$\n= \\begin{pmatrix}\n    \\epsilon       & 1-\\epsilon \\\\\n    1-\\epsilon       & \\epsilon\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\epsilon       & 1-\\epsilon \\\\\n    1-\\epsilon       & \\epsilon\n\\end{pmatrix}\n$$","f67a696a":"### compute the states when using the edge probabilites from a random start state","d62b03b0":"### Try iterating the coarse-graining to show approach to the fixed point","d85e976d":"We can now write a probability distribution as a weighted sum of the eigenvectors:\n$$\np = \\alpha_1 v_1 + \\alpha_2 v_2\n$$\n\nwhich when transformed into \"eigenvector space\" is\n\n$$\n\\begin{pmatrix}\\alpha_1 \\\\ \\alpha_2\\end{pmatrix}\n$$","b42c343b":"However, setting $\\epsilon = \\frac34$ gives us","abf3195a":"We use powers of the **stochastic matrix** to change the coarse-graining:\n$$\n\\begin{pmatrix}\n    p(A|A)       & p(A|B) \\\\\n    p(B|A)       & p(B|B) \n\\end{pmatrix}\n$$\n\nlike this:\n$$\n\\begin{pmatrix}\n    p(A|A)       & p(A|B) \\\\\n    p(B|A)       & p(B|B) \n\\end{pmatrix}\n\\begin{pmatrix}\n    p(A)     \\\\\n    p(B)       \n\\end{pmatrix}\n$$","ae3eeb35":"Giving us:\n\n$$\nT^2 = \\begin{pmatrix}\n    1-2\\epsilon + 2\\epsilon^2      & 2 \\epsilon-2 \\epsilon^2  \\\\\n    2 \\epsilon-2 \\epsilon^2       & 1-2\\epsilon + 2\\epsilon^2\n\\end{pmatrix}\n$$","2846723c":"Given a coarse-grained model:\n\n$$\nA^2 = T\n$$\n\nCan we find the original model (before coarse-graining)?\n\n$$\nA = \\begin{pmatrix}\n    a       & 1-b \\\\\n    1-a       & b\n\\end{pmatrix}\n$$\n\n$$\nA^2 = \\begin{pmatrix}\n    a       & 1-b \\\\\n    1-a       & b\n\\end{pmatrix}\n\\begin{pmatrix}\n    a       & 1-b \\\\\n    1-a       & b\n\\end{pmatrix}\n$$","b662d380":"## Lecture 4","a5e12bf9":"## Now, coarse-grain the Markov Matrix (2 time steps instead of 1)","0e53d96b":"# Lecture 2","004f743f":"Taking eigenvectors and eigenvalues of $T$:\n\n$$\nT v_1 = \\lambda_1 v_1\n$$\n\n$$\nT v_2 = \\lambda_1 v_2\n$$\n\nwhere $v_1, v_2$ are the eigenvectors and $\\lambda_1, \\lambda_2$ are the eigenvalues.","86f69c81":"$$\n = \\begin{pmatrix}\n    \\epsilon^2 + (1-\\epsilon)^2      & 2 \\epsilon (1-\\epsilon)  \\\\\n    2 \\epsilon (1-\\epsilon)       & (1-\\epsilon)^2 + \\epsilon^2\n\\end{pmatrix}\n$$","744336f3":"# Markov Process Renormalization\n\nGalen Wilkerson\n\n\n\n## Here we look at what happens when we coarse-grain a Markov matrix in time.\n\n\nBased on lectures by Simon DeDeo:\n\nhttps:\/\/www.complexityexplorer.org\/courses\/67-introduction-to-renormalization","891fb2f8":"For large $n$, the term $(2\\epsilon-1)^n$ will get very small, so that in the limit as $n \\to \\infty$, \n\n$\nT^n\\begin{pmatrix}\\alpha_1 \\\\ \\alpha_2\\end{pmatrix} \\to \\begin{pmatrix}\\alpha_1 \\\\ 0\\end{pmatrix}\n$"}}