{"cell_type":{"cc27bb65":"code","bd1b3c5a":"code","21ecd4a6":"code","c386d47a":"code","00cabe09":"code","59843a68":"code","5f023403":"code","ae995eb4":"code","0bb6201d":"code","c8c53b9b":"code","5026cd42":"code","80a574c0":"code","4772dc07":"code","84f1fe93":"code","d5d995d0":"code","c7165362":"code","358bb63d":"code","814067d4":"code","32b9e942":"code","37d2565b":"code","44195e6b":"code","aa347fdf":"code","4d9a2e6c":"code","bb1d090f":"code","6f082811":"code","454455be":"code","62ded243":"code","89e95ef1":"code","69aa3084":"code","1e672185":"code","9ccffd75":"code","067b0d5a":"code","20111e79":"code","d62d7831":"code","45b65d96":"code","55d719aa":"code","b66c3c3a":"code","ae4cb06e":"code","e8e2b692":"code","d4b5e353":"code","3991eb61":"code","a5127aa2":"code","cb32e6dd":"code","b6787c19":"code","d464c1b1":"code","995cf9db":"code","8045470e":"code","0964c079":"code","d63eeb7b":"code","bf55f5b8":"code","b54991fc":"code","0b59d833":"code","fae8e211":"code","254d1c78":"code","612def3c":"code","50b7e789":"code","f6712906":"code","db7f7c5d":"code","e5793412":"markdown","7d923966":"markdown","8d9c5ea7":"markdown","4554c78b":"markdown","a1d80a5d":"markdown","1440027c":"markdown","5743104a":"markdown","dde8a810":"markdown","83bcb026":"markdown","f2def4b3":"markdown","b04e28ff":"markdown","6c3d4445":"markdown","4fc5902e":"markdown","35644572":"markdown","2f0269f9":"markdown","67b62736":"markdown","8950e4f0":"markdown","f667bdf2":"markdown","923aedfe":"markdown","5a2c8c1a":"markdown","ec72d4d4":"markdown","19b49635":"markdown","4a8dd27c":"markdown","b44bce85":"markdown","187ebef8":"markdown","b70d4e27":"markdown","bf4fc201":"markdown","06b2890b":"markdown","7ba05a25":"markdown","0fe5c32b":"markdown"},"source":{"cc27bb65":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\nimport ast\n\n%matplotlib inline\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0443\u0434\u043e\u0431\u043d\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430:\nfrom sklearn.model_selection import train_test_split\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nfrom sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bd1b3c5a":"# \u0432\u0441\u0435\u0433\u0434\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0439\u0442\u0435 RANDOM_SEED \u0438 \u0434\u0430\u0442\u0443, \u0447\u0442\u043e\u0431\u044b \u0432\u0430\u0448\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b!\nRANDOM_SEED = 42\n\nCURRENT_DATE = '03\/15\/2020'","21ecd4a6":"# \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u0440\u0441\u0438\u044e \u043f\u0430\u043a\u0435\u0442\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b:\n!pip freeze > requirements.txt","c386d47a":"DATA_DIR = '\/kaggle\/input\/sf-dst-restaurant-rating\/'\ndf_train = pd.read_csv(DATA_DIR+'\/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'\/sample_submission.csv')","00cabe09":"df_train.info()","59843a68":"# \u0412\u0410\u0416\u041d\u041e! \u0434\u0440\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ndf_train['sample'] = 1 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ndf_test['sample'] = 0 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\ndf_test['Rating'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f Rating, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c","5f023403":"data.info()","ae995eb4":"data.sample(5)","0bb6201d":"data.Reviews[1]","c8c53b9b":"# \u0414\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u044f \u0432\u043e\u0437\u044c\u043c\u0443 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 Number of Reviews\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')","5026cd42":"# \u0414\u0430\u043b\u0435\u0435 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 0, \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0438\u043b\u0438 \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443 \u0438 \u0442\u0434...\ndata['Number of Reviews'].fillna(0, inplace=True)","80a574c0":"data.nunique(dropna=False)","4772dc07":"data['Price Range'].value_counts()","84f1fe93":"def price_range(cell):\n    if pd.isna(cell):\n        return 2\n    elif cell == '$':\n        return 1\n    elif cell == '$$ - $$$':\n        return 2\n    elif cell == '$$$$':\n        return 3\n\ndata['Price Range'] = data['Price Range'].apply(price_range)\n# data = pd.get_dummies(data, columns=['Price Range', ])","d5d995d0":"def make_list(list_string):\n    list_string = list_string.replace('nan]', \"'This is Nan']\")\n    list_string = list_string.replace('[nan', \"['This is Nan'\")\n    result_list = ast.literal_eval(list_string)\n    return result_list\n\ndata['Reviews'] = data['Reviews'].fillna(\"[[], []]\")\ndata['Reviews'] = data['Reviews'].apply(make_list)","c7165362":"def delta_date(row):\n    if len(row[1]) == 0 or len(row[1]) == 1:\n        return 0\n    \n    elif len(row[1]) == 2:\n        date1 = datetime.strptime(row[1][0],'%m\/%d\/%Y')\n        date2 = datetime.strptime(row[1][1],'%m\/%d\/%Y')\n        return abs(date1 - date2).days\n\ndata['Days between reviews'] = data['Reviews'].apply(delta_date)","358bb63d":"current_date_dt = datetime.strptime(CURRENT_DATE,'%m\/%d\/%Y')\n\ndef since_last_days(row):\n    if len(row[1]) == 0:\n        date = datetime.strptime('01\/01\/2000','%m\/%d\/%Y')\n    \n    elif len(row[1]) == 1:\n        date = datetime.strptime(row[1][0],'%m\/%d\/%Y')\n    \n    else:\n        date1 = datetime.strptime(row[1][0],'%m\/%d\/%Y')\n        date2 = datetime.strptime(row[1][1],'%m\/%d\/%Y')\n        date = max(date1, date2)\n    \n    return (current_date_dt - date).days\n\ndata['Days since last review'] = data['Reviews'].apply(since_last_days)","814067d4":"data['Reviews count'] = data['Reviews'].apply(lambda x: len(x[0]))","32b9e942":"# https:\/\/www.geeksforgeeks.org\/python-nlp-analysis-of-restaurant-reviews\/\n# import re  \n  \n# # Natural Language Tool Kit \n# import nltk  \n  \n# nltk.download('stopwords')\n  \n# # to remove stopword \n# from nltk.corpus import stopwords \n  \n# # for Stemming propose  \n# from nltk.stem.porter import PorterStemmer \n\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.ensemble import RandomForestClassifier \n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import confusion_matrix\n\n# yelp = pd.read_csv('\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/yelp_labelled.txt',\n#                    sep='\\t', header=None, usecols=[0,1])\n# yelp.columns = ['Reviews', 'Rating']","37d2565b":"# def clean_corpus(my_list):\n    \n#     corpus = []\n    \n#     for item in my_list:  \n      \n#         # column : \"Review\", row ith \n#         review = re.sub('[^a-zA-Z]', ' ', item)  \n\n#         # convert all cases to lower cases \n#         review = review.lower()  \n\n#         # split to array(default delimiter is \" \") \n#         review = review.split()  \n\n#         # creating PorterStemmer object to \n#         # take main stem of each word \n#         ps = PorterStemmer()  \n\n#         # loop for stemming each word \n#         # in string array at ith row     \n#         review = [ps.stem(word) for word in review \n#                     if not word in set(stopwords.words('english'))]  \n\n#         # rejoin all string array elements \n#         # to create back into a string \n#         review = ' '.join(review)   \n\n#         # append each string to create \n#         # array of clean text  \n#         corpus.append(review)\n        \n#     return corpus\n\n# corpus = clean_corpus(yelp['Reviews'].to_list())\n\n \n# # To extract max 1500 feature. \n# # \"max_features\" is attribute to \n# # experiment with to get better results \n# cv = CountVectorizer(max_features = 1500)  \n  \n# # X contains corpus (dependent variable) \n# X = cv.fit_transform(corpus).toarray()  \n  \n# # y contains answers if review \n# # is positive or negative \n# y = yelp.iloc[:, 1].values\n\n# # experiment with \"test_size\" \n# # to get better results \n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n  \n# # n_estimators can be said as number of \n# # trees, experiment with n_estimators\n# # to get better results  \n# model = RandomForestClassifier(n_estimators = 501, criterion = 'entropy', random_state = 42) \n                              \n# model.fit(X_train, y_train) ","44195e6b":"# y_pred = model.predict(X_test)\n  \n# cm = confusion_matrix(y_test, y_pred) \n\n# cm","aa347fdf":"# comment1 = []\n# comment2 = []\n\n# for review in data['Reviews']:\n#     if len(review[0]) == 2:\n#         comment1.append(review[0][0])\n#         comment2.append(review[0][1])\n#     elif len(review[0]) == 1:\n#         comment1.append(review[0][0])\n#         comment2.append('Good')\n#     else:\n#         comment1.append('Good')\n#         comment2.append('Good')","4d9a2e6c":"# corpus1 = clean_corpus(comment1)\n\n# cv = CountVectorizer(max_features = 1500)  \n  \n# # X contains corpus (dependent variable) \n# corpus_X1 = cv.fit_transform(corpus1).toarray()\n# y_pred1 = model.predict(corpus_X1)\n\n# corpus2 = clean_corpus(comment2)\n# cv = CountVectorizer(max_features = 1500)  \n  \n# # X contains corpus (dependent variable) \n# corpus_X2 = cv.fit_transform(corpus2).toarray()\n# y_pred2 = model.predict(corpus_X2)","bb1d090f":"# data['Comment1'] = y_pred1\n# # data['Comment2'] = y_pred2","6f082811":"# data['Comment average'] = (data['Comment1'] + data['Comment2']) \/ 2\n\n# data = data.drop(['Comment1', 'Comment2'], axis=1)","454455be":"from collections import Counter\n\ndef get_most_common(row, count):\n    new = []\n    c = Counter(row)\n    most = c.most_common(count)\n\n    for item in most:\n        new.append(item[0])\n    new_str = ','.join(new)\n    return new\n\ndata['Cuisine Style'] = data['Cuisine Style'].str[2:-2].str.split(\"', '\")\ncuisines_list = data['Cuisine Style']\ncity_and_cuisines = pd.concat([data['City'], cuisines_list], axis=1)\ncity_and_cuisines = city_and_cuisines.dropna()\ncities_grouped = city_and_cuisines.groupby('City').agg({'Cuisine Style': sum}).reset_index()\ncities_grouped.columns = ['City', 'Cuisine List']\ncities_grouped_mean = city_and_cuisines.groupby('City')['Cuisine Style'].apply(lambda x: round(np.mean(x.str.len()))).reset_index()\ncities_grouped_mean.columns = ['City2', 'Cuisine Count']\nnew = pd.concat([cities_grouped, cities_grouped_mean], axis=1, join='inner')\nnew = new.drop('City2', axis=1)\nnew['Common Cuisine'] = new.apply(lambda x: get_most_common(x['Cuisine List'], x['Cuisine Count']), axis=1)\nnew = new.set_index('City')\ncommon_cuisine_dict = new['Common Cuisine'].to_dict()\ndata['Cuisine Style'] = data['Cuisine Style'].fillna(data['City'].map(common_cuisine_dict))","62ded243":"df_new = data['Cuisine Style']\ndf_new_dummy = df_new.apply(lambda x: pd.Series([1] * len(x), index=x)).fillna(0, downcast='infer')\n\ndata = pd.merge(data, df_new_dummy, left_index=True, right_index=True, how='left')","89e95ef1":"data['Cuisine count'] = data['Cuisine Style'].str.len()","69aa3084":"# Add capital sity 0 or 1, 'Edinburgh' is not capital\n\ncapitals = ['Paris', 'Stockholm', 'London', 'Berlin',\n       'Bratislava', 'Vienna', 'Rome', 'Madrid',\n       'Dublin', 'Brussels', 'Warsaw', 'Budapest', 'Copenhagen',\n       'Amsterdam', 'Lisbon', 'Prague', 'Oslo',\n       'Helsinki', 'Ljubljana', 'Athens', 'Luxembourg',]\n\n\ndef is_capital(city):\n    if city in capitals:\n        return 1\n    else:\n        return 0\n\ndata['is_capital'] = data['City'].apply(is_capital)","1e672185":"cities = {\n    'London': 8567000, \n    'Paris': 9904000, \n    'Madrid': 5567000,\n    'Barcelona': 4920000,\n    'Berlin': 3406000,\n    'Milan': 2945000,\n    'Rome': 3339000,\n    'Prague': 1162000,\n    'Lisbon': 2812000,\n    'Vienna': 2400000,\n    'Amsterdam': 1031000,\n    'Brussels': 1743000,\n    'Hamburg': 1757000,\n    'Munich': 1275000,\n    'Lyon': 1423000,\n    'Stockholm': 1264000,\n    'Budapest': 1679000,\n    'Warsaw': 1707000,\n    'Dublin': 1059000,\n    'Copenhagen': 1085000,\n    'Athens': 3242000,\n    'Edinburgh': 504966,\n    'Zurich': 1108000,\n    'Oporto': 1337000,\n    'Geneva': 1240000,\n    'Krakow': 756000,\n    'Oslo': 835000,\n    'Helsinki': 1115000,\n    'Bratislava': 423737,\n    'Luxembourg': 107260,\n    'Ljubljana': 314807,\n}\n\ndata['Population'] = data['City'].map(cities)","9ccffd75":"rest_count = data.groupby('City')['Restaurant_id'].count().to_dict()\ndata['Total count of restaurants'] = data['City'].map(rest_count)\ndata['Relative ranking'] = data['Ranking'] \/ data['Total count of restaurants']\n\n# data.drop(['Ranking', 'rest_total_count'], axis = 1, inplace=True)","067b0d5a":"data['People per restaurant'] = data['Population'] \/ data['Total count of restaurants']","20111e79":"countries = {\n    'London': 'GB',\n    'Paris': 'FR',\n    'Madrid': 'ES',\n    'Barcelona': 'ES',\n    'Berlin': 'DE',\n    'Milan': 'IT',\n    'Rome': 'IT',\n    'Prague': 'CZ',\n    'Lisbon': 'PT',\n    'Vienna': 'AT',\n    'Amsterdam': 'NL',\n    'Brussels': 'BE',\n    'Hamburg': 'DE',\n    'Munich': 'DE',\n    'Lyon': 'FR',\n    'Stockholm': 'SE',\n    'Budapest': 'HU',\n    'Warsaw': 'PL',\n    'Dublin': 'IE',\n    'Copenhagen': 'DK',\n    'Athens': 'GR',\n    'Edinburgh': 'GB',\n    'Zurich': 'CH',\n    'Oporto': 'PT',\n    'Geneva': 'CH',\n    'Krakow': 'PL',\n    'Oslo': 'NO',\n    'Helsinki': 'FI',\n    'Bratislava': 'SK',\n    'Luxembourg': 'LU',\n    'Ljubljana': 'SI',\n}\n\ndata['Country'] = data['City'].map(countries)\n\ncountries_le = LabelEncoder()\ncountries_le.fit(data['Country'])\ndata['Country Code'] = countries_le.transform(data['Country'])","d62d7831":"data","45b65d96":"data['Reviews on people'] = data['Reviews count'] \/ data['People per restaurant']","55d719aa":"cities_le = LabelEncoder()\ncities_le.fit(data['City'])\ndata['City Code'] = cities_le.transform(data['City'])","b66c3c3a":"restaurant_chain = set()\nfor chain in data['Restaurant_id']:\n    restaurant_chain.update(chain)\n    \ndef find_item1(cell):\n    if item in cell:\n        return 1\n    return 0\n\nfor item in restaurant_chain:\n    data['Restaurant chain'] = data['Restaurant_id'].apply(find_item1)","ae4cb06e":"data['ID_TA code'] = data['ID_TA'].apply(lambda x: int(x[1:]))","e8e2b692":"df_train['Ranking'].describe()","d4b5e353":"object_columns = [s for s in data.columns if data[s].dtypes == 'object']\ndata.drop(object_columns, axis = 1, inplace=True)\n\ndf_preproc = data\ndf_preproc.sample(10)","3991eb61":"df_preproc.info(verbose=True)","a5127aa2":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['Rating'], axis=1)","cb32e6dd":"# \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0435 train_test_split \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0432\u043a\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n# \u0432\u044b\u0434\u0435\u043b\u0438\u043c 20% \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e (\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","b6787c19":"# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","d464c1b1":"# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438:\nfrom sklearn.ensemble import RandomForestRegressor # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nfrom sklearn import metrics # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438","995cf9db":"# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u043e\u0434\u0435\u043b\u044c (\u041d\u0410\u0421\u0422\u0420\u041e\u0419\u041a\u0418 \u041d\u0415 \u0422\u0420\u041e\u0413\u0410\u0415\u041c)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","8045470e":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nmodel.fit(X_train, y_train)\n\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e y_pred\n# y_pred = model.predict(X_test)","0964c079":"# \u041e\u043a\u0440\u0443\u0433\u043b\u044f\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\u044e 0.5\n\ndef rating_round(x, base=0.5):\n    return base * round(x\/base)\n\ndef predict(ds):\n    return np.array([rating_round(x) for x in model.predict(ds)])\n\ny_pred = predict(X_test)","d63eeb7b":"# \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (y_pred) \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438 (y_test), \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f\n# \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f Mean Absolute Error (MAE) \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","bf55f5b8":"# Best MAE: 0.165875","b54991fc":"# \u0432 RandomForestRegressor \u0435\u0441\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u0430\u043c\u044b\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","0b59d833":"test_data.sample(10)","fae8e211":"test_data = test_data.drop(['Rating'], axis=1)","254d1c78":"sample_submission","612def3c":"# predict_submission = model.predict(test_data)","50b7e789":"predict_submission = predict(test_data)","f6712906":"predict_submission","db7f7c5d":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","e5793412":"![](https:\/\/www.pata.org\/wp-content\/uploads\/2014\/09\/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n","7d923966":"### 2. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0443 \u043d\u0430\u0441 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u043c\u0438.","8d9c5ea7":"\u041a\u0430\u043a \u0432\u0438\u0434\u0438\u043c, \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043e\u0447\u0438\u0441\u0442\u043a\u0438 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438.","4554c78b":"## \u0417\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0443\u0445\u043e\u043d\u044c \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u043c\u0438 \u043a\u0443\u0445\u043d\u044f\u043c\u0438 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435 \u0432 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u0440\u0430\u0432\u043d\u044b\u043c \u043c\u0435\u0434\u0438\u0430\u043d\u043d\u043e\u043c\u0443 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0433\u043e\u0440\u043e\u0434\u0435","a1d80a5d":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043d\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043e\u0442\u0437\u044b\u0432\u0430\u043c\u0438\"","1440027c":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\"","5743104a":"## 1. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 NAN \n\u0423 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0447\u0438\u043d\u044b, \u043d\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043d\u0443\u0436\u043d\u043e \u043b\u0438\u0431\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c, \u043b\u0438\u0431\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0438\u0437 \u043d\u0430\u0431\u043e\u0440\u0430 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e. \u041d\u043e \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438 \u043d\u0443\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, **\u0434\u0430\u0436\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c!**   \n\u041f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u0435\u0440\u0435\u0434 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 NAN \u043b\u0443\u0447\u0448\u0435 \u0432\u044b\u043d\u0435\u0441\u0442\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430 \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a ","dde8a810":"# DATA","83bcb026":"\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c:\n* City: \u0413\u043e\u0440\u043e\u0434 \n* Cuisine Style: \u041a\u0443\u0445\u043d\u044f\n* Ranking: \u0420\u0430\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u044d\u0442\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435\n* Price Range: \u0426\u0435\u043d\u044b \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0432 3 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u0445\n* Number of Reviews: \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432\n* Reviews: 2 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u043e\u0442\u0437\u044b\u0432\u0430 \u0438 \u0434\u0430\u0442\u044b \u044d\u0442\u0438\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432\n* URL_TA: \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043d\u0430 'www.tripadvisor.com' \n* ID_TA: ID \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 TripAdvisor\n* Rating: \u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430","f2def4b3":"# Sentiment analysis \u0441\u0435\u0431\u044f \u043d\u0435 \u043e\u043f\u0440\u0430\u0432\u0434\u0430\u043b","b04e28ff":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u0435 \u0433\u043e\u0440\u043e\u0434\u0430\"","6c3d4445":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432\"","4fc5902e":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 id\"","35644572":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043a \u0441\u0435\u0442\u0438\"","2f0269f9":"#### \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c","67b62736":"# Submission\n\u0415\u0441\u043b\u0438 \u0432\u0441\u0435 \u0443\u0441\u0442\u0440\u0430\u0435\u0432\u0430\u0435\u0442 - \u0433\u043e\u0442\u043e\u0432\u0438\u043c Submission \u043d\u0430 \u043a\u0430\u0433\u043b","8950e4f0":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u0421\u0442\u0440\u0430\u043d\u0430\"","f667bdf2":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"Dummy variable \u0434\u043b\u044f \u043a\u0443\u0445\u043e\u043d\u044c\"","923aedfe":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043d\u0435\u0439 \u043f\u0440\u043e\u0448\u0435\u0434\u0448\u0438\u0445 \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043e\u0442\u0437\u044b\u0432\u0430\"","5a2c8c1a":"**\u041f\u0435\u0440\u0435\u0434 \u0442\u0435\u043c \u043a\u0430\u043a \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435, \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0442\u0435\u0441\u0442 \u0438 \u0442\u0440\u0435\u0439\u043d, \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438. \n\u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043d\u0430\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u043a\u0430\u043a \u0445\u043e\u0440\u043e\u0448\u043e \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442, \u0434\u043e \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 submissiona \u043d\u0430 kaggle.**","ec72d4d4":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435\"","19b49635":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u0421\u0442\u043e\u043b\u0438\u0447\u043d\u044b\u0439 \u0433\u043e\u0440\u043e\u0434\"","4a8dd27c":"# Data Preprocessing","b44bce85":"#### \u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \"Price Range\".","187ebef8":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u0413\u043e\u0440\u043e\u0434\"","b70d4e27":"# import","bf4fc201":"# Cleaning and Prepping Data","06b2890b":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0436\u0438\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 \u043e\u0434\u0438\u043d \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\"","7ba05a25":"# Model \n\u0421\u0430\u043c ML","0fe5c32b":"# \u041f\u0440\u0438\u0437\u043d\u0430\u043a \"\u0426\u0435\u043d\u043e\u0432\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\""}}