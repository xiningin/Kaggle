{"cell_type":{"610afed5":"code","6f13635d":"code","1f13c1e7":"code","98755dcc":"code","f8418a8f":"code","220b35d5":"code","12ba2c81":"code","adb16693":"code","bc8907fe":"code","0cc22f87":"code","3d3f600f":"code","2f91a7f6":"code","ddeafb56":"code","6cc5c5bb":"code","08072e8a":"code","b8cd0662":"code","d500d78c":"code","2ede76e0":"code","1696ace1":"code","682653eb":"code","aede4622":"code","72ce9084":"code","fb98d00f":"code","315ab71d":"code","b4d2e218":"code","2a2bd441":"code","bf5b2518":"code","14286de7":"code","8fa28d2e":"code","3ec89b66":"markdown","666f650a":"markdown","c11d93bf":"markdown","be5d2b10":"markdown","6cd46200":"markdown","18d45324":"markdown","0571c108":"markdown","f9717dc2":"markdown","e0fd1009":"markdown","985b3d8a":"markdown","9389363f":"markdown","f6c7e185":"markdown"},"source":{"610afed5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f13635d":"import re, string\nimport gensim","1f13c1e7":"#change file path to point to where you have stored the zip file.\ndf = pd.read_csv('\/kaggle\/input\/nlp-specialization-data\/unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3) \nprint('Number of examples in Dataset: ', df.shape)\ndf.head()","98755dcc":"def clean_str(string):\n  \"\"\"\n  String cleaning before vectorization\n  \"\"\"\n  try:    \n    \n    string = re.sub(r'^https?:\\\/\\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n    words = string.strip().lower().split()    \n    words = [w for w in words if len(w)>=1]\n    return \" \".join(words)\n\n  except:\n    return \"\"","f8418a8f":"df['clean_review'] = df['review'].apply(clean_str)\ndf.head()","220b35d5":"df.loc[0, 'clean_review']","12ba2c81":"#List to hold all words in each review\ndocuments = []\n\n#Iterate over each review\nfor doc in df['clean_review']:\n    documents.append(doc.split(' '))\n\nprint(len(documents))","adb16693":"print(documents[0])","bc8907fe":"len(documents[0])","0cc22f87":"# ?gensim.models.Word2Vec","3d3f600f":"#Build the model\nmodel = gensim.models.Word2Vec(documents,       #Word list\n                               min_count=10,    #Ignore all words with total frequency lower than this                           \n                               workers=4,       #Number of CPU Cores\n                               vector_size=50,  #Embedding size -  Dimensionality of the feature vectors. - (50, 300)\n                               window=5,        #Maximum Distance between current and predicted word\n                               epochs =10       #Number of iterations over the text corpus\n                              )  ","2f91a7f6":"#Model size\nlen(model.wv.key_to_index)","ddeafb56":"# Vocablury of the model\n(model.wv.index_to_key)[100:110]","6cc5c5bb":"model.wv.most_similar('happy')","08072e8a":"model.wv.most_similar('great')","b8cd0662":"model.wv.doesnt_match(\"man woman child king\".split())","d500d78c":"model.save('word2vec-movie-50')","2ede76e0":"#Load model from memory\nmodel = gensim.models.Word2Vec.load('word2vec-movie-50')","1696ace1":"model.wv.most_similar(positive=['king','man'], negative=['queen'])","682653eb":"model.wv['king'] + model.wv['man'] - model.wv['queen']","aede4622":"model.wv['man']","72ce9084":"model.wv['woman']","fb98d00f":"X = model.wv[model.wv.index_to_key]\nprint(X.shape)","315ab71d":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\nprint(result.shape)","b4d2e218":"import numpy as np\nindex=np.random.randint(0,28321,300)\nprint(index.shape)\nindex","2a2bd441":"result_sample= result[index]\nprint(result_sample.shape)","bf5b2518":"type(model.wv.index_to_key)","14286de7":"vocab_sample=[]\nfor ind in index.tolist():\n    vocab_sample.append(model.wv.index_to_key[ind])\nprint(len(vocab_sample))","8fa28d2e":"import matplotlib.pyplot as plt\n%matplotlib inline\nfif = plt.figure(figsize = (12,7))\nplt.scatter(result_sample[:,0], result_sample[:,1])\nwords = vocab_sample\n\nfor i,w in enumerate(words):\n    plt.annotate(w, xy = (result_sample[i,0], result_sample[i,1]) ,)\n    \nplt.show()","3ec89b66":"### Load Gensim Library","666f650a":"### Find the word which is not like others","c11d93bf":"### Build the Model","be5d2b10":"# Exploring the model","6cd46200":"### Get an embedding for a word","18d45324":"### Saving the model","0571c108":"1. Equation king + man = queen + ?\n2. In this case there may not be enough data for this equation","f9717dc2":"### Clean the Data using routine above","e0fd1009":"### Convert Review to a Word List","985b3d8a":"### Finding Words which have similar meaning","9389363f":"### How many words in the model","f6c7e185":"### Function to Clean up data"}}