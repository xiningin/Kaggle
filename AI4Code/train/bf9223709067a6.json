{"cell_type":{"97745e16":"code","a8f08b19":"code","d07a862d":"code","12901a81":"code","b300029b":"code","aa2765bb":"code","22534881":"code","c03bcf82":"code","2ff7c3fd":"code","97dfd0e9":"code","32751239":"code","dd185584":"code","4330c605":"code","08218339":"code","8ca900c2":"code","58266e56":"code","8986cd7a":"code","0de4dfe0":"code","3be436a0":"code","de86bae8":"code","7b303ef8":"code","e7a9a8b3":"code","5758d34c":"code","b50c1497":"code","52f15f93":"code","b26f433d":"code","67a1016a":"code","2ffb09c0":"code","0262043a":"code","2f315440":"code","deccdf23":"code","e73e5b6f":"code","07859c70":"code","d6b83d68":"code","1dfa4d2e":"code","4e154dfc":"code","dbdb9b29":"markdown","a7cead35":"markdown","7659bb72":"markdown","bcceccac":"markdown","e8757aad":"markdown","2afc79f1":"markdown","54223e4e":"markdown","fc9f3c06":"markdown","b1335f00":"markdown","1a8f24a6":"markdown","87057dd5":"markdown","7c37f2c8":"markdown","b9516c74":"markdown","35cdf42c":"markdown","a251a2c6":"markdown","11b51219":"markdown","94bd8849":"markdown","add84af5":"markdown","0bdf3904":"markdown","537d387f":"markdown","82906cac":"markdown","acbd5cee":"markdown","77c965ab":"markdown","d957f837":"markdown","0557e3ec":"markdown","c0e9f9b6":"markdown","ca5a0246":"markdown","48b06d29":"markdown"},"source":{"97745e16":"# Data analysis libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\n\n# Data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Ignore possible warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a8f08b19":"# Load both the train and test data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# We combine them in a list to facilitate the data cleaning\ncombined = [train, test]","d07a862d":"# Shape of both train and test DataFrames\nprint('train_shape: {0} || test_shape: {1}'.format(train.shape, test.shape))\n\n# Random look at our DataFrame\ntrain.sample(5)","12901a81":"# Which are our variables\/columns\nprint('Length of train variables:', len(train.columns), '||', 'Lenght of test variables:', len(test.columns))\nprint('Survived' in test)\ntrain.columns","b300029b":"# Description of our numeric columns (default)\ntrain.describe()","aa2765bb":"# Description of out categorical columns\ntrain.describe(include='object')","22534881":"#train.profile_report()","c03bcf82":"for x in combined:\n    x.info()\n    print('\\n', '=' *40)","2ff7c3fd":"print('Qualitative data: ', train.columns[train.dtypes == 'object'].tolist())\nprint('Quantitative data: ', train.columns[(train.dtypes =='int64') | (train.dtypes == 'float64')].tolist())","97dfd0e9":"# Columns with any null value\nprint('Missing Age values: ', train['Age'].isna().sum())\ntrain.columns[train.isna().any()].tolist()","32751239":"# Number of null values by column\nfor x in combined:\n    print(x.isnull().sum())\n    print('\\n', '=' *40)","dd185584":"fig = plt.figure(figsize=(12,12))\n\n# Distributing axes along the figure\ngs = fig.add_gridspec(3, 2)\nax1 = fig.add_subplot(gs[0,0])\nax2 = fig.add_subplot(gs[0:2,1])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[2,:])\n\n# First ax (Sex Feature)\nsns.barplot(x='Sex', y='Survived', data=train, ax=ax1, palette='pastel')\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100, '\\n')\n\n# Pclass\nsns.barplot(x='Pclass', y='Survived', data=train, ax=ax3, palette='pastel')\n\n# SibSp\nsns.barplot(x='SibSp', y='Survived', data=train, ax=ax2, palette='pastel')\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n\n# Age\nsns.barplot(x=pd.cut(train[\"Age\"].fillna(-0.5), bins, labels=labels), y='Survived', data=train, palette='pastel')","4330c605":"# The idea behind recorded Cabins, is that they can be related with higher socio-economic status and thus to a higher survival chance\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nsns.barplot(x=train[\"Cabin\"].notna().astype('int'), y='Survived', data=train, ax=ax1, palette='pastel')","08218339":"train[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False)","8ca900c2":"train[['Sex', 'Survived']].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False)","58266e56":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False)","8986cd7a":"train[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False)","0de4dfe0":"f, ax = plt.subplots(figsize=(14,12))\nsns.heatmap(train.corr(), annot=True)","3be436a0":"# The alphanumeric feature 'Ticket' does not provide any insight for determining if a passenger survived or not so we can drop it. \n\n[ds.drop(labels='Ticket', inplace=True, axis=1) for ds in combined]\n\ntest.head(3)","de86bae8":"#Embarked has two null values in the train set (we fill them with the mode):\n[ds['Embarked'].fillna(ds['Embarked'].mode()[0], inplace=True) for ds in combined]\n\n# One hot encoding these variables:\ntrain, test = [pd.get_dummies(ds, columns=['Sex', 'Embarked']) for ds in combined]\ncombined = [train, test] # Dummies changes have been done in train and test, we must update combined\n\ntest.head()","7b303ef8":"# A lot of null values in cabin, we could consider drop these column. But we will keep it because \n# be registered in a cabin can be translated in some sort of socioeconomic status.\n\ntrain['Cabin'], test['Cabin'] = [[1 if x != 0 else 0 for x in ds['Cabin'].fillna(0)] for ds in combined]\ncombined = [train, test] # Same as before, we update combined\n\ntest.sample(15)","e7a9a8b3":"[ds.fillna(ds['Fare'].median(), inplace=True) for ds in combined]\n\ntrain['FareBand'], test['FareBand'] = [pd.qcut(ds['Fare'], 5, labels=[1,2,3,4,5]) for ds in combined]\ncombined = [train, test]\n\n[ds.drop(['Fare'], axis=1, inplace=True) for ds in combined]\n\nprint(train[['FareBand', 'Survived']].groupby(['FareBand']).mean().sort_values(by='Survived', ascending=False))\ntrain.sample(5)","5758d34c":"[ds['Age'].fillna(ds['Age'].astype('int').median(), inplace=True) for ds in combined]\n\ntrain['AgeBand'], test['AgeBand'] = [pd.cut(ds['Age'], bins=[0, 10, 18, 25, 35, 45, 55, 65, 99], labels=[1,2,3,4,5,6,7,8]) for ds in combined]\ncombined = [train, test]\n\n[ds.drop(['Age'], axis=1, inplace=True) for ds in combined]\n    \nprint(train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True))\ntrain.sample(15)","b50c1497":"train['Title'], test['Title'] = [ds['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False) for ds in combined]\ncombined = [train, test]\n\ntrain.head(3)","52f15f93":"[ds['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n                     'Rare', inplace=True) for ds in combined]\n[ds['Title'].replace('Mlle', 'Miss', inplace=True) for ds in combined]\n[ds['Title'].replace('Ms', 'Miss', inplace=True) for ds in combined]\n[ds['Title'].replace('Mme', 'Mrs', inplace=True) for ds in combined]\n    \ntrain[['Title', 'Survived']].groupby(['Title']).mean()","b26f433d":"# Again we could just label Title as follows:\n#title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} \n#train['Title'], test['Title'] = [ds['Title'].map(title_mapping) for ds in combined]\n#[ds.drop(['Name'], axis=1, inplace=True) for ds in combined]\n\n# But because of the same reasons exposed above it is better to one hot encode it:\ntrain, test = [pd.get_dummies(ds, columns=['Title']) for ds in combined]\ncombined = [train, test]\n[ds.drop(['Name'], axis=1, inplace=True) for ds in combined]\n\ntrain.sample(5)","67a1016a":"train['FamilySize'], test['FamilySize'] = [ds['SibSp'] + ds['Parch'] + 1 for ds in combined]\n[ds.drop(['SibSp', 'Parch'], axis=1, inplace=True) for ds in combined]\n    \ntrain.sample(5)\ntest.sample(5)","2ffb09c0":"from sklearn.model_selection import train_test_split\n\nX = train.drop(['Survived', 'PassengerId'], axis=1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)","0262043a":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel='rbf', C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\nlog_cols = ['Classifier', 'Accuracy', 'Log Loss']\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print('='*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print('Accuracy: {:.4%}'.format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print('Log Loss {}:'.format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \n    print('='*30)","2f315440":"f, ax = plt.subplots(1, 2, figsize=(15,6))\n\nsns.barplot(x='Classifier', y='Accuracy', data=log, ax=ax[0], palette='pastel')\nax[0].set_title('Classifier Accuracy')\nax[0].tick_params(axis='x', labelrotation=90)\n\nsns.barplot(x='Classifier', y='Log Loss', data=log, ax=ax[1], palette='pastel')\nax[1].set_title('Classifier Log Loss')\nax[1].tick_params(axis='x', labelrotation=90)\n\nplt.show()","deccdf23":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nRFC = RandomForestClassifier()\n\nparameters = {\n 'max_depth': [50, 100, 130, 160],\n 'max_features': ['auto'],\n 'min_samples_leaf': [2, 3],\n 'min_samples_split': [8, 12, 16],\n 'n_estimators': [10, 20, 30, 40]\n             }\n\nRFC_random = RandomizedSearchCV(estimator=RFC, param_distributions=parameters, n_iter=100, \n                               cv=5, verbose=2, random_state=42, n_jobs = -1)\n\nRFC_random.fit(X_train, y_train)\n%time RFC_random.best_params_","e73e5b6f":"#You should be testing which parameters are better.\n\nparameters = {\n 'max_depth': [150, 160, 170],\n 'max_features': ['auto'],\n 'min_samples_leaf': [2],\n 'min_samples_split': [10, 12, 14, 16],\n 'n_estimators': [21, 22, 23, 24]\n             }\n\ngrid_search = GridSearchCV(estimator = RFC, param_grid = parameters, \n                          cv = 5, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(X_train, y_train)\n%time grid_search.best_params_","07859c70":"# Best cross validation score\nprint('Cross Validation Score:', grid_search.best_score_, '\\n')\n\n# Best parameters which resulted in the best score\nprint('Best Parameters:', grid_search.best_params_)\n\nbest_grid = grid_search.best_estimator_\nbest_grid","d6b83d68":"from sklearn.model_selection import cross_val_score\n\nvalidation = cross_val_score(best_grid, X, y, cv=5)\nprint(validation, '\\n')\nprint('Cross validation mean:', np.mean(validation))","1dfa4d2e":"ids = test['PassengerId']\npredictions = best_grid.predict(test.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({'PassengerId': ids, 'Survived': predictions})\noutput.head()","4e154dfc":"#set the output as a dataframe and convert to csv file named submission.csv\noutput.to_csv('TitanicSubmission3.csv', index=False)","dbdb9b29":"<a id=\"ch5\"><\/a>\n## 5. Feature Engineering\n#### Title","a7cead35":"### 6.2 Which Classifier should I choose","7659bb72":"### 4.2 Converting categorical features to numerical\n#### Sex and Embarked","bcceccac":"#### Age","e8757aad":"<a id=\"ch1\"><\/a>\n## 1. Importing the necessary libraries\n\nAt a later stage we will further import the scikit-learn related libraries","2afc79f1":"### 3.1 Data types in the dataset","54223e4e":"### 6.3 Fitting and Tuning the Algorithm\n\nI decided to go with the Random Forest Classifier for simplicity and explainability.\n\nThe best way to think about hyperparameters (parameters inside the algorithm) is like the settings that can be adjusted to optimize performance. This unfortunately is a bit trial-and-error process. Tools like RandomizedSearchCV and GridSearchCV will facilitate this.\n\nPlease note that from now on you can find some code that is cpu demanding (lasting up to 30 secs to run depending on your machine).","fc9f3c06":"### 3.2 Null values","b1335f00":"### 6.1 Splitting up the training data","1a8f24a6":"# YET ANOTHER TITANIC NOTEBOOK","87057dd5":"### 4.3 Creating bins for Fare and Age\n#### Fare","7c37f2c8":"## What is this notebook all about:\n\nThis notebook, like many others, provides a simple step by step framework to start your first data science projects successfully. \nThe main purpose is to help newcomers by having their way a little paved, and at the same time, receive recommendations and guidance myself regarding in my work since I already have tons of things to learn.\nSome points before starting:\n- Even though, as stated, this notebook is aimed for beginners, I cannot be stopping at every detail or nuance that may arise. Believe me, if I tell you that documentation is your best friend in these cases.\n- There is not much visualization. Certainly, there are some very nice notebooks out there which you will learn much more than here about data viz.\n- The notebook invites you to tune it your way and experiment with all those things that you want. You will be learning much more this way.","b9516c74":"#### Cabin","35cdf42c":"<a id=\"ch9\"><\/a>\n## 9. Submit the file","a251a2c6":"<a id=\"ch2\"><\/a>\n## 2. Load train and test csv datasets provided by Kaggle","11b51219":"#### SibSp and Parch","94bd8849":"## Index\n[__1. Importing the necessary libraries__](#ch1)\n\n[__2. Load train and test csv datasets provided by Kaggle__](#ch2)\n\n[__3. First exploratory data analysis__](#ch3)\n    <br>3.1 Data types in the dataset\n    <br>3.2 Null values\n    <br>3.3 Some basic visualizations\n    <br>3.4 Some correlations\n    \n[__4. Cleaning data__](#ch4)\n    <br>4.1 Droping unnecessary columns\n    <br>4.2 Converting categorical features to numerical\n    <br>4.3 Creating bins for Fare and Age\n    \n[__5. Feature Engineering__](#ch5)\n\n[__6. Modelling__](#ch6)\n    <br>6.1 Splitting up the training data\n    <br>6.2 Which Classifier should i choose\n    <br>6.3 Fitting and Tuning the Algorithm \n    \n[__7. Validate the model__](#ch7)\n\n[__8. Predict the Actual Test Data__](#ch8)\n\n[__9. Submit the file__](#ch9)","add84af5":"<a id=\"ch7\"><\/a>\n## 7. Validate the model","0bdf3904":"<a id=\"ch4\"><\/a>\n## 4. Cleaning Data\n\nNow that we have a basic idea about our data, we can start cleaning it.","537d387f":"Another approach you could use is the `pandas_profiling`. \n\nI commented out its command because it would be a little overkill and a significant increase in the running time. \nEven though I recommend you to check it out.","82906cac":"<a id=\"ch8\"><\/a>\n## 8. Predict the Actual Test Data","acbd5cee":"### 4.1 Droping unnecessary columns","77c965ab":"### 3.3 Some basic visualizatons","d957f837":"![Image of Titanic](https:\/\/images.reference.com\/reference-production-images\/question\/aq\/1400px-788px\/deep-did-titanic-sink_c55eea735ea20796.jpg)","0557e3ec":"<a id=\"ch6\"><\/a>\n## 6. Modelling","c0e9f9b6":"<a id=\"ch3\"><\/a>\n## 3. First exploratory data analysis\n\nGet to know a little bit the data","ca5a0246":"### 3.4 Some relations with the target","48b06d29":"Some of the ideas showed are drawn from other extraordinary Kaggle notebooks:\n- https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner \n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish"}}