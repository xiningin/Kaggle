{"cell_type":{"7e8172f5":"code","84a37c23":"code","b9765f47":"code","3bfb0618":"code","cb921820":"code","660d25ca":"code","7cb42492":"code","8486e6de":"code","c52bd20e":"code","44330cbd":"code","93113df7":"code","a5750731":"code","496d3519":"code","0fba00ff":"code","aa1601e1":"code","a389765e":"code","d78cdd63":"code","b65cfcd8":"code","12209f44":"code","b06c9c9e":"code","54355197":"code","9de424d9":"code","b56d26aa":"code","cf830118":"code","4519c11a":"code","4f7186b4":"code","4ebb00b6":"code","afcaaf66":"code","6969d596":"code","eca74ea0":"code","46f37dda":"code","8a4fda02":"code","d96cad9a":"code","66d48599":"code","ccbf492e":"code","6aaf9657":"code","f981f445":"code","52711e17":"code","7f47f5a9":"code","1cf2aef8":"code","2c6d8e1a":"code","e449a4e4":"code","cadf4ff8":"markdown","4e711fb8":"markdown","fe14fcae":"markdown","2cb0ce2f":"markdown","5d3ec5d1":"markdown","fad6f5ea":"markdown","b157572e":"markdown","91ca3c55":"markdown","21803cb5":"markdown","054cbb2a":"markdown","480b4d2c":"markdown","f1f49426":"markdown","f5a8a072":"markdown","565e96f8":"markdown","543ccb1e":"markdown","39e4862a":"markdown","4c86630b":"markdown","9c0b45ec":"markdown","7bd48e8d":"markdown","ea8f990b":"markdown"},"source":{"7e8172f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84a37c23":"# Importing neccessary packages\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import recall_score, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.feature_selection import SelectKBest\nfrom collections import Counter\n\n\n# ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.ticker as mtick # for showing percentage in it","b9765f47":"data = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.head()","3bfb0618":"# Features types\ndata.dtypes","cb921820":"# Total charges are in object dtype so convert into Numerical feature \ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')","660d25ca":"# numerical feature\nnumerical_feature = {feature for feature in data.columns if data[feature].dtypes != 'O'}\nprint(f'Count of Numerical feature: {len(numerical_feature)}')\nprint(f'Numerical feature are:\\n {numerical_feature}')","7cb42492":"# Categorical feature\ncategorical_feature = {feature for feature in data.columns if data[feature].dtypes == 'O'}\nprint(f'Count of Categorical feature: {len(categorical_feature)}')\nprint(f'Categorical feature are:\\n {categorical_feature}')","8486e6de":"# plotting with target feature\nsns.countplot(data=data, x='Churn')\nplt.title('Count of Churn')\nplt.show()","c52bd20e":"l1 = data.loc[data['Churn']== 'Yes'].count()[0]\nprint(f\"Pecentage of Left: {l1\/len(data['Churn'])}\")\nprint(data.Churn.value_counts())","44330cbd":"### How many amount loss from customer churn\nloss = []\nfor values in data.loc[data['Churn'] == 'Yes', 'TotalCharges']:\n    value = float(values)\n    loss.append(value)\nprint(np.round(sum(loss)))","93113df7":"### Plotting numerical feature with probability distribution and checking outlier\nfor feature in numerical_feature:\n    if feature != 'SeniorCitizen':\n        plt.figure(figsize=(15,7))\n    \n        plt.subplot(1, 3, 1)\n        sns.histplot(data=data, x=feature, bins=30, kde=True)\n        plt.title('Histogram')\n    \n        plt.subplot(1, 3, 2)\n        stats.probplot(data[feature], dist=\"norm\", plot=plt)\n        plt.ylabel('RM quantiles')\n    \n        plt.subplot(1, 3, 3)\n        sns.boxplot(x=data[feature])\n        plt.title('Boxplot')\n    \nplt.show()","a5750731":"sns.pairplot(data.drop(columns='SeniorCitizen'),hue='Churn', kind='scatter')\nplt.show()","496d3519":"ax = (data['SeniorCitizen'].value_counts()*100.0 \/len(data)).plot.pie(autopct='%.1f%%', labels = ['No', 'Yes'],figsize =(5,5), fontsize = 12 )                                                                           \nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_ylabel('Senior Citizens',fontsize = 12)\nax.set_title('% of Senior Citizens', fontsize = 12)","0fba00ff":"for i, feature in enumerate(categorical_feature):\n    if feature != 'TotalCharges':\n        if feature != 'customerID':\n            plt.figure(i)\n            plt.figure(figsize=(12,6))\n            sns.countplot(data=data, x=feature, hue='Churn')\nplt.show()","aa1601e1":"data.head()","a389765e":"data.isnull().sum()","d78cdd63":"# replace NaN values with mean value\ndata.TotalCharges = data.TotalCharges.fillna(data.TotalCharges.mean())","b65cfcd8":"data.TotalCharges.hist()","12209f44":"print(categorical_feature)","b06c9c9e":"encoder = LabelEncoder()\nfor feature in categorical_feature:\n    data[feature] = encoder.fit_transform(data[feature])","54355197":"data.head()","9de424d9":"data.drop(columns=['customerID'], inplace=True)","b56d26aa":"#Get Correlation of \"Churn\" with other variables:\nplt.figure(figsize=(15,8))\ndata.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')","cf830118":"# Finding the correlation between the independent and dependent feature\nplt.figure(figsize=(20, 9))\nsns.heatmap(data.corr(), annot=True)","4519c11a":"# using minmaxscaler methods to scale down the value of features between 0 to 1\nscaler = MinMaxScaler()\nfor feature in numerical_feature:\n    data[[feature]] = scaler.fit_transform(data[[feature]])","4f7186b4":"# splitting dataset into dependent and independent feature\nX = data.drop(columns='Churn')\ny = data['Churn']","4ebb00b6":"# selects the feature which has more correlation\nselection = SelectKBest()\nX = selection.fit_transform(X, y)","afcaaf66":"# splitting for train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","6969d596":"# its an imbalance dataset\ny.value_counts()","eca74ea0":"Log_reg = LogisticRegression(C=150, max_iter=150)\nLog_reg.fit(X_train, y_train)\nlog_pred = Log_reg.predict(X_test)\n\nprint(f'Accuracy score : {accuracy_score(log_pred, y_test)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(log_pred, y_test)}')\nprint(f'Classification report :\\n {classification_report(log_pred, y_test)}')","46f37dda":"# Random forest classifier\nRfc = RandomForestClassifier(n_estimators=120,criterion='gini', max_depth=15, min_samples_leaf=10, min_samples_split=5)\nRfc.fit(X_train, y_train)\nrfc_pred = Rfc.predict(X_test)\n\nprint(f'Accuracy score : {accuracy_score(rfc_pred, y_test)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(rfc_pred, y_test)}')\nprint(f'Classification report :\\n {classification_report(rfc_pred, y_test)}')","8a4fda02":"# decisionTree Classifier\nDtc = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=15)\nDtc.fit(X_train, y_train)\ndtc_pred = Dtc.predict(X_test)\n\nprint(f'Accuracy score : {accuracy_score(dtc_pred, y_test)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(dtc_pred, y_test)}')\nprint(f'Classification report :\\n {classification_report(dtc_pred, y_test)}')","d96cad9a":"# finding the best K value for finding the nearest neighbours\nBK_value = []\nfor x in range(1, 100):\n    Knn = KNeighborsClassifier(n_neighbors=x, p=2)\n    Knn.fit(X_train, y_train)\n    knn_pred = Knn.predict(X_test)\n    BK_value.append(np.mean(knn_pred != y_test))","66d48599":"# plot the Kvalue\nplt.figure(figsize=(15,7))\nplt.xlabel('K value')\nplt.ylabel('count')\nplt.plot(range(1,100), BK_value, marker='*')\nplt.xticks(range(1,100)[::5])\nplt.show()","ccbf492e":"Knn = KNeighborsClassifier(n_neighbors=64, p=2)\nKnn.fit(X_train, y_train)\nknn_pred = Knn.predict(X_test)","6aaf9657":"print(f'Accuracy score : {accuracy_score(knn_pred, y_test)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(knn_pred, y_test)}')\nprint(f'Classification report :\\n {classification_report(knn_pred, y_test)}')","f981f445":"st=SMOTEENN()\nX_train_st,y_train_st = st.fit_resample(X_train, y_train)\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_st)))","52711e17":"# splitting the over sampling dataset \nX_train_sap, X_test_sap, y_train_sap, y_test_sap = train_test_split(X_train_st, y_train_st, test_size=0.2)","7f47f5a9":"# decisionTree Classifier\nDtc_sampling = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=7, min_samples_leaf=15)\nDtc_sampling.fit(X_train_sap, y_train_sap)\ndtc_sampling_pred = Dtc_sampling.predict(X_test_sap)\n\nprint(f'Accuracy score : {accuracy_score(dtc_sampling_pred, y_test_sap)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(dtc_sampling_pred, y_test_sap)}')\nprint(f'Classification report :\\n {classification_report(dtc_sampling_pred, y_test_sap)}')","1cf2aef8":"# Random forest classifier\nRfc_sampling = RandomForestClassifier(n_estimators=150,criterion='gini', max_depth=15, min_samples_leaf=10, min_samples_split=6)\nRfc_sampling.fit(X_train_sap, y_train_sap)\nrfc_sampling_pred = Rfc_sampling.predict(X_test_sap)\n\nprint(f'Accuracy score : {accuracy_score(rfc_sampling_pred, y_test_sap)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(rfc_sampling_pred, y_test_sap)}')\nprint(f'Classification report :\\n {classification_report(rfc_sampling_pred, y_test_sap)}')","2c6d8e1a":"# logistic regression\nLog_reg_sampling = LogisticRegression(C=10, max_iter=150)\nLog_reg_sampling.fit(X_train_sap, y_train_sap)\nLog_sampling_pred = Log_reg_sampling.predict(X_test_sap)\n\nprint(f'Accuracy score : {accuracy_score(Log_sampling_pred, y_test_sap)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(Log_sampling_pred, y_test_sap)}')\nprint(f'Classification report :\\n {classification_report(Log_sampling_pred, y_test_sap)}')","e449a4e4":"# KNN classifier algorithm\nKnn_sampling = KNeighborsClassifier(n_neighbors=3, p=2)  # using Euclidean_distance\nKnn_sampling.fit(X_train_sap, y_train_sap)\nknn_sampling_pred = Knn_sampling.predict(X_test_sap)\n\nprint(f'Accuracy score : {accuracy_score(knn_sampling_pred, y_test_sap)}')\nprint(f'Confusion matrix :\\n {confusion_matrix(knn_sampling_pred, y_test_sap)}')\nprint(f'Classification report :\\n {classification_report(knn_sampling_pred, y_test_sap)}')","cadf4ff8":"#### Separate into Numerical and Categorical feature","4e711fb8":"##### **From sklearn using feature selection modules importing the SelectKBest to select the important feature**","fe14fcae":"#### **Using SMOTEENN for imbalance dataset:**\n     Over-sampling using SMOTE and cleaning using ENN. Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours","2cb0ce2f":"#### Perform Feature Scaling","5d3ec5d1":"#### **Apply into machine learning algorithm:**","fad6f5ea":"##### **In this dataset there is no null values, so we dont want to perform Handling missing values. Only perform the Feature Encoding techiniques to convert the categorical feature into numerical feature**","b157572e":"**As we compare to the imbalance dataset our model are perform like okay not a better model to build for end to end project. So we need to over smpling data for reducing the TN, FN and increase the FP and TP for model building**","91ca3c55":"##### **Only 16.2% customers who are senior citizons but remaining 83.8% customers are young people**","21803cb5":"## Telecom Customer Churn:\n     Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n\n#### Content:\n     Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\n#### The data set includes information about:\n\n   * Customers who left within the last month \u2013 the column is called Churn\n   * Services that each customer has signed up for \u2013 phone, multiple lines, internet, online        * security, online backup, device protection, tech support, and streaming TV and movies\n   * Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, \n   * paperless billing, monthly charges, and total charges\n   * Demographic info about customers \u2013 gender, age range, and if they have partners and dependents\n   * Churn - dependent feature ('Yes' denotes customers left, 'No' denotes customer stay here)","054cbb2a":"**1869 of customer are left about 26.5 percentage from overall, this like an imbalance dataset**","480b4d2c":"**After Oversampling the dataset our model performs is pretty good. From our 4 model KNN classifier performs better than all.**","f1f49426":"#### Feature Selection:","f5a8a072":"**We have lost arround $2862927 due to customer churn**","565e96f8":"##### **After ploting histogram probability distribution and box plot to find numerical value are in normally distribution and our dataset has no outlier dataset.So, we don't want to remove the outlier in our dataset**","543ccb1e":"#### **Univariate Analysis:**","39e4862a":"**According to this ploting graph we know that Best k value for KNN Classifier is about 64 using Euclidean_distance to reduce the maximum of TN and FN value**","4c86630b":"#### **From analysis the dataset, we notice that some of the independent features are in numerical and most of feature are in categorical feature. Seperate into numerical and categorical data for EDA parts**","9c0b45ec":"#### EDA","7bd48e8d":"#### Splitting the dataset into train and test","ea8f990b":"#### Data Cleaning"}}