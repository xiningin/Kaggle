{"cell_type":{"6a28094c":"code","c59c5b61":"code","32630319":"code","c2e80d24":"code","b7dec2d4":"code","f2cb672a":"code","68197d55":"code","7f5ed9d9":"code","96a816d0":"code","de509729":"code","d8c145e9":"code","e70aa977":"markdown","ad18a55f":"markdown","a0bcbdd5":"markdown","f491a8f7":"markdown","9d9f98ee":"markdown"},"source":{"6a28094c":"import os\nimport re \nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical, Sequence, plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\nfrom keras.layers import Embedding, Dense, Dropout, LSTM, Input, BatchNormalization, concatenate\n\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(40)","c59c5b61":"data = pd.read_csv('..\/input\/facebook-antivaccine-post-data-scaled-features\/features_scaled.csv', \n                  index_col=0)\ntext = pd.read_csv('..\/input\/facebook-antivaccination-dataset\/posts_full.csv', \n                   index_col=0).text\nassert text.shape[0] == data.shape[0]\ndata['text'] = text\ndel text\ndata.head()","32630319":"#Remove unwanted punctuation\nunwanted = {x for x in '\"$%&()*+,.!?-\/:;<=>[\\\\]^_`{|}~\\t\\n'}\n\ndef filter_unwanted(x):\n    x = \"\".join([c if c not in unwanted else \" \" for c in x]).lower()\n    return x.encode(\"utf8\").decode(\"ascii\",'ignore')\ndata['text'] = [sentence for sentence in data.text.fillna('').apply(filter_unwanted)]\ndata.text.head()","c2e80d24":"#Add n-gram input sequences\nNUM_WORDS = 50_000\nMAX_SEQUENCE_LENGTH = 200\n\ntokenizer = Tokenizer(num_words=NUM_WORDS, filters='!\"$%&()*+,-.\/:;<=>?[\\\\]^_`{|}~\\t\\n', \n                      lower=True)\ntokenizer.fit_on_texts(data.text)\n\nX1 = tokenizer.texts_to_sequences(data.text)\nX1 = pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH)\nX1[-40:], data.text[0]","b7dec2d4":"X2 = data.drop(['anti_vax', 'text'], axis=1).values","f2cb672a":"y = data.anti_vax.values","68197d55":"X1_train, X1_eval, X2_train, X2_eval, y_train, y_eval = train_test_split(X1, X2, y, test_size=0.15, \n                                                                         random_state=3000)","7f5ed9d9":"input1 = Input(shape=(X1.shape[1],), name=\"Text\")\ninput2 = Input(shape=(X2.shape[1],), name=\"Text_Features\")\n\n#RNN of Text data\ntext_branch = Embedding(NUM_WORDS, 10)(input1)\ntext_branch = LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(text_branch)\ntext_branch = Dropout(rate=0.2)(text_branch)\ntext_branch = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(text_branch)\naux_output = Dense(1, activation='sigmoid', name='Aux_Output')(text_branch)\n\n#Text Features\ntext_feat = BatchNormalization()(input2)\n\n#Join branches\nx = concatenate([text_branch, text_feat])\nmain_branch = Dense(80, activation='relu')(x)\nmain_output = Dense(1, activation='sigmoid', name=\"Main_Output\")(main_branch)\n\n#Model\nmodel = Model(inputs=[input1, input2], outputs=[main_output, aux_output])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], \n              loss_weights={'Main_Output': 1.0, 'Aux_Output':0.2})\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)","96a816d0":"checkpoint = ModelCheckpoint(\"model-{epoch:02d}-{val_Main_Output_loss:.2f}.hdf5\", \n                             monitor='val_Main_Output_loss', verbose=0, \n                             save_best_only=True, period=1)\nstopping = EarlyStopping(monitor='val_Main_Output_loss', patience=5)\nhistory = model.fit({'Text': X1_train, 'Text_Features': X2_train}, [y_train, y_train], epochs=20, \n                    verbose=2, batch_size=32, validation_data=([X1_eval, X2_eval], [y_eval, y_eval]), \n                    callbacks=[checkpoint, stopping])","de509729":"def plot_epochs(results, col, **kwargs):\n    def plot_epoch_helper(hist_df, col, ax):\n        ax.plot(hist_df[col], **kwargs)\n        ax.set_title(col + ' per epoch')\n        ax.set_ylabel(col)\n        ax.set_xlabel('epoch')\n        for sp in ax.spines:\n            ax.spines[sp].set_visible(False)\n        ax.yaxis.grid(True, alpha=0.3)\n        ax.legend(labels=[n[0] for n in results])\n        ax.set_ylim(0, 1)\n    fig, ax = plt.subplots(figsize=(21, 10))\n    for name, hist in results:\n        plot_epoch_helper(hist, col, ax)\nplot_epochs([('Model', pd.DataFrame(history.history))], 'val_Main_Output_loss')","d8c145e9":"plot_epochs([('Model', pd.DataFrame(history.history))], 'val_Aux_Output_loss')","e70aa977":"## Build Model","ad18a55f":"## Comparing Loss per Epoch","a0bcbdd5":"## Train","f491a8f7":"<h1 style=\"text-align:center\">Final Model<\/h1>\n<img src=\"model.png\" width=\"700\">","9d9f98ee":"## Prepare Data"}}