{"cell_type":{"3c8bd3ab":"code","06fc5a2c":"code","4fb79a52":"code","865cef81":"code","1f5d45c9":"code","f20b8e98":"code","f0eb56e7":"code","a9ef20cb":"code","ee84aca5":"code","4c79f671":"markdown","e7a555ee":"markdown","212c8a77":"markdown","e9a18d4b":"markdown","3069c89e":"markdown"},"source":{"3c8bd3ab":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error\n\nfrom IPython.display import clear_output\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","06fc5a2c":"!pip install autoviz\n!pip install xlrd\n\nfrom autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\n\nclear_output()","4fb79a52":"sample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","865cef81":"ff = AV.AutoViz('..\/input\/30-days-of-ml\/train.csv')","1f5d45c9":"!nvidia-smi","f20b8e98":"from xgboost import XGBRegressor\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 65.1,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\n","f0eb56e7":"df = pd.read_csv(\"\/kaggle\/input\/train-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]","a9ef20cb":"final_predictions = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    print(f\"fold: {fold}, rmse: {mean_squared_error(yvalid, preds_valid, squared=False)}\")","ee84aca5":"preds = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.target = preds\nsample_submission.to_csv(\"xgb_submission_k\", index=False)","4c79f671":"# MODELING","e7a555ee":"# KFOLDS\n\nThis dataset is the same as abishek thakur created!","212c8a77":"I recently learnt that it is against Kaggle\u2019s rules to publish public notebook forks with non-meaningful edits, since it is a form of plagiarism. Since I have no idea if tuning hyperparameters to increase model score is called \"non-meaningful edits\", I'll make it clear that this is a forked version. All credit (except those for the tuned hyperparameters) goes to the author of the original version.","e9a18d4b":"# AUTO EDA","3069c89e":"# PREDICTIONS"}}