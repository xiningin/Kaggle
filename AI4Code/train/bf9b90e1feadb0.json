{"cell_type":{"c7c34865":"code","0ca813dd":"code","834e50ea":"code","a5a4460a":"code","b9f9b08f":"code","6323947c":"code","1726480f":"code","8bad2faf":"code","95d5b3eb":"code","b7fa63df":"code","53e1d8a1":"code","8790657c":"code","0267f3db":"code","404ba080":"code","286200c5":"code","8a63aa32":"code","0b47aab1":"code","d3c8dbf8":"code","b497e7ab":"code","935f16ff":"code","7df26d1f":"code","38dad127":"code","caab43ad":"code","bb399489":"markdown","23774d52":"markdown","db67ea25":"markdown","e684ecd1":"markdown","8536c68d":"markdown","126e798b":"markdown","bb01a70d":"markdown","779f6aef":"markdown","df32e83b":"markdown","4ee474d1":"markdown","df872013":"markdown"},"source":{"c7c34865":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nrandom.seed(2020)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRFClassifier","0ca813dd":"# Read in the datasets\ntrain_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain_raw.shape, test_raw.shape","834e50ea":"# Look at the training data\ntrain_raw.head()","a5a4460a":"# Check variable types\ntrain_raw.info()","b9f9b08f":"# Check missing values in the raw training set\ntrain_raw.isna().mean()","6323947c":"# The distribution of Age\ntrain_raw.Age.hist()","1726480f":"# Check the mean value and std value of the Age distribution\ntrain_raw.Age.mean(), train_raw.Age.std()","8bad2faf":"class Transformer:\n    def fit(self, X, y = None):\n        df = X.copy()\n        self.age_mean = int(df.Age.mean()) + 1\n        self.age_std = int(df.Age.std())\n        self.embark_mod = df.Embarked.mode()\n        self.ticket_mod = df.Ticket.map(lambda x: x.split(' ')[-1]).mode()[0]\n        self.fare_mean = df.Fare.mean()\n    \n    def transform(self, X, y = None):\n        df = X.copy()\n        df.Age = df.Age.map(lambda x: int(random.gauss(self.age_mean, self.age_std)) if type(x) == float else x)\n        df.Ticket = df.Ticket.map(lambda x: x.split(' ')[-1])\n        df.Ticket[df.Ticket == 'LINE'] = self.ticket_mod\n        df.Ticket = df.Ticket.map(lambda x: int(x))\n        df.Fare[df.Fare.isnull()] = self.fare_mean\n        df.Embarked[df.Embarked.isnull()] = self.embark_mod\n        df.drop(['PassengerId', 'Name', 'Cabin'], axis = 1, inplace = True)\n        df = pd.get_dummies(df, drop_first=True)\n   \n        return df\n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)","95d5b3eb":"# How's the dataframe look like after feature engineering\nTransformer().fit_transform(train_raw).head()","b7fa63df":"# Transform the training set and test set\ntransformer = Transformer()\ntrain = transformer.fit_transform(train_raw)\ntest = transformer.transform(test_raw)\n\ntrain.shape, test.shape","53e1d8a1":"# Extract response variable from training data\nxtrain = train.drop('Survived', axis = 1)\nytrain = train.Survived.values","8790657c":"# Split the training data into training set and validation set using a 80 vs. 20 split\nX_train, X_valid, y_train, y_valid = train_test_split(xtrain, ytrain, stratify = ytrain, \n                                                      test_size = 0.2, random_state = 2020)","0267f3db":"X_train.shape","404ba080":"# Build pipelines to wrap data rescale and model together\nstep = [('scale', MinMaxScaler()), \n       ('lr', LogisticRegressionCV(random_state=2020))]\nmod = Pipeline(step)\nmod.fit(X_train, y_train)","286200c5":"# Make predictions\npred_train = mod.predict(X_train)\npred_valid = mod.predict(X_valid)\n\nprob_train = mod.predict_proba(X_train)[:,1]\nprob_valid = mod.predict_proba(X_valid)[:,1]","8a63aa32":"print('Train Accuracy: {:.3f}'.format(accuracy_score(y_train, pred_train)))\nprint('Train AUC: {:.3f}'.format(roc_auc_score(y_train, prob_train)), '\\n')\n\nprint('Valid Accuracy: {:.3f}'.format(accuracy_score(y_valid, pred_valid)))\nprint('Valid AUC: {:.3f}'.format(roc_auc_score(y_valid, prob_valid)))","0b47aab1":"rf = RandomForestClassifier(random_state=2020)\nrf.fit(X_train, y_train)\n\npred_train = rf.predict(X_train)\npred_valid = rf.predict(X_valid)\n\nprob_train = rf.predict_proba(X_train)[:,1]\nprob_valid = rf.predict_proba(X_valid)[:,1]","d3c8dbf8":"print('Train Accuracy: {:.3f}'.format(accuracy_score(y_train, pred_train)))\nprint('Train AUC: {:.3f}'.format(roc_auc_score(y_train, prob_train)), '\\n')\n\nprint('Valid Accuracy: {:.3f}'.format(accuracy_score(y_valid, pred_valid)))\nprint('Valid AUC: {:.3f}'.format(roc_auc_score(y_valid, prob_valid)))","b497e7ab":"xgb = XGBRFClassifier(random_state=2020)\nxgb.fit(X_train, y_train)\n\npred_train = xgb.predict(X_train)\npred_valid = xgb.predict(X_valid)\n\nprob_train = xgb.predict_proba(X_train)[:,1]\nprob_valid = xgb.predict_proba(X_valid)[:,1]","935f16ff":"print('Train Accuracy: {:.3f}'.format(accuracy_score(y_train, pred_train)))\nprint('Train AUC: {:.3f}'.format(roc_auc_score(y_train, prob_train)), '\\n')\n\nprint('Valid Accuracy: {:.3f}'.format(accuracy_score(y_valid, pred_valid)))\nprint('Valid AUC: {:.3f}'.format(roc_auc_score(y_valid, prob_valid)))","7df26d1f":"pred_test = xgb.predict(test)\npred_test","38dad127":"test_result = pd.concat([test_raw['PassengerId'], pd.DataFrame(pred_test, columns=['Survived'])], axis = 1)\ntest_result.shape","caab43ad":"test_result.to_csv('titanic_prediction.csv', header=True, index=False)","bb399489":"### Without model tuning, use the predictions from the XGBoost model.","23774d52":"* There're 3 variabels having missing values. \n* There's over 77% of data missing in `Cabin`. I will simply drop it in my first try. If I have more time, I will do more research on `Cabin` to find out if I can impute those large number of missing values in `Cabin`.\n* There's around 20% of data missing in `Age`. I will look into its distribution and impute those missing values.\n* Only 2 data points are missing in `Embarked`. I will impute them with mode value for `Embarked`.","db67ea25":"### Model 2: Random Forest","e684ecd1":"### Feature engineering","8536c68d":"The `Age` is approximatelly normally distributed.","126e798b":"### Model 3: XGBoost","bb01a70d":"### Some simple EDA","779f6aef":"Less overfitting than the random forest.","df32e83b":"The accuracy is better than that from logistic regression. However, there's definitely overfitting problem exist here.","4ee474d1":"https:\/\/www.kaggle.com\/c\/titanic\/notebooks","df872013":"### Model 1: Logistic Regression"}}