{"cell_type":{"d315d6f2":"code","20b1b3ea":"code","62b66319":"code","ae8dc3d9":"code","37664821":"code","49a6629d":"code","3ffd36ff":"code","f55ed63c":"code","340fff1d":"code","30162343":"code","11b1da7f":"code","b6389bfb":"code","7f21177b":"code","f190c0b5":"code","1b31aa5f":"code","03804516":"code","00728d60":"code","557708da":"code","a2c2f22a":"code","80918fe5":"code","e6fe160c":"code","e7da3018":"code","b9fca6d9":"code","45e9c551":"code","eba7bc90":"code","2ba4bd35":"code","ca854886":"code","650c2c29":"code","b720ded8":"code","71bf54d1":"code","6e1a0afd":"code","2dbc7d66":"code","2e30e859":"code","aa2d8205":"code","fcd5dd48":"code","137d86ca":"code","d9e74588":"code","53ab4ee3":"code","49d457c1":"code","b2e8b971":"code","d3ffe298":"code","85f9b8ce":"code","ca40a66d":"code","77a08108":"code","e57c6da3":"code","5650e304":"code","9e039df8":"code","dd9dcf58":"code","1092ef29":"code","86659b3a":"code","1e5f7bc9":"code","831f6e1d":"code","dcdeff25":"code","9f70a6a4":"code","678dfba9":"code","562352de":"code","f2e9f76c":"code","97fd542e":"code","e23a01eb":"code","c466cd9b":"code","4d9aaa16":"code","7b3afe0c":"code","27958e29":"code","21d7797d":"code","25257670":"markdown","f22d64fe":"markdown","70811a49":"markdown","0597227a":"markdown","024d4f7d":"markdown","fbe741ca":"markdown","3065086e":"markdown","57e6de22":"markdown","9df28c7b":"markdown","d4a8b908":"markdown","6a0d2b93":"markdown","bbb759e5":"markdown","3678b0d4":"markdown","2effad3f":"markdown","d017e162":"markdown","746356d5":"markdown","7dbedb32":"markdown","6f016315":"markdown","f8a83faa":"markdown","d282bd4b":"markdown","09cecf56":"markdown","9afc0379":"markdown","43ae4a77":"markdown","5534046e":"markdown","dcd65872":"markdown","5a8dc0ef":"markdown","f8c782ea":"markdown","e66b67b6":"markdown","c966dc87":"markdown","69c215c4":"markdown","91265255":"markdown","666ea006":"markdown","676a68f5":"markdown","bcbae713":"markdown","e30cd6c8":"markdown","9e94f850":"markdown","ed38e126":"markdown","8547b67f":"markdown","ff8bf3ac":"markdown","49ed7a11":"markdown","9f40d1bd":"markdown","76715376":"markdown","d6564791":"markdown","3d71e5b6":"markdown","6a89d676":"markdown","9548d7d7":"markdown","940e8433":"markdown","bce5d36d":"markdown","943f254b":"markdown","4d35ddad":"markdown"},"source":{"d315d6f2":"import sys\nimport copy\n\n# Just because I have a conflicting Python 3.6 installation at root\n# Comment this when uploading to kaggle\n# try:\n#     sys.path.remove('C:\/Python36\/Lib\/site-packages')\n#     sys.path.remove('C:\/Python36\/Lib')\n# except:\n#     print(\"py36 not influencing\")\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier","20b1b3ea":"%matplotlib inline\nsns.set()\n\n# Loading data\n# adultTrain = pd.read_csv(\n#     \"C:\/Users\/bruno\/Desktop\/kaggle-adult-comp-knn\/data\/train_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# # For uploading to kaggle\nadultTrain = pd.read_csv(\n    \"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\n# adultTest = pd.read_csv(\n#     \"C:\/Users\/bruno\/Desktop\/kaggle-adult-comp-knn\/data\/test_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# # For uploading to kaggle\nadultTest = pd.read_csv(\n    \"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\nmodifyNames = {\n    \"fnlwgt\": \"weight\",\n    \"education.num\": \"educationNum\", \n    \"marital.status\": \"maritalStatus\",\n    \"capital.gain\": \"capitalGain\", \n    \"capital.loss\": \"capitalLoss\",\n    \"hours.per.week\": \"hoursPerWeek\", \n    \"native.country\": \"country\",\n    \"income\": \"target\"\n}\n\n# Changing columns names\nadultTrain.rename(columns=modifyNames, inplace=True)\nadultTest.rename(columns=modifyNames, inplace=True)\n\n# Casting appropriate datatypes\ndtypes = {\n    \"age\": int,\n    \"workclass\": str,\n    \"weight\": int,             \n    \"education\": str,\n    \"educationNum\": int,\n    \"maritalStatus\": str,\n    \"occupation\": str,\n    \"relationship\": str,\n    \"race\": str,\n    \"sex\": str,\n    \"capitalGain\": int,\n    \"capitalLoss\": int,\n    \"hoursPerWeek\": int,\n    \"country\": str,\n    \"target\": str\n}\n\nadultTrain.astype(dtypes, copy=False)\nadultTest.astype(dtypes.pop(\"target\"), copy=False)\n\n# Id is not relevant, so it is dropped\nadultTrain.pop(\"Id\")\nidTest = adultTest.pop(\"Id\")\n\n# weight is not important for testing\nweightTrain = adultTrain[\"weight\"]\nadultTest.pop(\"weight\")\n\nprint(\"\\n\\n#### TRAIN DATASET ####\")\n# (32560, 16)\nprint('\\nshape: ', adultTrain.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTrain.dtypes)\n# max of 4000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTrain.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTrain.duplicated().sum()) \n\nprint(\"\\n\\n#### TEST DATASET ####\")\n# (16280, 15)\nprint('\\nshape: ', adultTest.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTest.dtypes)\n# max of aprox 2000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTest.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTest.duplicated().sum()) ","62b66319":"# education can be dropped, since educationNum is givving all the information we want\n# there is notinh specific about a certain degree that will affect the target\nadultTrain.head(20)","ae8dc3d9":"adultTest.head(10)","37664821":"adultTrain.describe()","49a6629d":"# aprox 25 000 datapoints <= 50K and 7 500 < 50K -> relatively imbalanced dataset\n# most simple baseline is prediciting always <= 50K -> gives 0.76% accuracy\ncounts = adultTrain[\"target\"].value_counts().values\nimbalanceRatio = counts[0]\/counts[1]\nprint(imbalanceRatio)\nadultTrain[\"target\"].value_counts().plot(kind=\"bar\")\n","3ffd36ff":"# capitalGain and capitalLoss have very few examples\n# ideas\n    # 1. exclude these festures\n    # 2. cluster them in two bins -> will become boolean variables\nprint(adultTrain.astype(bool).sum(axis=0))","f55ed63c":"# hoursPerWeek could be dividid in three bins:  <30, 30-50, >50\n# educationNUm could be dividid in four bins: <8, 8-10, 10-12, >13\n# capitalGains and capitalLoss needs to actuallt only form one feature \n# that is capitalLiquid = capitalGains - capitalLoss. \n# The effect of this feature will be almost as of a imbalanced binary variable since almost all values are zero\n# and the other are in a small range\nadultTrain.hist(bins=30, figsize=(15, 10))","340fff1d":"# Private is way bigger than the rest (therefore the rest of the classes have little data)\n# Without pay and never work have very few examples (14) but these examples guarantee we know the target\n# Ideas:\n    # 1. Cluster into 3 bins: private, {without pay + ever worked},  and rest -> \n    # but need to see if private and rest have distinct relatinships with target\nprint('\"Without-pay\" or \"Never-worked\" datapoints: ', adultTrain[adultTrain[\"workclass\"] == (\"Without-pay\" or \"Never-worked\")].shape[0])\nadultTrain[\"workclass\"].value_counts().plot(kind=\"bar\")","30162343":"# This feature will be excluded, educationNum already gives us the info we need. There is nothing specific to a \n# certain category that would be relevant for predicting the target\nadultTrain[\"education\"].value_counts().plot(kind=\"bar\")","11b1da7f":"# A priori a would think only having a present spouse or not is important\n# So this could be cluster into two groups: present spouse and not present spouse\nadultTrain[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","b6389bfb":"# Each of the categories seem to be very important\nadultTrain[\"occupation\"].value_counts().plot(kind=\"bar\")","7f21177b":"# This feature seem a little weird, it doest provide mmuch new info, \n# and the categories dont seem to be mutually exclusive\n# Idea: exclude this feature\nadultTrain[\"relationship\"].value_counts().plot(kind=\"bar\")","f190c0b5":"# this could be divided into two bins: white and black \n# because the rest doesnt have data and my guess they would be very similar to white\nadultTrain[\"race\"].value_counts().plot(kind=\"bar\")","1b31aa5f":"# a priori seems to be important\nadultTrain[\"sex\"].value_counts().plot(kind=\"bar\")","03804516":"# Surely maintaning all these low data categories will fit statistical noise and ruin the accuracy\n# Ideas: \n    # 1. divide in two bins: developed and not developed ccontries\n    # 2. divide in two bins: USA and rest\nadultTrain[\"country\"].value_counts().plot(kind=\"bar\")","00728d60":"# no surprises here\nadultTest.hist(bins=30, figsize=(15, 10))","557708da":"# no surprises here\nadultTest[\"workclass\"].value_counts().plot(kind=\"bar\")","a2c2f22a":"# no surprises here\nadultTest[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","80918fe5":"# no surprises here\nadultTest[\"occupation\"].value_counts().plot(kind=\"bar\")","e6fe160c":"# no surprises here\nadultTest[\"race\"].value_counts().plot(kind=\"bar\")","e7da3018":"# no surprises here\nadultTest[\"country\"].value_counts().plot(kind=\"bar\")","b9fca6d9":"## OBS: the plots below dont consider the dataset imbalaca, therefore, all ratios are essentially multiplied\n# by a factor of 2.3 in favour of <50K.","45e9c551":"# for <30 it is almost certain that wage <50K; 30-40 roughly the same; 40-50 >50K has good advantage\n# <50K decays linearly with age, while 50K is like a normal function centered in 43\nsns.catplot(x=\"target\", y=\"age\", kind=\"violin\", inner=None, data=adultTrain)","eba7bc90":"# for <10 <50K has a good advantage; 10-12.5 same; >12.5 >50K has very good advantage\nsns.catplot(x=\"target\", y=\"educationNum\", kind=\"violin\", inner=None, data=adultTrain)","2ba4bd35":"# for <40 <50K has a good advantage; for >40 >50K has a good advantage\nsns.catplot(x=\"target\", y=\"hoursPerWeek\", kind=\"violin\", inner=None, data=adultTrain)","ca854886":"sns.catplot(x=\"target\", y=\"capitalGain\", kind=\"violin\", inner=None, data=adultTrain)","650c2c29":"sns.catplot(x=\"target\", y=\"capitalLoss\", kind=\"violin\", inner=None, data=adultTrain)","b720ded8":"# OBS: I am multiplying the counts of >50K by the imbalaceRatio to decouple \n# the fact that the dataset is imbalaced from differences in distribution of the feature","71bf54d1":"# Private & Self-empinc differ a little, the rest is roughly the same, so can be grouoed \n# into a single category called other\ncountsDf = adultTrain[[\"target\",\"workclass\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","6e1a0afd":"# all ctegories are different, thus maintaining all of them seems the way to go\ncountsDf = adultTrain[[\"target\",\"maritalStatus\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","2dbc7d66":"# tranposrt moving, tech support, sales, creaf repair dont seem to help distringuish, so could be grouped\n# into a single category named rest\ncountsDf = adultTrain[[\"target\",\"occupation\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","2e30e859":"# black and other dimishes for over >50K but white dominates in both\n# I think grouping into white and non-white is a valid approach here\ncountsDf = adultTrain[[\"target\",\"race\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","aa2d8205":"# i think this can be mexico and non-mexico because the rest of the categories have so little data\n# that it is likely that we are fittng statistical noise\ncountsDf = adultTrain[[\"target\",\"country\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(15, 10))","fcd5dd48":"# age limmits >50K even with high education and hours per week\n# age < 35 seems to be good indicator -> could maybe be binary variable\n\n# capitalGain > aprox 5 000 seems to be a great separator \n# capitalGain > 50 000 guarantees >50K \n# could be categorical variable\n\n# educationNum > 10 seems to be good indicator also\n \n# 1 000 < capital loss < 3 000 can be good \n\n# hours per week < 50 good\nsns.pairplot(adultTrain, hue=\"target\")","137d86ca":"# all numerical features with very low correlation\nsns.heatmap(adultTrain.corr())","d9e74588":"adultTrainDummies = pd.get_dummies(adultTrain[[\"workclass\", \"maritalStatus\", \"occupation\", \"race\", \"country\"]])\ndummy_features = adultTrainDummies.columns.values\npivots = []\nfor feature in dummy_features:\n    rest_of_features = dummy_features[dummy_features != feature]\n    new_pivot = adultTrainDummies.groupby(feature)[rest_of_features].sum().fillna(0)\n    pivots.append(new_pivot)\n\nfullPivot = pd.concat(pivots)[dummy_features]\nfullPivotOnes = fullPivot.iloc[lambda x: x.index > 0]\nfullPivotOnes.set_index(adultTrainDummies.columns, inplace=True)\n\ndef normalize_pivot_tables(fullPivot):\n    vec = np.array(fullPivot.sum(axis=1).values)\n    sizeDummies = vec.size\n    normMatrix = np.zeros((sizeDummies, sizeDummies))\n    for i, element in enumerate(vec):\n        for j, element2 in enumerate(vec):\n            normMatrix[i][j] = element + element2\n                        \n    normDf = pd.DataFrame(normMatrix, columns=fullPivot.columns)\n    normDf.set_index(fullPivot.columns, inplace=True)\n    fullPivotNorm = fullPivot.div(normDf)\n    return fullPivotNorm\n\nfullPivotNorm = normalize_pivot_tables(fullPivotOnes) # P(X1 = 1, X2 = 1)\n\n# dataset is too big, so will divide in two for plotting heatmaps\n#fullPivot2 = fullPivot.iloc[37:, :37] # down left -> not useful\n#fullPivot4 = fullPivot.iloc[37:, 37:] # down right -> country vs country -> not useful\nfullPivotNorm1 = fullPivotNorm.iloc[:37, :37] # top left\nfullPivotNorm3 = fullPivotNorm.iloc[:37:, 37:] # top right\n\n#OBS: dummy features with same prefix are mutually exclsusive, \n# therefore they will have joint prob equal to zero \n\n# max joint probability is aprox 0.1 in entire categorical combinations dataset, \n# therefore all categorical features are relatively independent from each other\n","53ab4ee3":"fullPivotNorm.describe()","49d457c1":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm1,ax=ax)","b2e8b971":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm3, ax=ax)","d3ffe298":"numColumns = [\"age\", \"capitalGain\", \"capitalLoss\", \"educationNum\", \"hoursPerWeek\"] # obs: left weight out\ncatColumns = [\"country\", \"education\", \"maritalStatus\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"] # obs: left target out\ntargetTrain = adultTrain[\"target\"]\nadultTrainNum = adultTrain[numColumns]\nadultTrainCat = adultTrain[catColumns]","85f9b8ce":"adultTrainNum.head()","ca40a66d":"adultTrainCat.head()","77a08108":"adultTrainNum = (adultTrainNum-adultTrainNum.mean())\/adultTrainNum.std()\nadultTrainNum.head()","e57c6da3":"# Target-encoding \n# encoder = TargetEncoder()\n# encoder.fit_transform(adultTrainCat, adultTrain[\"target\"])\n# Simple one-hot encoding (this will be chosen one for now)\nadultTrainCat = pd.get_dummies(adultTrainCat)\nadultTrainCat.head()","5650e304":"adultTrain = pd.concat([adultTrainNum, adultTrainCat], axis=1)\nadultTrain.head()","9e039df8":"# Two main options\n# 1. Just thorw away rows with missing values\n# 2. Replace with mean of colummn (this will be chosen one for now)\nadultTrain.fillna(adultTrain.mean(), inplace=True)","dd9dcf58":"# todo later","1092ef29":"# todo later","86659b3a":"adultTestNum = adultTest[numColumns]\nadultTestCat = adultTest[catColumns]\n\nadultTestNum = (adultTestNum-adultTestNum.mean())\/adultTestNum.std() # broadcasts to columns by default\n\nadultTestCat = pd.get_dummies(adultTestCat)\nadultTestCat = adultTestCat.reindex(columns = adultTrainCat.columns, fill_value=0) # equivalent to fit transform\n\nadultTest = pd.concat([adultTestNum, adultTestCat], axis=1)\n\nadultTest.fillna(adultTest.mean(), inplace=True)\n\nadultTest.head()","1e5f7bc9":"# Primising Engineered Datasets (v0)\n# Disct which will hold different feature engineered candidate datsets\npromisingDatasets = {}","831f6e1d":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # with the given weights, the rows can be resampled according to their weight\n# # number of rows = weight_factor*minMaxNormalized_weight where the weight factor is large enough so that the sampling\n# # can give different integer values for most of the rows, \n# # but if it is too large becomes computationally heavy\n# weightTrainNorm = ((weightTrain) - weightTrain.min())\/(weightTrain.max() - weightTrain.min())\n# weightFactor = 50 # (can be altered later)\n# knnNeighboursFactor = weightTrainNorm.mean()*50 # expected value of number of columns added\n# print('knnNeighboursFactor:', knnNeighboursFactor)\n\n# # adultTrain70Importance will contain replicas only of row in it, will used to train KNN, \n# # that will be tested in adultTrain30ImportanceCV, which wasnt replicated\n# adultTrainShuffled = adultTrain.sample(frac=1)\n# adultTrain70Importance, adultTrain30Importance = \\\n#     np.split(adultTrainShuffled, [int(.7*len(adultTrain))])\n# adultTrain30ImportanceCopy = adultTrain30Importance.copy()\n# # putting back target I removed earlier\n# adultTrain70Importance[\"target\"] = targetTrain[:len(adultTrain70Importance)]\n\n# for idx, row in adultTrain70Importance.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain70Importance = adultTrain70Importance.append([df]*numReplicatedRows, ignore_index=True)\n    \n# for idx, row in adultTrain30ImportanceCopy.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain30ImportanceCopy = adultTrain30ImportanceCopy.append([df]*numReplicatedRows, ignore_index=True)\n\n# #### IMPORTANT: this is 100% of the actual training dataset -> only used if survived CV\n# adultTrainImportance = pd.concat([adultTrain70Importance, adultTrain30ImportanceCopy])\n# promisingDatasets[\"importanceSampling\"] = adultTrainImportance","dcdeff25":"# todo later","9f70a6a4":"# todo later","678dfba9":"# Label encoder\nle = preprocessing.LabelEncoder()\n# Test data\nXtest = adultTest.values\n\nSEED = 10","562352de":"#### Baseline dataset\nXtrain = adultTrain.values\nYtrain = le.fit_transform(targetTrain)\n\n# shape check\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(Ytrain.shape)","f2e9f76c":"baselineKnn = KNeighborsClassifier()\nbaselineKnnAcc = cross_val_score(baselineKnn, Xtrain, Ytrain, cv=5, scoring='accuracy')\nbaselineKnnAccMean = baselineKnnAcc.mean()\nprint('mean accuracy for baseline knn: ', baselineKnnAccMean)\n\nbaselineKnn.fit(Xtrain, Ytrain)\n\ncurrentBestModel = {\n    'model': baselineKnn,\n    'modelFamily': 'KNN',\n    'cv': baselineKnnAccMean, \n    'X': Xtrain,\n    'Y': Ytrain\n}","97fd542e":"rf = RandomForestClassifier(random_state=SEED)\n\nrfConfig = {\n    'n_estimators': np.arange(10, 50),\n    'criterion': ['gini', 'entropy'],\n    'max_depth': np.arange(5, 50)\n}\n\nrfRandomSearch = (\n    RandomizedSearchCV(\n        rf, \n        rfConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nrfRandomSearch.fit(Xtrain, Ytrain)\n\nrfMean = rfRandomSearch.best_score_\nprint('mean accuracy for rf: ', rfMean)\n\nrfTuned = rfRandomSearch.best_estimator_\nprint('tuned RF: ', rfTuned)\n\nif rfMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': rfTuned,\n        'modelFamily': 'RF',\n        'cv': rfMean\n    }","e23a01eb":"xgb = XGBClassifier(random_state=SEED, use_label_encoder=False)\n\nxgbConfig = {\n    'n_estimators': np.arange(10, 50),\n    'learning_rate': np.arange(1e-3, 1),\n    'max_depth': np.arange(5, 50),\n    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n    'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n}\n\nxgbRandomSearch = (\n    RandomizedSearchCV(\n        xgb, \n        xgbConfig, \n        verbose=False, \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nxgbRandomSearch.fit(Xtrain, Ytrain)\n\nxgbMean = xgbRandomSearch.best_score_\nprint('mean accuracy for xgb: ', xgbMean)\n\nxgbTuned = xgbRandomSearch.best_estimator_\nprint('tuned XGB: ', xgbTuned)\n\nif xgbMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': xgbTuned,\n        'modelFamily': 'XGB',\n        'cv': xgbMean\n    }","c466cd9b":"svm = SVC(random_state=SEED, probability=True)\n\nsvmConfig = {\n    'C': np.arange(1e-3, 10),\n    'gamma': ['scale', 'auto']\n}\n\nsvmRandomSearch = (\n    RandomizedSearchCV(\n        svm, \n        svmConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nsvmRandomSearch.fit(Xtrain, Ytrain)\n\nsvmMean = svmRandomSearch.best_score_\nprint('mean accuracy for svm: ', svmMean)\n\nsvmTuned = svmRandomSearch.best_estimator_\nprint('tuned SVM: ', svmTuned)\n\nif svmMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': svmTuned,\n        'modelFamily': 'SVM',\n        'cv': svmMean\n    }","4d9aaa16":"# model definition\nnn = MLPClassifier(random_state=SEED, early_stopping=True)\n\n# RandomizedSearchCV\nnnConfig = {\n    'hidden_layer_sizes': [(2 ** i,) for i in np.arange(2, 7)], # just one hidden layer\n    'alpha': [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1e-0, 1e2],\n    'learning_rate': ['constant', 'adaptive']\n}\n\nnnRandomSearch = (\n    RandomizedSearchCV(\n        nn, \n        nnConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, \n        random_state=SEED\n    )\n)\n\nnnRandomSearch.fit(Xtrain, Ytrain)\n\nnnMean = nnRandomSearch.best_score_\nprint('mean accuracy for nn: ', nnMean)\n\nnnTuned = nnRandomSearch.best_estimator_\nprint('tuned NN: ', nnTuned)\n\nif nnMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': nnTuned,\n        'modelFamily': 'NN',\n        'cv': nnMean\n    }","7b3afe0c":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # 1. Importance Sampling dataset\n# Ytrain70ImportanceNotEncoded = adultTrain70Importance.pop(\"target\").values\n# print('Ytrain70ImportanceNotEncoded:', Ytrain70ImportanceNotEncoded) \n# Ytrain70Importance = le.fit_transform(Ytrain70ImportanceNotEncoded)\n# print('Ytrain70Importance:', Ytrain70Importance) ## remove afterwards\n# Xtrain70Importance = adultTrain70Importance.values\n\n# # for cross validation \n# # k-fold cross validation is not done here because, the duplicated rows would leak to the cv sets\n# #Ytrain30ImportanceNotEncoded = adultTrain30Importance.pop(\"target\")\n# Ytrain30Importance = le.fit_transform(Ytrain30ImportanceNotEncoded)\n# Xtrain30Importance = adultTrain30Importance.values\n\n# # shape check\n# print(Xtrain70Importance.shape)\n# print(Xtest.shape) \n# print(Ytrain70Importance.shape)\n\n# # 5 is the deafult n_neighbors\n# importanceSamplingKnn = KNeighborsClassifier(n_neighbors=int(5*knnNeighboursFactor))\n# importanceSamplingKnn.fit(Xtrain70Importance, Ytrain70Importance)\n# Ytrain30Prediction = importanceSamplingKnn.predict(Xtrain30Importance)\n# print('Ytrain30Prediction', Ytrain30Prediction) ### remove afterwards\n# importanceSamplingKnnAccMean = accuracy_score(Ytrain30Importance, Ytrain30Prediction)\n\n# print('mean accuracy for importanceSamplingKnnAccMean: ', importanceSamplingKnnAccMean)\n# if importanceSamplingKnnAccMean > currentBestModel['cv']:  \n#     # get whole dataset fro promisingDatasets\n#     adultTrainImportance = promisingDatasets[\"importanceSampling\"] \n    \n#     YtrainImportance = le.fit_transform(adultTrainImportance.pop(\"target\").values)\n#     XtrainImportance = adultTrainImportance.values\n        \n#     currentBestModel = {\n#         'model': importanceSamplingKnn,\n#         'cv': importanceSamplingKnnAccMean,\n#         'X': XtrainImportance,\n#         'Y': YtrainImportance\n#     }","27958e29":"bestFamily = currentBestModel['modelFamily']\ntrainedBestModel = currentBestModel['model']\nmeanAcc = currentBestModel['cv']\n\nprint('best family: ', bestFamily)\nprint('best model: ', trainedBestModel)\nprint('best cv mean accuracy: ', meanAcc)\n\npredictions = trainedBestModel.predict(Xtest) # numpy array","21d7797d":"# going back to array of strings <=50 K and >50K\npredictions = le.inverse_transform(predictions)\nsubmissionDf = pd.DataFrame({'Id': idTest.values, 'income': predictions})\n\nsubmissionDf.to_csv(\"submission.csv\", index=False)","25257670":"# Table of contents\n1. [Setup and imports](#Setup-and-imports)\n    1. [Libraries](##Libraries)\n    2. [Setup](##Setup)\n2. [EDA (Exploratory Data Analysis)](#EDA(Exploratory-Data-Analysis))\n    1. [Glance at data](##Glance-at-data)\n        1. [Train dataset](###Train-dataset)\n        2. [Test dataset](###Test-dataset)\n    2. [Summary statistics](##Summary-statistics)\n    3. [Target histogram](##Target-histogram)\n    4. [Non zero counts](##Non-zero-counts)\n    5. [Empirical distribution of features](##Empirical-distribution-of-features)\n        1. [Train dataset](###Train-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        2. [Test dataset](###Test-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        3. [Plots of target vs features](###Plots-of-target-vs-features)\n            1. [Numerical features](####Numerical-features)\n            2. [Categorical features](####Categorical-features)\n        4. [Pairwise plots](###Pairwise-plots)\n            1. [Scatter plot](####Numerical-vs-numerical)\n            2. [Correlation heatmap](####Correlation-heatmap)\n            3. [Categorical heatmap](####Categorical-heatmap)\n3. [Data engineering](#Data-engineering)\n    1. [Divide dataset into numerical and categorical subdatasets](##Divide-dataset-into-numerical-and-categorical-subdatasets)\n    1. [Normalize features](##Normalize-features)\n    2. [Treat categorical features](##Treat-categorical-features)\n    3. [Joining numerical and categorical dfs back](##Joining-numerical-and-categorical-dfs-back)\n    4. [Treat missing values](##Treat-missing-values)\n    5. [Treat outliers](##Treat-outliers)\n    6. [Feature tranformations](##Feature-tranformations)\n    7. [Mirror on test dataset](##Mirror-on-testdataset)\n4. [Feature Engineering](#Featur-engineering)\n    1. [Importance sampling](##Importance-sampling)\n    2. [Select features](##Select-features)\n    3. [Create new features](##Create-new-features)\n5. [Experiments](#Experiments)\n    1. [Base dataset](##Base-dataset)\n    2. [Baseline (KNN)](##Baseline-(KNN))\n    3. [4 Classifiers](##4-Classifiers)\n        1. [RF](###RF)\n        2. [XGBoost](###XGBoost)\n        3. [SVM](###SVM)\n        4. [NN](###NN)\n    4. [Engineered datasets](##Engineered-datasets)\n6. [Final model](#Final-model)\n7. [Submission](#Submission)","f22d64fe":"## Treat categorical features","70811a49":"### Numerical features","0597227a":"## Feature tranformations","024d4f7d":"# EDA (Exploratory Data Analysis)\n### Get to know data and draw insights on the problem of classifying income as > 50K.","fbe741ca":"## Importance sampling","3065086e":"### Test dataset","57e6de22":"### Categorical features","9df28c7b":"## Engineered datasets","d4a8b908":"### Test dataset","6a0d2b93":"### XGBoost","bbb759e5":"## Summary statistics","3678b0d4":"### SVM","2effad3f":"## Libraries","d017e162":"### Train dataset","746356d5":"# Data engineering\n### Prepare data for algorithm.","7dbedb32":"#### Histograms of numerical features","6f016315":"## Categorical heatmap","f8a83faa":"##  Target histogram","d282bd4b":"#### Bar plots for categorical features","09cecf56":"## Base dataset","9afc0379":"#### Bar plots for categorical features","43ae4a77":"## Correlation plot","5534046e":"## Treat missing values","dcd65872":"## Normalize features","5a8dc0ef":"### RF","f8c782ea":"## Plots of target vs features","e66b67b6":"## Mirror on test dataset","c966dc87":"### NN","69c215c4":"## Baseline (KNN)","91265255":"## Plot empirical distribution of each feature","666ea006":"## Setup","676a68f5":"#### Histograms of numerical features","bcbae713":"## 4 Classifiers","e30cd6c8":"## Joining numerical and categorical dfs back","9e94f850":"## Select features","ed38e126":"## Glance at data","8547b67f":"### Numerical vs numerical","ff8bf3ac":"### Train dataset","49ed7a11":"## Pairwise plots","9f40d1bd":"# Feature Engineering\n### Select and\/or create new features. Non-linear transformations affect more KNN performance","76715376":"# Submission\n### Save to csv in the required format.","d6564791":"## Divide dataset into numerical and categorical subdatasets","3d71e5b6":"# Setup and imports\n### Setup environment and import libraries.","6a89d676":"## Non zero counts","9548d7d7":"## Create new features","940e8433":"- University: University of S\u00e3o Paulo (USP) \n\n- Class: PMR3508 (2021) - Fundamentals of Machine Learning\n\n- Kaggle Competition: Adult","bce5d36d":"# Final model\n### Trained on entire train dataset.","943f254b":"# Experiments\n### Tune and compare 4 different classifiers. These are: Decision Trees (RF and XGBoost), SVM and NN.","4d35ddad":"## Treat outliers"}}