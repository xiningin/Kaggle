{"cell_type":{"c97bacb6":"code","dc18dd31":"code","dbeaaa39":"code","7c0531c2":"code","5eb37205":"code","0d03cb84":"code","2955cdf6":"code","d1e169ca":"code","a2eacf03":"code","b5a13b46":"code","7fbcb8c9":"code","a7545a06":"code","5e8fef74":"code","a9e437ad":"code","7147de4a":"code","cc263a7e":"code","785d5232":"code","fe8a2605":"code","5737c64a":"code","6c5af25e":"code","6e8a1fe7":"code","b9edb455":"code","6a8bae6e":"code","b8b884f4":"code","787aecae":"code","1ed3eec6":"code","5a7d2dea":"code","6af02141":"code","8d4c9e0e":"code","4b91f361":"code","b6ff76e1":"code","9529c569":"code","26dbb6aa":"code","0c35250e":"code","e813a548":"markdown","5e17433a":"markdown","b091ded8":"markdown","7f2c10e4":"markdown","072bc471":"markdown","95e26bb1":"markdown","5c9014be":"markdown","56117c03":"markdown","fd40d708":"markdown","8fdc2ad3":"markdown","2b781df4":"markdown","2d580c85":"markdown","0cca1df8":"markdown","2b7f85b2":"markdown","1810819f":"markdown","0ca3fe8b":"markdown","d5f47d6d":"markdown","52fb33b2":"markdown","2e8475c6":"markdown","cb06283a":"markdown","37090bf1":"markdown","d0d1adf3":"markdown"},"source":{"c97bacb6":"import os\nimport pandas as pd\nimport random\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport category_encoders\nfrom sklearn.decomposition import PCA","dc18dd31":"os.chdir(\"\/kaggle\/input\/ieee-fraud-detection\")\nos.listdir()","dbeaaa39":"path = ''\n# import the data\ntrain_identity = pd.read_csv(path + 'train_identity.csv')\ntrain_transaction = pd.read_csv(path + 'train_transaction.csv')\ntest_identity = pd.read_csv(path + 'test_identity.csv')\ntest_transaction = pd.read_csv(path + 'test_transaction.csv')\n\n# merge identity and transaction to one dataframe\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n# train_raw, test_raw = train.copy(), test.copy()\n\ndel train_identity, train_transaction, test_identity, test_transaction","7c0531c2":"sampleIdx = random.sample([i for i in range(train.shape[0])], k=5000)\ntrain = train.iloc[sampleIdx, :]\ntrain.head()","5eb37205":"print(f'Training dataset has {train.shape[0]} observations and {train.shape[1]} features.')\nprint(f'Test dataset has {test.shape[0]} observations and {test.shape[1]} features.')","0d03cb84":"Ytr = train[\"isFraud\"]\nX = train.drop([\"isFraud\",\"TransactionID\", \"TransactionDT\"], axis=1).append(test.drop([\"TransactionID\", \"TransactionDT\"], axis=1))\n# X_raw, Ytr_raw = X.copy(), Ytr.copy()","2955cdf6":"# proporation of missing values\nmissPropor = [X[col].isnull().sum() \/ X.shape[0] for col in X.columns]\nplt.hist(missPropor, bins=30)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Proportion of missing values\")\nplt.show()","d1e169ca":"# delete features with high proporation of missing values\nmany_null_cols = [X.columns[i] for i in range(X.shape[1]) if missPropor[i] > 0.7]\nX = X.drop(many_null_cols, axis=1)\nprint(f\"After deleting features with high proporation of missing values, there are {X.shape[1]} features.\")","a2eacf03":"# fill missing values in categorical variables with their mode.\n# fill missing values in numerical variables with their mean.\nfor i in range(X.shape[1]):\n    if missPropor[i] > 0:\n        if X.iloc[:, i].dtype == \"object\":\n            X.iloc[:, i] = X.iloc[:, i].fillna(X.iloc[:, i].mode()[0])\n        elif X.iloc[:, i].dtype in ['int64', 'float64']:\n            X.iloc[:, i] = X.iloc[:, i].fillna(X.iloc[:, i].mean())","b5a13b46":"# numeric encoding (label encoding)\nX_le = X.copy()\nfor f in X.columns:\n    if X_le[f].dtype == 'object': \n        le = preprocessing.LabelEncoder()\n        le.fit(list(X_le[f].values))\n        X_le[f] = le.transform(list(X_le[f].values))","7fbcb8c9":"# binary encoding\nX_be = X.copy()\nfor f in X.columns:\n    if X_be[f].dtype == 'object': \n        if X_be[f].nunique() <= 2:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(X_be[f].values))\n            X_be[f] = le.transform(list(X_be[f].values))\n        else:\n            be = category_encoders.BinaryEncoder(cols=f)\n            X_be = be.fit_transform(X_be)","a7545a06":"X_le_pca = X_le.copy()","5e8fef74":"# standardize the data\nscaler = preprocessing.StandardScaler()\nscaler.fit(X_le_pca)\nX_le_pca = scaler.transform(X_le_pca)","a9e437ad":"# apply PCA\n# choose the minimum number of principal components \n# such that 99% of the variance is retained.\npca = PCA(0.99)\npca.fit(X_le_pca)\nX_le_pca = pca.transform(X_le_pca)\nX_le_pca = pd.DataFrame(X_le_pca)","7147de4a":"print(f\"Number of features after PCA is {X_le_pca.shape[1]}.\")","cc263a7e":"Xtr_le = X_le.iloc[:train.shape[0], :]\nXte_le = X_le.iloc[train.shape[0]:, :]\nXtr_be = X_be.iloc[:train.shape[0], :]\nXte_be = X_be.iloc[train.shape[0]:, :]\nXtr_le_pca = X_le_pca.iloc[:train.shape[0], :]\nXte_le_pca = X_le_pca.iloc[train.shape[0]:, :]","785d5232":"# Xtr_le.to_csv(f\"{path}X_train_labelencoding.csv\", index=False)\n# Xte_le.to_csv(f\"{path}X_test_labelencoding.csv\", index=False)\n# Xtr_be.to_csv(f\"{path}X_train_binaryencoding.csv\", index=False)\n# Xte_be.to_csv(f\"{path}X_test_binaryencoding.csv\", index=False)\n# Xtr_le_pca.to_csv(f\"{path}X_train_labelencoding_pca.csv\", index=False)\n# Xte_le_pca.to_csv(f\"{path}X_test_labelencoding_pca.csv\", index=False)\n# Ytr.to_csv(f\"{path}Y_train.csv\", header=\"isFraud\", index=False)","fe8a2605":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb","5737c64a":"# path = '\/home\/wkm\/Documents\/Data set\/ieee-fraud-detection\/'\n# Xtr = pd.read_csv(f\"{path}X_train_binaryencoding.csv\")\n# Xte = pd.read_csv(f\"{path}X_test_binaryencoding.csv\")\n# Ytr = pd.read_csv(f\"{path}Y_train.csv\")","6c5af25e":"Xtr = Xtr_be\nXte = Xte_be","6e8a1fe7":"test_transaction = pd.read_csv(path + 'test_transaction.csv')\nsubmission = pd.DataFrame(test_transaction[\"TransactionID\"])\ndel test_transaction","b9edb455":"lr = LogisticRegression(penalty='l2', max_iter=500, n_jobs=6, tol=1e-6, solver=\"sag\")\nlr.fit(Xtr, np.ravel(Ytr))\nYhat_lr = lr.predict_proba(Xte)\nsubmission[\"isFraud\"] = Yhat_lr[:, 1]\n# submission.to_csv(f\"{path}Y_hat_logistic.csv\", index=False)","6a8bae6e":"treeCount = 100\n\nbagging = RandomForestClassifier(max_features=\"auto\", min_samples_leaf=1, n_estimators=treeCount)\nbagging.fit(Xtr, np.ravel(Ytr))\nYhat_bagging = bagging.predict_proba(Xte)\nsubmission[\"isFraud\"] = Yhat_bagging[:, 1]\n# submission.to_csv(f\"{path}Y_hat_bagging.csv\", index=False)","b8b884f4":"# use oob error to find the best max_features\nnFeatures = Xtr.shape[1]\noobErrList = list()\nmList = [m for m in range(10, nFeatures+1, 30)]\n\nfor m in mList:\n    rf = RandomForestClassifier(max_features=m, min_samples_leaf=1,\\\n                                oob_score=True, n_estimators=50)\n    rf.fit(Xtr, np.ravel(Ytr))\n    oobErrList.append(1-rf.oob_score_)\n    print(m, 1-rf.oob_score_)","787aecae":"print(oobErrList)\nplt.plot([m for m in range(10, nFeatures+1, 30)], oobErrList)\nplt.ylabel('OOB error with (n_estimators=50)')\nplt.xlabel('m, the number of variables considered at each split')\nplt.show()","1ed3eec6":"treeCount = 1000\nm = 15\n\nrf = RandomForestClassifier(max_features=m, min_samples_leaf=1, n_estimators=treeCount)\nrf.fit(Xtr, np.ravel(Ytr))\nYhat_rf = rf.predict_proba(Xte)","5a7d2dea":"submission[\"isFraud\"] = Yhat_rf[:, 1]\n# submission.to_csv(f\"{path}Y_hat_rf_m{m}_t{treeCount}.csv\", index=False)","6af02141":"# use default parameters\ngbm0 = GradientBoostingClassifier()\ngbm0.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = gbm0.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_gbm_default.csv\", index=False)","8d4c9e0e":"param_test1 = {'n_estimators':range(100, 1200, 100), 'learning_rate':[0.01, 0.1, 1]}\ngbm_tune1 = GradientBoostingClassifier(max_features='sqrt', min_samples_leaf=0.001, max_depth=4)\n\ngs1 = GridSearchCV(estimator=gbm_tune1, param_grid=param_test1, iid=False, scoring='roc_auc', n_jobs=6, cv=5)\ngs1.fit(Xtr, np.ravel(Ytr))","4b91f361":"print(f\"The best parameters: {gs1.best_params_}, and the highest mean_test_score is {gs1.best_score_}\")","b6ff76e1":"param_test2 = {'max_depth':range(2, 16, 2), 'min_samples_leaf':[10**i for i in range(-5,0)]}\ngbm_tune2 = GradientBoostingClassifier(max_features='sqrt', n_estimators=100, learning_rate=0.1)\n\ngs2 = GridSearchCV(estimator=gbm_tune2, param_grid=param_test2, iid=False, scoring='roc_auc', n_jobs=6, cv=5)\ngs2.fit(Xtr, np.ravel(Ytr))","9529c569":"print(f\"The best parameters: {gs2.best_params_}, and the highest mean_test_score is {gs2.best_score_}\")","26dbb6aa":"# use tuned parameters\ngbm1 = GradientBoostingClassifier(max_depth=10, min_samples_leaf=0.001, \n                                  learning_rate=0.1, n_estimators=100)\ngbm1.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = gbm1.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_gbm_tuned1.csv\", index=False)","0c35250e":"xgbc = xgb.XGBClassifier(n_jobs=4, max_depth=10, min_samples_leaf=0.001, \n                         learning_rate=0.1, n_estimators=100, eval_metric=\"auc\")\nxgbc.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = xgbc.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_xgb_tuned5.csv\", index=False)","e813a548":"### 1.3 Encode categorical variables\n\nIn the following part, I tried two different ways: numeric encoding and binary encoding. [Here](https:\/\/medium.com\/data-design\/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931) is the reason why I did that.\n\nNumeric encoding (label encoding) simply assigns a value to each category. Binary encoding hashes the cardinalities into binary values.","5e17433a":"I use RandomForestClassifier and set max_features=\"auto\", which means max_features=n_features, so it is bagging.","b091ded8":"### 1.4 PCA to reduce dimension\n\nAlthough PCA can reduce dimension and computation time, the result showed that the prediction performance with PCA is worse. So I will not use the data after PCA to fit the model and make predictions in the modeling part.","7f2c10e4":"Now let's do parameter tunning. The parameters n_estimators and learning_rate are corrlated, so we need to tune them together. Let's use grid search to find the best number of weak learners (n_estima****tors) and the best step size (learning_rate). ","072bc471":"### 1.5 Save processed data","95e26bb1":"Other than max_features (the number of variables considered at each split), the parameter n_estimators (the number of trees) is also important. However, the prediction performance will increase with the increase of n_estimators, so we should select the highest n_estimators as long as our machine can compute it. \n\nI tried different parameters (max_features and n_estimators) and get the prediction scores (evaluted by AUC) as following.\n\n|max_features | n_estimators | PCA | prediction score |\n|---|---|---|---|\n| 223 | 100 | 99%  | 0.871838 |\n| 190 | 100 | 100% | 0.892415 |\n| 100 | 100 | 100% | 0.894553 |\n| 50  | 100 | 100% | 0.895868 |\n| 223 | 100 | 100% | 0.896448 |\n| 90  | 200 | 100% | 0.898140 |\n| 100 | 200 | 100% | 0.899070 |\n| 50  | 200 | 100% | 0.900798 |\n| 15  | 1000| 100% | 0.904874 |\n\nFrom the table above, we can see that max_features does not infulence the prediction performance significantly, but n_estimators does.","5c9014be":"After parameters tuning, the prediction score (by AUC) on test data set is 0.919523. It is beeter than that without parameters tuning.","56117c03":"### 1.1 Import and merge the data","fd40d708":"At first, let's try gradient boosting with default parameters. The prediction scroe on test data set is 0.891210.","8fdc2ad3":"## 1. Data preprocessing","2b781df4":"### 2.1 Logistic regression","2d580c85":"The prediction score is 0.713617.","0cca1df8":"The prediction scroe on test data set by XGBoost with default parameters is 0.900976, and 0.931355 with the parameters tuned by GBT (see section 2.4 Gradient boosting).","2b7f85b2":"The computation on the whole data set is too expensive, so I randomly select a subset of size 5000 for demo. Note that you should not run the code in following cell if you want to train a precise model.","1810819f":"## 2. Modeling","0ca3fe8b":"### 2.3 Random forests","d5f47d6d":"### 1.2 Handle missing values\n\nAt first, I dropt features with high proporation (70%) of missing values. Then, I filled missing values in categorical variables with their mode, and filled missing values in numerical variables with their mean.","52fb33b2":"### 2.2 Bagging trees","2e8475c6":"However, I found that n_estimators=600 is unaffordable for computation, so I still set it to 100. Now let's tune the tree parameters max_depth and min_samples_leaf.","cb06283a":"### 2.4 Gradient boosting","37090bf1":"### 2.5 XGBoost","d0d1adf3":"Let's first do parameter tunning."}}