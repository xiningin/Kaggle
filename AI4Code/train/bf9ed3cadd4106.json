{"cell_type":{"cc6bab71":"code","07d1c39c":"code","c4eadf60":"code","49b8458e":"code","430661d9":"code","da496bc8":"code","ad382481":"code","70940b50":"code","fe5d2208":"code","25dd9284":"code","abc7690d":"code","1ce11ac6":"code","6fcbb1a9":"code","70e2cf83":"code","a52d7e7d":"code","e5e16add":"code","3663b54e":"markdown","fe8bc10f":"markdown","d967525a":"markdown","1e12711f":"markdown","e0cda028":"markdown","11bf075e":"markdown","1345536b":"markdown","62e4e7e9":"markdown"},"source":{"cc6bab71":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","07d1c39c":"df = pd.read_csv(\"..\/input\/social-network-ads\/Social_Network_Ads.csv\")\ndf.head()","c4eadf60":"X = df[['Age','EstimatedSalary']].values\nprint(X.shape)","49b8458e":"y = df[['Purchased']].values\nprint(y.shape)","430661d9":"from sklearn.model_selection import train_test_split\nx_train , x_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, random_state = 0)","da496bc8":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","ad382481":"X1 = x_train.T\nX1.shape","70940b50":"Y1 = y_train.T\nY1.shape","fe5d2208":"def sigmoid(z):\n    s = 1 \/ (1+np.exp(-z))\n    return s","25dd9284":"def initialize(dim):\n    w = np.zeros((dim,1))\n    b = 0.0\n    \n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","abc7690d":"def propagate(w, b, X, Y):\n    \n    m = X.shape[1]\n    z = np.dot(w.T, X) + b\n    A = sigmoid(z)\n    \n    cost = -1\/m * np.sum((Y * np.log(A)) + ((1-Y) * np.log(1 - A)))\n    \n    dz = (1\/m) * (A - Y)\n    dw = np.dot(X, dz.T)\n    db = np.sum(dz)\n    \n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","1ce11ac6":"def optimize(w, b, X, Y, num_iterations, learning_rate):\n    costs = []\n    \n    for i in range(num_iterations):   \n        grads, cost = propagate(w, b, X, Y)\n\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        w = w - (learning_rate * dw)\n        b = b - (learning_rate * db)\n        \n        if i % 10 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 10 training iterations\n        if i % 10 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n    ","6fcbb1a9":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T, X) + b)\n    \n    for i in range(m):\n        if A[0][i] > 0.5:\n            Y_prediction[0][i] = 1\n        else:\n            Y_prediction[0][i] = 0\n            \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","70e2cf83":"def model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.5):\n    \n    # Initialize the parameters\n    w, b = initialize(x_train.shape[0])\n    \n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, x_train, y_train, num_iterations, learning_rate)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test\/train set examples (\u2248 2 lines of code)\n    Y_prediction_train = predict(w, b, x_train)\n    Y_prediction_test = predict(w, b, x_test)\n    \n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","a52d7e7d":"X2 = x_test.T\nY2 = y_test.T\nd = model(X1, Y1, X2, Y2, num_iterations = 100, learning_rate = 0.1)","e5e16add":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per tenth)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","3663b54e":"### 4.Prediction","fe8bc10f":"### Sigmoid Function","d967525a":"# B.Building Neural Network","1e12711f":"## 1.Initialising Parameters","e0cda028":"# A.Preparing Data","11bf075e":"### 3.Optimization Using Gradient Descent","1345536b":"## 2.Forward Propagation and Gradient Calculation","62e4e7e9":"### 5. Combining all functions into  a model"}}